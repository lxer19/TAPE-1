URL: http://www.research.att.com/~dalia/pubs/omissionfd.ps.gz
Refering-URL: http://www.research.att.com/~dalia/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: dolev@cs.huji.ac.il  Email: roy@cs.cornell.edu  Email: idish@cs.huji.ac.il  Email: dalia@research.att.com  
Title: Failure Detectors in Omission Failure Environments  
Author: Danny Dolev Roy Friedman Idit Keidar Dahlia Malkhi 
Note: L AT&T Labs-Research TR 96-16-1 This work was supported by ARPA/ONR grants N00014-92-J-1866 and F30602-95-1-0047, and by grants from IBM, Siemens, and GTE Corporations and by the Ministry of Science of Israel, grant number 0327452.  
Date: September 20, 1996  
Address: Jerusalem.  Jerusalem.  Murray Hill, NJ 07974.  
Affiliation: Computer Science Institute, The Hebrew University of  Department of Computer Science, Cornell University.  Computer Science Institute, The Hebrew University of  AT&T Labs,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Y. Afek, H. Attiya, A. Fekete, M. Fischer, N. Lynch, Y. Mansour, W. Dai-Wei, and L. Zuck. </author> <title> Reliable communication over unreliable channels. </title> <journal> Journal of the ACM, </journal> <volume> 41(6) </volume> <pages> 1267-1297, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The logarithmic space requirement stems from our lack of assumption on message ordering. Note that there can exist no constant memory protocols providing reliable multicast over an unreliable network that can re-order messages <ref> [1] </ref>. In our solution, the memory requirements increase logarithmically due to the use of monotonic counters. In any practical execution, 64 or 128 bits of memory will be more than enough. Therefore our solution is reasonable for practical use.
Reference: [2] <author> O. Babaoglu, R. Davoli, and A. Montresor. </author> <title> Failure detectors, group membership and view-synchronous communication in partitionable asynchronous systems. </title> <type> Technical Report UBLCS-95-18, </type> <institution> Department of Computer Science, University of Bologna, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Babaoglu, Davoli, and Montresor also define failure detectors for omission failure environments and use them to develop protocols for view-synchronous communication <ref> [2] </ref>. Their definitions of permanent reachability and unreachability require, like ours, eventual stability of the system, but unlike ours, capture pairwise connectivity only.
Reference: [3] <author> Tushar Deepak Chandra, Vassos Hadzilacos, and Sam Toueg. </author> <title> The weakest failure detector for solving consensus. </title> <journal> Journal of the ACM. </journal> <note> To appear. </note>
Reference-contexts: Therefore our solution is reasonable for practical use. Finally, we prove minimality of 3W (om) for solving Consensus in our environment using similar arguments as the ones used in proving the minimality of 3W in the crash-failure environment model (see <ref> [3] </ref>). 1.1 Related Work Guerraoui and Schiper introduced the notion of "-Accurate" failure detectors in [8]. Their definitions are provided, again, in the crash failure model only. More precisely, they assume that messages between any pair of live processes are eventually received. <p> Theorem 5.1 3W (om) is the weakest failure detector allowing Consensus to be solved in an asynchronous model with message omissions with up to b n 2 c faulty process. 14 Proof: (Sketch) In <ref> [3] </ref>, Chandra, Hadzilacos and Toueg prove that 3W is the weakest failure detector for solving Consensus in an asynchronous environment with crash failures. <p> Therefore, the same reduction may be applied to reduce any failure detector D (om) that solves Consensus in our model, reducing it to 3W (om). Note that the notion of comparing between failure detectors using the implements relation, as defined by Chandra et al. <ref> [3] </ref>, does not take into account the space-complexity of the reduction. Therefore, a failure detector may exist, that requires a linear space (or more) implementation to implement 3W (om), but is nevertheless considered no weaker than 3W (om).
Reference: [4] <author> Tushar Deepak Chandra and Sam Toueg. </author> <title> Unreliable failure detectors for reliable distributed systems. </title> <journal> Journal of the ACM, </journal> <volume> 43(2) </volume> <pages> 225-267, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: Examples of services that are defined using failure detectors include Consensus [12], atomic commit protocols [7, 9] and primary component membership [11] services. In a seminal paper, Chandra and Toueg categorized failure detectors according to their degree of accuracy and completeness <ref> [4] </ref>. In particular, they defined an eventual weak failure detector 3W that can make an infinite number of mistakes but eventually some restrictions on its behavior hold. They present a protocol for solving Consensus using 3W. The failure detectors definitions and protocols presented in [4] are described in a model that <p> their degree of accuracy and completeness <ref> [4] </ref>. In particular, they defined an eventual weak failure detector 3W that can make an infinite number of mistakes but eventually some restrictions on its behavior hold. They present a protocol for solving Consensus using 3W. The failure detectors definitions and protocols presented in [4] are described in a model that is limited to crash failures only and cannot be trivially extended to allow message omissions. <p> Note also that, while stability has to formally hold forever, in any execution of our algorithms it needs to hold only for a finite time (though no prior bound on it exists). We define classes of failure detectors in our environment. Our definitions follow closely the ones given in <ref> [4] </ref>. In particular, we define the eventually weak failure detector class, 3W (om). <p> We begin by presenting a naive adaptation of the protocol in <ref> [4] </ref> to our environment. The naive protocol overcomes message loss by re-sending all past messages at each round. Unfortunately, this approach results in buffering requirements that grow linearly in the total size of exchanged data. <p> Their definitions are provided, again, in the crash failure model only. More precisely, they assume that messages between any pair of live processes are eventually received. Like the model in <ref> [4] </ref>, over an unreliable network, this assumption requires unbounded message re-transmission capability. In a practical environment with message losses, in which live processes may omit an unbounded number of messages, the main reduction presented in [8] does not work. <p> Using our definition, if a majority of the processes eventually becomes permanently connected, they are considered correct. This allows us to extend the definition of failure detectors of Chandra and Toueg <ref> [4] </ref> from a crash-failure asynchronous environment to a partitionable environment, in a natural way. It is important to note that in environments in which messages never get lost, the entire set of non crashed processes forms a connected component. <p> We reduce 3W (om) to 3S (om) using the algorithm suggested by Chandra and Toueg for reducing 3W to 3S <ref> [4] </ref>, described in Figure 2. We prove below that this algorithm reduces 3W (om) to 3S (om). We denote the given (input) failure detector by W, and the resulting (output) failure detector by S. <p> However, since we have shown how to implement 3S (om) from 3W (om), then the protocols we describe work with 3W (om) by superimposing the reduction from 3W (om) to 3S (om) onto them. 8 4.1 Consensus with 3S (om) Naive Solution Chandra and Toueg <ref> [4] </ref> suggested an algorithm that solves Consensus with a failure detector S 2 3S if there are no more than b n 2 c crash-failures. <p> Therefore, this protocol is not reasonable for use in practice. Theorem 4.1 The protocol given in <ref> [4] </ref> with the modifications above solves Consensus in the omission failure environment extended with a failure detector S 2 3S (om), given that a majority of the processes are correct. Proof: (Sketch) The protocol in [4] uses a majority to guarantee against inconsistent de cisions. <p> Theorem 4.1 The protocol given in <ref> [4] </ref> with the modifications above solves Consensus in the omission failure environment extended with a failure detector S 2 3S (om), given that a majority of the processes are correct. Proof: (Sketch) The protocol in [4] uses a majority to guarantee against inconsistent de cisions. Therefore, it is only left to show that the protocol with the above modifications guarantees termination. Assume to the contrary that there is an infinite run of the protocol above. <p> Therefore, we can construct an infinite run in the crash failure model by stopping all the incorrect processes at the point of detachments. The runs are indistinguishable to the correct processes, and therefore it does not terminate in the crash failure model as well. This contradicts the results of <ref> [4] </ref>. 4.2 Consensus with 3S (om) Practical Solution In this section, we offer an alternative Consensus algorithm to the one above, but whose memory requirement is bounded logarithmically in the number of messages sent, and does not require storing nor re-sending of old messages.
Reference: [5] <author> K. M. Chandy and L. Lamport. </author> <title> Distributed snapshots: determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: An execution is a collection of histories, one for each process, in which there is a mapping from each message-receive event to a corresponding message-send event. In this paper we consider only executions in which there are no causal cycles <ref> [5, 10] </ref>. An execution 0 is a sub-execution of another execution if they include histories of the same set of processes, and the history of each process p i in 0 is a prefix of p i 's history in .
Reference: [6] <author> F. Cristian and F. Schmuck. </author> <title> Agreeing on process group membership in asynchronous distributed systems. </title> <type> Technical Report CSE95-428, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <year> 1995. </year>
Reference-contexts: In a practical environment with message losses, in which live processes may omit an unbounded number of messages, the main reduction presented in [8] does not work. In the timed asynchronous model, Cristian and Schmuck <ref> [6] </ref> explicitly use the notions of time and timeouts to state sufficient conditions for reaching agreement on the membership of a group of processes. As discussed in [13], timeouts are only one possible aspect of failure detection. <p> As discussed in [13], timeouts are only one possible aspect of failure detection. Since our definitions and results use failure detectors as an abstract tool, our model is more general than the one presented in <ref> [6] </ref>. As in our model, their protocols are guaranteed to proceed only when the network stabilizes, with the important distinction that, in their work, a stable network delivers every message within a known delay.
Reference: [7] <author> R. Guerraoui and A. Schiper. </author> <title> The decentralized non-blocking atomic commitment protocol. </title> <booktitle> In IEEE International Symp. on Parallel and Distributed Processing (SPDP), </booktitle> <month> October </month> <year> 1995. </year>
Reference-contexts: Failure detectors provide a clear analysis of the effects of failures on the solvability of certain problems in distributed environments. Examples of services that are defined using failure detectors include Consensus [12], atomic commit protocols <ref> [7, 9] </ref> and primary component membership [11] services. In a seminal paper, Chandra and Toueg categorized failure detectors according to their degree of accuracy and completeness [4]. <p> We suggested a practical algorithm that can be used, as a building block, to make various distributed services more fault tolerant. For example, the primary component membership protocol in [11], and the atomic commitment protocol in <ref> [7] </ref> use Chandra and Toueg's Consensus protocol as a building block. Replacing that module with our Consensus protocol will make these services tolerant to omission faults. Acknowledgments We are grateful to Sam Toueg and Vassos Hadzilacos for pointing out weaknesses in our previous version.
Reference: [8] <author> R. Guerraoui and A. Schiper. </author> <title> "fl-accurate" failure detectors. </title> <booktitle> In International Workshop on Distributed Algorithms (WDAG), </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Finally, we prove minimality of 3W (om) for solving Consensus in our environment using similar arguments as the ones used in proving the minimality of 3W in the crash-failure environment model (see [3]). 1.1 Related Work Guerraoui and Schiper introduced the notion of "-Accurate" failure detectors in <ref> [8] </ref>. Their definitions are provided, again, in the crash failure model only. More precisely, they assume that messages between any pair of live processes are eventually received. Like the model in [4], over an unreliable network, this assumption requires unbounded message re-transmission capability. <p> Like the model in [4], over an unreliable network, this assumption requires unbounded message re-transmission capability. In a practical environment with message losses, in which live processes may omit an unbounded number of messages, the main reduction presented in <ref> [8] </ref> does not work. In the timed asynchronous model, Cristian and Schmuck [6] explicitly use the notions of time and timeouts to state sufficient conditions for reaching agreement on the membership of a group of processes. As discussed in [13], timeouts are only one possible aspect of failure detection.
Reference: [9] <author> I. Keidar and D. Dolev. </author> <title> Increasing the resilience of atomic commit at no additional cost. </title> <booktitle> In ACM Symp. on Prin. of Database Systems (PODS), </booktitle> <pages> pages 245-254, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Failure detectors provide a clear analysis of the effects of failures on the solvability of certain problems in distributed environments. Examples of services that are defined using failure detectors include Consensus [12], atomic commit protocols <ref> [7, 9] </ref> and primary component membership [11] services. In a seminal paper, Chandra and Toueg categorized failure detectors according to their degree of accuracy and completeness [4].
Reference: [10] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July 78. </month>
Reference-contexts: An execution is a collection of histories, one for each process, in which there is a mapping from each message-receive event to a corresponding message-send event. In this paper we consider only executions in which there are no causal cycles <ref> [5, 10] </ref>. An execution 0 is a sub-execution of another execution if they include histories of the same set of processes, and the history of each process p i in 0 is a prefix of p i 's history in .
Reference: [11] <author> C. Malloth and A. Schiper. </author> <title> View synchronous communication in large scale networks. </title> <booktitle> In Proceedings 2nd Open Workshop of the ESPRIT project BROADCAST (number 6360), </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Failure detectors provide a clear analysis of the effects of failures on the solvability of certain problems in distributed environments. Examples of services that are defined using failure detectors include Consensus [12], atomic commit protocols [7, 9] and primary component membership <ref> [11] </ref> services. In a seminal paper, Chandra and Toueg categorized failure detectors according to their degree of accuracy and completeness [4]. In particular, they defined an eventual weak failure detector 3W that can make an infinite number of mistakes but eventually some restrictions on its behavior hold. <p> We suggested a practical algorithm that can be used, as a building block, to make various distributed services more fault tolerant. For example, the primary component membership protocol in <ref> [11] </ref>, and the atomic commitment protocol in [7] use Chandra and Toueg's Consensus protocol as a building block. Replacing that module with our Consensus protocol will make these services tolerant to omission faults.
Reference: [12] <author> M. Pease, R. Shostak, and L. Lamport. </author> <title> Reaching agreement in the presence of faults. </title> <journal> Journal of the ACM, </journal> <volume> 27(2) </volume> <pages> 228-234, </pages> <year> 1980. </year>
Reference-contexts: 1 Introduction Failure detectors are useful abstractions for specifying services and protocols in a distributed environment prone to failures. Failure detectors provide a clear analysis of the effects of failures on the solvability of certain problems in distributed environments. Examples of services that are defined using failure detectors include Consensus <ref> [12] </ref>, atomic commit protocols [7, 9] and primary component membership [11] services. In a seminal paper, Chandra and Toueg categorized failure detectors according to their degree of accuracy and completeness [4]. <p> below, when we construct a logarithmic-space solution for the Consensus problem using a failure detector in 3S (om), and thus obtain a logarithmic-space solution for Consensus using a failure detector in 3W (om). 4 Consensus with Failure Detectors There are many (equivalent) formulations of the Consensus problem, originally introduced in <ref> [12] </ref>. We say that a protocol solves the Consensus problem if: Agreement All the processes that decide decide on the same value. Non-Triviality The protocol has different executions that decide on at least two different values. Termination All the correct processes eventually decide.
Reference: [13] <author> W. Vogels. </author> <title> World wide failures. </title> <booktitle> In ACM SIGOPS European Workshop, </booktitle> <year> 1996. </year> <note> To appear. 16 </note>
Reference-contexts: In the timed asynchronous model, Cristian and Schmuck [6] explicitly use the notions of time and timeouts to state sufficient conditions for reaching agreement on the membership of a group of processes. As discussed in <ref> [13] </ref>, timeouts are only one possible aspect of failure detection. Since our definitions and results use failure detectors as an abstract tool, our model is more general than the one presented in [6].
References-found: 13

