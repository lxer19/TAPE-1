URL: http://www.cs.cmu.edu/afs/cs/usr/pstone/public/papers/96ieee-survey/survey.ps.Z
Refering-URL: http://www.cs.umbc.edu/agents/introduction/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fpstone,velosog@cs.cmu.edu  
Title: Multiagent Systems: A Survey from a Machine Learning Perspective  
Author: Peter Stone and Manuela Veloso 
Date: February 1997.  
Note: Under review for journal publication.  Machine Learning to MAS are highlighted and robotic soccer is presented as an appropriate testbed for MAS.  
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract: Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally, DAI has focussed on the information management aspects of these systems. But in the past few years, a subfield of DAI focussing on behavior management, as opposedto information management, has emerged. This young subfield is called Multiagent Systems (MAS). This survey of MAS is intended to serve as an introduction to the field and as an organizational framework. It contains guidelines for when and how MAS should be used to build complex systems. A series of increasingly complex general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. When options exist, the techniques presented are biased towards Machine Learning approaches. Additional opportunities for applying 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. A. Pormerleau, </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: In the past several years, AI techniques have become more and more robust and complex. To mention just one of the many exciting successes, a car recently steered itself more than 95% of the way across the United States using the ALVINN system <ref> [1] </ref>. By meeting this and other such daunting challenges, AI researchers have earned the right to start examining the implications of multiple autonomous agents interacting in the real world. In fact, they have rendered this examination indispensable. If there is one self-steering car, there will surely be more.
Reference: [2] <author> S. J. Russell and P. Norvig, </author> <title> Artificial Intelligence: A Modern Approach. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: Multiagent Systems (MAS) is the emerging subfield of AI that aims to provide both principles for construction of complex systems involving multiple agents and mechanisms for coordination of independent agents' behaviors. While there is no generally accepted definition of agent in AI <ref> [2] </ref>, for the purposes of this article, we consider an agent to be an entity with goals, actions, and domain knowledge, situated in an environment.
Reference: [3] <author> G. Wei and S. Sen, eds., </author> <title> Adaptation and Learning in Multiagent Systems. </title> <publisher> Berlin: Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: The techniques presented are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. Because of the inherent complexity of MAS, there is much interest in using Machine Learning techniques to help deal with this complexity <ref> [3, 4] </ref>. When several different systems exist that could illustrate the same or similar MAS techniques, the systems presented here are biased towards those that use Machine Learning (ML) approaches. Furthermore, every effort is made to highlight additional opportunities for applying ML to MAS. <p> One possible definition of DAI uses the less general term, entities: Distributed Artificial Intelligence (DAI) is concerned with the study and design of systems consisting of several interacting entities which are logically and often spatially distributed and in some sense can be called autonomous and intelligent. Gerhard Wei <ref> [3] </ref> Because of the emphasis on information management in the past, DAI has been decomposed into Parallel AI and Distributed Expert Systems, the latter of which was broken into Distributed Knowledge Sources and Distributed Problem Solving (DPS) [7]. <p> However, these days people tend to break DAI systems into one of two classes: DPS and MAS <ref> [3] </ref> (see Figure 1). DPS deals with information management while MAS deals with behavior management. Their intersection arises in systems that use agents to do information management. Thus the boundary between DPS and MAS is not a clear one. <p> MAS is an active field with many open issues. Continuing research is presented at dedicated conferences and workshops such as the International Conference on Multi-Agent Systems <ref> [3, 4, 82] </ref>. MAS work also appears in many of the DAI conferences and workshops. This survey provides a framework within which the reader can situate both existing and future work.
Reference: [4] <author> AAAI, </author> <title> Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> (Menlo Park,CA), </address> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1996. </year> <note> AAAI Technical Report SS-96-01. Sandip SenChair. 36 </note>
Reference-contexts: The techniques presented are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. Because of the inherent complexity of MAS, there is much interest in using Machine Learning techniques to help deal with this complexity <ref> [3, 4] </ref>. When several different systems exist that could illustrate the same or similar MAS techniques, the systems presented here are biased towards those that use Machine Learning (ML) approaches. Furthermore, every effort is made to highlight additional opportunities for applying ML to MAS. <p> MAS is an active field with many open issues. Continuing research is presented at dedicated conferences and workshops such as the International Conference on Multi-Agent Systems <ref> [3, 4, 82] </ref>. MAS work also appears in many of the DAI conferences and workshops. This survey provides a framework within which the reader can situate both existing and future work.
Reference: [5] <author> M. Benda, V. Jagannathan, and R. Dodhiawala, </author> <title> On optimal cooperation of knowledge sources an empirical investigation, </title> <type> Tech. Rep. </type> <institution> BCSG201028, Boeing Advanced Technology Center, Boeing Computing Services, </institution> <address> Seattle, Washington, </address> <month> July </month> <year> 1986. </year>
Reference-contexts: However by first increasing heterogeneity and then communication, all of the important issues and techniques in MAS are encountered. For each multiagent scenario presented, a single example domain is presented in an appropriate instantiation for the purpose of illustration. In this extensively-studied domain, the Predator/Prey or Pursuit domain <ref> [5] </ref>, many MAS issues arise. Nevertheless, it is a toy domain. At the end of the article, a much more complex domainrobotic socceris presented in order to illustrate the full power of MAS. The article is organized as follows. <p> For discussion of a domain that has the full 8 range of complexities characteristic of more real-world domains, see Section 9. The pursuit domain was introduced by Benda et al. <ref> [5] </ref>. Over the years, researchers have studied several variations of its original formulation. In this section, a single instantiation of the domain is presented. However, care is taken to point out the parameters that can be varied. The pursuit domain is usually studied with four predators and one prey. <p> Observe that by taking MAS to the extreme of full communication, we may arrive at a single agent system. Benda et al., in the original presentation of the pursuit domain, also consider the full range of communication possibilities, all the way up to the central strategy <ref> [5] </ref>. They consider the possible organizations of the four predators when any pair can either exchange data, exchange data and goals, or have one control the other. The tradeoff between lower communication costs and better decisions is described.
Reference: [6] <author> V. R. Lesser, </author> <title> Multiagent systems: An emerging subdiscipline of AI, </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 27, </volume> <pages> pp. 340342, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: The resulting 3 research became known as DAI. It was distinct from traditional Distributed Computing in that it focussed on problem solving, communication, and coordination as opposed to the lower level parallelization and synchronization issues <ref> [6] </ref>. DAI systems do not necessarily involve agents. <p> From this perspective, the important characteristics of MAS are: * System function; * Agent architecture (degree of heterogeneity, reactive vs. deliberative); * System architecture (communication, protocols, human involvement). A useful contribution is that the dimensions are divided into agent and system characteristics. Other overviews of DAI and/or MAS include <ref> [6, 9, 10] </ref>. The taxonomy presented in this article is organized along the most important aspects of agents (as opposed to domains): degree of heterogeneity and degree of communication.
Reference: [7] <author> K. S. Decker, </author> <title> Distributed problem solving: A survey, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 17, </volume> <pages> pp. 729740, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: Gerhard Wei [3] Because of the emphasis on information management in the past, DAI has been decomposed into Parallel AI and Distributed Expert Systems, the latter of which was broken into Distributed Knowledge Sources and Distributed Problem Solving (DPS) <ref> [7] </ref>. However, these days people tend to break DAI systems into one of two classes: DPS and MAS [3] (see Figure 1). DPS deals with information management while MAS deals with behavior management. Their intersection arises in systems that use agents to do information management. <p> Table 1: Definitions of MAS and its related fields. 2.2 Intrafield Taxonomy Several taxonomies have been presented previously for the related field of DAI. One example is Decker's division mentioned above <ref> [7] </ref>. In this same survey of DPS, Decker presents four dimensions of DAI: 4 1. Agent granularity (coarse vs. fine); 2. Heterogeneity of agent knowledge (redundant vs. specialized); 3. <p> Single agent systems should be used in such cases. Otherwise, when parallelism and modularity are possible, system designers should opt for the simplest multiagent scenario possible (see Finally, multiagent systems can be useful for their illucidation of intelligence <ref> [7] </ref>. As Gerhard Wei put it: Intelligence is deeply and inevitably coupled with interaction [13]. In fact, it has been proposed that the best way 7 to develop intelligent machines at all might be to start by creating social machines [14].
Reference: [8] <author> H. V. D. </author> <title> Parunak, </title> <booktitle> Applications of distributed artificial intelligence in industry, in Foundations of Distributed Artificial Intelligence (G. </booktitle> <editor> M. P. O'Hare and N. R. Jennings, eds.), pp. </editor> <volume> 139164, </volume> <publisher> Wiley Interscience, </publisher> <year> 1996. </year>
Reference-contexts: In fact, the remaining dimensions are very prominent in this article: degree of heterogeneity is a major MAS dimension and all the methods of distributing control appear here as major issues. More recently, Parunak has presented a taxonomy of MAS from an application perspective <ref> [8] </ref>. From this perspective, the important characteristics of MAS are: * System function; * Agent architecture (degree of heterogeneity, reactive vs. deliberative); * System architecture (communication, protocols, human involvement). A useful contribution is that the dimensions are divided into agent and system characteristics.
Reference: [9] <author> E. H. Durfee, </author> <title> What your computer really needs to know, you learned in kindergarten, </title> <booktitle> in Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <address> (Philadelphia, PA), </address> <publisher> Morgan Kaufman, </publisher> <year> 1992. </year> <type> Invited Talk. </type>
Reference-contexts: From this perspective, the important characteristics of MAS are: * System function; * Agent architecture (degree of heterogeneity, reactive vs. deliberative); * System architecture (communication, protocols, human involvement). A useful contribution is that the dimensions are divided into agent and system characteristics. Other overviews of DAI and/or MAS include <ref> [6, 9, 10] </ref>. The taxonomy presented in this article is organized along the most important aspects of agents (as opposed to domains): degree of heterogeneity and degree of communication.
Reference: [10] <author> E. H. Durfee, V. R. Lesser, and D. D. Corkill, </author> <title> Trends in cooperative distributed problem solving, </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> vol. 1, </volume> <pages> pp. 6383, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: From this perspective, the important characteristics of MAS are: * System function; * Agent architecture (degree of heterogeneity, reactive vs. deliberative); * System architecture (communication, protocols, human involvement). A useful contribution is that the dimensions are divided into agent and system characteristics. Other overviews of DAI and/or MAS include <ref> [6, 9, 10] </ref>. The taxonomy presented in this article is organized along the most important aspects of agents (as opposed to domains): degree of heterogeneity and degree of communication.
Reference: [11] <author> K. S. Decker, </author> <title> Task environment centered simulation, in Simulating Organizations: Computational Models of Institutions and Groups (M. </title> <editor> Prietula, K. Carley, and L. Gasser, eds.), </editor> <publisher> AAAI Press/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: They must then be combined into a multiagent system with the aid of some of the techniques described in this article. Another example of a domain that requires MAS is hospital scheduling as presented in <ref> [11] </ref>. This domain from an actual case study requires different agents to represent the interests of different people within the hospital. Hospital employees have different interests, from nurses who want to minimize the patient's time in the hospital, to x-ray operators who want to maximize the throughput on their machines.
Reference: [12] <author> K. Decker, </author> <type> Personal correspondence, </type> <month> May </month> <year> 1996. </year>
Reference-contexts: Of course there are some domains, such as chess playing, that are more naturally approached from an omniscient perspectivebecause a global view is givenor with centralized controlbecause no parallelism is possible and there is no action uncertainty <ref> [12] </ref>. Single agent systems should be used in such cases. Otherwise, when parallelism and modularity are possible, system designers should opt for the simplest multiagent scenario possible (see Finally, multiagent systems can be useful for their illucidation of intelligence [7].
Reference: [13] <author> G. Wei, </author> <booktitle> Ecai-96 workshop on learning in distributed artificial intelligence. Call For Papers, </booktitle> <year> 1996. </year>
Reference-contexts: Otherwise, when parallelism and modularity are possible, system designers should opt for the simplest multiagent scenario possible (see Finally, multiagent systems can be useful for their illucidation of intelligence [7]. As Gerhard Wei put it: Intelligence is deeply and inevitably coupled with interaction <ref> [13] </ref>. In fact, it has been proposed that the best way 7 to develop intelligent machines at all might be to start by creating social machines [14]. This theory is based on the socio-biological theory that primate intelligence first evolved because of the need to deal with social interactions.
Reference: [14] <author> K. Dautenhahn, </author> <title> Getting to know each otherartificial social intelligence for autonomous robots, </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> vol. 16, </volume> <pages> pp. 333356, </pages> <year> 1995. </year>
Reference-contexts: As Gerhard Wei put it: Intelligence is deeply and inevitably coupled with interaction [13]. In fact, it has been proposed that the best way 7 to develop intelligent machines at all might be to start by creating social machines <ref> [14] </ref>. This theory is based on the socio-biological theory that primate intelligence first evolved because of the need to deal with social interactions. Reasons presented above to use MAS are summarized in Table 3.
Reference: [15] <author> G. Wei, </author> <title> Distributed reinforcement learning, </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> vol. 15, </volume> <pages> pp. 135142, </pages> <year> 1995. </year>
Reference-contexts: However multiagent learning is more concerned with learning issues that arise because of the multiagent aspect of a given domain. As described by Wei, multiagent learning is learning that is done by several agents and that becomes possible only because several agents are present <ref> [15] </ref>. This is the type of learning that is emphasized in the sections entitled Further Learning Opportunities. <p> Cohen and Levesque/Lux and Steiner [56, 57] * Learning social behaviors. Mataric [58] * Bayesian learning in negotiation: model others. Zeng and Sycara [59] * Multiagent Q-learning. Weiss <ref> [15] </ref> * Training other agents' Q-functions (track driving). Clouse [60] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [62] * Contract nets for electronic commerce. Sandholm and Lesser [63] * Market-based systems. Huberman and Clearwater [64] * Generalized Partial Global Planning (GPGP). <p> Similar to Tan's work on multiagent RL in the pursuit domain [52] is Wei's work with competing Q-learners. The agents compete with each other to earn the right to control a single system <ref> [15] </ref>. The highest bidder pays a certain amount to be allowed to act, then receives any reward that results from the action. Another Q-learning approach, this time with benevolent agents, has been to explore the interesting idea of having one agent teach another agent through communication.
Reference: [16] <author> A. S. Rao and M. P. Georgeff, </author> <title> Bdi agents: From theory to practice, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 312319, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: The first several of these characteristics are self-explanatory and do not need further mention. With respect to cost of failure, an example of a domain with high cost of failure is air-traffic control <ref> [16] </ref>. On the other hand, the directed improvisation domain considered by Hayes-Roth et al. has a very low cost of failure [17]. In this domain, entertainment agents accept all improvisation suggestions from each other. <p> There are also several existing systems and techniques that mix reactive and deliberative behaviors. One example is Rao and Georgeff's OASIS system (see Section 8) which reasons about when to be reactive and when to follow goal-directed plans <ref> [16] </ref>. Another example is Sahota's reactive deliberation technique [32]. As the name implies it mixes reactive and deliberative behavior: an agent reasons about which reactive behavior to follow under the constraint that it must choose actions at a rate of 60 Hz. <p> Sandholm and Lesser [63] * Market-based systems. Huberman and Clearwater [64] * Generalized Partial Global Planning (GPGP). Decker [65] * Internal, Social, and Collective (role) commitments. Castelfranchi [66] * Commitment states (potential, pre, and actual) as planning states. Haddadi [67] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff <ref> [16] </ref> * BDI commitments only over intentions. Rao and Georgeff [16] * Coalitions. <p> Decker [65] * Internal, Social, and Collective (role) commitments. Castelfranchi [66] * Commitment states (potential, pre, and actual) as planning states. Haddadi [67] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff <ref> [16] </ref> * BDI commitments only over intentions. Rao and Georgeff [16] * Coalitions. <p> Rao and Georgeff use the BDI model to build a system for air-traffic control, OASIS, which has been implemented for testing (in parallel with human operators who retain full control) at the airport in Sydney, Australia <ref> [16] </ref>. Each aircraft is represented by a controlling agent which deal with a global sequencing agent. OASIS mixes reactive and deliberative actions in the agents: they can break out of planned sequences when coming across situations that demand immediate reaction.
Reference: [17] <author> B. Hayes-Roth, L. Brownston, and R. van Gent, </author> <title> Multiagent collaboration in directed improvisation, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 148154, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: With respect to cost of failure, an example of a domain with high cost of failure is air-traffic control [16]. On the other hand, the directed improvisation domain considered by Hayes-Roth et al. has a very low cost of failure <ref> [17] </ref>. In this domain, entertainment agents accept all improvisation suggestions from each other. The idea is that the agents should not be afraid to make mistakes, but rather should just let the words flow [17]. Several multiagent systems include humans as one or more of the agents. <p> the directed improvisation domain considered by Hayes-Roth et al. has a very low cost of failure <ref> [17] </ref>. In this domain, entertainment agents accept all improvisation suggestions from each other. The idea is that the agents should not be afraid to make mistakes, but rather should just let the words flow [17]. Several multiagent systems include humans as one or more of the agents. In this case, the designer must consider the issue of communication between the human and computer agents [18]. Another example of user involvement is user feedback in an information filtering domain [19].
Reference: [18] <author> J. A. Sanchez, F. S. Azevedo, and J. J. Leggett, Paragente: </author> <title> Exploring the issues in agent-based user interfaces, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 320327, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Several multiagent systems include humans as one or more of the agents. In this case, the designer must consider the issue of communication between the human and computer agents <ref> [18] </ref>. Another example of user involvement is user feedback in an information filtering domain [19]. Decker distinguishes three different sources of uncertainty in a domain [20].
Reference: [19] <author> I. A. Ferguson and G. J. Karakoulas, </author> <title> Multiagent learning and adaptation in an information filtering market, in Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> (Menlo Park,CA), </address> <pages> pp. 2832, </pages> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1996. </year> <note> AAAI Technical Report SS-96-01. </note>
Reference-contexts: Several multiagent systems include humans as one or more of the agents. In this case, the designer must consider the issue of communication between the human and computer agents [18]. Another example of user involvement is user feedback in an information filtering domain <ref> [19] </ref>. Decker distinguishes three different sources of uncertainty in a domain [20]. The transitions in the domain itself might be non-deterministic; agents might not know the actions of other agents; and agents might not know the outcomes of their own actions.
Reference: [20] <author> K. S. Decker, </author> <title> Environment Centered Analysis and Design of Coordination Mechanisms. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1995. </year>
Reference-contexts: In this case, the designer must consider the issue of communication between the human and computer agents [18]. Another example of user involvement is user feedback in an information filtering domain [19]. Decker distinguishes three different sources of uncertainty in a domain <ref> [20] </ref>. The transitions in the domain itself might be non-deterministic; agents might not know the actions of other agents; and agents might not know the outcomes of their own actions. <p> the other domain characteristics are summarized in Table 5. 10 Table 5: Domain characteristics that are important when designing MAS * Number of agents * Amount of time pressure (real time?) * Dynamically arriving goals? * Cost of communication * Cost of failure * User involvement * Environmental uncertainty: Decker <ref> [20] </ref> a priori in the domain in the actions of other agents in outcomes of an agent's own actions 5 Single Agent vs. Multiagent Systems Before studying and categorizing MAS, we must first consider their most obvious alternative: centralized, single agent systems.
Reference: [21] <author> L. M. Stephens and M. B. Merx, </author> <title> The effect of agent control strategy on the performance of a dai pursuit problem, </title> <booktitle> in Proceedings of the 10th International Workshop on Distributed Artificial Intelligence, </booktitle> <address> (Bandera, Texas), </address> <month> October </month> <year> 1990. </year>
Reference-contexts: The pursuit domain with homogeneous agents is illustrated in Figure 7. (the same amount of) limited information about other agents' internal states. Within this framework, Stephens and Merx propose a simple heuristic behavior for each agent that is based on local information <ref> [21] </ref>. They define capture positions as the four positions adjacent to the prey. They then propose a local strategy whereby each predator agent determines the capture position to which it is closest and moves towards 13 that position. <p> Recall the local strategy defined by Stephens and Merx in which each predator simply moved to its closest capture position. In their instantiation of the domain, the predators can see the prey, but not each other. With communication possible, they define two more possible strategies for the predators <ref> [21] </ref>. When using a distributed strategy, the agents are still homogeneous, but they communicate to insure that each moves toward a different capture position. In particular, the predator farthest from the prey chooses the capture position closest to it, and announces that it will approach that position. <p> A distributed strategy, it is much more effective than the local policy and does not require very much communication. However there are situations in which it does not succeed. Stephens and Merx then present one more strategy that always succeeds but requires much more communication: the central strategy <ref> [21] </ref>. The central strategy is effectively a single agent system. Three predators transmit all of their sensory inputs to one central agent which then decides where all the predators should move and transmits its 27 decision back to them.
Reference: [22] <author> J. M. Vidal and E. H. Durfee, </author> <title> Recursive agent modeling using limited rationality, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 376383, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Such prediction can be useful when the agents move simultaneously and would like to base their actions on where the other predators will be at the next time step. Vidal and Durfee analyze such a situation using the Recursive Modeling Method (RMM) <ref> [22] </ref>. RMM is discussed in more detail below, but the basic idea is that predator A bases its move on the predicted move of predator B and vice versa. <p> Since the resulting reasoning can recurse indefinitely, it is important for the agents to bound the amount of reasoning they use either in terms of time or in terms of levels of recursion. Vidal and Durfee's Limited Rationality RMM algorithm is designed to take such considerations into account <ref> [22] </ref>. Levy and Rosenschein use a game theoretical approach to the pursuit domain [23]. They use a payoff function that allows selfish agents to cooperate. A requirement for their model is that each predator has full information about the location of other predators. <p> Durfee contends that for coordination to be possible, some potential knowledge must be ignored. As well as illustrating this concept in the pursuit domain <ref> [22] </ref>, Durfee goes into more detail and offers more generally applicable methodology in [28]. The point of the RMM is to model the internal state of another agent in order to predict its actions.
Reference: [23] <author> R. Levy and J. S. Rosenschein, </author> <title> A game theoretic approach to the pursuit problem, </title> <booktitle> in Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 195213, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: Vidal and Durfee's Limited Rationality RMM algorithm is designed to take such considerations into account [22]. Levy and Rosenschein use a game theoretical approach to the pursuit domain <ref> [23] </ref>. They use a payoff function that allows selfish agents to cooperate. A requirement for their model is that each predator has full information about the location of other predators. Their game model mixes cooperative and non-cooperative games. <p> The actual robot motion is a simple weighted sum of these vectors. At the other extreme is the pursuit domain work by Levy and Rosenschein that is mentioned above <ref> [23] </ref>. Their agents assume that each will act in service of its own goals. They use game theoretic techniques to find equilibrium points and thus to decide how to act [23]. These agents are clearly deliberative, as they search for actions rather than simply retrieving them. <p> At the other extreme is the pursuit domain work by Levy and Rosenschein that is mentioned above <ref> [23] </ref>. Their agents assume that each will act in service of its own goals. They use game theoretic techniques to find equilibrium points and thus to decide how to act [23]. These agents are clearly deliberative, as they search for actions rather than simply retrieving them. There are also several existing systems and techniques that mix reactive and deliberative behaviors. <p> As we have already seen in the pursuit domain, Korf advocates using greedy agents that minimize their own distance to the prey [24], and similarly, Levy and Rosenschein use Game Theory to study how the predators can cooperate despite maximizing their own utilities <ref> [23] </ref>. Many advocates of selfish agents point to nature for their justification, claiming that animals are not altruistic, but rather act always in their own self-interest.
Reference: [24] <author> R. E. Korf, </author> <title> A simple solution to pursuit games, </title> <booktitle> in Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pp. 183194, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: A requirement for their model is that each predator has full information about the location of other predators. Their game model mixes cooperative and non-cooperative games. Korf also takes the approach that each agent should try to greedily maximize its own local utility <ref> [24] </ref>. He introduces a policy for each predator based on an attractive force to the prey and a repulsive force from the other predators. Thus the predators tend to approach the prey from different sides. <p> Richard Korf <ref> [24] </ref> However, whether or not altruism occurs in nature, there is certainly some use for benevolent agents in MAS, as shown below. <p> As we have already seen in the pursuit domain, Korf advocates using greedy agents that minimize their own distance to the prey <ref> [24] </ref>, and similarly, Levy and Rosenschein use Game Theory to study how the predators can cooperate despite maximizing their own utilities [23]. Many advocates of selfish agents point to nature for their justification, claiming that animals are not altruistic, but rather act always in their own self-interest.
Reference: [25] <author> T. Haynes, K. Lau, and S. Sen, </author> <title> Learning cases to compliment rules for conflict resolution in multiagent systems, in Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> (Menlo Park,CA), </address> <pages> pp. 5156, </pages> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1996. </year> <note> AAAI Technical Report SS-96-01. </note>
Reference-contexts: More pressingly, if Korf's claim that the pursuit domain is easily solved with local greedy heuristics were true, there would be no point in studying the pursuit domain any further. Fortunately, Haynes and Sen show that Korf's heuristics do not work for certain instantiations of the domain <ref> [25] </ref> (see Section 7). 6.2 General Homogeneous MAS The general multiagent scenario with homogeneous agents is illustrated in Figure 8. There are several different agents with identical structure, but they have different sensor input and effector output. <p> In place of communicating planned actions to each other, the predators can evolve to know, or at least act as if knowing, each other's future actions. In a separate study, Haynes et al. use case-based reasoning to allow predators to learn to cooperate <ref> [25] </ref>. They begin with identical agents controlling each of the predators. The predators move simultaneously to their closest capture positions. But because predators that try to occupy the same position all remain stationary, cases of deadlock arise.
Reference: [26] <author> T. Balch and R. C. Arkin, </author> <title> Motor schema-based formation control for multiagent robot teams, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 1016, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year> <month> 37 </month>
Reference-contexts: The issues and techniques, as well as the learning opportunities discussed later, are summarized in Table 6. Homogeneous Non-Communicating Issues * Reactive vs. deliberative agents * Local or global perspective * Modeling of other agents' states * How to affect others Techniques * Reactive Behaviors for Formation maintenance. Balch <ref> [26] </ref> * Local knowledge sometimes better. Roychowdhury [27] * (limited) Recursive Modeling Method (RMM). Durfee [28] * Don't model othersjust pay attention to reward. Schmidhuber [29] * Stigmergy. Holland [30] * Q-learning for behaviors like foraging, homing, etc. <p> Here we describe one system at each extreme as well as two others that mix reactive and deliberative reasoning. Balch and Arkin use homogeneous, reactive, non-communicating agents to study formation maintenance in autonomous robots <ref> [26] </ref>. The robots' goal is to move together in a military formation such as a diamond, column, or wedge. They periodically come across obstacles which prevent one or more of the robots from moving in a straight line.
Reference: [27] <author> S. Roychowdhury, N. Arora, and S. Sen, </author> <title> Effects of local information on group behavior, in Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> (Menlo Park,CA), </address> <pages> pp. 7883, </pages> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1996. </year> <note> AAAI Technical Report SS-96-01. </note>
Reference-contexts: Homogeneous Non-Communicating Issues * Reactive vs. deliberative agents * Local or global perspective * Modeling of other agents' states * How to affect others Techniques * Reactive Behaviors for Formation maintenance. Balch [26] * Local knowledge sometimes better. Roychowdhury <ref> [27] </ref> * (limited) Recursive Modeling Method (RMM). Durfee [28] * Don't model othersjust pay attention to reward. Schmidhuber [29] * Stigmergy. Holland [30] * Q-learning for behaviors like foraging, homing, etc. <p> Roychowdhury et al. consider a case of multiple agents sharing a set of identical resources in which they have to learn (adapt) their resource usage policies <ref> [27] </ref>. Since the agents are identical and do not communicate, if they all have a global view of the current resource usage, they will all move simultaneously to the most under-used resource. <p> Mataric's robots actively affect each other through observation: a robot learning to follow another robot can base its action on the relative location of the other robot. 6.4 Further Learning Opportunities In addition to the existing learning approaches described above <ref> [27, 29, 31] </ref>, there are several previously unexplored learning opportunities that apply to homogeneous non-communicating systems (see Table 6). One unexplored learning opportunity that could apply in domains with homogeneous non-communicating agents is learning to enable others' actions.
Reference: [28] <author> E. H. Durfee, </author> <title> Blissful ignorance: Knowing just enough to coordinate well, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 406413, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Homogeneous Non-Communicating Issues * Reactive vs. deliberative agents * Local or global perspective * Modeling of other agents' states * How to affect others Techniques * Reactive Behaviors for Formation maintenance. Balch [26] * Local knowledge sometimes better. Roychowdhury [27] * (limited) Recursive Modeling Method (RMM). Durfee <ref> [28] </ref> * Don't model othersjust pay attention to reward. Schmidhuber [29] * Stigmergy. Holland [30] * Q-learning for behaviors like foraging, homing, etc. <p> Better performance by agents with less knowledge is occasionally summarized by the cliche Ignorance is Bliss. 6.3.3 Modeling of other agents' states Durfee gives another example of Blissful Ignorance, mentioning it explicitly in the title of his paper: Blissful Ignorance: Knowing Just Enough to Coordinate Well <ref> [28] </ref>. Now rather than referring to resource usage, the saying applies to the limited Recursive Modeling Method (RMM). As mentioned above in the context of the pursuit domain, 16 RMM could recurse indefinitely. <p> Durfee contends that for coordination to be possible, some potential knowledge must be ignored. As well as illustrating this concept in the pursuit domain [22], Durfee goes into more detail and offers more generally applicable methodology in <ref> [28] </ref>. The point of the RMM is to model the internal state of another agent in order to predict its actions. Even though the agents know each other's goals and structure (they are homogeneous), they may not know each other's future actions.
Reference: [29] <author> J. Schmidhuber, </author> <title> A general method for multi-agent reinforcement learning in unrestricted environments, in Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> (Menlo Park,CA), </address> <pages> pp. 8487, </pages> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1996. </year> <note> AAAI Technical Report SS-96-01. </note>
Reference-contexts: Balch [26] * Local knowledge sometimes better. Roychowdhury [27] * (limited) Recursive Modeling Method (RMM). Durfee [28] * Don't model othersjust pay attention to reward. Schmidhuber <ref> [29] </ref> * Stigmergy. Holland [30] * Q-learning for behaviors like foraging, homing, etc. Mataric [31] Learning opportunities * Enable others' actions * Sensor data ! Other agent's sensor data Table 6: The issues, techniques, and learning opportunities for homogeneous MAS as reflected in the literature. 6.3.1 Reactive vs. <p> Although it may be useful to build models of other agents in the environment, agent modeling is not done universally. Schmidhuber advocates a form of multiagent reinforcement learning (RL) with which agents do not model each other as agents <ref> [29] </ref>. Instead they consider each other as parts of the environment and affect each other's policies only as sensed objects. The agents pay attention to the reward they receive using a given policy and checkpoint their policies so they can return to successful ones. <p> Mataric's robots actively affect each other through observation: a robot learning to follow another robot can base its action on the relative location of the other robot. 6.4 Further Learning Opportunities In addition to the existing learning approaches described above <ref> [27, 29, 31] </ref>, there are several previously unexplored learning opportunities that apply to homogeneous non-communicating systems (see Table 6). One unexplored learning opportunity that could apply in domains with homogeneous non-communicating agents is learning to enable others' actions.
Reference: [30] <author> O. Holland, </author> <title> Multiagent systems: Lessons from social insects and collective robotics, in Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> (Menlo Park,CA), </address> <pages> pp. 57 62, </pages> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1996. </year> <note> AAAI Technical Report SS-96-01. </note>
Reference-contexts: Balch [26] * Local knowledge sometimes better. Roychowdhury [27] * (limited) Recursive Modeling Method (RMM). Durfee [28] * Don't model othersjust pay attention to reward. Schmidhuber [29] * Stigmergy. Holland <ref> [30] </ref> * Q-learning for behaviors like foraging, homing, etc. Mataric [31] Learning opportunities * Enable others' actions * Sensor data ! Other agent's sensor data Table 6: The issues, techniques, and learning opportunities for homogeneous MAS as reflected in the literature. 6.3.1 Reactive vs. <p> Actively, they can be sensed by other agents, or they may be able to change the state of another agent by, for example, pushing it. More indirectly, agents can affect other agents by one of two types of stigmergy <ref> [30] </ref>. First, active stigmergy occurs when an agent alters the environment so as to affect the sensory input of another agent. For example, a robotic agent might leave a marker behind it for other agents to observe. <p> Holland illustrates the concept of passive stigmergy with a robotic system designed to model the behavior of an ant colony confronted with many dead ants around its nest <ref> [30] </ref>. An ant from such a colony tends to periodically pick up a dead ant, carry it for a short distance, and then drop it. Although the behavior appears to be random, after several hours, the dead ants are clustered in a small number of heaps. <p> Although the ants behave homogeneously and, at least in this case, we have no evidence that they communicate explicitly, the ants manage to cooperate in achieving a task. Holland models this situation with a number of identical robots in a small area scattered with pucks <ref> [30] </ref>. The robots are programmed reactively to move straight (turning at walls) until they are pushing three or more pucks. At that point, the robots back up and turn away, leaving the three pucks in a cluster.
Reference: [31] <author> M. J. Mataric, </author> <title> Interaction and intelligent behavior, </title> <type> MIT EECS PhD Thesis AITR-1495, </type> <institution> MIT AI Lab, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Balch [26] * Local knowledge sometimes better. Roychowdhury [27] * (limited) Recursive Modeling Method (RMM). Durfee [28] * Don't model othersjust pay attention to reward. Schmidhuber [29] * Stigmergy. Holland [30] * Q-learning for behaviors like foraging, homing, etc. Mataric <ref> [31] </ref> Learning opportunities * Enable others' actions * Sensor data ! Other agent's sensor data Table 6: The issues, techniques, and learning opportunities for homogeneous MAS as reflected in the literature. 6.3.1 Reactive vs. <p> Like the ants, the robots use passive stigmergy to affect each other's behavior. A similar scenario with more deliberative robots is explored by Mataric. In this case, the robots use Q-learning to learn behaviors including foraging for pucks as well as homing and following <ref> [31] </ref>. The robots learn independent policies, dealing with the high-dimensional state space with the aid of progress estimators that give intermediate rewards, and with the aid of boolean value predicates that condense many states into one. <p> Mataric's robots actively affect each other through observation: a robot learning to follow another robot can base its action on the relative location of the other robot. 6.4 Further Learning Opportunities In addition to the existing learning approaches described above <ref> [27, 29, 31] </ref>, there are several previously unexplored learning opportunities that apply to homogeneous non-communicating systems (see Table 6). One unexplored learning opportunity that could apply in domains with homogeneous non-communicating agents is learning to enable others' actions. <p> The theory of communication as action is called speech acts [56, 57]. Mataric adds a learning dimension to the idea of speech acts. Starting with the foraging behavior mentioned above <ref> [31] </ref>, the agents can then learn to choose among a set of social behaviors that include broadcasting and listening [58]. Q-learning is extended so that reinforcement can be received for direct rewards or for rewards to other agents.
Reference: [32] <author> M. K. Sahota, </author> <title> Reactive deliberation: An architecture for real-time intelligent control in dynamic environments, </title> <booktitle> in Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 13031308, </pages> <year> 1994. </year>
Reference-contexts: There are also several existing systems and techniques that mix reactive and deliberative behaviors. One example is Rao and Georgeff's OASIS system (see Section 8) which reasons about when to be reactive and when to follow goal-directed plans [16]. Another example is Sahota's reactive deliberation technique <ref> [32] </ref>. As the name implies it mixes reactive and deliberative behavior: an agent reasons about which reactive behavior to follow under the constraint that it must choose actions at a rate of 60 Hz.
Reference: [33] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore, </author> <title> Reinforcement learning: A survey, </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> vol. 4, </volume> <pages> pp. 237285, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Typical RL situations with delayed reward encourage agents to learn to achieve their goals directly by propagating local reinforcement back to past states and actions <ref> [33] </ref>. However if an action leads to a reward by another agent, the acting agent may have no way of reinforcing that action. Techniques to deal with such a problem would be useful for building multiagent systems.
Reference: [34] <author> T. Haynes, R. Wainwright, S. Sen, and D. Schoenefeld, </author> <title> Strongly typed genetic programming in evolving cooperation strategies, </title> <booktitle> in Proceedings of the Sixth International Conference on Genetic Algorithms (S. </booktitle> <editor> Forrest, ed.), </editor> <address> (San Mateo, CA), </address> <pages> pp. 271278, </pages> <publisher> Morgan Kaufman, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Haynes and colleagues have done various studies with heterogeneous agents in the pursuit domain. They have evolved teams of predators, equipped predators with case bases, and competitively evolved the predators and the prey. First, Haynes et al. use genetic programming (GP) to evolve teams of four predators <ref> [34] </ref>. Rather than evolving predator agents in a single evolutionary pool and then combining them into teams to test performance, each individual in the population is actually a team of four agents already specifically assigned to different predators. Thus the predators can evolve to cooperate.
Reference: [35] <author> T. Haynes and S. Sen, </author> <title> Evolving behavioral strategies in predators and prey, in Adaptation and Learning in Multiagent Systems (G. </title> <editor> Weiand S. Sen, </editor> <booktitle> eds.), </booktitle> <pages> pp. 113126, </pages> <address> Berlin: </address> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Over time, the predators learn to stay out of each other's way while approaching the prey. Finally, Haynes and Sen explore the possibility of evolving both the predators and the prey so that they all try to improve their behaviors <ref> [35] </ref>. <p> Although there may yet be greedy solutions that can deal with different types of prey behavior, they have not yet been discovered. Thus the predator domain retains value for researchers in MAS. Although Haynes and Sen convince the reader that the pursuit domain is still worth studying <ref> [35] </ref>, the coevolutionary results are less than satisfying. As mentioned above, one would intuitively expect the predators to be able to adapt to the linearly moving prey. <p> Mor and Rosenschein [36] * Minimax-Q. Littman [37] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew <ref> [35, 38, 39] </ref> * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [41] * Model as a team (individual ! role). Tambe [42, 43] * Social reasoning: depend on others for goal (6= game theory). <p> Such systems that use competitive evolving agents are said to use a technique called competitive co-evolution. Systems that evolve benevolent agents are said to use cooperative co-evolution. The evolution of both predator and prey agents by Haynes and Sen <ref> [35] </ref> qualifies as competitive co-evolution. Grefenstette and Daley conduct a preliminary study of competitive and cooperative co-evolution in a domain that is loosely related to the pursuit domain [38]. Their domain has two robots that can move continuously and one morsel of (stationary) food that appears randomly in the world. <p> with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently [43]. 7.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described <ref> [35, 37, 38, 39, 42, 45, 46, 49, 50, 51] </ref>. A few of the other most obvious future ML applications to this scenario are described here and summarized in Table 7. 25 One challenge for system builders who use evolving agents is dealing with the credit/blame problem.
Reference: [36] <author> Y. Mor and J. Rosenschein, </author> <title> Time and the prisoner's dilemma, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 276282, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Heterogeneous Non-Communicating Issues * Benevolence vs. competitiveness * Stable vs. evolving agents (arms race, credit/blame) * Modeling of others' goals, actions, and knowledge * Resource management (interdependent actions) * Social conventions * Roles Techniques * Game theory, iterative play. Mor and Rosenschein <ref> [36] </ref> * Minimax-Q. Littman [37] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [35, 38, 39] * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [41] * Model as a team (individual ! role). <p> Nevertheless, even assuming that altruism does not exist (not a universal assumption by any means), in some situations it may be in an animal's (or agent's) interest to cooperate with other agents. 21 Mor and Rosenschein illustrate this possibility in the context of the prisoner's dilemma <ref> [36] </ref>. In the prisoner's dilemma, two agents try to act so as to maximize their own individual rewards. They are not actively out to thwart each other since it is not a zero-sum game, yet they place no inherent value on the other receiving reward.
Reference: [37] <author> M. L. Littman, </author> <title> Markov games as a framework for multi-agent reinforcement learning, </title> <booktitle> in Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> (San Mateo, CA), </address> <pages> pp. 157163, </pages> <publisher> Morgan Kaufman, </publisher> <year> 1994. </year>
Reference-contexts: Heterogeneous Non-Communicating Issues * Benevolence vs. competitiveness * Stable vs. evolving agents (arms race, credit/blame) * Modeling of others' goals, actions, and knowledge * Resource management (interdependent actions) * Social conventions * Roles Techniques * Game theory, iterative play. Mor and Rosenschein [36] * Minimax-Q. Littman <ref> [37] </ref> * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [35, 38, 39] * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [41] * Model as a team (individual ! role). <p> The players can block each other by trying to move to the same space. Littman introduces a variant of Q-learning called Minimax-Q which is designed to work on Markov games as opposed to Markov Decision Processes <ref> [37] </ref>. The competitive agents learn probabilistic policies since any deterministic policy can be completely counteracted by the opponent. The issue of benevolence (willingness to cooperate) vs. competitiveness comes up repeatedly in the systems described below. <p> with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently [43]. 7.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described <ref> [35, 37, 38, 39, 42, 45, 46, 49, 50, 51] </ref>. A few of the other most obvious future ML applications to this scenario are described here and summarized in Table 7. 25 One challenge for system builders who use evolving agents is dealing with the credit/blame problem.
Reference: [38] <author> J. Grefenstette and R. Daley, </author> <title> Methods for competitive and cooperative co-evolution, in Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> (Menlo Park,CA), </address> <pages> pp. 45 50, </pages> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1996. </year> <note> AAAI Technical Report SS-96-01. </note>
Reference-contexts: Mor and Rosenschein [36] * Minimax-Q. Littman [37] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew <ref> [35, 38, 39] </ref> * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [41] * Model as a team (individual ! role). Tambe [42, 43] * Social reasoning: depend on others for goal (6= game theory). <p> The evolution of both predator and prey agents by Haynes and Sen [35] qualifies as competitive co-evolution. Grefenstette and Daley conduct a preliminary study of competitive and cooperative co-evolution in a domain that is loosely related to the pursuit domain <ref> [38] </ref>. Their domain has two robots that can move continuously and one morsel of (stationary) food that appears randomly in the world. In the cooperative task, both robots must be at the food in order to capture it. <p> In a competitive task in the same domain, agents try to be the first to reach the food <ref> [38] </ref>. Again, different GA evaluation methods are considered for use in evolving rule sets to control the agents. One problem to contend with in competitive rather than cooperative co-evolution is the possibility of an escalating arms race with no end. <p> with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently [43]. 7.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described <ref> [35, 37, 38, 39, 42, 45, 46, 49, 50, 51] </ref>. A few of the other most obvious future ML applications to this scenario are described here and summarized in Table 7. 25 One challenge for system builders who use evolving agents is dealing with the credit/blame problem.
Reference: [39] <author> C. D. Rosin and R. K. Belew, </author> <title> Methods for competitive co-evolution: Finding opponents worth beating, </title> <booktitle> in Proceedings of the Sixth International Conference on Genetic Algorithms (S. </booktitle> <editor> Forrest, ed.), </editor> <address> (San Mateo,CA), </address> <pages> pp. 373380, </pages> <publisher> Morgan Kaufman, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Mor and Rosenschein [36] * Minimax-Q. Littman [37] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew <ref> [35, 38, 39] </ref> * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [41] * Model as a team (individual ! role). Tambe [42, 43] * Social reasoning: depend on others for goal (6= game theory). <p> Of course this method encourages the arms race more than ever. Nevertheless, Rosin and Belew use this technique, along with an interesting method for maintaining diversity in genetic populations, to evolve agents that can play TicTacToe, Nim, and a simple version of Go <ref> [39] </ref>. When it is a given agent's turn to evolve, it executes a standard GA generation. Individuals are tested against individuals from the competing population, but a technique called competitive fitness sharing is used to maintain diversity. <p> with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently [43]. 7.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described <ref> [35, 37, 38, 39, 42, 45, 46, 49, 50, 51] </ref>. A few of the other most obvious future ML applications to this scenario are described here and summarized in Table 7. 25 One challenge for system builders who use evolving agents is dealing with the credit/blame problem.
Reference: [40] <author> M. J. Huber and E. H. Durfee, </author> <title> Deciding when to commit to action during observation-based coordination, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 163170, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Mor and Rosenschein [36] * Minimax-Q. Littman [37] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [35, 38, 39] * Deduce intentions through observation. Huber and Durfee <ref> [40] </ref> * Autoepistemic reasoning (ignorance). Permpoontanalarp [41] * Model as a team (individual ! role). Tambe [42, 43] * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [44] * GAs to deal with Braes' paradox (more resource ! worse). <p> Without communication, agents are forced to model each other strictly through observation. Huber and Durfee consider a case of coordinated motion control among multiple mobile robots under the assumption that communication is prohibitively expensive <ref> [40] </ref>. Thus the agents try to deduce each other's plans by observing their actions. In particular, each robot (simulated or real) tries to figure out the destinations of the other robots by watching how they move.
Reference: [41] <author> Y. Permpoontanalarp, </author> <title> Generalised proof-theory for multi-agent autoepistemic reasoning, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 304311, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Mor and Rosenschein [36] * Minimax-Q. Littman [37] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [35, 38, 39] * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp <ref> [41] </ref> * Model as a team (individual ! role). Tambe [42, 43] * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [44] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [45] * Multiagent RL for adaptive Load Balancing. <p> When modeling other agents, it may be useful to reason not only about what is true and what is false, but also about what is not known. Such reasoning about ignorances is called Autoepistemic Reasoning. For a theoretical presentation of an autoepistemic reasoning method in MAS, see <ref> [41] </ref>. Just as RMM is useful for modeling the states of homogeneous agents, it can be used in the heterogeneous scenario as well. Tambe takes it one step further, studying how agents can learn models of teams of agents.
Reference: [42] <author> M. Tambe, </author> <title> Recursive agent and agent-group tracking in a real-time , dynamic environment, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 368375, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Mor and Rosenschein [36] * Minimax-Q. Littman [37] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [35, 38, 39] * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [41] * Model as a team (individual ! role). Tambe <ref> [42, 43] </ref> * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [44] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [45] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [46] * Focal points/Emergent conventions. <p> Tambe takes it one step further, studying how agents can learn models of teams of agents. In an air combat domain, agents can use RMM to try to deduce an opponents' plan based on its observable actions <ref> [42] </ref>. For example, a fired missile may not be visible, but the observation of a preparatory maneuver commonly used before firing could indicate that a missile has been launched. When teams of agents are involved, the situation becomes more complicated. <p> with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently [43]. 7.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described <ref> [35, 37, 38, 39, 42, 45, 46, 49, 50, 51] </ref>. A few of the other most obvious future ML applications to this scenario are described here and summarized in Table 7. 25 One challenge for system builders who use evolving agents is dealing with the credit/blame problem.
Reference: [43] <author> M. Tambe, </author> <title> Tracking dynamic team activity, </title> <booktitle> in Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> (Menlo Park, California), </address> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: Mor and Rosenschein [36] * Minimax-Q. Littman [37] * Competitive co-evolution. Haynes and Sen/Grefenstette and Daley/Rosin and Belew [35, 38, 39] * Deduce intentions through observation. Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [41] * Model as a team (individual ! role). Tambe <ref> [42, 43] </ref> * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [44] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [45] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [46] * Focal points/Emergent conventions. <p> When teams of agents are involved, the situation becomes more complicated. In this case, an opponent's actions may not make sense except in the context of a team maneuver. Then the agent's role within the team must be modeled. Tambe discusses the advantages of team modeling <ref> [43] </ref>. One reason that modeling other agents might be useful is that agents sometimes depend on each other for achieving their goals. Unlike in game theory where agents can cooperate or not depending on their utility estimation, there may be actions that require cooperation for successful execution. <p> When an agent is faced with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently <ref> [43] </ref>. 7.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described [35, 37, 38, 39, 42, 45, 46, 49, 50, 51].
Reference: [44] <author> J. S. Sichman and Y. Demazeau, </author> <title> Exploiting social reasoning to deal with agency level inconistency, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 352359, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Huber and Durfee [40] * Autoepistemic reasoning (ignorance). Permpoontanalarp [41] * Model as a team (individual ! role). Tambe [42, 43] * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau <ref> [44] </ref> * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [45] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [46] * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge [47, 48] * Design agents play different roles. <p> For example, two robots may be needed to successfully push a box, or, as in the pursuit domain, several agents may be needed to capture an opponent. Sichman and Demazeau analyze how the case of conflicting mutual models of different co-dependent agents can arise and be dealt with <ref> [44] </ref>. 7.3.4 Resource management Heterogeneous agents may have interdependent actions due to limited resources needed by several of the agents.
Reference: [45] <author> N. Arora and S. Sen, </author> <title> Resolving social dilemmas using genetic algorithms, in Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> (Menlo Park,CA), </address> <pages> pp. 15, </pages> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1996. </year> <note> AAAI Technical Report SS-96-01. 38 </note>
Reference-contexts: Permpoontanalarp [41] * Model as a team (individual ! role). Tambe [42, 43] * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [44] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen <ref> [45] </ref> * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [46] * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge [47, 48] * Design agents play different roles. <p> Glance and Hogg use GAs to represent different parts of a contrived sample network. Arora and Sen then improve the GA representation slightly and show that with the new representation, the system is actually able to find the globally optimal traffic flow <ref> [45] </ref>. Adaptive load balancing has been studied as a multiagent problem by allowing different agents to decide which processor to use at a given time. <p> with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently [43]. 7.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described <ref> [35, 37, 38, 39, 42, 45, 46, 49, 50, 51] </ref>. A few of the other most obvious future ML applications to this scenario are described here and summarized in Table 7. 25 One challenge for system builders who use evolving agents is dealing with the credit/blame problem.
Reference: [46] <author> A. Schaerf, Y. Shoham, and M. Tennenholtz, </author> <title> Adaptive load balancing: A study in multi-agent learning, </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> vol. 2, </volume> <pages> pp. 475500, </pages> <year> 1995. </year>
Reference-contexts: Tambe [42, 43] * Social reasoning: depend on others for goal (6= game theory). Sichman and Demazeau [44] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [45] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz <ref> [46] </ref> * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge [47, 48] * Design agents play different roles. <p> Adaptive load balancing has been studied as a multiagent problem by allowing different agents to decide which processor to use at a given time. Using Reinforcement Learning, Schaerf et al. show that the heterogeneous agents can achieve reasonable load balance without any central control and without communication among agents <ref> [46] </ref>. <p> with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently [43]. 7.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described <ref> [35, 37, 38, 39, 42, 45, 46, 49, 50, 51] </ref>. A few of the other most obvious future ML applications to this scenario are described here and summarized in Table 7. 25 One challenge for system builders who use evolving agents is dealing with the credit/blame problem.
Reference: [47] <author> M. Fenster, S. Kraus, and J. S. Rosenschein, </author> <title> Coordination without communication: Experimental validation of focal point techniques, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 102108, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Sichman and Demazeau [44] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [45] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [46] * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge <ref> [47, 48] </ref> * Design agents play different roles. <p> Thus even without communicating, people are sometimes able to coordinate actions. Apparently features that have been seen or used often present themselves as obvious choices. In the context of MAS, Fenster et al. define the Focal Point method <ref> [47] </ref>. They discuss the phenomenon of cultural (or programmed) preferences allowing agents to meet without communicating. They propose that, all else being equal, agents who need to meet should choose rare or extreme options. On a similar note, conventions might emerge over time.
Reference: [48] <author> A. Walker and M. Wooldridge, </author> <title> Understanding the emergence of conventions in multi-agent systems, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 384389, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Sichman and Demazeau [44] * GAs to deal with Braes' paradox (more resource ! worse). Arora and Sen [45] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [46] * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge <ref> [47, 48] </ref> * Design agents play different roles. <p> They propose that, all else being equal, agents who need to meet should choose rare or extreme options. On a similar note, conventions might emerge over time. Walker and Woolridge propose biasing agents towards options that have been chosen, for example, most recently or most frequently in the past <ref> [48] </ref>. Rather than coming from pre-analysis of the options as in the Focal Point method, conventions emerge over time. 7.3.6 Roles When agents have similar goals but different abilities, they can be organized into a team. Each agent then plays a separate role within the team.
Reference: [49] <author> M. V. N. Prasad, V. R. Lesser, and S. E. Lander, </author> <title> Learning organizational roles in a heterogeneous multi-agent system, in Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> (Menlo Park,CA), </address> <pages> pp. 7277, </pages> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1996. </year> <note> AAAI Technical Report SS-96-01. </note>
Reference-contexts: Arora and Sen [45] * Multiagent RL for adaptive Load Balancing. Schaerf, Shoham, and Tennenholtz [46] * Focal points/Emergent conventions. Fenster et al./Walker and Woolridge [47, 48] * Design agents play different roles. Prasad et al. <ref> [49] </ref> Learning opportunities * Credit/blame in competitive scenarios * Behaviors that blend well with team * Prediction of others' actions * Dynamic role assumption Table 7: The issues, techniques, and learning opportunities for heterogeneous MAS as reflected inthe literature. 7.3.1 Benevolence vs. competitiveness One of the most important issues to consider <p> However in some domains, the agents are flexible enough to interchange roles. The multiagent design of a steam pump is one such domain. Prasad et al. study design agents that can either initiate a design or extend a design <ref> [49] </ref>. In different situations, different agents are more effective at initiation and at extension. Thus a supervised learning technique is used to help agents learn what roles they should fill in different situations. <p> with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently [43]. 7.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described <ref> [35, 37, 38, 39, 42, 45, 46, 49, 50, 51] </ref>. A few of the other most obvious future ML applications to this scenario are described here and summarized in Table 7. 25 One challenge for system builders who use evolving agents is dealing with the credit/blame problem.
Reference: [50] <author> X. Wang, </author> <title> Planning while learning operators, </title> <booktitle> in Proceedings of the Third International Conference on AI Planning Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: In addition to modeling agents' goals through observation, it is also possible to learn their actions. Wang's OBSERVER system allows an agent to incrementally learn the preconditions and effects of planning actions by observing domain experts <ref> [50] </ref>. After observing for a time, the agent can then experimentally refine its model by 23 practicing the actions itself. When modeling other agents, it may be useful to reason not only about what is true and what is false, but also about what is not known. <p> with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently [43]. 7.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described <ref> [35, 37, 38, 39, 42, 45, 46, 49, 50, 51] </ref>. A few of the other most obvious future ML applications to this scenario are described here and summarized in Table 7. 25 One challenge for system builders who use evolving agents is dealing with the credit/blame problem.
Reference: [51] <author> N. S. Glance and T. Hogg, </author> <title> Dilemmas in computational societies, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 117124, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Designers of multiagent systems with limited resources must decide how the agents will share the resources. One interesting network traffic problem called Braess' paradox has been studied from a multiagent perspective using GAs <ref> [51] </ref>. Braess' paradox is the phenomenon of adding more resources to a network but getting worse performance. <p> Glance and Hogg claim that under certain conditions, including usage-dependent resource costs, agents that are sharing the network and reasoning separately about which path of the network to use cannot achieve global optimal performance <ref> [51] </ref>. Glance and Hogg use GAs to represent different parts of a contrived sample network. Arora and Sen then improve the GA representation slightly and show that with the new representation, the system is actually able to find the globally optimal traffic flow [45]. <p> with an opposing team of agents, it may be useful to model individual agents as filling roles within a team action rather than as acting independently [43]. 7.4 Further Learning Opportunities Throughout the above investigation of issues and techniques in the heterogeneous non-communicating multiagent scenario, many learning approaches are described <ref> [35, 37, 38, 39, 42, 45, 46, 49, 50, 51] </ref>. A few of the other most obvious future ML applications to this scenario are described here and summarized in Table 7. 25 One challenge for system builders who use evolving agents is dealing with the credit/blame problem.
Reference: [52] <author> M. Tan, </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents, in Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 330337, </pages> <year> 1993. </year>
Reference-contexts: However the predators can freely exchange information in order to help them capture the prey more effectively. The current situation is illustrated in Figure 11. predators can communicate with one another. Tan uses communicating agents in the pursuit domain to conduct some interesting multiagent Q-learning experiments <ref> [52] </ref>. In his instantiation of the domain, there are several prey agents and the predators have limited vision so that they may not always know where the prey are. Thus the predators can help each other by informing each other of their sensory input. <p> Zeng and Sycara study a competitive negotiation scenario in which agents use Bayesian Learning techniques to update models of each other based on bids and counter bids in a negotiation process [59]. Similar to Tan's work on multiagent RL in the pursuit domain <ref> [52] </ref> is Wei's work with competing Q-learners. The agents compete with each other to earn the right to control a single system [15]. The highest bidder pays a certain amount to be allowed to act, then receives any reward that results from the action.
Reference: [53] <author> E.-I. Osawa, </author> <title> A metalevel coordination strategy for reactive cooperative planning, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 297303, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Another way to frame this tradeoff is as one between cost and freedom: as communication cost (time) increases, freedom decreases. Osawa suggests that the predators should move through four phases. In increasing order of cost (decreasing freedom), they are: autonomy, communication, negotiation, and control <ref> [53] </ref>. When the predators stop making sufficient progress toward the prey using one strategy, they should move to the next most expensive strategy. Thus they can close in on the prey efficiently and effectively.
Reference: [54] <author> M. R. Genesereth and R. E. Fikes, </author> <title> Knowledge interchange format, version 3.0 reference manual, </title> <type> Technical Report Logic-92-1, </type> <institution> Computer Science Department, Stanford University, </institution> <year> 1992. </year>
Reference-contexts: Heterogeneous Communicating Issues * Understanding each other * Planning communicative acts * Benevolence vs. competitiveness * Resource management (schedule coordination) * Commitment/decommitment Techniques * Language protocols: KIF for content (Genesreth and Fikes <ref> [54] </ref>), KQML for message format (Finin et al. [55]). * Speech acts. Cohen and Levesque/Lux and Steiner [56, 57] * Learning social behaviors. Mataric [58] * Bayesian learning in negotiation: model others. Zeng and Sycara [59] * Multiagent Q-learning. Weiss [15] * Training other agents' Q-functions (track driving). <p> Independent aspects of protocols are information content, message format, and coordination conventions. Among many others, existing language protocols for these three levels are: KIF for content <ref> [54] </ref>, KQML for message format [55], and, more recently, COOL for coordination [69]. There has been a lot of research done on refining these and other communication protocols.
Reference: [55] <author> T. Finin, D. McKay, R. Fritzson, and R. McEntire, </author> <title> Kqml: An information and knowledge exchange protocol, in Knowledge Building and Knowledge Sharing (K. </title> <editor> Fuchi and T. Yokoi, eds.), </editor> <publisher> Ohmsha and IOS Press, </publisher> <year> 1994. </year>
Reference-contexts: Heterogeneous Communicating Issues * Understanding each other * Planning communicative acts * Benevolence vs. competitiveness * Resource management (schedule coordination) * Commitment/decommitment Techniques * Language protocols: KIF for content (Genesreth and Fikes [54]), KQML for message format (Finin et al. <ref> [55] </ref>). * Speech acts. Cohen and Levesque/Lux and Steiner [56, 57] * Learning social behaviors. Mataric [58] * Bayesian learning in negotiation: model others. Zeng and Sycara [59] * Multiagent Q-learning. Weiss [15] * Training other agents' Q-functions (track driving). Clouse [60] * Minimize the need for training. <p> Independent aspects of protocols are information content, message format, and coordination conventions. Among many others, existing language protocols for these three levels are: KIF for content [54], KQML for message format <ref> [55] </ref>, and, more recently, COOL for coordination [69]. There has been a lot of research done on refining these and other communication protocols.
Reference: [56] <author> P. R. Cohen and H. J. Levesque, </author> <title> Communicative actions for artificial agents, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 6572, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Heterogeneous Communicating Issues * Understanding each other * Planning communicative acts * Benevolence vs. competitiveness * Resource management (schedule coordination) * Commitment/decommitment Techniques * Language protocols: KIF for content (Genesreth and Fikes [54]), KQML for message format (Finin et al. [55]). * Speech acts. Cohen and Levesque/Lux and Steiner <ref> [56, 57] </ref> * Learning social behaviors. Mataric [58] * Bayesian learning in negotiation: model others. Zeng and Sycara [59] * Multiagent Q-learning. Weiss [15] * Training other agents' Q-functions (track driving). Clouse [60] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. <p> When combined with a model of other agents, the effect of a communication act might be to alter an agent's belief about the state of another agent or agents. The theory of communication as action is called speech acts <ref> [56, 57] </ref>. Mataric adds a learning dimension to the idea of speech acts. Starting with the foraging behavior mentioned above [31], the agents can then learn to choose among a set of social behaviors that include broadcasting and listening [58].
Reference: [57] <author> A. Lux and D. </author> <title> Steiner, Understanding cooperation: an agent's perspective, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 261268, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Heterogeneous Communicating Issues * Understanding each other * Planning communicative acts * Benevolence vs. competitiveness * Resource management (schedule coordination) * Commitment/decommitment Techniques * Language protocols: KIF for content (Genesreth and Fikes [54]), KQML for message format (Finin et al. [55]). * Speech acts. Cohen and Levesque/Lux and Steiner <ref> [56, 57] </ref> * Learning social behaviors. Mataric [58] * Bayesian learning in negotiation: model others. Zeng and Sycara [59] * Multiagent Q-learning. Weiss [15] * Training other agents' Q-functions (track driving). Clouse [60] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. <p> When combined with a model of other agents, the effect of a communication act might be to alter an agent's belief about the state of another agent or agents. The theory of communication as action is called speech acts <ref> [56, 57] </ref>. Mataric adds a learning dimension to the idea of speech acts. Starting with the foraging behavior mentioned above [31], the agents can then learn to choose among a set of social behaviors that include broadcasting and listening [58].
Reference: [58] <author> M. J. Mataric, </author> <title> Learning to behave socially, </title> <booktitle> in Third International Conference on Simulation of Adaptive Behavior, </booktitle> <year> 1994. </year>
Reference-contexts: Cohen and Levesque/Lux and Steiner [56, 57] * Learning social behaviors. Mataric <ref> [58] </ref> * Bayesian learning in negotiation: model others. Zeng and Sycara [59] * Multiagent Q-learning. Weiss [15] * Training other agents' Q-functions (track driving). Clouse [60] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [62] * Contract nets for electronic commerce. <p> The theory of communication as action is called speech acts [56, 57]. Mataric adds a learning dimension to the idea of speech acts. Starting with the foraging behavior mentioned above [31], the agents can then learn to choose among a set of social behaviors that include broadcasting and listening <ref> [58] </ref>. Q-learning is extended so that reinforcement can be received for direct rewards or for rewards to other agents. When using communication as a planning action, the possibility arises of communicating misinformation in order to satisfy a particular goal.
Reference: [59] <author> D. Zeng and K. Sycara, </author> <title> Bayesian learning in negotiation, in Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> (Menlo Park,CA), </address> <pages> pp. 99104, </pages> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1996. </year> <note> AAAI Technical Report SS-96-01. </note>
Reference-contexts: Cohen and Levesque/Lux and Steiner [56, 57] * Learning social behaviors. Mataric [58] * Bayesian learning in negotiation: model others. Zeng and Sycara <ref> [59] </ref> * Multiagent Q-learning. Weiss [15] * Training other agents' Q-functions (track driving). Clouse [60] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [62] * Contract nets for electronic commerce. Sandholm and Lesser [63] * Market-based systems. <p> In the current scenario, there are many more examples of competitive agents. Zeng and Sycara study a competitive negotiation scenario in which agents use Bayesian Learning techniques to update models of each other based on bids and counter bids in a negotiation process <ref> [59] </ref>. Similar to Tan's work on multiagent RL in the pursuit domain [52] is Wei's work with competing Q-learners. The agents compete with each other to earn the right to control a single system [15].
Reference: [60] <author> J. A. Clouse, </author> <title> Learning from an automated training agent, in Adaptation and Learning in Multiagent Systems (G. </title> <editor> Weiand S. Sen, eds.), </editor> <publisher> Berlin: Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Cohen and Levesque/Lux and Steiner [56, 57] * Learning social behaviors. Mataric [58] * Bayesian learning in negotiation: model others. Zeng and Sycara [59] * Multiagent Q-learning. Weiss [15] * Training other agents' Q-functions (track driving). Clouse <ref> [60] </ref> * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [62] * Contract nets for electronic commerce. Sandholm and Lesser [63] * Market-based systems. Huberman and Clearwater [64] * Generalized Partial Global Planning (GPGP). Decker [65] * Internal, Social, and Collective (role) commitments. <p> Eventually, the learner is able to perform the task without any guidance. Clouse studies the effect of different levels of advice in a road-following domain <ref> [60] </ref>. He concludes that moderate advice improves performance and speeds up learning, while too much advice leads to worse performance because the learner does not experience enough negative examples while training.
Reference: [61] <author> M. A. Potter, K. A. D. Jong, and J. J. Grefenstette, </author> <title> A coevolutionary approach to learning sequential decision rules, </title> <booktitle> in Proceedings of the Sixth International Conference on Genetic Algorithms (S. </booktitle> <editor> Forrest, ed.), </editor> <address> (San Mateo,CA), </address> <pages> pp. 366372, </pages> <publisher> Morgan Kaufman, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Cohen and Levesque/Lux and Steiner [56, 57] * Learning social behaviors. Mataric [58] * Bayesian learning in negotiation: model others. Zeng and Sycara [59] * Multiagent Q-learning. Weiss [15] * Training other agents' Q-functions (track driving). Clouse [60] * Minimize the need for training. Potter and Grefenstette <ref> [61] </ref> * Cooperative co-evolution. Bull et al. [62] * Contract nets for electronic commerce. Sandholm and Lesser [63] * Market-based systems. Huberman and Clearwater [64] * Generalized Partial Global Planning (GPGP). Decker [65] * Internal, Social, and Collective (role) commitments. <p> Potter and Grefenstette illustrate this effect in the domain described above in which two robots compete for a stationary pellet of food <ref> [61] </ref>. Subpopulations of rules are seeded to be more effective in different situations. Thus specialized subpopulations of rules corresponding to shaped behaviors tend to emerge. Rather than competitive co-evolution Bull et al. build a system system which uses cooperative co-evolution [62].
Reference: [62] <author> L. Bull, T. Fogarty, and M. Snaith, </author> <title> Evolution in multi-agent systems: Evolving communicating classifier systems for gait in a quadrupedal robot, </title> <booktitle> in Proceedings of the Sixth International Conference on Genetic Algorithms (S. </booktitle> <editor> Forrest, ed.), </editor> <address> (San Mateo,CA), </address> <pages> pp. 382388, </pages> <publisher> Morgan Kaufman, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Mataric [58] * Bayesian learning in negotiation: model others. Zeng and Sycara [59] * Multiagent Q-learning. Weiss [15] * Training other agents' Q-functions (track driving). Clouse [60] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. <ref> [62] </ref> * Contract nets for electronic commerce. Sandholm and Lesser [63] * Market-based systems. Huberman and Clearwater [64] * Generalized Partial Global Planning (GPGP). Decker [65] * Internal, Social, and Collective (role) commitments. Castelfranchi [66] * Commitment states (potential, pre, and actual) as planning states. <p> Subpopulations of rules are seeded to be more effective in different situations. Thus specialized subpopulations of rules corresponding to shaped behaviors tend to emerge. Rather than competitive co-evolution Bull et al. build a system system which uses cooperative co-evolution <ref> [62] </ref>. They use GAs to evolve separate communicating agents to control different legs of a quadrapedal robot. Drawing inspiration from competition in human societies, several researchers have designed systems based on the law of supply and demand.
Reference: [63] <author> T. Sandholm and V. Lesser, </author> <title> Issues in automated negotiation and electronic commerce: Extending the contract net framework, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 328335, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Zeng and Sycara [59] * Multiagent Q-learning. Weiss [15] * Training other agents' Q-functions (track driving). Clouse [60] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [62] * Contract nets for electronic commerce. Sandholm and Lesser <ref> [63] </ref> * Market-based systems. Huberman and Clearwater [64] * Generalized Partial Global Planning (GPGP). Decker [65] * Internal, Social, and Collective (role) commitments. Castelfranchi [66] * Commitment states (potential, pre, and actual) as planning states. Haddadi [67] * Belief/Desire/Intention (BDI) model: OASIS. <p> Agents must pay to contract their tasks out and thus shop around for the lowest bidder. Sandholm and Lesser discuss some of the issues that arise in contract nets <ref> [63] </ref>. In a similar spirit is an implemented multiagent system that controls air temperature in different rooms of a building [64]. A person can set one's thermostat to any temperature.
Reference: [64] <author> B. Huberman and S. H. Clearwater, </author> <title> A multi-agent system for controlling building environments, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 171176, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year> <month> 39 </month>
Reference-contexts: Weiss [15] * Training other agents' Q-functions (track driving). Clouse [60] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [62] * Contract nets for electronic commerce. Sandholm and Lesser [63] * Market-based systems. Huberman and Clearwater <ref> [64] </ref> * Generalized Partial Global Planning (GPGP). Decker [65] * Internal, Social, and Collective (role) commitments. Castelfranchi [66] * Commitment states (potential, pre, and actual) as planning states. Haddadi [67] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [16] * BDI commitments only over intentions. <p> Agents must pay to contract their tasks out and thus shop around for the lowest bidder. Sandholm and Lesser discuss some of the issues that arise in contract nets [63]. In a similar spirit is an implemented multiagent system that controls air temperature in different rooms of a building <ref> [64] </ref>. A person can set one's thermostat to any temperature. Then depending on the actual air temperature, the agent for that room tries to buy either hot or cold air from another room that has an excess.
Reference: [65] <author> K. S. Decker and V. R. Lesser, </author> <title> Designing a family of coordination algorithms, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 7380, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Clouse [60] * Minimize the need for training. Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [62] * Contract nets for electronic commerce. Sandholm and Lesser [63] * Market-based systems. Huberman and Clearwater [64] * Generalized Partial Global Planning (GPGP). Decker <ref> [65] </ref> * Internal, Social, and Collective (role) commitments. Castelfranchi [66] * Commitment states (potential, pre, and actual) as planning states. Haddadi [67] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [16] * BDI commitments only over intentions. Rao and Georgeff [16] * Coalitions. <p> In the current scenario, agents can also coordinate schedules. Decker's Generalized Partial Global Planning (GPGP) allows several heterogeneous agents to post constraints, or commitments to do a task by some time, to each other's local schedulers and thus coordinate without the aid of any centralized agent <ref> [65] </ref>. 8.3.5 Commitment/decommitment When agents communicate, they may decide to cooperate on a given task or for a given amount of time. In so doing, they make commitments to each other.
Reference: [66] <author> C. Castelfranchi, </author> <title> Commitments: From individual intentions to groups and organizations, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 4148, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Potter and Grefenstette [61] * Cooperative co-evolution. Bull et al. [62] * Contract nets for electronic commerce. Sandholm and Lesser [63] * Market-based systems. Huberman and Clearwater [64] * Generalized Partial Global Planning (GPGP). Decker [65] * Internal, Social, and Collective (role) commitments. Castelfranchi <ref> [66] </ref> * Commitment states (potential, pre, and actual) as planning states. Haddadi [67] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [16] * BDI commitments only over intentions. Rao and Georgeff [16] * Coalitions. <p> The theory of commitment and decommitment (when the commitment terminates) has consequently drawn considerable attention. For example, Castelfranchi defines three types of commitment: internal commitmentan agent binds itself to do something, social commitmentan agent commits to another agent, and collective commitmentan agent agrees to fill a certain role <ref> [66] </ref>. Setting an alarm clock is an example of internal commitment to wake up at a certain time. Haddadi discusses commitment states as planning states: potential cooperation, pre-commitment, and commitment [67]. Agents can then use means-ends analysis to plan for goals in terms of commitment opportunities.
Reference: [67] <author> A. Haddadi, </author> <title> Towards a pragmatic theory of interactions, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 133139, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Bull et al. [62] * Contract nets for electronic commerce. Sandholm and Lesser [63] * Market-based systems. Huberman and Clearwater [64] * Generalized Partial Global Planning (GPGP). Decker [65] * Internal, Social, and Collective (role) commitments. Castelfranchi [66] * Commitment states (potential, pre, and actual) as planning states. Haddadi <ref> [67] </ref> * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [16] * BDI commitments only over intentions. Rao and Georgeff [16] * Coalitions. <p> Setting an alarm clock is an example of internal commitment to wake up at a certain time. Haddadi discusses commitment states as planning states: potential cooperation, pre-commitment, and commitment <ref> [67] </ref>. Agents can then use means-ends analysis to plan for goals in terms of commitment opportunities. This work is conducted within a model called Belief/Desire/Intention, or BDI. BDI is a popular technique for modeling other agents.
Reference: [68] <author> G. Zlotkin and J. S. Rosenschein, </author> <title> Coalition, cryptography, and stability: Mechanisms for coalition formation in task oriented domains, </title> <booktitle> in Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 432437, </pages> <publisher> AAAI Press, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: Decker [65] * Internal, Social, and Collective (role) commitments. Castelfranchi [66] * Commitment states (potential, pre, and actual) as planning states. Haddadi [67] * Belief/Desire/Intention (BDI) model: OASIS. Rao and Georgeff [16] * BDI commitments only over intentions. Rao and Georgeff [16] * Coalitions. Zlotkin and Rosenschein <ref> [68] </ref> Learning opportunities * Evolving language * Effects of speech acts on global dynamics * Communication utility and truthfulness * Commitment utility Table 8: The issues, techniques, and learning opportunities for communicating multiagent systems as reflected in the literature. 29 8.3.1 Understanding each other In all communicating multiagent systems, and particularly <p> Finally, groups of agents may decide to commit to each other. Rather than the more usual two-agent or all-agent commitment scenarios, Zlotkin and Rosenschein study situations in which agents may want to form coalitions <ref> [68] </ref>.
Reference: [69] <author> M. Barbuceanu and M. S. Fox, </author> <title> Cool: A language for describing coordination in multi agent systems, </title> <booktitle> in Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park, California), </address> <pages> pp. 1724, </pages> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Independent aspects of protocols are information content, message format, and coordination conventions. Among many others, existing language protocols for these three levels are: KIF for content [54], KQML for message format [55], and, more recently, COOL for coordination <ref> [69] </ref>. There has been a lot of research done on refining these and other communication protocols.
Reference: [70] <author> R. G. Smith, </author> <title> The contract net protocol: High-level communication and control in a distributed problem solver., </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-29, </volume> <pages> pp. 11041113, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: Drawing inspiration from competition in human societies, several researchers have designed systems based on the law of supply and demand. In the contract nets framework, agents all have their own goals, are self-interested, and have limited reasoning resources <ref> [70] </ref>. They bid to accept tasks from other agents and then can either perform the tasks (if they have the proper resources) or subcontract them to still other agents. Agents must pay to contract their tasks out and thus shop around for the lowest bidder.
Reference: [71] <editor> J.-H. Kim, ed., </editor> <booktitle> Proceedings of the Micro-Robot World Cup Soccer Tournament, </booktitle> <address> (Taejon, Korea), </address> <month> November </month> <year> 1996. </year>
Reference-contexts: In this section a single domain which embodies most multiagent issues is presented. Robotic soccer is a particularly good domain for studying MAS. It has been gaining popularity in recent years, with international competitions in Korea in 1996 <ref> [71] </ref> and in Japan in 1997 [72]. If recognized as a standard testbed, it can be used to evaluate different MAS techniques in a straightforward manner: teams implemented with different techniques can play against each other.
Reference: [72] <author> H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda, and E. Osawa, </author> <title> Robocup: The robot world cup initiative, </title> <booktitle> in Proceedings of the First International Conference on Autonomous Agents, </booktitle> <address> (Marina Del Rey, California), </address> <pages> pp. 340347, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: In this section a single domain which embodies most multiagent issues is presented. Robotic soccer is a particularly good domain for studying MAS. It has been gaining popularity in recent years, with international competitions in Korea in 1996 [71] and in Japan in 1997 <ref> [72] </ref>. If recognized as a standard testbed, it can be used to evaluate different MAS techniques in a straightforward manner: teams implemented with different techniques can play against each other.
Reference: [73] <author> M. K. Sahota, A. K. Mackworth, R. A. Barman, and S. J. Kingdon, </author> <title> Real-time control of soccer-playing robots using off-board vision: the dynamite testbed, </title> <booktitle> in IEEE International Conference on Systems, Man, and Cybernetics, </booktitle> <pages> pp. 36903663, </pages> <year> 1995. </year>
Reference-contexts: Robotic soccer can be played either with real robots or in a simulator. Although more costly and time consuming to develop, a number of groups have developed real robotic systems. The first robotic soccer system was the Dynamo system <ref> [73] </ref>. Sahota et al. built a 1 vs. 1 version of the game. Asada et al. have used vision based RL with their soccer playing robots [74]. Achim et al. discuss some of the robotic issues involved in building robotic soccer players [75].
Reference: [74] <author> M. Asada, E. Uchibe, S. Noda, S. Tawaratsumida, and K. Hosoda, </author> <title> Coordination of multiple behaviors acquired by vision-based reinforcement learning, </title> <booktitle> in Proc. of IEEE/RSJ/GI International Conference on Intelligent Robots and Systems 1994 (IROS '94), </booktitle> <pages> pp. 917924, </pages> <year> 1994. </year>
Reference-contexts: The first robotic soccer system was the Dynamo system [73]. Sahota et al. built a 1 vs. 1 version of the game. Asada et al. have used vision based RL with their soccer playing robots <ref> [74] </ref>. Achim et al. discuss some of the robotic issues involved in building robotic soccer players [75]. Some robotic issues can only be studied in the real-world instantiation, but there are also many issues that can be studied in simulation.
Reference: [75] <author> S. Achim, P. Stone, and M. Veloso, </author> <title> Building a dedicated robotic soccer system, </title> <booktitle> in Proceedings of the IROS-96 Workshop on RoboCup, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: Sahota et al. built a 1 vs. 1 version of the game. Asada et al. have used vision based RL with their soccer playing robots [74]. Achim et al. discuss some of the robotic issues involved in building robotic soccer players <ref> [75] </ref>. Some robotic issues can only be studied in the real-world instantiation, but there are also many issues that can be studied in simulation. A particularly good simulator for this purpose is the soccerserver developed by Noda [76] and pictured in Figure 13.
Reference: [76] <author> I. Noda, </author> <title> Soccer server : a simulator of robocup, </title> <booktitle> in Proceedings of AI symposium '95, </booktitle> <pages> pp. </pages> <month> 2934, </month> <journal> Japanese Society for Artificial Intelligence, </journal> <month> December </month> <year> 1995. </year>
Reference-contexts: Some robotic issues can only be studied in the real-world instantiation, but there are also many issues that can be studied in simulation. A particularly good simulator for this purpose is the soccerserver developed by Noda <ref> [76] </ref> and pictured in Figure 13.
Reference: [77] <author> K. S. Decker, </author> <booktitle> Distributed artificial intelligence testbeds, in Foundations of Distributed Artificial Intelligence (G. </booktitle> <editor> M. P. O'Hare and N. R. Jennings, eds.), pp. </editor> <volume> 119138, </volume> <publisher> Wiley Interscience, </publisher> <year> 1996. </year>
Reference-contexts: The simulator provides a domain and supports users who wish to build their own agents. Furthermore, evaluation of agents is straightforward: they can compete against each other, or perhaps against standard teams. Thus robotic soccer satisfies Decker's criteria for DAI testbeds <ref> [77] </ref>. The main goal of any testbed is to facilitate the trial and evaluation of ideas that have promise in the real world. A wide variety of MAS issues can be studied in simulated robotic soccer.
Reference: [78] <author> P. Stone and M. Veloso, </author> <title> Beating a defender in robotic soccer: Memory-based learning of a continuous function, </title> <booktitle> in Advances in Neural Information Processing Systems 8 (D. </booktitle> <editor> S. Touretzky, M. C. Mozer, and M. E. Hasselmo, eds.), </editor> <address> (Cambridge, MA), </address> <pages> pp. 896902, </pages> <publisher> MIT press, </publisher> <year> 1996. </year>
Reference-contexts: As well as addressing most of the issues inherent in MAS, robotic soccer is a great domain for multiagent Machine Learning. In another soccer simulator, Stone and Veloso use Memory-based Learning to allow a player to learn when to shoot and when to pass the ball <ref> [78] </ref>. They then use Neural Networks to teach a player to shoot a moving ball into the goal [79]. They use similar techniques in the soccerserver system as well, extending the learned behavior as a part of a hierarchical learning system [80].
Reference: [79] <author> P. Stone and M. M. Veloso, </author> <title> Towards collaborative and adversarial learning: A case study in robotic soccer, </title> <note> To appear in International Journal of Human-Computer Systems (IJHCS), </note> <year> 1997. </year>
Reference-contexts: In another soccer simulator, Stone and Veloso use Memory-based Learning to allow a player to learn when to shoot and when to pass the ball [78]. They then use Neural Networks to teach a player to shoot a moving ball into the goal <ref> [79] </ref>. They use similar techniques in the soccerserver system as well, extending the learned behavior as a part of a hierarchical learning system [80]. Matsubar et al. also use a Neural Network to allow a player to learn when to shoot and when to pass in the soccerserver system [81].
Reference: [80] <author> P. Stone and M. Veloso, </author> <title> A layered approach to learning client behaviors in the robocup soccer server, </title> <note> To appear in Applied Artificial Intelligence (AAI) Journal, </note> <year> 1997. </year> <title> A shorter version, Using Machine Learning in the Soccer Server, </title> <booktitle> is available in the Proceedings of the IROS-96 Workshop on RoboCup, </booktitle> <address> Osaka, Japan, </address> <month> November, </month> <year> 1996. </year>
Reference-contexts: They then use Neural Networks to teach a player to shoot a moving ball into the goal [79]. They use similar techniques in the soccerserver system as well, extending the learned behavior as a part of a hierarchical learning system <ref> [80] </ref>. Matsubar et al. also use a Neural Network to allow a player to learn when to shoot and when to pass in the soccerserver system [81]. Once low-level behaviors have been developed, the opportunity to use ML techniques at the strategy level is particularly exciting.
Reference: [81] <author> H. Matsubara, I. Noda, and K. Hiraki, </author> <title> Learning of cooperative actions in multi-agent systems: a case study of pass play in soccer, in Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> (Menlo Park,CA), </address> <pages> pp. 6367, </pages> <publisher> AAAI Press, </publisher> <month> March </month> <year> 1996. </year> <note> AAAI Technical Report SS-96-01. </note>
Reference-contexts: They use similar techniques in the soccerserver system as well, extending the learned behavior as a part of a hierarchical learning system [80]. Matsubar et al. also use a Neural Network to allow a player to learn when to shoot and when to pass in the soccerserver system <ref> [81] </ref>. Once low-level behaviors have been developed, the opportunity to use ML techniques at the strategy level is particularly exciting. The advantages of robotic soccer as a testbed for MAS are summarized in Table 9.
Reference: [82] <editor> AAAI, </editor> <booktitle> Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <address> (Menlo Park,CA), </address> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1995. </year> <editor> Victor LessorGeneral Chair. </editor> <volume> 40 </volume>
Reference-contexts: MAS is an active field with many open issues. Continuing research is presented at dedicated conferences and workshops such as the International Conference on Multi-Agent Systems <ref> [3, 4, 82] </ref>. MAS work also appears in many of the DAI conferences and workshops. This survey provides a framework within which the reader can situate both existing and future work.
References-found: 82

