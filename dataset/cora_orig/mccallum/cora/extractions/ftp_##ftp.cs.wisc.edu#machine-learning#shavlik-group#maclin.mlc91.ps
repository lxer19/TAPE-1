URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/maclin.mlc91.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/maclin.mlc91.ps.abstract.html
Root-URL: 
Email: email: maclin@cs.wisc.edu  
Title: Refining Domain Theories Expressed as Finite-State Automata  
Phone: 1991.  
Author: Richard Maclin Jude W. Shavlik 
Affiliation: Computer Science Dept. University of Wisconsin  
Address: San Mateo, CA.,  Madison, WI 53706  
Note: Appears in Machine Learning: Proceedings of the Eighth International Workshop, Birnbaum, L., and Collins, G. (eds.), Morgan Kaufmann,  
Abstract: The KBANN system uses neural networks to refine domain theories. Currently, domain knowledge in KBANN is expressed as non-recursive, propositional rules. We extend KBANN to domain theories expressed as finite-state automata. We apply finite-state KBANN to the task of predicting how proteins fold, producing a small but statistically significant gain in accuracy over both a standard neural network approach and a non-learning algorithm from the biological literature. Our method shows promise at solving this and other real-world problems that can be described in terms of state-dependent decisions.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Chou, P. Y. and Fasman, G. D., </author> <title> "Prediction of the secondary structure of proteins from their amino acid sequence," </title> <journal> Advan. Enzymol. </journal> <volume> 47, </volume> <year> (1978), </year> <pages> 45-148. </pages>
Reference-contexts: One, it is a real-world problem; at present no simple solution exists that produces highly accurate predictions. Two, there have been a number of attempts to apply standard neural network methods [4, 10], which can be used for comparison purposes. Finally, there are a number of domain theories <ref> [1, 7, 11] </ref>, most notably the Chou-Fasman [1] method, which can not be naturally expressed in simple propositional rules, but which can be expressed as a finite-state automaton. The next section presents the KBANN method and how it is extended for finite-state automata. <p> Two, there have been a number of attempts to apply standard neural network methods [4, 10], which can be used for comparison purposes. Finally, there are a number of domain theories [1, 7, 11], most notably the Chou-Fasman <ref> [1] </ref> method, which can not be naturally expressed in simple propositional rules, but which can be expressed as a finite-state automaton. The next section presents the KBANN method and how it is extended for finite-state automata. This is followed by an experiment involving protein folding. <p> A sample mapping between a protein's primary and secondary structure is shown in Figure 3. A common method used to solve this problem by biologists, the Chou-Fasman method <ref> [1] </ref>, was shown to have a prediction accuracy of 58% for sample protein sequences [8]. Primary ....VYRNNFKSA.... iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Secondary ....bbcccccaa....c c c sample protein. 3.2 STANDARD NEURAL NETWORK APPROACHES TO PREDICTING SECONDARY STRUCTURE Several researchers have attempted to use neural networks to solve this problem [4, 10]. <p> The best test set accuracies reported from these efforts were 63.2% [4] and 62.7% [10]. 3.3 THE CHOU-FASMAN DOMAIN THEORY A standard approach in the biological literature to predicting secondary structure is the Chou-Fasman method <ref> [1] </ref>. The Chou-Fasman method is similar for both helix and sheet prediction; we will look at only helix prediction. The first step is to find helix nucleation sites.
Reference: 2. <author> Cleeremans, A., Servan-Schreiber, D. and McClelland, J. L., </author> <title> "Finite state automata and simple recurrent networks," </title> <booktitle> Neural Computation 1, 3 (1989), </booktitle> <pages> 372-381. </pages>
Reference-contexts: Each arc in the FSA is viewed as a rule where the antecedents are the source state and the input value; the consequent is the destination state. This representation of an FSA in a neural network is very similar to that of Cleeremanns et al. <ref> [2] </ref>. 2.3 A FINITE-STATE KBANN EXAMPLE As an example of translating an FSA, consider the automaton in Figure 2 (a) for recognizing numbers divisible by three. The FSA is translated into a set of rules with current state and input as antecedents and resulting state as consequent.
Reference: 3. <author> Elman, J. L., </author> <title> "Finding structure in time," Cog. </title> <journal> Sci. </journal> <volume> 14, 2 (1990), </volume> <pages> 179-211. </pages>
Reference-contexts: The input is a window of values around the central value, and the next input is obtained by advancing the window one place. To solve problem 2, the current state of the system is an input to the network at each step, as suggested by Jordan [5] and Elman <ref> [3] </ref>. After each step, when the window advances, the state from the last step is copied back as the next state. This copied-back state makes translation of an FSA a simple extension to the KBANN method.
Reference: 4. <author> Holley, L. H. and Karplus, M., </author> <title> "Protein structure prediction with a neural network," </title> <journal> Proc. Nat. Acad. Sci. </journal> <volume> 86, </volume> <year> (1989), </year> <pages> 152-156. </pages>
Reference-contexts: There are a number of reasons we chose this domain. One, it is a real-world problem; at present no simple solution exists that produces highly accurate predictions. Two, there have been a number of attempts to apply standard neural network methods <ref> [4, 10] </ref>, which can be used for comparison purposes. Finally, there are a number of domain theories [1, 7, 11], most notably the Chou-Fasman [1] method, which can not be naturally expressed in simple propositional rules, but which can be expressed as a finite-state automaton. <p> Primary ....VYRNNFKSA.... iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Secondary ....bbcccccaa....c c c sample protein. 3.2 STANDARD NEURAL NETWORK APPROACHES TO PREDICTING SECONDARY STRUCTURE Several researchers have attempted to use neural networks to solve this problem <ref> [4, 10] </ref>. The neural networks in these efforts had as input a window of amino acids consisting of the central amino acid being predicted, plus some number of the amino acids before and after it in the sequence. <p> Since such "short" sequences do not in general occur in real proteins [6], these predictions are replaced with coil. The best test set accuracies reported from these efforts were 63.2% <ref> [4] </ref> and 62.7% [10]. 3.3 THE CHOU-FASMAN DOMAIN THEORY A standard approach in the biological literature to predicting secondary structure is the Chou-Fasman method [1]. The Chou-Fasman method is similar for both helix and sheet prediction; we will look at only helix prediction.
Reference: 5. <author> Jordan, M. I., </author> <title> "Serial order: a parallel distributed processing approach," </title> <type> TR 8604, </type> <institution> UCal, Inst. for Cog. Sci., </institution> <address> San Diego, </address> <year> 1986. </year>
Reference-contexts: The input is a window of values around the central value, and the next input is obtained by advancing the window one place. To solve problem 2, the current state of the system is an input to the network at each step, as suggested by Jordan <ref> [5] </ref> and Elman [3]. After each step, when the window advances, the state from the last step is copied back as the next state. This copied-back state makes translation of an FSA a simple extension to the KBANN method.
Reference: 6. <author> Kapsch, W. and Sander, C., </author> <type> Biopolymers 22, </type> <year> (1983), </year> <pages> 2577-2637. </pages>
Reference-contexts: Since such "short" sequences do not in general occur in real proteins <ref> [6] </ref>, these predictions are replaced with coil. The best test set accuracies reported from these efforts were 63.2% [4] and 62.7% [10]. 3.3 THE CHOU-FASMAN DOMAIN THEORY A standard approach in the biological literature to predicting secondary structure is the Chou-Fasman method [1].
Reference: 7. <author> Lim, V. I., </author> <title> "Algorithms for prediction of a-helical and b structural regions in globular proteins," </title> <journal> J. Mol. Bio. </journal> <volume> 88, </volume> <year> (1974), </year> <pages> 873-894. </pages>
Reference-contexts: One, it is a real-world problem; at present no simple solution exists that produces highly accurate predictions. Two, there have been a number of attempts to apply standard neural network methods [4, 10], which can be used for comparison purposes. Finally, there are a number of domain theories <ref> [1, 7, 11] </ref>, most notably the Chou-Fasman [1] method, which can not be naturally expressed in simple propositional rules, but which can be expressed as a finite-state automaton. The next section presents the KBANN method and how it is extended for finite-state automata. <p> A better initial representation of the Chou-Fasman domain theory might raise accuracy since KBANN is dependent on the quality of its initial domain theory. Also, combining Chou-Fasman with other techniques such as the Lim <ref> [7] </ref> or Robson et al. [11] algorithms might produce better results. 4 CONCLUSIONS The finite-state KBANN method provides a useful and powerful extension to the standard KBANN technique. Finite-state KBANN represents domain theories expressed as finite-state automata.
Reference: 8. <author> Nishikawa, K., </author> <title> "Assessment of secondary-structure prediction of proteins comparison of computerized Chou-Fasman method with others," </title> <journal> Biochim. Biophys. Acta 748, </journal> <year> (1983), </year> <pages> 285-299. </pages>
Reference-contexts: A sample mapping between a protein's primary and secondary structure is shown in Figure 3. A common method used to solve this problem by biologists, the Chou-Fasman method [1], was shown to have a prediction accuracy of 58% for sample protein sequences <ref> [8] </ref>. Primary ....VYRNNFKSA.... iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Secondary ....bbcccccaa....c c c sample protein. 3.2 STANDARD NEURAL NETWORK APPROACHES TO PREDICTING SECONDARY STRUCTURE Several researchers have attempted to use neural networks to solve this problem [4, 10]. <p> Nishikawa <ref> [8] </ref> suggested that only the most likely nucleation sites should be chosen, thus the higher threshold. I L G D Q F L K Q Q Y V V F D R N G I RD L A P V A ..... .....
Reference: 9. <author> Noordewier, M. O., Towell, G. G. and Shavlik, J. W., </author> <title> "Training knowledge-based neural networks to recognize genes in DNA sequences," </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 3, </volume> <publisher> Morgan Kaufmann, </publisher> <address> Denver, CO, </address> <year> 1991. </year>
Reference-contexts: This approach uses the domain knowledge to determine the topology of the network and biases the network so that it will start with a "good" set of weights. This approach has been effective in complex domains, such as gene recognition <ref> [9, 14] </ref>, even when the initial domain theory is not particularly correct. In this paper we discuss how KBANN's domain theory vocabulary is extended. Application to the problem of protein folding demonstrates the promise of this approach.
Reference: 10. <author> Qian, N. and Sejnowski, T. J., </author> <title> "Predicting the secondary structure of globular proteins using neural network models.," </title> <journal> J. Mol. Bio. </journal> <volume> 202, </volume> <year> (1988), </year> <pages> 865-884. </pages>
Reference-contexts: There are a number of reasons we chose this domain. One, it is a real-world problem; at present no simple solution exists that produces highly accurate predictions. Two, there have been a number of attempts to apply standard neural network methods <ref> [4, 10] </ref>, which can be used for comparison purposes. Finally, there are a number of domain theories [1, 7, 11], most notably the Chou-Fasman [1] method, which can not be naturally expressed in simple propositional rules, but which can be expressed as a finite-state automaton. <p> Primary ....VYRNNFKSA.... iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Secondary ....bbcccccaa....c c c sample protein. 3.2 STANDARD NEURAL NETWORK APPROACHES TO PREDICTING SECONDARY STRUCTURE Several researchers have attempted to use neural networks to solve this problem <ref> [4, 10] </ref>. The neural networks in these efforts had as input a window of amino acids consisting of the central amino acid being predicted, plus some number of the amino acids before and after it in the sequence. <p> Since such "short" sequences do not in general occur in real proteins [6], these predictions are replaced with coil. The best test set accuracies reported from these efforts were 63.2% [4] and 62.7% <ref> [10] </ref>. 3.3 THE CHOU-FASMAN DOMAIN THEORY A standard approach in the biological literature to predicting secondary structure is the Chou-Fasman method [1]. The Chou-Fasman method is similar for both helix and sheet prediction; we will look at only helix prediction. The first step is to find helix nucleation sites. <p> Testing was done with the set of proteins used by Qian and Sejnowski <ref> [10] </ref>. The data set consists of 128 proteins. The proteins were randomly divided into training and test sets containing two-thirds (85 proteins) and one-third (43 proteins) of the original proteins, respectively. This process was repeated ten times.
Reference: 11. <author> Robson, B. and Suzuki, E., </author> <title> "Conformational properties of amino acid residues in globular proteins," </title> <journal> J. Mol. Bio. </journal> <volume> 107, </volume> <year> (1976), </year> <pages> 327-356. </pages>
Reference-contexts: One, it is a real-world problem; at present no simple solution exists that produces highly accurate predictions. Two, there have been a number of attempts to apply standard neural network methods [4, 10], which can be used for comparison purposes. Finally, there are a number of domain theories <ref> [1, 7, 11] </ref>, most notably the Chou-Fasman [1] method, which can not be naturally expressed in simple propositional rules, but which can be expressed as a finite-state automaton. The next section presents the KBANN method and how it is extended for finite-state automata. <p> A better initial representation of the Chou-Fasman domain theory might raise accuracy since KBANN is dependent on the quality of its initial domain theory. Also, combining Chou-Fasman with other techniques such as the Lim [7] or Robson et al. <ref> [11] </ref> algorithms might produce better results. 4 CONCLUSIONS The finite-state KBANN method provides a useful and powerful extension to the standard KBANN technique. Finite-state KBANN represents domain theories expressed as finite-state automata.
Reference: 12. <author> Rumelhart, D. E., Hinton, G. E. and Williams, R. J., </author> <title> "Learning internal representations by error propagation," </title> <booktitle> in Parallel Distributed Processing: Explorations in the microstructure of cognition. Volume 1: Foundations, </booktitle> <editor> Rumelhart, D. E. and McClelland, J. L. (ed.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986, </year> <pages> 318-363. </pages>
Reference-contexts: These connections allow the system to learn new antecedents to rules that might not have been part of the original domain theory. The resulting network is trained using backpropagation <ref> [12] </ref>. Backpropagation refines the domain theory to correctly classify any training examples not already covered. 2.2 FINITE-STATE KBANN A finite-state automaton (FSA) is represented by a set of nodes and arcs. Each node is a state. Each arc is a transition from one state to another.
Reference: 13. <author> Sejnowski, T. J. and Rosenberg, C. R., </author> <title> "Parallel Networks that Learn to Pronounce English Text," </title> <journal> Complex Systems 1, </journal> <year> (1987), </year> <pages> 145-168. </pages>
Reference-contexts: The problems with representing an FSA in KBANN are (1) how to represent the concepts of "scanning" a series of input values and (2) keeping track of the state. A solution to problem 1 is to "scan" the input as done in the NETtalk system <ref> [13] </ref>. The input is a window of values around the central value, and the next input is obtained by advancing the window one place.
Reference: 14. <author> Towell, G. G., Shavlik, J. W. and Noordewier, M. O., </author> <title> "Refinement of approximate domain theories by knowledge-base neural networks," </title> <booktitle> AAAI90, </booktitle> <year> 1990, </year> <pages> 861-866. </pages>
Reference-contexts: 1 INTRODUCTION Research in artificial neural networks (ANNs) has until recently largely ignored preexisting knowledge about the task at hand. One recent effort, the KBANN system <ref> [14] </ref>, addresses this problem by using domain knowledge to select a promising configuration for a neural network. This approach uses the domain knowledge to determine the topology of the network and biases the network so that it will start with a "good" set of weights. <p> This approach uses the domain knowledge to determine the topology of the network and biases the network so that it will start with a "good" set of weights. This approach has been effective in complex domains, such as gene recognition <ref> [9, 14] </ref>, even when the initial domain theory is not particularly correct. In this paper we discuss how KBANN's domain theory vocabulary is extended. Application to the problem of protein folding demonstrates the promise of this approach. <p> The paper finishes with a short discussion and conclusions. 2 FINITE-STATE DOMAIN THEORIES This section reviews the basic KBANN algorithm and presents our method for extending it to finite-state automata. We show examples of standard and finite-state KBANN. 2.1 STANDARD KBANN Standard KBANN <ref> [14] </ref> takes a domain theory expressed as simple rules and translates it into a corresponding network with initial weights. For example, consider the pair of rules in Figure 1 (a). A:- B,C. (a) (b) A (b) a KBANN translation of these rules.
References-found: 14

