URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P568.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts96.htm
Root-URL: http://www.mcs.anl.gov
Title: MPI-2: Extending the Message-Passing Interface  
Author: Al Geist, ORNL William Gropp, Steve Huss-Lederman, Andrew Lumsdaine, U. Ewing Lusk, William Saphir, NAS Tony Skjellum, Marc Snir, 
Address: Wis.  
Affiliation: MCS Division, ANL  ANL and the U. of  of Notre Dame  MCS Div., ANL  Mississippi State U.  IBM Corp.  
Abstract: This paper describes current activities of the MPI-2 Forum. The MPI-2 Forum is a group of parallel computer vendors, library writers, and application specialists working together to define a set of extensions to MPI (Message Passing Interface). MPI was defined by the same process and now has many implementations, both vendor-proprietary and publicly available, for a wide variety of parallel computing environments. In this paper we present the salient aspects of the evolving MPI-2 document as it now stands. We discuss proposed extensions and enhancements to MPI in the areas of dynamic process management, one-sided operations, collective operations, new language binding, real-time computing, external interfaces, and miscellaneous topics. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> World Wide Web MPI home page. </institution> <note> http://www.mcs.anl.gov/mpi/standard.html. </note>
Reference-contexts: The result of this effort was MPI (Message-Passing Interface) [8]. Implementations of MPI are now widely available, including portable and freely available implementations [2, 4, 9] and specialized versions from vendors. General information on MPI is available at <ref> [1] </ref>.
Reference: [2] <author> R. Alasdair, A. Bruce, James G. Mills, and A. Gordon Smith. </author> <title> CHIMP/MPI user guide. </title> <type> Technical Report EPCC-KTP-CHIMP-V2-USER 1.2, </type> <institution> Edinburgh Parallel Computing Centre, </institution> <month> June </month> <year> 1994. </year> <month> 13 </month>
Reference-contexts: 1 Introduction During 1993 and 1994, a group of parallel computer vendors, library writers, and application scientists met regularly to define a standard interface for message-passing libraries. The result of this effort was MPI (Message-Passing Interface) [8]. Implementations of MPI are now widely available, including portable and freely available implementations <ref> [2, 4, 9] </ref> and specialized versions from vendors. General information on MPI is available at [1].
Reference: [3] <author> Grady Booch. </author> <title> Object-Oriented Analysis and Design with Applications. </title> <publisher> Benjamin Cum--mings, </publisher> <year> 1994. </year>
Reference-contexts: requirement that there be a simple and clear one-to-one correspondence between the language-independent specification of MPI functions and the language-specific bindings of those functions. 6.1 C++ Bindings The C++ language is an object-oriented extension to the C programming language and has become the most popular object-oriented language in use today <ref> [3, 13] </ref>. Since C++ is a superset of C, one approach to providing C++ bindings is to simply reuse the C bindings. However, this approach discards much of the expressive power of C++.
Reference: [4] <author> Greg Burns, Raja Daoud, and James Vaigl. LAM: </author> <title> An open cluster environment for MPI. </title> <editor> In John W. Ross, editor, </editor> <booktitle> Proceedings of Supercomputing Symposium '94, </booktitle> <pages> pages 379-386. </pages> <institution> University of Toronto, </institution> <year> 1994. </year>
Reference-contexts: 1 Introduction During 1993 and 1994, a group of parallel computer vendors, library writers, and application scientists met regularly to define a standard interface for message-passing libraries. The result of this effort was MPI (Message-Passing Interface) [8]. Implementations of MPI are now widely available, including portable and freely available implementations <ref> [2, 4, 9] </ref> and specialized versions from vendors. General information on MPI is available at [1].
Reference: [5] <author> Peter Corbett, Dror Feitelson, Yarsun Hsu, Jean-Pierre Prost, Marc Snir, Sam Fineberg, Bill Nitzberg, Bernard Traversat, and Parkson Wong. </author> <title> MPI-IO: A parallel file I/O interface for MPI, version 0.3. </title> <type> Technical Report NAS-95-002, </type> <institution> NAS, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: MPI-2 is proposing nonblocking calls for all collective operations, many one-sided operations, and dynamic spawning. Although these significantly expand the areas covered by nonblocking operations, users still may want additional nonblocking operations. For example, in the current MPI-IO effort <ref> [5, 6] </ref>, nonblocking read and write operations are proposed. It would be advantageous to offer a standard MPI mechanism to perform these additional nonblocking operations.
Reference: [6] <author> Peter Corbett, Yarsun Hsu, Jean-Pierre Prost, Marc Snir, Sam Fineberg, Bill Nitzberg, Bernard Traversat, Parkson Wong, and Dror Feitelson. </author> <title> MPI-IO: A parallel file I/O interface for MPI, </title> <note> version 0.4. http://lovelace.nas.nasa.gov/MPI-IO, December 1995. </note>
Reference-contexts: MPI-2 is proposing nonblocking calls for all collective operations, many one-sided operations, and dynamic spawning. Although these significantly expand the areas covered by nonblocking operations, users still may want additional nonblocking operations. For example, in the current MPI-IO effort <ref> [5, 6] </ref>, nonblocking read and write operations are proposed. It would be advantageous to offer a standard MPI mechanism to perform these additional nonblocking operations.
Reference: [7] <author> Nathan E. Doss, Purushotam V. Bangalore, and Anthony Skjellum. </author> <title> MPI++ : Issues and Features. </title> <booktitle> In Proceedings of OONSKI '94, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: Thus, only minimal use of advanced features of C++ such as polymorphism would be available to MPI programmers. This is an approach similar to that taken in <ref> [7] </ref>. A full-fledged class library that uses such advanced features has been developed in conjunction with the bindings and can be found at [10]. 9 6.2 Fortran 90 Interface Fortran 90 adds a wide range of features to Fortran 77.
Reference: [8] <author> The MPI Forum. </author> <title> The MPI message-passing interface standard. </title> <note> http://www.mcs.anl.gov/mpi/standard.html, May 1995. </note>
Reference-contexts: 1 Introduction During 1993 and 1994, a group of parallel computer vendors, library writers, and application scientists met regularly to define a standard interface for message-passing libraries. The result of this effort was MPI (Message-Passing Interface) <ref> [8] </ref>. Implementations of MPI are now widely available, including portable and freely available implementations [2, 4, 9] and specialized versions from vendors. General information on MPI is available at [1].
Reference: [9] <author> William Gropp and Ewing Lusk. </author> <title> User's guide for mpich, a portable implementation of MPI. </title> <type> Technical Report ANL-95/6, </type> <institution> Argonne National Laboratory, </institution> <year> 1996. </year>
Reference-contexts: 1 Introduction During 1993 and 1994, a group of parallel computer vendors, library writers, and application scientists met regularly to define a standard interface for message-passing libraries. The result of this effort was MPI (Message-Passing Interface) [8]. Implementations of MPI are now widely available, including portable and freely available implementations <ref> [2, 4, 9] </ref> and specialized versions from vendors. General information on MPI is available at [1].
Reference: [10] <author> Andrew Lumsdaine, Brian M. McCandless, and Jeffrey M. Squyres. </author> <title> Object-oriented MPI, </title> <note> 1996. http://www.cse.nd.edu/ ~ lsc/research/oompi/. </note>
Reference-contexts: This is an approach similar to that taken in [7]. A full-fledged class library that uses such advanced features has been developed in conjunction with the bindings and can be found at <ref> [10] </ref>. 9 6.2 Fortran 90 Interface Fortran 90 adds a wide range of features to Fortran 77. These include the module facility, derived types, array syntax, dynamic memory allocation, "pointers", the ability to do strict type checking, and function overloading.
Reference: [11] <author> H. Schulzrinne, S. Casner, R. Frederick, and V. Jacobson. RTP: </author> <title> A Transport Protocol for Real-Time Applications, </title> <month> January </month> <year> 1996. </year> <institution> Internet Engineering Task Force RFC 1889; Network Working Group Standards Track. </institution>
Reference-contexts: Hence, one may eventually consider having both time- and priority-based support in concert. This is beyond what the committee currently is considering. 4.3 Other Possible Outcomes The growth of multimedia systems, including simultaneous video and sound, have led to extensions of Internet protocols. The RTP (Realtime Protocol) <ref> [11] </ref> is a good example. Developing RTP analogs for MPI-2 or beyond is a likely task for the real-time subcommittee, but such features may well have wider appeal and move into the main part of MPI-2 or later efforts. For instance, the RTP approach to message passing supports lossy protocols. <p> These issues tie in 8 closely with the programming requirements such systems. One potential outcome is that a parallel, stream-oriented protocol be considered in addition to the message-oriented protocol of MPI. This is consistent with <ref> [11] </ref> but most probably beyond the scope of MPI-2. It indicates the need for continued research and study, and possible standardization at a later date. Another area of consideration for MPI-2 is the addition of persistent collective operations.
Reference: [12] <author> Anthony Skjellum, Nathan E. Doss, and Kishore Viswanathan. </author> <title> Inter-communicator extensions to MPI in the MPIX (MPI eXtension) Library. </title> <type> Technical report, </type> <institution> Mississippi State University | Dept. of Computer Science, </institution> <month> April </month> <year> 1994. </year> <note> Draft version. </note>
Reference-contexts: Original proposals for extending intercommunicators to support collective operations, in addition to their MPI-1 point-to-point facilities, were first based on <ref> [12] </ref>, which included model implementations. The additional functionality came in three forms: more collective constructors and manipulators, what is now called "half-duplex" intercommunicator operations that extend intracommmunicator collective operations, and virtual topology-oriented versions of both the constructors and the communication procedures.
Reference: [13] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <note> second edition, 1991. This work was supported in part by the Mathematical, Information, </note> <institution> and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, </institution> <note> under Contract W-31-109-Eng-38. 14 </note>
Reference-contexts: requirement that there be a simple and clear one-to-one correspondence between the language-independent specification of MPI functions and the language-specific bindings of those functions. 6.1 C++ Bindings The C++ language is an object-oriented extension to the C programming language and has become the most popular object-oriented language in use today <ref> [3, 13] </ref>. Since C++ is a superset of C, one approach to providing C++ bindings is to simply reuse the C bindings. However, this approach discards much of the expressive power of C++.
References-found: 13

