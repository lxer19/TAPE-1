URL: http://www.cs.wisc.edu/condor/doc/carmi_fgcs_paper.ps
Refering-URL: http://www.cs.wisc.edu/~pruyne/CV.html
Root-URL: 
Email: mirong@cs.wisc.edu  
Title: Interfacing Condor and PVM to harness the cycles of workstation clusters  
Author: Jim Pruyne and Miron Livny 
Address: fpruyne,  
Affiliation: Department of Computer Sciences University of Wisconsin-Madison  
Abstract: A continuing challenge to the scientific research and engineering communities is how to fully utilize computational hardware. In particular, the proliferation of clusters of high performance workstations has become an increasingly attractive source of compute power. Developments to take advantage of this environment have previously focused primarily on managing the resources, or on providing interfaces so that a number of machines can be used in parallel to solve large problems. Both approaches are desirable, and indeed should be complementary. Unfortunately, the resource management and parallel processing systems are usually developed by independent groups, and they usually do not interact well together. To bridge this gap, we have developed a framework for interfacing these two sorts of systems. Using this framework, we have interfaced PVM, a popular system for parallel programming with Condor, a powerful resource management system. This combined system is operational, and we have made further developments to provide a single coherent environment. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam, </author> <title> "Pvm 3 user's guide and reference manual," </title> <type> Tech. Rep. </type> <institution> ORNL/TM-12187, Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The most successful and most widely used MPE so far has been the Parallel Virtual Machine (PVM) system which was developed at the Oak Ridge National Laboratory <ref> [1] </ref>. PVM provides an easy, reliable message passing model, and is freely available. Another approach to capturing the power of workstation cluster has been to develop distributed Resource Management (RM) systems. <p> The WoDi server in turn passes work steps on to worker processes, and forwards their results back to the master. Because WoDi handles all resource management concerns for the application, only the 12 main () - buf = pack_initialization_data (); num_classes = 2; class_needs [0] = 10; class_needs <ref> [1] </ref> = 5; wodi_init (buf, num_classes, class_needs, WORK_TAG, RESP_TAG); for (cycle = 1; cycle &lt;= CYCLES; cycle++) - buf = pack_cycle_initialization_data (); wodi_begin_cycle (cycle, buf, STEPS); for (step = 1; step &lt;= STEPS; step++) - buf = pack_workstep (); wodi_sendwork (step, buf); - for (step = 1; step &lt;= STEPS; step++)
Reference: [2] <author> M. J. Litzkow, M. Livny, and M. W. </author> <title> Mutka, "Condor: A hunter of idle workstations," </title> <booktitle> in Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 104-111, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: A number of research and commercial systems of this type have been developed as a result of these efforts including Condor <ref> [2] </ref>, DQS [3], LoadLeveler [4], and Utopia [5]. Condor is unique among these systems because it has, from its inception, been designed to be opportunistic, utilizing idle cycles on privately owned workstations.
Reference: [3] <author> D. Duke, T. Green, and J. Pasko, </author> <title> "Research toward a heterogeneous networked computing cluster: The distributed queuing system version 3.0," </title> <type> tech. rep., </type> <institution> Supercomputer Computations Research Institute, Florida State University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: A number of research and commercial systems of this type have been developed as a result of these efforts including Condor [2], DQS <ref> [3] </ref>, LoadLeveler [4], and Utopia [5]. Condor is unique among these systems because it has, from its inception, been designed to be opportunistic, utilizing idle cycles on privately owned workstations.
Reference: [4] <institution> IBM Corporation, IBM LoadLeveler: </institution> <note> User's Guide, </note> <year> 1993. </year>
Reference-contexts: A number of research and commercial systems of this type have been developed as a result of these efforts including Condor [2], DQS [3], LoadLeveler <ref> [4] </ref>, and Utopia [5]. Condor is unique among these systems because it has, from its inception, been designed to be opportunistic, utilizing idle cycles on privately owned workstations.
Reference: [5] <author> S. Zhou, J. Wang, X. Zheng, and P. Delisle, </author> <title> "Utopia: A load sharing facility for large, heterogeneous distributed computing systems," </title> <type> Tech. Rep. </type> <institution> CSRI-257, Computer Systems Research Institute, University of Toronto, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: A number of research and commercial systems of this type have been developed as a result of these efforts including Condor [2], DQS [3], LoadLeveler [4], and Utopia <ref> [5] </ref>. Condor is unique among these systems because it has, from its inception, been designed to be opportunistic, utilizing idle cycles on privately owned workstations.
Reference: [6] <author> A. Bricker, M. Litzkow, and M. Livny, </author> <title> "Condor technical summary," </title> <type> Tech. Rep. 1069, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: For the purpose of allocation decisions, all resources within a class are considered to be the same. Resource classes are defined using constraints in the same way that a user specifies the desired resource for a sequential job in Condor <ref> [6] </ref>. A resource is considered to be a member of a class if the constraint associated with the class evaluates to true when the keywords' values in the resource's classified are substituted. Sample constraints are shown in Figure 4.
Reference: [7] <author> D. Gelernter and D. Kaminsky, </author> <title> "Supercomputing out of recycled garbage: Preliminary experience with piranha," </title> <booktitle> in Proceedings of the ACM, International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: It is the responsibility of WoDi to monitor the status of all the resources allocated to a job, and insure that the results of each work step are returned to the master exactly once. In some respects, WoDi is similar to Piranha <ref> [7] </ref> which also helps master-workers applications adapt to dynamic resources. Piranha, however, is restricted to working with the Linda [8] tuple-space based communication environment where as WoDi is intended to work in a general message passing environment.
Reference: [8] <author> D. Gelernter, </author> <title> "Generative communications in linda," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 7, </volume> <pages> pp. 80-112, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: In some respects, WoDi is similar to Piranha [7] which also helps master-workers applications adapt to dynamic resources. Piranha, however, is restricted to working with the Linda <ref> [8] </ref> tuple-space based communication environment where as WoDi is intended to work in a general message passing environment. An additional benefit to users is that WoDi can monitor the history of both the resources it is using and the work it is distributing to make intelligent work assignments.
Reference: [9] <author> M. J. Litzkow and M. Solomon, </author> <title> "Supporting checkpointing and process migration outside the unix kernel," </title> <booktitle> in Proceedings of the Winter Usenix Conference, </booktitle> <address> (San Francisco, CA), </address> <year> 1992. </year>
Reference-contexts: With one shared server no juggling of resources is needed leading to higher utilization. 5.3 CoCheck: Checkpointing for PVM The principal mechanism used by Condor to insure jobs progress in spite of resource reclamations is checkpointing <ref> [9] </ref>, [10]. By checkpointing sequential processes, Condor is able to restart an application from the point it was forced to vacate a resource due to its owner returning. Check-pointing could serve a similar purpose for parallel applications, but checkpointing of parallel applications is significantly more complicated.
Reference: [10] <author> T. Tannenbaum and M. Litzkow, </author> <title> "The condor distributed processing system," </title> <journal> Dr. Dobb's Journal, </journal> <pages> pp. 40-48, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: With one shared server no juggling of resources is needed leading to higher utilization. 5.3 CoCheck: Checkpointing for PVM The principal mechanism used by Condor to insure jobs progress in spite of resource reclamations is checkpointing [9], <ref> [10] </ref>. By checkpointing sequential processes, Condor is able to restart an application from the point it was forced to vacate a resource due to its owner returning. Check-pointing could serve a similar purpose for parallel applications, but checkpointing of parallel applications is significantly more complicated.
Reference: [11] <author> G. Stellner and J. Pruyne, </author> <title> "Resource management and checkpointing for pvm," </title> <booktitle> in Proceedings of the 2nd European Users' Group Meeting, </booktitle> <month> Sept. </month> <year> 1995. </year> <month> 22 </month>
Reference-contexts: Check-pointing could serve a similar purpose for parallel applications, but checkpointing of parallel applications is significantly more complicated. We have addressed the problem of checkpoint-ing parallel applications, in collaboration with the Technical University of Munich, in a system called CoCheck (for Consistent Checkpointing) <ref> [11] </ref>. CoCheck was designed to provide checkpointing and process migration services to PVM programs. The principal problem to be overcome by CoCheck, or any checkpointing system for communication processes, is to insure that no messages are lost as a result of a checkpoint.
Reference: [12] <author> W. A. Shelton, G. M. Stocks, R. G. Jordan, Y. Liu, L. Qui, D. D. Johnson, F. J. Pinski, J. B. Staunton, and B. Ginatempo, </author> <title> "First principles simulation of materials properties," </title> <booktitle> in Proceedings of SHPCC '94, </booktitle> <pages> pp. 103-110, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: there were also no MIPS machines available to bring us back up to our desired level of sixty. 6.2 Experience with WoDi Among the applications which we are running with WoDi is a materials science application which is designed to predict the properties of new materials based on first principles <ref> [12] </ref>. This application was originally written using stand alone PVM and a master-workers approach to parallelism. Because it was written using PVM, it assumed a constant allocation of resources.
Reference: [13] <author> R. L. Graham, </author> <title> "Bounds on multiprocessing timing anomalies," </title> <journal> Siam Journal of Applied Mathematics, </journal> <volume> vol. 17, no. 2, </volume> <pages> pp. 416-429, </pages> <year> 1969. </year> <month> 23 </month>
Reference-contexts: To generate these simulated results, we assume that all resources are identical and that none are lost during a cycle, and that work steps are given to processors using the greedy approach described in <ref> [13] </ref> in which the longest work steps are distributed first. Each step is considered to be of constant length equal to its average. These results show that in this ideal environment, there is no further benefit to execution time beyond 16 processors. <p> Ordering Length Occupancy Time No 5:57 116:53 53:03 47% Table 2: The effect of ordering trade-off. 6.2.2 Work Step Ordering WoDi's primary decision making responsibility is ordering of work steps. Previously, we assumed a greedy work step distribution algorithm <ref> [13] </ref>, and this is exactly what WoDi employs. Table 2 shows the advantage of using this strategy as compared to the original application's approach of sending out work steps in sequential order.
References-found: 13

