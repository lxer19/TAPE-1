URL: http://arch.cs.ucdavis.edu/~chong/250C/sm-mp/asplos6_sm_mp.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/sm-mp/
Root-URL: http://www.cs.ucdavis.edu
Email: fchandra,larusg@cs.wisc.edu  amr@cs.princeton.edu  
Title: Where is Time Spent in Message-Passing and Shared-Memory Programs?  
Author: Satish Chandra James R. Larus Anne Rogers 
Address: 1210 West Dayton Street Madison, WI 53706 USA  35 Olden Street  Princeton, NJ 08544 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  Department of Computer Science  Princeton University  
Note: Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM.  
Abstract: Message passing and shared memory are two techniques parallel programs use for coordination and communication. This paper studies the strengths and weaknesses of these two mechanisms by comparing equivalent, well-written message-passing and shared-memory programs running on similar hardware. To ensure that our measurements are comparable, we produced two carefully tuned versions of each program and measured them on closely-related simulators of a message-passing and a shared-memory machine, both of which are based on same underlying hardware assumptions. We examined the behavior and performance of each program carefully. Although the cost of computation in each pair of programs was similar, synchronization and communication differed greatly. We found that message-passing's advantage over shared-memory is not clear-cut. Three of the four shared-memory programs ran at roughly the same speed as their message-passing equivalent, even though their communication patterns were different. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference-contexts: In tuning a program, we frequently found insights and techniques from one version helpful in improving the other. Both machine simulators are based on the Wis-consin Wind Tunnel [19]. We used its Dir n NB protocol simulation <ref> [1] </ref> as our cache-coherent shared-memory machine. We built a simulator of a message-passing machine similar to a CM-5, which was detailed enough to execute directly a slightly modified version of the Thinking Machine's active message library (CMAML) and an unmodified copy of the TMC message-passing library (CMMD). <p> Counters in these data structures keep track of the transmission in progress. High level send and receive functions initialize the appropriate send/receive data structures and handshake to exchange the receiver's channel number. 4.2 Shared Memory The shared-memory machine uses the invalidation-based Dir n NB cache-coherence protocol <ref> [1] </ref>. Each processor node's local memory locations have global addresses, so they can be referenced by other nodes. A node maintains a directory that manages coherence for shared cache blocks in its local memory. The data transfer and coherence unit is a cache block (32 bytes in our experiments).
Reference: [2] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathon Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This cost appeared both in increased computation time to manage buffers and the 3-42% of program time spent in communication library routines. Many alternatives|such as faster hardware <ref> [2] </ref>, faster libraries, and protocol compilers [7]|could reduce this overhead. Software overhead in processing low-latency (fast-turnaround) messages is a major weakness of the CM-5 message-passing system. These messages are fundamental to performing reductions or broadcasts in software (i.e., in Gauss).
Reference: [3] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishna-murthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 262-273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Two of the four programs (Gauss and MSE) started as message-passing programs. From them, we wrote shared-memory programs. One program (LCP) started as a shared-memory program. From it, we wrote a message passing program. The final program (EM3D) started as a Split-C <ref> [3] </ref> program, which was the basis for message-passing and conventional shared-memory programs. In tuning a program, we frequently found insights and techniques from one version helpful in improving the other. Both machine simulators are based on the Wis-consin Wind Tunnel [19]. <p> The average queuing delay at a directory is 200 cycles. These delays, which are lower than the software latencies in the message passing code, will become untenable for larger systems. 5.3 EM3D EM3D models propagation of electromagnetic waves through objects in three dimensions <ref> [3] </ref>. The problem is framed as a computation on a bipartite graph with directed edges from E nodes, which represent electric fields, to H nodes, which represent magnetic fields, and vice versa. <p> The next section discusses the program's data structures. Then, we examine the performance of the initialization phase and main loop. 5.3.1 Data Structures Before analyzing performance results, we need to explain the program's data structures. The Split-C version <ref> [3] </ref> heavily influenced our implementation of EM3D-MP; both use ghost graph nodes to shadow remote source nodes. A ghost node holds a local copy of a remote node's value. EM3D-MP differs slightly from the Split-C code in this respect.
Reference: [4] <author> David Culler, Richard Karp, David Patterson, Abhijit Sa-hay, Klaus Erik Schauser, Eunice Santos, Ramesh Subra-monian, and Thorsten von Eicken. </author> <title> LogP: Toward a Realistic Model of Parallel Computation. </title> <booktitle> In Fifth ACM SIG-PLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Most of the effort in optimizing this program was obtaining efficient implementations of these primitives. Gauss-MP uses carefully tuned reduction and broadcast routines. Both routines are based on a lop-sided tree, whose superior performance was suggested by the LogP model <ref> [4] </ref> under the assumption that message send and receive overhead are higher than network latency. We experimented with several approaches before settling on lop-sided trees.
Reference: [5] <author> Phillip M. Dickens, Philip Heidelberger, and David M. Nicol. </author> <title> A Distributed Memory LAPSE: Parallel Simulation of Message-Passing Programs. </title> <booktitle> In Proceedings of the 8th Workshop on Parallel and Distributed Simulation (PADS '94), </booktitle> <pages> pages 32-38, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: For the most part, this change involved disabling the shared address space, adding calls into WWT to simulate the memory-mapped locations in the CM-5 network interface, and using WWT's event mechanisms to simulate message transmission. Our message-passing simulator is similar to LAPSE <ref> [5] </ref>, a direct execution simulator for message-passing machines that runs on the Intel Paragon. One difference between the two simulators is that LAPSE models network contention. To provide a communication library for message-passing programs, we ported the Active Message [22] layer from Thinking Machines' CMMD library to the Wind Tunnel.
Reference: [6] <author> Babak Falsafi, Alvin Lebeck, Steven Reinhardt, Ioannis Schoinas, Mark D. Hill, James Larus, Anne Rogers, and David Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <month> November </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Finally, EM3D benefited the most from message passing. Dir n NB 's invalidation-based cache-coherence policy proved to be an expensive way (in time and number of messages) of transferring data between a producer and consumer <ref> [6] </ref>. The four programs used the communication mechanisms in very different ways, which (except for EM3D) did not demonstrate a convincing advantage for either approach. This comparison of message passing and shared memory should be useful to designers and users of parallel machines. <p> Note that this caching comes at a high cost because each update requires four messages (two to invalidate, one to request a value, and one to send it) <ref> [6, 20] </ref>. The main loop references only the value field from remote nodes. The rest of a node's data is unnecessary. <p> A better approach is to replace the invalidation-based cache-coherence protocol with a bulk update protocol, which requires only a single message to transmit new values from Processor P to Processor Q. Falsafi et al. replaced an invalidation-based protocol with a bulk update protocol in a shared-memory version of EM3D <ref> [6] </ref>. The resulting program performed equivalently with EM3D-MP. Similar protocol changes could benefit other programs, most notably the broadcasts in Gauss. 5.4 Linear Complementarity Problem Our final application solves the linear complementarity problem (LCP) using a multi-sweep synchronous successive over-relaxation algorithm [14].
Reference: [7] <author> Edward W. Felten. </author> <title> Protocol Compilation: High-Performance Communication for Parallel Programs. </title> <type> Technical Report 93-09-09, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> September </month> <year> 1993. </year>
Reference: [8] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <month> November </month> <year> 1993. </year> <note> Earlier version appeared in ASPLOS V, </note> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: In the example above, Processor Q could flush its copy of a remote H node from its cache, thereby changing a 2-message invalidate into a single-message cache replacement operation. 6 In addition, Processor Q could hide some latency by prefetching. The cooperative prefetch operation in CSM <ref> [8] </ref> works well in this situation, because a consumer need not worry about issuing a prefetch too 6 As the data set outgrows a cache, the likelihood that a line will still be in the cache when the remote write occurs diminishes, which reduces the value of flushes. 10 Appears in:
Reference: [9] <author> W. Daniel Hillis and Lewis W. Tucker. </author> <title> The CM-5 Connection Machine: A Scalable Supercomputer. </title> <journal> Communications of the ACM, </journal> <volume> 36(11) </volume> <pages> 31-40, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Both machines are close to the hardware of a Thinking Machines CM-5 <ref> [9] </ref>. Our message-passing interface matches the CM-5 data network interface closely enough that we ran the full TMC message-passing libraries. The shared-memory system uses a full-map write-invalidate protocol. More importantly, both machines share common assumptions about the base hardware.
Reference: [10] <author> Intel Corporation. </author> <title> Paragon Technical Summary. Intel Supercomputer Systems Division, </title> <year> 1993. </year>
Reference-contexts: Unlike the CM-5, our simulation does not support broadcast or reduction hardware in either machine. This change makes comparison with shared memory easier and enables us to study the cost of implementing these operations in software (as is necessary on other machines <ref> [10] </ref>). 4.1 Message Passing The message-passing machine sends and receives messages through a memory-mapped network interface. 2 TMC's CMMD library is implemented to use 20 byte packets. 3 The 40 byte size corresponds to the cache block size plus some control information.
Reference: [11] <author> Alexander C. Klaiber and Henry M. Levy. </author> <title> A Comparison of Message Passing and Shared Memory Architectures for Data Parallel Programs. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 94-105, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Reprinted by permission of ACM. not compare the performance of these two types of machines or programs directly. The second group of papers used simulation to compare machines. This approach opens the possibility of more detailed measurements, but requires two accurate, comparable simulators. Klaiber and Levy <ref> [11] </ref> compared message traffic in message-passing and shared-memory programs using a combination of direct execution and simulation. They compiled data-parallel programs, written in C*, with a compiler that invoked a run-time communication library. Their instrumented libraries produced a trace of off-processor references for message-passing and shared-memory simulators.
Reference: [12] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubia-towicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: This comparison of message passing and shared memory should be useful to designers and users of parallel machines. Designers need to know which approach offers the highest performance. Recent evidence <ref> [12] </ref> (and our results) suggests than neither approach dominates the other, which argues for incorporating both in a machine. When this occurs, programmers and compiler writers will need to make frequent choices between alternative mechanisms. <p> Finally, their simulators reported only message traffic (both number and amount), not execution time. The relationship between traffic and elapsed time is unclear. Our measurements focused on execution time, although we also collected traffic information. Kranz et al. <ref> [12] </ref> explored the use of the message passing integrated in the MIT Alewife shared-memory machine and discussed how messages improved the performance of several common operations, such as barriers, thread invocation, thread scheduling, and bulk data transfer. <p> Programs that exploit this feature, such as Gauss, can perform better than equivalent message-passing code, which interposes a level of software to respond to data requests. Another important contribution of this research is development and demonstration of the tools to study 9 Kranz et al. <ref> [12] </ref> show that with prefetching this cost can be decreased. 12 Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM. the tradeoffs between message passing and shared memory.
Reference: [13] <author> Thomas J. LeBlanc and Evangelos P. Markatos. </author> <title> Shared Memory Vs. Message Passing in Shared-Memory Multiprocessors. </title> <booktitle> In Fourth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> Dallas, TX, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: The best message-passing implementation reduced message traffic significantly, which improved execution time, with a small degradation in solution quality. This work explored parallel programming techniques and ways by which programmers can manage parallel data efficiently. LeBlanc and Markatos <ref> [13] </ref> studied load balancing for message-passing and shared-memory programs. They again simulated message passing on a shared-memory machine and compared a light-weight thread model (in shared memory) against a static partitioning model in message passing. These experiments did 2 Appears in: "ASPLOS VI," Oct. 1994.
Reference: [14] <author> R. De Leone, O.L. Mangasarian, and T.-H. Shiau. </author> <title> Multi-Sweep Asynchronous Parallel Successive Overrelaxation for the Nonsymmetric Linear Complementarity Problem. </title> <journal> Annals of Operations Research, </journal> <volume> 22 </volume> <pages> 43-54, </pages> <year> 1990. </year>
Reference-contexts: The resulting program performed equivalently with EM3D-MP. Similar protocol changes could benefit other programs, most notably the broadcasts in Gauss. 5.4 Linear Complementarity Problem Our final application solves the linear complementarity problem (LCP) using a multi-sweep synchronous successive over-relaxation algorithm <ref> [14] </ref>. The linear complementarity problem finds a vector z that satisfies the equations M z + q 0, z 0 and z (M z + q) = 0. In our case, M is a symmetric sparse matrix, q is a (dense) vector, and the problem has 4096 variables. <p> De Leone et al. showed that faster convergence results if updates to the global solution vector become available to other processors as soon as they are computed <ref> [14] </ref>. This approach, however, increases the amount of communication. We implemented this alternative as well. ALCP-SM writes new values directly to the global solution vector. Processors synchronize every five iterations to test for convergence.
Reference: [15] <author> Calvin Lin and Lawrence Snyder. </author> <title> A Comparison of Programming Models for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II-163-170, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The first group compared a shared-memory program against a similar program written with a message-passing library that was implemented in shared memory on the same machine. Lin and Snyder <ref> [15] </ref> compared shared-memory programs written under a naive model and a more accurate model. They found that the latter programs performed better on both a cache-coherent and a NUMA shared-memory machine. Ngo and Sny-der [18] compared several shared-memory programs against message-passing versions running on the same shared-memory machine.
Reference: [16] <author> Margaret Martonosi and Anoop Gupta. </author> <title> Tradeoffs in Message Passing and Shared Memory Implementations of a Standard Cell Router. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing (Vol. III Algorithms and Applications), </booktitle> <pages> pages III88-96, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: In addition, the two sets of programs are not comparable since the shared-memory codes were written naively. Also, the programs were executed on a real machine, which limited the comparison to elapsed time and speedup. Martonosi and Gupta <ref> [16] </ref> compared, on a shared-memory machine, a variety of shared-memory and message-passing implementations of the Locus-Route standard cell routing program. They measured performance in terms of message traffic, execution time, and solution quality.
Reference: [17] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: At the beginning, only node 0 executes. After preliminary initialization it invokes the create (f) routine, which duplicates its data segments and starts subroutine f on all other nodes. The lock and unlock operations use MCS locks <ref> [17] </ref>. 5 The barrier function uses the hardware barrier. 5 Results This section discusses our measurement of four programs. <p> Gauss-SM reductions and broadcasts exploit shared-memory's fine-grain, low-latency communication. Reductions use the same approach as the upward phase of MCS barriers <ref> [17] </ref> and account for only 6% of total time. Gauss-SM broadcasts a value by letting all processors read it. Since all processors wait at a barrier until the write completes (11.3M cycles), these invalidates are on the program's critical path. However, they occur at hardware, not software speed.
Reference: [18] <author> Ton A. Ngo and Lawrence Snyder. </author> <title> On the Influence of Programming Models on Shared Memory Computer Performance. </title> <booktitle> In Scalable High Performance Computing Conference (SHPCC '92), </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: Lin and Snyder [15] compared shared-memory programs written under a naive model and a more accurate model. They found that the latter programs performed better on both a cache-coherent and a NUMA shared-memory machine. Ngo and Sny-der <ref> [18] </ref> compared several shared-memory programs against message-passing versions running on the same shared-memory machine. The message-passing programs, again written to distinguish local and nonlocal data, performed better. These papers compared two styles of writing a program.
Reference: [19] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmet-rics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The final program (EM3D) started as a Split-C [3] program, which was the basis for message-passing and conventional shared-memory programs. In tuning a program, we frequently found insights and techniques from one version helpful in improving the other. Both machine simulators are based on the Wis-consin Wind Tunnel <ref> [19] </ref>. We used its Dir n NB protocol simulation [1] as our cache-coherent shared-memory machine. <p> This result accords with our findings that, in many situations, shared-memory communication is effective and has overhead as low as message-passing communication. 3 Simulation Methodology We used the Wisconsin Wind Tunnel (WWT) to simulate the message-passing and shared-memory machines accurately and efficiently <ref> [19] </ref>. WWT is a system for virtual prototyping of parallel computers. It directly executes a parallel program written and compiled for a proposed parallel computer (the target). WWT accurately calculates the target program's execution with a distributed, discrete-event simulation running on a parallel host computer, in our case a CM-5. <p> The Wind Tunnel system provides two key functions: the ability to invoke simulation code a miss in the target machine's cache and a mechanism for deterministically simulating interactions among nodes in a parallel computer. For more details, see the paper by Reinhardt et al. <ref> [19] </ref>. The shared-memory simulation uses the first feature to detect and model both local and shared address cache misses. The message-passing simulation uses it only for local misses. Both simulators use the discrete event simulation to ensure causality among processors and compute a program's execution time.
Reference: [20] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Note that this caching comes at a high cost because each update requires four messages (two to invalidate, one to request a value, and one to send it) <ref> [6, 20] </ref>. The main loop references only the value field from remote nodes. The rest of a node's data is unnecessary. <p> Reprinted by permission of ACM. early. In addition, the Stache policy <ref> [20] </ref> would reduce the cost of the cache replacements by storing data in local memory, rather than returning it to its home node. <p> EM3D-MP sends a couple hundred messages to transfer the data that requires several hundred thousand cache misses and many times that many protocol messages. Mechanisms for bulk data transfer and more efficient protocols have been proposed <ref> [23, 20] </ref>. We identified two major sources of overhead in shared-memory programs.
Reference: [21] <author> F. Traenkle. </author> <title> Parallel Programming Models and Boundary Integral Equation Methods for Microstructure Electrostatics. </title> <type> Master's thesis, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1993. </year>
Reference-contexts: Although a small number of examples, the programs' behavior varies dramatically enough to demonstrate clearly many advantages and disadvantages of message passing and shared memory. 5.1 Microstructure Electrostatics Our first benchmark is a program that computes boundary integral solutions of the Laplace equation arising from simulating microstructure electrostatics <ref> [21] </ref>. Both the shared-memory (MSE-SM) and message-passing (MSE-MP) versions were written and optimized by researchers in the University of Wisconsin Chemical Engineering department. The program solves an N body system, where each body is discretized into M boundary elements.
Reference: [22] <author> Thorsten von Eicken, David E. Culler, Seth Copen Gold-stein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrating Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Our message-passing simulator is similar to LAPSE [5], a direct execution simulator for message-passing machines that runs on the Intel Paragon. One difference between the two simulators is that LAPSE models network contention. To provide a communication library for message-passing programs, we ported the Active Message <ref> [22] </ref> layer from Thinking Machines' CMMD library to the Wind Tunnel. The complete CMMD library runs as part of the target program, as it does on a CM-5.
Reference: [23] <author> Steven Cameron Woo, Jaswinder Pal Singh, and John L. Hennessy. </author> <title> The Performance Advantages of Integrating Message Passing in Cache-Coherent Multiprocessors. </title> <type> Technical Report CSL-TR-93-593, </type> <institution> Department of Computer Science, Stanford University, </institution> <month> November </month> <year> 1993. </year> <note> To appear in ASPLOS VI. 13 </note>
Reference-contexts: They measured several versions of a simple Jacobian SOR code on an Alewife simulator, which supported both shared memory and message passing. Their results agreed with our finding (Section 5) that message passing and shared memory can perform equally well. Woo et al. <ref> [23] </ref> studied the implications of adding a message passing-like block transfer facility to shared memory. They added this feature to an architectural simulator of the shared-memory Stanford FLASH machine and modified five programs to use it. <p> EM3D-MP sends a couple hundred messages to transfer the data that requires several hundred thousand cache misses and many times that many protocol messages. Mechanisms for bulk data transfer and more efficient protocols have been proposed <ref> [23, 20] </ref>. We identified two major sources of overhead in shared-memory programs.
References-found: 23

