URL: http://www.eecs.umich.edu/Rio/papers/rioFileCacheTech.ps
Refering-URL: http://www.eecs.umich.edu/Rio/papers.html
Root-URL: http://www.eecs.umich.edu
Email: -pmchen,weeteck,gurur,caycock-@eecs.umich.edu  
Title: The Rio File Cache: Surviving Operating System Crashes 1 The Rio File Cache: Surviving Operating
Author: Peter M. Chen, Wee Teck Ng, Gurushankar Rajamani, Christopher M. Aycock 
Address: Michigan  
Affiliation: Computer Science and Engineering Division Department of Electrical Engineering and Computer Science University of  
Abstract: One of the fundamental limits to high-performance, high-reliability file systems is memorys vulnerability to system crashes. Because memory is viewed as unsafe, systems periodically write data back to disk. The extra disk traffic lowers performance, and the delay period before data is safe lowers reliability. The goal of the Rio (RAM I/O) file cache is to make ordinary main memory safe for persistent storage by enabling memory to survive operating system crashes. Reliable memory enables a system to achieve the best of both worlds: reliability equivalent to a write-through file cache, where every write is instantly safe, and performance equivalent to a pure write-back cache, with no reliability-induced writes to disk. To achieve reliability, we protect memory during a crash and restore it during a reboot (a warm reboot). Extensive crash tests show that even without protection, warm reboot enables memory to achieve reliability close to that of a write-through file system while performing 20 times faster. Rio makes all writes immediately permanent, yet performs faster than systems that lose 30 seconds of data on a crash: 35% faster than a standard delayed-write file system and 8% faster than a system that delays both data and meta-data. For applications that demand even higher levels of reliability, Rios optional protection mechanism makes memory even safer than a write-through file system while while lowering performance 20% compared to a pure write-back system. 
Abstract-found: 1
Intro-found: 1
Reference: [Abbott94] <author> M. Abbott, D. Har, L. Herger, M. Kauffmann, K. Mak, J. Murdock, C. Schulz, T. B. Smith, B. Tremaine, D. Yeh, and L. Wong. </author> <title> Durable Memory RS/6000 System Design. In Proceedings of the The Rio File Cache: </title> <booktitle> Surviving Operating System Crashes 10 1994 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 414423, </pages> <year> 1994. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures <ref> [Abbott94] </ref>.
Reference: [Akyurek95] <author> Sedat Akyurek and Kenneth Salem. </author> <title> Management of partially safe buffers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(3):394407, </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>, and countless papers have sought to improve disk performance via data placement, logging, scheduling, and so forth. 7 Conclusions We have made a case for reliable file caches: main memory that can survive operating system crashes and be as safe and permanent as disk.
Reference: [APC96] <author> The Power Protection Handbook. </author> <title> American Power Conversion, </title> <year> 1996. </year>
Reference-contexts: Memorys vulnerability to power outages is easy to understand and fix. A $119 uninterruptible power supply can keep a system running long enough to dump memory to disk in the event of a power outage <ref> [APC96] </ref>, or one can use non-volatile memory such as Flash RAM [Wu94]. We do not consider power outages further in this paper. Memorys vulnerability to OS crashes is more challenging.
Reference: [Baker91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198212, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Unix file systems mitigate the performance lost in reliability-induced disk writes by waiting 30 seconds before writing data, but this ensures the loss of data written within 30 seconds of a crash [Ousterhout85]. In addition, 1/3 to 2/3 of newly written data lives longer than 30 seconds <ref> [Baker91, Hartman93] </ref>, so a large fraction of writes must eventually be written through to disk under this policy. A longer delay decreases disk traffic due to writes but risks losing even more data. <p> Reliable file caches have striking implications for future system designers: Write-backs to disk are no longer needed except when the file cache fills up, changing the assumptions about write traffic behind some file system research such as LFS <ref> [Rosenblum92, Baker91] </ref>. Delaying writes to disk until the file cache fills up enables the largest possible number of files to die in memory and enables remaining files to be written out efficiently in arbitrarily large units. Thus Rio improves performance moderately over delayed-write systems.
Reference: [Baker92a] <author> Mary Baker, Satoshi Asami, Etienne Deprit, John Ousterhout, and Margo Seltzer. </author> <title> Non-Volatile Memory for Fast Reliable File Systems. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 1022, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: It is hence relatively easy for many simple software errors (such as de-referencing an uninitialized pointer) to accidentally corrupt the contents of memory <ref> [Baker92a] </ref>. The main issue in protection is how to control accesses to the file cache. We want to make it unlikely that non-file-cache procedures will accidentally corrupt the file cache, essentially making the file cache a protected module within the monolithic kernel. <p> To make data accessible during a hardware failure, it should be possible to move a memory board to a different machine without losing power (just as disks can be moved without losing data) <ref> [Moran90, Baker92a] </ref>. 6 Related Work We divide the research related to this paper into two areas: field studies/fault injection and protection schemes. 6.1 Field Studies and Fault Injection Studies have shown that software is the dominant cause of system outages [Gray90], and several studies have investigated system software errors. <p> The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>, and countless papers have sought to improve disk performance via data placement, logging, scheduling, and so forth. 7 Conclusions We have made a case for reliable file caches: main memory that can survive operating system crashes and be as safe and permanent as disk.
Reference: [Baker92b] <author> Mary Baker and Mark Sullivan. </author> <title> The Recovery Box: Using Fast Recovery to Provide High Availability in the UNIX Environment. </title> <booktitle> In Proceedings USENIX Summer Conference, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: We use two strategies to detect file corruption: checksums detect direct corruption, and a synthetic workload called memTest detects direct and indirect corruption. The first method to detect corruption maintains a checksum of each memory block in the file cache <ref> [Baker92b] </ref>. We update the checksum in all procedures that write the file cache; unintentional changes to file cache buffers result in an inconsistent checksum. <p> Harp protects a log of recent modifications by replicating it in volatile, battery-backed memory across several server nodes [Liskov91]. The Recovery Box keeps special system state in a region of memory accessed only through a rigid interface <ref> [Baker92b] </ref>. No attempt is made to prevent other procedures from accidentally modifying the recovery box, although the system detects corruption by maintaining checksums.
Reference: [Banatre91] <author> Michel Banatre, Gilles Muller, Bruno Rochat, and Patrick Sanchez. </author> <title> Design decisions for the FTM: a general purpose fault tolerant machine. </title> <booktitle> In Proceedings of the 1991 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 7178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: A small amount of hardware support at the memory level would make protection easier. An ideal memory controller would enable file system procedures to prevent writes to certain physical pages <ref> [Banatre91] </ref>. One simple way to implement this is for the controller to store a write-permission bit for each memory page and map the write-permission bits into the processors address space. <p> Banatre, et. al. implement stable transactional memory, which protects memory contents with dual The Rio File Cache: Surviving Operating System Crashes 9 memory banks, a special memory controller, and explicit calls to allow write access to specified memory blocks <ref> [Banatre91] </ref>. Our work seeks to make all files in memory reliable without special-purpose hardware or replication. General mechanisms may be used to help protect memory from software faults. [Needham83] suggests changing a machines microcode to check certain conditions when writing a memory word.
Reference: [Barton90] <author> James H. Barton, Edward W. Czeck, Zary Z. Segall, and Daniel P. Siewiorek. </author> <title> Fault injection experiments using FIAT. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4):575582, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: Most crashes occurred within 15 seconds after the fault was injected. If a fault does not crash the machine after ten minutes, we discard the run and reboot the system. 4 The first category of faults ips random bits in the kernels address space <ref> [Barton90, Kanawati95] </ref>. We target three areas of the kernels address space: the kernel text, heap, and stack. These faults are easy to inject, and they cause a variety of different crashes. They are the least realistic of our bugs, however. <p> Software fault injection is a popular technique for evaluating the behavior of prototype systems in the presence of hardware and software faults. See [Iyer95] for an excellent introduction to the overall area and a summary of much of the past work on fault injection, such as FINE [Kao93], FIAT <ref> [Barton90] </ref>, and FER-RARI [Kanawati95]. As with field studies of system crashes, these papers on fault injection inspired many of the fault categories used in this paper.
Reference: [Biswas93] <author> Prabuddha Biswas, K. K. Ramakrishnan, Don Towsley, and C. M. Krishna. </author> <title> Performance Analysis of Distributed File Systems with Non-Volatile Caches. </title> <booktitle> In Proceedings of the 1993 International Symposium on High Performance Distributed Computing (HPDC-2), </booktitle> <pages> pages 252262, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>, and countless papers have sought to improve disk performance via data placement, logging, scheduling, and so forth. 7 Conclusions We have made a case for reliable file caches: main memory that can survive operating system crashes and be as safe and permanent as disk.
Reference: [Chapin95] <author> John Chapin, Mendel Rosenblum, Scott Devine, Tirthankar Lahiri, Dan Teodosiu, and Anoop Gupta. Hive: </author> <title> Fault Containment for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1995 Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations [Johnson82, Wahbe92]. Hive uses the Flash firewall to protect memory against wild writes by other processors in a multiprocessor <ref> [Chapin95] </ref>. Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. Object code modification has been suggested as a way to provide data breakpoints [Kessler90, Wahbe92] and fault isolation between software modules [Wahbe93].
Reference: [Chutani92] <author> Sailesh Chutani, Owen T. Anderson, Michael L. Kazar, Bruce W. Leverett, W. Anthony Mason, and Robert N. Sidebotham. </author> <title> The Episode File System. </title> <booktitle> In Proceedings of the 1992 Summer USENIX Conference, </booktitle> <pages> pages 4360, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Memorys perceived unreliability forces a tradeoff between performance and reliability: Applications requiring high reliability, such as transaction processing, write data through to disk, but this limits throughput to that of disk. While optimizations such as logging and group commit can increase effective disk throughput <ref> [Rosenblum92, Chutani92, DeWitt84] </ref>, disk throughput is still far slower than memory throughput. Unix file systems mitigate the performance lost in reliability-induced disk writes by waiting 30 seconds before writing data, but this ensures the loss of data written within 30 seconds of a crash [Ousterhout85].
Reference: [Copeland89] <author> George Copeland, Tom Keller, Ravi Krishnamurthy, and Marc Smith. </author> <title> The Case for Safe RAM. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 327335, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: To accomplish this, we use ideas from existing protection techniques such as virtual memory and sandboxing [Wahbe93]. At first glance, the virtual memory protection of a system seems ideally suited to protect the file cache from unauthorized stores <ref> [Copeland89] </ref>. By turning off the write-permission bits in the page table for file cache pages, the system will cause most unauthorized stores to encounter a protection violation. File cache procedures must enable the write-permission bit in the page table before writing a page and disable writes afterwards. <p> However, we know of no paper on fault injection that has specifically measured the effects of faults on permanent data in memory. 6.2 Protecting Memory Several researchers have proposed ways to protect memory from software failures <ref> [Copeland89] </ref>, though to our knowledge none have evaluated how effectively memory withstood these failures. The only file system we are aware of that attempts to make all permanent files reliable while in memory is Phoenix [Gait90]. Phoenix keeps two versions of an in-memory file system. <p> The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>, and countless papers have sought to improve disk performance via data placement, logging, scheduling, and so forth. 7 Conclusions We have made a case for reliable file caches: main memory that can survive operating system crashes and be as safe and permanent as disk.
Reference: [DEC94] <institution> DEC 3000 300/400/500/600/700/800/900 AXP Models System Programmers Manual. </institution> <type> Technical report, </type> <institution> Digital Equipment Corporation, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: A system should be able to be reset without erasing memory; and CPU caches, because they contain memory data, should also preserve their contents on a normal reset. DEC Alphas allow a reset and boot without erasing memory or the CPU caches <ref> [DEC94] </ref>; the PCs we have tested do not.
Reference: [DEC95] <author> August 1995. </author> <title> Digital Unix development team, </title> <type> Personal Communication. </type>
Reference-contexts: We do not consider power outages further in this paper. Memorys vulnerability to OS crashes is more challenging. Most people would feel nervous if their system crashed while the sole copy of important data was in memory, even if the power stayed on <ref> [DEC95, Tanenbaum95 page 146, Silberschatz94 page 200] </ref>. Consequently, file systems periodically write data to disk, and transaction processing applications view transactions as committed only when data is written to disk. The focus of this paper is enabling memory to survive operating system crashes without writing data to disk.
Reference: [DeWitt84] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation Techniques for Main Memory Database Systems. </title> <booktitle> In Proceedings of the 1984 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 18, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Memorys perceived unreliability forces a tradeoff between performance and reliability: Applications requiring high reliability, such as transaction processing, write data through to disk, but this limits throughput to that of disk. While optimizations such as logging and group commit can increase effective disk throughput <ref> [Rosenblum92, Chutani92, DeWitt84] </ref>, disk throughput is still far slower than memory throughput. Unix file systems mitigate the performance lost in reliability-induced disk writes by waiting 30 seconds before writing data, but this ensures the loss of data written within 30 seconds of a crash [Ousterhout85].
Reference: [Gait90] <author> Jason Gait. </author> <title> Phoenix: A Safe In-Memory File System. </title> <journal> Communications of the ACM, </journal> <volume> 33(1):8186, </volume> <month> Jan-uary </month> <year> 1990. </year>
Reference-contexts: The only file system we are aware of that attempts to make all permanent files reliable while in memory is Phoenix <ref> [Gait90] </ref>. Phoenix keeps two versions of an in-memory file system. One of these versions is kept write-protected; the other version is unprotected and evolves from the write-protected one via copy-on-write. At periodic checkpoints, the system write-protects the unprotected version and deletes obsolete pages in the original version.
Reference: [Ganger94] <author> Gregory R. Ganger and Yale N. Patt. </author> <title> Metadata Update Performance in File Systems. </title> <booktitle> 1994 Operating Systems Design and Implementation (OSDI), </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Rio without protection makes data permanent after each write, yet has performance 35% better than the standard Unix file system and 8% better than a system where metadata updates are delayed by 30 seconds before being written to disk <ref> [Ganger94] </ref>. Adding protection slows Rio performance down by 20%, but some applications may require the extra margin of safety this provides. Other file systems that guarantee data permanence after each file write or close perform 3-20 times slower than Rio. <p> each write after write 305.4 seconds Rio without protection after write 14.2 seconds Rio with protection after write 17.0 seconds The Rio File Cache: Surviving Operating System Crashes 8 through) metadata updates, so we also measure UFS after delaying all metadata updates by 30 seconds (the optimal no-order system in <ref> [Ganger94] </ref>). Rio still performs 8% faster than this system due to the sync every 30 seconds. Yet while these systems lose 30 seconds of recently written data on a crash, Rio loses none. MFS, which is completely memory-resident and does no disk I/O, is shown to illustrate optimal performance [McKusick90].
Reference: [Gray90] <author> Jim Gray. </author> <title> A Census of Tandem System Availability between 1985 and 1990. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> 39(4), </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: power (just as disks can be moved without losing data) [Moran90, Baker92a]. 6 Related Work We divide the research related to this paper into two areas: field studies/fault injection and protection schemes. 6.1 Field Studies and Fault Injection Studies have shown that software is the dominant cause of system outages <ref> [Gray90] </ref>, and several studies have investigated system software errors. Sullivan and Chillarege classify software faults in the MVS operating system; in particular, they analyze faults that corrupt program memory (overlays) [Sullivan91]. Lee and Iyer study and classify software failures in Tandems Guardian operating system [Lee93a].
Reference: [Hartman93] <author> John H. Hartman and John K. Ousterhout. </author> <title> Letter to the Editor. Operating Systems Review, </title> <address> 27(1):79, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Unix file systems mitigate the performance lost in reliability-induced disk writes by waiting 30 seconds before writing data, but this ensures the loss of data written within 30 seconds of a crash [Ousterhout85]. In addition, 1/3 to 2/3 of newly written data lives longer than 30 seconds <ref> [Baker91, Hartman93] </ref>, so a large fraction of writes must eventually be written through to disk under this policy. A longer delay decreases disk traffic due to writes but risks losing even more data.
Reference: [Howard88] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1):5181, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: As a final check for corruption, we keep two copies of all files that are not modified by our workload and check that the two copies are equal. These files were not corrupted in our tests. In addition to memTest, we run four copies of the Andrew benchmark <ref> [Howard88, Ousterhout90] </ref>, a general-purpose file-system workload. <p> Rio also improves performance by eliminating all reliability-induced writes to disk. Table 2 compares the performance (on the Andrew file system benchmark) of Rio with several variations on the Unix file system, each providing different guarantees on when data is made permanent <ref> [Howard88, Ousterhout90] </ref>. Rio without protection performs 3-20 times faster than systems with comparable reliability guarantees (write-through on write, write-through on close). Rio also performs 35% faster than the standard Unix file system. Much of this advantage is due to UFSs synchronous (write Table 2: Performance Comparison.
Reference: [Iyer95] <author> Ravishankar K. Iyer. </author> <title> Experimental Evaluation. </title> <booktitle> In Proceedings of the 1995 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 115132, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Software fault injection is a popular technique for evaluating the behavior of prototype systems in the presence of hardware and software faults. See <ref> [Iyer95] </ref> for an excellent introduction to the overall area and a summary of much of the past work on fault injection, such as FINE [Kao93], FIAT [Barton90], and FER-RARI [Kanawati95].
Reference: [Johnson82] <author> Mark Scott Johnson. </author> <title> Some Requirements for Architectural Support of Software Debugging. </title> <booktitle> In Proceedings of the 1982 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 140148, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations <ref> [Johnson82, Wahbe92] </ref>. Hive uses the Flash firewall to protect memory against wild writes by other processors in a multiprocessor [Chapin95]. Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory.
Reference: [Kanawati95] <author> Ghani A. Kanawati, Nasser A. Kanawati, and Jacob A. Abraham. FERRARI: </author> <title> A Flexible Software-Based Fault and Error Injection System. </title> <journal> IEEE Transactions on Computers, </journal> <month> 44(2):248260, </month> <title> February The Rio File Cache: Surviving Operating System Crashes 11 1995. </title>
Reference-contexts: Most crashes occurred within 15 seconds after the fault was injected. If a fault does not crash the machine after ten minutes, we discard the run and reboot the system. 4 The first category of faults ips random bits in the kernels address space <ref> [Barton90, Kanawati95] </ref>. We target three areas of the kernels address space: the kernel text, heap, and stack. These faults are easy to inject, and they cause a variety of different crashes. They are the least realistic of our bugs, however. <p> See [Iyer95] for an excellent introduction to the overall area and a summary of much of the past work on fault injection, such as FINE [Kao93], FIAT [Barton90], and FER-RARI <ref> [Kanawati95] </ref>. As with field studies of system crashes, these papers on fault injection inspired many of the fault categories used in this paper.
Reference: [Kane92] <author> Gerry Kane and Joe Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: Or the file cache procedures can create a shadow copy and implement atomic writes. Unfortunately, many systems allow certain kernel accesses to bypass the virtual memory protection mechanism and directly access physical memory <ref> [Kane92, Sites92] </ref>. For example, addresses in the DEC 1. We will see in Section 3.3 that even without protection, most crashes do not corrupt files in memory. Hence we recommend that protection be turned off for most systems.
Reference: [Kao93] <author> Wei-Lun Kao, Ravishankar K. Iyer, and Dong Tang. </author> <title> FINE: A Fault Injection and Monitoring Environment for Tracing the UNIX System Behavior under Faults. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(11):11051118, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: The second category of fault changes individual instructions in the kernel text. These faults are intended to approximate the assembly-level manifestation of real C-level programming errors <ref> [Kao93] </ref>. We corrupt assignment statements by changing the source or destination register. We corrupt conditional constructs by deleting branches. We also delete random instructions (both branch and non-branch). The last and most extensive category of faults imitate specific programming errors in the kernel [Sullivan91]. <p> The last and most extensive category of faults imitate specific programming errors in the kernel [Sullivan91]. These are more targeted at specific programming errors than the previous fault category. We inject an initialization fault by deleting instructions responsible for initializing a variable at the start of a procedure <ref> [Kao93, Lee93a] </ref>. We inject pointer corruption by 1) finding a register that is used as a base register of a load or store and 2) deleting the most recent instruction before the load/store that modifies that register [Sullivan91, Lee93a]. <p> We plan to trace how faults propagate to corrupt files and crash the system instead of treating the system as a black box. This is extremely challenging, however, and is beyond the scope of this paper <ref> [Kao93] </ref>. Table 1: Comparing Disk and Memory Reliability. This table shows how often each type of error corrupted data for three systems. The disk-based system uses fsync after every write, achieving write-through reliability. <p> In addition to the standard sanity checks written by programmers, the virtual memory system implicitly checks each load/store address to make sure it is a valid address. Particularly on a 64-bit machine, most errors are first detected by issuing an illegal address <ref> [Kao93, Lee93a] </ref>. Thus, even without protection, Rio stores files about as reliably as a write-through file system, and this is the configuration we recommend for most systems. However, some applications will require even higher levels of safety. <p> Software fault injection is a popular technique for evaluating the behavior of prototype systems in the presence of hardware and software faults. See [Iyer95] for an excellent introduction to the overall area and a summary of much of the past work on fault injection, such as FINE <ref> [Kao93] </ref>, FIAT [Barton90], and FER-RARI [Kanawati95]. As with field studies of system crashes, these papers on fault injection inspired many of the fault categories used in this paper.
Reference: [Kessler90] <author> Peter B. Kessler. </author> <title> Fast breakpoints: </title> <booktitle> Design and implementation. In Proceedings of the 1990 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 7884, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. Object code modification has been suggested as a way to provide data breakpoints <ref> [Kessler90, Wahbe92] </ref> and fault isolation between software modules [Wahbe93].
Reference: [Lee93] <author> Inhwan Lee and Ravishankar K. Iyer. </author> <title> Faults, Symptoms, and Software Fault Tolerance in the Tandem GUARDIAN Operating System. </title> <booktitle> In International Symposium on Fault-Tolerant Computing (FTCS), pages 2029, </booktitle> <year> 1993. </year>
Reference: [Leffler89] <author> Samuel J. Leffler, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference: [Liskov91] <author> Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Will-iams. </author> <title> Replication in the Harp File System. </title> <booktitle> In Proceedings of the 1991 Symposium on Operating System Principles, </booktitle> <pages> pages 226238, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Harp protects a log of recent modifications by replicating it in volatile, battery-backed memory across several server nodes <ref> [Liskov91] </ref>. The Recovery Box keeps special system state in a region of memory accessed only through a rigid interface [Baker92b]. No attempt is made to prevent other procedures from accidentally modifying the recovery box, although the system detects corruption by maintaining checksums.
Reference: [McKusick90] <author> Marshall Kirk McKusick, Michael J. Karels, and Keith Bostic. </author> <title> A Pageable Memory Based Filesystem. </title> <booktitle> In Proceedings USENIX Summer Conference, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Further, main memory is random-access, unlike special-purpose devices. The goal of the Rio (RAM I/O) file cache is to achieve the performance of main memory with the reliability of disk: write-back performance with write-through reliability. We achieve memory performance by eliminating all reliability-induced writes to disk <ref> [McKusick90, Ohta90] </ref>. We achieve reliability by protecting memory during a crash and restoring it during a reboot (a warm reboot). Extensive crash tests show that even without protection, warm reboot enables memory to achieve reliability close to that of a write-through file system while performing 20 times faster. <p> Rio still performs 8% faster than this system due to the sync every 30 seconds. Yet while these systems lose 30 seconds of recently written data on a crash, Rio loses none. MFS, which is completely memory-resident and does no disk I/O, is shown to illustrate optimal performance <ref> [McKusick90] </ref>.
Reference: [Moran90] <author> J. Moran, Russel Sandberg, D. Coleman, J. Kepecs, and Bob Lyon. </author> <title> Breaking Through the NFS Performance Barrier. </title> <booktitle> In Proceedings of EUUG Spring 1990, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: Existing choices for reliable memory are attached via an I/O or backplane bus rather than the memory bus. These special-purpose devices include solid-state disks, non-volatile disk controllers, and write-buffers such as Prestoserve <ref> [Moran90] </ref>. While these can improve performance over disks, their performance is limited by the low bandwidth and high overhead of the I/O bus and device interface. <p> To make data accessible during a hardware failure, it should be possible to move a memory board to a different machine without losing power (just as disks can be moved without losing data) <ref> [Moran90, Baker92a] </ref>. 6 Related Work We divide the research related to this paper into two areas: field studies/fault injection and protection schemes. 6.1 Field Studies and Fault Injection Studies have shown that software is the dominant cause of system outages [Gray90], and several studies have investigated system software errors.
Reference: [Needham83] <author> R. M. Needham, A. J. Herbert, and J. G. Mitchell. </author> <title> How to Connect Stable Memory to a Computer. Operating System Review, </title> <address> 17(1):16, </address> <month> January </month> <year> 1983. </year>
Reference-contexts: Our work seeks to make all files in memory reliable without special-purpose hardware or replication. General mechanisms may be used to help protect memory from software faults. <ref> [Needham83] </ref> suggests changing a machines microcode to check certain conditions when writing a memory word. This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations [Johnson82, Wahbe92].
Reference: [Ohta90] <author> Masataka Ohta and Hiroshi Tezuka. </author> <title> A Fast /tmp File System by Delay Mount Option. </title> <booktitle> In Proceedings USENIX Summer Conference, </booktitle> <pages> pages 145150, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Further, main memory is random-access, unlike special-purpose devices. The goal of the Rio (RAM I/O) file cache is to achieve the performance of main memory with the reliability of disk: write-back performance with write-through reliability. We achieve memory performance by eliminating all reliability-induced writes to disk <ref> [McKusick90, Ohta90] </ref>. We achieve reliability by protecting memory during a crash and restoring it during a reboot (a warm reboot). Extensive crash tests show that even without protection, warm reboot enables memory to achieve reliability close to that of a write-through file system while performing 20 times faster. <p> First, reliability-induced writes to disk are no longer needed, because files in memory are as permanent and safe as files on disk. Digital Unix includes tunable parameters to turn off reliable writes for the UBC. We disable buffer cache writes as in <ref> [Ohta90] </ref> by turning most bwrite and bawrite calls to bdwrite; we modify sync and fsync calls to return immediately 3 ; and we modify the panic procedure to avoid writing dirty data back to disk before a crash.
Reference: [Ousterhout85] <author> John K. Ousterhout, Herve Da Costa, et al. </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System. </title> <booktitle> In Proceedings of the 1985 Symposium on Operating System Principles, </booktitle> <pages> pages 1524, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Unix file systems mitigate the performance lost in reliability-induced disk writes by waiting 30 seconds before writing data, but this ensures the loss of data written within 30 seconds of a crash <ref> [Ousterhout85] </ref>. In addition, 1/3 to 2/3 of newly written data lives longer than 30 seconds [Baker91, Hartman93], so a large fraction of writes must eventually be written through to disk under this policy. A longer delay decreases disk traffic due to writes but risks losing even more data.
Reference: [Ousterhout90] <author> John K. Ousterhout. </author> <title> Why arent operating systems getting faster as fast as hardware? In Proceedings USENIX Summer Conference, </title> <booktitle> pages 247256, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: As a final check for corruption, we keep two copies of all files that are not modified by our workload and check that the two copies are equal. These files were not corrupted in our tests. In addition to memTest, we run four copies of the Andrew benchmark <ref> [Howard88, Ousterhout90] </ref>, a general-purpose file-system workload. <p> Rio also improves performance by eliminating all reliability-induced writes to disk. Table 2 compares the performance (on the Andrew file system benchmark) of Rio with several variations on the Unix file system, each providing different guarantees on when data is made permanent <ref> [Howard88, Ousterhout90] </ref>. Rio without protection performs 3-20 times faster than systems with comparable reliability guarantees (write-through on write, write-through on close). Rio also performs 35% faster than the standard Unix file system. Much of this advantage is due to UFSs synchronous (write Table 2: Performance Comparison.
Reference: [Rosenblum92] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The Design and Implementation of a Log-Structured File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1):2652, </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: Memorys perceived unreliability forces a tradeoff between performance and reliability: Applications requiring high reliability, such as transaction processing, write data through to disk, but this limits throughput to that of disk. While optimizations such as logging and group commit can increase effective disk throughput <ref> [Rosenblum92, Chutani92, DeWitt84] </ref>, disk throughput is still far slower than memory throughput. Unix file systems mitigate the performance lost in reliability-induced disk writes by waiting 30 seconds before writing data, but this ensures the loss of data written within 30 seconds of a crash [Ousterhout85]. <p> Reliable file caches have striking implications for future system designers: Write-backs to disk are no longer needed except when the file cache fills up, changing the assumptions about write traffic behind some file system research such as LFS <ref> [Rosenblum92, Baker91] </ref>. Delaying writes to disk until the file cache fills up enables the largest possible number of files to die in memory and enables remaining files to be written out efficiently in arbitrarily large units. Thus Rio improves performance moderately over delayed-write systems.
Reference: [Silberschatz94] <author> Abraham Silberschatz and Peter B. Galvin. </author> <title> Operating System Concepts. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference: [Sites92] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: Or the file cache procedures can create a shadow copy and implement atomic writes. Unfortunately, many systems allow certain kernel accesses to bypass the virtual memory protection mechanism and directly access physical memory <ref> [Kane92, Sites92] </ref>. For example, addresses in the DEC 1. We will see in Section 3.3 that even without protection, most crashes do not corrupt files in memory. Hence we recommend that protection be turned off for most systems.
Reference: [Srivastava94] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A System for Building Customized Program Analysis Tools. </title> <booktitle> In Proceedings of the 1994 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 196205, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The checking code is very efficient: 6 instructions for a virtual address (the normal case), 28 instructions for a physical address. We gain efficiency over more general tools such as ATOM <ref> [Srivastava94] </ref> by inlining the check for virtual addresses and by increasing each procedures stack rather than creating a temporary stack frame for each check. Modifications to the stack pointer occur much less frequently than stores to memory that use the stack pointer.
Reference: [Sullivan91] <author> Mark Sullivan and R. Chillarege. </author> <title> Software Defects and Their Impact on System AvailabilityA Study of Field Failures in Operating Systems. </title> <booktitle> In Proceedings of the 1991 International Symposium on Fault-Tolerant Computing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: We corrupt assignment statements by changing the source or destination register. We corrupt conditional constructs by deleting branches. We also delete random instructions (both branch and non-branch). The last and most extensive category of faults imitate specific programming errors in the kernel <ref> [Sullivan91] </ref>. These are more targeted at specific programming errors than the previous fault category. We inject an initialization fault by deleting instructions responsible for initializing a variable at the start of a procedure [Kao93, Lee93a]. <p> We inject pointer corruption by 1) finding a register that is used as a base register of a load or store and 2) deleting the most recent instruction before the load/store that modifies that register <ref> [Sullivan91, Lee93a] </ref>. We do not corrupt the stack pointer register, as this is used to access local variables instead of as a pointer variable. <p> The Rio File Cache: Surviving Operating System Crashes 5 follows: 50% corrupt one byte; 44% corrupt 2-1024 bytes; 6% corrupt 2-4 KB. This distribution was chosen by starting with the data gathered in <ref> [Sullivan91] </ref> and modifying it somewhat according to our specific platform and experience. bcopy is set to inject this error every 1000-4000 times it is called; this occurs approximately every 15 seconds. We inject off-by-one errors by changing conditions such as &gt; to &gt;=, and &lt; to &lt;=, and so on. <p> Sullivan and Chillarege classify software faults in the MVS operating system; in particular, they analyze faults that corrupt program memory (overlays) <ref> [Sullivan91] </ref>. Lee and Iyer study and classify software failures in Tandems Guardian operating system [Lee93a]. These studies provide valuable information about failures in production environments; in fact, many of the fault types in Section 3.1 were inspired by the major error categories from [Sullivan91] and [Lee93a]. <p> analyze faults that corrupt program memory (overlays) <ref> [Sullivan91] </ref>. Lee and Iyer study and classify software failures in Tandems Guardian operating system [Lee93a]. These studies provide valuable information about failures in production environments; in fact, many of the fault types in Section 3.1 were inspired by the major error categories from [Sullivan91] and [Lee93a]. However, these studies do not provide data on how often system crashes corrupt the file cache, which may have different failure characteristics than randomly accessed data structures [Sullivan95].
Reference: [Sullivan95] <author> Mark Sullivan, </author> <month> December </month> <year> 1995. </year> <type> personal communication. </type>
Reference-contexts: However, these studies do not provide data on how often system crashes corrupt the file cache, which may have different failure characteristics than randomly accessed data structures <ref> [Sullivan95] </ref>. Software fault injection is a popular technique for evaluating the behavior of prototype systems in the presence of hardware and software faults.
Reference: [Tanenbaum95] <author> Andrew S. Tanenbaum. </author> <title> Distributed Operating Systems. </title> <publisher> Prentice-Hall, </publisher> <year> 1995. </year>
Reference-contexts: I/O devices such as disks and tapes are considered reliable places to store long-term data such as files. However, random-access memory is viewed as an unreliable place to store permanent data (files) because it is vulnerable to power outages and operating system crashes <ref> [Tanenbaum95, page 146] </ref>. Memorys vulnerability to power outages is easy to understand and fix. A $119 uninterruptible power supply can keep a system running long enough to dump memory to disk in the event of a power outage [APC96], or one can use non-volatile memory such as Flash RAM [Wu94].
Reference: [Wahbe92] <author> Robert Wahbe. </author> <title> Efficient Data Breakpoints. </title> <booktitle> In Proceedings of the 1992 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations <ref> [Johnson82, Wahbe92] </ref>. Hive uses the Flash firewall to protect memory against wild writes by other processors in a multiprocessor [Chapin95]. Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. <p> Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. Object code modification has been suggested as a way to provide data breakpoints <ref> [Kessler90, Wahbe92] </ref> and fault isolation between software modules [Wahbe93].
Reference: [Wahbe93] <author> Robert Wahbe, Steven Lucco, Thomas E. Anderson, and Susan L. Graham. </author> <title> Efficient Software-Based Fault Isolation. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 203216, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: We want to make it unlikely that non-file-cache procedures will accidentally corrupt the file cache, essentially making the file cache a protected module within the monolithic kernel. To accomplish this, we use ideas from existing protection techniques such as virtual memory and sandboxing <ref> [Wahbe93] </ref>. At first glance, the virtual memory protection of a system seems ideally suited to protect the file cache from unauthorized stores [Copeland89]. By turning off the write-permission bits in the page table for file cache pages, the system will cause most unauthorized stores to encounter a protection violation. <p> Rio uses two different methods to protect against these physical addresses. Our current method, called code patching, is to modify the kernel object code by inserting a check before every kernel store <ref> [Wahbe93] </ref>. If the address is a physical address, the inserted code checks to make sure the address is not in the file cache, or that the file cache has explicitly registered the address as writable. <p> In addition, the stack pointer is almost always modified in small increments, and these small increments cannot change a virtual address to a physical address. We can hence replace the checks on local, stack variables with a few checks on the stack pointer <ref> [Wahbe93] </ref>. We replace individual checks in commonly used loops with a few higher-level checks. For example, procedures such as bcopy modify sequential blocks of data; these blocks can be checked once rather than checking every individual store. <p> Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. Object code modification has been suggested as a way to provide data breakpoints [Kessler90, Wahbe92] and fault isolation between software modules <ref> [Wahbe93] </ref>.
Reference: [Wu94] <author> Michael Wu and Willy Zwaenepoel. eNVy: </author> <title> A Non-Volatile, Main Memory Storage System. </title> <booktitle> In Proceedings of the 1994 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Memorys vulnerability to power outages is easy to understand and fix. A $119 uninterruptible power supply can keep a system running long enough to dump memory to disk in the event of a power outage [APC96], or one can use non-volatile memory such as Flash RAM <ref> [Wu94] </ref>. We do not consider power outages further in this paper. Memorys vulnerability to OS crashes is more challenging. <p> Other projects seek to improve the reliability of memory against hardware faults such as power outages and board failures. eNVy implements a memory board based on non-volatile, ash RAM <ref> [Wu94] </ref>. eNVy uses copy-on-write, page remapping, and a small, battery-backed, SRAM buffer to hide ash RAMs slow writes and bulk erases. The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94].
References-found: 45

