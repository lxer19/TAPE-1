URL: http://www-csag.cs.uiuc.edu/papers/memnetint.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Title: The Impact of Message Traffic on Multicomputer Memory Hierarchy Performance  
Author: Scott Pakin Andrew Chien 
Keyword: message traffic, memory hierarchy, cache, multicomputer, network  
Note: Submitted to HPCA '95  
Address: 1304 W. Springfield Ave. Urbana, IL 61801  
Affiliation: Department of Computer Science  
Email: fpakin,achieng@cs.uiuc.edu  
Date: July 24, 1994  
Abstract: Multicomputer cache performance is highly sensitive to interprocessor message traffic. The widening gap between microprocessor speeds and primary memory latencies means slight increases in cache miss rate can have a severe impact on application performance. It is therefore critical to reduce cache misses. While there are a number of factors that may contribute to an increase in cache misses, of particular concern is multicomputer message traffic. In this paper, we examine the extent to which handling message traffic increases cache misses. Using register-transfer-level trace-based simulators of two representative multicomputer nodes, we performed extensive tests (over 70 billion memory references) with a variety of system configurations. Our results indicate that message traffic has a tremendous impact on multicomputer node performance. A Paragon-style multicomputer with a 16 kilobyte (KB) cache that processes an average of one word of message data per 50 clock periods can be expected to observe up to 20% more misses on a given application than a similarly-configured uniprocessor. Furthermore, increasing the cache size increases the importance of messaging. For example, doubling the cache size from 16KB to 32KB decreases cache misses by 55% for one application on a uniprocessor but by only 35% on a CM-5-style multicomputer. Finally, we show that control and data messages (i.e. short and long messages) have different effects on the cache. We quantify the regions of message length space in which each multicomputer node architecture performs best. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Carl J. Beckmann. CARL: </author> <title> An architecture simulation language. </title> <type> Report 1066, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, 305 Talbot, 104 South Wright St.; Urbana, </institution> <address> IL 61801-2932, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: To do so, we constructed register-transfer-level trace-based simulators of a generic uniprocessor node and two multicomputer nodes|a CM-5-style node and a Paragon-style node|using the Parsim simulation engine [2] and the CARL simulation language <ref> [1] </ref>. The conceptual model for the simulator is that the processor continuously issues memory read and write requests, pausing occasionally to send a message to the network.
Reference: [2] <author> John Bruner. </author> <title> Parsim user interface reference manual. </title> <type> Report 1002, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, 305 Talbot, 104 South Wright St.; Urbana, </institution> <address> IL 61801-2932, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: To do so, we constructed register-transfer-level trace-based simulators of a generic uniprocessor node and two multicomputer nodes|a CM-5-style node and a Paragon-style node|using the Parsim simulation engine <ref> [2] </ref> and the CARL simulation language [1]. The conceptual model for the simulator is that the processor continuously issues memory read and write requests, pausing occasionally to send a message to the network.
Reference: [3] <author> Lynn Choi and Andrew A. Chien. </author> <title> Integrating networks and memory hierarchies in a multicomputer node architecture. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: We have already begun one such effort by starting to examine the relationship of network traffic to cache misses in the proposed DI-multicomputer <ref> [3] </ref>, in which messages can be routed either directly from registers or memory, and the programmer or compiler chooses which is more appropriate on a per-message basis.
Reference: [4] <author> SPEC Steering Committee. </author> <title> Introduction to SPEC CPU floating point benchmark suite: CFP92. INTRO file in CFP92 distribution, </title> <month> January </month> <year> 1992. </year>
Reference-contexts: curve as the corresponding application's working set size. 8 Benchmark Memory Working Application suite Language references set size 5 Description barnes ("Barnes-Hut") SPLASH C 128,299,383 32KB Simulate evolution of galaxies using Barnes-Hut hierarchical N - body method [15] doduc CFP92 Fortran 113,957,528 64KB Monte Carlo simulation of nuclear reactor component <ref> [4] </ref>; we used the "small" input set ear CFP92 C 256,352,000 8KB Simulate propagation of sound in human cochlea [4]; we used the "short" input set fpppp CFP92 Fortran 294,101,082 16KB Two electron integral derivative from GaussianXX quantum chemistry programs [4]; we traced a 5-atom run mdljsp2 CFP92 Fortran 64,567,619 128KB <p> Description barnes ("Barnes-Hut") SPLASH C 128,299,383 32KB Simulate evolution of galaxies using Barnes-Hut hierarchical N - body method [15] doduc CFP92 Fortran 113,957,528 64KB Monte Carlo simulation of nuclear reactor component <ref> [4] </ref>; we used the "small" input set ear CFP92 C 256,352,000 8KB Simulate propagation of sound in human cochlea [4]; we used the "short" input set fpppp CFP92 Fortran 294,101,082 16KB Two electron integral derivative from GaussianXX quantum chemistry programs [4]; we traced a 5-atom run mdljsp2 CFP92 Fortran 64,567,619 128KB Molecular dynamics program modelling the interaction of 500 atoms in a Lennard-Jones potential [4]; we used the "short" input <p> Fortran 113,957,528 64KB Monte Carlo simulation of nuclear reactor component <ref> [4] </ref>; we used the "small" input set ear CFP92 C 256,352,000 8KB Simulate propagation of sound in human cochlea [4]; we used the "short" input set fpppp CFP92 Fortran 294,101,082 16KB Two electron integral derivative from GaussianXX quantum chemistry programs [4]; we traced a 5-atom run mdljsp2 CFP92 Fortran 64,567,619 128KB Molecular dynamics program modelling the interaction of 500 atoms in a Lennard-Jones potential [4]; we used the "short" input set pthor SPLASH C 58,133,474 64KB Event-driven logic circuit simulator [15]|uses Chandy-Misra's distributed-time algorithm xlisp ("Li") CINT92 C 63,666,825 64KB LISP <p> of sound in human cochlea <ref> [4] </ref>; we used the "short" input set fpppp CFP92 Fortran 294,101,082 16KB Two electron integral derivative from GaussianXX quantum chemistry programs [4]; we traced a 5-atom run mdljsp2 CFP92 Fortran 64,567,619 128KB Molecular dynamics program modelling the interaction of 500 atoms in a Lennard-Jones potential [4]; we used the "short" input set pthor SPLASH C 58,133,474 64KB Event-driven logic circuit simulator [15]|uses Chandy-Misra's distributed-time algorithm xlisp ("Li") CINT92 C 63,666,825 64KB LISP interpreter running a program to solve the N -queens prob lem [5]; we specified 7 queens Table 1: Application traces the CM-5 and|to a
Reference: [5] <author> SPEC Steering Committee. </author> <title> Introduction to SPEC CPU integer benchmark suite: CINT92. INTRO file in CINT92 distribution, </title> <month> January </month> <year> 1992. </year>
Reference-contexts: dynamics program modelling the interaction of 500 atoms in a Lennard-Jones potential [4]; we used the "short" input set pthor SPLASH C 58,133,474 64KB Event-driven logic circuit simulator [15]|uses Chandy-Misra's distributed-time algorithm xlisp ("Li") CINT92 C 63,666,825 64KB LISP interpreter running a program to solve the N -queens prob lem <ref> [5] </ref>; we specified 7 queens Table 1: Application traces the CM-5 and|to a lesser extent|Paragon spend so much time handling messages, they can make no other progress. This is useful to know because it provides a lower bound on current node architectures communication granularity.
Reference: [6] <institution> Cray Research, Inc., Eagan, Minnesota 55121. CRAY T3D Software Overview Technical Note, </institution> <year> 1992. </year>
Reference-contexts: That is, they directly involve the cache in message sends and/or receives. An alternative design is to logically connect the network to primary memory, using DMA transfers for interpro-cessor communication. Such an approach is utilized by a wealth of machines, for instance the Intel Paragon [10], Cray T3D <ref> [6] </ref>, MIT Alewife [11], Bull/ECRC/ICL/Siemens EDS [17], Meiko CS-2 [9], and Caltech Mosaic-C [13], as well as systems designed around the Inmos T9000 Transputer [12]. The tradeoff between the two interfaces is|at the highest level|parallel performance versus sequential performance.
Reference: [7] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year>
Reference-contexts: Messages are introduced in a Poisson distribution. Data provided in <ref> [7] </ref> show that for the machines and applications examined in that paper, the message rate varies from 0.00004 to 0.00674 64-bit words per clock, with a mean of 0.00114. However, those numbers include the time spent executing non-memory instructions, while our simulations execute exclusively memory instructions. <p> However, those numbers include the time spent executing non-memory instructions, while our simulations execute exclusively memory instructions. In other words, the numbers calculated from <ref> [7] </ref> are unrealistically low for a memory-only study. Hence, we adjusted our range of message rates upward. The message rates used emphasize "interesting" regions of the data|where the curves are not flat|and were selected based on preliminary results. <p> Because the Paragon-style system uses DMA to transport messages directly from memory to the network interface, no additional cache misses are generated for message sends. 4.4 Misses versus Message Size Message-passing applications employ a large variety of message lengths <ref> [7] </ref>. Therefore, we investigated the effect of message size on cache performance. We expect message size to impact the cache for two reasons: First, a small number of long messages exhibits more spatial locality than a large number of short messages.
Reference: [8] <institution> Digital Equipment Corporation, Maynard, </institution> <address> MA. </address> <note> DECchip 21064-AA Microprocessor Hardware Reference Manual, 1st edition, </note> <month> October </month> <year> 1992. </year> <title> Order number EC-N0079-72. </title> <type> 20 </type>
Reference-contexts: The logical structure shared by all three versions of the simulator is shown in Figure 5 and contains the following modules, the first four of which are based specifically on the DECchip 21064-AA implementation <ref> [8] </ref> of the Alpha architecture: CPU 64-bit CPU, assumed 100% hit rate for translation lookaside buffer (TLB) and instruction cache, continuously issues memory read/write requests taken from memory-reference traces of actual programs (i.e. only memory instructions are simulated) C physically-addressed direct-mapped cache, four 64-bit words per line, read-allocate scheme for misses <p> (TLB) and instruction cache, continuously issues memory read/write requests taken from memory-reference traces of actual programs (i.e. only memory instructions are simulated) C physically-addressed direct-mapped cache, four 64-bit words per line, read-allocate scheme for misses WB four-line-deep write buffer, four-word lines, write merging of multiple writes to the same line|see <ref> [8] </ref> for further details Bus 128 data + 32 address lines BIU bus interface unit, arbitrates fairly between cache and write buffer for bus control M primary memory NI network interface, provides memory-mapped message first-in first-out buffers (FIFOs) DMA direct memory access controller, bridges network interface and memory, used only in
Reference: [9] <author> M. Homewood and M. McLaren. </author> <title> Meiko CS-2 interconnect Elan Elite design. </title> <booktitle> In Proceedings of the IEEE Hot Interconnects Symposium. IEEE TCMM, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: An alternative design is to logically connect the network to primary memory, using DMA transfers for interpro-cessor communication. Such an approach is utilized by a wealth of machines, for instance the Intel Paragon [10], Cray T3D [6], MIT Alewife [11], Bull/ECRC/ICL/Siemens EDS [17], Meiko CS-2 <ref> [9] </ref>, and Caltech Mosaic-C [13], as well as systems designed around the Inmos T9000 Transputer [12]. The tradeoff between the two interfaces is|at the highest level|parallel performance versus sequential performance. Transmitting messages between the network and the cache benefits parallel performance because it reduces messaging latency.
Reference: [10] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: That is, they directly involve the cache in message sends and/or receives. An alternative design is to logically connect the network to primary memory, using DMA transfers for interpro-cessor communication. Such an approach is utilized by a wealth of machines, for instance the Intel Paragon <ref> [10] </ref>, Cray T3D [6], MIT Alewife [11], Bull/ECRC/ICL/Siemens EDS [17], Meiko CS-2 [9], and Caltech Mosaic-C [13], as well as systems designed around the Inmos T9000 Transputer [12]. The tradeoff between the two interfaces is|at the highest level|parallel performance versus sequential performance.
Reference: [11] <author> John Kubiatowicz and Anant Agarwal. </author> <title> The anatomy of a message in the Alewife multiprocessor. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year> <note> Available from cag.lcs.mit.edu/pub/papers/anatomy.ps.Z. </note>
Reference-contexts: An alternative design is to logically connect the network to primary memory, using DMA transfers for interpro-cessor communication. Such an approach is utilized by a wealth of machines, for instance the Intel Paragon [10], Cray T3D [6], MIT Alewife <ref> [11] </ref>, Bull/ECRC/ICL/Siemens EDS [17], Meiko CS-2 [9], and Caltech Mosaic-C [13], as well as systems designed around the Inmos T9000 Transputer [12]. The tradeoff between the two interfaces is|at the highest level|parallel performance versus sequential performance.
Reference: [12] <author> David May, Roger Shepherd, and Peter Thompson. </author> <title> The T9000 Transputer. </title> <type> Technical report, Inmos Limited, </type> <address> 1000 Aztec West, Almondsbury, Bristol BS12 4SQ, UK, </address> <year> 1993. </year> <note> Available from ftp.inmos.co.uk:/inmos/info/T9000/T9000.ps.Z. </note>
Reference-contexts: Such an approach is utilized by a wealth of machines, for instance the Intel Paragon [10], Cray T3D [6], MIT Alewife [11], Bull/ECRC/ICL/Siemens EDS [17], Meiko CS-2 [9], and Caltech Mosaic-C [13], as well as systems designed around the Inmos T9000 Transputer <ref> [12] </ref>. The tradeoff between the two interfaces is|at the highest level|parallel performance versus sequential performance. Transmitting messages between the network and the cache benefits parallel performance because it reduces messaging latency.
Reference: [13] <author> C. Seitz, N. Boden, J. Seizovic, and W. Su. </author> <title> The design of the Caltech Mosaic C multicomputer. </title> <booktitle> In Proceedings of the University of Washington Symposium on Integrated Systems, </booktitle> <year> 1993. </year>
Reference-contexts: An alternative design is to logically connect the network to primary memory, using DMA transfers for interpro-cessor communication. Such an approach is utilized by a wealth of machines, for instance the Intel Paragon [10], Cray T3D [6], MIT Alewife [11], Bull/ECRC/ICL/Siemens EDS [17], Meiko CS-2 [9], and Caltech Mosaic-C <ref> [13] </ref>, as well as systems designed around the Inmos T9000 Transputer [12]. The tradeoff between the two interfaces is|at the highest level|parallel performance versus sequential performance. Transmitting messages between the network and the cache benefits parallel performance because it reduces messaging latency.
Reference: [14] <author> Toshiyuki Shimizu, Takeshi Horie, and Hiroaki Ishihata. </author> <title> Low-latency message communication support for the AP1000. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 288-297, </pages> <year> 1992. </year>
Reference-contexts: The performance penalty is determined by how much message traffic affects the cache for various memory-network interfaces and applications. There are two basic approaches to interfacing memory hierarchies and networks. Machines like the Thinking Machines CM-5 [16] and Fujitsu AP1000 <ref> [14] </ref> logically connect the network to the cache. That is, they directly involve the cache in message sends and/or receives. An alternative design is to logically connect the network to primary memory, using DMA transfers for interpro-cessor communication.
Reference: [15] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford University, </institution> <address> CA 94305, </address> <year> 1991. </year>
Reference-contexts: the miss rate against the cache size and took the knee in each curve as the corresponding application's working set size. 8 Benchmark Memory Working Application suite Language references set size 5 Description barnes ("Barnes-Hut") SPLASH C 128,299,383 32KB Simulate evolution of galaxies using Barnes-Hut hierarchical N - body method <ref> [15] </ref> doduc CFP92 Fortran 113,957,528 64KB Monte Carlo simulation of nuclear reactor component [4]; we used the "small" input set ear CFP92 C 256,352,000 8KB Simulate propagation of sound in human cochlea [4]; we used the "short" input set fpppp CFP92 Fortran 294,101,082 16KB Two electron integral derivative from GaussianXX quantum
Reference: [16] <institution> Thinking Machines Corporation, Cambridge, Massachusetts. </institution> <type> CM-5 Technical Summary, </type> <month> October </month> <year> 1991. </year>
Reference-contexts: The performance penalty is determined by how much message traffic affects the cache for various memory-network interfaces and applications. There are two basic approaches to interfacing memory hierarchies and networks. Machines like the Thinking Machines CM-5 <ref> [16] </ref> and Fujitsu AP1000 [14] logically connect the network to the cache. That is, they directly involve the cache in message sends and/or receives. An alternative design is to logically connect the network to primary memory, using DMA transfers for interpro-cessor communication.
Reference: [17] <author> Gunter Watzlawik and Franz Hutner. </author> <title> A pipelined network interface for a parallel computer. </title> <type> Technical report, </type> <institution> Bull/ECRC/ICL/Siemens, ZFE ST SN 22, Siemens AG, </institution> <address> Otto-Hahn-Ring 6, 81730 Munchen, Germany, 1993. e-mail: watz@zfe.siemens.de. </address> <month> 21 </month>
Reference-contexts: An alternative design is to logically connect the network to primary memory, using DMA transfers for interpro-cessor communication. Such an approach is utilized by a wealth of machines, for instance the Intel Paragon [10], Cray T3D [6], MIT Alewife [11], Bull/ECRC/ICL/Siemens EDS <ref> [17] </ref>, Meiko CS-2 [9], and Caltech Mosaic-C [13], as well as systems designed around the Inmos T9000 Transputer [12]. The tradeoff between the two interfaces is|at the highest level|parallel performance versus sequential performance. Transmitting messages between the network and the cache benefits parallel performance because it reduces messaging latency.
References-found: 17

