URL: http://www.cs.washington.edu/homes/leveson/papers/critics.ps
Refering-URL: http://www.cs.washington.edu/homes/leveson/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A REPLY TO THE CRITICISMS OF THE KNIGHT LEVESON EXPERIMENT  
Author: John C. Knight Nancy G. Lev eson 
Address: Charlottesville, VA 22903 Irvine, CA 92717  
Affiliation: University of Virginia University of California  
Note: .nr 9901.s.nr 9801.f .ps 10.f t 2.ps 0199.f t 0198  
Abstract-found: 0
Intro-found: 1
Reference: [AVI77] <author> A. Avizienis and L. Chen, </author> <title> On the Implementation of N-Version Programming for Software Fault-Tolerance During Program Execution, </title> <booktitle> Proc. of Compsac '77, </booktitle> <month> November </month> <year> 1977, </year> <pages> pp. 149-155. </pages>
Reference-contexts: system development methods. [JOS88] and Avizienis states: The claims that the NVP process was investigated are not supported by the documentation of the software development process in [Knight/Leveson]. [AVI89] We used the methodology used by Chen [CHE78] and Kelly [KEL83, AVI84] and followed exactly what was stated by Avizienis in <ref> [AVI77, AVI85b] </ref>. According to these definitions, N-version programming means writing N versions independently. This is what we did. To claim that we did not use N-version programming is ridiculous.
Reference: [AVI84] <author> A. Avizienis and J.P.J. Kelly, </author> <title> Fault Tolerance by Design Diversity: Concepts and Experiments, </title> <journal> IEEE Computer, </journal> <volume> Vol. 17, No. 8, </volume> <month> August </month> <year> 1984, </year> <pages> pp. 67-80. </pages>
Reference-contexts: For example, Avizienis states: By combining software versions that have not been subjected to V&V [verification and validation] testing to produce highly reliable multiversion software, we may be able to decrease cost while increasing reliability. <ref> [AVI84] </ref> The higher initial cost may be balanced by significant gains, such as faster release of trustworthy software, less investment and criticality in verification and validation, ... [AVI89] The primary argument for the attainment of ultra-high reliability using this technique is given by Avizienis: It is the fundamental conjecture of the <p> Kelly used three different specifications, written in OBJ, PDL, and English, to generate 18 programs which he executed on 100 input cases <ref> [KEL83, AVI84] </ref>. Kelly was also involved in a team effort headed by NASA involving 4 universities (UCSB, Virginia, Illinois, and NCSU) in which 20 programs were generated from a single specification [ECK89]. We refer to this experiment here as the NASA experiment. <p> We refer to this experiment here as the NASA experiment. Avizienis, Lyu, and Schutz generated 6 programs written in 6 languages [AVI87] from a single specification. We refer to this experiment as the UCLA/H experiment. Joseph makes the following claim: Several experiments on NVP performed at UCLA <ref> [AVI84, AVI87] </ref> have not discovered the high rates of failure as reported in Knight/Leveson. [JOS88] There are two types of failure rates to which Joseph may be referring: individual version failure rates and 3-version failure rates. <p> the study did not use NVP due to inadequacies in proper system development methods. [JOS88] and Avizienis states: The claims that the NVP process was investigated are not supported by the documentation of the software development process in [Knight/Leveson]. [AVI89] We used the methodology used by Chen [CHE78] and Kelly <ref> [KEL83, AVI84] </ref> and followed exactly what was stated by Avizienis in [AVI77, AVI85b]. According to these definitions, N-version programming means writing N versions independently. This is what we did. To claim that we did not use N-version programming is ridiculous. <p> Six of the twenty-seven programs did not fail on any of the million input cases. The average failure probability was 0.0007 and the worst was 0.009. This should be compared to the published UCLA experiments [CHE78], [KEL83], and <ref> [AVI84] </ref>, which all had much poorer quality versions than we did in our experiment. No failure probabilities have been published for the UCLA/H experiment but the reported number of faults per version is comparable with ours.
Reference: [AVI85a] <author> A. Avizienis, et al., </author> <title> The UCLA Dedix System: A Distributed Testbed for Multiple-Version Software, </title> <booktitle> 15th Int. Symposium on Fault-Tolerant Computing, </booktitle> <address> Michigan, </address> <month> June </month> <year> 1985, </year> <pages> pp. 126-134. </pages>
Reference-contexts: The effectiveness of the NVP approach depends on the validity of this conjecture... [AVI85b] Since the versions are written independently, it is hypothesized that they are not likely to contain the same errors, i.e., that errors in their results are uncorrelated. <ref> [AVI85a] </ref> As Avizienis notes, this hypothesis is important because the degree to which it holds will determine the amount of reliability improvement that is realized.
Reference: [AVI85b] <author> A. Avizienis, </author> <title> The N-Version Approach to Fault-Tolerant Software, </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> Vol. SE-11, No. 12, </volume> <month> December </month> <year> 1985, </year> <pages> pp. 1491-1501 </pages>
Reference-contexts: The effectiveness of the NVP approach depends on the validity of this conjecture... <ref> [AVI85b] </ref> Since the versions are written independently, it is hypothesized that they are not likely to contain the same errors, i.e., that errors in their results are uncorrelated. [AVI85a] As Avizienis notes, this hypothesis is important because the degree to which it holds will determine the amount of reliability improvement that <p> system development methods. [JOS88] and Avizienis states: The claims that the NVP process was investigated are not supported by the documentation of the software development process in [Knight/Leveson]. [AVI89] We used the methodology used by Chen [CHE78] and Kelly [KEL83, AVI84] and followed exactly what was stated by Avizienis in <ref> [AVI77, AVI85b] </ref>. According to these definitions, N-version programming means writing N versions independently. This is what we did. To claim that we did not use N-version programming is ridiculous.
Reference: [AVI87] <author> A. Avizienis, M.R. Lyu, and W. Schutz, </author> <title> In Search of Effective Div ersity: A Six-Language Study of Fault-Tolerant Control Software, </title> <type> Tech. Report CSD-870060, </type> <institution> UCLA, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: For example, Avizienis states: It is our conjecture that a rigorous application of the design paradigm, as described in this paper would have led to the elimination of most faults described in [KNI86] before acceptance of the programs. <ref> [AVI87, AVI88] </ref> It is easy to assert that changes in experimental procedures would yield different results. Such conjectures, however, need to be supported with scientific proof before they can be accepted. Professor Avizienis and former students use two arguments to support their conjecture. <p> Although early experiments caused concern that this assumption may not hold [KNI86], additional empirical work has shown that this assumption can hold if a `best practice' development paradigm is used <ref> [BIS85, AVI87] </ref>. [KEL89] These results [of the UCLA/H experiment] are different from previously published results by Knight and Leveson. [AVI87, AVI88] Their second argument is that the claimed difference in results is accounted for by the significant differences between their experiments and ours and the inadequacies of our software development method: <p> Although early experiments caused concern that this assumption may not hold [KNI86], additional empirical work has shown that this assumption can hold if a `best practice' development paradigm is used [BIS85, AVI87]. [KEL89] These results [of the UCLA/H experiment] are different from previously published results by Knight and Leveson. <ref> [AVI87, AVI88] </ref> Their second argument is that the claimed difference in results is accounted for by the significant differences between their experiments and ours and the inadequacies of our software development method: [The Knight/Leveson and Scott, et al. studies] fail to recognize that NVP is a rigorous process of software development. <p> We refer to this experiment here as the NASA experiment. Avizienis, Lyu, and Schutz generated 6 programs written in 6 languages <ref> [AVI87] </ref> from a single specification. We refer to this experiment as the UCLA/H experiment. <p> We refer to this experiment here as the NASA experiment. Avizienis, Lyu, and Schutz generated 6 programs written in 6 languages [AVI87] from a single specification. We refer to this experiment as the UCLA/H experiment. Joseph makes the following claim: Several experiments on NVP performed at UCLA <ref> [AVI84, AVI87] </ref> have not discovered the high rates of failure as reported in Knight/Leveson. [JOS88] There are two types of failure rates to which Joseph may be referring: individual version failure rates and 3-version failure rates. <p> There have been many statements that the UCLA/H study got different results than we did. For example: Although early experiments caused concern that this assumption may not hold [KNI86], additional empirical work has shown that this assumption can hold if a `best practice' development paradigm is used <ref> [BIS85, AVI87] </ref>. [KEL89] The comparison of the results in the V/UCI and UCLA/H studies thus shows major disagreements [AVI88] From the published papers, we can find no evidence that the independence assumption was tested in the UCLA/H study. <p> It is surprising that Professor Avizienis believes that it is too early to look at numerical results but from his statements does not seem to believe that it is too early to use this technique in safety-critical applications such as commercial aircraft <ref> [AVI87, AVI88, AVI89, etc.] </ref>. Avizienis makes the following truly outrageous statement when describing our work: The use of the term ``experiment'' is misleading, since it implies repeatability of the experimental procedure that is taken for granted in science. [AVI89] Our experimental procedure is completely repeatable. <p> The algorithms for these operations were exactly specified, however, different choices of which primitive operations to implement as subprograms have been made, mainly whether the integrators include limits on the magnitude of the output value (as is required in most cases), or not. <ref> [AVI87] </ref> Two factors that limit actual diversity have been observed in the course of this assessment... algorithms specified by figures were generally implemented by following the corresponding figure from top to bottom... <p> In retrospect, a second reason for this lack of diversity is that we have concluded that the logic part of the Logic Mode was overspecified. <ref> [AVI87] </ref> The H [oneywell]/S [perry] concept of ``test points'' is the second factor that tends to limit diversity. Their purpose is to output and compare not only the final result of the major subfunctions, but also some intermediate results. <p> However, that restricted the programmers on their choices of which primitive operations to combine (efficiently!) into one programming language s tatement. In effect, the intermediate values to be computed were chosen for them. <ref> [AVI87] </ref> In their post-assessment of the diversity of the versions [AVI87] obtained from this detailed design specification, the major differences seemed to be syntactic rather than semantic, e.g., the use of parameter passing vs. global variables, differences in the calling structure of the procedures, the use of subprograms vs. functions. <p> However, that restricted the programmers on their choices of which primitive operations to combine (efficiently!) into one programming language s tatement. In effect, the intermediate values to be computed were chosen for them. <ref> [AVI87] </ref> In their post-assessment of the diversity of the versions [AVI87] obtained from this detailed design specification, the major differences seemed to be syntactic rather than semantic, e.g., the use of parameter passing vs. global variables, differences in the calling structure of the procedures, the use of subprograms vs. functions. <p> of diversity stems from the use of cross-checks points to vote on intermediate results and what they call Community Error Recovery [TSO87]: [The cross-check points] have to be executed in a certain predetermined order, but again great care was taken not to overly restrict the possible choices of computation sequence. <ref> [AVI87] </ref> One recovery point is used to recover a failed version by supplying it with a set of new internal state variables that are obtained from the other versions by the Community Error Recovery Technique. [AVI87] In order to effect this type of recovery, the internal states of the versions must <p> again great care was taken not to overly restrict the possible choices of computation sequence. <ref> [AVI87] </ref> One recovery point is used to recover a failed version by supplying it with a set of new internal state variables that are obtained from the other versions by the Community Error Recovery Technique. [AVI87] In order to effect this type of recovery, the internal states of the versions must be identical. <p> A new design rule for multi-version software must be stated as ``Do not introduce any `underground' variables.'' <ref> [AVI87] </ref> It is not surprising that few identical faults were found by a test procedure that involved checking the results of these programs against each other since the designs were, for all practical purposes, identical. <p> Avizienis, describes these difficulties with respect to the versions in his UCLA/H experiment: It was decided that different algorithms were not suitable for the scope of FCCs [ight control computers] due to potential timing problems and difficulties in proving their correctness (guaranteed matching among them). <ref> [AVI87] </ref> In real-life systems that use this technique, differences between the supposedly ``diverse'' modules are often minor.
Reference: [AVI88] <author> A. </author> <title> Avizienis and M.R. Lyu, On the Effectiveness of Multiversion Software in Digital Avionics, </title> <booktitle> AIAA/IEEE 8th Digital Avionics Systems Conference, </booktitle> <address> San Jose, </address> <month> October </month> <year> 1988, </year> <pages> pp. 422-427. </pages>
Reference-contexts: For example, Avizienis states: It is our conjecture that a rigorous application of the design paradigm, as described in this paper would have led to the elimination of most faults described in [KNI86] before acceptance of the programs. <ref> [AVI87, AVI88] </ref> It is easy to assert that changes in experimental procedures would yield different results. Such conjectures, however, need to be supported with scientific proof before they can be accepted. Professor Avizienis and former students use two arguments to support their conjecture. <p> Although early experiments caused concern that this assumption may not hold [KNI86], additional empirical work has shown that this assumption can hold if a `best practice' development paradigm is used [BIS85, AVI87]. [KEL89] These results [of the UCLA/H experiment] are different from previously published results by Knight and Leveson. <ref> [AVI87, AVI88] </ref> Their second argument is that the claimed difference in results is accounted for by the significant differences between their experiments and ours and the inadequacies of our software development method: [The Knight/Leveson and Scott, et al. studies] fail to recognize that NVP is a rigorous process of software development. <p> of the software development process. [AVI89] In summary, we have reviewed the V/UCI experiment and found reasons that may account for this outcome: the small scale and limited diversity potential of the experiment, lack of MVS [Multi-Version Software] software development disciplines, and apparently inadequate testing and processing of MVS systems. <ref> [AVI88] </ref> Both of these arguments are unfounded. We examine each of these in turn. 2. Different Results The first claim by our critics is that their results are different from ours. <p> For example: Although early experiments caused concern that this assumption may not hold [KNI86], additional empirical work has shown that this assumption can hold if a `best practice' development paradigm is used [BIS85, AVI87]. [KEL89] The comparison of the results in the V/UCI and UCLA/H studies thus shows major disagreements <ref> [AVI88] </ref> From the published papers, we can find no evidence that the independence assumption was tested in the UCLA/H study. <p> During post-acceptance testing and inspection, five faults were uncovered by testing. One pair again was identical. Six more faults were discovered by code inspection, all unrelated and different ... These results are different from previously published results by Knight and Leveson. <ref> [AVI88] </ref> The relevant factor is not whether the faults are identical, but whether failures are coincident. <p> It is surprising that Professor Avizienis believes that it is too early to look at numerical results but from his statements does not seem to believe that it is too early to use this technique in safety-critical applications such as commercial aircraft <ref> [AVI87, AVI88, AVI89, etc.] </ref>. Avizienis makes the following truly outrageous statement when describing our work: The use of the term ``experiment'' is misleading, since it implies repeatability of the experimental procedure that is taken for granted in science. [AVI89] Our experimental procedure is completely repeatable. <p> The only significant difference between the version development methodology we used and that used in the UCLA/H experiment (which occurred 3 years after ours) is that our methodology is more likely to result in design diversity. The current ``paradigm'' promoted by Avizienis and Lyu <ref> [AVI88] </ref> involves overspecifying the design and thus limiting potential diversity. The specific criticisms in the area of methodology that have been leveled at us have to do with quality of versions, testing, voting procedure, diversity and scale, specification, communication and isolation of programmers, and programming effort involved. <p> We can see no possible basis for an argument that our versions are low quality. 3.2. Testing Avizienis, Lyu, and Joseph state: In summary, we have reviewed the V/UCI experiment and found reasons that may account for this outcome: ... apparently inadequate testing and processing of MVS versions. <ref> [AVI88] </ref> Acceptance tests for each version were too small (i.e., only 200 test cases). Also, operational testing used randomly generated test cases. For critical and life-critical computer systems this is completely unacceptable. [JOS88] This has been one of the frequent criticisms about our experiment. <p> theirs): In the testing and processing of the MVS systems, the failure detection and granularity for the V/UCI experiment was coarse, since it used one Boolean variable to represent 241 Boolean conditions for a ``missile launching decision.'' The UCLA/H experiment employed real number comparisons for inexact matching with specified tolerances. <ref> [AVI88] </ref> We used all 241 boolean conditions in the voting as is clearly stated in our paper [KNI86, page 99]. 3.4. Diversity and Scale Avizienis states: The scale of the problem (and the potential for diversity) [in Knight and Leveson's experiment] is smaller [than the UCLA/H study]. [AVI88] and: We can <p> with specified tolerances. <ref> [AVI88] </ref> We used all 241 boolean conditions in the voting as is clearly stated in our paper [KNI86, page 99]. 3.4. Diversity and Scale Avizienis states: The scale of the problem (and the potential for diversity) [in Knight and Leveson's experiment] is smaller [than the UCLA/H study]. [AVI88] and: We can see that the V/UCI experiment was of rather small scale, since the specification was 6 pages long and could be programmed in 327 lines. The scale of the UCLA/H experiment with 64 pages of specification and at least 1250 lines of code was significantly larger. [AVI88] Our <p> study]. <ref> [AVI88] </ref> and: We can see that the V/UCI experiment was of rather small scale, since the specification was 6 pages long and could be programmed in 327 lines. The scale of the UCLA/H experiment with 64 pages of specification and at least 1250 lines of code was significantly larger. [AVI88] Our Pascal programs ranged in size from 310 to 781 lines of code with an average of 554 lines (they did not include any I/O statements). The UCLA/H Pascal program was 1288 lines of code with 491 executable statements (including I/O statements). <p> Communication and Isolation Avizienis states: There was no required communication protocol for the programmers in the V/UCI experiment, while the communication protocol for the UCLA/H experiment was well-defined, and rigorously enforced. <ref> [AVI88] </ref> The papers [describing Knight and Leveson's experiment] do not document the rules of isolation, and the C&D protocol that are indicators of NVP quality. [AVI89] The communication protocols for the two experiments are identical. Our protocol is documented in our paper [KNI86, page 98]. <p> On the contrary, pro grammers in the UCLA/H experiment worked in two-member teams and were paid a full-time research assistant (RA) salary during a class-free 12 week period during the summer. <ref> [AVI88] </ref> The real problem is that students were used as opposed to professional programmers in all of these experiments, including ours.
Reference: [AVI89] <author> A. Avizienis, </author> <title> Software Fault Tolerance, </title> <booktitle> IFIP XI World Computer Congress '89, </booktitle> <address> San Francisco, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: subjected to V&V [verification and validation] testing to produce highly reliable multiversion software, we may be able to decrease cost while increasing reliability. [AVI84] The higher initial cost may be balanced by significant gains, such as faster release of trustworthy software, less investment and criticality in verification and validation, ... <ref> [AVI89] </ref> The primary argument for the attainment of ultra-high reliability using this technique is given by Avizienis: It is the fundamental conjecture of the NVP [N-Version Programming] approach that the independence of ``Conjecture'' is defined by Webster's New World Dictionary to be ``an inference, theory, or prediction based on guesswork''. programming <p> The claims that the NVP process was investigated are not supported by the documentation of the software development process. <ref> [AVI89] </ref> In summary, we have reviewed the V/UCI experiment and found reasons that may account for this outcome: the small scale and limited diversity potential of the experiment, lack of MVS [Multi-Version Software] software development disciplines, and apparently inadequate testing and processing of MVS systems. [AVI88] Both of these arguments are <p> Avizienis refers to our study and another one that got similar results [SCO87] and states: These efforts serve to illustrate the pitfalls of premature preoccupation with numerical results. <ref> [AVI89] </ref> Without looking at numerical results, one cannot do the necessary statistical analysis to determine whether the independence hypothesis holds or determine what type of benefits can be expected from using N-version programming. <p> It is surprising that Professor Avizienis believes that it is too early to look at numerical results but from his statements does not seem to believe that it is too early to use this technique in safety-critical applications such as commercial aircraft <ref> [AVI87, AVI88, AVI89, etc.] </ref>. Avizienis makes the following truly outrageous statement when describing our work: The use of the term ``experiment'' is misleading, since it implies repeatability of the experimental procedure that is taken for granted in science. [AVI89] Our experimental procedure is completely repeatable. <p> Avizienis makes the following truly outrageous statement when describing our work: The use of the term ``experiment'' is misleading, since it implies repeatability of the experimental procedure that is taken for granted in science. <ref> [AVI89] </ref> Our experimental procedure is completely repeatable. We published precisely what we did and the requirements specification we used. Any researcher could have repeated our experiment. In fact, the result has been confirmed. <p> It is proposed that the study did not use NVP due to inadequacies in proper system development methods. [JOS88] and Avizienis states: The claims that the NVP process was investigated are not supported by the documentation of the software development process in [Knight/Leveson]. <ref> [AVI89] </ref> We used the methodology used by Chen [CHE78] and Kelly [KEL83, AVI84] and followed exactly what was stated by Avizienis in [AVI77, AVI85b]. According to these definitions, N-version programming means writing N versions independently. This is what we did. <p> We examine each in turn. 3.1. Quality of Versions Avizienis and Joseph state: [The Knight and Leveson] numerical results uniquely take the measure of the quality of their casual programming process. <ref> [AVI89] </ref> NVP does not mean low quality versions. In [KNI86], no software development standards or methods were required of the programmers. <p> As such, the V-spec needs to state the functional requirements completely and unambiguously, while leaving the widest possible choice of implementations to the N programming efforts.... Such specific suggestions of ``how'' reduce the chances for diversity among the versions and should be systematically eliminated from the V-spec. <ref> [AVI89] </ref> The introduction of a new term, V-spec, clouds the issues. <p> On a related issue, Avizienis states in a recent paper: The specification for simplex software tend to contain guidance not only ``what'' needs to be done, but also ``how'' the solution ought to be approached. <ref> [AVI89] </ref> Specifications for simplex (single-version) software are no different in this respect than for N-version software. In fact, the same specifications can be used for both types of software development, and requirements specifications for simplex software can just as easily contain only ``what'' as requirements specifications for multi-version software. <p> This is rather frightening since this real-life software often is depended on for safety-critical activities. 3.5. Specification Avizienis states: The V-specs [of Knight and Leveson] do not show the essential NVS attributes. <ref> [AVI89] </ref> No explanation of what attributes are missing is given, and we are at a loss to figure out what they could be. Avizienis' only specific criticism of our specification seems to be one of length. <p> was no required communication protocol for the programmers in the V/UCI experiment, while the communication protocol for the UCLA/H experiment was well-defined, and rigorously enforced. [AVI88] The papers [describing Knight and Leveson's experiment] do not document the rules of isolation, and the C&D protocol that are indicators of NVP quality. <ref> [AVI89] </ref> The communication protocols for the two experiments are identical. Our protocol is documented in our paper [KNI86, page 98].
Reference: [AVW87] <author> Airbus 320, </author> <title> the New Generation Aircraft, </title> <booktitle> Aviation Week & Space Technology, </booktitle> <month> February 2, </month> <year> 1987, </year> <pages> pp. 45-66. </pages>
Reference-contexts: Real world experience, not a class room assignment as in [KNI86], is needed. Currently, several systems in Europe are using NVP (e.g., the European designed Airbus 320 [ROU86] <ref> [AVW87] </ref>). [JOS88] Our results are not misleading. Our conclusion is simple and clearly stated.
Reference: [BIS85] <author> P.G. Bishop, et al., </author> <title> Project on Diverse Software An Experiment in Software Reliability, </title> <booktitle> Proceedings IFAC Workshop Safecomp '85, </booktitle> <address> Como, Italy 1985. </address>
Reference-contexts: Although early experiments caused concern that this assumption may not hold [KNI86], additional empirical work has shown that this assumption can hold if a `best practice' development paradigm is used <ref> [BIS85, AVI87] </ref>. [KEL89] These results [of the UCLA/H experiment] are different from previously published results by Knight and Leveson. [AVI87, AVI88] Their second argument is that the claimed difference in results is accounted for by the significant differences between their experiments and ours and the inadequacies of our software development method: <p> There have been many statements that the UCLA/H study got different results than we did. For example: Although early experiments caused concern that this assumption may not hold [KNI86], additional empirical work has shown that this assumption can hold if a `best practice' development paradigm is used <ref> [BIS85, AVI87] </ref>. [KEL89] The comparison of the results in the V/UCI and UCLA/H studies thus shows major disagreements [AVI88] From the published papers, we can find no evidence that the independence assumption was tested in the UCLA/H study.
Reference: [CHE78] <author> L. Chen and A A. Avizienis, </author> <title> N-Version Programming: A Fault-Tolerance Approach to Reliability of Software Operation, </title> <booktitle> Digest FTCS-8: Eighth International Symposium on Fault-Tolerant Computing, </booktitle> <address> Tolouse, France, </address> <month> June </month> <year> 1978, </year> <pages> pp 3-9. </pages>
Reference-contexts: This was our only conclusion. Table 1 shows data from other relevant studies that have been conducted. Chen generated 16 programs, chose 4 of these to consider further, and added 3 programs written by ``the authors'' for a total of 7 programs written in PL/I <ref> [CHE78] </ref>. Kelly used three different specifications, written in OBJ, PDL, and English, to generate 18 programs which he executed on 100 input cases [KEL83, AVI84]. <p> is proposed that the study did not use NVP due to inadequacies in proper system development methods. [JOS88] and Avizienis states: The claims that the NVP process was investigated are not supported by the documentation of the software development process in [Knight/Leveson]. [AVI89] We used the methodology used by Chen <ref> [CHE78] </ref> and Kelly [KEL83, AVI84] and followed exactly what was stated by Avizienis in [AVI77, AVI85b]. According to these definitions, N-version programming means writing N versions independently. This is what we did. To claim that we did not use N-version programming is ridiculous. <p> Six of the twenty-seven programs did not fail on any of the million input cases. The average failure probability was 0.0007 and the worst was 0.009. This should be compared to the published UCLA experiments <ref> [CHE78] </ref>, [KEL83], and [AVI84], which all had much poorer quality versions than we did in our experiment. No failure probabilities have been published for the UCLA/H experiment but the reported number of faults per version is comparable with ours.
Reference: [ECK85] <author> D.E. Eckhardt and L.D. Lee, </author> <title> A Theoretical Basis for the Analysis of Multiversion Software Subject to Coincident Errors, </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> Vol. SE-11, No. 12, </volume> <month> December </month> <year> 1985, </year> <pages> pp. 1511-1516. </pages>
Reference-contexts: Eckhardt and Lee <ref> [ECK85] </ref> have shown that even small probabilities of correlated failures, i.e., deviation from statistically independent failures, cause a substantial reduction in potential reliability improvement. The phrases ``low probability'' and ``not likely'' used by Avizienis are not quantified, and his conjecture and hypothesis are, therefore, not testable.
Reference: [ECK89] <author> D.E. Eckhardt, et al., </author> <title> An Experimental Evaluation of Software Redundancy as a Strategy for Improving Reliability, </title> <type> Technical Report, </type> <institution> NASA/Langley Research Center, </institution> <note> submitted for publication. </note>
Reference-contexts: Kelly was also involved in a team effort headed by NASA involving 4 universities (UCSB, Virginia, Illinois, and NCSU) in which 20 programs were generated from a single specification <ref> [ECK89] </ref>. We refer to this experiment here as the NASA experiment. Avizienis, Lyu, and Schutz generated 6 programs written in 6 languages [AVI87] from a single specification. We refer to this experiment as the UCLA/H experiment. <p> Using the NASA programs, which were developed using a method closely related to that used in the UCLA/H study, a group (including Dr. Kelly) led by Dr. David Eckhardt of NASA's Langley Research Center collected the same type of data and came to the same conclusion that we did. <ref> [ECK89] </ref>. In summary, the claims that our critics did not get our results are unsupported and appear to be based more on wishful thinking than scientific analysis. 3. Methodological Comparisons The second set of criticisms of our experiment have to do with the software development method that we used.
Reference: [JOS88] <author> M.K. Joseph, </author> <title> Architectural Issues in Fault-Tolerant, Secure Computing Systems, </title> <type> Ph.D. Dissertation, </type> <institution> Dept. of Computer Science, UCLA, </institution> <year> 1988. </year>
Reference-contexts: We refer to this experiment as the UCLA/H experiment. Joseph makes the following claim: Several experiments on NVP performed at UCLA [AVI84, AVI87] have not discovered the high rates of failure as reported in Knight/Leveson. <ref> [JOS88] </ref> There are two types of failure rates to which Joseph may be referring: individual version failure rates and 3-version failure rates. Failure rate is calculated by dividing the number of failures by the number of input cases. <p> For example, Joseph states: It would be a mistake to accept the Knight and Leveson work at face value without considering its many weaknesses. It is proposed that the study did not use NVP due to inadequacies in proper system development methods. <ref> [JOS88] </ref> and Avizienis states: The claims that the NVP process was investigated are not supported by the documentation of the software development process in [Knight/Leveson]. [AVI89] We used the methodology used by Chen [CHE78] and Kelly [KEL83, AVI84] and followed exactly what was stated by Avizienis in [AVI77, AVI85b]. <p> In [KNI86], no software development standards or methods were required of the programmers. This is an essential requirement for all development and raises doubts about the quality of the generated versions. <ref> [JOS88] </ref> There is a misunderstanding here about what we said in our paper. First of all, we did not say that no software development methods were used. We said that no one particular method was imposed on all the programmers. <p> Also, operational testing used randomly generated test cases. For critical and life-critical computer systems this is completely unacceptable. <ref> [JOS88] </ref> This has been one of the frequent criticisms about our experiment. The critics have refused to believe our statements (both public and private, oral and written) that they are misinformed. The programs were tested by their authors before they were submitted to the acceptance procedure. <p> In [KNI86] an NVP system fails if a majority of versions fail at the same time, regardless of whether the errors produced were similar. An NVP system will produce the wrong result only if similar, coincident errors are generated. <ref> [JOS88] </ref> The author's statement implies that a program that does not produce an output when required has not failed; a program only fails when it produces incorrect output. This differs from every definition of failure we have seen. <p> Real world experience, not a class room assignment as in [KNI86], is needed. Currently, several systems in Europe are using NVP (e.g., the European designed Airbus 320 [ROU86] [AVW87]). <ref> [JOS88] </ref> Our results are not misleading. Our conclusion is simple and clearly stated. <p> We feel strongly that careful, controlled experimentation in a realistic environment is required. However, advocating careful laboratory experimentation is different from advocating the accumulation of experience by using the technique in real, safety-critical applications where loss of life is possible, such as commercial aircraft <ref> [JOS88] </ref>. Attacking one experiment by John Knight and Nancy Lev eson does not make N-version programming a reasonable way to ensure the safety of software. All of the university experiments, including ours, have been limited in one way or another.
Reference: [KEL83] <author> J.P.J. Kelly, and A. Avizienis, </author> <title> A Specification-Oriented Multi-Version Software Experiment, </title> <booktitle> Proc. 13th International Symposium on Fault-Tolerant Computing, </booktitle> <address> Milan, Italy, </address> <month> June </month> <year> 1983, </year> <pages> pp. 120-126. </pages>
Reference-contexts: Kelly used three different specifications, written in OBJ, PDL, and English, to generate 18 programs which he executed on 100 input cases <ref> [KEL83, AVI84] </ref>. Kelly was also involved in a team effort headed by NASA involving 4 universities (UCSB, Virginia, Illinois, and NCSU) in which 20 programs were generated from a single specification [ECK89]. We refer to this experiment here as the NASA experiment. <p> the study did not use NVP due to inadequacies in proper system development methods. [JOS88] and Avizienis states: The claims that the NVP process was investigated are not supported by the documentation of the software development process in [Knight/Leveson]. [AVI89] We used the methodology used by Chen [CHE78] and Kelly <ref> [KEL83, AVI84] </ref> and followed exactly what was stated by Avizienis in [AVI77, AVI85b]. According to these definitions, N-version programming means writing N versions independently. This is what we did. To claim that we did not use N-version programming is ridiculous. <p> Six of the twenty-seven programs did not fail on any of the million input cases. The average failure probability was 0.0007 and the worst was 0.009. This should be compared to the published UCLA experiments [CHE78], <ref> [KEL83] </ref>, and [AVI84], which all had much poorer quality versions than we did in our experiment. No failure probabilities have been published for the UCLA/H experiment but the reported number of faults per version is comparable with ours.
Reference: [KEL86] <editor> J.P.J. Kelly, et al., </editor> <booktitle> Multi-Version Software Development, Proc. </booktitle> <address> Safecomp '86, Sarlat, France, </address> <month> October </month> <year> 1986, </year> <pages> pp. 43-49 </pages>
Reference-contexts: The purpose of fault masking in N-version systems is to continue to provide service in the presence of faults. This will not be possible if a majority of versions fail no matter how they fail. In <ref> [KEL86] </ref> it was stated that we took an extreme position by using vector voting. We hav e revoted the programs using element-by-element voting, as was suggested, and the results are identical to the ones we published originally [MAR87].
Reference: [KEL89] <author> J.P.J. Kelly, </author> <title> Current Experiences with Fault Tolerant Software Design: Dependability Through Diverse Formal Specifications, </title> <booktitle> Conference on Fault-Tolerant Computing Systems, </booktitle> <address> Germany, </address> <month> September </month> <year> 1989, </year> <pages> pp. 134-149. </pages>
Reference-contexts: Although early experiments caused concern that this assumption may not hold [KNI86], additional empirical work has shown that this assumption can hold if a `best practice' development paradigm is used [BIS85, AVI87]. <ref> [KEL89] </ref> These results [of the UCLA/H experiment] are different from previously published results by Knight and Leveson. [AVI87, AVI88] Their second argument is that the claimed difference in results is accounted for by the significant differences between their experiments and ours and the inadequacies of our software development method: [The Knight/Leveson <p> There have been many statements that the UCLA/H study got different results than we did. For example: Although early experiments caused concern that this assumption may not hold [KNI86], additional empirical work has shown that this assumption can hold if a `best practice' development paradigm is used [BIS85, AVI87]. <ref> [KEL89] </ref> The comparison of the results in the V/UCI and UCLA/H studies thus shows major disagreements [AVI88] From the published papers, we can find no evidence that the independence assumption was tested in the UCLA/H study. <p> Kelly also claims that our testing was inadequate and that this explains our results: Ke y results from empirical studies addressing the similar error problem include: ... (c) Versions which have not undergone systematic testing contain related faults [KNI86, SCO87]. <ref> [KEL89] </ref> The implication in this statement is that versions that have undergone systematic testing will not contain related faults. However, we note that all experiments on N-version programming that we know about have found related faults, no matter what type of testing they hav e undergone.
Reference: [KNI85] <author> J.C. Knight and N.G. Leveson, </author> <title> A Large Scale Experiment In N-Version Programming, </title> <booktitle> Digest of Papers FTCS-15: Fifteenth International Symposium on Fault-Tolerant Computing, </booktitle> <address> June 1985, Ann Arbor, MI. </address> <pages> pp. 135-139. </pages>
Reference-contexts: 1. Introduction In July 1985, we presented a paper at the Fifteenth International Symposium on Fault-Tolerant Computing <ref> [KNI85] </ref> describing the results of an experiment that we performed examining an hypothesis about one aspect of N-version programming, i.e., the statistical independence of version failure. A longer journal paper on that research appeared in the IEEE Transactions on Software Engineering in January 1986 [KNI86].
Reference: [KNI86] <author> J.C. Knight and N.G. Leveson, </author> <title> An Experimental Evaluation of the Assumption of Independence in Multi-version Programming, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-12, No. </volume> <month> 1 (January </month> <year> 1986), </year> <pages> pp. 96-109. </pages>
Reference-contexts: A longer journal paper on that research appeared in the IEEE Transactions on Software Engineering in January 1986 <ref> [KNI86] </ref>. Since our original paper appeared, some proponents of N-version programming have criticized us and our papers, making inaccurate statements about what we have done and what we have concluded. We hav e spoken and written to them privately attempting to explain their misunderstandings about our work. <p> For example, Avizienis states: It is our conjecture that a rigorous application of the design paradigm, as described in this paper would have led to the elimination of most faults described in <ref> [KNI86] </ref> before acceptance of the programs. [AVI87, AVI88] It is easy to assert that changes in experimental procedures would yield different results. Such conjectures, however, need to be supported with scientific proof before they can be accepted. Professor Avizienis and former students use two arguments to support their conjecture. <p> Although early experiments caused concern that this assumption may not hold <ref> [KNI86] </ref>, additional empirical work has shown that this assumption can hold if a `best practice' development paradigm is used [BIS85, AVI87]. [KEL89] These results [of the UCLA/H experiment] are different from previously published results by Knight and Leveson. [AVI87, AVI88] Their second argument is that the claimed difference in results is <p> The papers do not document the rules of isolation, and the C [ommunication]&D [ocumentation] protocol ... that are indicators of NVP quality. The V-specs of <ref> [KNI86] </ref> do not show the essential NVS [N-Version Software] attributes. <p> We examine each of these in turn. 2. Different Results The first claim by our critics is that their results are different from ours. For the benefit of the reader, we repeat the conclusion we drew in our 1986 paper <ref> [KNI86] </ref>: ``For the particular problem that was programmed for this experiment, we conclude that the assumption of independence of errors that is fundamental to some analyses of N-version programming does not hold. <p> Without collecting data on failure rates, it is not possible to draw any conclusions about the effectiveness of N-version programming. There have been many statements that the UCLA/H study got different results than we did. For example: Although early experiments caused concern that this assumption may not hold <ref> [KNI86] </ref>, additional empirical work has shown that this assumption can hold if a `best practice' development paradigm is used [BIS85, AVI87]. [KEL89] The comparison of the results in the V/UCI and UCLA/H studies thus shows major disagreements [AVI88] From the published papers, we can find no evidence that the independence assumption <p> We examine each in turn. 3.1. Quality of Versions Avizienis and Joseph state: [The Knight and Leveson] numerical results uniquely take the measure of the quality of their casual programming process. [AVI89] NVP does not mean low quality versions. In <ref> [KNI86] </ref>, no software development standards or methods were required of the programmers. This is an essential requirement for all development and raises doubts about the quality of the generated versions. [JOS88] There is a misunderstanding here about what we said in our paper. <p> Kelly also claims that our testing was inadequate and that this explains our results: Ke y results from empirical studies addressing the similar error problem include: ... (c) Versions which have not undergone systematic testing contain related faults <ref> [KNI86, SCO87] </ref>. [KEL89] The implication in this statement is that versions that have undergone systematic testing will not contain related faults. However, we note that all experiments on N-version programming that we know about have found related faults, no matter what type of testing they hav e undergone. <p> We note that in the UCLA/H study a much higher percentage of the faults found after testing were ``identical'' than those found during testing. 3.3. Voting Procedure In discussing our definition of failure, Joseph states: The definition of complete NVP failure is incorrect. In <ref> [KNI86] </ref> an NVP system fails if a majority of versions fail at the same time, regardless of whether the errors produced were similar. <p> V/UCI experiment was coarse, since it used one Boolean variable to represent 241 Boolean conditions for a ``missile launching decision.'' The UCLA/H experiment employed real number comparisons for inexact matching with specified tolerances. [AVI88] We used all 241 boolean conditions in the voting as is clearly stated in our paper <ref> [KNI86, page 99] </ref>. 3.4. <p> Our protocol is documented in our paper <ref> [KNI86, page 98] </ref>. Isolation, which is also documented in the same paper [KNI86], was enforced in the same way as in the UCLA experiment with the added factor that some of the programmers were separated by 3000 miles and were unknown to each other. <p> Our protocol is documented in our paper [KNI86, page 98]. Isolation, which is also documented in the same paper <ref> [KNI86] </ref>, was enforced in the same way as in the UCLA experiment with the added factor that some of the programmers were separated by 3000 miles and were unknown to each other. All correlated failures involved versions from both schools. 3.7. <p> Conclusions Joseph states: Thus, [the Knight and Leveson] results are misleading and should not be used by themselves as a basis for a decision about the effectiveness of NVP. Real world experience, not a class room assignment as in <ref> [KNI86] </ref>, is needed. Currently, several systems in Europe are using NVP (e.g., the European designed Airbus 320 [ROU86] [AVW87]). [JOS88] Our results are not misleading. Our conclusion is simple and clearly stated. We repeat again part of the conclusion section from our paper [KNI86] (italics are from the original paper): ``For <p> not a class room assignment as in <ref> [KNI86] </ref>, is needed. Currently, several systems in Europe are using NVP (e.g., the European designed Airbus 320 [ROU86] [AVW87]). [JOS88] Our results are not misleading. Our conclusion is simple and clearly stated. We repeat again part of the conclusion section from our paper [KNI86] (italics are from the original paper): ``For the particular problem that was programmed for this experiment, we conclude that the assumption of independence of errors that is fundamental to some analyses of N-version programming does not hold.
Reference: [MAR87] <author> A.J. Margosis, </author> <title> Empirical Studies of Multi-Version System Performance, </title> <type> Master's Thesis, </type> <institution> University of Virginia, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: In [KEL86] it was stated that we took an extreme position by using vector voting. We hav e revoted the programs using element-by-element voting, as was suggested, and the results are identical to the ones we published originally <ref> [MAR87] </ref>.
Reference: [MAR83] <author> D.J. Martin, </author> <title> Dissimilar software in high integrity applications in ight controls, Software for Avionics, </title> <booktitle> AGARD Conference Proceedings, </booktitle> <volume> No. 330, </volume> <pages> pp. 36-1 to 36-9, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: The phrases ``low probability'' and ``not likely'' used by Avizienis are not quantified, and his conjecture and hypothesis are, therefore, not testable. However, statistical independence, i.e., uncorrelated failures, is well defined, implied by much of the discussion about the technique, and assumed by some practitioners <ref> [MAR83, YOU85] </ref>. In order to test for statistical independence, we designed and executed an experiment in which 27 versions of a program were prepared independently from the same requirements specification by graduate and senior undergraduate students at two universities.
Reference: [NAG82] <author> P.M. Nagel and J.A. Skrivan, </author> <title> Software Reliability: Repetitive Run Experimentation and Modelling, </title> <type> Technical Report NASA CR-165836, </type> <institution> NASA/Langley Research Center, </institution> <month> February </month> <year> 1982. </year>
Reference-contexts: We were trying to simulate the lifetime production use of the software. That is why we used inputs that were randomly generated. Note, however, that they were randomly generated according to what the people at Boeing felt was a realistic operational profile for this application <ref> [NAG82] </ref>. We agree, of course, that the procedures we followed were not sufficiently complete for life-critical software. We never claimed that they were.
Reference: [ROU86] <author> J.C. Rouquet and P.J. Traverse, </author> <title> Safe and Reliable Computing on Board the Airbus and ATR Aircraft, </title> <booktitle> Proc. </booktitle> <address> SAFECOMP '86, Sarlat, France, </address> <month> October </month> <year> 1986, </year> <pages> pp. 93-97. </pages>
Reference-contexts: Real world experience, not a class room assignment as in [KNI86], is needed. Currently, several systems in Europe are using NVP (e.g., the European designed Airbus 320 <ref> [ROU86] </ref> [AVW87]). [JOS88] Our results are not misleading. Our conclusion is simple and clearly stated.
Reference: [SCO87] <author> R.K. Scott, J.W. Gault, and D.F. McAllister, </author> <title> Fault-Tolerant Software Reliability Modeling, </title> <journal> IEEE Tr ansactions on Software Engineering, </journal> <volume> Vol. SE-13, No. 5, </volume> <month> May </month> <year> 1987, </year> <pages> pp. 582-592. </pages>
Reference-contexts: Avizienis refers to our study and another one that got similar results <ref> [SCO87] </ref> and states: These efforts serve to illustrate the pitfalls of premature preoccupation with numerical results. [AVI89] Without looking at numerical results, one cannot do the necessary statistical analysis to determine whether the independence hypothesis holds or determine what type of benefits can be expected from using N-version programming. <p> Kelly also claims that our testing was inadequate and that this explains our results: Ke y results from empirical studies addressing the similar error problem include: ... (c) Versions which have not undergone systematic testing contain related faults <ref> [KNI86, SCO87] </ref>. [KEL89] The implication in this statement is that versions that have undergone systematic testing will not contain related faults. However, we note that all experiments on N-version programming that we know about have found related faults, no matter what type of testing they hav e undergone.
Reference: [SHI88] <author> T.J. Shimeall and N.G. Leveson, </author> <title> An Empirical Comparison of Software Fault Tolerance and Fault Elimination, </title> <booktitle> Proc. 2nd Workshop on Software Testing, Verification, and Analysis, </booktitle> <address> Banff, </address> <month> July </month> <year> 1988. </year> <note> (A more complete description is available as Tech. Report NPS52-89-047, </note> <institution> Naval Postgraduate School, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: However, we note that all experiments on N-version programming that we know about have found related faults, no matter what type of testing they hav e undergone. Shimeall and Leveson <ref> [SHI88] </ref>, in a study that compared fault elimination and fault tolerance methods using eight software versions, found that extensive testing was not likely to find the faults that resulted in correlated failures. <p> Attacking one experiment by John Knight and Nancy Lev eson does not make N-version programming a reasonable way to ensure the safety of software. All of the university experiments, including ours, have been limited in one way or another. None (except Shimeall and Leveson <ref> [SHI88] </ref>) has attempted to compare N-version programming with the alternatives to find out whether the money and resources could have been more effectively spent on other techniques such as sophisticated testing or formal verification.
Reference: [TSO87] <author> K.S. Tso and A. Avizienis, </author> <title> Community Error Recovery in N-Version Software: A Design Study with Experimentation, </title> <booktitle> Digest 17th Int. Symposium on Fault-Tolerant Computing, </booktitle> <address> Pittsburgh, </address> <month> July </month> <year> 1987, </year> <pages> pp. 127-133. </pages>
Reference-contexts: None of these seem to us to be very significant in terms of providing fault tolerance of design errors. Another source of their lack of diversity stems from the use of cross-checks points to vote on intermediate results and what they call Community Error Recovery <ref> [TSO87] </ref>: [The cross-check points] have to be executed in a certain predetermined order, but again great care was taken not to overly restrict the possible choices of computation sequence. [AVI87] One recovery point is used to recover a failed version by supplying it with a set of new internal state variables
Reference: [WAT78] <author> H.E. Waterman, </author> <title> FAA's Certification Position on Advanced Avionics, </title> <booktitle> AIAA Astronautics and Aeronautics, </booktitle> <month> May </month> <year> 1978, </year> <pages> pp. 49-51. </pages>
Reference-contexts: The FAA requires that failures of critical systems in commercial air transports be ``extremely improbable''. The phrase ``extremely improbable'' is defined by the FAA as ``not expected to occur within the total life span of the whole eet of the model <ref> [WAT78] </ref>.'' In practice, where such reliability can be analyzed, the phrase is taken to mean no more than 10-9 failures per hour of operation or per event for activities such as landing. No one has demonstrated that N-version programming can guarantee achievement of this level of reliability.
Reference: [YOU85] <author> L.J. Yount, et al., </author> <title> Fault Effect Protection and Partitioning for Fly-by-Wire/Fly-by-Light Avionics Systems, </title> <booktitle> AIAA Computer in Aerospace V Conference, </booktitle> <address> Long Beach, </address> <month> August </month> <year> 1985. </year>
Reference-contexts: The phrases ``low probability'' and ``not likely'' used by Avizienis are not quantified, and his conjecture and hypothesis are, therefore, not testable. However, statistical independence, i.e., uncorrelated failures, is well defined, implied by much of the discussion about the technique, and assumed by some practitioners <ref> [MAR83, YOU85] </ref>. In order to test for statistical independence, we designed and executed an experiment in which 27 versions of a program were prepared independently from the same requirements specification by graduate and senior undergraduate students at two universities.
References-found: 27

