URL: http://www.cs.umn.edu/Research/Agassiz/Paper/tsai.two-dim.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: E-mail: jtsai@cs.umn.edu  
Phone: Phone: +1-612- 625-7876 Fax: +1-612-625-0572  
Title: Improving Instruction Throughput and Memory Latency Using Two-Dimensional Superthreading  
Author: Jenn-Yuan Tsai Bixia Zheng and Pen-Chung Yew Jenn-Yuan Tsai 
Note: Corresponding author:  
Address: Urbana, IL 61801 Minneapolis, MN 55455  Building 200 Union Street, SE Minneapolis, MN 55455-0159 USA  
Affiliation: Department of Computer Science Department of Computer Science University of Illinois University of Minnesota  4-192 EE/CS  
Abstract: In this paper, we describe a two-dimensional concurrent multithreaded architecture which combines aggressive thread-level and instruction-level speculation on control dependences, runtime data dependence checking both within and between threads, compiler-controlled data speculation, and a thread pipelining execution model to exploit program parallelism both at the compile time and at the run time. It can then expend such program parallelism for memory latency hiding. Such an architecture allows parallelism to be exploited from complicated program structures such as doubly-nested WHILE loops, loops with multiple control blocks in the loop body, loops with ambiguous data dependences between loop iterations, etc. It also allows system resources to be shared more efficiently among threads. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, G. D'Souza, K Johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, B. H. Lim, G. Maa, D. Nussbaum, M. Parkin, and D. A. Yeung. </author> <title> The mit alewife machine: A large-scale distributed 13 memory multiprocessor. </title> <booktitle> In Proceedings of Workshop Multithreaded Computers, Supercomputing 91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP [8], Horizon [9], Tera [2], and Alewife <ref> [1] </ref>. To achieve high instruction issue rate, the XIMD [15], Elementary Multithreading [6], M-machine [4], Simultaneous Multithreading [14], Multiscalar [5, 12], and SPSM [3] machines allow instructions from different threads to be issued and executed concurrently.
Reference: [2] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera computer system. </title> <booktitle> In Conference Proceedings, 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June 11-15, </month> <year> 1990. </year>
Reference-contexts: There are several major obstacles which limit the amount of parallelism that can be exploited. Most state-of-the-art microprocessors use single-threaded superscalar architecture to exploit instruction-level parallelism. Its parallelism is limited by the size of the single instruction window. Multithreaded architectures, such as Tera <ref> [2] </ref> and M-Machine [4], rely totally on parallelizing compilers to partition a program into multiple threads for context switching. <p> Systems that adopt multithreading for hiding latency include HEP [8], Horizon [9], Tera <ref> [2] </ref>, and Alewife [1]. To achieve high instruction issue rate, the XIMD [15], Elementary Multithreading [6], M-machine [4], Simultaneous Multithreading [14], Multiscalar [5, 12], and SPSM [3] machines allow instructions from different threads to be issued and executed concurrently.
Reference: [3] <author> Pradeep K. Dubey, Kevin O'Brien, Kathryn O'Brien, and Charles Barton. </author> <title> Single-program speculative multithreading (SPSM) architecture: Compiler-assisted fine-grained multithreading. </title> <booktitle> In Proceedings of the IFIP WG 10.3 Working Conference on Parallel Architectures and Compilation Techniques, PACT '95, </booktitle> <pages> pages 109-121, </pages> <month> June 27-29, </month> <year> 1995. </year>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP [8], Horizon [9], Tera [2], and Alewife [1]. To achieve high instruction issue rate, the XIMD [15], Elementary Multithreading [6], M-machine [4], Simultaneous Multithreading [14], Multiscalar [5, 12], and SPSM <ref> [3] </ref> machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Multithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [4] <author> Marco Fillo, Stephen W. Keckler, Dally William J, Nicholas P. Carter, Andrew Chang, Yevgeny Gure-vich, and Whay S. Lee. </author> <title> The m-machine multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 146-156, </pages> <month> November 29-December 1, </month> <year> 1995. </year>
Reference-contexts: There are several major obstacles which limit the amount of parallelism that can be exploited. Most state-of-the-art microprocessors use single-threaded superscalar architecture to exploit instruction-level parallelism. Its parallelism is limited by the size of the single instruction window. Multithreaded architectures, such as Tera [2] and M-Machine <ref> [4] </ref>, rely totally on parallelizing compilers to partition a program into multiple threads for context switching. Program structures such as pointer aliases, loops with early exits, or complicated dependences across loop iterations cannot be analyzed at compile time, which often resulting in a huge loss in "exploitable" program parallelism. <p> Systems that adopt multithreading for hiding latency include HEP [8], Horizon [9], Tera [2], and Alewife [1]. To achieve high instruction issue rate, the XIMD [15], Elementary Multithreading [6], M-machine <ref> [4] </ref>, Simultaneous Multithreading [14], Multiscalar [5, 12], and SPSM [3] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Multithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [5] <author> Manoj Franklin and Gurindar S. Sohi. </author> <title> The expandable split window paradigm for exploiting fine-grained parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 58-67, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP [8], Horizon [9], Tera [2], and Alewife [1]. To achieve high instruction issue rate, the XIMD [15], Elementary Multithreading [6], M-machine [4], Simultaneous Multithreading [14], Multiscalar <ref> [5, 12] </ref>, and SPSM [3] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Multithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [6] <author> Hiroaki Hirata, Kozo Kimura, Satoshi Nagamine, Yoshiyuki Mochizuki, Akio Nishimura, Yoshimori Nakase, and Teiji Nishizawa. </author> <title> An elementary processor architecture with simultaneous instruction issuing from multiple threads. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 136-145, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP [8], Horizon [9], Tera [2], and Alewife [1]. To achieve high instruction issue rate, the XIMD [15], Elementary Multithreading <ref> [6] </ref>, M-machine [4], Simultaneous Multithreading [14], Multiscalar [5, 12], and SPSM [3] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Multithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [7] <author> Zhenzhen Jiang. </author> <title> Performance study of superthreaded architectures. </title> <type> Master's thesis, </type> <institution> University of Minnesota, Department of Computer Science, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: By exploiting more parallelism from programs, it can then be used to hide memory latency and to allow more efficient usage of processor recourses. To study the performance of such an architecture, we are modifying an existing single-dimension su-perthreaded architecture [13] simulator to perform cycle-by-cycle trace-driven simulation <ref> [7] </ref>. Compiler techniques are also being developed to take advantage of such an architecture. Preliminary results show that such an approach can be very useful to several time-consuming routines in some important benchmarks, such as SPEC95 and Perfect.
Reference: [8] <author> Harry F. Jordan. </author> <title> Performance measurements on HEP | a pipelined MIMD computer. </title> <booktitle> In Proceedings of the 10th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 207-212, </pages> <month> June 13-17, </month> <year> 1983. </year>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP <ref> [8] </ref>, Horizon [9], Tera [2], and Alewife [1]. To achieve high instruction issue rate, the XIMD [15], Elementary Multithreading [6], M-machine [4], Simultaneous Multithreading [14], Multiscalar [5, 12], and SPSM [3] machines allow instructions from different threads to be issued and executed concurrently.
Reference: [9] <author> J. T. Kuehn and B. J. Smith. </author> <title> The horizon supercomputing system: </title> <booktitle> Architecture and software. In Proceedings of Supercomputing 1988, </booktitle> <month> November </month> <year> 1988. </year>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP [8], Horizon <ref> [9] </ref>, Tera [2], and Alewife [1]. To achieve high instruction issue rate, the XIMD [15], Elementary Multithreading [6], M-machine [4], Simultaneous Multithreading [14], Multiscalar [5, 12], and SPSM [3] machines allow instructions from different threads to be issued and executed concurrently.
Reference: [10] <author> J. K. F. Lee and A. J. Smith. </author> <title> Branch prediction strategies and branch target buffer design. </title> <journal> IEEE Computer, </journal> <volume> 17(1) </volume> <pages> 6-22, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: In case of a tie,the instruction fetch logic will choose the oldest y-thread. To allow instructions fetch beyond a conditional branch, the instruction fetch logic can use a conventional branch target buffer <ref> [10] </ref> to support instruction-level control speculation within each y-thread. In every cycle, the instruction dispatch unit examines the top instructions in the instruction queues of all thread slots. If the required resources are available, the dispatch unit will send the instructions to the reservation stations of the appropriate functional units.
Reference: [11] <author> J. E. Smith and A. R. Pleszkun. </author> <title> Implementation of precise interrupts in pipelined processors. </title> <booktitle> In Proceedings of the 12th International Symposium on Computer Architecture, </booktitle> <pages> pages 36-44, </pages> <month> June </month> <year> 1985. </year> <month> 14 </month>
Reference-contexts: Our scheduling policy is to execute instructions from earlier threads as soon as possible, since they may produce results needed by later threads. Although instructions of a thread can be executed out of order, their execution results are completed in-order by using a reorder buffer <ref> [11] </ref> as in a conventional superscalar processor. 3.2 Memory Buffers The primary and the secondary memory buffers of a thread processing element have four major functions: * Performing run-time data dependence checking for data synchronization and data speculation, * Providing a thread with dependent store data from predecessor y-threads or x-threads,
Reference: [12] <author> Gurindar S. Sohi, Scott E. Breach, and T. N. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP [8], Horizon [9], Tera [2], and Alewife [1]. To achieve high instruction issue rate, the XIMD [15], Elementary Multithreading [6], M-machine [4], Simultaneous Multithreading [14], Multiscalar <ref> [5, 12] </ref>, and SPSM [3] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Multithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [13] <author> Jenn-Yuan Tsai and Pen-Chung Yew. </author> <title> The superthreaded architecture: Thread pipelining with run-time data dependence checking and control speculation. </title> <booktitle> In Proceedings of the 1996 Conference on Parallel Architectures and Compilation Techniques, PACT '96, </booktitle> <pages> pages 35-46, </pages> <month> October 20-23, </month> <year> 1996. </year>
Reference-contexts: By exploiting more parallelism from programs, it can then be used to hide memory latency and to allow more efficient usage of processor recourses. To study the performance of such an architecture, we are modifying an existing single-dimension su-perthreaded architecture <ref> [13] </ref> simulator to perform cycle-by-cycle trace-driven simulation [7]. Compiler techniques are also being developed to take advantage of such an architecture. Preliminary results show that such an approach can be very useful to several time-consuming routines in some important benchmarks, such as SPEC95 and Perfect.
Reference: [14] <author> Dean M. Tullsen, Susan J. Eggers, and Henry M. Levy. </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392-403, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP [8], Horizon [9], Tera [2], and Alewife [1]. To achieve high instruction issue rate, the XIMD [15], Elementary Multithreading [6], M-machine [4], Simultaneous Multithreading <ref> [14] </ref>, Multiscalar [5, 12], and SPSM [3] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Multithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
Reference: [15] <author> Andrew Wolfe and John P. Shen. </author> <title> A variable instruction stream extension to the VLIW architecture. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-14, </pages> <month> April 8-11, </month> <year> 1991. </year> <month> 15 </month>
Reference-contexts: Systems that adopt multithreading for hiding latency include HEP [8], Horizon [9], Tera [2], and Alewife [1]. To achieve high instruction issue rate, the XIMD <ref> [15] </ref>, Elementary Multithreading [6], M-machine [4], Simultaneous Multithreading [14], Multiscalar [5, 12], and SPSM [3] machines allow instructions from different threads to be issued and executed concurrently. The Simultaneous Multithreading and SPSM machines support only independent parallel threads, such as parallel loops without cross-iteration dependences (i.e. do-all loops).
References-found: 15

