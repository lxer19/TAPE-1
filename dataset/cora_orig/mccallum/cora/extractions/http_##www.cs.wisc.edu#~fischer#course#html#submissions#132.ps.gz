URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/132.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/
Root-URL: http://www.cs.wisc.edu
Email: ftrinder,kh,simonpjg@dcs.glasgow.ac.uk  
Title: GUM: a portable parallel implementation of Haskell a portable, parallel implementation of the Haskell functional
Author: PW Trinder K Hammond JS Mattson Jr AS Partridge SL Peyton Jones 
Keyword: Initial performance figures demonstrate absolute speedups  
Note: Email:  GUM is  GUM is one of the first such systems to be made publicly available.  relative to the  
Abstract: Abstract GUM is message-based, and portability is facilitated by using the PVM communications harness that is available on many multi-processors. As a result, GUM is available for both shared-memory (Sun SPARCserver multiprocessors) and distributed-memory (networks of workstations) architectures. The high message-latency of distributed machines is ameliorated by sending messages asynchronously, and by sending large packets of related data in each message. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arvind and Iannucci RA, </author> <title> "Two Fundamental Issues in Multiprocessing", </title> <booktitle> Proc DFVLR Conference on Parallel Processing in Science and Engineering, </booktitle> <month> Bonn-Bad Godesberg (June </month> <year> 1987). </year>
Reference-contexts: Since the sending PE retains its copy, locality is not lost. * All messages are asynchronous. The idea | which is standard in the multithreading community <ref> [1] </ref> | is that once a processor has sent a message it can forget all about it and schedule further threads or messages without waiting for a reply (Section 2.3.4). <p> Indeed, sometimes the reply may be delayed a long time, if (for example) it requests the value of a remote thunk that is being evaluated by some other thread. All of this is standard in the multithreading community <ref> [1] </ref>. 2.1 Thread Management A thread is a virtual processor. It is represented by a (heap-allocated) Thread State Object (TSO) containing slots for the thread's registers. The TSO in turn points to the thread's (heap-allocated) Stack Object (SO).
Reference: [2] <author> Bevan DI, </author> <title> "Distributed Garbage Collection using Reference Counting", </title> <booktitle> Proc PARLE, </booktitle> <editor> deBakker JW, Nij-man L and Treleaven PC (eds), </editor> <address> Eindhoven, Netherlands (June 1987). </address>
Reference-contexts: Again following standard practice [22], we use weighted reference counting to recover local identifiers, and hence the closures they identify <ref> [2, 35] </ref>. We augment both the GIT and the GA ! LA table to hold a weight as well as the local address.
Reference: [3] <author> Augustsson L, and Johnsson T, </author> <title> "Parallel Graph Reduction with the &lt; -; G &gt;-Machine", </title> <booktitle> Proc. FPCA '89, </booktitle> <address> London, UK, </address> <year> (1989), </year> <pages> pp. 202-213. </pages>
Reference-contexts: To avoid duplicating work, each thunk was locked when it was entered. This is a potential weakness of the design: locking can be costly, and consumes significant shared-memory bandwidth, generally unnecessarily. Relative speedups of 6 to 10 were achieved with 12 processors. The &lt; -; G &gt;-Machine <ref> [3] </ref> was based on the sequential Chalmers G-Machine compiler, and ran on a 16-processor Sequent Symmetry. There was no stack, but instead thunks were built with enough space to hold all necessary arguments plus some local workspace for temporary variables.
Reference: [4] <author> Burton FW, and Rayward-Smith VJ, </author> <title> "Worst Case Scheduling for Parallel Functional Programming", </title> <journal> Journal of Functional Programming, </journal> <volume> 4(1), </volume> <month> (January </month> <year> 1994), </year> <pages> pp. 65-75. </pages>
Reference-contexts: When a thread is chosen for execution it is run until either space is exhausted, the thread blocks (either on another thread or accessing remote data), or the thread completes. Compared with fair scheduling, this has the advantage of tending to decrease both space usage and overall run-time <ref> [4] </ref>, at the cost of making concurrent and speculative execution rather harder. 2.1.1 Sparks Parallelism is initiated by the par combinator in the source program. (At present these combinators are added by the programmer, though we would of course like this task to be 2 automated.) When the expression x `par`
Reference: [5] <author> Cann DC, </author> <title> "Retire Fortran? A Debate Rekindled", </title> <journal> CACM, </journal> <volume> 35(8), </volume> <month> (August </month> <year> 1992), </year> <pages> pp. 81-89. </pages>
Reference-contexts: There are many good implementations for strict languages, including Lisp derivatives such as Qlisp [11] or Mul-T [19], ML-based implementations such as Concurrent ML [32] or Caml Flight [7], and dataflow languages such as Sisal [24] and Id [16]. Indeed, in some cases, the Sisal implementation outperforms parallelis-ing Fortran <ref> [5] </ref>. A more detailed comparison with these and other implementations will be provided in the full paper. 5.1 Shared-Memory Implementations Shared-memory implementations have been quite successful, often showing good relative speedup for limited numbers of processors on simple programs.
Reference: [6] <author> Chakravarty MMT, </author> <title> "A Self-Scheduling, Non-Blocking Parallel Abstract Machine for Non- Strict Functional Languages", </title> <booktitle> Proc 6th. Intl. Workshop on Implementation of Functional Languages, </booktitle> <institution> Glauert JRW (ed.), University of East Anglia, </institution> <month> (September </month> <year> 1994). </year>
Reference: [7] <author> Foisy C, and Chailloux E, </author> <title> "Caml Flight: a Portable SPMD Extension of ML for Distributed Memory Multiprocessors", </title> <booktitle> Proc. High Performance Functional Computing '95, </booktitle> <address> Denver, Colorado, </address> <month> (April </month> <year> 1995), </year> <pages> pp. 83-96. </pages>
Reference-contexts: There are many good implementations for strict languages, including Lisp derivatives such as Qlisp [11] or Mul-T [19], ML-based implementations such as Concurrent ML [32] or Caml Flight <ref> [7] </ref>, and dataflow languages such as Sisal [24] and Id [16]. Indeed, in some cases, the Sisal implementation outperforms parallelis-ing Fortran [5].
Reference: [8] <editor> Goldberg B, and Hudak P, "Alfalfa: </editor> <title> Distributed Graph Reduction on a Hypercube Multiprocessor", Proc. Workshop on Graph Reduction, </title> <editor> Fasel RMKJF (ed.), </editor> <address> Santa Fe, NM, </address> <publisher> Springer Verlag LNCS 279, </publisher> <year> (1986), </year> <pages> pp. 94-113. </pages>
Reference-contexts: It is claimed that eliminating locking improves performance by up to a factor of two on 4 processors. 5.2 Distributed-Memory Implementations There have been several Transputer-based distributed-memory implementations, and a few on other architectures. Alfalfa was a distributed-memory implementation for the In-tel iPSC, similar to, but predating Buckwheat <ref> [8] </ref>. Unfortunately, the communication overhead on this system was extremely high. Because of this, and perhaps because the implementation techniques used were less suited to distributed-memory than shared-memory, performance results were extremely disappointing: relative speedups of around 4 to 8 being achieved for 32 processors.
Reference: [9] <author> Goldberg BF, "Buckwheat: </author> <title> Graph Reduction on a Shared Memory Multiprocessor", </title> <booktitle> Proc. ACM Conf. on Lisp and Functional Programming, </booktitle> <address> Snowbird, Utah, </address> <year> (1988), </year> <month> ppp. </month> <pages> 40-51. </pages>
Reference-contexts: A more detailed comparison with these and other implementations will be provided in the full paper. 5.1 Shared-Memory Implementations Shared-memory implementations have been quite successful, often showing good relative speedup for limited numbers of processors on simple programs. One of the first successful implementations was Buckwheat <ref> [9] </ref>, which ran on the Encore Multimax. This used a fairly conventional stack-based implementation of compiled graph-reduction, with a single shared heap, and a two-level task queue, which aimed to reduce memory contention. To avoid duplicating work, each thunk was locked when it was entered.
Reference: [10] <author> Goldberg B, </author> <title> Multiprocessor Execution of Functional Programs, </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <month> (April </month> <year> 1988). </year>
Reference: [11] <author> Goldman R, and Gabriel RP, </author> <title> "Qlisp: </title> <booktitle> Parallel Processing in Lisp", IEEE Software, </booktitle> <pages> pp. 51-59, </pages> <year> (1989). </year>
Reference-contexts: This section describes the implementations that are most closely related to GUM: those based on compiled graph reduction for non-strict purely functional languages. There are many good implementations for strict languages, including Lisp derivatives such as Qlisp <ref> [11] </ref> or Mul-T [19], ML-based implementations such as Concurrent ML [32] or Caml Flight [7], and dataflow languages such as Sisal [24] and Id [16]. Indeed, in some cases, the Sisal implementation outperforms parallelis-ing Fortran [5].
Reference: [12] <author> Hammond K, and Peyton Jones SL, </author> <title> "Profiling Scheduling Strategies on the GRIP Multiprocessor", </title> <booktitle> Proc 4th. Intl. Workshop on Parallel Implementation of Functional Languages, </booktitle> <editor> Kuchen H (ed.), </editor> <address> Aachen University, </address> <month> (September </month> <year> 1992). </year>
Reference: [13] <author> Hammond K, </author> <title> "Parallel Functional Programming: an Introduction", </title> <booktitle> Proc. PASCO '94, </booktitle> <address> Linz, Austria, </address> <publisher> World Scientific, </publisher> <month> (September </month> <year> 1994), </year> <pages> pp. 181-193. </pages>
Reference-contexts: This result is in close agreement with the PVM example program timing, which shows that for the machine configuration we used, most of the benefit of increasing message size is gained when messages are around 13KByte. 5 Related Work There has been much work on parallel functional programming. Hammond <ref> [13] </ref> provides a historical overview and introduces the principal approaches that have been taken at both the language and implementation levels. This section describes the implementations that are most closely related to GUM: those based on compiled graph reduction for non-strict purely functional languages.
Reference: [14] <author> Hammond K, Loidl H-W, </author> <title> and Partridge AS, "Visu-alising Granularity in Parallel Programs: A Graphical Winnowing System for Haskell", </title> <booktitle> Proc HPFC'95 | High Performance Functional Computing, </booktitle> <address> Denver, Col-orado, </address> <month> (April </month> <year> 1995). </year>
Reference-contexts: GranSim was constructed to investigate aspects of parallel graph reduction <ref> [14] </ref>, and it can be configured to simulate the running of programs compiled for GUM on most parallel machines. When a GUM program is run with profiling enabled, each PE accumulates a record of significant execution events events (notably thread creation, blocking, and termination).
Reference: [15] <author> Herlihy M, and Liskov B, </author> <title> "A value transmission method for abstract data types", </title> <journal> ACM TOPLAS 4(4), </journal> <year> (1982), </year> <pages> pp. 527-551 </pages>
Reference-contexts: Packing arbitrary graph is a non-trivial problem, and the full paper discusses related work in this area <ref> [34, 27, 15] </ref>, together with the algorithms and heuristics used in GUM to pack graph into packets. It also carefully describes the mechanism that ensures that, even when a thunk is transferred between PEs, there is always exactly one copy of it.
Reference: [16] <author> Hicks J, Chiou D, Ang BS, and Arvind, </author> <title> "Performance Studies of Id on the Monsoon Dataflow System", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18, </volume> <year> (1993), </year> <pages> pp. 273-300. </pages>
Reference-contexts: There are many good implementations for strict languages, including Lisp derivatives such as Qlisp [11] or Mul-T [19], ML-based implementations such as Concurrent ML [32] or Caml Flight [7], and dataflow languages such as Sisal [24] and Id <ref> [16] </ref>. Indeed, in some cases, the Sisal implementation outperforms parallelis-ing Fortran [5]. A more detailed comparison with these and other implementations will be provided in the full paper. 5.1 Shared-Memory Implementations Shared-memory implementations have been quite successful, often showing good relative speedup for limited numbers of processors on simple programs.
Reference: [17] <author> Kesseler M, </author> <title> "Reducing Graph Copying Costs Time to Wrap it up", </title> <booktitle> Proc. PASCO '94 | First Intl. Symposium on Parallel Symbolic Computation, </booktitle> <address> Hagen-berg/Linz, Austria, </address> <publisher> World Scientific, </publisher> <month> (September </month> <year> 1994), </year> <pages> pp. 244-253. </pages>
Reference-contexts: In contrast to the GUM fishing strategy, tasks are statically allocated to processors by means of annotations. Relative speedups of 8.2 to 14.8 are reported for simple benchmarks on a 16-processor Transputer system <ref> [17] </ref>. 5.3 GRIP GUM's design is a development and simplification of our earlier work on the GRIP multiprocessor [30]. GRIP's memory was divided into fast unshared memory that was local to a PE, with separate banks of globally addressed memory that could be accessed through a fast packet-switched network.
Reference: [18] <author> Kingdon H, Lester D, and Burn GL, </author> <title> "The HDG-Machine: a Highly Distributed Graph Reducer for a Transputer Network", </title> <journal> The Computer Journal, </journal> <volume> 34(4), </volume> <month> (April </month> <year> 1991), </year> <pages> pp. 290-302. 11 </pages>
Reference-contexts: The improvement with problem size suggests either that the dispersal algorithm may not work effectively for small problems, or that there is significant overhead to exporting tasks. The HDG-Machine <ref> [18] </ref> uses a packet-based approach to memory allocation that is similar to that of the &lt; -; G &gt;- Machine, but with a distributed weighted reference-counting garbage collection scheme [22]. Task distribution is similar to ZAPP.
Reference: [19] <author> Kranz DA, Halstead RH, and Mohr E, "Mul-T: </author> <title> a High--Performance Parallel Lisp", </title> <booktitle> Proc. PLDI '89, </booktitle> <address> Portland, Oregon, </address> <year> (1989), </year> <pages> pp. 81-90. </pages>
Reference-contexts: This section describes the implementations that are most closely related to GUM: those based on compiled graph reduction for non-strict purely functional languages. There are many good implementations for strict languages, including Lisp derivatives such as Qlisp [11] or Mul-T <ref> [19] </ref>, ML-based implementations such as Concurrent ML [32] or Caml Flight [7], and dataflow languages such as Sisal [24] and Id [16]. Indeed, in some cases, the Sisal implementation outperforms parallelis-ing Fortran [5].
Reference: [20] <author> Kranz DA, Johnson K, Agarwal A, Kabiatowicz J, and Lim B-H, </author> <title> "Integrating Message-Passing and Shared-Memory: Early Experience", </title> <booktitle> Proc. 4th ACM Symp. on Principles and Practice of Parallel Programming (PPOPP), </booktitle> <address> San Diego, CA, </address> <month> (May </month> <year> 1993), </year> <pages> pp. 54-63. </pages>
Reference: [21] <author> Langendoen K, </author> <title> "Graph Reduction on Shared Memory Multiprocessors", </title> <type> PhD Thesis, </type> <institution> University of Amster-dam, </institution> <year> 1993. </year>
Reference-contexts: Clearly, such a strategy can lead to starvation, and should be used only on programs which are coarse-grained or which generate continual parallelism. On the Sequent Balance, GAML achieves relative speedups of between 3.3 and 5.8 for small programs []. WYBERT <ref> [21] </ref> is based on the FAST/FCG sequential compiler, and runs on a 4-processor Motorola HYPERmodule. Rather than defining a general primitive for parallelism, as with the previous approaches, the implementation uses an explicit divide-and-conquer skeleton.
Reference: [22] <editor> Lester D "An Efficient Distributed Garbage Collection Algorithm", </editor> <booktitle> Proc. PARLE '89, </booktitle> <publisher> LNCS 365, Springer Verlag, </publisher> <month> (June </month> <year> 1989). </year>
Reference-contexts: Again following standard practice <ref> [22] </ref>, we use weighted reference counting to recover local identifiers, and hence the closures they identify [2, 35]. We augment both the GIT and the GA ! LA table to hold a weight as well as the local address. <p> The HDG-Machine [18] uses a packet-based approach to memory allocation that is similar to that of the &lt; -; G &gt;- Machine, but with a distributed weighted reference-counting garbage collection scheme <ref> [22] </ref>. Task distribution is similar to ZAPP. Only incremental fetching strategies were tested with this scheme, though presumably a bulk fetching strategy would also be possible. Relative speedups for naive Fi-bonacci were of 3.6 on 4 transputers. Concurrent Clean runs on transputers and networks of Macintoshes [28].
Reference: [23] <author> McBurney DL, </author> <title> and Sleep MR, "Transputer Based Experiments with the ZAPP Architecture", </title> <booktitle> Proc. PARLE '87, </booktitle> <publisher> LNCS 258/259, Springer Verlag, </publisher> <year> (1987), </year> <pages> pp. 242-259. </pages>
Reference-contexts: Unfortunately, the communication overhead on this system was extremely high. Because of this, and perhaps because the implementation techniques used were less suited to distributed-memory than shared-memory, performance results were extremely disappointing: relative speedups of around 4 to 8 being achieved for 32 processors. Like WYBERT, ZAPP <ref> [23] </ref> aims to implement only divide-and-conquer parallelism, using an explicit fork-and-join skeleton. Once generated, tasks can either be executed on the processor that generated them or stolen by a neighbour-ing processor. There is no task migration, so the program retains a high degree of locality.
Reference: [24] <author> McGraw J, Skedzielewski S, Allan S, Odehoeft R, Glauert JRW, Kirkham C, Noyce W, and Thomas R, </author> <title> "SISAL: Streams and Iteration in a Single-Assignment Language: Reference Manual Version 1.2", Manual M-146, </title> <type> Rev. 1, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> (March </month> <year> 1985). </year>
Reference-contexts: There are many good implementations for strict languages, including Lisp derivatives such as Qlisp [11] or Mul-T [19], ML-based implementations such as Concurrent ML [32] or Caml Flight [7], and dataflow languages such as Sisal <ref> [24] </ref> and Id [16]. Indeed, in some cases, the Sisal implementation outperforms parallelis-ing Fortran [5].
Reference: [25] <author> Maranget L, "GAML: </author> <title> a Parallel Implementation of Lazy ML" Proc. </title> <booktitle> FPCA '91, </booktitle> <publisher> Springer Verlag LNCS 523, </publisher> <address> p.102-123, </address> <year> (1991). </year>
Reference: [26] <author> Mattson JS, </author> <title> An Effective Speculative Evaluation Technique for Parallel Supercombinator Graph Reduction, </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <year> (1993). </year>
Reference-contexts: As a consequence, while the implementation did achieve real speedups, they were less than might have been expected: a factor of 5 to 11 on a 16-processor configuration. Mattson observed similar problems with a similar shared-memory implementation of the STG-Machine on a 64-processor BBN Butterfly <ref> [26] </ref>. In this case the best overall performance was achieved using approximately 16 processors, and adding more processors had a negative effect on performance.
Reference: [27] <author> Newcomer JM, </author> <title> "Efficient Binary I/O of IDL objects", </title> <journal> ACM SIGPLAN Notices 22(11), </journal> <month> (November </month> <year> 1987), </year> <pages> pp. 35-43. </pages>
Reference-contexts: Packing arbitrary graph is a non-trivial problem, and the full paper discusses related work in this area <ref> [34, 27, 15] </ref>, together with the algorithms and heuristics used in GUM to pack graph into packets. It also carefully describes the mechanism that ensures that, even when a thunk is transferred between PEs, there is always exactly one copy of it.
Reference: [28] <author> Nocker EGJMH, Smetsers JEW, van Eekelen MCJD and Plasmeijer MJ, </author> <title> "Concurrent Clean", </title> <booktitle> Proc. PARLE '91, </booktitle> <publisher> Springer Verlag LNCS 505/506, </publisher> <pages> pp. 202-220, </pages> <year> (1991). </year>
Reference-contexts: Task distribution is similar to ZAPP. Only incremental fetching strategies were tested with this scheme, though presumably a bulk fetching strategy would also be possible. Relative speedups for naive Fi-bonacci were of 3.6 on 4 transputers. Concurrent Clean runs on transputers and networks of Macintoshes <ref> [28] </ref>. Like GUM, it is stack-based, and uses tables of "in-pointers" to allow independent local garbage collection.
Reference: [29] <institution> Oak Ridge National Laboratory, University of Ten-nessee, </institution> <note> "Parallel Virtual Machine Reference Manual, Version 3.2", </note> <month> (August </month> <year> 1993). </year>
Reference-contexts: The goal of this paper is to give a technical overview of GUM, highlighting our main design choices, and present preliminary performance measurements. GUM has the following features: * GUM is portable. It is message based, and uses PVM <ref> [29] </ref>, a communication infrastructure available on almost every multiprocessor, including both shared-memory and distributed-memory machines, as well as networks of workstations. The basic assumed architecture is that of a collection of processor-memory units (which we will call PEs) connected by some kind of network that is accessed through PVM.
Reference: [30] <author> Peyton Jones SL, Clack C, Salkild J, </author> <title> "High-performance parallel graph reduction", </title> <booktitle> Proc PARLE '89, </booktitle> <publisher> Springer Verlag LNCS 365 (June 1989). </publisher>
Reference-contexts: Nevertheless, such tests provide an important sanity check: if the system does badly here then all is lost. garbage-collect its local heap independently of any other PE, a property we found to be crucial on the GRIP multiprocessor <ref> [30] </ref>. * Thread distribution is performed lazily, but data distribution is performed somewhat eagerly. Threads are never exported to other PE to try to "balance" the load. Instead, work is only moved when a processor is idle (Section 2.2). Moving work prematurely can have a very bad effect on locality. <p> Relative speedups of 8.2 to 14.8 are reported for simple benchmarks on a 16-processor Transputer system [17]. 5.3 GRIP GUM's design is a development and simplification of our earlier work on the GRIP multiprocessor <ref> [30] </ref>. GRIP's memory was divided into fast unshared memory that was local to a PE, with separate banks of globally addressed memory that could be accessed through a fast packet-switched network. Closures were fetched from global memory singly on demand rather than using GUM-style bulk fetching.
Reference: [31] <author> Peyton Jones SL, Gordon AD, and Finne SO, </author> <title> "Concurrent Haskell", </title> <booktitle> To appear in Proc. ACM Symposium on Principles of Programming Languages, </booktitle> <address> St Petersburg Beach, Florida, </address> <month> (January </month> <year> 1996). </year>
Reference-contexts: In the longer term, we plan to investigate adding speculative evaluation and support for explicit concurrent processes <ref> [31] </ref>. We hope that the public availability of the system will encourage others to join us in these developments.
Reference: [32] <author> Reppy JH, </author> <title> "CML: a Higher-Order Concurrent Language", </title> <booktitle> Proc. PLDI '91, </booktitle> <address> Toronto, Canada, </address> <month> (June </month> <year> 1991), </year> <pages> pp. 293-305. </pages>
Reference-contexts: This section describes the implementations that are most closely related to GUM: those based on compiled graph reduction for non-strict purely functional languages. There are many good implementations for strict languages, including Lisp derivatives such as Qlisp [11] or Mul-T [19], ML-based implementations such as Concurrent ML <ref> [32] </ref> or Caml Flight [7], and dataflow languages such as Sisal [24] and Id [16]. Indeed, in some cases, the Sisal implementation outperforms parallelis-ing Fortran [5].
Reference: [33] <author> Sansom PM, </author> <title> "Time Profiling in a Lazy Functional Language", </title> <booktitle> Proc. 1993 Glasgow Workshop on Functional Programming, </booktitle> <address> Ayr, </address> <publisher> Springer-Verlag WICS, </publisher> <month> (July </month> <year> 1993), </year> <pages> pp. 227-239. </pages>
Reference-contexts: Choosing the time base against which to record measurements in a distributed, multi-programmed environment is hard. The full paper considers this issue and the consequences of using both elapsed-time and virtual-time i.e. the time when the program is actually scheduled. 3.1 PE Activity Profiles GUM uses cost-centre profiling <ref> [33] </ref> internally to record the activity of each PE during reduction. Figure 5 gives the PE activity profile for one of four PEs evaluating a linear 6 equation program.
Reference: [34] <author> Toyn I, and Dix AJ, </author> <title> "Efficient Binary Transfer of Pointer Structures", </title> <journal> Software Practice and Experience 24(11), </journal> <month> (November </month> <year> 1994), </year> <pages> pp. 1001-1023. </pages>
Reference-contexts: Packing arbitrary graph is a non-trivial problem, and the full paper discusses related work in this area <ref> [34, 27, 15] </ref>, together with the algorithms and heuristics used in GUM to pack graph into packets. It also carefully describes the mechanism that ensures that, even when a thunk is transferred between PEs, there is always exactly one copy of it.

References-found: 34

