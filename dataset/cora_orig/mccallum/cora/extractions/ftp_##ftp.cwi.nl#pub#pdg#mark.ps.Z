URL: ftp://ftp.cwi.nl/pub/pdg/mark.ps.Z
Refering-URL: http://www.cwi.nl/~pdg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: msteyver@indiana.edu  pdg@cwi.nl  
Title: A Recurrent Network that performs a Context-Sensitive Prediction Task  
Author: Mark Steijvers Peter Gr unwald 
Address: Bloomington IN 47405  CWI, P.O. Box 4079 1009 AB Amsterdam, The Netherlands  
Affiliation: Department of Psychology Indiana University  Department of Algorithmics  
Abstract: We address the problem of processing a context-sensitive language with a recurrent neural network (RN). So far, the language processing capabilities of RNs have only been investigated for regular and context-free languages. We present an extremely simple RN with only one parameter z for its two hidden nodes that can perform a prediction task on sequences of symbols from the language f(ba k ) n j k 0; n &gt; 0g, a language that is context-sensitive but not context-free. The input to the RN consists of any string of the language, one symbol at a time. The network should then, at all times, predict the symbol that should follow. This means that the network must be able to count the number of a's in the first subsequence and to retain this number for future use. We present a value for the parameter z for which our RN can solve the task for k = 1 up to k = 120. As we do not give any method to find a good value for z, this does not say anything about the learning capabilities of our network. It does, however, show that context-sensitive information (the count of a's) can be represented by the network; we analyse in detail how this is done. Hence our work shows that, at least from a representational point of view, connectionist architectures can handle more complex formal languages than was previously known. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cleeremans, A., Servan-Schreiber, D., & McClelland, J. </author> <year> (1989). </year> <title> Finite state automata and simple recurrent networks. Neural Computation, </title> <publisher> 1(3),372-381. </publisher>
Reference: <author> Elman, J. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, 14,179-211. </journal>
Reference: <author> Giles, C., Miller, C., Chen, D., Chen, H., Sun, G. & Lee, Y. </author> <year> (1992). </year> <title> Learning and extracting finite state automata with second-order recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 2, </volume> <pages> 331-349. </pages>
Reference: <author> Hopcroft, J. and Ullman, J. </author> <year> (1979). </year> <title> Introduction to Automata Theory, </title> <booktitle> Languages and Computation. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: In this paper we show that a very simple recurrent network of a kind that has often been studied before is able to process a fairly complex language: a language that is neither regular nor context-free. Regular languages represent the simplest class of formal languages in the Chomsky hierarchy <ref> (Hopcroft & Ullman, 1979) </ref>. Regular languages are generated by regular grammars. Each regular language L has an associated deterministic finite state automaton (DFA) M and vice versa: M accepts all correct sentences of L and rejects all incorrect sentences. <p> An even more complex language class is that of the context sensitive languages with the associated linear bounded automata. This class properly includes all context-free languages. The theory of these languages and automata from a symbol processing perspective is well established <ref> (Hopcroft & Ullman, 1979) </ref>. It is not clear however, what kind of automata RNs can implement. So far, only performance on regular and context-free languages has been reported (Cleeremans, Servan-Schreiber & McClelland, 1989; Giles et al., 1992; Sun et al., 1993; Wiles & Elman, 1995). <p> Just as one cannot use a push-down automaton to recognize a language that is not context-free, one cannot use it to predict the consecutive symbols of the correct strings of such a language either <ref> (Hopcroft & Ullman, 1979) </ref>. It is in this sense that the power of a recurrent network that would perform well on our prediction task goes beyond the power of context-free grammars or, equivalently, push-down automata.
Reference: <author> Jordan, M. </author> <year> (1986). </year> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the Ninth Annual conference of the Cognitive Science Society (pp. </booktitle> <pages> 531-546). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Minsky, M. & Papert, S. </author> <year> (1988). </year> <title> Perceptrons. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Omlin, C. & Giles, C. </author> <year> (1994). </year> <title> Constructing Deterministic Finite-State Automata in recurrent neural networks. </title> <type> Technical report 94-3. </type> <institution> Troy, NY: Rensselaer Polytechnic Institute, Department of Computer Science. </institution>
Reference-contexts: When RNs are applied to processing languages, the solutions provided by the network are often best understood from a dynamical systems perspective <ref> (Omlin & Giles, 1994) </ref>. This perspective can sometimes offer new insights and provide new mechanisms for solving tasks that are usually dealt with from a more traditional symbolic framework. <p> If we then interpreted O &lt; 0:5 as a and O 0:5 as b, this would always yield the same predictions as our threshold unit. For recurrent neural networks, usually this latter approach is taken <ref> (Omlin & Giles, 1994) </ref>. We have opted for the equivalent threshold approach in order to clarify the analysis of the hidden node activation space. z = 3:9924 are shown for k = 1 to 6. for the sequences k = 1 to 120. for z = 3.
Reference: <author> Pollack, J. </author> <year> (1991). </year> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> 227-252. </pages>
Reference: <author> Sun, G., Giles, C., Chen, H., & Lee, Y. </author> <year> (1993). </year> <title> The neural network push-down automaton: model, stack and learning simulations. </title> <type> Technical Report UMIACS-TR-93-77 & CS-TR-3118. </type> <institution> College Park, MD: UMIACS. </institution>

References-found: 9

