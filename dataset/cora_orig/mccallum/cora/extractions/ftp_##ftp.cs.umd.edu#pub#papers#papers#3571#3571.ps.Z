URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3571/3571.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: fwak,pughg@cs.umd.edu  
Title: Minimizing Communication while Preserving Parallelism  
Author: Wayne Kelly and William Pugh 
Date: December 1, 1995  
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland,  
Abstract: To compile programs for message passing architectures and to obtain good performance on NUMA architectures it is necessary to control how computations and data are mapped to processors. Languages such as High-Performance Fortran use data distributions supplied by the programmer and the owner computes rule to specify this. However, the best data and computation decomposition may differ from machine to machine and require substantial expertise to determine. Therefore, automated decomposition is desirable. All existing methods for automated data/computation decomposition share a common failing: they are very sensitive to the original loop structure of the program. While they find a good decomposition for that loop structure, it may be possible to apply transformations (such as loop interchange and distribution) so that a different decomposition gives even better results. We have developed automatic data/computation decomposition methods that are not sensitive to the original program structure. We can model static and dynamic data decompositions as well as computation decompositions that cannot be represented by data decompositions and the owner computes rule. We make use of both parallel loops and doacross/pipelined loops to exploit parallelism. We describe an automated translation of the decomposition problem into a weighted graph that incorporates estimates of both parallelism and communication for various candidate computation decompositions. We solve the resulting search problem exactly in a very short time using a new algorithm that has shown to be able to prune away a majority of the vast search space. We assume that the selection of the computation decomposition is followed by a transformation phase that reorders the iterations to best match the selected computation decomposition. Our graph includes constraints to ensure that a reordering transformation giving the predicted parallelism exists. 
Abstract-found: 1
Intro-found: 1
Reference: [AAL95] <author> J. Anderson, S. Amarasinghe, and M. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In Proc. of the 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: These space mappings result in the first four statements being parallelized and three of the last four statements being pipelined, but they result in only 4 n 3 and 4 n 2 inter-processor communications. Other researchers <ref> [AAL95] </ref> have shown that data distributions analogous to these space mappings produce close to linear speedups on several shared 3 Registered trademark of Pure Software Inc. memory machines.
Reference: [AL93] <author> Saman P. Amarasinghe and Monica S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In ACM '93 Conf. on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: We can generate dynamic data decompositions by adding constraints to force the computation distribution to be equivalent to a dynamic data distribution. The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93, RKU93, GAL95, SSP + 95] </ref>. Our work improves on most previous work in the following ways: 1. We are not influenced by the order of the computation in the original program. <p> If we cannot, perhaps due to the use of an indirection array, extensions beyond what are described in this paper will be necessary. Related work 1 2 3 4 5 6 [Gup92] <ref> [AL93] </ref> [RKU93] [Fea94] [GAL95] [SSP + 95] (a) Our system p p p p p p (a) Starts with a functional language, so not relevant Table 4: Properties (from Section 1) of related work 7 Related Work Table 4 shows which of the desirable properties enumerated in Section 1 hold for
Reference: [Fea94] <author> Paul Feautrier. </author> <title> Toward automatic distribution. </title> <journal> Parallel Processing Letters, </journal> <volume> 4(3) </volume> <pages> 233-244, </pages> <month> Septem-ber </month> <year> 1994. </year>
Reference-contexts: We can generate dynamic data decompositions by adding constraints to force the computation distribution to be equivalent to a dynamic data distribution. The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93, RKU93, GAL95, SSP + 95] </ref>. Our work improves on most previous work in the following ways: 1. We are not influenced by the order of the computation in the original program. <p> If we cannot, perhaps due to the use of an indirection array, extensions beyond what are described in this paper will be necessary. Related work 1 2 3 4 5 6 [Gup92] [AL93] [RKU93] <ref> [Fea94] </ref> [GAL95] [SSP + 95] (a) Our system p p p p p p (a) Starts with a functional language, so not relevant Table 4: Properties (from Section 1) of related work 7 Related Work Table 4 shows which of the desirable properties enumerated in Section 1 hold for a number <p> Our work is most distinguished from all other related work by the fact that we are not influenced by the order of the computation in the original program. Most related works estimate parallelism and/or partition the program into phases based on the original loop structure. Feautrier's approach <ref> [Fea94] </ref> is to find a schedule for executing the program with maximum parallelism, ignoring locality and latency.
Reference: [GAL95] <author> Jordi Garcia, Eduard Ayguade, and Jesus Labarta. </author> <title> A novel approach towards automatic data distribution. In Workshop on Automatic Data Layout and Performance Predition, </title> <month> April </month> <year> 1995. </year>
Reference-contexts: We can generate dynamic data decompositions by adding constraints to force the computation distribution to be equivalent to a dynamic data distribution. The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93, RKU93, GAL95, SSP + 95] </ref>. Our work improves on most previous work in the following ways: 1. We are not influenced by the order of the computation in the original program. <p> If we cannot, perhaps due to the use of an indirection array, extensions beyond what are described in this paper will be necessary. Related work 1 2 3 4 5 6 [Gup92] [AL93] [RKU93] [Fea94] <ref> [GAL95] </ref> [SSP + 95] (a) Our system p p p p p p (a) Starts with a functional language, so not relevant Table 4: Properties (from Section 1) of related work 7 Related Work Table 4 shows which of the desirable properties enumerated in Section 1 hold for a number of <p> By making use of pipelining, we can often obtain parallelism close to that afforded by a doall loop we decide to ignore. Although other systems such as <ref> [RKU93, GAL95] </ref> also use exact rather than greedy heuristic algorithms, the size of the problems and the methods used are very different. We consider a list of candidate distributions for each statement, whereas these systems consider a list of candidate distributions for each array in each phase. <p> Our search spaces will therefore tend to be much larger. These systems use 0-1 integer programming formulations, whereas we have developed our own graph search algorithm. The performance numbers given in <ref> [GAL95] </ref> (which uses a commercial 0-1 integer programming system called LINGO) tend to suggest that our search algorithm is significantly faster. <p> The system is very depended on obtaining a good partitioning of the program into phases and on having a good method to generate and evaluate distributions for each phase. The system described in <ref> [GAL95] </ref> uses a static data decomposition for the entire program. They minimize communication volume and insure that the program can be executed in parallel simply by making one of the loops in the original program a doall loop.
Reference: [Gup92] <author> M. Gupta. </author> <title> Automatic Data Partioning on Distributed Memory Multicomputers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, U. of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: We can generate dynamic data decompositions by adding constraints to force the computation distribution to be equivalent to a dynamic data distribution. The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93, RKU93, GAL95, SSP + 95] </ref>. Our work improves on most previous work in the following ways: 1. We are not influenced by the order of the computation in the original program. <p> If we cannot, perhaps due to the use of an indirection array, extensions beyond what are described in this paper will be necessary. Related work 1 2 3 4 5 6 <ref> [Gup92] </ref> [AL93] [RKU93] [Fea94] [GAL95] [SSP + 95] (a) Our system p p p p p p (a) Starts with a functional language, so not relevant Table 4: Properties (from Section 1) of related work 7 Related Work Table 4 shows which of the desirable properties enumerated in Section 1 hold
Reference: [HKT91] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for FORTRAN D on MIMD distributed memory machines. </title> <booktitle> In Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: One of the things that makes our work difficult is that previous work has not developed techniques that, given a data decomposition, reorders the program so as to exploit parallelism. The only real discussion of this issue is in <ref> [HKT91] </ref>. While the techniques described there work well for simple stencil computations, they are not theoretically sound and make bad or indecisive decisions in a number of cases, including some realistic cases such as Gaussian elimination kernels [KP95].
Reference: [KP95] <author> Wayne Kelly and William Pugh. </author> <title> Identifying reordering transformations that minimize idle processor time. </title> <type> Technical Report CS-TR-3431, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> February </month> <year> 1995. </year> <month> 10 </month>
Reference-contexts: The only real discussion of this issue is in [HKT91]. While the techniques described there work well for simple stencil computations, they are not theoretically sound and make bad or indecisive decisions in a number of cases, including some realistic cases such as Gaussian elimination kernels <ref> [KP95] </ref>. Our work includes a model of how to reorder computations so as to exploit parallelism. Most previous work determines a static data distribution 1 for each array (i.e., a mapping from the elements of that ar-ray to virtual processors).
Reference: [KPRS95] <author> Wayne Kelly, William Pugh, Evan Rosser, and Tatiana Shpeisman. </author> <title> Transitive closure of infinite graphs and its applications. </title> <booktitle> In Eighth Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: On the other hand, it is clearly not sufficient to examine each statement in isolation. What we need to do is consider all direct and transitive self data dependences of each statement. In previous work <ref> [KPRS95] </ref> we have described how to compute transitive self dependences using a very precise abstraction called dependence relations. That approach gives very accurate information about parallelism; however, we have found that it can be expensive for very large programs.
Reference: [Pug94] <author> William Pugh. </author> <title> Counting solutions to presburger formulas: How and why. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: This allows us to associate with each value based flow dependence a dimensionality (or rank). This, in turn, allows us to obtain accurate indications of the relative volumes of different inter-processor communications without having to resort to complex and expensive symbolic volume estimation algorithms <ref> [Pug94] </ref>. For each value-based flow dependence, we consider each combination of candidate space mappings for the statements involved in the dependence.
Reference: [PW93] <author> William Pugh and David Wonnacott. </author> <title> An exact method for analysis of value-based array data dependences. </title> <booktitle> In Lecture Notes in Computer Science 768: Sixth Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year> <note> Springer-Verlag. </note>
Reference-contexts: We obtain accurate indications of the relative volumes of different inter-processor communications by computing the dimensionality of value-based flow dependence relations <ref> [PW93] </ref> (an abstraction that precisely describes which iterations actually read values written by which other iterations). Without this, it would be impossible to analyze the communication costs without knowing where an array was going to be redistributed. 5. <p> Our primary assumption is that communication will only be required between processors if one processor writes a value to a location and some other processor later reads that value from that location. We use value-based flow dependence relations <ref> [PW93] </ref> to obtain accurate indications of the relative volumes of different inter-processor communications. Value based dependence relations precisely describes which iterations actually read values written by which other iterations.
Reference: [RKU93] <author> R.Bixby, K.Kennedy, and U.Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <type> Technical Report CRPC-TR93349-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: We can generate dynamic data decompositions by adding constraints to force the computation distribution to be equivalent to a dynamic data distribution. The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93, RKU93, GAL95, SSP + 95] </ref>. Our work improves on most previous work in the following ways: 1. We are not influenced by the order of the computation in the original program. <p> If we cannot, perhaps due to the use of an indirection array, extensions beyond what are described in this paper will be necessary. Related work 1 2 3 4 5 6 [Gup92] [AL93] <ref> [RKU93] </ref> [Fea94] [GAL95] [SSP + 95] (a) Our system p p p p p p (a) Starts with a functional language, so not relevant Table 4: Properties (from Section 1) of related work 7 Related Work Table 4 shows which of the desirable properties enumerated in Section 1 hold for a <p> By making use of pipelining, we can often obtain parallelism close to that afforded by a doall loop we decide to ignore. Although other systems such as <ref> [RKU93, GAL95] </ref> also use exact rather than greedy heuristic algorithms, the size of the problems and the methods used are very different. We consider a list of candidate distributions for each statement, whereas these systems consider a list of candidate distributions for each array in each phase. <p> These systems use 0-1 integer programming formulations, whereas we have developed our own graph search algorithm. The performance numbers given in [GAL95] (which uses a commercial 0-1 integer programming system called LINGO) tend to suggest that our search algorithm is significantly faster. In Kremer's system <ref> [RKU93] </ref>, an admittedly arbitrary 9 for i = 1 to n 1 a (i,j) = a (i,j)/a (j,j) 2 a (i,j+1) = a (i,j+1)-a (i,k)*a (j+1,k) 3 a (i,i) = sqrt (a (i,i)) 1 : f [i; j] ! [i]g 3 : f [i] ! [i]g for k = 1 to <p> Parallelism is exploited within each phase but not between them. Using techniques not described in <ref> [RKU93] </ref>, a set of candidate distributions are generated for each phase, and the system determines the cost of executing each phase in each distribution and the cost of the remapping variables between each transition.
Reference: [SSP + 95] <author> T. J. She*er, R. Schreiber, W. Pugh, J. R. Gilbert, and S. Chatterjee. </author> <title> Efficient distribution analysis via graph contraction. </title> <booktitle> In Eighth Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: We can generate dynamic data decompositions by adding constraints to force the computation distribution to be equivalent to a dynamic data distribution. The problem of automatically distributing computation has been addressed by a large number of authors <ref> [Gup92, Fea94, AL93, RKU93, GAL95, SSP + 95] </ref>. Our work improves on most previous work in the following ways: 1. We are not influenced by the order of the computation in the original program. <p> If we cannot, perhaps due to the use of an indirection array, extensions beyond what are described in this paper will be necessary. Related work 1 2 3 4 5 6 [Gup92] [AL93] [RKU93] [Fea94] [GAL95] <ref> [SSP + 95] </ref> (a) Our system p p p p p p (a) Starts with a functional language, so not relevant Table 4: Properties (from Section 1) of related work 7 Related Work Table 4 shows which of the desirable properties enumerated in Section 1 hold for a number of related
Reference: [WB87] <author> Michael Wolfe and Utpal Banerjee. </author> <title> Data dependence and its application to parallel processing. </title> <journal> Internation J. Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: So, for large programs, we have developed a more efficient but potentially less accurate approach to computing transitive self dependences. We first perform data dependence analysis to produce a set of extended dependence direction vectors <ref> [WB87, Wol91] </ref> between each pair of statements.
Reference: [Wol91] <author> Michael Wolfe. </author> <title> Experiences with data dependence abstractions. </title> <booktitle> In Proc. of the 1991 International Conference on Supercomputing, </booktitle> <pages> pages 321-329, </pages> <month> June </month> <year> 1991. </year> <month> 11 </month>
Reference-contexts: So, for large programs, we have developed a more efficient but potentially less accurate approach to computing transitive self dependences. We first perform data dependence analysis to produce a set of extended dependence direction vectors <ref> [WB87, Wol91] </ref> between each pair of statements.
References-found: 14

