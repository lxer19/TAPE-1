URL: http://www.cs.berkeley.edu/~russell/papers/colt98-uncertainty.ps
Refering-URL: http://www.cs.berkeley.edu/~russell/publications.html
Root-URL: 
Email: russell@cs.berkeley.edu  
Title: Learning agents for uncertain environments (extended abstract)  
Author: Stuart Russell 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California  
Abstract: This talk proposes a very simple baseline architecture for a learning agent that can handle stochastic, partially observable environments. The architecture uses reinforcement learning together with a method for representing temporal processes as graphical models. I will discuss methods for learning the parameters and structure of such representations from sensory inputs, and for computing posterior probabilities. Some open problems remain before we can try out the complete agent; more arise when we consider scaling up. A second theme of the talk will be whether reinforcement learning can provide a good model of animal and human learning. To answer this question, we must do inverse reinforcement learning: given the observed behaviour, what reward signal, if any, is being optimized? This seems to be a very interesting problem for the COLT, UAI, and ML communities, and has been addressed in econometrics under the heading of structural estimation of Markov decision processes.
Abstract-found: 1
Intro-found: 1
Reference: <author> Astrom, K. J. </author> <year> (1965). </year> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> J. Math. Anal. Applic., </journal> <volume> 10, </volume> <pages> 174-205. </pages>
Reference: <author> Bertsekas, D. C., & Tsitsiklis, J. N. </author> <year> (1996). </year> <title> Neuro-dynamic programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Mass. </address>
Reference: <author> Binder, J., Koller, D., Russell, S., & Kanazawa, K. </author> <year> (1997a). </year> <title> Adaptive probabilistic networks with hidden variables. </title> <journal> Machine Learning, </journal> <volume> 29, </volume> <pages> 213-244. </pages>
Reference-contexts: an approximately correct DBN model from scratch, then this baseline architecture has the capacity, in principle, to be thrown into more or less any environment and to learn to behave reasonably. 1 The talk will cover a variety of research topics arising from this proposal: * Parametric learning in DBNs <ref> (Binder, Koller, Russell, & Kanazawa, 1997a) </ref>. * Structural learning in DBNs (Friedman, Murphy, & Rus sell, 1998). * Approximate inference in DBNs (Kanazawa, Koller, & Russell, 1995; Boyen & Koller, 1998). * Space-efficient inference in DBNs (Binder, Murphy, & Russell, 1997b). * Reinforcement learning with DBN modelsthat is, how to do
Reference: <author> Binder, J., Murphy, K., & Russell, S. </author> <year> (1997b). </year> <title> Space-efficient inference in dynamic probabilistic networks. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-97) Nagoya, </booktitle> <address> Japan. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: a variety of research topics arising from this proposal: * Parametric learning in DBNs (Binder, Koller, Russell, & Kanazawa, 1997a). * Structural learning in DBNs (Friedman, Murphy, & Rus sell, 1998). * Approximate inference in DBNs (Kanazawa, Koller, & Russell, 1995; Boyen & Koller, 1998). * Space-efficient inference in DBNs <ref> (Binder, Murphy, & Russell, 1997b) </ref>. * Reinforcement learning with DBN modelsthat is, how to do Q-learning with the belief state information provided by the DBN. Some tentative ideas will be pre sented but as yet there are no convincing solutions.
Reference: <author> Boyen, X., & Koller, D. </author> <year> (1998). </year> <title> Tractable inference for complex stochastic processes. </title> <booktitle> In Proc. 14th Annual Conference on Uncertainty in AI (UAI). to appear. </booktitle>
Reference: <author> Dean, T., & Kanazawa, K. </author> <year> (1989). </year> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3), </volume> <pages> 142-150. </pages>
Reference-contexts: Neither approach is likely to scale up to situations with large numbers of state variables and long-term temporal dependencies. What is needed is a way of representing the model compactly and updating the belief state efficiently given the model and each new observation. Dynamic Bayesian networks <ref> (Dean & Kanazawa, 1989) </ref> seem to have some of the required properties; in particular, they have significant advantages over other approaches such as Kalman filters and hidden Markov models. Our baseline architecture, shown in as new sensor information arrives.
Reference: <author> Doya, K., & Sejnowski, T. </author> <year> (1995). </year> <title> A novel reinforcement model of birdsong vocalization learning. </title> <editor> In Tesauro, G., Touretzky, D., & Leen, T. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 8, </volume> <pages> pp. </pages> <address> 101-8 Denver, </address> <publisher> CO. MIT Press. </publisher>
Reference-contexts: Connections have been made between reinforcement learning and operant conditioning models of animal learning (see, e.g., Schmajuk & Zanutto, 1997; Touretzky & Saksida, 1997). There is also neurophysiological evidence that reinforcement learning occurs in bee foraging (Montague et al., 1995) and in songbird vocalization <ref> (Doya & Sejnowski, 1995) </ref>. In this work, it is generally assumed that the reward function is fixed and known.
Reference: <author> Farley, C. T., & Taylor, C. R. </author> <year> (1991). </year> <title> A mechanical trigger for the trot-gallop transition in horses. </title> <journal> Science, </journal> <volume> 253(5017), </volume> <pages> 306-308. </pages>
Reference-contexts: For example, it was assumed initially that horses' gait selection for a given speed was determined by energetic economy (Hoyt & Taylor, 1981); this turns out not to be the case <ref> (Farley & Taylor, 1991) </ref>. * The parameters of a multiattribute reward function can surely not be determined a priori; e.g., for running, attributes might be speed, efficiency, stability against perturbations, wear and tear on muscles, tendons, and bones, etc.
Reference: <author> Friedman, N., Murphy, K., & Russell, S. </author> <year> (1998). </year> <title> Learning the structure of dynamic probabilistic networks. </title> <booktitle> In Uncertainty in Artificial Intelligence: Proceedings of the Fourteenth Conference Madison, </booktitle> <address> Wisconsin. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: has the capacity, in principle, to be thrown into more or less any environment and to learn to behave reasonably. 1 The talk will cover a variety of research topics arising from this proposal: * Parametric learning in DBNs (Binder, Koller, Russell, & Kanazawa, 1997a). * Structural learning in DBNs <ref> (Friedman, Murphy, & Rus sell, 1998) </ref>. * Approximate inference in DBNs (Kanazawa, Koller, & Russell, 1995; Boyen & Koller, 1998). * Space-efficient inference in DBNs (Binder, Murphy, & Russell, 1997b). * Reinforcement learning with DBN modelsthat is, how to do Q-learning with the belief state information provided by the DBN.
Reference: <author> Hoyt, D., & Taylor, C. </author> <year> (1981). </year> <title> Gait and the energetics of locomotion in horses. </title> <journal> Nature, </journal> <volume> 292, </volume> <pages> 239-240. </pages>
Reference-contexts: The reasons for this are straightforward: * The specification of a given reward function is an empirical hypothesis and may turn out to be wrong. For example, it was assumed initially that horses' gait selection for a given speed was determined by energetic economy <ref> (Hoyt & Taylor, 1981) </ref>; this turns out not to be the case (Farley & Taylor, 1991). * The parameters of a multiattribute reward function can surely not be determined a priori; e.g., for running, attributes might be speed, efficiency, stability against perturbations, wear and tear on muscles, tendons, and bones, etc.
Reference: <author> Kaelbling, L. P., Littman, M. L., & Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 237-285. </pages>
Reference: <author> Kanazawa, K., Koller, D., & Russell, S. </author> <year> (1995). </year> <title> Stochastic simulation algorithms for dynamic probabilistic networks. </title> <booktitle> In Uncertainty in Artificial Intelligence: Proceedings of the Eleventh Conference, </booktitle> <pages> pp. </pages> <address> 346-351 Montreal, Canada. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Keeney, R. L., & Raiffa, H. </author> <year> (1976). </year> <title> Decisions with Multiple Objectives: Preferences and Value Tradeoffs. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: The closest work is in economics, where the task of multiattribute utility assessment has been studied in depththat is, how does a person actually combine the various attributes of each available choice when making a decision. The theory is well-developed <ref> (Keeney & Raiffa, 1976) </ref>, and the applications numerous. However, this field studies only one-shot decisions where a single action is taken and the outcome is immediate.
Reference: <author> McCallum, A. R. </author> <year> (1993). </year> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 190-196 Amherst, Massachusetts. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Montague, P. R., Dayan, P., Person, C., & Sejnowski, T. J. </author> <year> (1995). </year> <title> Bee foraging in uncertain environments using predictive heb-bian learning. </title> <journal> Nature, </journal> <volume> 377, </volume> <pages> 725-728. </pages>
Reference-contexts: Connections have been made between reinforcement learning and operant conditioning models of animal learning (see, e.g., Schmajuk & Zanutto, 1997; Touretzky & Saksida, 1997). There is also neurophysiological evidence that reinforcement learning occurs in bee foraging <ref> (Montague et al., 1995) </ref> and in songbird vocalization (Doya & Sejnowski, 1995). In this work, it is generally assumed that the reward function is fixed and known.
Reference: <author> Parr, R., & Russell, S. </author> <year> (1995). </year> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95) Montreal, </booktitle> <address> Canada. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Parr, R., & Russell, S. </author> <year> (1998). </year> <title> Reinforcement learning with hierarchies of machines. </title> <editor> In Kearns, M. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 10. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: Some tentative ideas will be pre sented but as yet there are no convincing solutions. Scaling up the environment will inevitably overtax the resources of the baseline architecture. There are several obvious directions for improvement, including hierarchical and first-order models, hierarchical representations of behaviour <ref> (Parr & Russell, 1998) </ref>, and model-based lookahead methods for decision making.
Reference: <author> Russell, S. J., & Norvig, P. </author> <year> (1995). </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference: <author> Rust, J. </author> <year> (1994). </year> <title> Do people behave according to bellman's principal of optimality?. </title> <note> Submitted to Journal of Economic Perspectives. </note>
Reference-contexts: In the last decade, the area of structural estimation of Markov decision processes has grown rapidly in econometrics <ref> (Rust, 1994) </ref>. Many of the basic results carry over to our setting, although virtually nothing has been done on computational aspects, experimentation, or control-type applications.
Reference: <author> Sargent, T. J. </author> <year> (1978). </year> <title> Estimation of dynamic labor demand schedules under rational expectations. </title> <journal> Journal of Political Economy, </journal> <volume> 86(6), </volume> <pages> 1009-1044. </pages>
Reference: <author> Schmajuk, N. A., & Zanutto, B. S. </author> <year> (1997). </year> <title> Escape, avoidance, and imitation: a neural network approach. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 6(1), </volume> <pages> 63-129. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Touretzky, D. S., & Saksida, L. M. </author> <year> (1997). </year> <title> Operant conditioning in Skinnerbots. </title> <booktitle> Adaptive Behavior, </booktitle> <pages> 5(3-4), 219-47. </pages>
References-found: 23

