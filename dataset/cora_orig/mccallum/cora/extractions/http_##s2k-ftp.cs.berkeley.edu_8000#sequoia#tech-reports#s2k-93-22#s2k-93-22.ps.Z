URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-93-22/s2k-93-22.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-93-22/
Root-URL: http://www.cs.berkeley.edu
Email: marti@cs.berkeley.edu  
Title: Cases as Structured Indexes for Full-Length Documents  
Author: Marti A. Hearst and 
Address: 571 Evans Hall  Berkeley, CA 94720  
Affiliation: Computer Science Division,  University of California, Berkeley  Xerox Palo Alto Research Center  
Abstract: Two long, full-length texts are not likely to discuss all, or almost all, of the same subtopics or sub-points. Even if the documents contain many of the same terms, the ways the terms are grouped to form subtopical discussions still might be quite different. A solution is to create a description of a document which lists all of its subtopical discussions as well as its main topics. An index that indicates this structure is an abstract representation of the document, and we can think of this index as a case in the Case-Based Reasoning (CBR) sense. This paper proposes the use of cases to represent the high-level structure of full-length documents for the purpose of information retrieval. The cases are to be used both for assessing document similarity and for helping the user construct viable queries. The case can be transformed in various ways in order to make it more similar to the descriptions of other documents; these transformations include generalizing, substituting, and emphasizing subtopic descriptions. An advantage of this approach is that the cases that represent the document are automatically generable. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ashley, K. D. </author> <year> (1987). </year> <title> Modeling Legal Argument: Reasoning with Cases and Hypotheticals. </title> <type> PhD thesis, </type> <institution> Univeristy of Massachusetts, Amherst. </institution>
Reference-contexts: The system must specify which components of the case can be allowed to vary in order for two cases to be considered to be related to one another. This is a tricky part of the implementation for most CBR systems. (In HYPO <ref> (Ashley 1987) </ref>, similarity is determined according to dimensions, in CYRUS (Kolodner 1983), according to values of features.) Often the similarities between features is determined by relationships in domain knowledge. For the scheme proposed here, there are several ways to organize the similarity determination.
Reference: <author> Bareiss, R. </author> <year> (1989). </year> <title> Exemplar-Based Knowledge Acquisition. </title> <booktitle> Perspectives in Artificial Intelligence. </booktitle> <publisher> Academic Press, Inc. </publisher>
Reference-contexts: Perhaps the most glaring omission is that it does not make extensive use of domain knowledge in order to decide how to make one case better match another. In other words, it is missing the `R' from CBR. <ref> (Bareiss 1989) </ref> lists six characteristics for comparing case-based reasoning systems: (1) How cases are indexed for efficient retrieval. (2) How the similarity between a new problem and a retrieved case is assessed. (3) How cases are selected for retention. (4) How indexing information is learned. (5) How any additional domain knowledge
Reference: <author> Croft, W. B., R. Krovetz, & H. </author> <title> Turtle (1990). Interactive retrieval of complex documents. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 26(5) </volume> <pages> 593-616. </pages>
Reference-contexts: For two sections to be similar, they must be similar both globally, at the paragraph level, and at the sentence level. Salton and Buckley's results show that their procedure is quite effective in many cases. The OFFICER system of <ref> (Croft et al. 1990) </ref> provides a retrieval interface to complex full-length documents, where the documents are represented according to their orthographical markings: title, author, sections, paragraphs, figures, and so on. Users are able to create queries that are sensitive to these structuring elements.
Reference: <author> Croft, W. B. & H. R. </author> <title> Turtle (1992). Text retrieval and inference. </title> <editor> In P. S. Jacobs, editor, </editor> <booktitle> Text-Based Intelligent Systems: Current Research and Practice in Information Extraction and Retrieval, </booktitle> <pages> pages 127-156. </pages> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Many sophisticated probabilistic models have been devised for how to combine term counts and work has been done on how to incorporate information external to term counts as well. See, e.g., <ref> (Croft & Turtle 1992) </ref>. However, the basic assumption of most vector-space and probabilistic methods is that a calculation of some sort is done based on overall frequency counts (Salton 1988).
Reference: <author> Cutting, D. R., J. O. Pedersen, D. Karger, & J. W. </author> <title> Tukey (1992). Scatter/gather: A cluster-based approach to browsing large document collections. </title> <booktitle> In Proceedings of SIGIR, </booktitle> <pages> pages 318-329, </pages> <address> Copenhagen, Denmark. </address>
Reference-contexts: By determining the subtopical structure we are in effect imposing some high-level structure on the document. An index that indicates this structure is an abstract representation of the document. We can think of this index as a case in the CBR sense. <ref> (Cutting et al. 1992) </ref> mention the benefits of browsing a document via the table of contents (TOC) at the front of it, as opposed to the index at the back (which is the standard procedure in IR if we follow this analogy).
Reference: <author> Hearst, M. A. </author> <year> (1993). </year> <title> TextTiling: A quantitative approach to discourse segmentation. </title> <note> submitted. </note>
Reference-contexts: Furthermore, recall that Salton and Buckley combine global and local comparisons in order to make a similarity judgement. Instead, we should keep this kind of information distinct. <ref> (Hearst & Plaunt 1993) </ref> describe a retrieval paradigm in which a user can specify not only the subtopic to retrieve on, but also which main topic the subtopic should appear in the context of. <p> In order to realistically integrate IR with CBR, however, case construction must be automated. In this proposal, cases can be built up automatically, their structure being based on the data rather than on a predefined framework. <ref> (Hearst 1993) </ref> describes TextTiling, a method for partitioning full-length expository texts into multiparagraph discourse units. Each `tile', or segment, is intended to represent a dense discussion of a subtopic. Thus if a term is only mentioned in passing it will not be identified as a true subtopic of the document.
Reference: <author> Hearst, M. A. & C. </author> <month> Plaunt </month> <year> (1993). </year> <title> Subtopic structuring for full-length document access. </title> <note> submitted. </note>
Reference-contexts: Furthermore, recall that Salton and Buckley combine global and local comparisons in order to make a similarity judgement. Instead, we should keep this kind of information distinct. <ref> (Hearst & Plaunt 1993) </ref> describe a retrieval paradigm in which a user can specify not only the subtopic to retrieve on, but also which main topic the subtopic should appear in the context of. <p> In order to realistically integrate IR with CBR, however, case construction must be automated. In this proposal, cases can be built up automatically, their structure being based on the data rather than on a predefined framework. <ref> (Hearst 1993) </ref> describes TextTiling, a method for partitioning full-length expository texts into multiparagraph discourse units. Each `tile', or segment, is intended to represent a dense discussion of a subtopic. Thus if a term is only mentioned in passing it will not be identified as a true subtopic of the document.
Reference: <author> Kolodner, J. L. </author> <year> (1983). </year> <title> Maintaining organization in a dynamic long-term memory. </title> <journal> Cognitive Science, </journal> <volume> 7(4) </volume> <pages> 243-280. </pages>
Reference-contexts: This is a tricky part of the implementation for most CBR systems. (In HYPO (Ashley 1987), similarity is determined according to dimensions, in CYRUS <ref> (Kolodner 1983) </ref>, according to values of features.) Often the similarities between features is determined by relationships in domain knowledge. For the scheme proposed here, there are several ways to organize the similarity determination.
Reference: <author> Lehnert, W. & B. </author> <title> Sundheim (1991). A performance evaluation of text-analysis technologies. </title> <journal> AI Magazine, </journal> <volume> 12(3) </volume> <pages> 81-94. </pages>
Reference-contexts: methods for converting full-length texts to complex, detailed representations are still not feasible. 2 2 Recently researchers have become more successful at converting short, domain-specific texts into template-like representations. (Liddy 1991) reports work on converting empirical abstracts into knowledge structures, and several of the researchers participating in the MUC competition <ref> (Lehnert & Sundheim 1991) </ref> are showing promising results at classifying the contents of newswire articles.
Reference: <author> Liddy, E. </author> <year> (1991). </year> <title> The discourse level structure of empirical abstracts an exploratory study. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 27(1) </volume> <pages> 55-81. </pages>
Reference-contexts: system that can scale to the demands of an IR system for unrestricted text, this element must be missing, since automated methods for converting full-length texts to complex, detailed representations are still not feasible. 2 2 Recently researchers have become more successful at converting short, domain-specific texts into template-like representations. <ref> (Liddy 1991) </ref> reports work on converting empirical abstracts into knowledge structures, and several of the researchers participating in the MUC competition (Lehnert & Sundheim 1991) are showing promising results at classifying the contents of newswire articles.
Reference: <author> Salton, G. </author> <year> (1988). </year> <title> Automatic text processing : the transformation, analysis, and retrieval of information by computer. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: See, e.g., (Croft & Turtle 1992). However, the basic assumption of most vector-space and probabilistic methods is that a calculation of some sort is done based on overall frequency counts <ref> (Salton 1988) </ref>. Salton and Buckley (Salton & Buckley 1991b),(Salton & Buckley 1991a), who take pains to normalize the lengths of the documents that they compare. They have compared paragraphs within a large document (e.g., Salton's book), articles within an online encyclopedia, and electronic mail messages (inquiries and their replies). <p> After first organizing cases according to their main topic (s), documents can then be grouped according to which and how many of their cases' vectors are similar to one another, using a standard similarity measure, such as cosine <ref> (Salton 1988) </ref>. Documents which have the same number of vectors, all of which are similar, are considered to be closest, documents which have one differing vector are slightly less close, and so on.
Reference: <author> Salton, G. & C. </author> <title> Buckley (1991a). Automatic text structuring and retrieval: Experiments in automatic encyclopedia searching. </title> <booktitle> In Proceedings of SIGIR, </booktitle> <pages> pages 21-31. </pages>
Reference: <author> Salton, G. & C. </author> <title> Buckley (1991b). Global text matching for information retrieval. </title> <journal> Science, </journal> <volume> 253 </volume> <pages> 1012-1015. </pages>
Reference-contexts: See, e.g., (Croft & Turtle 1992). However, the basic assumption of most vector-space and probabilistic methods is that a calculation of some sort is done based on overall frequency counts (Salton 1988). Salton and Buckley <ref> (Salton & Buckley 1991b) </ref>,(Salton & Buckley 1991a), who take pains to normalize the lengths of the documents that they compare. They have compared paragraphs within a large document (e.g., Salton's book), articles within an online encyclopedia, and electronic mail messages (inquiries and their replies).
Reference: <author> Smith, P. J., S. J. Shute, D. Galdes, & M. H. </author> <month> Chignell </month> <year> (1989). </year> <title> Knowledge-based search tactics for an intelligent intermediary system. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 7(3) </volume> <pages> 246-270. </pages>
Reference-contexts: It is straightforward to see how substitution of a new section or removal of an existing one can be accommodated by such a setup. <ref> (Smith et al. 1989) </ref> report a study which monitored the suggestions made by professional searchers who were helping information seekers find relevant abstracts from the Chemical Abstracts. <p> That these tactics are similar to several of the operations suggested above, and that they were observed in the behavior of professional searchers, confirms their feasibility. The information retrieval system developed by <ref> (Smith et al. 1989) </ref> is one in which the abstracts are represented by hand-built semantic frames. Fittingly, they suggest that the adjustment tactics can be considered equivalent to adding or deleting slots or slot fillers from semantic frames.
Reference: <author> Yarowsky, D. </author> <year> (1992). </year> <title> Word sense disambiguation using statistical models of roget's categories trained on large corpora. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Computational Linguistics, </booktitle> <pages> pages 454-460, </pages> <address> Nantes, France. </address>
Reference-contexts: One way to do this is to pick relevant terms from those used to identify the tile. Another is to classify the segments' relevant terms according to an ontology or thesaurus. Experiments with employing a disambiguation algorithm proposed by <ref> (Yarowsky 1992) </ref> to this problem, have promising initial results. This procedure in effect incorporates domain knowledge since the classifications are based on statistics gathered by use of a lexical thesaurus. If a domain specific thesaurus is used, a domain specific classification can be made.
References-found: 15

