URL: http://www.cs.brandeis.edu/~brendy/duality2.ps.gz
Refering-URL: http://www.cs.brandeis.edu/~brendy/recoding.html
Root-URL: http://www.cs.brandeis.edu
Email: Email: brendy@cs.brandeis.edu  
Title: Representation Operators and Computation  
Author: Brendan Kitts 
Keyword: representation, redescription, recoding  
Address: Waltham, MA. 02254. USA.  
Affiliation: Department of Computer Science, Center for Complex System, Brandeis University,  
Date: June 6, 1998 Page: 1  
Abstract: This paper derives results about the impact of representation and search operators on the Computational Complexity of a problem. A model of computation is introduced based on a directed graph structure, and representation and search are defined to be respectively the vertices and edges of this graph. Changing either the representation or the search algorithm leads to different possible graphs, and hence have different complexity classes. The final section explores the role of representation in reducing time complexity in Artificial Intelligence. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Boden, M. </author> <year> (1994), </year> <title> Prcis of the Creative Mind, </title> <journal> Behavioural and Brain Sciences, </journal> <volume> Vol. 17, </volume> <pages> pp. 519-570. </pages>
Reference: <author> Chudler, E. </author> <year> (1997), </year> <title> Brain Facts and Figures, </title> <note> http://weber.u.washington.edu/ ~chudler/facts.html#neuron Churchland, </note> <author> P. and Sejnowski, T. </author> <year> (1992), </year> <title> The Computational Brain, </title> <publisher> MIT Press, </publisher> <address> MA. </address>
Reference-contexts: Human cognitive responses generally span a range of 0 max -max max 2 +1 0 June 6, 1998 Page: 18 50 to 200 miliseconds (Churchland and Sejnowski, 1992). Axon conduction velocity is between 0.6m/s and 120m/s <ref> (Chudler, 1997) </ref>. Lets assume signals travel 2 meters to their targets, and then 2 meters back to their effectors. That gives 33ms in travel time. Assume that there are 5 interneurons on afferent and efferent pathway respectively. Membrane integration time is about 7ms (Sah and Bekkers, 1996).
Reference: <institution> June 6, </institution> <note> 1998 Page: 27 Clark, </note> <author> A. and Thornton, C. (forthcomming), </author> <title> Trading Spaces: Computation, Representation and the Limits of Uninformed Learning, </title> <journal> Brain and Behavioural Sciences. </journal> <note> Also available as Technical Report, </note> <institution> Washington University, St. Louis, MO. </institution>
Reference: <author> Crutchfield, J. </author> <year> (1994), </year> <title> The Calculi of Emergence: Computation, Dynamics, and Induction, </title> <journal> PhysicaD, </journal> <volume> Vol. 75, </volume> <pages> pp. 11-54. </pages>
Reference: <author> Donoho, S. and Rendell, L. </author> <year> (1995), </year> <title> Rerepresenting and Restructuring Domain Theories, </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> Vol. 2, </volume> <pages> pp. 411-446. </pages>
Reference-contexts: Koza has been highly successful with the use of Automatically Defined Functions or program modules, which embody a large sub-program and can be used in further genetic search. Other innovative work in representation has come from the Decision Tree Induction field. Systems such as TGCI <ref> (Donoho and Rendell, 1995) </ref> and AQ17-HCI (Wnek and Michalski, 1994) look for complex patterns in a decision tree which appear to be repeated, and then recode this as a new attribute or feature.
Reference: <author> Goldberg, D. and Bridges, C. </author> <year> (1990), </year> <title> An Analysis of a Reordering Operator on a GA-Hard Problem, </title> <journal> Biological Cybernetics, </journal> <volume> Vol. 62, </volume> <pages> pp. 397-405. </pages>
Reference-contexts: However, it has not been until recently that machine learning practicioners have begun to use representation operators in a practical sense. In Genetic Algorithms, Hollands inversion operator <ref> (Goldberg and Bridges, 1990) </ref> takes bits on a bit-string and simply re-orders them in the hope of reducing the tendency of a GA to become trapped by artifacts of the bit encoding.
Reference: <author> Goldfarb, L., et. al. </author> <year> (1994), </year> <title> Can a Vector space-based learning model Discover Inductive Class Generalisations in a Symbolic Environment? in Proceedings of the 10th Biennial Conference of the Computer Society for Computational Study of Intelligence, </title> <publisher> Morgan Kaufmann, </publisher> <address> CA. </address>
Reference: <author> Hubel, D., Wiesel, T., and Stryker, P. </author> <year> (1978), </year> <title> Anatomical Demonstration of Orientation Columns in Macaque Monkey, </title> <journal> Journal of Computational Neurology, </journal> <volume> Vol. 177, </volume> <pages> pp. 361-380. </pages>
Reference-contexts: There is neurological evidence which supports the idea of multiple representations, or maps, in the brain. In addition to detectors for primitive features such as edges, the brain also has detectors for amalgams of these features, complex objects such as hands and faces <ref> (Hubel, Weisel and Stryker, 1978) </ref>. Visual, auditory, and other topographic maps are also found at multiple places all over the cortical surface (Kandel, Schwartz and Jessel, 1992).
Reference: <author> Jones, T. </author> <year> (1995), </year> <title> Evolutionary Algorithms, Fitness Landscapes and Search, </title> <type> Doctoral Dissertation, </type> <institution> University of New Mexico. </institution>
Reference-contexts: Running the same computation over S now solves the June 6, 1998 Page: 17 problem immediately. Theorem 9: <ref> (Jones, 1995) </ref> If a computation is recursive, a search algorithm exists which minimizes the time complexity of the computation. Proof Simply reconstruct graph G, with a transition from each initial state to the accepting state, and from each non-accepting state to a non-accepting node. <p> CMACs and Kohonen nets store and manipu late prototypes which represent a much larger set of objects. Many to many: DNA is both distributed and redundant. A single gene can encode several phenotypic traits (pleitropy) and a single phenotypic trait is affected by more than one gene (polytropy). <ref> (Jones, 1995) </ref>. 3 Time is at a preimum only for execution (and is presumably why myelinated neurons were evolved in the first place - the need for rapid signal transfer).
Reference: <institution> June 6, </institution> <note> 1998 Page: 28 Kandel, </note> <author> E., Schwartz, J., and Jessell, T. </author> <year> (1992), </year> <booktitle> Principles of Neural Science, Appleton and Lange. </booktitle> <pages> CI. </pages>
Reference: <author> Karmiloff-Smith, A. </author> <year> (1992), </year> <title> Beyond Modularity, </title> <publisher> MIT Press. </publisher>
Reference: <author> Kingdon, J. and Dekker, L. </author> <year> (1996), </year> <title> The Shape of Space, </title> <type> Technical Report, </type> <institution> Department of Computer Science, University College London. </institution>
Reference: <author> Kosslyn, S. </author> <year> (1994), </year> <title> Image and Brain: The Resolution of the Imagery Debate, </title> <publisher> MIT Press. </publisher>
Reference-contexts: More bluntly, why would the brain need to have pictures inside the head? For a homunculus? The debate was fueled by a predominance of experimental evidence which actually supported the existence of all these internal pictures! <ref> (Kosslyn, 1994) </ref>. This paper suggests that these phenomena might exist because there is a time and complexity advantage in maintaining multiple, redundant, partially processed recodings.
Reference: <author> Lenat, D. </author> <year> (1995), </year> <title> CYC: </title> <journal> A Large-Scale Investment in Knowledge Infrastructure Communications of the ACM, </journal> <volume> Vol. 38, No. 11, </volume> <pages> pp. 32-38 McCarthy, </pages> <editor> J. </editor> <year> (1968), </year> <title> Programs with Common Sense, </title> <editor> in Minsky, M. (ed), </editor> <booktitle> Semantic Information Processing, </booktitle> <publisher> MIT Press, </publisher> <address> MA. </address>
Reference: <author> Mendelson, E. </author> <year> (1964), </year> <title> Introduction to Mathematical Logic, </title> <publisher> Van Nostrand Company, </publisher> <address> New York. </address>
Reference-contexts: system is a system which has the following components: (i) reachable state space or wffs, which is a set of states which the system could be in; (ii) axioms or initial states; (iii) state transitions, update equations, or rules of inference, which is a mapping between one state and another <ref> (Mendelson, 1964) </ref>. Let the vertices of G, sS represent the wffs, let arcs tT represent inference rules from one wff to another, let -s initial -S represent initial state (s). Formal Systems respect the determinism principle. Therefore a Chain can be defined for any Formal System.
Reference: <author> Pinker, S. and Mechler, J. (eds), </author> <year> (1988), </year> <title> Connections and Symbols, </title> <publisher> MIT Press, </publisher> <address> MA. </address>
Reference: <author> Radcliffe, N. </author> <year> (1994), </year> <title> The Algebra of Genetic Algorithms, </title> <journal> Annals of Maths and Artificial Intelligence, </journal> <volume> Vol. 10, </volume> <pages> pp. 339-384. </pages>
Reference: <institution> June 6, </institution> <note> 1998 Page: 29 Rumelhart, </note> <editor> D., McClelland, J., et. al. </editor> <booktitle> (1986), Parallel Distributed Processing, </booktitle> <volume> Vol. 2, </volume> <publisher> Mit Press, </publisher> <address> MA. </address>
Reference: <author> Sah, P. and Bekkers, J. </author> <year> (1996), </year> <title> Apical Dendritic Location of Slow Afterhyperpo-larization Current in Hippocampal Pyramidal Neurons: Implications for the Integration of Long-Term Potentiation, </title> <journal> Journal of Neuroscience, </journal> <volume> Vol. 16, No. 15, </volume> <pages> pp. 4537-4542. </pages>
Reference-contexts: Lets assume signals travel 2 meters to their targets, and then 2 meters back to their effectors. That gives 33ms in travel time. Assume that there are 5 interneurons on afferent and efferent pathway respectively. Membrane integration time is about 7ms <ref> (Sah and Bekkers, 1996) </ref>. Therefore membrane integration consumes 70ms. This leaves on the order of 100ms for computation in the central nervous system.
Reference: <author> Toth, G., Kovacs, S. and Lorincz, A. </author> <year> (1995), </year> <title> Genetic Algorithm with Alphabet Optimisation, </title> <journal> Biological Cybernetics, </journal> <volume> Vol. 73, </volume> <pages> pp. 61-68. </pages>
Reference-contexts: In Genetic Algorithms, Hollands inversion operator (Goldberg and Bridges, 1990) takes bits on a bit-string and simply re-orders them in the hope of reducing the tendency of a GA to become trapped by artifacts of the bit encoding. Kingdon and Dekker (1995) <ref> (also Toth, et. al., 1995) </ref> suggest a representation operator for Genetic Algorithms which automatically switched number base from binary to base 3, 4, or higher, when a problem appeared to be improperly converged.
Reference: <author> Tye, M. </author> <year> (1991), </year> <title> The Imagery Debate, </title> <publisher> MIT Press. </publisher>
Reference-contexts: Visual, auditory, and other topographic maps are also found at multiple places all over the cortical surface (Kandel, Schwartz and Jessel, 1992). The central issue in The Imagery Debate <ref> (Tye, 1991) </ref> was why is there any need to re-detect stimuli at new areas in the brain after they have been registered at the periphery.
Reference: <author> Wnek, J. and Michalski, R. </author> <year> (1994), </year> <title> Hypothesis-driven Constructive Induction in AQ17-HCI: A Method and Experiments, </title> <journal> Machine Learning, </journal> <volume> Vol. 14, </volume> <pages> pp. 139-168, </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Eventually, these abstract representations might have provided a means for yet more sophisticated computations, because the underlying representations generated a more powerful language (representation) for building more complex computations. An example of this phenomenon is provided by the decision tree system AQ17-HCI <ref> (Wnek and Michalski, 1994) </ref>, which recodes subtrees into new primitives and re-induces tree with the more abstract language. <p> Other innovative work in representation has come from the Decision Tree Induction field. Systems such as TGCI (Donoho and Rendell, 1995) and AQ17-HCI <ref> (Wnek and Michalski, 1994) </ref> look for complex patterns in a decision tree which appear to be repeated, and then recode this as a new attribute or feature. Theoretical aspects of representation have been touched on by Kolen and Pollack June 6, 1998 Page: 23 (1995) and Crutchfield (1993).

Reference: <author> Dennis, J. and Schnabel, R. </author> <year> (1983), </year> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <publisher> Prentice-Hall, </publisher> <address> NJ. </address>
Reference: <author> Elman, J.L. </author> <year> (1993), </year> <title> Learning and development in neural networks: The importance of starting small, </title> <journal> Cognition, </journal> <volume> Vol. 48, </volume> <pages> pp. 71-99. </pages>
Reference: <author> Haugeland, J. </author> <year> (1989), </year> <booktitle> Artificial Intelligence: The very idea, </booktitle> <publisher> MIT Press, </publisher> <address> MA. </address>
Reference: <author> Hertz, J., Krogh, A., and Palmer, R. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley, </publisher> <address> Sydney. </address>
Reference: <author> Holland, J. </author> <year> (1992), </year> <title> Adaptation in Natural and Artificial Systems, </title> <publisher> MIT Press, </publisher> <address> MA. </address>
Reference: <author> Maly, K., and Hanson, K. </author> <year> (1979), </year> <booktitle> Fundamentals of the Computing Sciences, </booktitle> <address> Pren-tice-Hall, NJ. </address>
Reference: <author> Quinlan, J. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kauffman, </publisher> <address> CA. </address>
Reference: <author> Radcliffe, N. and Surry, P. </author> <title> (in press), Fundamental Limitations on Search Algorithms, </title> <note> to appear in Lecture Notes in Computer Science, Vol. 1000, Springer-Ver-lag. </note>
Reference: <institution> June 6, </institution> <note> 1998 Page: 36 Rendell, </note> <author> L. and Ragavan, H. </author> <year> (1993), </year> <title> Improving the design of induction methods by analyzing algorithm functionality and data-based concept complexity, </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 952-958, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Revesz, G. </author> <year> (1983), </year> <title> Introduction to Formal Languages, </title> <publisher> Dover Publications.Turing, A. </publisher> <year> (1969), </year> <note> Intelligent Machinery, reprinted in Ince, </note> <editor> D. (ed), </editor> <booktitle> The Collected Works of A.M. Turing, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 107-131. </pages> <publisher> North Holland. </publisher>
Reference: <author> Wellman, H. </author> <year> (1992), </year> <title> The Childs Theory of Mind. </title> <publisher> MIT Press, </publisher> <address> MA. </address>
Reference: <author> Wolpert, D. and Macready, W. </author> <year> (1995), </year> <title> No Free Lunch Theorems for Search, </title> <type> Technical Report SFI-TR-95-02-010, </type> <institution> Santa Fe Institute. </institution>
References-found: 34

