URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/reinforcement/papers/boyan.funapprox-dp.ps.Z
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/reinforcement/mosaic/cmu-rl-papers.html
Root-URL: http://www.cs.cmu.edu
Email: jab@cs.cmu.edu, awm@cs.cmu.edu  
Title: Generalization in Reinforcement Learning: Safely Approximating the Value Function  
Author: Justin A. Boyan and Andrew W. Moore 
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract: To appear in: G. Tesauro, D. S. Touretzky and T. K. Leen, eds., Advances in Neural Information Processing Systems 7, MIT Press, Cambridge MA, 1995. A straightforward approach to the curse of dimensionality in reinforcement learning and dynamic programming is to replace the lookup table with a generalizing function approximator such as a neural net. Although this has been successful in the domain of backgammon, there is no guarantee of convergence. In this paper, we show that the combination of dynamic programming and function approximation is not robust, and in even very benign cases, may produce an entirely wrong policy. We then introduce Grow-Support, a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization.
Abstract-found: 1
Intro-found: 1
Reference: [ Barto et al., 1989 ] <author> A. Barto, R. Sutton, and C. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical Report COINS 89-95, </type> <institution> Univ. of Massachusetts, </institution> <year> 1989. </year>
Reference-contexts: We discuss potential computational advantages of this method and demonstrate its success on some example problems for which the conventional DP algorithm fails. 2 DISCRETE AND SMOOTH VALUE ITERATION Many popular reinforcement learning algorithms, including Q-learning and TD (0), are based on the dynamic programming algorithm known as value iteration <ref> [ Watkins, 1989, Sutton, 1988, Barto et al., 1989 ] </ref> , which for clarity we will call discrete value iteration.
Reference: [ Bellman et al., 1963 ] <author> R. Bellman, R. Kalaba, and B. Kotkin. </author> <title> Polynomial approximation|a new computational technique in dynamic programming: Allocation processes. </title> <journal> Mathematics of Computation, </journal> <volume> 17, </volume> <year> 1963. </year>
Reference-contexts: A natural way to incorporate generalization into DP is to use a function approximator, rather than a lookup table, to represent the value function. This approach, which dates back to uses of Legendre polynomials in DP <ref> [ Bellman et al., 1963 ] </ref> , has recently worked well on several dynamic control problems [ Mahadevan and Connell, 1990, Lin, 1993 ] and succeeded spectacularly on the game of backgammon [ Tesauro, 1992, Boyan, 1992 ] .
Reference: [ Boyan, 1992 ] <author> J. A. Boyan. </author> <title> Modular neural networks for learning context-dependent game strategies. </title> <type> Master's thesis, </type> <institution> Cambridge University, </institution> <year> 1992. </year>
Reference-contexts: This approach, which dates back to uses of Legendre polynomials in DP [ Bellman et al., 1963 ] , has recently worked well on several dynamic control problems [ Mahadevan and Connell, 1990, Lin, 1993 ] and succeeded spectacularly on the game of backgammon <ref> [ Tesauro, 1992, Boyan, 1992 ] </ref> . On the other hand, many sensible implementations have been less successful [ Bradtke, 1993, Schraudolph et al., 1994 ] .
Reference: [ Bradtke, 1993 ] <author> S. J. Bradtke. </author> <title> Reinforcement learning applied to linear quadratic regulation. </title> <editor> In S. J. Hanson, J. Cowan, and C. L. Giles, editors, NIPS-5. </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: On the other hand, many sensible implementations have been less successful <ref> [ Bradtke, 1993, Schraudolph et al., 1994 ] </ref> . Indeed, given the well-established success on backgammon, the absence of similarly impressive results appearing for other games is perhaps an indication that using function approximation in reinforcement learning does not always work well.
Reference: [ Cleveland and Delvin, 1988 ] <author> W. S. Cleveland and S. J. Delvin. </author> <title> Locally weighted regression: An approach to regression analysis by local fitting. </title> <journal> JASA, </journal> <volume> 83(403) </volume> <pages> 596-610, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: types of instabilities appear with even a highly Table 1: Summary of convergence results: Smooth value iteration Domain Linear Quadratic LWR Backprop 2-D gridworld lucky diverge good lucky 2-D puddle world | | diverge diverge Car-on-the-hill | | good diverge local memory-based function approximator such as local weighted regression (LWR) <ref> [ Cleveland and Delvin, 1988 ] </ref> . Figure 4 shows the continuous gridworld augmented to include two oval "puddles" through which it is costly to step. Although LWR can fit the corresponding J fl function nearly perfectly, smooth value iteration with LWR nonetheless reliably diverges.
Reference: [ Lin, 1993 ] <author> L.-J. Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: This approach, which dates back to uses of Legendre polynomials in DP [ Bellman et al., 1963 ] , has recently worked well on several dynamic control problems <ref> [ Mahadevan and Connell, 1990, Lin, 1993 ] </ref> and succeeded spectacularly on the game of backgammon [ Tesauro, 1992, Boyan, 1992 ] . On the other hand, many sensible implementations have been less successful [ Bradtke, 1993, Schraudolph et al., 1994 ] .
Reference: [ Mahadevan and Connell, 1990 ] <author> S. Mahadevan and J. Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <type> Technical report, </type> <institution> IBM T. J. Watson Research Center, </institution> <address> NY 10598, </address> <year> 1990. </year>
Reference-contexts: This approach, which dates back to uses of Legendre polynomials in DP [ Bellman et al., 1963 ] , has recently worked well on several dynamic control problems <ref> [ Mahadevan and Connell, 1990, Lin, 1993 ] </ref> and succeeded spectacularly on the game of backgammon [ Tesauro, 1992, Boyan, 1992 ] . On the other hand, many sensible implementations have been less successful [ Bradtke, 1993, Schraudolph et al., 1994 ] .
Reference: [ Sabes, 1993 ] <author> P. Sabes. </author> <title> Approximating Q-values with basis function representations. </title> <booktitle> In Proceedings of the Fourth Connectionist Models Summer School, </booktitle> <year> 1993. </year>
Reference-contexts: In [ Singh and Yee, 1994 ] and [ Williams, 1993 ] bounds are derived for the maximum reduction in optimality that can be produced by a given error in function approximation. If a basis function approximator is used, then the reduction can be large <ref> [ Sabes, 1993 ] </ref> .
Reference: [ Schraudolph et al., 1994 ] <author> N. Schraudolph, P. Dayan, and T. Sejnowski. </author> <title> Using TD() to learn an evaluation function for the game of Go. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, NIPS-6. </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: On the other hand, many sensible implementations have been less successful <ref> [ Bradtke, 1993, Schraudolph et al., 1994 ] </ref> . Indeed, given the well-established success on backgammon, the absence of similarly impressive results appearing for other games is perhaps an indication that using function approximation in reinforcement learning does not always work well.
Reference: [ Singh and Yee, 1994 ] <author> S. P. Singh and R. Yee. </author> <title> An upper bound on the loss from approximate optimal-value functions. </title> <booktitle> Machine Learning, </booktitle> <year> 1994. </year> <note> Technical Note (to appear). </note>
Reference-contexts: In [ Thrun and Schwartz, 1993 ] one case is analyzed with the assumption that errors due to function approximation bias are independently distributed. Another area of theoretical analysis concerns inadequately approximated J fl functions. In <ref> [ Singh and Yee, 1994 ] </ref> and [ Williams, 1993 ] bounds are derived for the maximum reduction in optimality that can be produced by a given error in function approximation. If a basis function approximator is used, then the reduction can be large [ Sabes, 1993 ] .
Reference: [ Sutton, 1988 ] <author> R. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: We discuss potential computational advantages of this method and demonstrate its success on some example problems for which the conventional DP algorithm fails. 2 DISCRETE AND SMOOTH VALUE ITERATION Many popular reinforcement learning algorithms, including Q-learning and TD (0), are based on the dynamic programming algorithm known as value iteration <ref> [ Watkins, 1989, Sutton, 1988, Barto et al., 1989 ] </ref> , which for clarity we will call discrete value iteration.
Reference: [ Tesauro, 1992 ] <author> G. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8(3/4), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: This approach, which dates back to uses of Legendre polynomials in DP [ Bellman et al., 1963 ] , has recently worked well on several dynamic control problems [ Mahadevan and Connell, 1990, Lin, 1993 ] and succeeded spectacularly on the game of backgammon <ref> [ Tesauro, 1992, Boyan, 1992 ] </ref> . On the other hand, many sensible implementations have been less successful [ Bradtke, 1993, Schraudolph et al., 1994 ] .
Reference: [ Thrun and Schwartz, 1993 ] <author> S. Thrun and A. Schwartz. </author> <title> Issues in using function approximation for reinforcement learning. </title> <booktitle> In Proceedings of the Fourth Connectionist Models Summer School, </booktitle> <year> 1993. </year>
Reference-contexts: In <ref> [ Thrun and Schwartz, 1993 ] </ref> one case is analyzed with the assumption that errors due to function approximation bias are independently distributed. Another area of theoretical analysis concerns inadequately approximated J fl functions.
Reference: [ Watkins, 1989 ] <author> C. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: We discuss potential computational advantages of this method and demonstrate its success on some example problems for which the conventional DP algorithm fails. 2 DISCRETE AND SMOOTH VALUE ITERATION Many popular reinforcement learning algorithms, including Q-learning and TD (0), are based on the dynamic programming algorithm known as value iteration <ref> [ Watkins, 1989, Sutton, 1988, Barto et al., 1989 ] </ref> , which for clarity we will call discrete value iteration.
Reference: [ Williams, 1993 ] <author> R. Williams. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-13, </type> <institution> Northeastern University, </institution> <year> 1993. </year>
Reference-contexts: In [ Thrun and Schwartz, 1993 ] one case is analyzed with the assumption that errors due to function approximation bias are independently distributed. Another area of theoretical analysis concerns inadequately approximated J fl functions. In [ Singh and Yee, 1994 ] and <ref> [ Williams, 1993 ] </ref> bounds are derived for the maximum reduction in optimality that can be produced by a given error in function approximation. If a basis function approximator is used, then the reduction can be large [ Sabes, 1993 ] .
Reference: [ Yee, 1992 ] <author> R. Yee. </author> <title> Abstraction in control learning. </title> <type> Technical Report COINS 92-16, </type> <institution> Univ. of Mas-sachusetts, </institution> <year> 1992. </year>
Reference-contexts: Table 1 summarizes our results. In light of such experiments, we conclude that the straightforward combination of DP and function approximation is not robust. A general-purpose learning method will require either using a function approximator constrained to be robust during DP <ref> [ Yee, 1992 ] </ref> , or an algorithm which explicitly prevents divergence even in the face of imperfect function approximation, such as the Grow-Support algorithm we present in Section 3. 2.2 RELATED WORK Theoretically, it is not surprising that inserting a smoothing process into a recursive DP procedure can lead to
References-found: 16

