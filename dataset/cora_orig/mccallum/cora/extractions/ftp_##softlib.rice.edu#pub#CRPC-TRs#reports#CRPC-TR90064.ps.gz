URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR90064.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Loop Distribution with Arbitrary Control Flow  
Author: Ken Kennedy Kathryn S. McKinley 
Address: P.O. Box 1892 Houston, TX 77251  
Affiliation: Rice University Department of Computer Science  
Abstract: Loop distribution is an integral part of transforming a sequential program into a parallel one. It is used extensively in parallelization, vectorization, and memory management. For loops with control flow, previous methods for loop distribution have significant drawbacks. We present a new algorithm for loop distribution in the presence of control flow modeled by a control dependence graph. This algorithm is shown optimal in that it generates the minimum number of new arrays and tests possible. We also present a code generation algorithm that produces code for the resulting program without replicating statements or conditions. Although these algorithms are being developed for use in an interactive parallel programming environment for Fortran, they are very general and can be used in automatic parallelization and vectorization systems. Keywords: parallelization, vectorization, transformation, control dependence, data dependence, loop distribution 
Abstract-found: 1
Intro-found: 1
Reference: [ABC + 87] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: However, it has several drawbacks. If vectorization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals. For these reasons, research in automatic paral-lelization has concentrated on an alternative approach that uses control dependences <ref> [FOW87, ABC + 87, ABC + 88] </ref> to model control flow. Reconstructing sequential code from a control dependence graph is not trivial, but it is easier than reconstructing from code that has been subject to if-conversion [FM85, FMS88, CFS90]. Unfortunately, the control dependence representation complicates loop distribution. <p> They discuss three transformations that restructure control flow: loop fusion, dead code elimination, and branch deletion. Other research concerned with the definition and use of the program dependence graph does not address distribution [FOW87, FM85, FMS88]. The papers describing the ptran project <ref> [CFS90, ABC + 87] </ref>, which also performs code generation based on G cd , do not address distribution. Work in memory management and name space adjustment [KKP + 81, Por89] uses distribution, but only when no control dependences are present.
Reference: [ABC + 88] <author> F. Allen, M. Burke, P. Charles, J. Ferrante, W. Hsieh, and V. Sarkar. </author> <title> A framework for detecting useful parallelism. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: However, it has several drawbacks. If vectorization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals. For these reasons, research in automatic paral-lelization has concentrated on an alternative approach that uses control dependences <ref> [FOW87, ABC + 87, ABC + 88] </ref> to model control flow. Reconstructing sequential code from a control dependence graph is not trivial, but it is easier than reconstructing from code that has been subject to if-conversion [FM85, FMS88, CFS90]. Unfortunately, the control dependence representation complicates loop distribution.
Reference: [AC72] <author> F. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In J. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: Hence, the creation of an execution variable will replace control dependences between partitions with data dependences. Execution variables are arrays, with one value for each iteration of the loop, because each iteration can give rise to a different control decision. If desired, loop invariant decisions can be detected <ref> [AC72] </ref> and represented with scalar execution variables. All other known techniques, whether they are G cd based or not, use boolean logic when introducing arrays to record branch decisions. This requires either testing and recording the path taken in previous loops or introducing additional arrays.
Reference: [AK84] <author> J. R. Allen and K. Kennedy. </author> <title> PFC: A program to convert Fortran to parallel form. </title> <editor> In K. Hwang, editor, </editor> <booktitle> Supercomputers: Design and Applications, </booktitle> <pages> pages 186-203. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Silver Spring, MD, </address> <year> 1984. </year>
Reference-contexts: We use the following definitions of postdominance and control dependence, which are taken from the literature [FOW87, CFS90]. 1 A data dependence exists between two statements if they reference the same memory location and at least one of them is a write <ref> [Ber66, Ban88, AK84, Wol82] </ref>. Def: x is postdominated by y in the control flow graph G f if every path from x to the exit node of G f contains y.
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: This approach, called if-conversion [AKPW83, All83], has been used successfully in a vari-ety of vectorization systems <ref> [AK87, SK86, KKLW84] </ref>. However, it has several drawbacks. If vectorization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals. <p> The method we present is designed to work on any partition that is legal, i.e., any partition that preserves the control and data dependences of the original program. A partition can preserve all dependences if and only if there exists no dependence cycle spanning more than one output loop <ref> [KKP + 81, AK87] </ref>. If there is a cycle involving control and/or data dependences, it must be contained entirely within a single partition (there are a few additional considerations for loops with exit branches, which are discussed in Section 3.4). This condition is necessary and sufficient.
Reference: [AKPW83] <author> J. R. Allen, K. Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Conference Record of the Tenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1983. </year>
Reference-contexts: This approach, called if-conversion <ref> [AKPW83, All83] </ref>, has been used successfully in a vari-ety of vectorization systems [AK87, SK86, KKLW84]. However, it has several drawbacks. If vectorization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals.
Reference: [All83] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: This approach, called if-conversion <ref> [AKPW83, All83] </ref>, has been used successfully in a vari-ety of vectorization systems [AK87, SK86, KKLW84]. However, it has several drawbacks. If vectorization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals.
Reference: [All90] <author> J. R. Allen. </author> <title> Private communication, </title> <month> February </month> <year> 1990. </year>
Reference-contexts: The papers describing the ptran project [CFS90, ABC + 87], which also performs code generation based on G cd , do not address distribution. Work in memory management and name space adjustment [KKP + 81, Por89] uses distribution, but only when no control dependences are present. The Stardent compiler <ref> [All90] </ref> distributes loops with structured control flow by keeping groups of statements with the same control flow constraints together. For example, all the statements in the true branch of a block if must stay together, so only the outer level of if nests can be considered.
Reference: [Ban88] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: We use the following definitions of postdominance and control dependence, which are taken from the literature [FOW87, CFS90]. 1 A data dependence exists between two statements if they reference the same memory location and at least one of them is a write <ref> [Ber66, Ban88, AK84, Wol82] </ref>. Def: x is postdominated by y in the control flow graph G f if every path from x to the exit node of G f contains y.
Reference: [BB89] <author> W. Baxter and H. R. Bauer, III. </author> <title> The program dependence graph and vectorization. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: Because the data and control dependence graphs, as well as the program statements are correct on entry to the code generation phase, a variety of code generation algorithms could be used. For example, any of the code generation algorithms based on the program dependence graph <ref> [FM85, FMS88, CFS90, BB89] </ref> could be used in conjunction with the above algorithm. A very simple code generation scheme is described here. It is designed to be used in ParaScope, an interactive parallelizing environment. <p> DO I = 1, N S 1 IF (EV 1 [I] .EQ. true) GOTO 6 S 2 ENDDO 4.2 Structured Code Generation Because code generation based on G cd when it is a tree, is relatively simple <ref> [FM85, FMS88, BB89] </ref>, this discussion emphasizes properly selecting and inserting the appropriate control structures for newly created guards. Other G cd code generation algorithms must select and create control structures for all branches. <p> Because their code generation algorithm is based on G f , rather than G cd , the proof of how an execution variable is used is much more difficult and is not given. Towle [Tow76] and Baxter and Bauer <ref> [BB89] </ref> use similar approaches for inserting conditional arrays. Ferrante, Mace, and Simons present related algorithms whose goals are to avoid replication and branch variables when possible [FM85, FMS88]. Their code generation algorithms convert parallel programs into sequential ones, and like ours, are based on G cd .
Reference: [Ber66] <author> A. J. Bernstein. </author> <title> Analysis of programs for parallel processing. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 15(5) </volume> <pages> 757-763, </pages> <month> October </month> <year> 1966. </year>
Reference-contexts: We use the following definitions of postdominance and control dependence, which are taken from the literature [FOW87, CFS90]. 1 A data dependence exists between two statements if they reference the same memory location and at least one of them is a write <ref> [Ber66, Ban88, AK84, Wol82] </ref>. Def: x is postdominated by y in the control flow graph G f if every path from x to the exit node of G f contains y.
Reference: [BJ66] <author> C. Bohm and G. Jacopini. </author> <title> Flow diagrams, turing machines, and languages with only two formation rules. </title> <journal> Communications of the ACM, </journal> <volume> 19(5), </volume> <month> May </month> <year> 1966. </year>
Reference-contexts: Exceptions to the single statement per node rule are inner loops and irreducible regions; all of their statements are represented with a single node. If G f is structured, rooted and acyclic, the resulting G cd is a tree, where structured is as defined by Bohm and Jacopini <ref> [BJ66] </ref>. Also, if G f is unstructured, rooted and acyclic, the resulting G cd is a dag [CFS90].
Reference: [BKK + 89] <author> V. Balasundaram, K. Kennedy, U. Kremer, K. S. M c Kinley, and J. Subhlok. </author> <title> The ParaScope Editor: An interactive parallel programming tool. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> Novem-ber </month> <year> 1989. </year>
Reference-contexts: This algorithm, which is intended for use in an interactive program transformation system called ParaScope, generates code that is very close to the original. Although this approach was inspired by the ParaScope transformation system <ref> [BKK + 89] </ref>, it is also suitable for use in automatic par-allelization and vectorization systems. The algorithms are very fast, both asymptotically and practically. 2 Loop Distribution Distribution is a program transformation, introduced by Muraoka [Mur71], that converts a single loop into multiple loops. <p> The generality of our system will allow future research to focus on discovering partitioning algorithms that are effective in deciding if and when a distribution can be profitably used. This work was motivated by the desire to handle loops with control flow in the ParaScope Editor <ref> [BKK + 89] </ref>, which supports a variety of transformations, including loop distribution. An implementa tion of this work is in progress. 7 Acknowledgments We would like to thank the reviewers for their valuable suggestions and comments, all of which were in corporated.
Reference: [CFS90] <author> R. Cytron, J. Ferrante, and V. Sarkar. </author> <title> Experiences using control dependence in PTRAN. </title> <editor> In D. Gelern-ter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Reconstructing sequential code from a control dependence graph is not trivial, but it is easier than reconstructing from code that has been subject to if-conversion <ref> [FM85, FMS88, CFS90] </ref>. Unfortunately, the control dependence representation complicates loop distribution. <p> The placement of statements into loops must preserve the data 1 and control dependences of the original loop. We use the following definitions of postdominance and control dependence, which are taken from the literature <ref> [FOW87, CFS90] </ref>. 1 A data dependence exists between two statements if they reference the same memory location and at least one of them is a write [Ber66, Ban88, AK84, Wol82]. <p> If G f is structured, rooted and acyclic, the resulting G cd is a tree, where structured is as defined by Bohm and Jacopini [BJ66]. Also, if G f is unstructured, rooted and acyclic, the resulting G cd is a dag <ref> [CFS90] </ref>. <p> Because the data and control dependence graphs, as well as the program statements are correct on entry to the code generation phase, a variety of code generation algorithms could be used. For example, any of the code generation algorithms based on the program dependence graph <ref> [FM85, FMS88, CFS90, BB89] </ref> could be used in conjunction with the above algorithm. A very simple code generation scheme is described here. It is designed to be used in ParaScope, an interactive parallelizing environment. <p> They discuss three transformations that restructure control flow: loop fusion, dead code elimination, and branch deletion. Other research concerned with the definition and use of the program dependence graph does not address distribution [FOW87, FM85, FMS88]. The papers describing the ptran project <ref> [CFS90, ABC + 87] </ref>, which also performs code generation based on G cd , do not address distribution. Work in memory management and name space adjustment [KKP + 81, Por89] uses distribution, but only when no control dependences are present.
Reference: [CK87] <author> D. Callahan and M. Kalem. </author> <title> Control dependences. Supercomputer Software Newsletter 15, </title> <institution> Dept. of Computer Science, Rice University, </institution> <month> October </month> <year> 1987. </year>
Reference-contexts: N IF (EV 1 [I] .EQ. true) GOTO 5 IF (EV 2 [I] .EQ. true) THEN S 3 ELSE IF (EV 2 [I] .EQ. false) THEN 5 S 5 ENDIF ENDDO 5 Related Work Callahan and Kalem present two methods for generating loop distributions in the presence of control flow <ref> [CK87] </ref>. The first, which works for structured or unstructured control flow, replicates the control flow of the original loop in each of the new loops by using G f . Branch variables are inserted to record decisions made in one loop and used in other loops.
Reference: [Die88] <author> H. Dietz. </author> <title> Finding large-grain parallelism in loops with serial control dependences. </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: Branch variables are inserted to record decisions made in one loop and used in other loops. An additional pass then trims the new loops of any empty control flow. Dietz uses a very similar approach <ref> [Die88] </ref>. It has some of the same drawbacks of if-conversion. Callahan and Kalem's second method, which works only for structured control flow, uses G f , G cd , and boolean execution variables.
Reference: [FM85] <author> J. Ferrante and M. Mace. </author> <title> On linearizing parallel code. </title> <booktitle> In Conference Record of the Twelfth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> New Orleans, LA, </address> <month> January </month> <year> 1985. </year>
Reference-contexts: Reconstructing sequential code from a control dependence graph is not trivial, but it is easier than reconstructing from code that has been subject to if-conversion <ref> [FM85, FMS88, CFS90] </ref>. Unfortunately, the control dependence representation complicates loop distribution. <p> Because the data and control dependence graphs, as well as the program statements are correct on entry to the code generation phase, a variety of code generation algorithms could be used. For example, any of the code generation algorithms based on the program dependence graph <ref> [FM85, FMS88, CFS90, BB89] </ref> could be used in conjunction with the above algorithm. A very simple code generation scheme is described here. It is designed to be used in ParaScope, an interactive parallelizing environment. <p> DO I = 1, N S 1 IF (EV 1 [I] .EQ. true) GOTO 6 S 2 ENDDO 4.2 Structured Code Generation Because code generation based on G cd when it is a tree, is relatively simple <ref> [FM85, FMS88, BB89] </ref>, this discussion emphasizes properly selecting and inserting the appropriate control structures for newly created guards. Other G cd code generation algorithms must select and create control structures for all branches. <p> Towle [Tow76] and Baxter and Bauer [BB89] use similar approaches for inserting conditional arrays. Ferrante, Mace, and Simons present related algorithms whose goals are to avoid replication and branch variables when possible <ref> [FM85, FMS88] </ref>. Their code generation algorithms convert parallel programs into sequential ones, and like ours, are based on G cd . They discuss three transformations that restructure control flow: loop fusion, dead code elimination, and branch deletion. <p> They discuss three transformations that restructure control flow: loop fusion, dead code elimination, and branch deletion. Other research concerned with the definition and use of the program dependence graph does not address distribution <ref> [FOW87, FM85, FMS88] </ref>. The papers describing the ptran project [CFS90, ABC + 87], which also performs code generation based on G cd , do not address distribution. Work in memory management and name space adjustment [KKP + 81, Por89] uses distribution, but only when no control dependences are present.
Reference: [FMS88] <author> J. Ferrante, M. Mace, and B. Simons. </author> <title> Generating sequential code from parallel code. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Reconstructing sequential code from a control dependence graph is not trivial, but it is easier than reconstructing from code that has been subject to if-conversion <ref> [FM85, FMS88, CFS90] </ref>. Unfortunately, the control dependence representation complicates loop distribution. <p> Because the data and control dependence graphs, as well as the program statements are correct on entry to the code generation phase, a variety of code generation algorithms could be used. For example, any of the code generation algorithms based on the program dependence graph <ref> [FM85, FMS88, CFS90, BB89] </ref> could be used in conjunction with the above algorithm. A very simple code generation scheme is described here. It is designed to be used in ParaScope, an interactive parallelizing environment. <p> DO I = 1, N S 1 IF (EV 1 [I] .EQ. true) GOTO 6 S 2 ENDDO 4.2 Structured Code Generation Because code generation based on G cd when it is a tree, is relatively simple <ref> [FM85, FMS88, BB89] </ref>, this discussion emphasizes properly selecting and inserting the appropriate control structures for newly created guards. Other G cd code generation algorithms must select and create control structures for all branches. <p> Towle [Tow76] and Baxter and Bauer [BB89] use similar approaches for inserting conditional arrays. Ferrante, Mace, and Simons present related algorithms whose goals are to avoid replication and branch variables when possible <ref> [FM85, FMS88] </ref>. Their code generation algorithms convert parallel programs into sequential ones, and like ours, are based on G cd . They discuss three transformations that restructure control flow: loop fusion, dead code elimination, and branch deletion. <p> They discuss three transformations that restructure control flow: loop fusion, dead code elimination, and branch deletion. Other research concerned with the definition and use of the program dependence graph does not address distribution <ref> [FOW87, FM85, FMS88] </ref>. The papers describing the ptran project [CFS90, ABC + 87], which also performs code generation based on G cd , do not address distribution. Work in memory management and name space adjustment [KKP + 81, Por89] uses distribution, but only when no control dependences are present.
Reference: [FOW87] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: However, it has several drawbacks. If vectorization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals. For these reasons, research in automatic paral-lelization has concentrated on an alternative approach that uses control dependences <ref> [FOW87, ABC + 87, ABC + 88] </ref> to model control flow. Reconstructing sequential code from a control dependence graph is not trivial, but it is easier than reconstructing from code that has been subject to if-conversion [FM85, FMS88, CFS90]. Unfortunately, the control dependence representation complicates loop distribution. <p> The placement of statements into loops must preserve the data 1 and control dependences of the original loop. We use the following definitions of postdominance and control dependence, which are taken from the literature <ref> [FOW87, CFS90] </ref>. 1 A data dependence exists between two statements if they reference the same memory location and at least one of them is a write [Ber66, Ban88, AK84, Wol82]. <p> They discuss three transformations that restructure control flow: loop fusion, dead code elimination, and branch deletion. Other research concerned with the definition and use of the program dependence graph does not address distribution <ref> [FOW87, FM85, FMS88] </ref>. The papers describing the ptran project [CFS90, ABC + 87], which also performs code generation based on G cd , do not address distribution. Work in memory management and name space adjustment [KKP + 81, Por89] uses distribution, but only when no control dependences are present.
Reference: [KKLW84] <author> D. Kuck, R. Kuhn, B. Leasure, and M. J. Wolfe. </author> <title> The structure of an advanced retargetable vector-izer. In Supercomputers: </title> <booktitle> Design and Applications, </booktitle> <pages> pages 163-178. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Silver Spring, MD, </address> <year> 1984. </year>
Reference-contexts: This approach, called if-conversion [AKPW83, All83], has been used successfully in a vari-ety of vectorization systems <ref> [AK87, SK86, KKLW84] </ref>. However, it has several drawbacks. If vectorization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals.
Reference: [KKP + 81] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: The method we present is designed to work on any partition that is legal, i.e., any partition that preserves the control and data dependences of the original program. A partition can preserve all dependences if and only if there exists no dependence cycle spanning more than one output loop <ref> [KKP + 81, AK87] </ref>. If there is a cycle involving control and/or data dependences, it must be contained entirely within a single partition (there are a few additional considerations for loops with exit branches, which are discussed in Section 3.4). This condition is necessary and sufficient. <p> The papers describing the ptran project [CFS90, ABC + 87], which also performs code generation based on G cd , do not address distribution. Work in memory management and name space adjustment <ref> [KKP + 81, Por89] </ref> uses distribution, but only when no control dependences are present. The Stardent compiler [All90] distributes loops with structured control flow by keeping groups of statements with the same control flow constraints together.
Reference: [Mur71] <author> Y. Muraoka. </author> <title> Parallelism Exposure and Exploitation in Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1971. </year> <note> Report No. 71-424. </note>
Reference-contexts: Although this approach was inspired by the ParaScope transformation system [BKK + 89], it is also suitable for use in automatic par-allelization and vectorization systems. The algorithms are very fast, both asymptotically and practically. 2 Loop Distribution Distribution is a program transformation, introduced by Muraoka <ref> [Mur71] </ref>, that converts a single loop into multiple loops. The placement of statements into loops must preserve the data 1 and control dependences of the original loop.
Reference: [Por89] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: The papers describing the ptran project [CFS90, ABC + 87], which also performs code generation based on G cd , do not address distribution. Work in memory management and name space adjustment <ref> [KKP + 81, Por89] </ref> uses distribution, but only when no control dependences are present. The Stardent compiler [All90] distributes loops with structured control flow by keeping groups of statements with the same control flow constraints together.
Reference: [SK86] <author> R. G. Scarborough and H. G. Kolsky. </author> <title> A vectorizing Fortran compiler. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 30(2) </volume> <pages> 163-171, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: This approach, called if-conversion [AKPW83, All83], has been used successfully in a vari-ety of vectorization systems <ref> [AK87, SK86, KKLW84] </ref>. However, it has several drawbacks. If vectorization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals.
Reference: [Tow76] <author> R. A. Towle. </author> <title> Control and Data Dependence for Program Transformation. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> March </month> <year> 1976. </year>
Reference-contexts: Also, one execution variables may be needed for every successor in the descendent partition. Because their code generation algorithm is based on G f , rather than G cd , the proof of how an execution variable is used is much more difficult and is not given. Towle <ref> [Tow76] </ref> and Baxter and Bauer [BB89] use similar approaches for inserting conditional arrays. Ferrante, Mace, and Simons present related algorithms whose goals are to avoid replication and branch variables when possible [FM85, FMS88].
Reference: [Wol82] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> Octo-ber </month> <year> 1982. </year>
Reference-contexts: We use the following definitions of postdominance and control dependence, which are taken from the literature [FOW87, CFS90]. 1 A data dependence exists between two statements if they reference the same memory location and at least one of them is a write <ref> [Ber66, Ban88, AK84, Wol82] </ref>. Def: x is postdominated by y in the control flow graph G f if every path from x to the exit node of G f contains y.
References-found: 26

