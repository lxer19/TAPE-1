URL: http://www.cs.gatech.edu/grads/d/Srinivas.Doddapaneni/papers/affinity-transforms.ps
Refering-URL: http://www.cs.gatech.edu/grads/d/Srinivas.Doddapaneni/pubs.html
Root-URL: 
Title: Program Transformation for Locality Using Affinity Regions  
Author: Bill Appelbe, Charles Hardnett, and Sri Doddapaneni 
Address: Atlanta, GA 30332  
Affiliation: College of Computing, Georgia Institute of Technology,  
Abstract: Affinity regions ensure that a shared processor schedule, mapping loop iterations to processors, is used in consecutive parallel loop nests. Using affinity regions can improve locality without affecting parallelism. Unlike loop fusion, affinity regions are always safe. Also, unlike parallel regions, affinity regions do not require explicit code for mapping loop iterations to processors. While affinity regions improve locality, there may be cases where more than one affinity region is possible for the same pair of loop nests. Also, loop transformations such as interchange and alignment can affect the profitability of affinity regions. In this paper we discuss the interaction between loop transformations and affinity regions, and the relationship between affinity regions and global optimization for parallelism and locality. The algorithms in this paper are being implemented in the parallelization tool, PAT, targeted at generating of affinity regions for the KSR-1.
Abstract-found: 1
Intro-found: 1
Reference: 1. <institution> KSR Fortran Programming. Kendall Square Research, Waltham, Massachusetts, </institution> <year> 1991. </year>
Reference-contexts: If threads (logical processors) are used, then the program must still ensure that the threads do not migrate to a different processor within the region. Using the affinity region construct of the KSR-1 together with PTILEs (par allel loops), we can rewrite Program 1 in KSR Fortran <ref> [1] </ref> as: DO 10 J = 1, N A (I,J) = 0 20 CONTINUE 10 CONTINUE C*KSR* AFFINITY REGION (I:1,N, K:1,N) DO 30 J = 1, N C*KSR* PTILE (I,K,TILESIZE=(I:1,K:N),AFFMEMBER=1) DO 40 I = 1, N A (I,J) = A (I,J) + B (K,I)*C (K,J) 50 CONTINUE 40 CONTINUE 30 CONTINUE
Reference: 2. <institution> KSR Parallel Programming. Kendall Square Research, Waltham, Massachusetts, </institution> <year> 1991. </year>
Reference-contexts: The primary reason for using affinity regions is to minimize data movement between processors. Scheduling using affinity regions can ensure that processors access the same array sections in successive loop nests. Affinity regions are targeted at shared-memory or NUMA (Non-Uniform Memory Access) multiprocessors, such as the Kendall Square KSR-1 <ref> [2] </ref>. However, the algorithms used to find affinity regions are applicable to optimization for distributed memory systems as explained below. In principle, the scheduling of parallel loops in an affinity region can use either self-scheduling or pre-scheduling strategies.
Reference: 3. <author> Anderson, J., and Lam, M. S. </author> <title> Global optimzations for parallelism and locality on scalable parallel machines. </title> <booktitle> In SIGPLAN Programming Language Design and Implementation (1993), </booktitle> <pages> pp. 112-125. </pages>
Reference-contexts: This is related to the approach of Anderson and Lam <ref> [3] </ref>, who use an explicit iteration to processor mapping vecc and an explicit array to processor mapping d. Composing these mappings yields an iteration to iteration mapping which can be equivalent to L MAP . However, there are some significant differences in the two approaches: 1.
Reference: 4. <author> Appelbe, W. F., and Lakshmanan, B. </author> <title> Optimizing parallel programs using affinity regions. </title> <booktitle> In International Conference on Parallel Processing (August 1993). </booktitle>
Reference-contexts: In practice, aside from affinity regions, there are two other ways of achieving data locality between loop nests loop fusion and PARALLEL REGIONS. However, affinity regions are more general than loop fusion, and a higher level construct than parallel regions <ref> [4] </ref>. <p> Strategy How do affinity regions interact with other program transformations, especially intraloop transformations such as interchange and skewing? Answers to the first two questions were provided in <ref> [4] </ref>. In summary, there needs to be a reaching dependence (including input dependences) between the two loop nests, of an array variable A, whose index is dependent upon a parallel loop index in both loops. <p> Thus, the insertion of affinity regions might appear to be carried out in a simple post-pass after parallelization. However, the following analysis shows that the interactions are not as simple as they appear. The analysis below assumes a general model for affinity regions <ref> [4] </ref>, which is not constrained by the limitations of the current implementations imposed by the KSR-1. 2 Transformations for Affinity There may be more than one possible affinity region to encompass two loop nests. <p> This in turn implies that the affinity regions may differ in the common subscripted variable being used. It would be profitable if affinity regions can be merged in some way. The merging of affinity regions can be made possible by the use of transformations in <ref> [4] </ref>. <p> Although tiling for L 2 improves data locality, the execution of the two loop nests using different scheduling strategies results in a large amount of data movement. However, the two loop nests meet the criteria necessary to be put into an affinity region <ref> [4] </ref>. The affinity region construct insists that the same scheduling strategy be used in both loop nests. Since L 2 requires tiling for better data locality, the same strategy will be used in L 1 if they are both in the affinity region. <p> iterations LOOP J = 1, N LOOP I = 2, N-1 A (I,J) = C (I,J) END LOOP END LOOP C Peeled iterations Program 6: Aligned Loop nest 1 for Parallelism The loop nests L 1 and L 2 are also affinity adjacent (sharing a variable with a reaching dependence <ref> [4] </ref>) based on their access to A. However, there are potentially two possible affinity regions for the above loop nests one based on the accesses to A and the other based on the accesses to the subscripted variable B.
Reference: 5. <author> Wolfe, M. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: We know that loop distribution can be used to improve parallelism. Also, it has been shown that loop fusion results in better data locality in the presence of memory hierarchy <ref> [5, 6] </ref>. For example: LOOP I = 1, N A (I) = .... END LOOP LOOP J = 1, N A (I) = ....
Reference: 6. <author> Zima, H., and Chapman, B. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1990. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Pre-scheduling specifies the iteration to processor mapping statically, whereas self-scheduling is a dynamic mapping. The shared schedule used in an affinity region can either be determined statically, or by the first loop executed in the region. Although, self-scheduling results in proper load balancing, pre-scheduling can result in better locality <ref> [6] </ref>. In practice, aside from affinity regions, there are two other ways of achieving data locality between loop nests loop fusion and PARALLEL REGIONS. However, affinity regions are more general than loop fusion, and a higher level construct than parallel regions [4]. <p> We know that loop distribution can be used to improve parallelism. Also, it has been shown that loop fusion results in better data locality in the presence of memory hierarchy <ref> [5, 6] </ref>. For example: LOOP I = 1, N A (I) = .... END LOOP LOOP J = 1, N A (I) = ....
References-found: 6

