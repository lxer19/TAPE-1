URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/aperez/Web/sss96.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/aperez/Web/publications.html
Root-URL: 
Title: Learning from a Domain Expert to Generate Good Plans  
Author: M. Alicia Perez 
Address: Boston College Chestnut Hill, MA 02167  
Affiliation: Computer Science Department  
Abstract: E-mail: perezab@cs.bc.edu In Acquisition, Learning and Demonstration: Automating Tasks for Users. Papers from the 1996 AAAI Symposium. March 1996, Stanford, CA. Technical Report SS-96-02. AAAI Press. Menlo Park, CA Abstract This paper describes quality, a domain- independent architecture that learns operational quality-improving search-control knowledge given a domain theory, a domain-specific metric of plan quality, and problems which provide planning experience. quality can (optionally) interact with a human expert in the planning application domain who suggests improvements to the plans at the operator (plan step) level. The framework includes two distinct domain-independent learning mechanisms which differ in the language used to represent the learned knowledge, namely control rules and control knowledge trees, and in the kinds of quality metrics for which they are best suited. quality is fully implemented on top of the prodigy4.0 nonlinear planner and its empirical evaluation has shown that the learned knowledge is able to substantially improve plan quality.
Abstract-found: 1
Intro-found: 1
Reference: [ Dent et al., 1992 ] <author> Lisa Dent, Jesus G. Boticario, John McDermott, Tom Mitchell, and David Zabowski. </author> <title> A personal learning apprentice. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 96-103, </pages> <address> San Jose, CA, </address> <year> 1992. </year>
Reference: [ Etzioni, 1990 ] <author> Oren Etzioni. </author> <title> A Structural Theory of Explanation-Based Learning. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1990. </year> <note> Also appeared as Technical Report CMU-CS-90-185. </note>
Reference-contexts: The example in Section 2 shows how those choices are relevant to obtaining good plans. Previous algorithms developed in the context of the prodigy architecture learned control rules for prodigy2.0, the initial, linear planner of prodigy, with the goal of planning efficiency <ref> [ Minton, 1988; Etzioni, 1990; Perez and Etzioni, 1992 ] </ref> . quality focuses on plan quality. * Learning control knowledge trees. In the second learning mechanism within quality the learned control knowledge is represented using a formalism that we call control knowledge trees (ck- trees).
Reference: [ Gil, 1991 ] <author> Yolanda Gil. </author> <title> A specification of process planning for PRODIGY. </title> <type> Technical Report CMU-CS-91-179, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Integrating the two learning mechanisms to take advantage of their complementary characteristics remains an open issue. 5 Experimental Results We have fully implemented all the algorithms described and evaluated their performance in a complex process- planning domain <ref> [ Gil, 1991 ] </ref> , in which they lead to significant plan quality improvements. We have also used a small transportation domain to further test the performance of control knowledge trees.
Reference: [ Golding et al., 1987 ] <author> Andrew Golding, Paul S. Rosenbloom, and John E. Laird. </author> <title> Learning general search control from outside guidance. </title> <booktitle> In Proceedings of the Tenth International Conference on Artificial Intelligence, </booktitle> <pages> pages 334-337, </pages> <address> Milan, Italy, </address> <year> 1987. </year>
Reference-contexts: Learning apprentices rely on the interaction with an expert in different degrees ranging from a non-intrusive observation (for example [ Mitchell et al., 1990; Dent et al., 1992; Wang, 1995 ] ) to direct advice at decision points <ref> [ Golding et al., 1987; Laird et al., 1990 ] </ref> or illustrative examples supplied by the expert within the system's current knowledge and abilities [ Golding et al., 1987 ] . quality can work autonomously (without human expert interaction) as the quality metric is known to the planner and it can <p> ranging from a non-intrusive observation (for example [ Mitchell et al., 1990; Dent et al., 1992; Wang, 1995 ] ) to direct advice at decision points [ Golding et al., 1987; Laird et al., 1990 ] or illustrative examples supplied by the expert within the system's current knowledge and abilities <ref> [ Golding et al., 1987 ] </ref> . quality can work autonomously (without human expert interaction) as the quality metric is known to the planner and it can always solve the problem from first principles.
Reference: [ Gratch et al., 1993 ] <author> Jonathan Gratch, Steve Chien, and Gerald DeJong. </author> <title> Learning search control knowledge for deep space network scheduling. </title> <booktitle> In Machine Learning. Proceedings of the Tenth International Conference, </booktitle> <pages> pages 135142, </pages> <address> Amherst, MA, June 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Gruber, 1989 ] <author> Thomas R. Gruber. </author> <title> Automated knowledge acquisition for strategic knowledge. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 293-336, </pages> <year> 1989. </year>
Reference-contexts: Search control knowledge can also be acquired by knowledge acquisition methods <ref> [ Gruber, 1989; Joseph, 1992 ] </ref> , by extracting from the human experts justifications for their choices. quality instead allows a fully automated acquisition task by using a purely machine- learning approach. The expert, if present, does not need to make explicit the reasons for the choices of plan steps.
Reference: [ Hamazaki, 1992 ] <author> Takashi Hamazaki. </author> <title> High quality produc-tion scheduling system. </title> <booktitle> In Proceedings of SPICIS 92, </booktitle> <pages> pages 195-200, </pages> <year> 1992. </year>
Reference-contexts: The expert, if present, does not need to make explicit the reasons for the choices of plan steps. In addition, its focus is on quality-enhancing control knowledge. CABINS [ Sycara and Miyashita, 1994 ] and the system described in <ref> [ Hamazaki, 1992 ] </ref> acquire user preferences to generate good schedules.
Reference: [ Huffman and Laird, 1994 ] <author> Scott B. Huffman and John E. Laird. </author> <title> Learning from higly flexible tutorial instruction. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 506-512, </pages> <address> Seattle, WA, July 1994. </address> <publisher> AAAI Press/The MIT Press. </publisher>
Reference: [ Iwamoto, 1994 ] <author> Masahiko Iwamoto. </author> <title> A planner with quality goal and its speedup learning for optimization problem. </title> <booktitle> In Proceedings of the Second International Conference on AI Planning Systems, </booktitle> <pages> pages 281-286, </pages> <address> Chicago, IL, </address> <year> 1994. </year>
Reference-contexts: Iwamoto's system <ref> [ Iwamoto, 1994 ] </ref> has developed an extension to prodigy to solve optimization problems and an EBL method to learn control rules to find near- optimal solutions in LSI design. hamlet [ Veloso et al., 1995 ] learns control rules that improve both planning efficiency and also the quality (length) of
Reference: [ Joseph, 1992 ] <author> Robert L. Joseph. </author> <title> Graphical Knowledge Acquisition for Visually-Oriented Planning Domains. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <month> August </month> <year> 1992. </year> <note> Also appeared as Technical Report CMU-CS-92-188. </note>
Reference-contexts: Search control knowledge can also be acquired by knowledge acquisition methods <ref> [ Gruber, 1989; Joseph, 1992 ] </ref> , by extracting from the human experts justifications for their choices. quality instead allows a fully automated acquisition task by using a purely machine- learning approach. The expert, if present, does not need to make explicit the reasons for the choices of plan steps.
Reference: [ Laird et al., 1990 ] <author> John E. Laird, Michael Hucka, Eric S. Yager, and Christopher M. Tuck. </author> <title> Correcting and ex-tending domain knowledge using outside guidance. </title> <editor> In Bruce W. Porter and Ray J. Mooney, editors, </editor> <booktitle> Machine Learning: Proceedings of the Seventh International Conference, ML90, </booktitle> <pages> pages 235-243, </pages> <address> Austin, TX, June 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Learning apprentices rely on the interaction with an expert in different degrees ranging from a non-intrusive observation (for example [ Mitchell et al., 1990; Dent et al., 1992; Wang, 1995 ] ) to direct advice at decision points <ref> [ Golding et al., 1987; Laird et al., 1990 ] </ref> or illustrative examples supplied by the expert within the system's current knowledge and abilities [ Golding et al., 1987 ] . quality can work autonomously (without human expert interaction) as the quality metric is known to the planner and it can
Reference: [ Lieberman, 1994 ] <author> Henry Lieberman. </author> <title> A user interface for knowledge acquisition from video. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 527-534, </pages> <address> Seattle, WA, July 1994. </address> <publisher> AAAI Press/The MIT Press. </publisher>
Reference: [ Martin and Redmond, 1989 ] <author> Joel D. Martin and Michael Redmond. </author> <title> Acquiring knowledge for explaining observed problem solving. </title> <journal> SIGART Newsletter, Knowledge Acquisition Special Issue, </journal> <volume> 108 </volume> <pages> 77-83, </pages> <month> April </month> <year> 1989. </year>
Reference: [ Minton et al., 1989 ] <author> Steven Minton, Jaime G. Carbonell, Craig A. Knoblock, Daniel R. Kuokka, Oren Etzioni, and Yolanda Gil. </author> <title> Explanation-based learning: A problemsolving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 63-118, </pages> <year> 1989. </year>
Reference-contexts: The first mechanism learns control knowledge in the form of control rules, in particular of prodigy's prefer control rules <ref> [ Minton et al., 1989; Veloso et al., 1995 ] </ref> .
Reference: [ Minton, 1988 ] <author> Steven Minton. </author> <title> Learning Effective Search Control Knowledge: An Explanation-based Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year> <note> PhD the-sis available as Technical Report CMU-CS-88-133, </note> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: The example in Section 2 shows how those choices are relevant to obtaining good plans. Previous algorithms developed in the context of the prodigy architecture learned control rules for prodigy2.0, the initial, linear planner of prodigy, with the goal of planning efficiency <ref> [ Minton, 1988; Etzioni, 1990; Perez and Etzioni, 1992 ] </ref> . quality focuses on plan quality. * Learning control knowledge trees. In the second learning mechanism within quality the learned control knowledge is represented using a formalism that we call control knowledge trees (ck- trees).
Reference: [ Mitchell et al., 1986 ] <author> Tom M. Mitchell, Richard Keller, and Smadar Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1986. </year>
Reference: [ Mitchell et al., 1990 ] <author> Tom M. Mitchell, Sridhar Mahadevan, and Louis I. Steinberg. </author> <title> LEAP: A learning apprentice sys-tem for VLSI design. </title> <editor> In Yves Kodratoff and Ryszard Michalski, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, volume III, </booktitle> <pages> pages 271-289. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference: [ Perez and Etzioni, 1992 ] <author> M. Alicia Perez and Oren Etzioni. </author> <title> DYNAMIC: A new role for training problems in EBL. </title> <editor> In D. Sleeman and P. Edwards, editors, </editor> <booktitle> Machine Learning: Proceedings of the Ninth International Conference, ML92, </booktitle> <pages> pages 367-372. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA., </address> <year> 1992. </year>
Reference-contexts: The example in Section 2 shows how those choices are relevant to obtaining good plans. Previous algorithms developed in the context of the prodigy architecture learned control rules for prodigy2.0, the initial, linear planner of prodigy, with the goal of planning efficiency <ref> [ Minton, 1988; Etzioni, 1990; Perez and Etzioni, 1992 ] </ref> . quality focuses on plan quality. * Learning control knowledge trees. In the second learning mechanism within quality the learned control knowledge is represented using a formalism that we call control knowledge trees (ck- trees).
Reference: [ Perez, 1995 ] <author> M. Alicia Perez. </author> <title> Learning search control knowl-edge to improve plan quality. </title> <type> Technical Report CMU-CS95-175, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> July </month> <year> 1995. </year> <type> PhD thesis. </type>
Reference-contexts: In its interactive mode quality asks a human for a better plan and then calls the planner to produce a search trace that leads to that plan. Details on how the search trace is generated from the plan can be found in <ref> [ Perez, 1995 ] </ref> . quality assumes that the expert's model of plan quality and the quality metric Q D are consistent. In particular if the expert's plan is worse than the initial one, the expert's plan is rejected. <p> Instead, a more globally-scoped method is required. Control knowledge trees provide a more global 3 Describing these algorithms is out of the scope of this paper. Details on the algorithms, implementation, and experimental evaluation can be found in <ref> [ Perez, 1995 ] </ref> . view of the planning decisions and are used, together with the quality metric, to estimate the quality of each available alternative at a given planning decision point. <p> The experiments also showed that the learned knowledge does not degrade considerably planning efficiency; in fact it improves it in many cases due to shorter solutions. <ref> [ Perez, 1995 ] </ref> shows detailed results on these experiments. 6 Related Work Other learning research has focused on finding good plans.
Reference: [ Porter and Kibler, 1986 ] <author> Bruce Porter and Dennus Kibler. </author> <title> Experimental goal regression: A method for learning problem-solving heuristics. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 249-286, </pages> <year> 1986. </year>
Reference: [ Sycara and Miyashita, 1994 ] <author> Katia Sycara and Kazuo Miyashita. </author> <title> Case-based acquisition of user preferences for solution improvement in ill-structured domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 44-49, </pages> <address> Seattle, WA, July 1994. </address> <publisher> AAAI Press/The MIT Press. </publisher>
Reference-contexts: The expert, if present, does not need to make explicit the reasons for the choices of plan steps. In addition, its focus is on quality-enhancing control knowledge. CABINS <ref> [ Sycara and Miyashita, 1994 ] </ref> and the system described in [ Hamazaki, 1992 ] acquire user preferences to generate good schedules.
Reference: [ Tadepalli, 1989 ] <author> Prasad Tadepalli. </author> <title> Lazy explanation-based learning: A solution to the intractable theory problem. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 694-700, </pages> <address> Detroit, MI, </address> <year> 1989. </year>
Reference: [ Tadepalli, 1990 ] <author> Prasad Tadepalli. </author> <title> Tractable Learning and Planning in Games. </title> <type> PhD thesis, </type> <institution> Rutgers, The State University of New Jersey, Department of Computer Science, </institution> <month> May </month> <year> 1990. </year> <note> Technical Report ML-TR-31. </note>
Reference-contexts: Thus, the goal of learning problem solving expertise can be seen as translating a non-operational domain theory into an operational one <ref> [ Tadepalli, 1990 ] </ref> . In the previous section we referred to it as the quality mapping problem. In our case the domain theory consists of the description of the domain (planning operators, inference rules, and type hierarchy) and the domain-specific quality metric.
Reference: [ Tecucci, 1992 ] <author> Gheorghe D. Tecucci. </author> <title> Automating knowl-edge acquisition as extending, updating, and improving a knowledge base. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 22(6), </volume> <month> November/ December </month> <year> 1992. </year>
Reference: [ Veloso et al., 1995 ] <author> Manuela Veloso, Jaime Carbonell, M. Alicia Perez, Daniel Borrajo, Eugene Fink, and Jim Blythe. </author> <title> Integrating planning and learning: The PRODIGY architecture. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 7(1), </volume> <month> January </month> <year> 1995. </year>
Reference-contexts: Section 3 describes the motivation for and characteristics of the domain expert interaction. Sec- tion 4 briefly presents two representation formalisms for quality-enhancing control knowledge that are learned automatically by quality. quality has been been fully implemented on top of the prodigy4.0 nonlinear planner <ref> [ Veloso et al., 1995 ] </ref> and some of the experimental results obtained are described in Section 5. <p> The first mechanism learns control knowledge in the form of control rules, in particular of prodigy's prefer control rules <ref> [ Minton et al., 1989; Veloso et al., 1995 ] </ref> . <p> Iwamoto's system [ Iwamoto, 1994 ] has developed an extension to prodigy to solve optimization problems and an EBL method to learn control rules to find near- optimal solutions in LSI design. hamlet <ref> [ Veloso et al., 1995 ] </ref> learns control rules that improve both planning efficiency and also the quality (length) of the plans generated, by a combination of bounded explanation and induction. However neither method can take advantage of user guidance because they use exhaustive search to find the best solution.
Reference: [ Wang, 1995 ] <author> Xuemei Wang. </author> <title> Learning by observation and practice: An incremental approach for planning operator acquisition. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> Tahoe City, CA, </address> <year> 1995. </year>
Reference: [ Wilkins, 1988 ] <author> David C. Wilkins. </author> <title> Knowledge base refine-ment using apprenticeship learning techniques. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 646-651, </pages> <address> St. Paul, MN, </address> <year> 1988. </year>
References-found: 27

