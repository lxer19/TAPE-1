URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1998/TR06.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Title: Empirical Studies of a Prediction Model for Regression Test Selection recently proposed coverage-based predictors for
Author: Mary Jean Harrold David Rosenblum Gregg Rothermel Elaine Weyuker White, Rosenblum and Weyuker 
Note: Rosenblum and Weyuker  
Abstract: Regression testing is an important testing activity that can account for a large proportion of the cost of software maintenance. One approach to reducing the cost of regression testing is to employ a selective regression testing technique that (1) selects a subset of a test suite that was used to test the software before the modifications, and then (2) uses this subset to test the modified software. Selective regression testing techniques reduce the cost of regression testing if the cost of selecting the subset from the test suite together with the cost of running the selected subset of test cases is less than the cost of running the entire test suite. To further investigate the applicability of the Rosenblum-Weyuker (RW) predictor, additional empirical studies have been performed. The RW predictor was applied to a number of subjects, using two different selective regression testing tools, DejaVu and TestTube. These studies support two conclusions. First, they show that there is some variability in the success with which the predictors work, and second, they suggest that these results can be improved by incorporating information about the distribution of modifications. It is shown how the RW prediction model can be improved to provide such an accounting.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Agrawal, J. Horgan, E. Krauser, and S. </author> <title> London. Incremental regression testing. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1993, </booktitle> <pages> pages 348-357, </pages> <month> September </month> <year> 1993. </year>
Reference: [2] <author> M. Balcer, W. Hasling, and T. </author> <title> Ostrand. Automatic generation of test scripts from formal test specifications. </title> <booktitle> In Proceedings of the Third Symposium on Software Testing, Analysis, and Verification, </booktitle> <pages> pages 210-218, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: initial set of black-box test cases "according to good testing practices, based on the tester's understanding of the program's functionality and knowledge of special values and boundary points that are easily observable in the code" [14, p. 194], using the category partition method and the Siemens Test Specification Language tool <ref> [2, 20] </ref>. They then augmented this set with manually-created white-box test cases to ensure that each executable statement, edge, and definition-use pair in the base program or its control flow graph was exercised by at least 30 test cases.
Reference: [3] <author> T. Ball. </author> <title> On the limit of control flow analysis for regression test selection. </title> <booktitle> In Proceedings of the 1998 International Symposium on Software Testing and Analysis (ISSTA), </booktitle> <month> March </month> <year> 1998. </year>
Reference: [4] <author> S. Bates and S. Horwitz. </author> <title> Incremental program testing using program dependence graphs. </title> <booktitle> In Proceedings of the 20th ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1993. </year>
Reference: [5] <author> B. Beizer. </author> <title> Software Testing Techniques. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Regression testing is an important testing activity that can account for a large proportion of the cost of software maintenance <ref> [5, 17] </ref>. Regression testing is performed on modified software to provide confidence that the software behaves correctly and that modifications have not adversely impacted the software's quality. One approach to reducing the cost of regression testing is to employ a selective regression testing technique.
Reference: [6] <author> P. Benedusi, A. Cimitile, and U. De Carlini. </author> <title> Post-maintenance testing based on path change analysis. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1988, </booktitle> <pages> pages 352-361, </pages> <month> October </month> <year> 1988. </year>
Reference: [7] <author> D. Binkely. </author> <title> Semantics guided regression test cost reduction. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 23(9), </volume> <month> August </month> <year> 1997. </year>
Reference: [8] <author> Y.F. Chen, D.S. Rosenblum, and K.P. Vo. TestTube: </author> <title> A system for selective regression testing. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering, </booktitle> <pages> pages 211-222, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: For an overview and analytical comparison of these techniques see [24]. 1 other studies performed independently by Rosenblum and Weyuker with a different selective regression test-ing algorithm, implemented as a tool called TestTube <ref> [8] </ref>, also show that such methods are not always cost-effective [23]. When selective regression testing is not efficient, the resources spent performing the test case selection are wasted.
Reference: [9] <author> K.F. Fischer, F. Raji, and A. Chruscicki. </author> <title> A methodology for retesting modified software. </title> <booktitle> In Proceedings of the National Telecommunications Conference B-6-3, </booktitle> <pages> pages 1-6, </pages> <month> November </month> <year> 1981. </year>
Reference: [10] <author> R. Gupta, M.J. Harrold, </author> <title> and M.L. Soffa. An approach to regression testing using slicing. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1992, </booktitle> <pages> pages 299-308, </pages> <month> November </month> <year> 1992. </year>
Reference: [11] <author> W. Harrison and C. Cook. </author> <title> Insights on improving the maintenance process through software measurement. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1990, </booktitle> <pages> pages 37-45, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: One approach is to utilize change history information about the program, often available from configuration management systems. Assuming that the change histories do accurately model the pattern of future modifications (a result suggested by the work of Harrison and Cook <ref> [11] </ref>), we can use this information to compute weights for M .
Reference: [12] <author> M.J. Harrold and M.L. Soffa. </author> <title> An incremental approach to unit testing during maintenance. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1988, </booktitle> <pages> pages 362-367, </pages> <month> October </month> <year> 1988. </year>
Reference: [13] <author> J. Hartmann and D.J. Robson. </author> <title> Techniques for selective revalidation. </title> <journal> IEEE Software, </journal> <volume> 16(1) </volume> <pages> 31-38, </pages> <month> January </month> <year> 1990. </year>
Reference: [14] <author> M. Hutchins, H. Foster, T. Goradia, and T. </author> <title> Ostrand. Experiments on the effectiveness of dataflow-and controlflow-based test adequacy criteria. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering, </booktitle> <pages> pages 191-200, </pages> <month> May </month> <year> 1994. </year> <month> 18 </month>
Reference-contexts: Therefore, to empirically investigate the effectiveness of the predictor, we performed two new studies. For these studies, we used seven C programs as subjects; these programs had been used in an earlier study by researchers at Siemens Corporate Research to compare control flow-based and data flow-based coverage criteria <ref> [14] </ref>. The researchers at Siemens sought to study the fault-detecting effectiveness of different coverage criteria. Therefore, they created faulty modified versions of the seven base programs by manually seeding those programs with faults, usually by modifying a single line of code in the base version. <p> In a few cases they modified between two and five lines of code. Their goal was to introduce faults that were as realistic as possible, based on their experience with real programs. Ten people performed the fault seeding, working "mostly without knowledge of each other's work" <ref> [14, p. 196] </ref>. For each base program, Hutchins et al. created a large test pool containing possible test cases for the program. <p> To populate these test pools, they first created an initial set of black-box test cases "according to good testing practices, based on the tester's understanding of the program's functionality and knowledge of special values and boundary points that are easily observable in the code" <ref> [14, p. 194] </ref>, using the category partition method and the Siemens Test Specification Language tool [2, 20]. <p> To obtain meaningful results with the seeded versions of the programs, the researchers retained only faults that were "neither too easy nor too hard to detect" <ref> [14, p. 196] </ref>, which they defined as being detectable by at least three and at most 350 test cases in the test pool associated with each program. For our studies, we used the Siemens test pools from which we selected smaller test suites.
Reference: [15] <author> J. Laski and W. Szermer. </author> <title> Identification of program modifications and its applications in software main-tenance. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1992, </booktitle> <pages> pages 282-290, </pages> <month> November </month> <year> 1992. </year>
Reference: [16] <author> J.A.N. Lee and X. </author> <title> He. A methodology for test selection. </title> <journal> The Journal of Systems and Software, </journal> <volume> 13(1) </volume> <pages> 177-185, </pages> <month> September </month> <year> 1990. </year>
Reference: [17] <author> H.K.N. Leung and L.J. White. </author> <title> Insights into regression testing. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1989, </booktitle> <pages> pages 60-69, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Regression testing is an important testing activity that can account for a large proportion of the cost of software maintenance <ref> [5, 17] </ref>. Regression testing is performed on modified software to provide confidence that the software behaves correctly and that modifications have not adversely impacted the software's quality. One approach to reducing the cost of regression testing is to employ a selective regression testing technique.
Reference: [18] <author> H.K.N. Leung and L.J. White. </author> <title> A study of integration testing and software regression at the integration level. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1990, </booktitle> <pages> pages 290-300, </pages> <month> November </month> <year> 1990. </year>
Reference: [19] <author> H.K.N. Leung and L.J. White. </author> <title> A cost model to compare regression test strategies. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1991, </booktitle> <pages> pages 201-208, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: One of these predictors is used to predict whether a safe selective regression testing strategy (one that selects all test cases that cover affected entities) will be cost-effective. Using the regression testing cost model of Leung and White <ref> [19] </ref>, Rosenblum and Weyuker demonstrate the usefulness of this predictor by describing the results of a case study they performed involving 31 versions of the KornShell [23]. In that study, the predictor reported that, on average, it was expected that 87.3% of the test cases would be selected. <p> Their model builds on work by Leung and White on modeling the cost of employing a regression testing method <ref> [19] </ref>. In both models, the total cost of regression testing incorporates two factors: the cost of executing test cases, and the cost of performing analyses to support test selection. A number of simplifying assumptions are made in the representation of the cost in these models: 1.
Reference: [20] <author> T.J. Ostrand and M.J. Balcer. </author> <title> The category-partition method for specifying and generating functional tests. </title> <journal> Communications of the ACM, </journal> <volume> 31(6), </volume> <month> June </month> <year> 1988. </year>
Reference-contexts: initial set of black-box test cases "according to good testing practices, based on the tester's understanding of the program's functionality and knowledge of special values and boundary points that are easily observable in the code" [14, p. 194], using the category partition method and the Siemens Test Specification Language tool <ref> [2, 20] </ref>. They then augmented this set with manually-created white-box test cases to ensure that each executable statement, edge, and definition-use pair in the base program or its control flow graph was exercised by at least 30 test cases.
Reference: [21] <author> T.J. Ostrand and E.J. Weyuker. </author> <title> Using dataflow analysis for regression testing. </title> <booktitle> In Sixth Annual Pacific Northwest Software Quality Conference, </booktitle> <pages> pages 233-247, </pages> <month> September </month> <year> 1988. </year>
Reference: [22] <author> D. Rosenblum and G. Rothermel. </author> <title> An empirical comparison of regression test selection techniques. </title> <booktitle> In Proceedings of the International Workshop for Empirical Studies of Software Maintenance, </booktitle> <pages> pages 89-94, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: Thus, many of the statements in printtokens2 are actually executed by fewer than 50% of the test cases that enter their enclosing functions. When modifications occur in these less-frequently executed statements, DejaVu selects much smaller test suites than TestTube. (For further empirical comparison of TestTube and DejaVu, see <ref> [22] </ref>.) This is the case for approximately half of the modified versions of printtokens2 utilized in this study.
Reference: [23] <author> D. S. Rosenblum and E. J. Weyuker. </author> <title> Using coverage information to predict the cost-effectiveness of regression testing strategies. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 23(3) </volume> <pages> 146-156, </pages> <month> March </month> <year> 1997. </year>
Reference-contexts: For an overview and analytical comparison of these techniques see [24]. 1 other studies performed independently by Rosenblum and Weyuker with a different selective regression test-ing algorithm, implemented as a tool called TestTube [8], also show that such methods are not always cost-effective <ref> [23] </ref>. When selective regression testing is not efficient, the resources spent performing the test case selection are wasted. Thus, Rosenblum and Weyuker argue in [23] that it would be desirable to have a predictor that is inexpensive to apply but could indicate whether or not using a selective regression testing method <p> Rosenblum and Weyuker with a different selective regression test-ing algorithm, implemented as a tool called TestTube [8], also show that such methods are not always cost-effective <ref> [23] </ref>. When selective regression testing is not efficient, the resources spent performing the test case selection are wasted. Thus, Rosenblum and Weyuker argue in [23] that it would be desirable to have a predictor that is inexpensive to apply but could indicate whether or not using a selective regression testing method is likely to be worthwhile. With this motivation, Rosenblum and Weyuker [23] propose coverage-based predictors for use in predicting the cost-effectiveness of selective regression <p> Thus, Rosenblum and Weyuker argue in <ref> [23] </ref> that it would be desirable to have a predictor that is inexpensive to apply but could indicate whether or not using a selective regression testing method is likely to be worthwhile. With this motivation, Rosenblum and Weyuker [23] propose coverage-based predictors for use in predicting the cost-effectiveness of selective regression testing strategies. <p> Using the regression testing cost model of Leung and White [19], Rosenblum and Weyuker demonstrate the usefulness of this predictor by describing the results of a case study they performed involving 31 versions of the KornShell <ref> [23] </ref>. In that study, the predictor reported that, on average, it was expected that 87.3% of the test cases would be selected. Using the TestTube approach, 88.1% were actually selected on average over the 31 versions. <p> The authors explain, however, that because of the way their selective regression testing model employs averages, the accuracy of their predictor might vary significantly in practice from version to version. In particular, this is an issue if there is a wide variation in the distribution of changes among entities <ref> [23] </ref>. However, because their predictor is intended to be used for predicting the long-term behavior of a method over multiple versions, they argue that the use of averages is acceptable. <p> In the following sections we discuss the results of our studies. 2 The Rosenblum-Weyuker Regression Test Selection Model and Predictor Rosenblum and Weyuker present a formal model of regression testing to support the definition and computation of predictors of cost-effectiveness <ref> [23] </ref>. Their model builds on work by Leung and White on modeling the cost of employing a regression testing method [19]. In both models, the total cost of regression testing incorporates two factors: the cost of executing test cases, and the cost of performing analyses to support test selection. <p> The models view cost-effectiveness as being an inherent attribute of test selection over the complete maintenance life-cycle, rather than an attribute of individual versions. As in Rosenblum and Weyuker's model <ref> [23] </ref>, we let P denote the system under test and let T denote the regression test suite for P , with jT j denoting the size of T . <p> N C CC Then the fraction of the test suite that must be rerun is denoted M , the predictor for jT M j=jT j: M = M = jE C jjT j Rosenblum and Weyuker <ref> [23] </ref> discuss results of a case study in which test selection and prediction results are compared for 31 versions of the KornShell using the TestTube selective regression testing method. <p> In the earlier KornShell study <ref> [23] </ref>, it was determined that the relation covers M (t; e) changes very little during maintenance. <p> Assume that the patterns are generalized over a large number of entities, n. As discussed by Rosenblum and Weyuker <ref> [23] </ref>, the value of M predicted for each pattern is 2=n.
Reference: [24] <author> G. Rothermel and M.J. Harrold. </author> <title> Analyzing regression test selection techniques. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 22(8) </volume> <pages> 529-551, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: For an overview and analytical comparison of these techniques see <ref> [24] </ref>. 1 other studies performed independently by Rosenblum and Weyuker with a different selective regression test-ing algorithm, implemented as a tool called TestTube [8], also show that such methods are not always cost-effective [23]. <p> During the preliminary phase, changes are made to the software, and the new version of the software is built. During the critical phase, the new version of the software is tested prior to its release to customers <ref> [24] </ref>. 17 6 Acknowledgments This work was supported in part by a grant from Microsoft, Inc., by NSF under NYI award CCR-9696157 to Ohio State University, CAREER award CCR-9703108 to Oregon State University, and Experimental Software Systems award CCR-9707792 to Ohio State University and Oregon State University, and by an Ohio
Reference: [25] <author> G. Rothermel and M.J. Harrold. </author> <title> A safe, efficient regression test selection technique. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 6(2) </volume> <pages> 173-210, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Empirical results obtained by Rothermel and Harrold on the effectiveness of their selective regression testing algorithms, implemented as a tool called DejaVu, suggest that test selection can be effective in reduc ing the cost of regression testing by reducing the number of test cases that need to be rerun <ref> [25] </ref>. However, these studies also show that there are cases for which their algorithm is not cost-effective.
Reference: [26] <author> B. Sherlund and B. Korel. </author> <title> Logical modification oriented software testing. </title> <booktitle> In Proceedings: Twelfth International Conference on Testing Computer Software, </booktitle> <month> June </month> <year> 1995. </year>
Reference: [27] <author> A.B. Taha, S.M. Thebaut, and S.S. Liu. </author> <title> An approach to software fault localization and revalidation based on incremental data flow analysis. </title> <booktitle> In Proceedings of the 13th Annual International Computer Software and Applications Conference, </booktitle> <pages> pages 527-534, </pages> <month> September </month> <year> 1989. </year>
Reference: [28] <author> F. Vokolos and P. Frankl. </author> <title> Pythia: A regression test selection tool based on textual differencing. </title> <booktitle> In ENCRESS '97, Third International Conference on Reliability, Quality, and Safety of Software Intensive Systems, </booktitle> <month> May </month> <year> 1997. </year>
Reference: [29] <author> S.S. Yau and Z. Kishimoto. </author> <title> A method for revalidating modified programs in the maintenance phase. </title> <booktitle> In COMPSAC '87: The Eleventh Annual International Computer Software and Applications Conference, </booktitle> <pages> pages 272-277, </pages> <month> October </month> <year> 1987. </year> <month> 19 </month>
References-found: 29

