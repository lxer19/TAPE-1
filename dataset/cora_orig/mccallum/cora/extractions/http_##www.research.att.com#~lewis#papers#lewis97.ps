URL: http://www.research.att.com/~lewis/papers/lewis97.ps
Refering-URL: http://www.research.att.com/~lewis/chronobib.html
Root-URL: 
Email: lewis@research.att.com  
Title: The TREC-5 Filtering Track  
Author: David D. Lewis E. M. Voorhees and D. K. Harman, eds., 
Date: 1997, pp. 75-96.  
Note: Appeared (with different pagination) in  The Fifth Text Retrieval Conference (TREC-5). National  
Web: http://www.research.att.com/~lewis  
Address: 600 Mountain Avenue, 2A-410 Murray Hill, NJ 07974  Gaithersburg, MD,  
Affiliation: AT&T Labs|Research  Information Technology:  Institute of Standards and Technology,  
Abstract: The TREC-5 filtering track, an evaluation of binary text classification systems, was a repeat of the filtering evaluation run in a trial version for TREC-4, with only the data set and participants changing. Seven sites took part, submitting a total of ten runs. We review the nature of the task, the effectiveness measures and evaluation methods used, and briefly discuss the results. Some deficiencies in the evaluation are examined, with an eye toward improving future filtering evaluations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> William G. Cochran. </author> <title> Sampling Techniques. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <note> 3rd edition, </note> <year> 1977. </year>
Reference-contexts: The TREC-5 filtering evaluation estimated utilities using stratified sampling. In stratified sampling we use additional knowledge about a population to divide the population into groups or strata <ref> [1, p. 89] </ref>. We then take a simple random sample separately from each stratum, estimate the quantity of interest for each stratum, and combine the stratum estimates to get an overall estimate for the population. <p> We then add up the estimated stratum proportions, weighting them by the relative size of their stratum in the submitted set <ref> [1, p. 91] </ref>: X N h ^p h (7) Here h ranges over the strata that make up the R i set, N h is the size of stratum h, N i is the size of the R i set, and ^p h is an estimate of the proportion of relevant <p> This ^p h is an unbiased estimate of p h <ref> [1, p. 51] </ref>. 5.3 Sample Sizes in Stratified Sampling for TREC-5 To be consistent with the TREC-5 routing evaluation, at most 100 documents were judged from the three sets of documents submitted by a site for each filtering topic. <p> A common measure of the distance between an estimate, ^, and the quantity we want to estimate, , is the mean square error (MSE) <ref> [1, p. 15] </ref>. <p> We cannot know the exact variance of this estimate without knowing the actual value of p, which is of course what we are trying to estimate in the first place. However, an unbiased estimate of the variance of our estimate of the proportion is <ref> [1, p. 52] </ref>: d Var [^p] = (n 1)N = (n 1)N a fi n (N n)a (n a) (29) Suppose we used a simple random sample of size n i from set R i to estimate the utility of set R i . <p> proportion of relevant in each stratum and combine these estimates to get an estimate of the overall proportion: ^p i = h2R i N i By the properties of the variance of linear combinations of random variables, and the fact that our samples from the strata are independent, we have <ref> [1, p. 92] </ref>: Var [^p i ] = h2R i 2 2 Var [^p h ] (32) Each ^p h is an estimate of the proportion of relevant in a stratum, based on a simple random sample from the stratum. <p> a smaller MSE than an estimate based on simple random sampling when: X N h (N h n h )a h (n h a h ) &lt; n 2 (n 1) This is almost always true when reasonable strata are defined and appropriately sized samples are chosen from those strata <ref> [1, p. 99] </ref>.
Reference: [2] <author> William S. Cooper. </author> <title> On selecting a measure of retrieval effectiveness. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 24 </volume> <pages> 87-100, </pages> <month> March-April </month> <year> 1973. </year>
Reference-contexts: The expense of running the filtering track was thereby reduced, since there was considerable overlap between filtering and routing submissions. 4.4 The Utility Measures The family of effectiveness measures used in the filtering track were based on assigning a numeric value or utility to each retrieved document <ref> [2, 9] </ref>. Retrieved relevant documents received a positive utility, and retrieved nonrelevant documents received a negative utility.
Reference: [3] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: In fact, if the document scores are accurate estimates of probability of relevance, the thresholds to use can be derived directly from the effectiveness measure by decision theoretic principles <ref> [3, Ch. 2] </ref>.
Reference: [4] <author> Donna Harman. </author> <title> Overview of the fourth Text REtrieval Conference (TREC-4). </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Fourth Text REtrieval Conference (TREC-4), </booktitle> <address> Gaithersburg, MD, </address> <year> 1996. </year> <institution> U. S. Dept. of Commerce, National Institute of Standards and Technology. </institution>
Reference-contexts: For that reason, total utility for filtering runs was estimated using samples of the submitted documents. Two different sampling and estimation methods were used in the TREC-5 filtering track, as described below. 5.1 Pooling The first approach to sampling was the usual TREC pooling strategy <ref> [4] </ref>. This approach assumes that some known pool of documents contains all the relevant documents in the test set.
Reference: [5] <author> Marti Hearst, Jan Pedersen, Peter Pirolli, Hinrich Schutze, Gregory Grefenstette, and David Hull. </author> <title> Xerox site report: Four TREC-4 tracks. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Fourth Text REtrieval Conference (TREC-4), </booktitle> <pages> pages 97-119, </pages> <address> Gaithersburg, MD, </address> <year> 1996. </year> <institution> U. S. Dept. of Commerce, National Institute of Standards and Technology. </institution>
Reference-contexts: A second motivation is that the demands of the filtering task may encourage the development of IR methods with other desirable properties. For instance, accurately estimating the probability of relevance of documents is useful not only in filtering <ref> [7, 9, 5] </ref>, but also for self-monitoring of effectiveness [9], estimating the number of relevant documents [9], and selection of training data [11]. Finally, we hope that a binary classification task will attract a broader range of researchers and approaches to TREC.
Reference: [6] <author> Paul S. Jacobs. </author> <title> GE in TREC-2: Results of a boolean approximation method for routing and retrieval. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Second Text Retrieval Conference (TREC-2), </booktitle> <pages> pages 191-199, </pages> <address> Gaithersburg, MD, </address> <month> March </month> <year> 1994. </year> <institution> U. S. Dept. of Commerce, National Institute of Standards and Technology. </institution> <note> NIST Special Publication 500-215. </note>
Reference-contexts: Finally, we hope that a binary classification task will attract a broader range of researchers and approaches to TREC. The requirement that TREC results be ranked makes it awkward for approaches that are not ranking-oriented to be tried <ref> [6, 14] </ref>. These approaches include boolean querying by human experts, as well as the use of binary classifiers (e.g. decision trees) produced by machine learning techniques. 4.2 Structure The structure of the TREC-5 filtering evaluation was as follows.
Reference: [7] <author> K. L. Kwok, L. Grunfeld, and D. D. Lewis. TREC-3 ad-hoc, </author> <title> routing retrieval and thresholding experiments using PIRCS. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> Overview of the Third Text REtrieval Conference (TREC-3), </booktitle> <pages> pages 247-255, </pages> <address> Gaithersburg, MD, </address> <month> April </month> <year> 1995. </year> <institution> U. S. Dept. of Commerce, National Institute of Standards and Technology. </institution>
Reference-contexts: A second motivation is that the demands of the filtering task may encourage the development of IR methods with other desirable properties. For instance, accurately estimating the probability of relevance of documents is useful not only in filtering <ref> [7, 9, 5] </ref>, but also for self-monitoring of effectiveness [9], estimating the number of relevant documents [9], and selection of training data [11]. Finally, we hope that a binary classification task will attract a broader range of researchers and approaches to TREC.
Reference: [8] <author> David D. Lewis. </author> <title> Evaluating text categorization. </title> <booktitle> In Proceedings of Speech and Natural Language Workshop, </booktitle> <pages> pages 312-318. </pages> <publisher> Defense Advanced Research Projects Agency, Morgan Kaufmann, </publisher> <month> February </month> <year> 1991. </year>
Reference-contexts: The system makes Yes/No decisions about documents, and the user only sees the results of those decisions. This affects the kind of evaluation appropriate for the system, as discussed in the next section. 3 Evaluation for Binary Text Classification We have discussed evaluation for binary classification at length elsewhere <ref> [8, 9] </ref>, and so here will concentrate on how it differs from the evaluation of ranked retrieval in the main TREC-5 tasks. Effectiveness measures for ranked retrieval typically have two components.
Reference: [9] <author> David D. Lewis. </author> <title> Evaluating and optimizing autonomous text classification systems. </title> <editor> In Ed-ward A. Fox, Peter Ingwersen, and Raya Fidel, editors, </editor> <booktitle> SIGIR '95: Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 246-254, </pages> <address> New York, </address> <year> 1995. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: by briefly discussing the results of the TREC-5 filtering track, with an emphasis on what was learned about the evaluation methods. 2 Binary Text Classification By binary text classification systems, we mean information retrieval (IR) systems that decide for each document processed whether the document should be accepted or rejected <ref> [9] </ref>. What it means to be accepted varies between systems. Some applications that make use of binary text classification are: * A company provides an SDI (selective dissemination of information) service which filters newswire feeds. <p> The system makes Yes/No decisions about documents, and the user only sees the results of those decisions. This affects the kind of evaluation appropriate for the system, as discussed in the next section. 3 Evaluation for Binary Text Classification We have discussed evaluation for binary classification at length elsewhere <ref> [8, 9] </ref>, and so here will concentrate on how it differs from the evaluation of ranked retrieval in the main TREC-5 tasks. Effectiveness measures for ranked retrieval typically have two components. <p> Doing so will result in an optimal score under essentially any reasonable measure of ranking effectivenes, a property which has been formalized as the Probability Ranking Principle <ref> [9, 13] </ref>. In contrast, binary classification systems make the separation into accepted and rejected documents themselves, rather than leaving this up to the effectiveness measure used in evaluation. The binary classification system must choose what separation to make in order to optimize the effectiveness measure used. <p> A second motivation is that the demands of the filtering task may encourage the development of IR methods with other desirable properties. For instance, accurately estimating the probability of relevance of documents is useful not only in filtering <ref> [7, 9, 5] </ref>, but also for self-monitoring of effectiveness [9], estimating the number of relevant documents [9], and selection of training data [11]. Finally, we hope that a binary classification task will attract a broader range of researchers and approaches to TREC. <p> A second motivation is that the demands of the filtering task may encourage the development of IR methods with other desirable properties. For instance, accurately estimating the probability of relevance of documents is useful not only in filtering [7, 9, 5], but also for self-monitoring of effectiveness <ref> [9] </ref>, estimating the number of relevant documents [9], and selection of training data [11]. Finally, we hope that a binary classification task will attract a broader range of researchers and approaches to TREC. <p> For instance, accurately estimating the probability of relevance of documents is useful not only in filtering [7, 9, 5], but also for self-monitoring of effectiveness <ref> [9] </ref>, estimating the number of relevant documents [9], and selection of training data [11]. Finally, we hope that a binary classification task will attract a broader range of researchers and approaches to TREC. The requirement that TREC results be ranked makes it awkward for approaches that are not ranking-oriented to be tried [6, 14]. <p> The expense of running the filtering track was thereby reduced, since there was considerable overlap between filtering and routing submissions. 4.4 The Utility Measures The family of effectiveness measures used in the filtering track were based on assigning a numeric value or utility to each retrieved document <ref> [2, 9] </ref>. Retrieved relevant documents received a positive utility, and retrieved nonrelevant documents received a negative utility. <p> Unlike recall, but like precision, our utility measures take into account only those documents submitted by the system for a run. (It is possible to define utility measures to take into account rejected documents as well <ref> [9] </ref>.) Unlike both recall and precision, the total utility is not normalized to lie between 0 and 1. Indeed, the minimum and maximum achievable utilities can be determined only if the total number of relevant documents in the test set is known. <p> Utility measures which are the sum of utilities for individual documents, like the one used in the TREC-5 filtering evaluation, can be optimized by thresholding if the scores computed are monotonic with probability of class membership <ref> [9] </ref>. In fact, if the document scores are accurate estimates of probability of relevance, the thresholds to use can be derived directly from the effectiveness measure by decision theoretic principles [3, Ch. 2].
Reference: [10] <author> David D. Lewis. </author> <title> The TREC-4 filtering track. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Fourth Text REtrieval Conference (TREC-4), </booktitle> <pages> pages 165-180, </pages> <address> Gaithersburg, MD, </address> <year> 1996. </year> <institution> U. S. Dept. of Commerce, National Institute of Standards and Technology. </institution>
Reference-contexts: The design used in the TREC-5 filtering track was identical to that tested with four sites in TREC-4, thus much of this paper is identical to the description of the TREC-4 filtering track <ref> [10] </ref>. We begin by defining binary text classification and presenting some applications of it. We then discuss a particular binary text classification task, filtering, used in TREC-5. The effectiveness of filtering submissions was evaluated using utility as a measure.
Reference: [11] <author> David D. Lewis and William A. Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <editor> In W. Bruce Croft and C. J. van Rijsbergen, editors, </editor> <booktitle> SIGIR 94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 3-12, </pages> <address> London, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: For instance, accurately estimating the probability of relevance of documents is useful not only in filtering [7, 9, 5], but also for self-monitoring of effectiveness [9], estimating the number of relevant documents [9], and selection of training data <ref> [11] </ref>. Finally, we hope that a binary classification task will attract a broader range of researchers and approaches to TREC. The requirement that TREC results be ranked makes it awkward for approaches that are not ranking-oriented to be tried [6, 14].
Reference: [12] <author> David S. Moore and George P. McCabe. </author> <title> Introduction to the Practice of Statistics. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: MSE [^u 1 ] = Var [^u i ]. Making the usually reasonable assumption that ^u i has a roughly normal distribution, then a 95% confidence interval around ^u i is <ref> [12, ch. 7] </ref>: ^u i 1:96 Var [^u i ]: Then combining Equations 20-22 with Equations 36-38, and using the above expression for the confidence interval gives: u 1 = 2:672 12:3 u 3 = 717:2 287:1 We of course arranged this example so that the true utilities (Equations 17 to
Reference: [13] <author> S. E. Robertson. </author> <title> The probability ranking principle in IR. </title> <journal> Journal of Documentation, </journal> <volume> 33(4) </volume> <pages> 294-304, </pages> <month> December </month> <year> 1977. </year>
Reference-contexts: Doing so will result in an optimal score under essentially any reasonable measure of ranking effectivenes, a property which has been formalized as the Probability Ranking Principle <ref> [9, 13] </ref>. In contrast, binary classification systems make the separation into accepted and rejected documents themselves, rather than leaving this up to the effectiveness measure used in evaluation. The binary classification system must choose what separation to make in order to optimize the effectiveness measure used.
Reference: [14] <author> Richard M. Tong and Lee A. Appelbaum. </author> <title> Machine learning for knowledge-based document routing (a report on the TREC-2 experiment). </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Second Text Retrieval Conference (TREC-2), </booktitle> <pages> pages 253-264, </pages> <address> Gaithersburg, MD, </address> <month> March </month> <year> 1994. </year> <institution> U. S. Dept. of Commerce, National Institute of Standards and Technology. </institution> <note> NIST Special Publication 500-215. </note>
Reference-contexts: Finally, we hope that a binary classification task will attract a broader range of researchers and approaches to TREC. The requirement that TREC results be ranked makes it awkward for approaches that are not ranking-oriented to be tried <ref> [6, 14] </ref>. These approaches include boolean querying by human experts, as well as the use of binary classifiers (e.g. decision trees) produced by machine learning techniques. 4.2 Structure The structure of the TREC-5 filtering evaluation was as follows.
References-found: 14

