URL: ftp://ftp.stat.berkeley.edu/pub/users/breiman/games.ps.Z
Refering-URL: http://www.research.att.com/~schapire/boost.html
Root-URL: 
Email: leo@stat.berkeley.edu  
Title: PREDICTION GAMES AND ARCING ALGORITHMS  
Author: Leo Breiman 
Abstract: Technical Report 504 December 19, 1997 Statistics Department University of California Berkeley, CA. (4720 Abstract The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost (Freund and Schapire [1995].[1996]) and others in reducing generalization error has not been well understood. By formulating prediction, both classification and regression, as a game where one player makes a selection from instances in the training set and the other a convex linear combination of predictors from a finite set, existing arcing algorithms are shown to be algorithms for finding good game strategies. An optimal game strategy finds a combined predictor that minimizes the maximum of the error over the training set. A bound on the generalization error for the combined predictors in terms of their maximum error is proven that is sharper than bounds to date. Arcing algorithms are described that converge to the optimal strategy. Schapire et.al. [1997] offered an explanation of why Adaboost works in terms of its ability to reduce the margin. Comparing Adaboost to our optimal arcing algorithm shows that their explanation is not valid and that the answer lies elsewhere. In this situation the VC-type bounds are misleading. Some empirical results are given to explore the situation. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bauer, E. and Kohavi, </author> <title> R.[1998]An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting and Variants, </title> <journal> Machine Learning 1-33. </journal> <volume> 2 8 Breiman, </volume> <editor> L. </editor> <title> [1997] Bias, Variance, and Arcing Classifiers, </title> <type> Technical Report 460, </type> <institution> Statistics Depar tment, </institution> <note> University of California (available at www.stat.berkeley.edu) Breiman, </note> <editor> L. </editor> <booktitle> [1996b] Bagging predictors , Machine Learning 26, </booktitle> <volume> No. 2, </volume> <pages> pp. 123-140 Drucker, </pages> <editor> H. </editor> <title> [1997] Improving Regressors using Boosting Techniques, </title> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <month> 107-115.. </month>
Reference: <author> Drucker, H. and Cortes, C. </author> <title> [1995] Boosting decision trees, </title> <booktitle> Advances in Neural Information Processing Systems Vol. </booktitle> <volume> 8, </volume> <pages> pp. 479-485. </pages>
Reference: <author> Freund, Y. and Schapire, R. </author> <title> [1995] A decision-theoretic generalization of online learning and an application to boosting. </title> <note> to appear , Journal of Computer and System Sciences. </note>

Reference: <author> Quinlan, J.R.[1996] Bagging, </author> <title> Boosting, </title> <booktitle> and C4.5, Proceedings of AAAI'96 National Conference on Artificial Intelligence, </booktitle> <pages> pp. 725-730. </pages> <editor> Robinson, </editor> <title> J.[1951] An iterative method of solving a game, </title> <journal> Ann. Math 154, </journal> <pages> pp. 296-301 Schapire, </pages> <note> R.,Freund, </note> <author> Y., Bartlett, P., and Lee, </author> <note> W[1997] Boosting the Margin, (available at http://www.research.att.com/~yoav look under "publications".) 2 9 Szep, </note> <author> J. and Forgo, </author> <title> F.[1985] Introduction to the Theory of Games, </title> <address> D. </address> <publisher> Reidel Publishing Co. </publisher>
References-found: 4

