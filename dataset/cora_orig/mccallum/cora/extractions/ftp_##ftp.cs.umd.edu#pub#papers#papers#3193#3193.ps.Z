URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3193/3193.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: wak@cs.umd.edu pugh@cs.umd.edu  
Title: A Framework for Unifying Reordering Transformations  
Author: Wayne Kelly William Pugh 
Note: This work is supported by an NSF PYI grant CCR-9157384 and by a Packard Fellowship.  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies Dept. of Computer Science Dept. of Computer Science Univ. of Maryland,  
Date: November, 1992  Revised April, 1993  
Pubnum: UMIACS-TR-93-134  CS-TR-3193  
Abstract: We present a framework for unifying iteration reordering transformations such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering. The framework is based on the idea that a transformation can be represented as a schedule that maps the original iteration space to a new iteration space. The framework is designed to provide a uniform way to represent and reason about transformations. As part of the framework, we provide algorithms to assist in the building and use of schedules. In particular, we provide algorithms to test the legality of schedules, to align schedules and to generate optimized code for schedules. 
Abstract-found: 1
Intro-found: 1
Reference: [ACK87] <author> R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Conference Record of the Fourteenth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 63-76, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: By generalizing in these ways, we can represent a much broader set of reordering transformations, including any transformation that can be obtained by some combination of: * loop interchange * loop reversal * loop skewing, * statement reordering * loop distribution * loop fusion * loop alignment <ref> [ACK87] </ref> * loop interleaving [ST92] * loop blocking 1 (or tiling) [AK87] * index set splitting 1 [Ban79] * loop coalescing 1 [Pol88] * loop scaling 1 [LP92] 1.2 Examples 1.3 Overview Our framework is designed to provide a uniform way to represent and reason about reordering transformations.
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Optimizing compilers reorder iterations of statements to improve instruction scheduling, register use, and cache utilization, and to expose parallelism. Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Pol88, Wol89b, Wol90, CK92] </ref>. Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code. <p> a much broader set of reordering transformations, including any transformation that can be obtained by some combination of: * loop interchange * loop reversal * loop skewing, * statement reordering * loop distribution * loop fusion * loop alignment [ACK87] * loop interleaving [ST92] * loop blocking 1 (or tiling) <ref> [AK87] </ref> * index set splitting 1 [Ban79] * loop coalescing 1 [Pol88] * loop scaling 1 [LP92] 1.2 Examples 1.3 Overview Our framework is designed to provide a uniform way to represent and reason about reordering transformations. The framework itself is not designed to decide which transformation should be applied. <p> Therefore, we require that the schedule components be specified level by level starting at the first level, (i.e., during step k, f k 1 ; : : : ; f k p are specified). This strategy, is in some ways a generalization of Allen and Kennedy's codegen algorithm <ref> [AK87] </ref>. 5.2 Ensuring legality If we use T k p to represent the schedule: [i 1 m p p ; : : : ; f k then after each step k we require that: 8i; j; p; q; Sym i ! j 2 d pq ) T k q (j) (3)
Reference: [AKPW83] <author> J.R. Allen, K. Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Conf. Rec. Tenth ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 177-189, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: We can represent simple relations containing non-convex constraints such as f [i] ! [i] j i even g by introducing wildcard variables (denoted by greek letters): f [i] ! [i] j 9ff s:t: i = 2ff g. 2.3 Control dependence We require that conditionals be removed using if-conversion <ref> [AKPW83] </ref> and that all loop bounds be affine functions of surrounding loop variables and symbolic constants. All control dependences can therefore be implicitly represented by describing the iteration space using a set of linear inequalities on the loop variables and symbolic constants.
Reference: [B + 89] <author> M. Berry et al. </author> <title> The PERFECT Club benchmarks: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: The framework does however provide some algorithms that would aid the surrounding system in its task. 1 Our current implementation cannot handle all cases of these transformations. 2 Code adapted from OLDA in Perfect club (TI) <ref> [B + 89] </ref> LU Decomposition without pivoting Original code do 20 mp = 1, np do 20 mi = 1, morb 10 xrsiq (mi,mq)=xrsiq (mi,mq)+ $ xrspq ((mp-1)*mp/2+mq)*v (mp,mi) 20 xrsiq (mi,mp)=xrsiq (mi,mp)+ $ xrspq ((mp-1)*mp/2+mq)*v (mq,mi) Original code do 20 k = 1, n 10 a (i,k) = a (i,k)
Reference: [Ban79] <author> U. Banerjee. </author> <title> Speedup of Ordinary Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, U. of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: transformations, including any transformation that can be obtained by some combination of: * loop interchange * loop reversal * loop skewing, * statement reordering * loop distribution * loop fusion * loop alignment [ACK87] * loop interleaving [ST92] * loop blocking 1 (or tiling) [AK87] * index set splitting 1 <ref> [Ban79] </ref> * loop coalescing 1 [Pol88] * loop scaling 1 [LP92] 1.2 Examples 1.3 Overview Our framework is designed to provide a uniform way to represent and reason about reordering transformations. The framework itself is not designed to decide which transformation should be applied.
Reference: [Ban90] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proc. of the 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 192-219, </pages> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code. Unimodular transformations <ref> [Ban90, WL91a] </ref> go some way towards solving this problem. Unimodular transformations is a unified framework that is able to describe any transformation that can be obtained by composing loop interchange, loop skewing, and loop reversal. <p> Note: Unimodular transformations include loop interchange, skewing and reversal <ref> [Ban90, WL91b] </ref>. <p> Using permutable schedules to generate optimized code is also easy because we know that all permutations are legal, so we can concentrate on performance issues while ignoring legality. 9 Related Work The framework of Unimodular transformations <ref> [Ban90, WL91a, ST92, KKB92] </ref> has the same goal as our work, in that it attempts to provide a unified framework for describing loop transformations.
Reference: [CK92] <author> Steve Carr and Ken Kennedy. </author> <title> Compiler blockability of numerical algorithms. </title> <booktitle> In Proceedings Supercomputing'92, </booktitle> <pages> pages 114-125, </pages> <address> Minneapolis, Minnesota, </address> <month> Nov </month> <year> 1992. </year> <month> 22 </month>
Reference-contexts: 1 Introduction Optimizing compilers reorder iterations of statements to improve instruction scheduling, register use, and cache utilization, and to expose parallelism. Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Pol88, Wol89b, Wol90, CK92] </ref>. Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code.
Reference: [Fea92a] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part I, One--dimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(5), </volume> <month> Oct </month> <year> 1992. </year> <note> Postscript available as pub.ibp.fr:ibp/reports/masi.92/78.ps.Z. </note>
Reference-contexts: It uses less sophisticated methods for aligning schedules than our current techniques, and does not give methods to generate efficient code. Paul Feautrier <ref> [Fea92a, Fea92b] </ref> generates the same type of schedules that we do (generating a separate schedule for each statement). His methods are designed to generate a single schedule that produces code with a "maximal" amount of parallelism.
Reference: [Fea92b] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part II, Multidimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(6), </volume> <month> Dec </month> <year> 1992. </year> <note> Postscript available as pub.ibp.fr:ibp/reports/masi.92/28.ps.Z. </note>
Reference-contexts: It uses less sophisticated methods for aligning schedules than our current techniques, and does not give methods to generate efficient code. Paul Feautrier <ref> [Fea92a, Fea92b] </ref> generates the same type of schedules that we do (generating a separate schedule for each statement). His methods are designed to generate a single schedule that produces code with a "maximal" amount of parallelism.
Reference: [JDH90] <author> I. Duff J.J. Dongarra, J. DuCroz and S. Hammarling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans. on Math. Soft., </journal> <volume> 16 </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: required normally * index set splitting * loop distribution * triangular loop interchange * loop fusion Transformations required normally * strip mining * index set splitting * loop distribution * imperfectly nested triangular loop interchange Code adapted from CHOSOL in the Perfect club (SD) Banded SYR2K [LP92] adapted from BLAS <ref> [JDH90] </ref> Original code do 30 i=2,n do 20 j=1,i-1 30 b (i) = b (i) - sum (i) Original code do 10 i = 1, n do 10 k = max (i-b+1,j-b+1,1),min (i+b-1,j+b-1,n) $ alpha*A (k,i-k+b)*B (k,j-k+b) + $ alpha*A (k,j-k+b)*B (k,i-k+b) Schedule (for parallelism) T 10 : f [ i
Reference: [KKB92] <author> K. G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Deriving good transformations for mapping nested loops on hieracical parallel machines in polynomial time. </title> <booktitle> In Proc. of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 82-92, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Using permutable schedules to generate optimized code is also easy because we know that all permutations are legal, so we can concentrate on performance issues while ignoring legality. 9 Related Work The framework of Unimodular transformations <ref> [Ban90, WL91a, ST92, KKB92] </ref> has the same goal as our work, in that it attempts to provide a unified framework for describing loop transformations.
Reference: [LP92] <author> Wei Li and Keshav Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 249-260, </pages> <institution> Yale University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: combination of: * loop interchange * loop reversal * loop skewing, * statement reordering * loop distribution * loop fusion * loop alignment [ACK87] * loop interleaving [ST92] * loop blocking 1 (or tiling) [AK87] * index set splitting 1 [Ban79] * loop coalescing 1 [Pol88] * loop scaling 1 <ref> [LP92] </ref> 1.2 Examples 1.3 Overview Our framework is designed to provide a uniform way to represent and reason about reordering transformations. The framework itself is not designed to decide which transformation should be applied. <p> max (k+1,iB), iB+63 Transformations required normally * index set splitting * loop distribution * triangular loop interchange * loop fusion Transformations required normally * strip mining * index set splitting * loop distribution * imperfectly nested triangular loop interchange Code adapted from CHOSOL in the Perfect club (SD) Banded SYR2K <ref> [LP92] </ref> adapted from BLAS [JDH90] Original code do 30 i=2,n do 20 j=1,i-1 30 b (i) = b (i) - sum (i) Original code do 10 i = 1, n do 10 k = max (i-b+1,j-b+1,1),min (i+b-1,j+b-1,n) $ alpha*A (k,i-k+b)*B (k,j-k+b) + $ alpha*A (k,j-k+b)*B (k,i-k+b) Schedule (for parallelism) T 10 <p> It can therefore not represent some important transformations such as loop fusion, loop distribution and statement reordering. Unimodular transformations are generalized in <ref> [LP92, Ram92] </ref> to include mappings that are invertable but not unimodular. This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Unimodular transformations are combined with blocking in [WL91a, ST92].
Reference: [Lu91] <author> Lee-Chung Lu. </author> <title> A unified framework for systematic loop transformations. </title> <booktitle> In Proc. of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 28-38, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: include a constant term (possibly symbolic). * We require the schedules to be invertable, but not necessarily unimodular (i.e., 1-1 but not necessarily onto). * We allow the dimensionality of the old and new iteration spaces to be different. * We allow the schedules to be piecewise (as suggested by <ref> [Lu91] </ref>): we can specify a schedule T p as S i T pi j C pi where the T pi j C pi 's are schedules with disjoint domains. * We allow the f j 0 s to be functions that include integer division and modular operations provided the denominator is <p> This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Unimodular transformations are combined with blocking in [WL91a, ST92]. A similar approach, although not using a unimodular framework, is described in [Wol89a]. Lu describes in <ref> [Lu91] </ref> a classification of scheduling techniques into various generality classes. Using their classification scheme, our schedules fit into the Mixed-Nonuniform class which is the most general class.
Reference: [Pol88] <author> C. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction Optimizing compilers reorder iterations of statements to improve instruction scheduling, register use, and cache utilization, and to expose parallelism. Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Pol88, Wol89b, Wol90, CK92] </ref>. Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code. <p> can be obtained by some combination of: * loop interchange * loop reversal * loop skewing, * statement reordering * loop distribution * loop fusion * loop alignment [ACK87] * loop interleaving [ST92] * loop blocking 1 (or tiling) [AK87] * index set splitting 1 [Ban79] * loop coalescing 1 <ref> [Pol88] </ref> * loop scaling 1 [LP92] 1.2 Examples 1.3 Overview Our framework is designed to provide a uniform way to represent and reason about reordering transformations. The framework itself is not designed to decide which transformation should be applied.
Reference: [Pug91] <author> William Pugh. </author> <title> Uniform techniques for loop optimization. </title> <booktitle> In 1991 International Conference on Supercomputing, </booktitle> <pages> pages 341-352, </pages> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: These free variables correspond to symbolic constants or parameters in the source program. We use Sym to represent the set of all symbolic constants. Table 1 gives a brief description of the operations on integer tuple sets and relations that we have implemented. 4 See <ref> [Pug91] </ref> for a more thorough description. 2.2 Simple relations Internally, a relation is represented as the union of a set of simple relations: relations that can be described by the conjunction of a set of linear constraints. <p> A similar approach, although not using a unimodular framework, is described in [Wol89a]. Lu describes in [Lu91] a classification of scheduling techniques into various generality classes. Using their classification scheme, our schedules fit into the Mixed-Nonuniform class which is the most general class. Our previous paper <ref> [Pug91] </ref> gives techniques to represent loop fusion, loop distribution and statement reordering in addition to the transformations representable by unimodular transformations.
Reference: [Pug92] <author> William Pugh. </author> <title> The Omega test: a fast and practical integer programming algorithm for dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 8 </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: For our purposes, these abstractions are too crude. We evaluate and represent dependences exactly using linear constraints over integer variables. We use the Omega test <ref> [Pug92, PW92] </ref> to manipulate and simplify these constraints. This approach allows us to accurately compose dependences and provides more information about which transformations are allowable.
Reference: [PW92] <author> William Pugh and David Wonnacott. </author> <title> Going beyond integer programming with the Omega test to eliminate false data dependences. </title> <type> Technical Report CS-TR-3191, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> December </month> <year> 1992. </year> <note> An earlier version of this paper appeared at the SIGPLAN PLDI'92 conference. </note>
Reference-contexts: In Section 6 we describe our code generation algorithm. This algorithm takes a schedule and produces optimized code corresponding to the transformation represented by that schedule. By making use of the gist operation <ref> [PW92] </ref> we are able to produce code with a minimal number of conditionals and loop bounds. In Section 7 we extend our schedule syntax to allow us to denote the fact that a schedule produces fully-permutable loop nests [WL91a, WL91b]. <p> For our purposes, these abstractions are too crude. We evaluate and represent dependences exactly using linear constraints over integer variables. We use the Omega test <ref> [Pug92, PW92] </ref> to manipulate and simplify these constraints. This approach allows us to accurately compose dependences and provides more information about which transformations are allowable. <p> A rough approximation of transitive closure works well for the applications to which we put it (the approximation has to be a lower bound on the transitive closure). 2.6 The gist and approx operations We make use of the gist operation that was originally developed in <ref> [PW92] </ref>. Intuitively, (gist p given q) is defined as the new information contained in p, given that we already know q.
Reference: [Qui87] <author> Patrice Quinton. </author> <title> The systematic design of systolic arrays. </title> <editor> In F. Fogelman, Y. Robert, and M. Tschuente, editors, </editor> <booktitle> Automata networks in Computer Science, </booktitle> <pages> pages 229-260. </pages> <publisher> Manchester University Press, </publisher> <month> December </month> <year> 1987. </year>
Reference-contexts: 1 2 2 + c 1 , 8n (9c 1 1 s:t: c 1 1 c 1 , 8n T rue 12 5.5.2 A complete technique If we wish to construct exactly the set of constraints described by Equation 9 then we can use the following techniques proposed by Quinton <ref> [Qui87] </ref>. The vertex method [Qui87] relies on the fact that a linear constraint holds everywhere inside a polyhedron if and only if it holds at all vertices of the polyhedron. <p> c 1 , 8n (9c 1 1 s:t: c 1 1 c 1 , 8n T rue 12 5.5.2 A complete technique If we wish to construct exactly the set of constraints described by Equation 9 then we can use the following techniques proposed by Quinton <ref> [Qui87] </ref>. The vertex method [Qui87] relies on the fact that a linear constraint holds everywhere inside a polyhedron if and only if it holds at all vertices of the polyhedron.
Reference: [Ram92] <author> J. Ramanujam. </author> <title> Non-unimodular transformations of nested loops. </title> <booktitle> In Supercomputing `92, </booktitle> <pages> pages 214-223, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: It can therefore not represent some important transformations such as loop fusion, loop distribution and statement reordering. Unimodular transformations are generalized in <ref> [LP92, Ram92] </ref> to include mappings that are invertable but not unimodular. This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Unimodular transformations are combined with blocking in [WL91a, ST92].
Reference: [ST92] <author> Vivek Sarkar and Radhika Thekkath. </author> <title> A general framework for iteration-reordering loop transformations. </title> <booktitle> In ACM SIGPLAN'92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 175-187, </pages> <address> San Francisco, California, </address> <month> Jun </month> <year> 1992. </year>
Reference-contexts: generalizing in these ways, we can represent a much broader set of reordering transformations, including any transformation that can be obtained by some combination of: * loop interchange * loop reversal * loop skewing, * statement reordering * loop distribution * loop fusion * loop alignment [ACK87] * loop interleaving <ref> [ST92] </ref> * loop blocking 1 (or tiling) [AK87] * index set splitting 1 [Ban79] * loop coalescing 1 [Pol88] * loop scaling 1 [LP92] 1.2 Examples 1.3 Overview Our framework is designed to provide a uniform way to represent and reason about reordering transformations. <p> Using permutable schedules to generate optimized code is also easy because we know that all permutations are legal, so we can concentrate on performance issues while ignoring legality. 9 Related Work The framework of Unimodular transformations <ref> [Ban90, WL91a, ST92, KKB92] </ref> has the same goal as our work, in that it attempts to provide a unified framework for describing loop transformations. <p> Unimodular transformations are generalized in [LP92, Ram92] to include mappings that are invertable but not unimodular. This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Unimodular transformations are combined with blocking in <ref> [WL91a, ST92] </ref>. A similar approach, although not using a unimodular framework, is described in [Wol89a]. Lu describes in [Lu91] a classification of scheduling techniques into various generality classes. Using their classification scheme, our schedules fit into the Mixed-Nonuniform class which is the most general class.
Reference: [WL91a] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code. Unimodular transformations <ref> [Ban90, WL91a] </ref> go some way towards solving this problem. Unimodular transformations is a unified framework that is able to describe any transformation that can be obtained by composing loop interchange, loop skewing, and loop reversal. <p> By making use of the gist operation [PW92] we are able to produce code with a minimal number of conditionals and loop bounds. In Section 7 we extend our schedule syntax to allow us to denote the fact that a schedule produces fully-permutable loop nests <ref> [WL91a, WL91b] </ref>. Given a permutable schedule, it is easy to reorder or tile the loops for parallelism and locality without continual concern about legality. In Section 8 we discuss surrounding systems and the interface between surrounding systems and our framework. <p> [i; k; k]; T 20 : [k; i; j] ! [i; k; j] T 10 : [k; i] ! [k; k; i]; T 20 : [k; i; j] ! [j; k; i] This situation is common and is typified by a nested set of adjacent loops that are fully permutable <ref> [WL91a, WL91b] </ref>. A set of loops is fully permutable iff all permutations of the loops are legal. We have developed an extension to our schedule syntax that allows us to succinctly describe a large number of 18 related schedules. Expressions in this extended syntax are called permutable schedules. <p> [k; i; j] ! [hk; j; ii 1 ] We use the term permutation list set to refer to a set of permutation lists with the same subscript. 7.1 Schedules for blocking/tiling Direct generation of blocked or tiled loops is only possible if there exists a fully permutable loop nest <ref> [WL91a, Wol89a] </ref>. A permutable schedule represents a set of schedules, all of which produce fully permutable loop nests. It is therefore easy to build a schedule corresponding to a blocking transformation from a permutable schedule. <p> Using permutable schedules to generate optimized code is also easy because we know that all permutations are legal, so we can concentrate on performance issues while ignoring legality. 9 Related Work The framework of Unimodular transformations <ref> [Ban90, WL91a, ST92, KKB92] </ref> has the same goal as our work, in that it attempts to provide a unified framework for describing loop transformations. <p> Unimodular transformations are generalized in [LP92, Ram92] to include mappings that are invertable but not unimodular. This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Unimodular transformations are combined with blocking in <ref> [WL91a, ST92] </ref>. A similar approach, although not using a unimodular framework, is described in [Wol89a]. Lu describes in [Lu91] a classification of scheduling techniques into various generality classes. Using their classification scheme, our schedules fit into the Mixed-Nonuniform class which is the most general class.
Reference: [WL91b] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> In IEEE Transactions on Parallel and Distributed Systems, </journal> <month> July </month> <year> 1991. </year>
Reference-contexts: By making use of the gist operation [PW92] we are able to produce code with a minimal number of conditionals and loop bounds. In Section 7 we extend our schedule syntax to allow us to denote the fact that a schedule produces fully-permutable loop nests <ref> [WL91a, WL91b] </ref>. Given a permutable schedule, it is easy to reorder or tile the loops for parallelism and locality without continual concern about legality. In Section 8 we discuss surrounding systems and the interface between surrounding systems and our framework. <p> Note: Unimodular transformations include loop interchange, skewing and reversal <ref> [Ban90, WL91b] </ref>. <p> [i; k; k]; T 20 : [k; i; j] ! [i; k; j] T 10 : [k; i] ! [k; k; i]; T 20 : [k; i; j] ! [j; k; i] This situation is common and is typified by a nested set of adjacent loops that are fully permutable <ref> [WL91a, WL91b] </ref>. A set of loops is fully permutable iff all permutations of the loops are legal. We have developed an extension to our schedule syntax that allows us to succinctly describe a large number of 18 related schedules. Expressions in this extended syntax are called permutable schedules.
Reference: [Wol89a] <author> Michael Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proc. Supercomputing 89, </booktitle> <pages> pages 655-664, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: [k; i; j] ! [hk; j; ii 1 ] We use the term permutation list set to refer to a set of permutation lists with the same subscript. 7.1 Schedules for blocking/tiling Direct generation of blocked or tiled loops is only possible if there exists a fully permutable loop nest <ref> [WL91a, Wol89a] </ref>. A permutable schedule represents a set of schedules, all of which produce fully permutable loop nests. It is therefore easy to build a schedule corresponding to a blocking transformation from a permutable schedule. <p> This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Unimodular transformations are combined with blocking in [WL91a, ST92]. A similar approach, although not using a unimodular framework, is described in <ref> [Wol89a] </ref>. Lu describes in [Lu91] a classification of scheduling techniques into various generality classes. Using their classification scheme, our schedules fit into the Mixed-Nonuniform class which is the most general class.
Reference: [Wol89b] <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Optimizing compilers reorder iterations of statements to improve instruction scheduling, register use, and cache utilization, and to expose parallelism. Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Pol88, Wol89b, Wol90, CK92] </ref>. Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code.
Reference: [Wol90] <author> Michael Wolfe. </author> <title> Massive parallelism through program restructuring. </title> <booktitle> In Symposium on Frontiers on Massively Parallel Computation, </booktitle> <pages> pages 407-415, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Optimizing compilers reorder iterations of statements to improve instruction scheduling, register use, and cache utilization, and to expose parallelism. Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Pol88, Wol89b, Wol90, CK92] </ref>. Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code.
Reference: [Wol91] <author> Michael Wolfe. </author> <title> The tiny loop restructuring research tool. </title> <booktitle> In Proc of 1991 International Conference on Parallel Processing, </booktitle> <address> pages II-46 II-53, </address> <year> 1991. </year> <month> 23 </month>
Reference-contexts: Michael Wolfe notes that this transformation requires imperfect triangular loop interchange, distribution, and index set splitting <ref> [Wol91] </ref>.
References-found: 26

