URL: http://www.cse.ogi.edu/~denni/Publications/jetnet30.ps.Z
Refering-URL: http://www.cse.ogi.edu/~denni/publications.html
Root-URL: http://www.cse.ogi.edu
Title: LU TP 93-29  JETNET 3.0 A Versatile Artificial Neural Network Package  
Author: Carsten Peterson and Thorsteinn Rognvaldsson Leif Lonnblad 
Note: Submitted to Computer Physics Communications  
Address: Solvegatan 14 A, S-223 62 Lund, Sweden  CH 1211 Geneva 23, Switzerland  
Affiliation: Department of Theoretical Physics, University of Lund,  Theory Division, CERN,  
Date: December 1993  
Pubnum: CERN-TH.7135/94  
Abstract: PROGRAM SUMMARY Title of Program: JETNET version 3.0 Catalogue number: Program obtainable from: denni@thep.lu.se or via anonymous ftp from thep.lu.se in directory pub/Jetnet/ or from freehep.scri.fsu.edu in directory freehep/analysis/jetnet. Computer for which the programme is designed: DEC Alpha, DECstation, SUN, Apollo, VAX, IBM, Hewlett-Packard, and others with a F77 compiler Computer: DEC Alpha 3000; installation: Department of Theoretical Physics, University of Lund, Lund, Sweden Operating system: DEC OSF 1.3 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Pattern Recognition in High Energy Physics with Artificial Neural Networks", </title> <journal> Comput. Phys. Commun. </journal> <volume> 70, </volume> <month> 167 </month> <year> (1992). </year>
Reference-contexts: To date, the most commonly used architectures and procedures are the Multilayer Perceptron (MLP) with backpropagation updating and self-organizing networks. Both these approaches were implemented in JETNET 2.0. For the self-organizing networks nothing is changed in JETNET 3.0 and we refer the reader to refs. <ref> [1, 4] </ref> for information on this part. For the MLP the most important additions and changes concern additional learning algorithm variants, learning parameters and various tools for gauging performance and estimating error surfaces. <p> Hidden units The standard choice is the sigmoid function, eq. (4), either in symmetric <ref> [1; 1] </ref> or asymmetric [0; 1] form. The sigmoid function is global in the sense that it divides the feature space into two halves, one where the response is approaching 1 and another where it is approaching 0 (1). <p> Hidden units The standard choice is the sigmoid function, eq. (4), either in symmetric [1; 1] or asymmetric <ref> [0; 1] </ref> form. The sigmoid function is global in the sense that it divides the feature space into two halves, one where the response is approaching 1 and another where it is approaching 0 (1). Hence it is very efficient for making sweeping cuts in the feature space. <p> For translational symmetries one can use so-called "receptive fields", in which the input field is divided into subfields with shared weights <ref> [1] </ref>. Also, if it is known that the important feature only occupies a small part of the input field, then one can use "selective fields", which is essentially the same as "receptive fields" without the shared weights property. <p> Normalization of the input is done to prevent "stiffness", i.e. when weights need to be updated with very different learning rates. Two simple normalization options are; either scale the inputs to the range <ref> [0; 1] </ref>, or translate them to their mean values and rescale to unit variance. The former method is useful if the data is more or less evenly distributed over a limited range, whereas the latter is useful when the data contains outliers. <p> We strongly advocate the use of an on-line updating procedure where MSTJN (2) is small. Routinely we use ten patterns per update for most applications 17 occasionally an order of magnitude more. The learning rate is the parameter that requires most attention. Typical initial values are in the range <ref> [0:1; 1] </ref> and it is usually profitable to scale the learning rate in inverse proportion to the fan-in of the units so that different learning rates are used for different weight layers. The momentum should be in the range [0,1]. For HEP problems momentum values above 0.5 are seldom required. <p> The above solution is inefficient for large input matrices, since all weights are nevertheless updated. 4.9 The Saturation Measure The saturation s is defined as s = j (1 2 h 2 P j if g (x) 2 <ref> [1; 1] </ref> and measures the resolution of the units. An s-value close to unity signals that the unit has "made up its mind" whereas an s value close to zero means that it is still learning. <p> The subroutines associated with the feed-forward net all begin with the letters JN, as in JetNet, whereas the self-organizing map 23 subroutines all start with JM, as in JetMap. We will not discuss any of the JetMap subroutines and components here since they are basically unchanged from JETNET 2.0 <ref> [1] </ref>. JETNET 3.0 is backwards compatible with earlier versions. 5.1 Feed-forward Network (JN) 5.1.1 Main subroutines The main routines are JNINIT, JNTRAL, JNTEST, JNDUMP and JNREAD (JNROLD). These are usually the only routines the user has to invoke. The network is initialized by setting switches in /JNDAT1/ and calling JNINIT. <p> scale-up factor fl + used in Rprop PARJN (31) (D=0.5) scale-down factor fl used in Rprop PARJN (32) (D=50.0) maximum scale-up factor in Rprop PARJN (33) (D=10 6 ) minimum scale-down factor in Rprop PARJN (34-40) not used All parameters can be changed at any time by the user. (See <ref> [1] </ref> for descriptions on switches MSTJM and parameters PARJM.) OIN is the vector used to pass the values of the input nodes to the program. 29 OUT is a vector used both to pass the desired value of the output nodes to the program during supervised training and to pass the
Reference: [2] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Using Neural Networks to Identify Jets", Nucl. </title> <journal> Phys. </journal> <volume> B 349, </volume> <month> 675 </month> <year> (1991). </year>
Reference: [3] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Finding Gluon Jets with a Neural trigger", </title> <journal> Phys. Rev. Lett. </journal> <volume> 65, </volume> <month> 1321 </month> <year> (1990). </year>
Reference: [4] <author> L. Lonnblad, C. Peterson, H. Pi and T. Rognvaldsson, </author> <title> "Self-organizing Networks for Extracting Jet Features", </title> <journal> Comput. Phys. Commun. </journal> <volume> 67, </volume> <month> 193 </month> <year> (1991). </year>
Reference-contexts: To date, the most commonly used architectures and procedures are the Multilayer Perceptron (MLP) with backpropagation updating and self-organizing networks. Both these approaches were implemented in JETNET 2.0. For the self-organizing networks nothing is changed in JETNET 3.0 and we refer the reader to refs. <ref> [1, 4] </ref> for information on this part. For the MLP the most important additions and changes concern additional learning algorithm variants, learning parameters and various tools for gauging performance and estimating error surfaces. <p> It is sometimes argued that statistical non-parametric methods, like decision trees etc., are preferable to ANN models since the former are easier to interpret. We disagree with this view. With the aid of a self-organizing network it is quite easy to interpret an ANN model <ref> [4] </ref>. 3.1.2 Choice of ANN Model Classification In classification problems, the task is to model the decision boundary between a set of distributions in the feature space [34]. This decision boundary is a surface of dimension N 1, where N is the number of relevant features/inputs.
Reference: [5] <author> D. E. Rumelhart, G. E. Hinton and R. J. Williams, </author> <title> "Learning Internal Representations by Error Propagation", </title> <editor> in D. E. Rumelhart and J. L. McClelland (eds.) </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition (Vol. </booktitle> <volume> 1), </volume> <publisher> MIT Press (1986). </publisher>
Reference-contexts: For the MLP the most important additions and changes concern additional learning algorithm variants, learning parameters and various tools for gauging performance and estimating error surfaces. The following learning algorithms are included in JETNET 3.0: * Standard Gradient Descent (back-propagation) <ref> [5] </ref> * Langevin Updating [6] 3 * Conjugate Gradient [7] * Scaled Conjugate Gradient [8] * "Quickprop" [9] * "Rprop" [10] Also, among other things, the following options are included. * Dynamic Learning Rates * Saturation Measurement * Computation and Monitoring of Hessian Eigenvalues * Limited Precision Besides a full description <p> It amounts to updating the weights according to the back-propagation (BP) learning rule <ref> [5] </ref> ! t+1 = ! t + ! t (6) where ! t = @! Here ! refers to the whole vector of weights and thresholds used in the network 1 .
Reference: [6] <author> T. Rognvaldsson, </author> <title> "On Langevin Updating in Multilayer Perceptrons", </title> <note> Lund Preprint LU TP 93-13 (to appear in Neural Comput.) </note> <year> (1994). </year>
Reference-contexts: For the MLP the most important additions and changes concern additional learning algorithm variants, learning parameters and various tools for gauging performance and estimating error surfaces. The following learning algorithms are included in JETNET 3.0: * Standard Gradient Descent (back-propagation) [5] * Langevin Updating <ref> [6] </ref> 3 * Conjugate Gradient [7] * Scaled Conjugate Gradient [8] * "Quickprop" [9] * "Rprop" [10] Also, among other things, the following options are included. * Dynamic Learning Rates * Saturation Measurement * Computation and Monitoring of Hessian Eigenvalues * Limited Precision Besides a full description of the functionality and <p> Initial "flat-spot" problems and local minima can to a large extent be avoided by introducing noise to the gradient descent updating rule of eq. (7). This is conveniently done by adding a properly normalized Gaussian noise term <ref> [6] </ref> ! = rE + (9) which we refer to as Langevin updating, or by using the more crude non-strict gradient descent procedure provided by the Manhattan [17] updating rule 2 ! = sgn [ @! 2.2 Second-Order Algorithms Gradient descent assumes a flat metric where the learning rate in eq. <p> On-line BP is much faster than batch mode BP. For networks with more than one hidden layer it is beneficial to use the Langevin updating variant (eq. (9)), where noise is added to the BP equations <ref> [6] </ref>. This is because the Hessian matrix easily becomes ill-conditioned with a flat subspace where the random search in Langevin updating is very efficient as compared to other alternatives. 3.2.2 Quickprop In using "Quickprop" [9] we frequently encounter problems with getting a stable performance. <p> However, its superb performance in this test is related to its use of individual learning rates. Normalizing the data in the way described below makes BP perform as well, if not better <ref> [6] </ref>. 3.2.4 Conjugate Gradients In ref. [7] the CG method outperformed BP on the parity problem. However, our experience with CG on HEP problems is the opposite; it is often unable to find the true global minimum. <p> In our view, LV is the most powerful of all the algorithms for networks with many hidden layers, even though it requires somewhat more CPU time <ref> [6, 49] </ref>. <p> The noise level used in LV updating should also decrease with time, preferably faster than the learning rate. We use an exponential decay governed by the scale parameter PARJN (20). This procedure is sufficient to significantly improve the learning for networks with many hidden layers <ref> [6] </ref>. From the perspective of simulated annealing and global optimization, an exponentially decreasing noise level can also be justified given that the simulation time is finite [74]. Also implemented in JETNET 3.0 are options of having the momentum and the temperature change each epoch.
Reference: [7] <author> E. M. Johansson, F. U. Dowla and D. M. Goodman, </author> <title> "Backpropagation Learning for Multilayer Feed-forward Neural Networks using the Conjugate Gradient Method", </title> <journal> Int. J. Neur. Syst. </journal> <volume> 2, </volume> <month> 291 </month> <year> (1992). </year>
Reference-contexts: For the MLP the most important additions and changes concern additional learning algorithm variants, learning parameters and various tools for gauging performance and estimating error surfaces. The following learning algorithms are included in JETNET 3.0: * Standard Gradient Descent (back-propagation) [5] * Langevin Updating [6] 3 * Conjugate Gradient <ref> [7] </ref> * Scaled Conjugate Gradient [8] * "Quickprop" [9] * "Rprop" [10] Also, among other things, the following options are included. * Dynamic Learning Rates * Saturation Measurement * Computation and Monitoring of Hessian Eigenvalues * Limited Precision Besides a full description of the functionality and the use of the various <p> We refer the reader to <ref> [7] </ref> and [23] for a thorough discussion on these matters. <p> However, its superb performance in this test is related to its use of individual learning rates. Normalizing the data in the way described below makes BP perform as well, if not better [6]. 3.2.4 Conjugate Gradients In ref. <ref> [7] </ref> the CG method outperformed BP on the parity problem. However, our experience with CG on HEP problems is the opposite; it is often unable to find the true global minimum. The same conclusion was reached in an extensive benchmark test of different ANN learning algorithms [59].
Reference: [8] <author> M. F. Mtller, </author> <title> "A Scaled Conjugate Gradient Algorithm for Fast Supervised Learning", </title> <booktitle> Neural Networks 6, </booktitle> <month> 525 </month> <year> (1993). </year>
Reference-contexts: The following learning algorithms are included in JETNET 3.0: * Standard Gradient Descent (back-propagation) [5] * Langevin Updating [6] 3 * Conjugate Gradient [7] * Scaled Conjugate Gradient <ref> [8] </ref> * "Quickprop" [9] * "Rprop" [10] Also, among other things, the following options are included. * Dynamic Learning Rates * Saturation Measurement * Computation and Monitoring of Hessian Eigenvalues * Limited Precision Besides a full description of the functionality and the use of the various JETNET 3.0 subroutines this writeup <p> We refer the reader to [7] and [23] for a thorough discussion on these matters. The line search part of CG minimization can be tricky and there exists a variant, Scaled Conjugate Gradient (SCG) <ref> [8] </ref>, that avoids the line search by estimating the minimization step t through t = d t (s t + t d t ) where is a fudge factor to make the denominator positive and s is a difference approximation of Hd.
Reference: [9] <author> S. E. Fahlman, </author> <title> "An Empirical Study of Learning Speed in Back-propagation Networks", </title> <institution> Carnegie-Mellon Computer Science Rpt. </institution> <month> CMU-CS-88-162 </month> <year> (1988). </year>
Reference-contexts: The following learning algorithms are included in JETNET 3.0: * Standard Gradient Descent (back-propagation) [5] * Langevin Updating [6] 3 * Conjugate Gradient [7] * Scaled Conjugate Gradient [8] * "Quickprop" <ref> [9] </ref> * "Rprop" [10] Also, among other things, the following options are included. * Dynamic Learning Rates * Saturation Measurement * Computation and Monitoring of Hessian Eigenvalues * Limited Precision Besides a full description of the functionality and the use of the various JETNET 3.0 subroutines this writeup also contains a <p> One therefore has to resort to approximate methods. Below, we discuss those approximate methods that are implemented in JETNET 3.0 an extensive review of second order methods for ANN is found in [19]. 2.2.1 Heuristic Methods One well-known method to approximate the curvature information is the Quickprop (QP) algorithm <ref> [9] </ref>, where the basic idea is to estimate the weight changes by assuming a parabolic shape for the error surface. The weight changes are then modified by the use of heuristic rules to ensure downhill motion at all times. <p> This is because the Hessian matrix easily becomes ill-conditioned with a flat subspace where the random search in Langevin updating is very efficient as compared to other alternatives. 3.2.2 Quickprop In using "Quickprop" <ref> [9] </ref> we frequently encounter problems with getting a stable performance. <p> These are the "learning rate" PARJN (1) (), the sigmoid-prime addition PARJN (23) ("), the maximum growth factor PARJN (21) and the maximum weight magnitude PARJN (22). The latter is quite unimportant. Default values recommended in <ref> [9] </ref> for the other three parameters are; PARJN (1) of order unity, PARJN (23)= 0.1 and PARJN (21) = 1.75. We have not done extensive tests to to determine the problem dependence of these parameters.
Reference: [10] <author> M. Riedmiller and H. Braun, </author> <title> "A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm", </title> <booktitle> Proc. ICNN, </booktitle> <address> San Fransisco (1993). </address>
Reference-contexts: The following learning algorithms are included in JETNET 3.0: * Standard Gradient Descent (back-propagation) [5] * Langevin Updating [6] 3 * Conjugate Gradient [7] * Scaled Conjugate Gradient [8] * "Quickprop" [9] * "Rprop" <ref> [10] </ref> Also, among other things, the following options are included. * Dynamic Learning Rates * Saturation Measurement * Computation and Monitoring of Hessian Eigenvalues * Limited Precision Besides a full description of the functionality and the use of the various JETNET 3.0 subroutines this writeup also contains a set of "rules-of-thumb" <p> The algorithm is also restarted if the weights grow too large [20]. Another heuristic method, suggested by several authors <ref> [10, 21, 22] </ref>, is the use of individual learning rates for each weight that are adjusted according to how "well" the actual weight is doing. Ideally, these individual learning rates adjust to the curvature of the error surface and reflect the inverse of the Hessian. <p> Ideally, these individual learning rates adjust to the curvature of the error surface and reflect the inverse of the Hessian. In our view, the most promising of these schemes is Rprop <ref> [10] </ref>.
Reference: [11] <author> B. Denby, </author> <title> "Neural Networks and Cellular Automata in Experimental High Energy Physics", </title> <journal> Comput. Phys. Commun. </journal> <volume> 49, </volume> <month> 429 </month> <year> (1988). </year>
Reference-contexts: However, we emphasize that in addition to feature recognition and function mapping there are ANN applications in HEP that require feed-back networks, which are not included in this package. In particular, we think of optimization networks used for track finding <ref> [11, 12, 13, 14, 15, 16] </ref>. This write-up is organized as follows. In Sect. 2 we very briefly discuss the basic steps and variants when using feed-forward networks for learning. Discussions and prescriptions on what methods to use in various situations are found in Sect. 3.
Reference: [12] <author> C. Peterson, </author> <title> "Track Finding with Neural Networks", </title> <institution> Nucl. Instrum. Methods A279, </institution> <month> 537 </month> <year> (1989). </year>
Reference-contexts: However, we emphasize that in addition to feature recognition and function mapping there are ANN applications in HEP that require feed-back networks, which are not included in this package. In particular, we think of optimization networks used for track finding <ref> [11, 12, 13, 14, 15, 16] </ref>. This write-up is organized as follows. In Sect. 2 we very briefly discuss the basic steps and variants when using feed-forward networks for learning. Discussions and prescriptions on what methods to use in various situations are found in Sect. 3.
Reference: [13] <author> M. Gyulassy and H. Harlander, </author> <title> "Elastic Tracking and Neural Network Algorithms for Complex Pattern Recognition", </title> <journal> Comput. Phys. Commun. </journal> <volume> 66, </volume> <month> 31 </month> <year> (1991). </year>
Reference-contexts: However, we emphasize that in addition to feature recognition and function mapping there are ANN applications in HEP that require feed-back networks, which are not included in this package. In particular, we think of optimization networks used for track finding <ref> [11, 12, 13, 14, 15, 16] </ref>. This write-up is organized as follows. In Sect. 2 we very briefly discuss the basic steps and variants when using feed-forward networks for learning. Discussions and prescriptions on what methods to use in various situations are found in Sect. 3.
Reference: [14] <author> A. Yuille, K. Honda and C. Peterson, </author> <title> "Particle Tracking by Deformable Templates", </title> <booktitle> Proceedings of 1991 IEEE INNS International Joint Conference on Neural Networks, </booktitle> <volume> Vol. 1, </volume> <pages> pp 7-12, </pages> <address> Seattle, WA (July 1991) </address>
Reference-contexts: However, we emphasize that in addition to feature recognition and function mapping there are ANN applications in HEP that require feed-back networks, which are not included in this package. In particular, we think of optimization networks used for track finding <ref> [11, 12, 13, 14, 15, 16] </ref>. This write-up is organized as follows. In Sect. 2 we very briefly discuss the basic steps and variants when using feed-forward networks for learning. Discussions and prescriptions on what methods to use in various situations are found in Sect. 3.
Reference: [15] <author> M. Ohlsson, C. Peterson and A. Yuille, </author> <title> "Track Finding with Deformable Templates The Elastic Arms Approach", </title> <journal> Comput. Phys. Commun. </journal> <volume> 71, </volume> <month> 77 </month> <year> (1992). </year>
Reference-contexts: However, we emphasize that in addition to feature recognition and function mapping there are ANN applications in HEP that require feed-back networks, which are not included in this package. In particular, we think of optimization networks used for track finding <ref> [11, 12, 13, 14, 15, 16] </ref>. This write-up is organized as follows. In Sect. 2 we very briefly discuss the basic steps and variants when using feed-forward networks for learning. Discussions and prescriptions on what methods to use in various situations are found in Sect. 3.
Reference: [16] <author> M. Ohlsson, </author> <title> "Extensions and Explorations of the Elastic Arms Algorithm", </title> <journal> Comput. Phys. Commun. </journal> <volume> 77, </volume> <month> 19 </month> <year> (1992). </year>
Reference-contexts: However, we emphasize that in addition to feature recognition and function mapping there are ANN applications in HEP that require feed-back networks, which are not included in this package. In particular, we think of optimization networks used for track finding <ref> [11, 12, 13, 14, 15, 16] </ref>. This write-up is organized as follows. In Sect. 2 we very briefly discuss the basic steps and variants when using feed-forward networks for learning. Discussions and prescriptions on what methods to use in various situations are found in Sect. 3.
Reference: [17] <author> C. Peterson and E. Hartman, </author> <title> "Explorations of the Mean Field Theory Learning Algorithm", </title> <booktitle> Neural Networks 2, 475 (1989). </booktitle> <pages> 39 </pages>
Reference-contexts: This is conveniently done by adding a properly normalized Gaussian noise term [6] ! = rE + (9) which we refer to as Langevin updating, or by using the more crude non-strict gradient descent procedure provided by the Manhattan <ref> [17] </ref> updating rule 2 ! = sgn [ @! 2.2 Second-Order Algorithms Gradient descent assumes a flat metric where the learning rate in eq. (7) is identical in all directions in !-space.
Reference: [18] <author> S. Saarinen, R. Bramley and G. Cybenko, </author> <title> "Ill-conditioning in Neural Network Training Prob--lem", </title> <journal> SIAM J. Sci. Comp. </journal> <volume> 14, </volume> <month> 693 </month> <year> (1993). </year>
Reference-contexts: Also, H is often singular or ill-conditioned <ref> [18] </ref>, in which case the Newton method breaks down. One therefore has to resort to approximate methods.
Reference: [19] <author> R. Battiti, </author> <title> "First- and Second-Order Methods for Learning: Between Steepest Descent and Newton's Method", </title> <journal> Neural Comput. </journal> <volume> 4, </volume> <month> 141 </month> <year> (1992). </year>
Reference-contexts: Also, H is often singular or ill-conditioned [18], in which case the Newton method breaks down. One therefore has to resort to approximate methods. Below, we discuss those approximate methods that are implemented in JETNET 3.0 an extensive review of second order methods for ANN is found in <ref> [19] </ref>. 2.2.1 Heuristic Methods One well-known method to approximate the curvature information is the Quickprop (QP) algorithm [9], where the basic idea is to estimate the weight changes by assuming a parabolic shape for the error surface.
Reference: [20] <author> A. C. Veitch and G. Holmes, </author> <title> "A Modified Quickprop Algorithm", </title> <journal> Neural Comput. </journal> <volume> 3, </volume> <month> 310 </month> <year> (1991). </year>
Reference-contexts: To prevent the weights from growing too large, which indicates that QP is going wrong, a maximum scale is set on the weight update and it is recommended to use a weight decay term (see below). The algorithm is also restarted if the weights grow too large <ref> [20] </ref>. Another heuristic method, suggested by several authors [10, 21, 22], is the use of individual learning rates for each weight that are adjusted according to how "well" the actual weight is doing.
Reference: [21] <author> R. Jacobs, </author> <title> "Increased Rates of Convergence Through Learning Rate Adaption", </title> <booktitle> Neural Networks 1, </booktitle> <month> 295 </month> <year> (1988). </year>
Reference-contexts: The algorithm is also restarted if the weights grow too large [20]. Another heuristic method, suggested by several authors <ref> [10, 21, 22] </ref>, is the use of individual learning rates for each weight that are adjusted according to how "well" the actual weight is doing. Ideally, these individual learning rates adjust to the curvature of the error surface and reflect the inverse of the Hessian.
Reference: [22] <author> T. Tollenaere, "SuperSAB: </author> <title> Fast Adaptive Backpropagation with Good Scaling Properties", </title> <booktitle> Neural Networks 3, </booktitle> <month> 561 </month> <year> (1990). </year>
Reference-contexts: The algorithm is also restarted if the weights grow too large [20]. Another heuristic method, suggested by several authors <ref> [10, 21, 22] </ref>, is the use of individual learning rates for each weight that are adjusted according to how "well" the actual weight is doing. Ideally, these individual learning rates adjust to the curvature of the error surface and reflect the inverse of the Hessian.
Reference: [23] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky and W. T. Vetterling, </author> <title> Numerical Recipes, </title> <publisher> Cam-bridge Univ. Press, </publisher> <address> Cambridge UK (1986). </address>
Reference-contexts: 0 &lt; fl &lt; 1 &lt; fl + . 2.2.2 Conjugate Gradients A somewhat different technique to use the (approximately) correct metric, without direct computation of the Hessian, is the method of Conjugate Gradients (CG), where E is iteratively minimized 7 within separate one-dimensional subspaces of !-space (see e.g. ref. <ref> [23] </ref>). The updating hence reads ! t = t d t (15) where the step length is chosen, by employing a line search, such that E is minimized along the direction d. <p> We refer the reader to [7] and <ref> [23] </ref> for a thorough discussion on these matters. <p> Most importantly, an identical set of training patterns must be used for each evaluation of the error, otherwise the line search gets confused. 4.4.1 The line search Although inspired by algorithms in <ref> [23, 73] </ref>, the line search algorithm implemented in JETNET 3.0 is somewhat unorthodox. <p> All parameters used in the line search are stored in the common block /JNINT4/. * SUBROUTINE JNSCGR Controls the Scaled Conjugate Gradient training. It calls the subroutine JNCGBE. * SUBROUTINE JNTRED and SUBROUTINE JNTQLI These routines are taken directly from <ref> [23] </ref> and are used to diagonalize the Hessian matrix and compute its eigenvectors and eigenvalues. 5.1.4 Internal functions * INTEGER FUNCTION JNINDX (IL,I,J) Gives the node vector index of node I in layer IL if J=0, otherwise it gives the weight vector 26 index of the weight between node I in
Reference: [24] <author> M. D. Richard and R. P. Lippmann, </author> <title> "Neural Network Classifiers Estimate Bayesian a posteriori Probabilities", </title> <journal> Neural Comput. </journal> <volume> 3, </volume> <month> 461 </month> <year> (1991). </year>
Reference-contexts: However, one can interpret these outputs in a very useful way; they correspond to Bayesian a posteriori probabilities <ref> [24] </ref> provided that: 1. The training is accurate. 2. The outputs are of 1-of-M -type (the task is coded such that only one output unit is "on" at a time). 3. A mean square, cross entropy or Kullback error function is used. 4. <p> Derivatives with respect to the inputs can therefore be computed, which simplifies error estimation. * As discussed above the output nodes approximate the Bayes a posteriori probabilities <ref> [24] </ref>, which are useful to make final decisions that minimize the overall risk [34]. * Sigmoid units are not "orthogonal" and two hidden units may well perform identical tasks, which to some extent avoids overfitting.
Reference: [25] <author> S. Geman, E. Bienenstock and R. Doursat, </author> <title> "Neural Networks and the Bias/Variance Dilemma", </title> <journal> Neural Comput. </journal> <volume> 4, </volume> <month> 1 </month> <year> (1992). </year>
Reference-contexts: In this sense they are more powerful than parametric methods that try to fit reality into a specific parametric form. However, non-parametric methods like ANN contain more free parameters and hence require more training data than parametric ones in order to achieve good generalization performance <ref> [25] </ref>. Fortunately, for most HEP problems one has access to big data samples, making it possible to exploit the capabilities of non-parametric models like ANN. Tests of ANN versus standard methods on pattern recognition HEP problems are therefore in favour of ANN models [26, 27, 28, 29, 30, 31].
Reference: [26] <author> K. H. Becks, F. Block, J. Drees, P. Langefeld and F. Seidel, </author> <title> "B-quark Tagging using Neural Networks and Multivariate Statistical Methods A Comparison of Both Techniques", </title> <institution> Nucl. Instrum. Methods A329, </institution> <month> 501 </month> <year> (1993). </year>
Reference-contexts: Fortunately, for most HEP problems one has access to big data samples, making it possible to exploit the capabilities of non-parametric models like ANN. Tests of ANN versus standard methods on pattern recognition HEP problems are therefore in favour of ANN models <ref> [26, 27, 28, 29, 30, 31] </ref>. Also, unbiased comparisons of ANN and non-ANN methods on prediction tasks are in favour of ANN [32]. Inevitably, the choice of method depends on many problem dependent factors. <p> The former method is useful if the data is more or less evenly distributed over a limited range, whereas the latter is useful when the data contains outliers. In some cases, such normalizations reduce the learning time for the network by an order of magnitude. A method suggested in <ref> [26] </ref> is to let the network handle the normalization by adding an extra layer of units.
Reference: [27] <author> J. Proriol et. al., </author> <title> "Tagging B Quark Events in Aleph with Neural Networks", </title> <booktitle> Proceedings of Workshop in Neural Networks: From Biology to High Energy Physics, </booktitle> <month> June </month> <year> 1991, </year> <title> Elba, Italy, </title> <editor> eds. O. Benhar, C. Bosio, P. Del Giudice and E. Tabet, </editor> <publisher> ETS EDITRICE (Pisa 1991). </publisher>
Reference-contexts: Fortunately, for most HEP problems one has access to big data samples, making it possible to exploit the capabilities of non-parametric models like ANN. Tests of ANN versus standard methods on pattern recognition HEP problems are therefore in favour of ANN models <ref> [26, 27, 28, 29, 30, 31] </ref>. Also, unbiased comparisons of ANN and non-ANN methods on prediction tasks are in favour of ANN [32]. Inevitably, the choice of method depends on many problem dependent factors.
Reference: [28] <author> R. Belloti et al., </author> <title> "A Comparison Between a Neural Network and the Likelihood Method to Evaluate the Performance of a Transition Radiation Detector", </title> <journal> Comput. Phys. Commun. </journal> <volume> 78, </volume> <month> 17 </month> <year> (1993). </year>
Reference-contexts: Fortunately, for most HEP problems one has access to big data samples, making it possible to exploit the capabilities of non-parametric models like ANN. Tests of ANN versus standard methods on pattern recognition HEP problems are therefore in favour of ANN models <ref> [26, 27, 28, 29, 30, 31] </ref>. Also, unbiased comparisons of ANN and non-ANN methods on prediction tasks are in favour of ANN [32]. Inevitably, the choice of method depends on many problem dependent factors.
Reference: [29] <author> G. Stimpf-Abele, </author> <title> "Recognition of Charged Tracks with Neural Network Techniques", </title> <journal> Comput. Phys. Commun. </journal> <volume> 67, </volume> <month> 183 </month> <year> (1991). </year>
Reference-contexts: Fortunately, for most HEP problems one has access to big data samples, making it possible to exploit the capabilities of non-parametric models like ANN. Tests of ANN versus standard methods on pattern recognition HEP problems are therefore in favour of ANN models <ref> [26, 27, 28, 29, 30, 31] </ref>. Also, unbiased comparisons of ANN and non-ANN methods on prediction tasks are in favour of ANN [32]. Inevitably, the choice of method depends on many problem dependent factors.
Reference: [30] <author> G. Stimpf-Abele and P. Yepes, </author> <title> "Higgs Search and Neural Net Analysis", </title> <journal> Comput. Phys. Com-mun. </journal> <volume> 78, </volume> <month> 1 </month> <year> (1993). </year>
Reference-contexts: Fortunately, for most HEP problems one has access to big data samples, making it possible to exploit the capabilities of non-parametric models like ANN. Tests of ANN versus standard methods on pattern recognition HEP problems are therefore in favour of ANN models <ref> [26, 27, 28, 29, 30, 31] </ref>. Also, unbiased comparisons of ANN and non-ANN methods on prediction tasks are in favour of ANN [32]. Inevitably, the choice of method depends on many problem dependent factors.
Reference: [31] <author> W. S. Babbage and L. F. Thompson, </author> <title> "The Use of Neural Networks in fl 0 Discrimination", </title> <institution> Nucl. Instrum. Methods A330, </institution> <month> 482 </month> <year> (1993). </year>
Reference-contexts: Fortunately, for most HEP problems one has access to big data samples, making it possible to exploit the capabilities of non-parametric models like ANN. Tests of ANN versus standard methods on pattern recognition HEP problems are therefore in favour of ANN models <ref> [26, 27, 28, 29, 30, 31] </ref>. Also, unbiased comparisons of ANN and non-ANN methods on prediction tasks are in favour of ANN [32]. Inevitably, the choice of method depends on many problem dependent factors.
Reference: [32] <author> J. F. Kreider and J. S. Haberl, </author> <title> "Predicting Hourly Building Energy Usage: The Great Energy Predictor Shootout Overview and Discussion of Results", </title> <note> to appear in 1994 ASHRAE Trans. 100, part 2 (1994). </note>
Reference-contexts: Tests of ANN versus standard methods on pattern recognition HEP problems are therefore in favour of ANN models [26, 27, 28, 29, 30, 31]. Also, unbiased comparisons of ANN and non-ANN methods on prediction tasks are in favour of ANN <ref> [32] </ref>. Inevitably, the choice of method depends on many problem dependent factors.
Reference: [33] <author> B. D. Ripley, </author> <title> "Flexible Non-linear Approaches to Classification", </title> <editor> in V. Cherkassky, J. H. Fried-man and H. Wechsler (eds.) </editor> <booktitle> From Statistics to Neural Networks NATO ASI Proceedings, subseries F, </booktitle> <publisher> Springer-Verlag (1993). </publisher>
Reference-contexts: Is the problem complex enough to call for a non-parametric method like ANN? Is data easily available? Does the application require real-time execution? Hence, it is impossible to give a general rule on what strategy to follow (see e.g. ref. <ref> [33] </ref> for a discussion of the subject). However, ANN methods have a number of features that make them particularly attractive: * The output nodes (o i ) are analytic functions of the arguments x i , if the activation function g is analytic.
Reference: [34] <author> R. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley: </publisher> <address> New York (1973). </address> <month> 40 </month>
Reference-contexts: Derivatives with respect to the inputs can therefore be computed, which simplifies error estimation. * As discussed above the output nodes approximate the Bayes a posteriori probabilities [24], which are useful to make final decisions that minimize the overall risk <ref> [34] </ref>. * Sigmoid units are not "orthogonal" and two hidden units may well perform identical tasks, which to some extent avoids overfitting. Also, this property can be very practical and even desirable if the goal is to produce a distributed system that is robust to weight losses. <p> We disagree with this view. With the aid of a self-organizing network it is quite easy to interpret an ANN model [4]. 3.1.2 Choice of ANN Model Classification In classification problems, the task is to model the decision boundary between a set of distributions in the feature space <ref> [34] </ref>. This decision boundary is a surface of dimension N 1, where N is the number of relevant features/inputs. The conventional ANN algorithms for classification problems are the MLP and Learning Vector Quantization (LVQ) [37]. <p> For a classification problem, the optimal classification performance is the Bayes limit or Bayes error <ref> [34] </ref>, which equals the Bayes risk with zero-one loss function. This upper classification limit can be estimated by the use of simpler classifiers, like the k-nearest-neighbours or Parzen windows [34, 50, 62] 4 . <p> For a classification problem, the optimal classification performance is the Bayes limit or Bayes error [34], which equals the Bayes risk with zero-one loss function. This upper classification limit can be estimated by the use of simpler classifiers, like the k-nearest-neighbours or Parzen windows <ref> [34, 50, 62] </ref> 4 . With such an estimate at hand, it is much easier to evaluate the quality of the ANN model. To determine the termination point for the training it is customary to use a validation data set.
Reference: [35] <author> A. Murray, </author> <title> "Multilayer Perceptron Learning Optimized for On-chip Implementation: A Noise Robust System", </title> <journal> Neural Comput. </journal> <volume> 4, </volume> <month> 366 </month> <year> (1992). </year>
Reference-contexts: By a "smart" addition of noise in the training process, the network can be forced to choose a solution where the information is maximally distributed among the weights <ref> [35] </ref>. * An ANN network is not a linear function of all its weights. This implies a very beneficial 9 scaling property for some functions and networks the learning curves are independent of the number of inputs [36].
Reference: [36] <author> A. R. Barron, </author> <title> "Approximation and Estimation Bounds for Artificial Neural Networks", </title> <booktitle> Machine Learning 14, </booktitle> <month> 115 </month> <year> (1994). </year>
Reference-contexts: This implies a very beneficial 9 scaling property for some functions and networks the learning curves are independent of the number of inputs <ref> [36] </ref>. Due to their generality, ANN methods also have some drawbacks, the most prominent one being long training times. Other statistical methods learn in general much quicker. For instance, models with "orthogonal" units (e.g. polynomial ones) may just need one inversion of a matrix in order to be trained. <p> For an open volume the minimum number of hidden units is much smaller. In function fitting problems, estimates similar to eq. (21) can be made for certain classes of functions and networks. In ref. <ref> [36] </ref> the following scaling relationship is given for the number of hidden nodes that minimize the generalization error N h ~ C f N p =(N log N p ) (22) provided that a one hidden layer MLP with linear output is used.
Reference: [37] <author> T. Kohonen, </author> <title> Self-organization and Associative Memory 3rd ed., </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg (1990). </address>
Reference-contexts: This decision boundary is a surface of dimension N 1, where N is the number of relevant features/inputs. The conventional ANN algorithms for classification problems are the MLP and Learning Vector Quantization (LVQ) <ref> [37] </ref>. The MLP needs N h ~ a N1 hidden units to create the decision surface, whereas a nearest neighbour approach, like LVQ, needs N h ~ a N units [38]. Hence, the MLP is in general more parsimonious in parameters than nearest neighbour approaches for pattern classification.
Reference: [38] <author> T. Rognvaldsson, </author> <title> "Pattern Discrimination Using Feedforward Networks: A Benchmark Study of Scaling Behavior", </title> <journal> Neural Comput. </journal> <volume> 5, </volume> <month> 483 </month> <year> (1993). </year>
Reference-contexts: The conventional ANN algorithms for classification problems are the MLP and Learning Vector Quantization (LVQ) [37]. The MLP needs N h ~ a N1 hidden units to create the decision surface, whereas a nearest neighbour approach, like LVQ, needs N h ~ a N units <ref> [38] </ref>. Hence, the MLP is in general more parsimonious in parameters than nearest neighbour approaches for pattern classification. In special cases, when the decision surface is highly disconnected, the LVQ approach may work better.
Reference: [39] <author> J. Proriol, </author> <title> "Multi-modular Networks for the Classification of e + e Hadronic Events", </title> <note> to appear in Nucl. Instrum. Methods A, </note> <year> (1994). </year>
Reference-contexts: In special cases, when the decision surface is highly disconnected, the LVQ approach may work better. We have found the MLP to work better than LVQ for all HEP problems encountered so far. Approaches that combine the advantages of MLP and LVQ <ref> [39] </ref> seem to work better than just using an MLP (see below on modular architectures). Some MLP-like approaches with skip-layer connections and iterative construction algorithms, like the Cascade Correlation algorithm [40], can construct very complex decision boundaries with a small number of hidden units. <p> Such modular systems are often more efficient and easier to train than systems based upon a single architecture only. They are also easier to train. One example is presented in <ref> [39] </ref> where an MLP with a superficial LVQ network is shown to be more efficient than just the single MLP for classifying hadronic events. The superficial LVQ layer is able to resolve non-linearities that remain even after the final hidden layer in the MLP.
Reference: [40] <author> S. E. Fahlman and C. Lebiere, </author> <title> "The Cascade Correlation Learning Architecture", </title> <institution> Carnegie Mellon Computer Science Rpt. </institution> <month> CMU-CS-90-100 </month> <year> (1990). </year>
Reference-contexts: Approaches that combine the advantages of MLP and LVQ [39] seem to work better than just using an MLP (see below on modular architectures). Some MLP-like approaches with skip-layer connections and iterative construction algorithms, like the Cascade Correlation algorithm <ref> [40] </ref>, can construct very complex decision boundaries with a small number of hidden units. It is, however, uncertain how sensitive they are to overtraining. Function fitting and prediction In a function fitting problem, the task is to model a real-valued target function f from a number of (noisy) examples.
Reference: [41] <author> E. Hartman and J. D. Keeler, </author> <title> "Predicting the Future: Advantages of Semilocal Units", </title> <journal> Neural Comput. </journal> <volume> 3, </volume> <month> 566 </month> <year> (1991). </year>
Reference-contexts: Function fitting and prediction In a function fitting problem, the task is to model a real-valued target function f from a number of (noisy) examples. The straightforward ANN approach is to use the MLP with appropriate number of layers and units <ref> [41, 43] </ref>. Another is the "local map" where a partitioning algorithm, like k-means clustering [44], is used to divide the feature space into subregions. Each subregion is then associated with a function a local map [42, 45, 46]. <p> We discuss below how this pruning can be done. 11 3.1.4 Number of hidden layers In theory, an MLP with one hidden layer is sufficient to model any continuous function [55]. In practice, two hidden layers can be more efficient <ref> [41, 43, 49] </ref> but more difficult to train. In our experience, MLP networks with one hidden layer are sufficient for most classification tasks, whereas two hidden layers are preferable for function fitting problems. <p> Hence it is very efficient for making sweeping cuts in the feature space. Other choices are the Gaussian bar <ref> [41] </ref>, which replaces the sigmoid function with a Gaussian, and the radial basis function [42]. These are examples of local activation functions that can be useful if the effective dimension of the problem is lower than the actual number of variables, or if the problem is local.
Reference: [42] <author> J. Moody and C. J. Darken, </author> <title> "Fast Learning in Networks of Locally-tuned Processing Units", </title> <journal> Neural Comput. </journal> <volume> 1, </volume> <month> 281 </month> <year> (1989). </year>
Reference-contexts: Another is the "local map" where a partitioning algorithm, like k-means clustering [44], is used to divide the feature space into subregions. Each subregion is then associated with a function a local map <ref> [42, 45, 46] </ref>. This method is similar in spirit to statistical methods like regression trees and splines [47, 48]. Both the MLP and the local map approaches work well and which method to choose depends on how local the problem is. <p> Hence it is very efficient for making sweeping cuts in the feature space. Other choices are the Gaussian bar [41], which replaces the sigmoid function with a Gaussian, and the radial basis function <ref> [42] </ref>. These are examples of local activation functions that can be useful if the effective dimension of the problem is lower than the actual number of variables, or if the problem is local. Output units For classification tasks, the standard choice is the sigmoid.
Reference: [43] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Mass Reconstruction with a Neural Network", </title> <journal> Phys. Lett. </journal> <volume> B278, </volume> <month> 181 </month> <year> (1992). </year>
Reference-contexts: Function fitting and prediction In a function fitting problem, the task is to model a real-valued target function f from a number of (noisy) examples. The straightforward ANN approach is to use the MLP with appropriate number of layers and units <ref> [41, 43] </ref>. Another is the "local map" where a partitioning algorithm, like k-means clustering [44], is used to divide the feature space into subregions. Each subregion is then associated with a function a local map [42, 45, 46]. <p> We discuss below how this pruning can be done. 11 3.1.4 Number of hidden layers In theory, an MLP with one hidden layer is sufficient to model any continuous function [55]. In practice, two hidden layers can be more efficient <ref> [41, 43, 49] </ref> but more difficult to train. In our experience, MLP networks with one hidden layer are sufficient for most classification tasks, whereas two hidden layers are preferable for function fitting problems.
Reference: [44] <author> J. MacQueen, </author> <title> "Some Methods for Classification and Analysis of Multivariate Observations", </title> <booktitle> Proc. 5th Berkeley Symposium Math. </booktitle> <editor> Stat. and Prob., J. M. LeCam and J. Neyman (eds.), </editor> <publisher> Univ. of California Press, </publisher> <address> Berkeley (1967). </address>
Reference-contexts: The straightforward ANN approach is to use the MLP with appropriate number of layers and units [41, 43]. Another is the "local map" where a partitioning algorithm, like k-means clustering <ref> [44] </ref>, is used to divide the feature space into subregions. Each subregion is then associated with a function a local map [42, 45, 46]. This method is similar in spirit to statistical methods like regression trees and splines [47, 48].
Reference: [45] <author> T. M. Martinetz, H. Ritter and K. J. Schulten, </author> <title> "Three-dimensional Neural Net for Learning Visuomotor-Coordination of a Robot Arm", </title> <journal> IEEE Trans. Neur. Netw. </journal> <volume> 1, </volume> <month> 131 </month> <year> (1989). </year>
Reference-contexts: Another is the "local map" where a partitioning algorithm, like k-means clustering [44], is used to divide the feature space into subregions. Each subregion is then associated with a function a local map <ref> [42, 45, 46] </ref>. This method is similar in spirit to statistical methods like regression trees and splines [47, 48]. Both the MLP and the local map approaches work well and which method to choose depends on how local the problem is.
Reference: [46] <author> T. M. Martinetz, S. G. Berkovich and K. J.Schulten, </author> <title> "Neural-Gas Network for Vector Quantization and its Application to Time-Series Prediction", </title> <journal> IEEE Trans. Neur. Netw. </journal> <volume> 4, </volume> <month> 558 </month> <year> (1993). </year>
Reference-contexts: Another is the "local map" where a partitioning algorithm, like k-means clustering [44], is used to divide the feature space into subregions. Each subregion is then associated with a function a local map <ref> [42, 45, 46] </ref>. This method is similar in spirit to statistical methods like regression trees and splines [47, 48]. Both the MLP and the local map approaches work well and which method to choose depends on how local the problem is.
Reference: [47] <author> L. Breiman, J. H. Friedman, R. A. Olsen and C. J. Stone, </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth, </publisher> <address> Monterey CA (1984). </address>
Reference-contexts: Each subregion is then associated with a function a local map [42, 45, 46]. This method is similar in spirit to statistical methods like regression trees and splines <ref> [47, 48] </ref>. Both the MLP and the local map approaches work well and which method to choose depends on how local the problem is. A third approach, which is often suggested for time-series prediction, is to use recurrent networks 10 with feed-back connections.
Reference: [48] <author> J. H. Friedman, </author> <title> "Multivariate Adaptive Regression Splines", </title> <journal> Ann. of Stat. </journal> <volume> 19, </volume> <month> 1 </month> <year> (1991). </year>
Reference-contexts: Each subregion is then associated with a function a local map [42, 45, 46]. This method is similar in spirit to statistical methods like regression trees and splines <ref> [47, 48] </ref>. Both the MLP and the local map approaches work well and which method to choose depends on how local the problem is. A third approach, which is often suggested for time-series prediction, is to use recurrent networks 10 with feed-back connections.
Reference: [49] <author> M. Ohlsson, C. Peterson, H. Pi, T.Rognvaldsson and B. Soderberg, </author> <title> "Predicting Utility Loads with Artificial Neural Networks Methods and Results from the Great Energy Predictor Shootout", </title> <note> Lund Preprint LU TP 93-24 (to appear in 1994 ASHRAE Trans. 100, part2), </note> <year> (1994). </year>
Reference-contexts: However, in our experience with time series the simple MLP produces as good solutions as recurrent networks, within much shorter training times, given that one is using the appropriate time lagged inputs <ref> [49] </ref>. 3.1.3 Number of hidden units There is a trade-off between bias, which is the networks ability to solve the problem, and variance, which is the risk of overfitting the data. <p> We discuss below how this pruning can be done. 11 3.1.4 Number of hidden layers In theory, an MLP with one hidden layer is sufficient to model any continuous function [55]. In practice, two hidden layers can be more efficient <ref> [41, 43, 49] </ref> but more difficult to train. In our experience, MLP networks with one hidden layer are sufficient for most classification tasks, whereas two hidden layers are preferable for function fitting problems. <p> In our view, LV is the most powerful of all the algorithms for networks with many hidden layers, even though it requires somewhat more CPU time <ref> [6, 49] </ref>.
Reference: [50] <author> K. Fukunaga, </author> <title> Introduction to Statistical Pattern Recognition, 2:nd ed., </title> <publisher> Academic Press Inc., </publisher> <address> San Diego CA (1990). </address>
Reference-contexts: Hence, it is necessary to estimate the generalization error to select the appropriate number of hidden units. Experimentally, this can be done with Cross Validation (CV), Jack-knife, or Bootstrap methods <ref> [50, 51] </ref>. For instance, in v-fold Cross Validation the data set is divided into v disjoint subsets, of which v 1 are used for training and one for testing. <p> For a classification problem, the optimal classification performance is the Bayes limit or Bayes error [34], which equals the Bayes risk with zero-one loss function. This upper classification limit can be estimated by the use of simpler classifiers, like the k-nearest-neighbours or Parzen windows <ref> [34, 50, 62] </ref> 4 . With such an estimate at hand, it is much easier to evaluate the quality of the ANN model. To determine the termination point for the training it is customary to use a validation data set.
Reference: [51] <author> A. A. Chilingarian and G. Z. Zazian, </author> <title> "A Bootstrap Method of Distribution Mixture Proportion Determination", Pat. </title> <journal> Rec. Lett. </journal> <volume> 11, </volume> <month> 781 </month> <year> (1990). </year>
Reference-contexts: Hence, it is necessary to estimate the generalization error to select the appropriate number of hidden units. Experimentally, this can be done with Cross Validation (CV), Jack-knife, or Bootstrap methods <ref> [50, 51] </ref>. For instance, in v-fold Cross Validation the data set is divided into v disjoint subsets, of which v 1 are used for training and one for testing.
Reference: [52] <author> J. Utans and J. Moody, </author> <title> "Selecting Neural Network Architecture via the Prediction Risk: Application to Corporate Bond Rating Prediction", </title> <booktitle> Proc. First Intl. Conf. on AI Appl. on Wall Street, </booktitle> <publisher> IEEE Press, </publisher> <address> Los Alamitos CA (1991). </address> <month> 41 </month>
Reference-contexts: is repeated, identically, until all subsets have been used for testing and the CV estimate of the generalization error is the average error over these v experiments E gen hE test i v (20) To save time one can instead of experimental methods use analytical estimates for the generalization error <ref> [52, 53, 54] </ref>. One approximate form for the (summed square) generalization error is [52], E gen E train N ! where N ! is the number of weights in the network and N p is the number of patterns in the training set. <p> One approximate form for the (summed square) generalization error is <ref> [52] </ref>, E gen E train N ! where N ! is the number of weights in the network and N p is the number of patterns in the training set. This measure agrees well with the experimental CV measure above [52]. <p> One approximate form for the (summed square) generalization error is <ref> [52] </ref>, E gen E train N ! where N ! is the number of weights in the network and N p is the number of patterns in the training set. This measure agrees well with the experimental CV measure above [52]. However, the above methods are all a posteriori and work only in "trial and error" experiments where the generalization performance of different architectures are compared after training. Needless to say it is desirable to have an a priori method that selects the optimal number of hidden units before training.
Reference: [53] <author> J. Moody, </author> <title> "The Effective Number of Parameters: An Analysis of Generalization and Regular--ization in Nonlinear Learning Systems", </title> <journal> Adv. in Neur. Inf. Proc. Syst. </journal> <volume> 4, </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA (1992). </address>
Reference-contexts: is repeated, identically, until all subsets have been used for testing and the CV estimate of the generalization error is the average error over these v experiments E gen hE test i v (20) To save time one can instead of experimental methods use analytical estimates for the generalization error <ref> [52, 53, 54] </ref>. One approximate form for the (summed square) generalization error is [52], E gen E train N ! where N ! is the number of weights in the network and N p is the number of patterns in the training set.
Reference: [54] <author> N. Murata, S. Yoshizawa and S. Amari, </author> <title> "Network Information Criterion Determining the Number of Hidden Units for an Artificial Neural Network Model", </title> <note> to appear in IEEE Trans. Neur. Netw., </note> <year> (1993). </year>
Reference-contexts: is repeated, identically, until all subsets have been used for testing and the CV estimate of the generalization error is the average error over these v experiments E gen hE test i v (20) To save time one can instead of experimental methods use analytical estimates for the generalization error <ref> [52, 53, 54] </ref>. One approximate form for the (summed square) generalization error is [52], E gen E train N ! where N ! is the number of weights in the network and N p is the number of patterns in the training set.
Reference: [55] <author> G. Cybenko, </author> <title> "Approximation by Superposition of a Sigmoidal Function", </title> <journal> Math. Control Signals Systems 2, </journal> <month> 303 </month> <year> (1989). </year>
Reference-contexts: It is possible to start out with more units than needed and remove superfluous units during or after training. We discuss below how this pruning can be done. 11 3.1.4 Number of hidden layers In theory, an MLP with one hidden layer is sufficient to model any continuous function <ref> [55] </ref>. In practice, two hidden layers can be more efficient [41, 43, 49] but more difficult to train. In our experience, MLP networks with one hidden layer are sufficient for most classification tasks, whereas two hidden layers are preferable for function fitting problems.
Reference: [56] <author> H. H. Szu, X. Yang, B. A. Telfer, Y. Sheng, </author> <title> "Neural Network and Wavelet Transform for Scale-invariant Data Classification", </title> <journal> Phys. Rev. </journal> <volume> E 48, </volume> <month> 48 </month> <year> (1993). </year>
Reference-contexts: If possible, the most robust and time saving technique is to preprocess the data such that it is presented to the network in an invariant form <ref> [56] </ref>.
Reference: [57] <author> S. J. Nowlan and G. E. Hinton, </author> <title> "Simplifying Neural Networks by Soft Weight-Sharing", </title> <journal> Neural Comput. </journal> <volume> 4, </volume> <month> 473 </month> <year> (1992). </year>
Reference-contexts: If possible, the most robust and time saving technique is to preprocess the data such that it is presented to the network in an invariant form [56]. If it is suspected, but unknown, that the problem has a symmetry, then it is possible to use "soft weight sharing" <ref> [57] </ref>, which clusters the weight values by adding a complexity term to the usual error measure (see the section on pruning below). 3.1.7 Modular methods The optimal model is not necessarily one single model. <p> Of course, it is by no means necessary that an optimal network solution contains a set of small weights and only a few large weights. It may well be that the optimal weight distribution is multimodal, as is the case for problems with symmetries and shared weights. In ref. <ref> [57] </ref> a procedure is suggested, where the weight distribution is assumed to be a multimodal mixture of Gaussians whose means and widths are adjusted during learning. This method, which is valuable if the problem has unknown symmetries, is not implemented in JETNET 3.0.
Reference: [58] <author> J. M. J. Murre, "Neurosimulators", </author> <note> review to appear in M. </note> <editor> A. Arbib (ed.) </editor> <booktitle> Handbook of Brain Research and Neural Networks, </booktitle> <publisher> MIT Press (1995). </publisher>
Reference-contexts: It is a very stable learning algorithm that reaches as low or lower errors than any alternative algorithm. In what follows we summarize our experiences with the different learning algorithms implemented in JETNET 3.0. Ref. <ref> [58] </ref> contains a review of other ANN packages that implement other learning algorithms. 3.2.1 Back-propagation Back-propagation is the most widely used learning algorithm since it is very simple to implement and, most importantly, it often outperforms other methods in spite of its simplicity.
Reference: [59] <author> W. Schiffmann, M. Joost and R. Werner, </author> <title> "Comparison of Optimized Backpropagation Algorithms", </title> <booktitle> Proc. </booktitle> <address> ESANN 93 Brussels (1993). </address>
Reference-contexts: It works well on parity and decoder problems but has difficulties with HEP problems it often reaches very large weights and gets stuck 3 . 3.2.3 Rprop In a recent benchmark test on a medical data set <ref> [59] </ref> "Rprop" was reported to outperform all other learning algorithms, both in speed and quality. However, its superb performance in this test is related to its use of individual learning rates. <p> However, our experience with CG on HEP problems is the opposite; it is often unable to find the true global minimum. The same conclusion was reached in an extensive benchmark test of different ANN learning algorithms <ref> [59] </ref>. Consequently, we see no reason to recommend using CG, although it learns toy problems very fast. The strength of CG is in the rare cases when the path to the minimum follows a few long narrow valleys. <p> This can be avoided by proper weight initialization. If the input is normalized to unit size, one simply scales the weights in proportion to the number of units feeding to a unit (the "fan-in"). A suitable normalization for this is PARJN (4) = 0:1 (26) Another method, suggested in <ref> [59] </ref>, is to set the width PARJN (4) to any value and then process the training data through the network once and adjust the thresholds to make the average argument of each unit equal to zero. Other suggestions are found in refs. [67, 68]. <p> It has two learning parameters and two control parameters besides the learning rates (in vector ETAV); the scale factors fl + and fl (PARJN (28-29)) and the maximum allowed scale-up and scale-down factors (PARJN (30-31)). According to <ref> [59] </ref> the final result is not very sensitive to the choice of the scale factors. Hence the only concern are the initial learning rates, which are set as in the BP case.
Reference: [60] <author> D. Buskulic, et. al., </author> <title> "Measurement of the Ratio b b = had using Event Shape Variables", </title> <journal> Phys. Lett. </journal> <volume> B313, </volume> <month> 549 </month> <year> (1993). </year>
Reference-contexts: the data Preprocessing the data is important for many reasons. * To prevent overfitting by reducing the number of inputs and hence the number of weights. * To avoid "stiffness" in the learning process by rescaling the data. * To simplify the problem by precomputing useful signatures from the data <ref> [60] </ref>. The input space dimension can be reduced by performing a Principal Component Analysis (PCA) and select the n first principal axes as the basis in feature space.
Reference: [61] <author> H. Pi and C. Peterson, </author> <title> "Finding the Embedding Dimension and Variable Dependencies in Time Series", </title> <note> Lund Preprint LU TP 93-4 (to appear in Neural Comput.) </note> <year> (1994). </year>
Reference-contexts: Also, one should keep in mind that PCA assumes linear dependencies and one might hence loose nonlinear information by employing it. For function mapping problems the most powerful method, to our knowledge, for extracting functional dependencies between input and output is the so-called ffi-test <ref> [61] </ref>. This test only assumes that the function is continuous and uses conditional probabilities to select the significant input variables. Normalization of the input is done to prevent "stiffness", i.e. when weights need to be updated with very different learning rates.
Reference: [62] <author> A. A. Chilingarian, </author> <title> "Statistical Decisions under Nonparametric a Priori Information", </title> <journal> Comput. Phys. Commun. </journal> <volume> 54, </volume> <month> 381 </month> <year> (1989). </year>
Reference-contexts: For a classification problem, the optimal classification performance is the Bayes limit or Bayes error [34], which equals the Bayes risk with zero-one loss function. This upper classification limit can be estimated by the use of simpler classifiers, like the k-nearest-neighbours or Parzen windows <ref> [34, 50, 62] </ref> 4 . With such an estimate at hand, it is much easier to evaluate the quality of the ANN model. To determine the termination point for the training it is customary to use a validation data set. <p> This validation set is not used directly in the training, i.e. not presented to the network, but used indirectly to monitor the performance on unknown data. A deteriorating performance on the validation set signals that the ANN is overlearning the training data and that training should 4 Ref. <ref> [62] </ref> provides F77 code for doing this estimation 15 be stopped. When the training is stopped, a test set can be used to estimate the generalization performance.
Reference: [63] <author> A. Weigend, B. Huberman and D. Rumelhart, </author> <title> "Predicting Sunspots and Exchange Rates with Connectionist Networks", Nonlin. Modeling and Forecasting, </title> <publisher> Addison-Wesley (1991). </publisher>
Reference-contexts: Eq. (24) constrains the weights to a prior Gaussian probability distribution P (!) / exp [! 2 =2] with ln P as the complexity cost. A slightly more sophisticated pruning option is <ref> [63] </ref> E ! E + ij ij =! 2 1 + ! 2 0 which has zero cost for small weights and cost for large weights. <p> The Lagrange multipliers correspond to parameters PARJN (5) and PARJN (14) respectively. Hessian-based pruning can be done by computing the Hessian matrix, its eigenvalues and eigenvectors: Small weights that lie inside a flat subspace can be omitted. Following <ref> [63] </ref>, we tune in eq. (25) according to * If (E t &lt; D or E t &lt; E t1 ) ) increment = + . * If (E t E t1 and E t &lt; A t and E t D) ) decrease = . * If (E t E <p> D : The desired error, which acts as a threshold for the procedure. Solutions with error above D are not pruned unless the training error is decreasing. In ref. <ref> [63] </ref> it is advised for hard problems that D is set to random performance, which in practice means that pruning is always on. Altough it is quite tricky to get it to work properly, we have used this procedure successfully on both toy and real-world problems. <p> Altough it is quite tricky to get it to work properly, we have used this procedure successfully on both toy and real-world problems. The recommended value for ! 0 in <ref> [63] </ref> is of order unity if the activation functions for the units are of order unity. This agrees with our experience, with the modification that ! 0 should follow ! ~ 1="fan-in". <p> This agrees with our experience, with the modification that ! 0 should follow ! ~ 1="fan-in". However, our tests were performed on problems where the number of inputs ranged between two and ten, whereas the largest networks in <ref> [63] </ref> have up to forty inputs. Also, on toy problems we find that the parameter can be increased considerable (orders of magnitude) above the suggested default value of 1 10 6 .
Reference: [64] <author> M. C. Mozer and P. Smolensky, </author> <title> "Using Relevance to Reduce Network Size Automatically", </title> <booktitle> Connection Science 1, </booktitle> <month> 3 </month> <year> (1989). </year>
Reference-contexts: This method, which is valuable if the problem has unknown symmetries, is not implemented in JETNET 3.0. There also exist a posteriori methods for pruning trained networks by measuring the relevance of the units <ref> [64] </ref> or by computing the Hessian matrix to remove superfluous weights [65, 66]. One extremely simple method that works surprisingly well is a posteriori pruning by visual inspection: Remove all weights with a magnitude less than some threshold, provided that the inputs have been normalized.
Reference: [65] <author> Y. Le Cun, J. S. Denker and S. A. Solla, </author> <title> "Optimal Brain Damage", </title> <booktitle> Neur. Inform. Proc. Systems 2, </booktitle> <month> 598 </month> <year> (1990). </year>
Reference-contexts: This method, which is valuable if the problem has unknown symmetries, is not implemented in JETNET 3.0. There also exist a posteriori methods for pruning trained networks by measuring the relevance of the units [64] or by computing the Hessian matrix to remove superfluous weights <ref> [65, 66] </ref>. One extremely simple method that works surprisingly well is a posteriori pruning by visual inspection: Remove all weights with a magnitude less than some threshold, provided that the inputs have been normalized.
Reference: [66] <author> B. Hassibi and D. G. Stork, </author> <title> "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon", </title> <booktitle> Neur. Inform. Proc. Systems 5, </booktitle> <month> 164 </month> <year> (1993). </year>
Reference-contexts: This method, which is valuable if the problem has unknown symmetries, is not implemented in JETNET 3.0. There also exist a posteriori methods for pruning trained networks by measuring the relevance of the units [64] or by computing the Hessian matrix to remove superfluous weights <ref> [65, 66] </ref>. One extremely simple method that works surprisingly well is a posteriori pruning by visual inspection: Remove all weights with a magnitude less than some threshold, provided that the inputs have been normalized.
Reference: [67] <author> L. F. A. Wessels and E. Barnard, </author> <title> "Avoiding False Local Minima by Proper Initialization of Connections", </title> <journal> IEEE Trans. Neur. Netw. </journal> <volume> 3, </volume> <month> 899 </month> <year> (1992). </year>
Reference-contexts: Other suggestions are found in refs. <ref> [67, 68] </ref>. None of these methods are automated in JETNET 3.0. 4.1.2 Backpropagation The BP algorithm, eq. (7), is selected by setting MSTJN (5) = 0 (default).
Reference: [68] <author> T. Denoeux and R. Lengelle, </author> <title> "Initializing Back Propagation Networks with Prototypes", </title> <booktitle> Neural Networks 6, </booktitle> <month> 351 </month> <year> (1993). </year>
Reference-contexts: Other suggestions are found in refs. <ref> [67, 68] </ref>. None of these methods are automated in JETNET 3.0. 4.1.2 Backpropagation The BP algorithm, eq. (7), is selected by setting MSTJN (5) = 0 (default).
Reference: [69] <author> T. P. Vogl et al., </author> <title> "Accelerating the Convergence of the Back-Propagation Method", </title> <journal> Biol. Cy-bern. </journal> <volume> 59, </volume> <month> 257 </month> <year> (1988). </year>
Reference-contexts: Initial weight adjustments in general need to be large, since the probability of being close to the minimum is small, whereas final adjustments should be small in order for the network to settle properly. For BP and LV we use a so-called "bold driver" method <ref> [69] </ref> where the learning rate is increased if the error is 19 decreasing, and decreased if the error increases: t+1 = t fl if E t+1 E t 10 ) otherwise (27) The scale factor fl, which is set by the parameter PARJN (11), is close to but less than one.
Reference: [70] <author> C. Darken, J. Chang and J. Moody, </author> <title> "Learning Rate Schedules for Faster Stochastic Gradient Descent", </title> <booktitle> Proc. 1992 IEEE Worksh. Neur. Netw. Signal Processing, 3 (1992). </booktitle> <pages> 42 </pages>
Reference-contexts: For MH learning we recommend an exponential decrease of the learning rate, realized by choosing a negative value for PARJN (11). Examples of other more advanced methods for regulating the learning rate are found in refs. <ref> [70, 71, 72] </ref>. The noise level used in LV updating should also decrease with time, preferably faster than the learning rate. We use an exponential decay governed by the scale parameter PARJN (20). This procedure is sufficient to significantly improve the learning for networks with many hidden layers [6].
Reference: [71] <author> Y. LeCun, P. Y. Simard and B. Pearlmutter, </author> <title> "Automatic Learning Rate Maximization by On-Line Estimation of the Hessian's Eigenvectors", </title> <journal> Proc. Neur. Inf. Proc. Syst. </journal> <volume> 5, </volume> <month> 156 </month> <year> (1993). </year>
Reference-contexts: For MH learning we recommend an exponential decrease of the learning rate, realized by choosing a negative value for PARJN (11). Examples of other more advanced methods for regulating the learning rate are found in refs. <ref> [70, 71, 72] </ref>. The noise level used in LV updating should also decrease with time, preferably faster than the learning rate. We use an exponential decay governed by the scale parameter PARJN (20). This procedure is sufficient to significantly improve the learning for networks with many hidden layers [6].
Reference: [72] <author> T. M. Heskes and B. Kappen, </author> <title> "Learning-parameter Adjustment in Neural Networks", </title> <journal> Phys. </journal> <note> Rev. A 45, 8885 (1992). </note>
Reference-contexts: For MH learning we recommend an exponential decrease of the learning rate, realized by choosing a negative value for PARJN (11). Examples of other more advanced methods for regulating the learning rate are found in refs. <ref> [70, 71, 72] </ref>. The noise level used in LV updating should also decrease with time, preferably faster than the learning rate. We use an exponential decay governed by the scale parameter PARJN (20). This procedure is sufficient to significantly improve the learning for networks with many hidden layers [6].
Reference: [73] <author> F. James, </author> <title> "MINUIT", </title> <journal> Comput. Phys. Commun. </journal> <volume> 10, </volume> <month> 343 </month> <year> (1975) </year>
Reference-contexts: Most importantly, an identical set of training patterns must be used for each evaluation of the error, otherwise the line search gets confused. 4.4.1 The line search Although inspired by algorithms in <ref> [23, 73] </ref>, the line search algorithm implemented in JETNET 3.0 is somewhat unorthodox.
Reference: [74] <author> O. Catoni, </author> <title> "Rough Large Deviation Estimates for Simulated Annealing Applications to Exponential Schedules", Ann. </title> <booktitle> of Probability 20, </booktitle> <month> 1109 </month> <year> (1992). </year>
Reference-contexts: This procedure is sufficient to significantly improve the learning for networks with many hidden layers [6]. From the perspective of simulated annealing and global optimization, an exponentially decreasing noise level can also be justified given that the simulation time is finite <ref> [74] </ref>. Also implemented in JETNET 3.0 are options of having the momentum and the temperature change each epoch.
Reference: [75] <author> F. James, </author> <title> "A Review of Pseudorandom Number Generators", </title> <journal> Comput. Phys. Commun. </journal> <volume> 60, </volume> <month> 329 </month> <year> (1990). </year>
Reference-contexts: The routine is taken, basically unchanged, from the Lund program JETSET but the code owes very much to <ref> [75] </ref>. 5.2 Common blocks 5.2.1 Interface common blocks The user interface common blocks are /JNDAT1/ and /JNDAT2/. /JNDAT1/ is the main common block while /JNDAT2/ is intended for the "advanced" user. * COMMON /JNDAT1/ MSTJN (40),PARJN (40),MSTJM (20),PARJM (20),OIN (1000),OUT (1000), MXNDJM MSTJN is a vector of switches used to define
References-found: 75

