URL: http://www.robotics.stanford.edu/~koller/papers/uai97oobn.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/uai97oobn.html
Root-URL: http://www.robotics.stanford.edu
Email: koller@cs.stanford.edu  avi@cs.stanford.edu  
Title: Object-Oriented Bayesian Networks  
Author: Daphne Koller Avi Pfeffer 
Affiliation: Stanford University  Stanford University  
Date: August 1-3, 1997  
Address: Providence, Rhode Island,  
Note: In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-97), pages 302-313,  
Abstract: Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains. They have been successfully applied in a variety of medium-scale applications. However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of programming using logical circuits. In this paper, we describe an object-oriented Bayesian network (OOBN) language, which allows complex domains to be described in terms of inter-related objects. We use a Bayesian network fragment to describe the probabilistic relations between the attributes of an object. These attributes can themselves be objects, providing a natural framework for encoding part-of hierarchies. Classes are used to provide a reusable probabilistic model which can be applied to multiple similar objects. Classes also support inheritance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once. Our language has clear declarative semantics: an OOBN can be interpreted as a stochastic functional program, so that it uniquely specifies a probabilistic model. We provide an inference algorithm for OOBNs, and show that much of the structural information encoded by an OOBNparticularly the encapsulation of variables within an object and the reuse of model fragments in different contextscan also be used to speed up the inference process.
Abstract-found: 1
Intro-found: 1
Reference: [ Banerjee et al., 1987 ] <author> J. Banerjee, H.-T. Chou, J.F. Garza, W. Kim, D. Woelk, N. Ballou, and H.-J. Kim. </author> <title> Data model issues for object-oriented applications. </title> <journal> ACM Transactions on Office Information Systems, </journal> <volume> 5(1):327, </volume> <year> 1987. </year>
Reference-contexts: introduction of abstract data types. Object-oriented programming languages [ Goldberg and Robson, 1983 ] provide a framework for organizing abstract data types in a way that allows for robust, flexible and efficient construction of programs. Similarly, object-oriented database systems <ref> [ Banerjee et al., 1987 ] </ref> provide tools for managing rich, complex data. In this paper, we present object-oriented Bayesian networks (OOBNs), a powerful and general framework for large-scale knowledge representation using Bayesian networks.
Reference: [ Breese, 1992 ] <author> J.S. Breese. </author> <title> Construction of belief and decision networks. </title> <booktitle> Computational Intelligence, </booktitle> <year> 1992. </year>
Reference-contexts: Of course, OOBNs are not the first proposal for extending Bayesian networks beyond the attribute-based level. However, OOBNs differ from prior proposals in a crucial way. Virtually all of the prior work on this topic focuses on combining Bayesian networks with logic-programming-like rules (see, for example, <ref> [ Breese, 1992; Ngo et al., 1995; Poole, 1993 ] </ref> ).
Reference: [ Goldberg and Robson, 1983 ] <author> A. Goldberg and D. Robson. </author> <title> Smalltalk-80: The Language and its Implementation. </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: introduction of abstract data types. Object-oriented programming languages <ref> [ Goldberg and Robson, 1983 ] </ref> provide a framework for organizing abstract data types in a way that allows for robust, flexible and efficient construction of programs. Similarly, object-oriented database systems [ Banerjee et al., 1987 ] provide tools for managing rich, complex data.
Reference: [ Koller et al., 1997a ] <author> D. Koller, A. Levy, and A. Pfeffer. P-Classic: </author> <title> A tractable probabilistic description logic. </title> <booktitle> In Proc. </booktitle> <address> AAAI-97, </address> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: Other objects will depend on properties of the set as a whole, rather than on individual objects within the set. In <ref> [ Koller et al., 1997a ] </ref> , we developed a language and inference algorithm that deals with such sets; we believe the techniques we used can be applied to OOBNs.
Reference: [ Koller et al., 1997b ] <author> D. Koller, D. McAllester, and A. Pfef-fer. </author> <title> Effective Bayesian inference for stochastic programs. </title> <booktitle> In Proc. </booktitle> <address> AAAI-97, </address> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: We use Bayesian networks to define a probabilistic model over the assignments of values to an object. As usual, this probabilistic model must take into account the influences of the environment on the object. Based on our work in <ref> [ Koller et al., 1997b ] </ref> , we view each object as a stochastic function from its inputsthe attributes which influence it to its outputs. Briefly, a stochastic function is a function that, for each value of its inputs, returns a probability distribution over the value of its outputs. <p> The model defines a distribution over the object's value as a function of the values of its input attributes. More precisely, we associate a stochastic function <ref> [ Koller et al., 1997b ] </ref> with the object that defines, for each assignment of values to the object's inputs, a distribution over the possible values for the object. Definition 2.10: Let t = t 1 ; : : : ; t k and u be value types. <p> This observation allows us to reformulate OOBN inference in terms of basic variables. Note that this property is specific to the language we have chosen. In a richer language, such as that of <ref> [ Koller et al., 1997b ] </ref> , the structured objects play a much more important role, preventing us from simplifying the inference algorithm by restricting to simple objects. For the remainder of this section, let B be an OOBN, and let X be the set of objects defined in B. <p> Our approach is based on a stochastic functional lan 3 We note that, while MSBNs do not currently allow part of the model to be changed at runtime, they can be adapted to support this type of interaction. guage <ref> [ Koller et al., 1997b ] </ref> , with the object-oriented framework a natural extension. We believe that our approach has several important advantages: The ability to naturally represent objects that are composed of lower level objects.
Reference: [ Laskey and Mahoney, 1997 ] <author> K.B. Laskey and S.M. Ma-honey. </author> <title> Network fragments: Representing knowledge for constructing probabilistic models. </title> <booktitle> In Proc. </booktitle> <address> UAI-97, </address> <year> 1997. </year> <note> In this proceedings. </note>
Reference-contexts: We have also shown that the encapsulation of objects within other objects and the code reuse can provide significant advantages in inference. Independently of our work, Laskey and Mahoney <ref> [ Laskey and Mahoney, 1997 ] </ref> have developed a framework for representing probabilistic knowledge that shares some features with OOBNs. In their framework, based on network fragments, complex fragments are built out of simpler ones, in much the same way as complex OONFs are built out of simpler ones.
Reference: [ Lauritzen and Spiegelhalter, 1988 ] <author> S. L. Lauritzen and D. J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society, </journal> <pages> pages 157 224, </pages> <year> 1988. </year>
Reference-contexts: In principle, one could define a Bayesian network over the variables # + (X) for every object X in an OOBN. One could then apply a standard BN inference algorithm such as junction trees <ref> [ Lauritzen and Spiegelhalter, 1988 ] </ref> . Unfortunately, the BN produced in this manner is not structured in a way that supports effective inference. The random variables corresponding to complex objects usually range over a very large set of values, rendering most algorithms impractical.
Reference: [ Mahoney and Laskey, 1996 ] <author> S.M. </author> <title> Mahoney and K.B. Laskey. Network engineering for complex belief networks. </title> <booktitle> In Proc. UAI-96, </booktitle> <pages> pages 389396, </pages> <year> 1996. </year>
Reference: [ Ngo et al., 1995 ] <author> L. Ngo, P. Haddawy, and J. Helwig. </author> <title> A theoretical framework for context-sensitive temporal probability model construction with application to plan projection. </title> <booktitle> In Proc. UAI-95, </booktitle> <pages> pages 419426, </pages> <year> 1995. </year>
Reference-contexts: Of course, OOBNs are not the first proposal for extending Bayesian networks beyond the attribute-based level. However, OOBNs differ from prior proposals in a crucial way. Virtually all of the prior work on this topic focuses on combining Bayesian networks with logic-programming-like rules (see, for example, <ref> [ Breese, 1992; Ngo et al., 1995; Poole, 1993 ] </ref> ).
Reference: [ Pearl, 1988 ] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: The base case is trivial: a simple OONF is a CPT, which clearly defines a conditional distribution of the type that we want. The inductive step essentially uses a very similar chain rule to the one used for standard Bayesian networks <ref> [ Pearl, 1988 ] </ref> . Let A 1 ; : : : ; A n be the value attributes in A (F ), ordered in a way which is compatible with the DAG defined by F .
Reference: [ Poole, 1993 ] <author> D. Poole. </author> <title> Probabilistic Horn abduction and Bayesian networks. </title> <journal> Artificial Intelligence, </journal> <volume> 64(1):81129, </volume> <year> 1993. </year>
Reference-contexts: Of course, OOBNs are not the first proposal for extending Bayesian networks beyond the attribute-based level. However, OOBNs differ from prior proposals in a crucial way. Virtually all of the prior work on this topic focuses on combining Bayesian networks with logic-programming-like rules (see, for example, <ref> [ Breese, 1992; Ngo et al., 1995; Poole, 1993 ] </ref> ).
Reference: [ Srinivas, 1994 ] <author> S. Srinivas. </author> <title> A probabilistic approach to hierarchical model-based diagnosis. </title> <booktitle> In Proc. UAI-94, </booktitle> <pages> pages 538545, </pages> <year> 1994. </year>
Reference-contexts: A complex object is defined by assigning stochastic functions to each of its attributes, and connecting the attributes in a Bayesian network. The result is a stochastic function for the complex object. A very simple version of this representation was used by Srinivas <ref> [ Srinivas, 1994 ] </ref> in the context of model-based fault diagnosis in a hierarchical component model. OOBNs allow us to generalize over multiple objects. Formally, we define classes of objects, all of which are described using the same probabilistic model. <p> This is precisely the process used by Srinivas <ref> [ Srinivas, 1994 ] </ref> in his work on hierarchical model-based diagnosis. Unfortunately, even with the locality property of an OOBN, the interfaces corresponding to I/O-sets can still be quite large.
Reference: [ Xiang et al., 1993 ] <author> Y. Xiang, D. Poole, </author> <title> and M.P. Beddoes. Multiply sectioned Bayesian networks and junction forests for large knowledge based systems. </title> <booktitle> Computational Intelligence, </booktitle> <address> 9(2):171220, </address> <year> 1993. </year>
Reference-contexts: This separation property can be utilized to localize probabilistic computation within objects, with only a limited interaction between them. The multiply sectioned Bayesian network (MSBN) framework of Xiang et al. <ref> [ Xiang et al., 1993 ] </ref> turns out to be a particularly appropriate mechanism for utilizing this structure. Since objects of the same class have the same probabilistic model, we can precompute certain parts of the inference task on the level of the class, and reuse them for different instances. <p> Unfortunately, even with the locality property of an OOBN, the interfaces corresponding to I/O-sets can still be quite large. We now provide a more efficient construction, based on the MSBN (multiply-section Bayesian network) framework of Xiang, Poole, and Beddoes <ref> [ Xiang et al., 1993 ] </ref> . Their construction follows the same general lines as the simple one described above. However, they show how to construct the various junction trees in a way that allows the clique corresponding to the I/O-set to be decomposed, while still supporting correct probabilistic propagation. <p> For example, in Figure 2, there is no edge between the subnets for Weather and Road even though they share Weather.Wetness. The communication concerning this attribute is passed between these subnets via the containing Accident Model subnet. The efficient localized inference algorithms of <ref> [ Xiang et al., 1993 ] </ref> apply directly to MSBNs of hypertree structure. They organize the junction-tree according to the OOBN structure, thereby exploiting the separation properties of Lemma 3.5. <p> A full discussion of this issue is beyond the scope of this paper; see <ref> [ Xiang et al., 1993 ] </ref> for details. The organizational information provided by the object-structure of the OOBN plays two roles in our construction. First, it helps identify a partition of the BN nodes which is more likely to support locality of inference.
Reference: [ Xiang, 1995 ] <author> Y. Xiang. </author> <title> Optimization of inter-subnet belief updating in multiply sectioned Bayesian networks. </title> <booktitle> In Proc. UAI-95, </booktitle> <pages> pages 565573, </pages> <year> 1995. </year>
Reference-contexts: Space considerations prevent us from describing the MSBN framework in its entirety. We simply survey some of the basic data structures and their application in our framework. (The simplified definitions are adapted from <ref> [ Xiang, 1995 ] </ref> .) An MSBN partitions the random variables in a BN into a set of non-disjoint subnets. Each subnet contains some localized set of random variables of the BN, with their associated edges.
References-found: 14

