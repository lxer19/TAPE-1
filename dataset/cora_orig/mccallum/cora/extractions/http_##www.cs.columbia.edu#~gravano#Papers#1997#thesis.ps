URL: http://www.cs.columbia.edu/~gravano/Papers/1997/thesis.ps
Refering-URL: http://www.cs.columbia.edu/~gravano/
Root-URL: http://www.cs.columbia.edu
Title: QUERYING MULTIPLE DOCUMENT COLLECTIONS ACROSS THE INTERNET  
Author: Luis Gravano 
Degree: a dissertation submitted to the department of computer science and the committee on graduate studies of stanford university in partial fulfillment of the requirements for the degree of doctor of philosophy By  
Date: August 1997  
Abstract-found: 0
Intro-found: 1
Reference: [Age95] <author> Z39.50 Maintenance Agency. </author> <title> Attribute set Bib-1 (Z39.50-1995): Semantics, </title> <month> September </month> <year> 1995. </year> <note> Accessible at ftp://ftp.loc.gov/pub/z3950/- defs/bib1.txt. </note>
Reference-contexts: Other related efforts focus on defining attribute sets for documents and sources. As discussed in Chapter 2, we have built on some of these efforts in defining our protocol. Relevant attribute sets for documents include the Z39.50 Bib-1 attribute set <ref> [Age95] </ref>, the Dublin Core [WGMJ95], and the Warwick Framework [LLJ96]. The Bib-1 attribute set registers a large set of bibliographic attributes that have been widely adopted in library cataloging.
Reference: [BC92] <author> Daniel Barbara and Chris Clifton. </author> <title> Information Brokers: Sharing knowledge in a heterogeneous distributed system. </title> <type> Technical Report MITL-TR-31-92, </type> <institution> Matsushita Information Technology Laboratory, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The generator objects associated with the brokers are gathered by a "directory of servers," which is queried initially by the users to obtain a list of the brokers whose generator rules match the given query. See also [DANO91]. <ref> [BC92] </ref>, [OM92], and [SA89] are other examples of this type of approach in which users query "meta-information" databases. A "content-based routing" system is used in [SDW + 94] to address the resource-discovery problem.
Reference: [BCGP97a] <author> Michelle Baldonado, Chen-Chuan K. Chang, Luis Gravano, and An-dreas Paepcke. </author> <title> Metadata for digital libraries: Architecture and design rationale. </title> <booktitle> In Proceedings of the Second ACM International Conference on Digital Libraries (DL'97), </booktitle> <month> July </month> <year> 1997. </year>
Reference-contexts: Examples of meta-searchers include MetaCrawler [SE95] (http://www.metacrawler.com), SavvySearch (http://guaraldi.cs.colostate.edu:2000/), and ProFusion [GW96]. Also, the Stanford InfoBus, designed within the Digital Library project [PCGM + 96, RBC + 97], hosts a variety of metasearchers. In <ref> [BCGP97a, BCGP97b] </ref>, we discuss a metadata architecture for the InfoBus. This architecture is based on the requirements of the 163 164 CHAPTER 7. RELATED WORK InfoBus services, and uses the STARTS information that sources should export. MetaCrawler, SavvySearch, and Profusion support the three metasearch tasks above to some degree.
Reference: [BCGP97b] <author> Michelle Baldonado, Chen-Chuan K. Chang, Luis Gravano, and An-dreas Paepcke. </author> <title> The Stanford Digital Library Metadata Architecture. </title> <journal> International Journal of Digital Libraries, </journal> <volume> 1(2), </volume> <year> 1997. </year>
Reference-contexts: Examples of meta-searchers include MetaCrawler [SE95] (http://www.metacrawler.com), SavvySearch (http://guaraldi.cs.colostate.edu:2000/), and ProFusion [GW96]. Also, the Stanford InfoBus, designed within the Digital Library project [PCGM + 96, RBC + 97], hosts a variety of metasearchers. In <ref> [BCGP97a, BCGP97b] </ref>, we discuss a metadata architecture for the InfoBus. This architecture is based on the requirements of the 163 164 CHAPTER 7. RELATED WORK InfoBus services, and uses the STARTS information that sources should export. MetaCrawler, SavvySearch, and Profusion support the three metasearch tasks above to some degree.
Reference: [BDGM95] <author> Sergey Brin, James Davis, and Hector Garca-Molina. </author> <title> Copy detection mechanisms for digital documents. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Management of Data (SIGMOD'95), </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: We believe such prevention schemes are cumbersome, and may make it difficult for honest users to share information. Furthermore, such prevention schemes can be broken by using software emulators <ref> [BDGM95] </ref> and recording documents. Instead of placing restrictions on the distribution of documents, another approach to protecting digital documents (one we subscribe to) is to detect illegal copies using registration server mechanisms such as SCAM [SGM95, SGM96] or COPS [BDGM95]. <p> Furthermore, such prevention schemes can be broken by using software emulators <ref> [BDGM95] </ref> and recording documents. Instead of placing restrictions on the distribution of documents, another approach to protecting digital documents (one we subscribe to) is to detect illegal copies using registration server mechanisms such as SCAM [SGM95, SGM96] or COPS [BDGM95]. Once we know a document to be an illegal copy, it is sometimes useful to know the originator of the illegal copy.
Reference: [BDH + 94] <author> C. Mic Bowman, Peter B. Danzig, Darren R. Hardy, Udi Manber, and Michael F. Schwartz. Harvest: </author> <title> A scalable, customizable discovery and 179 180 BIBLIOGRAPHY access system. </title> <type> Technical Report CU-CS-732-94, </type> <institution> Department of Computer Science, University of Colorado-Boulder, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: In addition to the Z39.50 standard effort, other projects focus on providing a framework for indexing and querying multiple document sources. One such project, Harvest <ref> [BDH + 94] </ref>, includes a set of tools for gathering and accessing information on the Internet. The Harvest gatherers collect and extract indexing information from one or more sources. Then, the brokers retrieve this information from one or more gatherers, or from other brokers.
Reference: [BDO94] <author> Michael W. Berry, Susan T. Dumais, and Gavin W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <type> Technical Report CS-94-270, </type> <institution> Computer Science Department, University of Tennessee, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: By interpreting the answers to these queries, the metasearcher might decide if the source is likely to be useful when it receives a user query. An interesting direction to design this small set of queries is to use the Latent Semantic 175 Indexing technique (LSI) <ref> [FDD + 88, BDO94, Dum94] </ref>. LSI is used in the information retrieval community for document retrieval. LSI constructs compact representations of the sources' contents. These representations could in turn be approximated by carefully choosing a limited set of queries to issue to the sources.
Reference: [BLMO94] <author> J. Brassil, S. Low, N. Maxemchuk, and L. O'Gorman. </author> <title> Document marking and identification using both line and word shifting. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1994. </year>
Reference-contexts: Once we know a document to be an illegal copy, it is sometimes useful to know the originator of the illegal copy. There have been several proposals <ref> [BLMO94, CMPS94] </ref> to add unique "watermarks" to documents (encoded in word spacing or in images) so that one can trace back to the original buyer of that illegal document. A variety of mechanisms have been suggested for registration servers.
Reference: [CDY95] <author> Surajit Chaudhuri, Umeshwar Dayal, and Tak W. Yan. </author> <title> Join queries with external text sources: execution and optimization techniques. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Management of Data (SIGMOD'95), </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: To extract the information that users need, a metasearcher might perform join-like operations involving, say, two repositories of text documents. Reference <ref> [CDY95] </ref> is an interesting step towards integrating relational-like and text sources for querying. We will explore the meaningful combinations of data types and operations, and define their semantics precisely so that metasearchers can translate user requests into potentially complex queries spanning multiple sources. * Defining expressive query languages.
Reference: [CG96] <author> Surajit Chaudhuri and Luis Gravano. </author> <title> Optimizing queries over multimedia repositories. </title> <booktitle> In Proceedings of the 1996 ACM International Conference on Management of Data (SIGMOD'96), </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: Example 29: Let Q 1 be the single-attribute query for Q and the Location attribute, and Q 2 be the single-attribute query for Q and the Price attribute. Consider the 2 The notion of cover is related to that of a complete set of atomic conditions in <ref> [CG96] </ref>. (See Section 7.4.) 114 CHAPTER 5. THE RESULT MERGING PROBLEM Target and Source scores of Example 26. Then, the set fQ 1 g is a cover for Q. In effect, for any 0 g &lt; 1, we can define G = 0:5 (g + 1). <p> on single-attribute queries, it follows that * 1 = * 2 = 0 (Steps (2) 3 Algorithm Top reduces the problem of finding the top Target objects for Q in S to the problem of finding all objects t in S with Target (Q; t) &gt; G, for some G. <ref> [CG96] </ref> uses a similar strategy for processing queries over a multimedia repository. 116 CHAPTER 5. THE RESULT MERGING PROBLEM and (3)). We can use any 0 g 1 ; g 2 &lt; 1 and G = 0:5 (g 1 + g 2 ) in the definition of cover (Definition 3). <p> We will conduct a more exhaustive experimental analysis of the algorithm in the near future. Another interesting open issue is the optimization of queries over multiple sources, perhaps using statistics on the sources' contents. A promising direction is to adapt the work in <ref> [CG96] </ref> and [Fag96] to our distributed, heterogeneous scenario. Another interesting issue is how to deal with sources that do not satisfy the properties and assumptions that our results need. <p> These objects might have attributes like images and text. Thus, the matches between query values and such multimedia attributes are inherently fuzzy, and the objects are ranked according to how well they match the query values. The work in <ref> [CG96] </ref> and [Fag96] studies how to query such repositories efficiently. In particular, [Fag96] studies upper and lower bounds on the number of objects that we need to extract from a repository so that the overall top objects are retrieved and returned 170 CHAPTER 7. <p> In particular, [Fag96] studies upper and lower bounds on the number of objects that we need to extract from a repository so that the overall top objects are retrieved and returned 170 CHAPTER 7. RELATED WORK to the user that issued a query. <ref> [CG96] </ref> addresses the cost-based optimization of queries over such repositories. This work assumes that a single repository handles all attributes of an object. Therefore, there is no need to "calibrate" the scores that an object gets for a particular attribute, for example.
Reference: [CGMP96a] <author> Chen-Chuan K. Chang, Hector Garca-Molina, and Andreas Paepcke. </author> <title> Boolean query mapping across heterogeneous information sources. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(4) </volume> <pages> 515-521, </pages> <month> August </month> <year> 1996. </year>
Reference: [CGMP96b] <author> Chen-Chuan K. Chang, Hector Garca-Molina, and Andreas Paepcke. </author> <title> Predicate rewriting for translating Boolean queries in a heterogeneous information system. </title> <type> Technical Report SIDL-WP-1996-0028, </type> <institution> Stanford University, </institution> <year> 1996. </year> <note> Accessible at http://www-diglib.stanford.edu/- cgi-bin/WP/get/SIDL-WP-1996-0028. </note>
Reference: [Cha88] <author> Alice Y. Chamis. </author> <title> Selection of online databases using switching vocabularies. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 39(3), </volume> <year> 1988. </year> <note> BIBLIOGRAPHY 181 </note>
Reference-contexts: When a query arrives in a site, it is forwarded through the most promising link according to these statistics. References [MDT93], [ZC92], and [MTD92] follow an expert-systems approach to solving the related problem of selecting online business databases. A complementary approach to GlOSS is taken by Chamis <ref> [Cha88] </ref>. Briefly, the approach this paper takes is to expand a user query with thesaurus terms. The expanded query is compared with a set of databases, and the query terms with exact matches, thesauri matches, and "associative" matches are counted for each database.
Reference: [Chr97] <author> Eliot Christian. </author> <title> Application profile for the government information locator service GILS, </title> <type> Version 2, </type> <month> August </month> <year> 1997. </year> <note> Accessible at http://- www.usgs.gov/gils/prof v2.html. </note>
Reference-contexts: In contrast, we chose to support only a simple, "flat" document model, albeit with the ability to mix different attribute models [GCGMP96]. Regarding source-metadata attribute sets, the most notable efforts include the Z39.50 Exp-1 attribute set [Org95] and the GILS profile <ref> [Chr97] </ref>, upon which we based our 166 CHAPTER 7.
Reference: [CLC95] <author> James P. Callan, Zhihong Lu, and W. Bruce Croft. </author> <title> Searching distributed collections with inference networks. </title> <booktitle> In Proceedings of the Eighteenth ACM International Conference on Research and Development in Information Retrieval (SIGIR'95), </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Each database is then ranked as a function of these counts. We believe that this approach is complementary in its emphasis on thesauri to expand the meaning of a user query. Reference <ref> [CLC95] </ref> has applied inference networks (from information retrieval) to the text-source discovery problem. Their approach summarizes databases using document-frequency information for each term (the same type of information that GlOSS keeps about the databases), together with the "inverse collection frequency" of the different terms. <p> Another approach is to calibrate the document scores from each collection using statistics about the word distribution in the collections <ref> [CLC95] </ref>. One important difference between this line of work and ours is that we want to guarantee that metasearchers extract the top Target objects from the sources and return these objects ordered according to their Target scores.
Reference: [CLR91] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Thus, p i = Sel (w i ; db). By minimizing the added selectivity we will tend to minimize the number of documents that we retrieve from db. We will find an optimal solution for this problem by reducing it to the 0-1 knapsack problem <ref> [CLR91] </ref>. The new formulation of the problem is as follows. A thief robbing a store finds N items (the words). The ith item is worth p i dollars (the selectivity of word w i ) and weighs C i (db; s) pounds (the maximum contribution of w i ). <p> Assuming that T , the C i 's, and the p i 's have a fixed number of significant decimals, we can use dynamic programming to solve the problem in O (T N ) time, where N is the number of words in the suspicious document <ref> [CLR91] </ref>. 6.6 Experiments This section presents experimental results for dSCAM.
Reference: [CMPS94] <author> A. Choudhury, N. Maxemchuk, S. Paul, and H. Schulzrinne. </author> <title> Copyright protection for electronic publishing over computer networks. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1994. </year>
Reference-contexts: Once we know a document to be an illegal copy, it is sometimes useful to know the originator of the illegal copy. There have been several proposals <ref> [BLMO94, CMPS94] </ref> to add unique "watermarks" to documents (encoded in word spacing or in images) so that one can trace back to the original buyer of that illegal document. A variety of mechanisms have been suggested for registration servers.
Reference: [CSM + 97] <author> Shih-Fu Chang, John R. Smith, Horace J. Meng, Hualu Wang, and Di Zhong. </author> <title> Finding images/video in large archives. </title> <journal> D-Lib Magazine, </journal> <month> February </month> <year> 1997. </year>
Reference-contexts: A particularly challenging open issue is how to summarize the contents of such sources in an automatic and scalable way so that metasearchers can reason about the sources when processing user queries. Image features like color histograms are commonly used to search over image repositories <ref> [FBF + 94, NBE + 93, OS95, CSM + 97] </ref>. These features are typically represented as weight vectors. Similarly, the vector-space retrieval model also models text documents as weight vectors.
Reference: [DADA96] <author> Ron Dolin, Divyakant Agrawal, Laura Dillon, and Amr El Abbadi. Pharos: </author> <title> A scalable distributed architecture for locating heterogeneous information sources. </title> <type> Technical Report TRCS96-05, </type> <institution> Computer Science Department, University of California at Santa Barbara, </institution> <month> July </month> <year> 1996. </year>
Reference-contexts: An inference network then uses this information to rank the databases for a given query. Two interesting alternative approaches are Pharos and the Information Manifold. The Pharos system <ref> [DADA96] </ref> combines browsing and searching for resource discovery. This system keeps information on the number of objects that each source has for each category of a subject hierarchy like the Library of Congress's LC Classification System.
Reference: [DANO91] <author> Peter B. Danzig, Jongsuk Ahn, John Noll, and Katia Obraczka. </author> <title> Distributed indexing: a scalable mechanism for distributed information retrieval. </title> <booktitle> In Proceedings of the Fourteenth ACM International Conference on Research and Development in Information Retrieval (SIGIR'91), </booktitle> <month> Oc-tober </month> <year> 1991. </year>
Reference-contexts: The generator objects associated with the brokers are gathered by a "directory of servers," which is queried initially by the users to obtain a list of the brokers whose generator rules match the given query. See also <ref> [DANO91] </ref>. [BC92], [OM92], and [SA89] are other examples of this type of approach in which users query "meta-information" databases. A "content-based routing" system is used in [SDW + 94] to address the resource-discovery problem.
Reference: [Den95] <author> Peter J. Denning. </author> <title> Editorial: Plagiarism in the web. </title> <journal> Communications of the ACM, </journal> <volume> 38(12), </volume> <month> December </month> <year> 1995. </year> <note> 182 BIBLIOGRAPHY </note>
Reference-contexts: These characteristics also impact the optimization of queries over these sources. Chapter 6 dSCAM: A Non-Traditional Metasearcher In a renowned 1995 case <ref> [Den95] </ref>, an author, who we will refer to as Mr. X for legal reasons, plagiarized several technical reports and conference papers, and resubmitted them under his own name to other conferences and journals. <p> The Stanford Copy Analysis Mechanism (SCAM) [SGM95, SGM96] played an important role in identifying the papers that Mr. X had plagiarized. (See <ref> [Den95] </ref> for further details.) SCAM is a registration server mechanism that helps flag document-copyright violations in Digital Libraries. The target is not simply academic plagiarism, but any type of copying that can financially hurt authors and commercial publishers.
Reference: [DLO92] <author> Peter B. Danzig, Shih-Hao Li, and Katia Obraczka. </author> <title> Distributed indexing of autonomous Internet services. </title> <journal> Computer Systems, </journal> <volume> 5(4), </volume> <year> 1992. </year>
Reference-contexts: Also, sites are organized into "specialization subgraphs." If one node of such a graph is reached during the search process, the search proceeds "non-randomly" in this subgraph, if it corresponds to a specialization relevant to the query being executed. See also [Sch93]. In Indie (shorthand for "Distributed Indexing") <ref> [DLO92] </ref>, information is indexed by "Indie brokers," each of which has associated, among other administrative data, a Boolean query (called a "generator rule"). Each broker indexes (not necessarily local) documents that satisfy its generator rule.
Reference: [DS94] <author> Andrzej Duda and Mark A. Sheldon. </author> <title> Content routing in a network of WAIS servers. </title> <booktitle> In Proceedings of the Fourteenth IEEE International Conference on Distributed Computing Systems, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: However, freeWAIS [FW + 93] automatically adds the most frequently occurring words in an information server to the associated description in the directory of servers. Another drawback is that in general, databases containing relevant documents might be missed if they are not chosen during the database-selection phase. <ref> [DS94] </ref> shows sample queries for which very few of the existing relevant servers are found by querying the WAIS directory of servers (e.g., only 6 out of 223 relevant WAIS servers). 7.3.
Reference: [Dum94] <author> Susan T. Dumais. </author> <title> Latent semantic indexing (LSI) and TREC-2. </title> <booktitle> In Proceedings of the Second Text Retrieval Conference (TREC-2), </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: By interpreting the answers to these queries, the metasearcher might decide if the source is likely to be useful when it receives a user query. An interesting direction to design this small set of queries is to use the Latent Semantic 175 Indexing technique (LSI) <ref> [FDD + 88, BDO94, Dum94] </ref>. LSI is used in the information retrieval community for document retrieval. LSI constructs compact representations of the sources' contents. These representations could in turn be approximated by carefully choosing a limited set of queries to issue to the sources.
Reference: [Fag96] <author> Ronald Fagin. </author> <title> Combining fuzzy information from multiple systems. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Principles of Database Systems (PODS'96), </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: We will conduct a more exhaustive experimental analysis of the algorithm in the near future. Another interesting open issue is the optimization of queries over multiple sources, perhaps using statistics on the sources' contents. A promising direction is to adapt the work in [CG96] and <ref> [Fag96] </ref> to our distributed, heterogeneous scenario. Another interesting issue is how to deal with sources that do not satisfy the properties and assumptions that our results need. <p> These objects might have attributes like images and text. Thus, the matches between query values and such multimedia attributes are inherently fuzzy, and the objects are ranked according to how well they match the query values. The work in [CG96] and <ref> [Fag96] </ref> studies how to query such repositories efficiently. In particular, [Fag96] studies upper and lower bounds on the number of objects that we need to extract from a repository so that the overall top objects are retrieved and returned 170 CHAPTER 7. <p> Thus, the matches between query values and such multimedia attributes are inherently fuzzy, and the objects are ranked according to how well they match the query values. The work in [CG96] and <ref> [Fag96] </ref> studies how to query such repositories efficiently. In particular, [Fag96] studies upper and lower bounds on the number of objects that we need to extract from a repository so that the overall top objects are retrieved and returned 170 CHAPTER 7.
Reference: [FBF + 94] <author> C. Faloutsos, R. Barber, M. Flickner, J. Hafner, W. Niblack, D. Petkovic, and W. Equitz. </author> <title> Efficient and effective querying by image content. </title> <journal> Journal of Intelligent Information Systems, </journal> <volume> 3 </volume> <pages> 231-262, </pages> <year> 1994. </year>
Reference-contexts: A particularly challenging open issue is how to summarize the contents of such sources in an automatic and scalable way so that metasearchers can reason about the sources when processing user queries. Image features like color histograms are commonly used to search over image repositories <ref> [FBF + 94, NBE + 93, OS95, CSM + 97] </ref>. These features are typically represented as weight vectors. Similarly, the vector-space retrieval model also models text documents as weight vectors.
Reference: [FDD + 88] <author> George W. Furnas, Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, Richard A. Harshman, Lynn A. Streeter, and Karen E. Lochbaum. </author> <title> Information retrieval using a singular value decomposition model of latent semantic structure. </title> <booktitle> In Proceedings of the Eleventh ACM International Conference on Research and Development in Information Retrieval (SIGIR'88), </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: By interpreting the answers to these queries, the metasearcher might decide if the source is likely to be useful when it receives a user query. An interesting direction to design this small set of queries is to use the Latent Semantic 175 Indexing technique (LSI) <ref> [FDD + 88, BDO94, Dum94] </ref>. LSI is used in the information retrieval community for document retrieval. LSI constructs compact representations of the sources' contents. These representations could in turn be approximated by carefully choosing a limited set of queries to issue to the sources.
Reference: [FK93] <author> Jean-Claude Franchitti and Roger King. Amalgame: </author> <title> a tool for creating interoperating persistent, heterogeneous components. </title> <booktitle> In Advanced Database Systems, </booktitle> <pages> pages 313-36. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: In Chapter 5 we assume that all sources export a uniform interface so they can all answer queries over the same set of attributes. We can use the techniques in <ref> [FK93, PGMGU95] </ref>, for example, to build wrappers around the sources and provide the illusion of such a uniform interface. 7.5 Distributed Copy Detection Protecting digital documents from illegal copying has received a lot of attention recently.
Reference: [FW + 93] <author> Jim Fullton, Archie Warnock, et al. </author> <note> Release notes for freeWAIS 0.2, </note> <month> October </month> <year> 1993. </year>
Reference-contexts: One disadvantage is that the master-database documents have to be written by hand to cover the relevant topics, and have to be manually kept up to date as the underlying database changes. However, freeWAIS <ref> [FW + 93] </ref> automatically adds the most frequently occurring words in an information server to the associated description in the directory of servers.
Reference: [FY93] <author> David W. Flater and Yelena Yesha. </author> <title> An information retrieval system for network resources. </title> <booktitle> In Proceedings of the International Workshop on Next Generation Information Technologies and Systems, </booktitle> <month> June </month> <year> 1993. </year> <note> BIBLIOGRAPHY 183 </note>
Reference-contexts: The index server centroids may be passed to other index servers, and so on. A query that is presented to an index server is forwarded to the (index) servers whose centroids match the query. 168 CHAPTER 7. RELATED WORK In <ref> [FY93] </ref>, every site keeps statistics about the type of information it receives along each link connecting to other sites. When a query arrives in a site, it is forwarded through the most promising link according to these statistics.
Reference: [GCGMP96] <author> Luis Gravano, Chen-Chuan K. Chang, Hector Garca-Molina, and Andreas Paepcke. </author> <title> STARTS: Stanford protocol proposal for Internet retrieval and search. </title> <type> Technical Report SIDL-WP-1996-0043, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1996. </year> <note> Accessible at http://- www-diglib.stanford.edu/cgi-bin/WP/get/SIDL-WP-1996-0043. </note>
Reference-contexts: The Warwick Framework proposes a container architecture as a mechanism for incorporating attribute values from different sets in a single information object. In contrast, we chose to support only a simple, "flat" document model, albeit with the ability to mix different attribute models <ref> [GCGMP96] </ref>. Regarding source-metadata attribute sets, the most notable efforts include the Z39.50 Exp-1 attribute set [Org95] and the GILS profile [Chr97], upon which we based our 166 CHAPTER 7.
Reference: [GCGMP97] <author> Luis Gravano, Chen-Chuan K. Chang, Hector Garca-Molina, and An-dreas Paepcke. </author> <title> STARTS: Stanford proposal for Internet meta-searching. </title> <booktitle> In Proceedings of the 1997 ACM International Conference on Management of Data (SIGMOD'97), </booktitle> <month> May </month> <year> 1997. </year>
Reference: [GGM95a] <author> Luis Gravano and Hector Garca-Molina. </author> <title> Generalizing GlOSS for vector-space databases and broker hierarchies. </title> <booktitle> In Proceedings of the Twenty-first International Conference on Very Large Databases (VLDB'95), </booktitle> <pages> pages 78-89, </pages> <month> September </month> <year> 1995. </year>
Reference: [GGM95b] <author> Luis Gravano and Hector Garca-Molina. </author> <title> Generalizing GlOSS to vector-space databases and broker hierarchies. </title> <type> Technical Report STAN-CS-TN-95-21, </type> <institution> Computer Science Department, Stanford University, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: In this section we explore alternative ideal database ranks for a query, similarly as in Section 3.4.3 for Boolean databases. (Even other possibilities are discussed in <ref> [GGM95b] </ref>.) We can organize the different database ranks for a query into two classes, according to whether the ranks depend on the number of relevant documents for the query in each database or not (Section 3.4.3). The first two alternative ranks belong to the first class.
Reference: [GGM97] <author> Luis Gravano and Hector Garca-Molina. </author> <title> Merging ranks from heterogeneous Internet sources. </title> <booktitle> In Proceedings of the Twenty-third International Conference on Very Large Databases (VLDB'97), </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: much does the metasearcher need to know about the source scoring function? Turning to a negative scenario, are there "uncooperative" source scoring functions for which there is no strategy whatsoever that avoids an exhaustive full retrieval of the source contents? In this chapter we address these and other related questions <ref> [GGM97] </ref>. We start by proposing a searching and ranking model for sources with structured data (Section 5.1). Within this model, we then precisely characterize the classes of source and target functions that make retrieval "efficient" or "exhaustive" (Section 5.4).
Reference: [GGMT93] <author> Luis Gravano, Hector Garca-Molina, and Anthony Tomasic. </author> <title> The efficacy of GlOSS for the text-database discovery problem. </title> <type> Technical Report STAN-CS-TN-93-002, </type> <institution> Computer Science Department, Stanford University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: If we dedicate one byte for the field designation and three bytes for the document identifier, we end up with four bytes per posting. Let us assume that, after compression, two bytes suffice per posting (compression of 50% is typical for inverted lists). 4 In <ref> [GGMT93] </ref> we explore an alternative estimate for the "subject" frequencies whose corresponding experimental results were very similar to those for the Equation 3.15 estimate. 72 CHAPTER 3. <p> The vocabulary for INSPEC 5 , including only indexes that appear in TRACE INSPEC queries, consists of 819; 437 words. If we dedicate four bytes to store each keyword (see <ref> [GGMT93] </ref>), around 4 fi 819; 437 bytes, or 3:13 MBytes are needed to store the INSPEC vocabulary. This is shown in the "Vocabulary" row of Figure 3.23.
Reference: [GGMT94a] <author> Luis Gravano, Hector Garca-Molina, and Anthony Tomasic. </author> <title> The effectiveness of GlOSS for the text-database discovery problem. </title> <booktitle> In Proceedings of the 1994 ACM International Conference on Management of Data (SIGMOD'94), </booktitle> <month> May </month> <year> 1994. </year> <note> 184 BIBLIOGRAPHY </note>
Reference: [GGMT94b] <author> Luis Gravano, Hector Garca-Molina, and Anthony Tomasic. </author> <title> Precision and recall of GlOSS estimators for database discovery. </title> <booktitle> In Proceedings of the Third International Conference on Parallel and Distributed Information Systems (PDIS'94), </booktitle> <month> September </month> <year> 1994. </year>
Reference: [GMGS96] <author> Hector Garca-Molina, Luis Gravano, and Narayanan Shivakumar. dSCAM: </author> <title> Finding document copies across multiple databases. </title> <booktitle> In Proceedings of the Fourth International Conference on Parallel and Distributed Information Systems (PDIS'96), </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: In Section 6.5 we present the extraction (query translation) mechanisms. Finally, in Section 6.6 we discuss an experimental evaluation of our techniques, using a collection of 50 databases and two sets of suspicious documents <ref> [GMGS96] </ref>. 6.1 Using SCAM for Copy Detection Given a suspicious document s and a registered document d, SCAM detects whether d is a potential copy of s by deciding whether they overlap significantly. We have explored [SGM95, SGM96] a variety of overlap measures.
Reference: [Gri93] <author> Gary N. Griswold. </author> <title> A method for protecting copyright on networks. In Joint Harvard MIT Workshop on Technology Strategies for Protecting Intellectual Property in the Networked Multimedia Environment, </title> <month> April </month> <year> 1993. </year>
Reference-contexts: Some systems favor the copy prevention approach, for example, by physically isolating information (e.g., by placing information on stand-alone CD-ROM systems), by using special-purpose hardware for authorization [PK79], or by using active documents (e.g., documents encapsulated by programs <ref> [Gri93] </ref>). We believe such prevention schemes are cumbersome, and may make it difficult for honest users to share information. Furthermore, such prevention schemes can be broken by using software emulators [BDGM95] and recording documents.
Reference: [GW96] <author> Susan Gauch and Guijun Wang. </author> <title> Information fusion with ProFusion. </title> <booktitle> In Proceedings of the World Conference of the Web Society (WebNet'96), </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: However, not all of them support the three major metasearch tasks described in Chapter 1 (i.e., selecting the best sources for a query, querying these sources, and merging the query results from the sources). Examples of meta-searchers include MetaCrawler [SE95] (http://www.metacrawler.com), SavvySearch (http://guaraldi.cs.colostate.edu:2000/), and ProFusion <ref> [GW96] </ref>. Also, the Stanford InfoBus, designed within the Digital Library project [PCGM + 96, RBC + 97], hosts a variety of metasearchers. In [BCGP97a, BCGP97b], we discuss a metadata architecture for the InfoBus. This architecture is based on the requirements of the 163 164 CHAPTER 7.
Reference: [Har96] <author> Darren Hardy. </author> <title> Resource description messages (RDM), </title> <month> July </month> <year> 1996. </year> <note> Accessible at http://www.w3.org/pub/WWW/TR/NOTE-rdm.html. </note>
Reference-contexts: The Harvest gatherers collect and extract indexing information from one or more sources. Then, the brokers retrieve this information from one or more gatherers, or from other brokers. The brokers provide a querying interface to the gathered information. A project related to Harvest is Netscape's RDM (Resource Description Messages) <ref> [Har96] </ref>, which focuses on indexing and accessing structured metadata descriptions of information objects. Our work complements Harvest and RDM in that we define the information and functionality that sources should export to help in metasearching.
Reference: [INSS92] <author> Yannis E. Ioannidis, Raymond T. Ng, Kyuseok Shim, and Timos K. Sellis. </author> <title> Parametric query optimization. </title> <booktitle> In Proceedings of the Eighteenth International Conference on Very Large Databases (VLDB'92), </booktitle> <pages> pages 103-14, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Mr. X also plagiarized papers from the database field, notably a paper in DAPD by Tal and Alonso [TA94] on three-phase locking, a paper in VLDB '92 by Ioannidis et al. <ref> [INSS92] </ref> on parametric query optimization, and a paper in ICDE '90 by Leung and Muntz [LM90] on temporal query processing. The Stanford Copy Analysis Mechanism (SCAM) [SGM95, SGM96] played an important role in identifying the papers that Mr.
Reference: [Kah92] <author> Robert E. Kahn. Deposit, </author> <title> registration and recordation in an electronic copyright management system. </title> <type> Technical report, </type> <institution> Corporation for National Research Initiatives, Reston, Virginia, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Documents flagged by SCAM have to be checked manually for actual violations since the copying may have been legal and since SCAM may produce some false positives. The basic SCAM system requires a database of registered documents. In the future, publishers may indeed establish such "copyright registration servers" <ref> [Kah92] </ref>, and these servers can then automatically check public sources such as netnews articles and WWW/FTP sites for copies of the registered documents.
Reference: [KLSS95] <author> Thomas Kirk, Alon Y. Levy, Yehoshua Sagiv, and Divesh Srivastava. </author> <title> The Information Manifold. </title> <booktitle> In Proceedings of the AAAI Spring Symposium Series, </booktitle> <month> March </month> <year> 1995. </year> <note> BIBLIOGRAPHY 185 </note>
Reference-contexts: The Pharos system [DADA96] combines browsing and searching for resource discovery. This system keeps information on the number of objects that each source has for each category of a subject hierarchy like the Library of Congress's LC Classification System. Alternatively, the Information Manifold system <ref> [KLSS95, LRO96] </ref> uses declarative, hand-written descriptions of the sources' contents and capabilities. <p> Source Discovery over Overlapping Text Sources Knowing how sources' contents overlap is especially important on the Internet, where mirroring of sources is commonplace. The Information Manifold system developed at AT&T Bell Laboratories <ref> [KLSS95] </ref> uses description logic to summarize the source contents. Such descriptions can express when a source has complete information for a query, thus making other sources redundant for the query at hand.
Reference: [KM91] <author> Brewster Kahle and Art Medlar. </author> <title> An information system for corporate users: Wide Area Information Servers. </title> <type> Technical Report TMC199, </type> <institution> Thinking Machines Corporation, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: A different approach is to keep a database of "meta-information" about the available databases and have users query this database to obtain the set of databases to search. For example, WAIS <ref> [KM91] </ref> provides a "directory of servers." This "master" database contains a set of documents, each describing (in English) the contents of a database on the network. The users first query the master database, and once they have identified potential databases, direct their query to these databases.
Reference: [Lie95] <author> Henry Lieberman. Letizia: </author> <title> An agent that assists web browsing. </title> <booktitle> In Proceedings of the 1995 International Joint Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: Other sources of information to employ include available query logs, response times, user feedback, and quality reports. For example, initial work on mining query logs tries to predict what pages are likely to be useful to users based on their browsing behavior <ref> [Lie95] </ref>, and that of previous users [YJGMD96]. Source Discovery over Uncooperative Text Sources Metasearchers choose the best sources to evaluate queries by using summaries of each source's contents, assuming that they can extract these summaries with the sources' cooperation by using the STARTS protocol of Chapter 2, for example.
Reference: [LLJ96] <author> Carl Lagoze, Clifford A. Lynch, and Ron Daniel Jr. </author> <title> The Warwick Framework: A container architecture for aggregating sets of metadata. </title> <type> Technical Report TR 96-1593, </type> <institution> Computer Science Department, Cornell University, </institution> <month> June </month> <year> 1996. </year>
Reference-contexts: Other related efforts focus on defining attribute sets for documents and sources. As discussed in Chapter 2, we have built on some of these efforts in defining our protocol. Relevant attribute sets for documents include the Z39.50 Bib-1 attribute set [Age95], the Dublin Core [WGMJ95], and the Warwick Framework <ref> [LLJ96] </ref>. The Bib-1 attribute set registers a large set of bibliographic attributes that have been widely adopted in library cataloging.
Reference: [LM90] <author> T. Y. Cliff Leung and Richard R. Muntz. </author> <title> Query processing for temporal databases. </title> <booktitle> In Proceedings of the Sixth International Conference on Data Engineering, </booktitle> <pages> pages 200-8, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Mr. X also plagiarized papers from the database field, notably a paper in DAPD by Tal and Alonso [TA94] on three-phase locking, a paper in VLDB '92 by Ioannidis et al. [INSS92] on parametric query optimization, and a paper in ICDE '90 by Leung and Muntz <ref> [LM90] </ref> on temporal query processing. The Stanford Copy Analysis Mechanism (SCAM) [SGM95, SGM96] played an important role in identifying the papers that Mr. X had plagiarized. (See [Den95] for further details.) SCAM is a registration server mechanism that helps flag document-copyright violations in Digital Libraries.
Reference: [LRO96] <author> Alon Y. Levy, Anand Rajaraman, and Joann J. Ordille. </author> <title> Querying heterogeneous information sources using source descriptions. </title> <booktitle> In Proceedings of the Twenty-second International Conference on Very Large Databases (VLDB'96), </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: The Pharos system [DADA96] combines browsing and searching for resource discovery. This system keeps information on the number of objects that each source has for each category of a subject hierarchy like the Library of Congress's LC Classification System. Alternatively, the Information Manifold system <ref> [KLSS95, LRO96] </ref> uses declarative, hand-written descriptions of the sources' contents and capabilities.
Reference: [MDT93] <author> Anne Morris, Hilary Drenth, and Gwyneth Tseng. </author> <title> The development of an expert system for online company database selection. </title> <journal> Expert Systems, </journal> <volume> 10(2) </volume> <pages> 47-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: RELATED WORK In [FY93], every site keeps statistics about the type of information it receives along each link connecting to other sites. When a query arrives in a site, it is forwarded through the most promising link according to these statistics. References <ref> [MDT93] </ref>, [ZC92], and [MTD92] follow an expert-systems approach to solving the related problem of selecting online business databases. A complementary approach to GlOSS is taken by Chamis [Cha88]. Briefly, the approach this paper takes is to expand a user query with thesaurus terms.
Reference: [MMM96] <author> Alberto O. Mendelzon, George H. Mihaila, and Tova Milo. </author> <title> Querying the World Wide Web. </title> <booktitle> In Proceedings of the Fourth International Conference on Parallel and Distributed Information Systems (PDIS'96), </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: Users should express their requests using simple interfaces. Metasearchers should translate user requests into queries written in a query language that models the wide variety of sources and data types available on the Internet. We will start with recent work on query languages for the WWW like WebSQL <ref> [MMM96] </ref>, and incorporate the notion of sources as first class objects, so that we can express source discovery in our queries, and optimize the queries using source properties.
Reference: [MTD92] <author> Anne Morris, Gwyneth Tseng, and Hilary Drenth. </author> <title> Expert systems for online business database selection. </title> <type> Library Hi Tech, </type> <institution> 10(1-2):65-68, </institution> <year> 1992. </year> <note> 186 BIBLIOGRAPHY </note>
Reference-contexts: RELATED WORK In [FY93], every site keeps statistics about the type of information it receives along each link connecting to other sites. When a query arrives in a site, it is forwarded through the most promising link according to these statistics. References [MDT93], [ZC92], and <ref> [MTD92] </ref> follow an expert-systems approach to solving the related problem of selecting online business databases. A complementary approach to GlOSS is taken by Chamis [Cha88]. Briefly, the approach this paper takes is to expand a user query with thesaurus terms.
Reference: [MW94] <author> Udi Manber and Sun Wu. Glimpse: </author> <title> A tool to search through entire file systems. </title> <booktitle> In Proceedings of the 1994 Winter USENIX Conference, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: There have been several proposals [BLMO94, CMPS94] to add unique "watermarks" to documents (encoded in word spacing or in images) so that one can trace back to the original buyer of that illegal document. A variety of mechanisms have been suggested for registration servers. In <ref> [MW94] </ref>, a few words in a document are chosen as anchors and checksums of a following window 7.5. DISTRIBUTED COPY DETECTION 171 of characters are computed. "Similar" files can then be found by comparing these checksums that are registered into a database.
Reference: [NBE + 93] <author> W. Niblack, R. Barber, W. Equitz, M. Flickner, E. Glasman, D. Petkovic, P. Yanker, and C. Faloutsos. </author> <title> The QBIC project: Querying images by content using color, texture, and shape. </title> <booktitle> In Storage and retrieval for image and video databases (SPIE), </booktitle> <pages> pages 173-187, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Although text sources are probably the best known example, sources with multimedia objects like images are also becoming common. Matches between query values and objects in such sources are inherently "fuzzy" <ref> [NBE + 93] </ref>. Example 22: Consider a World-Wide Web search engine for images like Image Surfer (http://isurf.interpix.com/). Given an image of interest, Image Surfer returns a 105 106 CHAPTER 5. <p> A particularly challenging open issue is how to summarize the contents of such sources in an automatic and scalable way so that metasearchers can reason about the sources when processing user queries. Image features like color histograms are commonly used to search over image repositories <ref> [FBF + 94, NBE + 93, OS95, CSM + 97] </ref>. These features are typically represented as weight vectors. Similarly, the vector-space retrieval model also models text documents as weight vectors.
Reference: [Neu92] <author> B. Clifford Neuman. </author> <title> The Prospero File System: A global file system based on the Virtual System model. </title> <journal> Computer Systems, </journal> <volume> 5(4), </volume> <year> 1992. </year>
Reference-contexts: Different systems follow different approaches to this: one such approach is to let users "browse" through information about the different resources. A typical example of this paradigm is Yahoo! (http://www.yahoo.com). As another example, the Prospero File System <ref> [Neu92] </ref> lets users organize information available in the Internet through the definition (and sharing) of customized views of the different objects and services available to them.
Reference: [ODL93] <author> Katia Obraczka, Peter B. Danzig, and Shih-Hao Li. </author> <title> Internet resource discovery services. </title> <booktitle> IEEE Computer, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: "MBasic-1" attribute set (Section 2.3.3). 7.3 Text-Source Discovery Many solutions have been presented recently for the text-source discovery problem, or, more generally, for the resource-discovery problem: the text-source discovery problem is a subcase of the resource-discovery problem, since the latter generally deals with a larger variety of types of information <ref> [ODL93, SEKN92] </ref>. One solution to the text-source discovery problem is to let the database selection be driven by the user. Thus, the user will be aware of and an active participant in this selection process.
Reference: [OM92] <author> Joann J. Ordille and Barton P. Miller. </author> <title> Distributed active catalogs and meta-data caching in descriptive name services. </title> <type> Technical Report #1118, </type> <institution> University of Wisconsin-Madison, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: The generator objects associated with the brokers are gathered by a "directory of servers," which is queried initially by the users to obtain a list of the brokers whose generator rules match the given query. See also [DANO91]. [BC92], <ref> [OM92] </ref>, and [SA89] are other examples of this type of approach in which users query "meta-information" databases. A "content-based routing" system is used in [SDW + 94] to address the resource-discovery problem.
Reference: [Org95] <author> National Information Standards Organization. </author> <title> Information retrieval (Z39.50): Application service definition and protocol specification (ANSI/NISO Z39.50-1995), </title> <note> 1995. Accessible at http://- lcweb.loc.gov/z3950/agency/. </note>
Reference-contexts: These confidence factors measure how useful each source was for a set of 25 sample queries. 7.2 Protocols The most relevant standards effort in terms of shared goals is the Z39.50-1995 standard <ref> [Org95] </ref>, which provides most of the functionality we have described for STARTS. <p> In contrast, we chose to support only a simple, "flat" document model, albeit with the ability to mix different attribute models [GCGMP96]. Regarding source-metadata attribute sets, the most notable efforts include the Z39.50 Exp-1 attribute set <ref> [Org95] </ref> and the GILS profile [Chr97], upon which we based our 166 CHAPTER 7.
Reference: [OS95] <author> Virginia E. Ogle and Michael Stonebraker. Chabot: </author> <title> retrieval from a relational database of images. </title> <journal> Computer, </journal> <volume> 28(9), </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: A particularly challenging open issue is how to summarize the contents of such sources in an automatic and scalable way so that metasearchers can reason about the sources when processing user queries. Image features like color histograms are commonly used to search over image repositories <ref> [FBF + 94, NBE + 93, OS95, CSM + 97] </ref>. These features are typically represented as weight vectors. Similarly, the vector-space retrieval model also models text documents as weight vectors.
Reference: [PCGM + 96] <author> Andreas Paepcke, Steve B. Cousins, Hector Garca-Molina, Scott W. Hassan, Steven K. Ketchpel, Martin Roscheisen, and Terry Winograd. </author> <title> Towards interoperability in digital libraries: Overview and selected highlights of the Stanford Digital Library Project. </title> <journal> IEEE Computer Magazine, </journal> <month> May </month> <year> 1996. </year> <note> BIBLIOGRAPHY 187 </note>
Reference-contexts: Examples of meta-searchers include MetaCrawler [SE95] (http://www.metacrawler.com), SavvySearch (http://guaraldi.cs.colostate.edu:2000/), and ProFusion [GW96]. Also, the Stanford InfoBus, designed within the Digital Library project <ref> [PCGM + 96, RBC + 97] </ref>, hosts a variety of metasearchers. In [BCGP97a, BCGP97b], we discuss a metadata architecture for the InfoBus. This architecture is based on the requirements of the 163 164 CHAPTER 7. RELATED WORK InfoBus services, and uses the STARTS information that sources should export.
Reference: [PGMGU95] <author> Yannis Papakonstantinou, Hector Garcia-Molina, Ashish Gupta, and Jeffrey Ullman. </author> <title> A query translation scheme for rapid implementation of wrappers. </title> <booktitle> In Fourth International Conference on Deductive and Object-Oriented Databases, </booktitle> <pages> pages 161-186, </pages> <year> 1995. </year>
Reference-contexts: In Chapter 5 we assume that all sources export a uniform interface so they can all answer queries over the same set of attributes. We can use the techniques in <ref> [FK93, PGMGU95] </ref>, for example, to build wrappers around the sources and provide the illusion of such a uniform interface. 7.5 Distributed Copy Detection Protecting digital documents from illegal copying has received a lot of attention recently.
Reference: [PIHS96] <author> Viswanath Poosala, Yannis E. Ioannidis, Peter J. Haas, and Eugene J. Shekita. </author> <title> Improved histograms for selectivity estimation of range predicates. </title> <booktitle> In Proceedings of the 1996 ACM International Conference on Management of Data (SIGMOD'96), </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: An approach for summarizing relational-like sources is to use human-generated descriptions of the sources (e.g., the Information Manifold). An interesting direction for generating source summaries automatically is to adapt results from the database community on result size and selectivity estimation of queries over relational data (e.g., <ref> [PIHS96] </ref>). These results have been used extensively for query optimization. Thus, we will consider using frequency histograms for succinctly describing relational tables.
Reference: [PK79] <author> Gerald J. Popek and Charles S. Kline. </author> <title> Encryption and secure computer networks. </title> <journal> ACM Computing Surveys, </journal> <volume> 11(4) </volume> <pages> 331-356, </pages> <month> December </month> <year> 1979. </year>
Reference-contexts: Some systems favor the copy prevention approach, for example, by physically isolating information (e.g., by placing information on stand-alone CD-ROM systems), by using special-purpose hardware for authorization <ref> [PK79] </ref>, or by using active documents (e.g., documents encapsulated by programs [Gri93]). We believe such prevention schemes are cumbersome, and may make it difficult for honest users to share information. Furthermore, such prevention schemes can be broken by using software emulators [BDGM95] and recording documents.
Reference: [RBC + 97] <author> Martin Roscheisen, Michelle Baldonado, Chen-Chuan K. Chang, Luis Gravano, Steven Ketchpel, and Andreas Paepcke. </author> <title> The Stanford InfoBus and its service layers: Augmenting the Internet with higher-level information management protocols. </title> <booktitle> In MeDoc Dagstuhl Workshop: Electronic Publishing and Digital Libraries in Computer Science, </booktitle> <month> July </month> <year> 1997. </year>
Reference-contexts: Examples of meta-searchers include MetaCrawler [SE95] (http://www.metacrawler.com), SavvySearch (http://guaraldi.cs.colostate.edu:2000/), and ProFusion [GW96]. Also, the Stanford InfoBus, designed within the Digital Library project <ref> [PCGM + 96, RBC + 97] </ref>, hosts a variety of metasearchers. In [BCGP97a, BCGP97b], we discuss a metadata architecture for the InfoBus. This architecture is based on the requirements of the 163 164 CHAPTER 7. RELATED WORK InfoBus services, and uses the STARTS information that sources should export.
Reference: [SA89] <author> Patricia Simpson and Rafael Alonso. </author> <title> Querying a network of autonomous databases. </title> <type> Technical Report CS-TR-202-89, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> January </month> <year> 1989. </year>
Reference-contexts: The generator objects associated with the brokers are gathered by a "directory of servers," which is queried initially by the users to obtain a list of the brokers whose generator rules match the given query. See also [DANO91]. [BC92], [OM92], and <ref> [SA89] </ref> are other examples of this type of approach in which users query "meta-information" databases. A "content-based routing" system is used in [SDW + 94] to address the resource-discovery problem.
Reference: [Sal89] <author> Gerard Salton. </author> <title> Automatic Text Processing: The transformation, analysis, and retrieval of information by computer. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Although the Boolean model of document retrieval is widely used, it is a rather primitive one. One of the most popular alternative models is the vector-space retrieval model <ref> [Sal89, SM83] </ref>. This model represents both the documents in a database and the queries themselves as weight vectors. Given a query, the documents are ranked according to how "similar" their corresponding vectors are to the given query vector. <p> Then, this word will tend to have a low associated weight in db 1 (e.g., if db 1 uses the tfidf formula for computing weights <ref> [Sal89] </ref>). The word databases, on the other hand, might have a high associated weight in a database db 2 that is totally unrelated to computer science and contains very few documents with that word. <p> Typically, the weight of a word t j in a document d is a function of the number of times that t j appears in d and the number of documents in the database that contain t j <ref> [Sal89] </ref>. Although the information that gGlOSS stores about each database is incomplete, it will prove useful to generate database ranks that resemble the ideal database rank of Section 4.1, as we will see in Section 4.4.2. <p> The definitions of the gGlOSS ranks below reflect this fact. Also, note that the vectors with which gGlOSS represents each database can be viewed as cluster centroids <ref> [Sal89] </ref>, where each database is considered as a single document cluster 1 . Because the information that gGlOSS keeps about each database is incomplete, it has to make assumptions regarding the distribution of query keywords and weights across the documents of each database. <p> To keep our experiments simple, we chose the same weighting algorithms for the queries and the documents across all of the databases. We indexed the documents using the SMART ntc formula, which generates document weight vectors using the cosine-normalized tfidf product <ref> [Sal89] </ref>. We indexed the queries using the SMART nnn formula, which generates query weight vectors using the word frequencies in the queries. The similarity coefficient between a document vector and a query vector is computed by taking the inner product of the two vectors. <p> As we have seen in Chapter 4, a typical example of this kind of sources is a source that indexes text documents and answers queries using some variation of the vector-space model of document retrieval <ref> [Sal89] </ref>. Example 21: Consider a World-Wide Web search engine like Excite (http://www.- excite.com). Given a query consisting of a series of words, like distributed databases, Excite returns the matching documents sorted according to how well they match the query. <p> To gather finer information at the query level, we will use more sophisticated schemes. A possibility is to obtain a document sample from S 1 and S 2 , and cluster these documents using some predefined clustering scheme <ref> [Sal89] </ref>. For each cluster, we can analyze how the two sources overlap. When a query arrives, the metasearcher classifies it into the most relevant clusters, and uses the overlap information for these clusters.
Reference: [Sch90] <author> Michael F. Schwartz. </author> <title> A scalable, non-hierarchical resource discovery mechanism based on probabilistic protocols. </title> <type> Technical Report CU-CS-474-90, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: TEXT-SOURCE DISCOVERY 167 Reference <ref> [Sch90] </ref> follows a probabilistic approach to the resource-discovery problem, and presents a resource-discovery protocol that consists of two phases: a dissemination phase, during which information about the contents of the databases is replicated at randomly chosen sites, and a search phase, where several randomly chosen sites are searched in parallel.
Reference: [Sch93] <author> Michael F. </author> <type> Schwartz. </type> <institution> Internet resource discovery at the University of Colorado. IEEE Computer, </institution> <month> September </month> <year> 1993. </year> <note> 188 BIBLIOGRAPHY </note>
Reference-contexts: Also, sites are organized into "specialization subgraphs." If one node of such a graph is reached during the search process, the search proceeds "non-randomly" in this subgraph, if it corresponds to a specialization relevant to the query being executed. See also <ref> [Sch93] </ref>. In Indie (shorthand for "Distributed Indexing") [DLO92], information is indexed by "Indie brokers," each of which has associated, among other administrative data, a Boolean query (called a "generator rule"). Each broker indexes (not necessarily local) documents that satisfy its generator rule.
Reference: [SDW + 94] <author> Mark A. Sheldon, Andrzej Duda, Ron Weiss, James W. O'Toole, and David K. Gifford. </author> <title> A content routing system for distributed information servers. </title> <booktitle> In Proceedings of the Fourth International Conference on Extending Database Technology, </booktitle> <year> 1994. </year>
Reference-contexts: See also [DANO91]. [BC92], [OM92], and [SA89] are other examples of this type of approach in which users query "meta-information" databases. A "content-based routing" system is used in <ref> [SDW + 94] </ref> to address the resource-discovery problem. The "content routing system" keeps a "content label" for each information server (or collection of objects, more generally), with attributes describing the contents of the collection.
Reference: [SE95] <author> Erik Selberg and Oren Etzioni. </author> <title> Multi-service search and comparison using the MetaCrawler. </title> <booktitle> In Proceedings of the Fourth International WWW Conference, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: However, not all of them support the three major metasearch tasks described in Chapter 1 (i.e., selecting the best sources for a query, querying these sources, and merging the query results from the sources). Examples of meta-searchers include MetaCrawler <ref> [SE95] </ref> (http://www.metacrawler.com), SavvySearch (http://guaraldi.cs.colostate.edu:2000/), and ProFusion [GW96]. Also, the Stanford InfoBus, designed within the Digital Library project [PCGM + 96, RBC + 97], hosts a variety of metasearchers. In [BCGP97a, BCGP97b], we discuss a metadata architecture for the InfoBus.
Reference: [SEKN92] <author> Michael F. Schwartz, Alan Emtage, Brewster Kahle, and B. Clifford Neuman. </author> <title> A comparison of Internet resource discovery approaches. </title> <journal> Computer Systems, </journal> <volume> 5(4), </volume> <year> 1992. </year>
Reference-contexts: "MBasic-1" attribute set (Section 2.3.3). 7.3 Text-Source Discovery Many solutions have been presented recently for the text-source discovery problem, or, more generally, for the resource-discovery problem: the text-source discovery problem is a subcase of the resource-discovery problem, since the latter generally deals with a larger variety of types of information <ref> [ODL93, SEKN92] </ref>. One solution to the text-source discovery problem is to let the database selection be driven by the user. Thus, the user will be aware of and an active participant in this selection process.
Reference: [SFV83] <author> Gerard Salton, Edward A. Fox, and Ellen M. Voorhees. </author> <title> A comparison of two methods for Boolean query relevance feedback. </title> <type> Technical Report TR 83-564, </type> <institution> Computer Science Department, Cornell University, </institution> <month> July </month> <year> 1983. </year>
Reference: [SGM95] <author> Narayanan Shivakumar and Hector Garca-Molina. </author> <title> SCAM: A copy detection mechanism for digital documents. </title> <booktitle> In Proceedings of the Second International Conference in Theory and Practice of Digital Libraries, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The Stanford Copy Analysis Mechanism (SCAM) <ref> [SGM95, SGM96] </ref> played an important role in identifying the papers that Mr. X had plagiarized. (See [Den95] for further details.) SCAM is a registration server mechanism that helps flag document-copyright violations in Digital Libraries. <p> We have explored <ref> [SGM95, SGM96] </ref> a variety of overlap measures. For example, we can say that s and d overlap if they contain at least some fraction of common sentences. <p> More precisely, given a fixed * &gt; 2, the closeness set for s and d, c (s; d), contains the words w i with a similar number of occurrences in the two documents <ref> [SGM95] </ref>: w i 2 c (s; d) , F i (d) F i (d) &lt; * where F i (d) is the frequency of word w i in document d. If either F i (s) or F i (d) are zero, then w i is not in the closeness set. <p> Assuming * = 2:5 (a value that worked well in the experiments in <ref> [SGM95] </ref>), Accept (w 3 ; F 3 (s)) = Accept (w 3 ; 3) = [2; 5]. <p> So, for T = 0:80, SCAM would not consider d 2 to be a potential copy of s. However, SCAM would find d 1 suspiciously close to s. Even though the SCAM similarity does not take into account word sequencing, the experiments in <ref> [SGM95, SGM96] </ref> show that it detects potential copies relatively well. 6.2. THE DSCAM INFORMATION ABOUT THE DATABASES 141 In these experiments, conducted with 50,000 netnews articles, false positives were very rare: the similarity measure flagged unrelated documents as copies (because they shared common vocabulary) in only 0:01% of the cases. <p> Furthermore, such prevention schemes can be broken by using software emulators [BDGM95] and recording documents. Instead of placing restrictions on the distribution of documents, another approach to protecting digital documents (one we subscribe to) is to detect illegal copies using registration server mechanisms such as SCAM <ref> [SGM95, SGM96] </ref> or COPS [BDGM95]. Once we know a document to be an illegal copy, it is sometimes useful to know the originator of the illegal copy.
Reference: [SGM96] <author> Narayanan Shivakumar and Hector Garca-Molina. </author> <title> Building a scalable and accurate copy detection mechanism. </title> <booktitle> In Proceedings of the First ACM International Conference on Digital Libraries (DL'96), </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: The Stanford Copy Analysis Mechanism (SCAM) <ref> [SGM95, SGM96] </ref> played an important role in identifying the papers that Mr. X had plagiarized. (See [Den95] for further details.) SCAM is a registration server mechanism that helps flag document-copyright violations in Digital Libraries. <p> We have explored <ref> [SGM95, SGM96] </ref> a variety of overlap measures. For example, we can say that s and d overlap if they contain at least some fraction of common sentences. <p> In effect, F 3 (s) F 3 (d 2 ) 5 + 5 3 = 2:27 &lt; * = 2:5. For the remaining cases, 1 It also helps to ignore altogether words that occur frequently across documents <ref> [SGM96] </ref>. Our experiments of Section 6.6 use the stop words in [SGM96]. 140 CHAPTER 6. DSCAM: A NON-TRADITIONAL METASEARCHER Accept (w 1 ; F 1 (s)) = [1; 1], Accept (w 2 ; F 2 (s)) = [2; 5], and Accept (w 4 ; F 4 (s)) = [5; 17]. <p> In effect, F 3 (s) F 3 (d 2 ) 5 + 5 3 = 2:27 &lt; * = 2:5. For the remaining cases, 1 It also helps to ignore altogether words that occur frequently across documents <ref> [SGM96] </ref>. Our experiments of Section 6.6 use the stop words in [SGM96]. 140 CHAPTER 6. DSCAM: A NON-TRADITIONAL METASEARCHER Accept (w 1 ; F 1 (s)) = [1; 1], Accept (w 2 ; F 2 (s)) = [2; 5], and Accept (w 4 ; F 4 (s)) = [5; 17]. <p> So, for T = 0:80, SCAM would not consider d 2 to be a potential copy of s. However, SCAM would find d 1 suspiciously close to s. Even though the SCAM similarity does not take into account word sequencing, the experiments in <ref> [SGM95, SGM96] </ref> show that it detects potential copies relatively well. 6.2. THE DSCAM INFORMATION ABOUT THE DATABASES 141 In these experiments, conducted with 50,000 netnews articles, false positives were very rare: the similarity measure flagged unrelated documents as copies (because they shared common vocabulary) in only 0:01% of the cases. <p> Furthermore, such prevention schemes can be broken by using software emulators [BDGM95] and recording documents. Instead of placing restrictions on the distribution of documents, another approach to protecting digital documents (one we subscribe to) is to detect illegal copies using registration server mechanisms such as SCAM <ref> [SGM95, SGM96] </ref> or COPS [BDGM95]. Once we know a document to be an illegal copy, it is sometimes useful to know the originator of the illegal copy.
Reference: [SM83] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to modern information retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: Although the Boolean model of document retrieval is widely used, it is a rather primitive one. One of the most popular alternative models is the vector-space retrieval model <ref> [Sal89, SM83] </ref>. This model represents both the documents in a database and the queries themselves as weight vectors. Given a query, the documents are ranked according to how "similar" their corresponding vectors are to the given query vector. <p> As in Section 3.4.3, the definition of this section is based solely on the answers (i.e., the document ranks and their scores) that each database produces when presented with the query in question. This definition does not use the relevance <ref> [SM83] </ref> of the documents to the end user who submitted the query. Using relevance would be appropriate for evaluating the search engines at each database; instead, we are evaluating how well gGlOSS can predict the answers that the databases return. <p> CHOOSING DATABASES 83 That is, document d 1 contains the word computer with weight 0.8 (for some weight-computation algorithm <ref> [SM83] </ref>), document d 2 , with weight 0.7, and so on. <p> In our definitions below, we assume that a query q is expressed as a weight vector Q = (q 1 ; : : : ; q j ; : : : ; q t ) <ref> [SM83] </ref>, where q j is the weight of word t j in query q. For example, this weight can simply be the number of times that word t j appears in the query.
Reference: [TA94] <author> A. Tal and Rafael Alonso. </author> <title> Commit protocols for externalized-commit heterogeneous database systems. </title> <booktitle> Distributed and Parallel Databases, </booktitle> <volume> 2(2) </volume> <pages> 209-34, </pages> <month> April </month> <year> 1994. </year> <note> BIBLIOGRAPHY 189 </note>
Reference-contexts: The topics of these papers ranged from Steiner routing in VLSI CAD, to massively parallel genetic algorithms, complexity theory, and network protocols. Mr. X also plagiarized papers from the database field, notably a paper in DAPD by Tal and Alonso <ref> [TA94] </ref> on three-phase locking, a paper in VLDB '92 by Ioannidis et al. [INSS92] on parametric query optimization, and a paper in ICDE '90 by Leung and Muntz [LM90] on temporal query processing.
Reference: [TGL + 97] <author> Anthony Tomasic, Luis Gravano, Calvin Lue, Peter Schwarz, and Laura Haas. </author> <title> Data structures for efficient broker implementation. </title> <journal> ACM Transactions on Information Systems, </journal> <year> 1997. </year>
Reference-contexts: wants both high values of P and R, then Ind would be chosen for Right = Best, and Bin for Right = Matching. 3.7 GlOSS's Storage Requirements In this section we study the space requirements of GlOSS and compare them with those of a full index of the databases. (See <ref> [TGL + 97] </ref> for a study of data structures to maintain the GlOSS information efficiently both for queries and for updates.) We 3.7. <p> Thus, this section uses the metrics of this chapter to demonstrate that Boolean GlOSS can select relevant databases effectively from among a large set of candidates <ref> [TGL + 97] </ref>. For our new Boolean GlOSS experiments, we used as data the complete set of United States patents for 1991.
Reference: [VGJL95] <author> Ellen M. Voorhees, Narendra K. Gupta, and Ben Johnson-Laird. </author> <title> The collection fusion problem. </title> <booktitle> In Proceedings of the Third Text Retrieval Conference (TREC-3), </booktitle> <month> March </month> <year> 1995. </year>
Reference-contexts: An approach to address these problems is to learn from the results of training queries. Given a new query, the closest training queries are used to determine how many documents to extract from each available collection, and how to interleave them into a single document rank <ref> [VGJL95, VT97] </ref>. Another approach is to calibrate the document scores from each collection using statistics about the word distribution in the collections [CLC95].
Reference: [VT97] <author> Ellen M. Voorhees and Richard M. Tong. </author> <title> Multiple search engines in database merging. </title> <booktitle> In Proceedings of the Second ACM International Conference on Digital Libraries (DL'97), </booktitle> <month> July </month> <year> 1997. </year>
Reference-contexts: An approach to address these problems is to learn from the results of training queries. Given a new query, the closest training queries are used to determine how many documents to extract from each available collection, and how to interleave them into a single document rank <ref> [VGJL95, VT97] </ref>. Another approach is to calibrate the document scores from each collection using statistics about the word distribution in the collections [CLC95].
Reference: [WGMJ95] <author> Stuart Weibel, Jean Godby, Eric Miller, and Ron Daniel Jr. </author> <note> OCLC/NCSA metadata workshop report. Accessible at http://www.oclc.org:5047/oclc/research/publications/- weibel/metadata/dublin core report.html, </note> <month> March </month> <year> 1995. </year>
Reference-contexts: Other related efforts focus on defining attribute sets for documents and sources. As discussed in Chapter 2, we have built on some of these efforts in defining our protocol. Relevant attribute sets for documents include the Z39.50 Bib-1 attribute set [Age95], the Dublin Core <ref> [WGMJ95] </ref>, and the Warwick Framework [LLJ96]. The Bib-1 attribute set registers a large set of bibliographic attributes that have been widely adopted in library cataloging.
Reference: [YGM95a] <author> Tak W. Yan and Hector Garca-Molina. </author> <title> Duplicate detection in information dissemination. </title> <booktitle> In Proceedings of the Twenty-first International Conference on Very Large Databases (VLDB'95), </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: The target is not simply academic plagiarism, but any type of copying that can financially hurt authors and commercial publishers. SCAM is also useful for removing duplicates and near-duplicates in information retrieval systems <ref> [YGM95a] </ref>. Essentially, SCAM keeps a large database of documents along with 134 135 indices to support efficient retrieval of stored documents that are "potential copies." SCAM attempts to find not just identical copies, but also cases of "substantial" overlap.
Reference: [YGM95b] <author> Tak W. Yan and Hector Garca-Molina. </author> <title> SIFT-a tool for wide-area information dissemination. </title> <booktitle> In Proceedings of the 1995 USENIX Technical Conference, </booktitle> <pages> pages 177-86, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: The queries that we used where profiles that real users submitted to the SIFT Netnews server developed at Stanford <ref> [YGM95b] </ref>. Users send profiles in the form of Boolean or vector-space queries to the SIFT server, which in turn filters Netnews articles every day and sends the articles matching the profiles to the corresponding users. We used the 6800 vector-space profiles that were active on the server in December 1994.
Reference: [YJGMD96] <author> Tak W. Yan, Matthew Jacobsen, Hector Garca-Molina, and Umeshwar Dayal. </author> <title> From user access patterns to dynamic hypertext linking. </title> <booktitle> In Proceedings of the Fifth International World Wide Web Conference, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Other sources of information to employ include available query logs, response times, user feedback, and quality reports. For example, initial work on mining query logs tries to predict what pages are likely to be useful to users based on their browsing behavior [Lie95], and that of previous users <ref> [YJGMD96] </ref>. Source Discovery over Uncooperative Text Sources Metasearchers choose the best sources to evaluate queries by using summaries of each source's contents, assuming that they can extract these summaries with the sources' cooperation by using the STARTS protocol of Chapter 2, for example.
Reference: [YL96] <author> Budi Yuwono and Dik L. Lee. </author> <title> Search and ranking algorithms for locating resources on the World Wide Web. </title> <booktitle> In Proceedings of the Twelfth International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1996. </year> <note> 190 BIBLIOGRAPHY </note>
Reference-contexts: The more times a page is cited, the more important this page is considered by this system. Furthermore, if an "important" page points to another page, the latter inherits part of the former's importance. Citation information has also been used, together with other factors, in <ref> [YL96] </ref>. A fascinating research issue is how to use all the information available on the WWW to do a better job at ranking pages for queries. The page citations, coupled with additional knowledge available on the WWW, contain valuable nuggets of information to be mined.
Reference: [Z3997] <author> ZDSR profile: </author> <title> Z39.50 profile for simple distributed search and ranked retrieval, </title> <type> Draft 5, </type> <month> March </month> <year> 1997. </year> <note> Accessible at http://lcweb.loc.gov/- z3950/agency/profiles/zdsr.html. </note>
Reference-contexts: As we mentioned in Chapter 2, there are currently efforts under way to define a simple profile of the Z39.50 standard based on STARTS <ref> [Z3997] </ref>. In addition to the Z39.50 standard effort, other projects focus on providing a framework for indexing and querying multiple document sources. One such project, Harvest [BDH + 94], includes a set of tools for gathering and accessing information on the Internet.
Reference: [ZC92] <author> Sajjad Zahir and Chew Lik Chang. Online-Expert: </author> <title> An expert system for online database selection. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 43(5) </volume> <pages> 340-357, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: RELATED WORK In [FY93], every site keeps statistics about the type of information it receives along each link connecting to other sites. When a query arrives in a site, it is forwarded through the most promising link according to these statistics. References [MDT93], <ref> [ZC92] </ref>, and [MTD92] follow an expert-systems approach to solving the related problem of selecting online business databases. A complementary approach to GlOSS is taken by Chamis [Cha88]. Briefly, the approach this paper takes is to expand a user query with thesaurus terms.
References-found: 87

