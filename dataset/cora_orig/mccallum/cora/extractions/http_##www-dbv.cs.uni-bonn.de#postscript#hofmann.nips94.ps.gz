URL: http://www-dbv.cs.uni-bonn.de/postscript/hofmann.nips94.ps.gz
Refering-URL: http://www-dbv.cs.uni-bonn.de/abstracts/hofmann.scaling.nips.95.html
Root-URL: http://cs.uni-bonn.de
Email: email:fth,jbg@cs.uni-bonn.de  
Title: Multidimensional Scaling and Data Clustering  
Author: Thomas Hofmann Joachim Buhmann Rheinische Friedrich-Wilhelms-Universitat 
Address: D-53117 Bonn, Germany  
Affiliation: Institut fur Informatik III, Romerstrae 164  
Abstract: Visualizing and structuring pairwise dissimilarity data are difficult combinatorial optimization problems known as multidimensional scaling or pairwise data clustering. Algorithms for embedding dissimilarity data set in a Euclidian space, for clustering these data and for actively selecting data to support the clustering process are discussed in the maximum entropy framework. Active data selection provides a strategy to discover structure in a data set efficiently with partially unknown data. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Buhmann, J., Hofmann, T. </author> <year> (1994a). </year> <title> Central and Pairwise Data Clustering by Competitive Neural Networks. </title> <booktitle> Pages 104-111 of: Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The coordinates in the embedding space are the free parameters for this optimization problem. Clustering of non-metric dissimilarity data, also called pairwise clustering <ref> (Buhmann, Hof-mann, 1994a) </ref>, is a combinatorial optimization problem which depends on Boolean assignments M i 2 f0; 1g of datum i to cluster .
Reference: <author> Buhmann, J., Hofmann, T. </author> <year> (1994b). </year> <title> A Maximum Entropy Approach to Pairwise Data Clustering. </title> <booktitle> Pages 207-212 of: Proceedings of the International Conference on Pattern Recognition, </booktitle> <address> Hebrew University, Jerusalem, </address> <booktitle> vol. II. </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: This method is equivalent to minimizing an upper bound on the free energy <ref> (Buhmann, Hofmann, 1994b) </ref>, F (E K ) F 0 (E 0 K ) + hV K i 0 ; with V K = E pc K ; (10) h:i 0 denoting the average over all configurations of the cost function without interactions. <p> The described algorithm for simultaneous Euclidian embedding and data clustering can be used for dimensionality reduction, e.g., high dimensional data can be projected to a low dimensional subspace in a nonlinear fashion which resembles local principle component analysis <ref> (Buhmann, Hofmann, 1994b) </ref>. Figure (1) shows the clustering result for a real-world data set of 145 protein sequences. The similarity values between pairs of sequences are determined by a sequence alignment program which takes biochemical and structural information into account.
Reference: <author> Gower, J. C. </author> <year> (1966). </year> <title> Some distance properties of latent root and vector methods used in multivariate analysis. </title> <journal> Biometrika, </journal> <volume> 53, </volume> <pages> 325-328. </pages>
Reference-contexts: The phenomenon that the data clusters are arranged in a circular fashion is explained by the lack of small dissimilarity values. The solution in Fig. 1d is about a factor of two better than the embedding found by a classical MDS program <ref> (Gower, 1966) </ref>. This program determines a (N 1)- space where the ranking of the dissimilarities is preserved and uses principle component analysis to project this tentative embedding down to two dimensions.
Reference: <author> Hertz, J., Krogh, A., Palmer, R. G. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <address> New York: </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: All algorithms are derived from the maximum entropy principle <ref> (Hertz et al., 1991) </ref> which guarantees robust statistics (Tikochinsky et al., 1984). The data are given by a real-valued, symmetric proximity matrix D 2 IR NfiN , D kl being the pairwise dissimilarity between the data points k; l.
Reference: <author> Tikochinsky, Y., Tishby, N.Z., Levine, R. D. </author> <year> (1984). </year> <title> Alternative Approach to Maximum-Entropy Inference. </title> <journal> Physical Review A, </journal> <volume> 30, </volume> <pages> 2638-2644. </pages>
Reference-contexts: All algorithms are derived from the maximum entropy principle (Hertz et al., 1991) which guarantees robust statistics <ref> (Tikochinsky et al., 1984) </ref>. The data are given by a real-valued, symmetric proximity matrix D 2 IR NfiN , D kl being the pairwise dissimilarity between the data points k; l.
References-found: 5

