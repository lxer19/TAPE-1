URL: http://www.cs.wisc.edu/~shatdal/papers/vldb94.ps
Refering-URL: http://www.cs.wisc.edu/~shatdal/shatdal.html
Root-URL: 
Email: fshatdal,ck,naughtong@cs.wisc.edu  
Title: Cache Conscious Algorithms for Relational Query Processing  
Author: Ambuj Shatdal Chander Kant Jeffrey F. Naughton 
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: The current main memory (DRAM) access speeds lag far behind CPU speeds. Cache memory, made of static RAM, is being used in today's architectures to bridge this gap. It provides access latencies of 2-4 processor cycles, in contrast to main memory which requires 15-25 cycles. Therefore, the performance of the CPU depends upon how well the cache can be utilized. We show that there are significant benefits in redesigning our traditional query processing algorithms so that they can make better use of the cache. The new algorithms run 8%-200% faster than the traditional ones.
Abstract-found: 1
Intro-found: 1
Reference: [BE77] <author> M. W. Blasgen and K. P. Eswaran. </author> <title> Storage and access in relational databases. </title> <journal> IBM Systems Journal, </journal> <volume> 16(4), </volume> <year> 1977. </year>
Reference-contexts: In this section we describe the algorithms, point out the optimizations we made and the speedup obtained on the four machines. 4.2.1 The Sort Merge Join The in-memory sort merge join <ref> [BE77] </ref> works as follows. First, both relations R and S are sorted on the join attribute by using an efficient sorting mechanism e.g. quicksort. Then the sorted relations are merged and the matching tuples are output.
Reference: [DKO + 84] <author> David J. DeWitt, Randy H. Katz, Frank Olken, Lenard D. Shapiro, Michael R. Stonebraker, and David Wood. </author> <title> Implementation techniques for main memory database systems. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 1-8, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: The number of groups was 20. In section 4.3 we show that our results hold even when we vary these parameters. All reported timings are in seconds. 4.1 Case Study: Optimizing Hash Joins We describe how we optimized the basic in-memory hash join algorithm <ref> [DKO + 84] </ref> in detail on the DECsta-tion 5000/125. <p> This implies that an infinite (or no cache) would actually show that 1 A keen observer would note that this is analogous to the GRACE algorithm <ref> [DKO + 84] </ref> for join processing of disk resident relations. the basic algorithm is the best. However, this also shows the importance of cache optimization, because it demonstrates that theoretically similar algorithms can have significantly differing performance depending on the way they utilize the cache.
Reference: [Eps79] <author> Robert Epstein. </author> <title> Techniques for Processing of Aggregates in Relational Database Systems. </title> <institution> Memorandum UCB/ERL M79/8, Electronics Research Laboratory, College of Engineering, University of California, Berkeley, </institution> <month> February </month> <year> 1979. </year>
Reference-contexts: 10/51 doesn't show much improvement is because of the improved performance of the base case as all the key pointers fit in the 1MB secondary cache even in the base case. 4.2.3 Aggregation Algorithms Aggregation with the group by clause involves more than a simple scan of the participating relation <ref> [Eps79] </ref>. We consider the two popular aggregation algorithms: the hash based aggregation and the sort based aggregation. Hash Based Aggregation The in-memory hash aggregation on relation R works as follows.
Reference: [HS89] <author> Mark D. Hill and Alan Jay Smith. </author> <title> Evaluating Associativity in CPU Caches. </title> <journal> IEEE Transaca-tions on Computers, </journal> <volume> 38(12) </volume> <pages> 1612-1630, </pages> <month> Decem-ber </month> <year> 1989. </year>
Reference-contexts: In associative caches, LRU replacement policy is used to decide which cache block will be replaced. Most caches, in practice, are either direct mapped or have very small set-associativity. Cache misses can be categorized into following three disjoint types <ref> [HS89] </ref>. The relation of the cache miss types to the cache characteristics is also described. Compulsory A reference that misses because it is the very first reference to a cache block is classified as a compulsory miss. By definition, compulsory misses can not be reduced without changing the basic algorithm.
Reference: [KH92] <author> R. E. Kessler and Mark D. Hill. </author> <title> Page Placement Algorithms for Real-Indexed Caches. </title> <journal> ACM Transactions in Computer Systems, </journal> <volume> 10(4) </volume> <pages> 338-359, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: However, since this simulation is based on virtual addresses, the results are only approximately true for a physically addressed Page 2 cache (like the level 2 cache in the DEC 3000) as non-conflicting virtual addresses may conflict in a physically mapped cache. <ref> [KH92] </ref> shows that this effect is minor in most cases (especially when virtual address space is much larger than the cache size, which holds for typical database applications) and operating systems can use simple techniques to overcome the problem of introduction of cache conflicts due to virtual to physical memory mapping.
Reference: [Lar93] <author> James R. Larus. </author> <title> Efficient Program Tracing. </title> <journal> IEEE Computer, </journal> <volume> 26(5) </volume> <pages> 52-61, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Increasing the associativity of a cache will decrease the conflict misses. A cache profiler like cprof [LW94] finds the cache behavior of an algorithm by simulating all data accesses in the appropriately configured cache. The address traces are generated by an instruction level profiler like qpt <ref> [Lar93] </ref> which are fed to a cache simulator which finds the requisite properties of the data references. Cprof classifies the cache misses in the above categories by every line of code and by every data structure.
Reference: [LW94] <author> Alvin R. Lebeck and David A. Wood. </author> <title> Cache Profiling and the SPEC Benchmarks: A Case Study. </title> <note> IEEE Computer (to appear), </note> <month> June </month> <year> 1994. </year>
Reference-contexts: Once the designer is aware of the presence of cache and its behavior, some techniques do not seem arcane. Page 1 However, in general, cache behavior is fairly complex and one needs to use some cache profiling tool like cprof <ref> [LW94] </ref> as an aid to study the cache behavior of a particular algorithm. Sometimes, we find ourselves revisiting some of the same optimizations in a different guise as made for the memory/disk portion of the memory hierarchy. <p> Data structures would, in general, have to be remapped so as to minimize conflicting addresses. Increasing the associativity of a cache will decrease the conflict misses. A cache profiler like cprof <ref> [LW94] </ref> finds the cache behavior of an algorithm by simulating all data accesses in the appropriately configured cache. The address traces are generated by an instruction level profiler like qpt [Lar93] which are fed to a cache simulator which finds the requisite properties of the data references. <p> All reported timings are in seconds. 4.1 Case Study: Optimizing Hash Joins We describe how we optimized the basic in-memory hash join algorithm [DKO + 84] in detail on the DECsta-tion 5000/125. We used the cache profiler cprof <ref> [LW94] </ref> to gain detailed information about the cache perfor Page 4 Machine Microprocessor Data Cache Size DECstation 5000/125 MIPS R3000 64K SUN Sparcstation 10/51 SPARC/Viking 16K (on-chip) + 1M DEC 3000/300 Alpha AXP A 8K (on-chip) + 256K HP Apollo 9000 Series 700/710 HPPA-RISC 1.1 64K Table 1: Machine Configurations mance
Reference: [NBC + 94] <author> Chris Nyberg, Tom Barclay, Zarca Cvetanovic, Jim Gray, and Dave Lomet. AlphaSort: </author> <title> A RISC Machine Sort. </title> <booktitle> In Proc. of the 1994 ACM SIG-MOD Conf., </booktitle> <pages> pages 233-242, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: We have challenged the conventional beliefs by showing that by designing cache conscious algorithms one can significantly speed up the CPU processing portion of query processing. For main-memory database systems (or largely-memory resident database systems) this is very significant. Further, the recent work by Nyberg et al. <ref> [NBC + 94] </ref> suggests that the I/O response time can be reduced through the use of software assisted disk striping thus making the CPU cost of the query processing dominate i.e. a relation can be read into the memory faster than it is processed. <p> This clearly makes cache optimizations, which speed up CPU processing, extremely relevant for disk-resident data also. In related work, Nyberg et al. <ref> [NBC + 94] </ref> have shown that for achieving high performance sorting, one should worry about cache memory. They have emphasized a large cache and do not explore alternative optimization techniques. In some sense, our work picks up where they have left off. <p> These partitions are created such that each partition fits in the cache. Alpha-Sort <ref> [NBC + 94] </ref> uses this technique to speedup the in-memory sorting. There is an overhead of creating the partitions but in most cases the benefit gained overshadows it. In many database algorithms, a good way of generating partitions is by hash partitioning the relations. <p> This more than compensates for the extra merge step resulting in greater overall sorting speed (for large enough N). This is the essence of the in-memory AlphaSort <ref> [NBC + 94] </ref> algorithm. 2 Note that blocking and partitioning are distinct techniques. In blocking we restructure the algorithm and do not change the layout of the data, whereas in partitioning we reorganize the layout of the data to make maximum use of the cache. <p> This optimization can be taken a little further as in using only key prefixes instead of keys for sorting <ref> [NBC + 94] </ref>. For example, from a 200 byte record, we could extract an 12 byte key and a 4 byte pointer to do the sorting. <p> First, both relations R and S are sorted on the join attribute by using an efficient sorting mechanism e.g. quicksort. Then the sorted relations are merged and the matching tuples are output. As mentioned earlier, we use the optimization proposed in <ref> [NBC + 94] </ref> to extract the join attribute and a pointer to the tuple. The basic algorithm sorts both the relations and merges them.
Reference: [Smi82] <author> Alan J. Smith. </author> <title> Cache Memories. </title> <journal> Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: In section 4 we present the query processing algorithms and study how cache optimization helps. Section 5 offers our conclusions. 2 Overview of Cache Memories Cache memories are small, fast static RAM memories that improve program performance by holding recently referenced data <ref> [Smi82] </ref>. Memory references satisfied by the cache, called hits, proceed at processor speed; those unsatisfied, called misses, incur a cache miss penalty and have to fetch the corresponding cache block from the main memory. The management of the cache is done entirely by the hardware with no direct user control.
Reference: [Val87] <author> Patrick Valduriez. </author> <title> Join Indices. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 12(2):218 - 246, </volume> <month> June </month> <year> 1987. </year> <pages> Page 12 </pages>
Reference-contexts: Under these assumptions, in practice and in our study, an algorithm starts its processing on the relations stored in the buffer pool. The join result is left in the form of an in-memory join index <ref> [Val87] </ref>. In section 4.4 we discuss the tradeoffs and options in the generation of result relation. Furthermore, we incorporated the optimization of extracting the join attribute (group by attribute in case of aggregation) from the tuples for the processing whenever it was appropriate. <p> generation in join algorithms. 1. the result tuple is produced on the fly (e.g. as soon as a match is found in case of a join). 2. upon finding a match two pointers to the participating tuples are stored along with a projection list thus generating an in-memory join index <ref> [Val87] </ref>. Later, depending upon need, the result is generated by accessing the pointed to tuples and doing the projection. This can be considered as lazy eval uation of the result relation.
References-found: 10

