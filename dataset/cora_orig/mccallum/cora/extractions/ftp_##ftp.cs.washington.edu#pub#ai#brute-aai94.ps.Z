URL: ftp://ftp.cs.washington.edu/pub/ai/brute-aai94.ps.Z
Refering-URL: http://www.cs.washington.edu/homes/segal/resume.html
Root-URL: 
Title: Representation Design and Brute-force Induction in a Boeing Manufacturing Domain  
Author: Patricia Riddle Richard Segal Oren Etzioni 
Date: 8:125-147, 1994.  
Note: Appears in Applied Artificial Intelligence,  
Address: MS 7L-66 Seattle, WA 98124-0346  Seattle, WA 98195  
Affiliation: Boeing Computer Services  Dept. of Comp. Sci. Eng. University of Washington  
Abstract: We applied inductive classification techniques to data collected in a Boeing plant with the goal of uncovering possible flaws in the manufacturing process. This application led us to explore two aspects of classical decision-tree induction: (1) Preprocessing and postprocessing and (2) brute-force induction. For preprocessing and postprocess ing, much of our effort was focused on the pre-processing of raw data to make it suitable for induction and the post-processing of learned rules to make them useful to factory personnel. For brute-force induction, in contrast with standard methods, which perform a greedy search of the space of decision trees, we formulated an algorithm that conducts an exhaustive, depth-bounded search for accurate predictive rules. We demonstrate the efficacy of our approach with specific examples of learned rules and by quantitative comparisons with decision-tree algorithms (C4 and CART). fl This research was funded in part by a Boeing Computer Services contract with the University of Wash-ington, by Office of Naval Research Grant 92-J-1946, and by National Science Foundation Grants IRI-9211045 and IRI-9357772 (NYI Award to Etzioni). Richard Segal is supported, in part, by a GTE fellowship. The C routines, implementing the Brute and Gold-digger algorithms, are available by sending mail to segal@cs.washington.edu. We are grateful to Wray Buntine for distributing his IND package, which contains re-implementations of C4 and CART. IND greatly facilitated our research. Thanks are due to Ruth Etzioni for her expert advice on statistical testing, to Usama Fayyad and Jeff Schlimmer for helpful suggestions, and to Mike Barley for many fruitful discussions and reviews of drafts of this paper. Finally, we acknowledge the other members of the Boeing project: Mike Healy, Dave Newman, and Carl Pearson. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J., </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth. </publisher>
Reference-contexts: Section 3.1.5 discusses setting the parameters of the induction algorithms. 3.1.1 Choosing Instances In most traditional machine learning domains, the definition of an instance falls naturally out of the domain. One of the traditional machine learning data sets is the Cancer data set <ref> ( Breiman et al., 1984 ) </ref> . An instance could be a person and all his symptoms and test results over the entire year, or an instance could be all the symptoms and test results performed during a particular day over all people. <p> To evaluate each test, this function averages the purity of each branch, weighted by the number of examples that satisfy the test value at the branch. Although the exact measure of purity varies from one algorithm to another, standard measures exhibit fairly similar behavior in practice <ref> ( Breiman et al., 1984, Mingers, 1989 ) </ref> . Test selection functions of this form can lead decision-tree algorithms to overlook valuable nuggets for several reasons. <p> The following purity function, which simply computes the proportion of positive examples, has the desired behavior: P (i) = p i + n i Note that although our choice for P (i) could be approximated by introducing the appropriate priors or misclassification costs into classical selection functions <ref> ( Breiman et al., 1984 ) </ref> , choosing the test with the maximal accuracy branch cannot be approximated. <p> By only expanding one branch, Gold-digger avoids the pitfalls associated with trying to build a balanced tree. Finally, Gold-digger utilizes a pruning method very similar to that used by CART <ref> ( Breiman et al., 1984 ) </ref> . A pseudo-code description of Gold-digger appears in Table 1. As described thus far, Gold-digger would only find a single predictive rule. This is unsatisfactory since there may be multiple nuggets to be uncovered in the data.
Reference: <author> Buntine, W. and Caruana, R., </author> <year> 1991. </year> <title> Introduction to IND and recursive partitioning. </title> <institution> NASA Ames Research Center, Mail Stop 269-2 Moffet Field, </institution> <address> CA 94035. </address>
Reference-contexts: We will discuss the obstacles involved in preprocessing the data (e.g., tuning the representation) and postprocessing the results (e.g., filtering out uninteresting patterns). We will discuss an example of this process where rules for process improvement were learned for a Boeing manufacturing environment. Initially we used IND <ref> ( Buntine and Caruana, 1991 ) </ref> for inductive learning but later developed two new algorithms which are better suited to our domain. The paper is organized as follows. In section 2 we discuss the Boeing manufacturing domain. <p> The decision-tree output of C4 or CART is automatically converted to the equivalent rule set. Due to the nature of the application, we only consider rules that predict a positive classification. Note that C4 and CART refer to the IND <ref> ( Buntine and Caruana, 1991 ) </ref> re-implementations of these algorithms. 12 The results shown are averaged over the runs that did not collapse. Gold-digger performed better than either CART or C4 on the occupancy data. <p> In their domain, adding two user defined attributes increased accuracy so significantly that they chose to automate the process. They derived new attributes by applying induction recursively. <ref> ( Buntine and Stirling, 1991 ) </ref> discusses altering inductively learned rules to make them more acceptable to human experts. In particular, they discuss the importance of generating statistics concerning the applicability and confidence in each rule.
Reference: <author> Buntine, W. and Stirling, D., </author> <year> 1991. </year> <title> Interactive induction. </title> <journal> Machine Intelligence, </journal> <volume> 12 </volume> <pages> 121-137. </pages>
Reference-contexts: We will discuss the obstacles involved in preprocessing the data (e.g., tuning the representation) and postprocessing the results (e.g., filtering out uninteresting patterns). We will discuss an example of this process where rules for process improvement were learned for a Boeing manufacturing environment. Initially we used IND <ref> ( Buntine and Caruana, 1991 ) </ref> for inductive learning but later developed two new algorithms which are better suited to our domain. The paper is organized as follows. In section 2 we discuss the Boeing manufacturing domain. <p> The decision-tree output of C4 or CART is automatically converted to the equivalent rule set. Due to the nature of the application, we only consider rules that predict a positive classification. Note that C4 and CART refer to the IND <ref> ( Buntine and Caruana, 1991 ) </ref> re-implementations of these algorithms. 12 The results shown are averaged over the runs that did not collapse. Gold-digger performed better than either CART or C4 on the occupancy data. <p> In their domain, adding two user defined attributes increased accuracy so significantly that they chose to automate the process. They derived new attributes by applying induction recursively. <ref> ( Buntine and Stirling, 1991 ) </ref> discusses altering inductively learned rules to make them more acceptable to human experts. In particular, they discuss the importance of generating statistics concerning the applicability and confidence in each rule.
Reference: <author> Evans, B. and Fisher, D. </author> <title> Process delay analysis using decision tree induction. </title> <note> To appear in IEEE Expert. </note>
Reference: <author> Fayyad, U., Doyle, R., Weir, W. N., and Djorgovski, S., </author> <year> 1992. </year> <title> Applying machine learning classification techniques to automate sky object cataloguing. </title> <booktitle> In Proceedings of International Space Year Conference on Earth & Space Science Information Systems. </booktitle>
Reference-contexts: In (Evans and Fisher, In press), Evans and Fisher discuss how they determined the class attributes Banded and NotBanded. They also discuss the iterative process they used for attribute selection. <ref> ( Fayyad et al., 1992 ) </ref> discusses the use of derived attributes. In their domain, adding two user defined attributes increased accuracy so significantly that they chose to automate the process.
Reference: <author> Holte, R. C., </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 63-90. </pages>
Reference-contexts: As the feature space gets too large, it becomes infeasible to run brute without limiting its beam. Whether Beam-Brute is a better choice than other approximate algorithms depends mostly on the size of the 18 domain and how many conjuncts the rules must contain to be effective. Recent work <ref> ( Holte, 1993 ) </ref> has suggested that many domains are adequately represented by surprisingly short rules. 5 The Boeing Success Story of the form "If W, then it is N times more likely to Y," means that when W is true, Y is N times more likely to occur than the
Reference: <author> McAllester, D. and Rosenblitt, D., </author> <year> 1991. </year> <title> Systematic nonlinear planning. </title> <booktitle> In Proceedings of AAAI-91, </booktitle> <pages> pages 634-639. </pages>
Reference-contexts: Brute would also learn many variations of rules that use continuous attributes if it were left unchecked. Often a rule containing a test of the form val &lt; Y will have similar perfor 5 Avoiding redundant search is referred to as systematicity in the planning literature <ref> ( McAllester and Rosenblitt, 1991 ) </ref> . Brute is systematic in this sense. 15 mance to the same rule with this test replaced with val &lt; Z, where Z is numerically close to Y .
Reference: <author> Mingers, J., </author> <year> 1989. </year> <title> An empirical comparison of selection measures for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 319-342. </pages>
Reference: <author> Pagallo, G. and Haussler, D., </author> <year> 1990. </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-100. </pages>
Reference-contexts: In summary, we believe that, when searching for nuggets, the max-accuracy test-selection function shown below is preferable to standard functions. 2 max-accuracy (T; E) = max t2T max p i ! This function is equivalent to the select-literal function employed by GREEDY3 <ref> ( Pagallo and Haussler, 1990, page 85 ) </ref> . We make one modification. The function in Eq. 1 runs the 2 Note that the function applies readily to multi-outcome tests and to multi-class data sets. <p> However, a single exhaustive search would not work for learning decision lists because the performance of each rule must be considered in relation to its position in the decision list. GREEDY3, developed by Pagallo and Haussler, also extends Rivest's algorithm to do greedy rather than exhaustive search <ref> ( Pagallo and Haussler, 1990 ) </ref> . The result is very much like Gold-digger. GREEDY3 uses the same iterative structure as Gold-digger and uses a selection function very similar to max-accuracy. GREEDY3's selection function differs from Gold-digger's in that it does not have a MinPositives parameter.
Reference: <author> Quinlan, J. R., </author> <year> 1986. </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106. </pages> <note> 23 Rivest, </note> <author> R., </author> <year> 1987. </year> <title> Learning decision trees. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 229-246. </pages>
Reference-contexts: T can exceed 1,000 in real-world applications, implying that any straightforward enumeration of decision trees is impractical <ref> ( Quinlan, 1986, page 87 ) </ref> . However, we are seeking concise conjunctive rules. The number of conjunctive rules is singly exponential in T , and the number of concise conjunctive rules is even smaller.
Reference: <author> Schlimmer, J. C., </author> <year> 1991. </year> <title> Database consistency via inductive learning. </title> <editor> In Birnbaum, Lawrence A. and Collins, Gregg C., editors, </editor> <booktitle> Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> pages 640-644. </pages>
Reference: <author> Schlimmer, J. C., </author> <year> 1991. </year> <title> Learning meta-knowledge for database checking. </title> <booktitle> In Proceedings of AAAI-91, </booktitle> <pages> pages 335-340. </pages>
Reference: <author> Schlimmer, J. C., </author> <year> 1993. </year> <title> Efficiently inducing determinations: A complete and systematic search algorithm that uses optimal pruning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, MA. </address>
Reference-contexts: Brute does this by removing rules statistically similar to their parent rules and by removing rules that perform worse than the same rule with different values for their numeric comparisons. Finally, a recent paper by Schlimmer uses exhaustive depth-bounded search to find determinations <ref> ( Schlimmer, 1993 ) </ref> . A determination for an attribute is a set of attributes which are relevant for determining the attribute's value.
Reference: <author> Smyth, P. and Goodman, R. M., </author> <year> 1991. </year> <title> Rule induction using information theory. </title> <booktitle> In Knowledge Discovery in Databases, </booktitle> <pages> pages 159-176. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Brute's restriction to purely conjunctive rules greatly reduces the search space, allowing greater depths to be pursued. Smyth and Goodman also recognized that exhaustive search of conjunctive rules can effectively be used for searching for nuggets <ref> ( Smyth and Goodman, 1991 ) </ref> . Their system, ITRULE, also performs a depth bounded exhaustive search for good predictive rules. ITRULE uses an evaluation function based on information theory that has a greater bias 21 towards finding high-coverage rules than our max-accuracy function.
Reference: <author> Weiss, S. M., Galen, R. S., and Tadepalli, P. V., </author> <year> 1990. </year> <title> Maximizing the predictive value of production rules. </title> <journal> Artificial Intelligence, </journal> <volume> 45 </volume> <pages> 47-71. 24 </pages>
Reference-contexts: However, CARPER does this as an efficiency optimization, not as a means of improving the classification accuracy of the algorithm. CARPER does not share Gold-digger's iterative structure or the max-accuracy function. One of the first systems to apply massive search to learning is Weiss, Galen, and Tade-palli's PVM system <ref> ( Weiss et al., 1990 ) </ref> . PVM uses a beam search and pruning heuristics to do a semi-exhaustive search for short classification rules. PVM differs from Brute in that PVM is designed to find classification rules rather than predictive rules and because its search is not complete.
References-found: 15

