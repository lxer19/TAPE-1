URL: http://www-swiss.ai.mit.edu/ftpdir/users/lyn/slag.ps
Refering-URL: http://www-swiss.ai.mit.edu/~lyn/slag.html
Root-URL: 
Email: lyn@lcs.mit.edu  
Title: Synchronized Lazy Aggregates  
Author: Franklyn Turbak 
Date: December 1, 1993  
Note: Draft of  
Affiliation: Laboratory for Computer Science, Massachusetts Institute of Technology  
Abstract: We present a new solution to the problem of expressing single-pass, space-efficient list and tree computations in a modular fashion. Synchronized lazy aggregates augment existing methods for expressing programs as networks of operators acting on aggregate data, but overcome many limitations. The technique is based on a dynamic model of lock step processing that enables the operators in a network to simulate the operational behavior of a monolithic loop or recursion. The key to the technique is the synchron, a new first-class object that allows concurrently executing operators to participate in a barrier synchronization. We describe the design of synchronized lazy aggregates and present examples of their implementation and use. 
Abstract-found: 1
Intro-found: 1
Reference: [Ada91] <author> Stephen Adams. </author> <title> Modular Grammars for Programming Language Prototyp-ing. </title> <type> PhD thesis, </type> <institution> Department of Electronics and Computer Science, University of Southampton, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Modular attribute grammars <ref> [DC90, FMY92, Ada91] </ref> specify tree decoration programs in a declarative, modular fashion. Modular attribute grammars allow different forms of tree processing to be interwoven. Although some versions limit number of traversals or space consumption, most approaches provide little control over these fundamental operational issues.
Reference: [ANP89] <author> Arvind, Rishiyur S. Nikhil, and Ke shav K. Pingali. I-structures: </author> <title> Data structures for parallel computing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 598-632, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: In the channel approach, devices are wired together by some sort of explicit communication channel. This approach encompasses communicating threads (e.g. [Bir89], CSP [Hoa85], ML threads [CM90]), producer/consumer coroutines (e.g., Unix pipes [KP84], CLU iterators [L + 79], Id's I-structures <ref> [ANP89] </ref> and M-structures [Bar92]), and dataflow techniques (e.g., [Den75], [WA85]). <p> Since all processes lose access to the synchron after the rendezvous, there can only be one rendezvous per synchron. The rendezvous protocol of synchrons sets it apart from other synchronization structures (e.g., semaphores/locks [Bir89], synchronous messages [Hoa85], I-structures <ref> [ANP89] </ref>, and M-structures [Bar92]). Synchronization typically involves some processes waiting in a suspended state for a shared synchronization entity to be released by the process that currently owns it. Traditional protocols supply explicit wait and release operations.
Reference: [ASS85] <author> Harold Abelson, Gerald Jay Sussman, and Julie Sussman. </author> <title> Structure and Interpretation of Computer Programs. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: In the aggregate data approach, SPS devices are represented as operators (e.g., generate, map, filter, accumulate) that manipulate aggregate data structures (e.g., lists, trees, arrays). This approach includes APL's array operators [Ive87], Common Lisp's series procedures [Wat90], Scheme's stream procedures <ref> [ASS85] </ref>, and Haskell's list comprehensions [HJW + 92]. The major problem with existing aggregate data techniques is their inability to express operational control. Strict operators processing large intermediate structures in a staged fashion cannot simulate the time or space profiles of a monolithic recursion. <p> tight coupling of slags can be flexibly mixed with the loose coupling afforded 3 There are methods of encoding trees as linear streams, but manipulations of the resulting streams often don't accurately reflect the tree-shaped nature of the corresponding monolithic computations. by other mechanisms (such as lazy lists and trees <ref> [ASS85, Hug90] </ref>). Synchronized lazy aggregates are similar in spirit to Waters's series extension to Common Lisp [Wat90, Wat91]. However, whereas series can express only iterations, slags can express general linear and tree recursions. There is also a difference in focus between the systems. <p> The key feature of the trace is the interleaving of the 1+ and + operations; in a modular program based on Scheme streams (a kind of lazy list) <ref> [ASS85] </ref> rather than SLLs, fan-out would prevent these operations from being interleaved and would result in unnecessary buffering. 7 The traces presented here were automatically generated by our prototype Opera interpreter. 8 They do not include the operations needed to manage slags. <p> GenL bundles fresh synchrons (created by the synchron constructor) with a delayed computation that generates the next element of the SLL. A delayed computation is created by lazon, which is similar to Scheme's delay <ref> [ASS85] </ref> except that it is implicitly forced in contexts requiring its value. MapL uses the rebundle abstraction to propagate synchrons from its input SLL to its output SLL.
Reference: [Axe86] <author> Tim S. Axelrod. </author> <title> Effects of synchroniza tion barriers on multiprocessor performance. </title> <journal> Parallel Computing, </journal> <volume> 3(2) </volume> <pages> 129-140, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: So multi-tasking on a single physical processor is sufficient for our purposes. * Synchronization: The barrier synchronization among operators at calls and returns requires a novel form of synchronization. 5 In traditional barrier synchronization <ref> [Axe86] </ref>, the number of participating processes is known in advance, and synchronization can easily be implemented by a counter. However, due to the dynamic nature of slag components, the number of operators participating in the barrier synchronization cannot generally be predicted from the program.
Reference: [Bar92] <author> Paul S. Barth. </author> <title> Atomic data struc tures for parallel computing. </title> <type> Technical Report MIT/LCS/TR-532, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: In the channel approach, devices are wired together by some sort of explicit communication channel. This approach encompasses communicating threads (e.g. [Bir89], CSP [Hoa85], ML threads [CM90]), producer/consumer coroutines (e.g., Unix pipes [KP84], CLU iterators [L + 79], Id's I-structures [ANP89] and M-structures <ref> [Bar92] </ref>), and dataflow techniques (e.g., [Den75], [WA85]). <p> Since all processes lose access to the synchron after the rendezvous, there can only be one rendezvous per synchron. The rendezvous protocol of synchrons sets it apart from other synchronization structures (e.g., semaphores/locks [Bir89], synchronous messages [Hoa85], I-structures [ANP89], and M-structures <ref> [Bar92] </ref>). Synchronization typically involves some processes waiting in a suspended state for a shared synchronization entity to be released by the process that currently owns it. Traditional protocols supply explicit wait and release operations.
Reference: [Bel84] <author> Francoise Bellegarde. </author> <title> Rewriting sys tems on FP expressions that reduce the number of sequences they yield. </title> <booktitle> In Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 63-73. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1984. </year>
Reference-contexts: In fact, Hughes shows that any sequential functional language must exhibit undesirable buffering for networks with fan-out [Hug83]. Transformations that remove intermediate aggregates from a program either provide no guarantees as to what will be removed (e.g., <ref> [Bel84] </ref>) or limit the class of expressible computations (e.g., Waters's series compiler [Wat90] handles only iterative computations; deforestation techniques [Wad88] disallow fan-out). In the channel approach, devices are wired together by some sort of explicit communication channel.
Reference: [Bir89] <author> Andrew Birrel. </author> <title> An introduction to programming with threads. </title> <type> SRC Report 35, </type> <institution> Digital Equipment Corporation, </institution> <month> January </month> <year> 1989. </year>
Reference-contexts: In the channel approach, devices are wired together by some sort of explicit communication channel. This approach encompasses communicating threads (e.g. <ref> [Bir89] </ref>, CSP [Hoa85], ML threads [CM90]), producer/consumer coroutines (e.g., Unix pipes [KP84], CLU iterators [L + 79], Id's I-structures [ANP89] and M-structures [Bar92]), and dataflow techniques (e.g., [Den75], [WA85]). <p> Pointers to a synchron are classified into two types: waiting and non-waiting. When a process wishes to rendezvous at a synchron, it enters a waiting 5 Due to atomicity problems introduced by concurrency, a base language with imperative features also needs to support a traditional locking mechanism <ref> [Bir89] </ref>, but we won't focus on this point. 5 state that refers to the synchron with a distinguished waiting pointer. For all other manipulations, a process holds a synchron with a non-waiting pointer. <p> This means that any non-waiting pointer to a synchron effectively blocks a rendezvous. Since all processes lose access to the synchron after the rendezvous, there can only be one rendezvous per synchron. The rendezvous protocol of synchrons sets it apart from other synchronization structures (e.g., semaphores/locks <ref> [Bir89] </ref>, synchronous messages [Hoa85], I-structures [ANP89], and M-structures [Bar92]). Synchronization typically involves some processes waiting in a suspended state for a shared synchronization entity to be released by the process that currently owns it. Traditional protocols supply explicit wait and release operations.
Reference: [CM90] <author> Eric Cooper and J. Gregory Mor risett. </author> <title> Adding threads to standard 13 ML. </title> <type> Technical Report CMU-CS-90-186, </type> <institution> Carnegie Mellon University Computer Science Department, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: In the channel approach, devices are wired together by some sort of explicit communication channel. This approach encompasses communicating threads (e.g. [Bir89], CSP [Hoa85], ML threads <ref> [CM90] </ref>), producer/consumer coroutines (e.g., Unix pipes [KP84], CLU iterators [L + 79], Id's I-structures [ANP89] and M-structures [Bar92]), and dataflow techniques (e.g., [Den75], [WA85]).
Reference: [CR + 91] <editor> William Clinger, Jonathan Rees, et al. </editor> <title> Revised 4 report on the algorithmic language Scheme. Lisp Pointers, </title> <type> 4(3), </type> <year> 1991. </year>
Reference-contexts: An iterative averaging operator can be defined as follows: (define (down-average sll) (/ (down-+ sll) (down-length sll))) (define (down-+ sll) (downL 0 + sll)) (define (down-length sll) (downL 0 (lambda (ignore len) (1+ len)) sll)) 6 This evaluation strategy is not allowed in standard Scheme <ref> [CR + 91] </ref>, which requires that the subexpressions be evaluated in some sequential order. (genL init next done?) Generates a SLL whose first element is init and each of whose subsequent elements is determined from the previous by next.
Reference: [DC90] <author> G. D. P. Dueck and G. V. Cor mack. </author> <title> Modular attribute grammars. </title> <journal> The Computer Journal, </journal> <volume> 33(2) </volume> <pages> 164-172, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Modular attribute grammars <ref> [DC90, FMY92, Ada91] </ref> specify tree decoration programs in a declarative, modular fashion. Modular attribute grammars allow different forms of tree processing to be interwoven. Although some versions limit number of traversals or space consumption, most approaches provide little control over these fundamental operational issues.
Reference: [Den75] <author> Jack B. Dennis. </author> <title> First version of a data flow procedure language. Computation Structure Group Memo MIT/LCS/TM-61, </title> <institution> MIT Laboratory for Computer Science, </institution> <month> May </month> <year> 1975. </year>
Reference-contexts: In the channel approach, devices are wired together by some sort of explicit communication channel. This approach encompasses communicating threads (e.g. [Bir89], CSP [Hoa85], ML threads [CM90]), producer/consumer coroutines (e.g., Unix pipes [KP84], CLU iterators [L + 79], Id's I-structures [ANP89] and M-structures [Bar92]), and dataflow techniques (e.g., <ref> [Den75] </ref>, [WA85]). Many channel techniques disallow fan-out, recursion, nesting, or the transmission of tree-structured 3 data. 3 The space profiles of monolithic recursions can often be modelled by bounded channels, but the device buffering and loose coupling exhibited by many channel networks makes it difficult to simulate their time profiles.
Reference: [FMY92] <author> R. Farrow, T. J. Marlowe, and D. M. Yellin. </author> <title> Composable attribute grammars: Support for modularity in translator design and implmentation. </title> <booktitle> In Nineteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 223-234, </pages> <year> 1992. </year>
Reference-contexts: Modular attribute grammars <ref> [DC90, FMY92, Ada91] </ref> specify tree decoration programs in a declarative, modular fashion. Modular attribute grammars allow different forms of tree processing to be interwoven. Although some versions limit number of traversals or space consumption, most approaches provide little control over these fundamental operational issues.
Reference: [HA87] <author> Paul Hudak and Steve Anderson. </author> <title> Pom set interpretation of parallel functional programs. </title> <booktitle> In Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 234-256, </pages> <month> September </month> <year> 1987. </year> <booktitle> Lecture Notes in Computer Science, Number 274. </booktitle>
Reference-contexts: Also, any slag compilation technique would presumably have to employ some sort of shape analysis. * Formalizing time and space profiles: The no tions of time and space profiles introduced above are informal. However, we believe that the pomset model described in <ref> [HA87] </ref> can be extended to make these notions precise. * Pedagogy: We believe that the computational models developed for this work can have important pedagogical applications because they provide alternate approaches for reasoning about the structure of monolithic recursions.
Reference: [Hal85] <author> Robert Halstead. </author> <title> Multilisp: A lan guage for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 501-528, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Call/down-slag and call/down-up-slag specially handle the case where the first argument is a slag. To allow demand to propagate through a slag component, it is necessary to eagerly evaluate the (otherwise delayed) structural node of the slag. Eager evaluation is achieved via futures (see <ref> [Hal85, Mil87] </ref>). genL generator, mapL mapper, and upL accumulator. GenL bundles fresh synchrons (created by the synchron constructor) with a delayed computation that generates the next element of the SLL.
Reference: [HJW + 92] <editor> Paul Hudak, Simon Petyon Jones, Philip Wadler, et al. </editor> <title> Report on the programming language Haskell, version 1.2. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 27(5), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: In the aggregate data approach, SPS devices are represented as operators (e.g., generate, map, filter, accumulate) that manipulate aggregate data structures (e.g., lists, trees, arrays). This approach includes APL's array operators [Ive87], Common Lisp's series procedures [Wat90], Scheme's stream procedures [ASS85], and Haskell's list comprehensions <ref> [HJW + 92] </ref>. The major problem with existing aggregate data techniques is their inability to express operational control. Strict operators processing large intermediate structures in a staged fashion cannot simulate the time or space profiles of a monolithic recursion.
Reference: [Hoa85] <author> C.A.R. Hoare. </author> <title> Communicating Sequen tial Processes. </title> <publisher> Prentice-Hall, </publisher> <year> 1985. </year>
Reference-contexts: In the channel approach, devices are wired together by some sort of explicit communication channel. This approach encompasses communicating threads (e.g. [Bir89], CSP <ref> [Hoa85] </ref>, ML threads [CM90]), producer/consumer coroutines (e.g., Unix pipes [KP84], CLU iterators [L + 79], Id's I-structures [ANP89] and M-structures [Bar92]), and dataflow techniques (e.g., [Den75], [WA85]). <p> This means that any non-waiting pointer to a synchron effectively blocks a rendezvous. Since all processes lose access to the synchron after the rendezvous, there can only be one rendezvous per synchron. The rendezvous protocol of synchrons sets it apart from other synchronization structures (e.g., semaphores/locks [Bir89], synchronous messages <ref> [Hoa85] </ref>, I-structures [ANP89], and M-structures [Bar92]). Synchronization typically involves some processes waiting in a suspended state for a shared synchronization entity to be released by the process that currently owns it. Traditional protocols supply explicit wait and release operations.
Reference: [Hug83] <author> R. J. M. Hughes. </author> <title> The Design and Implementation of Programming Languages. </title> <type> PhD thesis, </type> <institution> Oxford Universiy Computing Laboratory, Programming Research Group, </institution> <month> July </month> <year> 1983. </year>
Reference-contexts: The operator interleaving provided by laziness [Hug90] suitably handles tree-structured networks, but interacts poorly with fan-out. In fact, Hughes shows that any sequential functional language must exhibit undesirable buffering for networks with fan-out <ref> [Hug83] </ref>. Transformations that remove intermediate aggregates from a program either provide no guarantees as to what will be removed (e.g., [Bel84]) or limit the class of expressible computations (e.g., Waters's series compiler [Wat90] handles only iterative computations; deforestation techniques [Wad88] disallow fan-out). <p> In contrast, issues of expressiveness motivate us to develop a dynamic model for the lock step processing of slag operators. The mixture of concurrency, synchronization, and laziness on which slags are based resembles Hughes's technique for controlling space consumption in modular functional programs <ref> [Hug83, Hug84] </ref>. <p> A base language must allow the expression of delayed computations in order to represent the lazy nature of slags. * Concurrency: The lock step processing of a slag component requires its operators to execute concurrently. As shown by Hughes <ref> [Hug83, Hug84] </ref>, the coroutining behavior associated with demand driven evaluation strategies is an insufficient form of concurrency for obtaining lock step behavior in the presence of fan-out.
Reference: [Hug84] <author> R. J. M. Hughes. </author> <title> Parallel functional languages use less space. </title> <type> Technical report, </type> <institution> Oxford University Programming Research Group, </institution> <year> 1984. </year>
Reference-contexts: In contrast, issues of expressiveness motivate us to develop a dynamic model for the lock step processing of slag operators. The mixture of concurrency, synchronization, and laziness on which slags are based resembles Hughes's technique for controlling space consumption in modular functional programs <ref> [Hug83, Hug84] </ref>. <p> A base language must allow the expression of delayed computations in order to represent the lazy nature of slags. * Concurrency: The lock step processing of a slag component requires its operators to execute concurrently. As shown by Hughes <ref> [Hug83, Hug84] </ref>, the coroutining behavior associated with demand driven evaluation strategies is an insufficient form of concurrency for obtaining lock step behavior in the presence of fan-out.
Reference: [Hug90] <author> R. J. M. Hughes. </author> <title> Why functional pro gramming matters. </title> <editor> In David Turner, editor, </editor> <booktitle> Research Topics in Functional Programming, </booktitle> <pages> pages 17-42. </pages> <publisher> Addison Wesley, </publisher> <year> 1990. </year>
Reference-contexts: The major problem with existing aggregate data techniques is their inability to express operational control. Strict operators processing large intermediate structures in a staged fashion cannot simulate the time or space profiles of a monolithic recursion. The operator interleaving provided by laziness <ref> [Hug90] </ref> suitably handles tree-structured networks, but interacts poorly with fan-out. In fact, Hughes shows that any sequential functional language must exhibit undesirable buffering for networks with fan-out [Hug83]. <p> We shall see that achieving this goal requires a base language with appropriate support for laziness, concurrency, and synchronization. Synchronized lazy aggregates (abbreviated slags) augment lazy aggregates <ref> [Hug90] </ref> with a mechanism for aligning the recursive call structures of networked operators to simulate the behavior of a monolithic recursion. <p> tight coupling of slags can be flexibly mixed with the loose coupling afforded 3 There are methods of encoding trees as linear streams, but manipulations of the resulting streams often don't accurately reflect the tree-shaped nature of the corresponding monolithic computations. by other mechanisms (such as lazy lists and trees <ref> [ASS85, Hug90] </ref>). Synchronized lazy aggregates are similar in spirit to Waters's series extension to Common Lisp [Wat90, Wat91]. However, whereas series can express only iterations, slags can express general linear and tree recursions. There is also a difference in focus between the systems.
Reference: [Ive87] <author> Kenneth E. Iverson. </author> <title> A dictionary of APL. </title> <journal> APL QUOTE QUAD, </journal> <volume> 18(1) </volume> <pages> 5-40, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: Existing SPS techniques fail to meet one or more of these goals. In the aggregate data approach, SPS devices are represented as operators (e.g., generate, map, filter, accumulate) that manipulate aggregate data structures (e.g., lists, trees, arrays). This approach includes APL's array operators <ref> [Ive87] </ref>, Common Lisp's series procedures [Wat90], Scheme's stream procedures [ASS85], and Haskell's list comprehensions [HJW + 92]. The major problem with existing aggregate data techniques is their inability to express operational control.
Reference: [Joh87] <author> Thomas Johnsson. </author> <title> Attribute grammars as a functional programming paradigm. </title> <booktitle> In Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 154-173, </pages> <year> 1987. </year> <booktitle> Lecture Notes in Computer Science, Number 274. </booktitle>
Reference-contexts: Although some versions limit number of traversals or space consumption, most approaches provide little control over these fundamental operational issues. Also, attribute grammar formalisms are often closely tied to parsing technology, which limits their use for general SPS programs (but see <ref> [Joh87] </ref>). 3 Design We describe the design of a new SPS technique that satisfies all of the goals set forth above. To guide the design, we add the constraint that the technique should support the traditional aggregate data style of programming.
Reference: [KP84] <author> Brian W. Kernighan and Rob Pike. </author> <title> The UNIX Programming Environment. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1984. </year>
Reference-contexts: In the channel approach, devices are wired together by some sort of explicit communication channel. This approach encompasses communicating threads (e.g. [Bir89], CSP [Hoa85], ML threads [CM90]), producer/consumer coroutines (e.g., Unix pipes <ref> [KP84] </ref>, CLU iterators [L + 79], Id's I-structures [ANP89] and M-structures [Bar92]), and dataflow techniques (e.g., [Den75], [WA85]).
Reference: [L + 79] <author> Barbara Liskov et al. </author> <title> CLU ref erence manual. </title> <type> Technical Report MIT/LCS/TR-225, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: In the channel approach, devices are wired together by some sort of explicit communication channel. This approach encompasses communicating threads (e.g. [Bir89], CSP [Hoa85], ML threads [CM90]), producer/consumer coroutines (e.g., Unix pipes [KP84], CLU iterators <ref> [L + 79] </ref>, Id's I-structures [ANP89] and M-structures [Bar92]), and dataflow techniques (e.g., [Den75], [WA85]).
Reference: [Mil87] <author> James S. Miller. MultiScheme: </author> <title> A parallel processing system based on MIT Scheme. </title> <type> Technical Report MIT/LCS/TR-402, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: Call/down-slag and call/down-up-slag specially handle the case where the first argument is a slag. To allow demand to propagate through a slag component, it is necessary to eagerly evaluate the (otherwise delayed) structural node of the slag. Eager evaluation is achieved via futures (see <ref> [Hal85, Mil87] </ref>). genL generator, mapL mapper, and upL accumulator. GenL bundles fresh synchrons (created by the synchron constructor) with a delayed computation that generates the next element of the SLL.
Reference: [Pey87] <editor> Simon L. Peyton Jones. </editor> <booktitle> The Implemen tation of Functional Programming Languages. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: This lets the programmer concentrate on the high level structure of the program, encouraging reuse and experimentation. For example, the renaming strategy of the alpha renamer can be changed by modifying a single component: a deBruijn numbering program <ref> [Pey87] </ref> can be obtained from alpha-rename simply by replacing lr-pre-scanT with down-scanT. 4.3 Implementing Slag Operators To show how slag operators can satisfy the requirements listed in Section 3.4, we present Opera implementations of a few synchronized lazy list operators.
Reference: [Ste77] <author> Guy L. Steele Jr. </author> <title> Debunking the "ex pensive procedure call" myth, or procedure call implementations considered harmful, or LAMBDA, the ultimate 14 Goto. </title> <type> Technical Report AIM-443, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> Oc-tober </month> <year> 1977. </year>
Reference-contexts: Each rendezvous represents the call or return of a monolithic recursive procedure that interleaves the processing of all the operators of a slag component. A slag operator that makes a tail call <ref> [Ste77] </ref> never returns from the call and so should not participate in the barrier synchronization associated with the return. If all operators within a slag component make corresponding tail calls, then there is no global return associated with the call.
Reference: [Tur94] <author> Franklyn Albin Turbak. Slivers: </author> <title> Computational Modularity via Synchronized Lazy Aggregates (in preparation). </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Synchron unification extends the usual notion of unifying logic variables to unifying the points in time represented by independently generated synchrons. Below, we shall see how networks with fan-in require this feature. Synchrons can be formally specified by a graph rewriting model; see <ref> [Tur94] </ref> for details. 3.3 Slag Structure The lock step processing of a slag component implies that all of its operators share the same synchron for a given call or return. This sharing is achieved by the generation, propagation, and combination of synchrons. <p> Instead, we have developed a suite of higher order slag operators that have proven useful in a wide variety of simple applications. Below, we show examples of some of these operators. Due to space limitations, the examples are necessarily brief; for more extended examples, see <ref> [Tur94] </ref>. All of the examples described here are written in Opera, a dialect of Scheme that supports the laziness, concurrency, and synchronization features outlined in Section 3.2. <p> It turns out that handling filtering in a fully reusable manner requires slags to carry extra information per element and synchrons to have parent/child relationships with special garbage collection properties. Also, writing certain kinds of transducers can be much trickier than the above examples indicate. <ref> [Tur94] </ref> describes these complexities in detail. 5 Experience We have implemented a prototype Opera interpreter based on an explicit-demand graph rewriting model (see [Tur94] for details). Fine-grained con-currency, synchrons, lazons, and futures are relatively straightforward to handle in this model. <p> Also, writing certain kinds of transducers can be much trickier than the above examples indicate. <ref> [Tur94] </ref> describes these complexities in detail. 5 Experience We have implemented a prototype Opera interpreter based on an explicit-demand graph rewriting model (see [Tur94] for details). Fine-grained con-currency, synchrons, lazons, and futures are relatively straightforward to handle in this model.
Reference: [WA85] <author> William W. Wadge and Edward A. Ashcroft. </author> <title> Lucid, the Dataflow Programming Language. </title> <publisher> Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: In the channel approach, devices are wired together by some sort of explicit communication channel. This approach encompasses communicating threads (e.g. [Bir89], CSP [Hoa85], ML threads [CM90]), producer/consumer coroutines (e.g., Unix pipes [KP84], CLU iterators [L + 79], Id's I-structures [ANP89] and M-structures [Bar92]), and dataflow techniques (e.g., [Den75], <ref> [WA85] </ref>). Many channel techniques disallow fan-out, recursion, nesting, or the transmission of tree-structured 3 data. 3 The space profiles of monolithic recursions can often be modelled by bounded channels, but the device buffering and loose coupling exhibited by many channel networks makes it difficult to simulate their time profiles.
Reference: [Wad88] <author> Philip Wadler. </author> <title> Deforestation: Trans forming programs to eliminate trees. </title> <booktitle> In 2nd European Symposium on Programming, </booktitle> <pages> pages 344-358, </pages> <year> 1988. </year> <booktitle> Lecture Notes in Computer Science, Number 300. </booktitle>
Reference-contexts: Transformations that remove intermediate aggregates from a program either provide no guarantees as to what will be removed (e.g., [Bel84]) or limit the class of expressible computations (e.g., Waters's series compiler [Wat90] handles only iterative computations; deforestation techniques <ref> [Wad88] </ref> disallow fan-out). In the channel approach, devices are wired together by some sort of explicit communication channel.
Reference: [Wat90] <author> Richard C. Waters. </author> <title> Series. </title> <editor> In Guy L. Steele Jr., editor, </editor> <title> Common Lisp: </title> <booktitle> The Language, </booktitle> <pages> pages 923-955. </pages> <publisher> Digital Press, </publisher> <year> 1990. </year>
Reference-contexts: Synchronized lazy aggregates are more expressive than existing SPS techniques because they permit fine-grained operational control in a framework that supports tree-structured data, arbitrary network topologies, imperative features, and hierarchical program organization. Synchronized lazy aggregates resemble Waters's series package for Common Lisp <ref> [Wat90, Wat91] </ref>, except that (1) they can express general linear and tree recursions in addition to iterations and (2) they provide a dynamic model of lock step processing rather than a static one. <p> Existing SPS techniques fail to meet one or more of these goals. In the aggregate data approach, SPS devices are represented as operators (e.g., generate, map, filter, accumulate) that manipulate aggregate data structures (e.g., lists, trees, arrays). This approach includes APL's array operators [Ive87], Common Lisp's series procedures <ref> [Wat90] </ref>, Scheme's stream procedures [ASS85], and Haskell's list comprehensions [HJW + 92]. The major problem with existing aggregate data techniques is their inability to express operational control. Strict operators processing large intermediate structures in a staged fashion cannot simulate the time or space profiles of a monolithic recursion. <p> In fact, Hughes shows that any sequential functional language must exhibit undesirable buffering for networks with fan-out [Hug83]. Transformations that remove intermediate aggregates from a program either provide no guarantees as to what will be removed (e.g., [Bel84]) or limit the class of expressible computations (e.g., Waters's series compiler <ref> [Wat90] </ref> handles only iterative computations; deforestation techniques [Wad88] disallow fan-out). In the channel approach, devices are wired together by some sort of explicit communication channel. <p> Synchronized lazy aggregates are similar in spirit to Waters's series extension to Common Lisp <ref> [Wat90, Wat91] </ref>. However, whereas series can express only iterations, slags can express general linear and tree recursions. There is also a difference in focus between the systems. Emphasizing the efficient compilation of series expressions into loops, Waters develops a static model for the lock step processing of series operators.
Reference: [Wat91] <author> Richard C. Waters. </author> <title> Automatic trans formation of series expressions into loops. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(1) </volume> <pages> 52-98, </pages> <month> January </month> <year> 1991. </year> <month> 15 </month>
Reference-contexts: Synchronized lazy aggregates are more expressive than existing SPS techniques because they permit fine-grained operational control in a framework that supports tree-structured data, arbitrary network topologies, imperative features, and hierarchical program organization. Synchronized lazy aggregates resemble Waters's series package for Common Lisp <ref> [Wat90, Wat91] </ref>, except that (1) they can express general linear and tree recursions in addition to iterations and (2) they provide a dynamic model of lock step processing rather than a static one. <p> Synchronized lazy aggregates are similar in spirit to Waters's series extension to Common Lisp <ref> [Wat90, Wat91] </ref>. However, whereas series can express only iterations, slags can express general linear and tree recursions. There is also a difference in focus between the systems. Emphasizing the efficient compilation of series expressions into loops, Waters develops a static model for the lock step processing of series operators. <p> Laziness means that the slag dynamically unfolds over time. The dynamically computed elements of a slag correspond to the time-dependent values of a variable in a monolithic recursion. Waters uses the term temporal data structure <ref> [Wat91] </ref> to refer to this notion. His series data structure is a particular instance of the more general slag. <p> The dynamic approach is to develop more efficient concurrency and synchronization mechanisms. The static approach 12 is to eliminate concurrency, synchronization, and data manipulation overheads by compiling slag networks into monolithic recursions. We expect that the techniques for compiling series <ref> [Wat91] </ref> can be extended to a restricted subset of slag networks, and that the explicit synchronization information available in slags can facilitate their compilation. * Shape analysis: In analogy with type systems, the notion of shape introduce above suggests a shape system in which the shape of a network can be
References-found: 31

