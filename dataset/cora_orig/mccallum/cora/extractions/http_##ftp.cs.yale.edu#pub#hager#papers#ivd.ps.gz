URL: http://ftp.cs.yale.edu/pub/hager/papers/ivd.ps.gz
Refering-URL: http://ftp.cs.yale.edu/pub/hager/papers/
Root-URL: http://www.cs.yale.edu
Email: Email: belhumeur@yale.edu  belhumeur@yale.edu  
Phone: Phone: 203 432-4249 Fax: 203 432-7481  
Title: Tracking in 3D: Image Variability Decomposition for Recovering Object Pose and Illumination  
Author: Peter N. Belhumeur Gregory D. Hager date 
Keyword: Tracking, Image Sequence Analysis, Pose Estimation, Illumination Modeling  
Address: New Haven, CT 06520-8267  
Affiliation: Dept. of Electrical Engineering Dept. of Computer Science Center for Computational Vision and Control Yale University  
Abstract: As an object moves through space, it changes its orientation relative to the viewing camera and relative to light sources which illuminate it. As a consequence, the images of the object produced by the viewing camera may change dramatically. Thus to successfully track a moving object, image changes due to varying pose and illumination must be accounted for. In this paper, we develop a method for object tracking that can not only accommodate large changes in object pose and illumination, but can recover these parameters as well. To do this, we separately model the image variation of the object produced by changes in pose and illumination. To track the object through each image in the sequence, we then locally search the models to find the best match, recovering the object's orientation and illumination in the process. Throughout, we present experimental results, achieved in real-time, demonstrating the effectiveness of our methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Belhumeur. </author> <title> A Bayesian approach to binocular stereopsis. </title> <journal> Int. Journal of Computer Vision, </journal> <volume> 19, </volume> <year> 1996. </year>
Reference-contexts: The basis vectors spanning the pose subspace were crudely determined by first determining the correspondence using the optical flow techniques of [18]. (The methods for establishing a dense correspondence can be improved by borrowing from work in binocular stereopsis <ref> [14, 7, 1] </ref>.) We then randomly sampled the pose manifold, recreating the images of the object at the new pose. The pictures at the bottom of the figure are artificially generated images of the human face.
Reference: [2] <author> P. Belhumeur and D. Kriegman. </author> <title> What is the set of images of an object under all possible lighting conditions? In IEEE Proc. </title> <booktitle> Conf. Computer Vision and Pattern Recognition, </booktitle> <year> 1996. </year>
Reference-contexts: As a consequence, not only is the tracking of the object's position more reliable, but we are also able to recover the 3D orientation of the object through the image sequence. In the case of illumination variation, we exploit the results in <ref> [2] </ref> which show that the set of images of an object seen from a fixed viewpoint, under all possible illumination conditions, is a convex cone in the space of images, and, this cone (termed the "illumination cone") can often be constructed from as few as three images. <p> The set of images of an object with arbitrary reflectance functions seen under arbitrary illumination conditions is a convex cone in IR n where n is the number of pixels in each image <ref> [2] </ref>. And, if the object has a convex shape and a Lambertian reflectance function, the set of images under an arbitrary number of point light sources at infinity is a convex polyhedral cone in IR n , which can be determined exactly from as few as three images, see again [2]. <p> <ref> [2] </ref>. And, if the object has a convex shape and a Lambertian reflectance function, the set of images under an arbitrary number of point light sources at infinity is a convex polyhedral cone in IR n , which can be determined exactly from as few as three images, see again [2]. To express the stated relations through equations, let us assume we have a target object seen from a fixed point, but under varying illumination. <p> If the object's surface has a Lambertian reflectance function and the shape of the object is roughly convex, then the extreme rays defining the illumination cone can be generated from a 3D linear subspace in the ndimensional image space <ref> [2] </ref>. Furthermore, this "illumination subspace" can be generated from as few as three images [27, 5, 2]. Even if the surface reflectance function is more general than Lambertian and the object is non-convex in shape, this method still seems to work quite well. <p> Furthermore, this "illumination subspace" can be generated from as few as three images <ref> [27, 5, 2] </ref>. Even if the surface reflectance function is more general than Lambertian and the object is non-convex in shape, this method still seems to work quite well. Figure 2 gives a demonstration of such an approximation for varying illumination of a face.
Reference: [3] <author> M. Black and A. Jepson. Eigentracking: </author> <title> Robust matching and tracking of articulated objects using a view-based representation. </title> <booktitle> In European Conf. on Computer Vision, </booktitle> <pages> pages I:329-342, </pages> <year> 1996. </year>
Reference-contexts: This differs from earlier work on handling pose variation by ourselves and others <ref> [10, 3, 28] </ref> in that image coordinate deformations are not restricted to be affine. For planar or nearly planar targets, affine models work quite well. Yet, most targets are not planar, and, consequently, as the target rotates in space affine models quickly break down. <p> These methods have been directly applied to tracking problems [20] and have been also been applied in combination with image-level deformations <ref> [3] </ref>. Although these methods can be used to both track an object and to recover object pose, they require that the target has been seen from all possible viewpoints.
Reference: [4] <author> M. Black and Y. Yacoob. </author> <title> Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion. </title> <booktitle> In Proceedings of the ICCV, </booktitle> <pages> pages 374-381, </pages> <year> 1995. </year>
Reference-contexts: Previous work on region-based tracking has almost entirely concentrated on the use of 2D linear models of image deformations, ranging from simple rigid translation [21, 29], to affine or low-order polynomial deformations <ref> [28, 10, 4, 25] </ref>. Affine models are correct for planar surfaces viewed under orthographic projection.
Reference: [5] <author> R. Epstein, A. Yuille, and P. N. Belhumeur. </author> <title> Learning and recognizing objects using illumination subspaces. </title> <booktitle> In Proc. of the Int. Workshop on Object Representation for Computer Vision, </booktitle> <year> 1996. </year>
Reference-contexts: Furthermore, this "illumination subspace" can be generated from as few as three images <ref> [27, 5, 2] </ref>. Even if the surface reflectance function is more general than Lambertian and the object is non-convex in shape, this method still seems to work quite well. Figure 2 gives a demonstration of such an approximation for varying illumination of a face.
Reference: [6] <author> D. Gavrila and L. Davis. </author> <title> 3-d model-based tracking of human upper body movement: A multi-view approach. </title> <booktitle> In Proceedings of the Symposium on Computer Vision, </booktitle> <pages> pages 253-258, </pages> <year> 1995. </year>
Reference-contexts: Finally, Section 5 presents experimental results from tracking both a "calibration" sphere and a human face. 2 Related Work Visual tracking can be usefully categorized as region-based or feature-based. Most of the tracking systems that are able to use and/or compute 3D pose are feature-based <ref> [8, 17, 6, 26] </ref>. These systems 2 overcome the potentially deleterious effects of changes due to illumination by concentrating on a sparse set of edges or corners in the images, i.e. discontinuities in the image intensity.
Reference: [7] <author> D. Geiger, B. Ladendorf, and A. Yuille. </author> <title> Occlusions in binocular stereo. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <year> 1992. </year>
Reference-contexts: The basis vectors spanning the pose subspace were crudely determined by first determining the correspondence using the optical flow techniques of [18]. (The methods for establishing a dense correspondence can be improved by borrowing from work in binocular stereopsis <ref> [14, 7, 1] </ref>.) We then randomly sampled the pose manifold, recreating the images of the object at the new pose. The pictures at the bottom of the figure are artificially generated images of the human face.
Reference: [8] <author> D. B. Gennery. </author> <title> Visual tracking of known three-dimensional objects. </title> <journal> Int'l Journal of Computer Vision, </journal> <volume> 7(3) </volume> <pages> 243-270, </pages> <year> 1992. </year>
Reference-contexts: Finally, Section 5 presents experimental results from tracking both a "calibration" sphere and a human face. 2 Related Work Visual tracking can be usefully categorized as region-based or feature-based. Most of the tracking systems that are able to use and/or compute 3D pose are feature-based <ref> [8, 17, 6, 26] </ref>. These systems 2 overcome the potentially deleterious effects of changes due to illumination by concentrating on a sparse set of edges or corners in the images, i.e. discontinuities in the image intensity.
Reference: [9] <author> M. Goresky and R. Macpherson. </author> <title> Stratified Morse Theory. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: The set of all possible 3D object coordinates under 3D rotation and translation is then a 6-D manifold in this 3ndimensional coordinate space. (Due to possible symmetries in the object, the set of coordinates may not be a manifold, but a stratified set <ref> [9] </ref> composed of manifolds and singular sets of dimension six or less.
Reference: [10] <author> G. D. Hager and P. N. Belhumeur. </author> <title> Real-time tracking of image regions with changes in geometry and illumination. </title> <booktitle> In Proc. IEEE Conf. Computer Vision and Pattern Recognition, </booktitle> <pages> pages 403-410. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year> <month> 15 </month>
Reference-contexts: This differs from earlier work on handling pose variation by ourselves and others <ref> [10, 3, 28] </ref> in that image coordinate deformations are not restricted to be affine. For planar or nearly planar targets, affine models work quite well. Yet, most targets are not planar, and, consequently, as the target rotates in space affine models quickly break down. <p> This differs from earlier work <ref> [10] </ref> in that the changes in the image of the target region are not assumed to lie anywhere within a low-dimensional linear subspace, but are restricted to lie on or within a convex cone. <p> Previous work on region-based tracking has almost entirely concentrated on the use of 2D linear models of image deformations, ranging from simple rigid translation [21, 29], to affine or low-order polynomial deformations <ref> [28, 10, 4, 25] </ref>. Affine models are correct for planar surfaces viewed under orthographic projection. <p> It follows that the temporal correspondence of the target region across an image sequence can be determined by finding the pose parameters and the illumination parameters that minimize O (a; b; c) = kI (x; y; t) M (f (b); g (c); a)k 2 : (6) As recently shown <ref> [10] </ref>, it is possible to efficiently compute the parameters in a problem of this form. Briefly, the procedure is to linearize the above expression and solve for the pose and illumination parameters incrementally in each frame.
Reference: [11] <author> G. D. Hager and K. Toyama. XVision: </author> <title> A portable substrate for real-time vision applications. </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <year> 1996. </year> <note> In Press. </note>
Reference: [12] <author> P. Hallinan. </author> <title> A Deformable Model for Face Recognition Under Arbitrary Lighting Conditions. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1995. </year>
Reference-contexts: Note that there is little variation in the original four images shown in the top, left of the figure, yet the artificially generated images demonstrate a range of pose variation. 6 the directions and strengths of the light sources <ref> [12, 19] </ref>. This fact has implications not only for object recognition, but also for object tracking. The variability due to illumination may be much larger then that due to pose as the set of possible lighting conditions is infinite dimensional.
Reference: [13] <author> D. W. Jacobs. </author> <title> Space efficient 3D model indexing. </title> <booktitle> In Proc. IEEE Conf. on Comp. Vision and Patt. Recog., </booktitle> <pages> pages 439-444, </pages> <year> 1992. </year>
Reference-contexts: We decompose the variability into its component parts pose and illumination each of which, when analyzed separately, is well behaved. We call this approach to handling image changes "Image Variability Decomposition." In the case of pose variation, we exploit the results of Ullman and Basri [32], Jacobs <ref> [13] </ref>, and Tomasi and Kanade [30] which show that, under weak perspective, the set of image coordinates of a rigidly moving object lies in low-dimensional linear subspace of the image coordinate space. <p> In particular, we demonstrate how, from as few as three images of a target, it is possible to generate a complete set of images of the target over a range of pose and illumination variation. 3.1 Pose In this section, we apply the results of Ullman and Basri [32], Jacobs <ref> [13] </ref>, and Tomasi and Kanade [30] which show that, under weak perspective, the set of image coordinates of a rigidly moving object lies in low-dimensional linear subspace of the image coordinate space. Unlike [30], however, our goal is not to explicitly determine the object's structure.
Reference: [14] <author> D. Jones and J. Malik. </author> <title> A computational framework for determining stereo correspondence from a set of linear spatial filters. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <year> 1992. </year>
Reference-contexts: The basis vectors spanning the pose subspace were crudely determined by first determining the correspondence using the optical flow techniques of [18]. (The methods for establishing a dense correspondence can be improved by borrowing from work in binocular stereopsis <ref> [14, 7, 1] </ref>.) We then randomly sampled the pose manifold, recreating the images of the object at the new pose. The pictures at the bottom of the figure are artificially generated images of the human face.
Reference: [15] <author> J. Koenderink and A. Van Doorn. </author> <title> Affine structure from motion. </title> <journal> J. Opt. Soc. Am., </journal> <volume> 8(2) </volume> <pages> 337-385, </pages> <year> 1991. </year>
Reference-contexts: From the correspondences of each pixel in the first image with those in the second image, we can determine the pose subspace [30]. Note that determining the pose subspace only fixes the affine structure of the object <ref> [15] </ref>.
Reference: [16] <author> L. Sirovitch and M. Kirby. </author> <title> Low-dimensional procedure for the characterization of human faces. </title> <journal> J. Optical Soc. of America A, </journal> <volume> 2 </volume> <pages> 519-524, </pages> <year> 1987. </year>
Reference-contexts: Affine models are correct for planar surfaces viewed under orthographic projection. More recently, "appearance-based" approaches been developed in an effort to use intensity information to model or learn a representation that captures a large set of the possible images of an object under pose and/or illumination variation <ref> [20, 23, 24, 16, 31] </ref>. These methods have been directly applied to tracking problems [20] and have been also been applied in combination with image-level deformations [3].
Reference: [17] <author> D. Lowe. </author> <title> Robust model-based motion tracking through the integration of search and estimation. </title> <journal> Int. Journal of Computer Vision, </journal> <volume> 8(2) </volume> <pages> 113-122, </pages> <year> 1992. </year>
Reference-contexts: A somewhat more difficult challenge is to estimation the (3D) pose and illumination of the object through an image sequence, see <ref> [20, 26, 17] </ref>. If this determination can be made reliably, it may not only increase the accuracy of the tracking process, fl P. N. Belhumeur was supported by ARO grant DAAH04-95-1-0494. y G. <p> Finally, Section 5 presents experimental results from tracking both a "calibration" sphere and a human face. 2 Related Work Visual tracking can be usefully categorized as region-based or feature-based. Most of the tracking systems that are able to use and/or compute 3D pose are feature-based <ref> [8, 17, 6, 26] </ref>. These systems 2 overcome the potentially deleterious effects of changes due to illumination by concentrating on a sparse set of edges or corners in the images, i.e. discontinuities in the image intensity. <p> However, models using only local features are impoverished they throw out a great deal of information present in the original image. Furthermore, since features are typically local, feature-based tracking methods require good dynamical models and effective search methods to avoid feature matching ambiguity <ref> [26, 17] </ref>. If the objects do not have piecewise constant albedo patterns, the detection and localization of the edge or corner points are sensitive to changes in illumination.
Reference: [18] <author> B. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Proc. of the 7th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 674-679, </pages> <year> 1981. </year>
Reference-contexts: Here we have used twenty images, of which four are shown, to generate the pose subspace in which the pose manifold is embedded. The basis vectors spanning the pose subspace were crudely determined by first determining the correspondence using the optical flow techniques of <ref> [18] </ref>. (The methods for establishing a dense correspondence can be improved by borrowing from work in binocular stereopsis [14, 7, 1].) We then randomly sampled the pose manifold, recreating the images of the object at the new pose. <p> Yet, the same object seen from the same pose can appear drastically different depending 5 to generate the pose subspace in which the pose manifold is embedded. The basis vectors spanning the pose subspace were crudely determined by first determining the correspondence using the optical flow techniques of <ref> [18] </ref>. We then randomly sampled the pose manifold, recreating the images of the object at the new pose. The pictures at the bottom of the figure are artificially generated images of the human face.
Reference: [19] <author> Y. Moses, Y. Adini, and S. Ullman. </author> <title> Face recognition: The problem of compensating for changes in illumination direction. </title> <booktitle> In European Conf. on Computer Vision, </booktitle> <pages> pages 286-296, </pages> <year> 1994. </year>
Reference-contexts: Note that there is little variation in the original four images shown in the top, left of the figure, yet the artificially generated images demonstrate a range of pose variation. 6 the directions and strengths of the light sources <ref> [12, 19] </ref>. This fact has implications not only for object recognition, but also for object tracking. The variability due to illumination may be much larger then that due to pose as the set of possible lighting conditions is infinite dimensional.
Reference: [20] <author> H. Murase and S. Nayar. </author> <title> Visual learning and recognition of 3-D objects from appearence. </title> <journal> Int. J. Computer Vision, </journal> <pages> 14(5-24), </pages> <year> 1995. </year>
Reference-contexts: A somewhat more difficult challenge is to estimation the (3D) pose and illumination of the object through an image sequence, see <ref> [20, 26, 17] </ref>. If this determination can be made reliably, it may not only increase the accuracy of the tracking process, fl P. N. Belhumeur was supported by ARO grant DAAH04-95-1-0494. y G. <p> Affine models are correct for planar surfaces viewed under orthographic projection. More recently, "appearance-based" approaches been developed in an effort to use intensity information to model or learn a representation that captures a large set of the possible images of an object under pose and/or illumination variation <ref> [20, 23, 24, 16, 31] </ref>. These methods have been directly applied to tracking problems [20] and have been also been applied in combination with image-level deformations [3]. <p> These methods have been directly applied to tracking problems <ref> [20] </ref> and have been also been applied in combination with image-level deformations [3]. Although these methods can be used to both track an object and to recover object pose, they require that the target has been seen from all possible viewpoints. <p> First, we can simply gather a collection of images of the target regions illuminated by point light sources of different directions and use these to define the extreme rays. This method is similar to that of <ref> [20] </ref>, except the illumination cone representation implicitly models multiple light sources since it allows for convex combinations of the extreme rays (images). A drawback of this method (and that of [20]) is that many images are needed to construct the model. <p> This method is similar to that of <ref> [20] </ref>, except the illumination cone representation implicitly models multiple light sources since it allows for convex combinations of the extreme rays (images). A drawback of this method (and that of [20]) is that many images are needed to construct the model. A second method for determining the extreme rays is to gather only a small number of images of the object under varying illumination and use these to generate the complete set of extreme rays. <p> Likewise, the 9 illumination coefficients each correspond to different light source directions, with the magnitudes of the coefficients proportional to the light source strengths. We should add that, while the method in <ref> [20] </ref> could also determine pose, an advantage of this method is that for objects with similar geometry, but different albedo patterns, all that is required is that the illumination cone be transformed. There is no need to relearn the pose manifold or illumination subspace.
Reference: [21] <author> N. P. Papanikolopoulos, P. K. Khosla, and T. Kanade. </author> <title> Visual Tracking of a Moving Target by a Camera Mounted on a Robot: A Combination of Vision and Control. </title> <journal> IEEE Trans. Robot. Automat., </journal> <volume> 9(1) </volume> <pages> 14-35, </pages> <year> 1993. </year>
Reference-contexts: Previous work on region-based tracking has almost entirely concentrated on the use of 2D linear models of image deformations, ranging from simple rigid translation <ref> [21, 29] </ref>, to affine or low-order polynomial deformations [28, 10, 4, 25]. Affine models are correct for planar surfaces viewed under orthographic projection.
Reference: [22] <author> R. Paul. </author> <title> Robot Manipulators. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1981. </year>
Reference-contexts: The result of this process is both a vector of coefficients characterizing the object's pose and a vector of coefficients describing a point in the illumination cone. To determine the actual pose, we choose a specific representation for rotations <ref> [22] </ref> as R = R z (fl)R y (fi)R x (ff) (7) where R i () is a rotation by an angle about the axis i: We are particularly interested in the out-of-plane rotations ff and fi which we subsequently refer to as "tilt" and "pan," respectively.
Reference: [23] <author> A. Pentland, B. Moghaddam, and Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Proc. IEEE Conf. Computer Vision and Pattern Recognition, </booktitle> <pages> pages 84-91, </pages> <year> 1994. </year>
Reference-contexts: Affine models are correct for planar surfaces viewed under orthographic projection. More recently, "appearance-based" approaches been developed in an effort to use intensity information to model or learn a representation that captures a large set of the possible images of an object under pose and/or illumination variation <ref> [20, 23, 24, 16, 31] </ref>. These methods have been directly applied to tracking problems [20] and have been also been applied in combination with image-level deformations [3].
Reference: [24] <author> T. Poggio and K. Sung. </author> <title> Example-based learning for view-based human face detection. </title> <booktitle> In Proc. Image Understanding Workshop, </booktitle> <pages> pages II:843-850, </pages> <year> 1994. </year>
Reference-contexts: Affine models are correct for planar surfaces viewed under orthographic projection. More recently, "appearance-based" approaches been developed in an effort to use intensity information to model or learn a representation that captures a large set of the possible images of an object under pose and/or illumination variation <ref> [20, 23, 24, 16, 31] </ref>. These methods have been directly applied to tracking problems [20] and have been also been applied in combination with image-level deformations [3].
Reference: [25] <author> J. Rehg and A. Witkin. </author> <title> Visual tracking with deformation models. </title> <booktitle> In Proc. IEEE Int. Conf. Robot. and Automat., </booktitle> <pages> pages 844-850, </pages> <year> 1991. </year> <month> 16 </month>
Reference-contexts: Previous work on region-based tracking has almost entirely concentrated on the use of 2D linear models of image deformations, ranging from simple rigid translation [21, 29], to affine or low-order polynomial deformations <ref> [28, 10, 4, 25] </ref>. Affine models are correct for planar surfaces viewed under orthographic projection.
Reference: [26] <author> D. Reynard, A. Wildenberg, A. Blake, and J. Marchant. </author> <title> Learning dynamics of complex motions from image sequences. </title> <booktitle> In European Conf. on Computer Vision, </booktitle> <pages> pages I:357-368, </pages> <year> 1996. </year>
Reference-contexts: A somewhat more difficult challenge is to estimation the (3D) pose and illumination of the object through an image sequence, see <ref> [20, 26, 17] </ref>. If this determination can be made reliably, it may not only increase the accuracy of the tracking process, fl P. N. Belhumeur was supported by ARO grant DAAH04-95-1-0494. y G. <p> Finally, Section 5 presents experimental results from tracking both a "calibration" sphere and a human face. 2 Related Work Visual tracking can be usefully categorized as region-based or feature-based. Most of the tracking systems that are able to use and/or compute 3D pose are feature-based <ref> [8, 17, 6, 26] </ref>. These systems 2 overcome the potentially deleterious effects of changes due to illumination by concentrating on a sparse set of edges or corners in the images, i.e. discontinuities in the image intensity. <p> However, models using only local features are impoverished they throw out a great deal of information present in the original image. Furthermore, since features are typically local, feature-based tracking methods require good dynamical models and effective search methods to avoid feature matching ambiguity <ref> [26, 17] </ref>. If the objects do not have piecewise constant albedo patterns, the detection and localization of the edge or corner points are sensitive to changes in illumination.
Reference: [27] <author> A. Shashua. </author> <title> Geometry and Photometry in 3D Visual Recognition. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1992. </year>
Reference-contexts: Furthermore, this "illumination subspace" can be generated from as few as three images <ref> [27, 5, 2] </ref>. Even if the surface reflectance function is more general than Lambertian and the object is non-convex in shape, this method still seems to work quite well. Figure 2 gives a demonstration of such an approximation for varying illumination of a face.
Reference: [28] <author> J. Shi and C. Tomasi. </author> <title> Good features to track. </title> <booktitle> In Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 593-600. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: This differs from earlier work on handling pose variation by ourselves and others <ref> [10, 3, 28] </ref> in that image coordinate deformations are not restricted to be affine. For planar or nearly planar targets, affine models work quite well. Yet, most targets are not planar, and, consequently, as the target rotates in space affine models quickly break down. <p> Previous work on region-based tracking has almost entirely concentrated on the use of 2D linear models of image deformations, ranging from simple rigid translation [21, 29], to affine or low-order polynomial deformations <ref> [28, 10, 4, 25] </ref>. Affine models are correct for planar surfaces viewed under orthographic projection.
Reference: [29] <author> C. Tomasi and T. Kanade. </author> <title> Detection and tracking of point features. </title> <type> Technical report, </type> <institution> Carnegie-Mellon, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Previous work on region-based tracking has almost entirely concentrated on the use of 2D linear models of image deformations, ranging from simple rigid translation <ref> [21, 29] </ref>, to affine or low-order polynomial deformations [28, 10, 4, 25]. Affine models are correct for planar surfaces viewed under orthographic projection.
Reference: [30] <author> C. Tomasi and T. Kanade. </author> <title> Shape and motion from image streams under orthography: A factorization method. </title> <journal> Int. J. Computer Vision, </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <year> 1992. </year>
Reference-contexts: We call this approach to handling image changes "Image Variability Decomposition." In the case of pose variation, we exploit the results of Ullman and Basri [32], Jacobs [13], and Tomasi and Kanade <ref> [30] </ref> which show that, under weak perspective, the set of image coordinates of a rigidly moving object lies in low-dimensional linear subspace of the image coordinate space. <p> from as few as three images of a target, it is possible to generate a complete set of images of the target over a range of pose and illumination variation. 3.1 Pose In this section, we apply the results of Ullman and Basri [32], Jacobs [13], and Tomasi and Kanade <ref> [30] </ref> which show that, under weak perspective, the set of image coordinates of a rigidly moving object lies in low-dimensional linear subspace of the image coordinate space. Unlike [30], however, our goal is not to explicitly determine the object's structure. <p> illumination variation. 3.1 Pose In this section, we apply the results of Ullman and Basri [32], Jacobs [13], and Tomasi and Kanade <ref> [30] </ref> which show that, under weak perspective, the set of image coordinates of a rigidly moving object lies in low-dimensional linear subspace of the image coordinate space. Unlike [30], however, our goal is not to explicitly determine the object's structure. Rather, our approach follows that 3 of [32] in that we want to use the low-dimensionality of the image coordinate variation to predict what the object will look like over a range of viewing directions. <p> From the correspondences of each pixel in the first image with those in the second image, we can determine the pose subspace <ref> [30] </ref>. Note that determining the pose subspace only fixes the affine structure of the object [15]. To determine the 3D Euclidean structure of the object or, equivalently, to determine the pose manifold, a third image is needed. (This, of course, was implicit in work of [30], which demonstrated how these subspaces <p> can determine the pose subspace <ref> [30] </ref>. Note that determining the pose subspace only fixes the affine structure of the object [15]. To determine the 3D Euclidean structure of the object or, equivalently, to determine the pose manifold, a third image is needed. (This, of course, was implicit in work of [30], which demonstrated how these subspaces could be computed efficiently and, from them, the 3D coordinates of the points determined.) Yet, we stress that the goal of Pose Decomposition is not to necessarily determine the precise 3D structure of objects.
Reference: [31] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> J. of Cognitive Neuroscience, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: Affine models are correct for planar surfaces viewed under orthographic projection. More recently, "appearance-based" approaches been developed in an effort to use intensity information to model or learn a representation that captures a large set of the possible images of an object under pose and/or illumination variation <ref> [20, 23, 24, 16, 31] </ref>. These methods have been directly applied to tracking problems [20] and have been also been applied in combination with image-level deformations [3].
Reference: [32] <author> S. Ullman and R. Basri. </author> <title> Recognition by linear combinations of models. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 992-1006, </pages> <year> 1991. </year> <month> 17 </month>
Reference-contexts: We decompose the variability into its component parts pose and illumination each of which, when analyzed separately, is well behaved. We call this approach to handling image changes "Image Variability Decomposition." In the case of pose variation, we exploit the results of Ullman and Basri <ref> [32] </ref>, Jacobs [13], and Tomasi and Kanade [30] which show that, under weak perspective, the set of image coordinates of a rigidly moving object lies in low-dimensional linear subspace of the image coordinate space. <p> In particular, we demonstrate how, from as few as three images of a target, it is possible to generate a complete set of images of the target over a range of pose and illumination variation. 3.1 Pose In this section, we apply the results of Ullman and Basri <ref> [32] </ref>, Jacobs [13], and Tomasi and Kanade [30] which show that, under weak perspective, the set of image coordinates of a rigidly moving object lies in low-dimensional linear subspace of the image coordinate space. Unlike [30], however, our goal is not to explicitly determine the object's structure. <p> Unlike [30], however, our goal is not to explicitly determine the object's structure. Rather, our approach follows that 3 of <ref> [32] </ref> in that we want to use the low-dimensionality of the image coordinate variation to predict what the object will look like over a range of viewing directions. <p> For simplicity we will refer to these sets as manifolds.) As evident from the work of <ref> [32] </ref>, under scaled orthographic projection, the set of 2D image coordinates lie in an 8-D linear subspace of the 2ndimensional image coordinate space. <p> It follows that the set of all image coordinates viewed under scaled orthography lie on a 6-D manifold embedded within the 8-D linear subspace represented by the (unconstrained) matrix A and vector d: We call this 6-D manifold the "pose manifold." As noted in the work of Ullman and Basri <ref> [32] </ref>, because of the symmetry between the x and y image coordinates, both the x and y coordinates lie in the same 4-D linear subspace of an ndimensional coordinate space.
References-found: 32

