URL: http://cag-www.lcs.mit.edu/~rinard/paper/ppopp97.ps
Refering-URL: http://cag-www.lcs.mit.edu/~rinard/paper/index.html
Root-URL: 
Email: martin@cs.ucsb.edu  
Title: Effective Fine-Grain Synchronization For Automatically Parallelized Programs Using Optimistic Synchronization Primitives  
Author: Martin Rinard 
Web: http://www.cs.ucsb.edu/~martin  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: As shared-memory multiprocessors become the dominant commodity source of computation, parallelizing compilers must support mainstream computations that manipulate irregular, pointer-based data structures such as lists, trees and graphs. Our experience with a parallelizing compiler for this class of applications shows that their synchronization requirements differ significantly from those of traditional parallel computations. Instead of coarse-grain barrier synchronization, irregular computations require synchronization primitives that support efficient fine-grain atomic operations. The standard implementation mechanism for atomic operations uses mutual exclusion locks. But the overhead of acquiring and releasing locks can reduce the performance. Locks can also consume significant amounts of memory. Optimisitic synchronization primitives such as load linked/store conditional are an attractive alternative. They require no additional memory and eliminate the use of heavyweight blocking synchronization constructs. This paper presents our experience using optimistic synchronization to implement fine-grain atomic operations in the context of a parallelizing compiler for irregular object-based programs. We have implemented two versions of the compiler. One version generates code that uses mutual exclusion locks to make operations execute atomically. The other version uses optimistic synchronization. This paper presents the first published algorithm that enables compilers to automatically generate optimistically synchronized parallel code. The presented experimental results indicate that optimistic synchronization is clearly the superior choice for our set of applications. Our results show that it can significantly reduce the memory consumption and improve the overall performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: To our knowledge, our compiler is the first compiler to automatically generate optimistically synchronized code. Although the algorithms presented in this paper are designed for commodity multiprocessors, they are also useful for more aggressive machines, such as Tera <ref> [1] </ref> or Monsoon [14], that augment each word of memory with state bits. These machines provide a synchronizing read instruction that suspends until a state bit is set, then atomically reads the value in the word and clears the bit.
Reference: [2] <author> T. Anderson. </author> <title> The performance of spin lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1):616, </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: When the implementation attempts to acquire a lock that is not free or the instruction sequence that implements the test and set fails, it must retry until it acquires the lock. The implementation uses exponential backoff to eliminate immediate, repeated attempts to acquire an unavailable lock <ref> [2, 21] </ref>. In practice, we expect programs compiled for multiprogrammed machines to use less efficient primitives that invoke the operating system to suspend a thread if it is unable to acquire a lock [17].
Reference: [3] <author> D. Bacon, S. Graham, and O. Sharp. </author> <title> Compiler transformations for high-performance computing. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(4), </volume> <month> De-cember </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Parallelizing compilers have traditionally exploited a specific, restricted form of concurrency: the concurrency available in loops that access dense matrices using affine access functions <ref> [3] </ref>. The generated parallel programs use a correspondingly restricted kind of synchronization: coarse-grain barrier synchronization at the end of each parallel loop. As shared-memory multiprocessors become the dominant commodity source of computation, parallelizing compilers must sup port a wider class of computations.
Reference: [4] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> pages 446449, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: The nonblocking nature of optimistic synchronization ensures that the combination of the two different synchronization mechanisms never causes deadlock. 6 Experimental Results We next present experimental results that characterize the performance and memory impact of using optimistic synchronization. We present results for three automatically parallelized applications: Barnes-Hut <ref> [4] </ref>, a hierarchical N-body solver, String [10], which builds a velocity model of the geology between two oil wells, and Water [26], which simulates water molecules in the liquid state. Each application performs a complete computation of interest to the scientific computing community.
Reference: [5] <author> P. Barth, R. Nikhil, and Arvind. M-structures: </author> <title> Extending a parallel, non-strict, functional language with state. </title> <booktitle> In Proceedings of the Fifth ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 538568. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: Several software systems use the state bits and synchronizing read and write instructions on machines such as Tera and Monsoon to support efficient fine-grain synchronization. The implementation of M-structures in the dataflow language Id uses the state bits to support efficient, implicitly synchronized atomic operations <ref> [5] </ref>. The Tera compiler exploits the state bits to automatically paral-lelize loops that update indirectly accessed arrays. The generated code uses the bits to make the updates execute atomically.
Reference: [6] <institution> Digital Equipment Corporation. Alpha Architecture Handbook. </institution> <year> 1992. </year>
Reference-contexts: In this section we focus on hardware primitives available on MIPS processors such as the MIPS R4400 and R10000, although primitives that provide similar functionality are available in most modern processors <ref> [6, 15] </ref>. The two key instructions are the load linked (ll) and store conditional (sc) instructions [16], which can be used together to atomically update a memory location as follows. The program first uses a load linked instruction to load the original value from the memory location into a register.
Reference: [7] <author> P. Diniz and M. Rinard. </author> <title> Synchronization transformations for parallel computing. </title> <booktitle> In Proceedings of the Twenty-fourth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Paris, France, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: The overhead of executing the acquire and release constructs may also reduce the performance. Locking the computation at a coarse granularity (by mapping multiple pieces of data to the same lock) addresses these problems <ref> [7] </ref>. It reduces the impact on the memory system and may allow the program to amortize the lock overhead over many updates to different pieces of data. Unfortunately, a coarse lock granularity may also introduce false exclusion. <p> But the compiler analyzes the program to detect sequences of operations that acquire and release the same lock. It then transforms the sequence so that it acquires the lock once, executes the operations without synchronization, then releases the lock <ref> [7] </ref>. The Optimistic versions synchronize at the granularity of individual data items. The Item Lock versions also synchronize at this granularity, but use locks instead of optimistic synchronization.
Reference: [8] <author> K. Gharachorloo. </author> <title> Memory Consistency Models for Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <year> 1996. </year>
Reference-contexts: Modern machines that implement relaxed memory consistency models typically enforce this order by waiting for all outstanding writes to complete globally before releasing a lock <ref> [8] </ref>. But unlike lock synchronization, optimistic synchronization imposes no additional order between accesses to different memory locations.
Reference: [9] <author> S. Graham, P. Kessler, and M. McKusick. </author> <title> gprof: a call graph execution profiler. </title> <booktitle> In Proceedings of the SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <address> Boston, MA, </address> <month> June </month> <year> 1982. </year>
Reference-contexts: Serial 140.75 - Optimistic 154.49 42.51 23.19 17.07 13.64 11.82 10.54 Item Lock 175.38 48.25 26.05 18.89 15.22 13.16 11.73 Object Lock 159.54 43.10 24.12 17.57 14.21 12.30 10.94 Coarse Lock 147.35 40.32 22.44 16.36 13.28 11.52 10.29 Table 4: Execution Times for Barnes-Hut (seconds) We used program counter sampling <ref> [9, 19] </ref> to measure how much time each version spends in different parts of the parallel computation. We break the execution time down into the following components: 7 The speedup is the running time of the serial version divided by the running time of the parallel version.
Reference: [10] <author> J. Harris, S. Lazaratos, and R. Michelena. </author> <title> Tomographic string inversion. </title> <booktitle> In 60th Annual International Meeting, Society of Exploration and Geophysics, Extended Abstracts, </booktitle> <pages> pages 8285, </pages> <year> 1990. </year>
Reference-contexts: We present results for three automatically parallelized applications: Barnes-Hut [4], a hierarchical N-body solver, String <ref> [10] </ref>, which builds a velocity model of the geology between two oil wells, and Water [26], which simulates water molecules in the liquid state. Each application performs a complete computation of interest to the scientific computing community.
Reference: [11] <author> J. Heinrich. </author> <title> MIPS R4000 Microprocessor User's Manual. </title> <publisher> Prentice--Hall, </publisher> <year> 1993. </year>
Reference-contexts: The acquire is implemented as an inlined code sequence that uses ll and sc to atomically test and set a value that indicates whether the lock is free or not <ref> [11] </ref>. The release simply clears the value. When the implementation attempts to acquire a lock that is not free or the instruction sequence that implements the test and set fails, it must retry until it acquires the lock.
Reference: [12] <author> M. Herlihy. </author> <title> A methodology for implementing highly concurrent data objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(9):745770, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: As implemented in the MIPS processor family, ll and sc directly support atomic operations only on 32 bit and 64 bit data items. Although it is possible to use the instructions to synthesize atomic operations on larger data items, the transformations may impose substantial data structure modifications <ref> [12] </ref>. Because these modifications may degrade the performance, the current compiler uses optimistic synchronization only for updates to 32 or 64 bit data items. <p> These problems include poor responsiveness, lock convoys, priority inversions and deadlock <ref> [12] </ref>. Optimistically synchronized data structures such as atomic queues allow programmers to avoid these problems. Such data structures were a key component of the extremely efficient Synthesis kernel [20], a complete operating system kernel built without blocking synchronization. Herlihy has developed a general methodology for implementing optimistically synchronized data structures [12], <p> <ref> [12] </ref>. Optimistically synchronized data structures such as atomic queues allow programmers to avoid these problems. Such data structures were a key component of the extremely efficient Synthesis kernel [20], a complete operating system kernel built without blocking synchronization. Herlihy has developed a general methodology for implementing optimistically synchronized data structures [12], and other researchers have implemented and measured the performance of several such data structures [23]. Our research explores the use of optimistic synchronization in a different context (a parallelizing compiler for irregular, object-based computations) and for a different reason (to enable efficient fine-grain synchronization).
Reference: [13] <author> M. Herlihy and J. Moss. </author> <title> Transactional memory: architectural support for lock-free data structures. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Because these modifications may degrade the performance, the current compiler uses optimistic synchronization only for updates to 32 or 64 bit data items. Transactional memory <ref> [13] </ref> would support optimistically synchronized atomic operations on larger objects, but no hardware implementation of this mechanism currently exists. 3 An Example We next provide an example that illustrates how the compiler can use optimistic synchronization to implement atomic operations. The program in Figure 2 implements a graph traversal.
Reference: [14] <author> J. Hicks, D. Chiou, B. Ang, and Arvind. </author> <title> Performance studies of Id on the Monsoon dataflow system. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(3):273300, </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: To our knowledge, our compiler is the first compiler to automatically generate optimistically synchronized code. Although the algorithms presented in this paper are designed for commodity multiprocessors, they are also useful for more aggressive machines, such as Tera [1] or Monsoon <ref> [14] </ref>, that augment each word of memory with state bits. These machines provide a synchronizing read instruction that suspends until a state bit is set, then atomically reads the value in the word and clears the bit.
Reference: [15] <author> Motorola Incorporated. </author> <title> PowerPC 601 RISC Microprocessor User's Manual. </title> <institution> IBM Microelectronics, Phoenix, AR, </institution> <year> 1993. </year>
Reference-contexts: In this section we focus on hardware primitives available on MIPS processors such as the MIPS R4400 and R10000, although primitives that provide similar functionality are available in most modern processors <ref> [6, 15] </ref>. The two key instructions are the load linked (ll) and store conditional (sc) instructions [16], which can be used together to atomically update a memory location as follows. The program first uses a load linked instruction to load the original value from the memory location into a register.
Reference: [16] <author> E. Jensen, G. Hagensen, and J. Broughton. </author> <title> A new approach to exclusive data access in shared memory multiprocessors. </title> <type> Technical Report UCRL-97663, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: In this section we focus on hardware primitives available on MIPS processors such as the MIPS R4400 and R10000, although primitives that provide similar functionality are available in most modern processors [6, 15]. The two key instructions are the load linked (ll) and store conditional (sc) instructions <ref> [16] </ref>, which can be used together to atomically update a memory location as follows. The program first uses a load linked instruction to load the original value from the memory location into a register.
Reference: [17] <author> A. Karlin, K. Li, M. Manasse, and S. Owicki. </author> <title> Empirical studies of competitive spinning for a shared-memory multiprocessor. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: The implementation uses exponential backoff to eliminate immediate, repeated attempts to acquire an unavailable lock [2, 21]. In practice, we expect programs compiled for multiprogrammed machines to use less efficient primitives that invoke the operating system to suspend a thread if it is unable to acquire a lock <ref> [17] </ref>. The presented results therefore overstate the efficiency of the versions that use locks. 6.2 Cost Of Basic Operations Table 1 presents the execution times for a single update implemented using different synchronization mechanisms.
Reference: [18] <author> J. King. </author> <title> Symbolic execution and program testing. </title> <journal> Communications of the ACM, </journal> <volume> 19(7):385394, </volume> <month> July </month> <year> 1976. </year>
Reference-contexts: Except where noted, the update and invocation expressions contain only instance variables and parameters the algorithm uses symbolic execution to eliminate local variables from the update and invocation expres-sions <ref> [18, 25] </ref>. An update expression of the form v=exp represents an update to a scalar instance variable v. The symbolic expression exp denotes the new value of v. An update expression v [exp 0 ]=exp represents an update to the array instance variable v.
Reference: [19] <author> D. Knuth. </author> <title> An empirical study of FORTRAN programs. </title> <journal> Software Practice and Experience, </journal> <volume> 1:105133, </volume> <year> 1971. </year>
Reference-contexts: Serial 140.75 - Optimistic 154.49 42.51 23.19 17.07 13.64 11.82 10.54 Item Lock 175.38 48.25 26.05 18.89 15.22 13.16 11.73 Object Lock 159.54 43.10 24.12 17.57 14.21 12.30 10.94 Coarse Lock 147.35 40.32 22.44 16.36 13.28 11.52 10.29 Table 4: Execution Times for Barnes-Hut (seconds) We used program counter sampling <ref> [9, 19] </ref> to measure how much time each version spends in different parts of the parallel computation. We break the execution time down into the following components: 7 The speedup is the running time of the serial version divided by the running time of the parallel version.
Reference: [20] <author> H. Massalin and C. Pu. </author> <title> Threads and input/output in the Synthesis kernel. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: These problems include poor responsiveness, lock convoys, priority inversions and deadlock [12]. Optimistically synchronized data structures such as atomic queues allow programmers to avoid these problems. Such data structures were a key component of the extremely efficient Synthesis kernel <ref> [20] </ref>, a complete operating system kernel built without blocking synchronization. Herlihy has developed a general methodology for implementing optimistically synchronized data structures [12], and other researchers have implemented and measured the performance of several such data structures [23].
Reference: [21] <author> J. Mellor-Crummey and M. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1):2165, </volume> <month> February </month> <year> 1991. </year>
Reference-contexts: When the implementation attempts to acquire a lock that is not free or the instruction sequence that implements the test and set fails, it must retry until it acquires the lock. The implementation uses exponential backoff to eliminate immediate, repeated attempts to acquire an unavailable lock <ref> [2, 21] </ref>. In practice, we expect programs compiled for multiprogrammed machines to use less efficient primitives that invoke the operating system to suspend a thread if it is unable to acquire a lock [17].
Reference: [22] <author> M. Michael and M. Scott. </author> <title> Implementation of atomic primitives on distributed shared memory multiprocessors. </title> <booktitle> In Proceedings of the First IEEE Symposium on High-Performance Computer Architecture, </booktitle> <address> Raleigh, NC, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: Register 4 ($4) contains a pointer to the variable to increment. The standard implementation mechanism for load linked and store conditional uses a reservation address register that holds the address from the last load linked instruction and a reservation bit that is invalidated when the address is written <ref> [22] </ref>. As implemented in the MIPS processor family, ll and sc directly support atomic operations only on 32 bit and 64 bit data items. Although it is possible to use the instructions to synthesize atomic operations on larger data items, the transformations may impose substantial data structure modifications [12].
Reference: [23] <author> M. Michael and M. Scott. </author> <title> Simple, fast and practical non-blocking and blocking concurrent queue algorithms. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Distributed Computing, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Herlihy has developed a general methodology for implementing optimistically synchronized data structures [12], and other researchers have implemented and measured the performance of several such data structures <ref> [23] </ref>. Our research explores the use of optimistic synchronization in a different context (a parallelizing compiler for irregular, object-based computations) and for a different reason (to enable efficient fine-grain synchronization).
Reference: [24] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy task creation: a technique for increasing the granularity of parallel programs. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 185197, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The parallel visit operation executes the recursive calls concurrently using the spawn construct, which creates a new task for each operation. A straightforward application of lazy task creation <ref> [24] </ref> can increase the granularity of the resulting parallel computation. version that uses locks, there is no change to the graph objects. Instead of acquiring and releasing locks, the parallel visit operation uses a load linked instruction to fetch the value of sum.
Reference: [25] <author> M. Rinard and P. Diniz. </author> <title> Commutativity analysis: A new analysis framework for parallelizing compilers. </title> <booktitle> In Proceedings of the SIG-PLAN '96 Conference on Program Language Design and Implementation, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: It will be especially important to support irregular, object-based computations that access dynamic, pointer-based data structures such as lists, trees and graphs. We have implemented a parallelizing compiler for this class of computations <ref> [25] </ref>. Our experience using this compiler shows that the automatically generated parallel computations exhibit a very different kind of concurrency. Instead of parallel loops with coarse-grain barrier synchronization, the computations are structured as a set of lightweight threads that synchronize using fine-grained atomic operations. <p> This paper describes our experience using optimistic synchronization to implement atomic operations in the context of a paral-lelizing compiler for object-based programs. The compiler accepts an unannotated serial program written in a subset of C++ and automatically generates a parallel program that performs the same computation <ref> [25] </ref>. The compiler is designed to parallelize irregular computations that manipulate dynamic, pointer-based data structures such as lists, trees and graphs. Because it uses commutativity analysis [25] as its primary analysis technique, the compiler views the computation as consisting of a sequence of operations on objects. <p> The compiler accepts an unannotated serial program written in a subset of C++ and automatically generates a parallel program that performs the same computation <ref> [25] </ref>. The compiler is designed to parallelize irregular computations that manipulate dynamic, pointer-based data structures such as lists, trees and graphs. Because it uses commutativity analysis [25] as its primary analysis technique, the compiler views the computation as consisting of a sequence of operations on objects. If all of the operations in a given computation commute (i.e. generate the same result regardless of the order in which they execute), the compiler can automatically generate parallel code. <p> The way to parallelize this computation is to execute the two recursive invocations in parallel. Our compiler is able to use commutativity analysis to statically detect this source of concurrency <ref> [25] </ref>. But because the data structure may be a graph, the parallel traversal may visit the same node multiple times. <p> The commutativity analysis algorithm extracts some information that the synchronization selection algorithm uses. Specifically, it produces the set of operations that the parallel phase may invoke and the set of instance variables that the phase may update <ref> [25] </ref>. For each operation, it also produces a set of update expressions that represent how the operation updates instance variables and a multiset of invocation expressions that represent the multiset of operations that the operation may invoke. <p> Except where noted, the update and invocation expressions contain only instance variables and parameters the algorithm uses symbolic execution to eliminate local variables from the update and invocation expres-sions <ref> [18, 25] </ref>. An update expression of the form v=exp represents an update to a scalar instance variable v. The symbolic expression exp denotes the new value of v. An update expression v [exp 0 ]=exp represents an update to the array instance variable v. <p> The analysis takes place at the granularity of instance variables and multisets of invoked operations two operations commute if the instance variables and multisets of invoked operations are the same in both execution orders <ref> [25] </ref>. But optimistically synchronized updates execute atomically only at the granularity of individual updates, not at the coarser granularity of complete operations (each operation may perform multiple updates). <p> The commutativity analysis algorithm ensures that all invocations of operations that may access potentially updated variables occur after the invoking operation has completed its last access to a potentially updated variable <ref> [25] </ref>. This separability property ensures that the generated code never attempts to acquire more than one lock, which in turn ensures that it never deadlocks. It is possible for an operation to synchronize some of its updates using locks and other updates using optimistic synchronization. <p> The serial version executes with no parallelization overhead, and, like the serial version of Water, performs slightly better than a highly optimized version written in C <ref> [25] </ref>.
Reference: [26] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. Computer Architecture News, </title> <address> 20(1):544, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: We present results for three automatically parallelized applications: Barnes-Hut [4], a hierarchical N-body solver, String [10], which builds a velocity model of the geology between two oil wells, and Water <ref> [26] </ref>, which simulates water molecules in the liquid state. Each application performs a complete computation of interest to the scientific computing community.
References-found: 26

