URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3162/3162.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Phone: 2  
Title: Evaluation of Pattern Classifiers for Fingerprint and OCR Applications  
Author: J.L. Blue G.T. Candela P.J. Grother R. Chellappa C.L. Wilson 
Address: Gaithersburg, MD 20899  College Park, MD 20742-3275  
Affiliation: 1 National Institute of Standards and Technology  Department of Electrical Engineering Computer Vision Laboratory Center for Automation Research University of Maryland  
Date: October 1993  
Pubnum: CAR-TR-691  
Abstract: In this paper we evaluate the classification accuracy of four statistical and three neural network classifiers for two image based pattern classification problems. These are fingerprint classification and optical character recognition (OCR) for isolated handprinted digits. The evaluation results reported here should be useful for designers of practical systems for these two important commercial applications. For the OCR problem, the Karhunen-Loeve (K-L) transform of the images is used to generate the input feature set. Similarly for the fingerprint problem, the K-L transform of the ridge directions is used to generate the input feature set. The statistical classifiers used were Euclidean minimum distance, quadratic minimum distance, normal, and k-nearest neighbor. The neural network classifiers used were multilayer perceptron, radial basis function, and probabilistic. The OCR data consisted of 7,480 digit images for training and 23,140 digit images for testing. The fingerprint data consisted of 2,000 training and 2,000 testing images. In addition to evaluation for accuracy, the multilayer perceptron and radial basis function networks were evaluated for size and generalization capability. For the evaluated datasets the best accuracy obtained for either problem was provided by the probabilistic neural network, where the minimum classification error was 2.5% for OCR and 7.2% for fingerprints. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.T. Musavi, K.H. Chan, D.M. Hummels, K. Kalantri, and W. Ahmed. </author> <title> A probabilistic model for evaluation of neural network classifiers. </title> <journal> Pattern Recognition, </journal> <volume> 25 </volume> <pages> 1241-1251, </pages> <year> 1992. </year>
Reference-contexts: Since the mid-eighties, NN techniques have raised the possibility of realizing fast, adaptive systems for pattern classification. In spite of all these advances, very little has been done on evaluating the different classifiers for one or more applications. Only recently, a probabilistic model has been proposed <ref> [1] </ref> for the evaluation of NN classifiers, with results on synthetic data. Known theoretical results on error bounds and probabilities are often based on ideal distributions of class conditional densities and/or on infinite samples. <p> using the Boltzmann method was carried out by selecting a normalized temperature, T , and removing weights based on a probability of removal P i = exp (w 2 The values of P i are compared to a set of uniformly distributed random numbers, R i , on the interval <ref> [0; 1] </ref>. If the probability P i is greater than R i then the weight is set to zero. The process is carried out for each iteration of the SCG optimization process and is dynamic.
Reference: [2] <author> J. Cao, M. Shridhar, F. Kimura, and M. Ahmadi. </author> <title> Statistical and neural classification of handwritten numerals: A comparative study. </title> <booktitle> In 11th IAPR International Conference on Pattern Recognition, </booktitle> <pages> pages 643-646, </pages> <address> The Hague, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: We hope that with the availability of enormous computational resources, more such evaluation studies on large datasets can be undertaken. A limited performance evaluation study using a statistical classifier and a backpropagation algorithm for the recognition of handwritten numerals can be found in <ref> [2] </ref>. The organization of the paper is as follows. After a brief introduction to the OCR and fingerprint classification problems in Section 1, we discuss, in Section 2, the datasets used in the experiments. The system components are described in Section 3.
Reference: [3] <author> R.A. Wilkinson, J. Geist, S. Janet, P.J. Grother, C.J.C. Burges, R. Creecy, B. Ham mond, J.J. Hull, N.J. Larsen, T.P. Vogl, and C.L. Wilson. </author> <title> The First Optical Character Recognition Systems Conference. </title> <type> Technical Report NISTIR 4912, </type> <institution> National Institute of Standards and Technology, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: The ready availability of image samples and the continuing challenge of commercially viable recognition has kept OCR research ongoing. Classification of loosely constrained handwritten digits, at least, is largely a solved problem <ref> [3] </ref>. A good review of OCR can be found in [4]. A huge quantity of research from academia and industry has yielded a multitude of algorithms for normalization [5, 6], feature extraction [7], and classification [8, 9, 10, 11], that are capable of OCR of digits.
Reference: [4] <author> T. Pavlidis and S. Mori, </author> <title> editors. Special Issue on Optical Character Recognition. </title> <journal> Proc. IEEE, </journal> <volume> Volume 80, Number 7, </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: The ready availability of image samples and the continuing challenge of commercially viable recognition has kept OCR research ongoing. Classification of loosely constrained handwritten digits, at least, is largely a solved problem [3]. A good review of OCR can be found in <ref> [4] </ref>. A huge quantity of research from academia and industry has yielded a multitude of algorithms for normalization [5, 6], feature extraction [7], and classification [8, 9, 10, 11], that are capable of OCR of digits.
Reference: [5] <author> F.L. Alt. </author> <title> Digital pattern recognition by moments. In G.L. </title> <editor> Fischer et al., editors, </editor> <booktitle> Optical Character Recognition, </booktitle> <pages> pages 159-179. </pages> <editor> McGregor & Werner, </editor> <year> 1962. </year>
Reference-contexts: Classification of loosely constrained handwritten digits, at least, is largely a solved problem [3]. A good review of OCR can be found in [4]. A huge quantity of research from academia and industry has yielded a multitude of algorithms for normalization <ref> [5, 6] </ref>, feature extraction [7], and classification [8, 9, 10, 11], that are capable of OCR of digits. The popularity of OCR research has increased with the advent of NN paradigms applicable to feature extraction and classification. The advantage of many NN classifiers, once trained, is their efficiency.
Reference: [6] <author> R.G. Casey. </author> <title> Moment normalization of handprinted characters. </title> <institution> IBM J. Res. Dev., </institution> <year> 1970. </year> <month> 34 </month>
Reference-contexts: Classification of loosely constrained handwritten digits, at least, is largely a solved problem [3]. A good review of OCR can be found in [4]. A huge quantity of research from academia and industry has yielded a multitude of algorithms for normalization <ref> [5, 6] </ref>, feature extraction [7], and classification [8, 9, 10, 11], that are capable of OCR of digits. The popularity of OCR research has increased with the advent of NN paradigms applicable to feature extraction and classification. The advantage of many NN classifiers, once trained, is their efficiency.
Reference: [7] <author> G.L. Cash and M. Hatamian. </author> <title> Optical character recognition by the method of moments. </title> <journal> CVGIP, </journal> <volume> 39 </volume> <pages> 291-310, </pages> <year> 1987. </year>
Reference-contexts: Classification of loosely constrained handwritten digits, at least, is largely a solved problem [3]. A good review of OCR can be found in [4]. A huge quantity of research from academia and industry has yielded a multitude of algorithms for normalization [5, 6], feature extraction <ref> [7] </ref>, and classification [8, 9, 10, 11], that are capable of OCR of digits. The popularity of OCR research has increased with the advent of NN paradigms applicable to feature extraction and classification. The advantage of many NN classifiers, once trained, is their efficiency.
Reference: [8] <author> T.M. Cover and P.E. Hart. </author> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Processing, </journal> <volume> 13 </volume> <pages> 21-27, </pages> <year> 1967. </year>
Reference-contexts: Classification of loosely constrained handwritten digits, at least, is largely a solved problem [3]. A good review of OCR can be found in [4]. A huge quantity of research from academia and industry has yielded a multitude of algorithms for normalization [5, 6], feature extraction [7], and classification <ref> [8, 9, 10, 11] </ref>, that are capable of OCR of digits. The popularity of OCR research has increased with the advent of NN paradigms applicable to feature extraction and classification. The advantage of many NN classifiers, once trained, is their efficiency. <p> The 1-NN classification of an unknown vector is simply the class of the nearest prototype. This rule is intuitively appealing, and Cover and Hart <ref> [8] </ref> have shown it to have good asymptotic behavior: under mild assumptions, its large-sample probability of error is bounded above by twice the Bayes (i.e. minimum possible) probability of error.
Reference: [9] <author> Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. </author> <title> Handwritten digit recognition with a back-propagation network. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Classification of loosely constrained handwritten digits, at least, is largely a solved problem [3]. A good review of OCR can be found in [4]. A huge quantity of research from academia and industry has yielded a multitude of algorithms for normalization [5, 6], feature extraction [7], and classification <ref> [8, 9, 10, 11] </ref>, that are capable of OCR of digits. The popularity of OCR research has increased with the advent of NN paradigms applicable to feature extraction and classification. The advantage of many NN classifiers, once trained, is their efficiency.
Reference: [10] <author> J.S. Denker, W.R. Gardner, H.P. Graf, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, H.S. Baird, and I. Guyon. </author> <title> Neural network recognizer for hand-written zip code digits. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 1, </volume> <pages> pages 323-331. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: Classification of loosely constrained handwritten digits, at least, is largely a solved problem [3]. A good review of OCR can be found in [4]. A huge quantity of research from academia and industry has yielded a multitude of algorithms for normalization [5, 6], feature extraction [7], and classification <ref> [8, 9, 10, 11] </ref>, that are capable of OCR of digits. The popularity of OCR research has increased with the advent of NN paradigms applicable to feature extraction and classification. The advantage of many NN classifiers, once trained, is their efficiency.
Reference: [11] <author> K. Fukushima, T. Imagawa, and E Ashida. </author> <title> Character recognition with selective atten tion. </title> <booktitle> In Proceedings of the IJCNN, </booktitle> <volume> volume 1, </volume> <pages> pages 593-598, </pages> <year> 1991. </year>
Reference-contexts: Classification of loosely constrained handwritten digits, at least, is largely a solved problem [3]. A good review of OCR can be found in [4]. A huge quantity of research from academia and industry has yielded a multitude of algorithms for normalization [5, 6], feature extraction [7], and classification <ref> [8, 9, 10, 11] </ref>, that are capable of OCR of digits. The popularity of OCR research has increased with the advent of NN paradigms applicable to feature extraction and classification. The advantage of many NN classifiers, once trained, is their efficiency.
Reference: [12] <author> G. Martin and J. Pittman. </author> <title> Recognizing handprinted letters and digits using backprop agation. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 258-267, </pages> <year> 1991. </year>
Reference-contexts: The popularity of OCR research has increased with the advent of NN paradigms applicable to feature extraction and classification. The advantage of many NN classifiers, once trained, is their efficiency. In future commercial segmentation and recognition efforts <ref> [12, 13] </ref>, lack of efficiency will preclude using numerous techniques from the literature because of their computational requirements.
Reference: [13] <author> G.L. Martin. </author> <title> Centered-object integrated segmentation and recognition for visual char acter recognition. </title> <editor> In J. Moody, S. Hanson, and R. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 504-511. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: The popularity of OCR research has increased with the advent of NN paradigms applicable to feature extraction and classification. The advantage of many NN classifiers, once trained, is their efficiency. In future commercial segmentation and recognition efforts <ref> [12, 13] </ref>, lack of efficiency will preclude using numerous techniques from the literature because of their computational requirements.
Reference: [14] <editor> M.K. Sparrow and P.J. </editor> <title> Sparrow. A Topological Approach to the Matching of Single Fingerprints: Development of Algorithms for Use on Rolled Impressions. </title> <note> Technical Report Special Publication SW-124, </note> <institution> National Bureau of Standards, </institution> <address> Washington, DC, </address> <month> October </month> <year> 1985. </year>
Reference-contexts: The trade-off between classification performance and computational requirements has prompted this study of digit classifier efficacy. 2 1.2 The fingerprint problem At least three major approaches have been taken to automatic fingerprint classification. These are the structural, syntactic, and artificial neural network (ANN) approaches. In the structural approach <ref> [14, 15, 16] </ref>, one extracts features based on minutiae and represents the features using a graph data structure. Structural matching is done by exploiting the topology of the features.
Reference: [15] <author> P.K. Isenor and S.A. Zapy. </author> <title> Fingerprint identification using graph matching. </title> <journal> Pattern Recognition, </journal> <volume> 19 </volume> <pages> 113-122, </pages> <year> 1986. </year> <month> 35 </month>
Reference-contexts: The trade-off between classification performance and computational requirements has prompted this study of digit classifier efficacy. 2 1.2 The fingerprint problem At least three major approaches have been taken to automatic fingerprint classification. These are the structural, syntactic, and artificial neural network (ANN) approaches. In the structural approach <ref> [14, 15, 16] </ref>, one extracts features based on minutiae and represents the features using a graph data structure. Structural matching is done by exploiting the topology of the features.
Reference: [16] <author> A.K. Hrechak and J.A. McHugh. </author> <title> Automated fingerprint identification using structured matching. </title> <journal> Pattern Recognition, </journal> <volume> 23 </volume> <pages> 893-904, </pages> <year> 1990. </year>
Reference-contexts: The trade-off between classification performance and computational requirements has prompted this study of digit classifier efficacy. 2 1.2 The fingerprint problem At least three major approaches have been taken to automatic fingerprint classification. These are the structural, syntactic, and artificial neural network (ANN) approaches. In the structural approach <ref> [14, 15, 16] </ref>, one extracts features based on minutiae and represents the features using a graph data structure. Structural matching is done by exploiting the topology of the features.
Reference: [17] <author> K.S. Fu. </author> <title> Syntactic Pattern Recognition. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: These are the structural, syntactic, and artificial neural network (ANN) approaches. In the structural approach [14, 15, 16], one extracts features based on minutiae and represents the features using a graph data structure. Structural matching is done by exploiting the topology of the features. In the syntactic approach <ref> [17] </ref>, one typically approximates ridge patterns [18] as strings of primitives and models the plausible strings from a class such as Tented Arch using production rules as in grammar. Depending on the type of grammar used one can see a significant drop in the number of production rules required.
Reference: [18] <author> B. Moayer and K.S. Fu. </author> <title> A syntactic approach to fingerprint pattern recognition. </title> <journal> Pattern Recognition, </journal> <volume> 7 </volume> <pages> 1-23, </pages> <year> 1975. </year>
Reference-contexts: In the structural approach [14, 15, 16], one extracts features based on minutiae and represents the features using a graph data structure. Structural matching is done by exploiting the topology of the features. In the syntactic approach [17], one typically approximates ridge patterns <ref> [18] </ref> as strings of primitives and models the plausible strings from a class such as Tented Arch using production rules as in grammar. Depending on the type of grammar used one can see a significant drop in the number of production rules required.
Reference: [19] <author> B. Moayer and K.S. Fu. </author> <title> An application of stochastic languages to fingerprint pattern recognition. </title> <journal> Pattern Recognition, </journal> <volume> 8 </volume> <pages> 173-179, </pages> <year> 1976. </year>
Reference-contexts: Whether the parsing is successful or not, a description of the input string is always generated. The more general the grammar is, the more complex the parser tends to be. Applications of the syntactic approach using more complex grammars such as stochastic grammars <ref> [19] </ref> (where probabilities are associated with the production rules), tree grammars [20], and programmed grammars [21] have been considered for the fingerprint classification problem. The main stumbling block of the syntactic approach is that mechanisms for the inference of grammars from training samples have not been well understood [22, 23].
Reference: [20] <author> B. Moayer and K.S. Fu. </author> <title> A tree system approach for fingerprint pattern recognition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 25 </volume> <pages> 262-274, </pages> <year> 1976. </year>
Reference-contexts: The more general the grammar is, the more complex the parser tends to be. Applications of the syntactic approach using more complex grammars such as stochastic grammars [19] (where probabilities are associated with the production rules), tree grammars <ref> [20] </ref>, and programmed grammars [21] have been considered for the fingerprint classification problem. The main stumbling block of the syntactic approach is that mechanisms for the inference of grammars from training samples have not been well understood [22, 23].
Reference: [21] <author> K. Rao and K. Balck. </author> <title> Type classification of fingerprints: A syntactic approach. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 2 </volume> <pages> 223-231, </pages> <year> 1980. </year>
Reference-contexts: The more general the grammar is, the more complex the parser tends to be. Applications of the syntactic approach using more complex grammars such as stochastic grammars [19] (where probabilities are associated with the production rules), tree grammars [20], and programmed grammars <ref> [21] </ref> have been considered for the fingerprint classification problem. The main stumbling block of the syntactic approach is that mechanisms for the inference of grammars from training samples have not been well understood [22, 23].
Reference: [22] <author> K.S. Fu and T.L. Booth. </author> <title> Grammatical inference: Introduction and survey|Part I. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 5 </volume> <pages> 95-111, </pages> <year> 1975. </year>
Reference-contexts: The main stumbling block of the syntactic approach is that mechanisms for the inference of grammars from training samples have not been well understood <ref> [22, 23] </ref>. Recent advances in learning the structure of a grammar from training samples using neural nets [24] look promising. The Image Recognition Group at NIST has recently implemented a massively parallel NN fingerprint classification system using a parallel computer of the SIMD variety (single-instruction-stream, multiple-data-stream) [25].
Reference: [23] <author> K.S. Fu and T.L. Booth. </author> <title> Grammatical inference: Introduction and survey|Part II. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 5 </volume> <pages> 409-423, </pages> <year> 1975. </year>
Reference-contexts: The main stumbling block of the syntactic approach is that mechanisms for the inference of grammars from training samples have not been well understood <ref> [22, 23] </ref>. Recent advances in learning the structure of a grammar from training samples using neural nets [24] look promising. The Image Recognition Group at NIST has recently implemented a massively parallel NN fingerprint classification system using a parallel computer of the SIMD variety (single-instruction-stream, multiple-data-stream) [25].
Reference: [24] <author> C.L. Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <booktitle> Neural Computation, </booktitle> <address> 4:380, </address> <year> 1992. </year>
Reference-contexts: The main stumbling block of the syntactic approach is that mechanisms for the inference of grammars from training samples have not been well understood [22, 23]. Recent advances in learning the structure of a grammar from training samples using neural nets <ref> [24] </ref> look promising. The Image Recognition Group at NIST has recently implemented a massively parallel NN fingerprint classification system using a parallel computer of the SIMD variety (single-instruction-stream, multiple-data-stream) [25]. This system uses image-based ridge-valley features. Using K-L transforms, a significant reduction in feature vector dimensions is achieved.
Reference: [25] <author> C.L. Wilson, G.T. Candela, P.J. Grother, C.I. Watson, and R.A. Wilkinson. </author> <title> Massively Parallel Neural Network Fingerprint Classification System. </title> <type> Technical Report NISTIR 4880, </type> <institution> National Institute of Standards and Technology, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: Recent advances in learning the structure of a grammar from training samples using neural nets [24] look promising. The Image Recognition Group at NIST has recently implemented a massively parallel NN fingerprint classification system using a parallel computer of the SIMD variety (single-instruction-stream, multiple-data-stream) <ref> [25] </ref>. This system uses image-based ridge-valley features. Using K-L transforms, a significant reduction in feature vector dimensions is achieved. A MLP network trained using a conjugate gradient method is used for classification. It takes about 2.7 seconds to preprocess and classify a fingerprint using a massively parallel computer. <p> The NCIC method for producing a card's class from the classes of its individual fingerprints is different than the summarizing method used in the Henry system. 6 of the ridges at 840 equally-spaced locations (a 28 by 30 grid) are then measured, using an orientation finder described in <ref> [25] </ref>. The orientation finder is based on a "ridge-valley" fingerprint binarizer described in [38]. It computes an orientation at the location of each pixel, then averages these basic orientations in nonoverlapping 16 fi 16-pixel squares to produce the grid of 840 orientations.
Reference: [26] <author> P. Baldi and Y. Chauvin. </author> <title> Neural networks for fingerprint matching and classification (abstract). </title> <booktitle> In Proceedings, Neural Information Processing Systems Conference, </booktitle> <year> 1992. </year>
Reference-contexts: A MLP network trained using a conjugate gradient method is used for classification. It takes about 2.7 seconds to preprocess and classify a fingerprint using a massively parallel computer. The system is capable of 93% classification accuracy with 10% rejects. More recently <ref> [26] </ref>, a fingerprint matching and classification system that uses a hierarchical 3 pyramid structure has been reported. The matching module takes two images as inputs and outputs a number between 0 and 1.
Reference: [27] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed 36 Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, Chapter 8, </booktitle> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This finding supports the choice of ridge direction components used as features in the NIST system. 1.3 Generalization The focus of most NN applications has been on error minimization. A standard method of error minimization for real world problems is backpropagation <ref> [27] </ref> although more powerful methods of optimization have also been used [28, 29]. In addition to the problem of error reduction, effective generalization also requires that the information content of the network be reduced to some minimum value [30, 31, 32].
Reference: [28] <author> M.F. Mtller. </author> <title> A Scaled Conjugate Gradient Algorithm for Fast Supervised Learning. </title> <type> Technical Report PB-339, </type> <institution> University of Aarhus, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: A standard method of error minimization for real world problems is backpropagation [27] although more powerful methods of optimization have also been used <ref> [28, 29] </ref>. In addition to the problem of error reduction, effective generalization also requires that the information content of the network be reduced to some minimum value [30, 31, 32]. <p> This has been shown to increase the generalization ability of the network [29]. Networks of the MLP type are the most commonly used "neural nets" in use today, and they are usually trained using a "backpropagation" algorithm [45]. A "scaled conjugate gradient" training method <ref> [46, 47, 28, 29] </ref> was used in our research instead of the ubiquitous backpropagation method, training speed gains of an order of magnitude being typical. input, 48 hidden unit network. discriminant value, difference in highest two discriminant values. 15 4.4.2 Radial Basis Functions Neural nets of the RBF type get their <p> In order to simplify the gradient calculation, the inverses of the widths, s ij = 1= ij , were used as variables. The optimization (training) was done using a combination of scaled conjugate gradients <ref> [28, 29] </ref> and a limited-memory quasi-Newton algorithm [52]. The program was written to allow any combination of the centers, widths, and weights to be learned, and the remainder to stay fixed.
Reference: [29] <author> J.L. Blue and P.J. Grother. </author> <title> Training feed forward networks using conjugate gradients. </title> <booktitle> In Conference on Character Recognition and Digitizer Technologies, SPIE volume 1661, </booktitle> <pages> pages 179-190, </pages> <address> San Jose, CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: A standard method of error minimization for real world problems is backpropagation [27] although more powerful methods of optimization have also been used <ref> [28, 29] </ref>. In addition to the problem of error reduction, effective generalization also requires that the information content of the network be reduced to some minimum value [30, 31, 32]. <p> We have used the Boltzmann method as a secondary method of optimization to prune the networks used here [30, 31]. The method can be used in conjunction with a primary method of optimization such as a scaled conjugate gradient scheme <ref> [29] </ref>. The resulting optimized MLP network has been used for both fingerprint pattern classification and OCR. In the case of RBF networks, the explicit network pruning used on MLP's is unnecessary. RBF networks are self-pruning to some degree. <p> This equals a tunable constant, , multiplied by the mean square weight, w 2 ij . This term prevents large weights which are associated with overtraining, i.e. the overfitting of the weights to the training data. This has been shown to increase the generalization ability of the network <ref> [29] </ref>. Networks of the MLP type are the most commonly used "neural nets" in use today, and they are usually trained using a "backpropagation" algorithm [45]. <p> This has been shown to increase the generalization ability of the network [29]. Networks of the MLP type are the most commonly used "neural nets" in use today, and they are usually trained using a "backpropagation" algorithm [45]. A "scaled conjugate gradient" training method <ref> [46, 47, 28, 29] </ref> was used in our research instead of the ubiquitous backpropagation method, training speed gains of an order of magnitude being typical. input, 48 hidden unit network. discriminant value, difference in highest two discriminant values. 15 4.4.2 Radial Basis Functions Neural nets of the RBF type get their <p> The scaled conjugate gradient (SCG) method of <ref> [29] </ref> is used to obtain a starting network for the Boltzmann weight pruning algorithm. For the OCR problem the network has an input layer with 48 nodes, a hidden layer with 64 nodes, and an output layer with 10 nodes. <p> In order to simplify the gradient calculation, the inverses of the widths, s ij = 1= ij , were used as variables. The optimization (training) was done using a combination of scaled conjugate gradients <ref> [28, 29] </ref> and a limited-memory quasi-Newton algorithm [52]. The program was written to allow any combination of the centers, widths, and weights to be learned, and the remainder to stay fixed.
Reference: [30] <author> O.M. Omidvar and C.L. Wilson. </author> <title> Optimization of neural network topology and informa tion content using Boltzmann methods. </title> <booktitle> In Proceedings of the IJCNN, </booktitle> <volume> Volume 4, </volume> <pages> pages 594-599, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: In addition to the problem of error reduction, effective generalization also requires that the information content of the network be reduced to some minimum value <ref> [30, 31, 32] </ref>. The resulting reduced network has the advantage of increased speed achieved by using fewer connections and is more effective in terms of the use of information capacity to achieve a specified pattern recognition accuracy. <p> This results in a smaller network with a very high information content that allows the use of a reasonably small training set. We have used the Boltzmann method as a secondary method of optimization to prune the networks used here <ref> [30, 31] </ref>. The method can be used in conjunction with a primary method of optimization such as a scaled conjugate gradient scheme [29]. The resulting optimized MLP network has been used for both fingerprint pattern classification and OCR.
Reference: [31] <author> O.M. Omidvar and C.L. Wilson. </author> <title> Topological separation versus weight sharing in neural network optimization. </title> <editor> In S.S. Chen, editor, </editor> <booktitle> Neural and Stochastic Methods in Image and Signal Processing, SPIE volume 1766, </booktitle> <address> San Diego, CA, </address> <year> 1992. </year>
Reference-contexts: In addition to the problem of error reduction, effective generalization also requires that the information content of the network be reduced to some minimum value <ref> [30, 31, 32] </ref>. The resulting reduced network has the advantage of increased speed achieved by using fewer connections and is more effective in terms of the use of information capacity to achieve a specified pattern recognition accuracy. <p> This results in a smaller network with a very high information content that allows the use of a reasonably small training set. We have used the Boltzmann method as a secondary method of optimization to prune the networks used here <ref> [30, 31] </ref>. The method can be used in conjunction with a primary method of optimization such as a scaled conjugate gradient scheme [29]. The resulting optimized MLP network has been used for both fingerprint pattern classification and OCR.
Reference: [32] <author> I. Guyon, V.N. Vapnick, B.E. Boser, L.Y. Botton, and S.A. Solla. </author> <title> Structural risk minimization for character recognition. </title> <editor> In R. Lippmann, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 471-479. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: In addition to the problem of error reduction, effective generalization also requires that the information content of the network be reduced to some minimum value <ref> [30, 31, 32] </ref>. The resulting reduced network has the advantage of increased speed achieved by using fewer connections and is more effective in terms of the use of information capacity to achieve a specified pattern recognition accuracy.
Reference: [33] <author> M.D. Garris and R.A. Wilkinson. </author> <title> Handwritten Segmented Characters Database. </title> <type> Tech nical Report Special Database 3, </type> <institution> National Institute of Standards and Technology, </institution> <month> Febru-ary </month> <year> 1992. </year>
Reference-contexts: More pruning is done with small training sets than with large ones, and more with large networks than with small ones. 2 Databases 2.1 OCR The classifiers described in this report were trained and tested using feature vectors derived from the digit images of NIST Special Database 3 <ref> [33] </ref>. This database consists of binary 128 by 128 pixel raster images segmented from the sample forms of 2100 writers published on CD as [34]. Other results on segmentation and recognition of this database have been reported [35]. <p> Only the training and recognition parts of the system were involved in the test. For the OCR problem two sets of 10,000 K-L features derived from characters taken from NIST Special Database 3 <ref> [33] </ref> were used. For the fingerprint problem 22 Table 3: Fingerprint classification error percentages as a function of feature dimensionality. NRML produced a smaller error percentage for a number of features not in the table: 11.3, for 28 features. <p> This indicates that the information transfer during training is more efficient for eigenvalue-weighted training and that more information is retained. 6.2 Sample Size Based Methods The RBF generalization experiments used training and test sets, each containing 10,000 character images from NIST Special Database 3 <ref> [33] </ref>. Each feature set was a truncated K-L expansion of a 32 by 32 pixel binary image of a hand-printed digit.
Reference: [34] <author> C.L. Wilson and M.D. Garris. </author> <title> Handprinted Character Database. Special Database 1, </title> <institution> National Institute of Standards and Technology, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: This database consists of binary 128 by 128 pixel raster images segmented from the sample forms of 2100 writers published on CD as <ref> [34] </ref>. Other results on segmentation and recognition of this database have been reported [35]. The relative difficulties of the NIST OCR databases have been discussed in [36].
Reference: [35] <author> R.G. Casey and H. Takahashi. </author> <title> Experience in segmenting and recognizing the NIST database. </title> <booktitle> In Proceedings of the International Workshop on Frontiers of Handwriting Recognition, </booktitle> <year> 1991. </year>
Reference-contexts: This database consists of binary 128 by 128 pixel raster images segmented from the sample forms of 2100 writers published on CD as [34]. Other results on segmentation and recognition of this database have been reported <ref> [35] </ref>. The relative difficulties of the NIST OCR databases have been discussed in [36]. For this study samples were drawn randomly from the first 250 writers to yield a training set of 7480 digits with a priori class probabilities all equal to 0:1.
Reference: [36] <author> P.J. Grother. </author> <title> Cross validation comparison of NIST OCR databases. In D.P. </title> <editor> D'Amato, editor, </editor> <booktitle> SPIE volume 1906, </booktitle> <address> San Jose, CA, </address> <year> 1993. </year> <month> 37 </month>
Reference-contexts: This database consists of binary 128 by 128 pixel raster images segmented from the sample forms of 2100 writers published on CD as [34]. Other results on segmentation and recognition of this database have been reported [35]. The relative difficulties of the NIST OCR databases have been discussed in <ref> [36] </ref>. For this study samples were drawn randomly from the first 250 writers to yield a training set of 7480 digits with a priori class probabilities all equal to 0:1.
Reference: [37] <author> C.I. Watson and C.L. Wilson. </author> <title> Fingerprint database. Special Database 4, </title> <institution> National Institute of Standards and Technology, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: is effected by shearing rows by an amount determined by the leftmost and rightmost pixels in the first and last rows defining a vertical line. 2.2 Fingerprints The classifiers described in this report were trained and tested using feature vectors derived from the fingerprint images of NIST Special Database 4 <ref> [37] </ref>. This database consists of 8 bit per pixel gray level raster images of two inked impressions ("rollings") of each of 2000 different fingers. <p> 18.1 19.7 20.7 23.0 1-NN 10.7 9.6 9.7 9.3 9.1 9.0 9.3 WSNN 10.3 9.3 9.1 9.1 8.9 8.9 9.0 MLP 9.1 8.8 8.6 8.2 8.2 8.4 8.5 RBF2 10.7 9.5 10.7 8.1 8.8 8.4 8.2 two sets of 2,000 K-L features derived from fingerprints from NIST Special Database 4 <ref> [37] </ref> were used. The scaled conjugate gradient (SCG) method of [29] is used to obtain a starting network for the Boltzmann weight pruning algorithm. For the OCR problem the network has an input layer with 48 nodes, a hidden layer with 64 nodes, and an output layer with 10 nodes.
Reference: [38] <author> R.M. Stock and C.W. Swonger. </author> <title> Development and evaluation of a reader of finger print minutiae. </title> <institution> Cornell Aeronautical Laboratory, </institution> <type> Technical Report XM-2478-X-1:13-17, </type> <year> 1969. </year>
Reference-contexts: The orientation finder is based on a "ridge-valley" fingerprint binarizer described in <ref> [38] </ref>. It computes an orientation at the location of each pixel, then averages these basic orientations in nonoverlapping 16 fi 16-pixel squares to produce the grid of 840 orientations. These are used as input to a translational registration module that attempts to standardize core location.
Reference: [39] <author> G.T. Candela and R. Chellappa. </author> <title> Comparative Performance of Classification Methods for Fingerprints. </title> <type> Technical Report NISTIR 5163, </type> <institution> National Institute of Standards and Technology, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: In the experiments reported here, the reject option was not exercised. Experiments including rejectors of the following form are reported in <ref> [39] </ref>. The outputs of the discriminant functions are fed to a "confidence function", which produces a number that is treated as if it were a measure of reliability of the classification decision made by the maximum finder.
Reference: [40] <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1990. </year>
Reference-contexts: The classifiers are separated into three categories. It is, however, notable that the category names are somewhat arbitrary and that some classifiers have attributes of more than one category. In the statistical pattern recognition literature <ref> [40] </ref> parametric classifiers use variables such as the estimated means and covariances to express the class density functions. The decision surfaces implicit in the EMD classifier are linear. Those of the QMD and NRML classifiers are quadratic. We categorize the EMD, QMD, and NRML methods as parametric classifiers.
Reference: [41] <author> M. Minsky and S. Papert. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1969. </year>
Reference-contexts: Each cluster region is bounded by a convex polygon. In the one cluster per class case these regions are the hypothetical class decision regions. This classifier suffers from the same linear separability limitations as the Perceptron critiqued by Minsky and Papert <ref> [41] </ref>. In the many clusters per class case the union of the cluster regions defines the class decision region whose boundary is then piecewise linear. Figure 2 shows the class for regions when only two features and one cluster per class are used.
Reference: [42] <author> N.J. Nilsson. </author> <title> Learning Machines: </title> <booktitle> Foundations of Trainable Pattern-Classifying Sys tems, Chapter 3, </booktitle> <pages> page 45. </pages> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1965. </year>
Reference-contexts: In this case, the Bayesian classifier is the one that classifies each unknown x to the class i for which the a posteriori probability p (ijx) is highest. According to Bayes' rule <ref> [42] </ref>, p (ijx) = p (x) Since the value of the mixture density p (x) has no effect on which possible i value maximizes p (ijx), p (x) can always be omitted.
Reference: [43] <editor> B.V. Dasarathy, editor. </editor> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Tech niques. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: The hypothetical class regions are given in Figure 2. 4.3 Nearest Neighbor Classifiers Nearest-neighbor classifiers have been the subject of decades of research (see, for example, Dasarathy's collection of papers <ref> [43] </ref>).
Reference: [44] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78 </volume> <pages> 1481-1497, </pages> <year> 1990. </year>
Reference-contexts: The error function is modified by the addition of a scalar "regularization" term <ref> [44] </ref>. This equals a tunable constant, , multiplied by the mean square weight, w 2 ij . This term prevents large weights which are associated with overtraining, i.e. the overfitting of the weights to the training data. This has been shown to increase the generalization ability of the network [29].
Reference: [45] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning representations by back propagating errors. </title> <journal> Nature, </journal> <volume> 332 </volume> <pages> 533-536, </pages> <year> 1986. </year>
Reference-contexts: This has been shown to increase the generalization ability of the network [29]. Networks of the MLP type are the most commonly used "neural nets" in use today, and they are usually trained using a "backpropagation" algorithm <ref> [45] </ref>.
Reference: [46] <author> R. Fletcher and C.M. Reeves. </author> <title> Function minimization by conjugate gradients. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 149-154, </pages> <year> 1964. </year>
Reference-contexts: This has been shown to increase the generalization ability of the network [29]. Networks of the MLP type are the most commonly used "neural nets" in use today, and they are usually trained using a "backpropagation" algorithm [45]. A "scaled conjugate gradient" training method <ref> [46, 47, 28, 29] </ref> was used in our research instead of the ubiquitous backpropagation method, training speed gains of an order of magnitude being typical. input, 48 hidden unit network. discriminant value, difference in highest two discriminant values. 15 4.4.2 Radial Basis Functions Neural nets of the RBF type get their
Reference: [47] <author> E.M. Johansson, F.U. Dowla, and D.M. Goodman. </author> <title> Backpropagation learning for multi layer feed-forward neural networks using the conjugate gradient method. </title> <journal> IEEE Transactions on Neural Networks. </journal> <note> To be published. </note>
Reference-contexts: This has been shown to increase the generalization ability of the network [29]. Networks of the MLP type are the most commonly used "neural nets" in use today, and they are usually trained using a "backpropagation" algorithm [45]. A "scaled conjugate gradient" training method <ref> [46, 47, 28, 29] </ref> was used in our research instead of the ubiquitous backpropagation method, training speed gains of an order of magnitude being typical. input, 48 hidden unit network. discriminant value, difference in highest two discriminant values. 15 4.4.2 Radial Basis Functions Neural nets of the RBF type get their
Reference: [48] <author> M.T. Musavi, W. Ahmed, K.H. Chan, K.B. Faris, and K.M. Hummels. </author> <title> On the training of radial basis function classifiers. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 595-603, </pages> <year> 1992. </year> <month> 38 </month>
Reference-contexts: They are trained initially using the cluster means (from a "k-means" algorithm applied to the prototype set) as the center vectors c (j) . The width vectors (j) are set to a single tunable positive value. More sophisticated methods of determining RBF parameters can be found in <ref> [48, 49] </ref>.
Reference: [49] <author> D. Wettschereck and T. Dietterich. </author> <title> Improving the performance of radial basis function networks by learning center locations. </title> <editor> In J.E. Moody and S.J. Hanson and R.P. Lipp-mann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 1133-1140, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: They are trained initially using the cluster means (from a "k-means" algorithm applied to the prototype set) as the center vectors c (j) . The width vectors (j) are set to a single tunable positive value. More sophisticated methods of determining RBF parameters can be found in <ref> [48, 49] </ref>.
Reference: [50] <author> D.F. Specht. </author> <title> Probabilistic neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 109-118, </pages> <year> 1990. </year>
Reference-contexts: RBF1 class regions resulting from the use of 1, 2, 4, and 6 hidden nodes per class, and Figure 7 shows RBF2 class regions for the same numbers of hidden nodes per class. 1 2 3 4.4.3 Probabilistic Neural Net This classifier was proposed in a recent paper by Specht <ref> [50] </ref>. Each training example becomes the center of a kernel function which takes its maximum at the example and recedes gradually as one moves away from the example in feature space.
Reference: [51] <author> J.J. Atick. </author> <title> Could information theory provide an ecological theory of sensory processing? Networks, </title> <booktitle> 3 </booktitle> <pages> 213-251, </pages> <year> 1992. </year>
Reference-contexts: This method of pruning is referred to as eigenvalue-weighted pruning. During this optimization process two important measures of information content are calculated <ref> [51] </ref>.
Reference: [52] <author> D.C. Liu and J. Nocedal. </author> <title> On the limited memory BFGS method for large scale opti mization. </title> <journal> Mathematical Programming, </journal> <volume> 45 </volume> <pages> 503-528, </pages> <year> 1989. </year>
Reference-contexts: In order to simplify the gradient calculation, the inverses of the widths, s ij = 1= ij , were used as variables. The optimization (training) was done using a combination of scaled conjugate gradients [28, 29] and a limited-memory quasi-Newton algorithm <ref> [52] </ref>. The program was written to allow any combination of the centers, widths, and weights to be learned, and the remainder to stay fixed. Training was done with varying training set sizes, from 156 patterns to the entire 10,000 patterns; testing was done on the entire 10,000 pattern testing set.
Reference: [53] <author> J.A. Hartigan. </author> <title> Clustering Algorithms. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1975. </year> <month> 39 </month>
Reference-contexts: Training was done with varying training set sizes, from 156 patterns to the entire 10,000 patterns; testing was done on the entire 10,000 pattern testing set. The initial values for the RBF centers were obtained from a k-means algorithm <ref> [53] </ref>. The widths produced by the k-means algorithm were not directly useful. Instead, uniform widths, several times the typical widths from the k-means algorithm, were used.
References-found: 53

