URL: http://www.cs.berkeley.edu/~xiaoye/simax97.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~xiaoye/
Root-URL: 
Title: An Asynchronous Parallel Supernodal Algorithm for Sparse Gaussian Elimination  
Author: James W. Demmel John R. Gilbert Xiaoye S. Li 
Keyword: sparse Gaussian elimination; unsymmetric linear systems; supernodes; parallelism; dynamic scheduling and load balancing.  
Note: AMS(MOS) subject classifications: 65F50, 65F05.  
Date: September 27, 1997  
Abstract: Although Gaussian elimination with partial pivoting is a robust algorithm to solve unsym-metric sparse linear systems of equations, it is difficult to implement efficiently on parallel machines, because of its dynamic and somewhat unpredictable way of generating work and intermediate results at run time. In this paper, we present an efficient parallel algorithm that overcomes this difficulty. The high performance of our algorithm is achieved through (1) using a graph reduction technique and a supernode-panel computational kernel for high single processor utilization, and (2) scheduling two types of parallel tasks for a high level of concurrency. One such task is factoring the independent panels in the disjoint subtrees of the column elimination tree of A. Another task is updating a panel by previously computed supernodes. A scheduler assigns tasks to free processors dynamically and facilitates the smooth transition between the two types of parallel tasks. No global synchronization is used in the algorithm. The algorithm is well suited for shared memory machines (SMP) with a modest number of processors. We demonstrate 4-7 fold speedups on a range of 8 processor SMPs, and more on larger SMPs. One realistic problem arising from a 3-D flow calculation achieves factorization rates of 1.0, 2.5, 0.8 and 0.8 Gigaflops, on the 12 processor Power Challenge, 8 processor Cray C90, 16 processor Cray J90, and 8 processor AlphaServer 8400. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. R. Amestoy and I. S. Duff. MUPS: </author> <title> a parallel package for solving sparse unsymmetric sets of linear equations. </title> <type> Technical report, </type> <institution> CERFACS, Toulouse, France, </institution> <year> 1994. </year>
Reference-contexts: Previous results showed much lower factorization rates because the machines used were relatively slow and the computational kernel in the earlier parallel algorithms was based on Level 1 BLAS. The closest work is the parallel symmetric pattern multifrontal factorization by Amestoy and Duff <ref> [1] </ref>, also on shared memory machines. However, that approach may result in too many nonzeros and so be inefficient for unsymmetric pattern sparse matrices. Another contribution is to provide detailed performance analysis and modeling for the underlying algorithm.
Reference: [2] <author> Patrick R. Amestoy. </author> <title> Factorization of large unsymmetric sparse matrices based on a multi-frontal approach in a multiprocessor environment. </title> <type> Technical Report TH/PA/91/2, </type> <institution> CERFACS, Toulouse, France, </institution> <month> February </month> <year> 1991. </year> <type> Ph.D thesis. 34 35 </type>
Reference-contexts: Several methods have been proposed to perform sparse Cholesky factorization [13, 24, 28] and sparse LU factorization <ref> [2, 5, 17, 20] </ref> on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed.
Reference: [3] <author> C. Ashcraft and R. Grimes. </author> <title> The influence of relaxed supernode partitions on the multifrontal method. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 15 </volume> <pages> 291-309, </pages> <year> 1989. </year>
Reference-contexts: In what follows, we describe how this upper bound can facilitate our storage management for the L supernodes. First, we need a notion of fundamental supernode, which was introduced by Ashcraft and Grimes <ref> [3] </ref> for symmetric matrices. In a fundamental supernode, every column except the last (numbered highest) is an only child in the elimination tree.
Reference: [4] <author> J. Bilmes, K. Asanovic, J. Demmel, D. Lam, and C.-W. Chin. </author> <title> Optimizing matrix multiply using PHiPAC: a portable, high-performance, ANSI C coding methodology. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-96-326, </type> <institution> University of Tennessee, Knoxville, </institution> <month> May </month> <year> 1996. </year> <note> (LAPACK Working Note #111). </note>
Reference-contexts: Most DGEMM and DGEMV Mflop rates were measured using vendor-supplied BLAS libraries. When the vendors do not provide a BLAS library, we report the results from PHiPAC <ref> [4] </ref>, with an asterisk ( fl ) beside such a number.
Reference: [5] <author> T. A. Davis and P. C. Yew. </author> <title> A nondeterministic parallel algorithm for general unsymmetric sparse LU factorization. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 11 </volume> <pages> 384-402, </pages> <year> 1990. </year>
Reference-contexts: Several methods have been proposed to perform sparse Cholesky factorization [13, 24, 28] and sparse LU factorization <ref> [2, 5, 17, 20] </ref> on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed.
Reference: [6] <author> James W. Demmel, Stanley C. Eisenstat, John R. Gilbert, Xiaoye S. Li, and Joseph W.H. Liu. </author> <title> A supernodal approach to sparse partial pivoting. </title> <type> Technical Report UCB//CSD-95-883, </type> <institution> Computer Science Division, U.C. Berkeley, </institution> <month> July </month> <year> 1995. </year> <note> (Xerox PARC report CSL-95-03, LAPACK Working Note #103). To appear in SIAM J. Matrix Analysis and Applications. </note>
Reference-contexts: This work was supported in part by the Director, Office of Computational and Technology Research, Division of Mathematical, Information, and Computational Sciences of the U.S. Department of Energy under contract number 76SF00098. 1 partial pivoting <ref> [6] </ref>. This left-looking, blocked algorithm includes symmetric structural reduction for fast symbolic factorization, and supernode-panel updates to achieve better data reuse in cache and floating-point registers. Here, we assume the reader has prior knowledge of SuperLU and the material upon which SuperLU is based [6, 25]. <p> This left-looking, blocked algorithm includes symmetric structural reduction for fast symbolic factorization, and supernode-panel updates to achieve better data reuse in cache and floating-point registers. Here, we assume the reader has prior knowledge of SuperLU and the material upon which SuperLU is based <ref> [6, 25] </ref>. In this paper we study an efficient parallel algorithm based on SuperLU. The primary objective of this work is to achieve good efficiency on shared memory systems with a modest number of processors (for example, between 10 and 20). <p> Both panel and column symbolic steps use depth-first search (DFS). A further refinement, a two-dimensional supernode partitioning (defined by the blocking parameters t and b in Figure 2), enhances performance for large matrices and machines with small caches. A more detailed description of SuperLU is in the paper <ref> [6] </ref>. We conducted extensive performance evaluation for SuperLU on several recent superscalar architectures. For large sparse matrices, SuperLU achieves up to 40% of the peak floating-point performance on both IBM RS/6000-590 and MIPS R8000. <p> The matrices are sorted in increasing order of f lops=nnz (F ), the ratio of the number of floating-point operations to the number of nonzeros nnz (F ). This "figure of merit" gives the maximum potential data reuse, as described by Demmel et al. <ref> [6] </ref>. <p> An extreme example is a dense matrix, the etree of which is a single chain. In this case, the parallel SuperLU "reduces to" a pipelined column-oriented dense LU algorithm. 5.2 Panel tasks As studied by Demmel et al. <ref> [6] </ref>, the introduction of supernodes and panels makes the computational kernels highly efficient. <p> Choosing a panel as scheduling unit affords the best granularity on the SMPs we targeted, and requires only modest changes to the serial code <ref> [6] </ref>. The alternative, blocking the matrix by rows and columns [23, 29], introduces too much synchronization overhead to make it worthwhile on SMPs with modest parallelism. A panel task consists of two distinct subtasks. The first corresponds to the outer factorization, which accumulates the updates from the descendant supernodes.
Reference: [7] <author> J. Dongarra, J. Du Croz, I. S. Duff, and S. Hammarling. </author> <title> A Set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: We store a supernode as a rectangular block, including the triangle of U in rows and columns r through s (see Figure 2). This allows us to address each supernode as a two-dimensional array in calls to BLAS routines <ref> [8, 7] </ref>, and so get high performance. To increase the average size of supernodes (and hence performance), we merge groups of consecutive columns (usually no more than 4 columns) at the fringe of the column elimination tree (Section 5.1) into relaxed supernodes regardless of their row structures.
Reference: [8] <author> J. Dongarra, J. Du Croz, S. Hammarling, and Richard J. Hanson. </author> <title> An Extended Set of FORTRAN Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: We store a supernode as a rectangular block, including the triangle of U in rows and columns r through s (see Figure 2). This allows us to address each supernode as a two-dimensional array in calls to BLAS routines <ref> [8, 7] </ref>, and so get high performance. To increase the average size of supernodes (and hence performance), we merge groups of consecutive columns (usually no more than 4 columns) at the fringe of the column elimination tree (Section 5.1) into relaxed supernodes regardless of their row structures.
Reference: [9] <author> I. S. Duff, R. Grimes, and J. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Trans. Mathematical Software, </journal> <volume> 15 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: Li [25] presented and analyzed the performance results in more detail. 3 Test matrices To evaluate our algorithms, we have collected matrices from various sources, with their characteristics summarized in Table 1. Some of the matrices are from the Harwell-Boeing collection <ref> [9] </ref>. Many of the larger matrices are from the ftp site maintained by Tim Davis of the University of Florida. 1 Those matrices are as follows. Memplus is a circuit simulation matrix from Steve Hamm of Motorola.
Reference: [10] <author> S. C. Eisenstat and J. W. H. Liu. </author> <title> Exploiting structural symmetry in sparse unsymmetric symbolic factorization. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 13 </volume> <pages> 202-211, </pages> <year> 1992. </year>
Reference-contexts: Our code tries the static scheme first and switches to the dynamic scheme only if the static scheme requests more space than is available. 5.4 Nonblocking pruning and depth-first search The idea of symmetric pruning <ref> [10, 11] </ref> is to use a graph G 0 with fewer edges than the graph G of L T to represent the structure of L. Traversing G 0 gives the same reachable sets as traversing G, but is less expensive.
Reference: [11] <author> S. C. Eisenstat and J. W. H. Liu. </author> <title> Exploiting structural symmetry in a sparse partial pivoting code. </title> <journal> SIAM J. Scientific and Statistical Computing, </journal> <volume> 14 </volume> <pages> 253-257, </pages> <year> 1993. </year>
Reference-contexts: Our code tries the static scheme first and switches to the dynamic scheme only if the static scheme requests more space than is available. 5.4 Nonblocking pruning and depth-first search The idea of symmetric pruning <ref> [10, 11] </ref> is to use a graph G 0 with fewer edges than the graph G of L T to represent the structure of L. Traversing G 0 gives the same reachable sets as traversing G, but is less expensive. <p> Traversing G 0 gives the same reachable sets as traversing G, but is less expensive. As shown by Eisenstat and Liu <ref> [11] </ref>, this technique significantly reduces the symbolic factorization time. In the sequential algorithm, in addition to the adjacency structure for G, there is another adjacency structure to represent the reduced graph G 0 .
Reference: [12] <author> David M. Fenwick, Denis J. Foley, William B. Gist, Stephen R. VanDoren, and Daniel Wissel. </author> <title> The AlphaServer 8000 series: High-end server platform development. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1) </volume> <pages> 43-65, </pages> <year> 1995. </year>
Reference-contexts: Thus, we expect uniprocessor performance to increase with increasing f lops=nnz (F ). 4 Shared memory multiprocessor systems used for testing We evaluated the parallel algorithm on several commercially popular machines, including the Sun SPARCcenter 2000 [31], SGI Power Challenge [30], DEC AlphaServer 8400 <ref> [12] </ref>, and Cray C90/J90 [33, 34]. Table 2 summarizes the configurations and several key parameters of the five parallel systems.
Reference: [13] <author> Alan George, Michael T. Heath, Joseph Liu, and Esmond Ng. </author> <title> Solution of sparse positive definitive systems on a shared-memory multiprocessor. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 309-325, </pages> <year> 1986. </year>
Reference-contexts: The numbers in both figures are collected on a single processor Alpha 21164. 6 The asynchronous scheduling algorithm Having described the parallel strategies, we are now in a position to describe the parallel factorization algorithm. Several methods have been proposed to perform sparse Cholesky factorization <ref> [13, 24, 28] </ref> and sparse LU factorization [2, 5, 17, 20] on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed.
Reference: [14] <author> Alan George, Joseph Liu, and Esmond Ng. </author> <title> A data structure for sparse QR and LU factorizations. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 </volume> <pages> 100-121, </pages> <year> 1988. </year>
Reference-contexts: Furthermore, the columns in independent subtrees can be computed without referring to any common memory, because the columns they depend on have completely disjoint row indices [20, Theorem 3.2]. It has been shown in a series of studies <ref> [14, 15, 19, 20] </ref> that the column etree gives the information about all potential dependencies. In general we cannot predict the nonzero structure of U precisely before the factorization, because the pivoting choices and hence the exact nonzero structure depend on numerical values. <p> The times are measured on the RS/6000-590. L I = i (L i I). 3 We shall make use of the following structure containment property in our storage scheme. Here we only quote the result without proof. Theorem 2 <ref> [14, 16] </ref> Consider the QR factorization A = QR using Householder transformations. Let H be the symbolic Householder matrix consisting of the sequence of Householder vectors used to represent the factored form of Q.
Reference: [15] <author> Alan George and Esmond Ng. </author> <title> An implementation of Gaussian elimination with partial pivoting for sparse systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6(2) </volume> <pages> 390-409, </pages> <year> 1985. </year>
Reference-contexts: Furthermore, the columns in independent subtrees can be computed without referring to any common memory, because the columns they depend on have completely disjoint row indices [20, Theorem 3.2]. It has been shown in a series of studies <ref> [14, 15, 19, 20] </ref> that the column etree gives the information about all potential dependencies. In general we cannot predict the nonzero structure of U precisely before the factorization, because the pivoting choices and hence the exact nonzero structure depend on numerical values.
Reference: [16] <author> Alan George and Esmond Ng. </author> <title> Symbolic factorization for sparse Gaussian elimination with partial pivoting. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8(6) </volume> <pages> 877-898, </pages> <year> 1987. </year>
Reference-contexts: The times are measured on the RS/6000-590. L I = i (L i I). 3 We shall make use of the following structure containment property in our storage scheme. Here we only quote the result without proof. Theorem 2 <ref> [14, 16] </ref> Consider the QR factorization A = QR using Householder transformations. Let H be the symbolic Householder matrix consisting of the sequence of Householder vectors used to represent the factored form of Q.
Reference: [17] <author> Alan George and Esmond Ng. </author> <title> Parallel sparse Gaussian elimination with partial pivoting. </title> <journal> Annals of Operation Research, </journal> <volume> 22 </volume> <pages> 219-240, </pages> <year> 1990. </year>
Reference-contexts: Several methods have been proposed to perform sparse Cholesky factorization [13, 24, 28] and sparse LU factorization <ref> [2, 5, 17, 20] </ref> on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed.
Reference: [18] <author> J. R. Gilbert, C. Moler, and R. Schreiber. </author> <title> Sparse matrices in Matlab: Design and implementation. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 13 </volume> <pages> 333-356, </pages> <year> 1992. </year> <month> 36 </month>
Reference-contexts: Vavasis3 is an unsymmetric augmented matrix for a 2-D PDE with highly varying coefficients [32]. Dense1000 is a dense 1000 fi 1000 random matrix. This paper does not address the performance of column preordering for sparsity. We simply use the existing ordering algorithms provided by Matlab <ref> [18] </ref>. For all matrices except 1 (Memplus), 15 (Venkat01) and 21 (Wang3), the columns were permuted by Matlab's minimum degree ordering of A T A, also known as "column minimum degree" ordering. <p> * one n-by-w integer array to keep track of the position of the first nonzero of each supernodal segment in U ; * one n-by-w integer array to temporarily store the row subscripts of the nonzeros filled in the panel; * one n-by-w real array used as the sparse accumulator <ref> [18] </ref>. * one scratch space of size (t + b) fi w to help BLAS calls. See Figure 1 for the definition of t, b and w. This amount of local storage should be multiplied by P , where P is the number of threads created.
Reference: [19] <author> J. R. Gilbert and E. Ng. </author> <title> Predicting structure in nonsymmetric sparse matrix factorizations. </title> <editor> In Alan George, John R. Gilbert, and Joseph W. H. Liu, editors, </editor> <title> Graph Theory and Sparse Matrix Computation. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: The column etree can be computed from A in time almost linear in the number of nonzeros of A by a variation of an algorithm of Liu [26]. Theorem 1 (Column Elimination Tree) <ref> [19] </ref> Let A be a square, nonsingular, possibly unsym-metric matrix, and let P A = LU be any factorization of A with pivoting by row interchanges. Let T be the column elimination tree of A. 1. <p> Furthermore, the columns in independent subtrees can be computed without referring to any common memory, because the columns they depend on have completely disjoint row indices [20, Theorem 3.2]. It has been shown in a series of studies <ref> [14, 15, 19, 20] </ref> that the column etree gives the information about all potential dependencies. In general we cannot predict the nonzero structure of U precisely before the factorization, because the pivoting choices and hence the exact nonzero structure depend on numerical values.
Reference: [20] <author> John R. Gilbert. </author> <title> An efficient parallel sparse partial pivoting algorithm. </title> <type> Technical Report CMI No. 88/45052-1, </type> <institution> Christian Michelsen Institute, Bergen, Norway, </institution> <year> 1988. </year>
Reference-contexts: Furthermore, the columns in independent subtrees can be computed without referring to any common memory, because the columns they depend on have completely disjoint row indices <ref> [20, Theorem 3.2] </ref>. It has been shown in a series of studies [14, 15, 19, 20] that the column etree gives the information about all potential dependencies. <p> Furthermore, the columns in independent subtrees can be computed without referring to any common memory, because the columns they depend on have completely disjoint row indices [20, Theorem 3.2]. It has been shown in a series of studies <ref> [14, 15, 19, 20] </ref> that the column etree gives the information about all potential dependencies. In general we cannot predict the nonzero structure of U precisely before the factorization, because the pivoting choices and hence the exact nonzero structure depend on numerical values. <p> Several methods have been proposed to perform sparse Cholesky factorization [13, 24, 28] and sparse LU factorization <ref> [2, 5, 17, 20] </ref> on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed. <p> This is in contrast to some implementations of sparse Cholesky, which can schedule work to processors carefully and cheaply ahead of time [23]. The dynamic nature of partial pivoting prevents us from doing this. Our scheduling approach used some techniques from the parallel column-oriented algorithm developed by Gilbert <ref> [20] </ref>. Figure 10 sketches the top level scheduling loop. Each processor executes this loop until its termination criterion is met, that is, until all panels have been factorized. The parallel algorithm maintains a central priority queue of tasks (panels), that are ready to be executed by any free processor.
Reference: [21] <author> John R. Gilbert, Xiaoye S. Li, Esmond G. Ng, and Barry W. Peyton. </author> <title> Computing row and column counts for sparse QR factorization. </title> <note> In preparation. (Talk presented at SIAM Symposium on Applied Linear Algebra, </note> <month> June </month> <year> 1994). </year>
Reference-contexts: In fact, if we use fundamental L supernodes and ignore numerical cancellation (which we must do anyway for symmetric pruning), we can show that an L supernode is always contained in an H supernode <ref> [21] </ref>. 3 This L is different from the ^ L in P A = ^ LU. Both L and ^ L contain the same nonzero values, but in different positions. <p> Finding the first vertex and computing the column count can be done using a variant of the QR-column-count algorithm by Gilbert et al. <ref> [21] </ref>. The modified QR-column-count algorithm takes Struct (A) and the postordered T as inputs, and computes nnz (H flj ) and S H .
Reference: [22] <author> John R. Gilbert, Esmond G. Ng, and Barry W. Peyton. </author> <title> An efficient algorithm to compute row and column counts for sparse Cholesky factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 15 </volume> <pages> 1075-1091, </pages> <year> 1994. </year>
Reference-contexts: In practice, it is as fast as computing the column etree T [25, Table 5.2]. In both the etree and QR-column-count algorithms, the disjoint set union operations are implemented using path halving and no union by rank (see Gilbert et al. <ref> [22] </ref> for details.) One remaining issue is what we should do if the static storage given by an upper bound structure is much too generous than actually needed. We developed a dynamic prediction scheme as a fallback for this situation.
Reference: [23] <author> A. Gupta and V. Kumar. </author> <title> Optimally scalable parallel sparse Cholesky factorization. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Proceesing for Scientific Computing, </booktitle> <pages> pages 442-447. </pages> <publisher> SIAM, </publisher> <year> 1995. </year>
Reference-contexts: Choosing a panel as scheduling unit affords the best granularity on the SMPs we targeted, and requires only modest changes to the serial code [6]. The alternative, blocking the matrix by rows and columns <ref> [23, 29] </ref>, introduces too much synchronization overhead to make it worthwhile on SMPs with modest parallelism. A panel task consists of two distinct subtasks. The first corresponds to the outer factorization, which accumulates the updates from the descendant supernodes. The second subtask is to perform the panel's inner factorization. <p> Our scheduling algorithm employs this model as well. This is in contrast to some implementations of sparse Cholesky, which can schedule work to processors carefully and cheaply ahead of time <ref> [23] </ref>. The dynamic nature of partial pivoting prevents us from doing this. Our scheduling approach used some techniques from the parallel column-oriented algorithm developed by Gilbert [20]. Figure 10 sketches the top level scheduling loop.
Reference: [24] <author> A. Gupta, E. Rothberg, E. Ng, and B. W. Peyton. </author> <title> Parallel sparse Cholesky factorization algorithms for shared-memory multiprocessor systems. </title> <editor> In R. Vichnevetsky, D. Knight, and G. Richter, editors, </editor> <booktitle> Advances in Computer Methods for Partial Differential Equations-VII. IMACS, </booktitle> <year> 1992. </year>
Reference-contexts: The numbers in both figures are collected on a single processor Alpha 21164. 6 The asynchronous scheduling algorithm Having described the parallel strategies, we are now in a position to describe the parallel factorization algorithm. Several methods have been proposed to perform sparse Cholesky factorization <ref> [13, 24, 28] </ref> and sparse LU factorization [2, 5, 17, 20] on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed.
Reference: [25] <author> Xiaoye S. Li. </author> <title> Sparse Gaussian elimination on high performance computers. </title> <type> Technical Report UCB//CSD-96-919, </type> <institution> Computer Science Division, U.C. Berkeley, </institution> <month> September </month> <year> 1996. </year> <type> Ph.D dissertation. </type>
Reference-contexts: This left-looking, blocked algorithm includes symmetric structural reduction for fast symbolic factorization, and supernode-panel updates to achieve better data reuse in cache and floating-point registers. Here, we assume the reader has prior knowledge of SuperLU and the material upon which SuperLU is based <ref> [6, 25] </ref>. In this paper we study an efficient parallel algorithm based on SuperLU. The primary objective of this work is to achieve good efficiency on shared memory systems with a modest number of processors (for example, between 10 and 20). <p> Li <ref> [25] </ref> presented and analyzed the performance results in more detail. 3 Test matrices To evaluate our algorithms, we have collected matrices from various sources, with their characteristics summarized in Table 1. Some of the matrices are from the Harwell-Boeing collection [9]. <p> This restriction on supernodes would mean that the maximum size of supernodes would be bounded by the panel size. As discussed in Section 7.5 (also Li <ref> [25] </ref>), for best per-processor efficiency and parallelism, we would like to have large supernodes but relatively small panels. These conflicting demands make it hard to find one good size for both supernodes and panels. We conducted an experiment with this scheme for the sequential algorithm. <p> The complexity of the algorithm is O (m ff (m; n)), where m = nnz (A) and ff (m; n) is the slowly-growing inverse of Ackermann's function coming from disjoint set union operations. In practice, it is as fast as computing the column etree T <ref> [25, Table 5.2] </ref>.
Reference: [26] <author> J. W. H. Liu. </author> <title> The role of elimination trees in sparse factorization. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 11 </volume> <pages> 134-172, </pages> <year> 1990. </year>
Reference-contexts: The column etree can be computed from A in time almost linear in the number of nonzeros of A by a variation of an algorithm of Liu <ref> [26] </ref>. Theorem 1 (Column Elimination Tree) [19] Let A be a square, nonsingular, possibly unsym-metric matrix, and let P A = LU be any factorization of A with pivoting by row interchanges. Let T be the column elimination tree of A. 1.
Reference: [27] <author> Joseph W.H. Liu, Esmond G. Ng, and Barry W. Peyton. </author> <title> On finding supernodes for sparse matrix computations. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14(1) </volume> <pages> 242-252, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: First, we need a notion of fundamental supernode, which was introduced by Ashcraft and Grimes [3] for symmetric matrices. In a fundamental supernode, every column except the last (numbered highest) is an only child in the elimination tree. Liu et al. <ref> [27] </ref> gave several reasons why fundamental supernodes are appropriate, one of which is that the set of fundamental supernodes is the same regardless of the particular etree postordering. For consistency, we now also impose this restriction on the supernodes in L and H.
Reference: [28] <author> Esmond G. Ng and Barry W. Peyton. </author> <title> A supernodal Cholesky factorization algorithm for shared-memory multiprocessors. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14(4) </volume> <pages> 761-769, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The numbers in both figures are collected on a single processor Alpha 21164. 6 The asynchronous scheduling algorithm Having described the parallel strategies, we are now in a position to describe the parallel factorization algorithm. Several methods have been proposed to perform sparse Cholesky factorization <ref> [13, 24, 28] </ref> and sparse LU factorization [2, 5, 17, 20] on shared memory machines. A common practice is to organize the program as a self-scheduling loop, interacting with a global pool of tasks that are ready to be executed.
Reference: [29] <author> Edward Rothberg. </author> <title> Performance of panel and block approaches to sparse Cholesky factorization on the iPSC/860 and Paragon multicomputers. </title> <journal> SIAM J. Scientific Computing, </journal> <volume> 17(3) </volume> <pages> 699-713, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Choosing a panel as scheduling unit affords the best granularity on the SMPs we targeted, and requires only modest changes to the serial code [6]. The alternative, blocking the matrix by rows and columns <ref> [23, 29] </ref>, introduces too much synchronization overhead to make it worthwhile on SMPs with modest parallelism. A panel task consists of two distinct subtasks. The first corresponds to the outer factorization, which accumulates the updates from the descendant supernodes. The second subtask is to perform the panel's inner factorization.
Reference: [30] <editor> SGI Power Challenge. </editor> <title> Silicon Graphics, 1995. </title> <type> Technical Report. </type>
Reference-contexts: Thus, we expect uniprocessor performance to increase with increasing f lops=nnz (F ). 4 Shared memory multiprocessor systems used for testing We evaluated the parallel algorithm on several commercially popular machines, including the Sun SPARCcenter 2000 [31], SGI Power Challenge <ref> [30] </ref>, DEC AlphaServer 8400 [12], and Cray C90/J90 [33, 34]. Table 2 summarizes the configurations and several key parameters of the five parallel systems.
Reference: [31] <institution> SPARCcenter 2000 architecture and implementation. Sun Microsystems, Inc., </institution> <month> November </month> <year> 1993. </year> <type> Technical White Paper. </type>
Reference-contexts: Thus, we expect uniprocessor performance to increase with increasing f lops=nnz (F ). 4 Shared memory multiprocessor systems used for testing We evaluated the parallel algorithm on several commercially popular machines, including the Sun SPARCcenter 2000 <ref> [31] </ref>, SGI Power Challenge [30], DEC AlphaServer 8400 [12], and Cray C90/J90 [33, 34]. Table 2 summarizes the configurations and several key parameters of the five parallel systems.
Reference: [32] <author> S. A. Vavasis. </author> <title> Stable finite elements for problems with wild coefficients. </title> <type> Technical Report 93-1364, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1993. </year> <note> To appear in SIAM J. Numerical Analysis. </note>
Reference-contexts: Wang3 is from solving a coupled nonlinear PDE system in a 3-D (30 fi 30 fi 30 uniform mesh) semiconductor device simulation, as provided by Song Wang of the University of New South Wales, Sydney. Vavasis3 is an unsymmetric augmented matrix for a 2-D PDE with highly varying coefficients <ref> [32] </ref>. Dense1000 is a dense 1000 fi 1000 random matrix. This paper does not address the performance of column preordering for sparsity. We simply use the existing ordering algorithms provided by Matlab [18].
Reference: [33] <institution> The Cray C90 series. http://www.cray.com/PUBLIC/product-info/C90/. Cray Research, Inc. </institution>
Reference-contexts: Thus, we expect uniprocessor performance to increase with increasing f lops=nnz (F ). 4 Shared memory multiprocessor systems used for testing We evaluated the parallel algorithm on several commercially popular machines, including the Sun SPARCcenter 2000 [31], SGI Power Challenge [30], DEC AlphaServer 8400 [12], and Cray C90/J90 <ref> [33, 34] </ref>. Table 2 summarizes the configurations and several key parameters of the five parallel systems.
Reference: [34] <institution> The Cray J90 series. http://www.cray.com/PUBLIC/product-info/J90/. Cray Research, Inc. </institution> <month> 37 </month>
Reference-contexts: Thus, we expect uniprocessor performance to increase with increasing f lops=nnz (F ). 4 Shared memory multiprocessor systems used for testing We evaluated the parallel algorithm on several commercially popular machines, including the Sun SPARCcenter 2000 [31], SGI Power Challenge [30], DEC AlphaServer 8400 [12], and Cray C90/J90 <ref> [33, 34] </ref>. Table 2 summarizes the configurations and several key parameters of the five parallel systems.
References-found: 34

