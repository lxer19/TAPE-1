URL: ftp://ftpipr.ira.uka.de/pub/papers/1995/icar95mk.ps.gz
Refering-URL: ftp://ftpipr.ira.uka.de/.public_html/papersna.html
Root-URL: 
Title: ROBOT SKILL ACQUISITION VIA HUMAN DEMONSTRATION  
Author: Kaiser, M. Retey, A. and Dillmann, R. 
Keyword: Key Words. Robots, Man-Machine Systems, Programming Support, Neural Nets  
Address: Kaiserstrae 12, D-76128 Karlsruhe, Germany  
Affiliation: Institute for Real-Time Computer Systems Robotics, University of Karlsruhe,  
Note: In: 7 th International Conference on Advanced Robotics (ICAR '95), Sant Feliu de Guixols, Spain  
Abstract: phone: +49 721 608-4051; fax: +49 721 606740; e-mail: kaiser@ira.uka.de Abstract. This paper presents an approach to the acquisition of basic robot manipulation skills, such as the ability to apply a constant force to a surface or to perform a compliant action, from a sample of the very same task obtained from a human demonstration. The sampled data are analyzed and preprocessed in order to be suitable as training data for a learning algorithm. It is shown that, for example, a neural network trained with the processed data does actually realize the skill that had to be acquired. In order to go beyond the performance that the human operator exhibited, it is demonstrated that the network can be tuned on-line according to some performance measure. As well as the initial skill, the forward model necessary for adaptation can be obtained from the demonstration of the task.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Asada and S. Liu. </author> <title> Transfer of human skills to neural net robot controllers. </title> <booktitle> In Proceedings of the 1991 IEEE International Conference on Robotics and Automation, </booktitle> <year> 1991. </year>
Reference-contexts: Did the operator apply a constant strategy, i.e., are the recorded actions consistent? Similar to Asada <ref> [1] </ref>, we answer this question by checking the continuity of the function represented by the recorded example. <p> The ITD 1 algorithm as presented in [11] employs E and E c to define a dependency index D (u; y) := (1 E (u) ) 2 with D (u; y) 2 <ref> [0; 1] </ref>.
Reference: [2] <author> H. Asada and B.-H. Yang. </author> <title> Skill acquisition from human experts through pattern processing of teaching data. </title> <booktitle> In Proceedings of the 1989 IEEE International Conference on Robotics and Automation, </booktitle> <year> 1989. </year>
Reference-contexts: Several approaches to skill acquisition can be found in robotics. E.g., Pomerleau's ALVINN [19] can be considered as skill acquisition by human demonstra tion. Also, Asada employs human-generated examples for teaching deburring skills to a robot <ref> [2] </ref> and, similar to Pomerleau, often represents the acquired skill by means of a neural network [14]. Other approaches to skill learning can, for instance, be found in [22, 6, 8].
Reference: [3] <author> C. Baroglio, A. Giordana, M. Kaiser, M. Nuttin, and R. Piola. </author> <title> Learning controllers for industrial robots. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference-contexts: Gray arrows indicate feedback loops. Afterwards, it is necessary to analyze the demonstration in order to obtain training data that are suitable to let a function approximator such as a neural network, a fuzzy system, or a regression tree (see <ref> [3] </ref> for a comparison of these approximation techniques) learn the demonstrated skill. Two questions have to be answered to achieve this goal: 1. <p> In case of networks employing local receptive fields, such as radial-basis function networks [16], an initial clustering of the training data may result in the network's structure <ref> [3] </ref>. Following the configuration step, the learning method is applied off-line until a sufficient degree of performance w.r.t. the demonstrated skill has been achieved. The desired approximation accuracy depends on the quality of the demonstration.
Reference: [4] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neu-ronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <pages> pages 835-846, </pages> <year> 1983. </year>
Reference-contexts: Therefore, the actual application of an acquired skill must in any case include an adaptation phase. De--pending on the type of information that is available on-line, the off-line initialized skill can, for instance, be refined by means of supervised learning [23, 18] or a reinforcement learning scheme <ref> [4, 25] </ref>. In both cases, additional information is required in order to setup the adaptation mechanism. from [15, 18]. A generic adaptive control scheme on the base of neural networks is shown in Fig. 2.
Reference: [5] <author> M. R. Berthold. </author> <title> A time delay radial basis function network for phoneme recognition. </title> <booktitle> In IEEE International Conference on Neural Networks, pages 4470 - 4472a, </booktitle> <address> Or-lando, Florida, </address> <year> 1994. </year>
Reference-contexts: The analysis of the sampled data therefore leads to designing a Time-Delay RBF Network <ref> [5] </ref> as force controller, featuring 1 input unit with 2 time delays, 1 output unit, and a hidden layer with 15 units obtained through initial clustering of the training data by means of an extension of the algorithm described in [17].
Reference: [6] <author> N. Delson and H. West. </author> <title> Robot programming by human demonstration: subtask compliance controller identification. </title> <booktitle> In Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems, </booktitle> <address> Yokohama, Japan, </address> <year> 1993. </year>
Reference-contexts: Also, Asada employs human-generated examples for teaching deburring skills to a robot [2] and, similar to Pomerleau, often represents the acquired skill by means of a neural network [14]. Other approaches to skill learning can, for instance, be found in <ref> [22, 6, 8] </ref>. This paper presents a unified approach to skill acquisition that explicitely considers what is usually done ad-hoc in skill acquisition the selection and preprocessing of training data and the configuration of the employed learning algorithm.
Reference: [7] <author> A. Giordana, M. Kaiser, and M. Nuttin. </author> <title> On the reduction of costs for robot controller synthesis. </title> <booktitle> In International Symposium on Intelligent Robotic Systems (IRS '94), </booktitle> <pages> pages 187 - 197, </pages> <address> Grenoble, France, </address> <year> 1994. </year>
Reference-contexts: If, for instance, a neural network is employed to learn the skill, the network type and topology have to be selected. While the type of network does not significantly influence its approximation capability <ref> [7] </ref>, it determines if and how the network structure can be generated automatically. In case of multilayer perceptrons [21], the necessary number of hidden neurons can be estimated from a qualitative approximation of the function to be learned [10].
Reference: [8] <author> V. Gullapalli, J. A. Franklin, and H. Benbrahim. </author> <title> Acquiring robot skills via reinforcement learning. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 14(1):13 - 24, </volume> <year> 1994. </year>
Reference-contexts: Also, Asada employs human-generated examples for teaching deburring skills to a robot [2] and, similar to Pomerleau, often represents the acquired skill by means of a neural network [14]. Other approaches to skill learning can, for instance, be found in <ref> [22, 6, 8] </ref>. This paper presents a unified approach to skill acquisition that explicitely considers what is usually done ad-hoc in skill acquisition the selection and preprocessing of training data and the configuration of the employed learning algorithm. <p> The Puma 260 in use is controlled at about 30 [Hz] in cumulative ALTER mode, resulting in a minimal positional offset of 0:1 [mm]: 2 It is also possible to assign the dependency indices as eligibility values <ref> [8] </ref> to the action performed at the corresponding time t. However, for the simple skill described here, this option was not considered. throughout five runs. F z in [lb]. workpiece.
Reference: [9] <author> R. Heise. </author> <title> Demonstration instead of programming: Focussing attention in robot task acquisition. Research report no. </title> <type> 89/360/22, </type> <institution> Department of Computer Science, University of Calgary, </institution> <year> 1989. </year>
Reference-contexts: Especially the use of advanced sensor systems and the existence of strong requirements with respect to the robot's flexibility ask for very skillful programmers and sophisticated programming environments. These circumstances let the interest in a new programming paradigm, namely Robot Programming by Demonstration (RPD) <ref> [9, 13] </ref> grow rapidly. RPD is an intuitive method to program a robot. The programmer shows how a particular task is performed, using an interface device that allows the measurement and recording of the human's motions and the data simultaneously perceived by the robot's sensors.
Reference: [10] <author> M. Kaiser. </author> <title> Time-delay neural networks for control. </title> <booktitle> In Proceedings of the Symposium on Robot Control '94 (SY-ROCO '94), </booktitle> <address> Capri, Italy, </address> <year> 1994. </year>
Reference-contexts: In case of multilayer perceptrons [21], the necessary number of hidden neurons can be estimated from a qualitative approximation of the function to be learned <ref> [10] </ref>. In case of networks employing local receptive fields, such as radial-basis function networks [16], an initial clustering of the training data may result in the network's structure [3].
Reference: [11] <author> M. Kaiser and R. Dillmann. </author> <title> Interpreting user actions: Finding the task structure and identifying dependencies. </title> <note> to appear, </note> <year> 1995. </year>
Reference-contexts: It is also possible that the contradicting actions of the operator indicate either an incomplete identification of the set P , or the operator has changed his/her strategy during execution. The identification procedure presented in <ref> [11] </ref> and shortly described below takes these possibilites into account and generates a distinct P for each of the identified strategies. Based on the identification of P , the data used for training by the selected learning algorithm can be generated and the learning method itself has to be configured. <p> The ITD 1 algorithm as presented in <ref> [11] </ref> employs E and E c to define a dependency index D (u; y) := (1 E (u) ) 2 with D (u; y) 2 [0; 1].
Reference: [12] <author> M. Kaiser, H. Friedrich, and R. Dillmann. </author> <title> Obtaining good performance from a bad teacher. </title> <booktitle> In International Conference on Machine Learning, Workshop on Programming by Demonstration, </booktitle> <address> Tahoe City, </address> <year> 1995. </year>
Reference-contexts: After the example has been recorded, it must be preprocessed in order to reduce noise in the data that originates, for instance, from obviously undesired actions of the operator such as breaks, or from corrective actions <ref> [12] </ref>. Also, samples taken before and after the actual skill was demonstrated have to be removed, and the quality of the demonstrated solution must be assessed in order to obtain convergence criteria for the o*ine learning phase [12]. Gray arrows indicate feedback loops. <p> obviously undesired actions of the operator such as breaks, or from corrective actions <ref> [12] </ref>. Also, samples taken before and after the actual skill was demonstrated have to be removed, and the quality of the demonstrated solution must be assessed in order to obtain convergence criteria for the o*ine learning phase [12]. Gray arrows indicate feedback loops.
Reference: [13] <author> Y. Kuniyoshi, I. Masayuki, and H. Inoue. </author> <title> Learning by watching: Reusable task knowledge from visual observation of human performance. </title> <journal> IEEE Transactions pn Robotics and Automation, </journal> <volume> 10(6) </volume> <pages> 799-822, </pages> <year> 1995. </year>
Reference-contexts: Especially the use of advanced sensor systems and the existence of strong requirements with respect to the robot's flexibility ask for very skillful programmers and sophisticated programming environments. These circumstances let the interest in a new programming paradigm, namely Robot Programming by Demonstration (RPD) <ref> [9, 13] </ref> grow rapidly. RPD is an intuitive method to program a robot. The programmer shows how a particular task is performed, using an interface device that allows the measurement and recording of the human's motions and the data simultaneously perceived by the robot's sensors.
Reference: [14] <author> S. Liu and H. Asada. </author> <title> Teaching and learning of deburring robots using neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <address> Atlanta, Georgia, </address> <year> 1993. </year>
Reference-contexts: E.g., Pomerleau's ALVINN [19] can be considered as skill acquisition by human demonstra tion. Also, Asada employs human-generated examples for teaching deburring skills to a robot [2] and, similar to Pomerleau, often represents the acquired skill by means of a neural network <ref> [14] </ref>. Other approaches to skill learning can, for instance, be found in [22, 6, 8]. This paper presents a unified approach to skill acquisition that explicitely considers what is usually done ad-hoc in skill acquisition the selection and preprocessing of training data and the configuration of the employed learning algorithm.
Reference: [15] <author> W. T. Miller, R. S. Sutton, and P. J. Werbos. </author> <title> Neural networks for control. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: De--pending on the type of information that is available on-line, the off-line initialized skill can, for instance, be refined by means of supervised learning [23, 18] or a reinforcement learning scheme [4, 25]. In both cases, additional information is required in order to setup the adaptation mechanism. from <ref> [15, 18] </ref>. A generic adaptive control scheme on the base of neural networks is shown in Fig. 2.
Reference: [16] <author> J. Moody and C. Darken. </author> <title> Learning with localized receptive fields. </title> <booktitle> In Proceedings of the Connectionist Models Summer School. </booktitle> <institution> Carnegie Mellon University, </institution> <year> 1988. </year>
Reference-contexts: In case of multilayer perceptrons [21], the necessary number of hidden neurons can be estimated from a qualitative approximation of the function to be learned [10]. In case of networks employing local receptive fields, such as radial-basis function networks <ref> [16] </ref>, an initial clustering of the training data may result in the network's structure [3]. Following the configuration step, the learning method is applied off-line until a sufficient degree of performance w.r.t. the demonstrated skill has been achieved. The desired approximation accuracy depends on the quality of the demonstration. <p> Instead, for incremental learning involved in adaptive control, networks using a local representation of the input space such as radial-basis function networks <ref> [16] </ref> are more useful.
Reference: [17] <author> M.T. Musavi, W. Ahmed, K.H. Chan, </author> <title> K.B. Faris, and D.M. Hummels. On the training of radial basis function classifiers. Neural Networks, </title> <address> 5:595 - 603, </address> <year> 1992. </year>
Reference-contexts: sampled data therefore leads to designing a Time-Delay RBF Network [5] as force controller, featuring 1 input unit with 2 time delays, 1 output unit, and a hidden layer with 15 units obtained through initial clustering of the training data by means of an extension of the algorithm described in <ref> [17] </ref>. For comparison purposes, table 2 shows error rates on the sampled data that have been obtained from networks generated using different levels of preprocessing of the recorded data. The values are averaged over five trials each, with each trial consisting of a 150 epochs lasting training process.
Reference: [18] <author> K. S. Narendra and S. Mukhopadhyay. </author> <title> Adaptive control of nonlinear multivariable systems using neural networks. Neural Networks, </title> <type> 7, </type> <year> 1994. </year>
Reference-contexts: Therefore, the actual application of an acquired skill must in any case include an adaptation phase. De--pending on the type of information that is available on-line, the off-line initialized skill can, for instance, be refined by means of supervised learning <ref> [23, 18] </ref> or a reinforcement learning scheme [4, 25]. In both cases, additional information is required in order to setup the adaptation mechanism. from [15, 18]. A generic adaptive control scheme on the base of neural networks is shown in Fig. 2. <p> De--pending on the type of information that is available on-line, the off-line initialized skill can, for instance, be refined by means of supervised learning [23, 18] or a reinforcement learning scheme [4, 25]. In both cases, additional information is required in order to setup the adaptation mechanism. from <ref> [15, 18] </ref>. A generic adaptive control scheme on the base of neural networks is shown in Fig. 2.
Reference: [19] <author> D. A. Pomerleau. </author> <title> Efficient training of artificial neural networks for autonomous navigation. </title> <journal> Neural Computation, </journal> <volume> 3:88 - 97, </volume> <year> 1991. </year>
Reference-contexts: They represent the interface between the planning and the control level in the robot's architecture and usually provide a direct coupling between the robot's sensors and its actuators. Several approaches to skill acquisition can be found in robotics. E.g., Pomerleau's ALVINN <ref> [19] </ref> can be considered as skill acquisition by human demonstra tion. Also, Asada employs human-generated examples for teaching deburring skills to a robot [2] and, similar to Pomerleau, often represents the acquired skill by means of a neural network [14].
Reference: [20] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1:81 - 106, </volume> <year> 1986. </year>
Reference-contexts: Following the approach employed, for instance, in Quinlan's ID3 <ref> [20] </ref> and its successors, the best attribute - i.e., the component y of y on which u is most dependent is the one that minimizes the conditional entropy E c (ujy) = i=1 T + 1 j=1 n i k ji : (2) Here, y is discretized into jC I j
Reference: [21] <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing : Explorations in the Microstructure of Coginition, Parts I & II. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: While the type of network does not significantly influence its approximation capability [7], it determines if and how the network structure can be generated automatically. In case of multilayer perceptrons <ref> [21] </ref>, the necessary number of hidden neurons can be estimated from a qualitative approximation of the function to be learned [10]. In case of networks employing local receptive fields, such as radial-basis function networks [16], an initial clustering of the training data may result in the network's structure [3].
Reference: [22] <author> C. Sammut, S. Hurst, D. Kedzier, and D. Michie. </author> <title> Learning to fly. </title> <booktitle> In Proceedings of the 9th International Conference on Machine Learning, </booktitle> <pages> pages 385 - 393, </pages> <year> 1992. </year>
Reference-contexts: Also, Asada employs human-generated examples for teaching deburring skills to a robot [2] and, similar to Pomerleau, often represents the acquired skill by means of a neural network [14]. Other approaches to skill learning can, for instance, be found in <ref> [22, 6, 8] </ref>. This paper presents a unified approach to skill acquisition that explicitely considers what is usually done ad-hoc in skill acquisition the selection and preprocessing of training data and the configuration of the employed learning algorithm.
Reference: [23] <author> W. H. Schiffmann and H. W. Geffers. </author> <title> Adaptive control of dynamic systems by backpropagation networks. Neural Networks, </title> <type> 6, </type> <year> 1993. </year>
Reference-contexts: Therefore, the actual application of an acquired skill must in any case include an adaptation phase. De--pending on the type of information that is available on-line, the off-line initialized skill can, for instance, be refined by means of supervised learning <ref> [23, 18] </ref> or a reinforcement learning scheme [4, 25]. In both cases, additional information is required in order to setup the adaptation mechanism. from [15, 18]. A generic adaptive control scheme on the base of neural networks is shown in Fig. 2. <p> Consequently, the adaptation algorithm must associate the error in the measured force to an action that has been undertaken 3 or 4 timesteps before the measurement 2 . To perform the actual adaptation, an adaptive model reference control scheme like the one used in <ref> [23] </ref> was employed. Fig. 6 shows the effect of adaptation. Obviously, adaptation took place with respect to two different goals. During the first trial, the robot learned to apply the real target force (10 [lb]).
Reference: [24] <author> C. E. Shannon and W. Weaver. </author> <title> The mathematical theory of communication. </title> <institution> The University of Illinois, Urbana, IL, </institution> <year> 1949. </year>
Reference-contexts: Shannon's information theory <ref> [24] </ref> provides a suitable framework for this kind of analysis.
Reference: [25] <author> R. S. Sutton, A. G. Barto, and R. J. Williams. </author> <title> Reinforcement learning is direct adaptive control. </title> <journal> IEEE Control Systems Magazine, </journal> <pages> pages 19 - 22, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Therefore, the actual application of an acquired skill must in any case include an adaptation phase. De--pending on the type of information that is available on-line, the off-line initialized skill can, for instance, be refined by means of supervised learning [23, 18] or a reinforcement learning scheme <ref> [4, 25] </ref>. In both cases, additional information is required in order to setup the adaptation mechanism. from [15, 18]. A generic adaptive control scheme on the base of neural networks is shown in Fig. 2.
References-found: 25

