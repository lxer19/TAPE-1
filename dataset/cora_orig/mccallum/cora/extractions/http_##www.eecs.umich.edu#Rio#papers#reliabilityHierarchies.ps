URL: http://www.eecs.umich.edu/Rio/papers/reliabilityHierarchies.ps
Refering-URL: http://www.eecs.umich.edu/Rio/papers.html
Root-URL: http://www.eecs.umich.edu
Abstract: Hierarchies of diverse storage levels have been analyzed extensively for their ability to achieve both good performance and low cost. This position paper argues that we should view hierarchies also as a way to achieve both good reliability and low overhead. After discussing the design of a reliability hierarchy, we suggest two metrics to use in evaluating the overall reliability of a reliability hierarchy: mean time to data loss (MTTDL) and data loss rate. These and other metrics allow researchers to evaluate the reliability/performance tradeoffs quantitatively for new storage devices and hierarchies. We use this framework to evaluate how the Rio file cache affects the mean time to data loss and data loss rate of an existing storage hierarchy. Rio improves MTTDL over a standard delayed-write file cache by an order of magnitude and can be used with a long write-back delay without increasing data loss rate. Rio fills in the reliability gap between ordinary memory and disk by providing a storage level with high performance and intermediate reliability. 
Abstract-found: 1
Intro-found: 1
Reference: [APC97] <institution> Measuring High Availability Power Protection Systems - The Power Availability Index (PA Index). </institution> <type> Technical report, </type> <note> American Power Conversion, 1997. http://www.apcc.com/english/prods/dcent/symme/. </note>
Reference-contexts: We use a separate category for these crashes because they can affect file system data on all on-line storage devices. Power: North American power may fail once every few months. An uninterruptible power supply (UPS) can increase the MTTF to 5-50 years <ref> [APC97] </ref>. Motherboard: This category includes hardware failures in the processor, memory, and motherboard systems. Field data and common experience indicate that hardware faults are much rarer than software faults [Gray91]. In particular, design faults in hardware are rare and can cause quite a stir when discovered (e.g. Intels FDIV bug).
Reference: [Carreira97] <author> Joao Carreira, Diamantino Costa, and Joao Gabriel Silva. WinFT: </author> <title> Software Implemented Fault Tolerance for Win32 Applications. </title> <journal> Byte, </journal> <month> February </month> <year> 1997. </year>
Reference-contexts: Mature operating systems can have an MTTF measured in months, while newer operating systems may crash every few days. Unfortunately, the most common operating system in use today is reputed to fail every few days <ref> [Carreira97] </ref>. File system: A small percentage of operating system crashes corrupt permanent file system data [Chen96, Ng99]. We use a separate category for these crashes because they can affect file system data on all on-line storage devices. Power: North American power may fail once every few months.
Reference: [Chen96] <author> Peter M. Chen, Wee Teck Ng, Subhachandra Chandra, Christopher M. Aycock, Gurushan-kar Rajamani, and David Lowell. </author> <title> The Rio File Cache: Surviving Operating System Crashes. </title> <booktitle> In Proceedings of the 1996 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 7483, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: We describe different aspects of designing reliability hierarchies, and we propose two simple metrics for quantifying the reliability of a given hierarchy. These and other metrics will help system designers to quantitatively evaluate tradeoffs between performance and reliability. We also show how the Rio file cache <ref> [Chen96] </ref> could fit into a reliability hierarchy, and we describe how to provide Rio for current PCs (a platform that presents several difficulties for the original approach). 2. Designing and analyzing reliability hierarchies The basic scheme of memory hierarchies to improve performance is well known. <p> Unfortunately, the most common operating system in use today is reputed to fail every few days [Carreira97]. File system: A small percentage of operating system crashes corrupt permanent file system data <ref> [Chen96, Ng99] </ref>. We use a separate category for these crashes because they can affect file system data on all on-line storage devices. Power: North American power may fail once every few months. An uninterruptible power supply (UPS) can increase the MTTF to 5-50 years [APC97]. <p> This transfer policy between disk and tape is necessitated by the slow speed and high management overhead of tape backup. 4. Rios place in the reliability hierarchy 4.1. Effect on reliability The Rio file cache enables data in memory to survive operating system crashes as well as disks do <ref> [Chen96] </ref>. As shown in Table 3, this improves the MTTF of memory by 12x (1.9 years). Rio removes the dominant component of MTTDL in the example hierarchy in Section 3.3, improving MTTDL from 1.8 months to 1.4 years (a factor of 9x).
Reference: [Gibson91] <author> Garth Alan Gibson. </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <note> De-cember 1991. also available from MIT Press, </note> <year> 1992. </year>
Reference-contexts: Most past research in storage hierarchies has focused on improving the performance of memory and storage hierarchies [Smith82]. Some have focused on reliability and performance but have concentrated on a single storage level in isolation <ref> [Gibson91] </ref>. To our knowledge, none have investigated or quantified in a systematic way the overall reliability of a hierarchy of storage levels. Our position is that we should view a hierarchy of reliability levels in much the same way that we now view hierarchies for performance.
Reference: [Gray91] <author> Jim Gray and Daniel P. </author> <type> Siewiorek. </type> <institution> High-Availability Computer Systems. IEEE Computer, 24(9):3948, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: An uninterruptible power supply (UPS) can increase the MTTF to 5-50 years [APC97]. Motherboard: This category includes hardware failures in the processor, memory, and motherboard systems. Field data and common experience indicate that hardware faults are much rarer than software faults <ref> [Gray91] </ref>. In particular, design faults in hardware are rare and can cause quite a stir when discovered (e.g. Intels FDIV bug). Most hardware faults are due to physical phenomena such as wear-out and environmental conditions (overheating). These types of faults are relatively easy to guard against by using redundant hardware.
Reference: [Lampson83] <author> Butler W. Lampson. </author> <title> Hints for Computer System Design. </title> <booktitle> In Proceedings of the 1983 Symposium on Operating System Principles, </booktitle> <pages> pages 3348, </pages> <year> 1983. </year>
Reference-contexts: Always writing through to the lowest level essentially reduces the reliability hierarchy to a single level. Most systems use a variety of levels because a single level does not suit the needs of varying applications <ref> [Lampson83] </ref>. Not all applications require the highest reliability, so they should not be forced to pay the high overhead associated with that high reliability. Instead, most applications choose to sacrifice some reliability to achieve lower overhead. One common transfer policy is based on time.
Reference: [Mullender93] <author> Sape Mullender, </author> <title> editor. Distributed Systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year> <note> Chapter 6. </note>
Reference-contexts: A common perception is that data is either safe on stable storage, or not safe at all, at least to a first-order approximation. Textbooks often draw a sharp dividing line between non-stable and stable storage <ref> [Mullender93] </ref>, as though one were absolutely unsafe and the other absolutely safe. In reality, systems are composed of several types of storage, each with different degrees of reliability.
Reference: [Ng99] <author> Wee Teck Ng and Peter M. Chen. </author> <title> The Systematic Improvement of Fault Tolerance in the Rio File Cache. </title> <booktitle> In Proceedings of the 1999 Symposium on Fault-Tolerant Computing (FTCS), </booktitle> <month> June </month> <year> 1999. </year>
Reference-contexts: Unfortunately, the most common operating system in use today is reputed to fail every few days [Carreira97]. File system: A small percentage of operating system crashes corrupt permanent file system data <ref> [Chen96, Ng99] </ref>. We use a separate category for these crashes because they can affect file system data on all on-line storage devices. Power: North American power may fail once every few months. An uninterruptible power supply (UPS) can increase the MTTF to 5-50 years [APC97]. <p> Second, the PC reset button (if it exists) often erases memory. Third, the CPU cache uses a write-back policy and is also reset at the beginning of boot. We have re-implemented Rio on PCs using the FreeBSD 2.2.7 operating system <ref> [Ng99] </ref> 1 . The key new technique is called safe sync. Whereas warm reboot writes the file cache to disk during reboot, safe sync writes the file cache to disk during the last phase of the crash. <p> Because this takes place before the system resets, this avoids the problems mentioned above. Unix kernels already try to sync data when they panic, but this fails 40% of the time on FreeBSD <ref> [Ng99] </ref>. Rios safe sync uses several techniques to improve the probability of success. First, we minimize the use of kernel state and machine resources during safe sync.
Reference: [Smith82] <author> Alan J. Smith. </author> <title> Cache Memories. </title> <journal> Computing Surveys, </journal> <volume> 14(3), </volume> <month> September </month> <year> 1982. </year>
Reference-contexts: For example, DRAM is not typically reliable enough (nor cheap enough) to store all files, and backup tape is not fast enough to serve as the sole on-line storage medium. Most past research in storage hierarchies has focused on improving the performance of memory and storage hierarchies <ref> [Smith82] </ref>. Some have focused on reliability and performance but have concentrated on a single storage level in isolation [Gibson91]. To our knowledge, none have investigated or quantified in a systematic way the overall reliability of a hierarchy of storage levels.
References-found: 9

