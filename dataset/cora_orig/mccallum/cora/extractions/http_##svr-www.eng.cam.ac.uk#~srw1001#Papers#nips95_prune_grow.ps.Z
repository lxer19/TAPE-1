URL: http://svr-www.eng.cam.ac.uk/~srw1001/Papers/nips95_prune_grow.ps.Z
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: Email: srw1001, ajr @eng.cam.ac.uk  
Phone: Tel: [+44] 1223 332754, Fax: [+44] 1223 332662,  
Title: Constructive Algorithms for Hierarchical Mixtures of Experts  
Author: S.R.Waterhouse A.J.Robinson 
Address: Trumpington St., Cambridge, CB2 1PZ, England.  
Affiliation: Cambridge University Engineering Department,  
Abstract: We present two additions to the hierarchical mixture of experts (HME) architecture. We view the HME as a tree structured classifier. Firstly, by applying a likelihood splitting criteria to each expert in the HME we "grow" the tree adaptively during training. Secondly, by considering only the most probable path through the tree we may "prune" branches away, either temporarily, or permanently if they become redundant. We demonstrate results for the growing and pruning algorithms which show significant speed ups and more efficient use of parameters over the conventional algorithms in discriminating between two interlocking spirals and classifying 8-bit parity patterns.
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J., Olshen, R. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadswoth and Brooks/Cole. </publisher>
Reference-contexts: In order to overcome this drawback, we introduce the idea of "path pruning" which considers only those paths from the root node which have probability greater than a certain threshold. The HME also has similarities with tree based statistical methods such as Classification and Regression Trees (CART) <ref> (Breiman, Friedman, Olshen & Stone 1984) </ref>. We may consider the gate as replacing the set of "questions" which are asked at each branch of CART. From this analogy, we may consider the application of the splitting rules used to build CART. <p> TREE GROWING The standard HME differs from most tree based statistical models in that its architecture is fixed. By relaxing this constraint and allowing the tree to grow, we achieve a greater degree of flexibility in the network. Following the work on Classification and Regression Trees <ref> (Breiman et al. 1984) </ref> we start with a simple tree, for instance with two experts and one gate which we train for a small number of cycles. Given this semi-trained network, we then make a set of candidate splits S i of terminal nodes z i .
Reference: <author> Bridle, J. S. </author> <year> (1989), </year> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition, </title> <editor> in F. Fougelman-Soulie & J. Herault, eds, `Neuro-computing: </editor> <booktitle> Algorithms, Architectures and Applicatations', </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 227-236. </pages>
Reference-contexts: This conditional probability is computed using the Softmax function <ref> (Bridle 1989) </ref>, P (z ij jx, h ij ) = exp (h ij , k T x) , where the sum is over all daughters of the non-terminal node z i Each terminal node z ij consists of an "expert" whose outputs are the conditional expected value of the correct output
Reference: <author> Dempster, A. P., Laird, N. M. & Rubin, D. B. </author> <year> (1977), </year> <title> `Maximum likelihood from incomplete data via the EM algorithm', </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> Series B 39, </volume> <pages> 1-38. </pages>
Reference-contexts: This confirms that given this probability model, the output of the network is the conditional expected value of the correct output given the input and the parameters of the network fi. A common method for training mixture models is the Expectation Maximisation (EM) algorithm <ref> (Dempster, Laird & Rubin 1977) </ref>. When applied to the HME (Jordan & Jacobs 1994), EM de-couples the maximum likelihood problem into a series of independent ML problems for each gate and expert.
Reference: <author> Fahlman, S. E. & Lebiere, C. </author> <year> (1990), </year> <title> The Cascade-Correlation learning architecture, </title> <type> Technical Report CMU-CS-90-100, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213. </address>
Reference-contexts: The split which yields the best increase in log-likelihood is then added permanently to the tree. This process of training followed by growing continues until the desired modelling power is reached. This approach is reminiscent of Cascade Correlation <ref> (Fahlman & Lebiere 1990) </ref> in which new hidden nodes are added to a multi-layer perceptron and trained while the rest of the network is kept fixed. HIERARCHICAL MIXTURES OF EXPERTS The HME is a supervised feedforward network which may be used for classification or regression.
Reference: <author> Jordan, M. I. & Jacobs, R. A. </author> <year> (1994), </year> <title> `Hierarchical Mixtures of Experts and the EM algorithm', </title> <booktitle> Neural Computation 6, </booktitle> <pages> 181-214. </pages>
Reference-contexts: INTRODUCTION The HME <ref> (Jordan & Jacobs 1994) </ref> is a tree structured network whose terminal nodes are simple function approximators in the case of regression or classifiers in the case of classification. <p> A common method for training mixture models is the Expectation Maximisation (EM) algorithm (Dempster, Laird & Rubin 1977). When applied to the HME <ref> (Jordan & Jacobs 1994) </ref>, EM de-couples the maximum likelihood problem into a series of independent ML problems for each gate and expert. The E step reduces to computing the set of conditional posterior probabilities of all nodes, given the input and output data.
Reference: <author> Waterhouse, S. R. & Robinson, A. J. </author> <year> (1994), </year> <title> Classification using hierarchical mixtures of experts, </title> <booktitle> in `IEEE Workshop on Neural Networks for Signal Processing', </booktitle> <pages> pp. 177-186. </pages>
Reference: <author> Waterhouse, S. R., MacKay, D. & Robinson, A. J. </author> <year> (1995), </year> <title> Bayesian inference of hierarchical mixtures of experts, </title> <type> Technical report, </type> <institution> Cambridge University Engineering Department. </institution>
Reference: <author> Wolpert, D. H. </author> <year> (1993), </year> <title> Stacked generalization, </title> <type> Technical Report LA-UR-90-3460, </type> <institution> The Santa Fe Institute, 1660 Old Pecos Trail, Suite A, </institution> <address> Santa Fe, NM, </address> <month> 87501. </month> = <title> 15,(iv) no pruning, (b) training set for two-spirals task; the two classes are indicated by crosses and circles, (c) Solution to two spirals problem. </title>
Reference-contexts: The outputs of the terminal nodes or experts are recursively combined upwards towards the root node, to form the overall output of the network, by "gates" which are situated at the non-terminal nodes. The HME has obvious similarities with model merging techniques such as stacked regression <ref> (Wolpert 1993) </ref> in which explicit partitions of the training set are combined. However the HME differs from model merging in that each expert considers the whole input space in forming its output.
References-found: 8

