URL: ftp://ftp.cs.rochester.edu/pub/u/jag/icra97.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/jag/publications.html
Root-URL: 
Email: fjag,fuentes,nelsong@cs.rochester.edu  
Title: Experimental Evaluation of Uncalibrated Visual Servoing for Precision Manipulation  
Author: Martin Jagersand, Olac Fuentes, Randal Nelson 
Web: http://www.cs.rochester.edu/u/fjag,fuentes,nelsong/  
Address: Rochester, Rochester, NY 14627  
Affiliation: Department of Computer Science, University of  
Date: 1997  
Note: In Proc. of International Conference on Robotics and Automation,  
Abstract: In this paper we present an experimental evaluation of adaptive and non-adaptive visual servoing in 3, 6 and 12 degrees of freedom (DOF), comparing it to traditional joint feedback control. While the purpose of experiments in most other work has been to show that the particular algorithm presented indeed also works in practice, we do not focus on the algorithm, but rather on properties important to visual servoing in general. Our main results are: positioning of a 6 axis PUMA 762 arm is up to 5 times more precise under visual control, than under joint control. Positioning of a Utah/MIT dextrous hand is better under visual control than under joint control by a factor of 2. We also found that a trust-region-based adaptive visual feedback controller is very robust. For m tracked visual features the algorithm can successfully estimate online the m fi 3 (m 3) image Jacobian (J) without any prior information, while carrying out a 3 DOF manipulation task. For 6 and higher DOF manipulation, a rough initial estimate of J is beneficial. We also verified that redundant visual information is valuable. Errors due to imprecise tracking and goal specification were reduced as the number of visual features, m, was increased. Furthermore highly redundant systems allow us to detect outliers in the feature vector, and deal with partial occlusion. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Weiss L. E. Sanderson A. C. Neumann C. P. </author> <title> Dynamic Sensor-Based Control of Robots with Visual Feedback J. of Robotics and Aut. </title> <publisher> v. </publisher> <month> RA-3 </month> <year> 1987 </year>
Reference-contexts: The changes in visual appearance are recorded in a perception or feature vector y = (y 1 : : : y m ) T . Visual features can be drawn from a large class of visual measurements <ref> [1, 11] </ref>, but we have found that the ones which can be represented as points or point vectors in camera space are suitable, since they yield smooth transfer functions f [14]. We track features such as boundary discontinuities (lines,corners) and surface markings. <p> We use a two level approach. The motor commands (ffi k in eq. 4 above) are used to generate a desired joint space trajectory, where the low level joint control is implemented using RCCL. We achieve smooth visual servoing movement, rather than a look and move (in Weiss terminology <ref> [1] </ref>), by queuing the next trajectory segment before the current is finished, see fig. 3. Note that J k in eq. 4 is updated by eq. 3 in each step. <p> In the middle of the movement the Jacobian is disturbed in one updating cycle by adding a random matrix: J dist = a fl J + (1 a) fl J random where a 2 <ref> [0; 1] </ref> (model accuracy) changes the magnitude of the disturbance and J random is a random m fi n matrix with elements drawn from a uniform distribution on the interval [j2 J i;j j; j2 J i;j j].
Reference: [2] <author> Feddema J. T. Lee G. C. S. </author> <title> Adaptive Image Feature Prediction and Control for Visual Tracking with a Hand-Eye Coordinated Camera IEEE Tr. </title> <journal> on Systems, Man and Cyber., </journal> <volume> v 20, no 5 1990 </volume>
Reference-contexts: k ), valid around the current system configuration x k , and described by the image [18] or visual-motor Jacobian defined as (J j;i )(x k ) = @x i The image Jacobian not only relates visual changes to motor changes, as has been previously exploited in visual feedback control <ref> [2] </ref>, but highly constrains the possible visual changes to the set of possible solutions y k+1 of y k+1 = J Dx + y k . <p> Vectors written bold, scalars plain and matrices capitalized. (Remember we have m n so the solution set is only a small subspace of &lt; m .) 2.1 Model estimation In most visual servoing work a Jacobian has been either (1) derived analytically, (2) derived partially analytically and partially estimated (eg. <ref> [2, 7] </ref>), or (3) determined experimentally by physically executing a set of orthogonal calibration movements e i (eg. [9, 22]) and approximating the Jacobian with finite differences: J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 where <p> Feddema points out that standard frame rate (30 Hz) is too low for direct joint motor feedback, and suggests that this can be overcome by also using joint feedback <ref> [2] </ref>. We use a two level approach. The motor commands (ffi k in eq. 4 above) are used to generate a desired joint space trajectory, where the low level joint control is implemented using RCCL.
Reference: [3] <author> Conkie A. Chongstitvatana P. </author> <title> An Uncalibrated Stereo Visual Servo System DAITR#475, </title> <address> Edinburgh 1990 </address>
Reference-contexts: To estimate the Jacobian, large scale movements, where the relative effects of this sticktion is small need to be added to fine manipulation tasks. While non-adaptive visual servoing methods have been shown to converge in simple settings (eg. stationary cameras, world coordinate robot control <ref> [3, 9] </ref>), we have found that in more complex cases the adaptiveness is crucial. Eye-in-hand type manipulation for instance does not work well without on-line Jacobian estimation [13].
Reference: [4] <author> Curwen R. Blake A. </author> <title> Dynamic Contours: Real time active splines Active VisionBlake, </title> <publisher> Yuille MIT Press 1992. </publisher>
Reference-contexts: The Oxford snake trackers <ref> [4] </ref> we usually use for tracking rely on an affine constraint. We could not get them to track the foam edges through non-rigid deformations. Instead a point representation of the foam outline is tracked using special purpose trackers on the markers seen in fig. 8.
Reference: [5] <author> Espiau B. Chaumette F. Rives P. </author> <title> A New Approach to Visual Servoing in Robotics IEEE Tr. </title> <booktitle> on Robotics and Automation p 313-326 v 8 no 3, </booktitle> <year> 1992. </year>
Reference: [6] <author> Wijesoma S. W. Wolfe D. F. H. Richards R. J. </author> <title> Eye-to-Hand Coordination for vision guided Robot Control Applications Int. </title> <journal> J. of Robotics Research, </journal> <volume> v 12 No 1 1993 </volume>
Reference-contexts: Experimental results in most papers have been limited to a few runs, only to validate that a particular algorithm indeed works in practice. The closest general experimental evaluations we have found are the work by Wijesoma et al. <ref> [6] </ref> and Chen et al. [17].
Reference: [7] <author> Papanikolopoulos N. P. Khosla P. K. </author> <title> Adaptive Robotic Visual Tracking: </title> <journal> Theory and Experiments IEEE Tr. on Aut. </journal> <note> Control Vol 38 no 3 1993 </note>
Reference-contexts: Vectors written bold, scalars plain and matrices capitalized. (Remember we have m n so the solution set is only a small subspace of &lt; m .) 2.1 Model estimation In most visual servoing work a Jacobian has been either (1) derived analytically, (2) derived partially analytically and partially estimated (eg. <ref> [2, 7] </ref>), or (3) determined experimentally by physically executing a set of orthogonal calibration movements e i (eg. [9, 22]) and approximating the Jacobian with finite differences: J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 where
Reference: [8] <author> Harris M. </author> <title> Vision Guided Part Alignment with Degraded Data DAI TR #615, </title> <address> Edinburgh 1993 </address>
Reference-contexts: Recently, uncalibrated visual ser-voing control has been demonstrated in a variety of settings (e.g. [1, 3, 2, 5, 6, 7, 11, 14, 13, 18, 19, 21] 1 ). Visual models suitable for specifying simple visual alignments have also been studied <ref> [22, 9, 8] </ref>. In [15] we show how to derive the robust visual servoing controllers used in this paper. In [13, 14, 11] we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments.
Reference: [9] <author> Hollinghurst N. Cipolla R. </author> <title> Uncalibrated Stereo Hand-Eye Coordination Brit. </title> <booktitle> Machine Vision Conf 1993 </booktitle>
Reference-contexts: Recently, uncalibrated visual ser-voing control has been demonstrated in a variety of settings (e.g. [1, 3, 2, 5, 6, 7, 11, 14, 13, 18, 19, 21] 1 ). Visual models suitable for specifying simple visual alignments have also been studied <ref> [22, 9, 8] </ref>. In [15] we show how to derive the robust visual servoing controllers used in this paper. In [13, 14, 11] we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. <p> only a small subspace of &lt; m .) 2.1 Model estimation In most visual servoing work a Jacobian has been either (1) derived analytically, (2) derived partially analytically and partially estimated (eg. [2, 7]), or (3) determined experimentally by physically executing a set of orthogonal calibration movements e i (eg. <ref> [9, 22] </ref>) and approximating the Jacobian with finite differences: J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 where D = diag (d); d 2 &lt; n . <p> To estimate the Jacobian, large scale movements, where the relative effects of this sticktion is small need to be added to fine manipulation tasks. While non-adaptive visual servoing methods have been shown to converge in simple settings (eg. stationary cameras, world coordinate robot control <ref> [3, 9] </ref>), we have found that in more complex cases the adaptiveness is crucial. Eye-in-hand type manipulation for instance does not work well without on-line Jacobian estimation [13].
Reference: [10] <author> Shapiro L. Zisserman A. Brady M. </author> <title> Motion from Point Matches Using Affine Epipolar Geometry Proc. </title> <month> ECCV </month> <year> 1994,p </year> <month> 73-84. </month>
Reference-contexts: In 3D computer vision the value of redundant information has long been recognized. Viewing geometry models, and full or partial camera calibration are used to relate redundant feature measurements to improve accuracy in pose estimation, e.g. see <ref> [10] </ref>. However, our vision system does not make any viewing geometry assumptions, and our cameras are uncalibrated. But as explained in the theory section, the visual-motor manipulation model gives a new way of constraining the DOF's of the visual feature vector to the smaller robot action space.
Reference: [11] <author> Jagersand M. Nelson R. </author> <title> Adaptive Differential Visual Feedback for uncalibrated hand-eye coordination and motor control TR# 579, </title> <type> U. </type> <institution> of Rochester 1994. </institution>
Reference-contexts: Visual models suitable for specifying simple visual alignments have also been studied [22, 9, 8]. In [15] we show how to derive the robust visual servoing controllers used in this paper. In <ref> [13, 14, 11] </ref> we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. Surprisingly, a thorough experimental evaluation of the accuracy-related properties of visual feedback control, to our knowledge, has never been performed. <p> We have published a detailed description in <ref> [11, 15] </ref>. An active vision agent has control over its actions, and can watch the results of an action by observing the changes 1 In Proc. of International Conference on Robotics and Automation, 1997 part runs in. Arrows indicate information exchange. in visual appearance. <p> The changes in visual appearance are recorded in a perception or feature vector y = (y 1 : : : y m ) T . Visual features can be drawn from a large class of visual measurements <ref> [1, 11] </ref>, but we have found that the ones which can be represented as points or point vectors in camera space are suitable, since they yield smooth transfer functions f [14]. We track features such as boundary discontinuities (lines,corners) and surface markings. <p> In previous work we have had best results with this asymmetric correction formula <ref> [11, 15, 27] </ref>: J k+1 = J k + (Dy measured J k Dx)Dx T Dx T Dx This is a rank 1 updating formula in the Broyden hierarchy. <p> In section 3.3 convergence of adaptive and non-adaptive controllers are experimentally compared. Repeatability of the dextrous hand is evaluated in section 3.5. For an in depth treatment of the experimental conditions and results we refer to our technical report <ref> [11] </ref>. 3.1 Repeatability We tested repeatability under closed loop visual control and compared the results to traditional joint control, both with our own experiments and with published figures. Positions were measured through a 0.001 accuracy dial meter mounted on the robot end effector.
Reference: [12] <author> Kutulakos K. Jagersand M. </author> <title> Exploring objects by purposive viewpoint control and invariant-based hand-eye coordination Workshop on vision for robots In conjunction with IROS 1995. </title>
Reference-contexts: Eye-in-hand type manipulation for instance does not work well without on-line Jacobian estimation [13]. The Utah/MIT hand control we presented, as well as control of large, complex object rotations we have shown earlier <ref> [12] </ref>, needed the trust region controller, and we could not make the 12 DOF nonrigid control in section 3.4 work without the way points, and the homotopy method. In addition the visual space way point generation is the natural way of doing trajectory planning in a hand-eye system [13].
Reference: [13] <author> Jagersand M. Nelson R. </author> <title> Visual Space Task Specification, </title> <booktitle> Planning and Control In Proc. IEEE Int. Symp. on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: Visual models suitable for specifying simple visual alignments have also been studied [22, 9, 8]. In [15] we show how to derive the robust visual servoing controllers used in this paper. In <ref> [13, 14, 11] </ref> we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. Surprisingly, a thorough experimental evaluation of the accuracy-related properties of visual feedback control, to our knowledge, has never been performed. <p> In the two following sections we review the online model acquisition. The control is described in section 2.2. Visual goal assignment, trajectory generation and what kind of visual features to use is not dealt with in this paper. We refer to <ref> [13] </ref> where we describe the high (task) level parts of the system and how to solve real world manipulation problems with this system, such as solving a shape sorting puzzle, handling flexible materials and exchanging a light bulb. <p> A high DOF (e.g. 6 DOF) partial initial estimate can be obtained from a lower (e.g. 3 DOF) Jacobian. See our visual space task programming paper <ref> [13] </ref> on how to naturally integrate this into manipulation tasks. 2.2 Control The active robot agent specifies its actions in terms of desired perceptions y fl . We need a control system capable of turning these goal perceptions into motor actions x. <p> For details on the latter see <ref> [13] </ref>, in which we describe a method for high level visual space task specification, planning, and trajectory generation. The trust region method adjusts a parameter ff so that the controller never moves out of the validity region of the current Jacobian estimate. <p> In these experiments trajectories have been selected uniformly randomly sampled in the manipulator workspace. Most other published results are from just one or a few repetitions. In <ref> [13] </ref> we have shown how visual servoing can be integrated into a system solving several real world visual manipulation tasks (6 DOF insertions, solving a child's shape sorter puz Measured (mm) Spec. (mm) Robot Visual Perf. vis. <p> While non-adaptive visual servoing methods have been shown to converge in simple settings (eg. stationary cameras, world coordinate robot control [3, 9]), we have found that in more complex cases the adaptiveness is crucial. Eye-in-hand type manipulation for instance does not work well without on-line Jacobian estimation <ref> [13] </ref>. The Utah/MIT hand control we presented, as well as control of large, complex object rotations we have shown earlier [12], needed the trust region controller, and we could not make the 12 DOF nonrigid control in section 3.4 work without the way points, and the homotopy method. <p> In addition the visual space way point generation is the natural way of doing trajectory planning in a hand-eye system <ref> [13] </ref>. We believe that the results we show are of general value in that the repeatability for a visual feedback method is insensitive to the exact controller parameters.
Reference: [14] <author> Jagersand M. </author> <title> Perception level control for uncalibrated hand-eye coordination and motor actions Thesis proposal, </title> <institution> University of Rochester, </institution> <month> May </month> <year> 1995. </year> <month> ftp://ftp.cs.rochester.edu/pub/u/jag/lic.ps.gz. </month>
Reference-contexts: Visual models suitable for specifying simple visual alignments have also been studied [22, 9, 8]. In [15] we show how to derive the robust visual servoing controllers used in this paper. In <ref> [13, 14, 11] </ref> we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. Surprisingly, a thorough experimental evaluation of the accuracy-related properties of visual feedback control, to our knowledge, has never been performed. <p> Wijesoma et al. showed for fl Support was provided by NSF grant CDA-9401142, ONR grant N00014-93-0671 and U of Maryland/ARPA subcontract Z840902. 1 For a review of this work we direct the reader to <ref> [14] </ref> or [18]. a 2 DOF implementation, the advantage of visual feedback over open loop control when model errors are large. <p> Visual features can be drawn from a large class of visual measurements [1, 11], but we have found that the ones which can be represented as points or point vectors in camera space are suitable, since they yield smooth transfer functions f <ref> [14] </ref>. We track features such as boundary discontinuities (lines,corners) and surface markings. Redundant visual perceptions (m n) are desirable as they are used to constrain the raw visual sensory information. <p> Left: Condition numbers of different DOF Jacobians. setpoints remaining the same. 4 Discussion The estimation and control algorithms we developed have strong theoretical properties (see <ref> [14, 15] </ref>). It is still very important to evaluate experimentally how they perform in real environments, with real process disturbances/noise and manipulators with real mechanically caused errors and finite precision.
Reference: [15] <author> Jagersand M. </author> <title> Visual Servoing using Trust Region Methods and Estimation of the Full Coupled Visual-Motor Jacobian In Proc. </title> <booktitle> IASTED Applications of Robotics and Control, </booktitle> <year> 1996. </year>
Reference-contexts: Recently, uncalibrated visual ser-voing control has been demonstrated in a variety of settings (e.g. [1, 3, 2, 5, 6, 7, 11, 14, 13, 18, 19, 21] 1 ). Visual models suitable for specifying simple visual alignments have also been studied [22, 9, 8]. In <ref> [15] </ref> we show how to derive the robust visual servoing controllers used in this paper. In [13, 14, 11] we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. <p> We have published a detailed description in <ref> [11, 15] </ref>. An active vision agent has control over its actions, and can watch the results of an action by observing the changes 1 In Proc. of International Conference on Robotics and Automation, 1997 part runs in. Arrows indicate information exchange. in visual appearance. <p> We want to update the Jacobian in such a way as to satisfy the most re cent observation (secant condition): Dy measured = J k+1 Dx The above condition is under-determined, and a family of Broyden updating formulas can be defined <ref> [26, 15] </ref>. In previous work we have had best results with this asymmetric correction formula [11, 15, 27]: J k+1 = J k + (Dy measured J k Dx)Dx T Dx T Dx This is a rank 1 updating formula in the Broyden hierarchy. <p> In previous work we have had best results with this asymmetric correction formula <ref> [11, 15, 27] </ref>: J k+1 = J k + (Dy measured J k Dx)Dx T Dx T Dx This is a rank 1 updating formula in the Broyden hierarchy. <p> Left: Condition numbers of different DOF Jacobians. setpoints remaining the same. 4 Discussion The estimation and control algorithms we developed have strong theoretical properties (see <ref> [14, 15] </ref>). It is still very important to evaluate experimentally how they perform in real environments, with real process disturbances/noise and manipulators with real mechanically caused errors and finite precision.
Reference: [16] <author> Jagersand M. Fuentes O. Nelson R. </author> <title> Acquiring Visual-Motor Models for Preciion Manipulation with Robot Hands In Proc. </title> <booktitle> of ECCV, </booktitle> <year> 1996. </year>
Reference-contexts: Creating an accurate manipulation model is difficult if not impossible due to the many possible objects that could be grasped, and the inherent manipulator inaccuracies. In <ref> [16] </ref> we augment the visual model estimation and control with fine manipulation capabilities for the Utah/MIT hand. This is a review of the experiments in that paper. We tested repeatability under closed loop visual control and compared the results to traditional joint control.
Reference: [17] <author> W. Z. Chen U. A. Korde S. B. </author> <title> Skaar Position Control Experiments Using Vision Int. </title> <journal> Journal of Robotics Research, </journal> <volume> v13 No 3 p199-208, </volume> <year> 1994 </year>
Reference-contexts: Experimental results in most papers have been limited to a few runs, only to validate that a particular algorithm indeed works in practice. The closest general experimental evaluations we have found are the work by Wijesoma et al. [6] and Chen et al. <ref> [17] </ref>.
Reference: [18] <author> Corke P. I. </author> <title> High-Performance Visual Closed-Loop Robot Control PhD thesis U of Melbourne 1994. </title>
Reference-contexts: Wijesoma et al. showed for fl Support was provided by NSF grant CDA-9401142, ONR grant N00014-93-0671 and U of Maryland/ARPA subcontract Z840902. 1 For a review of this work we direct the reader to [14] or <ref> [18] </ref>. a 2 DOF implementation, the advantage of visual feedback over open loop control when model errors are large. <p> At any time k we want to estimate a first order model f (x) f (x k ) + J (x k )(x x k ), valid around the current system configuration x k , and described by the image <ref> [18] </ref> or visual-motor Jacobian defined as (J j;i )(x k ) = @x i The image Jacobian not only relates visual changes to motor changes, as has been previously exploited in visual feedback control [2], but highly constrains the possible visual changes to the set of possible solutions y k+1 of
Reference: [19] <author> Hosoda K. Asada M. </author> <title> Versatile Visual Servoing without Knowledge of True Jacobian Proc. </title> <booktitle> IROS 1994. </booktitle>
Reference-contexts: This finding is in line with Hosoda and Asada's observations for their visual servoing controller In Proc. of International Conference on Robotics and Automation, 1997 in <ref> [19] </ref>. Instead of studying internal aspects of the adaptive and non-adaptive controllers, we wanted to compare how robustly the two controllers react to a suddenly introduced discrepancy between the internal model and the real world. <p> The convergence results in section 3.3 should be similar among non-adaptive methods, and also valuable for adaptive methods using other Jacobian estimation schemes (eg. <ref> [19] </ref>), or (partially) model based schemas (eg.[2]), as long as the estimate accuracy is similar.
Reference: [20] <author> Fuentes O. Nelson R. </author> <title> Experiments on Dextrous Manipulation without Prior Object Models Proc. </title> <month> ISIC </month> <year> 1996. </year>
Reference-contexts: In 6 DOF we had some problems with singularities and used a slightly modified tracking discussed below. The two joint feedback methods differ in that one only uses joint feedback, while in the Cartesian one, the error vector is projected onto the 6DOF space defined by the grasp tetrahedron <ref> [20] </ref> (The tetrahedron formed by the grasp points of the four fingers). Visual feedback gets more difficult in high DOF systems. One reason is that the number of parameters estimated in the Jacobian increases. Another is that high DOF tasks are often more ill-conditioned.
Reference: [21] <author> B. H. Yoshimi P. K. </author> <title> Allen Active, </title> <booktitle> Uncalibrated Visual Ser-voing Proc. of ICRA 1995. </booktitle>
Reference: [22] <author> Hager G. </author> <title> Calibration-Free Visual Control Using Projective Invariance In Proc. </title> <booktitle> of 5th ICCV 1995. </booktitle>
Reference-contexts: Recently, uncalibrated visual ser-voing control has been demonstrated in a variety of settings (e.g. [1, 3, 2, 5, 6, 7, 11, 14, 13, 18, 19, 21] 1 ). Visual models suitable for specifying simple visual alignments have also been studied <ref> [22, 9, 8] </ref>. In [15] we show how to derive the robust visual servoing controllers used in this paper. In [13, 14, 11] we have shown how different visual servoing behaviors can be combined to solve complex real-world tasks in unstructured environments. <p> only a small subspace of &lt; m .) 2.1 Model estimation In most visual servoing work a Jacobian has been either (1) derived analytically, (2) derived partially analytically and partially estimated (eg. [2, 7]), or (3) determined experimentally by physically executing a set of orthogonal calibration movements e i (eg. <ref> [9, 22] </ref>) and approximating the Jacobian with finite differences: J (x; d) = (f (x+d 1 e 1 )f (x); : : : ; f (x+d n e n )f (x))D 1 where D = diag (d); d 2 &lt; n .
Reference: [23] <author> Andersen J. N. </author> <title> Specifications In Int. Enc. of Robotics, </title> <editor> Ed: Dorf R. C., </editor> <publisher> Wiley 1988. </publisher>
Reference-contexts: Compared to joint control repeatability measured under the same condi 3 Repeatability is the ability of the robot to reachieve a previously attained pose. Accuracy measures the ability to achieve any prespecified pose <ref> [23] </ref>. In Proc. of International Conference on Robotics and Automation, 1997 tions, visual space repeatability is about 5 times better for Bill. Results for Hillary show that for a robot in good mechanical shape, visual control is at least as good as joint control.
Reference: [24] <author> Broyden C. G. </author> <title> Mathematics of Computation, v 19 p 577-593, </title> <year> 1965. </year>
Reference: [25] <author> Garcia, </author> <title> Zangwill Pathways to solutions, fixed points, and equilibria, </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: We use two ideas from optimization: (1) A trust region method [27] estimates the current model validity online, and controller response is restricted to be inside this region of trust. (2) A homotopy or path following method <ref> [25] </ref> is used to divide a potentially non-convex In Proc. of International Conference on Robotics and Automation, 1997 problem into several smaller convex problems by creating subgoals along trajectories planned in visual space.
Reference: [26] <author> Fletcher R. </author> <title> Practical Methods of Optimization Chichester, second ed. </title> <year> 1987 </year>
Reference-contexts: We want to update the Jacobian in such a way as to satisfy the most re cent observation (secant condition): Dy measured = J k+1 Dx The above condition is under-determined, and a family of Broyden updating formulas can be defined <ref> [26, 15] </ref>. In previous work we have had best results with this asymmetric correction formula [11, 15, 27]: J k+1 = J k + (Dy measured J k Dx)Dx T Dx T Dx This is a rank 1 updating formula in the Broyden hierarchy.
Reference: [27] <author> Dahlquist G. Bjorck A. </author> <title> Numerical Methods Second Ed, Pren-tice Hall, </title> <type> 199x, preprint. </type>
Reference-contexts: In previous work we have had best results with this asymmetric correction formula <ref> [11, 15, 27] </ref>: J k+1 = J k + (Dy measured J k Dx)Dx T Dx T Dx This is a rank 1 updating formula in the Broyden hierarchy. <p> While previous work in visual servoing have mostly used proportional control, typically with slight modifications to account for some dynamics, we have found that a more sophisticated approach is beneficial. We use two ideas from optimization: (1) A trust region method <ref> [27] </ref> estimates the current model validity online, and controller response is restricted to be inside this region of trust. (2) A homotopy or path following method [25] is used to divide a potentially non-convex In Proc. of International Conference on Robotics and Automation, 1997 problem into several smaller convex problems by
References-found: 27

