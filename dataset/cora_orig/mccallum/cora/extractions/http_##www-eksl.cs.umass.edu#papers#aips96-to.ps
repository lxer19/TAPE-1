URL: http://www-eksl.cs.umass.edu/papers/aips96-to.ps
Refering-URL: http://www-eksl.cs.umass.edu/iil/iil-papers.html
Root-URL: 
Email: oates@cs.umass.edu, cohen@cs.umass.edu  
Title: Learning Planning Operators with Conditional and Probabilistic Effects  
Author: Tim Oates and Paul R. Cohen 
Keyword: learning, knowledge acquisition, uncertainty  
Address: Box 34610 Amherst, MA 01003-4610  
Affiliation: Computer Science Department, LGRC University of Massachusetts  
Abstract: Providing a complete and accurate domain model for an agent situated in a complex environment can be an extremely difficult task. Actions may have different effects depending on the context in which they are taken, and actions may or may not induce their intended effects, with the probability of success again depending on context. In addition, the contexts and probabilities that govern the effects and success of actions may change over time. We present an algorithm for automatically learning planning operators with context-dependent and probabilistic effects in environments where exogenous events change the state of the world. Our approach assumes that a situated agent has knowledge of the types of actions that it can take, but initially knows nothing of the contexts in which an action produces change in the environment, nor what that change is likely to be. The algorithm accepts as input a history of state descriptions observed by an agent while taking actions in its domain, and produces as output descriptions of planning operators that capture structure in the agent's interactions with its environment. We present results for a sample domain showing that the computational requirements of our algorithm scale approximately linearly with the size of the agent's state vector, and that the algorithm successfully locates operators that capture true structure and avoids those that incorporate noise. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Paul R. Cohen. </author> <title> Empirical Methods for Artificial Intelligence. </title> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: of generality, due to the sort in step 2, repeatedly retaining the most general operator and removing from further consideration any other operators that it subsumes and that do 3 We can, of course, test whether a value of G is statistically significant, but this raises problems of multiple testing <ref> [1] </ref> and it is not necessary if we use G only to rank operators. 9 Algorithm 4.1 filter filter (D; H; n; g) 1. remove from D all dependencies d such that n (d) &lt; n or e (d) contains only wildcards 2. sort D in non-increasing order of generality 3.
Reference: [2] <author> Richard E. Fikes and Nils J. Nilsson. </author> <title> STRIPS: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2(2) </volume> <pages> 189-208, </pages> <year> 1971. </year>
Reference-contexts: = fACTION, BP, GC, GD, HBg A = fDRY, NEW, PAINT, PICKUPg T ACT ION = fDRY, NEW, PAINT, PICKUP, NONEg T BP = fBP, NOT-BPg; T GC = fGC, NOT-GCg 2.2 Planning Operators The STRIPS operator representation includes a set of preconditions, an add list, and a delete list <ref> [2] </ref>. The STRIPS planner assumed that actions taken in a world state matching an operator's preconditions would result in the state changes indicated by the operator's add and delete lists without fail.
Reference: [3] <author> Yolanda Gil. </author> <title> Learning by experimentation: Incremental refinement of incomplete planning domains. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 87-95, </pages> <year> 1994. </year>
Reference-contexts: Symbolic approaches to learning planning knowledge via interaction with the environment have typically assumed a deterministic world in which actions always have their intended effects, and the state of the world never changes in the absence of an action <ref> [3] </ref> [12]. The work described 12 in this paper applies in domains that contain uncertainties associated with the outcomes of actions, and noise from exogenous events. Subsymbolic approaches to learning environmental dynamics, such as reinforcement learning [6], are capable of handling a variety of forms of noise.
Reference: [4] <author> Donald E. Knuth. </author> <booktitle> The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: but this structure includes subtle effects that we may or may not wish to enshrine in a planning operator. 6 Related Work msdd's approach to expanding the search tree is similar to that of Rymon's Set Enumeration trees (SE-trees) [11], which in turn are related to Knuth's trie data structure <ref> [4] </ref>. Rymon uses SE-trees to systematically enumerate the elements of the power set of a set given a total ordering on the set's elements. He demonstrates the flexibility of that approach by implementing efficient SE-tree based versions of several algorithms (such as Reiter's hitting-set algorithm).
Reference: [5] <author> Nicholas Kushmerick, Steve Hanks, and Daniel Weld. </author> <title> An algorithm for probabilistic least-commitment planning. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1074-1078, </pages> <year> 1994. </year>
Reference-contexts: These assumptions, which are unrealistic for many real-world domains, are being relaxed by current research in AI planning systems <ref> [5] </ref> [7]. However, as planning domains become more complex, so does the task of generating domain models. In this paper, we present an algorithm for automatically learning planning operators with context-dependent and probabilistic effects in environments where exogenous events change the state of the world. <p> We assume that the state of the world can change due to an agent action, an exogenous event, or both simultaneously. The latter case complicates the learning problem. Consider a simple robot whose task it is to pick up and paint blocks. (This domain is adapted from <ref> [5] </ref>, where it is used to explicate the Buridan probabilistic planner.) The robot has four sensors and can determine whether it is holding a block (HB), has a dry gripper (GD), has a clean gripper (GC), and whether the block is painted (BP).
Reference: [6] <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55(2-3):189-208, </volume> <year> 1992. </year>
Reference-contexts: The work described 12 in this paper applies in domains that contain uncertainties associated with the outcomes of actions, and noise from exogenous events. Subsymbolic approaches to learning environmental dynamics, such as reinforcement learning <ref> [6] </ref>, are capable of handling a variety of forms of noise. Reinforcement learning requires a reward function that allows the agent to learn a mapping from states to actions that maximizes reward.
Reference: [7] <author> Todd Michael Mansell. </author> <title> A method for planning given uncertain and incomplete information. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 350-358, </pages> <year> 1993. </year>
Reference-contexts: These assumptions, which are unrealistic for many real-world domains, are being relaxed by current research in AI planning systems [5] <ref> [7] </ref>. However, as planning domains become more complex, so does the task of generating domain models. In this paper, we present an algorithm for automatically learning planning operators with context-dependent and probabilistic effects in environments where exogenous events change the state of the world.
Reference: [8] <author> Tim Oates. </author> <title> MSDD as a tool for classification. </title> <type> EKSL Memorandum 94-29. </type> <institution> Department of Computer Science, University of Massachusetts at Amherst, </institution> <year> 1994. </year>
Reference-contexts: Our contribution with msdd is a novel approach to the problem of finding structure in multiple streams of data, and showing how that approach is both general and efficient. We have successfully applied msdd to classification problems <ref> [8] </ref> and to learning rules in a shipping network that relate current states to future pathologies [9]. That work involved data sets with more than 50 streams, indicating that msdd scales well with problem size.
Reference: [9] <author> Tim Oates, Matthew D. Schmill, Dawn E. Gregory, and Paul R. Cohen. </author> <title> Detecting complex dependencies in categorical data. </title> <editor> In Doug Fisher and Hans Lenz, editors, </editor> <title> Finding Structure in Data: </title> <booktitle> Artificial Intelligence and Statistics V. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: The search is performed by an algorithm called Multi-Stream Dependency Detection (msdd, see section 3) which finds statistical dependencies among categorical values in multiple data streams over time <ref> [9] </ref>. msdd is a general search algorithm; it relies on domain knowledge to guide its search and decide when to prune in an otherwise exponential space of planning operators (see section 4). <p> * *), 0.2&gt; &lt;new, (BP * * *), (NOT-BP * * *), 1.0&gt; &lt;new, (* * * HB), (* * * NOT-HB), 1.0&gt; &lt;new, (* * NOT-GD *), (* * GD *), 0.3&gt; 3 The MSDD Algorithm The msdd algorithm finds dependencies|unexpected co-occurrences of values|in multiple streams of categorical data <ref> [9] </ref>. msdd is general in that it performs a simple best-first search over the space of possible dependencies. <p> We have successfully applied msdd to classification problems [8] and to learning rules in a shipping network that relate current states to future pathologies <ref> [9] </ref>. That work involved data sets with more than 50 streams, indicating that msdd scales well with problem size.
Reference: [10] <author> Ashwin Ram and David B. Leake. </author> <title> Goal-Driven Learning. </title> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: With this information, we define a space of possible planning operators. 1 Clearly, random exploration is inefficient, especially when actions need to be combined into long sequences to achieve certain states. The utility of goal-directed exploration and learning is well documented <ref> [10] </ref>.
Reference: [11] <author> Ron Rymon. </author> <title> Search through systematic set enumeration. </title> <booktitle> In Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <year> 1992. </year>
Reference-contexts: 30 to 20 we have found more structure in the environment, but this structure includes subtle effects that we may or may not wish to enshrine in a planning operator. 6 Related Work msdd's approach to expanding the search tree is similar to that of Rymon's Set Enumeration trees (SE-trees) <ref> [11] </ref>, which in turn are related to Knuth's trie data structure [4]. Rymon uses SE-trees to systematically enumerate the elements of the power set of a set given a total ordering on the set's elements.
Reference: [12] <author> Wei-Men Shen. </author> <title> Discovery as autonomous learning from the environment. </title> <booktitle> Machine Learning, </booktitle> <address> 12(1-3):143-165, </address> <year> 1993. </year>
Reference-contexts: Symbolic approaches to learning planning knowledge via interaction with the environment have typically assumed a deterministic world in which actions always have their intended effects, and the state of the world never changes in the absence of an action [3] <ref> [12] </ref>. The work described 12 in this paper applies in domains that contain uncertainties associated with the outcomes of actions, and noise from exogenous events. Subsymbolic approaches to learning environmental dynamics, such as reinforcement learning [6], are capable of handling a variety of forms of noise.
Reference: [13] <author> David E. Wilkins. </author> <title> Practical Planning: Extending the Classical AI Planning Paradigm. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year> <month> 14 </month>
Reference-contexts: 1 Introduction Research in classical planning has assumed that the effects of actions are deterministic and the state of the world is never altered by exogenous events, simplifying the task of encoding domain knowledge in the form of planning operators <ref> [13] </ref>. These assumptions, which are unrealistic for many real-world domains, are being relaxed by current research in AI planning systems [5] [7]. However, as planning domains become more complex, so does the task of generating domain models.
References-found: 13

