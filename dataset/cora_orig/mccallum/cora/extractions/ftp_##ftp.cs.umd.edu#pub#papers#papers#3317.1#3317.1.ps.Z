URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3317.1/3317.1.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: wak@cs.umd.edu pugh@cs.umd.edu ejr@cs.umd.edu  
Title: Code Generation for Multiple Mappings  
Author: Wayne Kelly William Pugh Evan Rosser 
Note: This work is supported by an NSF PYI grant CCR-9157384 and by a Packard Fellowship.  
Address: College Park, MD 20742  
Affiliation: Dept. of Computer Science Institute for Advanced Computer Studies Dept. of Computer Science Dept. of Computer Science Univ. of Maryland,  
Date: November, 1994  
Pubnum: UMIACS-TR-94-87.1  CS-TR-3317.1  
Abstract: There has been a great amount of recent work toward unifying iteration reordering transformations. Many of these approaches represent transformations as affine mappings from the original iteration space to a new iteration space. These approaches show a great deal of promise, but they all rely on the ability to generate code that iterates over the points in these new iteration spaces in the appropriate order. This problem has been fairly well-studied in the case where all statements use the same mapping. We have developed an algorithm for the less well-studied case where each statement uses a potentially different mapping. Unlike many other approaches, our algorithm can also generate code from mappings corresponding to loop blocking. We address the important trade-off between reducing control overhead and duplicating code. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Corinne Ancourt and Franccois Irigoin. </author> <title> Scanning polyhe-dra with DO loops. </title> <booktitle> In Proc. of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 39-50, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: i m ] ! [f 1 ; : : : ; f n ] where: * i 1 ; : : : ; i m are the index variables of the loops nested around statement s p . * The f j 's (called mapping components) are quasi- affine functions <ref> [1] </ref> of the iteration variables and symbolic constants. <p> The problem of generating perfectly nested loops to iterate over all and only those points in such a convex region has been studied by a number of researchers starting with the seminal work of Ancourt and Irigoin <ref> [1] </ref>. If the original iteration space is non-convex (as a consequence of non-unit loop steps), or if the mapping applied is not onto, then the transformed iteration space may be non-convex. <p> & t2&lt;=63+t1 & n&lt;=62+t2 then for t4 = max (t2,t1) to min (t1+63,t3-1) s2 [t4,t5,t3] if t3&lt;=62+t2 & t3&lt;=63+t1 & t3&lt;n then for t5 = max (t2, t3+1) to min (t2+63,n) 8 Related work The problem of generating code for a convex region was first addressed by Ancourt and Irigoin <ref> [1] </ref>. They use Fourier pairwise elimination at each level to pro <br>- vide bounds on each of the index variables. They then form the union of all of these projections to produce a single set of constraints which explicitly contains all of the information necessary to generate code.
Reference: [2] <author> Eduard Ayguade and Jordi Torres. </author> <title> Partitioning the state-ment per iteration space using non-singular matrices. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 407415, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Li and Pingali [15] consider the non-convex case resulting from mappings that are not necessarily onto. They use a linear algebra framework and compute loop bounds and steps using Hermite normal form. They do not consider the multiple mapping case. Ayguade and Torres <ref> [2] </ref> consider a limited case of the multiple mapping case where each statement can have a potentially different mapping but all mappings must have the same linear part (i.e., they only differ in their constant parts). Chamski [4, 5] generates Nested Loop Structures, which are similar to our AST.
Reference: [3] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proc. of the 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 192-219, </pages> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of a sequence of these transformations without actually performing the transformations and analyzing the resulting code. This complexity has inspired a great deal of recent work toward unified systems for iteration reordering transformations <ref> [3, 20, 15, 10, 8] </ref>. These approaches use a variety of formalisms, but most can be considered as special cases of a formalism we have developed [12]. In our formalism, transformations are represented as one- to-one mappings from the original iteration space to a new iteration space.
Reference: [4] <author> Zbigniew Chamski. </author> <title> Fast and efficient generation of loop bounds. </title> <note> Publication interne 771, </note> <institution> Institut de Recherche en Informatique et Systemes Aleatoires, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: They do not consider the multiple mapping case. Ayguade and Torres [2] consider a limited case of the multiple mapping case where each statement can have a potentially different mapping but all mappings must have the same linear part (i.e., they only differ in their constant parts). Chamski <ref> [4, 5] </ref> generates Nested Loop Structures, which are similar to our AST. He discusses generating code only for the single mapping convex case. He reduces control overhead by generating sequences of loops to remove all min and max expressions in loop bounds. <p> We are able to eliminate control overhead from sources other than min's and max's and we selectively decide which overheads to eliminate by considering both the amount of control overhead and the amount of code duplication that would occur. Chamski claims <ref> [4] </ref> that Fourier variable elimination is prohibitively expensive for code generation. We have found it to be a very efficient method, and suspect he used unrealistic examples and/or a poor implementation of Fourier variable elimination.
Reference: [5] <author> Zbigniew Chamski. </author> <title> Nested loop sequences: Towards effi-cient loop structures in automatic parallelization. </title> <note> Publication interne 772, </note> <institution> Institut de Recherche en Informatique et Systemes Aleatoires, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: They do not consider the multiple mapping case. Ayguade and Torres [2] consider a limited case of the multiple mapping case where each statement can have a potentially different mapping but all mappings must have the same linear part (i.e., they only differ in their constant parts). Chamski <ref> [4, 5] </ref> generates Nested Loop Structures, which are similar to our AST. He discusses generating code only for the single mapping convex case. He reduces control overhead by generating sequences of loops to remove all min and max expressions in loop bounds.
Reference: [6] <author> J.-F. Collard and P. Feautrier. </author> <title> Automatic generation of data parallel code. </title> <editor> In H.J. Sips, editor, </editor> <booktitle> Proceedings of the Fourth International Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 321-332, </pages> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Collard, Feautrier and Risset [7] show how PIP, a parametrized version of the Dual Simplex Method, can be used to solve the simple case. Collard and Feautrier <ref> [6] </ref> address the multiple mapping case; however, only one dimensional iteration spaces are considered and many guards are generated. They provide some interesting solutions to the situation where statements have incompatible stride constraints (e.g., t 1 is even and t 1 is odd).
Reference: [7] <author> Jean-Franccois Collard, Paul Feautrier, and Tanguy Risset. </author> <title> Construction of DO loops from systems of affine constraints. </title> <type> Technical Report N o 93-15, </type> <institution> Laboratoire de l'Informatique du Parallelisme, Ecolo Normal Superieure de Lyon, Instiut IMAG, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: However, the constraints we have seen in both dependence analysis and code generation are quite sparse, and Fourier elimination is quite efficient for sparse constraints [19]. Collard, Feautrier and Risset <ref> [7] </ref> show how PIP, a parametrized version of the Dual Simplex Method, can be used to solve the simple case. Collard and Feautrier [6] address the multiple mapping case; however, only one dimensional iteration spaces are considered and many guards are generated.
Reference: [8] <author> Alain Darte and Yves Robert. </author> <title> Scheduling uniform loop nests. </title> <booktitle> In Proceedings of ISMN International Conference on Parallel and Distributed Computer Systems, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of a sequence of these transformations without actually performing the transformations and analyzing the resulting code. This complexity has inspired a great deal of recent work toward unified systems for iteration reordering transformations <ref> [3, 20, 15, 10, 8] </ref>. These approaches use a variety of formalisms, but most can be considered as special cases of a formalism we have developed [12]. In our formalism, transformations are represented as one- to-one mappings from the original iteration space to a new iteration space. <p> The schedules produced by Feautrier [10] are not, by themselves, one-to-one, but when they are combined with the space mappings, they become one-to-one. Feautrier allows a potentially different schedule to be used for each atomic statement. Schedules are also used by a number of other researchers <ref> [14, 8] </ref>.
Reference: [9] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part I, One-dimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(5), </volume> <month> Oct </month> <year> 1992. </year>
Reference-contexts: They provide some interesting solutions to the situation where statements have incompatible stride constraints (e.g., t 1 is even and t 1 is odd). Stride constraints such as this arise frequently in Feautrier's parallelization framework <ref> [9, 10] </ref>, while we [13, 11] try to avoid them in our framework, since generating good code for them is difficult. 9 Conclusion We have presented an algorithm to generate transformed code from the original code and a set of one-to- one statement mappings representing the transformation.
Reference: [10] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part II, Multidimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(6), </volume> <month> Dec </month> <year> 1992. </year>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of a sequence of these transformations without actually performing the transformations and analyzing the resulting code. This complexity has inspired a great deal of recent work toward unified systems for iteration reordering transformations <ref> [3, 20, 15, 10, 8] </ref>. These approaches use a variety of formalisms, but most can be considered as special cases of a formalism we have developed [12]. In our formalism, transformations are represented as one- to-one mappings from the original iteration space to a new iteration space. <p> The extended unimodular transformations developed by Li and Pingali [15] removes the onto restriction. The schedules produced by Feautrier <ref> [10] </ref> are not, by themselves, one-to-one, but when they are combined with the space mappings, they become one-to-one. Feautrier allows a potentially different schedule to be used for each atomic statement. Schedules are also used by a number of other researchers [14, 8]. <p> They provide some interesting solutions to the situation where statements have incompatible stride constraints (e.g., t 1 is even and t 1 is odd). Stride constraints such as this arise frequently in Feautrier's parallelization framework <ref> [9, 10] </ref>, while we [13, 11] try to avoid them in our framework, since generating good code for them is difficult. 9 Conclusion We have presented an algorithm to generate transformed code from the original code and a set of one-to- one statement mappings representing the transformation.
Reference: [11] <author> Wayne Kelly and William Pugh. </author> <title> Determining schedules based on performance estimation. </title> <type> Technical Report CSTR-3108, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> July </month> <year> 1993. </year> <note> to appear in Parallel Processing Letters (1994). </note>
Reference-contexts: Finding legal mappings that produce efficient code is an important and difficult problem, but is not discussed in this paper. We refer interested readers to our earlier work in that area <ref> [12, 11, 13] </ref>. This paper deals with the problem of generating transformed code given an original program and a mapping. This involves creating loops and conditionals that iterate over all and only those points in the new iteration space. <p> They provide some interesting solutions to the situation where statements have incompatible stride constraints (e.g., t 1 is even and t 1 is odd). Stride constraints such as this arise frequently in Feautrier's parallelization framework [9, 10], while we <ref> [13, 11] </ref> try to avoid them in our framework, since generating good code for them is difficult. 9 Conclusion We have presented an algorithm to generate transformed code from the original code and a set of one-to- one statement mappings representing the transformation.
Reference: [12] <author> Wayne Kelly and William Pugh. </author> <title> A framework for unify-ing reordering transformations. </title> <type> Technical Report CS-TR3193, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: This complexity has inspired a great deal of recent work toward unified systems for iteration reordering transformations [3, 20, 15, 10, 8]. These approaches use a variety of formalisms, but most can be considered as special cases of a formalism we have developed <ref> [12] </ref>. In our formalism, transformations are represented as one- to-one mappings from the original iteration space to a new iteration space. We allow a potentially different mapping to be used for each atomic statement. We restrict the mappings to be those that can be represented using affine constraints. <p> Finding legal mappings that produce efficient code is an important and difficult problem, but is not discussed in this paper. We refer interested readers to our earlier work in that area <ref> [12, 11, 13] </ref>. This paper deals with the problem of generating transformed code given an original program and a mapping. This involves creating loops and conditionals that iterate over all and only those points in the new iteration space.
Reference: [13] <author> Wayne Kelly and William Pugh. </author> <title> Finding legal reordering transformations using mappings. </title> <type> Technical Report CS-TR3297, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> June </month> <year> 1994. </year> <note> also appeared at the 7th annual LCPC workshop. </note>
Reference-contexts: Finding legal mappings that produce efficient code is an important and difficult problem, but is not discussed in this paper. We refer interested readers to our earlier work in that area <ref> [12, 11, 13] </ref>. This paper deals with the problem of generating transformed code given an original program and a mapping. This involves creating loops and conditionals that iterate over all and only those points in the new iteration space. <p> They provide some interesting solutions to the situation where statements have incompatible stride constraints (e.g., t 1 is even and t 1 is odd). Stride constraints such as this arise frequently in Feautrier's parallelization framework [9, 10], while we <ref> [13, 11] </ref> try to avoid them in our framework, since generating good code for them is difficult. 9 Conclusion We have presented an algorithm to generate transformed code from the original code and a set of one-to- one statement mappings representing the transformation.
Reference: [14] <author> Herve Leverge, Christophe Mauras, and Patrice Quinton. </author> <title> A language-orientied approach to the design of systolic chips. </title> <journal> Journal of VLSI Signal Processing, </journal> <pages> pages 173-182, </pages> <month> Mar </month> <year> 1991. </year>
Reference-contexts: The schedules produced by Feautrier [10] are not, by themselves, one-to-one, but when they are combined with the space mappings, they become one-to-one. Feautrier allows a potentially different schedule to be used for each atomic statement. Schedules are also used by a number of other researchers <ref> [14, 8] </ref>.
Reference: [15] <author> Wei Li and Keshav Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 249-260, </pages> <institution> Yale University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of a sequence of these transformations without actually performing the transformations and analyzing the resulting code. This complexity has inspired a great deal of recent work toward unified systems for iteration reordering transformations <ref> [3, 20, 15, 10, 8] </ref>. These approaches use a variety of formalisms, but most can be considered as special cases of a formalism we have developed [12]. In our formalism, transformations are represented as one- to-one mappings from the original iteration space to a new iteration space. <p> Unimodular transformations can be viewed as the special case where there is a single atomic statement (the body of a set of perfectly nested loops) and the mapping is restricted to be linear and onto. The extended unimodular transformations developed by Li and Pingali <ref> [15] </ref> removes the onto restriction. The schedules produced by Feautrier [10] are not, by themselves, one-to-one, but when they are combined with the space mappings, they become one-to-one. Feautrier allows a potentially different schedule to be used for each atomic statement. <p> In these cases it is still possible to generate suitable perfectly nested loops; however, some of the loop steps will be non-unit. Tech- niques for handling this case are described by Li and Pingali <ref> [15] </ref>. Our work addresses the case where a potentially different mapping is used for each statement. The corresponding transformed iteration space can be "very" non-convex; that is, there is no set of perfectly nested loops without conditionals, even with non-unit steps, that can scan the space. <p> They propose that fast inexact techniques be used to remove redundancies from this set before it is used to generate code. They consider only the single mapping convex case. Li and Pingali <ref> [15] </ref> consider the non-convex case resulting from mappings that are not necessarily onto. They use a linear algebra framework and compute loop bounds and steps using Hermite normal form. They do not consider the multiple mapping case.
Reference: [16] <author> William Pugh. </author> <title> The Omega test: a fast and practical inte-ger programming algorithm for dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 8 </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Tuple relations and sets are represented using the Omega Li- brary <ref> [16, 18] </ref>, which is a set of routines for manipulating affine constraints over integer variables. The relations and sets may involve symbolic constants such as n in the following example: f [i] ! [i + 1] j 1 i n g.
Reference: [17] <author> William Pugh. </author> <title> Counting solutions to presburger formulas: How and why. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Roughly speaking, the cost of an overhead nested inside of d loops will be O (n d ), where n is the number of iterations of a loop. We could use more exact methods <ref> [17] </ref> to evaluate the cost of an overhead.
Reference: [18] <author> William Pugh and David Wonnacott. </author> <title> Going beyond in-teger programming with the Omega test to eliminate false data dependences. </title> <type> Technical Report CS-TR-3191, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Tuple relations and sets are represented using the Omega Li- brary <ref> [16, 18] </ref>, which is a set of routines for manipulating affine constraints over integer variables. The relations and sets may involve symbolic constants such as n in the following example: f [i] ! [i + 1] j 1 i n g.
Reference: [19] <author> William Pugh and David Wonnacott. </author> <title> Experiences with constraint-based array dependence analysis. </title> <booktitle> In Principles and Practice of Constraint Programming Workshop, </booktitle> <address> Orcas Island, Washington, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: However, the constraints we have seen in both dependence analysis and code generation are quite sparse, and Fourier elimination is quite efficient for sparse constraints <ref> [19] </ref>. Collard, Feautrier and Risset [7] show how PIP, a parametrized version of the Dual Simplex Method, can be used to solve the simple case. Collard and Feautrier [6] address the multiple mapping case; however, only one dimensional iteration spaces are considered and many guards are generated.
Reference: [20] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality op-timizing algorithm. </title> <booktitle> In ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: These checks and rules make it hard to analyze or predict the effects of a sequence of these transformations without actually performing the transformations and analyzing the resulting code. This complexity has inspired a great deal of recent work toward unified systems for iteration reordering transformations <ref> [3, 20, 15, 10, 8] </ref>. These approaches use a variety of formalisms, but most can be considered as special cases of a formalism we have developed [12]. In our formalism, transformations are represented as one- to-one mappings from the original iteration space to a new iteration space.
Reference: [21] <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Traditionally, reordering transformations have been used by applying a sequence of pre-specified transformations such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [21] </ref>. Each of these transformations has its own legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of a sequence of these transformations without actually performing the transformations and analyzing the resulting code.
References-found: 21

