URL: http://www.ics.uci.edu/~kibler/gala2.ps
Refering-URL: http://www.ics.uci.edu/~kibler/
Root-URL: 
Email: fyhu, kiblerg@ics.uci.edu  
Title: GalaII: Integrating the Construction of Boolean and Prototypical Features  
Author: Yuh-Jyh Hu and Dennis Kibler 
Address: Irvine  
Affiliation: Information and Computer Science Department University of California,  
Abstract: We examine a new approach to constructive induction that combines Boolean feature construction with prototypical feature construction. This particular representation language appears to be useful in many naturally occurring domains. Despite its representational power, the results are reasonably comprehensible. Also, the learning process is efficient. We present evidence for the effectiveness of this approach on real and artificial domains. Through a series of systematic experiments, we partially characterize the generality of the approach. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <title> "Learning Symbolic Rules Using Artificial Neural Networks", </title> <booktitle> in Proceeding of the 10th International Conference on Machine learning, </booktitle> <address> p73-80, </address> <year> 1993. </year>
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <title> "Extracting Tree-Structured Representations of Trained Networks", </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <volume> Vol 8, </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference: <author> Dietterich, T. G. & Michalski, R. S. </author> <title> "Inductive Learning of Structural Description : Evaluation Criteria and Comparative Review of Selected Methods", </title> <booktitle> Artificial Intelligence 16 (3), </booktitle> <address> p257-294, </address> <year> 1981. </year>
Reference-contexts: The need for useful new features has been suggested by many researchers (Matheus, 1991; Ragavan et. al., 1993). Constructing new features by hand is often difficult (Quin-lan 1993). The goal of constructive induction is to automatically generate 1 new features so that the regularity is more apparent <ref> (Dietterich & Michal--ski, 1981) </ref>, thus yielding improved classification accuracy. The constructed features should make concept learning easier for many learning algorithms.
Reference: <author> Fu, L. </author> <title> "Rule Learning by Searching on Adapted Nets", </title> <booktitle> in Proceeding of the 9th National Conference on Artificial Intelligence, </booktitle> <address> p590-595. </address>
Reference: <author> Hu, Y. & Kibler, D. </author> <title> "Generation of Attributes for Learning Algorithms", </title> <booktitle> 14 in Proceeding of the 13th National Conference on Artificial Intelligence, </booktitle> <address> p806--811, </address> <year> 1996. </year>
Reference-contexts: There are currently many constructive induction algorithms based on the strategy of constructing new attributes, including FRINGE (Pagallo, 1989), GREEDY3 (Pagallo & Haussler, 1990), DCFringe (Yang et. al., 1991), CITRE (Matheus & Rendell, 1989), LFC (Ragavan & Rendell, 1993; Ragavan et. al., 1993), MRP (Perez & Rendell 1995), GALA <ref> (Hu & Kibler 1996) </ref>, etc. Unfortunately, most of the current constructive induction algorithms degrade in performance as the target concept becomes larger and more complex, when measured by the number of Boolean combinations (Perez & Rendell 1996). <p> We demonstrate that GalaII generates features that are useful for several different learning algorithms on several domains. We also obtain a partial characterization of GalaII, via empirical studies on artificial domains. 2 Motivation One important issue in constructive induction is the types of new attributes constructed <ref> (Hu & Kibler 1996) </ref>. If we represent the new attributes in disjunctive normal form, we could build a spectrum of the attributes based on the number of terms involved. At one end are the features with relatively few terms; at the other end, those with many terms. <p> We aim at covering those portions of the spectrum that appear to occur naturally. GALA is an approach to constructive induction which uses selective lookahead to generating complex but relatively small Boolean features <ref> (Hu & Kibler 1996) </ref>. This covers one end of the spectrum. As concepts becomes larger in terms of the number of relevant attributes involved, selective lookahead algorithms such as GALA fail to find useful new attributes.
Reference: <author> Kung, S. Y. </author> <title> "Digital Neural Networks", </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference: <author> Matheus, C. J. & Rendell, L. A. </author> <title> "Constructive Induction on Decision Trees", </title> <booktitle> in Proceeding of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <address> p645-650, </address> <year> 1989. </year>
Reference-contexts: The constructed features should make concept learning easier for many learning algorithms. There are currently many constructive induction algorithms based on the strategy of constructing new attributes, including FRINGE (Pagallo, 1989), GREEDY3 (Pagallo & Haussler, 1990), DCFringe (Yang et. al., 1991), CITRE <ref> (Matheus & Rendell, 1989) </ref>, LFC (Ragavan & Rendell, 1993; Ragavan et. al., 1993), MRP (Perez & Rendell 1995), GALA (Hu & Kibler 1996), etc.
Reference: <author> Pagallo, G. </author> <title> "Learning DNF by Decision Trees", </title> <booktitle> in Proceeding of the 11th International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Perez, E. & Rendell, L. </author> <title> "Using Multidimensional Projection to Find Relations", </title> <booktitle> in Proceeding of the 12th Machine Learning Conference, </booktitle> <address> p447-455, </address> <year> 1995. </year>
Reference-contexts: There are currently many constructive induction algorithms based on the strategy of constructing new attributes, including FRINGE (Pagallo, 1989), GREEDY3 (Pagallo & Haussler, 1990), DCFringe (Yang et. al., 1991), CITRE (Matheus & Rendell, 1989), LFC (Ragavan & Rendell, 1993; Ragavan et. al., 1993), MRP <ref> (Perez & Rendell 1995) </ref>, GALA (Hu & Kibler 1996), etc. Unfortunately, most of the current constructive induction algorithms degrade in performance as the target concept becomes larger and more complex, when measured by the number of Boolean combinations (Perez & Rendell 1996).
Reference: <author> Perez, E. & Rendell, L. </author> <title> "Learning Despite Concept Variation by Finding Structure in Attribute-based Date", </title> <booktitle> in Proceeding of the 13th Machine Learning Conference, </booktitle> <address> p391-399, </address> <year> 1996. </year>
Reference-contexts: Unfortunately, most of the current constructive induction algorithms degrade in performance as the target concept becomes larger and more complex, when measured by the number of Boolean combinations <ref> (Perez & Rendell 1996) </ref>. Though MRP is demonstrated to learn several complex relations, the meaning of its extensional representation is implicit and usually difficult to interpret. Constructive induction has been directed towards improved classification accuracy.
Reference: <author> Quinlan, J. R. </author> <title> C4.5 : Programs for Machine Learning, </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: By this we mean that the SLF process generated the feature which was identical to the linear threshold feature that used in generating the Boolean concept. The third and fourth (also seventh and eighth) columns denote the accuracy of the neural net and C4.5 <ref> (Quinlan 1993) </ref> respectively. The new accuracy of C4.5 after adding the new SLF is presented in column 5 and 9. With only 5% of the data, GalaII was usually able to find the prototype, except for the difficult concept P4.
Reference: <author> Ragavan, H., Rendell, L., Shaw, M., Tessmer, A. </author> <title> "Complex Concept Acquisition through Directed Search and Feature Caching", </title> <booktitle> in Proceeding of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <address> p946-958, </address> <year> 1993. </year>
Reference: <author> Ragavan, H. & Rendell, L. </author> <title> "Lookahead Feature Construction for Learning Hard Concepts", </title> <booktitle> in Proceeding of the 10th Machine Learning Conference, </booktitle> <address> p252-259, </address> <year> 1993. </year>
Reference: <author> Rumelhart, D.E., Hinton, G. E., Williams, R. J. </author> <title> "Learning Internal Representations by Error Propagation" in Parallel Distributed Processing: </title> <journal> Explorations in Microstructures of Cognition, </journal> <volume> Vol 1, p318-362, </volume> <year> 1986. </year>
Reference: <author> Spackman, K. </author> <title> "Learning Categorical Decision Criteria in Biomedical Domains", </title> <booktitle> in Proceeding of the 5th International Workshop on Machine Learning, </booktitle> <address> p36-46, </address> <year> 1988. </year> <month> 15 </month>
References-found: 15

