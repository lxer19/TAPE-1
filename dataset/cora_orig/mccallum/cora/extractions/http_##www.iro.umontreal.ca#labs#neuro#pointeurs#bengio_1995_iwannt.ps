URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/bengio_1995_iwannt.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/proc.html
Root-URL: http://www.iro.umontreal.ca
Email: e-mail: fbengio,fessant,colloberg@lannion.cnet.fr  
Title: A Connectionist System for Medium-Term Horizon Time Series Prediction  
Author: Samy Bengio, Francoise Fessant, and Daniel Collobert 
Address: 2, avenue Pierre Marzin, 22307 Lannion, FRANCE.  
Affiliation: France Telecom, Centre National d' Etudes des Telecommunications, LAB/RIO/TNT,  
Abstract: In this paper, we propose some improvements for the problem of time series prediction with neural networks where a medium-term prediction horizon is needed. In particular, the ionospheric prediction service of the french Centre National d' Etudes des Telecommunications needs a six-month ahead prediction of a sunspots related time series which has a strong influence on wave propagation in ionosphere. The proposed improvements consist in two different modular architectures and a way to increase the size of the training set. Experimental results are compared to those of a simple multi-layer perceptron. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. L. Elman, </author> <title> "Finding structure in time," </title> <journal> Cognitive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference-contexts: For the IR5 series, the best network we found had 40 input units, 23 hidden units, and 6 output units. o was set arbitrary to 1. Hidden and output units used a nonlinear squashing function with output in <ref> [1; 1] </ref>. paper. 4 Improvements Over a Simple Use of Neu ral Networks for Prediction In this section, we present three improvements over this simple use of a neural network for time series prediction. <p> All three are based on the fact that generalization (prediction) error is related to training set error, network capacity (which itself is related to the number of free parameters), training 1 The series is usually normalized to get a zero mean with values in <ref> [1; 1] </ref>. set size and the use of prior knowledge in system design [7]. The first two improvements are architectural: a better system design and less parameters. <p> All gave better results for a particular and known difficult time series. We shall now try to mix some of these ideas, as well as use other models such as Elman recurrent networks <ref> [1] </ref> or hierarchical mixtures of experts [3].
Reference: [2] <author> A. Izenman, "J. R. </author> <title> Wolf and the Zurich sunspot relative numbers," </title> <journal> The Mathematical Intelligencer, </journal> <volume> vol. 7, no. 1, </volume> <pages> pp. 27-33, </pages> <year> 1985. </year>
Reference-contexts: Ionospheric state depends directly on solar activity, so in order to make a prediction of its future state we have to predict the solar activity, which is here represented as the number of sunspots R <ref> [2] </ref>.
Reference: [3] <author> M. I. Jordan and R. A. Jacobs, </author> <title> "Hierarchical mixtures of experts and the EM algorithm," </title> <journal> Neural Computation, </journal> <volume> vol. 6, no. 2, </volume> <pages> pp. 181-214, </pages> <year> 1994. </year>
Reference-contexts: All gave better results for a particular and known difficult time series. We shall now try to mix some of these ideas, as well as use other models such as Elman recurrent networks [1] or hierarchical mixtures of experts <ref> [3] </ref>.
Reference: [4] <author> D. C. Park, M. A. El-Sharkawi, and R. J. Marks II, </author> <title> "Electric load forecasting using an artificial neural network," </title> <journal> IEEE Transaction on Power Systems, </journal> <volume> vol. 6, no. 2, </volume> <pages> pp. 442-449, </pages> <year> 1991. </year>
Reference-contexts: This is the reason why our test set is kept small in comparison to the training set size. 3 A Simple Connectionist Solution The simplest way to use neural networks for prediction, which has already been used in many applications <ref> [4, 8] </ref>, is to use a multi-layer perceptron with one hidden layer, trained with backpropagation [5]. All input units are connected to all hidden units and all hidden units are connected to all output units.
Reference: [5] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> "Learning internal representations by error propagation," in Parallel Distributed Processing (D. </title> <editor> E. Rumelhart and J. L. McClelland, eds.), </editor> <volume> vol. 1, </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: our test set is kept small in comparison to the training set size. 3 A Simple Connectionist Solution The simplest way to use neural networks for prediction, which has already been used in many applications [4, 8], is to use a multi-layer perceptron with one hidden layer, trained with backpropagation <ref> [5] </ref>. All input units are connected to all hidden units and all hidden units are connected to all output units.
Reference: [6] <author> F. Takens, </author> <title> "Detecting strange attractors in turbulence," in Dynamical Systems and Turbulence (D. </title> <editor> A. Rand and L.-S. Young, eds.), </editor> <volume> vol. </volume> <booktitle> 898 of Lecture Notes in Mathematics, (Warwick 1980), </booktitle> <pages> pp. 366-381, </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: Prediction then consists to find the future values fx N+1 , x N+2 , g. It has been shown in <ref> [6] </ref> that if the series is deterministic, there exists an integer d (which is called the embedding dimension), an integer o (which is an arbitrary delay) and a function f () such that for every t &gt; (d o ): x t = f (x to ; x t2o ; ;
Reference: [7] <author> V. N. Vapnik, </author> <title> Estimation of Dependencies Based on Empirical Data. </title> <address> New-York, NY, USA: </address> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: that generalization (prediction) error is related to training set error, network capacity (which itself is related to the number of free parameters), training 1 The series is usually normalized to get a zero mean with values in [1; 1]. set size and the use of prior knowledge in system design <ref> [7] </ref>. The first two improvements are architectural: a better system design and less parameters.
Reference: [8] <author> A. Varfis and C. Versino, </author> <title> "Univariate economic time series forecasting by connectionist methods," </title> <booktitle> in Proceedings of the International Neural Network Conference (INNC), (Paris, France), </booktitle> <pages> pp. 342-345, </pages> <year> 1990. </year>
Reference-contexts: This is the reason why our test set is kept small in comparison to the training set size. 3 A Simple Connectionist Solution The simplest way to use neural networks for prediction, which has already been used in many applications <ref> [4, 8] </ref>, is to use a multi-layer perceptron with one hidden layer, trained with backpropagation [5]. All input units are connected to all hidden units and all hidden units are connected to all output units.
Reference: [9] <author> A. S. Weigend, B. A. Huberman, and D. E. Rumelhart, </author> <title> "Predicting sunspots and exchange rates with connectionist networks," in Nonlinear modeling and forecasting (M. </title> <editor> Casdagli and S. Eubank, </editor> <booktitle> eds.), </booktitle> <pages> pp. 395-431, </pages> <publisher> Addison Wesley, </publisher> <year> 1992. </year>
Reference-contexts: The sunspots time series is known to be difficult to predict and has served as a benchmark in the statistics literature <ref> [9] </ref>. 2 Problem Description The CNET ionospheric prediction service has given us the IR5 time series which is a non-centered five-month mean of the monthly sunspots number mean MR: IR5 t = 5 where MR t is the mean sunspots number of month t.
References-found: 9

