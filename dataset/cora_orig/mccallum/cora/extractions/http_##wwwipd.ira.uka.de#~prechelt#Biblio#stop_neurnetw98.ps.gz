URL: http://wwwipd.ira.uka.de/~prechelt/Biblio/stop_neurnetw98.ps.gz
Refering-URL: 
Root-URL: 
Email: (prechelt@ira.uka.de)  
Phone: Phone: +49/721/608-4068, Fax: -7343  
Title: Automatic Early Stopping Using Cross Validation: Quantifying the Criteria  
Author: Lutz Prechelt 
Address: D-76128 Karlsruhe; Germany  
Affiliation: Fakultat fur Informatik; Universitat Karlsruhe  
Date: 1998  December 12, 1997  
Note: Appeared in Neural Networks  
Abstract: Cross validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the over-fitting ("early stopping"). The exact criterion used for cross validation based early stopping, however, is chosen in an ad-hoc fashion by most researchers or training is stopped interactively. To aid a more well-founded selection of the stopping criterion, 14 different automatic stopping criteria from 3 classes were evaluated empirically for their efficiency and effectiveness in 12 different classification and approximation tasks using multi layer perceptrons with RPROP training. The experiments show that on the average slower stopping criteria allow for small improvements in generalization (on the order of 4%), but cost about factor 4 longer training time. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baldi, P. and Chauvin, Y. </author> <year> (1991). </year> <title> Temporal evolution of generalization during learning in linear networks. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 589-603. </pages>
Reference-contexts: This approach uses the cross validation set to anticipate the behavior on the test set (or in real use), assuming that the error on both will be similar. However, the real situation is a lot more complex. Real generalization curves almost always have more than one local minimum. <ref> (Baldi & Chauvin, 1991) </ref> showed for linear networks with n inputs and n outputs that up to n such local minima are possible; for multi layer networks, the situation is even worse.
Reference: <editor> Cowan, J. D., Tesauro, G., and Alspector, J., editors (1994). </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: <author> Cun, Y. L., Denker, J. S., and Solla, S. A. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In (Touretzky, </editor> <year> 1990), </year> <pages> pages 598-605. </pages> <editor> (efiesler@idiap.ch), Fiesler, E. </editor> <year> (1994). </year> <title> Comparative bibliography of ontogenic neural networks. </title> <note> (submitted for publication). </note>
Reference: <author> Fahlman, S. E. </author> <year> (1988). </year> <title> An empirical study of learning speed in back-propagation networks. </title> <type> Technical Report CMU-CS-88-162, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, PA. </institution> <note> 10 Fahlman, </note> <author> S. E. and Lebiere, C. </author> <year> (1990). </year> <title> The Cascade-Correlation learning architecture. </title> <editor> In (Touretzky, </editor> <year> 1990), </year> <pages> pages 524-532. </pages>
Reference-contexts: RPROP is a fast backpropagation variant similar in spirit to quickprop <ref> (Fahlman, 1988) </ref>. It is about 6 as fast as quickprop but more stable without adjustment of the parameters. RPROP requires epoch learning, i.e., the weights are updated only once per epoch. Therefore, the algorithm is fast without parameter tuning for small training sets but not recommendable for large training sets.
Reference: <author> Finnoff, W., Hergert, F., and Zimmermann, H. G. </author> <year> (1993). </year> <title> Improving model selection by nonconvergent methods. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 771-783. </pages>
Reference-contexts: The corresponding NN techniques for reducing the size of each parameter dimension are regularization such as weight decay (e.g. Krogh & Hertz, 1992) and others (e.g. Weigend, Rumelhart & Huberman, 1991) or early stopping (Morgan & Bourlard, 1990). See also (Reed, 1993; Fiesler, 1994) for an overview and <ref> (Finnoff, Hergert & Zimmermann, 1993) </ref> for an experimental comparison. Early stopping is widely used because it is simple to understand and implement and has been reported to be superior to regularization methods in many cases, e.g. in (Finnoff, Hergert & Zimmermann, 1993). <p> See also (Reed, 1993; Fiesler, 1994) for an overview and <ref> (Finnoff, Hergert & Zimmermann, 1993) </ref> for an experimental comparison. Early stopping is widely used because it is simple to understand and implement and has been reported to be superior to regularization methods in many cases, e.g. in (Finnoff, Hergert & Zimmermann, 1993). The method can be used either interactively, i.e., based on human judgement, or automatically, i.e., based on some formal stopping criterion. However, such automatic stopping criteria are usually chosen in an ad-hoc fashion today.
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference-contexts: Generalization performance means small error on examples not seen during training. Because standard neural network architectures such as the fully connected multi layer perceptron almost always have too large a parameter space, such architectures are prone to overfitting <ref> (Geman, Bienenstock & Doursat, 1992) </ref>. While the network seems to get better and better (the error on the training set decreases), at some point during training it actually begins to get worse again (the error on unseen examples increases).
Reference: <editor> Hanson, S. J., Cowan, J. D., and Giles, C. L., editors (1993). </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: <author> Hassibi, B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In (Hanson et al., </editor> <year> 1993), </year> <pages> pages 164-171. </pages>
Reference: <author> Krogh, A. and Hertz, J. A. </author> <year> (1992). </year> <title> A simple weight decay can improve generalization. </title> <editor> In (Moody et al., </editor> <year> 1992), </year> <pages> pages 950-957. </pages>
Reference-contexts: Fahlman & Lebiere, 1990), pruning (e.g. Le Cun, Denker & Solla, 1990; Hassibi & Stork, 1992; Levin, Leen & Moody, 1994), or weight sharing (e.g. Nowlan & Hinton, 1992). The corresponding NN techniques for reducing the size of each parameter dimension are regularization such as weight decay <ref> (e.g. Krogh & Hertz, 1992) </ref> and others (e.g. Weigend, Rumelhart & Huberman, 1991) or early stopping (Morgan & Bourlard, 1990). See also (Reed, 1993; Fiesler, 1994) for an overview and (Finnoff, Hergert & Zimmermann, 1993) for an experimental comparison.
Reference: <author> Levin, A. U., Leen, T. K., and Moody, J. E. </author> <year> (1994). </year> <title> Fast pruning using principal components. </title> <editor> In (Cowan et al., </editor> <year> 1994). </year>
Reference: <editor> Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors (1991). </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: <editor> Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors (1992). </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: <author> Morgan, N. and Bourlard, H. </author> <year> (1990). </year> <title> Generalization and parameter estimation in feedfor-ward nets: Some experiments. </title> <editor> In (Touretzky, </editor> <year> 1990), </year> <pages> pages 630-637. </pages>
Reference-contexts: Nowlan & Hinton, 1992). The corresponding NN techniques for reducing the size of each parameter dimension are regularization such as weight decay (e.g. Krogh & Hertz, 1992) and others (e.g. Weigend, Rumelhart & Huberman, 1991) or early stopping <ref> (Morgan & Bourlard, 1990) </ref>. See also (Reed, 1993; Fiesler, 1994) for an overview and (Finnoff, Hergert & Zimmermann, 1993) for an experimental comparison.
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 473-493. </pages>
Reference-contexts: Fahlman & Lebiere, 1990), pruning (e.g. Le Cun, Denker & Solla, 1990; Hassibi & Stork, 1992; Levin, Leen & Moody, 1994), or weight sharing <ref> (e.g. Nowlan & Hinton, 1992) </ref>. The corresponding NN techniques for reducing the size of each parameter dimension are regularization such as weight decay (e.g. Krogh & Hertz, 1992) and others (e.g. Weigend, Rumelhart & Huberman, 1991) or early stopping (Morgan & Bourlard, 1990).
Reference: <author> Prechelt, L. </author> <year> (1994). </year> <title> PROBEN1 | A set of benchmarks and benchmarking rules for neural network training algorithms. </title> <type> Technical Report 21/94, </type> <institution> Fakultat fur Informatik, Univer-sitat Karlsruhe, Germany. </institution> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-21.ps.gz on ftp.ira.uka.de. </note>
Reference-contexts: This decision was made in order to obtain pure stopping criteria results. In a real application this would be a waste of training data and should be changed. 12 different problems were used, all from the Proben1 NN benchmark set <ref> (Prechelt, 1994) </ref>. These problems form a sample of a quite broad class of domains, but are of course not universally representative of learning; see (Prechelt, 1994) for a discussion of how to characterize the Proben1 domains. 5 Experimental setup The stopping criteria examined were GL 1 , GL 2 , GL <p> real application this would be a waste of training data and should be changed. 12 different problems were used, all from the Proben1 NN benchmark set <ref> (Prechelt, 1994) </ref>. These problems form a sample of a quite broad class of domains, but are of course not universally representative of learning; see (Prechelt, 1994) for a discussion of how to characterize the Proben1 domains. 5 Experimental setup The stopping criteria examined were GL 1 , GL 2 , GL 3 , GL 5 , P Q 0:5 , P Q 0:75 , P Q 1 , P Q 2 , P Q 3 <p> A popular rule of thumb recommends to always use sigmoidal output units for classification tasks and linear output units for regression (approximation) tasks. This rule was not applied since it is too far from always being good; see <ref> (Prechelt, 1994) </ref>.
Reference: <author> Reed, R. </author> <year> (1993). </year> <title> Pruning algorithms | a survey. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(5) </volume> <pages> 740-746. </pages>
Reference: <author> Riedmiller, M. and Braun, H. </author> <year> (1993). </year> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm. </title> <booktitle> In Proc. of the IEEE Intl. Conf. on Neural Networks, </booktitle> <pages> pages 586-591, </pages> <address> San Francisco, CA. </address>
Reference-contexts: A series of simulations using all of the above criteria was run, in which all criteria where evaluated simultaneously, i.e., each single training run returned one result for each of the criteria. This approach reduces the variance of the estimation. All runs were done using the RPROP training algorithm <ref> (Riedmiller & Braun, 1993) </ref> using the squared error function and the parameters + = 1:1, = 0:5, 0 2 0:05 : : : 0:2 randomly per weight, max = 50, min = 0, initial weights 0:5 : : : 0:5 randomly.
Reference: <editor> Touretzky, D. S., editor (1990). </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: <author> Wang, C., Venkatesh, S. S., and Judd, J. S. </author> <year> (1994). </year> <title> Optimal stopping and effective machine complexity in learning. </title> <editor> In (Cowan et al., </editor> <year> 1994). </year> <note> 11 Weigend, </note> <author> A. S., Rumelhart, D. E., and Huberman, B. A. </author> <year> (1991). </year> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In (Lippmann et al., </editor> <year> 1991), </year> <pages> pages 875-882. </pages>
References-found: 19

