URL: http://www.ai.mit.edu/people/unamay/papers/submission.ps
Refering-URL: http://www.ai.mit.edu/people/unamay/papers.html
Root-URL: 
Email: unamay@ai.mit.edu  
Title: Investigating the Generality of Automatically Defined Functions  
Author: Una-May O'Reilly 
Web: http://www.ai.mit.edu/people/unamay/unamay.html  
Address: Cambridge, MA, 02139  
Affiliation: M.I.T. Artificial Intelligence Lab,  
Abstract: This paper studies how well the combination of simulated annealing and ADFs solves genetic programming (GP) style program discovery problems. On a suite composed of the even-k-parity problems for k = 3,4,5, it analyses the performance of simulated annealing with ADFs as compared to not using ADFs. In contrast to GP results on this suite, when simulated annealing is run with ADFs, as problem size increases, the advantage to using them over a standard GP program representation is marginal. When the performance of simulated annealing is compared to GP with both algorithm using ADFs on the even-3-parity problem GP is advantageous, on the even-4-parity problem SA and GP are equal, and on the even-5-parity problem SA is advantageous.
Abstract-found: 1
Intro-found: 1
Reference: [ Aarts and Korst, 1989 ] <author> E. Aarts and J. Korst. </author> <title> Simulated Annealing and Boltzmann Machines,. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1989. </year>
Reference-contexts: It then restricts where it chooses a crossover point in the second parent to the program area that uses the same set. Based on this design, both main program and procedures co-evolve in GP with ADFs. Simulating Annealing <ref> [ Aarts and Korst, 1989 ] </ref> is a widely used optimization algorithm that contrasts with GP's population and crossover based approach, because it is a single point search which generates a new program from the current one via mutation and chooses to transfer the search to it depending on an acceptance
Reference: [ Angeline and Pollack, 1991 ] <author> P. J. Angeline and J. B. Pol-lack. </author> <title> Coevolving high-level representations. </title> <editor> In C. G. Lang-ton, editor, </editor> <booktitle> Artificial Life III, SFI Studies in the Sciences of Complexity, </booktitle> <volume> volume XVII. </volume> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference: [ Kinnear, Jr., 1994 ] <author> K. E. Kinnear, Jr. </author> <title> Alternatives in automatic function definition: A comparison of performance. </title> <editor> In K. E. Kinnear, Jr., editor, </editor> <booktitle> Advances in Genetic Programming, chapter 6. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Fitness evaluation of the mutant is exactly like GP: the rpb is executed and its results are measured against the desired ones. 3 Motivation and Goals Progress has been made into understanding how and why ADFs work. Kinnear compared GP with ADFs to Ange-line's Module Acquisition in <ref> [ Kinnear, Jr., 1994 ] </ref> . For the even-4-parity problem he attributed the success of ADFs partially to a form of structural regularity that strongly increases the likelihood of evolving a correct solution.
Reference: [ Koza, 1990 ] <author> J. R. Koza. </author> <title> Genetic programming: A paradigm for genetically breeding populations of computer programs to solve problems. </title> <type> Technical Report STAN-CS-90-1314, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1990. </year>
Reference: [ Koza, 1992 ] <author> J. R. Koza. </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference: [ Koza, 1994 ] <author> J. R. Koza. </author> <title> Genetic Programming II: Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1994. </year>
Reference-contexts: Basically, a candidate solution has a fixed template of main program and procedures and the definitions and invocations of the main program and procedures co-evolve in the process of the search. The ADF approach to representation is well described by its designer in <ref> [ Koza, 1994 ] </ref> and summarized in Section 2 of this paper. O'Reilly and Oppacher have shown [ O'Reilly and Op-pacher, 1996 ] that, with a hierarchical variable length mutation operator, simulated annealing (SA) is able to perform program discovery on problems expressed in exactly the style of GP. <p> This regularity is much less likely to evolve without ADFs than with them. One change to an ADF has multiple and identical effects. The capacity of the ADF representation for concise repetition thus assures a form of structural regularity. As the first two main points of Genetic Programming II <ref> [ Koza, 1994 ] </ref> , Koza, first, asserts that it is possible to interpret GP with ADFs as performing either a top-down process of problem decomposition or a bottom-up process of representational change to exploit identified regularities in the problem. <p> Second, ADFs work by exploiting inherent patterns and modularities within a problem, though perhaps in ways that are very different from the style of programmers. Koza also offers the "lens effect" in <ref> [ Koza, 1994 ] </ref> as one explanation for why ADFs are advantageous over representations without them. <p> The difference, termed the "lens effect" appears to make usings ADFs advantageous since correct solutions are highly fit outliers. Koza shows many examples in <ref> [ Koza, 1994 ] </ref> which reinforce the assertion that, for some problems, ADFs pay off because they require less computational effort and produce smaller solutions than using GP without them. He reports that on three problems this payoff increased as problem size grows. <p> the problem represented with ADFs is ideally suited to one algorithm or the other. 4 Experimental Procedure Problem Suite To examine the scaling effects of ADFs with SA we chose one progression of a problem (i.e. versions that increase in size) which was used by Koza in Genetic Programming II <ref> [ Koza, 1994 ] </ref> : even-k-parity in three versions - even-3-parity, even-4-parity, and even-5-parity. By choosing a problem Koza uses, we can compare GP with ADFs to SA with ADFs as well as make comparisons between GP, with and without ADFs, to SA, with and without ADFs. <p> By choosing a problem Koza uses, we can compare GP with ADFs to SA with ADFs as well as make comparisons between GP, with and without ADFs, to SA, with and without ADFs. Instead of re-conducting the GP experiments we cite the results of Koza <ref> [ Koza, 1994 ] </ref> . The same source should be consulted for details of the GP algorithm used. It is fairly standard in terms of minor and major parameters though the population size of 16000 is somewhat larger than normal. <p> Tableaux and text which give the number of ADFs and formal parameters for each version of even-k-parity are available in sections 6.3 to 6.5 and sections 6.10 to 6.12 of <ref> [ Koza, 1994 ] </ref> . SA Algorithm and Parameter Choices Our SA algorithm is standard [ Rich and Knight, 1991 ] . It always starts from a temperature of 1.5 and decreases it at regular intervals (parameter stepsize) towards a final temperature (parameter T-final) at an exponential rate. <p> These values are in the fifth column of Table 1. Assessment We did not base our comparisons on computational effort, E, defined by Koza <ref> [ Koza, 1994 ] </ref> . <p> Table 3, columns 3 and 4 correspond in terms of maximum individuals processed. Column 3 lists the probability that an SA execution is successful. Column 4 lists the P (M,i) of GP runs from <ref> [ Koza, 1994 ] </ref> where M multiplied by (i+1) equals the same maximum number of individuals processed by the SA run. For example, on even-3-parity each SA execution processed a maximum of 48,000 individuals.
Reference: [ O'Reilly and Oppacher, 1994 ] <author> U.M. O'Reilly and F. Op-pacher. </author> <title> Program search with a hierarchical variable length representation: Genetic programming, simulated annealing and hill climbing. </title> <editor> In Y. Davidor, H.-P. Schwefel, and R. Manner, editors, </editor> <booktitle> Parallel Problem Solving From Nature - PPSN III, Volume 866 of Lecture Notes in Computer Science, </booktitle> <pages> pages 397-406, </pages> <address> Berlin, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The temperature usually decreases exponentially over the course of the search. SA can solve GP style problems competently by using a suitably designed mutation operator that mutates programs expressed as parse trees. O'Reilly and Oppacher <ref> [ O'Reilly and Oppacher, 1994 ] </ref> introduced such a mutation operator, called HVL-mutate. HVL-mutate generates a candidate solution that is syntactically correct and which may differ in size and structure from its "parent".
Reference: [ O'Reilly and Oppacher, 1996 ] <author> U.M. O'Reilly and F. Op-pacher. </author> <title> A comparative analysis of genetic programming. </title> <editor> In P. Angeline and K. E. Kinnear, Jr., editors, </editor> <booktitle> Advances in Genetic Programming 2, chapter 9. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, USA, </address> <year> 1996. </year>
Reference: [ O'Reilly, 1995 ] <author> U.M. O'Reilly. </author> <title> An Analysis of Genetic Programming. </title> <type> PhD thesis, </type> <institution> Carleton University, Ottawa, Ont, </institution> <month> September </month> <year> 1995. </year> <note> Available at http://www.santafe.edu/~unamay. </note>
Reference-contexts: HVL-mutate works by performing a sub-operation of substitution, deletion or insertion (with equal probability) to the copy of the parent's program tree. It is described and depicted in detail in <ref> [ O'Reilly, 1995 ] </ref> . ADFs are a static form of program representation (i.e. a template and separate primitive sets), so it is straight forward to modify SA to function with them. In SA with ADFs a candidate solution is, in fact, exactly like one in GP with ADFs.
Reference: [ Press, 1992 ] <author> W. H. </author> <title> Press. Numerical Recipes in C. </title> <publisher> Cam-bridge Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: The average fitness values of the best individual at the end of each execution are also compared and expressed as a percent where a perfect solution has a fitness of 100%. The use of a Student's t-test (different means) measuring 95% confidence <ref> [ Press, 1992 ] </ref> is implied when we indicate the difference between two averages is statistically significant.
Reference: [ Rich and Knight, 1991 ] <author> E. Rich and K. Knight. </author> <booktitle> Artificial Intelligence. </booktitle> <address> McGraw Hill, New York, </address> <year> 1991. </year>
Reference-contexts: Tableaux and text which give the number of ADFs and formal parameters for each version of even-k-parity are available in sections 6.3 to 6.5 and sections 6.10 to 6.12 of [ Koza, 1994 ] . SA Algorithm and Parameter Choices Our SA algorithm is standard <ref> [ Rich and Knight, 1991 ] </ref> . It always starts from a temperature of 1.5 and decreases it at regular intervals (parameter stepsize) towards a final temperature (parameter T-final) at an exponential rate. The parameter T-final is always set to 0.01% of the minimum fitness change.
Reference: [ Rosca and Ballard, 1996 ] <author> J. P. Rosca and D. H. Ballard. </author> <title> Discovery of subroutines in genetic programming. </title> <editor> In P. An-geline and K. E. Kinnear, Jr., editors, </editor> <booktitle> Advances in Genetic Programming 2, chapter 9. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, USA, </address> <year> 1996. </year>
References-found: 12

