URL: http://www.cs.colorado.edu/homes/rupa/public_html/mypapers/p93.2.ps
Refering-URL: http://www.cs.colorado.edu/~rupa/mypapers.html
Root-URL: http://www.cs.colorado.edu
Title: A Connectionist Symbol Manipulator That Discovers the Structure of Context-Free Languages  
Author: Michael C. Mozer and Sreerupa Das 
Address: Boulder, CO 80309-0430  
Affiliation: Department of Computer Science Institute of Cognitive Science University of Colorado  
Abstract: We present a neural net architecture that can discover hierarchical and recursive structure in symbol strings. To detect structure at multiple levels, the architecture has the capability of reducing symbols substrings to single symbols, and makes use of an external stack memory. In terms of formal languages, the architecture can learn to parse strings in an LR(0) context-free grammar. Given training sets of positive and negative exemplars, the architecture has been trained to recognize many different grammars. The architecture has only one layer of modifiable weights, allowing for a Many cognitive domains involve complex sequences that contain hierarchical or recursive structure, e.g., music, natural language parsing, event perception. To illustrate, "the spider that ate the hairy fly" is a noun phrase containing the embedded noun phrase "the hairy fly." Understanding such multilevel structures requires forming reduced descriptions (Hinton, 1988) in which a string of symbols or states ("the hairy fly") is reduced to a single symbolic entity (a noun phrase). We present a neural net architecture that learns to encode the structure of symbol strings via such reduction transformations. The difficult problem of extracting multilevel structure from complex, extended sequences has been studied by Mozer (1992), Ring (1993), Rohwer (1990), and Schmidhuber (1992), among others. While these previous efforts have made some straightforward interpretation of its behavior.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bridle, J. </author> <year> (1990). </year> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In D. S. Touretzky (Ed.), </editor> <booktitle> Advances in neural information processing systems 2 (pp. </booktitle> <pages> 211-217). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Das, S., Giles, C. L., & Sun, G. Z. </author> <year> (1992). </year> <title> Learning context-free grammars: Capabilities and limitations of neural network with an external stack memory. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science (pp. </booktitle> <pages> 791-795). </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: The first three grammars were successfully learned by the model of Giles et al. (1990), although the analysis required to interpret the weights is generally more cumbersome and tentative. The last grammar could not be learned by their model <ref> (Das et al., 1992) </ref>.
Reference: <author> Giles, C. L., Sun, G. Z., Chen, H. H., Lee, Y. C., & Chen, D. </author> <year> (1990). </year> <title> Higher order recurrent networks and grammatical inference. </title> <editor> In D. S. Touretzky (Ed.), </editor> <booktitle> Advances in neural information processing systems 2 (pp. </booktitle> <pages> 380-387). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Its activity, s def , is computed like that of any demon unit with dist def = b def . 4 CONTINUOUS STACK Because demon units can be partially active, stack operations need to be performed partially. This can be accomplished with a continuous stack <ref> (Giles et al., 1990) </ref>. Unlike a discrete stack where an item is either present or absent, items can be present to varying degrees. Each item on the stack has an associated thickness, a scalar in the interval [0; 1] indicating what fraction of the item is present (Figure 3).
Reference: <author> Hinton, G. E. </author> <year> (1988). </year> <title> Representing part-whole hierarchies in connectionist networks. </title> <booktitle> Proceedings of the Eighth Annual Conference of the Cognitive Science Society. </booktitle>
Reference: <author> Mozer, M. C. </author> <year> (1992). </year> <title> The induction of multiscale temporal structure. </title> <editor> In J. E. Moody, S. J. Hanson, & R. P. Lippman (Eds.), </editor> <booktitle> Advances in neural information processing systems IV (pp. </booktitle> <pages> 275-282). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mozer, M. C., & Bachrach, J. </author> <year> (1991). </year> <title> SLUG: A connectionist architecture for inferring the structure of finite-state environments. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> 139-160. </pages>
Reference-contexts: It is a remarkable achievement that the numerical optimization framework of neural net learning can be used to discover symbolic rules <ref> (see also Mozer & Bachrach, 1991) </ref>. The first three grammars were successfully learned by the model of Giles et al. (1990), although the analysis required to interpret the weights is generally more cumbersome and tentative. The last grammar could not be learned by their model (Das et al., 1992).
Reference: <author> Ring, M. </author> <year> (1993). </year> <title> Learning sequential tasks by incrementally adding higher orders. This volume. </title>
Reference: <author> Rohwer, R. </author> <year> (1990). </year> <title> The 'moving targets' training algorithm. </title> <editor> In D. S. Touretzky (Ed.), </editor> <booktitle> Advances in neural information processing systems 2 (pp. </booktitle> <pages> 558-565). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition. </booktitle> <volume> Volume I: </volume> <pages> Foundations (pp. 318-362). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press/Bradford Books. </publisher>
Reference-contexts: The derivative of the objective function is computed with respect to the weight parameters using a form of back propagation through time <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>. This involves "unfolding" the architecture in time and back propagating through the stack.
Reference: <author> Rumelhart, D. E. </author> <title> (in press). Connectionist processing and learning as statistical inference. </title> <editor> In Y. Chauvin & D. E. Rumelhart (Eds.), Backpropagation: </editor> <booktitle> Theory, architectures, and applications. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1992). </year> <title> Learning unambiguous reduced sequence descriptions. </title> <editor> In J. E. Moody, S. </editor> <publisher> J. </publisher>
Reference: <editor> Hanson, & R. P. Lippman (Eds.), </editor> <booktitle> Advances in neural information processing systems IV (pp. </booktitle> <pages> 291-298). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Stolcke, A., & Omohundro, S. </author> <year> (1993). </year> <title> Hidden markov model induction by Bayesian model merging. This volume. </title>
Reference: <author> Sun, G. Z., Chen, H. H., Giles, C. L., Lee, Y. C., & Chen, D. </author> <year> (1990). </year> <title> Connectionist pushdown automata that learn context-free grammars. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (pp. </booktitle> <address> I-577). Hillsdale, NJ: </address> <publisher> Erlbaum Associates. </publisher>
References-found: 14

