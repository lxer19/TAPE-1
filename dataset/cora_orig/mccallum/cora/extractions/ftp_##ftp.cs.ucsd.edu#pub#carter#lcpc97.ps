URL: ftp://ftp.cs.ucsd.edu/pub/carter/lcpc97.ps
Refering-URL: http://www.cs.ucsd.edu/users/ferrante/papers.html
Root-URL: http://www.cs.ucsd.edu
Title: Quantifying the Multi-Level Nature of Tiling Interactions  
Author: Nicholas Mitchell, Larry Carter, Jeanne Ferrante, Karin Hogstedt 
Address: La Jolla CA 92093-0114  
Affiliation: Computer Science and Engineering Department, UCSD,  
Abstract: Optimizations, including tiling, often target a single level of memory or parallelism, such as cache. These optimizations usually operate on a level-by-level basis, guided by a cost function parameterized by features of that single level. The benefit of optimizations guided by these one-level cost functions decreases as architectures trend towards a hierarchy of memory and parallelism. We look at three common architectural scenarios. For each, we quantify the improvement a single tiling choice could realize by using information from multiple levels in concert. To do so, we derive multilevel cost functions which guide the optimal choice of tile size and shape. We give both analyses and simulation results to support our points. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Anant Agarwal, David Kranz, and Venkat Natarajan. </author> <title> Automatic partitioning of parallel loops and data arrays for distributed shared memory multiprocessors. </title> <booktitle> In Proceedings of International Conference on Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: Quantifying performance: We employ counting arguments similar to <ref> [21, 13, 11, 1, 26] </ref> to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. <p> Quantifying performance: We employ counting arguments similar to [21, 13, 11, 1, 26] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as <ref> [21, 11, 1] </ref> give methods for choosing tile size in a nested loop; [21] uses a "fits-in" constraint based only on memory capacity (not block size), [11]'s "fits-in" constraint does not fully utilize block size information, and [1] limits block size to one. <p> Single-level tile characteristics: Works such as [21, 11, 1] give methods for choosing tile size in a nested loop; [21] uses a "fits-in" constraint based only on memory capacity (not block size), [11]'s "fits-in" constraint does not fully utilize block size information, and <ref> [1] </ref> limits block size to one.
Reference: 2. <author> Corinne Ancourt and Fran~cois Irigoin. </author> <title> Scanning polyhedra with DO loops. </title> <booktitle> In Proceedings of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 39-50, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality [28, 5, 7, 6] and exploit parallelism <ref> [2, 29, 12, 18, 22] </ref>. For instance, a tiling for cache may consider only cache size, cache line size, and cache associativity [11]. However, recent trends towards greater architectural complexity have increased the amount of information available to an optimizing compiler.
Reference: 3. <author> Utpal Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of the 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 31, 32, 21, 24, 28, 29, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 4. <author> Steve Carr. </author> <title> Combining optimization for cache and instruction-level parallelism. </title> <booktitle> In PACT '96, </booktitle> <pages> pages 238-247, </pages> <year> 1996. </year>
Reference-contexts: None of these works seeks to unify guidance for multiple levels of the memory hierarchy. Multi-level unification: Unroll-and-jam can guide locality and instruction level parallelism in concert <ref> [4] </ref>. Loop fusion and distribution affect both parallelism and locality [20]. These two works do not directly address tiling or the multi-level nature of the interactions. Rather than use multi-level cost functions, [30] performs a pruned search on the space of possible combinations of minimizations of one-level cost functions.
Reference: 5. <author> Steve Carr and Ken Kennedy. </author> <title> Compiler blockability of numerical algorithms. </title> <journal> The Journal of Supercomputing, </journal> <pages> pages 114-124, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality <ref> [28, 5, 7, 6] </ref> and exploit parallelism [2, 29, 12, 18, 22]. For instance, a tiling for cache may consider only cache size, cache line size, and cache associativity [11]. However, recent trends towards greater architectural complexity have increased the amount of information available to an optimizing compiler.
Reference: 6. <author> Steve Carr and Ken Kennedy. </author> <title> Improving the ratio of memory operations to floatingpoint operations in loops. </title> <journal> Transactions on Programming Languages and Systems, </journal> <volume> 16(6) </volume> <pages> 1768-1810, </pages> <month> November </month> <year> 1994. </year> <month> 14 </month>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality <ref> [28, 5, 7, 6] </ref> and exploit parallelism [2, 29, 12, 18, 22]. For instance, a tiling for cache may consider only cache size, cache line size, and cache associativity [11]. However, recent trends towards greater architectural complexity have increased the amount of information available to an optimizing compiler.
Reference: 7. <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality <ref> [28, 5, 7, 6] </ref> and exploit parallelism [2, 29, 12, 18, 22]. For instance, a tiling for cache may consider only cache size, cache line size, and cache associativity [11]. However, recent trends towards greater architectural complexity have increased the amount of information available to an optimizing compiler.
Reference: 8. <author> Larry Carter, Jeanne Ferrante, and S. Flynn Hummel. </author> <title> Efficient parallelism via hierarchical tiling. </title> <booktitle> In Proceedings of SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> February </month> <year> 1995. </year>
Reference: 9. <author> Larry Carter, Jeanne Ferrante, and S. Flynn Hummel. </author> <title> Hierarchical tiling for improved superscalar perfomance. </title> <booktitle> In Proceedings of International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference: 10. <author> Larry Carter, Jeanne Ferrante, Susan Flynn Hummel, Bowen Alpern, and Kang Su Gatlin. </author> <title> Hierarchical tiling: A methodology for high performance. </title> <type> Technical Report CS96-508, UCSD, </type> <institution> Department of Computer Science and Engineering, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: The opportunity to execute iterations from independent planes has been lost. Instruction Level Parallelism only: What tiling choice maximizes instruction level parallelism? Here we use a similarly simple function for ILP execution time, given as a table of values; an example for the IBM Power2 with values taken from <ref> [10] </ref> is given in Table 3 (b). The third column gives the execution time in cycles per iteration per point of the original ISG, ILP T ime (p), for p = 1 to 3 planes.
Reference: 11. <author> Stephanie Coleman and Kathryn S. McKinley. </author> <title> Tile size selection using cache organization and data layout. </title> <booktitle> In Proceedings of Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Such one-level cost functions commonly increase locality [28, 5, 7, 6] and exploit parallelism [2, 29, 12, 18, 22]. For instance, a tiling for cache may consider only cache size, cache line size, and cache associativity <ref> [11] </ref>. However, recent trends towards greater architectural complexity have increased the amount of information available to an optimizing compiler. Many machines now have multiple levels of memory and parallelism, typically arranged hierarchically. Memory appears as registers, several levels of cache and a translation look-aside buffer. <p> Researchers have tried several solutions to this information expansion problem. Many have simply ignored the multi-level information, instead relying on one-level cost functions <ref> [28, 11, 26] </ref>. Others rephrase program optimization as a search problem and invent heuristics to prune the search [15, 30]. <p> Quantifying performance: We employ counting arguments similar to <ref> [21, 13, 11, 1, 26] </ref> to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. <p> Quantifying performance: We employ counting arguments similar to [21, 13, 11, 1, 26] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as <ref> [21, 11, 1] </ref> give methods for choosing tile size in a nested loop; [21] uses a "fits-in" constraint based only on memory capacity (not block size), [11]'s "fits-in" constraint does not fully utilize block size information, and [1] limits block size to one. <p> As the length of the longest dependent chain is a function of the shape of the iteration space, a hierarchical strategy may define shorter dependence chains than a one-level strategy. So, in choosing tile shapes and sizes for each level independently, previous work such as <ref> [28, 11] </ref> misses out on an opportunity for performance gains. We illustrate the importance of considering all levels of the memory hierarchy for the architectural scenario in Figure 1 (c).
Reference: 12. <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part I, one-dimensional time. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 21(5), </volume> <month> October </month> <year> 1992. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality [28, 5, 7, 6] and exploit parallelism <ref> [2, 29, 12, 18, 22] </ref>. For instance, a tiling for cache may consider only cache size, cache line size, and cache associativity [11]. However, recent trends towards greater architectural complexity have increased the amount of information available to an optimizing compiler.
Reference: 13. <author> Jeanne Ferrante, Vivek Sarkar, and Wendy Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <booktitle> In Proceedings of the Fourth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Quantifying performance: We employ counting arguments similar to <ref> [21, 13, 11, 1, 26] </ref> to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy.
Reference: 14. <author> Dennis Gannon, William Jalby, and Kyle Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5), </volume> <month> October </month> <year> 1988. </year>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 31, 32, 21, 24, 28, 29, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 15. <author> Dennis Gannon and Ko-Yang Wang. </author> <title> Applying AI Techniques to Program Optimization for Parallel Computers, chapter 12. </title> <publisher> McGraw Hill Co., </publisher> <year> 1989. </year>
Reference-contexts: Researchers have tried several solutions to this information expansion problem. Many have simply ignored the multi-level information, instead relying on one-level cost functions [28, 11, 26]. Others rephrase program optimization as a search problem and invent heuristics to prune the search <ref> [15, 30] </ref>. We suggest a different solution which first formulates the system to be optimized by quantifying both the effects of tiling choices and the interactions between such choices, and then proceeds to minimize these formulas. <p> The work in [27] incorporates a larger set of transformations and unifies the transformation legality checks. AI search techniques on a decision tree of possible optimizations may find a good sequence of transformations to parallelize a given program <ref> [15] </ref>. None of these works seeks to unify guidance for multiple levels of the memory hierarchy. Multi-level unification: Unroll-and-jam can guide locality and instruction level parallelism in concert [4]. Loop fusion and distribution affect both parallelism and locality [20].
Reference: 16. <author> Karin Hogstedt, Larry Carter, and Jeanne Ferrante. </author> <title> Calculating the idle time of a tiling. </title> <booktitle> In Proceedings of Principles of Programming Languages, </booktitle> <year> 1997. </year>
Reference-contexts: In particular, for architectures with two levels of parallelism, tiling optimizations which do not consider the characteristics of both levels in concert may not perform optimally. Due to data dependences, the features of a tiling determine the time spent synchronizing or waiting for data (idle time) <ref> [16] </ref>. Different tile shapes and sizes lead to different dependences between the tiles. These tile dependences arise from data dependences that cross tile boundaries. We can imagine linking together dependent tiles into chains. The length of the longest such chain determines the execution time for the iteration space. <p> A good optimization strategy chooses h 3 and 3 to minimize total processor idle time and consequently (as we show) total execution time. When tiling for multiple levels of parallelism, different optimization strategies can determine the parameters of each of the tilings. Based on the theoretical work in <ref> [16] </ref>, we derive three sensible strategies. Each minimization in the first two strategies uses characteristics of only a single level. The first one-level strategy at level i chooses tile characteristics h i and i as an atomic step in the optimization process. <p> Proof. Both the exact form and derivation of the idle time function appear in <ref> [16] </ref>. ut 15 The theoretical basis for this work parameterizes a tiling by height and a concept called rise [16]. <p> Proof. Both the exact form and derivation of the idle time function appear in <ref> [16] </ref>. ut 15 The theoretical basis for this work parameterizes a tiling by height and a concept called rise [16].
Reference: 17. <author> Fran~cois Irigoin and Remi Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proceedings of the 15th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 319-328, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 31, 32, 21, 24, 28, 29, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 18. <author> Wayne Kelly and William Pugh. </author> <title> A unifying framework for iteration reordering transformations. </title> <booktitle> In Proceedings of IEEE First International Conference on Algorithms and Architectures for Parallel Processing, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality [28, 5, 7, 6] and exploit parallelism <ref> [2, 29, 12, 18, 22] </ref>. For instance, a tiling for cache may consider only cache size, cache line size, and cache associativity [11]. However, recent trends towards greater architectural complexity have increased the amount of information available to an optimizing compiler.
Reference: 19. <author> Ken Kennedy and Kathryn S. McKinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 31, 32, 21, 24, 28, 29, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 20. <author> Ken Kennedy and Kathryn S. McKinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <booktitle> In Proceedings of the Fourth International Workshop on Languages and, </booktitle> <year> 1993. </year>
Reference-contexts: None of these works seeks to unify guidance for multiple levels of the memory hierarchy. Multi-level unification: Unroll-and-jam can guide locality and instruction level parallelism in concert [4]. Loop fusion and distribution affect both parallelism and locality <ref> [20] </ref>. These two works do not directly address tiling or the multi-level nature of the interactions. Rather than use multi-level cost functions, [30] performs a pruned search on the space of possible combinations of minimizations of one-level cost functions.
Reference: 21. <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In ASPLOS-IV, </booktitle> <address> Palo Alto, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 31, 32, 21, 24, 28, 29, 3, 19] </ref> can improve both data locality and parallel execution time. <p> Quantifying performance: We employ counting arguments similar to <ref> [21, 13, 11, 1, 26] </ref> to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. <p> Quantifying performance: We employ counting arguments similar to [21, 13, 11, 1, 26] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as <ref> [21, 11, 1] </ref> give methods for choosing tile size in a nested loop; [21] uses a "fits-in" constraint based only on memory capacity (not block size), [11]'s "fits-in" constraint does not fully utilize block size information, and [1] limits block size to one. <p> No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as [21, 11, 1] give methods for choosing tile size in a nested loop; <ref> [21] </ref> uses a "fits-in" constraint based only on memory capacity (not block size), [11]'s "fits-in" constraint does not fully utilize block size information, and [1] limits block size to one.
Reference: 22. <author> Daniel Lavery and Wen-mei Hwu. </author> <title> Unrolling-based optimizations for modulo scheduling. </title> <booktitle> In Proceedings of the 28th International Symposium on Microarchitecture, </booktitle> <pages> pages 126-141, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality [28, 5, 7, 6] and exploit parallelism <ref> [2, 29, 12, 18, 22] </ref>. For instance, a tiling for cache may consider only cache size, cache line size, and cache associativity [11]. However, recent trends towards greater architectural complexity have increased the amount of information available to an optimizing compiler.
Reference: 23. <author> David A. Padua and Michael J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: in quantify the difference in results of using one-level and versus multi-level cost functions. 2 Background Given a loop nest, the Iteration Space Graph (ISG) [24] is a directed acyclic graph whose nodes represent the initial values and computations in the loop body, and whose edges represent data 1 dependences <ref> [23] </ref>. for the tiled program. The loop nest gives an order for executing the nodes of the ISG. Tiling [14, 17, 31, 32, 21, 24, 28, 29, 3, 19] can improve both data locality and parallel execution time.
Reference: 24. <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling multidimensional iteration spaces for nonshared memory machines. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: In Section 3, we consider several kernel codes and three different architectural scenarios (summarized in quantify the difference in results of using one-level and versus multi-level cost functions. 2 Background Given a loop nest, the Iteration Space Graph (ISG) <ref> [24] </ref> is a directed acyclic graph whose nodes represent the initial values and computations in the loop body, and whose edges represent data 1 dependences [23]. for the tiled program. The loop nest gives an order for executing the nodes of the ISG. <p> The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 31, 32, 21, 24, 28, 29, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 25. <author> Rafael H. Saavedra, Weihua Mao, Daeyeon Park, Jacqueline Chame, and Sungo Moon. </author> <title> The combined effectiveness of unimodular transformations, tiling, and software prefetching. </title> <booktitle> In Proceedings of International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: Any program written for the simulator may be parametrized by characteristics of the target architecture, such as C k and S k . and i k for each module k. 12 Note that Saavedra's observations <ref> [25] </ref> that multiple levels of tiling can have excessive overhead do not apply to our approach, since he used a nine-deep loop nest. 13 These formulas apply to any level k of the memory hierarchy for which we have considered the characteristics of two consecutive levels. 9 We simulated the three
Reference: 26. <author> Vivek Sarkar, Guang R. Gao, and Shaohua Han. </author> <title> Locality analysis for distributed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Fourth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1996. </year>
Reference-contexts: Researchers have tried several solutions to this information expansion problem. Many have simply ignored the multi-level information, instead relying on one-level cost functions <ref> [28, 11, 26] </ref>. Others rephrase program optimization as a search problem and invent heuristics to prune the search [15, 30]. <p> Quantifying performance: We employ counting arguments similar to <ref> [21, 13, 11, 1, 26] </ref> to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy.
Reference: 27. <author> Vivek Sarkar and Radhika Thekkath. </author> <title> A general framework for iteration-reordering loop transformations (Technical Summary). </title> <booktitle> In Proceedings of Programming Language Design and Implementation, </booktitle> <year> 1992. </year>
Reference-contexts: Single-level unification: Unimodular transformations can guide loop transformations for locality [28] and parallelism [29]. These works unify only improvement-enabling transformations such as skewing, interchange, and reversal, and do not consider locality and parallelism in concert. The work in <ref> [27] </ref> incorporates a larger set of transformations and unifies the transformation legality checks. AI search techniques on a decision tree of possible optimizations may find a good sequence of transformations to parallelize a given program [15]. <p> Our aim is to provide a system whereby a compiler (or human) may consider (quantitatively) the combined effect of memory hierarchy optimizations. In this sense, our work is in a similar vein to the "first-class" transformation representation of unimodular matrices and <ref> [27] </ref>. 3 Multi-level cost functions yield better performance architectural scenario instance superscalar TLB clustered SMP section 3.1 3.2 3.3 (a) (b) (c) Fig. 1. Three architectural features identified as having deleterious tiling interactions. Higher modules represent larger, slower memory units; lower modules are smaller and faster caches or processors.
Reference: 28. <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality <ref> [28, 5, 7, 6] </ref> and exploit parallelism [2, 29, 12, 18, 22]. For instance, a tiling for cache may consider only cache size, cache line size, and cache associativity [11]. However, recent trends towards greater architectural complexity have increased the amount of information available to an optimizing compiler. <p> Researchers have tried several solutions to this information expansion problem. Many have simply ignored the multi-level information, instead relying on one-level cost functions <ref> [28, 11, 26] </ref>. Others rephrase program optimization as a search problem and invent heuristics to prune the search [15, 30]. <p> In this paper, we present evidence to support two claims about cost functions - One-level cost functions may not globally optimize. We show that for the tiling transformation <ref> [31, 28] </ref>, tiling for a single level using a one-level cost function leads to a globally suboptimal choice. <p> The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 31, 32, 21, 24, 28, 29, 3, 19] </ref> can improve both data locality and parallel execution time. <p> Single-level unification: Unimodular transformations can guide loop transformations for locality <ref> [28] </ref> and parallelism [29]. These works unify only improvement-enabling transformations such as skewing, interchange, and reversal, and do not consider locality and parallelism in concert. The work in [27] incorporates a larger set of transformations and unifies the transformation legality checks. <p> As the length of the longest dependent chain is a function of the shape of the iteration space, a hierarchical strategy may define shorter dependence chains than a one-level strategy. So, in choosing tile shapes and sizes for each level independently, previous work such as <ref> [28, 11] </ref> misses out on an opportunity for performance gains. We illustrate the importance of considering all levels of the memory hierarchy for the architectural scenario in Figure 1 (c).
Reference: 29. <author> Michael E. Wolf and Monica S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality [28, 5, 7, 6] and exploit parallelism <ref> [2, 29, 12, 18, 22] </ref>. For instance, a tiling for cache may consider only cache size, cache line size, and cache associativity [11]. However, recent trends towards greater architectural complexity have increased the amount of information available to an optimizing compiler. <p> The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 31, 32, 21, 24, 28, 29, 3, 19] </ref> can improve both data locality and parallel execution time. <p> Single-level unification: Unimodular transformations can guide loop transformations for locality [28] and parallelism <ref> [29] </ref>. These works unify only improvement-enabling transformations such as skewing, interchange, and reversal, and do not consider locality and parallelism in concert. The work in [27] incorporates a larger set of transformations and unifies the transformation legality checks.
Reference: 30. <author> Michael E. Wolf, Dror Maydan, and Ding-Kai Chen. </author> <title> Combining loop transformations considering caches and scheduling. </title> <booktitle> In Proceedings of the 29th International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: Researchers have tried several solutions to this information expansion problem. Many have simply ignored the multi-level information, instead relying on one-level cost functions [28, 11, 26]. Others rephrase program optimization as a search problem and invent heuristics to prune the search <ref> [15, 30] </ref>. We suggest a different solution which first formulates the system to be optimized by quantifying both the effects of tiling choices and the interactions between such choices, and then proceeds to minimize these formulas. <p> Multi-level unification: Unroll-and-jam can guide locality and instruction level parallelism in concert [4]. Loop fusion and distribution affect both parallelism and locality [20]. These two works do not directly address tiling or the multi-level nature of the interactions. Rather than use multi-level cost functions, <ref> [30] </ref> performs a pruned search on the space of possible combinations of minimizations of one-level cost functions. It is not clear whether this method of extending to multiple levels is equivalent to a multi-level cost function. <p> In general, such a table can be calculated for compile-time use from the computation structure of the body of the loop, given the details of the underlying processor. In fact, scheduling optimizations such as <ref> [30] </ref> typically construct such tables. Note that as the number of planes increases, the rate of execution time improvement decreases and can even become negative. A compiler that only considers ILP would choose the minimum number of planes, in this case p = 3, which realizes the minimum execution time. <p> In this simple case, using the cache-specific cost function or the ILP-specific cost function did not yield the best solution, which is found by minimizing the sum of the two cost level-specific functions. To end this section, we consider some related work <ref> [30] </ref>. That paper recognizes the interdependence of optimization choices and uses a search procedure to take into account many of these interactions. However, the search procedure is pruned by making decisions for ILP before decisions for tiling for cache locality. <p> Factors such as the associativity of the TLB and the alignment of the matrix can influence whether A remains resident; the determination of these factors is beyond the scope of this paper. Instead, like <ref> [30] </ref>, we use the estimated effective cache size. In particular, the rest of this section assumes fully associative cache 9 . In this case we assume that the A submatrix will reside in the TLB with the assumption B t (H; W ) :75C t .
Reference: 31. <author> Michael J. Wolfe. </author> <title> Iteration space tiling for memory hierarchies. </title> <booktitle> In Parallel Processing for Scientific Computing, </booktitle> <pages> pages 357-361, </pages> <year> 1987. </year>
Reference-contexts: In this paper, we present evidence to support two claims about cost functions - One-level cost functions may not globally optimize. We show that for the tiling transformation <ref> [31, 28] </ref>, tiling for a single level using a one-level cost function leads to a globally suboptimal choice. <p> The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 31, 32, 21, 24, 28, 29, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 32. <author> Michael J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <pages> pages 655-664, </pages> <year> 1989. </year> <month> 15 </month>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 31, 32, 21, 24, 28, 29, 3, 19] </ref> can improve both data locality and parallel execution time.
References-found: 32

