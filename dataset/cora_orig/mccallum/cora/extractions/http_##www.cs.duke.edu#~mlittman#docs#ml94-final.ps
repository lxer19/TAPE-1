URL: http://www.cs.duke.edu/~mlittman/docs/ml94-final.ps
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: 
Email: mlittman@cs.brown.edu  
Title: Markov games as a framework for multi-agent reinforcement learning  
Author: Michael L. Littman 
Address: Providence, RI 02912-1910  
Affiliation: Brown University Bellcore Department of Computer Science Brown University  
Abstract: In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.
Abstract-found: 1
Intro-found: 1
Reference: [ Barto et al., 1989 ] <author> Barto, A. G.; Sutton, R. S.; and Watkins, C. J. C. H. </author> <year> 1989. </year> <title> Learning and sequential decision making. </title> <type> Technical Report 89-95, </type> <institution> Department of Computer and Information Science, University of Massachusetts, Amherst, Massachusetts. </institution> <note> Also published in Learning and Computational Neuroscience: Foundations of Adaptive Networks, </note> <editor> Michael Gabriel and John Moore, editors. </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1991. </year>
Reference-contexts: Reinforcement learning is a promising technique for creating agents that co-exist [ Tan, 1993, Yanco and Stein, 1993 ] , but the mathematical framework that justifies it is inappropriate for multi-agent environments. The theory of Markov Decision Processes (MDP's) <ref> [ Barto et al., 1989, Howard, 1960 ] </ref> , which underlies much of the recent work on reinforcement learning, assumes that the agent's environment is stationary and as such contains no other adaptive agents.
Reference: [ Bertsekas, 1987 ] <author> Bertsekas, D. P. </author> <year> 1987. </year> <title> Dynamic Pro gramming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: The method of value iteration starts with estimates for Q and V and generates new estimates by treating the equal signs in Equations 1-2 as assignment operators. It can be shown that the estimated values for Q and V converge to their true values <ref> [ Bertsekas, 1987 ] </ref> . 4.3 MARKOV GAMES Given Q (s; a), an agent can maximize its reward using the greedy strategy of always choosing the action with the highest Q-value.
Reference: [ Boyan, 1992 ] <author> Boyan, Justin A. </author> <year> 1992. </year> <title> Modular neural net works for learning context-dependent game strategies. </title> <type> Master's thesis, </type> <institution> Department of Engineering and Computer Laboratory, University of Cambridge, </institution> <address> Cambridge, England. </address>
Reference-contexts: Simultaneously training two adaptive agents using Q-learning is not mathematically justified and in practice is prone to locking up, that is, reaching a mutual local maximum in which both agents stop learning prematurely (see, e.g., <ref> [ Boyan, 1992 ] </ref> ). In spite of this, some researchers have reported amazing success with this approach [ Tesauro, 1992, Boyan, 1992 ] and it seemed to have been successful in this instance as well. <p> In spite of this, some researchers have reported amazing success with this approach <ref> [ Tesauro, 1992, Boyan, 1992 ] </ref> and it seemed to have been successful in this instance as well. The third experiment was intended to measure the worst case performance of each of the policies. The learned policies were held fixed while a challenger was trained to beat them. <p> Iterative methods are also quite promising since the relevant linear programs change slowly over time. For most applications of reinforcement learning to zero-sum games, this is not an impediment. Games such as checkers [ Samuel, 1959 ] , tic-tac-toe <ref> [ Boyan, 1992 ] </ref> , backgammon [ Tesauro, 1992 ] , and Go [ Schraudolph et al., 1994 ] consist of a series of alternating moves and in such games the minimax operator can be implemented extremely efficiently.
Reference: [ Heger, 1994 ] <author> Heger, </author> <title> Matthias 1994. Consideration of risk in reinforcement learning. </title> <booktitle> In Proceedings of the Machine Learning Conference. To appear. </booktitle>
Reference-contexts: The use of the minimax criterion and probabilistic policies is closely connected to other current research. First, a minimax criterion can be used in single-agent environments to produce more risk-averse behavior <ref> [ Heger, 1994 ] </ref> . Here, the random transitions of the environment play the role of the opponent. Secondly, probabilistic policies have been used in the context of acting optimally in environments where the agent's perception is incomplete [ Singh et al., 1994 ] .
Reference: [ Howard, 1960 ] <author> Howard, Ronald A. </author> <year> 1960. </year> <title> Dynamic Pro gramming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Massachusetts. </address>
Reference-contexts: Reinforcement learning is a promising technique for creating agents that co-exist [ Tan, 1993, Yanco and Stein, 1993 ] , but the mathematical framework that justifies it is inappropriate for multi-agent environments. The theory of Markov Decision Processes (MDP's) <ref> [ Barto et al., 1989, Howard, 1960 ] </ref> , which underlies much of the recent work on reinforcement learning, assumes that the agent's environment is stationary and as such contains no other adaptive agents. <p> Only the specific case of two-player zero-sum games is addressed, but even in this restricted version there are insights that can be applied to open questions in the field of reinforcement learning. 2 DEFINITIONS An MDP <ref> [ Howard, 1960 ] </ref> is defined by a set of states, S, and actions, A.
Reference: [ Owen, 1982 ] <author> Owen, </author> <title> Guillermo 1982. Game Theory: Sec ond edition. </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida. </address>
Reference-contexts: A discount factor, 0 fl &lt; 1 controls how much effect future rewards have on the optimal decisions, with small values of fl emphasizing near-term gain and larger values giving significant weight to later rewards. In its general form, a Markov game, sometimes called a stochastic game <ref> [ Owen, 1982 ] </ref> , is defined by a set of states, S, and a collection of action sets, A 1 ; : : : ; A k , one for each agent in the environment. <p> It is possible to define a notion of undiscounted rewards [ Schwartz, 1993 ] , but not all Markov games have optimal strategies in the undiscounted case <ref> [ Owen, 1982 ] </ref> . This is because, in many games, it is best to postpone risky actions indefinitely. <p> the quality of action a against action o in state s is Q (s; a; o) = R (s; a; o) + fl s 0 The resulting recursive equations look much like Equations 1-2 and indeed the analogous value iteration algorithm can be shown to converge to the correct values <ref> [ Owen, 1982 ] </ref> . It is worth noting that in games with alternating turns, the value function need not be computed by linear programming since there is an optimal deterministic policy.
Reference: [ Samuel, 1959 ] <author> Samuel, A. L. </author> <year> 1959. </year> <title> Some studies in ma chine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development 3 </journal> <pages> 211-229. </pages> <note> Reprinted in E. </note> <editor> A. Feigenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York 1963. </address>
Reference-contexts: It is possible that approximate solutions to the linear programs would suffice. Iterative methods are also quite promising since the relevant linear programs change slowly over time. For most applications of reinforcement learning to zero-sum games, this is not an impediment. Games such as checkers <ref> [ Samuel, 1959 ] </ref> , tic-tac-toe [ Boyan, 1992 ] , backgammon [ Tesauro, 1992 ] , and Go [ Schraudolph et al., 1994 ] consist of a series of alternating moves and in such games the minimax operator can be implemented extremely efficiently.
Reference: [ Schraudolph et al., 1994 ] <author> Schraudolph, Nicol N.; Dayan, Peter; and Sejnowski, Terrence J. </author> <year> 1994. </year> <title> Using the td(lambda) algorithm to learn an evaluation function for the game of go. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA. </address> <note> Morgan Kaufman. To appear. </note>
Reference-contexts: For most applications of reinforcement learning to zero-sum games, this is not an impediment. Games such as checkers [ Samuel, 1959 ] , tic-tac-toe [ Boyan, 1992 ] , backgammon [ Tesauro, 1992 ] , and Go <ref> [ Schraudolph et al., 1994 ] </ref> consist of a series of alternating moves and in such games the minimax operator can be implemented extremely efficiently.
Reference: [ Schwartz, 1993 ] <author> Schwartz, </author> <title> Anton 1993. A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, Massachusetts. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 298-305. </pages>
Reference-contexts: As in MDP's, the discount factor, fl, can be thought of as the probability that the game will be allowed to continue after the current move. It is possible to define a notion of undiscounted rewards <ref> [ Schwartz, 1993 ] </ref> , but not all Markov games have optimal strategies in the undiscounted case [ Owen, 1982 ] . This is because, in many games, it is best to postpone risky actions indefinitely.
Reference: [ Singh et al., 1994 ] <author> Singh, Satinder Pal; Jaakkola, Tommi; and Jordan, Michael I. </author> <year> 1994. </year> <title> Model-free reinforcement learning for non-markovian decision problems. </title> <booktitle> In Proceedings of the Machine Learning Conference. To appear. </booktitle>
Reference-contexts: Here, the random transitions of the environment play the role of the opponent. Secondly, probabilistic policies have been used in the context of acting optimally in environments where the agent's perception is incomplete <ref> [ Singh et al., 1994 ] </ref> . In these environments, random actions are used to combat the agent's uncertainty as to the true state of its environment much as random actions in games help deal with the agent's uncertainty of the opponent's move.
Reference: [ Strang, 1980 ] <author> Strang, </author> <title> Gilbert 1980. Linear Algebra and its applications: second edition. </title> <publisher> Academic Press, </publisher> <address> Or-lando, Florida. </address>
Reference-contexts: The inequalities in Table 2, with 0, constrain the components of to represent exactly those policies any solution to the inequalities would suffice. For to be optimal, we must identify the largest V for which there is some value of that makes the constraints hold. Linear programming (see, e.g., <ref> [ Strang, 1980 ] </ref> ) is a general technique for solving problems of this kind. In this example, linear programming finds a value of 0 for V and (1/3, 1/3, 1/3) for .
Reference: [ Tan, 1993 ] <author> Tan, M. </author> <year> 1993. </year> <title> Multi-agent reinforcement learning: independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, Massachusetts. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: 1 INTRODUCTION No agent lives in a vacuum; it must interact with other agents to achieve its goals. Reinforcement learning is a promising technique for creating agents that co-exist <ref> [ Tan, 1993, Yanco and Stein, 1993 ] </ref> , but the mathematical framework that justifies it is inappropriate for multi-agent environments.
Reference: [ Tesauro, 1992 ] <author> Tesauro, G. J. </author> <year> 1992. </year> <title> Practical issues in temporal difference. </title> <editor> In Moody, J. E.; Lippman, D. S.; and Hanson, S. J., editors 1992, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Mateo, CA. </address> <publisher> Mor-gan Kaufman. </publisher> <pages> 259-266. </pages>
Reference-contexts: In spite of this, some researchers have reported amazing success with this approach <ref> [ Tesauro, 1992, Boyan, 1992 ] </ref> and it seemed to have been successful in this instance as well. The third experiment was intended to measure the worst case performance of each of the policies. The learned policies were held fixed while a challenger was trained to beat them. <p> Iterative methods are also quite promising since the relevant linear programs change slowly over time. For most applications of reinforcement learning to zero-sum games, this is not an impediment. Games such as checkers [ Samuel, 1959 ] , tic-tac-toe [ Boyan, 1992 ] , backgammon <ref> [ Tesauro, 1992 ] </ref> , and Go [ Schraudolph et al., 1994 ] consist of a series of alternating moves and in such games the minimax operator can be implemented extremely efficiently.
Reference: [ Van Der Wal, 1981 ] <author> Van Der Wal, J. </author> <year> 1981. </year> <title> Stochastic dy namic programming. In Mathematical Centre Tracts 139. </title> <publisher> Morgan Kaufmann, Amsterdam. </publisher> [ <editor> von Neumann and Morgenstern, 1947 ] von Neumann, J. and Morgenstern, O. </editor> <year> 1947. </year> <title> Theory of Games and Economic Behavior. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey. </address>
Reference-contexts: The theory of games [ von Neumann and Morgenstern, 1947 ] is explicitly designed for reasoning about multi-agent systems. Markov games (see e.g., <ref> [ Van Der Wal, 1981 ] </ref> ) is an extension of game theory to MDP-like environments. This paper considers the consequences of using the Markov game framework in place of MDP's in reinforcement learning.
Reference: [ Watkins and Dayan, 1992 ] <author> Watkins, C. J. C. H. and Dayan, P. </author> <year> 1992. </year> <title> Q-learning. </title> <booktitle> Machine Learning 8(3) </booktitle> <pages> 279-292. </pages>
Reference-contexts: This learning rule converges to the correct values for Q and V , assuming that every action is tried in every state infinitely often and that new estimates are blended with previous ones using a slow enough exponentially weighted average <ref> [ Watkins and Dayan, 1992 ] </ref> . It is straightforward, though seemingly novel, to apply the same technique to solving Markov games. A completely specified version of the algorithm is given in Figure 1.
Reference: [ Watkins, 1989 ] <author> Watkins, C. J.C.H. </author> <year> 1989. </year> <title> Learning with Delayed Rewards. </title> <type> Ph.D. Dissertation, </type> <institution> Cambridge University. </institution>
Reference-contexts: In this case we can write V (s) = max a min o Q (s; a; o). 5 LEARNING OPTIMAL POLICIES Traditionally, solving an MDP using value iteration involves applying Equations 1-2 simultaneously over all s 2 S. Watkins <ref> [ Watkins, 1989 ] </ref> proposed an alternative approach that involves performing the updates asynchronously without the use of the transition function, T .
Reference: [ Yanco and Stein, 1993 ] <author> Yanco, Holly and Stein, </author> <title> Lynn An drea 1993. An adaptive communication protocol for cooperating mobile robots. </title> <editor> In Meyer, Jean-Arcady; Roit-blat, H. L.; and Wilson, Stewart W., editors 1993, </editor> <booktitle> From Animals to Animats: Proceedings of the Second International Conference on the Simultion of Adaptive Behavior. </booktitle> <publisher> MIT Press/Bradford Books. </publisher> <pages> 478-485. </pages>
Reference-contexts: 1 INTRODUCTION No agent lives in a vacuum; it must interact with other agents to achieve its goals. Reinforcement learning is a promising technique for creating agents that co-exist <ref> [ Tan, 1993, Yanco and Stein, 1993 ] </ref> , but the mathematical framework that justifies it is inappropriate for multi-agent environments.
References-found: 17

