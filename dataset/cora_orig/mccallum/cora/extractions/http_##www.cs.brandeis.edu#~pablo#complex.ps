URL: http://www.cs.brandeis.edu/~pablo/complex.ps
Refering-URL: http://www.cs.brandeis.edu/~pablo/indexe.html
Root-URL: http://www.cs.brandeis.edu
Title: Complexity 1 Complexity measures for complex systems and complex objects Pablo Funes A certain extraordinarily
Author: Roger Penrose, 
Keyword: Merrian-Websters Collegiate Dictionary  
Abstract: Complex suggests the unavoidable result of a necessary combining and does not imply a fault or failure &lt;a complex recipe&gt;. Complicated applies to what offers great difficulty in understanding, solving, or explaining &lt;complicated legal procedures&gt;. 
Abstract-found: 1
Intro-found: 1
Reference: [C-T] <author> Cover, T. and Thomas, J. </author> <title> Elements of Information Theory, </title> <publisher> Wiley Interscience, </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [KO] <author> Kolmogorov, A. N. </author> <title> Three Approaches to the Quantitative Definition of Information. Problems of Information Theory 1, </title> <year> 1965. </year>
Reference-contexts: Now we will want to use the opposite argument: Your fractal may take a long time to compute, but theres not much more there than a very simple mathematical formula. Kolmogorov Complexity (AIC) Kolmogorov introduced this concept in the sixties <ref> [KO] </ref>. Although Cover and Thomas use the name Kolmogorov Complexity, other authors prefer a different name. Gell-Mann [GM] calls it simply Algorithmic Information Content (AIC). The reason is not only that also Chaitin [CH] and Solomonoff discovered the concept independently, but the conicting word Complexity.
Reference: [CH] <author> Chaitin, </author> <title> G.J. </title> <journal> Journal of the A.C.M. </journal> <volume> 22, </volume> <year> 1975. </year>
Reference-contexts: Kolmogorov Complexity (AIC) Kolmogorov introduced this concept in the sixties [KO]. Although Cover and Thomas use the name Kolmogorov Complexity, other authors prefer a different name. Gell-Mann [GM] calls it simply Algorithmic Information Content (AIC). The reason is not only that also Chaitin <ref> [CH] </ref> and Solomonoff discovered the concept independently, but the conicting word Complexity. Chaitin himself seems more comfortable with randomness [CH2]. We have already seen that it is desirable to separate the concepts of randomness and complexity. AIC is however the most powerful concept behind all this further discussion.
Reference: [BCSS] <author> Blum, L., Cucker, F., Shub, M. and Smale, S. </author> <title> Complexity and Real Computation. </title>
Reference-contexts: We are left with a common language, that of strings of symbols over a finite alphabet, that can be examined with available theoretical tools: Theory of computation, Shannons information theory, Kolmogorov complexity, etc. Some lines of research (see <ref> [BCSS] </ref>) might suggest that other approaches, are based in continuous instead of discrete models, are possible. On the rest of this discussion we will assume that such a symbolic coding is available. The observed entity is a string (x 1 ,...,x n ,...) of symbols.
Reference: [GR1] <author> Grassberger, P. </author> <title> Problems in Quantifying Self-generated Complexity, </title> <journal> Helvetica Physica Acta 62, </journal> <year> 1989. </year>
Reference-contexts: More often, they are complex in a similar sense to what has been discussed above about complex mathematical objects. In this paper we will follow mainly the work of german physicist Peter Grassberger in two papers, <ref> [GR1] </ref> and [GR2], where he addresses the problem of finding a general useful complexity measure for objects expressed as strings of numbers or binary digits. <p> Its AIC is constant because the program generating the automata is short, so this type of object has the property of being logically deep. Considering time, however, leas us to, question the validity of the principle of the Occams razor as just formulated ([C-T], page 161, <ref> [GR1] </ref>, page 499). Assuming that we observe a certain string, is it reasonable to assume that it was produced by the simplest program? If the simplest program is too slow, this is not the case.
Reference: [GR2] <author> Grassberger, P. </author> <title> Information and Complexity Measures in Dynamical Systems, Information Dynamics. </title> <publisher> Plenum press, </publisher> <year> 1990. </year>
Reference-contexts: More often, they are complex in a similar sense to what has been discussed above about complex mathematical objects. In this paper we will follow mainly the work of german physicist Peter Grassberger in two papers, [GR1] and <ref> [GR2] </ref>, where he addresses the problem of finding a general useful complexity measure for objects expressed as strings of numbers or binary digits.
Reference: [H-U] <author> Hopcroft, J. E. and Ullman, J. </author> <title> Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: In general we are interested in observing mea sures that can be observed as complexity rates, that is, limits of the form (1) Space and time complexity of algorithms The theory of computation has brought Computational Complexity Theory (see <ref> [H-U] </ref>). This theory classifies algorithms by their time and space requirements. A certain Turing Machine has space complexity S if for every input word of length n it scans at most S (n) cells of storage.
Reference: [G-L] <author> Gell-Mann, Murray and Lloyd, Seth. </author> <title> Information Measures, Effective COmplexity and Total Information. </title>
Reference-contexts: Without the aid of Shannons entropy theory, the best that Grassberger has found are the unsatisfactory concepts of logical depth and sophistication. Gell-Manns Effective Complexity Gell-Manns definition of Effective Complexity takes a different approach. Effective complexity is just Kolmogorov Complexity ([GM], page 56; <ref> [G-L] </ref>, page 12) but measured not on an observed phenomena but through a subjective interpretation of interest to the observer, that is, a model. A model is an algorithm for specifying a probability distribution over the observed data.
Reference: [GM] <author> Gell-Mann, Murray. </author> <title> The Quark and the Jaguar. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <year> 1994. </year>
Reference-contexts: Kolmogorov Complexity (AIC) Kolmogorov introduced this concept in the sixties [KO]. Although Cover and Thomas use the name Kolmogorov Complexity, other authors prefer a different name. Gell-Mann <ref> [GM] </ref> calls it simply Algorithmic Information Content (AIC). The reason is not only that also Chaitin [CH] and Solomonoff discovered the concept independently, but the conicting word Complexity. Chaitin himself seems more comfortable with randomness [CH2].
Reference: [CH2] <author> Chaitin, Gregory. </author> <title> Randomness and Mathematical Proof. </title>
Reference-contexts: Gell-Mann [GM] calls it simply Algorithmic Information Content (AIC). The reason is not only that also Chaitin [CH] and Solomonoff discovered the concept independently, but the conicting word Complexity. Chaitin himself seems more comfortable with randomness <ref> [CH2] </ref>. We have already seen that it is desirable to separate the concepts of randomness and complexity. AIC is however the most powerful concept behind all this further discussion.
Reference: [BE] <author> Bennet, C. H. </author> <booktitle> Emerging Syntheses in Science. Pines, </booktitle> <year> 1985. </year>
Reference-contexts: But an algorithm such as Ziv-Lempel, as well as any statistical criterion, will assign maximal entropy to the string. Logical Depth (Bennet) This notion was introduced by C. Bennet <ref> [BE] </ref>. The logical depth of a string is defined as the complexity (as defined in theory of computation) of the shortest program that produces it.
Reference: [K-A] <author> Koppel, M. and Atlan, H. </author> <title> Program-Length Complexity, Sophistication and Induction. </title> <year> 1987. </year>
Reference-contexts: When n becomes large, however, the smallest program will have approximately hn+c bits, where h is the entropy of the sequence and c the proper program length, or sophistication, as defined by Koppel and Atlan in <ref> [K-A] </ref>. fig. 3: Koppel and Atlans sophistication ([GR1], page 501) 3 - 3 - Program size Sophistication Complexity 9 What makes this measure interesting is the fact ([GR1]) that it quantifies the importance of rules or correlations in the sequence.
References-found: 12

