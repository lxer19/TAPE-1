URL: http://www.cs.berkeley.edu/~inderjit/public_papers/thesis.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~inderjit/
Root-URL: http://www.cs.berkeley.edu
Title: A New O(n 2 Algorithm for the Symmetric Tridiagonal Eigenvalue/Eigenvector Problem  
Author: by Inderjit Singh Dhillon 
Degree: B.Tech. (Indian Institute of Technology, Bombay) 1989 A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA, BERKELEY Committee in charge: Professor James W. Demmel, Chair Professor Beresford N. Parlett Professor Phil Colella  
Date: 1997  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Ham-marling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <note> LAPACK Users' Guide (second edition). SIAM, Philadelphia, </note> <year> 1995. </year> <pages> 324 pages. </pages>
Reference-contexts: We then discuss and compare existing methods of solving the resulting tridiagonal problem in Sections 2.2 through 2.6. Later, in Sections 2.7 and 2.8, we show how various issues that arise in implementing inverse iteration are handled in existing LAPACK <ref> [1] </ref> and EISPACK [128] software, and present some examples where they fail to deliver correct answers. Finally, we sketch our alternate approach on handling these issues in Section 2.9. 2.1 Background Eigenvalue computations arise in a rich variety of contexts. <p> A variety of methods exploit the tridiagonal structure to compute (2.1.1). Extensive research has led to plenty of software, especially in the linear algebra software libraries, EISPACK [128] and the more recent LAPACK <ref> [1] </ref>. We will examine existing algorithms and related software in the next section. We now briefly discuss the relative costs of the various components involved in solving the dense symmetric eigenproblem. <p> We now look in detail at two existing implementations of inverse iteration and see how they address the issues discussed in the previous section. EISPACK [128] and 22 LAPACK <ref> [1] </ref> are linear algebra software libraries that contain routines to solve various eigenvalue problems. <p> We hope to present more details in the near future [43]. 6.4 Numerical Results In this section, we present a numerical comparison between Algorithm Y and four other software routines for solving the symmetric tridiagonal eigenproblem that are included in the EISPACK [128] and LAPACK <ref> [1] </ref> libraries. These are 1. LAPACK INVIT : The LAPACK implementation of bisection and inverse iteration [88, 84, 87] (subroutines DSTEBZ and DSTEIN); 2. EISPACK INVIT : The EISPACK implementation of inverse iteration after finding the eigenvalues by bisection [118] (subroutine DSTEBZ from LAPACK followed by TINVIT from EISPACK); 3. <p> Such a strategy has been adopted to speed up the LAPACK implementation of bisection on some computer architectures <ref> [107, 1] </ref>. We say that such a strategy leads to a level-2 BLAS-like operation since it involves a quadratic amount of data and a quadratic amount of work [67]. We hope to resolve the above mentioned algorithmic enhancements in the near future. <p> We hope to resolve the above mentioned algorithmic enhancements in the near future. These should result in Algorithm Z, and be included in future releases of LAPACK <ref> [1] </ref> and and ScaLAPACK [22]. Finally, we remind the reader that the timing and accuracy results presented in this chapter must be considered to be transitory. Future improvements could lead to further increase in speed of our new O (n 2 ) algorithm.
Reference: [2] <author> ANSI/IEEE, </author> <title> New York. IEEE Standard for Binary Floating Point Arithmetic, </title> <address> Std 754-1985 edition, </address> <year> 1985. </year>
Reference-contexts: There is no need to spoil the inner loop with tests. It is no longer true that L + D + U + = J = U D L but equality does hold for all entries except for those at or adjacent to any infinite pivot. IEEE arithmetic <ref> [2] </ref> allows computer implementations to handle 1 and take advantage of this feature of tridiagonals. We now show that proceeding thus, it is meaningful to pick jfl r j = min k jfl k j, omit the rth equation and solve for z (r) even when triangular factorization breaks down. <p> The IEEE standard also specifies a Double-Extended precision format (sometimes referred to as "80-bit" arithmetic) and on some machines, these extra precise computations may be performed in hardware <ref> [2, 65] </ref>. Quadruple precision arithmetic is generally simulated in software [91, 120]. In order to get single precision accuracy, we may try and execute Algorithm X in double precision arithmetic. However, there are cases when this simple strategy will not work.
Reference: [3] <author> P. Arbenz, K. Gates, and Ch. Sprenger. </author> <title> A parallel implementation of the symmetric tridiagonal QR algorithm. </title> <booktitle> In Frontier's 92. </booktitle> <address> McLean, </address> <year> 1992. </year>
Reference-contexts: However, the O (n 3 ) computation in accumulating the Givens' rotations into the eigenvector matrix is trivially and efficiently parallelized, see <ref> [3] </ref> for more details. But the higher operation count and inability to exploit fast matrix-multiply based operations make the QR algorithm much slower than divide and conquer and also, slower on average than bisection followed by inverse iteration.
Reference: [4] <author> S. O. Asplund. </author> <title> Finite boundary value problems solved by Green's matrix. </title> <journal> Math. Scand., </journal> <volume> 7 </volume> <pages> 49-56, </pages> <year> 1959. </year>
Reference-contexts: This result has appeared in <ref> [4, 14, 83, 78, 42] </ref>. Lemma 3.3.2 Let J be a nonsingular unreduced tridiagonal matrix of order n.
Reference: [5] <author> I. Babuska. </author> <title> Numerical stability in problems of linear algebra. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 9 </volume> <pages> 53-77, </pages> <year> 1972. </year>
Reference-contexts: We will show some of these applications in this thesis, for other applications see [42, 115, 74]. In addition to the papers reviewed in [108], twisted factorizations have appeared in various contexts in the literature, see <ref> [77, 94, 5, 58, 130, 133, 44] </ref>. For a brief review, see Section 4.1 of [117].
Reference: [6] <author> Z. Bai, </author> <year> 1996. </year> <title> private communication. </title>
Reference-contexts: In fact, in the recent past, some people have turned to LAPACK INVIT instead of divide and conquer to solve their large problems solely for this reason <ref> [6, 53, 52] </ref>. Of course, 151 workspace of only about 5-10 n double precision words is required by Algorithm Y, and EISPACK and LAPACK INVIT. The QR method is seen to uniformly take O (n 3 ) time and is less sensitive to the eigenvalue distribution.
Reference: [7] <author> V. Bargmann, D. Montgomery, and J. von Neumann. </author> <title> Solution of linear systems of high order. Report prepared for Navy Bureau of Ordnance, </title> <booktitle> 1946. Reprinted in [131, </booktitle> <pages> pp. 421-477]. </pages>
Reference-contexts: There is no software available as yet that uses the fast multipole method for the eigenproblem. 2.5 Other Methods The oldest method for solving the symmetric eigenproblem is one due to Jacobi that dates back to 1846 [86], and was rediscovered by von Neumann and colleagues in 1946 <ref> [7] </ref>. Jacobi's method does not reduce the dense symmetric matrix A to tridiagonal form, as most other methods do, but instead works on A. It performs a sequence of plane rotations each of which annihilates an off-diagonal element (which is filled in during later steps).
Reference: [8] <author> J. Barlow, </author> <year> 1996. </year> <title> private communication. </title>
Reference-contexts: As we shall emphasize in later chapters, we will use Algorithm 3.2.1 only when j is sufficiently isolated. We have noted, and so has Jessie Barlow <ref> [8] </ref>, that a simple recurrence will yield all values of kz (k) k for O (n) operations. Consequently it would be feasible to minimize jfl k j=kz (k) k instead of jfl k j to obtain a possibly smaller residual norm.
Reference: [9] <author> J. Barlow and J. Demmel. </author> <title> Computing accurate eigensystems of scaled diagonally dominant matrices. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 27(3) </volume> <pages> 762-791, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Let ~ L + ffi ~ L = D T 1 ~ LD 2 , where D 1 and D 2 are nonsingular matrices. Then j 1 k kD 1 j + ffi j j kD 1 k kD 2 k: Corollary 4.3.1 (Barlow and Demmel <ref> [9, Thm. 1] </ref>, Deift et al. [29, Thm. 2.12], Demmel and Kahan [35, Cor. 2], Eisenstat and Ipsen [49, Cor. 4.2]). Let ~ L and ~ L + ffi ~ L be bidiagonal matrices as in (4.3.3) and (4.3.4).
Reference: [10] <author> D. Bernholdt and R. Harrison. </author> <title> Orbital invariant second order many-body perturbation on parallel computers: An approach for large molecules. </title> <journal> J. Chem. Physics., </journal> <year> 1995. </year>
Reference-contexts: In addition, the following symmetric tridiagonal matrices arise in certain quantum chemistry computations. For more details on these problems, the reader is referred to <ref> [10, 55] </ref>. 13) Biphenyl. This positive definite matrix with n = 966 occurs in the modeling of biphenyl using Mtller-Plesset theory. Most of its eigenvalues are quite small com pared to the norm. See Figure 6.1 for a plot of its eigenvalue distribution. 14) SiOSi 6 .
Reference: [11] <author> H. J. Bernstein and M. Goldstein. </author> <title> Parallel implementation of bisection for the calculation of eigenvalues of a tridiagonal symmetric matrices. </title> <journal> Computing, </journal> <volume> 37 </volume> <pages> 85-91, </pages> <year> 1986. </year>
Reference-contexts: However, bisection is slow if all the eigenvalues are needed. A faster root-finder, such as Zeroin [31, 13], speeds up computation when an eigenvalue is isolated in an interval. Multisection maintains the simplicity of bisection and in certain situations, can speed up the performance on a parallel machine <ref> [106, 11, 127] </ref>. When the k eigenvalues are well separated, inverse iteration can find the eigenvectors independently, each in O (n) time. However, to find eigenvectors of k close eigenvalues, all existing implementations resort to reorthogonalization and this costs O (nk 2 ) operations.
Reference: [12] <author> Christian H. Bischof and Charles F. Van Loan. </author> <title> The WY representation for products of Householder matrices. </title> <note> SIAM J. </note> <institution> Sci. Stat. Comput., 8(1):s2-s13, </institution> <year> 1987. </year>
Reference-contexts: The extent to which it is a bottleneck can be much greater than suggested by the above numbers because the other two phases, Householder reduction and back-transformation can exploit fast matrix-multiply based operations <ref> [12, 45] </ref>, whereas most algorithms for the tridiagonal problem are sequential in nature and/or cannot be expressed in terms of matrix multiplication.
Reference: [13] <author> R. P. Brent. </author> <title> Algorithms for minimization without derivatives. </title> <publisher> Prentice-Hall, </publisher> <year> 1973. </year> <month> 159 </month>
Reference-contexts: The bisection algorithm discussed in Section 2.3 is a reliable way to compute eigenvalues. However, it can be quite slow and there have been many attempts to find faster zero-finders such as the Rayleigh Quotient Iteration [110], Laguerre's method [90, 113] and the Zeroin scheme <ref> [31, 13] </ref>. These zero-finders can considerably speed up the computation of isolated eigenvalues but they seem to stumble when eigenvalues cluster. Homotopy methods for the symmetric eigenproblem were suggested by Chu in [23, 14 24]. <p> The bisection algorithm enables any subset of k eigenvalues to be computed with O (nk) operations. Each eigenvalue can be found independently and this makes it suitable for parallel computation. However, bisection is slow if all the eigenvalues are needed. A faster root-finder, such as Zeroin <ref> [31, 13] </ref>, speeds up computation when an eigenvalue is isolated in an interval. Multisection maintains the simplicity of bisection and in certain situations, can speed up the performance on a parallel machine [106, 11, 127].
Reference: [14] <author> B. Bukhberger and G.A. Emel'yanenko. </author> <title> Methods of inverting tridiagonal matrices. </title> <journal> USSR Computat. Math. and Math. Phy., </journal> <volume> 13 </volume> <pages> 10-20, </pages> <year> 1973. </year>
Reference-contexts: This result has appeared in <ref> [4, 14, 83, 78, 42] </ref>. Lemma 3.3.2 Let J be a nonsingular unreduced tridiagonal matrix of order n.
Reference: [15] <author> J. Bunch, P. Nielsen, and D. Sorensen. </author> <title> Rank-one modification of the symmetric eigenproblem. </title> <journal> Num. Math., </journal> <volume> 31 </volume> <pages> 31-48, </pages> <year> 1978. </year>
Reference-contexts: It is quite remarkable that this method can also be faster than other implementations on a serial computer! The matrix T may be expressed as a modification of a direct sum of two smaller tridiagonal matrices. This modification may be a rank-one update <ref> [15] </ref>, or may be obtained by crossing out a row and column of T [72]. The eigenproblem of T can then be solved in terms of the eigenproblems of the smaller tridiagonal matrices, and this may be done recursively.
Reference: [16] <author> J. Carrier, L. Greengard, and V. Rokhlin. </author> <title> A fast adaptive multipole algorithm for particle simulations. </title> <journal> SIAM, J. Sci. Stat. Comput., </journal> <volume> 9(4) </volume> <pages> 669-686, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: In [71, 73], Gu and Eisenstat show that by using the fast multipole method of Carrier, Greengard and Rokhlin <ref> [70, 16] </ref>, the complexity of solving the symmetric tridiagonal eigenproblem can be considerably lowered. All the eigenvalues and eigenvectors can be found in O (n 2 ) operations while all the eigenvalues can be computed in O (n log 2 n) operations.
Reference: [17] <author> S. Chakrabarti, J. Demmel, and K. Yelick. </author> <title> Modeling the benefits of mixed data and task parallelism. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <address> Santa Barbara, California, </address> <month> july </month> <year> 1995. </year>
Reference-contexts: This method was designed to work well on parallel computers, offering both task and data parallelism [46]. Efficient parallel implementations are not straightforward to program, and the decision to switch from task to data parallelism depends on the characteristics of the underlying machine <ref> [17] </ref>. Due to such complications, all the currently available parallel software libraries, such as ScaLAPACK [22] and PeIGS [52], use algorithms based on bisection and inverse iteration.
Reference: [18] <author> T. Chan. </author> <title> Rank revealing QR factorizations. </title> <journal> Lin. Alg. Appl., </journal> 88/89:67-82, 1987. 
Reference-contexts: This proves the existence of a column permutation of B such that the bottom element of R in B's QR decomposition is tiny when B is nearly singular. This result was proved by Chan in <ref> [18] </ref> and earlier by Golub, Klema and Stewart in [66]. Thus twisted factorizations can be rank-revealing. Rank-revealing LU and QR factorizations have been extensively studied and several algorithms to compute such fac torizations exist. Twisted factorizations seem to have been overlooked and may offer com putational advantages. <p> Rank-revealing LU and QR factorizations have been extensively studied and several algorithms to compute such fac torizations exist. Twisted factorizations seem to have been overlooked and may offer com putational advantages. We consider our results outlined above to be stronger than those of Chan <ref> [19, 18] </ref> and Golub et al. [66] since the permutations we consider are restricted. In particular, as seen in the tridiagonal and Hessenberg case, twisted factorizations respect the sparsity structure of the given matrix, and thus may offer computational advantages in terms of speed and accuracy.
Reference: [19] <author> T. F. Chan. </author> <title> On the existence and computation of LU-factorizations with small pivots. </title> <journal> Math. Comp., </journal> <volume> 42 </volume> <pages> 535-547, </pages> <year> 1984. </year>
Reference-contexts: A generalization of such a result was proved by Tony Chan in 1984 <ref> [19] </ref> (he considered non-normal B as well, and arbitrary row and column permutations of B). We briefly compare our results with Chan's results of [19] at the end of this section. <p> A generalization of such a result was proved by Tony Chan in 1984 <ref> [19] </ref> (he considered non-normal B as well, and arbitrary row and column permutations of B). We briefly compare our results with Chan's results of [19] at the end of this section. We now formally define a row twisted Q decomposition of B at position k as B = Q k N k (3.6.44) where Q k is orthogonal and N k is a permuted triangular matrix as made precise earlier in this section. <p> Rank-revealing LU and QR factorizations have been extensively studied and several algorithms to compute such fac torizations exist. Twisted factorizations seem to have been overlooked and may offer com putational advantages. We consider our results outlined above to be stronger than those of Chan <ref> [19, 18] </ref> and Golub et al. [66] since the permutations we consider are restricted. In particular, as seen in the tridiagonal and Hessenberg case, twisted factorizations respect the sparsity structure of the given matrix, and thus may offer computational advantages in terms of speed and accuracy.
Reference: [20] <author> S. Chandrasekaran. </author> <title> When is a Linear System Ill-Conditioned? PhD thesis, </title> <institution> Departement of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <year> 1994. </year>
Reference-contexts: Even when a very accurate eigenvalue ap proximation is available, the following may influence the choice of the shift when more than one eigenvector is desired. * The pairing problem. In <ref> [20] </ref>, Chandrasekaran gives a surprising example showing how inverse iteration can fail to give small residuals in exact arithmetic if the eigenvalues and eigenvectors are not paired up properly. We reproduce the example in Section 2.8. <p> Thus computed eigenvalues may not be accurate "enough" leading to the above failures surprisingly often. We give examples of such occurrences in Section 2.8.1. In response to the above problems, Chandrasekaran proposes a new version of inverse iteration that is considerably different from the EISPACK and LAPACK implementations in <ref> [20] </ref>. The differences include an alternate convergence criterion. <p> Surprisingly, as the following example shows, this implementation can fail to give small residual norms even in exact arithmetic by incorrectly pairing up the eigenvalues and eigenvectors. Example 2.8.1 [The Pairing Error.] (Chandrasekaran <ref> [20] </ref>) Let 1 be an arbitrary real number, and 2 = 1 + "; i+1 i = i 1 ; n 1; i = 2; : : :; n 1 where " is of the order of the machine precision. Explicitly, i = 1 + 2 i2 ". <p> To cure this problem, Chandrasekaran proposes that ^ i O (n"kAk) be used as the shifts for inverse iteration so that all shifts are guaranteed to lie to the left of the actual eigenvalues <ref> [20] </ref>. Neither EISPACK nor LAPACK do this `artificial' perturbation. The discerning reader will realize that the above problem is not the failure of the basic inverse iteration process. Iterates do converge to the closest eigenvector that is orthogonal to the eigenvectors computed earlier. <p> However, as we saw in earlier sections, the goal of orthogonality (1.1.2) can lead to a myriad of problems. Most of the "difficult" errors in the EISPACK and LAPACK implementations arise due to the explicit 33 orthogonalization of iterates when eigenvalues are close. In <ref> [20] </ref>, Chandrasekaran explains the theory behind some of these failures, and proposes an alternate version of inverse iteration that is provably correct. However, this version is more involved and potentially requires much more orthogonalization than in existing implementations.
Reference: [21] <author> Bruce W. Char, Keith O. Geddes, Gaston H. Gonnet, Benton L. Leong, Michael B. Monagan, and Stephen M. Watt. </author> <title> Maple V Library Reference Manual. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: + ffiv 1 = 2 6 4 " 5 " 1 p (1 + 3 " 1 p (1 2 ") + O (") 7 7 : 1 we artfully constructed this matrix to have the desired behavior which may be verified by using a symbol manipulator such as Maple <ref> [21] </ref> or Mathematica [137] 72 v 2 = 6 6 " p 2 ) + O (" 5=4 ) 2 p 2 ) + O (") 2 4 ) + O (" 3=2 ) 7 7 ; v 2 + ffiv 2 = 2 6 4 q 2 (1 p 2
Reference: [22] <author> J. Choi, J. Demmel, I. Dhillon, J. Dongarra, Ostrouchov, S., A. Petitet, K. Stanley, D. Walker, and R. C. Whaley. </author> <title> ScaLAPACK, a portable linear algebra library for distributed memory computers-design issues and performance. </title> <journal> Computer Physics Communications, </journal> <volume> 97(1-2):1-15, </volume> <year> 1996. </year>
Reference-contexts: Efficient parallel implementations are not straightforward to program, and the decision to switch from task to data parallelism depends on the characteristics of the underlying machine [17]. Due to such complications, all the currently available parallel software libraries, such as ScaLAPACK <ref> [22] </ref> and PeIGS [52], use algorithms based on bisection and inverse iteration. A drawback of the current divide and conquer software in LAPACK is that it needs extra workspace of more than 2n 2 floating point numbers, which can be prohibitively excessive for large problems. <p> We hope to resolve the above mentioned algorithmic enhancements in the near future. These should result in Algorithm Z, and be included in future releases of LAPACK [1] and and ScaLAPACK <ref> [22] </ref>. Finally, we remind the reader that the timing and accuracy results presented in this chapter must be considered to be transitory. Future improvements could lead to further increase in speed of our new O (n 2 ) algorithm.
Reference: [23] <author> M. Chu. </author> <title> A simple application of the homotopy method to symmetric eigenvalue problems. </title> <journal> Lin. Alg. Appl, </journal> <volume> 59 </volume> <pages> 85-90, </pages> <year> 1984. </year>
Reference-contexts: These zero-finders can considerably speed up the computation of isolated eigenvalues but they seem to stumble when eigenvalues cluster. Homotopy methods for the symmetric eigenproblem were suggested by Chu in <ref> [23, 14 24] </ref>. These methods start from an eigenvalue of a simpler matrix D and follow a smooth curve to find an eigenvalue of A (t) D + t (A D).
Reference: [24] <author> M. Chu. </author> <title> A note on the homotopy method for linear algebraic eigenvalue problems. </title> <journal> Lin. Alg. Appl, </journal> <volume> 105 </volume> <pages> 225-236, </pages> <year> 1988. </year>
Reference: [25] <author> J.J.M. Cuppen. </author> <title> A divide and conquer method for the symmetric tridiagonal eigenproblem. </title> <journal> Numer. Math., </journal> <volume> 36 </volume> <pages> 177-195, </pages> <year> 1981. </year>
Reference-contexts: However, when eigenvalues are close, current implementations can take up to 10n 3 operations due to the orthogonalization. 2.4 Divide and Conquer Methods In 1981, Cuppen proposed a solution to the symmetric tridiagonal eigenproblem that was meant to be efficient for parallel computation <ref> [25, 46] </ref>. It is quite remarkable that this method can also be faster than other implementations on a serial computer! The matrix T may be expressed as a modification of a direct sum of two smaller tridiagonal matrices.
Reference: [26] <author> C. Davis and W. Kahan. </author> <title> Some new bounds on perturbation of subspaces. </title> <journal> Bulletin of the American Mathematical Society, </journal> <volume> 77(4) </volume> <pages> 863-868, </pages> <month> July </month> <year> 1969. </year>
Reference-contexts: Then j sin 6 (v; y)j gap () where gap () = minfj- j : - 6= ; - 2 spectrum (A)g. The extension of this theorem to higher-dimensional subspaces is due to Davis and Kahan, see <ref> [26] </ref> and [27] for more details. 97 6 ? (u j ) Z ^ Z k (^v j ) dtwqds exact dtwqds computed change each d i by 1 ulp, 1 i &lt; k, l i by 3 ulps, 1 i &lt; k, d k by 4 ulps, l k by <p> Similarly, using the Davis-Kahan Sinfi theorem <ref> [26, 27] </ref> and the subspace theorems given in Section 4.3, it can be shown that j sin 6 (^v j ; U 1:j )j = reldist 2 ( j ; j+1 ) ; (4.5.58) O (n") reldist 2 ( j ; j1 ) ; (4.5.59) where U 1:j and U j:n <p> Proof. The proof is almost identical to that of the above Theorem 4.5.3 except that instead of applying Theorem 4.5.1 and Corollary 4.3.2 to get bounds on the angles between individual vectors, we apply the Davis-Kahan Sinfi Theorem <ref> [26, 27] </ref> and Corollary 4.3.3 to get similar bounds on the angles between corresponding subspaces. tu Theorem 4.5.5 Let ^ 2 j be the approximation used to compute z j by Step (4d) of Algorithm X, and r be such that (4.5.62) is satisfied.
Reference: [27] <author> C. Davis and W. Kahan. </author> <title> The rotation of eigenvectors by a perturbation III. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 7 </volume> <pages> 248-263, </pages> <year> 1970. </year>
Reference-contexts: Then j sin 6 (v; y)j gap () where gap () = minfj- j : - 6= ; - 2 spectrum (A)g. The extension of this theorem to higher-dimensional subspaces is due to Davis and Kahan, see [26] and <ref> [27] </ref> for more details. 97 6 ? (u j ) Z ^ Z k (^v j ) dtwqds exact dtwqds computed change each d i by 1 ulp, 1 i &lt; k, l i by 3 ulps, 1 i &lt; k, d k by 4 ulps, l k by 3 1 <p> Similarly, using the Davis-Kahan Sinfi theorem <ref> [26, 27] </ref> and the subspace theorems given in Section 4.3, it can be shown that j sin 6 (^v j ; U 1:j )j = reldist 2 ( j ; j+1 ) ; (4.5.58) O (n") reldist 2 ( j ; j1 ) ; (4.5.59) where U 1:j and U j:n <p> Proof. The proof is almost identical to that of the above Theorem 4.5.3 except that instead of applying Theorem 4.5.1 and Corollary 4.3.2 to get bounds on the angles between individual vectors, we apply the Davis-Kahan Sinfi Theorem <ref> [26, 27] </ref> and Corollary 4.3.3 to get similar bounds on the angles between corresponding subspaces. tu Theorem 4.5.5 Let ^ 2 j be the approximation used to compute z j by Step (4d) of Algorithm X, and r be such that (4.5.62) is satisfied.
Reference: [28] <author> D. Day. </author> <title> The differential qd algorithm for the tridiagonal eigenvalue problem. </title> <note> 1997. in preparation. 160 </note>
Reference-contexts: Before we do so, we emphasize that the particular qd-like transformations presented in this section are new. Similar qd recurrences have been studied by Rutishauser [121, 122, 123], Henrici [76], Fernando and Parlett [56], Yao Yang [138] and David Day <ref> [28] </ref>. 4.4.2 Roundoff Error Analysis First, we introduce our model of arithmetic.
Reference: [29] <author> P. Deift, J. Demmel, L.-C. Li, and C. Tomei. </author> <title> The bidiagonal singular values decomposition and Hamiltonian mechanics. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 28(5) </volume> <pages> 1463-1516, </pages> <month> October </month> <year> 1991. </year> <note> (LAPACK Working Note #11). </note>
Reference-contexts: In the rest of this chapter, we show how to exploit this property of bidiagonal matrices to compute numerically orthogonal eigenvectors without any explicit orthogonalization. The relative perturbation results for bidiagonal matrices mentioned above have appeared in <ref> [89, 29, 35, 49, 50, 51, 100, 101] </ref>. <p> Then j 1 k kD 1 j + ffi j j kD 1 k kD 2 k: Corollary 4.3.1 (Barlow and Demmel [9, Thm. 1], Deift et al. <ref> [29, Thm. 2.12] </ref>, Demmel and Kahan [35, Cor. 2], Eisenstat and Ipsen [49, Cor. 4.2]). Let ~ L and ~ L + ffi ~ L be bidiagonal matrices as in (4.3.3) and (4.3.4).
Reference: [30] <author> L. S. DeJong. </author> <title> Towards a formal definition of numerical stability. </title> <journal> Numer. Math., </journal> <volume> 28 </volume> <pages> 211-220, </pages> <year> 1977. </year>
Reference-contexts: As we mentioned above, this is not a pure backward error analysis. We have put small perturbations not only on the input but also on the output in order to obtain an exact dstqds transform. This property is called mixed stability in <ref> [30] </ref> but note that our perturbations are relative ones. Theorem 4.4.2 Let the dstqds transformation be computed as in Algorithm 4.4.3.
Reference: [31] <author> T. J. Dekker. </author> <title> Finding a zero by means of successive linear interpolation. </title> <editor> In B. Dejon and P. Henrici, editors, </editor> <booktitle> Constructive aspects of the fundamanetal theorem of algebra. </booktitle> <address> New York: </address> <publisher> Wiley-Interscience, </publisher> <year> 1969. </year>
Reference-contexts: The bisection algorithm discussed in Section 2.3 is a reliable way to compute eigenvalues. However, it can be quite slow and there have been many attempts to find faster zero-finders such as the Rayleigh Quotient Iteration [110], Laguerre's method [90, 113] and the Zeroin scheme <ref> [31, 13] </ref>. These zero-finders can considerably speed up the computation of isolated eigenvalues but they seem to stumble when eigenvalues cluster. Homotopy methods for the symmetric eigenproblem were suggested by Chu in [23, 14 24]. <p> The bisection algorithm enables any subset of k eigenvalues to be computed with O (nk) operations. Each eigenvalue can be found independently and this makes it suitable for parallel computation. However, bisection is slow if all the eigenvalues are needed. A faster root-finder, such as Zeroin <ref> [31, 13] </ref>, speeds up computation when an eigenvalue is isolated in an interval. Multisection maintains the simplicity of bisection and in certain situations, can speed up the performance on a parallel machine [106, 11, 127].
Reference: [32] <author> J. Demmel. </author> <title> The inherent inaccuracy of implicit tridiagonal QR. </title> <type> Technical Report Report 963, IMA, </type> <institution> University of Minnesota, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Is it not possible to use twisted factorizations to find the eigenvalues also? We believe that the latter might indeed be the right approach not only in speeding up the algorithm but also to get higher accuracy since the standard QR algorithm may violently rearrange the matrix thereby destroying accuracy <ref> [32] </ref>. However, this is not straightforward due to the complication that A 1 in (3.5.42) is not tridiagonal.
Reference: [33] <author> J. Demmel and W. Gragg. </author> <title> On computing accurate singular values and eigenvalues of acyclic matrices. </title> <journal> Lin. Alg. Appl., </journal> <volume> 185 </volume> <pages> 203-218, </pages> <year> 1993. </year>
Reference-contexts: Matrices that belong to this class are completely characterized by the sparsity pattern of their non-zeros and have been called the set of biacyclic matrices, a term coined by Demmel and Gragg in <ref> [33] </ref>. Besides bidiagonal matrices, the twisted factors introduced in Section 3.1 and triangular factors of arrowhead matrices (see [73, p.175]) belong to this class of matrices. Thus the theory developed in this section is applicable not only to a bidiagonal matrix but also to any biacyclic matrix.
Reference: [34] <author> J. Demmel, M. Gu, S. Eisenstat, I. Slapnicar, K. Veselic, and Z. Drmac. </author> <title> Computing the singular value decomposition with high relative accuracy. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-97-348, </type> <institution> University of Tennessee, Knoxville, </institution> <month> February </month> <year> 1997. </year> <note> (LAPACK Working Note #119, available electronically on netlib). </note>
Reference-contexts: = sign () n X ! m f (m) 2 l=k+1 (b) @b k b k = sign () k X ! i p (i) 2 j=1 n X v (m) 2 sign () n X ! l p (l) 2 : For more work of a similar flavor, see <ref> [34] </ref>. 5.2.2 Examples We now examine relative condition numbers of some factorizations T I = ~ L ~ L T ; where T is tridiagonal and ~ L is lower bidiagonal. In particular, we show that 120 i.
Reference: [35] <author> J. Demmel and W. Kahan. </author> <title> Accurate singular values of bidiagonal matrices. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11(5) </volume> <pages> 873-912, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: In the rest of this chapter, we show how to exploit this property of bidiagonal matrices to compute numerically orthogonal eigenvectors without any explicit orthogonalization. The relative perturbation results for bidiagonal matrices mentioned above have appeared in <ref> [89, 29, 35, 49, 50, 51, 100, 101] </ref>. <p> This result appears to 73 have lain neglected in the technical report [89] until Demmel and Kahan used it in 1990 to devise a method to compute the singular values of a bidiagonal matrix to high relative accuracy <ref> [35] </ref>. Subsequently, these results have been extended and simplified, and we cite some contributions during the course of this section. <p> Then j 1 k kD 1 j + ffi j j kD 1 k kD 2 k: Corollary 4.3.1 (Barlow and Demmel [9, Thm. 1], Deift et al. [29, Thm. 2.12], Demmel and Kahan <ref> [35, Cor. 2] </ref>, Eisenstat and Ipsen [49, Cor. 4.2]). Let ~ L and ~ L + ffi ~ L be bidiagonal matrices as in (4.3.3) and (4.3.4).
Reference: [36] <author> J. Demmel and A. McKenney. </author> <title> A test matrix generation suite. </title> <institution> Computer science dept. </institution> <type> technical report, </type> <institution> Courant Institute, </institution> <address> New York, NY, </address> <month> July </month> <year> 1989. </year> <note> (LAPACK Working Note #9). </note>
Reference-contexts: We now give an example where this perturbation is too big. As a result, the shifts used to compute the eigenvectors are quite different from the computed eigenvalues and prevent the convergence criterion from being attained. Example 2.8.2 [Excessive Perturbation.] Using LAPACK's test matrix generator <ref> [36] </ref>, we generated a 200 fi 200 tridiagonal matrix such that ^ 1 ^ 100 "; ^ 101 ^ 199 "; ^ n = 1 where " 1:2 fi 10 7 (this run was in single precision). kT k R = O (1), and the shift used by Tinvit to compute <p> + p 1 + (n 2) ", and the last eigenvalue at 2, i.e., 1 = "; i = 1 + (i 1) "; i = 2; : : : ; n 1; and n = 2: We generated matrices of the above type using the LAPACK test matrix generator <ref> [36] </ref>, which first forms a random dense symmetric matrix with the given spectrum and Householder reduction of this dense matrix then yields a tridiagonal of the desired type. <p> Some of our example tridiagonals come from quantum chemistry applications. The first eleven among the following types of tridiagonal matrices are obtained by Householder reduction of random dense symmetric matrices that have the given eigenvalue distributions (see <ref> [36] </ref> for more on the generation of such matrices).
Reference: [37] <author> J. Demmel and K. Veselic. </author> <title> Jacobi's method is more accurate than QR. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 13(4) </volume> <pages> 1204-1246, </pages> <year> 1992. </year> <note> (also LAPACK Working Note #15). </note>
Reference-contexts: Jacobi methods cost O (n 3 ) or more operations but the constant is larger than in any of the algorithms discussed above. Despite their slowness, these methods are still valuable as they seem to be more accurate than other methods <ref> [37] </ref>. They can also be quite fast on strongly diagonally dominant matrices. The bisection algorithm discussed in Section 2.3 is a reliable way to compute eigenvalues.
Reference: [38] <author> J. W. Demmel. </author> <type> personal communication, </type> <year> 1996. </year>
Reference-contexts: Indeed such an approach has been taken in [60]. Another alternative is to perform bisection and inverse iteration in higher precision. This may be achieved by simulating quadrupled precision, i.e., doubling the precision of the machine's native arithmetic, in software in an attempt to obviate the need for orthogonalization <ref> [38] </ref>. We choose to take a different approach in our thesis. By revisiting the problem at a more fundamental level, we have been able to arrive at an algorithm that shares the attractive features of inverse iteration and the divide and conquer method.
Reference: [39] <author> I. Dhillon, G. Fann, and B. Parlett. </author> <title> Application of a new algorithm for the symmetric eigen-problem to computational quantum chemistry. </title> <booktitle> In Proceedings of the Eighth SIAM Conference on Parallel Proceesing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <month> March </month> <year> 1997. </year>
Reference-contexts: As an example of the speedups possible due to our new algorithm, the parallel solution of a 966 fi 966 dense symmetric eigenproblem, that comes from the modeling of a biphenyl molecule by the Mtller-Plesset theory, is now nearly 3 times faster than an earlier implementation <ref> [39] </ref>. This speedup is a direct consequence of a 10-fold increase in speed of the tridiagonal solution, which previously accounted for 80-90% of the total time. Detailed numerical results are presented in Chapter 6. <p> For example, on the biphenly matrix our new parallel implementation is 100 times faster on a 128 processor IBM SP2 machine. More performance results are given in <ref> [39] </ref>. 6.5 Future Enhancements to Algorithm Y We now list various ways in which our current implementation of Algorithm Y may be improved. Multiple Representations. We would like to complete the relative perturbation theory for factorizations of indefinite matrices. Some preliminary results and conjectures were presented in Chapter 5.
Reference: [40] <author> I. S. Dhillon. </author> <title> Current inverse iteration software can fail. </title> <note> 1997. Submitted for publication. </note>
Reference-contexts: Although many surprising aspects of current implementations are revealed by our careful examination, the reader who is pressed for time may skip on to Chapter 3 for the main results of this thesis. The material 16 in the upcoming sections also appears in <ref> [40] </ref>. 2.7 Issues in Inverse Iteration Inverse Iteration is a method to find an eigenvector when an approximate eigen value ^ is known : v (0) = b; (A ^ I)v (i+1) = t (i) v (i) ; i = 0; 1; 2; : : :: (2.7.4) Here b is the
Reference: [41] <author> I. S. Dhillon. </author> <title> Perfect shifts and twisted factorizations. </title> <note> 1997. In preparation. </note>
Reference-contexts: An eigenvalue close to may be deflated by crossing out the entire kth row and column of A 1 . Thus deflation occurs in exactly one step when perfect shifts are used. For a more detailed treatment, the reader should wait for <ref> [41] </ref>. Each matrix in the sequence obtained in the standard QR algorithm is tridiago-nal. <p> It should also be possible to find k by using a procedure free from square roots as in the PWK algorithm. But as we mentioned earlier, this section is a digression from the main theme of this thesis and we plan to explore these questions elsewhere <ref> [41] </ref>. The reader may wonder about our inconsistency in finding the eigenvalues by a standard QR algorithm but using twisted factorizations in finding the eigenvectors. <p> It also solves the problem of immediately deflating a known eigenpair from a symmetric matrix by deleting one of its rows and columns. We intend to substantiate these claims with numerical experiments in the near future <ref> [41] </ref>. Non-symmetric Eigenproblem. Since Algorithm Y does not invoke any Gram-Schmidt like process, it does not explicitly try to compute eigenvectors that are orthogonal. Orthogonality is a result of the matrix being symmetric.
Reference: [42] <author> I. S. Dhillon. </author> <title> Stable computation of the condition number of a tridiagonal matrix in O(n) time. </title> <note> 1997. Submitted for publication. </note>
Reference-contexts: Our contribution is in recognizing the importance of twisted factorizations and successfully applying them to 44 solve some elusive problems in numerical linear algebra. We will show some of these applications in this thesis, for other applications see <ref> [42, 115, 74] </ref>. In addition to the papers reviewed in [108], twisted factorizations have appeared in various contexts in the literature, see [77, 94, 5, 58, 130, 133, 44]. For a brief review, see Section 4.1 of [117]. <p> This result has appeared in <ref> [4, 14, 83, 78, 42] </ref>. Lemma 3.3.2 Let J be a nonsingular unreduced tridiagonal matrix of order n. <p> Of course, Step (4c) of Algorithm X needs to be modified when fl r is not small enough. For an alternate approach to compute a good r that does not involve orthogonal factorizations the reader is referred to <ref> [42] </ref>. In summary, we have seen how to ensure that r is "good" and either (4.5.50) or (4.5.53) is satisfied. Of course, we look for a small residual norm since it implies that the computed vector is close to the exact eigenvector.
Reference: [43] <author> I. S. Dhillon and B.N. Parlett. </author> <title> Orthogonal eigenvectors without Gram-Schmidt. </title> <note> 1997. in preparation. 161 </note>
Reference-contexts: It is beyond the scope of this thesis to discuss the above approach in greater detail. The theory that justifies this scheme is quite involved and intricate, and a cursory treatment would not do it justice. We hope to present more details in the near future <ref> [43] </ref>. 6.4 Numerical Results In this section, we present a numerical comparison between Algorithm Y and four other software routines for solving the symmetric tridiagonal eigenproblem that are included in the EISPACK [128] and LAPACK [1] libraries. These are 1.
Reference: [44] <author> J. Dongarra, J. Bunch, C. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1979. </year>
Reference-contexts: We will show some of these applications in this thesis, for other applications see [42, 115, 74]. In addition to the papers reviewed in [108], twisted factorizations have appeared in various contexts in the literature, see <ref> [77, 94, 5, 58, 130, 133, 44] </ref>. For a brief review, see Section 4.1 of [117].
Reference: [45] <author> J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> A set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The extent to which it is a bottleneck can be much greater than suggested by the above numbers because the other two phases, Householder reduction and back-transformation can exploit fast matrix-multiply based operations <ref> [12, 45] </ref>, whereas most algorithms for the tridiagonal problem are sequential in nature and/or cannot be expressed in terms of matrix multiplication.
Reference: [46] <author> J. Dongarra and D. Sorensen. </author> <title> A fully parallel algorithm for the symmetric eigenproblem. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8(2) </volume> <pages> 139-154, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: However, when eigenvalues are close, current implementations can take up to 10n 3 operations due to the orthogonalization. 2.4 Divide and Conquer Methods In 1981, Cuppen proposed a solution to the symmetric tridiagonal eigenproblem that was meant to be efficient for parallel computation <ref> [25, 46] </ref>. It is quite remarkable that this method can also be faster than other implementations on a serial computer! The matrix T may be expressed as a modification of a direct sum of two smaller tridiagonal matrices. <p> In fact, for some matrices, such as a small perturbation of the identity matrix, just O (n) operations are sufficient to solve the eigenproblem. This method was designed to work well on parallel computers, offering both task and data parallelism <ref> [46] </ref>. Efficient parallel implementations are not straightforward to program, and the decision to switch from task to data parallelism depends on the characteristics of the underlying machine [17].
Reference: [47] <author> J. Dongarra, R. van de Geijn, and D. Walker. </author> <title> A look at scalable dense linear algebra libraries. </title> <booktitle> In Scalable High-Performance Computing Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1992. </year>
Reference-contexts: Despite these drawbacks, the embarrassingly parallel nature of bisection followed by inverse iteration makes it easy and efficient to program on a parallel computer. As a result, this is the method that has been implemented in current software libraries for distributed memory computers, such as ScaLAPACK <ref> [47] </ref>, PeIGS [52, 54] and in [75]. Like the recent divide and conquer methods, the QR algorithm guarantees numerical orthogonality of the computed eigenvectors.
Reference: [48] <author> J. DuCroz, </author> <month> December </month> <year> 1994. </year> <title> private communication. </title>
Reference-contexts: A similar overflow occurrence (in IEEE double precision arithmetic) on an 8 fi 8 matrix, with a largest element of magnitude 2 484 10 145 , was reported to us by Jeremy DuCroz <ref> [48] </ref>. The problems reported above can be cured by reverting back to the choice of scale factor in EISPACK's Tinvit. IV. Orthogonality. Tinvit and xStein use the modified Gram-Schmidt (MGS) procedure to orthogonalize iterates corresponding to eigenvalues whose separation is less than 10 3 kT k.
Reference: [49] <author> S. Eisenstat and I. Ipsen. </author> <title> Relative perturbation techniques for singular value problems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 32(6), </volume> <year> 1995. </year>
Reference-contexts: In the rest of this chapter, we show how to exploit this property of bidiagonal matrices to compute numerically orthogonal eigenvectors without any explicit orthogonalization. The relative perturbation results for bidiagonal matrices mentioned above have appeared in <ref> [89, 29, 35, 49, 50, 51, 100, 101] </ref>. <p> ; ff 1 ff 3 ff 2 ff 4 ff 6 ; : : :: : : ; ff 1 ff 3 ff 5 ff 2n3 ; and D 2 = diag ff 1 ff 3 ; ff 2 ff 4 ff 1 ff 3 ff 5 ff 2n1 In <ref> [49] </ref>, Eisenstat and Ipsen considered such multiplicative perturbations for sin gular value problems and proved the results presented below which show that if D 1 and D 2 are close to unitary matrices then the singular values of ~ L + ffi ~ L are close in a relative measure to <p> ffi j denote the jth singular values of ~ L and ~ L + ffi ~ L respectively, while u j , u j + ffiu j , v j and v j + ffiv j denote the corresponding left and right singular vectors. 74 Theorem 4.3.1 (Eisenstat and Ipsen <ref> [49, Thm. 3.1] </ref>). Let ~ L + ffi ~ L = D T 1 ~ LD 2 , where D 1 and D 2 are nonsingular matrices. <p> Then j 1 k kD 1 j + ffi j j kD 1 k kD 2 k: Corollary 4.3.1 (Barlow and Demmel [9, Thm. 1], Deift et al. [29, Thm. 2.12], Demmel and Kahan [35, Cor. 2], Eisenstat and Ipsen <ref> [49, Cor. 4.2] </ref>). Let ~ L and ~ L + ffi ~ L be bidiagonal matrices as in (4.3.3) and (4.3.4). Then 1 j j + ffi j (1 + ) j ; where = Q 2n1 i=1 maxfjff i j; 1=jff i jg 1. Proof. <p> See also Eisenstat and Ipsen <ref> [49, Theorem 3.3 and Cor. 4.5] </ref>. The above results can be generalized to singular subspaces of larger dimension. Before we present the results, we need to introduce additional notation. <p> By Eisenstat and Ipsen's Theorem 2.1 in <ref> [49] </ref>, the eigenvalues of D T 1 AD 1 are small relative perturbations of the eigenvalues of A if kD 1 k 1.
Reference: [50] <author> S. C. Eisenstat and I. C. F. Ipsen. </author> <title> Relative perturbation bounds for eigenspaces and singular vector subspaces. </title> <editor> In J. G. Lewis, editor, </editor> <booktitle> Proceedings of the Fifth SIAM Conference on Applied Linear Algebra, </booktitle> <pages> pages 62-65. </pages> <publisher> SIAM, </publisher> <year> 1994. </year>
Reference-contexts: In the rest of this chapter, we show how to exploit this property of bidiagonal matrices to compute numerically orthogonal eigenvectors without any explicit orthogonalization. The relative perturbation results for bidiagonal matrices mentioned above have appeared in <ref> [89, 29, 35, 49, 50, 51, 100, 101] </ref>. <p> See also Eisenstat and Ipsen <ref> [50, Theorem 3.1] </ref>. 4.4 Using Products of Bidiagonals We now show how to exploit the properties of a bidiagonal matrix that were outlined in the previous section. Consider the tridiagonal matrix T 1 given in Example 4.2.1.
Reference: [51] <author> S.C. Eisenstat and I.C.F. Ipsen. </author> <title> Relative perturbation results for eigenvalues and eigenvectors of diagonalisable matrices. </title> <type> Technical Report CRSC-TR96-6, </type> <institution> Center for Research in Scientific Computation, Department of Mathematics, North Carolina State University, </institution> <year> 1996. </year> <pages> (14 pages). </pages>
Reference-contexts: In the rest of this chapter, we show how to exploit this property of bidiagonal matrices to compute numerically orthogonal eigenvectors without any explicit orthogonalization. The relative perturbation results for bidiagonal matrices mentioned above have appeared in <ref> [89, 29, 35, 49, 50, 51, 100, 101] </ref>.
Reference: [52] <author> D. Elwood, G. Fann, and D. Littlefield. </author> <title> PeIGS User's Manual. </title> <institution> Pacific Northwest National Laboratory, Richland, WA, </institution> <year> 1993. </year>
Reference-contexts: Efficient parallel implementations are not straightforward to program, and the decision to switch from task to data parallelism depends on the characteristics of the underlying machine [17]. Due to such complications, all the currently available parallel software libraries, such as ScaLAPACK [22] and PeIGS <ref> [52] </ref>, use algorithms based on bisection and inverse iteration. A drawback of the current divide and conquer software in LAPACK is that it needs extra workspace of more than 2n 2 floating point numbers, which can be prohibitively excessive for large problems. <p> Despite these drawbacks, the embarrassingly parallel nature of bisection followed by inverse iteration makes it easy and efficient to program on a parallel computer. As a result, this is the method that has been implemented in current software libraries for distributed memory computers, such as ScaLAPACK [47], PeIGS <ref> [52, 54] </ref> and in [75]. Like the recent divide and conquer methods, the QR algorithm guarantees numerical orthogonality of the computed eigenvectors. <p> In fact, in the recent past, some people have turned to LAPACK INVIT instead of divide and conquer to solve their large problems solely for this reason <ref> [6, 53, 52] </ref>. Of course, 151 workspace of only about 5-10 n double precision words is required by Algorithm Y, and EISPACK and LAPACK INVIT. The QR method is seen to uniformly take O (n 3 ) time and is less sensitive to the eigenvalue distribution. <p> We have also implemented a parallel variant of Algorithm Y in collaboration with computational chemists at Pacific Northwest National Laboratories (PNNL). This will replace the earlier tridiagonal eigensolver based on inverse iteration that was part of the PeIGS version 2.0 library <ref> [52] </ref>. Our new method is more easily parallelized, and coupled with its lower operation count, offers an even bigger computational advantage on a parallel 152 computer. For example, on the biphenly matrix our new parallel implementation is 100 times faster on a 128 processor IBM SP2 machine.
Reference: [53] <author> G.I. Fann, </author> <year> 1996. </year> <title> private communication. </title>
Reference-contexts: In fact, in the recent past, some people have turned to LAPACK INVIT instead of divide and conquer to solve their large problems solely for this reason <ref> [6, 53, 52] </ref>. Of course, 151 workspace of only about 5-10 n double precision words is required by Algorithm Y, and EISPACK and LAPACK INVIT. The QR method is seen to uniformly take O (n 3 ) time and is less sensitive to the eigenvalue distribution.
Reference: [54] <author> G.I. Fann and R. J. Littlefield. </author> <title> Parallel inverse iteration with re-orthogonalization. </title> <editor> In R. Sin-covec et. al., editor, </editor> <booktitle> Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <volume> volume 1, </volume> <pages> pages 409-413. </pages> <publisher> SIAM, </publisher> <year> 1993. </year>
Reference-contexts: Despite these drawbacks, the embarrassingly parallel nature of bisection followed by inverse iteration makes it easy and efficient to program on a parallel computer. As a result, this is the method that has been implemented in current software libraries for distributed memory computers, such as ScaLAPACK [47], PeIGS <ref> [52, 54] </ref> and in [75]. Like the recent divide and conquer methods, the QR algorithm guarantees numerical orthogonality of the computed eigenvectors.
Reference: [55] <author> G.I. Fann, R. J. Littlefield, </author> <title> and D.M. Elmwood. Performance of a fully parallel dense real symmetric eigensolver in quantum chemistry appliacation. </title> <booktitle> In Proceedings of the High Performance Computing '95, Simulation MultiConference. The Society of Computer Simulation, </booktitle> <address> San Diego, </address> <year> 1995. </year>
Reference-contexts: In addition, the following symmetric tridiagonal matrices arise in certain quantum chemistry computations. For more details on these problems, the reader is referred to <ref> [10, 55] </ref>. 13) Biphenyl. This positive definite matrix with n = 966 occurs in the modeling of biphenyl using Mtller-Plesset theory. Most of its eigenvalues are quite small com pared to the norm. See Figure 6.1 for a plot of its eigenvalue distribution. 14) SiOSi 6 .
Reference: [56] <author> K. Fernando and B. Parlett. </author> <title> Accurate singular values and differential qd algorithms. </title> <journal> Nu-merische Mathematik, </journal> <volume> 67 </volume> <pages> 191-229, </pages> <year> 1994. </year>
Reference-contexts: Since T 1 is positive definite we can compute its bidiagonal Cholesky factor ~ L 1 . The singular values, j , of ~ L 1 may now be computed to high relative accuracy using either bisection or the much faster and more elegant dqds algorithm given in <ref> [56] </ref> (remember that in exact arithmetic the eigenvalues of T 1 are the squares of the singular values of ~ L 1 and the eigenvectors of T 1 are the left singular vectors of ~ L 1 ). <p> More recently, Fernando and Parlett have developed another qd algorithm that gives a fast way of computing the singular values of a bidiagonal matrix to high relative accuracy <ref> [56] </ref>. The term `stationary' is used for (4.4.17) since it represents an identity transformation when = 0. Rutishauser used the term `progressive' instead for the formation of U D U T from LDL T . <p> Before we do so, we emphasize that the particular qd-like transformations presented in this section are new. Similar qd recurrences have been studied by Rutishauser [121, 122, 123], Henrici [76], Fernando and Parlett <ref> [56] </ref>, Yao Yang [138] and David Day [28]. 4.4.2 Roundoff Error Analysis First, we introduce our model of arithmetic. <p> Nevertheless, we will give a hybrid interpretation involving both backward and forward relative errors. Our error analysis is on the lines of that presented in <ref> [56] </ref>. The best way to understand our first result is by studying Figure 4.1. <p> Find kT k such that T + I is positive (or negative) definite. 2. Compute T + I = LDL T . 3. Compute the eigenvalues, ^ 2 j , of LDL T to high relative accuracy (by bisection or the dqds algorithm <ref> [56] </ref>). 4. <p> Compute T + I = L p D p L T p . 3. Compute the eigenvalues, ^ j , of L p D p L T p to high relative accuracy by the dqds algo rithm <ref> [56] </ref>. 4. Set l 1; m n. 133 5. <p> In its present form, the dqds algorithm finds the eigenvalues in sequential order, from the smallest to the largest, and always operates on a positive definite matrix. See <ref> [56] </ref> for more details. The main difficulty in trying to employ the dqds algorithm to find the locally small eigenvalues of an RRR is that in most cases, the RRR will be the factorization of an indefinite matrix.
Reference: [57] <author> K. V. Fernando. </author> <title> On computing an eigenvector of a tridiagonal matrix. </title> <type> Technical Report TR4/95, </type> <institution> Nag Ltd., Oxford, UK, </institution> <year> 1995. </year> <note> submitted for publication. 162 </note>
Reference-contexts: Sections 3.1 and 3.2 introduce twisted factorizations that provide an answer to Wilkin-son's problem of choosing which equation to omit. This is due to pioneering work by Fernando, and enables us to discard LAPACK's random choice of starting vector to compute an eigenvector of an isolated eigenvalue <ref> [57, 117] </ref>. 2. In Section 3.3 we show how to adapt the results of Sections 3.1 and 3.2 when triangular factorizations don't exist. Some of the material presented in Sections 3.1-3.3 has appeared in [117]. 3. <p> For a brief review, see Section 4.1 of [117]. Twisted factorizations have also been referred to as BABE factorizations (Begin, or Burn, at Both Ends) in <ref> [78, 57, 74] </ref>. 3.2 The Eigenvector Connection Given an eigenvalue approximation ^ of ~ J, we can compute the double factorization of ~ J ^ I by Theorem 3.1.2. <p> In 1995, Fernando, in an equivalent formulation, proposed choosing the index for which jfl k j is minimum, say r, and then solving ( ~ J I)z (r) = fl r e r to compute an approximate eigenvector z (r) . See <ref> [57] </ref> for his subsequent work. Earlier, in 1985, Godunov and his collaborates proposed a similar but more obscure method for obtaining a provably accurate approximation to an eigenvector by `sewing' together two "Sturm Sequences" that start at either end of the matrix.
Reference: [58] <author> D. Fischer, G. H. Golub, O. Hald, C. Leiva, and O. Widlund. </author> <title> On Fourier Toeplitz methods for separable elliptic problems. </title> <journal> Math. Comp., </journal> <volume> 28 </volume> <pages> 349-368, </pages> <year> 1974. </year>
Reference-contexts: We will show some of these applications in this thesis, for other applications see [42, 115, 74]. In addition to the papers reviewed in [108], twisted factorizations have appeared in various contexts in the literature, see <ref> [77, 94, 5, 58, 130, 133, 44] </ref>. For a brief review, see Section 4.1 of [117].
Reference: [59] <author> G. J. F. Francis. </author> <title> The QR transformation, parts I and II. </title> <journal> Computer J., </journal> <volume> 4 265-271,332-345, </volume> <pages> 1961-62. </pages>
Reference-contexts: We now discuss these existing algorithms. 2.2 The QR Algorithm Till recently, the method of choice for the symmetric tridiagonal eigenproblem was the QR Algorithm which was independently invented by Francis <ref> [59] </ref> and Kublanovskaja [95].
Reference: [60] <author> K. E. Gates. </author> <title> Using inverse iteration to improve the divide and conquer algorithm. </title> <type> Technical Report 159, </type> <institution> ETH Department Informatik, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: The above observations suggest a hybrid algorithm that solves clusters by the divide and conquer algorithm and computes eigenvectors of isolated eigenvalues by inverse iteration. Indeed such an approach has been taken in <ref> [60] </ref>. Another alternative is to perform bisection and inverse iteration in higher precision. This may be achieved by simulating quadrupled precision, i.e., doubling the precision of the machine's native arithmetic, in software in an attempt to obviate the need for orthogonalization [38].
Reference: [61] <author> W. </author> <title> Givens. The characteristic value-vector problem. </title> <journal> J. ACM, </journal> <volume> 4 </volume> <pages> 298-307, </pages> <year> 1957. </year> <note> also unpublished report. </note>
Reference-contexts: When normalized in the same way v and ~v will yield the same eigenvector in exact arithmetic. The method described above is `obvious' and was mentioned by W. Givens in 1957, see <ref> [61] </ref>. It often gives good results when realized on a computer but, at other times, delivers vectors pointing in completely wrong directions for the following reason. It is rare that an eigenvalue of a tridiagonal (or any other) matrix is representable in limited precision.
Reference: [62] <author> Wallace J. </author> <title> Givens. Numerical computation of the characteristic values of a real symmetric matrix. </title> <type> Technical Report ORNL-1574, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, USA, </institution> <year> 1954. </year>
Reference-contexts: It seems natural to try and transform A to a tridiagonal matrix instead. In 1954, Givens proposed the reduction of A to tridiagonal form by using orthogonal plane rotations <ref> [62] </ref>. However, most current efficient algorithms work by reducing A to a tridiagonal matrix T by a sequence of n 2 orthogonal reflectors, now named after Householder who first introduced them in 1958, see [81]. <p> A simpler proof of global convergence is due to Hoffman and Parlett, see [79]. An excellent treatment of the QR method is given by Parlett in [110]. Each orthogonal matrix Q i in (2.2.2) is a product of n 1 elementary rotations, known as Givens rotations <ref> [62] </ref>. The tridiagonal matrices T i converge to diagonal form that gives the eigenvalues of T . The eigenvector matrix of T is then given by the product Q 1 Q 2 Q 3 . <p> not found to work much better than Wilkinson's shift strategy, taking an average of about 11 2 QR iterations per eigenvalue [69, 98]. 2.3 Bisection and Inverse Iteration In 1954, Wallace Givens proposed the method of bisection to find some or all of the eigenvalues of a real, symmetric matrix <ref> [62] </ref>. This method is based on the availability of a simple recurrence to count the number of eigenvalues less than a floating point number .
Reference: [63] <author> S. K. Godunov, A. G. Antonov, O. P. Kiriljuk, and V. I. Kostin. </author> <title> Guaranted Accuracy in Numerical Linear Algebra. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, Netherlands, </address> <year> 1993. </year> <note> A revised translation of a Russian text first published in 1988 in Novosibirsk. </note>
Reference-contexts: See [57] for his subsequent work. Earlier, in 1985, Godunov and his collaborates proposed a similar but more obscure method for obtaining a provably accurate approximation to an eigenvector by `sewing' together two "Sturm Sequences" that start at either end of the matrix. See [64] and <ref> [63] </ref> for their work, and Section 4.2 of [117] for interpretation in our notation. Fernando's approach leads to the following algorithm Algorithm 3.2.1 [Computing an eigenvector of an isolated eigenvalue.] 1. Compute ~ J I = L + D + U + = U D L . 2.
Reference: [64] <author> S. K. Godunov, V. I. Kostin, and A. D. Mitchenko. </author> <title> Computation of an eigenvector of symmetric tridiagonal matrices. </title> <journal> Siberian Math. J., </journal> <volume> 26 </volume> <pages> 71-85, </pages> <year> 1985. </year>
Reference-contexts: See [57] for his subsequent work. Earlier, in 1985, Godunov and his collaborates proposed a similar but more obscure method for obtaining a provably accurate approximation to an eigenvector by `sewing' together two "Sturm Sequences" that start at either end of the matrix. See <ref> [64] </ref> and [63] for their work, and Section 4.2 of [117] for interpretation in our notation. Fernando's approach leads to the following algorithm Algorithm 3.2.1 [Computing an eigenvector of an isolated eigenvalue.] 1. Compute ~ J I = L + D + U + = U D L . 2.
Reference: [65] <author> D. Goldberg. </author> <title> What every computer scientist should know about floating point arithmetic. </title> <journal> ACM Computing Surveys, </journal> <volume> 23(1), </volume> <year> 1991. </year>
Reference-contexts: The IEEE standard also specifies a Double-Extended precision format (sometimes referred to as "80-bit" arithmetic) and on some machines, these extra precise computations may be performed in hardware <ref> [2, 65] </ref>. Quadruple precision arithmetic is generally simulated in software [91, 120]. In order to get single precision accuracy, we may try and execute Algorithm X in double precision arithmetic. However, there are cases when this simple strategy will not work.
Reference: [66] <author> G. H. Golub, V. Klema, and G. W. Stewart. </author> <title> Rank degeneracy and least squares problems. </title> <type> Technical Report STAN-CS-76-559, </type> <institution> Computer Science Dept., Stanford Univ., </institution> <year> 1976. </year>
Reference-contexts: This proves the existence of a column permutation of B such that the bottom element of R in B's QR decomposition is tiny when B is nearly singular. This result was proved by Chan in [18] and earlier by Golub, Klema and Stewart in <ref> [66] </ref>. Thus twisted factorizations can be rank-revealing. Rank-revealing LU and QR factorizations have been extensively studied and several algorithms to compute such fac torizations exist. Twisted factorizations seem to have been overlooked and may offer com putational advantages. <p> Twisted factorizations seem to have been overlooked and may offer com putational advantages. We consider our results outlined above to be stronger than those of Chan [19, 18] and Golub et al. <ref> [66] </ref> since the permutations we consider are restricted. In particular, as seen in the tridiagonal and Hessenberg case, twisted factorizations respect the sparsity structure of the given matrix, and thus may offer computational advantages in terms of speed and accuracy.
Reference: [67] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix computations. Johns Hopkins Studies in the Mathematical Sciences. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, USA, third edition, </address> <year> 1996. </year>
Reference-contexts: angle between the eigenvector u j and the invariant subspace U 1 + ffiU 1 , j k, as k sin 6 (u j ; U 1 + ffiU 1 )k = k (U T 2 + ffiU T 2 )u j k: For more on angles between subspaces, see <ref> [67, Section 12.4.3] </ref>. The following is an easily obtained extension of Theorem 4.8 in [101]. Theorem 4.3.4 Let ~ L + ffi ~ L = D T 1 ~ LD 2 , where D 1 and D 2 are nonsingular matrices. <p> Such a strategy has been adopted to speed up the LAPACK implementation of bisection on some computer architectures [107, 1]. We say that such a strategy leads to a level-2 BLAS-like operation since it involves a quadratic amount of data and a quadratic amount of work <ref> [67] </ref>. We hope to resolve the above mentioned algorithmic enhancements in the near future. These should result in Algorithm Z, and be included in future releases of LAPACK [1] and and ScaLAPACK [22].
Reference: [68] <author> Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. </author> <title> Concrete Mathematics: A Foundation for Computer Science. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, USA, </address> <year> 1989. </year>
Reference-contexts: Just as we did in the above sentence and in equations (1.1.1) and (1.1.2), we will continue to abuse the "big oh" notation. Normally the O notation, introduced by Bachmann in 1894 <ref> [68, Section 9.2] </ref>, implies a limiting process. For example, when we say that an algorithm takes O (n 2 ) time, we mean that the algorithm performs less than Kn 2 operations for some constant K as n ! 1.
Reference: [69] <author> A. Greenbaum and J. Dongarra. </author> <title> Experiments with QL/QR methods for the symmetric tridi-agonal eigenproblem. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-89-92, </type> <institution> University of Ten-nessee, Knoxville, </institution> <year> 1989. </year> <note> (LAPACK Working Note #17, available electronically on netlib). </note>
Reference-contexts: However this perfect shift strategy was not found to work much better than Wilkinson's shift strategy, taking an average of about 11 2 QR iterations per eigenvalue <ref> [69, 98] </ref>. 2.3 Bisection and Inverse Iteration In 1954, Wallace Givens proposed the method of bisection to find some or all of the eigenvalues of a real, symmetric matrix [62]. <p> As a result, immediate deflation is not always seen in the perfect-shift QR algorithm, and it was found not to perform any better than the standard QR algorithm with Wilkinson's shift <ref> [69] </ref>. The situation is similar to that with D + (n) discused in Section 3.1. And it is natural for us, as in Section 3.1, to turn to twisted factorizations to detect singularity of T I. Let be a twisted Q factorization of T I. <p> LAPACK D&C : The LAPACK implementation of the divide and conquer method that uses a rank-one tear to subdivide the problem [71, 124] (subroutine DSTEDC); 140 4. LAPACK QR : The LAPACK implementation of the QR algorithm that uses Wilkin-son's shifts to compute both eigenvalues and eigenvectors <ref> [69] </ref> (subroutine DSTEQR). 6.4.1 Test Matrices We have chosen many different types of tridiagonals as our test matrices. They differ mainly in their eigenvalue distributions which highlight the sensitivity of the above algorithms as discussed in Chapter 2. Some of our example tridiagonals come from quantum chemistry applications.
Reference: [70] <author> L. Greengard and V. Rokhlin. </author> <title> A fast algorithm for particle simulations. </title> <journal> J. Comp. Phys., </journal> <volume> 73 </volume> <pages> 325-348, </pages> <year> 1987. </year>
Reference-contexts: In [71, 73], Gu and Eisenstat show that by using the fast multipole method of Carrier, Greengard and Rokhlin <ref> [70, 16] </ref>, the complexity of solving the symmetric tridiagonal eigenproblem can be considerably lowered. All the eigenvalues and eigenvectors can be found in O (n 2 ) operations while all the eigenvalues can be computed in O (n log 2 n) operations.
Reference: [71] <author> M. Gu. </author> <title> Studies in numerical linear algebra. </title> <type> Ph.D. thesis, </type> <year> 1993. </year> <month> 163 </month>
Reference-contexts: For some discussion on how to relate the above goals to backward errors in T , see [87, Theorem 2.1] and <ref> [71] </ref>. However it may not be possible to achieve (1.1.1) and (1.1.2) in all cases and we will aim for bounds that grow slowly with n. 3. An embarrassingly parallel algorithm that allows independent computation of each eigenvalue and eigenvector making it easy to implement on a parallel computer. 4. <p> In the worst case when no deflation occurs O (n 3 ) operations are needed, but on matrices where eigenvalues cluster and the eigenvector ma 13 trix contains many tiny entries, substantial deflation occurs and many fewer than O (n 3 ) operations are required [73]. In <ref> [71, 73] </ref>, Gu and Eisenstat show that by using the fast multipole method of Carrier, Greengard and Rokhlin [70, 16], the complexity of solving the symmetric tridiagonal eigenproblem can be considerably lowered. <p> EISPACK INVIT : The EISPACK implementation of inverse iteration after finding the eigenvalues by bisection [118] (subroutine DSTEBZ from LAPACK followed by TINVIT from EISPACK); 3. LAPACK D&C : The LAPACK implementation of the divide and conquer method that uses a rank-one tear to subdivide the problem <ref> [71, 124] </ref> (subroutine DSTEDC); 140 4. LAPACK QR : The LAPACK implementation of the QR algorithm that uses Wilkin-son's shifts to compute both eigenvalues and eigenvectors [69] (subroutine DSTEQR). 6.4.1 Test Matrices We have chosen many different types of tridiagonals as our test matrices.
Reference: [72] <author> M. Gu and S. C. Eisenstat. </author> <title> A stable and efficient algorithm for the rank-1 modification of the symmetric eigenproblem. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 15(4) </volume> <pages> 1266-1276, </pages> <month> October </month> <year> 1994. </year> <type> Yale Tech report YALEU/DCS/RR-916, </type> <month> Sept </month> <year> 1992. </year>
Reference-contexts: This modification may be a rank-one update [15], or may be obtained by crossing out a row and column of T <ref> [72] </ref>. The eigenproblem of T can then be solved in terms of the eigenproblems of the smaller tridiagonal matrices, and this may be done recursively. For several years after its inception, it was not known how to guarantee numerically orthogonality of the eigenvector approximations obtained by this approach. <p> For several years after its inception, it was not known how to guarantee numerically orthogonality of the eigenvector approximations obtained by this approach. However in 1992, Gu and Eisenstat found a clever solution to this problem, and paved the way for robust software based on their algorithms <ref> [72, 73, 124] </ref> and Li's work on a faster zero-finder [102]. The main reason for the unexpected success of divide and conquer methods on serial machines is deflation, which occurs when an eigenpair of a submatrix of T is an acceptable eigenpair of a larger matrix.
Reference: [73] <author> M. Gu and S. C. Eisenstat. </author> <title> A divide-and-conquer algorithm for the symmetric tridiagonal eigenproblem. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 16(1) </volume> <pages> 172-191, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: For several years after its inception, it was not known how to guarantee numerically orthogonality of the eigenvector approximations obtained by this approach. However in 1992, Gu and Eisenstat found a clever solution to this problem, and paved the way for robust software based on their algorithms <ref> [72, 73, 124] </ref> and Li's work on a faster zero-finder [102]. The main reason for the unexpected success of divide and conquer methods on serial machines is deflation, which occurs when an eigenpair of a submatrix of T is an acceptable eigenpair of a larger matrix. <p> In the worst case when no deflation occurs O (n 3 ) operations are needed, but on matrices where eigenvalues cluster and the eigenvector ma 13 trix contains many tiny entries, substantial deflation occurs and many fewer than O (n 3 ) operations are required <ref> [73] </ref>. In [71, 73], Gu and Eisenstat show that by using the fast multipole method of Carrier, Greengard and Rokhlin [70, 16], the complexity of solving the symmetric tridiagonal eigenproblem can be considerably lowered. <p> In the worst case when no deflation occurs O (n 3 ) operations are needed, but on matrices where eigenvalues cluster and the eigenvector ma 13 trix contains many tiny entries, substantial deflation occurs and many fewer than O (n 3 ) operations are required [73]. In <ref> [71, 73] </ref>, Gu and Eisenstat show that by using the fast multipole method of Carrier, Greengard and Rokhlin [70, 16], the complexity of solving the symmetric tridiagonal eigenproblem can be considerably lowered. <p> The fastest current implementation is the divide and conquer method of <ref> [73] </ref>. As mentioned in Section 2.4, many fewer than O (n 3 ) operations are needed when heavy deflation occurs. In fact, for some matrices, such as a small perturbation of the identity matrix, just O (n) operations are sufficient to solve the eigenproblem. <p> Besides bidiagonal matrices, the twisted factors introduced in Section 3.1 and triangular factors of arrowhead matrices (see <ref> [73, p.175] </ref>) belong to this class of matrices. Thus the theory developed in this section is applicable not only to a bidiagonal matrix but also to any biacyclic matrix.
Reference: [74] <author> F. G. Gustavson and A. Gupta. </author> <title> A new parallel algorithm for tridiagonal symmetric positive definite systems of equations. </title> <type> unpublished report, </type> <year> 1996. </year>
Reference-contexts: Our contribution is in recognizing the importance of twisted factorizations and successfully applying them to 44 solve some elusive problems in numerical linear algebra. We will show some of these applications in this thesis, for other applications see <ref> [42, 115, 74] </ref>. In addition to the papers reviewed in [108], twisted factorizations have appeared in various contexts in the literature, see [77, 94, 5, 58, 130, 133, 44]. For a brief review, see Section 4.1 of [117]. <p> For a brief review, see Section 4.1 of [117]. Twisted factorizations have also been referred to as BABE factorizations (Begin, or Burn, at Both Ends) in <ref> [78, 57, 74] </ref>. 3.2 The Eigenvector Connection Given an eigenvalue approximation ^ of ~ J, we can compute the double factorization of ~ J ^ I by Theorem 3.1.2. <p> This may result in a faster computer implementation to compute an eigenvector. However, elimination of divisions results in a method that is susceptible to over/underflow and requires some care to ensure correctness. This section was inspired by Fred Gustavson's observation that n divisions are sufficient for a related application <ref> [74] </ref>. In this section, we will assume that T is the given unreduced real, symmetric tridi agonal matrix with diagonal elements ff 1 ; ff 2 ; : : : ; ff n and off-diagonal elements fi 1 ; : : : ; fi n1 .
Reference: [75] <author> B. Hendrickson, E. Jessup, and C. Smith. </author> <title> A parallel eigensolver for dense symmetric matrices. </title> <type> Technical Report SAND96-0822, </type> <institution> Sandia National Labs, </institution> <address> Albuquerque, NM, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: As a result, this is the method that has been implemented in current software libraries for distributed memory computers, such as ScaLAPACK [47], PeIGS [52, 54] and in <ref> [75] </ref>. Like the recent divide and conquer methods, the QR algorithm guarantees numerical orthogonality of the computed eigenvectors.
Reference: [76] <author> P. Henrici. </author> <title> The quotient-difference algorithm. </title> <journal> Nat. Bur. Standards Appl. Math. Series, </journal> <volume> 19 </volume> <pages> 23-46, </pages> <year> 1958. </year>
Reference-contexts: Before we do so, we emphasize that the particular qd-like transformations presented in this section are new. Similar qd recurrences have been studied by Rutishauser [121, 122, 123], Henrici <ref> [76] </ref>, Fernando and Parlett [56], Yao Yang [138] and David Day [28]. 4.4.2 Roundoff Error Analysis First, we introduce our model of arithmetic.
Reference: [77] <author> P. Henrici. </author> <title> Bounds for eigenvalues of certain tridiagonal matrices. </title> <journal> SIAM J., </journal> <volume> 11 </volume> <pages> 281-290, </pages> <year> 1963. </year>
Reference-contexts: We will show some of these applications in this thesis, for other applications see [42, 115, 74]. In addition to the papers reviewed in [108], twisted factorizations have appeared in various contexts in the literature, see <ref> [77, 94, 5, 58, 130, 133, 44] </ref>. For a brief review, see Section 4.1 of [117].
Reference: [78] <author> N. J. Higham. </author> <title> Efficient algorithms for computing the condition number of a tridiagonal matrix. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 </volume> <pages> 150-165, </pages> <year> 1986. </year>
Reference-contexts: For a brief review, see Section 4.1 of [117]. Twisted factorizations have also been referred to as BABE factorizations (Begin, or Burn, at Both Ends) in <ref> [78, 57, 74] </ref>. 3.2 The Eigenvector Connection Given an eigenvalue approximation ^ of ~ J, we can compute the double factorization of ~ J ^ I by Theorem 3.1.2. <p> This result has appeared in <ref> [4, 14, 83, 78, 42] </ref>. Lemma 3.3.2 Let J be a nonsingular unreduced tridiagonal matrix of order n.
Reference: [79] <author> Hoffman and B. N. Parlett. </author> <title> A new proof of global convergence for the tridiagonal QL algorithm. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 15 </volume> <pages> 929-937, </pages> <year> 1978. </year>
Reference-contexts: In 1968, Wilkinson proved that the tridiagonal QL iteration (the QL method is intimately related to the QR method) always converges using his shift strategy. A simpler proof of global convergence is due to Hoffman and Parlett, see <ref> [79] </ref>. An excellent treatment of the QR method is given by Parlett in [110]. Each orthogonal matrix Q i in (2.2.2) is a product of n 1 elementary rotations, known as Givens rotations [62]. The tridiagonal matrices T i converge to diagonal form that gives the eigenvalues of T .
Reference: [80] <author> R. A. Horn and C. R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1985. </year> <note> Includes Ostrowski's relative perturbation theory on pp 224-225. </note>
Reference-contexts: These results are also an immediate consequence of Ostrowski's theorem in <ref> [80, Thm. 4.5.9] </ref>.
Reference: [81] <author> Alston S. </author> <title> Householder. Unitary triangularization of a nonsymmetric matrix. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 5 </volume> <pages> 339-342, </pages> <year> 1958. </year>
Reference-contexts: However, most current efficient algorithms work by reducing A to a tridiagonal matrix T by a sequence of n 2 orthogonal reflectors, now named after Householder who first introduced them in 1958, see <ref> [81] </ref>. <p> However, as we will see in Section 2.8.1, xStein also suffers from some of the same problems as Tinvit in addition to introducing a new serious error. Both EISPACK and LAPACK solve the dense symmetric eigenproblem by reducing the dense matrix to tridiagonal form by Householder transformations <ref> [81] </ref>, and then finding the eigenvalues and eigenvectors of the tridiagonal matrix. Both Tinvit and xStein operate on a symmetric tridiagonal matrix.
Reference: [82] <author> IBM. </author> <title> Engineering and Scientific Subroutine Library, Guide and Reference, Release 3, Program 5668-863, </title> <address> 4 edition, </address> <year> 1988. </year>
Reference-contexts: All the numerical experiments presented in this section were conducted on an IBM RS/6000-590 processor that has a peak rating of 266 MFlops. Fortran BLAS, instead of those from the machine optimized ESSL library <ref> [82] </ref>, were used in this preliminary testing 1 . We hope to include the ESSL BLAS in our future comparisons. The rest of the tables, Table 6.3 through 6.6, indicate the accuracy of the methods tested.
Reference: [83] <author> I. Ikebe. </author> <title> On inverses of Hessenberg matrices. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 24 </volume> <pages> 93-97, </pages> <year> 1979. </year>
Reference-contexts: This result has appeared in <ref> [4, 14, 83, 78, 42] </ref>. Lemma 3.3.2 Let J be a nonsingular unreduced tridiagonal matrix of order n.
Reference: [84] <author> I. Ipsen and E. Jessup. </author> <title> Solving the symmetric tridiagonal eigenvalue problem on the hypercube. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11(2) </volume> <pages> 203-230, </pages> <year> 1990. </year>
Reference-contexts: These are 1. LAPACK INVIT : The LAPACK implementation of bisection and inverse iteration <ref> [88, 84, 87] </ref> (subroutines DSTEBZ and DSTEIN); 2. EISPACK INVIT : The EISPACK implementation of inverse iteration after finding the eigenvalues by bisection [118] (subroutine DSTEBZ from LAPACK followed by TINVIT from EISPACK); 3.
Reference: [85] <author> I. C. F. Ipsen. </author> <title> Computing an eigenvector with inverse iteration. </title> <note> SIAM Review, 1997. to appear. </note>
Reference-contexts: A similar error, where there is no gradual underflow, occurs on (1=")T where T is as in (2.8.12). tu In xStein, t is chosen to be t = kbk 1 where T ^ I = P LU and u nn is the last diagonal element of U <ref> [85] </ref>. <p> It checks to see if overflow would occur, and if so, perturbs tiny entries on the diagonal of U <ref> [85] </ref>. This check is in the inner loop when solving U y = x where x = t L 1 P 1 b. Coupled with the extra iterations done after convergence, this results in xStein being slower than Tinvit.
Reference: [86] <author> C. G. F. </author> <title> Jacobi. Concerning an easy process for solving equations occurring in the theory of secular disturbances. </title> <journal> J. Reine Angnew. Math., </journal> <volume> 30 </volume> <pages> 51-94, 1846. </pages>
Reference-contexts: There is no software available as yet that uses the fast multipole method for the eigenproblem. 2.5 Other Methods The oldest method for solving the symmetric eigenproblem is one due to Jacobi that dates back to 1846 <ref> [86] </ref>, and was rediscovered by von Neumann and colleagues in 1946 [7]. Jacobi's method does not reduce the dense symmetric matrix A to tridiagonal form, as most other methods do, but instead works on A.
Reference: [87] <author> E. Jessup and I. Ipsen. </author> <title> Improving the accuracy of inverse iteration. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 13(2) </volume> <pages> 550-572, </pages> <year> 1992. </year> <month> 164 </month>
Reference-contexts: For some discussion on how to relate the above goals to backward errors in T , see <ref> [87, Theorem 2.1] </ref> and [71]. However it may not be possible to achieve (1.1.1) and (1.1.2) in all cases and we will aim for bounds that grow slowly with n. 3. <p> Faster iterations that are superlinearly convergent can beat bisection and we give some references in Section 2.5. Once an accurate eigenvalue approximation ^ is known, the method of inverse iteration may be used to compute an approximate eigenvector <ref> [118, 87] </ref> : v (0) = b; (A ^ I)v (i+1) = t (i) v (i) ; i = 0; 1; 2; : : :; where b is the starting vector and t (i) is a scalar. <p> A random starting vector is a popular choice since the probability that it has a negligible component in the desired direction is extremely low, see <ref> [87] </ref> for a detailed study. III. Scaling of right hand side. Equation (2.7.6) implies that k^v 1 k = O (jt (1) =( 1 )j) where t (1) is the scale factor in the first iteration of (2.7.5). <p> In order to achieve accuracy comparable to that of the divide and conquer and QR/QL methods, the search for a better implementation of inverse iteration led to xStein <ref> [87] </ref>. However, as we will see in Section 2.8.1, xStein also suffers from some of the same problems as Tinvit in addition to introducing a new serious error. <p> On the other hand, xStein chooses a random starting vector, each of whose elements comes from a uniform (1; 1) distribution. Neither choice of starting vectors is likely to be pathologically deficient in the desired eigenvector. The random starting vectors are designed to be superior to Tinvit's choice <ref> [87] </ref>. II. Choice of shift. Even though in exact arithmetic all the eigenvalues of an unreduced tridiagonal matrix are distinct, some of the computed eigenvalues may be identical to working accuracy. <p> The hope is to get greater linear independence of the iterates before orthogo-nalization by the MGS method <ref> [87] </ref>. However, as we now show, the Tinvit error as reported above persists. Example 2.8.7 [Linear Dependence Persists.] Consider again the matrix T given in (2.8.16). <p> We want to avoid such failures and indicate our approach to the various aspects of inverse iteration discussed above. I. Choice of shift. Of the various issues discussed in Section 2.7, the choice of starting vector and convergence criterion have been extensively studied <ref> [136, 118, 119, 87] </ref>. Surprisingly, the choice of shift for inverse iteration seems to have drawn little attention. We feel that the shift is probably the most important variable in inverse iteration. Examples 2.8.6 and 2.8.7 highlight the importance of shifts that are as accurate as possible. <p> A random starting vector is used in the LAPACK implementation of inverse iteration <ref> [87] </ref>. In this chapter, we show the following 1. Sections 3.1 and 3.2 introduce twisted factorizations that provide an answer to Wilkin-son's problem of choosing which equation to omit. <p> These are 1. LAPACK INVIT : The LAPACK implementation of bisection and inverse iteration <ref> [88, 84, 87] </ref> (subroutines DSTEBZ and DSTEIN); 2. EISPACK INVIT : The EISPACK implementation of inverse iteration after finding the eigenvalues by bisection [118] (subroutine DSTEBZ from LAPACK followed by TINVIT from EISPACK); 3. <p> See Section 2.8.1 for more details. EISPACK's implementation is always faster than LAPACK's but is generally, less accurate. In fact, the latter was designed to improve the accuracy of EISPACK <ref> [87] </ref>. Algorithm Y is much less sensitive to the arrangement of eigenvalues | on matrices of order 2000, the time taken by it ranges from about 30 seconds to about 60 seconds in most cases.
Reference: [88] <author> E. R. Jessup. </author> <title> Parallel Solution of the Symmetric Tridiagonal Eigenproblem. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1989. </year>
Reference-contexts: These are 1. LAPACK INVIT : The LAPACK implementation of bisection and inverse iteration <ref> [88, 84, 87] </ref> (subroutines DSTEBZ and DSTEIN); 2. EISPACK INVIT : The EISPACK implementation of inverse iteration after finding the eigenvalues by bisection [118] (subroutine DSTEBZ from LAPACK followed by TINVIT from EISPACK); 3.
Reference: [89] <author> W. Kahan. </author> <title> Accurate eigenvalues of a symmetric tridiagonal matrix. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS41, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> July </month> <year> 1966 </year> <month> (revised June </month> <year> 1968). </year>
Reference-contexts: However, it was soon observed that recurrence (2.3.3) was prone to overflow with a limited exponent range. An alternate recurrence that computes d j () = j ()= j1 () is now used in most software <ref> [89] </ref>, d j+1 () = ( T j+1;j+1 ) T 2 The bisection algorithm permits an eigenvalue to be computed in about 2bn addition and bn division operations where b is the number of bits of precision in the numbers (b = 24 in IEEE single while b = 53 in <p> In the rest of this chapter, we show how to exploit this property of bidiagonal matrices to compute numerically orthogonal eigenvectors without any explicit orthogonalization. The relative perturbation results for bidiagonal matrices mentioned above have appeared in <ref> [89, 29, 35, 49, 50, 51, 100, 101] </ref>. <p> This result appears to 73 have lain neglected in the technical report <ref> [89] </ref> until Demmel and Kahan used it in 1990 to devise a method to compute the singular values of a bidiagonal matrix to high relative accuracy [35]. Subsequently, these results have been extended and simplified, and we cite some contributions during the course of this section.
Reference: [90] <author> W. Kahan. </author> <title> Notes on Laguerre's iteration. </title> <institution> University of California Computer Science Division preprint, </institution> <year> 1992. </year>
Reference-contexts: The bisection algorithm discussed in Section 2.3 is a reliable way to compute eigenvalues. However, it can be quite slow and there have been many attempts to find faster zero-finders such as the Rayleigh Quotient Iteration [110], Laguerre's method <ref> [90, 113] </ref> and the Zeroin scheme [31, 13]. These zero-finders can considerably speed up the computation of isolated eigenvalues but they seem to stumble when eigenvalues cluster. Homotopy methods for the symmetric eigenproblem were suggested by Chu in [23, 14 24].
Reference: [91] <author> W. Kahan. </author> <title> Lecture notes on the status of IEEE standard 754 for binary floating point arithmetic. </title> <address> http://HTTP.CS.Berkeley.EDU/ wkahan/ieee754status/ieee754.ps, </address> <year> 1995. </year>
Reference-contexts: The IEEE standard also specifies a Double-Extended precision format (sometimes referred to as "80-bit" arithmetic) and on some machines, these extra precise computations may be performed in hardware [2, 65]. Quadruple precision arithmetic is generally simulated in software <ref> [91, 120] </ref>. In order to get single precision accuracy, we may try and execute Algorithm X in double precision arithmetic. However, there are cases when this simple strategy will not work.
Reference: [92] <author> W. Kahan, </author> <year> 1996. </year> <title> private communication. </title>
Reference-contexts: To overcome this, these quantities need to be monitored and rescaled when approaching overflow or underflow. Some modern computers allow such monitoring to be done in hardware but alas, this facility is not presently visible to the software developer who programs in a high level language <ref> [92] </ref>. 3.4.1 Heuristics for choosing r In Algorithm 3.2.1, the 2n quantities D + (1); : : :; D + (n) and D (1); : : :; D (n) are computed to find the minimum among all the fl k 's.
Reference: [93] <author> L. Kaufman. </author> <title> A parallel QR algorithm for the symmetric tridiagonal eigenvalue problem. </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 23(3) </volume> <pages> 429-434, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: The O (n 2 ) computation performed in the QR method to find all the eigenvalues is sequential in nature and is not easily parallelized on modern parallel machines despite the attempts in <ref> [96, 132, 93] </ref>. However, the O (n 3 ) computation in accumulating the Givens' rotations into the eigenvector matrix is trivially and efficiently parallelized, see [3] for more details.
Reference: [94] <author> D. Kershaw. </author> <title> Inequalities on elements of the inverse of a certain tridiagonal matrix. </title> <journal> Math. Comp., </journal> <volume> 24 </volume> <pages> 155-158, </pages> <year> 1970. </year>
Reference-contexts: We will show some of these applications in this thesis, for other applications see [42, 115, 74]. In addition to the papers reviewed in [108], twisted factorizations have appeared in various contexts in the literature, see <ref> [77, 94, 5, 58, 130, 133, 44] </ref>. For a brief review, see Section 4.1 of [117].
Reference: [95] <author> V. N. Kublanovskaya. </author> <title> On some algorithms for the solution of the complete eigenvalue problem. </title> <journal> Zh. Vych. Mat., </journal> <volume> 1 </volume> <pages> 555-570, </pages> <year> 1961. </year>
Reference-contexts: We now discuss these existing algorithms. 2.2 The QR Algorithm Till recently, the method of choice for the symmetric tridiagonal eigenproblem was the QR Algorithm which was independently invented by Francis [59] and Kublanovskaja <ref> [95] </ref>.
Reference: [96] <author> D. Kuck and A. Sameh. </author> <title> A parallel QR algorithm for symmetric tridiagonal matrices. </title> <journal> IEEE Trans. Computers, </journal> <volume> C-26(2), </volume> <year> 1977. </year>
Reference-contexts: The O (n 2 ) computation performed in the QR method to find all the eigenvalues is sequential in nature and is not easily parallelized on modern parallel machines despite the attempts in <ref> [96, 132, 93] </ref>. However, the O (n 3 ) computation in accumulating the Givens' rotations into the eigenvector matrix is trivially and efficiently parallelized, see [3] for more details.
Reference: [97] <author> C. </author> <title> Lanczos. Solution of systems of linear equations by minimized iterations. </title> <institution> J. Res. Natl. Bur. Stand, </institution> <month> 49 </month> <pages> 33-53, </pages> <year> 1952. </year>
Reference-contexts: Orthogonality is a result of the matrix being symmetric. We believe that many of our ideas can be applied to the non-symmetric eigenproblem in order to obtain the "best-conditioned" eigenvectors. Lanczos Algorithm. The method proposed by Lanczos in 1952 <ref> [97] </ref>, after many modifications to account for roundoff [125, 110, 126], has become the champion among algorithms to find some of the extreme eigenvalues of a sparse symmetric matrix.
Reference: [98] <author> J. Le and B. Parlett. </author> <title> Forward instability of tridiagonal QR. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 14(1) </volume> <pages> 279-316, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: However this perfect shift strategy was not found to work much better than Wilkinson's shift strategy, taking an average of about 11 2 QR iterations per eigenvalue <ref> [69, 98] </ref>. 2.3 Bisection and Inverse Iteration In 1954, Wallace Givens proposed the method of bisection to find some or all of the eigenvalues of a real, symmetric matrix [62].
Reference: [99] <author> K. Li and T.-Y. Li. </author> <title> An algorithm for symmetric tridiagonal eigenproblems | divide and conquer with homotopy continuation. </title> <journal> SIAM J. Sci. Comp., </journal> <volume> 14(3), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: D was chosen to be the diagonal of the tridiagonal in [103], but greater success was obtained by taking D to be a direct sum of submatrices of T <ref> [105, 99] </ref>. An alternate divide and conquer method that finds the eigenvalues by using Laguerre's iteration instead of homotopy methods is given in [104].
Reference: [100] <author> R.-C. Li. </author> <title> Relative perturbation theory: (I) eigenvalue and singular value variations. </title> <type> Technical Report UCB//CSD-94-855, </type> <institution> Computer Science Division, Department of EECS, University of California at Berkeley, </institution> <year> 1994. </year> <note> (revised January 1996). </note>
Reference-contexts: In the rest of this chapter, we show how to exploit this property of bidiagonal matrices to compute numerically orthogonal eigenvectors without any explicit orthogonalization. The relative perturbation results for bidiagonal matrices mentioned above have appeared in <ref> [89, 29, 35, 49, 50, 51, 100, 101] </ref>. <p> On the other hand, the measures reldist p (ff; fi) def jff fij jffj p + jfij p : (4.3.6) are symmetric for 1 p 1. Note that reldist 1 (ff; fi) def jff fij max (jffj; jfij) : (4.3.7) For more discussion and comparison between these measures, see <ref> [100] </ref>. Relative gaps between singular values will figure prominently in our discussion, and we define relgap (-; fg) def y2fg reldist (-; y); (4.3.8) where is a real number while fg denotes a set of real numbers. Typically, fg will denote a subset of the singular values.
Reference: [101] <author> R.-C. Li. </author> <title> Relative perturbation theory: (II) eigenspace and singular subspace variations. </title> <institution> Technical Rep University of California at Berkeley UCB//CSD-94-856, Computer Science Division, Department of EECS, University of California at Berkeley, </institution> <year> 1994. </year> <note> (revised January 1996 and April 1996). </note>
Reference-contexts: In the rest of this chapter, we show how to exploit this property of bidiagonal matrices to compute numerically orthogonal eigenvectors without any explicit orthogonalization. The relative perturbation results for bidiagonal matrices mentioned above have appeared in <ref> [89, 29, 35, 49, 50, 51, 100, 101] </ref>. <p> Similarly, we define the relative gaps for the symmetric measures to be relgap p (-; fg) def y2fg reldist p (-; y): The following theorem bounds the perturbation angle in terms of the relative gaps. It is a special case of Li's Theorem 4.7 <ref> [101] </ref>. Theorem 4.3.3 Let ~ L + ffi ~ L = D T 1 ~ LD 2 , where D 1 and D 2 are nonsingular matrices. <p> The following is an easily obtained extension of Theorem 4.8 in <ref> [101] </ref>. Theorem 4.3.4 Let ~ L + ffi ~ L = D T 1 ~ LD 2 , where D 1 and D 2 are nonsingular matrices.
Reference: [102] <author> R.-C. Li. </author> <title> Solving secular equations stably and efficiently. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-94-260, </type> <institution> University of Tennessee, Knoxville, </institution> <month> November </month> <year> 1994. </year> <note> (LAPACK Working Note #89). 165 </note>
Reference-contexts: However in 1992, Gu and Eisenstat found a clever solution to this problem, and paved the way for robust software based on their algorithms [72, 73, 124] and Li's work on a faster zero-finder <ref> [102] </ref>. The main reason for the unexpected success of divide and conquer methods on serial machines is deflation, which occurs when an eigenpair of a submatrix of T is an acceptable eigenpair of a larger matrix. For symmetric tridiagonal matrices, this phenomenon is quite common.
Reference: [103] <author> T.-Y. Li and N. H. Rhee. </author> <title> Homotopy algorithm for symmetric eigenvalue problems. </title> <journal> Num. Math., </journal> <volume> 55 </volume> <pages> 265-280, </pages> <year> 1989. </year>
Reference-contexts: These methods start from an eigenvalue of a simpler matrix D and follow a smooth curve to find an eigenvalue of A (t) D + t (A D). D was chosen to be the diagonal of the tridiagonal in <ref> [103] </ref>, but greater success was obtained by taking D to be a direct sum of submatrices of T [105, 99]. An alternate divide and conquer method that finds the eigenvalues by using Laguerre's iteration instead of homotopy methods is given in [104].
Reference: [104] <author> T. Y. Li and Z. Zeng. </author> <title> The Laguerre iteration in solving the symmetric tridiagonal eigenprob-lem, revisited. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 15(5) </volume> <pages> 1145-1173, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: An alternate divide and conquer method that finds the eigenvalues by using Laguerre's iteration instead of homotopy methods is given in <ref> [104] </ref>. The corresponding eigenvectors in these methods are obtained by inverse iteration. 2.6 Comparison of Existing Methods All currently implemented software for finding all the eigenvalues and eigenvectors of a symmetric tridiagonal matrix requires O (n 3 ) work in the worst case.
Reference: [105] <author> T.-Y. Li, H. Zhang, and X. H. Sun. </author> <title> Parallel homotopy algorithm for symmetric tridiagonal eigenvalue problems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 12 </volume> <pages> 464-485, </pages> <year> 1991. </year>
Reference-contexts: D was chosen to be the diagonal of the tridiagonal in [103], but greater success was obtained by taking D to be a direct sum of submatrices of T <ref> [105, 99] </ref>. An alternate divide and conquer method that finds the eigenvalues by using Laguerre's iteration instead of homotopy methods is given in [104].
Reference: [106] <author> S.-S. Lo, B. Phillipe, and A. Sameh. </author> <title> A multiprocessor algorithm for the symmetric eigenprob-lem. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8(2) </volume> <pages> 155-165, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: However, bisection is slow if all the eigenvalues are needed. A faster root-finder, such as Zeroin [31, 13], speeds up computation when an eigenvalue is isolated in an interval. Multisection maintains the simplicity of bisection and in certain situations, can speed up the performance on a parallel machine <ref> [106, 11, 127] </ref>. When the k eigenvalues are well separated, inverse iteration can find the eigenvectors independently, each in O (n) time. However, to find eigenvectors of k close eigenvalues, all existing implementations resort to reorthogonalization and this costs O (nk 2 ) operations.
Reference: [107] <author> A. McKenney, </author> <year> 1997. </year> <title> private communication. </title>
Reference-contexts: Such a strategy has been adopted to speed up the LAPACK implementation of bisection on some computer architectures <ref> [107, 1] </ref>. We say that such a strategy leads to a level-2 BLAS-like operation since it involves a quadratic amount of data and a quadratic amount of work [67]. We hope to resolve the above mentioned algorithmic enhancements in the near future.
Reference: [108] <author> G. Meurant. </author> <title> A review on the inverse of symmetric tridiagonal and block tridiagonal matrices. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 13(3) </volume> <pages> 707-728, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: See also Theorem 3.1.1 which proves that fl n = 0 when J is singular. The Double Factorization theorem is not new. In 1992, in <ref> [108] </ref>, Meurant reviewed a significant portion of the literature on the inverses of band matrices and presented the main ideas in a nice unified framework. The inexpensive additive formulae for (J 1 ) kk (Theorem 3.1.2 and Corollary 3.1.1) are included in Theorem 3.1 of [108], while our Corollary 3.1.3 that <p> In 1992, in <ref> [108] </ref>, Meurant reviewed a significant portion of the literature on the inverses of band matrices and presented the main ideas in a nice unified framework. The inexpensive additive formulae for (J 1 ) kk (Theorem 3.1.2 and Corollary 3.1.1) are included in Theorem 3.1 of [108], while our Corollary 3.1.3 that gives the quotient/product form of (J 1 ) kk is given in Theorem 2.3 of [108]. We believe that such formulae have been known for quite some time in the differential equations community. <p> The inexpensive additive formulae for (J 1 ) kk (Theorem 3.1.2 and Corollary 3.1.1) are included in Theorem 3.1 of <ref> [108] </ref>, while our Corollary 3.1.3 that gives the quotient/product form of (J 1 ) kk is given in Theorem 2.3 of [108]. We believe that such formulae have been known for quite some time in the differential equations community. <p> Our contribution is in recognizing the importance of twisted factorizations and successfully applying them to 44 solve some elusive problems in numerical linear algebra. We will show some of these applications in this thesis, for other applications see [42, 115, 74]. In addition to the papers reviewed in <ref> [108] </ref>, twisted factorizations have appeared in various contexts in the literature, see [77, 94, 5, 58, 130, 133, 44]. For a brief review, see Section 4.1 of [117].
Reference: [109] <author> J. M. Ortega and H. F. Kaiser. </author> <title> The LL T and QR Methods for Symmetric Tridiagonal Matrices. </title> <journal> Numer. Math., </journal> <volume> 5 </volume> <pages> 211-225, </pages> <year> 1963. </year>
Reference-contexts: When only eigenvalues are desired, the QR transformation can be reorganized to eliminate all square roots that are required to form the Givens rotations. This was first observed by Ortega and Kaiser in 1963 <ref> [109] </ref> and a fast, stable algorithm was developed by Pal, Walker and Kahan (PWK) in 1968-69 [110]. Since a square root operation can be about 20 or more times as expensive as addition or multiplication, this yields a much faster method.
Reference: [110] <author> B. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year> <note> A new SIAM edition is under preparation. </note>
Reference-contexts: A simpler proof of global convergence is due to Hoffman and Parlett, see [79]. An excellent treatment of the QR method is given by Parlett in <ref> [110] </ref>. Each orthogonal matrix Q i in (2.2.2) is a product of n 1 elementary rotations, known as Givens rotations [62]. The tridiagonal matrices T i converge to diagonal form that gives the eigenvalues of T . <p> This was first observed by Ortega and Kaiser in 1963 [109] and a fast, stable algorithm was developed by Pal, Walker and Kahan (PWK) in 1968-69 <ref> [110] </ref>. Since a square root operation can be about 20 or more times as expensive as addition or multiplication, this yields a much faster method. <p> In the hope of cutting down this work by half, Parlett suggested the alternate strategy of computing the eigenvalues by the PWK algorithm, and then executing the QR algorithm using the previously computed eigenvalues as origin shifts to find the eigenvectors <ref> [110, p.173] </ref>. <p> They can also be quite fast on strongly diagonally dominant matrices. The bisection algorithm discussed in Section 2.3 is a reliable way to compute eigenvalues. However, it can be quite slow and there have been many attempts to find faster zero-finders such as the Rayleigh Quotient Iteration <ref> [110] </ref>, Laguerre's method [90, 113] and the Zeroin scheme [31, 13]. These zero-finders can considerably speed up the computation of isolated eigenvalues but they seem to stumble when eigenvalues cluster. Homotopy methods for the symmetric eigenproblem were suggested by Chu in [23, 14 24]. <p> It is common practice now to compute eigenvalues first, and then invoke inverse iteration with very accurate . Due to the fundamental limitations of finite precision arithmetic, eigenvalues of symmetric matrixes can, in general, only be computed to a guaranteed accuracy of O ("kAk) <ref> [110] </ref>. Even when a very accurate eigenvalue ap proximation is available, the following may influence the choice of the shift when more than one eigenvector is desired. * The pairing problem. <p> However when 1 is in a cluster, goal (1.1.2) is not automatic. As we now discuss, the methods used to compute numerically orthogonal vectors can impact the choice of the convergence criterion. V. Orthogonality. Standard perturbation theory <ref> [110, Section 11-7] </ref> says that if ^v is a unit vector, is the eigenvalue closest to ^ and v is 's eigenvector then j sin 6 (v; ^v)j gap ( ^ ) where gap ( ^ ) = min i 6= j ^ i j. <p> Section 3.4 shows how to eliminate the divisions in the method outlined in Section 3.2. 4. In Section 3.5, we introduce twisted Q factorizations and give an alternate method to compute an eigenvector. We also show how the perfect shift strategy suggested by Parlett <ref> [110, p.173] </ref> can be made to succeed. 5. In Section 3.6, we show that twisted factorizations may also be used to detect singu larity of Hessenberg and dense matrices. In preparation for the upcoming theory, we state a few well known results without proof. <p> No component of v is zero. Proof. The latter is not so obvious and follows from the fact that when J is normal jv (i)j 2 0 () = 1:i1 () i+1:n () (3.0.8) where l:m () = det (I ~ J l:m ). See <ref> [110, Section 7-9] </ref> to derive the above. tu 3.1 Twisted Factorizations Despite Wilkinson's pessimism, we ask the question: can we find reliable indicators of the sizes of various components of a normalized eigenvector without knowing the eigenvector itself? We turn to triangular factorization in search of an answer and examine the <p> Form the vector q r , where q r is as in Theorem 3.5.2. tu The above results may appear somewhat surprising but the reader is reminded of the intimate connection between the LR algorithm, the QR algorithm and inverse iteration <ref> [110] </ref>. For a non-normal matrix also, as the following theorem implies, there is at least one ffi k that indicates its nearness to singularity. Theorem 3.5.4 Let B be a nonsingular matrix of order n. <p> It may be expected that if is close to , then R nn would be small and deflation would again occur in one iteration. Based on this expectation, Parlett proposed the following strategy to compute all the eigenvalues and eigenvectors of a real, symmetric tridiagonal matrix T <ref> [110, p.173] </ref>. 1. Compute eigenvalues by a fast algorithm such as the PWK algorithm, which is a QR algorithm free of square roots. 2. Run the QR algorithm using the computed eigenvalues as shifts, accumulating the rotations to form the eigenvector matrix. <p> Thus deflation occurs in exactly one step when perfect shifts are used. For a more detailed treatment, the reader should wait for [41]. Each matrix in the sequence obtained in the standard QR algorithm is tridiago-nal. Does (3.5.42) preserve tridiagonal form? The reduction uniqueness theorem in <ref> [110, Section 7-2] </ref> states that a tridiagonal formed by an orthogonal similarity transformation, T = Q fl AQ, is determined by A and the first or last column of Q. <p> When deflation does not occur, chasing the bulges in A 1 off the top or bottom of the matrix will give the standard QR or QL algorithm respectively (by the reduction uniqueness theorem in <ref> [110, Section 7-2] </ref>). Thus in order to develop a "twisted algorithm", we must give up the tridiagonal nature of the intermediate matrices. The number of bulges increase by 2 at each step, and it is for future research to determine if it is computationally feasible to handle this increase. <p> In summary, we have seen how to ensure that r is "good" and either (4.5.50) or (4.5.53) is satisfied. Of course, we look for a small residual norm since it implies that the computed vector is close to the exact eigenvector. The following sin theorem, see <ref> [110, Chapter 11] </ref>, is well known and shows that a small residual norm implies a good eigenvector if the corresponding eigenvalue is isolated. The theorem, which we will often refer to in the next few sections, is valid for all Hermitian matrices. <p> In Section 3.5.1 we showed how to effectively use the perfect shift strategy in a QR-like algorithm. This new scheme enables us to use "ultimate" or "perfect" shifts as envisaged by Parlett in <ref> [110] </ref>. It also solves the problem of immediately deflating a known eigenpair from a symmetric matrix by deleting one of its rows and columns. We intend to substantiate these claims with numerical experiments in the near future [41]. Non-symmetric Eigenproblem. <p> Orthogonality is a result of the matrix being symmetric. We believe that many of our ideas can be applied to the non-symmetric eigenproblem in order to obtain the "best-conditioned" eigenvectors. Lanczos Algorithm. The method proposed by Lanczos in 1952 [97], after many modifications to account for roundoff <ref> [125, 110, 126] </ref>, has become the champion among algorithms to find some of the extreme eigenvalues of a sparse symmetric matrix. It proceeds by incrementally forming, one row and column at a time, a tridiagonal matrix that is similar to the sparse matrix.
Reference: [111] <author> B. Parlett. </author> <booktitle> Acta Numerica, chapter The new qd algorithms, </booktitle> <pages> pages 459-491. </pages> <publisher> Cambridge University Press, </publisher> <year> 1995. </year>
Reference-contexts: Since we desire relative accuracy, we would like this backward error to be relative. However, our algorithms do not admit such a pure backward analysis (see <ref> [138, 111] </ref> for a backward analysis where the backward errors are absolute but not relative). Nevertheless, we will give a hybrid interpretation involving both backward and forward relative errors. Our error analysis is on the lines of that presented in [56].
Reference: [112] <author> B. Parlett. </author> <title> The construction of orthogonal eigenvectors for tight clusters by use of submatrices. </title> <institution> Center for Pure and Applied Mathematics PAM-664, University of California, Berkeley, </institution> <address> CA, </address> <month> January </month> <year> 1996. </year> <note> submitted to SIMAX. </note>
Reference-contexts: the eigenvector of , i.e., solve (T p:q I)s = 0 for s. (c) Output the vector v as an eigenvector, where v p:q = s and the rest of v is padded with zeroes. tu For some of the theory underlying this scheme, the reader is referred to Parlett <ref> [112, 115] </ref>. Clearly, besides the existence of suitable submatrices, the crucial question is: how do we choose the submatrices in Step (1a) of the above scheme. The computation of the eigenvector of an isolated eigenvalue in Step (1b) is easily done by using the methods discussed earlier. <p> The computation of the eigenvector of an isolated eigenvalue in Step (1b) is easily done by using the methods discussed earlier. We now have a more robust way of picking the appropriate submatrices than the approaches outlined in <ref> [112, 115] </ref>. We have included this enhancement in our implementation of Algorithm Y and found it to work accurately in practice. Note that the above algorithm is an alternate way of computing orthogonal eigenvectors without doing any explicit orthogonalization.
Reference: [113] <author> B. N. Parlett. </author> <title> Laguerre's Method Applied to the Matrix Eigenvalue Problem. </title> <journal> Math. Comp., </journal> <volume> 18 </volume> <pages> 464-485, </pages> <year> 1964. </year>
Reference-contexts: The bisection algorithm discussed in Section 2.3 is a reliable way to compute eigenvalues. However, it can be quite slow and there have been many attempts to find faster zero-finders such as the Rayleigh Quotient Iteration [110], Laguerre's method <ref> [90, 113] </ref> and the Zeroin scheme [31, 13]. These zero-finders can considerably speed up the computation of isolated eigenvalues but they seem to stumble when eigenvalues cluster. Homotopy methods for the symmetric eigenproblem were suggested by Chu in [23, 14 24].
Reference: [114] <author> B. N. Parlett. </author> <title> The rewards for maintaining semi-orthogonality among Lanczos vectors. </title> <journal> Journal of Numerical Linear Algebra with Applications, </journal> <volume> 1(2) </volume> <pages> 243-267, </pages> <year> 1992. </year>
Reference-contexts: Vectors that have dot products of O ( ") are sometimes referred to as a semi-orthogonal basis. In some cases, such a basis may be as good as an orthogonal basis, see <ref> [114] </ref>. 109 Matrix Time (LAPACK) Time (EISPACK) Time (Alg. X) Time (Alg.
Reference: [115] <author> B. N. Parlett. </author> <title> Invariant subspaces for tightly clustered eigenvalues of tridiagonals. </title> <journal> BIT, </journal> <volume> 36(3) </volume> <pages> 542-562, </pages> <year> 1996. </year>
Reference-contexts: Our contribution is in recognizing the importance of twisted factorizations and successfully applying them to 44 solve some elusive problems in numerical linear algebra. We will show some of these applications in this thesis, for other applications see <ref> [42, 115, 74] </ref>. In addition to the papers reviewed in [108], twisted factorizations have appeared in various contexts in the literature, see [77, 94, 5, 58, 130, 133, 44]. For a brief review, see Section 4.1 of [117]. <p> the eigenvector of , i.e., solve (T p:q I)s = 0 for s. (c) Output the vector v as an eigenvector, where v p:q = s and the rest of v is padded with zeroes. tu For some of the theory underlying this scheme, the reader is referred to Parlett <ref> [112, 115] </ref>. Clearly, besides the existence of suitable submatrices, the crucial question is: how do we choose the submatrices in Step (1a) of the above scheme. The computation of the eigenvector of an isolated eigenvalue in Step (1b) is easily done by using the methods discussed earlier. <p> The computation of the eigenvector of an isolated eigenvalue in Step (1b) is easily done by using the methods discussed earlier. We now have a more robust way of picking the appropriate submatrices than the approaches outlined in <ref> [112, 115] </ref>. We have included this enhancement in our implementation of Algorithm Y and found it to work accurately in practice. Note that the above algorithm is an alternate way of computing orthogonal eigenvectors without doing any explicit orthogonalization.
Reference: [116] <author> B. N. Parlett. </author> <title> Spectral sensitivity of products of bidiagonals. Linear Algebra and its Applications, </title> <note> 1996. submitted for inclusion in the proceedings of the 6th ILAS conference held in Chemnitz, Germany in August 1996. </note>
Reference-contexts: Proof. The proof follows by noting that kD 1 k kD 2 k 1 + and kD 1 1 k kD 1 and then applying Theorem 4.3.1. tu More recently, in <ref> [116] </ref> Parlett gives relative condition numbers that indicate the precise amount by which a singular value changes due to a relative perturbation in a par ticular element of a bidiagonal matrix. <p> Theorem 4.3.2 (Parlett <ref> [116, Thm. 1] </ref>) Let ~ L be a bidiagonal matrix as in (4.3.3), with a i 6= 0, b i 6= 0. Let denote a particular singular value of ~ L and let u, v be the corresponding singular vectors. <p> In <ref> [116] </ref>, Parlett also arrived at such condition numbers for a bidiagonal ~ L but by using calculus. <p> In [116], Parlett also arrived at such condition numbers for a bidiagonal ~ L but by using calculus. Theorem 5.2.2 (Parlett <ref> [116, Thm. 2] </ref>) Let ~ L be a bidiagonal matrix as in (4.3.3), with a k 6= 0, b k 6= 0 and let = diag (! 1 ; : : :; ! n ) with ! k = 1. <p> Note that none of the eigenvalues is small and the lack of relative accuracy implies an absence of any absolute accuracy! tu Other examples of RRRs may be found in Section 6 of <ref> [116] </ref>.
Reference: [117] <author> B.N. Parlett and I.S. Dhillon. </author> <title> Fernando's solution to Wilkinson's problem: an application of double factorization. Linear Algebra and its Applications, </title> <note> 1997. to appear. 166 </note>
Reference-contexts: Sections 3.1 and 3.2 introduce twisted factorizations that provide an answer to Wilkin-son's problem of choosing which equation to omit. This is due to pioneering work by Fernando, and enables us to discard LAPACK's random choice of starting vector to compute an eigenvector of an isolated eigenvalue <ref> [57, 117] </ref>. 2. In Section 3.3 we show how to adapt the results of Sections 3.1 and 3.2 when triangular factorizations don't exist. Some of the material presented in Sections 3.1-3.3 has appeared in [117]. 3. <p> In Section 3.3 we show how to adapt the results of Sections 3.1 and 3.2 when triangular factorizations don't exist. Some of the material presented in Sections 3.1-3.3 has appeared in <ref> [117] </ref>. 3. Section 3.4 shows how to eliminate the divisions in the method outlined in Section 3.2. 4. In Section 3.5, we introduce twisted Q factorizations and give an alternate method to compute an eigenvector. <p> In addition to the papers reviewed in [108], twisted factorizations have appeared in various contexts in the literature, see [77, 94, 5, 58, 130, 133, 44]. For a brief review, see Section 4.1 of <ref> [117] </ref>. Twisted factorizations have also been referred to as BABE factorizations (Begin, or Burn, at Both Ends) in [78, 57, 74]. 3.2 The Eigenvector Connection Given an eigenvalue approximation ^ of ~ J, we can compute the double factorization of ~ J ^ I by Theorem 3.1.2. <p> In this section, we see how double factorization can be used to find a "good" equation to omit when solving the system ( ~ J ^ I)x = 0, thereby obtaining a good approximation to the desired eigenvector. Some of the results of this section have appeared in <ref> [117] </ref>. <p> Earlier, in 1985, Godunov and his collaborates proposed a similar but more obscure method for obtaining a provably accurate approximation to an eigenvector by `sewing' together two "Sturm Sequences" that start at either end of the matrix. See [64] and [63] for their work, and Section 4.2 of <ref> [117] </ref> for interpretation in our notation. Fernando's approach leads to the following algorithm Algorithm 3.2.1 [Computing an eigenvector of an isolated eigenvalue.] 1. Compute ~ J I = L + D + U + = U D L . 2.
Reference: [118] <author> G. Peters and J.H. Wilkinson. </author> <title> The calculation of specified eigenvectors by inverse iteration, contribution II/18, </title> <booktitle> volume II of Handbook of Automatic Computation, </booktitle> <pages> pages 418-439. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, Heidelberg, Berlin, </address> <year> 1971. </year>
Reference-contexts: Faster iterations that are superlinearly convergent can beat bisection and we give some references in Section 2.5. Once an accurate eigenvalue approximation ^ is known, the method of inverse iteration may be used to compute an approximate eigenvector <ref> [118, 87] </ref> : v (0) = b; (A ^ I)v (i+1) = t (i) v (i) ; i = 0; 1; 2; : : :; where b is the starting vector and t (i) is a scalar. <p> We want to avoid such failures and indicate our approach to the various aspects of inverse iteration discussed above. I. Choice of shift. Of the various issues discussed in Section 2.7, the choice of starting vector and convergence criterion have been extensively studied <ref> [136, 118, 119, 87] </ref>. Surprisingly, the choice of shift for inverse iteration seems to have drawn little attention. We feel that the shift is probably the most important variable in inverse iteration. Examples 2.8.6 and 2.8.7 highlight the importance of shifts that are as accurate as possible. <p> These are 1. LAPACK INVIT : The LAPACK implementation of bisection and inverse iteration [88, 84, 87] (subroutines DSTEBZ and DSTEIN); 2. EISPACK INVIT : The EISPACK implementation of inverse iteration after finding the eigenvalues by bisection <ref> [118] </ref> (subroutine DSTEBZ from LAPACK followed by TINVIT from EISPACK); 3. LAPACK D&C : The LAPACK implementation of the divide and conquer method that uses a rank-one tear to subdivide the problem [71, 124] (subroutine DSTEDC); 140 4.
Reference: [119] <author> G. Peters and J.H. Wilkinson. </author> <title> Inverse iteration, ill-conditioned equations and Newton's method. </title> <journal> SIAM Review, </journal> <volume> 21 </volume> <pages> 339-360, </pages> <year> 1979. </year>
Reference-contexts: Earlier fears about loss of accuracy in solving the linear system given above due to the near singularity of T ^ I were allayed in <ref> [119] </ref>. Inverse iteration delivers a vector ^v that 12 has a small residual, i.e. small k (T ^ I)^vk, whenever ^ is close to . However small residual norms do not guarantee orthogonality of the computed vectors when eigenvalues are close together. <p> However Wilkinson showed that the errors made in computing v (i+1) , although large, are almost entirely in the direction of v 1 when 1 is isolated. Since we are interested only in computing the direction of v 1 these errors pose no danger, see <ref> [119] </ref>. Thus to compute the eigenvector of an isolated eigenvalue, the more accurate the shift is the better is the approximate eigenvector. It is common practice now to compute eigenvalues first, and then invoke inverse iteration with very accurate . <p> We want to avoid such failures and indicate our approach to the various aspects of inverse iteration discussed above. I. Choice of shift. Of the various issues discussed in Section 2.7, the choice of starting vector and convergence criterion have been extensively studied <ref> [136, 118, 119, 87] </ref>. Surprisingly, the choice of shift for inverse iteration seems to have drawn little attention. We feel that the shift is probably the most important variable in inverse iteration. Examples 2.8.6 and 2.8.7 highlight the importance of shifts that are as accurate as possible.
Reference: [120] <author> D. Priest. </author> <title> Algorithms for arbitrary precision floating point arithmetic. </title> <editor> In P. Kornerup and D. Matula, editors, </editor> <booktitle> Proceedings of the 10th Symposium on Computer Arithmetic, </booktitle> <pages> pages 132-145, </pages> <address> Grenoble, France, June 26-28 1991. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The IEEE standard also specifies a Double-Extended precision format (sometimes referred to as "80-bit" arithmetic) and on some machines, these extra precise computations may be performed in hardware [2, 65]. Quadruple precision arithmetic is generally simulated in software <ref> [91, 120] </ref>. In order to get single precision accuracy, we may try and execute Algorithm X in double precision arithmetic. However, there are cases when this simple strategy will not work.
Reference: [121] <author> H. Rutishauser. Der Quotienten-Differenzen-Algorithmus. Z. </author> <title> Angew. </title> <journal> Math. Phys., </journal> <volume> 5 </volume> <pages> 223-251, </pages> <year> 1954. </year>
Reference-contexts: This term was first coined by Rutishauser for similar transformations that formed the basis of his qd algorithm first developed in 1954 <ref> [121, 122, 123] </ref>. Although (4.4.17) is not identical to the stationary transformation given by Rutishauser, the differences are not significant enough to warrant inventing new terminology. <p> This term was again coined by Rutishauser in the context of similar transformations in <ref> [121, 122] </ref>. <p> In the next section, we exhibit desirable properties of the differential forms of our qd-like transformations in the face of roundoff errors. Before we do so, we emphasize that the particular qd-like transformations presented in this section are new. Similar qd recurrences have been studied by Rutishauser <ref> [121, 122, 123] </ref>, Henrici [76], Fernando and Parlett [56], Yao Yang [138] and David Day [28]. 4.4.2 Roundoff Error Analysis First, we introduce our model of arithmetic.
Reference: [122] <author> H. </author> <title> Rutishauser. Vorlesungen uber numerische Mathematik. </title> <publisher> Birkhauser, </publisher> <address> Basel, </address> <year> 1976. </year>
Reference-contexts: This term was first coined by Rutishauser for similar transformations that formed the basis of his qd algorithm first developed in 1954 <ref> [121, 122, 123] </ref>. Although (4.4.17) is not identical to the stationary transformation given by Rutishauser, the differences are not significant enough to warrant inventing new terminology. <p> This term was again coined by Rutishauser in the context of similar transformations in <ref> [121, 122] </ref>. <p> In the next section, we exhibit desirable properties of the differential forms of our qd-like transformations in the face of roundoff errors. Before we do so, we emphasize that the particular qd-like transformations presented in this section are new. Similar qd recurrences have been studied by Rutishauser <ref> [121, 122, 123] </ref>, Henrici [76], Fernando and Parlett [56], Yao Yang [138] and David Day [28]. 4.4.2 Roundoff Error Analysis First, we introduce our model of arithmetic.
Reference: [123] <author> H. </author> <title> Rutishauser. Lectures on Numerical Mathematics. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: This term was first coined by Rutishauser for similar transformations that formed the basis of his qd algorithm first developed in 1954 <ref> [121, 122, 123] </ref>. Although (4.4.17) is not identical to the stationary transformation given by Rutishauser, the differences are not significant enough to warrant inventing new terminology. <p> In the next section, we exhibit desirable properties of the differential forms of our qd-like transformations in the face of roundoff errors. Before we do so, we emphasize that the particular qd-like transformations presented in this section are new. Similar qd recurrences have been studied by Rutishauser <ref> [121, 122, 123] </ref>, Henrici [76], Fernando and Parlett [56], Yao Yang [138] and David Day [28]. 4.4.2 Roundoff Error Analysis First, we introduce our model of arithmetic.
Reference: [124] <author> J. Rutter. </author> <title> A serial implementation of Cuppen's divide and conquer algorithm for the symmetric eigenvalue problem. </title> <institution> Mathematics Dept. </institution> <note> Master's Thesis available by anonymous ftp to tr-ftp.cs.berkeley.edu, directory pub/tech-reports/csd/csd-94-799, file all.ps, </note> <institution> University of California, </institution> <year> 1994. </year>
Reference-contexts: For several years after its inception, it was not known how to guarantee numerically orthogonality of the eigenvector approximations obtained by this approach. However in 1992, Gu and Eisenstat found a clever solution to this problem, and paved the way for robust software based on their algorithms <ref> [72, 73, 124] </ref> and Li's work on a faster zero-finder [102]. The main reason for the unexpected success of divide and conquer methods on serial machines is deflation, which occurs when an eigenpair of a submatrix of T is an acceptable eigenpair of a larger matrix. <p> EISPACK INVIT : The EISPACK implementation of inverse iteration after finding the eigenvalues by bisection [118] (subroutine DSTEBZ from LAPACK followed by TINVIT from EISPACK); 3. LAPACK D&C : The LAPACK implementation of the divide and conquer method that uses a rank-one tear to subdivide the problem <ref> [71, 124] </ref> (subroutine DSTEDC); 140 4. LAPACK QR : The LAPACK implementation of the QR algorithm that uses Wilkin-son's shifts to compute both eigenvalues and eigenvectors [69] (subroutine DSTEQR). 6.4.1 Test Matrices We have chosen many different types of tridiagonals as our test matrices.
Reference: [125] <author> D. Scott. </author> <title> Analysis of the Symmetric Lanczos Algorithm. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, California, </institution> <year> 1978. </year>
Reference-contexts: Orthogonality is a result of the matrix being symmetric. We believe that many of our ideas can be applied to the non-symmetric eigenproblem in order to obtain the "best-conditioned" eigenvectors. Lanczos Algorithm. The method proposed by Lanczos in 1952 [97], after many modifications to account for roundoff <ref> [125, 110, 126] </ref>, has become the champion among algorithms to find some of the extreme eigenvalues of a sparse symmetric matrix. It proceeds by incrementally forming, one row and column at a time, a tridiagonal matrix that is similar to the sparse matrix. <p> The sparsity of the eigenvectors of the tridiagonal, see Figure 4.6 for an example, may also be exploited in reducing the amount of work in the selective orthogonalization phase. See <ref> [125] </ref> for details on the latter phase. Rank-Revealing Factorizations. The twisted factorizations introduced in Section 3.1 enable us to accurately compute a null vector of a nearly singular tridiagonal matrix. The particular twisted factorization used is successful because it transparently reveals the near singularity of the tridiagonal.
Reference: [126] <author> H. Simon. </author> <title> The Lanczos algorithm with partial reorthogonalization. </title> <journal> Math. Comp., </journal> <volume> 42(165) </volume> <pages> 115-142, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Orthogonality is a result of the matrix being symmetric. We believe that many of our ideas can be applied to the non-symmetric eigenproblem in order to obtain the "best-conditioned" eigenvectors. Lanczos Algorithm. The method proposed by Lanczos in 1952 [97], after many modifications to account for roundoff <ref> [125, 110, 126] </ref>, has become the champion among algorithms to find some of the extreme eigenvalues of a sparse symmetric matrix. It proceeds by incrementally forming, one row and column at a time, a tridiagonal matrix that is similar to the sparse matrix.
Reference: [127] <author> H. Simon. </author> <title> Bisection is not optimal on vector processors. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10(1) </volume> <pages> 205-209, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: However, bisection is slow if all the eigenvalues are needed. A faster root-finder, such as Zeroin [31, 13], speeds up computation when an eigenvalue is isolated in an interval. Multisection maintains the simplicity of bisection and in certain situations, can speed up the performance on a parallel machine <ref> [106, 11, 127] </ref>. When the k eigenvalues are well separated, inverse iteration can find the eigenvectors independently, each in O (n) time. However, to find eigenvectors of k close eigenvalues, all existing implementations resort to reorthogonalization and this costs O (nk 2 ) operations.
Reference: [128] <author> B. T. Smith, J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide, </title> <booktitle> volume 6 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1976. </year>
Reference-contexts: We then discuss and compare existing methods of solving the resulting tridiagonal problem in Sections 2.2 through 2.6. Later, in Sections 2.7 and 2.8, we show how various issues that arise in implementing inverse iteration are handled in existing LAPACK [1] and EISPACK <ref> [128] </ref> software, and present some examples where they fail to deliver correct answers. Finally, we sketch our alternate approach on handling these issues in Section 2.9. 2.1 Background Eigenvalue computations arise in a rich variety of contexts. <p> A variety of methods exploit the tridiagonal structure to compute (2.1.1). Extensive research has led to plenty of software, especially in the linear algebra software libraries, EISPACK <ref> [128] </ref> and the more recent LAPACK [1]. We will examine existing algorithms and related software in the next section. We now briefly discuss the relative costs of the various components involved in solving the dense symmetric eigenproblem. <p> We now look in detail at two existing implementations of inverse iteration and see how they address the issues discussed in the previous section. EISPACK <ref> [128] </ref> and 22 LAPACK [1] are linear algebra software libraries that contain routines to solve various eigenvalue problems. <p> In the absence of a reliable and cheap procedure to find r, Wilkinson compromised by choosing the starting vector for inverse iteration as P Le, where ~ J ^ I = P LU and e 37 is the vector of all 1's (this led to the choice made in EISPACK <ref> [128] </ref>). A random starting vector is used in the LAPACK implementation of inverse iteration [87]. In this chapter, we show the following 1. Sections 3.1 and 3.2 introduce twisted factorizations that provide an answer to Wilkin-son's problem of choosing which equation to omit. <p> We hope to present more details in the near future [43]. 6.4 Numerical Results In this section, we present a numerical comparison between Algorithm Y and four other software routines for solving the symmetric tridiagonal eigenproblem that are included in the EISPACK <ref> [128] </ref> and LAPACK [1] libraries. These are 1. LAPACK INVIT : The LAPACK implementation of bisection and inverse iteration [88, 84, 87] (subroutines DSTEBZ and DSTEIN); 2.
Reference: [129] <author> G. W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Then diag (J) = 0 , diag (J 1 ) = 0: Proof. In this proof, we will use the famous Cauchy-Binet formula J adj (J) = det (J) I (3.3.33) where adj (J) is the adjugate of J and is the transpose of the matrix of cofactors <ref> [129, p.402] </ref>, to get expressions for elements of J 1 . By (3.3.33), (J 1 ) ii = det (J) Suppose diag (J) = 0. The nonsingularity of J, and Lemma 3.3.1 imply that n must be even.
Reference: [130] <author> W. G. Strang. </author> <title> Implicit difference methods for initial boundary value problems. </title> <journal> J. Math. Anal. Appl., </journal> <volume> 16 </volume> <pages> 188-198, </pages> <year> 1966. </year>
Reference-contexts: We will show some of these applications in this thesis, for other applications see [42, 115, 74]. In addition to the papers reviewed in [108], twisted factorizations have appeared in various contexts in the literature, see <ref> [77, 94, 5, 58, 130, 133, 44] </ref>. For a brief review, see Section 4.1 of [117].
Reference: [131] <editor> A. H. Taub, editor. John von Neumann Collected Works, </editor> <title> volume V, Design of Computers, Theory of Automata and Numerical Analysis. </title> <publisher> Pergamon, Oxford, </publisher> <year> 1963. </year>
Reference: [132] <author> R. van de Geijn. </author> <title> Deferred shifting schemes for parallel QR methods. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 14(1) </volume> <pages> 180-194, </pages> <year> 1993. </year> <month> 167 </month>
Reference-contexts: The O (n 2 ) computation performed in the QR method to find all the eigenvalues is sequential in nature and is not easily parallelized on modern parallel machines despite the attempts in <ref> [96, 132, 93] </ref>. However, the O (n 3 ) computation in accumulating the Givens' rotations into the eigenvector matrix is trivially and efficiently parallelized, see [3] for more details.
Reference: [133] <author> H. A. van der Vorst. </author> <title> Analysis of a parallel solution method for tridiagonal linear systems. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 303-311, </pages> <year> 1987. </year>
Reference-contexts: We will show some of these applications in this thesis, for other applications see [42, 115, 74]. In addition to the papers reviewed in [108], twisted factorizations have appeared in various contexts in the literature, see <ref> [77, 94, 5, 58, 130, 133, 44] </ref>. For a brief review, see Section 4.1 of [117].
Reference: [134] <author> J. H. Wilkinson. </author> <title> The calculation of the eigenvectors of codiagonal matrices. </title> <journal> Computer J., </journal> <volume> 1 </volume> <pages> 90-96, </pages> <year> 1958. </year>
Reference-contexts: In the absence of an efficient procedure to find such a k, Wilkinson proposed choosing P Le as the starting vector, where T I = P LU and e is the vector of all 1's <ref> [134, 136] </ref>. A random starting vector is a popular choice since the probability that it has a negligible component in the desired direction is extremely low, see [87] for a detailed study. III. Scaling of right hand side. <p> 5 The vector obtained in exact arithmetic by ignoring the first equation, with the approxi mation ^ = 1, is x (1) = 6 6 4= " 1 7 7 ; k (T ^ I)x (1) k 2 = p ! 1 as " ! 0: In [136, pp.319-321] and <ref> [134] </ref>, Wilkinson presents a similar example where omitting the last equation results in a poor approximation to an eigenvector.
Reference: [135] <author> J. H. Wilkinson. </author> <title> Rounding Errors in Algebraic Processes. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1963. </year>
Reference: [136] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1965. </year>
Reference-contexts: The method suggested by these factorizations may be thought of as deterministically "picking a good starting vector" for inverse iteration thus avoiding the random choices currently used in LAPACK and solving a question posed by Wilkinson in <ref> [136, p.318] </ref>. Section 3.4 shows how to modify this new method in order to eliminate divisions. We then digress a little and in Sections 3.5 and 3.6 briefly discuss how twisted factorizations can be employed to reveal the rank of denser matrices and guarantee deflation in perfect shift strategies. 3. <p> The solution v (i+1) in (2.7.5) is very sensitive to small changes in when there is more than one eigenvalue near . In <ref> [136, p.329] </ref>, Wilkinson notes that `The extreme sensitivity of the computed eigenvector to very small changes in [ in our notation] may be turned to practical advantage and used to obtain independent eigenvectors corresponding to coincident or pathologically close eigenvalues'. <p> From (2.7.6), assuming that j 1 j t j i j for i 6= 1, ^v 1 is a good approximation to v 1 provided that ~ 1 is not "negligible", i.e., the starting vector must have a non-negligible component in the direction of the desired eigenvector. In <ref> [136, pp.315-321] </ref>, Wilkinson investigates and rejects the choice of e 1 or e n as a starting vector (where e i is the ith column of the n fi n identity matrix). e k is a desirable choice for a starting vector if the kth component of v 1 is above <p> In the absence of an efficient procedure to find such a k, Wilkinson proposed choosing P Le as the starting vector, where T I = P LU and e is the vector of all 1's <ref> [134, 136] </ref>. A random starting vector is a popular choice since the probability that it has a negligible component in the desired direction is extremely low, see [87] for a detailed study. III. Scaling of right hand side. <p> To guarantee (1.1.1), v (i+1) is usually accepted when the norm growth is O (1=n"kT k), see <ref> [136, p.324] </ref> for details. For the basic iteration of (2.7.4) this convergence criterion can always be met in a few iterations, provided the starting vector is not pathologically deficient in the desired eigenvector and j 1 ^ 1 j = O (n"kT k). <p> The random starting vectors are designed to be superior to Tinvit's choice [87]. II. Choice of shift. Even though in exact arithmetic all the eigenvalues of an unreduced tridiagonal matrix are distinct, some of the computed eigenvalues may be identical to working accuracy. In <ref> [136, p.329] </ref>, Wilkinson recommends that pathologically close eigenvalues be perturbed by a small amount in order to get an orthogonal basis of the desired subspace. <p> Even if ^ is a very good approximation to 1 , (2.8.15) indicates that u nn may not be small if v n1 is tiny. It is not at all uncommon for a component of an eigenvector of a tridiagonal matrix to be tiny <ref> [136, pp.317-321] </ref>. xStein's choice of scale factor may lead to unnecessary overflow as shown below. Example 2.8.5 [Undeserved overflow.] Consider the matrix given in (2.8.16). <p> We want to avoid such failures and indicate our approach to the various aspects of inverse iteration discussed above. I. Choice of shift. Of the various issues discussed in Section 2.7, the choice of starting vector and convergence criterion have been extensively studied <ref> [136, 118, 119, 87] </ref>. Surprisingly, the choice of shift for inverse iteration seems to have drawn little attention. We feel that the shift is probably the most important variable in inverse iteration. Examples 2.8.6 and 2.8.7 highlight the importance of shifts that are as accurate as possible. <p> ) 3 7 5 The vector obtained in exact arithmetic by ignoring the first equation, with the approxi mation ^ = 1, is x (1) = 6 6 4= " 1 7 7 ; k (T ^ I)x (1) k 2 = p ! 1 as " ! 0: In <ref> [136, pp.319-321] </ref> and [134], Wilkinson presents a similar example where omitting the last equation results in a poor approximation to an eigenvector. <p> As a result, in both these cases the natural method does not approximate the eigenvector well. Given an accurate ^ , (3.0.7) implies that x (r) will be a good approximation to v j provided the rth component of v j is not small. Wilkinson notes this in <ref> [136, p.318] </ref> and concludes, `Hence if the largest component of u k [v j in our notation] is the rth, then it is the rth equation which should be omitted when computing u k . <p> In order to get single precision accuracy, we may try and execute Algorithm X in double precision arithmetic. However, there are cases when this simple strategy will not work. Consider Wilkinson's matrix W + 21 where the largest pair of eigenvalues agree to more than 16 digits (see <ref> [136, p.309] </ref> for more details). By the theory developed in Section 4.5, the corresponding eigenvectors computed by Algorithm X can be nearly parallel even if we compute in double precision! And indeed in a numerical run, we observe large dot products. <p> We want all locally small eigenvalues, and not just the smallest, to be determined to high relative accuracy. An example sheds some light on the possible scenarios. Example 5.2.4 [2nd Smallest Eigenvalue should be Relatively Well-Conditioned.] Consider Wilkinson's matrix <ref> [136, p.308] </ref>, W + 2 6 6 6 6 6 6 6 4 1 9 1 : : : 1 9 1 3 7 7 7 7 7 7 7 5 125 which has pairs of eigenvalues that are close to varying degrees. <p> In such a case, the tridiagonal matrix is a direct sum of smaller tridiagonals, and orthogonal eigenvectors are trivially obtained from the eigenvectors of these disjoint submatrices by padding them with zeros. However, as Wilkinson observed, eigenvalues can be arbitrarily close without any off-diagonal element being small <ref> [136] </ref>. It turns out that even in such a case, a good orthogonal basis of the invariant subspace can be computed by using suitable, possibly overlapping, submatrices. Thus we can use the following scheme: Algorithm 6.3.1 [Computes orthogonal "eigenvectors" for tight clusters using submatrices.] 1.
Reference: [137] <author> Stephen Wolfram. </author> <title> Mathematica: A System for Doing Mathematics by Computer. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, USA, </address> <note> second edition, </note> <year> 1991. </year>
Reference-contexts: = 2 6 4 " 5 " 1 p (1 + 3 " 1 p (1 2 ") + O (") 7 7 : 1 we artfully constructed this matrix to have the desired behavior which may be verified by using a symbol manipulator such as Maple [21] or Mathematica <ref> [137] </ref> 72 v 2 = 6 6 " p 2 ) + O (" 5=4 ) 2 p 2 ) + O (") 2 4 ) + O (" 3=2 ) 7 7 ; v 2 + ffiv 2 = 2 6 4 q 2 (1 p 2 ) + O

References-found: 137

