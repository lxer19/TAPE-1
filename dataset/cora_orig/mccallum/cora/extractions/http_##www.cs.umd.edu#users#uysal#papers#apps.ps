URL: http://www.cs.umd.edu/users/uysal/papers/apps.ps
Refering-URL: http://www.cs.umd.edu/projects/hpsl/mambo/index.html
Root-URL: 
Email: fuysal,acha,saltzg@cs.umd.edu  
Title: Requirements of I/O Systems for Parallel Machines: An Application-driven Study  
Author: Mustafa Uysal Anurag Acharya Joel Saltz 
Address: College Park 20742  
Affiliation: Department of Computer Science University of Maryland,  
Abstract: I/O-intensive parallel programs have emerged as one of the leading consumers of cycles on parallel machines. This change has been driven by two trends. First, parallel scientific applications are being used to process larger datasets that do not fit in memory. Second, a large number of parallel machines are being used for non-scientific applications. Efficient execution of these applications requires high-performance I/O systems which have been designed to meet their I/O requirements. In this paper, we examine the I/O requirements for data-intensive parallel applications and the implications of these requirements for the design of I/O systems for parallel machines. We attempt to answer the following questions. First, what is the steady-state as well peak I/O rate required? Second, what spatial patterns, if any, occur in the sequence of I/O requests for individual applications? Third, what is the degree of intra-processor and inter-processor locality in I/O accesses? Fourth, does the application structure allow programmers to disclose future I/O requests to the I/O system? Fifth, what patterns, if any, exist in the sequence of inter-arrival times of I/O requests? To address these questions, we have analyzed I/O request traces for a diverse set of I/O-intensive parallel applications. This set includes seven scientific applications and four non-scientific applications.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya et al. </author> <title> Tuning the performance of I/O-intensive parallel applications. </title> <booktitle> In Proceedings of the 4th IOPADS, </booktitle> <pages> pages 15-27, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: These applications have been tuned, to various degrees, to achieve good I/O performance. We believe that this is important as studying applications which have not been tuned for I/O performance can lead to misleading conclusions <ref> [1] </ref>. We ran these applications on an IBM SP-2 and captured the I/O requests using the trace facility provided by AIX 4.1. In addition, we have acquired the I/O request traces made available by the Pablo group at the University of Illinois, Urbana-Champaign [34]. <p> Examples of the latter include programs being debugged, programs that are compute-bound or communication-bound or programs that run for relatively short periods. Acharya et al <ref> [1] </ref>, Karpovich et al [22] and Smirni et al [42] have looked at individual I/O-intensive applications to determine what is needed to make them run fast. Many of the conclusions reached by these studies are useful for designing I/O systems of parallel machines. <p> The dataset was stored in 8 files, one per processor, and the total execution time was 2,987 seconds. Sparse Cholesky: this application computes Cholesky decomposition for sparse, symmetric positive-definite matrices <ref> [1] </ref>. It stores the sparse matrix as panels (instead of blocks). This application performs I/O using synchronous read/write operations. To drive this application, we used the skirt matrix from NASA that contains over 45 million double-precision nonzeros and 45,361 columns for a total of 437 MB. <p> The first and second group of applications were run on 8 processors of an SP-2, hartree and render were run on 128 processors of a Paragon and escat and sar were run on 256 processors of a Paragon. later paper by Acharya et al <ref> [1] </ref> studied several I/O-intensive parallel applications and found that nested-strided patterns and small request sizes in those applications were caused by placement of I/O operations in inner-loops. Loop reordering following by simple coalescing allowed I/O operations to be placed in outer loops and greatly increased the request size. <p> Several techniques have been proposed for efficient pre-fetching (and caching) which assume that such information is available [24, 35]. It is conjectured that for most I/O-intensive applications, it is not difficult to acquire information about future I/O requests <ref> [1, 35] </ref>. Our experience with the applications examined in this study supports this conjecture. For all applications except web-server, it is not difficult to acquire precise information about future read requests. <p> For cholesky, the sequence of I/O requests can be computed using the elimination-tree which describes the sparsity structure of the matrix to be factorized <ref> [1] </ref>. 7 Possibility of predicting request inter-arrival times One of the problems with effective use of information about future I/O requests is that usually no information is available about the time intervals between successive read requests. <p> In the absence of this information, the I/O system has to guesstimate how far ahead in the request sequence should it prefetch <ref> [1, 35] </ref>. Patterson et al [35] assume that the time between read requests is constant; Acharya et al [1] use application-specific heuristics to compute the prefetch horizon. <p> In the absence of this information, the I/O system has to guesstimate how far ahead in the request sequence should it prefetch [1, 35]. Patterson et al [35] assume that the time between read requests is constant; Acharya et al <ref> [1] </ref> use application-specific heuristics to compute the prefetch horizon. The impact of this lacuna can be even larger for I/O systems that serve multiple applications as it is difficult to impose a reasonable total order on request sequences from different applications.
Reference: [2] <author> R. Agrawal and J. Shafer. </author> <title> Parallel mining of association rules. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(6) </volume> <pages> 962-9, </pages> <month> Dec </month> <year> 1996. </year>
Reference-contexts: First, parallel scientific applications are being used to process larger datasets that do not fit in memory [4, 10, 15, 37, 45]. Second, a large number of parallel machines are being used for non-scientific applications, for example databases [21, 32, 44], data mining <ref> [2, 3, 16] </ref>, web servers for busy web sites (e.g. Altavista and NCSA [23]). Efficient execution of these applications requires high-performance I/O systems which need to be designed to meet application I/O requirements.
Reference: [3] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> In Proc. of 20th Int'l Conf. on Very Large Databases (VLDB), </booktitle> <address> Santiago, Chile, </address> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: First, parallel scientific applications are being used to process larger datasets that do not fit in memory [4, 10, 15, 37, 45]. Second, a large number of parallel machines are being used for non-scientific applications, for example databases [21, 32, 44], data mining <ref> [2, 3, 16] </ref>, web servers for busy web sites (e.g. Altavista and NCSA [23]). Efficient execution of these applications requires high-performance I/O systems which need to be designed to meet application I/O requirements. <p> This application performs I/O using 3 synchronous read operations. We have used a database consisting of 50 million transactions, with an average transaction size of 10 items and maximal potentially frequent set size of 3 (see <ref> [3, 27] </ref> for details). The dataset size for this program was 4 GB and was partitioned into 8 files, one per processor. This application ran for 679 seconds. Parallel Web Server: this application uses a parallel web-server based on the round-robin DNS scheme described by Katz et al [23].
Reference: [4] <author> H. Allik and D. Moore. </author> <title> BBN corporation, </title> <type> personal communication, </type> <month> Sep </month> <year> 1994. </year>
Reference-contexts: This has been changing: I/O-intensive parallel programs have emerged as one of the leading consumers of cycles on parallel machines. This change has been driven by two trends. First, parallel scientific applications are being used to process larger datasets that do not fit in memory <ref> [4, 10, 15, 37, 45] </ref>. Second, a large number of parallel machines are being used for non-scientific applications, for example databases [21, 32, 44], data mining [2, 3, 16], web servers for busy web sites (e.g. Altavista and NCSA [23]).
Reference: [5] <author> V. Almedia, A. Bestavros, M. Crovella, and A. de Oliveira. </author> <booktitle> Characterizing reference locality in the WWW. In Proceedings of 4th Int'l Conf. Parallel and Distributed Systems, </booktitle> <pages> pages 96-103, </pages> <year> 1996. </year>
Reference-contexts: Figure 3 (c) plots the percentage of files with an access-distance less than 100 MB. We note that 40% of all files have an access-distance less than 1 MB. The average access-distance for all files was 20.9 MB (These findings are consistent with the previous research on web workloads <ref> [5] </ref>). This indicates that client-caching with reasonable-sized caches with LRU replacement are likely to be useful for web-server. Finally, for escat, there is read-after-write locality between the two phases.
Reference: [6] <institution> Apache 1.2 HTTP Server. </institution> <note> http://www.apache.org, 1995. </note>
Reference-contexts: This application ran for 679 seconds. Parallel Web Server: this application uses a parallel web-server based on the round-robin DNS scheme described by Katz et al [23]. Similar schemes are used by most busy commercial web sites. We used the Apache 1.2 server <ref> [6] </ref> as the base web server which is replicated on the participating hosts. This application uses multiple processes per processor to implement multiple threads of control. Over the period of a day, it creates a large number of processes (about 2000), most of which terminate relatively soon.
Reference: [7] <author> M. Baker, J. Hartman, M. Kupfer, K. Shirriff, and J. Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 198-212, </pages> <month> Oct </month> <year> 1991. </year>
Reference-contexts: Kotz et al [25, 36] traced I/O requests from a large number of applications at two supercomputing sites. This approach is similar, in spirit, to the well-known studies of distributed file-systems done at Berkeley <ref> [7, 33] </ref> and Carnegie Mellon [29]. This approach provides useful information about the manner in which the parallel machines and their I/O systems are used, it does not distinguish between applications for which I/O performance is critical and applications for which I/O performance is not important.
Reference: [8] <author> R. Bordawekar, A. Choudhary, and J. Del Rosario. </author> <title> An experimental performance evaluation of Touchstone Delta Concurrent File System. </title> <booktitle> In Proceedings of the 7th ACM International Conference on Supercomputing, </booktitle> <pages> pages 367-76, </pages> <year> 1993. </year>
Reference-contexts: Previous research on the characterization the I/O requirements of parallel applications has focused exclusively on scientific applications and has taken one of four approaches. French et al [17], Nitzberg [31] and Bordawekar et al <ref> [8] </ref> used synthetic benchmarks that are intended to emulate what the authors felt are common I/O behaviors. This approach provides information about the I/O capabilities of the machine that the experiments are being run on; does not provide information about application requirements.
Reference: [9] <author> C. Chang, B. Moon, A. Acharya, C. Shock, A. Sussman, and J. Saltz. </author> <title> Titan: a High-Performance Remote-Sensing Database. </title> <booktitle> In Proceedings of the 13th International Conference on Data Engineering, page To appear, </booktitle> <month> Apr </month> <year> 1997. </year> <month> 13 </month>
Reference-contexts: The execution times were 94 seconds for the hierarchy and 41 seconds for the tarfile, respectively. Titan: is a parallel scientific database for remote-sensing data <ref> [9] </ref>. In addition to retrieving data, Titan performs navigation, correction and composition to generate a land-cover image of the region of interest. The region of interest is specified by temporal and geographical (latitude-longitude) ranges. Titan contains two months of data from the NOAA-7 satellite. <p> The total database size is 30 GB which spans 60 data-files. To drive this application, we used three queries which span a 30-day period over the entire globe, North America and Asia respectively (see <ref> [9] </ref> for details). The total execution time for these three queries was 583 seconds. LU decomposition: this application computes the dense LU decomposition of an out-of-core matrix [18]. This application performs I/O using synchronous read/write operations.
Reference: [10] <editor> Dense out-of-core linear solver in a finite element library. </editor> <address> http://www.comco.com/main/phlex/- ProPhlex.html, </address> <year> 1996. </year>
Reference-contexts: This has been changing: I/O-intensive parallel programs have emerged as one of the leading consumers of cycles on parallel machines. This change has been driven by two trends. First, parallel scientific applications are being used to process larger datasets that do not fit in memory <ref> [4, 10, 15, 37, 45] </ref>. Second, a large number of parallel machines are being used for non-scientific applications, for example databases [21, 32, 44], data mining [2, 3, 16], web servers for busy web sites (e.g. Altavista and NCSA [23]).
Reference: [11] <author> P. Corbett et al. </author> <title> Proposal for a common parallel file system programming interface. </title> <note> http://www.cs.arizona.edu/sio/api1.0.ps, September 1996. Version 1.0. </note>
Reference-contexts: (a) Non-Scientific read requests (b) Scientific-1 read requests (c) Scientific-2 read requests (d) Scientific-1 write requests (e) Scientific-2 write requests Based primarily on the studies by Kotz et al, recent proposals for file-system interfaces have included support for scatter-gather requests with small component requests and/or for strided and nested-strided requests <ref> [11, 26, 30] </ref>. We note that for the variety of applications examined in this study, request sizes were 8 usually large and the access patterns were both simpler and more complex than nested strides.
Reference: [12] <author> P. Crandall, R. Aydt, A. Chien, and D. Reed. </author> <title> Input/Output Characteristics of Scalable Parallel Applications. </title> <booktitle> In Proceedings of the Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: However, these studies were limited by the small number of applications each of them studied. In particular, none of them included non-scientific applications. Finally, Cypher et al [14], Reddy et al [38] and Reed et al <ref> [12, 43] </ref> analyzed the behavior of groups of parallel applications that perform I/O. Cypher et al determined the 2 steady-state I/O rate for a set of eight programs. This helps answer one part of one of the questions mentioned above. <p> We refer to these applications as the scientific-2 group of applications. Rendering: this application uses a parallel ray-tracing algorithm to combine planetary imagery with topographic information to create a sequence of three-dimensional perspective views of planetary surfaces <ref> [12] </ref>. This application performs I/O using asynchronous iread operations. For this study, we selected the trace corresponding to a Mars flyby. The total dataset size is about 1 GB; the number of files is about 110. <p> Ab-initio quantum chemistry (Hartree-Fock): this application calculates the non-relativistic interactions among atomic nuclei and electrons. It consists of three parts the first two parts initialize out-of-core data that is used in the third part which dominates the I/O requirements <ref> [12] </ref>. This application performs I/O using synchronous read/write operations. For this study, we selected the trace corresponding to the third part working on a 16-atom dataset. The total I/O volume is 4 GB, the total number of files is 130. <p> The total I/O volume is 4 GB, the total number of files is 130. This trace was collected from a 128 processor run; the total execution time was 1008 seconds. Electron scattering: this application solves electron-scattering problems using the Schwinger multichannel method <ref> [12] </ref>. This application performs I/O using synchronous read/write operations. For this study, we selected the trace corresponding to a dataset with 13 channels (run 8 on the Pablo web-site). This trace was collected from a 256-processor run; the total execution time was about 6000 seconds. <p> This indicates that client-caching with reasonable-sized caches with LRU replacement are likely to be useful for web-server. Finally, for escat, there is read-after-write locality between the two phases. Each processor writes intermediate data to a private file in the earlier phase which it reads back in the later phase <ref> [12] </ref>.
Reference: [13] <author> P. Crandall, A. Chien, and D. Reed. </author> <title> Input/Output characteristics of a synthetic aperture radar application. </title> <note> http://www-pablo.cs.uiuc.edu/Projects/IO/sioDir/sar/sar-analysis.ps.Z, 1995. </note>
Reference-contexts: This trace was collected from a 256-processor run; the total execution time was about 6000 seconds. Synthetic Aperture Radar: this application processes four channels of SAR data to compute high-resolution ground surface images <ref> [13] </ref>. This application performs I/O using synchronous read/write operations. The total input dataset was about 536 MB in four equally-sized files (there are two other small files); the total output was about 51 MB in four equally-sized files.
Reference: [14] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Many of the conclusions reached by these studies are useful for designing I/O systems of parallel machines. However, these studies were limited by the small number of applications each of them studied. In particular, none of them included non-scientific applications. Finally, Cypher et al <ref> [14] </ref>, Reddy et al [38] and Reed et al [12, 43] analyzed the behavior of groups of parallel applications that perform I/O. Cypher et al determined the 2 steady-state I/O rate for a set of eight programs. This helps answer one part of one of the questions mentioned above.
Reference: [15] <author> Charbel Farhat. </author> <title> Large out-of-core calculation runs on the ibm sp-2. </title> <journal> NAS News, </journal> <volume> 2(11), </volume> <month> Jul-Aug </month> <year> 1995. </year>
Reference-contexts: This has been changing: I/O-intensive parallel programs have emerged as one of the leading consumers of cycles on parallel machines. This change has been driven by two trends. First, parallel scientific applications are being used to process larger datasets that do not fit in memory <ref> [4, 10, 15, 37, 45] </ref>. Second, a large number of parallel machines are being used for non-scientific applications, for example databases [21, 32, 44], data mining [2, 3, 16], web servers for busy web sites (e.g. Altavista and NCSA [23]).
Reference: [16] <author> A. Freitas and S. Lavington. </author> <title> Parallel data mining for very large relational databases. </title> <booktitle> In Proceedings of High-Performance Computing and Networking HPCN Europe, </booktitle> <pages> pages 158-63, </pages> <year> 1996. </year>
Reference-contexts: First, parallel scientific applications are being used to process larger datasets that do not fit in memory [4, 10, 15, 37, 45]. Second, a large number of parallel machines are being used for non-scientific applications, for example databases [21, 32, 44], data mining <ref> [2, 3, 16] </ref>, web servers for busy web sites (e.g. Altavista and NCSA [23]). Efficient execution of these applications requires high-performance I/O systems which need to be designed to meet application I/O requirements.
Reference: [17] <author> J. French, T. Pratt, and M. Das. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 17(1-2):115-21, </volume> <month> Jan-Feb </month> <year> 1993. </year>
Reference-contexts: We included these traces in our analysis to avoid bias due to choice of application domain. Previous research on the characterization the I/O requirements of parallel applications has focused exclusively on scientific applications and has taken one of four approaches. French et al <ref> [17] </ref>, Nitzberg [31] and Bordawekar et al [8] used synthetic benchmarks that are intended to emulate what the authors felt are common I/O behaviors. This approach provides information about the I/O capabilities of the machine that the experiments are being run on; does not provide information about application requirements.
Reference: [18] <author> B. Hendrickson and D. Womble. </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 15(5), </volume> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The total execution time for these three queries was 583 seconds. LU decomposition: this application computes the dense LU decomposition of an out-of-core matrix <ref> [18] </ref>. This application performs I/O using synchronous read/write operations. To drive this application, we used an 8192 fi 8192 double precision matrix (total size 536 MB) with a slab size of 64 columns.
Reference: [19] <author> J. Howard, M. Kazar, , S. Menees, D. Nichols, M. Satyanaryanan, R. Sidebotham, and M. West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> Feb </month> <year> 1988. </year>
Reference-contexts: Efficient execution of these applications requires high-performance I/O systems which need to be designed to meet application I/O requirements. The I/O requirements of I/O-intensive parallel applications are likely to be significantly different from the I/O requirements of Unix/Office applications which have driven the development of most distributed file systems <ref> [19, 40, 41] </ref>. In this paper, we examine the I/O requirements associated with I/O-intensive parallel applications and examine the implications of these requirements on the design of I/O systems for parallel machines.
Reference: [20] <author> IBM Corporation. </author> <title> IBM DATABASE 2 Parallel Edition for AIX Administration Guide and Reference. </title>
Reference-contexts: We also captured a trace of the message-passing activity and context-switches. This allowed us to accurately compute the inter-arrival times for I/O requests and to better understand the application behavior. DB2 Parallel Edition: this is the commercial-grade parallel RDBMS from IBM <ref> [20] </ref>. It partitions large relations using one of the attributes or a set of the attributes. To drive this application, we constructed a simulated database containing records for one year's operation of a large department store. <p> These queries perform complex join, set, and aggregate operations on indexed and non-indexed relations. For these queries, DB2 uses a directed outer-table join strategy in which rows of the outer table is hashed (based on the partitioning key) to the node where join is performed <ref> [20] </ref>. The total execution time for all five queries was 7,688 seconds. Data-mining: this application tries to extract association rules from retail data [27] in particular, buying patterns that characterize the shopping behavior of retail customers. This application performs I/O using 3 synchronous read operations.
Reference: [21] <institution> Informix online extended parallel server. </institution> <note> http://www.informix.com. </note>
Reference-contexts: This change has been driven by two trends. First, parallel scientific applications are being used to process larger datasets that do not fit in memory [4, 10, 15, 37, 45]. Second, a large number of parallel machines are being used for non-scientific applications, for example databases <ref> [21, 32, 44] </ref>, data mining [2, 3, 16], web servers for busy web sites (e.g. Altavista and NCSA [23]). Efficient execution of these applications requires high-performance I/O systems which need to be designed to meet application I/O requirements.
Reference: [22] <author> J. Karpovich, J. French, and A. Grimshaw. </author> <title> High-performance access to radio astronomy data: A case study. </title> <booktitle> In Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 240-9, </pages> <month> Sept </month> <year> 1994. </year>
Reference-contexts: Examples of the latter include programs being debugged, programs that are compute-bound or communication-bound or programs that run for relatively short periods. Acharya et al [1], Karpovich et al <ref> [22] </ref> and Smirni et al [42] have looked at individual I/O-intensive applications to determine what is needed to make them run fast. Many of the conclusions reached by these studies are useful for designing I/O systems of parallel machines.
Reference: [23] <author> E. Katz, M. Butler, and R. McGrath. </author> <title> A scalable HTTP server: The NCSA prototype. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 27(2) </volume> <pages> 155-64, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: Second, a large number of parallel machines are being used for non-scientific applications, for example databases [21, 32, 44], data mining [2, 3, 16], web servers for busy web sites (e.g. Altavista and NCSA <ref> [23] </ref>). Efficient execution of these applications requires high-performance I/O systems which need to be designed to meet application I/O requirements. <p> The dataset size for this program was 4 GB and was partitioned into 8 files, one per processor. This application ran for 679 seconds. Parallel Web Server: this application uses a parallel web-server based on the round-robin DNS scheme described by Katz et al <ref> [23] </ref>. Similar schemes are used by most busy commercial web sites. We used the Apache 1.2 server [6] as the base web server which is replicated on the participating hosts. This application uses multiple processes per processor to implement multiple threads of control.
Reference: [24] <author> T. Kimbrel, A. Tomkins, R. Patterson, B. Bershad, P. Cao, E. Felten, G. Gibson, A. Karlin, and K. Li. </author> <title> A trace-driven comparison of algorithms for parallel prefetching and caching. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 19-34, </pages> <month> Oct </month> <year> 1996. </year>
Reference-contexts: Does the application structure allow programmers to disclose future I/O requests to the I/O system? If this is indeed the case, it would provide an opportunity for the I/O system to improve the utilization of the storage devices as well as reduce the latency of I/O requests <ref> [24, 35] </ref>. 5. Finally, are there patterns in the inter-arrival times of I/O requests? This information would allow the I/O system to better schedule prefetches for I/O requests from individual applications as well as from multiple applications. <p> A key requirement for successful pre-fetching is information about future I/O requests. Several techniques have been proposed for efficient pre-fetching (and caching) which assume that such information is available <ref> [24, 35] </ref>. It is conjectured that for most I/O-intensive applications, it is not difficult to acquire information about future I/O requests [1, 35]. Our experience with the applications examined in this study supports this conjecture.
Reference: [25] <author> D. Kotz and N. Nieuwejaar. </author> <title> File-system workload on a scientific multiprocessor. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 3(1) </volume> <pages> 51-60, </pages> <month> Spring </month> <year> 1995. </year>
Reference-contexts: This approach provides information about the I/O capabilities of the machine that the experiments are being run on; does not provide information about application requirements. Kotz et al <ref> [25, 36] </ref> traced I/O requests from a large number of applications at two supercomputing sites. This approach is similar, in spirit, to the well-known studies of distributed file-systems done at Berkeley [7, 33] and Carnegie Mellon [29]. <p> All widely-used file-systems available on Unix platforms implement some form of read-ahead to take advantage of this access pattern. The validity of this observation for parallel platforms has, however, been in question. In their analysis of long-term traces from two parallel machines, Kotz et al <ref> [25, 36] </ref> found that on those machines, in addition to sequentially accessed files, many files were accessed in a strided or nested-strided pattern. <p> In addition, they found that a significant number of the requests were small (e.g., in the iPSC/860 study, 96% of the reads were smaller than 4 KB <ref> [25] </ref>).
Reference: [26] <author> MPI-IO: </author> <title> a parallel file I/O interface for MPI. </title> <address> http://lovelace.nas.nasa.gov/MPI-IO/mpi-io-report.0.5.ps, April 1996. </address>
Reference-contexts: (a) Non-Scientific read requests (b) Scientific-1 read requests (c) Scientific-2 read requests (d) Scientific-1 write requests (e) Scientific-2 write requests Based primarily on the studies by Kotz et al, recent proposals for file-system interfaces have included support for scatter-gather requests with small component requests and/or for strided and nested-strided requests <ref> [11, 26, 30] </ref>. We note that for the variety of applications examined in this study, request sizes were 8 usually large and the access patterns were both simpler and more complex than nested strides.
Reference: [27] <author> Andreas Mueller. </author> <title> Fast sequential and parallel algorithms for association rule mining: A comparison. </title> <type> Technical Report CS-TR-3515, </type> <institution> University of Maryland, College Park, </institution> <month> August </month> <year> 1995. </year> <month> 14 </month>
Reference-contexts: The total execution time for all five queries was 7,688 seconds. Data-mining: this application tries to extract association rules from retail data <ref> [27] </ref> in particular, buying patterns that characterize the shopping behavior of retail customers. This application performs I/O using 3 synchronous read operations. <p> This application performs I/O using 3 synchronous read operations. We have used a database consisting of 50 million transactions, with an average transaction size of 10 items and maximal potentially frequent set size of 3 (see <ref> [3, 27] </ref> for details). The dataset size for this program was 4 GB and was partitioned into 8 files, one per processor. This application ran for 679 seconds. Parallel Web Server: this application uses a parallel web-server based on the round-robin DNS scheme described by Katz et al [23].
Reference: [28] <author> S.S. Mukherjee, S.D. Sharma, M.D. Hill, J.R. Larus, A. Rogers, and J. Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Proceedings of the ACM Symposium on Principles and Practice of Parallel Processing'95, </booktitle> <pages> pages 68-79, </pages> <month> July </month> <year> 1995. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 30, No. </volume> <pages> 8. </pages>
Reference-contexts: This is similar to the record-replay technique used by Mukherjee et al <ref> [28] </ref> for determining the communication patterns for distributed shared memory machines. The primary issue that has to be taken care of is how to detect the end of a repetition.
Reference: [29] <author> L. Mummert and M. Satyanarayanan. </author> <title> Long-term distributed file reference tracing: Implementation and experience. </title> <journal> Software Practice and Experience, </journal> <volume> 26(6) </volume> <pages> 705-36, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Kotz et al [25, 36] traced I/O requests from a large number of applications at two supercomputing sites. This approach is similar, in spirit, to the well-known studies of distributed file-systems done at Berkeley [7, 33] and Carnegie Mellon <ref> [29] </ref>. This approach provides useful information about the manner in which the parallel machines and their I/O systems are used, it does not distinguish between applications for which I/O performance is critical and applications for which I/O performance is not important.
Reference: [30] <author> N. Nieuwejaar and D. Kotz. </author> <title> Low-level interfaces for high-level parallel I/O. </title> <editor> In Ravi Jain, John Werth, and James C. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, volume 362 of The Kluwer International Series in Engineering and Computer Science, chapter 9, </booktitle> <pages> pages 205-23. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: (a) Non-Scientific read requests (b) Scientific-1 read requests (c) Scientific-2 read requests (d) Scientific-1 write requests (e) Scientific-2 write requests Based primarily on the studies by Kotz et al, recent proposals for file-system interfaces have included support for scatter-gather requests with small component requests and/or for strided and nested-strided requests <ref> [11, 26, 30] </ref>. We note that for the variety of applications examined in this study, request sizes were 8 usually large and the access patterns were both simpler and more complex than nested strides.
Reference: [31] <author> B. Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> Dec </month> <year> 1992. </year>
Reference-contexts: We included these traces in our analysis to avoid bias due to choice of application domain. Previous research on the characterization the I/O requirements of parallel applications has focused exclusively on scientific applications and has taken one of four approaches. French et al [17], Nitzberg <ref> [31] </ref> and Bordawekar et al [8] used synthetic benchmarks that are intended to emulate what the authors felt are common I/O behaviors. This approach provides information about the I/O capabilities of the machine that the experiments are being run on; does not provide information about application requirements.
Reference: [32] <institution> Oracle 7 server scalable parallel architecture for open data warehousing. </institution> <address> http://www.oracle.com, Oct 1995. </address>
Reference-contexts: This change has been driven by two trends. First, parallel scientific applications are being used to process larger datasets that do not fit in memory [4, 10, 15, 37, 45]. Second, a large number of parallel machines are being used for non-scientific applications, for example databases <ref> [21, 32, 44] </ref>, data mining [2, 3, 16], web servers for busy web sites (e.g. Altavista and NCSA [23]). Efficient execution of these applications requires high-performance I/O systems which need to be designed to meet application I/O requirements.
Reference: [33] <author> J. Ousterhout, H. Costa, D. Harrison, J. Kunze, M. Kupfer, and J. Thompson. </author> <title> A trace-driven analysis of the Unix 4.2 BSD file system. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems, </booktitle> <pages> pages 15-24, </pages> <month> Dec </month> <year> 1985. </year>
Reference-contexts: Kotz et al [25, 36] traced I/O requests from a large number of applications at two supercomputing sites. This approach is similar, in spirit, to the well-known studies of distributed file-systems done at Berkeley <ref> [7, 33] </ref> and Carnegie Mellon [29]. This approach provides useful information about the manner in which the parallel machines and their I/O systems are used, it does not distinguish between applications for which I/O performance is critical and applications for which I/O performance is not important.
Reference: [34] <editor> I/O request traces from ESCAT, Render, SAR and Hartree-Fock. </editor> <address> http://www-pablo.cs.uiuc.edu/- Projects/IO/io.html, </address> <year> 1996. </year>
Reference-contexts: We ran these applications on an IBM SP-2 and captured the I/O requests using the trace facility provided by AIX 4.1. In addition, we have acquired the I/O request traces made available by the Pablo group at the University of Illinois, Urbana-Champaign <ref> [34] </ref>. These traces correspond to four parallel scientific applications from several domains. We included these traces in our analysis to avoid bias due to choice of application domain. <p> The dataset was stored in 8 files, one per processor, and the total execution time was 2,696 seconds. 4 2.2 Application traces we acquired We acquired four I/O request traces from the web repository provided by the Pablo research group at the University of Illinois <ref> [34] </ref>. These traces were generated using the Pablo instrumentation software [39] on the Intel Paragon at California Institute of Technology. We refer to these applications as the scientific-2 group of applications.
Reference: [35] <author> R. Patterson, G. Gibson, E. Ginting, D. Stodolsky, and J. Zalenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proc. of the 15th Symp. on Operating System Principles, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Does the application structure allow programmers to disclose future I/O requests to the I/O system? If this is indeed the case, it would provide an opportunity for the I/O system to improve the utilization of the storage devices as well as reduce the latency of I/O requests <ref> [24, 35] </ref>. 5. Finally, are there patterns in the inter-arrival times of I/O requests? This information would allow the I/O system to better schedule prefetches for I/O requests from individual applications as well as from multiple applications. <p> A key requirement for successful pre-fetching is information about future I/O requests. Several techniques have been proposed for efficient pre-fetching (and caching) which assume that such information is available <ref> [24, 35] </ref>. It is conjectured that for most I/O-intensive applications, it is not difficult to acquire information about future I/O requests [1, 35]. Our experience with the applications examined in this study supports this conjecture. <p> Several techniques have been proposed for efficient pre-fetching (and caching) which assume that such information is available [24, 35]. It is conjectured that for most I/O-intensive applications, it is not difficult to acquire information about future I/O requests <ref> [1, 35] </ref>. Our experience with the applications examined in this study supports this conjecture. For all applications except web-server, it is not difficult to acquire precise information about future read requests. <p> In the absence of this information, the I/O system has to guesstimate how far ahead in the request sequence should it prefetch <ref> [1, 35] </ref>. Patterson et al [35] assume that the time between read requests is constant; Acharya et al [1] use application-specific heuristics to compute the prefetch horizon. <p> In the absence of this information, the I/O system has to guesstimate how far ahead in the request sequence should it prefetch [1, 35]. Patterson et al <ref> [35] </ref> assume that the time between read requests is constant; Acharya et al [1] use application-specific heuristics to compute the prefetch horizon.
Reference: [36] <author> A. Purakayastha, C. Ellis, D. Kotz, N. Nieuwejaar, and M. </author> <title> Best. Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 165-72, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: This approach provides information about the I/O capabilities of the machine that the experiments are being run on; does not provide information about application requirements. Kotz et al <ref> [25, 36] </ref> traced I/O requests from a large number of applications at two supercomputing sites. This approach is similar, in spirit, to the well-known studies of distributed file-systems done at Berkeley [7, 33] and Carnegie Mellon [29]. <p> All widely-used file-systems available on Unix platforms implement some form of read-ahead to take advantage of this access pattern. The validity of this observation for parallel platforms has, however, been in question. In their analysis of long-term traces from two parallel machines, Kotz et al <ref> [25, 36] </ref> found that on those machines, in addition to sequentially accessed files, many files were accessed in a strided or nested-strided pattern.
Reference: [37] <author> J. Qin, T. Agarwal, O. Storaasali, D. Nguyen, and M. Baddourah. </author> <title> Parallel-vector out-of-core solver for computational mechanics. </title> <booktitle> In Proceedings of the 2nd Symposium on Parallel Computational Methods for Large-scale Structural Analysis and Design, </booktitle> <month> Feb </month> <year> 1993. </year>
Reference-contexts: This has been changing: I/O-intensive parallel programs have emerged as one of the leading consumers of cycles on parallel machines. This change has been driven by two trends. First, parallel scientific applications are being used to process larger datasets that do not fit in memory <ref> [4, 10, 15, 37, 45] </ref>. Second, a large number of parallel machines are being used for non-scientific applications, for example databases [21, 32, 44], data mining [2, 3, 16], web servers for busy web sites (e.g. Altavista and NCSA [23]).
Reference: [38] <author> A. Reddy and P. Bannerjee. </author> <title> A study of I/O behavior of the Perfect benchmarks on a multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 312-21, </pages> <year> 1990. </year>
Reference-contexts: Many of the conclusions reached by these studies are useful for designing I/O systems of parallel machines. However, these studies were limited by the small number of applications each of them studied. In particular, none of them included non-scientific applications. Finally, Cypher et al [14], Reddy et al <ref> [38] </ref> and Reed et al [12, 43] analyzed the behavior of groups of parallel applications that perform I/O. Cypher et al determined the 2 steady-state I/O rate for a set of eight programs. This helps answer one part of one of the questions mentioned above.
Reference: [39] <author> D. Reed et al. </author> <title> Scalable performance analysis: The PABLO performance analysis environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 104-13, </pages> <month> Oct </month> <year> 1993. </year>
Reference-contexts: These traces were generated using the Pablo instrumentation software <ref> [39] </ref> on the Intel Paragon at California Institute of Technology. We refer to these applications as the scientific-2 group of applications. Rendering: this application uses a parallel ray-tracing algorithm to combine planetary imagery with topographic information to create a sequence of three-dimensional perspective views of planetary surfaces [12].
Reference: [40] <author> M. Rosenblum and J. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 26-52, </pages> <month> Feb </month> <year> 1992. </year>
Reference-contexts: Efficient execution of these applications requires high-performance I/O systems which need to be designed to meet application I/O requirements. The I/O requirements of I/O-intensive parallel applications are likely to be significantly different from the I/O requirements of Unix/Office applications which have driven the development of most distributed file systems <ref> [19, 40, 41] </ref>. In this paper, we examine the I/O requirements associated with I/O-intensive parallel applications and examine the implications of these requirements on the design of I/O systems for parallel machines.
Reference: [41] <author> M. Satyanarayanan, J. Kistler, P. Kumar, M. Okasaki, E. Siegel, and D. Steere. Coda: </author> <title> A highly available file system for a distributed workstation environment. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4) </volume> <pages> 447-59, </pages> <month> April </month> <year> 1990. </year> <month> 15 </month>
Reference-contexts: Efficient execution of these applications requires high-performance I/O systems which need to be designed to meet application I/O requirements. The I/O requirements of I/O-intensive parallel applications are likely to be significantly different from the I/O requirements of Unix/Office applications which have driven the development of most distributed file systems <ref> [19, 40, 41] </ref>. In this paper, we examine the I/O requirements associated with I/O-intensive parallel applications and examine the implications of these requirements on the design of I/O systems for parallel machines.
Reference: [42] <author> E. Smirni, R. A. Aydt, A. A. Chien, and D. A. Reed. </author> <title> I/O requirements of scientific applications: An evolutionary view. </title> <booktitle> In Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 49-59, </pages> <address> Syracuse, NY, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Examples of the latter include programs being debugged, programs that are compute-bound or communication-bound or programs that run for relatively short periods. Acharya et al [1], Karpovich et al [22] and Smirni et al <ref> [42] </ref> have looked at individual I/O-intensive applications to determine what is needed to make them run fast. Many of the conclusions reached by these studies are useful for designing I/O systems of parallel machines. However, these studies were limited by the small number of applications each of them studied.
Reference: [43] <author> E. Smirni and D. A. Reed. </author> <title> Workload characterization of input/output intensive parallel applications. In Modelling Techniques and Tools for Computer Performance Evaluation, </title> <month> June </month> <year> 1997. </year> <note> to appear. </note>
Reference-contexts: However, these studies were limited by the small number of applications each of them studied. In particular, none of them included non-scientific applications. Finally, Cypher et al [14], Reddy et al [38] and Reed et al <ref> [12, 43] </ref> analyzed the behavior of groups of parallel applications that perform I/O. Cypher et al determined the 2 steady-state I/O rate for a set of eight programs. This helps answer one part of one of the questions mentioned above.
Reference: [44] <author> Sybase MPP: </author> <title> Parallel High Performance for Real World Workloads. </title> <address> http://www.sybase.com. </address>
Reference-contexts: This change has been driven by two trends. First, parallel scientific applications are being used to process larger datasets that do not fit in memory [4, 10, 15, 37, 45]. Second, a large number of parallel machines are being used for non-scientific applications, for example databases <ref> [21, 32, 44] </ref>, data mining [2, 3, 16], web servers for busy web sites (e.g. Altavista and NCSA [23]). Efficient execution of these applications requires high-performance I/O systems which need to be designed to meet application I/O requirements.
Reference: [45] <author> Sivan Toledo and Fred G. Gustavson. </author> <title> The design and implementation of SOLAR, a portable library for scalable out-of-core linear algebra computations. </title> <booktitle> In Proceedings of the Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 28-40, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: This has been changing: I/O-intensive parallel programs have emerged as one of the leading consumers of cycles on parallel machines. This change has been driven by two trends. First, parallel scientific applications are being used to process larger datasets that do not fit in memory <ref> [4, 10, 15, 37, 45] </ref>. Second, a large number of parallel machines are being used for non-scientific applications, for example databases [21, 32, 44], data mining [2, 3, 16], web servers for busy web sites (e.g. Altavista and NCSA [23]).
Reference: [46] <author> Sun Wu and Udi Manber. </author> <title> agrep A fast approximate pattern-matching tool. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 153-162, </pages> <address> San Francisco, CA, </address> <booktitle> Winter 1992. USENIX. </booktitle> <pages> 16 </pages>
Reference-contexts: Parallel text search: for this application, we used a modified parallel version of the agrep program from University of Arizona which is capable of partial match and approximate searches <ref> [46] </ref>. Our version can operate in one of two modes. In the first mode, the list of files to be searched is partitioned, round-robin, among the processors; in the second mode, each input file is block-partitioned among the processors. This application performs I/O using synchronous read operations.
References-found: 46

