URL: http://www.cs.duke.edu/~jsv/Papers/CKV93.practical-prefetching.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node49.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email pk@cs.brown.edu.  Email jsv@cs.duke.edu.  
Title: SIGMOD '93 Practical Prefetching via Data Compression (extended abstract)  
Author: Kenneth M. Curewitz P. Krishnan Jeffrey Scott Vitter 
Note: Support was provided in part by the Defense Advanced Research Projects Agency under contract N00014-91-J-4052, ARPA order 8225, and by the Office of Naval Research.  Support was provided in part by a National Science Foundation Presidential Young Investigator Award with matching funds from IBM, by Air Force Office of Scientific Research grant number F49620-92-J-0515, and by a Universities Space Research Association/CESDIS associate membership.  
Address: 146 Main Street Maynard, MA 01754  Providence, RI 02912-1910  Durham, NC 27708-0129  
Affiliation: Digital Equipment Corp.  Dept. of Computer Science Brown University  Dept. of Computer Science Duke University  
Abstract: An important issue that affects response time performance in current OODB and hypertext systems is the I/O involved in moving objects from slow memory to cache. A promising way to tackle this problem is to use prefetching, in which we predict the user's next page requests and get those pages into cache in the background. Current databases perform limited prefetching using techniques derived from older virtual memory systems. A novel idea of using data compression techniques for prefetching was recently advocated in [KrV, ViK], in which prefetchers based on the Lempel-Ziv data compressor (the UNIX compress command) were shown theoretically to be optimal in the limit. In this paper we analyze the practical aspects of using data compression techniques for prefetching. We adapt three well-known data compressors to get three simple, deterministic, and universal prefetchers. We simulate our prefetchers on sequences of page accesses derived from the OO1 and OO7 benchmarks and from CAD applications, and demonstrate significant reductions in fault-rate. We examine the important issues of cache replacement, size of the data structure used by the prefetcher, and problems arising from bursts of "fast" page requests (that leave virtually no time between adjacent requests for prefetching and book keeping). We conclude that prediction for prefetching based on data compression techniques holds great promise. fl Support was provided in part by a Digital Equipment Corporation GEEP fellowship. Email curewitz@mast.enet.dec.com.
Abstract-found: 1
Intro-found: 1
Reference: [Bel] <author> L. A. Belady, </author> <title> "A Study of Replacement Algorithms for Virtual Storage Computers," </title> <journal> IBM Systems Journal 5 (1966), </journal> <pages> 78-101. </pages>
Reference-contexts: We find that the page fault rate (number of page faults divided by the length of the access sequence) decreases significantly compared to that of demand fetching, in which the cache is organized using the least-recently used (LRU) heuristic or using the optimal o*ine algorithm, OPT <ref> [Bel] </ref> (in which the page evicted from cache is the one whose next access is furthest in the future). The reduction in fault-rate is also better than that of recent proposed schemes for prefetching [PaZb]. In Section 2 we describe the system environment.
Reference: [BCW] <author> T. C. Bell, J. C. Cleary & I. H. Witten, </author> <title> Text Compression, </title> <publisher> Prentice Hall Adv. Ref. Series, </publisher> <year> 1990. </year>
Reference-contexts: In typical databases, ff is large and k t ff. In this section, we describe our three simple, deterministic prefetching algorithms based on practical data compressors. (An elegant discussion of the data compressors appears in <ref> [BCW] </ref>.) We describe our prefetch-ers in Sections 3.1-3.3 in their "generic" form, as pure prefetchers that can store their entire data structure in cache. These prefetchers make k suggestions for prefetch ordered by their relative merit. To make these suggestions the algorithms use O (k) time. <p> The same comment holds for the PPM and FOM algorithms described below. In our simulations, we use a heuristic for LZ that parallels the Welsh implementation <ref> [BCW] </ref> of the Lempel-Ziv data compressor. While LZ is at a leaf, instead of fetching in k pages at random, it resets its current node to be the root (that is, it goes to the root one step early). <p> In our simulations we use PPM of order 3 and order 1. The various jth-order Markov predictors, j = 0; 1; : : : ; m, can be represented and updated simultaneously in an efficient manner using a forward tree with vine pointers <ref> [BCW] </ref>. (Details of data structure management are omitted in this abstract.) The data structure is "almost" a tree; there can be more than one edge into a node because of vine pointers. 3.3 Algorithm FOM Algorithm FOM is a limited memory prefetcher designed so it can always fit in a small
Reference: [Bra] <author> J. T. Brady, </author> <title> "A theory of productivity in the creative process," </title> <note> IEEE CG&A (May 1986). </note>
Reference-contexts: This method of anticipating and getting pages into cache in the background is called prefetching. Current database systems perform prefetching using techniques derived from older virtual memory systems. The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing <ref> [Bra] </ref>. This has stimulated renewed interest in developing improved algorithms for prefetching [ChB, Lai, MLG, PaZb, RoL].
Reference: [BuB] <author> S. Bunton & G. Borriello, </author> <title> "Practical Dictionary Management for Hardware Data Compression," </title> <institution> Department of Computer Science, University of Wash-ington, FR-35, </institution> <year> 1991. </year>
Reference-contexts: There are also more sophisticated techniques that use an LRU-type strategy on the data structure to maintain its size <ref> [BuB] </ref>.
Reference: [CDN] <author> M. J. Carey, D. J. DeWitt & J. F. Naughton, </author> <title> "The OO7 Benchmark," </title> <booktitle> Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, this proceeding. </booktitle>
Reference-contexts: Any specific knowledge about the sequence of page accesses can be utilized to improve the performance further using the techniques of [FKL]. prefetchers based on them. We simulate our prefetchers on page access sequences derived from the Object Operations (OO1) benchmark [CaS], the OO7 benchmark <ref> [CDN] </ref>, and from CAD applications used at DEC. <p> FOM data structure can always be updated since its data structure is always in cache.) 6 Experimental Results This section presents the results of simulating our prefetcher on access traces generated by a CAD application, the Object Operations Benchmark (OO1), and the OO7 benchmark written at the University of Wis-consin <ref> [CDN] </ref>. We first describe the access traces and then present our results. <p> The more interesting phases include traversal of the structure in both the forward and reverse directions. The OO1 benchmark tests aspects of a DBMS that are critical in computer-aided software engineering (CASE) and computer-aided design (CAD) applications [CaS]. The OO7 benchmark, developed at the University of Wisconsin <ref> [CDN] </ref>, tests critical aspects of object-oriented database systems not covered by other benchmarks. This suite of tests was also run on the DEC Object/DB product used for the OO1 tests. <p> The benchmark performs traversals, associative queries, insert/delete operations, and multiuser tests <ref> [CDN] </ref>. We tested our prefetcher running with traces from the traversal query portion of the benchmark. 3 The traces were provided as part of the DEC-ERP grant 1139. 4 DEC Object/DB is a trademark of Digital Equipment Corporation, Maynard MA.
Reference: [CaS] <author> R. G. G. Cattell & J. Skeen, </author> <title> "Object Operations Benchmark," </title> <journal> ACM Transactions on Database Systems 17 (March 1992), </journal> <pages> 1-31. </pages>
Reference-contexts: Any specific knowledge about the sequence of page accesses can be utilized to improve the performance further using the techniques of [FKL]. prefetchers based on them. We simulate our prefetchers on page access sequences derived from the Object Operations (OO1) benchmark <ref> [CaS] </ref>, the OO7 benchmark [CDN], and from CAD applications used at DEC. <p> The more interesting phases include traversal of the structure in both the forward and reverse directions. The OO1 benchmark tests aspects of a DBMS that are critical in computer-aided software engineering (CASE) and computer-aided design (CAD) applications <ref> [CaS] </ref>. The OO7 benchmark, developed at the University of Wisconsin [CDN], tests critical aspects of object-oriented database systems not covered by other benchmarks. This suite of tests was also run on the DEC Object/DB product used for the OO1 tests.
Reference: [ChB] <author> T. F. Chen & J. L. Baer, </author> <title> "Reducing Memory Latency via Non-blocking and Prefetching Caches," </title> <address> ASPLOS-V , Boston, MA (October 1992). </address>
Reference-contexts: Current database systems perform prefetching using techniques derived from older virtual memory systems. The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing [Bra]. This has stimulated renewed interest in developing improved algorithms for prefetching <ref> [ChB, Lai, MLG, PaZb, RoL] </ref>. Independently to our approach, there has been recent work by Palmer and Zdonik, who use a pattern matching approach to prediction [PaZb], by Salem, who computes various first-order statistics for prediction [Sal], and by Laird who uses a growing-order Markov predictor [Lai]. <p> include a software approach 1 in which the compiler reorders instructions and intro-duces explicit prefetching instructions to reduce the effect of cache misses [MLG], a hardware scheme of non-blocking and prefetching caches that lets processing continue when a cache miss occurs, blocking only when the missed data is actually needed <ref> [ChB] </ref>, and a combined hardware and software approach which uses an optimizing compiler and speculative loads to issue read requests in anticipation of a demand request [RoL]. The idea of using data compression techniques for prefetching was first advocated by Vitter and Krish-nan [KrV, ViK].
Reference: [FKL] <author> A. Fiat, R. M. Karp, M. Luby, L. A. McGeoch, D. D. Sleator & N. E. Young, </author> <title> "Competitive Paging Algorithms," </title> <address> CMU, CS-88-196, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: The usefulness of universality is extremely significant in current databases [Sal]. Any specific knowledge about the sequence of page accesses can be utilized to improve the performance further using the techniques of <ref> [FKL] </ref>. prefetchers based on them. We simulate our prefetchers on page access sequences derived from the Object Operations (OO1) benchmark [CaS], the OO7 benchmark [CDN], and from CAD applications used at DEC.
Reference: [GrR] <author> J. Gray & A. Reuter, </author> <title> Transaction Processing: Concepts and Techniques, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: The server has the ability to handle demand read requests from the application and prefetch read requests from the prefetcher. The server gives priority to the client's requests, flushing prefetch requests in its queue when a demand request arrives. Such provisions are generally available in prefetching systems <ref> [GrR, PaZa] </ref>. The prefetcher can be either part of the application or a separate entity distinct from the application. 2 It works by processing the sequence of the client's previous page requests and making requests for data from the server.
Reference: [HoV] <author> P. G. Howard & J. S. Vitter, </author> <title> "Analysis of Arithmetic Coding for Data Compression," </title> <booktitle> Information Processing and Management 28 (1992), </booktitle> <pages> 749-763, </pages> <note> invited paper in special issue on data compression for images and texts. </note>
Reference-contexts: The equivalent character-based algorithm E builds in an online fashion a probabilistic model that feeds probability information to an arithmetic coder <ref> [HoV, Lan, WNC] </ref>. (The exact compression method is irrelevant for our current discussion and is omitted.) We show by an example how the probabilistic model is built. Example 1 Assume for simplicity 2 that our alphabet is fa; bg.
Reference: [KPR] <author> A. R. Karlin, S. J. Phillips & P. Raghavan, </author> <title> "Markov Paging," </title> <booktitle> Proceedings of the 33rd Annual IEEE Conference on Foundations of Computer Science (October 1992). </booktitle>
Reference-contexts: Can our strategy of using LRU with prefetching be shown to be optimal in some reasonable models? Otherwise, is there some other provably optimal cache replacement strategy that can be blended with prefetchers? We expect that recent work on caching models in <ref> [KPR] </ref> may be relevant. Can our techniques be extended for prefetching in parallel environments? Acknowledgements. We would like to thank Mark Palmer from Digital for his support in providing us with access traces and for many useful discussions and comments.
Reference: [KoE] <author> D. F. Kotz & C. S. Ellis, </author> <title> "Prefetching in File Systems for MIMD Multiprocessors," </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1 (April 1990), </journal> <pages> 218-230. </pages>
Reference-contexts: Prefetching in a parallel environment is studied in <ref> [KoE] </ref>.
Reference: [KrV] <author> P. Krishnan & J. S. Vitter, </author> <title> "Optimal Prefetching in the Worst Case," manuscript (November 1992). </title>
Reference-contexts: The idea of using data compression techniques for prefetching was first advocated by Vitter and Krish-nan <ref> [KrV, ViK] </ref>. The intuition is that data compressors typically operate by postulating (either implicitly or explicitly) a dynamic probability distribution on the data to be compressed. Data expected with high probability are encoded with few bits, and unexpected data with many bits. <p> This is extended to worst-case page access sequences in <ref> [KrV] </ref>. In this paper we analyze the practical issues of using data compression techniques for prefetching. Although the pure prefetching assumption in [ViK] may be valid in some hypertext applications, in general the time between user page requests will not allow k prefetches at a time. <p> However, it updates the transition counts for both the leaf node and the root. 3.2 Algorithm PPM Although the LZ prefetcher is theoretically optimal in the limit <ref> [KrV, ViK] </ref>, convergence to optimality is slow. This motivates us to adapt for prefetching the prediction-by-partial-match (PPM) data compressors, which perform better in practice for compression of text than the Lempel-Ziv algorithm. <p> The negative slope of the lines suggest that making more than one prefetch at each time step (if possible) has added benefits. This justifies the argument presented at the end of Section 4.2. 7 Conclusions We started with the theoretical result from <ref> [KrV, ViK] </ref> that using data compression for prefetching is optimal in the limit. We observed that the practical issues in prefetching in databases are much different from the practical issues in data compression, and the pure prefetching assumption made in [KrV, ViK], although valid for some hypertext systems, needs to be <p> 4.2. 7 Conclusions We started with the theoretical result from <ref> [KrV, ViK] </ref> that using data compression for prefetching is optimal in the limit. We observed that the practical issues in prefetching in databases are much different from the practical issues in data compression, and the pure prefetching assumption made in [KrV, ViK], although valid for some hypertext systems, needs to be relaxed while looking at general databases. Motivated thus, we converted three practical data compressors to get three practical prefetchers.
Reference: [Lai] <author> P. Laird, </author> <title> "Discrete Sequence Prediction and its Applications," </title> <institution> AI Research Branch, NASA Ames Research Center, </institution> <type> manuscript, </type> <year> 1992. </year>
Reference-contexts: Current database systems perform prefetching using techniques derived from older virtual memory systems. The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing [Bra]. This has stimulated renewed interest in developing improved algorithms for prefetching <ref> [ChB, Lai, MLG, PaZb, RoL] </ref>. Independently to our approach, there has been recent work by Palmer and Zdonik, who use a pattern matching approach to prediction [PaZb], by Salem, who computes various first-order statistics for prediction [Sal], and by Laird who uses a growing-order Markov predictor [Lai]. <p> Independently to our approach, there has been recent work by Palmer and Zdonik, who use a pattern matching approach to prediction [PaZb], by Salem, who computes various first-order statistics for prediction [Sal], and by Laird who uses a growing-order Markov predictor <ref> [Lai] </ref>. Prefetching in a parallel environment is studied in [KoE].
Reference: [Lan] <author> G. G. Langdon, </author> <title> "An Introduction to Arithmetic Coding," </title> <institution> IBM J. Res. Develop. </institution> <month> 28 (March </month> <year> 1984), </year> <pages> 135-149. </pages>
Reference-contexts: The equivalent character-based algorithm E builds in an online fashion a probabilistic model that feeds probability information to an arithmetic coder <ref> [HoV, Lan, WNC] </ref>. (The exact compression method is irrelevant for our current discussion and is omitted.) We show by an example how the probabilistic model is built. Example 1 Assume for simplicity 2 that our alphabet is fa; bg.
Reference: [MLG] <author> T. C. Mowry, M. S. Lam & A. Gupta, </author> <title> "Design and Evaluation of a Compiler Algorithm for Prefetch-ing," </title> <address> ASPLOS-V , Boston, MA (October 1992). </address>
Reference-contexts: Current database systems perform prefetching using techniques derived from older virtual memory systems. The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing [Bra]. This has stimulated renewed interest in developing improved algorithms for prefetching <ref> [ChB, Lai, MLG, PaZb, RoL] </ref>. Independently to our approach, there has been recent work by Palmer and Zdonik, who use a pattern matching approach to prediction [PaZb], by Salem, who computes various first-order statistics for prediction [Sal], and by Laird who uses a growing-order Markov predictor [Lai]. <p> Prefetching in a parallel environment is studied in [KoE]. Research projects in prefetching at a lower level of abstraction include a software approach 1 in which the compiler reorders instructions and intro-duces explicit prefetching instructions to reduce the effect of cache misses <ref> [MLG] </ref>, a hardware scheme of non-blocking and prefetching caches that lets processing continue when a cache miss occurs, blocking only when the missed data is actually needed [ChB], and a combined hardware and software approach which uses an optimizing compiler and speculative loads to issue read requests in anticipation of a
Reference: [PaZa] <author> M. Palmer & S. Zdonik, </author> <title> "Predictive Caching," </title> <institution> Brown University, CS-90-29, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: The server has the ability to handle demand read requests from the application and prefetch read requests from the prefetcher. The server gives priority to the client's requests, flushing prefetch requests in its queue when a demand request arrives. Such provisions are generally available in prefetching systems <ref> [GrR, PaZa] </ref>. The prefetcher can be either part of the application or a separate entity distinct from the application. 2 It works by processing the sequence of the client's previous page requests and making requests for data from the server.
Reference: [PaZb] <author> M. Palmer & S. Zdonik, </author> <title> "Fido: A Cache that Learns to Fetch," </title> <booktitle> Proceedings of the 1991 International Conference on Very Large Databases, </booktitle> <address> Barcelona (September 1991). </address>
Reference-contexts: Current database systems perform prefetching using techniques derived from older virtual memory systems. The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing [Bra]. This has stimulated renewed interest in developing improved algorithms for prefetching <ref> [ChB, Lai, MLG, PaZb, RoL] </ref>. Independently to our approach, there has been recent work by Palmer and Zdonik, who use a pattern matching approach to prediction [PaZb], by Salem, who computes various first-order statistics for prediction [Sal], and by Laird who uses a growing-order Markov predictor [Lai]. <p> This has stimulated renewed interest in developing improved algorithms for prefetching [ChB, Lai, MLG, PaZb, RoL]. Independently to our approach, there has been recent work by Palmer and Zdonik, who use a pattern matching approach to prediction <ref> [PaZb] </ref>, by Salem, who computes various first-order statistics for prediction [Sal], and by Laird who uses a growing-order Markov predictor [Lai]. Prefetching in a parallel environment is studied in [KoE]. <p> The reduction in fault-rate is also better than that of recent proposed schemes for prefetching <ref> [PaZb] </ref>. In Section 2 we describe the system environment. We describe our three prefetchers in Section 3. In Section 4 we look closely at problems stemming from memory and time restrictions unique to prefetching in some systems. We propose solutions to these problems and bound their worst-case behavior. <p> Any cache replacement algorithm can be suitably modified to work with the "generic" prefetchers described earlier. In particular, we can use the probabilities of the generic prefetcher to determine what to evict from cache, or adapt strategies like the MLP replacement strategy from <ref> [PaZb] </ref>, or adapt well-known cache replacement algorithms like FIFO or LRU. In our simulations, we use a version of LRU suitably modified to handle prefetched pages. Prefetched items are put into cache as if they were demand fetched. <p> Statistics are given in Table 1. CAD1 and CAD2 are object ID (UID) traces from a CAD tool written at Digital's CAD/CAM Technology Center in Chelmsford MA. We include them here as a comparison to the Fido <ref> [PaZb] </ref> algorithm that analyzed prefetching on the same traces. The OO1 database benchmark, also known as the "Sun Benchmark," was run on the DEC Object/DB 4 product to generate page fault information for all phases of the benchmark. <p> The cache replacement strategy used in conjunction with the uniform prefetcher is extremely relevant. Our cache replacement scheme performs very well as seen. Some other cache replacement strategy may give even better improvements. For comparison with Fido <ref> [PaZb] </ref>, we simulated our algorithms on the same trace (CAD2) with the same cache sizes for LRU (2,000) and the prefetcher (1,500) as used in [PaZb]. Fido decreased the fault rate from 45.8% to about 23.5%. <p> Our cache replacement scheme performs very well as seen. Some other cache replacement strategy may give even better improvements. For comparison with Fido <ref> [PaZb] </ref>, we simulated our algorithms on the same trace (CAD2) with the same cache sizes for LRU (2,000) and the prefetcher (1,500) as used in [PaZb]. Fido decreased the fault rate from 45.8% to about 23.5%. Our improvement (for PPM of order 1) was better; from 45.8% to 18.2%. (In Fido, the predictor is trained on an access sequence, the model is frozen, and it is used for prefetching on access traces from similar applications.
Reference: [RoL] <author> A. Rogers & K. Li, </author> <title> "Software Support for Speculative Loads," </title> <address> ASPLOS-V , Boston, MA (October 1992). </address>
Reference-contexts: Current database systems perform prefetching using techniques derived from older virtual memory systems. The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing [Bra]. This has stimulated renewed interest in developing improved algorithms for prefetching <ref> [ChB, Lai, MLG, PaZb, RoL] </ref>. Independently to our approach, there has been recent work by Palmer and Zdonik, who use a pattern matching approach to prediction [PaZb], by Salem, who computes various first-order statistics for prediction [Sal], and by Laird who uses a growing-order Markov predictor [Lai]. <p> scheme of non-blocking and prefetching caches that lets processing continue when a cache miss occurs, blocking only when the missed data is actually needed [ChB], and a combined hardware and software approach which uses an optimizing compiler and speculative loads to issue read requests in anticipation of a demand request <ref> [RoL] </ref>. The idea of using data compression techniques for prefetching was first advocated by Vitter and Krish-nan [KrV, ViK]. The intuition is that data compressors typically operate by postulating (either implicitly or explicitly) a dynamic probability distribution on the data to be compressed.
Reference: [Sal] <author> K. Salem, </author> <title> "Adaptive Prefetching for Disk Buffers," </title> <type> CESDIS, </type> <institution> Goddard Space Flight Center, TR-91-64, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: This has stimulated renewed interest in developing improved algorithms for prefetching [ChB, Lai, MLG, PaZb, RoL]. Independently to our approach, there has been recent work by Palmer and Zdonik, who use a pattern matching approach to prediction [PaZb], by Salem, who computes various first-order statistics for prediction <ref> [Sal] </ref>, and by Laird who uses a growing-order Markov predictor [Lai]. Prefetching in a parallel environment is studied in [KoE]. <p> Older virtual memory prefetchers that prefetch pages in sequence, that is, prefetch page i + 1 when page i was being accessed, are not universal. The usefulness of universality is extremely significant in current databases <ref> [Sal] </ref>. Any specific knowledge about the sequence of page accesses can be utilized to improve the performance further using the techniques of [FKL]. prefetchers based on them.
Reference: [Sto] <author> J. A. Storer, </author> <title> Data Compression Methods and Theory, </title> <publisher> Computer Science Press, </publisher> <year> 1988. </year>
Reference-contexts: In some applications this is justified. However, we cannot expect all systems to have this facility. Several techniques are known for limiting data structure size in data compressors <ref> [Sto] </ref>. An explicit upper bound M is placed on the size of the data structure.
Reference: [TsN] <author> M. M. Tsangaris & J. F. Naughton, </author> <title> "On the Performance of Object Clustering Techniques," </title> <booktitle> Proc. of the 1992 ACM SIGMOD International Conference on Management of Data , San Diego, </booktitle> <address> California (June 1992). </address>
Reference-contexts: Clustering algorithms attempt to improve the performance of database systems by 9 placing related sets of objects on the same page in the hope of reducing the average number of I/Os needed to retrieve objects. There has been extensive work in clustering (e.g., <ref> [TsN] </ref> and references therein). It would be interesting to see the combination of clustering and prefetching on response-time performance. Using prefetch data structures for clustering could also be considered. There are many open problems that this work motivates, both theoretical and practical.
Reference: [ViK] <author> J. S. Vitter & P. Krishnan, </author> <title> "Optimal Prefetching via Data Compression," </title> <booktitle> Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science (October 1991), </booktitle> <institution> also appears as Brown Univ. </institution> <type> Tech. Rep. </type> <note> No. CS-91-46. </note>
Reference-contexts: The idea of using data compression techniques for prefetching was first advocated by Vitter and Krish-nan <ref> [KrV, ViK] </ref>. The intuition is that data compressors typically operate by postulating (either implicitly or explicitly) a dynamic probability distribution on the data to be compressed. Data expected with high probability are encoded with few bits, and unexpected data with many bits. <p> This is extended to worst-case page access sequences in [KrV]. In this paper we analyze the practical issues of using data compression techniques for prefetching. Although the pure prefetching assumption in <ref> [ViK] </ref> may be valid in some hypertext applications, in general the time between user page requests will not allow k prefetches at a time. It may actually be prudent in practice to prefetch less than k pages even if there is time (e.g., to avoid burning disk bandwidth). <p> Other changes to the generic algorithms in situations that arise in practice (for example, when the data structure cannot be stored entirely in cache) are discussed in Section 4. 3.1 Algorithm LZ We denote the empty string by . Algorithm LZ <ref> [ViK] </ref> is based on the character-based version E of the Lempel-Ziv algorithm for data compression. The original Lempel-Ziv algorithm [ZiL] is a word-based data compression algorithm. <p> However, it updates the transition counts for both the leaf node and the root. 3.2 Algorithm PPM Although the LZ prefetcher is theoretically optimal in the limit <ref> [KrV, ViK] </ref>, convergence to optimality is slow. This motivates us to adapt for prefetching the prediction-by-partial-match (PPM) data compressors, which perform better in practice for compression of text than the Lempel-Ziv algorithm. <p> The negative slope of the lines suggest that making more than one prefetch at each time step (if possible) has added benefits. This justifies the argument presented at the end of Section 4.2. 7 Conclusions We started with the theoretical result from <ref> [KrV, ViK] </ref> that using data compression for prefetching is optimal in the limit. We observed that the practical issues in prefetching in databases are much different from the practical issues in data compression, and the pure prefetching assumption made in [KrV, ViK], although valid for some hypertext systems, needs to be <p> 4.2. 7 Conclusions We started with the theoretical result from <ref> [KrV, ViK] </ref> that using data compression for prefetching is optimal in the limit. We observed that the practical issues in prefetching in databases are much different from the practical issues in data compression, and the pure prefetching assumption made in [KrV, ViK], although valid for some hypertext systems, needs to be relaxed while looking at general databases. Motivated thus, we converted three practical data compressors to get three practical prefetchers.
Reference: [WNC] <author> I. H. Witten, R. M. Neal & J. G. Cleary, </author> <title> "Arithmetic Coding for Data Compression," </title> <journal> Communications of the ACM 30 (June 1987), </journal> <pages> 520-540. </pages>
Reference-contexts: The equivalent character-based algorithm E builds in an online fashion a probabilistic model that feeds probability information to an arithmetic coder <ref> [HoV, Lan, WNC] </ref>. (The exact compression method is irrelevant for our current discussion and is omitted.) We show by an example how the probabilistic model is built. Example 1 Assume for simplicity 2 that our alphabet is fa; bg.
Reference: [ZiL] <author> J. Ziv & A. Lempel, </author> <title> "Compression of Individual Sequences via Variable-Rate Coding," </title> <journal> IEEE Transactions on Information Theory 24 (September 1978), </journal> <pages> 530-536. </pages>
Reference-contexts: Algorithm LZ [ViK] is based on the character-based version E of the Lempel-Ziv algorithm for data compression. The original Lempel-Ziv algorithm <ref> [ZiL] </ref> is a word-based data compression algorithm.
References-found: 25

