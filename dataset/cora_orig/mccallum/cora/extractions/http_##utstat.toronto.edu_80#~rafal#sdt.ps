URL: http://utstat.toronto.edu:80/~rafal/sdt.ps
Refering-URL: 
Root-URL: 
Author: Rafal Kustra 
Address: Toronto, Canada  
Affiliation: Dept of Prev. Medicine and Biostatistics University of Toronto  
Abstract-found: 0
Intro-found: 1
Reference: [Breiman et al., 1984] <author> Breiman, L., Friedman, J., Olshen, R., and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth. </publisher>
Reference-contexts: Trees are highly adaptive non-parametric models, thus introduce little bias due to distributional assumptions. They offer results which are easily interpretable, provide "automatic" covariate selection and interaction detection techniques and can include natural ways to deal with missing data (the surrogate variable method of <ref> [Breiman et al., 1984] </ref>, or missing level used in Splus [Clark and Pregibon, 1993]). One disadvantage of tree methods is that the resulting prediction surface is not continuous in x, which presents some problems. <p> The algorithm is flexible enough to accommodate various kinds of data by defining a suitable leaf model. Our treatment of trees follows closely the approach taken by <ref> [Breiman et al., 1984] </ref> (CART), who have established the tree methodology in statistics. The soft decision tree idea and the usage of EM algorithm are similar to Hierarchical Mixtures of Experts (HME) proposal [Jordan and Jacobs, 1994]. <p> The division is done in a recursive way: we split the whole pool of cases into two parts, then we work on each part separately and divide them again. There are many RP methods which differ mainly in splitting rules and pruning process. The CART proposal of <ref> [Breiman et al., 1984] </ref> has become a de facto standard among statisticians and biostatisticians. Following is 3 a brief description. 3.1 Growing a CART tree CART works by minimizing, at each split, an over-all loss function. <p> The split resulting in largest decrease is chosen. New partition is created and process begins again. This proceeds until an overly large tree is built, in principle one where no more decrease in loss function is possible. Standard set of questions was defined in <ref> [Breiman et al., 1984] </ref> to give an interpretable model. For continuous or ordered covariate, x t , splits of the type fi : x (i) t &lt; cg for all c 2 &lt; are considered 1 . <p> In <ref> [Breiman et al., 1984] </ref> it is proven that if each tree in the sequence is obtained by cutting off the branch that offers smaller improvement in error function, relative to the size of the branch, the resulting tree minimizes the cost-complexity measure (1).
Reference: [Chambers and Hastie, 1993] <author> Chambers, J. M. and Hastie, T. J., </author> <title> editors (1993). Statistical Models in S. </title> <publisher> Chap-man and Hall. </publisher>
Reference: [Clark and Pregibon, 1993] <author> Clark, L. A. and Pregibon, D. </author> <year> (1993). </year> <title> Tree-based models. In Chambers, </title> <editor> J. M. and Hastie, T. J., editors, </editor> <title> Statistical Models in S, chapter 9. </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: They offer results which are easily interpretable, provide "automatic" covariate selection and interaction detection techniques and can include natural ways to deal with missing data (the surrogate variable method of [Breiman et al., 1984], or missing level used in Splus <ref> [Clark and Pregibon, 1993] </ref>). One disadvantage of tree methods is that the resulting prediction surface is not continuous in x, which presents some problems. <p> The reason is that in our model we use predictors only to construct the structure of the recursive (soft) partitioning, which is used to combine the leaf models. This is what <ref> [Clark and Pregibon, 1993] </ref> call a structural component. The leaves determine the stochastic component, where the actual modeling of the data takes place.
Reference: [Crave and Wahba, 1979] <author> Crave, P. and Wahba, G. </author> <year> (1979). </year> <title> Smoothing noisy data with spline functions. </title> <journal> Nu-merishe Mathematic, </journal> <volume> 31 </volume> <pages> 377-403. </pages>
Reference: [Dempster et al., 1977] <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the em algorithm (with discussion). </title> <journal> Journal of Royal Statistical Society, series B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference-contexts: The G l quantities are products of splitting probabilities on the path to the leaf. Thus the soft decision tree likelihood is a mixture model, where the mixing proportions depend on the predictors. The Expectation-Maximization algorithm of <ref> [Dempster et al., 1977] </ref> for the two level HME model has been developed in [Jordan and Jacobs, 1994]. We have adapted it to the present situation with univariate splits and constant models in each leaf, and extended to cover trees of any depth and structure.
Reference: [Friedman, 1991] <author> Friedman, J. </author> <year> (1991). </year> <title> Multivariate adaptive regression splines (with discussion). </title> <journal> Annals of Statistics, </journal> <volume> 19 </volume> <pages> 1-141. </pages>
Reference-contexts: The parameter c controls the relative cost of the size of the tree. Limited number of simulations showed that c = 2 gave most satisfactory results. Eq. 15 degenerates to 3 times number of leaves if the splits are hard, which is consistent with <ref> [Friedman, 1991] </ref>. 4.4 Pruning of SDT An attempt was made to develop a procedure that would do pruning within the soft splits paradigm. The procedure presented so far uses CART pruning and this is clearly suboptimal with SDT. <p> The respective numbers for SDT were 0.508 (0.02) and 0.542 (0.03). SDT seem to offer a much less distorted view of the probability structure of the data together with a good classification abilities. Another set of simulations was performed, using the examples from <ref> [Friedman, 1991, $4.3,$4.4] </ref>.
Reference: [Hastie and Tibshirani, 1990] <author> Hastie, T. and Tibshirani, R. </author> <year> (1990). </year> <title> Generalized Additive Models. </title> <publisher> Chapman and Hall. </publisher>
Reference: [Jordan and Jacobs, 1994] <author> Jordan, M. and Jacobs, R. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the em algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214. </pages>
Reference-contexts: Our treatment of trees follows closely the approach taken by [Breiman et al., 1984] (CART), who have established the tree methodology in statistics. The soft decision tree idea and the usage of EM algorithm are similar to Hierarchical Mixtures of Experts (HME) proposal <ref> [Jordan and Jacobs, 1994] </ref>. There the model consists of mixture of Generalized Linear Models (GLM), with mixing proportions defined by the product of responsibilities. These are the splitting probabilities on the path to the leaf, which contains the GLM model. <p> Of course, a more flexible model would allow the parameters of the leaf models to depend on x (as is the case in HME <ref> [Jordan and Jacobs, 1994] </ref>); we believe that this would make the interpretation much harder. The reason is that in our model we use predictors only to construct the structure of the recursive (soft) partitioning, which is used to combine the leaf models. <p> Thus the soft decision tree likelihood is a mixture model, where the mixing proportions depend on the predictors. The Expectation-Maximization algorithm of [Dempster et al., 1977] for the two level HME model has been developed in <ref> [Jordan and Jacobs, 1994] </ref>. We have adapted it to the present situation with univariate splits and constant models in each leaf, and extended to cover trees of any depth and structure. EM works by specifying a full likelihood which we cannot evaluate directly due to some unobserved variables. <p> This quantity is easily maximized since it is (after summing over all observations) the weighted log-likelihood for the logistic regression with responses u t and weights h t . An appropriate method here is to use an Iteratively Re-weighted Least Squares (IRLS) with external weights ([McCullagh and Nelder, 1989], <ref> [Jordan and Jacobs, 1994, Appendix A] </ref>). Thus, to summarize, the EM algorithm for the Soft Decision Tree consists of: 1. Calculating h l for leaves, and h t ; u t for each interior node using the current estimate of parameters 2.
Reference: [McCullagh and Nelder, 1989] <author> McCullagh, P. and Nelder, J. </author> <year> (1989). </year> <title> Generalized Linear Models, 2nd ed. </title> <publisher> Chap-man and Hall, London. </publisher>
Reference: [Pei et al., 1995] <author> Pei, Y., Hercz, G., Greenwood, C., Segre, G., Manuel, A., Saiphoo, C., Fenton, S., and Sherrard, D. </author> <year> (1995). </year> <title> Risk factors for renal osteodystrophy: A multivariant analysis. </title> <journal> Journal of Bone and Mineral Research, </journal> <volume> 10(1) </volume> <pages> 149-156. </pages>
Reference-contexts: In Section 6 we will discuss current limitations of, and possible enhancements to the model. 1 type of the disease. 2 Bone disease in dialysis patients As an example of CART, consider a left tree in Fig. 1. The data comes from the study described in <ref> [Pei et al., 1992, Pei et al., 1995] </ref> and concerns different types of bone disease in dialysis patients. There are 4 types of disease: Aluminum, Aplastic, Osteitis Fibrosis/Mixed (OF/Mx) and Mild (the last type being the closest to the "normal" condition).
Reference: [Pei et al., 1992] <author> Pei, Y., Hercz, G., Greenwood, C., Sherrard, D., Segre, G., Manuel, A., Saiphoo, C., and Fenton, S. </author> <year> (1992). </year> <title> Non-invasive prediction of aluminum bone disease in hemo- and peritoneal dialysis patients. </title> <journal> Kidney International, </journal> <volume> 41 </volume> <pages> 1374-1382. </pages>
Reference-contexts: In Section 6 we will discuss current limitations of, and possible enhancements to the model. 1 type of the disease. 2 Bone disease in dialysis patients As an example of CART, consider a left tree in Fig. 1. The data comes from the study described in <ref> [Pei et al., 1992, Pei et al., 1995] </ref> and concerns different types of bone disease in dialysis patients. There are 4 types of disease: Aluminum, Aplastic, Osteitis Fibrosis/Mixed (OF/Mx) and Mild (the last type being the closest to the "normal" condition).
Reference: [Ripley, 1996] <author> Ripley, Brian, D. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Great Britain. </address> <month> 19 </month>
References-found: 12

