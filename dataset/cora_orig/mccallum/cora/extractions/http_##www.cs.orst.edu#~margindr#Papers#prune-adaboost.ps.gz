URL: http://www.cs.orst.edu/~margindr/Papers/prune-adaboost.ps.gz
Refering-URL: http://www.cs.orst.edu/~margindr/Data/publications.html
Root-URL: 
Email: margindr@cs.orst.edu  tgd@cs.orst.edu  
Title: Pruning Adaptive Boosting ICML-97 Final Draft  
Author: Dragos D.Margineantu Thomas G. Dietterich 
Address: Corvallis, OR 97331-3202  Corvallis, OR 97331-3202  
Affiliation: Department of Computer Science Oregon State University  Department of Computer Science Oregon State University,  
Abstract: The boosting algorithm AdaBoost, developed by Freund and Schapire, has exhibited outstanding performance on several benchmark problems when using C4.5 as the "weak" algorithm to be "boosted." Like other ensemble learning approaches, AdaBoost constructs a composite hypothesis by voting many individual hypotheses. In practice, the large amount of memory required to store these hypotheses can make ensemble methods hard to deploy in applications. This paper shows that by selecting a subset of the hypotheses, it is possible to obtain nearly the same levels of performance as the entire set. The results also provide some insight into the behavior of AdaBoost.
Abstract-found: 1
Intro-found: 1
Reference: <author> Agresti, A. </author> <year> (1990). </year> <title> Categorical Data Analysis. </title> <publisher> John Wiley and Sons., Inc. </publisher>
Reference: <author> Bakiri, G. </author> <year> (1991). </year> <title> Converting English text to speech: A machine learning approach. </title> <type> Tech. rep. </type> <institution> 91-30-2, Department of Computer Science, Ore-gon State University, Corvallis, </institution> <address> OR. </address>
Reference-contexts: However, each tree requires 295 Kbytes of memory, so an ensemble of 200 trees requires 59 Mbytes. Similarly, in an application of error-correcting output coding to the NETtalk task <ref> (Bakiri, 1991) </ref>, an ensemble based on 127 decision trees requires 1.3 Mbytes while storing the 20,003-word dictionary itself requires only 590Kbytes, so the ensemble is much bigger than the data set from which it was constructed.
Reference: <author> Bishop, Y. M. M., Fienberg, S. E., & Holland, P. W. </author> <year> (1975). </year> <title> Discrete multivariate analysis: Theory and practice. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference: <author> Breiman, L. </author> <year> (1996a). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24 (2), </volume> <pages> 123-140. </pages>
Reference: <author> Breiman, L. </author> <year> (1996b). </year> <title> Bias, variance, and arcing classifiers. </title> <type> Tech. rep., </type> <institution> Department of Statistics, University of California, Berkeley. </institution>
Reference: <author> Cohen, J. </author> <year> (1960). </year> <title> A coefficient of agreement for nominal scales. </title> <journal> Educational and Psychological Meas., </journal> <volume> 20, </volume> <pages> 37-46. </pages>
Reference: <author> Cover, T., & Thomas, J. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> J.Wiley and Sons, Inc. </publisher>
Reference: <author> Freund, Y., & Schapire, R. </author> <year> (1996). </year> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Proceedings of the International Conference in Machine Learning, </booktitle> <pages> pp. </pages> <address> 148-156 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1995). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <type> Tech. rep., </type> <institution> AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ. </address>
Reference: <author> Friedman, J. H., & Stuetzle, W. </author> <year> (1981). </year> <title> Projection pursuit regression. </title> <journal> J. American Statistical Association, </journal> <volume> 76 (376), </volume> <pages> 817-823. </pages>
Reference-contexts: Our goal is to choose the set of M classifiers that give the best voted performance on the pruning set. We could use a greedy algorithm to approximate this, but we decided to use a more sophisticated search method called backfitting <ref> (Friedman & Stuet-zle, 1981) </ref>. Backfitting proceeds as follows. Like a simple greedy algorithm, it is a procedure for constructing a set U of classifiers by growing U one classifier at a time. The first two steps are identical to the greedy algorithm.
Reference: <author> Merz, C. J., & Murphy, P. M. </author> <year> (1996). </year> <title> UCI repository of machine learning databases. </title> <type> Tech. rep., </type> <institution> U.C. </institution> <address> Irvine, Irvine, CA. [http://www.ics.uci.edu/ ~mlearn/MLRepository.html]. </address>
Reference-contexts: Then it takes another greedy step to expand U . This continues until U contains M classifiers. 4 Experiments and Results We tested these five pruning techniques on ten data sets (see Table 2). Except for the Expf and XD6 data sets, all were drawn from the Irvine Repository <ref> (Merz & Murphy, 1996) </ref>. Expf is a synthetic data set with only 2 features. Data points are drawn uniformly from the rectangle x 2 [10; +10]; y 2 [10; +10] and labeled according to the decision boundaries shown in Figure 2.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Empirical Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: 1 Introduction The adaptive boosting algorithm AdaBoost (Fre-und & Schapire, 1995) in combination with the decision-tree algorithm C4.5 <ref> (Quinlan, 1993) </ref> has been shown to be a very accurate learning procedure (Freund & Schapire, 1996; Quinlan, 1996; Breiman, 1996b). Like all ensemble methods, AdaBoost works by generating a set of classifiers and then voting them to classify test examples.
Reference: <author> Quinlan, J. </author> <year> (1996). </year> <title> Bagging, boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 725-730 Cambridge, MA. </address> <publisher> AAAI Press/MIT Press. </publisher>
References-found: 13

