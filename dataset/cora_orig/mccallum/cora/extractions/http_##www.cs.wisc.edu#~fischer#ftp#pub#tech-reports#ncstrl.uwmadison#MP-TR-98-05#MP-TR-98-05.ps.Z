URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-98-05/MP-TR-98-05.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-98-05/
Root-URL: http://www.cs.wisc.edu
Email: email: paulb@cs.wisc.edu, olvi@cs.wisc.edu  
Title: Massive Data Discrimination via Linear Support Vector Machines  
Author: P. S. Bradley and O. L. Mangasarian 
Address: 1210 West Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin  
Abstract: A linear support vector machine formulation is used to generate a fast, finitely-terminating linear-programming algorithm for discriminating between two massive sets in n-dimensional space, where the number of points can be orders of magnitude larger than n. The algorithm creates a succession of sufficiently small linear programs that separate chunks of the data at a time. The key idea is that a small number of support vectors, corresponding to linear programming constraints with positive dual variables, are carried over between the successive small linear programs, each of which containing a chunk of the data. We prove that this procedure is monotonic and terminates in a finite number of steps at an exact solution that leads to a globally optimal separating plane for the entire dataset. Numerical results on fully dense publicly available datasets, numbering 20,000 to 1 million points in 32-dimensional space, confirm the theoretical results and demonstrate the ability to handle very large problems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett. </author> <title> Combining support vector and mathematical programming methods for induction. </title> <type> Unpublished manuscript, </type> <institution> Department of Mathematical Sciences, Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1998. </year>
Reference-contexts: 1 Introduction We consider a linear programming approach <ref> [4, 1] </ref> to support vector machines (SVMs) [21, 22, 7] for the discrimination between two possibly massive datasets. The principal contribution of this work is a novel method for solving linear programs with extremely large number of constraints that is proven to be monotonic and finite.
Reference: [2] <author> K. P. Bennett and J. A. </author> <title> Blue. A support vector machine approach to decision trees. </title> <institution> Department of Mathematical Sciences Math Report No. 97-100, Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1997. </year> <note> http://www.math.rpi.edu/~bennek/. </note>
Reference-contexts: Their number is usually small and it is proportional to the generalization error of the classifier [19]. We note that the support vector machine problem is usually formulated using the 2-norm squared in the objective <ref> [21, 2, 19, 18, 20] </ref> thus leading to a quadratic program instead of our linear program (4), which is considerably more difficult to solve than our linear program (4). 3 LPC: Linear Programming Chunking We consider a general linear program min where c 2 R n , H 2 R mfin
Reference: [3] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrim-ination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: Hence, we attempt to satisfy (2) in some "best" sense, for example, by minimizing the average infeasibilities of (2) by introducing nonnegative slack variables y 2 R m and z 2 R k and solving the following robust linear programming (RLP) formulation originally proposed in <ref> [3] </ref> and effectively used on medical and other databases [14]: min m e T z jAw + efl + e y; Bw efl + e z; y 0; z 0 : (3) The linear program (3) generates defines a separating plane P that approximately satisfies the conditions (2) in the following
Reference: [4] <author> P. S. Bradley and O. L. Mangasarian. </author> <title> Feature selection via concave minimization and support vector machines. </title> <editor> In J. Shavlik, editor, </editor> <booktitle> Machine Learning Proceedings of the Fifteenth International Conference(ICML '98), </booktitle> <pages> pages 82-90, </pages> <address> San Francisco, California, 1998. </address> <publisher> Morgan Kaufmann. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/98-03.ps.Z. </publisher>
Reference-contexts: 1 Introduction We consider a linear programming approach <ref> [4, 1] </ref> to support vector machines (SVMs) [21, 22, 7] for the discrimination between two possibly massive datasets. The principal contribution of this work is a novel method for solving linear programs with extremely large number of constraints that is proven to be monotonic and finite.
Reference: [5] <author> C. J. C. Burges. </author> <title> A tutorial on support vector machines. Data Mining and Knowledge Discovery, </title> <type> 2, </type> <year> 1998. </year>
Reference-contexts: In contrast, the formulation here consists of solving a linear program which is considerably less difficult. For simplicity, our results are given here for a linear discriminating surface, a separating plane. However, extension to nonlinear surfaces such as quadratic [11] or more complex <ref> [5] </ref>, is straightforward and merely requires transforming the input data by some nonlinear transformation into a pos-sibly higher dimensional space in which a separating plane can again be found using linear programming. In Section 2 the linear support vector machine (LSVM) is formulated.
Reference: [6] <author> V. Chvatal. </author> <title> Linear Programming. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: In its dual form our algorithm can be interpreted as a block-column generation method related to column generation methods of Gilmore-Gomory [9], Dantzig-Wolfe [8], <ref> [6, pp 198-200,428-429] </ref> and others [17, pp 243-248], but it differs from active set methods [10, pp 326-330] in that it does not require the satisfaction of a working set of constraints as equalities.
Reference: [7] <author> C. Cortes and V. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-279, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction We consider a linear programming approach [4, 1] to support vector machines (SVMs) <ref> [21, 22, 7] </ref> for the discrimination between two possibly massive datasets. The principal contribution of this work is a novel method for solving linear programs with extremely large number of constraints that is proven to be monotonic and finite.
Reference: [8] <author> G. B. Dantzig and P. Wolfe. </author> <title> Decomposition principle for linear programs. </title> <journal> Operations Research, </journal> <volume> 8 </volume> <pages> 101-111, </pages> <year> 1960. </year>
Reference-contexts: We state now our chunking algorithm and establish its finite termination for the linear program (5), where m may be orders of magnitude larger than n. In its dual form our algorithm can be interpreted as a block-column generation method related to column generation methods of Gilmore-Gomory [9], Dantzig-Wolfe <ref> [8] </ref>, [6, pp 198-200,428-429] and others [17, pp 243-248], but it differs from active set methods [10, pp 326-330] in that it does not require the satisfaction of a working set of constraints as equalities.
Reference: [9] <author> P. C. Gilmore and R. E. Gomory. </author> <title> A linear programming approach to the cutting stock problem. </title> <journal> Operations Research, </journal> <volume> 9 </volume> <pages> 849-859, </pages> <year> 1961. </year>
Reference-contexts: We state now our chunking algorithm and establish its finite termination for the linear program (5), where m may be orders of magnitude larger than n. In its dual form our algorithm can be interpreted as a block-column generation method related to column generation methods of Gilmore-Gomory <ref> [9] </ref>, Dantzig-Wolfe [8], [6, pp 198-200,428-429] and others [17, pp 243-248], but it differs from active set methods [10, pp 326-330] in that it does not require the satisfaction of a working set of constraints as equalities.
Reference: [10] <author> D. G. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <address> second edition, </address> <year> 1984. </year>
Reference-contexts: In its dual form our algorithm can be interpreted as a block-column generation method related to column generation methods of Gilmore-Gomory [9], Dantzig-Wolfe [8], [6, pp 198-200,428-429] and others [17, pp 243-248], but it differs from active set methods <ref> [10, pp 326-330] </ref> in that it does not require the satisfaction of a working set of constraints as equalities.
Reference: [11] <author> O. L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference-contexts: In contrast, the formulation here consists of solving a linear program which is considerably less difficult. For simplicity, our results are given here for a linear discriminating surface, a separating plane. However, extension to nonlinear surfaces such as quadratic <ref> [11] </ref> or more complex [5], is straightforward and merely requires transforming the input data by some nonlinear transformation into a pos-sibly higher dimensional space in which a separating plane can again be found using linear programming. In Section 2 the linear support vector machine (LSVM) is formulated.
Reference: [12] <author> O. L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1969. </year> <note> Reprint: SIAM Classic in Applied Mathematics 10, 1994, Philadelphia. </note>
Reference-contexts: To establish its validity we first prove a lemma. Lemma 3.3 If x solves the linear program (5) and (x; u) 2 R n+m is a Karush-Kuhn-Tucker (KKT) <ref> [12] </ref> point (i.e. a primal-dual optimal pair) such that u I &gt; 0 where I f1; : : : ; mg and u J = 0, J f1; : : : ; mg, I [ J = f1; : : : ; mg, then x 2 arg minfc T x jH <p> Proof The KKT conditions <ref> [12] </ref> for (5) satisfied by (x; u) are: c = H T u; u 0; u T (H x b) = 0; H x b 0; which under the condition u I &gt; 0 imply that H I x = b I ; u J = 0; H J x b
Reference: [13] <author> O. L. Mangasarian. </author> <title> Arbitrary-norm separating plane. </title> <type> Technical Report 97-07, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> May </month> <year> 1997. </year> <note> Operations Research Letters, submitted. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-07.ps.Z. </note>
Reference-contexts: 2 R mfin and B 2 R kfin respectively, we wish to discriminate between them by a separating plane: P = fx j x 2 R n ; x T w = flg; (1) with normal w 2 R n and 1-norm distance to the origin of jflj kwk 1 <ref> [13] </ref>, where k k 1 denotes the 1-norm. Hence we wish to satisfy, in some best sense, the inequalities: Aw efl + e; Bw efl e; (2) where e is a vector of ones of appropriate dimension. <p> Each positive value of y i determines the distance y i kwk 0 <ref> [13, Theorem 2.2] </ref> between a point A i of A lying on the wrong side of the bounding plane x T w = fl + 1 for A, that is A i lying in the open halfspace fx fi fi x T w &lt; fl + 1 g, and the bounding <p> The distance, measured by some norm k k on R n , between these planes is precisely 2 kwk 0 <ref> [13, Theorem 2.2] </ref>. The appended term to the objective function of the RLP (3), kwk 0 2 , is the reciprocal of this distance, which has the effect of maximizing the distance between these two planes to obtain better separation.
Reference: [14] <author> O. L. Mangasarian, W. N. Street, and W. H. Wolberg. </author> <title> Breast cancer diagnosis and prognosis via linear programming. </title> <journal> Operations Research, </journal> <volume> 43(4) </volume> <pages> 570-577, </pages> <month> July-August </month> <year> 1995. </year>
Reference-contexts: satisfy (2) in some "best" sense, for example, by minimizing the average infeasibilities of (2) by introducing nonnegative slack variables y 2 R m and z 2 R k and solving the following robust linear programming (RLP) formulation originally proposed in [3] and effectively used on medical and other databases <ref> [14] </ref>: min m e T z jAw + efl + e y; Bw efl + e z; y 0; z 0 : (3) The linear program (3) generates defines a separating plane P that approximately satisfies the conditions (2) in the following sense.
Reference: [15] <author> G. Melli. </author> <title> Synthetic classification data sets (scds). </title> <note> http://fas.sfu.ca/cs/people/GradStudents/melli/SCDS/., 1997. </note>
Reference: [16] <author> B. A. Murtagh and M. A. Saunders. </author> <title> MINOS 5.0 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year> <note> MINOS 5.4 Release Notes, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: For larger problems with 200,000 points and more we used the Computer Sciences Department Ironsides cluster of 4 Sun Enterprise E6000 machines, each with 16 UltraSPARC II processors for a total of 64 processors and 8 gigabytes of RAM. All linear programs were solved with MINOS <ref> [16] </ref> called from a C implementation of the LPC algorithm. The parameter of (4) was set to 0.05 in all runs. Figures (a)-(b) show results for 200,000 points in R 32 .
Reference: [17] <author> K. G. Murty. </author> <title> Linear Programming. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: In its dual form our algorithm can be interpreted as a block-column generation method related to column generation methods of Gilmore-Gomory [9], Dantzig-Wolfe [8], [6, pp 198-200,428-429] and others <ref> [17, pp 243-248] </ref>, but it differs from active set methods [10, pp 326-330] in that it does not require the satisfaction of a working set of constraints as equalities.
Reference: [18] <author> E. Osuna, R. Freund, and F. Girosi. </author> <title> Improved training algorithm for support vector machines. </title> <booktitle> In Proceedings of IEEE NNSP'97, </booktitle> <address> Amelia Island, FL, </address> <month> September </month> <year> 1997, </year> <pages> 276-285, </pages> <year> 1997. </year> <note> http://www.ai.mit.edu/people/girosi/home-page/svm.html. </note>
Reference-contexts: The principal contribution of this work is a novel method for solving linear programs with extremely large number of constraints that is proven to be monotonic and finite. In the standard SVM formulation <ref> [19, 18, 20] </ref> very large quadratic programs are solved. In contrast, the formulation here consists of solving a linear program which is considerably less difficult. For simplicity, our results are given here for a linear discriminating surface, a separating plane. <p> Their number is usually small and it is proportional to the generalization error of the classifier [19]. We note that the support vector machine problem is usually formulated using the 2-norm squared in the objective <ref> [21, 2, 19, 18, 20] </ref> thus leading to a quadratic program instead of our linear program (4), which is considerably more difficult to solve than our linear program (4). 3 LPC: Linear Programming Chunking We consider a general linear program min where c 2 R n , H 2 R mfin
Reference: [19] <author> E. Osuna, R. Freund, and F. Girosi. </author> <title> Training support vector machines: An application to face detection. </title> <booktitle> In IEEE Confernece on Computer Vision and Pattern Recognition, </booktitle> <address> Puerto Rico, </address> <month> June </month> <year> 1997, </year> <pages> 130-136, </pages> <year> 1997. </year> <note> http://www.ai.mit.edu/people/girosi/home-page/svm.html. </note>
Reference-contexts: The principal contribution of this work is a novel method for solving linear programs with extremely large number of constraints that is proven to be monotonic and finite. In the standard SVM formulation <ref> [19, 18, 20] </ref> very large quadratic programs are solved. In contrast, the formulation here consists of solving a linear program which is considerably less difficult. For simplicity, our results are given here for a linear discriminating surface, a separating plane. <p> These points are the only data points that are relevant for determining the optimal separating plane. Their number is usually small and it is proportional to the generalization error of the classifier <ref> [19] </ref>. <p> Their number is usually small and it is proportional to the generalization error of the classifier [19]. We note that the support vector machine problem is usually formulated using the 2-norm squared in the objective <ref> [21, 2, 19, 18, 20] </ref> thus leading to a quadratic program instead of our linear program (4), which is considerably more difficult to solve than our linear program (4). 3 LPC: Linear Programming Chunking We consider a general linear program min where c 2 R n , H 2 R mfin
Reference: [20] <author> J. Platt. </author> <title> Sequential minimal optimization: A fast algorithm for training support vector machines. </title> <type> Technical Report 98-14, </type> <institution> Microsoft Research, </institution> <address> Redmond, Washington, </address> <month> April </month> <year> 1998. </year> <note> http://www.research.microsoft.com/~jplatt/smo.html. </note>
Reference-contexts: The principal contribution of this work is a novel method for solving linear programs with extremely large number of constraints that is proven to be monotonic and finite. In the standard SVM formulation <ref> [19, 18, 20] </ref> very large quadratic programs are solved. In contrast, the formulation here consists of solving a linear program which is considerably less difficult. For simplicity, our results are given here for a linear discriminating surface, a separating plane. <p> Their number is usually small and it is proportional to the generalization error of the classifier [19]. We note that the support vector machine problem is usually formulated using the 2-norm squared in the objective <ref> [21, 2, 19, 18, 20] </ref> thus leading to a quadratic program instead of our linear program (4), which is considerably more difficult to solve than our linear program (4). 3 LPC: Linear Programming Chunking We consider a general linear program min where c 2 R n , H 2 R mfin
Reference: [21] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction We consider a linear programming approach [4, 1] to support vector machines (SVMs) <ref> [21, 22, 7] </ref> for the discrimination between two possibly massive datasets. The principal contribution of this work is a novel method for solving linear programs with extremely large number of constraints that is proven to be monotonic and finite. <p> Their number is usually small and it is proportional to the generalization error of the classifier [19]. We note that the support vector machine problem is usually formulated using the 2-norm squared in the objective <ref> [21, 2, 19, 18, 20] </ref> thus leading to a quadratic program instead of our linear program (4), which is considerably more difficult to solve than our linear program (4). 3 LPC: Linear Programming Chunking We consider a general linear program min where c 2 R n , H 2 R mfin
Reference: [22] <author> G. Wahba. </author> <title> Support vector machines, reproducing kernel Hilbert spaces and the randomized GACV. </title> <type> Technical report no. 984, </type> <institution> Department of Statistics, University of Wisconsin, Madison, WI 53706, </institution> <year> 1997. </year> <month> ftp://ftp.stat.wisc.edu/pub/wahba/index.html. </month>
Reference-contexts: 1 Introduction We consider a linear programming approach [4, 1] to support vector machines (SVMs) <ref> [21, 22, 7] </ref> for the discrimination between two possibly massive datasets. The principal contribution of this work is a novel method for solving linear programs with extremely large number of constraints that is proven to be monotonic and finite.
References-found: 22

