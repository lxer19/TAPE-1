URL: ftp://archive.cis.ohio-state.edu/pub/neuroprose/dietterich.error-correcting.ps.Z
Refering-URL: http://vita.mines.edu:3857/0/lpratt/macs570.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Error-Correcting Output Codes: A General Method for Improving Multiclass Inductive Learning Programs  
Author: Thomas G. Dietterich and Ghulum Bakiri 
Address: Corvallis, OR 97331-3202  
Affiliation: Department of Computer Science Oregon State University  
Abstract: Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k &gt; 2 values (i.e., k "classes"). The definition is acquired by studying large collections of training examples of the form hx i ; f(x i )i. Existing approaches to this problem include (a) direct application of multiclass algorithms such as the decision-tree algorithms ID3 and CART, (b) application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and (c) application of binary concept learning algorithms with distributed output codes such as those employed by Sejnowski and Rosenberg in the NETtalk system. This paper compares these three approaches to a new technique in which BCH error-correcting codes are employed as a distributed output representation. We show that these output representations improve the performance of ID3 on the NETtalk task and of backpropagation on an isolated-letter speech-recognition task. These results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multi-class problems. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barnard, E. & Cole, R. A. </author> <year> (1989). </year> <title> A neural-net training program based on conjugate-gradient optimization. </title> <type> Rep. No. CSE 89-014. </type> <institution> Beaverton, OR: Oregon Graduate Institute. </institution>
Reference-contexts: The database is subdivided into 5 parts (named ISOLET1, ISOLET2, etc.) of 30 speakers each. Cole's group has developed a set of 617 features describing each example. Each feature has been scaled to fall in the range [1; +1]. We employed the opt <ref> (Barnard & Cole, 1989) </ref> implementation of backpropagation with conjugate gradient optimization in all of our experiments. In our experiments, we compared the one-per-class approach to a 30-bit (d = 15) BCH code and a 62-bit (d = 31) BCH code.
Reference: <author> Bose, R. C., & Ray-Chaudhuri, D. K. </author> <year> (1960). </year> <title> On a class of error-correcting binary group codes. </title> <journal> Inf. C-ntl., </journal> <volume> 3, </volume> <pages> pp. 68-79. </pages>
Reference: <author> Breiman, L, Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Monterey, CA: </address> <publisher> Wadsworth and Brooks. </publisher>
Reference-contexts: For cases in which f takes only the values f0; 1g|binary functions|there are many algorithms available. For example, the decision tree methods, such as ID3 (Quin-lan, 1983, 1986b) and CART <ref> (Breiman, Friedman, Ol-shen & Stone, 1984) </ref> can construct trees whose leaves are labelled with binary values. Most artificial neural network algorithms, such as the perceptron algorithm (Rosenblatt, 1958) and the error backpropagation (BP) algorithm (Rumelhart, Hinton & Williams, 1986), are best suited to learning binary functions.
Reference: <author> Cole, R. Muthusamy, Y. & Fanty, M. </author> <year> (1990). </year> <title> The ISOLET spoken letter database. </title> <type> Rep. No. CSE 90-004. </type> <institution> Beaverton, OR: Oregon Graduate Institute. </institution> <month> COLT </month> <year> (1988). </year> <editor> Haussler, D. & Pitt, L. (Eds.) </editor> <address> COLT '88, Cambridge, MA: </address> <publisher> Morgan Kaufmann. </publisher> <month> COLT </month> <year> (1989). </year> <editor> Rivest, R. L., Haussler, D., and War-muth, M. K. (Eds.) </editor> <booktitle> COLT '89: Proceedings of the Second Annual Workshop on Computational Learning Theory, </booktitle> <address> Santa Cruz, CA: </address> <publisher> Morgan Kaufmann. </publisher> <month> COLT </month> <year> (1990). </year> <editor> Fulk, M. A., and Case, J. (Eds.) </editor> <booktitle> COLT '90: Proceedings of the Third Annual Workshop on Computational Learning Theory. </booktitle> <address> Rochester, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Furthermore, as the length of the code n is increased, additional performance improvements are obtained. Following this, we replicate these results on the ISOLET isolated-letter speech recognition task <ref> (Cole, Muthusamy & Fanty, 1990) </ref> using a variation on the back propagation algorithm. Our error-correcting codes give the best performance attained so far by any method on this task. This shows that the method is domain-independent and algorithm-independent. <p> Ron Cole has made available to us his ISOLET database of 7,797 training examples of spoken letters <ref> (Cole, Muthusamy & Fanty, 1990) </ref>. The database was recorded from 150 speakers balanced for sex and representing many different accents and dialects. Each speaker spoke each of the 26 letters twice (except for a few cases).
Reference: <author> Dietterich, T. G., Hild, H., Bakiri, G. </author> <title> (1990a) A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> 7th Int. Conf. on Mach. Learn. </booktitle> <pages> (pp. 24-31). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dietterich, T. G., Hild, H., Bakiri, G. </author> <title> (1990b) A comparison of ID3 and backpropagation for English text-to-speech mapping. </title> <type> Rep. </type> <institution> No. 90-30-4. Corvallis, OR: Oregon State University. </institution>
Reference: <author> Hocquenghem, A. </author> <year> (1959). </year> <title> Codes corecteurs d'erreurs. </title> <journal> Chiffres, </journal> <volume> 2, </volume> <pages> pp. 147-156. </pages>
Reference: <author> Lang, K. J, Waibel, A. H, & Hinton, G. E. </author> <year> (1990). </year> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 33-43. </pages>
Reference: <author> Lin, S., & Costello, D. J. Jr. </author> <year> (1983). </year> <title> Error Control Coding: Fundamentals and Applications. </title> <address> En-glewood Cliffs: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Error Correcting Codes The satisfactory performance of distributed output codes prompted us to explore the utility of good error-correcting codes. We applied BCH methods <ref> (Lin & Costello, 1983) </ref> to design error-correcting codes of varying lengths. These methods guarantee that the rows of the code (i.e., the codewords) will be separated from each other by some minimum Hamming distance d.
Reference: <author> Nilsson, N. J., </author> <year> (1965). </year> <title> Learning Machines, </title> <address> New York: </address> <publisher> McGraw Hill. </publisher>
Reference-contexts: To assign a new case, x to one of these classes, each of the f i is evaluated on x, and x is assigned the class j of the function f j that returns the highest activation <ref> (Nilsson, 1965) </ref>. We will call this the one-per-class approach, since one binary function is learned for each class. Finally, a third approach is to employ a distributed output code.
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess endgames, </title> <editor> in Michalski, R. S., Carbonell, J. & Mitchell, T. M., (eds.), </editor> <booktitle> Machine learning, Vol. I, </booktitle> <address> Palo Alto: </address> <publisher> Tioga Press. </publisher> <pages> 463-482. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986a). </year> <title> The effect of noise on concept learning. </title> <editor> In Michalski, R. S., Carbonell, J. & Mitchell, T. M., (eds.), </editor> <booktitle> Machine learning, Vol. II, </booktitle> <address> Palo Alto: </address> <publisher> Tioga Press. </publisher> <pages> 149-166. </pages>
Reference-contexts: In Dietterich, Hild, and Bakiri (1990a), we called this "observed decoding." The ID3 Learning Algorithm ID3 is a simple decision-tree learning algorithm developed by Ross Quinlan (1983, 1986b). In our implementation, we did not employ windowing, CHI-square forward pruning <ref> (Quinlan, 1986a) </ref>, or any kind of reverse pruning (Quinlan, 1987). Experiments reported in Dietterich, Hild, and Bakiri (1990b) have shown that these pruning methods do not improve performance.
Reference: <author> Quinlan, J. R. </author> <year> (1986b). </year> <title> Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <pages> 81-106. </pages>
Reference: <author> Quinlan, J. R., </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <booktitle> Int. </booktitle>
Reference-contexts: In Dietterich, Hild, and Bakiri (1990a), we called this "observed decoding." The ID3 Learning Algorithm ID3 is a simple decision-tree learning algorithm developed by Ross Quinlan (1983, 1986b). In our implementation, we did not employ windowing, CHI-square forward pruning (Quinlan, 1986a), or any kind of reverse pruning <ref> (Quinlan, 1987) </ref>. Experiments reported in Dietterich, Hild, and Bakiri (1990b) have shown that these pruning methods do not improve performance.
Reference: <author> J. </author> <title> Man-Mach. </title> <journal> Stud., </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference: <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The perceptron. </title> <journal> Psych. Rev., </journal> <volume> 65 (6), </volume> <pages> 386-408. </pages>
Reference-contexts: For example, the decision tree methods, such as ID3 (Quin-lan, 1983, 1986b) and CART (Breiman, Friedman, Ol-shen & Stone, 1984) can construct trees whose leaves are labelled with binary values. Most artificial neural network algorithms, such as the perceptron algorithm <ref> (Rosenblatt, 1958) </ref> and the error backpropagation (BP) algorithm (Rumelhart, Hinton & Williams, 1986), are best suited to learning binary functions. Theoretical studies of learning have focused almost entirely on learning binary functions (Valiant, 1984; COLT 1988, 1989, 1990).
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. & McClelland, J. L., (eds.) </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol 1. </volume> <pages> 318-362. </pages>
Reference-contexts: For example, the decision tree methods, such as ID3 (Quin-lan, 1983, 1986b) and CART (Breiman, Friedman, Ol-shen & Stone, 1984) can construct trees whose leaves are labelled with binary values. Most artificial neural network algorithms, such as the perceptron algorithm (Rosenblatt, 1958) and the error backpropagation (BP) algorithm <ref> (Rumelhart, Hinton & Williams, 1986) </ref>, are best suited to learning binary functions. Theoretical studies of learning have focused almost entirely on learning binary functions (Valiant, 1984; COLT 1988, 1989, 1990).
Reference: <author> Sejnowski, T. J., and Rosenberg, C. R. </author> <year> (1987). </year> <title> Parallel networks that learn to pronouce English text. </title> <journal> Complex Syst., </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference-contexts: These binary functions are usually chosen to be meaningful, and often independent, properties in the domain. For example, in the NETtalk system <ref> (Sejnowski & Rosenberg, 1987) </ref>, a 26-bit distributed code was used to represent phonemes and stresses. <p> train than others (i.e., do they require more training examples to achieve the same level of performance)? Third, are there principled methods for designing good distributed output codes? To answer these questions, this paper begins with a study in which the decision-tree algorithm ID3 is applied to the NETtalk task <ref> (Sejnowski & Rosenberg, 1987) </ref> using three different techniques: the direct mul-ticlass approach, the one-per-class approach, and the distributed output code approach. The results show that the multiclass and distributed output code approaches generalize much better than the one-per-class approach.
Reference: <author> Shavlik, J. W., Mooney, R. J., and Towell, G. G. </author> <year> (1990). </year> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Mach. Learn., </journal> <volume> 6, </volume> <pages> 111-144. </pages>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> CACM, </journal> <volume> 27, </volume> <pages> 1134-1142. 6 </pages>
References-found: 20

