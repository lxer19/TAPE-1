URL: http://www.cs.cornell.edu/Info/People/singhal/papers/trec3.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/singhal/singhal.html
Root-URL: 
Title: Automatic Query Expansion Using SMART TREC 3  
Author: Chris Buckley Gerard Salton, James Allan, Amit Singhal 
Abstract: The Smart information retrieval project emphasizes completely automatic approaches to the understanding and retrieval of large quantities of text. We continue our work in TREC 3, performing runs in the routing, ad-hoc, and foreign language environments. Our major focus is massive query expansion: adding from 300 to 530 terms to each query. These terms come from known relevant documents in the case of routing, and from just the top retrieved documents in the case of ad-hoc and Spanish. This approach improves effectiveness from 7% to 25% in the various experiments. Other ad-hoc work extends our investigations into combining global similarities, giving an overall indication of how a document matches a query, with local similarities identifying a smaller part of the document which matches the query. Using an overlapping text window definition of "local", we achieve a 16% improvement. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Chris Buckley, James Allan, and Gerard Salton. </author> <title> Automatic routing and ad-hoc retrieval using SMART : TREC 2. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> Proceedings of the Second Text REtrieval Conference (TREC-2), </booktitle> <pages> pages 45-56. </pages> <note> NIST Special Publication 500-215, </note> <month> March </month> <year> 1994. </year>
Reference-contexts: Routing Experiments Our routing experiments in TREC 3 are only slightly different from those carried out for TREC 2. The basic routing approach chosen is the feedback approach of Rocchio <ref> [11, 15, 1] </ref>. Expressed in vector space terms, the final query vector is the initial query vector moved toward the centroid of the relevant documents, and away from the centroid of the non-relevant documents.
Reference: [2] <author> Chris Buckley, Gerard Salton, and James Allan. </author> <title> Automatic retrieval with locality information using SMART. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> Proceedings of the First Text REtrieval Conference (TREC-1), </booktitle> <pages> pages 59-72. </pages> <note> NIST Special Publication 500-207, </note> <month> March </month> <year> 1993. </year>
Reference: [3] <author> Chris Buckley, Gerard Salton, and James Allan. </author> <title> The effect of adding relevance information in a relevance feedback environment. </title> <editor> In W. Bruce Croft and C.J. van Rijsbergen, editors, </editor> <booktitle> Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 292-300, </pages> <address> New York, July 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Proper weighting allows us to massively expand the query by adding any term for which we have any evidence of usefulness. Experiments show that effectiveness improves linearly as the log of the number of added terms, up to a point of diminishing improvement <ref> [3] </ref>. This point of diminishing returns for the TREC environment seems to be about 300 terms. <p> The parameters of Rocchio's method are the relative importance of the original query, the relevant documents, and the non-relevant documents (A,B,C above); and then exactly which terms are to be considered part of the final vector. The investigations of TREC 2 and elsewhere <ref> [3] </ref> suggest that the decision of which terms to add is not a hard decision: just add all terms occurring in relevant documents that can be efficiently handled. <p> This forms the queries for our CrnlRR run. Query-by-Query Variations While the massive expansion Rocchio approach works well for most queries, examining past individual query results reveals that for about 15% of the queries, not expanding works better than massive expansion <ref> [3] </ref>. Run Best median &lt; median CrnlRR 1 44 5 CrnlQR 2 37 11 Table 1: Comparative Routing Results Run X:Y A:B:C R-prec Total Rel recall-prec 1. no fdbk 0.0 8.0.0 3461 5975 2985 2. no expand 0.0 8.8.4 3698 6342 3163 3. CrnlRR 300.30 8.16.4 4064 7134 3699 4.
Reference: [4] <author> James P. Callan and W. Bruce Croft. </author> <title> An evaluation of query processing strategies using the TIPSTER collection. </title> <editor> In Robert Korfhage, Edie Rasmussen, and Peter Willett, editors, </editor> <booktitle> Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 347-355, </pages> <address> New York, </address> <month> June </month> <year> 1993. </year> <institution> Association for Computer Machinery. </institution>
Reference-contexts: Our efforts to improve effectiveness using local disambiguation using sentences and short paragraphs did not work in TREC 1 and TREC 2. For TREC 3, we lengthen our local contexts, and treat the local passage as being a mini-document. Adopting the approach of UMass <ref> [17, 4, 5] </ref>, we define our local contexts to be a set of overlapping text windows, each of fixed size. This avoids the length and normalization problems that adversely affected our approach in TREC 2.
Reference: [5] <author> W. Bruce Croft, James Callan, and John Broglio. </author> <title> TREC-2 routing and ad-hoc retrieval evaluation using the INQUERY system. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> Proceedings of the Second Text REtrieval Conference (TREC-2), </booktitle> <pages> pages 75-84. </pages> <note> NIST Special Publication 500-215, </note> <month> March </month> <year> 1994. </year>
Reference-contexts: Our efforts to improve effectiveness using local disambiguation using sentences and short paragraphs did not work in TREC 1 and TREC 2. For TREC 3, we lengthen our local contexts, and treat the local passage as being a mini-document. Adopting the approach of UMass <ref> [17, 4, 5] </ref>, we define our local contexts to be a set of overlapping text windows, each of fixed size. This avoids the length and normalization problems that adversely affected our approach in TREC 2.
Reference: [6] <author> E. Efthimiadis and P. Biron. UCLA-Okapi at TREC-2: </author> <title> Query expansion experiments. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> Proceedings of the Second Text REtrieval Conference (TREC-2), </booktitle> <pages> pages 279-290. </pages> <note> NIST Special Publication 500-215, </note> <month> March </month> <year> 1994. </year>
Reference-contexts: So there is more of a chance for terms from the relevant retrieved documents to meaningfully distinguish themselves from the terms in the non-relevant documents. Other groups in TREC 2 were able to take advantage of this situation and improve performance, noticeably UCLA and CMU <ref> [6, 7] </ref>. Another focus of our work the past few years, both within TREC and outside it, has been trying to take advantage of local similarities between a small part of the document and the query.
Reference: [7] <author> D. Evans and R. Lefferts. </author> <title> Design and evaluation of the CLARIT-TREC-2 system. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> Proceedings of the Second Text REtrieval Conference (TREC-2), </booktitle> <pages> pages 137-150. </pages> <note> NIST Special Publication 500-215, </note> <month> March </month> <year> 1994. </year>
Reference-contexts: So there is more of a chance for terms from the relevant retrieved documents to meaningfully distinguish themselves from the terms in the non-relevant documents. Other groups in TREC 2 were able to take advantage of this situation and improve performance, noticeably UCLA and CMU <ref> [6, 7] </ref>. Another focus of our work the past few years, both within TREC and outside it, has been trying to take advantage of local similarities between a small part of the document and the query.
Reference: [8] <author> Norbert Fuhr. </author> <title> Models for retrieval with probabilistic indexing. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 25(1) </volume> <pages> 55-72, </pages> <year> 1989. </year>
Reference-contexts: We also want to examine other feedback approaches for those queries with no, or little, expansion. In TREC 1 and TREC 2 it was noticed that the probabilistic approaches, e.g., the classical probabilistic formula [10] and Dortmund's RPI formula <ref> [8, 9] </ref>, did better than the Rocchio approach if there was little expansion. Perhaps a choice among feedback methods would improve effectiveness; our TREC 2 results suggested just changing expansion amounts on a per-query basis would yield only a small improvement.
Reference: [9] <author> Norbert Fuhr and Chris Buckley. </author> <title> Optimizing document indexing and search term weighting based on probabilistic models. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> Proceedings of the First Text REtrieval Conference (TREC-1), </booktitle> <pages> pages 89-99. </pages> <note> NIST Special Publication 500-207, </note> <month> March </month> <year> 1993. </year>
Reference-contexts: We also want to examine other feedback approaches for those queries with no, or little, expansion. In TREC 1 and TREC 2 it was noticed that the probabilistic approaches, e.g., the classical probabilistic formula [10] and Dortmund's RPI formula <ref> [8, 9] </ref>, did better than the Rocchio approach if there was little expansion. Perhaps a choice among feedback methods would improve effectiveness; our TREC 2 results suggested just changing expansion amounts on a per-query basis would yield only a small improvement.
Reference: [10] <author> S.E. Robertson and K. Sparck Jones. </author> <title> Relevance weighting of search terms. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 27(3) </volume> <pages> 129-146, </pages> <month> May-June </month> <year> 1976. </year>
Reference-contexts: We also want to examine other feedback approaches for those queries with no, or little, expansion. In TREC 1 and TREC 2 it was noticed that the probabilistic approaches, e.g., the classical probabilistic formula <ref> [10] </ref> and Dortmund's RPI formula [8, 9], did better than the Rocchio approach if there was little expansion. Perhaps a choice among feedback methods would improve effectiveness; our TREC 2 results suggested just changing expansion amounts on a per-query basis would yield only a small improvement.
Reference: [11] <author> J.J. Rocchio. </author> <title> Relevance feedback in information retrieval. </title> <editor> In Gerard Salton, editor, </editor> <title> The SMART Retrieval System|Experiments in Automatic Document Processing. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1971. </year>
Reference-contexts: Routing Experiments Our routing experiments in TREC 3 are only slightly different from those carried out for TREC 2. The basic routing approach chosen is the feedback approach of Rocchio <ref> [11, 15, 1] </ref>. Expressed in vector space terms, the final query vector is the initial query vector moved toward the centroid of the relevant documents, and away from the centroid of the non-relevant documents.
Reference: [12] <author> Gerard Salton. </author> <title> Automatic Text Processing | the Transformation, Analysis and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference: [13] <author> Gerard Salton. </author> <title> Developments in automatic text retrieval. </title> <journal> Science, </journal> <volume> 253 </volume> <pages> 974-980, </pages> <month> August </month> <year> 1991. </year>
Reference: [14] <author> Gerard Salton and Chris Buckley. </author> <title> Term-weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 513-523, </pages> <year> 1988. </year>
Reference: [15] <author> Gerard Salton and Chris Buckley. </author> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(4) </volume> <pages> 288-297, </pages> <year> 1990. </year>
Reference-contexts: Routing Experiments Our routing experiments in TREC 3 are only slightly different from those carried out for TREC 2. The basic routing approach chosen is the feedback approach of Rocchio <ref> [11, 15, 1] </ref>. Expressed in vector space terms, the final query vector is the initial query vector moved toward the centroid of the relevant documents, and away from the centroid of the non-relevant documents.
Reference: [16] <author> Gerard Salton and Chris Buckley. </author> <title> Automatic text structuring and retrieval: Experiments in automatic encyclopedia searching. </title> <booktitle> In Proceedings of the Fourteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 21-30, </pages> <year> 1991. </year>
Reference-contexts: We've shown that local similarities can be used very effectively to ensure that terms in common between document and query are being used in the same semantic sense. However, while this semantic disambiguation is important in other environments <ref> [16] </ref>, the very long and rich TREC queries provide enough global context to disambiguate without going to a local level. Our efforts to improve effectiveness using local disambiguation using sentences and short paragraphs did not work in TREC 1 and TREC 2.
Reference: [17] <author> Craig Stanfill and David L. Waltz. </author> <title> Statistical methods, </title> <booktitle> artificial intelligence, and information retrieval. </booktitle> <editor> In Paul S. Jacobs, editor, </editor> <booktitle> Text-based Intelligent Systems: Current Research and Practice in Information Extraction and Retrieval. </booktitle> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <address> Hillsdale, NJ, </address> <year> 1971. </year>
Reference-contexts: Our efforts to improve effectiveness using local disambiguation using sentences and short paragraphs did not work in TREC 1 and TREC 2. For TREC 3, we lengthen our local contexts, and treat the local passage as being a mini-document. Adopting the approach of UMass <ref> [17, 4, 5] </ref>, we define our local contexts to be a set of overlapping text windows, each of fixed size. This avoids the length and normalization problems that adversely affected our approach in TREC 2.
References-found: 17

