URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr94/tr94-032.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr94-abstracts.html
Root-URL: http://www.cis.ufl.edu
Email: ted@cis.ufl.edu  
Title: A Performance Comparison of Fast Distributed Synchronization Algorithms  
Author: Theodore Johnson 
Keyword: Distributed Synchronization, Performance Analysis, Distributed Algorithms, Distributed Virtual Memory  
Address: Gainesville, Fl 32611-2024  
Affiliation: Dept. of CIS, University of Florida  
Abstract: Distributed synchronization is an essential component of parallel and distributed computing. Several fast and low-overhead distributed synchronization algorithms have been proposed. Each of these algorithms required O(log n) messages per critical section entry and O(log n) bits of storage per processor. Asymptotic performance estimates do not proclaim any of the algorithms to be a winner, but no performance comparison of the algorithms has yet been performed. In this paper, we make a comparative performance study of four distributed semaphore algorithms. Each of these algorithms represents a different approach to maintaining distributed information. Since the algorithms we study are the basis for distributed synchronization, distributed virtual memory, coherent caches, and distributed object systems, our results have implications about the best methods for their implementation. We find that the distributed synchronization algorithm of Chang, Singhal, and Liu has the overall best performance, though other algorithms are more efficient in special cases. In a system of 350 processors, the CSL algorithm requires only six messages per critical section entry, including the initial request and the token response messages. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> O.S.F. Carvalho and G. Roucairol. </author> <title> On mutual exclusion in computer networks. </title> <journal> Comm. of the ACM, </journal> <volume> 26(2) </volume> <pages> 146-147, </pages> <year> 1983. </year>
Reference-contexts: Lam-port [6] proposes a timestamp-based distributed synchronization algorithm. A processor broadcasts its request for the token to all of the other processors, which reply with a permission. A processor implicitly receives the token when it receives permissions from all other processors. Ricart and Agrawala [12] and Carvalho and Roucairol <ref> [1] </ref> improve on Lamport's algorithm by reducing the message passing overhead. However, all of these algorithms require O (n) messages per request. Thomas [14] introduces the idea of quorum consensus for distributed synchronization.
Reference: [2] <author> Y.I. Chang, M. Singhal, and M.T. Liu. </author> <title> An improved O(log(n)) mutual exclusion algorithm for distribtued systems. </title> <booktitle> In Int'l Conf. on Parallel Processing, </booktitle> <pages> pages III295-302, </pages> <year> 1990. </year>
Reference-contexts: Trehel and Naimi [16, 15] present two algorithms for distributed mutual exclusion that are similar to the `distributed dynamic' algorithm of Li and Hudak. Chang, Singhal, and Liu <ref> [2] </ref> present an improvement to the Trehel and Naimi algorithm [15] that reduces the average number of messages required per critical section entry. <p> However, these algorithms require the use of a centralized lock manager. Only a little work has been done to make a performance study of distributed synchronization algorithms. Ricart and Agrawala [13] make a simulation study of an O (n) message passing algorithm. Chang, Singhal, and Liu <ref> [2] </ref> use a simulation study to show that their improvements to Trehel and Naimi's algorithm actually does result in better performance. 2 The Algorithms All four algorithms that we study in this paper require O (log n) messages per critical section entry, and O (log n) bits per processor. <p> As a result, it is not clear which protocol is more efficient. 2.3 Chang, Singhal, and Liu's Algorithm The algorithm of Chang Singhal and Liu <ref> [2] </ref> (which we call the CSL algorithm) makes aggressive use of path compression to achieve good performance. Their algorithm is based on the algorithm by Li and Hudak. Each processor maintains a guess about which processor holds the token. This guess is stored in the variable dir.
Reference: [3] <author> D.V. James, A.T. Laundrie, S. Gjessing, and G.S. Sohi. </author> <title> Scalable coherent interface. </title> <journal> Computer, </journal> <volume> 23(6) </volume> <pages> 74-77, </pages> <year> 1990. </year>
Reference-contexts: Woo and Newman-Wolfe [17] use a fixed tree based on a Huffman code. Some shared memory synchronization algorithms can be easily modified to construct distributed synchronization algorithms. These algorithms include the MCS contention-free lock [9] and cache coherence protocols such as the Scalable Coherent Interface (SCI) <ref> [3] </ref>. However, these algorithms require the use of a centralized lock manager. Only a little work has been done to make a performance study of distributed synchronization algorithms. Ricart and Agrawala [13] make a simulation study of an O (n) message passing algorithm.
Reference: [4] <author> T. Johnson and R. Newman-Wolfe. </author> <title> A comparison of fast and low overhead distributed priority locks. </title> <type> Technical report, </type> <institution> U. Florida Dept. of CIS, </institution> <year> 1994. </year> <note> available at ftp.cis.ufl.edu:/cis/tech-reports/tr94/tr94-022.ps.Z. </note>
Reference-contexts: of the waiting list and the CSL algorithm has good performance in practice. 2.4 List Lock Algorithm In this section, we describe an algorithm, which we call the List Lock, that was inspired by our previous work on a distributed priority lock that used path compression to achieve good performance <ref> [4] </ref>. To transform a priority lock into a non-prioritized lock, we prioritize a request based on the time of the request (to ensure causality, we can use a Lamport timestamp).
Reference: [5] <author> A. Kumar. </author> <title> Hierarchical quorum consensus: A new algorithm for managing replicated data. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40(9) </volume> <pages> 994-1004, </pages> <year> 1991. </year>
Reference-contexts: Maekawa [8] presents an algorithm that requires O ( p n) messages per request and O ( p n log n) space per processor. Kumar <ref> [5] </ref> presents the hierarchical quorum consensus protocol, which requires O (n :63 ) votes for consensus, but is more fault tolerant than Maekawa's algorithm. Li and Hudak [7] present a distributed synchronization algorithm to enforce coherence in a distributed shared virtual memory (DSVM) system.
Reference: [6] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-564, </pages> <year> 1978. </year>
Reference-contexts: Since this includes the message that requests the token and the message that releases the token there are only two to four overhead messages. 1.1 Background Work Considerable attention has been paid to the problem of distributed synchronization. Lam-port <ref> [6] </ref> proposes a timestamp-based distributed synchronization algorithm. A processor broadcasts its request for the token to all of the other processors, which reply with a permission. A processor implicitly receives the token when it receives permissions from all other processors.
Reference: [7] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <year> 1989. </year>
Reference-contexts: Kumar [5] presents the hierarchical quorum consensus protocol, which requires O (n :63 ) votes for consensus, but is more fault tolerant than Maekawa's algorithm. Li and Hudak <ref> [7] </ref> present a distributed synchronization algorithm to enforce coherence in a distributed shared virtual memory (DSVM) system. In DSVM, a page of memory in a processor is treated as a cached version of a globally shared memory page.
Reference: [8] <author> M. Maekawa. </author> <title> A sqrt(n) algorithm for mutual exlcusion in decentralized systems. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 3(2) </volume> <pages> 145-159, </pages> <year> 1985. </year> <month> 14 </month>
Reference-contexts: The number of votes that are required to obtain the token can be reduced by observing that the only requirement for mutual exclusion is that any pair of processors require a vote from the same processor. Maekawa <ref> [8] </ref> presents an algorithm that requires O ( p n) messages per request and O ( p n log n) space per processor. Kumar [5] presents the hierarchical quorum consensus protocol, which requires O (n :63 ) votes for consensus, but is more fault tolerant than Maekawa's algorithm.
Reference: [9] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Trans. Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <year> 1991. </year>
Reference-contexts: Woo and Newman-Wolfe [17] use a fixed tree based on a Huffman code. Some shared memory synchronization algorithms can be easily modified to construct distributed synchronization algorithms. These algorithms include the MCS contention-free lock <ref> [9] </ref> and cache coherence protocols such as the Scalable Coherent Interface (SCI) [3]. However, these algorithms require the use of a centralized lock manager. Only a little work has been done to make a performance study of distributed synchronization algorithms.
Reference: [10] <author> M.L. Neilsen and M. Mizuno. </author> <title> A dag-based neilsen for distributed mutual exclusion. </title> <booktitle> In International Conference on Distributed Computer Systems, </booktitle> <pages> pages 354-360, </pages> <year> 1991. </year>
Reference-contexts: The algorithm organizes the participating processors in a fixed tree. Each processor points to the neighbor that lies on the path to the token holder. Neilsen and Mizuno <ref> [10] </ref> present an improved version of Raymond's algorithm that requires fewer messages because it passes tokens directly between processors instead of through the tree. Woo and Newman-Wolfe [17] use a fixed tree based on a Huffman code. <p> Neilsen and Misuno <ref> [10] </ref> observed that it is possible to pass the token directly to the requester, since the identity of the original requester can be attached to the request. Neisen and Mizuno used this observation to develop an algorithm which we will call the NM algorithm.
Reference: [11] <author> K. Raymond. </author> <title> A tree-based algorithm for distributed mutual exclusion. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 7(1) </volume> <pages> 61-77, </pages> <year> 1989. </year>
Reference-contexts: Chang, Singhal, and Liu [2] present an improvement to the Trehel and Naimi algorithm [15] that reduces the average number of messages required per critical section entry. Raymond <ref> [11] </ref> has proposed a simple synchronization algorithm that can be configured to require O (log n) storage per processor and O (log n) messages per critical section request. The algorithm organizes the participating processors in a fixed tree. <p> We give only brief descriptions of the algorithms here. We provide citations that give further details about the algorithm implementations. 2.1 Raymond's Algorithm Raymond <ref> [11] </ref> proposes an algorithm for maintaining a distributed lock which makes use of a tree structure that is imposed on the processes. Each processor keeps a pointer, dir, to the neighbor which is the root of the subtree where the token is located (see Figure 1).
Reference: [12] <author> G. Ricart and A.K. Agrawala. </author> <title> An optimal algorithm for mutual exclusion in computer networks. </title> <journal> Comm. of the ACM, </journal> <volume> 24(1) </volume> <pages> 9-17, </pages> <year> 1981. </year>
Reference-contexts: Lam-port [6] proposes a timestamp-based distributed synchronization algorithm. A processor broadcasts its request for the token to all of the other processors, which reply with a permission. A processor implicitly receives the token when it receives permissions from all other processors. Ricart and Agrawala <ref> [12] </ref> and Carvalho and Roucairol [1] improve on Lamport's algorithm by reducing the message passing overhead. However, all of these algorithms require O (n) messages per request. Thomas [14] introduces the idea of quorum consensus for distributed synchronization.
Reference: [13] <author> G. Ricart and A.K. Agrawala. </author> <title> Performance of a distributed network mutual exclusion algorithm. </title> <type> Technical Report TR-774, </type> <institution> U. Maryland College Park Department of Computer Science, </institution> <year> 1991. </year>
Reference-contexts: However, these algorithms require the use of a centralized lock manager. Only a little work has been done to make a performance study of distributed synchronization algorithms. Ricart and Agrawala <ref> [13] </ref> make a simulation study of an O (n) message passing algorithm.
Reference: [14] <author> R. H. Thomas. </author> <title> A majority consensus approach to concurrency control for multiple copy databases. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 4(2) </volume> <pages> 180-209, </pages> <year> 1979. </year>
Reference-contexts: A processor implicitly receives the token when it receives permissions from all other processors. Ricart and Agrawala [12] and Carvalho and Roucairol [1] improve on Lamport's algorithm by reducing the message passing overhead. However, all of these algorithms require O (n) messages per request. Thomas <ref> [14] </ref> introduces the idea of quorum consensus for distributed synchronization. When a processor requests the token, it sends a vote request to all of the other processors in the system. A processor will vote for the critical section entry of at most one processor at a time.
Reference: [15] <author> M. Trehel and M. Naimi. </author> <title> A distributed algorithm for mutual exclusion based on data structures and fault tolerance. </title> <booktitle> In IEEE Phoenix Conference on Computers and Communications, </booktitle> <pages> pages 36-39, </pages> <year> 1987. </year>
Reference-contexts: When the system is quiescent, the pointers form a tree that is rooted at the current page owner. The tree is kept short by using path compression, which guarantees an amortized O (log n) bound on the number of messages per request. Trehel and Naimi <ref> [16, 15] </ref> present two algorithms for distributed mutual exclusion that are similar to the `distributed dynamic' algorithm of Li and Hudak. Chang, Singhal, and Liu [2] present an improvement to the Trehel and Naimi algorithm [15] that reduces the average number of messages required per critical section entry. <p> Trehel and Naimi [16, 15] present two algorithms for distributed mutual exclusion that are similar to the `distributed dynamic' algorithm of Li and Hudak. Chang, Singhal, and Liu [2] present an improvement to the Trehel and Naimi algorithm <ref> [15] </ref> that reduces the average number of messages required per critical section entry. Raymond [11] has proposed a simple synchronization algorithm that can be configured to require O (log n) storage per processor and O (log n) messages per critical section request.
Reference: [16] <author> M. Trehel and M. Naimi. </author> <title> An improvement of the log(n) distributed algorithm for mutual exclusion. </title> <booktitle> In Proc. IEEE Intl. Conf. on Distributed Computer Systems, </booktitle> <pages> pages 371-375, </pages> <year> 1987. </year>
Reference-contexts: When the system is quiescent, the pointers form a tree that is rooted at the current page owner. The tree is kept short by using path compression, which guarantees an amortized O (log n) bound on the number of messages per request. Trehel and Naimi <ref> [16, 15] </ref> present two algorithms for distributed mutual exclusion that are similar to the `distributed dynamic' algorithm of Li and Hudak. Chang, Singhal, and Liu [2] present an improvement to the Trehel and Naimi algorithm [15] that reduces the average number of messages required per critical section entry.
Reference: [17] <author> T.K. Woo and R. Newman-Wolfe. </author> <title> Huffman trees as a basis for a dynamic mutual exclusion algorithm for distributed systems. </title> <booktitle> In Proceedings of the 12th IEEE International Conference on Distributed Computing Systems, </booktitle> <pages> pages 126-133, </pages> <booktitle> 1992. </booktitle> <volume> 15 16 17 18 19 20 </volume>
Reference-contexts: Each processor points to the neighbor that lies on the path to the token holder. Neilsen and Mizuno [10] present an improved version of Raymond's algorithm that requires fewer messages because it passes tokens directly between processors instead of through the tree. Woo and Newman-Wolfe <ref> [17] </ref> use a fixed tree based on a Huffman code. Some shared memory synchronization algorithms can be easily modified to construct distributed synchronization algorithms. These algorithms include the MCS contention-free lock [9] and cache coherence protocols such as the Scalable Coherent Interface (SCI) [3].
References-found: 17

