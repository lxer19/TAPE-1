URL: http://www.cs.wustl.edu/cs/techreports/1994/wucs-94-29.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Title: High Performance Training of Feedforward Simple Recurrent Networks  
Author: Barry L. Kalman and Stan C. Kwasny 
Note: This material is based upon work supported by the National Science Foundation under Grant No. IRI-9201987.  
Date: October 1994  
Pubnum: WUCS-94-29  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> E. Barnard, </author> <title> Optimization for training neural nets, </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> Vol. 3, No. 2, </volume> <month> March </month> <year> 1992, </year> <pages> pp. 232-241. </pages>
Reference-contexts: By carefully analyzing these assumptions, we have managed to introduce improvements in virtually every aspect of it. Below we present a variety of standard features together with some comments on what direction to take to improve them. The <ref> [0, 1] </ref> interval. The [-1, 1] interval is better for Boolean functions and many other problems. The sum of squares error function. Sum of squares was designed for infinite intervals and most neural network problems operate over a finite output interval. Layered connectivity. <p> By carefully analyzing these assumptions, we have managed to introduce improvements in virtually every aspect of it. Below we present a variety of standard features together with some comments on what direction to take to improve them. The [0, 1] interval. The <ref> [-1, 1] </ref> interval is better for Boolean functions and many other problems. The sum of squares error function. Sum of squares was designed for infinite intervals and most neural network problems operate over a finite output interval. Layered connectivity. <p> Therefore, the four criteria and the finite interval force the following differential equation to hold: (EQ 8) For the interval <ref> [-1, 1] </ref>, the only functional form which satisfies EQ 8 is: (EQ 9) If e pk = (t pk - a pk ), this relation leads to the error function: (EQ 10) We have shown in [11] that selecting the value: (EQ 11) maintains an equitable scaling of weight layers for <p> FIGURE 2 ABOUT HERE 2.3 Singular Value Decomposition for Preprocessing Input We discussed the use of singular value decomposition (SVD) to preprocess input in [12]. We reported that the use of SVD along with an affine transformation to place the inputs in the interval <ref> [-1, 1] </ref> often allowed training to proceed when it was previously impossible. <p> If SVD causes h columns to be removed then let be the number of remaining columns. U is , G is and V is . Let Q = UG and construct an affine transformation that maps the transformed patterns of Q into <ref> [-1, 1] </ref> by computing: (EQ 13) (EQ 15) A is the transformed input matrix of p patterns over I input units and: (EQ 17) The training takes place using A . <p> Others (see, for example <ref> [1] </ref>) have also found it very useful. Because of quadratic conver 17 gence it uses fewer epochs (often many fewer) than backprop especially if a Powell update [21] is used. The CGA requires linear storage (in weights) while Newton-style qua-dratically convergent methods require quadratic storage.
Reference: [2] <author> J. L. Elman, </author> <title> Finding Structure in Time, </title> <type> CRL Technical Report 8801, </type> <institution> Center for Research in Language, University of California, </institution> <address> San Diego, </address> <year> 1988. </year>
Reference-contexts: The middle number for each network size is the number of hidden units. The percentage of the variables taken up with skip connections reects the degree of linearity present in the problem. For our recurrent networks we use only feedback from the hidden layer ala Elman <ref> [2] </ref>. More general recurrent networks are discussed by Williams and Zipser [30]. We call the nodes to which the outputs of the hidden units are fed back the pseudo input layer. We connect the pseudo input layer just as we do the ordinary input layer. This is illustrated in themselves.
Reference: [3] <author> Shelly D. D. Goggin, Karl E. Gustafson and Kristina M. Johnson, </author> <title> An Asymptotic Singular Value Decomposition Analysis of Nonlinear Multilayer Neural Networks, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> July </month> <year> 1991, </year> <editor> v. </editor> <volume> 1, </volume> <pages> pp. 785-790. </pages>
Reference-contexts: We summarize those results here. Details of SVD are presented in [4] and [5]. Uses of SVD to analyze the hidden layer are presented in <ref> [3] </ref> and [31]. Let A be the original input matrix which is , where p is the number of patterns and I is the number of input units. Then the SVD of A is: (EQ 12) where U and V are orthonormal and G is diagonal.
Reference: [4] <author> Gene H. Golub and Charles F. Van Loan, </author> <booktitle> Matrix Computations, </booktitle> <pages> pp 16-20 and 285-295, </pages> <publisher> The Johns Hopkins University Press, </publisher> <year> 1983. </year>
Reference-contexts: We summarize those results here. Details of SVD are presented in <ref> [4] </ref> and [5]. Uses of SVD to analyze the hidden layer are presented in [3] and [31]. Let A be the original input matrix which is , where p is the number of patterns and I is the number of input units.
Reference: [5] <author> Gene H. Golub and Charles F. Van Loan, </author> <title> Matrix Computations 2d Edition, page 42, </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1989. </year> <month> 29 </month>
Reference-contexts: We summarize those results here. Details of SVD are presented in [4] and <ref> [5] </ref>. Uses of SVD to analyze the hidden layer are presented in [3] and [31]. Let A be the original input matrix which is , where p is the number of patterns and I is the number of input units. <p> There are terms of the type on the left side of the equation and each one involves the sum over H terms. If the sum part of EQ34 is evaluated as a scalar product the code to implement EQ 34 can be optimized for a scalar pipelined processor <ref> [5] </ref>.
Reference: [6] <author> D. Gorse, A. Shepherd and J. G. Taylor, </author> <title> A Classical Algorithm for Avoiding Local Minima, </title> <booktitle> Proceedings of the World Congress on Neural Networks, </booktitle> <address> San Diego, CA, </address> <month> July </month> <year> 1994, </year> <editor> v. </editor> <booktitle> III, </booktitle> <pages> pp. 364-369. </pages>
Reference-contexts: As an upper bound, we find the minimum hidden layer that can explain all the data. 3.5 Adjusting the Targets to Avoid Local Minima We have found that adjusting the targets for the output units in the fashion of Gorse, Shepeherd and Taylor <ref> [6] </ref> avoids local minima that stop the CGA if we hold the targets at c 2 21 . We have modified the method slightly. We start with targets of which avoids many of the local minima found with and also avoids extremum found with all outputs = 0. <p> This is found to be sufficient to obtain what appear to be global extrema. We then iteratively increase the targets to . We have found that occasionally the solution gets better and never gets worse during the target increase. This phenomena was not reported in <ref> [6] </ref>. We attribute it to our choice of error function. Since this error function approches at the wrong end of the interval big errors encountered as the targets are increased may be dramatically reduced, which yields the observed improvements.
Reference: [7] <author> Barry L. </author> <title> Kalman, Superlinear Learning in Back-Propagation Neural Networks, </title> <type> Technical Report WUCS-90-21, </type> <institution> Washington University, </institution> <month> June, </month> <year> 1990. </year>
Reference-contexts: The CGA requires linear storage (in weights) while Newton-style qua-dratically convergent methods require quadratic storage. CGAs generalization properties are very close to those of backprop. We originally reported on the behavior of CGA in <ref> [7] </ref>. Code for the CGA is discussed in [19, 21 and 22].
Reference: [8] <author> Gary M. Kuhn and Norman P. Herzberg, </author> <title> Some Variations on Training of Recurrent Networks, </title> <publisher> Academic Press, </publisher> <year> 1991. </year>
Reference: [9] <author> Stan C. Kwasny, Sahnny Johnson and Barry L. </author> <title> Kalman, Recurrent Natural Language Parsing, </title> <booktitle> Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society Atlanta, </booktitle> <address> GA, </address> <month> August </month> <year> 1994, </year> <pages> pp. 525-530. </pages>
Reference-contexts: Therefore, no more than 13 feedback units should be required, which we have verified empirically. In training a recurrent neural network to be a deterministic parser for English <ref> [9] </ref>, we found that the 7,659 patterns resulted in 5,433 unique states which suggested an initial choice of 13 feedback units. This too has been verified experimentally.
Reference: [10] <author> Barry L. Kalman and Stan C. Kwasny, </author> <title> A Superior Error Function for Training Neural Networks, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> July </month> <year> 1991, </year> <editor> v. </editor> <volume> 2, </volume> <pages> pp. 49-52. </pages>
Reference-contexts: These are the Kalman-Kwasny error function, skip connections, singular value decomposition of the inputs and use of vector best match as a correctness criterion. 2.1 The Kalman-Kwasny Error Function We first presented the Kalman-Kwasny error function in <ref> [10] </ref> and we gave its derivation along with the importance of the hyperbolic tangent sigmoidal in [11]. Here we summarize those ideas based on [11]. First we present the desirable behaviors of an error function on a finite interval. Next we show the derivation of the error function. <p> In <ref> [10] </ref> we reported on the performance of our error function on the problem of training a feedforward network to be a deterministic parser (see Marcus [16]) for a medium sized s pk ki pi b += pk pk 1 s 2 s' l 1 s = s x ( ) lx <p> The network had 53 inputs, 22 outputs, 30 hidden units and no skip connections. The data set consisted of 113 training patterns. Table 1 contains a summary of results from <ref> [10] </ref>. These results are strong evidence for using the Kalman-Kwasny error function. 2.2 Using Skip Connections Most problems have a significant linear component in their solutions. Some require no non-linear component and are therefore solvable by a perceptron.
Reference: [11] <author> Barry L. Kalman and Stan C. Kwasny, </author> <title> Why Tanh: Choosing a Sigmoidal Function, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> June </month> <year> 1992, </year> <editor> v. </editor> <booktitle> IV, </booktitle> <pages> pp 578-581. </pages>
Reference-contexts: Kalman-Kwasny error function, skip connections, singular value decomposition of the inputs and use of vector best match as a correctness criterion. 2.1 The Kalman-Kwasny Error Function We first presented the Kalman-Kwasny error function in [10] and we gave its derivation along with the importance of the hyperbolic tangent sigmoidal in <ref> [11] </ref>. Here we summarize those ideas based on [11]. First we present the desirable behaviors of an error function on a finite interval. Next we show the derivation of the error function. Finally, we review how to scale the sigmoidal function for maximum fairness. <p> of the inputs and use of vector best match as a correctness criterion. 2.1 The Kalman-Kwasny Error Function We first presented the Kalman-Kwasny error function in [10] and we gave its derivation along with the importance of the hyperbolic tangent sigmoidal in <ref> [11] </ref>. Here we summarize those ideas based on [11]. First we present the desirable behaviors of an error function on a finite interval. Next we show the derivation of the error function. Finally, we review how to scale the sigmoidal function for maximum fairness. <p> finite interval force the following differential equation to hold: (EQ 8) For the interval [-1, 1], the only functional form which satisfies EQ 8 is: (EQ 9) If e pk = (t pk - a pk ), this relation leads to the error function: (EQ 10) We have shown in <ref> [11] </ref> that selecting the value: (EQ 11) maintains an equitable scaling of weight layers for training purposes.
Reference: [12] <author> Barry L. Kalman, Stan C. Kwasny and Aurorita Abella, </author> <title> Decomposing Input Patterns to Facilitate Training, </title> <booktitle> Proceedings of the World Congress on Neural Networks, </booktitle> <address> Portland, OR, </address> <month> July </month> <year> 1993, </year> <editor> v. </editor> <booktitle> III, </booktitle> <pages> pp. 503-506. </pages>
Reference-contexts: TABLE 1. Results for Medium Grammar Problem Error Function Optimization Method Epochs Presentations CPU Time (sec) Sum of Squares Backprop Kalman-Kwasny Backprop 13000 146900 230185 Sum of Squares Conjugate Gradient 639 72151 6001 Kalman-Kwasny Conjugate Gradient 237 26781 2300 12 In <ref> [12] </ref> we reported on several problems on which we used TRAINREC. We employed skip connections in all of these cases. <p> This is illustrated in themselves. In section 4 we show how the pseudo input can be treated as ordinary input. FIGURE 2 ABOUT HERE 2.3 Singular Value Decomposition for Preprocessing Input We discussed the use of singular value decomposition (SVD) to preprocess input in <ref> [12] </ref>. We reported that the use of SVD along with an affine transformation to place the inputs in the interval [-1, 1] often allowed training to proceed when it was previously impossible. <p> After training it is useful to transform the parameters of the network to a network which uses the original input. We show in <ref> [12] </ref> that for weights, , on connections which involve the input units, weights that perform equivalently on the original patterns can be obtained as follows: p I T I' I h= p I' I' I' r min Q = i 1 k p ki c 2 i i -= i i
Reference: [13] <author> Stan C. Kwasny, Barry L. Kalman, A. Maynard Engebretson, and Weilan Wu, </author> <title> Identifying Language from Speech: An Example of High-Level, Statistically-Based Feature Extraction, </title> <booktitle> Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <month> July, </month> <year> 1992, </year> <pages> pp. 909-914. 30 </pages>
Reference-contexts: By reducing the frequency of derivative computations, we improve overall efficiency greatly. Use of settling and voting. In training a SRN for tasks involving noisy data (for example, our work on identifying which of two languages is being spoken <ref> [13] </ref> and [29]) we found that allowing the network to settle over a prefix of patterns is very helpful.
Reference: [14] <author> Stan C. Kwasny, Barry L. Kalman and Nancy Chang, </author> <title> Distributed Patterns as Hierarchical Structures, </title> <booktitle> Proceedings of the World Congress on Neural Networks, </booktitle> <address> Portland, OR, </address> <month> July </month> <year> 1993, </year> <editor> v. </editor> <volume> II, </volume> <pages> pp. 198-201. </pages>
Reference-contexts: This means that the forward connection from a context node only exists with the hidden node that spawned it. More general architec tures are desired. Static targets for Recursive Auto-Associative Memories. RAAMs cleverly use auto-asso ciativity to permit representation of complex structures as distributed patterns (see [20] and <ref> [14] </ref>). In our experience, the effect of the variability of the target is usually not properly considered under standard implementations of backprop. 1.2 Primary Discoveries In building TRAINREC we made several discoveries which are helping us improve the efficiency of training. <p> Because the growth of the recurrent iterations is , we are careful to limit the growth of the hidden layer. Here we analyze RAAMs where O = H + I. The SRN version of RAAMs that we use is described in <ref> [14] </ref>. By definition, we are not able to use skip connections with our RAAMs. <p> Since RAAM Tk depends on the network parameters, we add a correction term to EQ 30 to get (EQ 36) Where SRN is the term which corresponds to EQ 30. We reported in <ref> [14] </ref> that without this correction term, training for RAAMs usually does not converge and the error function often oscillates. A sample problem for which EQ 40 is important is a five level stack with a set of 3 symbols.
Reference: [15] <author> Samuel E. Lee and Bradley R. Holt, </author> <title> Regression Analysis of Spectroscopic Process Data Using a Combined Architecture of Linear and Nonlinear Artificial Neural Networks, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> June </month> <year> 1992, </year> <editor> v. </editor> <booktitle> IV, </booktitle> <pages> pp 549-554. </pages>
Reference-contexts: Some require no non-linear component and are therefore solvable by a perceptron. Connection of input units to output units places the linear component of the solution in those connections while the non-linear component is isolated in the hidden layer weights <ref> [15] </ref>. For a simple example, consider the simplest architecture capable of learning XOR. Connecting inputs to outputs is required to give a 2-1-1 network. Other evidence comes from work on genetic algorithms used to prune neural nets [27] which shows that connections from input to output are important.
Reference: [16] <author> Mitchell Marcus, </author> <title> A Theory of Syntactic Recognition for Natural Language, </title> <publisher> MIT Press, </publisher> <year> 1980. </year>
Reference-contexts: In [10] we reported on the performance of our error function on the problem of training a feedforward network to be a deterministic parser (see Marcus <ref> [16] </ref>) for a medium sized s pk ki pi b += pk pk 1 s 2 s' l 1 s = s x ( ) lx ( )tanh= g e 2 pk l 1.5= 11 subset of English grammar.
Reference: [17] <author> Alexander M. Mood, Franklin A. Graybill and Duane C. Boes, </author> <title> Introduction to the Theory of Statistics, </title> <publisher> 3rd ed., McGraw-Hill, </publisher> <year> 1974. </year>
Reference-contexts: This measure tends to be high when performance is distributed over both frequent and infrequent categories. Unfortunately it can be high under other conditions. The confusion matrix is the same as a two-way contingency table discussed in <ref> [17] </ref>. Construction of the confusion matrix is discussed in Section 2.4. The third measure concerns maximizing worst case performance. We save the network whenever the observed lowest fraction correct in the test set categories increases.
Reference: [18] <author> Peter J. McCann and Barry L. </author> <title> Kalman, Parallel Training of Simple Recurrent Neural Networks, </title> <booktitle> Proceedings of the IEEE World Congress on Computational Intelligence(WCCI 94), </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994, </year> <editor> v. </editor> <volume> I, </volume> <pages> pp. 167-170. </pages>
Reference-contexts: If the sum part of EQ34 is evaluated as a scalar product the code to implement EQ 34 can be optimized for a scalar pipelined processor [5]. Even more of the potential con-currency in EQ 34 can be implemented on a shared memory multi-processor computer <ref> [18] </ref>. w D a p a pi ji a d a ji a w s d li H 24 4.2 Feedback Derivatives for RAAMs RAAMs are designed to use auto-associative training to evolve a collection of representa tions for structures.
Reference: [19] <author> E. Polak, </author> <title> Computational Methods in Optimization: A Unified Approach, </title> <publisher> Academic Press, </publisher> <year> 1971. </year>
Reference-contexts: The CGA requires linear storage (in weights) while Newton-style qua-dratically convergent methods require quadratic storage. CGAs generalization properties are very close to those of backprop. We originally reported on the behavior of CGA in [7]. Code for the CGA is discussed in <ref> [19, 21 and 22] </ref>. The results in Table 1 also give strong evidence for the use of CGA over ordinary backprop. 3.2 Derivative-Free Line Search The CGA requires a line search algorithm to find a local minimum in a particular direction for each iteration.
Reference: [20] <author> Jordan Pollack, </author> <title> Recursive Distributed Representations, </title> <booktitle> Artificial Intelligence 46, </booktitle> <pages> 77-105. </pages>
Reference-contexts: This means that the forward connection from a context node only exists with the hidden node that spawned it. More general architec tures are desired. Static targets for Recursive Auto-Associative Memories. RAAMs cleverly use auto-asso ciativity to permit representation of complex structures as distributed patterns (see <ref> [20] </ref> and [14]). In our experience, the effect of the variability of the target is usually not properly considered under standard implementations of backprop. 1.2 Primary Discoveries In building TRAINREC we made several discoveries which are helping us improve the efficiency of training.
Reference: [21] <author> M. J. D. Powell, </author> <title> Restart Procedures for the Conjugate Gradient Method, </title> <journal> Mathematical Programming, </journal> <volume> vol. 12, No. 2, </volume> <month> April </month> <year> 1977, </year> <pages> pp 241-254. </pages>
Reference-contexts: Others (see, for example [1]) have also found it very useful. Because of quadratic conver 17 gence it uses fewer epochs (often many fewer) than backprop especially if a Powell update <ref> [21] </ref> is used. The CGA requires linear storage (in weights) while Newton-style qua-dratically convergent methods require quadratic storage. CGAs generalization properties are very close to those of backprop. We originally reported on the behavior of CGA in [7]. Code for the CGA is discussed in [19, 21 and 22].
Reference: [22] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky and William T. Vettering, </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year> <month> 31 </month>
Reference-contexts: The results in Table 1 also give strong evidence for the use of CGA over ordinary backprop. 3.2 Derivative-Free Line Search The CGA requires a line search algorithm to find a local minimum in a particular direction for each iteration. Two line search algorithms are Brent and Dbrent <ref> [22] </ref>. Dbrent requires the magnitude of the gradient in the search direction for each inner iteration. Brent only requires the value of the error function for each inner iteration. First we analyze training for ordinary feedforward networks to see the effect of using DFLS. <p> The algorithm mnbrak <ref> [22] </ref> locates such a region. It requires three points along the search direction to initialize it. At the beginning of CGA we use the ratio of the error function to the magnitude of the gradient as a start for mnbrak. <p> If making the correct prediction for the smaller category is important such a network will be a failure. The second measure is a computation (as specified in <ref> [22] </ref>) on the confusion matrix. This measure tends to be high when performance is distributed over both frequent and infrequent categories. Unfortunately it can be high under other conditions. The confusion matrix is the same as a two-way contingency table discussed in [17].
Reference: [23] <author> David E. Rumelhart, James L. </author> <title> McClelland and the PDP Research Group, Parallel Distributed Processing, </title> <publisher> v. 1, The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Target, , and activation, , are specific to an input pattern presentation and an output unit. The derivative equations are given by the generalized delta rule <ref> [23] </ref> with an additional term due to the feedback units.
Reference: [24] <author> David Servan-Schreiber, Axel Cleeremans and James L. McClelland, </author> <title> Encoding Sequential Structure in Simple Recurrent Networks, </title> <type> Technical Report CMU-CS-88-183, </type> <institution> Carnegie Mellon University, </institution> <month> November, </month> <year> 1988. </year>
Reference: [25] <author> Terrence J. Sejnowski and Charles R. Rosenberg, </author> <title> Parallel Networks that Learn to Pronounce English Text, </title> <journal> Complex Systems 1, </journal> <year> 1987, </year> <pages> pp 145-168. </pages>
Reference-contexts: VBM works especially well when one output unit is designated as a catchall for cases where the predicted category is none of the those which come from the model. This is because every vector is orthogonal to the zero vector. VBM is used quite effectively, for example, in NetTalk <ref> [25] </ref>. c 2 16 In our work on RAAMs, we have both distributed patterns and category vectors as outputs. To test which symbol is represented by the output vector that is extracted we use VBM. The choice of the empty symbol is very important in this work.
Reference: [26] <author> Jacques de Villiers and Etienne Barnard, </author> <title> Backpropagation Neural Nets with One and Two Hidden Layers, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol 4., No. 1, </volume> <month> January </month> <year> 1993, </year> <pages> pp 136-141. </pages>
Reference-contexts: De Vil-liers and Barnard found that with nearly equal numbers of weights feedforward networks with one and two hidden layers performed equally well and networks with two hidden layers had more frequent occurrences of local extrema during training <ref> [26] </ref>. If one is testing for the optimal number of hidden units for a network then the case of zero hidden units is a natural limiting case. Of course, TRAINREC permits disconnection of inputs and outputs by user choice. This is an essential choice, of course, for RAAM networks.
Reference: [27] <author> Darrell Whitley and Christopher Bogart, </author> <title> The Evolution of Connectivity: Pruning Neural Networks Using Genetic Algorithms, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <month> January </month> <year> 1990, </year> <editor> v. </editor> <volume> 1, </volume> <pages> pp 134-7. </pages>
Reference-contexts: For a simple example, consider the simplest architecture capable of learning XOR. Connecting inputs to outputs is required to give a 2-1-1 network. Other evidence comes from work on genetic algorithms used to prune neural nets <ref> [27] </ref> which shows that connections from input to output are important. Without skip connections more nodes are needed in the hidden layer to account for the linear component of the solution. We use one hidden layer because it is the smallest number for which non-linear separation can be effected.
Reference: [28] <author> Sholom M. Weiss and Casimir A. </author> <title> Kulikowski, Computer Systems That Learn, </title> <publisher> Mor-gan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: SVD may also reduce the number of input units needed to represent the input without a significant effort in feature extraction. (Weiss and Kulikowski <ref> [28] </ref> also have reported a preference for the [-1,1] interval.) We also reported a transformation back from the param TABLE 2.
Reference: [29] <author> Weilan Wu, Stan C. Kwasny, Barry L. Kalman and A. Maynard Engebretson, </author> <title> Identifying Language from Raw Speech: An Application of Recurrent Neural Networks, </title> <booktitle> Proceedings of the Midwest Artificial Intelligence and Cognitive Science Conference, </booktitle> <month> April </month> <year> 1993, </year> <pages> pp 53-57. </pages>
Reference-contexts: By reducing the frequency of derivative computations, we improve overall efficiency greatly. Use of settling and voting. In training a SRN for tasks involving noisy data (for example, our work on identifying which of two languages is being spoken [13] and <ref> [29] </ref>) we found that allowing the network to settle over a prefix of patterns is very helpful. <p> TABLE 3. Cumulative Binomial Probabilities p N CP 0.65 108 0.99904 k N k N N 26 In <ref> [29] </ref> we reported on a recurrent network which was trained by TRAINREC to identify which of two languages a speaker was using. The overall performance on patterns in the test set was 78.9%. The worst case was 100/190 where 96/190 would have been acceptable.
Reference: [30] <author> Ronald. J. Williams and David Zipser, </author> <title> A Learning Algorithm for Continually Running Fully Recurrent Neural Networks, </title> <booktitle> Neural Computation, 1989, v. </booktitle> <volume> 1, </volume> <pages> pp 270-280. 32 </pages>
Reference-contexts: The percentage of the variables taken up with skip connections reects the degree of linearity present in the problem. For our recurrent networks we use only feedback from the hidden layer ala Elman [2]. More general recurrent networks are discussed by Williams and Zipser <ref> [30] </ref>. We call the nodes to which the outputs of the hidden units are fed back the pseudo input layer. We connect the pseudo input layer just as we do the ordinary input layer. This is illustrated in themselves.

References-found: 30

