URL: http://www.cs.unm.edu/~kapur/abstracts/issac.95.ps.gz
Refering-URL: http://www.cs.unm.edu/~kapur/abstracts/issac.95.html
Root-URL: http://www.cs.unm.edu
Email: fkapur, saxenag@cs.albany.edu  
Author: Deepak Kapur and Tushar Saxena 
Address: Albany, NY 12222  
Affiliation: Institute for Programming and Logics Department of Computer Science State University of New York at Albany  
Abstract: Comparison of Various Multivariate Resultant Formulations fl Abstract Three most important resultant formulations are the Ma-caulay, Dixon and sparse resultant formulations. For most polynomial systems, however, the matrices constructed in these formulations become singular and the projection operator vanishes identically. In such cases, perturbation techniques for Macaulay formulation such as generalized characteristic polynomial (GCP) and a method based on rank submatrix computation (RSC), applicable to all three formulations, can be used, giving four methods, Macaulay/GCP, Macaulay/RSC, Dixon/RSC and Sparse/RSC, for computing nontrivial projection operators. In this paper, these four methods are compared. It is shown that the Dixon matrix is (by a factor up to O(e n ) for a certain class) smaller than the sparse resultant matrix which is (by a factor up to O(e n ) for a certain class) smaller than the Macaulay matrix. Empirical results confirm that Dixon/RSC is the most efficient, followed by Sparse/RSC then Macaulay/RSC and finally Macaulay/GCP, which is found to be almost impractical. All four methods are found to generate extraneous factors in the projection operator. Efficient heuristics for interpolation, used to expand the resultant matrices, are also discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bayer D., Stillman M., </author> <title> Macaulay User's Manual, </title> <publisher> Cor-nell University, </publisher> <address> Ithaca, NY. </address>
Reference-contexts: Besides resultant formulation timings, we also present timings for computing resultant using Grobner basis construction with block ordering (variables in one block and parameters in another) using the Macaulay <ref> [1] </ref> system. Grobner basis computations are also done in a finite field, in fact, with a much smaller characteristic (31991) than in the resultant computations.
Reference: [2] <author> Bernshtein D.N., </author> <title> The Number of Roots of a System of Equations, </title> <journal> Funktsional'nyi Analiz i Ego Prilozheniya, </journal> <volume> 9(3) </volume> <pages> 1-4. </pages>
Reference-contexts: The Dixon matrix can be rectangular, hence eliminating the possibility of computing the determi nant. Even if it is square, it may be singular, resulting in a trivial projection operator. 2.4 The Sparse Resultant Formulation This approach is based on the recent results pertaining to sparse polynomial systems (Bernshtein <ref> [2] </ref>, Gelfand et. al. [16] and Sturmfels [22]). Sparse resultants appeared in Stu-rmfels & Zelevinsky [23], its matrix formulation was given by Canny & Emiris [5] and was successively improved in [6, 11, 12] using better heuristics.
Reference: [3] <author> Buchberger B., </author> <title> Grobner bases: An Algorithmic Method in Polynomial Ideal Theory, Multidimensional Systems Theory, N.K. Bose, </title> <editor> ed., D. </editor> <publisher> Reidel Publ. Co., </publisher> <year> 1985. </year>
Reference-contexts: The resultant of a system of polynomials is an element of a Grobner basis of its ideal <ref> [3] </ref> if an elimination monomial ordering is used, but this is an expensive way to compute the resultant. Until recently, a classical method by Sylvester was used to compute the resultant [9].
Reference: [4] <author> Canny J., </author> <title> Generalized Characteristic Polynomials, </title> <journal> Journal of Symbolic Computation, </journal> <volume> 9 </volume> <pages> 241-250. </pages>
Reference-contexts: In this paper, we concentrate on such approaches. Three major multivariate resultant formulations are the Macaulay <ref> [20, 4] </ref>, Dixon [10, 18] and sparse [22, 5] resultant formulations. Given a set of polynomials, these formulations construct matrices, called the Macaulay matrix, the Dixon matrix and the sparse resultant matrix, respectively. <p> Such trivial projection operators are useless and give no information about the affine solutions of the system. One way to extract a nontrivial projection operator is by perturbation of the polynomial system <ref> [15, 4] </ref>. Ad hoc perturbations can be used with all three formulations, but they may not work. For Macaulay formulation, Canny [4] 1 Weyman and Zelevinsky [24] classify certain cases in which a pure determinantal formulation is possible. <p> One way to extract a nontrivial projection operator is by perturbation of the polynomial system [15, 4]. Ad hoc perturbations can be used with all three formulations, but they may not work. For Macaulay formulation, Canny <ref> [4] </ref> 1 Weyman and Zelevinsky [24] classify certain cases in which a pure determinantal formulation is possible. <p> Let M oe denote the Macaulay matrix for a permutation oe of polynomials in F . Macaulay also defined N oe , a certain square submatrix of M oe , such that the projection operator is the ratio of the determinants of M oe and N oe . See <ref> [20, 4, 17] </ref> for details of the construction and proofs of these properties. <p> Perturbation of polynomials can result in square and nonsingular resultant matrices <ref> [15, 4, 8] </ref>. One can try ad hoc perturbations of F in the hope of obtaining a nontrivial projection operator as follows. <p> Such ad hoc perturbations can be used in conjunction with all three resultant formulations, but they may not work. The following general perturbation mechanism applicable to Macaulay formulation was given by Canny in <ref> [4] </ref>. Let s be a new perturbation variable. Create a new set of polynomials F (s) by replacing each p j 2 F (for 1 j n) by p j sx j and p n+1 by p n+1 s. <p> Note that the size of the Macaulay matrix is same for F and F (s). For a proof of the fact that the projection operator so derived is not identically zero and it does vanish on all the affine zeros of the system, see <ref> [4] </ref>. We call this method Macaulay/GCP. We now finish the previous example using this method. Example: (Macaulay/GCP) For the example, det (M oe ) and det (N oe ), both are identically zero, so we compute the GCP.
Reference: [5] <author> Canny J. and Emiris I., </author> <title> An Efficient Algorithm for the Sparse Resultant, </title> <booktitle> Proc. of AAECC 93, </booktitle> <publisher> LNCS 263, Springer-Verlag, </publisher> <month> May </month> <year> 1993, </year> <pages> pp. 89-104. </pages>
Reference-contexts: In this paper, we concentrate on such approaches. Three major multivariate resultant formulations are the Macaulay [20, 4], Dixon [10, 18] and sparse <ref> [22, 5] </ref> resultant formulations. Given a set of polynomials, these formulations construct matrices, called the Macaulay matrix, the Dixon matrix and the sparse resultant matrix, respectively. <p> Sparse resultants appeared in Stu-rmfels & Zelevinsky [23], its matrix formulation was given by Canny & Emiris <ref> [5] </ref> and was successively improved in [6, 11, 12] using better heuristics. The Bezout bound on the number of solutions to a set of polynomials is quite loose. Instead, this formulation uses the recently developed, and significantly tighter, BKK bound [16] to construct a smaller resultant matrix than Macaulay.
Reference: [6] <author> Canny J. and Pedersen P., </author> <title> An Algorithm for New-ton Resultant, </title> <type> Technical Report, </type> <institution> Cornell University, CornellCS:TR93-1394, </institution> <year> 1993. </year>
Reference-contexts: Sparse resultants appeared in Stu-rmfels & Zelevinsky [23], its matrix formulation was given by Canny & Emiris [5] and was successively improved in <ref> [6, 11, 12] </ref> using better heuristics. The Bezout bound on the number of solutions to a set of polynomials is quite loose. Instead, this formulation uses the recently developed, and significantly tighter, BKK bound [16] to construct a smaller resultant matrix than Macaulay. <p> Recall that the size of the Macaulay matrix is jT j. Size of the sparse resultant matrix, jEj, is typically smaller than jT j, especially when the BKK bound is tighter than the Bezout bound. Algorithms in <ref> [11, 12, 6] </ref> construct matrices using some greedy heuristics which may result in smaller matrices, but in the worst case, the size can still be jEj.
Reference: [7] <author> Cayley A., </author> <title> On the Theory of Elimination. </title> <journal> Cambridge and Dublin Mathematical Journal, </journal> <volume> III, 1865, </volume> <pages> 210-270. </pages>
Reference-contexts: It turns out that more efficient methods, which eliminate all variables together from the set of polynomials, had been developed at the beginning of this century by mathematicians such as Cayley <ref> [7] </ref>, Dixon [10] and Macaulay [20]. These multivariate resultant formulations were recently resurrected by many researchers, and have numerous applications [21, 8, 17]. Typically, these multivariate resultant formulations try to express resultants as formulas involving determinants.
Reference: [8] <author> Chionh E., </author> <title> Base Points, Resultants and Implicit Representation of Rational Surfaces. </title> <type> Ph.D. thesis, </type> <institution> Dept. Comp. Sci., Univ. of Waterloo, </institution> <year> 1990. </year>
Reference-contexts: These multivariate resultant formulations were recently resurrected by many researchers, and have numerous applications <ref> [21, 8, 17] </ref>. Typically, these multivariate resultant formulations try to express resultants as formulas involving determinants. Ideally, one wants a single matrix whose determinant is the resultant, however this is not always possible 1 . <p> Perturbation of polynomials can result in square and nonsingular resultant matrices <ref> [15, 4, 8] </ref>. One can try ad hoc perturbations of F in the hope of obtaining a nontrivial projection operator as follows.
Reference: [9] <author> Collins G.E., </author> <title> The Calculation of Multivariate Polynomial Resultants. </title> <journal> JACM, </journal> <volume> 18 (4), </volume> <year> (1971), </year> <pages> 515-532. </pages>
Reference-contexts: The resultant of a system of polynomials is an element of a Grobner basis of its ideal [3] if an elimination monomial ordering is used, but this is an expensive way to compute the resultant. Until recently, a classical method by Sylvester was used to compute the resultant <ref> [9] </ref>. This method eliminates one variable from two polynomials, so to eliminate n variables from n + 1 polynomials, it has to be applied successively.
Reference: [10] <author> Dixon A.L., </author> <title> The Eliminant of Three Quantics in Two Independent Variables. </title> <journal> Proc. London Mathematical Society, </journal> <volume> 6, </volume> <year> 1908, </year> <pages> 468-478. </pages>
Reference-contexts: It turns out that more efficient methods, which eliminate all variables together from the set of polynomials, had been developed at the beginning of this century by mathematicians such as Cayley [7], Dixon <ref> [10] </ref> and Macaulay [20]. These multivariate resultant formulations were recently resurrected by many researchers, and have numerous applications [21, 8, 17]. Typically, these multivariate resultant formulations try to express resultants as formulas involving determinants. <p> In this paper, we concentrate on such approaches. Three major multivariate resultant formulations are the Macaulay [20, 4], Dixon <ref> [10, 18] </ref> and sparse [22, 5] resultant formulations. Given a set of polynomials, these formulations construct matrices, called the Macaulay matrix, the Dixon matrix and the sparse resultant matrix, respectively. <p> The coefficient matrix of ^ F is known as the Dixon matrix. It can be proven that for generic ndegree 4 polynomials, the Dixon matrix is square - hence its determinant is a projection operator <ref> [10, 18] </ref>.
Reference: [11] <author> Emiris I. and Canny J., </author> <title> A Practical Method for the Sparse Resultant, </title> <booktitle> Proc. ACM ISSAC, </booktitle> <pages> 183-192, </pages> <address> Kiev, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Sparse resultants appeared in Stu-rmfels & Zelevinsky [23], its matrix formulation was given by Canny & Emiris [5] and was successively improved in <ref> [6, 11, 12] </ref> using better heuristics. The Bezout bound on the number of solutions to a set of polynomials is quite loose. Instead, this formulation uses the recently developed, and significantly tighter, BKK bound [16] to construct a smaller resultant matrix than Macaulay. <p> Recall that the size of the Macaulay matrix is jT j. Size of the sparse resultant matrix, jEj, is typically smaller than jT j, especially when the BKK bound is tighter than the Bezout bound. Algorithms in <ref> [11, 12, 6] </ref> construct matrices using some greedy heuristics which may result in smaller matrices, but in the worst case, the size can still be jEj. <p> Division of the two determinants and extraction of the projection operator from the GCP was done on MAPLE. Implementation of RSC methods: We used our own MAPLE programs to construct the Macaulay and Dixon matrices, and Emiris' C program based on the algorithm in <ref> [11] </ref> to construct the sparse resultant matrix. The linear independence of any column in these matrices was checked probabilistically by substituting random numbers for parameters and performing the check on numeric (rather than symbolic) matrices.
Reference: [12] <author> Emiris I. and Canny J., </author> <title> Efficient Incremental Algorithms for the Sparse Resultant and the Mixed Volume, </title> <type> Technical Report, </type> <institution> Univ. of California, Berkeley, </institution> <year> 1994. </year>
Reference-contexts: Sparse resultants appeared in Stu-rmfels & Zelevinsky [23], its matrix formulation was given by Canny & Emiris [5] and was successively improved in <ref> [6, 11, 12] </ref> using better heuristics. The Bezout bound on the number of solutions to a set of polynomials is quite loose. Instead, this formulation uses the recently developed, and significantly tighter, BKK bound [16] to construct a smaller resultant matrix than Macaulay. <p> Recall that the size of the Macaulay matrix is jT j. Size of the sparse resultant matrix, jEj, is typically smaller than jT j, especially when the BKK bound is tighter than the Bezout bound. Algorithms in <ref> [11, 12, 6] </ref> construct matrices using some greedy heuristics which may result in smaller matrices, but in the worst case, the size can still be jEj.
Reference: [13] <author> Faugere J.C., </author> <title> Gb Reference Manual, PoSSo project, In-stitut Blaise Pascal, </title> <address> France. </address>
Reference-contexts: Grobner basis computations are also done in a finite field, in fact, with a much smaller characteristic (31991) than in the resultant computations. We also tried Grobner basis using GB system of Faugere <ref> [13] </ref>, but it does not support block ordering, and lexicographic ordering resulted in much inferior performance. Examples 3, 4 and 5 consist of generic polynomials with numerous parameters and dense resultants.
Reference: [14] <author> Gantmatcher F.R., </author> <title> Matrix Theory, </title> <journal> vol. </journal> <volume> 1. </volume> <publisher> Chelsea Publishing, </publisher> <year> 1959, </year> <note> Chap. 2. </note>
Reference-contexts: Macaulay/RSC, Dixon/RSC and Sparse/RSC. By rank submatrix we mean a maximal nonsingular submatrix, and its determinant can be computed by performing Gaussian elimination and returning the product of the (nonzero) pivots <ref> [18, 14] </ref>. In an earlier paper, we required a more complicated algorithm since we were deriving conditions for existence of solutions in C n , but for solutions in the algebraic torus (C f0g) n , this simple al gorithm suffices. See [18] for details.
Reference: [15] <author> Grigoryev D.Yu. and Chistov A.L., </author> <title> Sub-exponential Time Solving of Systems of Algebraic Equations. </title> <booktitle> LOMI Preprints E-9-83 and E-10-83, </booktitle> <address> Leningrad, </address> <year> 1983. </year>
Reference-contexts: Such trivial projection operators are useless and give no information about the affine solutions of the system. One way to extract a nontrivial projection operator is by perturbation of the polynomial system <ref> [15, 4] </ref>. Ad hoc perturbations can be used with all three formulations, but they may not work. For Macaulay formulation, Canny [4] 1 Weyman and Zelevinsky [24] classify certain cases in which a pure determinantal formulation is possible. <p> Perturbation of polynomials can result in square and nonsingular resultant matrices <ref> [15, 4, 8] </ref>. One can try ad hoc perturbations of F in the hope of obtaining a nontrivial projection operator as follows.
Reference: [16] <author> Gelfand I.M., </author> <title> Kapranov M.M. and Zelevinsky A.V., Discriminants, Resultants and Multidimensional Determinants, </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: Even if it is square, it may be singular, resulting in a trivial projection operator. 2.4 The Sparse Resultant Formulation This approach is based on the recent results pertaining to sparse polynomial systems (Bernshtein [2], Gelfand et. al. <ref> [16] </ref> and Sturmfels [22]). Sparse resultants appeared in Stu-rmfels & Zelevinsky [23], its matrix formulation was given by Canny & Emiris [5] and was successively improved in [6, 11, 12] using better heuristics. The Bezout bound on the number of solutions to a set of polynomials is quite loose. <p> The Bezout bound on the number of solutions to a set of polynomials is quite loose. Instead, this formulation uses the recently developed, and significantly tighter, BKK bound <ref> [16] </ref> to construct a smaller resultant matrix than Macaulay. Newton Polytope of a polynomial p is the convex hull of the set of exponents (treated as points in IR n ) of all terms in p.
Reference: [17] <author> Kapur D. and Lakshman Y.N., </author> <title> Elimination Methods: an Introduction. Symbolic and Numerical Computation for Artificial Intelligence B. </title> <editor> Donald et. al. (eds.), </editor> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: These multivariate resultant formulations were recently resurrected by many researchers, and have numerous applications <ref> [21, 8, 17] </ref>. Typically, these multivariate resultant formulations try to express resultants as formulas involving determinants. Ideally, one wants a single matrix whose determinant is the resultant, however this is not always possible 1 . <p> Let M oe denote the Macaulay matrix for a permutation oe of polynomials in F . Macaulay also defined N oe , a certain square submatrix of M oe , such that the projection operator is the ratio of the determinants of M oe and N oe . See <ref> [20, 4, 17] </ref> for details of the construction and proofs of these properties.
Reference: [18] <author> Kapur D., Saxena T. and Yang L., </author> <title> Algebraic and Geometric Reasoning using Dixon Resultants, </title> <booktitle> Proc. ACM ISSAC, </booktitle> <address> Oxford, England, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: In this paper, we concentrate on such approaches. Three major multivariate resultant formulations are the Macaulay [20, 4], Dixon <ref> [10, 18] </ref> and sparse [22, 5] resultant formulations. Given a set of polynomials, these formulations construct matrices, called the Macaulay matrix, the Dixon matrix and the sparse resultant matrix, respectively. <p> We call this the Macaulay/GCP method. Another method for extraction of a nontrivial projection in the face of singular resultant matrices was outlined in our previous paper <ref> [18] </ref>. This method uses rank subdeterminant computation (RSC) which, though developed in conjunction with Dixon formulation, is applicable to all three formulations. This gives three more methods, Macaulay/RSC, Dixon/RSC and Sparse/RSC, giving a total of four methods for computing nontrivial projection operators. <p> The coefficient matrix of ^ F is known as the Dixon matrix. It can be proven that for generic ndegree 4 polynomials, the Dixon matrix is square - hence its determinant is a projection operator <ref> [10, 18] </ref>. <p> Macaulay/RSC, Dixon/RSC and Sparse/RSC. By rank submatrix we mean a maximal nonsingular submatrix, and its determinant can be computed by performing Gaussian elimination and returning the product of the (nonzero) pivots <ref> [18, 14] </ref>. In an earlier paper, we required a more complicated algorithm since we were deriving conditions for existence of solutions in C n , but for solutions in the algebraic torus (C f0g) n , this simple al gorithm suffices. See [18] for details. <p> In an earlier paper, we required a more complicated algorithm since we were deriving conditions for existence of solutions in C n , but for solutions in the algebraic torus (C f0g) n , this simple al gorithm suffices. See <ref> [18] </ref> for details. Though there exist examples where the condition in step (2) is not true, they seem rare. In all examples we tried, R was degenerate many times, but the condition of step (2) was always satisfied.
Reference: [19] <author> Kapur D and Saxena T., </author> <title> Sparsity Considerations in the Dixon Resultant Formulation, </title> <type> manuscript under preparation. </type>
Reference-contexts: We will use this knowledge to carry out a comparison of various methods in a later section. For a comprehensive analysis of matrix sizes and comparison for multi-homogeneous systems, the reader may wish to consult <ref> [19] </ref>. 5 Implementational details Since the resultant matrices are symbolic (each entry being a polynomial in the parameters), their determinants (or rank subdeterminants) are computed using polynomial interpolation rather than direct Gaussian elimination. <p> Even though the real-world examples we presented show that the Dixon matrix is indeed much smaller than the other two, a theoretical analysis which takes into account sparsity needs to be done. Our preliminary work in this direction <ref> [19] </ref> shows that, like the sparse resultant formulation, Dixon formulation also exploits the Newton polytopes and takes into account the sparsity of the input polynomials, though in a different fashion. Extraneous factors occur in all methods in this paper, however, it is not clear which method generates least factors.
Reference: [20] <author> Macaulay F.S., </author> <title> The Algebraic Theory of Modular Systems, </title> <journal> Cambridge Tracts in Math. and Math. Phys., </journal> <volume> 19, </volume> <year> 1916. </year>
Reference-contexts: It turns out that more efficient methods, which eliminate all variables together from the set of polynomials, had been developed at the beginning of this century by mathematicians such as Cayley [7], Dixon [10] and Macaulay <ref> [20] </ref>. These multivariate resultant formulations were recently resurrected by many researchers, and have numerous applications [21, 8, 17]. Typically, these multivariate resultant formulations try to express resultants as formulas involving determinants. Ideally, one wants a single matrix whose determinant is the resultant, however this is not always possible 1 . <p> In this paper, we concentrate on such approaches. Three major multivariate resultant formulations are the Macaulay <ref> [20, 4] </ref>, Dixon [10, 18] and sparse [22, 5] resultant formulations. Given a set of polynomials, these formulations construct matrices, called the Macaulay matrix, the Dixon matrix and the sparse resultant matrix, respectively. <p> Let M oe denote the Macaulay matrix for a permutation oe of polynomials in F . Macaulay also defined N oe , a certain square submatrix of M oe , such that the projection operator is the ratio of the determinants of M oe and N oe . See <ref> [20, 4, 17] </ref> for details of the construction and proofs of these properties.
Reference: [21] <author> Sederberg T. and Goldman R., </author> <title> Algebraic Geometry for Computer-Aided Design, </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> Vol. 6, No. 6, </volume> <pages> pp. 52-59, </pages> <year> 1986. </year>
Reference-contexts: These multivariate resultant formulations were recently resurrected by many researchers, and have numerous applications <ref> [21, 8, 17] </ref>. Typically, these multivariate resultant formulations try to express resultants as formulas involving determinants. Ideally, one wants a single matrix whose determinant is the resultant, however this is not always possible 1 .
Reference: [22] <author> Sturmfels B., </author> <title> Sparse Elimination Theory, </title> <booktitle> Proc. Compu-tat. Algebraic Geom. and Commut. Algebra, </booktitle> <editor> D. Eisen-bud and L. Robbiano, eds., Cortona, </editor> <address> Italy, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: In this paper, we concentrate on such approaches. Three major multivariate resultant formulations are the Macaulay [20, 4], Dixon [10, 18] and sparse <ref> [22, 5] </ref> resultant formulations. Given a set of polynomials, these formulations construct matrices, called the Macaulay matrix, the Dixon matrix and the sparse resultant matrix, respectively. <p> Even if it is square, it may be singular, resulting in a trivial projection operator. 2.4 The Sparse Resultant Formulation This approach is based on the recent results pertaining to sparse polynomial systems (Bernshtein [2], Gelfand et. al. [16] and Sturmfels <ref> [22] </ref>). Sparse resultants appeared in Stu-rmfels & Zelevinsky [23], its matrix formulation was given by Canny & Emiris [5] and was successively improved in [6, 11, 12] using better heuristics. The Bezout bound on the number of solutions to a set of polynomials is quite loose.
Reference: [23] <author> Sturmfels B. and Zelevinsky A., </author> <title> Multigraded resultants of the Sylvester type, </title> <journal> Journal of Algebra, </journal> <year> 1992. </year>
Reference-contexts: Even if it is square, it may be singular, resulting in a trivial projection operator. 2.4 The Sparse Resultant Formulation This approach is based on the recent results pertaining to sparse polynomial systems (Bernshtein [2], Gelfand et. al. [16] and Sturmfels [22]). Sparse resultants appeared in Stu-rmfels & Zelevinsky <ref> [23] </ref>, its matrix formulation was given by Canny & Emiris [5] and was successively improved in [6, 11, 12] using better heuristics. The Bezout bound on the number of solutions to a set of polynomials is quite loose.
Reference: [24] <author> Weyman J. and Zelevinsky A., </author> <title> Determinantal Formulas for Multigraded Resultants, </title> <journal> Journal of Algebraic Geometry, </journal> <pages> pp 569-597, </pages> <year> 1994. </year>
Reference-contexts: One way to extract a nontrivial projection operator is by perturbation of the polynomial system [15, 4]. Ad hoc perturbations can be used with all three formulations, but they may not work. For Macaulay formulation, Canny [4] 1 Weyman and Zelevinsky <ref> [24] </ref> classify certain cases in which a pure determinantal formulation is possible.
Reference: [25] <author> Zippel R. </author> <title> Effective Polynomial Computation, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1993. </year>
Reference-contexts: Implementation of Macaulay/GCP: M oe (s) and N oe (s) (see section 3.1) for F (s) were constructed using our implementation in MAPLE. We used our C ++ implementation of Zippel's sparse polynomial interpolation algorithm <ref> [25] </ref> to compute their determinants. Division of the two determinants and extraction of the projection operator from the GCP was done on MAPLE. <p> At the end of the m th stage, the polynomial has been completely interpolated. See <ref> [25] </ref> for details. We performed all computations in a finite field modulo a large prime. If the exact projection operator is required, the computations can be performed in various finite fields modulo different primes until the product of the primes is larger than a predetermined coefficient bound.
References-found: 25

