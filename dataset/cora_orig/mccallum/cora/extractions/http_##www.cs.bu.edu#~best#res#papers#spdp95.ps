URL: http://www.cs.bu.edu/~best/res/papers/spdp95.ps
Refering-URL: http://cs-www.bu.edu/faculty/best/res/Home.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: (best@cs.bu.edu)  
Title: Demand-based Document Dissemination to Reduce Traffic and Balance Load in Distributed Information Systems  
Author: Azer Bestavros 
Address: Boston University, MA 02215  
Affiliation: Computer Science Department  
Date: Oct 1995.  
Note: In Proceedings ofSPDP'95: The 7th IEEE Symposium on Parallel and DistributedProcessing, San Anotonio, TX,  
Abstract: Research on replication techniques to reduce traffic and minimize the latency of information retrieval in a distributed system has concentrated on client-based caching, whereby recently/frequently accessed information is cached at a client (or at a proxy thereof) in anticipation of future accesses. We believe that such myopic solutions|focussing exclusively on a particular client or set of clients|are likely to have a limited impact. Instead, we offer a solution that allows the replication of information to be done on a global supply/demand basis. We propose a hierarchical demand-based replication strategy that optimally disseminates information from its producer to servers that are closer to its consumers. The level of dissemination depends on the relative popularity of documents, and on the expected reduction in traffic that results from such dissemination. We used extensive HTTP logs to validate an analytical model of server popularity and file access profiles. Using that model we show that by disseminating the most popular documents on servers closer to clients, network traffic could be reduced considerably, while servers are load-balanced. We argue that this process could be generalized to provide for an automated server-based information dissemination protocol that will be more effective in reducing both network bandwidth and document retrieval times than client-based caching protocols. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Swarup Acharya and Stanley B. Zdonik. </author> <title> An efficient scheme for dynamic data replication. </title> <type> Technical Report CS-93-43, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island 02912, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: In this study, a hierarchical caching system that caches files at Core Nodal Switching Subsystems is shown to reduce the NSFNET backbone traffic by 21%. The effect of data placement and replication on network traffic was also studied in <ref> [1] </ref>, where file access patterns are used to suggest a distributed dynamic replication scheme. A more static solution based on fixed network and stor age costs for the delivery of multimedia home enter-tainment was suggested in [12].
Reference: [2] <author> Azer Bestavros, Robert Carter, Mark Crovella, Carlos Cunha, Abdelsalam Heddaya, and Sulaiman Mirdad. </author> <title> Application level document caching in the internet. </title> <booktitle> In IEEE SDNE'96: The Second International Workshop on Services in Distributed and Networked Environments, </booktitle> <address> Whistler, British Columbia, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: (EffectiveURL) ; State ServerReply.Ack ; TempURL ServerReply.NextURL ; g If (State == Found) Fetch (EffectiveURL) ; Else FailFetch ("Document not found.") ; 4 Notice that our protocol does not preclude the EffectiveURL from pointing to the local cache of the client itself (whether at the session, machine, or LAN levels <ref> [2] </ref>). This makes for a natural integration of producer-based dissemination and consumer-based caching of documents. Server Query Protocol: DDD-WWW requires servers to maintain a (possibly one-to-many) mapping between local URLs and the URLs of corresponding disseminated copies.
Reference: [3] <author> Matthew Addison Blaze. </author> <title> Caching in Large Scale Distributed File Systems. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: A more static solution based on fixed network and stor age costs for the delivery of multimedia home enter-tainment was suggested in [12]. Multi-level caching was studied in [11], where simulations of a two-level caching system is shown to reduce both network and server loads. In <ref> [3] </ref>, a dynamic hierarchical file system, which supports demand-driven replication is proposed, whereby clients are allowed to service requests issued by other clients from the local disk cache. A similar cooperative caching idea was suggested in [5].
Reference: [4] <author> V. Cate. </author> <title> Alex | a global filesystem. </title> <booktitle> In Proceedings of the 1992 USENIX File System Workshop, </booktitle> <address> Ann Arbor, MI, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: This has implications regarding the overhead of maintaining the coherence of disseminated documents. In particular, given the rarity of popular documents updates, we argue that simple protocols such as the Time-To-Live (TTL) and Alex <ref> [4] </ref> protocols are attractive alternatives to the high-overhead invalidation-based protocols [15]. 3 System Model and Analysis We model the WWW (Internet) as a hierarchical set of clusters. A cluster consists of a number of servers. One of these servers acts as a service proxy (or front-end) for the cluster.
Reference: [5] <author> Michael D. Dahlin, Randolph Y. Wang, Thomas E. Anderson, and Dacid A. Patterson. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In First Symposium on Operating systems Design and Implementation (OSDI), </booktitle> <pages> pages 267-280, </pages> <year> 1994. </year>
Reference-contexts: In [3], a dynamic hierarchical file system, which supports demand-driven replication is proposed, whereby clients are allowed to service requests issued by other clients from the local disk cache. A similar cooperative caching idea was suggested in <ref> [5] </ref>. The proposed research work of Gwertzman and Seltzer sketched in [8] is the closest to ours.
Reference: [6] <author> Peter Danzig, Richard Hall, and Michael Schwartz. </author> <title> A case for cashing file objects inside internetworks. </title> <type> Technical Report CU-CS-642-93, </type> <institution> University of Col-orado at Boulder, Boulder, Colorado 80309-430, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Recently, there have been some attempts at extending caching and replication to distributed information systems (e.g. FTP and HTTP). Caching to reduce the bandwidth requirements for the FTP protocol on the NSFNET has been studied in <ref> [6] </ref>. In this study, a hierarchical caching system that caches files at Core Nodal Switching Subsystems is shown to reduce the NSFNET backbone traffic by 21%. <p> First, adding servers (i.e. proxies) to the internet is much cheaper than adding (upgrading) internet links <ref> [6] </ref>. Second, increasing the available bandwidth is a temporary solution; it's only a matter of time before the added bandwidth is consumed by the ever increasing number of users.
Reference: [7] <author> Michael Foster and Robert Jump. </author> <title> NSF Solicitation 94-75. </title> <booktitle> STIS database, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Perhaps the best "living" proof of the seriousness of this problem is the fate of many information servers on the Internet: they are unreacheable as soon as they become popular. In a recent solicitation <ref> [7] </ref> from the National Science Foundation's ES and MSA programs, the following research topics were fl This work has been partially supported by NSF (grant CCR-9308344). deemed critical for projected applications of the Na tional Information Infrastructure (NII): New techniques for organizing cache memories and other buffering schemes to alleviate memory
Reference: [8] <author> James Gwertzman and Margo Seltzer. </author> <title> The case for geographical push-caching. </title> <type> Technical Report HU TR-34-94 (excerpt), </type> <institution> Harvard University, DAS, </institution> <address> Cam-bridge, MA 02138, </address> <year> 1994. </year>
Reference-contexts: In [3], a dynamic hierarchical file system, which supports demand-driven replication is proposed, whereby clients are allowed to service requests issued by other clients from the local disk cache. A similar cooperative caching idea was suggested in [5]. The proposed research work of Gwertzman and Seltzer sketched in <ref> [8] </ref> is the closest to ours. In particular, they propose the implementation of what they termed as geographical push-cashing, which allows servers to decide when and where to cache information based on geographical information (such as the distance in actual miles between servers and clients). <p> Such a classification could be used by servers to decide which documents to disseminate. It is interesting to note that our update frequency measure 3 Multiple updates to a document within one day were counted as one update. ments (like those discussed in <ref> [8] </ref>) depart significantly from the synthetic workload used in recent WWW coherence studies [15]. This has implications regarding the overhead of maintaining the coherence of disseminated documents.
Reference: [9] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: There has been quite a bit of research on caching and replication to improve the availability and performance of scalable distributed file systems <ref> [9] </ref>. Example systems include the Sun NFS [13], the An-drew File System [10], and the Coda system [14]. Recently, there have been some attempts at extending caching and replication to distributed information systems (e.g. FTP and HTTP).
Reference: [10] <author> J.H. Morris, M. Satyanarayanan, M.H. Conner, J.H. Howard, D.S.H. Rosenthal, and F.D. Smith. An-drew: </author> <title> a distributed personal computing environment. </title> <journal> Comm. ACM, </journal> <volume> 29(3) </volume> <pages> 184-201, </pages> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: There has been quite a bit of research on caching and replication to improve the availability and performance of scalable distributed file systems [9]. Example systems include the Sun NFS [13], the An-drew File System <ref> [10] </ref>, and the Coda system [14]. Recently, there have been some attempts at extending caching and replication to distributed information systems (e.g. FTP and HTTP). Caching to reduce the bandwidth requirements for the FTP protocol on the NSFNET has been studied in [6].
Reference: [11] <author> D. Muntz and P. Honeyman. </author> <title> Multi-level caching in distributed file systems or your cache ain't nuthing but trash. </title> <booktitle> In Proceedings of the Winter 1992 USENIX, </booktitle> <pages> pages 305-313, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: A more static solution based on fixed network and stor age costs for the delivery of multimedia home enter-tainment was suggested in [12]. Multi-level caching was studied in <ref> [11] </ref>, where simulations of a two-level caching system is shown to reduce both network and server loads. In [3], a dynamic hierarchical file system, which supports demand-driven replication is proposed, whereby clients are allowed to service requests issued by other clients from the local disk cache.
Reference: [12] <author> Christos H. Papadimitriou, Srinivas Ramanathan, and P. Venkat Rangan. </author> <title> Information caching for delivery of personalized video programs on home entertainment channels. </title> <booktitle> In Proceedings of the International Confrence on Multimedia Computing and Systems, </booktitle> <pages> pages 214-223, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: A more static solution based on fixed network and stor age costs for the delivery of multimedia home enter-tainment was suggested in <ref> [12] </ref>. Multi-level caching was studied in [11], where simulations of a two-level caching system is shown to reduce both network and server loads.
Reference: [13] <author> R. Sandber, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and implementation of the sun network file system. </title> <booktitle> In Proceedings of USENIX Summer Conference, </booktitle> <year> 1985. </year>
Reference-contexts: There has been quite a bit of research on caching and replication to improve the availability and performance of scalable distributed file systems [9]. Example systems include the Sun NFS <ref> [13] </ref>, the An-drew File System [10], and the Coda system [14]. Recently, there have been some attempts at extending caching and replication to distributed information systems (e.g. FTP and HTTP). Caching to reduce the bandwidth requirements for the FTP protocol on the NSFNET has been studied in [6].
Reference: [14] <author> M. Satyanarayanan, J. Kistler, P. Ku-mar, M. Okasaki, E. Siegel, and D. Streere. Coda: </author> <title> A highly available file system for distributed workstation environments. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4), </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: There has been quite a bit of research on caching and replication to improve the availability and performance of scalable distributed file systems [9]. Example systems include the Sun NFS [13], the An-drew File System [10], and the Coda system <ref> [14] </ref>. Recently, there have been some attempts at extending caching and replication to distributed information systems (e.g. FTP and HTTP). Caching to reduce the bandwidth requirements for the FTP protocol on the NSFNET has been studied in [6].
Reference: [15] <author> K. Worrell. </author> <title> Invalidation in large scale network object caches, 1994. </title> <type> Master's Thesis, </type> <institution> University of Colorado, Boulder. </institution>
Reference-contexts: It is interesting to note that our update frequency measure 3 Multiple updates to a document within one day were counted as one update. ments (like those discussed in [8]) depart significantly from the synthetic workload used in recent WWW coherence studies <ref> [15] </ref>. This has implications regarding the overhead of maintaining the coherence of disseminated documents. In particular, given the rarity of popular documents updates, we argue that simple protocols such as the Time-To-Live (TTL) and Alex [4] protocols are attractive alternatives to the high-overhead invalidation-based protocols [15]. 3 System Model and Analysis <p> in recent WWW coherence studies <ref> [15] </ref>. This has implications regarding the overhead of maintaining the coherence of disseminated documents. In particular, given the rarity of popular documents updates, we argue that simple protocols such as the Time-To-Live (TTL) and Alex [4] protocols are attractive alternatives to the high-overhead invalidation-based protocols [15]. 3 System Model and Analysis We model the WWW (Internet) as a hierarchical set of clusters. A cluster consists of a number of servers. One of these servers acts as a service proxy (or front-end) for the cluster.
References-found: 15

