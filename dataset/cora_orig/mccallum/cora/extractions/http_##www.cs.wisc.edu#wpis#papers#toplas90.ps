URL: http://www.cs.wisc.edu/wpis/papers/toplas90.ps
Refering-URL: http://www.cs.wisc.edu/~reps/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: 1 Interprocedural Slicing Using Dependence Graphs  
Author: SUSAN HORWITZ, THOMAS REPS, and DAVID BINKLEY 
Keyword: CR Categories and Subject Descriptors: D.3.3 [Programming Languages]: Language Constructs control structures, procedures, functions, and subroutines; D.3.4 [Programming Languages]: Processors compilers, optimization General Terms: Algorithms, Design Additional Key Words and Phrases: attribute grammar, control dependence, data dependence, data-flow analysis, flow-insensitive summary information, program debugging, program dependence graph, program integration, program slicing, subordinate characteristic graph  
Affiliation: University of Wisconsin-Madison  
Abstract: The notion of a program slice, originally introduced by Mark Weiser, is useful in program debugging, automatic paral-lelization, and program integration. A slice of a program is taken with respect to a program point p and a variable x; the slice consists of all statements of the program that might affect the value of x at point p. This paper concerns the problem of interprocedural slicinggenerating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation. (It should be noted that our work concerns a somewhat restricted kind of slice: Rather than permitting a program to be sliced with respect to program point p and an arbitrary variable, a slice must be taken with respect to a variable that is defined or used at p.) The chief difficulty in interprocedural slicing is correctly accounting for the calling context of a called procedure. To handle this problem, system dependence graphs include some data-dependence edges that represent transitive dependences due to the effects of procedure calls, in addition to the conventional direct-dependence edges. These edges are constructed with the aid of an auxiliary structure that represents calling and parameter-linkage relationships. This structure takes the form of an attribute grammar. The step of computing the required transitive-dependence edges is reduced to the construction of the subordinate characteristic graphs for the grammar's nonterminals. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Aho, A., Sethi, R., and Ullman, J., </author> <booktitle> Compilers: Principles, Techniques and Tools, </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA (1986). </address>
Reference-contexts: In addition, G P includes three other categories of vertices: (1) There is a distinguished vertex called the entry vertex. (2) For each variable x for which there is a path in the standard control-flow graph for P on which x is used before being defined (see <ref> [1] </ref>), there is a vertex called the initial definition of x. This vertex represents an assignment to x from the initial state. The vertex is labeled x := InitialState (x). (3) For each variable x named in P's end statement, there is a vertex called the final use of x.
Reference: 2. <author> Babich, W.A. and Jazayeri, M., </author> <title> The method of attributes for data flow analysis: Part II. Demand analysis, </title> <note> Acta Informatica 10(3) pp. </note> <month> 265-272 (October </month> <year> 1978). </year>
Reference-contexts: One difference between the interprocedural slicing problem and the problems addressed by the work cited above, is that interprocedural slicing is a demand problem <ref> [2] </ref> whose goal is to determine information concerning a specific set of program points rather than an exhaustive problem in which the goal is to determine information for all program points.
Reference: 3. <author> Badger, Lee and Weiser, Mark, </author> <title> Minimizing Communication for Synchronizing Parallel Dataflow Programs, </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing II, Software pp. </booktitle> <address> 122-126 Penn State, </address> <month> (August </month> <year> 1988). </year>
Reference: 4. <author> Banning, J.P., </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables, pp. </title> <booktitle> 29-41 in Conference Record of the Sixth ACM Symposium on Principles of Programming Languages, </booktitle> <address> (San Antonio, TX, </address> <month> January 29-31, </month> <year> 1979), (1979). </year>
Reference-contexts: The appropriate interprocedural summary information consists of the following sets, which are computed for each procedure P <ref> [4] </ref>: GMOD (P): the set of variables that might be modified by P itself or by a procedure (transitively) called from P. GREF (P): the set of variables that might be referenced by P itself or by a procedure (transitively) called from P. <p> Our first approach to the problem of interprocedural slicing in the presence of aliasing is to reduce the problem to that of interprocedural slicing in the absence of aliasing. The conversion is performed by simulating the calling behavior of the system (using the usual activation-tree model of procedure calls <ref> [4] </ref>) to discover, for each instance of a procedure call, exactly how variables are aliased at that instance. (Although a recursive system's activation tree is infinite, the number of different alias configurations is finite; thus, only a finite portion of the activation tree is needed to compute aliasing information.) A new <p> Example. Figure 14 shows a system with aliasing, and the portion of the system's activation tree that is used to compute alias information for each call instance. We use the notation of <ref> [4] </ref>, in which each node of the activation tree is labeled with the mapping from variable names to memory locations. The transformed, alias-free version of the system is shown below.
Reference: 5. <author> Callahan, D., </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis, </title> <booktitle> Proceedings of the ACM SIG-PLAN 88 Conference on Programming Language Design and Implementation, </booktitle> <address> (Atlanta, GA, </address> <month> June 22-24, </month> <year> 1988), </year> <journal> ACM SIGPLAN Notices 23(7) pp. </journal> <month> 47-56 (July </month> <year> 1988). </year>
Reference-contexts: The vertex-reachability approach we have used here has some similarities to a technique used in [15], <ref> [5] </ref>, and [6] to transform data flow analysis problems to vertex-reachability problems.
Reference: 6. <author> Cooper, K.D. and Kennedy, K., </author> <title> Interprocedural side-effect analysis in linear time, </title> <booktitle> Proceedings of the ACM SIGPLAN 88 Conference on Programming Language Design and Implementation, </booktitle> <address> (Atlanta, GA, </address> <month> June 22-24, </month> <year> 1988), </year> <journal> ACM SIGPLAN Notices 23(7) pp. </journal> <month> 57-66 (July </month> <year> 1988). </year>
Reference-contexts: Flow insensitive interprocedural summary information (e.g. GMOD and GREF) can be determined particularly efficiently. In particular, in the absence of nested scopes, GMOD and GREF can be determined in time O (P 2 + Pmult_byTotalSites) steps by the algorithm described in <ref> [6] </ref>. Intraprocedural data flow analysis is used to determine the data dependences of procedure dependence graphs. <p> The vertex-reachability approach we have used here has some similarities to a technique used in [15], [5], and <ref> [6] </ref> to transform data flow analysis problems to vertex-reachability problems.
Reference: 7. <author> Ferrante, J., Ottenstein, K., and Warren, J., </author> <title> The program dependence graph and its use in optimization, </title> <journal> ACM Transactions on Programming Languages and Systems 9(3) pp. </journal> <month> 319-349 (July </month> <year> 1987). </year>
Reference-contexts: The program dependence graphs defined in <ref> [7] </ref> introduced the additional feature of an explicit representation for control dependences (see below). The definition of program dependence graph given below differs from [7] in two ways. <p> The program dependence graphs defined in <ref> [7] </ref> introduced the additional feature of an explicit representation for control dependences (see below). The definition of program dependence graph given below differs from [7] in two ways. First, our definition covers only a restricted language with scalar variables, assignment statements, conditional statements, while loops, and a restricted kind of output statement called an end statement, 2 and hence is less general than the one given in [7]. <p> program dependence graph given below differs from <ref> [7] </ref> in two ways. First, our definition covers only a restricted language with scalar variables, assignment statements, conditional statements, while loops, and a restricted kind of output statement called an end statement, 2 and hence is less general than the one given in [7]. Second, we omit certain classes of data dependence edges and make use of a class introduced in [8]. Despite these differences, the structures we define and those defined in [7] share the feature of explicitly representing both control and data dependences; therefore, we refer to our graphs as program dependence <p> restricted kind of output statement called an end statement, 2 and hence is less general than the one given in <ref> [7] </ref>. Second, we omit certain classes of data dependence edges and make use of a class introduced in [8]. Despite these differences, the structures we define and those defined in [7] share the feature of explicitly representing both control and data dependences; therefore, we refer to our graphs as program dependence graphs, borrowing the term from [7]. 2.1. <p> Despite these differences, the structures we define and those defined in <ref> [7] </ref> share the feature of explicitly representing both control and data dependences; therefore, we refer to our graphs as program dependence graphs, borrowing the term from [7]. 2.1. The Program Dependence Graph The program dependence graph for program P, denoted by G P , is a directed graph whose vertices are con nected by several kinds of edges. 3 The vertices of G P represent the assignment statements and control predicates that occur in program P. <p> A method for determining control dependence edges for arbitrary programs is given in <ref> [7] </ref>; however, because we are assuming that programs include only assignment, conditional, and while statements, the control dependence edges of G P can be determined in a much simpler fashion.
Reference: 8. <author> Horwitz, S., Prins, J., and Reps, T., </author> <title> Integrating non-interfering versions of programs, </title> <type> Technical Report 690, </type> <institution> Department of Computer Sciences, University of WisconsinMadison (March 1987). </institution>
Reference-contexts: Second, we omit certain classes of data dependence edges and make use of a class introduced in <ref> [8] </ref>. Despite these differences, the structures we define and those defined in [7] share the feature of explicitly representing both control and data dependences; therefore, we refer to our graphs as program dependence graphs, borrowing the term from [7]. 2.1. <p> Intraprocedural data flow analysis is used to determine the data dependences of procedure dependence graphs. For the structured language under consideration here, this analysis can be performed in a syntax ACM TOPLAS vol 12 no 1 (January 1990) - 30 - directed fashion (for example, using an attribute grammar) <ref> [8] </ref>. This involves propagating sets of program points, where each set consists of program points in a single procedure. This computation has total cost O (V 2 ).
Reference: 9. <author> Horwitz, S., Reps, T., and Binkley, D., </author> <title> Interprocedural slicing using dependence graphs, </title> <booktitle> Proceedings of the SIGPLAN 88 Conference on Programming Language Design and Implementation, </booktitle> <address> (Atlanta, GA, </address> <month> June 22-24, </month> <year> 1988), </year> <journal> ACM SIGPLAN Notices 23(7) pp. </journal> <month> 35-46 (July </month> <year> 1988). </year> <note> ACM TOPLAS vol 12 no 1 (January 1990) - 34 </note> - 
Reference-contexts: Authors' address: Computer Sciences Department, University of Wisconsin - Madison, 1210 W. Dayton St., Madison, WI 53706. An earlier version of this paper appeared in abridged form in the Proceedings of the ACM SIGPLAN 88 Conference on Programming Language Design and Implementation, <ref> (Atlanta, GA, June 22-24, 1988) </ref>, ACM SIGPLAN Notices 23 (7) (July 1988) [9]. ACM TOPLAS vol 12 no 1 (January 1990) - 2 - programmer understand complicated code, can aid in debugging [17], and can be used for automatic paral-lelization [213]. <p> Dayton St., Madison, WI 53706. An earlier version of this paper appeared in abridged form in the Proceedings of the ACM SIGPLAN 88 Conference on Programming Language Design and Implementation, (Atlanta, GA, June 22-24, 1988), ACM SIGPLAN Notices 23 (7) <ref> (July 1988) </ref> [9]. ACM TOPLAS vol 12 no 1 (January 1990) - 2 - programmer understand complicated code, can aid in debugging [17], and can be used for automatic paral-lelization [213]. <p> Dayton St., Madison, WI 53706. An earlier version of this paper appeared in abridged form in the Proceedings of the ACM SIGPLAN 88 Conference on Programming Language Design and Implementation, (Atlanta, GA, June 22-24, 1988), ACM SIGPLAN Notices 23 (7) (July 1988) <ref> [9] </ref>. ACM TOPLAS vol 12 no 1 (January 1990) - 2 - programmer understand complicated code, can aid in debugging [17], and can be used for automatic paral-lelization [213]. <p> After the initial publication of our interprocedural-slicing algorithm <ref> [9] </ref>, a different technique for computing interprocedural slices was presented by Hwang, Du, and Chou [12]. The slicing algorithm presented in [12] computes an answer that is as precise as our algorithm, but differs significantly in how it handles the calling-context problem.
Reference: 10. <author> Horwitz, S., Prins, J., and Reps, T., </author> <title> On the adequacy of program dependence graphs for representing programs, pp. </title> <booktitle> 146-157 in Conference Record of the Fifteenth ACM Symposium on Principles of Programming Languages, </booktitle> <address> (San Diego, CA, </address> <month> January 13-15, </month> <year> 1988), (1988). </year>
Reference-contexts: This kind of edge is left out of our definition because it is not necessary for our purposes. 5 For a complete discussion of the need for these edges and a comparison of def-order dependences with anti- and output dependences see <ref> [10] </ref>.
Reference: 11. <author> Horwitz, S., Prins, J., and Reps, T., </author> <title> Integrating non-interfering versions of programs, </title> <journal> ACM Transactions on Programming Languages and Systems 11(3) pp. </journal> <month> 345-387 (July </month> <year> 1989). </year>
Reference-contexts: ACM TOPLAS vol 12 no 1 (January 1990) - 2 - programmer understand complicated code, can aid in debugging [17], and can be used for automatic paral-lelization [213]. Program slicing is also used by the algorithm for automatically integrating program variants described in <ref> [11] </ref>; slices are used to compute a safe approximation to the change in behavior between a program P and a modified version of P, and to help determine whether two different modifications to P interfere.
Reference: 12. <author> Hwang, J.C., Du, M.W., and Chou, C.R., </author> <title> Finding program slices for recursive procedures, </title> <booktitle> in Proceedings of IEEE COMPSAC 88, </booktitle> <address> (Chicago, IL, </address> <month> Oct. </month> <pages> 3-7, </pages> <address> 1988), </address> <publisher> IEEE Computer Society, </publisher> <address> Washington, DC (1988). </address>
Reference-contexts: After the initial publication of our interprocedural-slicing algorithm [9], a different technique for computing interprocedural slices was presented by Hwang, Du, and Chou <ref> [12] </ref>. The slicing algorithm presented in [12] computes an answer that is as precise as our algorithm, but differs significantly in how it handles the calling-context problem. The algorithm from [12] constructs a sequence of slices of the systemwhere each slice of the sequence essentially permits there to be one additional <p> After the initial publication of our interprocedural-slicing algorithm [9], a different technique for computing interprocedural slices was presented by Hwang, Du, and Chou <ref> [12] </ref>. The slicing algorithm presented in [12] computes an answer that is as precise as our algorithm, but differs significantly in how it handles the calling-context problem. The algorithm from [12] constructs a sequence of slices of the systemwhere each slice of the sequence essentially permits there to be one additional level of recursionuntil a fixed-point is <p> publication of our interprocedural-slicing algorithm [9], a different technique for computing interprocedural slices was presented by Hwang, Du, and Chou <ref> [12] </ref>. The slicing algorithm presented in [12] computes an answer that is as precise as our algorithm, but differs significantly in how it handles the calling-context problem. The algorithm from [12] constructs a sequence of slices of the systemwhere each slice of the sequence essentially permits there to be one additional level of recursionuntil a fixed-point is reached (i.e., until no further elements appear in a slice that uses one additional level of recursion). <p> Hwang, Du, and Chou do not include an analysis of their algorithm's complexity in <ref> [12] </ref>, which makes a direct comparison with our algorithm difficult; however, there are several reasons why our algorithm may be more efficient. First, the algorithm from [12] computes a sequence of slices, each of which may involve re-slicing a procedure multiple times; in contrast, through its use of marks on system-dependence-graph <p> Hwang, Du, and Chou do not include an analysis of their algorithm's complexity in <ref> [12] </ref>, which makes a direct comparison with our algorithm difficult; however, there are several reasons why our algorithm may be more efficient. First, the algorithm from [12] computes a sequence of slices, each of which may involve re-slicing a procedure multiple times; in contrast, through its use of marks on system-dependence-graph vertices, our algorithm processes no vertex more than once during the computation of a slice. <p> The system dependence graph (with its subordinate-characteristic-graph edges) need be computed only once; each slicing operation can use this graph, and the cost of each such slice is linear in the size of the system dependence graph. In contrast, the approach of <ref> [12] </ref> would involve finding a new fixed point (a problem that appears to have complexity comparable to the computation of the subordinate characteristic graphs) for each new slice. <p> It is no doubt possible to formulate interprocedural slicing as a data flow analysis problem on a super graph, and to solve the problem using an algorithm akin to those described by Myers to account correctly for the calling context of a called procedure. As in the comparison with <ref> [12] </ref>, our algorithm has a significant advantage when one wishes to compute multiple slices of the same system.
Reference: 13. <author> Kastens, U., </author> <title> Ordered attribute grammars, </title> <journal> Acta Inf. </journal> <pages> 13(3) pp. </pages> <month> 229-256 </month> <year> (1980). </year>
Reference-contexts: For such grammars, it is possible to give a polynomial-time algorithm for constructing the (covering) subordinate characteristic graphs. The computation is performed by an algorithm, called ConstructSubCGraphs, that is a slight modification of an algorithm originally developed by Kastens to construct approximations to a grammar's transitive dependence relations <ref> [13] </ref>. The covering subordinate characteristic graph of a nonterminal X of the linkage grammar is captured in the graph TDS (X) (standing for Transitive Dependences among a Symbol's attributes). Initially, all the TDS graphs are empty. <p> The TDS graphs are generated by the procedure ConstructSubCGraphs, given in Figure 7, which is a slight modification of the first two steps of Kastens's algorithm for constructing a set of evaluation plans for an attribute grammar <ref> [13] </ref>. ConstructSubCGraphs performs a kind of closure operation on the TDP and TDS graphs. <p> No edge is induced more than once because of the marks on TDS edges; thus, the total cost of procedure ConstructSubCGraphs is bounded by O (Gmult_byX 2 mult_byD 2 ) <ref> [13] </ref>. 5.2. Slicing Costs An interprocedural slice is performed by two traversals of the system dependence graph, starting from some initial set of vertices.
Reference: 14. <author> Knuth, D. E., </author> <title> Semantics of context-free languages, </title> <journal> Math. Syst Theory 2(2) pp. </journal> <month> 127-145 </month> <year> (1968). </year>
Reference-contexts: APPENDIX: ATTRIBUTE GRAMMARS AND ATTRIBUTE DEPENDENCES An attribute grammar is a context-free grammar extended by attaching attributes to the terminal and nonterminal symbols of the grammar, and by supplying attribute equations to define attribute values <ref> [14] </ref>. In every production p : X 0 fi X 1 , ..., X k , each X i denotes an occurrence of one of the grammar symbols; associated with each such symbol occurrence is a set of attribute occurrences corresponding to the symbol's attributes.
Reference: 15. <author> Kou, L.T., </author> <title> On live-dead analysis for global data flow problems, </title> <journal> Journal of the ACM 24(3) pp. </journal> <month> 473-483 (July </month> <year> 1977). </year>
Reference-contexts: Whereas the system dependence graph can be computed once and then used for each slicing operation, the approach postulated above would involve solving a new data flow analysis problem from scratch for each slice. The vertex-reachability approach we have used here has some similarities to a technique used in <ref> [15] </ref>, [5], and [6] to transform data flow analysis problems to vertex-reachability problems.
Reference: 16. <author> Kuck, D. J., Muraoka, Y., and Chen, S. C., </author> <title> On the number of operations simultaneously executable in FORTRAN-like programs and their resulting speed-up, </title> <journal> IEEE Transactions on Computers C-21, </journal> <pages> pp. </pages> <month> 1293-1310 (December </month> <year> 1972). </year>
Reference-contexts: ACM TOPLAS vol 12 no 1 (January 1990) - 5 - 2. PROGRAM DEPENDENCE GRAPHS AND PROGRAM SLICES Different definitions of program dependence representations have been given, depending on the intended application; they are all variations on a theme introduced in <ref> [16] </ref>, and share the common feature of having an explicit representation of data dependences (see below). The program dependence graphs defined in [7] introduced the additional feature of an explicit representation for control dependences (see below). The definition of program dependence graph given below differs from [7] in two ways.
Reference: 17. <author> Lyle, J. and Weiser, M., </author> <title> Experiments on slicing-based debugging tools, </title> <booktitle> in Proceedings of the First Conference on Empirical Studies of Programming, </booktitle> <address> (June 1986), </address> <publisher> Ablex Publishing Co. </publisher> <year> (1986). </year>
Reference-contexts: ACM TOPLAS vol 12 no 1 (January 1990) - 2 - programmer understand complicated code, can aid in debugging <ref> [17] </ref>, and can be used for automatic paral-lelization [213].
Reference: 18. <author> Myers, E., </author> <title> A precise inter-procedural data flow algorithm, pp. </title> <booktitle> 219-230 in Conference Record of the Eighth ACM Symposium on Principles of Programming Languages, </booktitle> <address> (Williamsburg, VA, </address> <month> January 26-28, </month> <year> 1981), (1981). </year>
Reference-contexts: In contrast, the approach of [12] would involve finding a new fixed point (a problem that appears to have complexity comparable to the computation of the subordinate characteristic graphs) for each new slice. In <ref> [18] </ref>, Myers presents algorithms for a specific set of interprocedural data flow problems, all of which require keeping track of calling context; however, Myers's approach to handling this problem differs from ours.
Reference: 19. <author> Ottenstein, K.J. and Ottenstein, L.M., </author> <title> The program dependence graph in a software development environment, </title> <booktitle> Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <address> (Pittsburgh, PA, </address> <month> April 23-25, </month> <year> 1984), </year> <journal> ACM SIGPLAN Notices 19(5) pp. </journal> <month> 177-184 (May </month> <year> 1984). </year>
Reference-contexts: When slicing a program that consists of a single monolithic procedure (which we will call intraprocedural slicing), a slice can be determined from the closure of the directly-affects relation. Ottenstein and Ottenstein pointed out how well-suited program dependence graphs are for this kind of slicing <ref> [19] </ref>; once a program is represented by its program dependence graph, the slicing problem is simply a vertex-reachability problem, and thus slices may be computed in linear time. <p> Our algorithm for interprocedural slicing produces a more precise answer than that produced by the algorithm given by Weiser in [22]. Our work follows the example of Ottenstein and Ottenstein by defining the slicing algorithm in terms of operations on a dependence graph representation of programs <ref> [19] </ref>; however, in [19] Ottenstein and Ottenstein only discuss the case of programs that consist of a single monolithic procedure and do not discuss the more general case where slices cross procedure boundaries. <p> Our algorithm for interprocedural slicing produces a more precise answer than that produced by the algorithm given by Weiser in [22]. Our work follows the example of Ottenstein and Ottenstein by defining the slicing algorithm in terms of operations on a dependence graph representation of programs <ref> [19] </ref>; however, in [19] Ottenstein and Ottenstein only discuss the case of programs that consist of a single monolithic procedure and do not discuss the more general case where slices cross procedure boundaries. <p> The cost of each traversal is linear in the size of the system dependence graph, which is bounded by O (Pmult_by (V + E) + TotalSitesmult_byX). 6. RELATED WORK In recasting the interprocedural slicing problem as a reachability problem in a graph, we are following the example of <ref> [19] </ref>, which does the same for intraprocedural slicing. The reachability approach is conceptually simpler than the data-flow equation approach used in [22] and is also much more efficient when more than one slice is desired.
Reference: 20. <author> Reps, T. and Yang, W., </author> <title> The semantics of program slicing, </title> <type> Technical Report 777, </type> <institution> Department of Computer Sciences, </institution> <note> University of WisconsinMadison (June 1988). </note>
Reference-contexts: For intraprocedural slicing, a solution to Version (1) provides a solution to Version (2), since the reduced program required in Version (2) can be obtained by restricting the original program to just the statements and predicates found in the solution for Version (1) <ref> [20] </ref>. For interprocedural slicing, restricting the original program to just the statements and predicates found for Version (1) may yield a program that is syntactically incorrect (and thus certainly not a solution to Version (2)). <p> The relationship between a program's dependence graph and a slice of the graph has been addressed in <ref> [20] </ref>. We say that G is a feasible program dependence graph iff G is the program dependence graph of some program P. <p> For any S V (G), if G is a feasible program dependence graph, the slice G / S is also a feasible program dependence graph; it corresponds to the program P obtained by restricting the syntax tree of P to just the statements and predicates in V (G / S) <ref> [20] </ref>. Example. Figure 3 shows the graph that results from taking a slice of the program dependence graph from Figure 1 with respect to the final-use vertex for i, together with the one program to which it corresponds. <p> The significance of an intraprocedural slice is that it captures a portion of a program's behavior in the sense that, for any initial state on which the program halts, the program and the slice compute the same sequence of values for each element of the slice <ref> [20] </ref>. In our case, a program point may be (1) an assignment statement, (2) a control predicate, or (3) a final use of a variable in an end statement.
Reference: 21. <author> Weiser, M., </author> <title> Reconstructing sequential behavior from parallel behavior projections, </title> <note> Information Processing Letters 17 pp. </note> <month> 129-135 </month> <year> (1983). </year>
Reference: 22. <author> Weiser, M., </author> <title> Program slicing, </title> <journal> IEEE Transactions on Software Engineering SE-10(4) pp. </journal> <month> 352-357 (July </month> <year> 1984). </year> <note> ACM TOPLAS vol 12 no 1 (January 1990) ACM TOPLAS vol 12 no 1 (January 1990) </note>
Reference-contexts: 1. INTRODUCTION The slice of a program with respect to program point p and variable x consists of all statements and predi cates of the program that might affect the value of x at point p. This concept, originally discussed by Mark Weiser in <ref> [22] </ref>, can be used to isolate individual computation threads within a program. <p> This paper concerns the problem of interprocedural slicinggenerating a slice of an entire program, where the slice crosses the boundaries of procedure calls. Our algorithm for interprocedural slicing produces a more precise answer than that produced by the algorithm given by Weiser in <ref> [22] </ref>. <p> A solution to Version (2) requires either that the slice be extended or that it be transformed by duplicating code to specialize procedure bodies for particular parameter-usage patterns. Weiser's method for interprocedural slicing is described in <ref> [22] </ref> as follows: For each criterion C for a procedure P, there is a set of criteria UP 0 (C) which are those needed to slice callers of P, and a set of criteria DOWN 0 (C) which are those needed to slice procedures called by P. . . . <p> We then discuss modifications to the definition of the system dependence graph to permit more precise slicing and to extend the slicing algorithm's range of applicability. 4.1. An Algorithm for Interprocedural Slicing As discussed in the Introduction, the algorithm presented in <ref> [22] </ref>, while safe, is not as precise as possible. The difficult aspect of interprocedural slicing is keeping track of the calling context when a slice descends into a called procedure. <p> RELATED WORK In recasting the interprocedural slicing problem as a reachability problem in a graph, we are following the example of [19], which does the same for intraprocedural slicing. The reachability approach is conceptually simpler than the data-flow equation approach used in <ref> [22] </ref> and is also much more efficient when more than one slice is desired.
References-found: 22

