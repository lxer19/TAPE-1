URL: http://www.cs.ubc.ca/spider/cebly/Papers/_download_/merging.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: tldg@cs.brown.edu  cebly@cs.ubc.ca  
Title: Solving Very Large Weakly Coupled Markov Decision Processes  
Author: Nicolas Meuleau, Milos Hauskrecht, Kee-Eung Kim, Leonid Peshkin Leslie Pack Kaelbling, Thomas Dean fnm, milos, kek. ldp, lpk, Craig Boutilier 
Date: 1910  
Address: Box  Providence, RI 02912-1210  Vancouver, BC V6T 1Z4, Canada  
Affiliation: Computer Science Department,  Brown University,  Department of Computer Science University of British Columbia  
Abstract: We present a technique for computing approximately optimal solutions to stochastic resource allocation problems modeled as Markov decision processes (MDPs). We exploit two key properties to avoid explicitly enumerating the very large state and action spaces associated with these problems. First, the problems are composed of multiple tasks whose utilities are independent. Second, the actions taken with respect to (or resources allocated to) a task do not influence the status of any other task. We can therefore view each task as an MDP. However, these MDPs are weakly coupled by resource constraints: actions selected for one MDP restrict the actions available to others. We describe heuristic techniques for dealing with several classes of constraints that use the solutions for individual MDPs to construct an approximate global solution. We demonstrate this technique on problems involving thousands of tasks, approximating the solution to problems that are far beyond the reach of standard methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72:81138, </volume> <year> 1995. </year>
Reference-contexts: fi Time ! IR, specifying the reward conditional on the starting state, resulting state and action at each time; 3 * T is a vector of state transition distributions, T 1 ; : : : ; T n , where T i : S i fiA i fiS i ! <ref> [0; 1] </ref>, specifying the probability of a task entering a state given the previous state of the task and the action; * c is the cost for using a single unit of the resource; * M l is the instantaneous (local) resource constraint on the amount of resource that may be <p> The graph on the right describes a 100-task problem with local constraints. Values are averaged over 100 runs. ture contingencies. While on-line methods are popular <ref> [1] </ref>, crucial to the success of the on-line component of MTD is the ability to quickly construct good actions heuristically using the component value functions. MTD is a family of algorithms that exploit specific structure in the problem domain to make decisions effectively.
Reference: [2] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1957. </year>
Reference: [3] <author> D. A. Berry and B. Fristedt. </author> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1985. </year>
Reference: [4] <author> D. P. Bertsekas and J.. N. Tsitsiklis. </author> <title> Neuro-dynamic Programming. </title> <publisher> Athena, </publisher> <address> Belmont, MA, </address> <year> 1996. </year>
Reference: [5] <author> C. Boutilier, R. I. Brafman, and C. </author> <title> Geib. Prioritized goal decomposition of markov decision processes: Toward a synthesis of classical and decision theoretic planning. </title> <booktitle> Proc. IJCAI-97, </booktitle> <address> pp.11561163, Nagoya, </address> <year> 1997. </year>
Reference-contexts: It requires that the problem be specified in a specific form, taking advantage of utility independence and probabilistic independence in action effects. Much recent research has focussed on using representations for MDPs that make some of this structure explicit and automatically discovering appropriate problem abstractions and decompositions <ref> [9, 6, 14, 11, 5] </ref>. The extent to which effective Markov task decompositions can be automatically extracted from suitable problem representations remains an interesting open question.
Reference: [6] <author> C. Boutilier, R. Dearden, and M. Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> Proc. IJCAI-95, </booktitle> <address> pp.11041111, Montreal, </address> <year> 1995. </year>
Reference-contexts: It requires that the problem be specified in a specific form, taking advantage of utility independence and probabilistic independence in action effects. Much recent research has focussed on using representations for MDPs that make some of this structure explicit and automatically discovering appropriate problem abstractions and decompositions <ref> [9, 6, 14, 11, 5] </ref>. The extent to which effective Markov task decompositions can be automatically extracted from suitable problem representations remains an interesting open question.
Reference: [7] <author> T. Dean and R. Givan. </author> <title> Model minimization in markov decision processes. </title> <booktitle> Proc. </booktitle> <address> AAAI-97, pp.106111, Providence, </address> <year> 1997. </year>
Reference: [8] <author> T. Dean, L. P. Kaelbling, J. Kirman, and A. Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, </journal> <volume> 76:3574, </volume> <year> 1995. </year>
Reference: [9] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <booktitle> Computational Intelligence, </booktitle> <address> 5(3):142150, </address> <year> 1989. </year>
Reference-contexts: It requires that the problem be specified in a specific form, taking advantage of utility independence and probabilistic independence in action effects. Much recent research has focussed on using representations for MDPs that make some of this structure explicit and automatically discovering appropriate problem abstractions and decompositions <ref> [9, 6, 14, 11, 5] </ref>. The extent to which effective Markov task decompositions can be automatically extracted from suitable problem representations remains an interesting open question.
Reference: [10] <author> T. Dean and S. Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> Proc. IJCAI-95, </booktitle> <address> pp.1121 1127, Montreal, </address> <year> 1995. </year>
Reference: [11] <author> R. Dearden and C. Boutilier. </author> <title> Abstraction and approximate decision theoretic planning. </title> <journal> Artificial Intelligence, </journal> <volume> 89:219283, </volume> <year> 1997. </year>
Reference-contexts: It requires that the problem be specified in a specific form, taking advantage of utility independence and probabilistic independence in action effects. Much recent research has focussed on using representations for MDPs that make some of this structure explicit and automatically discovering appropriate problem abstractions and decompositions <ref> [9, 6, 14, 11, 5] </ref>. The extent to which effective Markov task decompositions can be automatically extracted from suitable problem representations remains an interesting open question.
Reference: [12] <author> R. A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1960. </year>
Reference: [13] <author> R. L. Keeney and H. Raiffa. </author> <title> Decisions with Multiple Objectives: Preferences and Value Trade-offs. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1978. </year>
Reference: [14] <author> A. E. Nicholson and L. P. Kaelbling. </author> <title> Toward approximate planning in very large stochastic domains. </title> <booktitle> AAAI Spring Symp. on Decision Theoretic Planning, </booktitle> <address> pp.190 196, Stanford, </address> <year> 1994. </year>
Reference-contexts: It requires that the problem be specified in a specific form, taking advantage of utility independence and probabilistic independence in action effects. Much recent research has focussed on using representations for MDPs that make some of this structure explicit and automatically discovering appropriate problem abstractions and decompositions <ref> [9, 6, 14, 11, 5] </ref>. The extent to which effective Markov task decompositions can be automatically extracted from suitable problem representations remains an interesting open question.
Reference: [15] <author> D. Precup and R. S. Sutton. </author> <title> Multi-time models for temporally abstract planning. </title> <editor> In M. Mozer, M. Jordan and T. Petsche editor, NIPS-11. </editor> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1998. </year>
Reference: [16] <author> M. L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference: [17] <author> S. P. Singh and D. Cohn. </author> <title> How to dynamically merge markov decision processes. </title> <editor> In M. Mozer, M. Jordan and T. Petsche editor, NIPS-11. </editor> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1998. </year>
References-found: 17

