URL: http://cbr-www.cs.umass.edu/~daniels/Papers/cb-24.ps
Refering-URL: http://cbr-www.cs.umass.edu/~daniels/
Root-URL: 
Email: Email: daniels@cs.umass.edu  
Phone: Phone: (413) 545-1985  
Title: Retrieval of Passages for Information Reduction problem of automatically locating text about these features, where
Author: Jody J. Daniels 
Note: This research addresses the  
Date: July 19, 1996  
Address: Amherst, MA 01003 USA  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Information Retrieval (IR) typically retrieves entire documents in response to a user's information need. However, many times a user would prefer to examine smaller portions of a document. One example of this is when building a frame-based representation of a text. The user would like to read all and only those portions of the text that are about predefined important features. We propose to use a small set of "annotations", textual segments, that we saved when creating our original case-base to generate queries and retrieve relevant passages. Annotations are associated with the slot about which they provide information. Using a case-base of annotations for each slot we generate and pose a query to an IR system that is aimed at the retrieval of passages within a relevant document. By locating passages for display to the user, we winnow a text down to sets of several sentences, greatly reducing the time and effort expended searching through each text for important features.
Abstract-found: 1
Intro-found: 1
Reference: [All96] <author> James Allan. </author> <title> Automatic Hypertext Link Typing. </title> <booktitle> In Hypertext 96, </booktitle> <address> Washington, D.C., </address> <month> March </month> <year> 1996. </year> <note> ACM. To be published. </note>
Reference-contexts: This would take advantage of the work done by Salton and Allan on visualization of document structure <ref> [SA93, All96] </ref>.
Reference: [Ash90] <author> Kevin D. Ashley. </author> <title> Modeling Legal Argument: Reasoning with Cases and Hypotheticals. </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: A case's actual indices may be automatically generated (e.g., CYRUS dynamically decides what features to use as indices and may reorganize memory if better features are later discerned. Ashley's HYPO <ref> [RA87, Ash90] </ref> automatically generates an intermediate case representation, an interpretation frame, and the indices, called "dimensions", for each case.) Because of the knowledge representation expense, there are few CBR systems with large case bases, that is, a thousand or more cases. <p> The system begins by performing an analysis of her client's problem case, with respect to this in-house case-base. In this instance, the CBR module uses a HYPO-style reasoner that uses a claim lattice to determine similarity <ref> [RA87, Ash90] </ref>. (See Section 3.1 for more details on CBR systems.) Next, the CBR module selects a small set of special texts on which to employ relevance feedback to generate a query. We call this set of texts the "RF-CKB". <p> Problem-solving CBR systems frequently use some form of nearest-neighbor metric, while precedent-based systems frequently use a "claim lattice". This means of measuring similarity is based on Ashley's HYPO system <ref> [RA87, Ash90] </ref>. A claim lattice is a partial ordering of the cases similar to the cfs. <p> A claim lattice is a partial ordering of the cases similar to the cfs. The ckb is sorted based on the intersection of each case's "dimensions" (or features) with those applicable in the cfs; cases with no shared dimensions are not considered since they are not deemed relevant <ref> [Ash90, RA87] </ref>. Dimensions address important aspects of cases and are used both to index and compare cases. In this sorting, Case A is considered more on-point than Case B if the set of applicable dimensions it shares with the cfs properly contains those shared by B and the cfs. <p> One alternative would be to see if we could determine with a high degree of belief whether or not a text contains any information about a slot or possibly even a dimension (using a HYPO-style definition <ref> [RA87, Ash90] </ref>). Being able to more generally claim that we believe a document does or does not contain information about a slot would allow the case-based reasoner to reason, at a gross level, about the new texts.
Reference: [ASP + 95] <author> D. B. Aronow, S. Soderland, J.M. Ponte, Feng F., W.B. Croft, and W. G. Lehnert. </author> <title> Automated Classificatin of Encounter Notes in a Computer Based Medical Record. </title> <booktitle> In Proceedings of the Eight World Congress on Medical Infomatics, </booktitle> <pages> pages 8-12, </pages> <address> Van-couver, Canada, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Our documents are much longer, making it more difficult for a system to identify the important terms for a concept. One set of experiments on classifying medical record encounter notes used approximately 1000 training documents, where documents rarely exceeded one page in length <ref> [ASP + 95] </ref>. Another medical classification task, that of assigning codes to discharge summaries, employed more than 10,000 training documents averaging 4.43 codes per document [LC95]. Discharge summaries average one fifth the length of our documents.
Reference: [Bai86] <author> W. M. Bain. </author> <title> Case-Based Reasoning: A Computer Model of Subjective Assessment. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1986. </year>
Reference-contexts: Cases are hand-coded into structures that support this process. Sadly, most CBR systems require the manual input of cases; they are not automatically generated from text or other sources (e.g., episodes in Kolodner's CYRUS [Kol84] and Bain's JUDGE <ref> [Bai86] </ref> used a conceptual dependency representation for their textual input.) A human makes the decisions about how to structure a case and what indexing structures to use. A case's input form is extracted from text or created from events, and information is put into either an intermediate or final form.
Reference: [BCB94] <author> Brian T. Bartell, Garrison W. Cotrell, and Richard K. Belew. </author> <title> Automatic Combination of Multiple Ranked Retrieval Systems. </title> <booktitle> In Proceedings of the 17th Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 173-181, </pages> <address> Dublin, Ireland, </address> <month> July </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: We now encounter the dilemma of how to merge the results so that we can identify a ranking of passages. Research into query combination has shown that using multiple representations for the same information need and merging the results generally improves retrieval performance <ref> [SK88, BCCC93, BCB94, BKFS95] </ref>. The issues here are how to generate more than one representation and the costs associated with doing so and how to effectively combine the results at query time.
Reference: [BCCC93] <author> N. J. Belkin, C. Cool, W. B. Croft, and J. P. Callan. </author> <title> The Effect of Multiple Query Representations on Information Retrieval System Performance. </title> <booktitle> In Proceedings of the 16th Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 339-346, </pages> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year> <journal> ACM. </journal> <volume> 42 </volume>
Reference-contexts: We now encounter the dilemma of how to merge the results so that we can identify a ranking of passages. Research into query combination has shown that using multiple representations for the same information need and merging the results generally improves retrieval performance <ref> [SK88, BCCC93, BCB94, BKFS95] </ref>. The issues here are how to generate more than one representation and the costs associated with doing so and how to effectively combine the results at query time.
Reference: [BKFS95] <author> N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw. </author> <title> Combining the Evidence of Multiple Query Representations for Information Retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 31(3) </volume> <pages> 431-448, </pages> <year> 1995. </year>
Reference-contexts: The former is the "data fusion" problem while the latter is the "query combination" problem. There is currently a great deal of on-going research into how to best combine the results of a query performed over multiple databases <ref> [VGJL95, CLC95, BKFS95] </ref>. (See [Har94, Har95] for additional references.) Here they must deal with the problems of normalizing statistics across the various collections, how to decide which collections might provide the most fruitful results, deciding how many documents to retrieve from each source, and how to actually combine the results from <p> We now encounter the dilemma of how to merge the results so that we can identify a ranking of passages. Research into query combination has shown that using multiple representations for the same information need and merging the results generally improves retrieval performance <ref> [SK88, BCCC93, BCB94, BKFS95] </ref>. The issues here are how to generate more than one representation and the costs associated with doing so and how to effectively combine the results at query time.
Reference: [BL88] <author> Steven Bradtke and Wendy G. Lehnert. </author> <title> Some Experiments with Case-Based Search. </title> <booktitle> In Proceedings, Seventh National Conference on Artificial Intelligence, </booktitle> <volume> volume 1, </volume> <pages> pages 133-138, </pages> <address> St. Paul, MN, </address> <month> July </month> <year> 1988. </year> <note> AAAI. </note>
Reference-contexts: Veloso was able to derive cases directly from the problem sets in her planning domain [Vel92], and Lehnert used 8-puzzle problems <ref> [Leh87b, BL88] </ref>.) CBR case structures typically represent more than one level of abstraction. Indices can be formed from these various levels so that a case can be retrieved and reasoned about from multiple perspectives and levels of abstraction.
Reference: [Buc85] <author> Chris Buckley. </author> <title> Implementation of the SMART Information Retrieval System. </title> <type> Technical report, </type> <institution> Computer Science Department, Cornell University, </institution> <address> Ithica, NY, </address> <month> May </month> <year> 1985. </year> <pages> 85-686. </pages>
Reference-contexts: As described in Section 1.3, when indexing each document, words may be stemmed and stop words may be removed. The most common models for representing documents include the vector space model [Sal89], as implemented in SMART <ref> [Buc85] </ref>, the probabilistic model, and inference networks as implemented in INQUERY [CCH92]. This research will be done using the inference net model as implemented in INQUERY, but there is no apparent reason why it could not be done within another framework.
Reference: [Cal94] <author> James P. Callan. </author> <title> Passage-Level Evidence in Document Retrieval. </title> <booktitle> In Proceedings of the 17th Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 302-310, </pages> <address> Dublin, Ireland, </address> <month> July </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: Callan provides a comprehensive study of the relative merits of different types of passages when dealing with different types of collections and documents <ref> [Cal94] </ref>. This work uses the inference net model and in all cases, the belief value for the best passage within a document was added to the belief for the document as a whole. Comparison was done among discourse and window passages. <p> large a segment should each be? Should the retrieval size be the same for every slot, or should it depend on the size of the user-provided annotations? Since the best results for retrieving documents when incorporating information about passages comes when using fixed-size passages, we will try this method first <ref> [Cal94] </ref>. While our task is not the same, we would like to retrieve the best passage instead of the best document, so we will utilize their evaluation technique to locate potentially good passages within each novel document. We run the risk of using an incorrect window size for retrieval.
Reference: [CC93] <author> James P. Callan and W. Bruce Croft. </author> <title> An Approach to Incorporating CBR Concepts in IR Systems. </title> <booktitle> In Working Notes of the AAAI Spring Symposium Series: Case-Based Reasoning and Information Retrieval Exploring the Opportunities for Technology Sharing, </booktitle> <pages> pages 28-34, </pages> <address> Stanford, CA, March 1993. </address> <publisher> AAAI. </publisher>
Reference-contexts: Concept recognizers enable NLP systems in their extraction task by locating important patterns or terms from which information can be drawn. They enable IR systems to index and utilize a second and more abstract representation of each text <ref> [CC93, RJ91] </ref>. Table lookup can be used when the items in the concept can be enumerated, such as foreign countries, U.S. cities, or pieces of furniture. Pattern recognition, such as a series of capitalized words may identify company and other proper names.
Reference: [CCH92] <author> James P. Callan, W. Bruce Croft, and Stephen M. Harding. </author> <title> The INQUERY Retrieval System. </title> <editor> In A. M. Tjoa and I. Ramos, editors, </editor> <booktitle> Database and Expert Systems Applications: Proceedings of the International Conference in Valencia, Spain, </booktitle> <pages> pages 78-83, </pages> <address> Valencia, Spain, 1992. </address> <publisher> Springer Verlag, </publisher> <address> NY. </address>
Reference-contexts: As described in Section 1.3, when indexing each document, words may be stemmed and stop words may be removed. The most common models for representing documents include the vector space model [Sal89], as implemented in SMART [Buc85], the probabilistic model, and inference networks as implemented in INQUERY <ref> [CCH92] </ref>. This research will be done using the inference net model as implemented in INQUERY, but there is no apparent reason why it could not be done within another framework. INQUERY uses an inference network model [TC91], [TC92] specifically, a Bayesian probabilistic inference net, to represent texts and queries. <p> In either case, we must create a method for converting the raw annotations into our query language. The next decision is what operators should we use? Natural language queries with INQUERY currently use a #sum operator wrapped around the terms <ref> [CCH92] </ref>. This seems a bit too simplistic, but it might work perfectly well for this problem. It may be reasonable to collect proximity information about the terms and add this data to the queries. <p> This is what is currently done with natural language queries by INQUERY <ref> [CCH92] </ref>. If there are multiple queries, then we simply run each of them. Removal of redundant items is slightly more difficult.
Reference: [CKP93] <author> Douglass R. Cutting, David R. Karger, and Jan O. Pedersen. </author> <title> Constant Interaction-Time Scatter/Gather Browsing of Very Large Document Collections. </title> <booktitle> In Proceedings of the 16th Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 126-134, </pages> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: Features may be the words, phrases, or concepts that compose the text or other values such as author, document length, date of publication, etc. Some clustering techniques are directed at the results of a query <ref> [CPKT92, CKP93] </ref>, while others use clustering to locate highly correlated text sections within documents [SB91b]. Unfortunately, the passages that this research aims to locate may not be highly correlated to other passages within the document or to the set of annotations as a group.
Reference: [CLC95] <author> James P. Callan, Zhihong Lu, and W. Bruce Croft. </author> <title> Searching Distributed Collections with Inference Networks. </title> <booktitle> In Proceedings of the 18th Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 21-28, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: The former is the "data fusion" problem while the latter is the "query combination" problem. There is currently a great deal of on-going research into how to best combine the results of a query performed over multiple databases <ref> [VGJL95, CLC95, BKFS95] </ref>. (See [Har94, Har95] for additional references.) Here they must deal with the problems of normalizing statistics across the various collections, how to decide which collections might provide the most fruitful results, deciding how many documents to retrieve from each source, and how to actually combine the results from
Reference: [Coo68] <author> William S. Cooper. </author> <title> Expected Search Length: A Single Measure of Retrieval Effectiveness Based on the Weak Ordering Action of Retrieval Systems. </title> <journal> American Documentation, </journal> <volume> 19 </volume> <pages> 30-41, </pages> <year> 1968. </year>
Reference-contexts: We should also look at 6., where we see if there are queries in which we have not satisfied the requirement to find any of the acceptable passages. One measure that incorporates these features is Expected Search Length (ESL) <ref> [Coo68] </ref>. It measures the amount of effort wasted when trying to find a particular number of relevant items while searching through a ranked list. From ESL we can compute the average ESL to compare values across queries when the collection and retrieval engine remain stable.
Reference: [CPKT92] <author> Douglass R. Cutting, Jan O. Pedersen, David R. Karger, and J. W. Tukey. Scatter/Gather: </author> <title> A Cluster-Based Approach to Browsing Large Document Collections. </title> <booktitle> In Proceedings of the 15th Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 318-329, </pages> <address> Copenhagen, Denmark, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: Features may be the words, phrases, or concepts that compose the text or other values such as author, document length, date of publication, etc. Some clustering techniques are directed at the results of a query <ref> [CPKT92, CKP93] </ref>, while others use clustering to locate highly correlated text sections within documents [SB91b]. Unfortunately, the passages that this research aims to locate may not be highly correlated to other passages within the document or to the set of annotations as a group.
Reference: [Cro83] <author> W. B. Croft. </author> <title> Experiments with Representation in a Document Retrieval System. </title> <journal> Information Technology: Research and Development., </journal> <volume> 2(1) </volume> <pages> 1-21, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: The user provides the proportional value to give to each of recall and precision [Rij79]. 5. Total number of relevant documents retrieved by a given cutoff. 6. Total number of queries with no relevant documents among the top n <ref> [Cro83] </ref>. 7. Mean square error 8. Statistical methods for comparing ranks - Kendall, Spearman but what does this tell you overall? 9. Probability of getting a relevant document as rank 1, probability of getting a relevant docu ment within the top 2 returned, etc. 10.
Reference: [DR95] <author> Jody J. Daniels and Edwina L. Rissland. </author> <title> A Case-Based Approach to Intelligent Information Retrieval. </title> <booktitle> In Proceedings of the 18th Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 238-245, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1995. </year> <journal> ACM. </journal> <volume> 43 </volume>
Reference-contexts: Using a CBR system in conjunction with an IR engine, we use these cases and their texts to retrieve an additional set of texts believed to be relevant to a current problem situation (one possible technique is described in <ref> [DR95] </ref> and [RD95].) The next 9 step is to see how closely the situations in the retrieved documents match our current problem. To be able to do this automatically, we must convert our newly retrieved texts into a representation with which the case-based reasoner can work. <p> We have already been able to exploit the reasoning ability of a CBR system working in concert with the indexing and retrieval abilities of an IR system <ref> [DR95, RD95] </ref>. The CBR system identifies those cases most similar to a current fact situation. The documents associated with these most similar cases can then be presented to the IR engine. <p> This subset is that which is believed to be relevant to the overall problem situation because it was retrieved in response to a query generated by the overall fact situation. We have already achieved good results in retrieving documents relevant to the current fact situation <ref> [DR95, RD95] </ref>. 27 It is possible that we missed domain-specific, relevant documents when retrieving based on our original query; however, that is not the concern of this research. It is also possible that there exist documents that were not retrieved in response to the original query that contain this feature. <p> to identify a simple, efficient method for constructing effective passage queries. * Case-Base/Annotations - There currently exists a case-base of annotations for the bankruptcy (Chapter 13, good faith) domain. (The texts for this work were previously used for the first portion of the CBR-IR processing cycle and are reported in <ref> [DR95, RD95] </ref>.) There are 13 cases containing approximately 60 slots. We derived annotations for approximately 55 slots.
Reference: [Dyk69] <author> Freeman H. Dyke, Jr. </author> <title> A Manual on Methods for Retrieving and Correlating Technical Data. American Society for Testing and Materials, </title> <year> 1969. </year>
Reference-contexts: Over 25 years ago Dyke summarized the IR process and its goals: The ideal data retrieval system will take a user's question, interpret it into the language of the system, search all of the available information and data, and supply the optimum answers in minimum time at a reasonable cost. <ref> [Dyk69] </ref> page 6. These are still the same goals as today: automatic indexing of documents (or other data items), with little or no manual input, and then retrieving those documents that match an information need stated in the form of a query as effectively and efficiently possible.
Reference: [Gol91] <author> Andrew R. Golding. </author> <title> Pronouncing Names by a Combination of Rule-Based and Case-Based Reasoning. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: A rather interesting byproduct of this manual input characteristic is that those systems that do contain a large number of cases, tend to have very small or simplistic cases or case structures (e.g., MBRtalk used letters for the word pronunciation problem [SW86] while Anapron used word segments <ref> [Gol91] </ref>, and PRO used word segments and associated phonemes [Leh87a]. Veloso was able to derive cases directly from the problem sets in her planning domain [Vel92], and Lehnert used 8-puzzle problems [Leh87b, BL88].) CBR case structures typically represent more than one level of abstraction. <p> Precedent-based or interpretive CBR systems will explain or justify the optimum answers based on comparisons with prior outcomes. As previously mentioned, most CBR systems require the manual input of cases. (Although there are systems with automatically generated case-bases: <ref> [Vel92, Gol91] </ref>.) The case and indexing structures are decided upon by a human. Each case's input form is extracted from text or created from events, and data is placed into the appropriate representation. The actual indices for a case may be automatically generated based on the input data.
Reference: [Goo89] <author> Marc Goodman. </author> <title> CBR in Battle Planning. </title> <booktitle> In Proceedings, Case-Based Reasoning Workshop, </booktitle> <pages> pages 264-269, </pages> <address> Pensacola Beach, FL, </address> <month> May </month> <year> 1989. </year> <pages> DARPA. </pages>
Reference-contexts: arrive at the "best case" in MBRtalk [SW86].) Interpretive CBR systems do not necessarily assign numerical values to relevant cases, but find other means for generating at least a partial ordering among the retrieved cases. (HYPO and Rissland and Skalak's CABARET [RS91] generate a claim lattice and Goodman's Battle Planner <ref> [Goo89] </ref> uses "case prototypes" or conjuncts of indices to select the "most on-point" cases for analysis.) Once a solution or evaluation is generated, the CBR system may wish to validate the solution.
Reference: [Goo91] <author> Marc Goodman. </author> <title> Prism: A Case-Based Telex Classifier. </title> <editor> In Alain Rappaport and Reid Smith, editors, </editor> <booktitle> Innovative Applications of Artificial Intelligence - 2., </booktitle> <pages> pages 25-37. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1991. </year>
Reference-contexts: There is another system the merges IR and CBR techniques, but it is used for the classification task. Prism is a system for classifying bank telexes for further distribution and routing <ref> [Goo91] </ref>. It uses a lexical pattern matcher to generate retrieval indices. These indices are used to select cases from a case base of over 9600 sample telexes.
Reference: [Hah90] <author> Udo Hahn. </author> <title> Topic Parsing: Accounting for Text Macro Structures in Full-Text Analysis. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 26(1) </volume> <pages> 135-170, </pages> <year> 1990. </year>
Reference-contexts: We have much smaller individual elements available as initial queries; our annotations are generally a sentence or shorter in length. Hahn argued for analyzing and indexing the conceptual or thematic structure of a text and implemented his ideas in the TOPIC system <ref> [Hah90] </ref>. TOPIC was tested over short texts describing computer systems. Hahn asserts that storing a text's conceptual structure makes possible three types of retrieval operations: abstracting or summarization, fact retrieval, and passage retrieval. Summarization is discussed below in Section 4.5.
Reference: [Ham87] <author> Kristian J. Hammond. </author> <title> Explaining and Repairing Plans that Fail. </title> <booktitle> In Proceedings, Tenth International Joint Conference on Artificial Intelligence, </booktitle> <volume> volume 1, </volume> <pages> pages 109-114, </pages> <address> Milan Italy, </address> <month> August </month> <year> 1987. </year> <pages> IJCAI. </pages>
Reference-contexts: designing, it is desirable to work with a single case or at least a small number 16 of cases for adaptation (or possibly a case for each problem component.) Therefore, problem-solving CBR systems generally have some means of weighting the relevant cases. (Kolodner's PARADYME [Kol89] uses preference heuristics, Hammond's CHEF <ref> [Ham87] </ref> uses a discrimination net that hierarchically orders features in terms of their relative importance, and Stanfill and Waltz applied a variety of distance metrics to arrive at the "best case" in MBRtalk [SW86].) Interpretive CBR systems do not necessarily assign numerical values to relevant cases, but find other means for
Reference: [Har94] <author> Donna K. Harman, </author> <title> editor. </title> <booktitle> The Second Text REtrieval Conference (TREC-2). </booktitle> <institution> National Institute of Standards and Technology, Gaithersburg, MD, </institution> <year> 1994. </year> <note> Special Publication 500-215. </note>
Reference-contexts: The former is the "data fusion" problem while the latter is the "query combination" problem. There is currently a great deal of on-going research into how to best combine the results of a query performed over multiple databases [VGJL95, CLC95, BKFS95]. (See <ref> [Har94, Har95] </ref> for additional references.) Here they must deal with the problems of normalizing statistics across the various collections, how to decide which collections might provide the most fruitful results, deciding how many documents to retrieve from each source, and how to actually combine the results from each source.
Reference: [Har95] <author> Donna K. Harman, </author> <title> editor. </title> <booktitle> The Third Text REtrieval Conference (TREC-3). </booktitle> <institution> National Institute of Standards and Technology, Gaithersburg, MD, </institution> <year> 1995. </year> <note> Special Publication 500-225. </note>
Reference-contexts: The former is the "data fusion" problem while the latter is the "query combination" problem. There is currently a great deal of on-going research into how to best combine the results of a query performed over multiple databases [VGJL95, CLC95, BKFS95]. (See <ref> [Har94, Har95] </ref> for additional references.) Here they must deal with the problems of normalizing statistics across the various collections, how to decide which collections might provide the most fruitful results, deciding how many documents to retrieve from each source, and how to actually combine the results from each source.
Reference: [Hea93] <author> Marti A. Hearst. </author> <title> Cases as Structured Indexes for Full-Length Documents. </title> <booktitle> In Working Notes of the AAAI Spring Symposium Series: Case-Based Reasoning and Information Retrieval Exploring the Opportunities for Technology Sharing, </booktitle> <pages> pages 140-145, </pages> <address> Stanford, CA, March 1993. </address> <publisher> AAAI. </publisher>
Reference-contexts: Passages are not retrieved in response to general queries, only in response to a concept. Another technique based on a document's thematic structure is TextTiling <ref> [Hea93, HP93] </ref>. With TextTiling, coherent subtopic discussions determine document indices. Subtopic discussion breaks are ascertained by evaluating the frequencies of the terms found in proximity to one another.
Reference: [Hea95] <author> Marti A. Hearst. TileBars: </author> <title> Visualization of Term Distribution Information in Full Text Information Access. </title> <booktitle> In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, </booktitle> <address> Denver, CO, </address> <month> May </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: This would take advantage of the work done by Salton and Allan on visualization of document structure [SA93, All96]. Other means of visualization include: cone trees [RMC91], a perspective wall [MRC91], a document lens [RM93], TileBars <ref> [Hea95] </ref>, and NavigationCones [HKW94]. (See [RPH + 95] for additional visualization technique references.) While these are all interesting options for the visualization of our results, this aspect will be left for future exploration. 4.3 Text Classification/Categorization Suppose that we are unable to directly locate slot fills from a text, or even
Reference: [HKW94] <author> Matthias Hemmje, Clemens Kunkel, and Alexander Willett. </author> <title> LyberWorld a Visualization User Interface Supporting Fulltext Retrieval. </title> <booktitle> In Proceedings of the 17th Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 249-259, </pages> <address> Dublin, Ireland, </address> <month> July </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: This would take advantage of the work done by Salton and Allan on visualization of document structure [SA93, All96]. Other means of visualization include: cone trees [RMC91], a perspective wall [MRC91], a document lens [RM93], TileBars [Hea95], and NavigationCones <ref> [HKW94] </ref>. (See [RPH + 95] for additional visualization technique references.) While these are all interesting options for the visualization of our results, this aspect will be left for future exploration. 4.3 Text Classification/Categorization Suppose that we are unable to directly locate slot fills from a text, or even to locate the
Reference: [HP93] <author> Marti A. Hearst and Christian Plaunt. </author> <title> Subtopic Structuring for Full-Length Document Access. </title> <booktitle> In Proceedings of the 16th Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 59-68, </pages> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: Passages are not retrieved in response to general queries, only in response to a concept. Another technique based on a document's thematic structure is TextTiling <ref> [Hea93, HP93] </ref>. With TextTiling, coherent subtopic discussions determine document indices. Subtopic discussion breaks are ascertained by evaluating the frequencies of the terms found in proximity to one another.
Reference: [Huf95] <author> Scott B. Huffman. </author> <title> Learning information extraction patterns from examples. </title> <booktitle> In Working Notes of the IJCAI Workshop on New Approaches to Learning for Natural Language Processing, </booktitle> <pages> pages 127-134, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year> <note> AAAI. 44 </note>
Reference-contexts: examination by the extraction system. (We do a similar thing in that we use our relevance feedback generated query to act as a filter on the collection and locate relevant texts.) Additionally, they further reduce the amount of semantic processing by only examining those sentences in which the keywords appear <ref> [Huf95] </ref>. AutoSlog-TS [RS95a] shows great promise in overcoming one of the limitations found with most natural language processing systems that of needing large quantities of training data to learn concepts and construct dictionaries. Primarily this training data has consisted of a domain-specific manually-annotated corpora. <p> Limited automatic extraction was obtained and with a lower than acceptable precision and recall rates. An additional problem with IE systems is that there appears to be an implicit or even explicit <ref> [Huf95] </ref> assumption that the information being extracted is about an "event". These systems further assume that the constituent elements being found fill "roles" within an event.
Reference: [JC94] <author> Yufeng Jing and W. Bruce Croft. </author> <title> An Association Thesaurus for Information Retrieval. </title> <booktitle> In Intelligent Multimedia Information Retrieval Systems and Management, </booktitle> <volume> RIAO '94, </volume> <pages> pages 146-160, </pages> <address> New York, NY, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Concept recognizers These are methods for identifying closely related ideas or patterns such as: proper names, dates, monetary values, and foreign countries. (monthly surplus: monetary value; loan-due-date: date) (See Section 3.6.) 6. Synonym expansion through use of a thesaurus, an association thesaurus, <ref> [JC94] </ref>, or a cooc currence thesaurus, [SP94]. We observe that for most slots, there will be multiple ways in which to describe the value for a slot fill. These different descriptors will vary both within and across texts. <p> We also have the option of expanding queries. We could try using a thesaurus to add new terms. Alternatively, we could further refine our expansion operation to take advantage of domain knowledge by building an association thesaurus <ref> [JC94] </ref>. We have several choices of database on which to base our association thesaurus. We could use either the entire collection or just those documents related to the problem case as found in the CKB.
Reference: [KB95] <author> Bruce Krulwich and Chad Burkley. ContactFinder: </author> <title> Extracting indications of expertise and answering questions with referrals. </title> <booktitle> In Working Notes of the AAAI Fall Symposium Series: AI Applications in Knowledge Navigation and Retrieval, </booktitle> <pages> pages 85-91, </pages> <address> Cambridge, MA, </address> <month> November </month> <year> 1995. </year> <note> AAAI. </note>
Reference-contexts: individuals, companies, or countries, look for a word that is all capitals surrounded by parenthesis, (which would denote an acronym,) look for the use of italics or underlining to signify important concepts, and find multiply repeated compound noun phrases (with the belief that some of these will be domain specific.) <ref> [KB95] </ref> (These techniques are related to concept recognizers, which are discussed in Section 3.6.) 20 One system filters a text stream based on keywords, then uses the ODIE information extraction system to do extraction on the remaining texts [SHH95].
Reference: [Kol84] <author> Janet L. Kolodner. </author> <title> Retrieval and Organizational Strategies in Conceptual Memory: </title>
Reference-contexts: Cases are hand-coded into structures that support this process. Sadly, most CBR systems require the manual input of cases; they are not automatically generated from text or other sources (e.g., episodes in Kolodner's CYRUS <ref> [Kol84] </ref> and Bain's JUDGE [Bai86] used a conceptual dependency representation for their textual input.) A human makes the decisions about how to structure a case and what indexing structures to use.
References-found: 34

