URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/icpr-norm.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/proc.html
Root-URL: http://www.iro.umontreal.ca
Title: Word Normalization for On-Line Handwritten Word Recognition  
Author: Yoshua Bengio Yann Le Cun 
Address: Montreal Holmdel, NJ 07733 Montreal, Qc H3C-3J7, Canada U.S.A.  
Affiliation: Dept. Informatique et Recherche AT&T Bell Laboratories Operationelle, Universite de  
Abstract: We introduce a new approach to normalizing words written with an electronic stylus that applies to all styles of handwriting (upper case, lower case, printed, cursive, or mixed). A geometrical model of the word spatial structure is fitted to the pen trajectory using the EM algorithm. The fitting process maximizes the likelihood of the trajectory given the model and a set a priors on its parameters. The method was evaluated and integrated to a recognition system that combines neural networks and hidden Markov models. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Bengio, R. De Mori, G. Flammia, and R. Kompe. </author> <title> Global optimization of a neural network-hidden Markov model hybrid. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(2) </volume> <pages> 252-259, </pages> <year> 1992. </year>
Reference-contexts: Further improvements were obtained by training at the word level (as in <ref> [1] </ref>) both the neural network rec-ognizer and the post-processor.
Reference: [2] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Various search techniques can be used to minimize the energy function (gradient descent, simulated annealing, combinatorial techniques), but formulating the problem in a probabilistic framework allows us to use the fast and powerful Expectation-Maximization algorithm (EM) <ref> [2] </ref>. In the probabilistic framework, the model is viewed as generating the data through a stochastic process. The energy function is interpreted as the negative logarithm of the likelihood that the data be generated by the model. <p> In fact, the function is likely to be non-convex and have local minima. Although minimizing this function can be done with standard techniques such as gradient descent or conjugate gradient, the probabilistic formulation allows us to use the more efficient EM algo rithm <ref> [2] </ref>. The idea of EM in this context is to express C as a function C (; Z) of and a of set of "hidden" random variables Z whose distribution depends on . Let C () represent the expected value of C (; Z) over Z.
Reference: [3] <author> Y. Le Cun and Y. Bengio. </author> <title> Word-level training of a handritten word recognizer based on convolutional neural networks. </title> <editor> In IEEE, editor, ICPR'94, </editor> <address> Jerusalem 1994, </address> <year> 1994. </year>
Reference-contexts: Once the model has been fitted to the data, the pen trajectories can be normalized (scaled, rotated, translated) according to the expectations of a character or word recognizer. This important preprocessing step has been integrated in a word recognition system described in a companion paper <ref> [3] </ref>. <p> These parameters can be used to normalize the original pen trajectory with respect to the word model. 5 Experiments on the Word Recog nizer The handwriting recognition system described in <ref> [3] </ref> is based on a spatial representation, convolutional neural networks, and a post-processor that integrates lexical and grammatical constraints. <p> We evaluated the improvements brought by the word normalization to a word recognition system that is based on optimally combining a large number of character candidates in order to form a word hypothesis. With the word recognition system described in a companion paper <ref> [3] </ref>, and before doing any word-level training, we obtained without word normalization (using character-level normalization) 7.3% and 3.5% word and character errors (adding insertions, deletions and substitutions) when the search was constrained within a 25461-word dictionary.
Reference: [4] <author> Y. Le Cun, Y. Matan, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, and H.S. Baird. </author> <title> Handwritten zip code recognition with multilayer networks. </title> <editor> In IAPR, editor, </editor> <booktitle> International Conference on Pattern Recognition, </booktitle> <address> Atlantic City, 1990. </address> <publisher> IEEE. </publisher>
Reference-contexts: This representation is particularly well suited for use in combination with Multi-Layer Convolutional Neural Networks (MCLNN) <ref> [6, 4] </ref>. These networks are feedforward and their architecture is tailored to minimizing sensitivity to translations, rotations or distortions of the input image. They are trained with a variation of the Back-Propagation algorithm [8, 5].
Reference: [5] <author> Y. LeCun. </author> <title> Learning processes in an asymmetric threshold network. </title> <editor> In E. Bienenstock, F. Fogelman-Soulie, and G. Weisbuch, editors, </editor> <booktitle> Disordered Systems and Biological Organization, </booktitle> <pages> pages 233-240. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Les Houches 1985, </address> <year> 1986. </year>
Reference-contexts: This representation is particularly well suited for use in combination with Multi-Layer Convolutional Neural Networks (MCLNN) [6, 4]. These networks are feedforward and their architecture is tailored to minimizing sensitivity to translations, rotations or distortions of the input image. They are trained with a variation of the Back-Propagation algorithm <ref> [8, 5] </ref>. The neural network gives scores associated to characters when the network has an input field, called segment, that covers a connected subset of the whole word input. A segmentation is a sequence of such segments that covers the whole word input.
Reference: [6] <author> Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. </author> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 541-551, </pages> <year> 1989. </year>
Reference-contexts: This representation is particularly well suited for use in combination with Multi-Layer Convolutional Neural Networks (MCLNN) <ref> [6, 4] </ref>. These networks are feedforward and their architecture is tailored to minimizing sensitivity to translations, rotations or distortions of the input image. They are trained with a variation of the Back-Propagation algorithm [8, 5].
Reference: [7] <author> O. Matan, C.J.C. Burges, Y. LeCun, and J.S. Denker. </author> <title> Multi-digit recognition using a space displacement neural network. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lipmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 488-495, </pages> <address> San Mateo CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is particularly true when scores from the recognizer are used in an integrated segmentation/recognition system. It is even more the case with so-called "Space Displacement Neural Networks" (SDNN) that take an entire unsegmented word as input <ref> [7] </ref>. Many recognition methods, particularly the ones that use pictorial representations of the characters, require that the size, position, and orientation of ob jects be approximately constant, at least within a single class.
Reference: [8] <editor> D.E. Rumelhart, J.L. McClelland, </editor> <booktitle> and the PDP Research Group. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: This representation is particularly well suited for use in combination with Multi-Layer Convolutional Neural Networks (MCLNN) [6, 4]. These networks are feedforward and their architecture is tailored to minimizing sensitivity to translations, rotations or distortions of the input image. They are trained with a variation of the Back-Propagation algorithm <ref> [8, 5] </ref>. The neural network gives scores associated to characters when the network has an input field, called segment, that covers a connected subset of the whole word input. A segmentation is a sequence of such segments that covers the whole word input.
Reference: [9] <author> C. Tappert, C. Suen, and T. Wakahara. </author> <title> The state of the art in on-line handwriting recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(12), </volume> <year> 1990. </year>
Reference-contexts: 1 Introduction Natural handwriting can be a mixture of different "styles", lower case printed, upper case, cursive, and punctuation. In order to improve the success of pen-based computers, we would like a recognizer that reliably handles such handwriting, but its implementation faces major technical challenges <ref> [9] </ref>. It has been long known that, although characters taken in isolation can be very ambiguous, considerable information is available from the context of the whole word.
References-found: 9

