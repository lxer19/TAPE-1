URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3626/3626.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Phone: 2  3  
Title: How Embedded Memory in Recurrent Neural Network Architectures Helps Learning Long-term Temporal Dependencies  
Author: Tsungnan Lin ; Bill G. Horne and C. Lee Giles ; 
Keyword: discrete-time, memory, long-term dependencies, recurrent neural networks, training, gradient descent  
Address: 4 Independence Way, Princeton, NJ 08540  Princeton, NJ 08540  College Park, MD 20742  
Affiliation: 1 NEC Research Institute,  Department of Electrical Engineering, Princeton University,  UMIACS, University of Maryland,  
Abstract: Learning long-term temporal dependencies with recurrent neural networks can be a difficult problem. It has recently been shown that a class of recurrent neural networks called NARX networks perform much better than conventional recurrent neural networks for learning certain simple long-term dependency problems. The intuitive explanation for this behavior is that the output memories of a NARX network can be manifested as jump-ahead connections in the time-unfolded network. These jump-ahead connections can propagate gradient information more efficiently, thus reducing the sensitivity of the network to long-term dependencies. This work gives empirical justification to our hypothesis that similar improvements in learning long-term dependencies can be achieved with other classes of recurrent neural network architectures simply by increasing the order of the embedded memory. In particular we explore the impact of learning simple long-term dependency problems on three classes of recurrent neural network architectures: globally recurrent networks, locally recurrent networks, and NARX (output feedback) networks. Comparing the performance of these architectures with different orders of embedded memory on two simple long-term dependencies problems shows that all of these classes of network architectures demonstrate significant improvement on learning long-term dependencies when the orders of embedded memory are increased. These results can be important to a user comfortable to a specific recurrent neural network architecture because simply increasing the embedding memory order will make the architecture more robust to the problem of long-term dependency learning. fl Computer Science Technical Report CS-TR-3626 and UMIACS-TR-96-28, University of Maryland, College Park, MD 20742 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.D. Back and A.C. Tsoi. </author> <title> FIR and IIR synapses, a new neural network architecture for time series modelling. </title> <journal> Neural Computation, </journal> <volume> 3(3) </volume> <pages> 337-350, </pages> <year> 1991. </year>
Reference-contexts: = f @ k1 j=1 ijm o j (t k) + k=1 ik u k (t) + w b 1 2.2 Locally recurrent networks In this class of networks, the feedback connections are only allowed from neurons to themselves, and the nodes are connected together in a feed forward architecture <ref> [1, 7, 21, 28] </ref>. Specifically, we consider networks proposed by Frasconi et al. [7] (we will call LR), as shown in Figure 2 (a).
Reference: [2] <author> Y. Bengio, P. Simard, and P. Frasconi. </author> <title> Learning long-term dependencies with gradient is difficult. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 157-166, </pages> <year> 1994. </year>
Reference-contexts: However, various empirical studies suggest that sometimes learning even simple behavior can be quite difficult when using gradient-descent learning algorithms. Recently, it has been demonstrated that at least part of this difficulty can be attributed to the problem of long-term dependencies <ref> [2, 18] </ref>, i.e. those problems for which the desired output of a system at time T depends on inputs presented at times t t T . <p> In particular Bengio et al. <ref> [2] </ref> showed that if a system is to latch information robustly, then the fraction of the gradient in a gradient-based training algorithm due to information n time steps in the past approaches zero as n becomes large. This effect is called the problem of vanishing gradient. <p> One possible approach is to preset initial weights by using prior knowledge [6, 9] but this is often not available in many applications. Another approach is to use alternative optimization methods instead of gradient-based methods <ref> [2] </ref>. But, those algorithms can perform as poorly as gradient methods, or require far more computational resources. Alternatively, the input data can be altered to represent a reduced description that makes global features more explicit and more readily detectable [18, 22, 23]. <p> Also note that in some cases the order of the embedded memory is the same. 3.1 The latching problem This experiment evaluates the performance of different recurrent network architectures with various order of embedded memory on a problem already used for studying the difficulty in learning long-term dependencies <ref> [2, 11, 16] </ref>. This problem is a minimal task designed as a test that must necessarily be passed in order for a network to robustly latch information [2]. <p> This problem is a minimal task designed as a test that must necessarily be passed in order for a network to robustly latch information <ref> [2] </ref>. In this two-class problem, the class of a sequence depends 7 only on the first 3 time steps, the remaining values in the sequence is uniform noise. There are three inputs u 1 (t), u 2 (t), and a noise input e (t). <p> Target information was only provided at the end of each sequence. For comparison, our training particulars are identical to those of <ref> [2] </ref>. For strings from class one, a target value of 0:8 was chosen, for class two, 0:8 was chosen. The length of the noisy sequence could be varied in order to control the span of long-term dependencies.
Reference: [3] <author> S. Chen, S.A. Billings, and P.M. Grant. </author> <title> Non-linear system identification using neural networks. </title> <journal> International Journal of Control, </journal> <volume> 51(6) </volume> <pages> 1191-1214, </pages> <year> 1990. </year>
Reference-contexts: In Section 2, we discuss three classes of conventional recurrent neural networks architectures: globally recurrent networks (the architecture, not the training procedure, used by Elman) [5]; locally recurrent networks (in particular the Frasconi, Gori and Soda's model) [7]; NARX networks <ref> [3, 20] </ref>, and their corresponding models with a high order embedded memory. In Section 3, we provide a empirical comparison of these architectures by investigating their performance on learning two simple long-term dependencies problems: the latching problem and a grammatical inference problem. <p> For another taxometric approach based on memory types, see Mozer [19]. For this study we picked three classes of networks: globally recurrent (GR) networks [5], locally recurrent networks (LR) [7], and NARX networks <ref> [3, 20] </ref>; and their corresponding architectures with high-order embedded memory. It should be pointed out that our embedded memory simply consists of simple tapped delayed values to various neurons and not more sophisticated embedded memory structures [19, 4]. NARX networks are a typical model of networks with observable states. <p> + j ij u j (t) + w b 1 usually differ in where and how much output feedback is permitted; see [28] for a discussion of architectural differences. 2.3 NARX recurrent neural networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model <ref> [3, 17, 26, 27] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (6) (a) (b) where u (t) and y (t) represent input and output of the network <p> When the function f can be approximated by a Multilayer Perceptron, the resulting system is called a NARX recurrent neural network <ref> [3, 20] </ref>. In this paper, we shall consider NARX networks with zero input order and .
Reference: [4] <author> B. de Vries and J. C. Principe. </author> <title> The gamma model | A new neural model for temporal processing. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 565-576, </pages> <year> 1992. </year>
Reference-contexts: It should be pointed out that our embedded memory simply consists of simple tapped delayed values to various neurons and not more sophisticated embedded memory structures <ref> [19, 4] </ref>. NARX networks are a typical model of networks with observable states.
Reference: [5] <author> J.L. Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year>
Reference-contexts: In this paper, we empirically justify this hypothesis by showing the relationship between memory order of a RNN and its sensitivity to long-term dependencies. In Section 2, we discuss three classes of conventional recurrent neural networks architectures: globally recurrent networks (the architecture, not the training procedure, used by Elman) <ref> [5] </ref>; locally recurrent networks (in particular the Frasconi, Gori and Soda's model) [7]; NARX networks [3, 20], and their corresponding models with a high order embedded memory. <p> For another taxometric approach based on memory types, see Mozer [19]. For this study we picked three classes of networks: globally recurrent (GR) networks <ref> [5] </ref>, locally recurrent networks (LR) [7], and NARX networks [3, 20]; and their corresponding architectures with high-order embedded memory. It should be pointed out that our embedded memory simply consists of simple tapped delayed values to various neurons and not more sophisticated embedded memory structures [19, 4].
Reference: [6] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda. Unified integration of explicit rules and learning by example in recurrent networks. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 7(2) </volume> <pages> 340-346, </pages> <year> 1995. </year>
Reference-contexts: Several approaches have been suggested to circumvent the problem of vanishing gradients in training RNNs. One possible approach is to preset initial weights by using prior knowledge <ref> [6, 9] </ref> but this is often not available in many applications. Another approach is to use alternative optimization methods instead of gradient-based methods [2]. But, those algorithms can perform as poorly as gradient methods, or require far more computational resources.
Reference: [7] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda. Local feedback multilayered networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 120-130, </pages> <year> 1992. </year>
Reference-contexts: In Section 2, we discuss three classes of conventional recurrent neural networks architectures: globally recurrent networks (the architecture, not the training procedure, used by Elman) [5]; locally recurrent networks (in particular the Frasconi, Gori and Soda's model) <ref> [7] </ref>; NARX networks [3, 20], and their corresponding models with a high order embedded memory. In Section 3, we provide a empirical comparison of these architectures by investigating their performance on learning two simple long-term dependencies problems: the latching problem and a grammatical inference problem. <p> For another taxometric approach based on memory types, see Mozer [19]. For this study we picked three classes of networks: globally recurrent (GR) networks [5], locally recurrent networks (LR) <ref> [7] </ref>, and NARX networks [3, 20]; and their corresponding architectures with high-order embedded memory. It should be pointed out that our embedded memory simply consists of simple tapped delayed values to various neurons and not more sophisticated embedded memory structures [19, 4]. <p> = f @ k1 j=1 ijm o j (t k) + k=1 ik u k (t) + w b 1 2.2 Locally recurrent networks In this class of networks, the feedback connections are only allowed from neurons to themselves, and the nodes are connected together in a feed forward architecture <ref> [1, 7, 21, 28] </ref>. Specifically, we consider networks proposed by Frasconi et al. [7] (we will call LR), as shown in Figure 2 (a). <p> Specifically, we consider networks proposed by Frasconi et al. <ref> [7] </ref> (we will call LR), as shown in Figure 2 (a).
Reference: [8] <author> C.L. Giles, G.M. Kuhn, and R.J. Williams. </author> <title> Dynamic recurrent neural networks: </title> <journal> Theory and applications. IEEE Transactions on Neural Networks, </journal> <volume> 5(2), </volume> <year> 1994. </year> <note> Special Issue. </note>
Reference-contexts: these classes of recurrent neural network architectures all demonstrate significant improvement on learning long-term dependencies when the embedded memory order is increased. 2 Embedding high order memory in recurrent neural net work architectures Several recurrent neural network architectures have been proposed; for a collection of papers on the variety see <ref> [8] </ref>. One taxometric classification for these architectures can be based on the observability of their states: specifically they can be broadly divided into two groups depending on whether or not the states of the network are observable or not [13].
Reference: [9] <author> C.L. Giles and C.W. Omlin. </author> <title> Inserting rules into recurrent neural networks. </title> <editor> In S.Y. Kung, F. Fallside, J. Aa. Sorenson, and C.A. Kamm, editors, </editor> <booktitle> Neural Networks for Signal Processing II, Proceedings of The 1992 IEEE Workshop, </booktitle> <pages> pages 13-22, </pages> <address> Piscataway, NJ, 1992. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: Several approaches have been suggested to circumvent the problem of vanishing gradients in training RNNs. One possible approach is to preset initial weights by using prior knowledge <ref> [6, 9] </ref> but this is often not available in many applications. Another approach is to use alternative optimization methods instead of gradient-based methods [2]. But, those algorithms can perform as poorly as gradient methods, or require far more computational resources.
Reference: [10] <author> M. Gori, M. Maggini, and G. </author> <title> Soda. Scheduling of modular architectures for inductive inference of regular grammars. </title> <booktitle> In ECAI'94 Workshop on Combining Symbolic and Connectionist Processing, Amsterdam, </booktitle> <pages> pages 78-87. </pages> <publisher> Wiley, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: Unfortunately, this approach may fail if short-term dependencies are equally as important. Hochreiter and Schmidhuber [12] propose a specific architectural approach which utilizes high-order gating units. Recently, it has been suggested that a network architecture that operates on multiple time scales might be useful <ref> [10, 11] </ref>. We have shown that a class of recurrent neural networks called NARX networks long-term dependencies when using a gradient descent training algorithm than previously reported in the literature [16, 15].
Reference: [11] <author> S. El Hihi and Y. Bengio. </author> <title> Hierarchical recurrent neural networks for long-term dependencies. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: Unfortunately, this approach may fail if short-term dependencies are equally as important. Hochreiter and Schmidhuber [12] propose a specific architectural approach which utilizes high-order gating units. Recently, it has been suggested that a network architecture that operates on multiple time scales might be useful <ref> [10, 11] </ref>. We have shown that a class of recurrent neural networks called NARX networks long-term dependencies when using a gradient descent training algorithm than previously reported in the literature [16, 15]. <p> Also note that in some cases the order of the embedded memory is the same. 3.1 The latching problem This experiment evaluates the performance of different recurrent network architectures with various order of embedded memory on a problem already used for studying the difficulty in learning long-term dependencies <ref> [2, 11, 16] </ref>. This problem is a minimal task designed as a test that must necessarily be passed in order for a network to robustly latch information [2].
Reference: [12] <author> S. Hochreiter and J. Schmidhuber. </author> <title> Long short term memory. </title> <type> Technical Report FKI-207-95, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen, Munchen, </institution> <year> 1995. </year>
Reference-contexts: Alternatively, the input data can be altered to represent a reduced description that makes global features more explicit and more readily detectable [18, 22, 23]. Unfortunately, this approach may fail if short-term dependencies are equally as important. Hochreiter and Schmidhuber <ref> [12] </ref> propose a specific architectural approach which utilizes high-order gating units. Recently, it has been suggested that a network architecture that operates on multiple time scales might be useful [10, 11]. <p> The long-term dependency problems investigated were the latching problem and a grammatical inference problem. These problems were chosen because they are simple and should be easy to learn but typlify the long-term dependency issue. For more complex problems involving long-term dependencies see <ref> [12] </ref>.
Reference: [13] <author> B.G. Horne and C.L. Giles. </author> <title> An experimental comparison of recurrent neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 697-704. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: One taxometric classification for these architectures can be based on the observability of their states: specifically they can be broadly divided into two groups depending on whether or not the states of the network are observable or not <ref> [13] </ref>. For another taxometric approach based on memory types, see Mozer [19]. For this study we picked three classes of networks: globally recurrent (GR) networks [5], locally recurrent networks (LR) [7], and NARX networks [3, 20]; and their corresponding architectures with high-order embedded memory. <p> Motivated by the analysis of the problem of learning long-term dependencies and the success of NARX networks on problems including grammatical inference and nonlinear system identification <ref> [13] </ref>, we explore the ability of other recurrent neural networks with a high order of embedded memory on problems that involve long-term dependencies. We chose three classes of recurrent neural network architectures based on state-observerability: hidden state globally recurrent and locally recurrent networks, and observeable state NARX networks.
Reference: [14] <author> M. I. Jordan. </author> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the Eighth Conference of the Cognitive Science Society, </booktitle> <pages> pages 531-546. </pages> <publisher> Erlbaum, </publisher> <year> 1986. </year>
Reference-contexts: It is worth noting that one of the first uses of embedded memory in recurrent network architectures was that of Jordan <ref> [14] </ref>. In this paper, we empirically justify this hypothesis by showing the relationship between memory order of a RNN and its sensitivity to long-term dependencies.
Reference: [15] <author> Tsungnan Lin, B.G. Horne, P. Tino, and C.L. Giles. </author> <title> Learning long-term dependencies in narx recurrent neural networks. </title> <journal> IEEE Transactions on Neural Networks. </journal> <note> Accepted. </note>
Reference-contexts: Recently, it has been suggested that a network architecture that operates on multiple time scales might be useful [10, 11]. We have shown that a class of recurrent neural networks called NARX networks long-term dependencies when using a gradient descent training algorithm than previously reported in the literature <ref> [16, 15] </ref>. The intuitive explanation for this behavior is that the output memories of a NARX neural network are manifested as jump-ahead connections in the time-unfolded network that is often associated with algorithms as Backpropagation Through Time (BPTT).
Reference: [16] <author> Tsungnan Lin, B.G. Horne, P. Tino, and C.L. Giles. </author> <title> Learning long-term dependencies is not as difficult with narx recurrent neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year> <note> In press. </note>
Reference-contexts: Recently, it has been suggested that a network architecture that operates on multiple time scales might be useful [10, 11]. We have shown that a class of recurrent neural networks called NARX networks long-term dependencies when using a gradient descent training algorithm than previously reported in the literature <ref> [16, 15] </ref>. The intuitive explanation for this behavior is that the output memories of a NARX neural network are manifested as jump-ahead connections in the time-unfolded network that is often associated with algorithms as Backpropagation Through Time (BPTT). <p> Also note that in some cases the order of the embedded memory is the same. 3.1 The latching problem This experiment evaluates the performance of different recurrent network architectures with various order of embedded memory on a problem already used for studying the difficulty in learning long-term dependencies <ref> [2, 11, 16] </ref>. This problem is a minimal task designed as a test that must necessarily be passed in order for a network to robustly latch information [2].
Reference: [17] <author> L. Ljung. </author> <title> System identification : Theory for the user. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: + j ij u j (t) + w b 1 usually differ in where and how much output feedback is permitted; see [28] for a discussion of architectural differences. 2.3 NARX recurrent neural networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model <ref> [3, 17, 26, 27] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (6) (a) (b) where u (t) and y (t) represent input and output of the network
Reference: [18] <author> M. C. Mozer. </author> <title> Induction of multiscale temporal structure. </title> <editor> In J.E. Moody, S. J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Neural Information Processing Systems 4, </booktitle> <pages> pages 275-282. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: However, various empirical studies suggest that sometimes learning even simple behavior can be quite difficult when using gradient-descent learning algorithms. Recently, it has been demonstrated that at least part of this difficulty can be attributed to the problem of long-term dependencies <ref> [2, 18] </ref>, i.e. those problems for which the desired output of a system at time T depends on inputs presented at times t t T . <p> But, those algorithms can perform as poorly as gradient methods, or require far more computational resources. Alternatively, the input data can be altered to represent a reduced description that makes global features more explicit and more readily detectable <ref> [18, 22, 23] </ref>. Unfortunately, this approach may fail if short-term dependencies are equally as important. Hochreiter and Schmidhuber [12] propose a specific architectural approach which utilizes high-order gating units. Recently, it has been suggested that a network architecture that operates on multiple time scales might be useful [10, 11].
Reference: [19] <author> Michael C. Mozer. </author> <title> Neural net architectures for temporal sequence processing. In A.S. </title> <editor> Weigend and N.A. Gershenfeld, editors, </editor> <booktitle> Time Series Prediction, </booktitle> <pages> pages 243-264. </pages> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: One taxometric classification for these architectures can be based on the observability of their states: specifically they can be broadly divided into two groups depending on whether or not the states of the network are observable or not [13]. For another taxometric approach based on memory types, see Mozer <ref> [19] </ref>. For this study we picked three classes of networks: globally recurrent (GR) networks [5], locally recurrent networks (LR) [7], and NARX networks [3, 20]; and their corresponding architectures with high-order embedded memory. <p> It should be pointed out that our embedded memory simply consists of simple tapped delayed values to various neurons and not more sophisticated embedded memory structures <ref> [19, 4] </ref>. NARX networks are a typical model of networks with observable states.
Reference: [20] <author> K.S. Narendra and K. Parthasarathy. </author> <title> Identification and control of dynamical systems using neural networks. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 1(1):4, </volume> <year> 1990. </year>
Reference-contexts: In Section 2, we discuss three classes of conventional recurrent neural networks architectures: globally recurrent networks (the architecture, not the training procedure, used by Elman) [5]; locally recurrent networks (in particular the Frasconi, Gori and Soda's model) [7]; NARX networks <ref> [3, 20] </ref>, and their corresponding models with a high order embedded memory. In Section 3, we provide a empirical comparison of these architectures by investigating their performance on learning two simple long-term dependencies problems: the latching problem and a grammatical inference problem. <p> For another taxometric approach based on memory types, see Mozer [19]. For this study we picked three classes of networks: globally recurrent (GR) networks [5], locally recurrent networks (LR) [7], and NARX networks <ref> [3, 20] </ref>; and their corresponding architectures with high-order embedded memory. It should be pointed out that our embedded memory simply consists of simple tapped delayed values to various neurons and not more sophisticated embedded memory structures [19, 4]. NARX networks are a typical model of networks with observable states. <p> When the function f can be approximated by a Multilayer Perceptron, the resulting system is called a NARX recurrent neural network <ref> [3, 20] </ref>. In this paper, we shall consider NARX networks with zero input order and .
Reference: [21] <author> P.S. Sastry, G. Santharam, </author> <title> and K.P. Unnikrishnan. Memory neuron networks for identification and control of dynamical systems. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 306-319, </pages> <year> 1994. </year>
Reference-contexts: = f @ k1 j=1 ijm o j (t k) + k=1 ik u k (t) + w b 1 2.2 Locally recurrent networks In this class of networks, the feedback connections are only allowed from neurons to themselves, and the nodes are connected together in a feed forward architecture <ref> [1, 7, 21, 28] </ref>. Specifically, we consider networks proposed by Frasconi et al. [7] (we will call LR), as shown in Figure 2 (a).
Reference: [22] <author> J. Schmidhuber. </author> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 234-242, </pages> <year> 1992. </year>
Reference-contexts: But, those algorithms can perform as poorly as gradient methods, or require far more computational resources. Alternatively, the input data can be altered to represent a reduced description that makes global features more explicit and more readily detectable <ref> [18, 22, 23] </ref>. Unfortunately, this approach may fail if short-term dependencies are equally as important. Hochreiter and Schmidhuber [12] propose a specific architectural approach which utilizes high-order gating units. Recently, it has been suggested that a network architecture that operates on multiple time scales might be useful [10, 11].
Reference: [23] <author> J. Schmidhuber. </author> <title> Learning unambiguous reduced sequence descriptions. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 291-298. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: But, those algorithms can perform as poorly as gradient methods, or require far more computational resources. Alternatively, the input data can be altered to represent a reduced description that makes global features more explicit and more readily detectable <ref> [18, 22, 23] </ref>. Unfortunately, this approach may fail if short-term dependencies are equally as important. Hochreiter and Schmidhuber [12] propose a specific architectural approach which utilizes high-order gating units. Recently, it has been suggested that a network architecture that operates on multiple time scales might be useful [10, 11].
Reference: [24] <author> D.R. Seidl and R.D. Lorenz. </author> <title> A structure by which a recurrent neural network can approximate a nonlinear dynamic system. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks 1991, </booktitle> <volume> volume II, </volume> <pages> pages 709-714, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical systems <ref> [24] </ref> and can be computationally quite powerful [25]. However, various empirical studies suggest that sometimes learning even simple behavior can be quite difficult when using gradient-descent learning algorithms.
Reference: [25] <author> H.T. Siegelmann and E.D. Sontag. </author> <title> On the computational power of neural nets. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50(1) </volume> <pages> 132-150, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Recurrent Neural Networks (RNNs) are capable of representing arbitrary nonlinear dynamical systems [24] and can be computationally quite powerful <ref> [25] </ref>. However, various empirical studies suggest that sometimes learning even simple behavior can be quite difficult when using gradient-descent learning algorithms.
Reference: [26] <author> H.-T. Su and T.J. McAvoy. </author> <title> Identification of chemical processes using recurrent networks. </title> <booktitle> In Proceedings of the American Controls Conference, </booktitle> <volume> volume 3, </volume> <pages> pages 2314-2319, </pages> <year> 1991. </year>
Reference-contexts: + j ij u j (t) + w b 1 usually differ in where and how much output feedback is permitted; see [28] for a discussion of architectural differences. 2.3 NARX recurrent neural networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model <ref> [3, 17, 26, 27] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (6) (a) (b) where u (t) and y (t) represent input and output of the network
Reference: [27] <author> H.-T. Su, T.J. McAvoy, and P. Werbos. </author> <title> Long-term predictions of chemical processes using recurrent neural networks: A parallel training approach. </title> <journal> Industrial Engineering and Chemical Research, </journal> <volume> 31 </volume> <pages> 1338-1352, </pages> <year> 1992. </year>
Reference-contexts: + j ij u j (t) + w b 1 usually differ in where and how much output feedback is permitted; see [28] for a discussion of architectural differences. 2.3 NARX recurrent neural networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model <ref> [3, 17, 26, 27] </ref>: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y (t D y ); : : : ; y (t 1) ; (6) (a) (b) where u (t) and y (t) represent input and output of the network
Reference: [28] <author> A.C. Tsoi and A. </author> <title> Back. Locally recurrent globally feedforward networks, a critical review of architectures. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 229-239, </pages> <year> 1994. </year> <month> 15 </month>
Reference-contexts: = f @ k1 j=1 ijm o j (t k) + k=1 ik u k (t) + w b 1 2.2 Locally recurrent networks In this class of networks, the feedback connections are only allowed from neurons to themselves, and the nodes are connected together in a feed forward architecture <ref> [1, 7, 21, 28] </ref>. Specifically, we consider networks proposed by Frasconi et al. [7] (we will call LR), as shown in Figure 2 (a). <p> For a network with embedded memory of order m, the output of the dynamic neurons becomes o i (t) = f @ n=1 ii o i (k n) + j ij u j (t) + w b 1 usually differ in where and how much output feedback is permitted; see <ref> [28] </ref> for a discussion of architectural differences. 2.3 NARX recurrent neural networks An important class of discrete-time nonlinear systems is the Nonlinear AutoRegressive with eXogenous inputs (NARX) model [3, 17, 26, 27]: y (t) = f u (t D u ); : : : ; u (t 1); u (t); y
References-found: 28

