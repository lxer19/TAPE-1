URL: ftp://ftp.cs.indiana.edu/pub/gasser/coling94.ps
Refering-URL: http://www.cs.indiana.edu/ai/Gasser/Morphophon/home.html
Root-URL: http://www.cs.indiana.edu
Title: MODULARITY IN A CONNECTIONIST MODEL OF MORPHOLOGY ACQUISITION  
Author: Michael Gasser 
Affiliation: Departments of Computer Science and Linguistics Indiana University  
Abstract: This paper describes a modular connectionist model of the acquisition of receptive inflectional morphology. The model takes inputs in the form of phones one at a time and outputs the associated roots and inflections. In its simplest version, the network consists of separate simple recurrent subnetworks for root and inflection identification; both networks take the phone sequence as inputs. It is shown that the performance of the two separate modular networks is superior to a single network responsible for both root and inflection identification. In a more elaborate version of the model, the network learns to use separate hidden-layer modules to solve the separate tasks of root and inflection identification. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Elman, J. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 179-211. </pages>
Reference-contexts: These approaches bring connectionist models somewhat more in line with the symbolic models which they seek to replace. In this paper I have shown how the ability of simple recurrent networks to extract "structure in time" <ref> (Elman, 1990) </ref> is enhanced by built-in modularity which permits the recurrent hidden-unit connections to develop in ways which are suitable for the root and inflection identification tasks.
Reference: <author> Gasser, M. </author> <year> (1994). </year> <title> Acquiring receptive morphology: a connectionist model. </title> <booktitle> Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 32. </pages>
Reference-contexts: The emphasis here is on the role of modularity at the level of root and inflection in the model. I show how this sort of modularity improves performance dramatically and consider how a network might learn to use modules it is provided with. A separate paper <ref> (Gasser, 1994) </ref> looks in detail at the model's performance for particular categories of morphology, in particular, template morphology and reduplication. The paper is organized as follows. I first provide a brief overview of the categories of morphological rules found in the world's languages.
Reference: <author> Jacobs, R. A., Jordan, M. I., & Barto, A. G. </author> <year> (1991). </year> <title> Task decomposition through competition in a modular connectionist architecture: the what and where vision tasks. </title> <journal> Cognitive Science, </journal> <volume> 15, </volume> <pages> 219-250. </pages>
Reference-contexts: Increasingly, however, the emphasis is moving in the direction of special-purpose modules for subtasks which may conflict with each other if handled by the same hardware <ref> (Jacobs et al., 1991) </ref>. These approaches bring connectionist models somewhat more in line with the symbolic models which they seek to replace.
Reference: <author> MacWhinney, B. & Leinbach, J. </author> <year> (1991). </year> <title> Implementations are not conceptualization: revising the verb learning model. </title> <journal> Cognition, </journal> <volume> 40, </volume> <pages> 121-157. </pages>
Reference: <author> Marslen-Wilson, W. D. & Tyler, L. K. </author> <year> (1980). </year> <title> The temporal structure of spoken language understanding. </title> <journal> Cognition, </journal> <volume> 8, </volume> <pages> 1-71. </pages>
Reference: <author> Plunkett, K. & Marchman, V. </author> <year> (1991). </year> <title> U-shaped learning and frequency effects in a multi-layered perceptron: implications for child language acquisition. </title> <journal> Cognition, </journal> <volume> 38, </volume> <pages> 1-60. </pages>
Reference: <author> Port, R. </author> <year> (1990). </year> <title> Representation and recognition of temporal patterns. </title> <journal> Connection Science, </journal> <volume> 2, </volume> <pages> 151-176. </pages>
Reference-contexts: Thus the task at hand requires a short-term memory of some sort. There are several ways of representing short-term memory in connectionist networks <ref> (Port, 1990) </ref>, in particular, through the use of time-delay connections out of input units and through the use of recurrent time-delay connections on some of the network units.
Reference: <author> Rumelhart, D. E. & McClelland, J. L. </author> <year> (1986). </year> <title> On learning the past tense of English verbs. </title> <editor> In McClelland, J. L. & Rumelhart, D. E. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Volume 2, </volume> <pages> pp. 216-271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The phone target is identical to the input phone. The root and inflection targets, which are constant throughout the presentation of a word, are the patterns associated with the root and inflection for the input word. The network is trained using the backpropagation learning algorithm <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>, which adjusts the weights on all of the network's connections in such a way as to minimize the error, that is, the difference between the network's outputs and the targets.
Reference: <author> Rumelhart, D. E., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. & Mc-Clelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Volume 1, </volume> <pages> pp. 318-364. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The phone target is identical to the input phone. The root and inflection targets, which are constant throughout the presentation of a word, are the patterns associated with the root and inflection for the input word. The network is trained using the backpropagation learning algorithm <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>, which adjusts the weights on all of the network's connections in such a way as to minimize the error, that is, the difference between the network's outputs and the targets.
References-found: 9

