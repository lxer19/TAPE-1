URL: http://cuneus.ai.mit.edu:8000/publications/icafagr96.ps.gz
Refering-URL: http://cuneus.ai.mit.edu:8000/personal/publications.html
Root-URL: 
Email: tonebone@ai.mit.edu tp-temp@ai.mit.edu  
Title: Facial Analysis and Synthesis Using Image-Based Models  
Author: Tony Ezzat Tomaso Poggio 
Address: 545 Technology Square Cambridge, MA 02139  
Affiliation: MIT Artificial Intelligence Laboratory  
Date: October 1996  
Note: Appears in Proceedings of the Second International Conference on Automatic Face and Gesture Recognition,  
Abstract: In this paper, we describe image-based modeling techniques that make possible the creation of photo-realistic computer models of real human faces. The image-based model is built using example views of the face, bypassing the need for any three-dimensional computer graphics models. A learning network is trained to associate each of the example images with a set of pose and expression parameters. For a novel set of parameters, the network synthesizes a novel, intermediate view using a morphing approach. This image-based synthesis paradigm can adequately model both rigid and non-rigid facial movements. We also describe an analysis-by-synthesis algorithm, which is capable of extracting a set of high-level parameters from an image sequence involving facial movement using embedded image-based models. The parameters of the models are perturbed in a local and independent manner for each image until a correspondence-based error metric is minimized. A small sample of experimental results is presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Bergen and R. Hingorani. </author> <title> Hierarchical motion-based frame rate conversion. </title> <type> Technical report, </type> <institution> David Sarnoff Research Center, </institution> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: To obtain such a dense, pixel-wise correspondence between the example images, we utilize optical flow algorithms borrowed from the computer vision literature. Specifically, we use the coarse-to-fine, gradient-based optical flow algorithms developed by Bergen and Hingo-rani <ref> [1] </ref>. In practice, these algorithms work very well when the two example images are similar and not too far apart. In cases where the example images are far apart, we have found that concatenating optical flow by using many intermediate images between examples produces very good final correspondences.
Reference: [2] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. </title> <type> Technical Report 1431, </type> <institution> MIT AI Lab, </institution> <year> 1993. </year>
Reference-contexts: by the fact that a number of researchers (<ref> [2] </ref>, [3], [8], [9]) have noticed the viability of a view interpolation approach to image synthesis, where novel, intermediate images of a scene are synthesized from example endpoints using a morphing technique. In particular, we adopt the approach of Beymer, Shashua, Poggio [2], who cast the view interpolation approach in a learning-by-example framework: each example image is associated with a position in a high-level, multi-dimensional parameter space denoting pose and expression. <p> By training on the examples, a learning network can then generalize, and generate suitable novel images that lie at intermediate points in the example space. The trained network, in essence, becomes a synthesis network, which generates images as output, for suitable parameters as input. Beymer, Shashua, Poggio <ref> [2] </ref>, in fact, showed that this technique is capable of modeling rigid facial transformations such as pose changes, as well as non-rigid transformations such as smiles. <p> The first contribution of this work is to extend the synthesis network paradigm of Beymer, Shashua, Poggio <ref> [2] </ref> into a synthesis module paradigm more suitable for analysis: Firstly, each synthesis network is additionally parameterized with a set of affine parameters, such as translation, rotation, and scale. <p> One axis denotes degree of horizontal pose, while the other denotes degree of vertical pose. The top-right example image, for instance, would be associated with the position in parameter space x = (1:0; 1:0). 2.2 Learning the Map from Parameters to Corre spondences Beymer, Shashua, and Poggio <ref> [2] </ref> framed the learning problem as a problem of approximating an unknown function y = f (x) that maps between the parameter space, x, and the example space, y, given a set of N training samples (x i , y i ) of the function f (x). <p> A simple forward warp operation that pushes the pixels of the reference example image along the synthesized correspondence vector is sufficient to generate the image. To utilize the image texture from all the examples, however, we also adopt a simple correspondence re-orientation procedure, described in <ref> [2] </ref>, that re-orients the synthesized correspondence vector from Equation 1 so that it originates from each of the other example images and points to the same position as the original synthesized correspondence. This allows us to subsequently forward warp all the examples along their respective re-oriented correspondence vectors.
Reference: [3] <author> S. E. Chen and L. Williams. </author> <title> View interpolation for image synthesis. </title> <booktitle> In SIGGRAPH '93 Proceedings, </booktitle> <pages> pages 279-288, </pages> <address> Anaheim, CA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: In this work, we adopt an image-based facial modelling approach, in which we model facial movements using example images. In doing do we are motivated by the fact that a number of researchers ([2], <ref> [3] </ref>, [8], [9]) have noticed the viability of a view interpolation approach to image synthesis, where novel, intermediate images of a scene are synthesized from example endpoints using a morphing technique.
Reference: [4] <author> T. Ezzat. </author> <title> Example-based analysis and synthesis for images of human faces. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: As described in <ref> [4] </ref>, several different synthesis modules were constructed to illustrate the ability of our approach in modeling various facial motions: in addition to the 9-example pose network shown in Figure 1, we have constructed a 5-example network that synthesizes intermediate images lying along open-mouth/smile axes, and a 14-example network that synthesizes pose, <p> The perturbations include the affine parameters, and vary each parameter independently in the positive and negative directions by a small delta factor. 2. For each set of perturbed parameters, the algorithm then synthesizes a correspondence from the module that corresponds to the perturbation. For reasons described in <ref> [4] </ref>, we have opted to obtain the correspondence associated with the perturbation by synthesizing the two perturbed images first, and then computing optical flow between them. 3.
Reference: [5] <author> F. Girosi, M. Jones, and T. Poggio. </author> <title> Priors, stabilizers, and basis functions: From regularization to radial, tensor, and additive splines. </title> <type> Technical Report 1430, </type> <institution> MIT AI Lab, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: We approximate the unknown function y = f (x) which maps from parameters to correspondences given the samples (y i ; x i ) N i=1 , using a radial basis function with Gaussian centers <ref> [5] </ref>, which is expressed in its dual form as: and their positions in the imposed parameter space y = i=1 where the y i are the example correspondences used for training.
Reference: [6] <author> M. Jones and T. Poggio. </author> <title> Model-based matching of line drawings by linear combinations of prototypes. </title> <booktitle> In Proc. of International Conference on Computer Vision, </booktitle> <pages> pages 531-536, </pages> <address> Cambridge, Massachusetts, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Beymer, Shashua, Poggio [2], in fact, showed that this technique is capable of modeling rigid facial transformations such as pose changes, as well as non-rigid transformations such as smiles. From the analysis standpoint, we are motivated in particular by the work of Jones and Poggio <ref> [6] </ref>, who constructed models of line drawings, and used a stochastic gradient descent algorithm to match the models to novel line drawings input by the user. <p> The second contribution of this work is to embed the synthesis modules mentioned previously in an analysis-by-synthesis algorithm similar to that of Jones and Poggio <ref> [6] </ref>. In our case, however, we define a correspondence-based error metric instead of a pixel-based error metric, in an attempt to make the analysis algorithm more robust to changes in lighting, position, scale, and rotation. <p> the previous section are themselves used for analysis. 3.2 A Correspondence-Based Error Metric A key feature of our analysis algorithm is that instead of using the embedded synthesis module to synthesize images to match to the novel images, and thereby have to rely on an image-based error metric, as in <ref> [6] </ref>, the algorithm instead tries to match novel correspondence. For every iteration, the algorithm computes the optical flow between two consecutive novel frames, and then attempts to find the best matching correspondence from within its embedded synthesis module.
Reference: [7] <author> T. Poggio and R. Brunelli. </author> <title> A novel approach to graphics. </title> <type> Technical Report 1354, </type> <institution> MIT AI Lab, </institution> <year> 1992. </year>
Reference-contexts: Rather than trying to approximate a function y = f (x) that maps between the parameter space x and an example space y of images, Poggio and Brunelli <ref> [7] </ref> instead argued that it is better to try to learn a map between a parameter space x and an example space y of correspondence vectors that define corresponding features across the example images.
Reference: [8] <author> S. M. Seitz and C. R. Dyer. </author> <title> Physically-valid view synthesis by image interpolation. </title> <booktitle> In Proc. IEEE Workshop on the Representation of Visual Scenes, </booktitle> <pages> pages 18-25, </pages> <address> Cambridge, Massachusetts, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: In this work, we adopt an image-based facial modelling approach, in which we model facial movements using example images. In doing do we are motivated by the fact that a number of researchers ([2], [3], <ref> [8] </ref>, [9]) have noticed the viability of a view interpolation approach to image synthesis, where novel, intermediate images of a scene are synthesized from example endpoints using a morphing technique.
Reference: [9] <author> T. Werner, R. D. Hersch, and V. Hlavac. </author> <title> Rendering real-world objects using view interpolation. </title> <booktitle> In Proc. of International Conference on Computer Vision, </booktitle> <pages> pages 957-962, </pages> <month> June </month> <year> 1995. </year> <title> the 9-example 3-by-3 pose network shown in 14-example network that can synthesize eye, pose, and mouth movement. the 14-example network used in Figure 7. the sequence in Figure 5. The x marks denote the positions of the 9 examples in pose space. Affine parameters are not shown. activity occurs in the mouth parameter, which denotes degree of openness. activity occurs in the eyes x and y parameters. </title>
Reference-contexts: In this work, we adopt an image-based facial modelling approach, in which we model facial movements using example images. In doing do we are motivated by the fact that a number of researchers ([2], [3], [8], <ref> [9] </ref>) have noticed the viability of a view interpolation approach to image synthesis, where novel, intermediate images of a scene are synthesized from example endpoints using a morphing technique.
References-found: 9

