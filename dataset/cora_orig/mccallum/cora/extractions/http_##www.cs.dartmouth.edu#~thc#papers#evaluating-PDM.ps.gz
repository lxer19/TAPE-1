URL: http://www.cs.dartmouth.edu/~thc/papers/evaluating-PDM.ps.gz
Refering-URL: http://www.cs.dartmouth.edu/~thc/papers.html
Root-URL: http://www.cs.dartmouth.edu
Email: hersheyg@cs.dartmouth.edu  
Title: Early Experiences in Evaluating the Parallel Disk Model with the ViC* Implementation ViC* run-time library.
Author: Thomas H. Cormen Melissa Hirschl 
Address: fthc,  
Affiliation: Dartmouth College Department of Computer Science  
Date: (Revised September 1996)  
Note: PCSTR96-293  The software interface to the PDM is part of the  
Abstract: Dartmouth College Computer Science Technical Report Abstract Although several algorithms have been developed for the Parallel Disk Model (PDM), few have been implemented. Consequently, little has been known about the accuracy of the PDM in measuring I/O time and total running time to perform an out-of-core computation. This paper analyzes timing results on multiple-disk platforms for two PDM algorithms, out-of-core radix sort and BMMC permutations, to determine the strengths and weaknesses of the PDM. The results indicate the following. First, good PDM algorithms are usually not I/O bound. Second, of the four PDM parameters, one (problem size) is a good indicator of I/O time and running time, one (memory size) is a good indicator of I/O time but not necessarily running time, and the other two (block size and number of disks) do not necessarily indicate either I/O or running time. Third, because PDM algorithms tend not to be I/O bound, using asynchronous I/O can reduce I/O wait times significantly. 
Abstract-found: 1
Intro-found: 1
Reference: [Arg95] <author> Lars Arge. </author> <title> The buffer tree: A new technique for optimal I/O-algorithms. </title> <booktitle> In 4th International Workshop on Algorithms and Data Structures (WADS), </booktitle> <pages> pages 334-345, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: The number of disk accesses, however, can be minimized by carefully designed algorithms. Optimal algorithms have appeared in the literature for fundamental problems such as sorting <ref> [Arg95, BGV96, NV93, NV95, VS94] </ref>, general permutations [VS94], and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. <p> This section describes the out-of-core radix sort algorithm and our implementation of it. This description is oriented toward a uniprocessor with multiple disks. Several other sorting algorithms for the PDM have appeared in the literature <ref> [Arg95, BGV96, NV93, NV95, VS94] </ref>. These use independent I/O and are asymptotically optimal. By contrast, our radix sort algorithm is suboptimal but it uses only striped I/O.
Reference: [AV88] <author> Alok Aggarwal and Jeffrey Scott Vitter. </author> <title> The input/output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Vitter and Shriver, followed by others, showed an upper bound of fi BD lg (M=B) parallel I/Os for sorting. This bound is asymptotically tight, because it matches the lower bounds proven earlier by Aggarwal and Vitter <ref> [AV88] </ref> using a model with one disk and D independent read/write heads, which is at least as powerful as the PDM. The quantity fi lg (N=B) represents the number of passes over the data required to sort.
Reference: [AVV95] <author> Lars Arge, Darren Erik Vengroff, and Jeffrey Scott Vitter. </author> <title> External-memory algorithms for processing line segments in geographic information systems. </title> <editor> In Paul Spi-rakis, editor, </editor> <booktitle> Proceedings of the Third Annual European Symposium on Algorithms (ESA '95), volume 979 of Lecture Notes in Computer Science, </booktitle> <pages> pages 295-310. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: Optimal algorithms have appeared in the literature for fundamental problems such as sorting [Arg95, BGV96, NV93, NV95, VS94], general permutations [VS94], and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems <ref> [AVV95, GTVV93] </ref>, and graph algorithms [CGG + 95]. We place some restrictions on the PDM parameters. We assume that B, D, M , and N are exact powers of 2.
Reference: [BGV96] <author> Rakesh D. Barve, Edward F. Grove, and Jeffrey Scott Vitter. </author> <title> Simple randomized mergesort for parallel disks. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 109-118, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The number of disk accesses, however, can be minimized by carefully designed algorithms. Optimal algorithms have appeared in the literature for fundamental problems such as sorting <ref> [Arg95, BGV96, NV93, NV95, VS94] </ref>, general permutations [VS94], and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. <p> This section describes the out-of-core radix sort algorithm and our implementation of it. This description is oriented toward a uniprocessor with multiple disks. Several other sorting algorithms for the PDM have appeared in the literature <ref> [Arg95, BGV96, NV93, NV95, VS94] </ref>. These use independent I/O and are asymptotically optimal. By contrast, our radix sort algorithm is suboptimal but it uses only striped I/O.
Reference: [CB95] <author> Thomas H. Cormen and Kristin Bruhl. </author> <title> Don't be too clever: Routing BMMC permutations on the MasPar MP-2. </title> <booktitle> In Proceedings of the 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 288-297, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: As we have just noted, that exclusive-or operation takes fi (1) time. Moreover, we can compute which column to use in only fi (1) amortized time; see <ref> [CB95] </ref> for details. Thus, we spend only fi (1) amortized time per record in each pass. 12 5 Radix sort for the PDM The other PDM algorithm that we have implemented and measured is an out-of-core, bucketized radix sort.
Reference: [CGG + 95] <author> Yi-Jen Chiang, Michael T. Goodrich, Edward F. Grove, Roberto Tamassia, Dar-ren Erik Vengroff, and Jeffrey Scott Vitter. </author> <title> External-memory graph algorithms. </title> <booktitle> In Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 139-149, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: appeared in the literature for fundamental problems such as sorting [Arg95, BGV96, NV93, NV95, VS94], general permutations [VS94], and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms <ref> [CGG + 95] </ref>. We place some restrictions on the PDM parameters. We assume that B, D, M , and N are exact powers of 2. In order for the memory to accomodate the records transferred in a parallel I/O operation to all D disks, we require that BD M .
Reference: [CGK + 88] <author> Peter Chen, Garth Gibson, Randy H. Katz, David A. Patterson, and Martin Schulze. </author> <title> Two papers on RAIDs. </title> <type> Technical Report UCB/CSD 88/479, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, </institution> <month> December </month> <year> 1988. </year>
Reference-contexts: It does not specify how many processors there are, nor how they are connected, and it does not distinguish between shared and distributed 1 A block might consist of several sectors of a physical device or, in the case of RAID <ref> [CGK + 88, Gib92, PGK88] </ref>, sectors from several physical devices. 3 of records on each disk. The records on each disk are partitioned into blocks of B records each (not shown here). Disk I/O transfers records between disks and memory that can hold M records.
Reference: [CLR90] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Radix sort has the further advantage that it is considerably simpler to implement than any of the optimal sorting algorithms. Summary of the bucketized radix sort algorithm for the PDM Our out-of-core, bucketized radix sort algorithm bears some similarity to the usual in-core radix sort method (see <ref> [CLR90, pp. 178-179] </ref> for a related algorithm). We make a number of passes over the data, where each pass examines K sort-key bits. We shall show in a moment how to choose the value of K.
Reference: [CN96] <author> Thomas H. Cormen and David M. Nicol. </author> <title> Performing out-of-core FFTs on parallel disk systems. </title> <type> Technical Report PCS-TR96-294, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> August </month> <year> 1996. </year> <month> 29 </month>
Reference-contexts: Moreover, these algorithms are useful; for example, the BMMC permutation code is at the heart of an efficient out-of-core Fast Fourier Transform implementation <ref> [CN96] </ref>. The PDM is designed to measure I/O complexity. <p> Optimal algorithms have appeared in the literature for fundamental problems such as sorting [Arg95, BGV96, NV93, NV95, VS94], general permutations [VS94], and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform <ref> [CN96, VS94] </ref>, matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. We place some restrictions on the PDM parameters. We assume that B, D, M , and N are exact powers of 2. <p> After all, in-core algorithms are not designed to make efficient use of multiple disks or of disk blocks. Moreover, RAIDs improve bandwidth but not latency. In-core algorithms on native virtual memory have one advantage, however: reduced file-system overhead. Based on out-of-core FFT experiments in <ref> [CN96] </ref>, we doubt that this advantage is enough to overcome the efficiency of PDM algorithms. Acknowledgments We thank Garth Gibson and Jim Zelenka for their help in implementing ViC* on top of SPFS; they made many changes to SPFS at our request.
Reference: [Cor92] <author> Thomas H. Cormen. </author> <title> Virtual Memory for Data-Parallel Computing. </title> <type> PhD thesis, </type> <institution> De--partment of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <year> 1992. </year> <note> Available as Technical Report MIT/LCS/TR-559. </note>
Reference-contexts: The number of disk accesses, however, can be minimized by carefully designed algorithms. Optimal algorithms have appeared in the literature for fundamental problems such as sorting [Arg95, BGV96, NV93, NV95, VS94], general permutations [VS94], and structured permutations <ref> [Cor92, Cor93, Wis96] </ref>, as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. We place some restrictions on the PDM parameters.
Reference: [Cor93] <author> Thomas H. Cormen. </author> <title> Fast permuting in disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):41-57, January and February 1993. </note>
Reference-contexts: The number of disk accesses, however, can be minimized by carefully designed algorithms. Optimal algorithms have appeared in the literature for fundamental problems such as sorting [Arg95, BGV96, NV93, NV95, VS94], general permutations [VS94], and structured permutations <ref> [Cor92, Cor93, Wis96] </ref>, as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. We place some restrictions on the PDM parameters.
Reference: [CSW94] <author> Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <type> Technical Report PCS-TR94-223, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1994. </year> <note> Preliminary version appeared in Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures. Revised version to appear in SIAM Journal on Computing. </note>
Reference-contexts: This paper presents our early experiences in evaluating how accurate a model the PDM is, based on two sophisticated algorithms for the PDM (radix sort and performing BMMC permutations <ref> [CSW94] </ref>) that we implemented with the ViC* run-time library. We ran these algorithms on two platforms: a uniprocessor with eight disks and a network of eight workstations. To evaluate any computational model, one needs an implementation of that model and representative programs for the model. <p> Moreover, we wanted to use one that requires independent I/O for optimal 9 performance. (If all parallel I/O operations are striped so that none are independent, then a RAID level 3 disk organization is efficient.) We chose the BMMC permutation algorithm of <ref> [CSW94] </ref>. This section defines the class of BMMC permutations, summarizes the BMMC permutation algorithm, and describes some issues in the implementation of the algorithm. BMMC permutations Any permutation is defined by a bijection of a set f0; 1; : : : ; N 1g onto itself. <p> That is, we perform the permutations characterized by the factors of a matrix from right to left. Summary of the BMMC algorithm for the PDM Each factor produced by the BMMC algorithm of <ref> [CSW94] </ref> characterizes a restricted form of BMMC permutation that can be performed in one pass over the data. (We refer the reader to [CSW94] 4 Matrix multiplication over GF (2) is like standard matrix multiplication over the reals but with all arithmetic performed modulo 2. <p> Summary of the BMMC algorithm for the PDM Each factor produced by the BMMC algorithm of <ref> [CSW94] </ref> characterizes a restricted form of BMMC permutation that can be performed in one pass over the data. (We refer the reader to [CSW94] 4 Matrix multiplication over GF (2) is like standard matrix multiplication over the reals but with all arithmetic performed modulo 2. <p> All processors store a copy of the characteristic matrix for the MLD permutation, which means that by agreeing upon the order in which records are sent, only the records|and not their source or target indices|need be sent. This optimization saves on network bandwidth. Using factoring techniques like those in <ref> [CSW94] </ref>, we order the interprocessor communication into rounds in which each sending processor sends to a unique receiving processor. We also designed a technique whereby the two processors agree on the order of the source indices of the records in the transmitted buffer.
Reference: [GBD + 94] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weichang Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM: Parallel Virtual Machine|A User's Guide and Tutorial for Networked Parallel Computing. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The boxes at the top of the figure denote these large sets of functions. Both in-core and out-of-core functions will require interprocessor communication. We have defined a set of macros for interprocessor communication and implemented them for three underlying communication models: MPI [GLS94, SOHL + 96], PVM <ref> [GBD + 94] </ref>, and a uniprocessor. Switching between models is as simple as recompiling with a different macro set and relinking with a different library. The ViC* API A more interesting question is how to implement the PDM abstraction in a portable fashion.
Reference: [Gib92] <author> Garth A. Gibson. </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year> <note> Also available as Technical Report UCB/CSD 91/613, </note> <institution> Computer Science Division (EECS), University of California, Berkeley, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: It does not specify how many processors there are, nor how they are connected, and it does not distinguish between shared and distributed 1 A block might consist of several sectors of a physical device or, in the case of RAID <ref> [CGK + 88, Gib92, PGK88] </ref>, sectors from several physical devices. 3 of records on each disk. The records on each disk are partitioned into blocks of B records each (not shown here). Disk I/O transfers records between disks and memory that can hold M records.
Reference: [GLS94] <author> William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The boxes at the top of the figure denote these large sets of functions. Both in-core and out-of-core functions will require interprocessor communication. We have defined a set of macros for interprocessor communication and implemented them for three underlying communication models: MPI <ref> [GLS94, SOHL + 96] </ref>, PVM [GBD + 94], and a uniprocessor. Switching between models is as simple as recompiling with a different macro set and relinking with a different library. The ViC* API A more interesting question is how to implement the PDM abstraction in a portable fashion.
Reference: [GSC + 95] <author> Garth A. Gibson, Daniel Stodolsky, Fay W. Chang, William V. Courtright II, Chris G. Demetriou, Eka Ginting, Mark Holland, Qingming Ma, LeAnn Neal, R. Hugo Patter-son, Jiawen Su, Rachad Youssef, and Jim Zelenka. </author> <title> The Scotch parallel storage systems. </title> <booktitle> In Proceedings of the IEEE CompCon Conference, </booktitle> <pages> pages 403-410, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: As Figure 3 shows, a particular implementation of the ViC* API will be as a set of wrapper functions on top of an existing file system interface. We have already implemented it on three systems: * Scotch Parallel File System (SPFS version 1.5) <ref> [GSC + 95] </ref>: SPFS uses a client-server model in which computing processes are clients and servers provide access to storage. It provides a Unix-like linear view of each parallel file.
Reference: [GTVV93] <author> Michael T. Goodrich, Jyh-Jong Tsay, Darren E. Vengroff, and Jeffrey Scott Vitter. </author> <title> External-memory computational geometry. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 714-723, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Optimal algorithms have appeared in the literature for fundamental problems such as sorting [Arg95, BGV96, NV93, NV95, VS94], general permutations [VS94], and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems <ref> [AVV95, GTVV93] </ref>, and graph algorithms [CGG + 95]. We place some restrictions on the PDM parameters. We assume that B, D, M , and N are exact powers of 2.
Reference: [MW95] <author> Sean S. B. Moore and Leonard F. Wisniewski. </author> <title> Complexity analysis of two permutations used by fast cosine transforms. </title> <type> Technical Report PCS-TR95-266, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: Matrix transpose (with power-of-2 dimensions), bit reversal (used in performing FFTs), vector reversal, and matrix reblocking are all BPC, and hence BMMC, permutations. Gray-code permutations, inverse Gray-code permutations, and permutations used by fast cosine transforms <ref> [MW95] </ref> are BMMC (but not BPC). BMMC permutations are closed under composition so that, for example, the composition of bit-reversal and Gray-code permutations is BMMC. We generally focus on the matrix-multiplication portion of BMMC permutations rather than on the complement vector.
Reference: [NK96a] <author> Nils Nieuwejaar and David Kotz. </author> <title> The Galley parallel file system. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <pages> pages 374-381, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: It provides a Unix-like linear view of each parallel file. SPFS runs on a network of workstations. * Galley File System <ref> [NK96a, NK96b] </ref>: Like SPFS, Galley is a parallel file system that uses a client-server model. Unlike SPFS, Galley allows applications a more sophisticated view of the parallel file system. In particular, 6 its interface provides for access to specific disks.
Reference: [NK96b] <author> Nils Nieuwejaar and David Kotz. </author> <title> Performance of the Galley parallel file system. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 83-94, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: It provides a Unix-like linear view of each parallel file. SPFS runs on a network of workstations. * Galley File System <ref> [NK96a, NK96b] </ref>: Like SPFS, Galley is a parallel file system that uses a client-server model. Unlike SPFS, Galley allows applications a more sophisticated view of the parallel file system. In particular, 6 its interface provides for access to specific disks.
Reference: [NV93] <author> Mark H. Nodine and Jeffrey Scott Vitter. </author> <title> Deterministic distribution sort in shared and distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 120-129, </pages> <month> June </month> <year> 1993. </year> <month> 30 </month>
Reference-contexts: The number of disk accesses, however, can be minimized by carefully designed algorithms. Optimal algorithms have appeared in the literature for fundamental problems such as sorting <ref> [Arg95, BGV96, NV93, NV95, VS94] </ref>, general permutations [VS94], and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. <p> This section describes the out-of-core radix sort algorithm and our implementation of it. This description is oriented toward a uniprocessor with multiple disks. Several other sorting algorithms for the PDM have appeared in the literature <ref> [Arg95, BGV96, NV93, NV95, VS94] </ref>. These use independent I/O and are asymptotically optimal. By contrast, our radix sort algorithm is suboptimal but it uses only striped I/O.
Reference: [NV95] <author> Mark H. Nodine and Jeffrey Scott Vitter. </author> <title> Greed sort: Optimal deterministic sorting on parallel disks. </title> <journal> Journal of the ACM, </journal> <volume> 42(4) </volume> <pages> 919-933, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The number of disk accesses, however, can be minimized by carefully designed algorithms. Optimal algorithms have appeared in the literature for fundamental problems such as sorting <ref> [Arg95, BGV96, NV93, NV95, VS94] </ref>, general permutations [VS94], and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. <p> This section describes the out-of-core radix sort algorithm and our implementation of it. This description is oriented toward a uniprocessor with multiple disks. Several other sorting algorithms for the PDM have appeared in the literature <ref> [Arg95, BGV96, NV93, NV95, VS94] </ref>. These use independent I/O and are asymptotically optimal. By contrast, our radix sort algorithm is suboptimal but it uses only striped I/O.
Reference: [PGK88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM International Conference on Management of Data (SIGMOD), </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: It does not specify how many processors there are, nor how they are connected, and it does not distinguish between shared and distributed 1 A block might consist of several sectors of a physical device or, in the case of RAID <ref> [CGK + 88, Gib92, PGK88] </ref>, sectors from several physical devices. 3 of records on each disk. The records on each disk are partitioned into blocks of B records each (not shown here). Disk I/O transfers records between disks and memory that can hold M records.
Reference: [SOHL + 96] <author> Marc Snir, Steve W. Otto, Steven Huss-Lederman, David W. Walker, and Jack Don-garra. </author> <title> MPI: The Complete Reference. </title> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: The boxes at the top of the figure denote these large sets of functions. Both in-core and out-of-core functions will require interprocessor communication. We have defined a set of macros for interprocessor communication and implemented them for three underlying communication models: MPI <ref> [GLS94, SOHL + 96] </ref>, PVM [GBD + 94], and a uniprocessor. Switching between models is as simple as recompiling with a different macro set and relinking with a different library. The ViC* API A more interesting question is how to implement the PDM abstraction in a portable fashion.
Reference: [SW95] <author> Elizabeth A. M. Shriver and Leonard F. Wisniewski. </author> <title> An API for choreographing data accesses. </title> <type> Technical Report PCS-TR95-267, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: We believe, however, that it will be useful in writing the ViC* run-time library. The ViC* compiler will produce direct calls to it as well. The ViC* API is derived from two other interfaces. The primary influence is the Whiptail File System API <ref> [SW95] </ref>, which was also designed to support the PDM. A secondary influence is the low-level file-system API currently being developed by the Scalable I/O (SIO) Initiative Working Group on Operating Systems.
Reference: [TMC93] <institution> Thinking Machines Corporation. </institution> <note> C* Programming Guide, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Although a variety of PDM algorithms have appeared in the literature, there have been few implementations of any of them. (A notable exception is Vengroff's TPIE project [Ven94, Ven97].) The ViC* project at Dartmouth will implement virtual memory for the data-parallel language C* <ref> [TMC93] </ref> with the PDM as its underlying abstract disk model. ViC* contains two major components: a compiler that takes a C* program with some variables declared outofcore and produces a C program with explicit I/O and library calls, and a run-time library that implements the I/O and library calls.
Reference: [Val90] <author> Leslie G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Future work These results suggest two research directions. First, can we develop a model that more accurately models I/O, computation, and communication, yet does not overwhelm the algorithm designer with parameters? Perhaps a hybrid of the PDM and Bulk Synchronous Processing (BSP) models <ref> [Val90] </ref> would be suitable. Or perhaps any accurate model would be too complex to design algorithms on.
Reference: [Ven94] <author> Darren Erik Vengroff. </author> <title> A transparent parallel I/O environment. </title> <booktitle> In Proceedings of the DAGS '94 Symposium, </booktitle> <pages> pages 117-134, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: To appear in Parallel Computing. 1 stored on multiple disks. Although a variety of PDM algorithms have appeared in the literature, there have been few implementations of any of them. (A notable exception is Vengroff's TPIE project <ref> [Ven94, Ven97] </ref>.) The ViC* project at Dartmouth will implement virtual memory for the data-parallel language C* [TMC93] with the PDM as its underlying abstract disk model.
Reference: [Ven97] <author> Darren Erik Vengroff. </author> <title> The Theory and Practice of I/O-Efficient Computation. </title> <type> PhD thesis, </type> <institution> Brown University Department of Computer Science, </institution> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: To appear in Parallel Computing. 1 stored on multiple disks. Although a variety of PDM algorithms have appeared in the literature, there have been few implementations of any of them. (A notable exception is Vengroff's TPIE project <ref> [Ven94, Ven97] </ref>.) The ViC* project at Dartmouth will implement virtual memory for the data-parallel language C* [TMC93] with the PDM as its underlying abstract disk model. <p> Several other sorting algorithms for the PDM have appeared in the literature [Arg95, BGV96, NV93, NV95, VS94]. These use independent I/O and are asymptotically optimal. By contrast, our radix sort algorithm is suboptimal but it uses only striped I/O. Vengroff <ref> [Ven97] </ref> has observed that for realistic ranges of PDM parameters, asymptotically optimal PDM algorithms often require more parallel I/Os than do suboptimal algorithms. This is because the asymptotically optimal sorting algorithms tend to have high constant factors. The constant factors for radix sort, on the other hand, are relatively low.
Reference: [VS90] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Optimal disk I/O with parallel block transfer. </title> <booktitle> In Proceedings of the Twenty Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 159-169, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Since its introduction in 1990, the Parallel Disk Model (PDM) of Vitter and Shriver <ref> [VS90, VS94] </ref> has become the predominant model for developing and analyzing algorithms that operate on data fl Supported in part by funds from Dartmouth College and in part by the National Science Foundation under Grants CCR-9308667 and CCR-9625894.
Reference: [VS94] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <journal> Algorithmica, </journal> 12(2/3):110-147, August and September 1994. 
Reference-contexts: 1 Introduction Since its introduction in 1990, the Parallel Disk Model (PDM) of Vitter and Shriver <ref> [VS90, VS94] </ref> has become the predominant model for developing and analyzing algorithms that operate on data fl Supported in part by funds from Dartmouth College and in part by the National Science Foundation under Grants CCR-9308667 and CCR-9625894. <p> The number of disk accesses, however, can be minimized by carefully designed algorithms. Optimal algorithms have appeared in the literature for fundamental problems such as sorting <ref> [Arg95, BGV96, NV93, NV95, VS94] </ref>, general permutations [VS94], and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. <p> The number of disk accesses, however, can be minimized by carefully designed algorithms. Optimal algorithms have appeared in the literature for fundamental problems such as sorting [Arg95, BGV96, NV93, NV95, VS94], general permutations <ref> [VS94] </ref>, and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. We place some restrictions on the PDM parameters. <p> Optimal algorithms have appeared in the literature for fundamental problems such as sorting [Arg95, BGV96, NV93, NV95, VS94], general permutations [VS94], and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform <ref> [CN96, VS94] </ref>, matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. We place some restrictions on the PDM parameters. We assume that B, D, M , and N are exact powers of 2. <p> Optimal algorithms have appeared in the literature for fundamental problems such as sorting [Arg95, BGV96, NV93, NV95, VS94], general permutations <ref> [VS94] </ref>, and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. We place some restrictions on the PDM parameters. We assume that B, D, M , and N are exact powers of 2. <p> This section describes the out-of-core radix sort algorithm and our implementation of it. This description is oriented toward a uniprocessor with multiple disks. Several other sorting algorithms for the PDM have appeared in the literature <ref> [Arg95, BGV96, NV93, NV95, VS94] </ref>. These use independent I/O and are asymptotically optimal. By contrast, our radix sort algorithm is suboptimal but it uses only striped I/O.
Reference: [WGWR93] <author> David Womble, David Greenberg, Stephen Wheat, and Rolf Riesen. </author> <title> Beyond core: Making parallel computer I/O practical. </title> <booktitle> In DAGS '93, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Optimal algorithms have appeared in the literature for fundamental problems such as sorting [Arg95, BGV96, NV93, NV95, VS94], general permutations [VS94], and structured permutations [Cor92, Cor93, Wis96], as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition <ref> [WGWR93] </ref>, computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. We place some restrictions on the PDM parameters. We assume that B, D, M , and N are exact powers of 2.
Reference: [Wis96] <author> Leonard F. Wisniewski. </author> <title> Structured permuting in place on parallel disk systems. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on I/O in Parallel and Distributed Systems (IOPADS), </booktitle> <pages> pages 128-139, </pages> <month> May </month> <year> 1996. </year> <month> 31 </month>
Reference-contexts: The number of disk accesses, however, can be minimized by carefully designed algorithms. Optimal algorithms have appeared in the literature for fundamental problems such as sorting [Arg95, BGV96, NV93, NV95, VS94], general permutations [VS94], and structured permutations <ref> [Cor92, Cor93, Wis96] </ref>, as well as higher-level domains such as Fast Fourier transform [CN96, VS94], matrix-matrix multiplication [VS94], LUP decomposition [WGWR93], computational geometry problems [AVV95, GTVV93], and graph algorithms [CGG + 95]. We place some restrictions on the PDM parameters.
References-found: 33

