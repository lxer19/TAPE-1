URL: http://www.cs.arizona.edu/sio/sio.ps
Refering-URL: http://www.cs.arizona.edu/sio/
Root-URL: http://www.cs.arizona.edu
Title: Operating System Support for High-Performance Parallel I/O Systems Scalable I/O Operating Systems Working Group of
Author: Brian Bershad, David Black, David DeWitt, Garth Gibson, Kai Li, Larry Peterson, Marc Snir, 
Note: Initiative Working Paper No. 4  
Affiliation: University of Washington  Open Software Foundation Research Institute  University of Wisconsin  Carnegie Mellon University  Princeton University  University of Arizona  IBM Watson Research  
Abstract: This document describes the operating system support component in the Scalable I/O Initiative. Our efforts cover three critical areas of scalable, parallel I/O for high-performance multicomputers: networking, memory management, and file and object store. We are leveraging off on-going research results in these three areas. Our main efforts in the scalable I/O initiative will be directed towards hardening, porting, tuning, and deploying our technology on the target platforms.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jose Brustoloni and Brian N. Bershad. </author> <title> Protocol Processing for High-bandwidth Low-latency Networks. </title> <note> Available as a CMU-CS Technical report CMU-CS-93-132. </note> <month> April </month> <year> 1993. </year>
Reference-contexts: Recently, work at CMU, and now at the University of Washington, has focussed on improving the performance of networking protocols within the context of the Mach operating system <ref> [16, 15, 1] </ref>. * OSF/1 AD from the Open Software Foundation [20]. AD extends Mach to provide single-node semantics for a distributed memory multicomputer such as the Intel Paragon. * The x-kernel from the University of Arizona [6, 17]. The x-kernel provides a platform-independent framework for implementing network protocols. <p> As two styles of communication must be accommodated, we propose using two distinct solutions. Unprotected Communication For communication within a logical process, we will adapt a set of low-overhead networking protocols that we have developed for local-area ATM (Asynchronous Transfer Mode) networks <ref> [1] </ref>. <p> We have implemented our protocol architecture on a switch-based ATM network consisting of DECstation 5000/200 workstations running the Mach 3.0 operating system. Our implementation achieves latencies and bandwidths close to the physical limitations imposed by the hardware, yet offers applications a high-level reliable transport interface <ref> [1] </ref>. Currently, these protocols run over a switch-based ATM network. The task of adapting them to the Paragon mesh would be a part of this contract.
Reference: [2] <author> Michael Carey et al. </author> <title> Shoring Up Persistent Applications. </title> <booktitle> Submitted to the 1994 ACM SIGMOD Conference, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: They currently run on iPSC/860 and are targeted to run on the Intel Paragon. * The SHORE persistent object manager being developed at the University of Wisconsin <ref> [2, 3] </ref>. SHORE provides a language-neutral, scalable persistent object store targeted to run on a variety of hardware platforms from networks of workstations to large parallel processors such as the Intel Paragon and the IBM SP2. <p> In particular, we propose to investigate replacing the conventional file system with a parallel persistent object store such as the one being developed as part of the ARPA-funded Scalable Heterogeneous Object REpository (SHORE) project <ref> [2] </ref>. There are a number of reasons for such an approach. First,the languages for such systems allow programmers to manipulate both transient and persistent data (e.g. a matrix or a complex data structure) in the same way, freeing the programmer from having to do explicit file I/O. <p> We plan on developing a full set of such classes for use in numerical applications. We will use software being developed as part of the SHORE project <ref> [2] </ref> as the basis for this component of the project. The goal of the SHORE is to develop a persistent object system capable of satisfying both object-oriented database system applications (e.g. CAD) as well as traditional legacy applications (e.g. compilers and editors) that currently depend on the Unix file system.
Reference: [3] <author> R. Cattell. </author> <title> The Object Database Standard: ODMG-93. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: They currently run on iPSC/860 and are targeted to run on the Intel Paragon. * The SHORE persistent object manager being developed at the University of Wisconsin <ref> [2, 3] </ref>. SHORE provides a language-neutral, scalable persistent object store targeted to run on a variety of hardware platforms from networks of workstations to large parallel processors such as the Intel Paragon and the IBM SP2. <p> It can run on a single processor, a network of workstations, or a large parallel processor such as the Intel Paragon or IBM SP2. SHORE's type system is based on ODL, a language-neutral, object definition language for persistent object systems that has been defined by the vendor consortium ODMG <ref> [3] </ref>. Typing persistent objects simplifies the task of supporting heterogeneous hardware environments and makes it feasible to support access to persistent objects from multiple programming languages. Currently, support for C++ and Ada are planned. As part of this effort, we will design and implement a language binding for HPF.
Reference: [4] <author> Peter Druschel and Larry L. Peterson. Fbufs: </author> <title> A High-Bandwidth Cross-Domain Transfer Facility. </title> <booktitle> In Proceedings of the 14th Symposium on Operating Systems Principles, </booktitle> <address> pages189-202, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: In other words, by implementing all protocols in the context of the x-kernel, we are able to identify and optimize the critical path. The x-kernel framework has been demonstrated on workstations connected to high-speed networks (FDDI and ATM) <ref> [4] </ref>. We propose to port the x-kernel to OSF/1 AD running on the Intel Paragon, and to tune/optimize its performance in support of the communications protocols described throughout this section. 3.2 Intranetworking Intranetworking is concerned with moving data between processors that are connected to the same logical system.
Reference: [5] <author> G.A. Gibson, R.H. Patterson, and M. Satyanarayanan. </author> <title> Disk Reads with DRAM Latency. </title> <booktitle> In Third Workshop on Workstation Operating Systems, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: High performance application programs generally succeed by explicitly managing asynchronous access or by relying on sequential file prefetching. These schemes are limited by insufficiently large file sizes, by overcommittment of memory resources in the former case, and by access pattern non-sequentiality in the latter case. Informed prefetching <ref> [5, 19] </ref> is an aggressive prefetching mechanism that combats these limitations. Informed prefetching uses hints from programmer or compiler annotations to build a detailed description of future accesses. Because these hint structures can span many small files, high efficiency can be achieved in a wide variety of workloads.
Reference: [6] <author> Norman C. Hutchinson and Larry L. Peterson. </author> <title> The x-kernel: An Architecture for Implementing Network Protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: AD extends Mach to provide single-node semantics for a distributed memory multicomputer such as the Intel Paragon. * The x-kernel from the University of Arizona <ref> [6, 17] </ref>. The x-kernel provides a platform-independent framework for implementing network protocols. It has already been integrated in Mach. * The shared virtual memory [11, 12], memory server [7], and checkpointing tools [13, 14] developed at Princeton University. <p> Much of the work described in this section will be done in the context of the x-kernel protocol implementation framework. 3.1 Protocol Framework We have implemented a protocol framework, called the x-kernel <ref> [6, 17] </ref>, that supports the rapid implementation of efficient network protocols. We have integrated the x-kernel protocol framework into the Mach 3.0 operating system in a way that allows a protocol graph to run across multiple protection domains, including the Mach microkernel, a network server, and application domains.
Reference: [7] <author> Liviu Iftode, Kai Li and Karin Petersen. </author> <title> Memory Servers for Multicomputers. </title> <booktitle> In IEEE COMPCON Spring '93, </booktitle> <pages> pages 538-547, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The x-kernel provides a platform-independent framework for implementing network protocols. It has already been integrated in Mach. * The shared virtual memory [11, 12], memory server <ref> [7] </ref>, and checkpointing tools [13, 14] developed at Princeton University. They currently run on iPSC/860 and are targeted to run on the Intel Paragon. * The SHORE persistent object manager being developed at the University of Wisconsin [2, 3]. <p> In conjunction with the memory servers, the shared virtual memory provides users with large shared virtual memory spaces. 4.2 Memory Servers Memory servers <ref> [7] </ref> for multicomputers allow sequential programs, message-passing programs, and shared virtual memory programs to use the entire physical memory resources effectively. The memory server model extends the memory hierarchy of multicomputers by introducing a remote memory layer whose latency lies somewhere between local memory and disk.
Reference: [8] <author> Intel Supercomputer Systems Division. </author> <title> Using Parallel File I/O. In Chapter 5, Paragon User Guide, </title> <month> November </month> <year> 1993. </year>
Reference-contexts: This is not true in the area of parallel file systems. For example, Intel's PFS tries to preserve the Unix-file system interface. Five different access modes are provided <ref> [8] </ref>, ranging in complexity from having each compute node maintain its own file pointer to a very centralized mode in which a single compute node reads the file and then distributes what it reads to the other compute nodes. Three of the five I/O modes requires a centralized coordinator.
Reference: [9] <author> Intel Supercomputer Systems Division. </author> <title> Parallel File System Performance. In Chapter 4, Paragon System Software Release 1.1 Release Notes for the Paragon XP/S System, </title> <month> November </month> <year> 1993. </year>
Reference-contexts: Parallel File System Benchmark Suite Together with the project component responsible for characterizing I/O characteristics of parallel applications, we will work on developing a comprehensive benchmark suite for parallel file systems. While a limited evaluation of the Intel Parallel File System (PFS) has been conducted <ref> [9] </ref>, no common, comprehensive benchmark for parallel file system performance exists. We propose to develop such a benchmark. Experience in the database system area has proven that such benchmarks are invaluable in obtaining the maximum performance from a system. Such a benchmark will serve several functions.
Reference: [10] <author> C. Lamb, G. Landis, J. Orenstein, and D. Weinreb. </author> <title> The ObjectStore database system. </title> <journal> Communications of the ACM, </journal> <volume> 34(10), </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: In order to make the integration with HPF as transparent as possible, we will study the use of shared virtual memory as an implementation strategy. The use of virtual memory faulting techniques has proven very successful in the implementation of object-oriented database systems <ref> [10] </ref>. These techniques make accessing persistent data transparent to the programmer.
Reference: [11] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transaction on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: AD extends Mach to provide single-node semantics for a distributed memory multicomputer such as the Intel Paragon. * The x-kernel from the University of Arizona [6, 17]. The x-kernel provides a platform-independent framework for implementing network protocols. It has already been integrated in Mach. * The shared virtual memory <ref> [11, 12] </ref>, memory server [7], and checkpointing tools [13, 14] developed at Princeton University. They currently run on iPSC/860 and are targeted to run on the Intel Paragon. * The SHORE persistent object manager being developed at the University of Wisconsin [2, 3]. <p> The techniques include shared virtual memory, remote memory servers, and checkpointing. 4.1 Shared Virtual Memory Shared virtual memory <ref> [11, 12] </ref> implements coherent shared memory on a multicomputer without physically shared memory. The shared virtual memory system presents all processors with a large coherent shared memory address space. Any processor can access any memory location at any time.
Reference: [12] <author> Kai Li and Richard Schaefer. </author> <title> A Hypercube Shared Virtual Memory System. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 125-132, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: AD extends Mach to provide single-node semantics for a distributed memory multicomputer such as the Intel Paragon. * The x-kernel from the University of Arizona [6, 17]. The x-kernel provides a platform-independent framework for implementing network protocols. It has already been integrated in Mach. * The shared virtual memory <ref> [11, 12] </ref>, memory server [7], and checkpointing tools [13, 14] developed at Princeton University. They currently run on iPSC/860 and are targeted to run on the Intel Paragon. * The SHORE persistent object manager being developed at the University of Wisconsin [2, 3]. <p> The techniques include shared virtual memory, remote memory servers, and checkpointing. 4.1 Shared Virtual Memory Shared virtual memory <ref> [11, 12] </ref> implements coherent shared memory on a multicomputer without physically shared memory. The shared virtual memory system presents all processors with a large coherent shared memory address space. Any processor can access any memory location at any time.
Reference: [13] <author> Kai Li, Jeffrey F. Naughton, and James S. Plank. </author> <title> A Real-Time, Concurrent Checkpoint and Recovery Algorithm For Parallel Programs. </title> <booktitle> In Proceedings of the 1990 ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, Washington, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: The x-kernel provides a platform-independent framework for implementing network protocols. It has already been integrated in Mach. * The shared virtual memory [11, 12], memory server [7], and checkpointing tools <ref> [13, 14] </ref> developed at Princeton University. They currently run on iPSC/860 and are targeted to run on the Intel Paragon. * The SHORE persistent object manager being developed at the University of Wisconsin [2, 3]. <p> Similarly, existing checkpointing systems work by halting the entire application during the construction of the checkpoint. This stoppage can have a substantial negative impact on the total execution time. Our research in the area of checkpointing has resulted in a set of algorithms for generating compact, low-latency checkpoints <ref> [13] </ref>. Checkpoints are compact because only the data changed since the previous checkpoint must be written to stable store (as opposed to the entire image). Checkpoints have low-latency because they are generated concurrently during the program's execution.
Reference: [14] <author> Kai Li, Jeffrey F. Naughton, and James S. Plank. </author> <title> An Efficient Checkpointing Method for Multicomputers with Wormhole Routing. </title> <journal> In International Journal of Parallel Programming, </journal> <volume> 20(3) </volume> <pages> 159-180, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The x-kernel provides a platform-independent framework for implementing network protocols. It has already been integrated in Mach. * The shared virtual memory [11, 12], memory server [7], and checkpointing tools <ref> [13, 14] </ref> developed at Princeton University. They currently run on iPSC/860 and are targeted to run on the Intel Paragon. * The SHORE persistent object manager being developed at the University of Wisconsin [2, 3]. <p> Distinct memory address spaces on MPPs make it difficult to stop a program in a consistent state, because there may be asynchronous messages still in transit through the network while the snapshot is being taken. The synchronization methods developed at Princeton and the University of Wisconsin <ref> [14] </ref> require a minimal number of synchronization messages and loggings for MPPs based on static wormhole routing networks. 5 File and Object Stores This component is comprised of two major tasks including defining and implementing a standard applications interface to both the Intel Parallel File System (PFS) and the IBM Vesta
Reference: [15] <author> Chris Maeda and Brian N. Bershad. </author> <title> Networking Performance for Microkernels. </title> <booktitle> In Proceedings of the Third Workshop on Workstation Operating Systems, </booktitle> <pages> pages 154-159, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Recently, work at CMU, and now at the University of Washington, has focussed on improving the performance of networking protocols within the context of the Mach operating system <ref> [16, 15, 1] </ref>. * OSF/1 AD from the Open Software Foundation [20]. AD extends Mach to provide single-node semantics for a distributed memory multicomputer such as the Intel Paragon. * The x-kernel from the University of Arizona [6, 17]. The x-kernel provides a platform-independent framework for implementing network protocols.
Reference: [16] <author> Chris Maeda and Brian N. Bershad. </author> <title> Protocol Service Decomposition for High-Performance Networking. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 244-255, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Recently, work at CMU, and now at the University of Washington, has focussed on improving the performance of networking protocols within the context of the Mach operating system <ref> [16, 15, 1] </ref>. * OSF/1 AD from the Open Software Foundation [20]. AD extends Mach to provide single-node semantics for a distributed memory multicomputer such as the Intel Paragon. * The x-kernel from the University of Arizona [6, 17]. The x-kernel provides a platform-independent framework for implementing network protocols.
Reference: [17] <author> Sean W. O'Malley and Larry L. Peterson. </author> <title> A Dynamic Network Architecture. </title> <journal> In ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 110-143, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: AD extends Mach to provide single-node semantics for a distributed memory multicomputer such as the Intel Paragon. * The x-kernel from the University of Arizona <ref> [6, 17] </ref>. The x-kernel provides a platform-independent framework for implementing network protocols. It has already been integrated in Mach. * The shared virtual memory [11, 12], memory server [7], and checkpointing tools [13, 14] developed at Princeton University. <p> Much of the work described in this section will be done in the context of the x-kernel protocol implementation framework. 3.1 Protocol Framework We have implemented a protocol framework, called the x-kernel <ref> [6, 17] </ref>, that supports the rapid implementation of efficient network protocols. We have integrated the x-kernel protocol framework into the Mach 3.0 operating system in a way that allows a protocol graph to run across multiple protection domains, including the Mach microkernel, a network server, and application domains.
Reference: [18] <author> Hilarie Orman, Ed Menze, Sean O'Malley and Larry Peterson. </author> <title> A Fast and General Implementation of Mach IPC on a Network. </title> <booktitle> In Proceedings of the Mach Usenix Symposium, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: Various protocol suites, including TCP/IP and Mach IPC have been implemented in the x-kernel <ref> [18] </ref>. The x-kernel provides protocol implementors with an interface that completely hides the details of the rest of the system. <p> The task of adapting them to the Paragon mesh would be a part of this contract. Protected Communication For protected communication, such as that employed by client-server interactions, we propose utilizing technology from the x -kernel's implementation of Mach IPC on loosely-coupled networks <ref> [18] </ref> to improve NORMA Inter Process Communication (IPC) for the Paragon and other platforms. Mach IPC is the communication abstraction of the Mach operating system.
Reference: [19] <author> R.H. Patterson, G.A. Gibson, M. Satyanarayanan. </author> <title> Status Report on Transparent Informed Prefetching. </title> <booktitle> In Operating Systems Review, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: High performance application programs generally succeed by explicitly managing asynchronous access or by relying on sequential file prefetching. These schemes are limited by insufficiently large file sizes, by overcommittment of memory resources in the former case, and by access pattern non-sequentiality in the latter case. Informed prefetching <ref> [5, 19] </ref> is an aggressive prefetching mechanism that combats these limitations. Informed prefetching uses hints from programmer or compiler annotations to build a detailed description of future accesses. Because these hint structures can span many small files, high efficiency can be achieved in a wide variety of workloads.
Reference: [20] <author> Roman Zajcew, Paul Roy, David Black, Chris Peak, Paulo Guedes, Bradford Kemp, John LoVerso, Michael Leibensperger, Michael Barnett, Faramarz Rabii, and Durriya Netterwala. </author> <title> An OSF/1 Unix for Massively Parallel Multicomputers. </title> <booktitle> In Proceedings of Winter 1993 Usenix Technical Conference, </booktitle> <address> San Diego, </address> <pages> Pages 449-468, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Recently, work at CMU, and now at the University of Washington, has focussed on improving the performance of networking protocols within the context of the Mach operating system [16, 15, 1]. * OSF/1 AD from the Open Software Foundation <ref> [20] </ref>. AD extends Mach to provide single-node semantics for a distributed memory multicomputer such as the Intel Paragon. * The x-kernel from the University of Arizona [6, 17]. The x-kernel provides a platform-independent framework for implementing network protocols.
References-found: 20

