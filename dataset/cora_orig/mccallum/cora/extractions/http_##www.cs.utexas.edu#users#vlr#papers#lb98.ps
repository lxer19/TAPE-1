URL: http://www.cs.utexas.edu/users/vlr/papers/lb98.ps
Refering-URL: http://www.cs.utexas.edu/users/vlr/pub.html
Root-URL: 
Email: philmac@cs.idbsu.edu  vlr@cs.utexas.edu  
Title: Computational Bounds for Fundamental Problems on General-Purpose Parallel Models  
Author: Philip D. MacKenzie Vijaya Ramachandran 
Date: April 21, 1998  
Address: 83712  Austin, TX 78712  
Affiliation: Dept. of Mathematics and Computer Science Boise State University Boise, ID  Department of Computer Sciences University of Texas  
Abstract: We present lower bounds for time needed to solve basic problems on three general-purpose models of parallel computation: the shared-memory models qsm and s-qsm, and the distributed-memory model, the bsp. For each of these models, we also obtain lower bounds for the number of rounds needed to solve these problems using a randomized algorithm on a p-processor machine. Our results on `rounds' is of special interest in the context of designing work-efficient algorithms on a machine where latency and synchronization costs are high. Many of our lower bound results are complemented by upper bounds that match the lower bound or are close to it. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Adler, P. Gibbons, Y. Matias, V. Ramachandran. </author> <title> Modeling parallel bandwidth: Local vs. global restrictions. </title> <booktitle> Proc. ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 94-105, </pages> <note> 1997; Algorithmica, to appear. </note>
Reference-contexts: However, we are not aware of many lower bounds results for the general-purpose models considered in this paper. A tight lower bound on the time needed for broadcasting on the qsm and the bsp is given in <ref> [1] </ref>. A lower bound for the number of rounds needed on the bsp to compute the OR by a deterministic algorithm is given in [11]. Among the results we present is the same lower bound as the one in [11], but one that holds for randomized algorithms as well. <p> a randomized algorithm that succeeds with probability at least 1=2 + *, * &gt; 0, requires (L q log log q+log (L=g) ) time, where q = minfn; pg. 9 For the qsm and s-qsm we have the following stronger lower bounds, which are obtained by using the result in <ref> [1] </ref> on adapting crcw pram lower bounds to the qsm and the lower bound of Beame and Hastad [3] on computing parity on the crcw pram. <p> Theorem 3.3 Any randomized algorithm for the n-element Parity problem on a p-processor qsm requires time ( g log n log log n+min (log log p; log log g) ). Proof: By using the result in <ref> [1] </ref> on adapting crcw pram lower bounds to the qsm and the lower bound of Beame and Hastad [3] on computing parity on the crcw pram using p processors, we obtain a lower bound of (g log n= log log p) for computing parity with a randomized algorithm on a qsm. <p> Thus the number of processors q that are affected by any input bit is bounded by q n g fi (log n= log log n) . By adaptation of the Beame-Hastad [3] lower bound to qsm given in <ref> [1] </ref>, this implies a lower bound of (log n= log log q), i.e., ( g log n log log n+log log g ) to compute parity by a randomized algorithm on the qsm. <p> Load Balancing Given h objects distributed among n processors, redistribute the objects so that each processor gets O (1 + h=n) objects. Padded Sort (or Padded U <ref> [0; 1] </ref> Sort) Given n values taken from a uniform distribution over the unit interval [0; 1], arrange them in sorted order in an array of size n + o (n), with the value NULL in all unfilled locations. <p> Load Balancing Given h objects distributed among n processors, redistribute the objects so that each processor gets O (1 + h=n) objects. Padded Sort (or Padded U <ref> [0; 1] </ref> Sort) Given n values taken from a uniform distribution over the unit interval [0; 1], arrange them in sorted order in an array of size n + o (n), with the value NULL in all unfilled locations.
Reference: [2] <author> M. Ajtai and M. Ben-Or. </author> <title> A Theorem on Probabilistic Constant Depth Computations. </title> <booktitle> Proc. 16th ACM Symp. on Theory of Computing, </booktitle> <pages> pp. 471-474, </pages> <year> 1984. </year>
Reference-contexts: The remaining part of the lower bound is obtained using [3] and extending it to randomized algorithms using either the Random Adversary technique, or <ref> [2] </ref>. 2 We note that the lower bounds we have obtained for the Parity problem imply corresponding lower bounds for other problems such as list ranking and sorting, since there are simple size-preserving reductions from parity to these other problems. 10 4 The Random Adversary Our lower bounds for Load Balancing
Reference: [3] <author> P. Beame, J. Hastad. </author> <title> Optimal bounds for decision problems on the CRCW PRAM. </title> <journal> JACM, </journal> <volume> vol. 36, </volume> <pages> pages 643-670, </pages> <year> 1989. </year>
Reference-contexts: Among the results we present is the same lower bound as the one in [11], but one that holds for randomized algorithms as well. Many of our results build on techniques and results developed earlier for the crcw pram <ref> [3, 15] </ref>, qrqw pram [9, 16, 17, 18, 14], erew pram [16], and the few-write pram [6]. The rest of this paper is organized as follows. <p> log log q+log (L=g) ) time, where q = minfn; pg. 9 For the qsm and s-qsm we have the following stronger lower bounds, which are obtained by using the result in [1] on adapting crcw pram lower bounds to the qsm and the lower bound of Beame and Hastad <ref> [3] </ref> on computing parity on the crcw pram. Theorem 3.3 Any randomized algorithm for the n-element Parity problem on a p-processor qsm requires time ( g log n log log n+min (log log p; log log g) ). <p> Proof: By using the result in [1] on adapting crcw pram lower bounds to the qsm and the lower bound of Beame and Hastad <ref> [3] </ref> on computing parity on the crcw pram using p processors, we obtain a lower bound of (g log n= log log p) for computing parity with a randomized algorithm on a qsm. This gives the stated lower bound if p is polynomial in n. <p> Thus the number of processors q that are affected by any input bit is bounded by q n g fi (log n= log log n) . By adaptation of the Beame-Hastad <ref> [3] </ref> lower bound to qsm given in [1], this implies a lower bound of (log n= log log q), i.e., ( g log n log log n+log log g ) to compute parity by a randomized algorithm on the qsm. <p> For p n, first we prove a lower bound of ( log n log (n=p)+log log p ). To do this, we change the proof of Theorem 4.1, part (b) of Beame and Hastad <ref> [3] </ref>. <p> We need s = 2 log 4p for the probabilities to work out as in <ref> [3] </ref>, along with the fact that p &gt; p n. After T steps we must have s 1 48 n ((96 (n=p)s)(48s)) (T 1) , and the lower bound of T = ( log n log (n=p)+log log p ) follows. <p> The remaining part of the lower bound is obtained using <ref> [3] </ref> and extending it to randomized algorithms using either the Random Adversary technique, or [2]. 2 We note that the lower bounds we have obtained for the Parity problem imply corresponding lower bounds for other problems such as list ranking and sorting, since there are simple size-preserving reductions from parity to
Reference: [4] <author> C. Berge. </author> <title> Graphs and Hypergraphs. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1976. </year>
Reference: [5] <author> D. E. Culler, R. M. Karp, D. A. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramo-nian and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> 4th ACM SIGPLAN Symp. on Princ. and Prac. of Para. Prog., </booktitle> <year> 1993. </year>
Reference-contexts: 1 Introduction Recently, there has been a great deal of interest in developing general-purpose models of parallel computation that incorporate features of real machines such as bandwidth limitations and the resulting cost for global memory accesses. The bsp [24] and logp <ref> [5] </ref> models are distributed memory models of this type, and the qsm [10] and s-qsm are shared-memory models of this type.
Reference: [6] <author> M. Dietzfelbinger, M. Kuty lowski, and R. Reischuk. </author> <title> Exact lower time bounds for computing boolean functions on CREW PRAMs. </title> <journal> J. Comput. System Sci., </journal> <volume> 48 </volume> <pages> 231-254, </pages> <year> 1994. </year>
Reference-contexts: Many of our results build on techniques and results developed earlier for the crcw pram [3, 15], qrqw pram [9, 16, 17, 18, 14], erew pram [16], and the few-write pram <ref> [6] </ref>. The rest of this paper is organized as follows. In Section 2 we define the qsm, s-qsm and bsp models, as well as our lower-bound model, the gsm, and we present some basic results on mapping lower bounds from the gsm to the other models. <p> Let t = log r= log log r. We assume that in each step, m rw fft and fit , since otherwise T &gt; log r= log log r and we are done. Thus, using an analysis as in <ref> [6] </ref> for the `Few Write' PRAM, after l steps the degree of the processor/memory functions is at most (t ) cl , for a suitable constant c.
Reference: [7] <author> F. Fich, M. Kowaluk, M. Kuty lowski, K. Lorys, and P. Ragde. </author> <title> Retrieval of scattered information by EREW, CREW, </title> <booktitle> and CRCW PRAMs. In Proc. 3rd Scand. Workshop on Alg. Theory, </booktitle> <pages> pages 30-41. </pages> <booktitle> Lec. Notes in Comp. Sci., </booktitle> <volume> Vol. 621, </volume> <year> 1992. </year>
Reference: [8] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> Efficient low-contention parallel algorithms. </title> <journal> In JCSS, </journal> <volume> vol. 53, </volume> <pages> pp. 395-416, </pages> <note> 1996 (Special Issue for SPAA'94. </note>
Reference: [9] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> The QRQW PRAM: Accounting for contention in parallel algorithms. </title> <booktitle> In 5th ACM-SIAM Symp. on Disc. Alg., </booktitle> <pages> pages 638-648, </pages> <year> 1994, </year> <note> SICOMP, to appear. </note>
Reference-contexts: The bsp [24] and logp [5] models are distributed memory models of this type, and the qsm [10] and s-qsm are shared-memory models of this type. As a shared-memory model, the qsm can be viewed as a generalization of the pram model [13] with the qrqw memory access rule <ref> [9] </ref> (which is intermediate between the erew and crcw rules), a parameter g to capture bandwidth limitations, and `bulk-synchrony' in place of synchronization after each step as in the pram model. <p> The lower bounds for this problem imply lower bounds for other important problems such as list ranking and sorting. There are a large number of lower bound results known for computation on the traditional pram models as well as some for the qrqw pram <ref> [9] </ref>. However, we are not aware of many lower bounds results for the general-purpose models considered in this paper. A tight lower bound on the time needed for broadcasting on the qsm and the bsp is given in [1]. <p> Among the results we present is the same lower bound as the one in [11], but one that holds for randomized algorithms as well. Many of our results build on techniques and results developed earlier for the crcw pram [3, 15], qrqw pram <ref> [9, 16, 17, 18, 14] </ref>, erew pram [16], and the few-write pram [6]. The rest of this paper is organized as follows. <p> The time of a qsm algorithm is the sum of the time costs for its phases. The particular instance of the Queuing Shared Memory model in which the gap parameter, g, equals 1 is the Queue-Read Queue-Write (qrqw) pram model defined in <ref> [9] </ref>. 2. The s-qsm Model. [10] This is essentially the qsm, except that there is a gap parameter with value g for processing each access at memory, in addition to the gap parameter at processors. <p> On the bsp this algorithm runs in time O ( p w.h.p. (provided the number of elements being compacted is O (n=(log n2 log n=(n=p) )). All of these results are obtained by an adaptation of the qrqw algorithm in <ref> [9] </ref>. The best algorithm that we know (deterministic or randomized) that computes in rounds is the simple algorithm based on computing prefix sums. This algorithm has the same performance as the algorithms that compute parity in rounds. <p> No better randomized algorithm is known in either model, unless unit-time concurrent reads are allowed, in which case, the OR can be computed with high probability on both models in O (g log n= log log n) time (by an adaptation of a qrqw algorithm given in <ref> [9] </ref>). This is still much larger than our corresponding lower bounds. On the bsp one can compute the OR in O (L log n= log (L=g)) [12]. On all three models we can match the lower bound on number of rounds for randomized algorithms by simple deterministic algorithms.
Reference: [10] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> Can a shared-memory model serve as a bridging model for parallel computation? In ACM Symp. </title> <booktitle> on Parallel Algorithms and Architectures, </booktitle> <pages> pages 72-83, </pages> <year> 1997. </year>
Reference-contexts: The bsp [24] and logp [5] models are distributed memory models of this type, and the qsm <ref> [10] </ref> and s-qsm are shared-memory models of this type. <p> Section 8 presents our upper bound results. 2 Definitions and Basic Results 2.1 General-purpose Models 1. The qsm model. <ref> [10] </ref> The Queuing Shared Memory (qsm) model consists of a number of identical processors, each with its own private memory, communicating by reading and writing locations in a shared memory. <p> The time of a qsm algorithm is the sum of the time costs for its phases. The particular instance of the Queuing Shared Memory model in which the gap parameter, g, equals 1 is the Queue-Read Queue-Write (qrqw) pram model defined in [9]. 2. The s-qsm Model. <ref> [10] </ref> This is essentially the qsm, except that there is a gap parameter with value g for processing each access at memory, in addition to the gap parameter at processors. <p> In order to derive lower bounds for the three models we consider, we define below the gsm model, which is stronger than all three of these models (and also stronger than the qsm (g; d) <ref> [10, 21] </ref>, which is a generalization of the qsm and s-qsm.) We derive most of our lower bounds on this stronger model, and then obtain the lower bounds for the three models of interest as corollaries of the lower bound on the gsm. <p> A p-processor algorithm on the qsm or s-qsm performs linear work if the processor-time product is O (g n), where n is the size of the input. As shown in <ref> [10] </ref>, (g n) is a lower bound on this product for most nontrivial problems. A p-processor algorithm on a gsm performs linear work if the processor-time product is O (n=); this is again a lower bound for most nontrivial problems. <p> We also note that a similar claim can be obtained for the qsm (g; d) model <ref> [10, 21] </ref>, and can be used to derive lower bounds for this model using the results we derive for the gsm. <p> This algorithm has the same performance as the algorithms that compute parity in rounds. Note that if we relax the definition of a round in a randomized algorithm to be a computation that terminates in O (gn=p) time w.h.p., then we can design algorithms (by adapting the algorithm in <ref> [10] </ref>) that beat our lower bounds (which were derived under the restriction that a round must terminate in O (gn=p) time). 27 OR.
Reference: [11] <author> M. Goodrich. </author> <title> Communication-efficient parallel sorting. </title> <booktitle> Proc. STOC, </booktitle> <pages> pages 247-256, </pages> <year> 1996. </year>
Reference-contexts: A tight lower bound on the time needed for broadcasting on the qsm and the bsp is given in [1]. A lower bound for the number of rounds needed on the bsp to compute the OR by a deterministic algorithm is given in <ref> [11] </ref>. Among the results we present is the same lower bound as the one in [11], but one that holds for randomized algorithms as well. <p> A lower bound for the number of rounds needed on the bsp to compute the OR by a deterministic algorithm is given in <ref> [11] </ref>. Among the results we present is the same lower bound as the one in [11], but one that holds for randomized algorithms as well. Many of our results build on techniques and results developed earlier for the crcw pram [3, 15], qrqw pram [9, 16, 17, 18, 14], erew pram [16], and the few-write pram [6].
Reference: [12] <author> B. H. H. Juurlink and H. A. G. Wijshoff. </author> <title> Communication primitives for BSP computers. </title> <journal> IPL, </journal> <volume> vol. 58, </volume> <pages> pages 303-310, </pages> <year> 1996. </year> <month> 28 </month>
Reference-contexts: This is still much larger than our corresponding lower bounds. On the bsp one can compute the OR in O (L log n= log (L=g)) <ref> [12] </ref>. On all three models we can match the lower bound on number of rounds for randomized algorithms by simple deterministic algorithms.
Reference: [13] <author> R. M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity, chapter 17, </booktitle> <pages> pages 869-941. </pages> <publisher> MIT Press/Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: The bsp [24] and logp [5] models are distributed memory models of this type, and the qsm [10] and s-qsm are shared-memory models of this type. As a shared-memory model, the qsm can be viewed as a generalization of the pram model <ref> [13] </ref> with the qrqw memory access rule [9] (which is intermediate between the erew and crcw rules), a parameter g to capture bandwidth limitations, and `bulk-synchrony' in place of synchronization after each step as in the pram model.
Reference: [14] <author> M. Kutylowski and K. Lorys. </author> <title> Limitations of the QRQW and EREW PRAM models. </title> <booktitle> In Proc. Foundations of Software Technology and Theoretical Computer Science (FST&TCS), </booktitle> <year> 1996. </year>
Reference-contexts: Among the results we present is the same lower bound as the one in [11], but one that holds for randomized algorithms as well. Many of our results build on techniques and results developed earlier for the crcw pram [3, 15], qrqw pram <ref> [9, 16, 17, 18, 14] </ref>, erew pram [16], and the few-write pram [6]. The rest of this paper is organized as follows. <p> In this section we present algorithms and lower bounds for parity and related problems. We start with a lower bound for deterministic algorithms on the gsm, even if unit-time concurrent reads are allowed. Our proof is an adaptation of a method used by <ref> [14] </ref> to obtain a lower bound for parity for the simd-qrqw model with concurrent reads (which is more restrictive than even the crqw model) but with the added feature of `latency detection', which is not present in the qrqw or qsm model. <p> Since t i 1 and t 0 i 1, we have T l. Also, the degree of the function specifying the contents of the output cell should be at least r at termination since the degree of a function f that computes the parity of r bits is r <ref> [14] </ref>. <p> Since this type of lower bound does not hold for a `strong queuing' model, it does not adapt to the gsm or the bsp. Our first lower bound is obtained by an adaptation of the lower bound for randomized parity on the simd-qrqw from <ref> [14] </ref>. Theorem 3.2 Computing the parity of n bits on a gsm by a randomized algorithm requires ( log (n=fl) log log (n=fl)+log ) time. Proof: Let r = n=fl. <p> Proof: We note that Theorem 10 from <ref> [14] </ref> also applies to rounds on the gsm. Assume there is a randomized algorithm for ((h=) + 1)-LAC that runs in time T .
Reference: [15] <author> P. D. MacKenzie. </author> <title> Load balancing requires (log fl n) expected time. </title> <booktitle> In 3rd ACM-SIAM Symp. on Disc. Alg., </booktitle> <pages> pages 94-99, </pages> <year> 1992. </year>
Reference-contexts: In many cases our lower bounds for this problem also hold for related problems such as load balancing and padded sort. (2.) Computing the OR. Our lower bound on time for randomized algorithms uses the Random Adversary Technique <ref> [15] </ref> in a novel way, which could be of independent interest. <p> Among the results we present is the same lower bound as the one in [11], but one that holds for randomized algorithms as well. Many of our results build on techniques and results developed earlier for the crcw pram <ref> [3, 15] </ref>, qrqw pram [9, 16, 17, 18, 14], erew pram [16], and the few-write pram [6]. The rest of this paper is organized as follows. <p> Proof Sketch: The first part of the lower bound for the qsm follows from the lower bound of <ref> [15] </ref> by assuming that a processor can read or write to n=p cells in any given state in a single step. (Note that the contention does not affect that lower bound, since it is for the CRCW PRAM, which allows any amount of contention.) The other parts of the lower bounds <p> This follows easily once we show that solving OR with the desired probability requires (log fl +1 (n=fl)) big-steps. (We use the fact that log fl n log fl z+1 n + log fl z + 2, for z 1 <ref> [15] </ref>.) 20 log fl +1 (n=fl). From Lemma 7.5, If line (4) or (10) is ever executed, then the expected time is ( log fl +1 (n=fl)). Now assume neither line (4) nor (10) are executed.
Reference: [16] <author> P. D. MacKenzie. </author> <title> Lower Bounds for Randomized Exclusive-Write PRAMs. </title> <booktitle> In Proc. 7th ACM Symp. on Para. Alg. and Arch., </booktitle> <pages> 254-263, </pages> <year> 1995, </year>
Reference-contexts: Among the results we present is the same lower bound as the one in [11], but one that holds for randomized algorithms as well. Many of our results build on techniques and results developed earlier for the crcw pram [3, 15], qrqw pram <ref> [9, 16, 17, 18, 14] </ref>, erew pram [16], and the few-write pram [6]. The rest of this paper is organized as follows. <p> Many of our results build on techniques and results developed earlier for the crcw pram [3, 15], qrqw pram [9, 16, 17, 18, 14], erew pram <ref> [16] </ref>, and the few-write pram [6]. The rest of this paper is organized as follows. <p> In Section 3 we present our lower bound results for Parity. In Section 4 we review the Random Adversary technique <ref> [16] </ref>. In Section 5 we develop a general lower bound proof based on the Random Adversary for the gsm, which we use in Section 6 to derive lower bounds for Linear Approximate Compaction (LAC) and related problems. <p> obtained for the Parity problem imply corresponding lower bounds for other problems such as list ranking and sorting, since there are simple size-preserving reductions from parity to these other problems. 10 4 The Random Adversary Our lower bounds for Load Balancing and the OR function use the Random Adversary technique <ref> [16] </ref>. We start by reviewing this method. The Random Adversary Technique allows one to prove a lower bound on the time required for a parallel randomized algorithm to solve a given problem. The first step of the technique is to decide on an input distribution for the problem. <p> Let r = n=fl. At termination, r (n=p) cl , hence the number of rounds l = (log r= log (n=p)). We now apply the Random-Adversary Technique, using the input distribution given for the lower bound on OR in <ref> [16] </ref> to obtain the same lower bound for randomized algorithms. 2 Corollary 7.3 Assuming n p, for any constant * &gt; 0 solving OR with probability greater than 1 2 (1 + *) requires ( log n log (gn=p) ) rounds on a Randomized qsm, ( log n log (n=p) )
Reference: [17] <author> P. D. MacKenzie. </author> <title> A lower bound for the QRQW PRAM. </title> <booktitle> In Proc. 7th IEEE Symp. on Para. and Distr. Proc., </booktitle> <pages> pages 231-237, </pages> <year> 1995. </year>
Reference-contexts: Among the results we present is the same lower bound as the one in [11], but one that holds for randomized algorithms as well. Many of our results build on techniques and results developed earlier for the crcw pram [3, 15], qrqw pram <ref> [9, 16, 17, 18, 14] </ref>, erew pram [16], and the few-write pram [6]. The rest of this paper is organized as follows.
Reference: [18] <author> P. D. MacKenzie. </author> <title> An improved lower bound for the QRQW PRAM. </title> <booktitle> In Workshop on Randomized Parallel Computing, IPPS, Hawaii, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: Among the results we present is the same lower bound as the one in [11], but one that holds for randomized algorithms as well. Many of our results build on techniques and results developed earlier for the crcw pram [3, 15], qrqw pram <ref> [9, 16, 17, 18, 14] </ref>, erew pram [16], and the few-write pram [6]. The rest of this paper is organized as follows.
Reference: [19] <author> P. D. MacKenzie and V. Ramachandran. </author> <title> Computational Bounds for Fundamental Problems on General-Purpose Parallel Models. </title> <booktitle> Proc. 1998 ACM Symp. on Parallel Algs. and Architectures, </booktitle> <month> June-July </month> <year> 1998, </year> <note> to appear. </note>
Reference-contexts: studying the time needed to solve a problem regardless of the number of processors used, we also obtain bounds on the number of rounds needed in a computation with p processors. fl An extended abstract of this work will appear in Proc. 1998 ACM Symp. on Parallel Algorithms and Architectures <ref> [19] </ref>. y Part of this work was performed at Sandia National Laboratories and was supported by the U.S. Department of Energy under contract DE-AC04-76DP00789.
Reference: [20] <author> N. Nisan. </author> <title> CREW PRAMs and decision trees. </title> <journal> SIAM J. Comput., </journal> <volume> 20 </volume> <pages> 999-1007, </pages> <year> 1991. </year>
Reference-contexts: Let C (f ) be the maximum of the numbers minfkj9 Sf1;:::;ng;jSj=k f (a) = f (b) if 8 i2S a i = b i g taken over all input maps a. (This was called the certificate complexity in Nisan <ref> [20] </ref>.) Fact 2.3 ([6]) C (f ) (deg (f )) 4 . For A f0; 1g n , we denote the characteristic function of A by A or (A).
Reference: [21] <author> V. Ramachandran. </author> <title> A general purpose shared-memory model for parallel computation. </title> <booktitle> Proc. IMA Workshop on Parallel Algorithms, </booktitle> <address> Springer Verlag, </address> <note> to appear. </note>
Reference-contexts: In order to derive lower bounds for the three models we consider, we define below the gsm model, which is stronger than all three of these models (and also stronger than the qsm (g; d) <ref> [10, 21] </ref>, which is a generalization of the qsm and s-qsm.) We derive most of our lower bounds on this stronger model, and then obtain the lower bounds for the three models of interest as corollaries of the lower bound on the gsm. <p> We also note that a similar claim can be obtained for the qsm (g; d) model <ref> [10, 21] </ref>, and can be used to derive lower bounds for this model using the results we derive for the gsm.
Reference: [22] <author> R. Smolensky. </author> <title> Algebraic methods in the theory of lower bounds for boolean circuit complexity. </title> <booktitle> In Proc. 19th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 77-82, </pages> <year> 1987. </year>
Reference: [23] <author> M. Szegedy. </author> <title> Algebraic methods in lower bounds for computational models with limited communication. </title> <type> PhD thesis, </type> <institution> University of Chicago, </institution> <year> 1989. </year>
Reference: [24] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Recently, there has been a great deal of interest in developing general-purpose models of parallel computation that incorporate features of real machines such as bandwidth limitations and the resulting cost for global memory accesses. The bsp <ref> [24] </ref> and logp [5] models are distributed memory models of this type, and the qsm [10] and s-qsm are shared-memory models of this type. <p> The qrqw pram is the same as the s-qsm with gap parameter 1. 3. The bsp Model. <ref> [24] </ref> The bsp model consists of p processor/memory components that communicate by sending point-to-point messages. The interconnection network supporting this communication is characterized by a bandwidth parameter g and a latency parameter L.

References-found: 24

