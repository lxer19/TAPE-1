URL: http://www.neci.nj.nec.com/homepages/giles/papers/DeBruijn.automata.learning.feedforward.nets.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Email: dclouse@ucsd.edu  giles@research.nj.nec.com  horne@research.nj.nec.com  gcottrell@ucsd.edu  
Title: Learning Large DeBruijn Automata with Feed-Forward Neural Networks  
Author: Daniel S. Clouse C. Lee Giles Bill G. Horne Garrison W. Cottrell 
Address: Dept.,  Princeton NJ,  Princeton NJ,  Dept.,  
Date: December 22, 1994  
Affiliation: University of California, San Diego Computer Science and Engineering  UCSD CSE  NEC Research Institute,  NEC Research Institute,  UCSD CSE  
Pubnum: Technical Report CS94-398  
Abstract: In this paper we argue that a class of finite state machines (FSMs) which is representable by the NNFIR (Neural Network Finite Impulse Response) architecture is equivalent to the definite memory sequential machines (DMMs) which are implementations of deBruijn automata. We support this claim by drawing parallels between circuit topologies of sequential machines used to implement FSMs and the architecture of the NNFIR. Further support is provided by simulation results that show that a NNFIR architecture is able to learn perfectly a large definite memory machine (2048 states) with very few training examples. We also discuss the effects that variations in the NNFIR architecture have on the class of problems easily learnable by the network. 
Abstract-found: 1
Intro-found: 1
Reference: [Cleeremans et al., 1989] <author> Cleeremans, A., Servan-Schreiber, D., and McClelland, J. L. </author> <year> (1989). </year> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 372-381. </pages>
Reference-contexts: 1 Introduction In the neural network language induction literature, induction of finite state automata is commonly thought of as the domain of recurrent network architectures <ref> [Cleeremans et al., 1989] </ref> [Elman, 1991] [Pollack, 1991] [Giles et al., 1992] [Watrous and Kuhn, 1992]. However, recent work [Giles et al., 1994] has shown that a restricted class of recurrent nets can learn a subclass of finite state automata called finite-memory automata.
Reference: [Day and Davenport, 1993] <author> Day, S. and Davenport, M. </author> <year> (1993). </year> <title> Continuous-time temporal back-progagation with adaptive time delays. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4 </volume> <pages> 348-354. </pages>
Reference-contexts: To our knowledge, there has been no work on adapting the number of delays, but techniques for adapting the number of time steps between delays have been developed [Pearlmutter, 1989], [Lin et al., 1992], <ref> [Day and Davenport, 1993] </ref>. 3 3 Description of IDNN Architecture A natural restriction of the general NNFIR topology is the class of NNFIR architectures which have delays only on the input units. We call these input delayed neural networks (IDNNs).
Reference: [Elman, 1991] <author> Elman, J. L. </author> <year> (1991). </year> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <journal> Machine Learning, </journal> 7(2/3):195-226. 
Reference-contexts: 1 Introduction In the neural network language induction literature, induction of finite state automata is commonly thought of as the domain of recurrent network architectures [Cleeremans et al., 1989] <ref> [Elman, 1991] </ref> [Pollack, 1991] [Giles et al., 1992] [Watrous and Kuhn, 1992]. However, recent work [Giles et al., 1994] has shown that a restricted class of recurrent nets can learn a subclass of finite state automata called finite-memory automata.
Reference: [Giles et al., 1994] <author> Giles, C. L., Horne, B. G., and Lin, T. </author> <year> (1994). </year> <title> Learning a class of large finite state machines with a recurrent neural network. </title> <institution> Technical Report UMIACS-TR-94-94 and CS-TR-3328, Intitute for Advanced Computer Studies, Univerity of Maryland. </institution>
Reference-contexts: 1 Introduction In the neural network language induction literature, induction of finite state automata is commonly thought of as the domain of recurrent network architectures [Cleeremans et al., 1989] [Elman, 1991] [Pollack, 1991] [Giles et al., 1992] [Watrous and Kuhn, 1992]. However, recent work <ref> [Giles et al., 1994] </ref> has shown that a restricted class of recurrent nets can learn a subclass of finite state automata called finite-memory automata. In this paper, we show that feedforward-only architectures can represent and learn a class of automata, DeBruijn automata.
Reference: [Giles et al., 1992] <author> Giles, C. L., Sun, G. Z., Chen, H. H., Lee, Y. C., and Chen, D. </author> <year> (1992). </year> <title> Higher order recurrent networks and grammatical inference. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 393-405. </pages>
Reference-contexts: 1 Introduction In the neural network language induction literature, induction of finite state automata is commonly thought of as the domain of recurrent network architectures [Cleeremans et al., 1989] [Elman, 1991] [Pollack, 1991] <ref> [Giles et al., 1992] </ref> [Watrous and Kuhn, 1992]. However, recent work [Giles et al., 1994] has shown that a restricted class of recurrent nets can learn a subclass of finite state automata called finite-memory automata.
Reference: [Golomb, 1982] <author> Golomb, S. </author> <year> (1982). </year> <title> Shift Register Sequences. </title> <publisher> Aegean Park Press, </publisher> <address> Laguna Hills, CA. </address>
Reference-contexts: The transition diagram is essentially that of a shift register. Such a transition diagram is called a directed binary deBruijn graph <ref> [Golomb, 1982] </ref>. Definition of Directed Binary DeBruijn Graph The directed binary deBruijn graph (henceforth deBruijn graph) of diameter d is the graph with 2 d nodes connected in a specific 9 pattern. Each node is named with a unique string of d 0s and 1s.
Reference: [Kohavi, 1978] <author> Kohavi, Z. </author> <year> (1978). </year> <title> Switching and Finite Automata Theory. </title> <publisher> McGraw-Hill, Inc., </publisher> <address> New York, NY, </address> <note> second edition. </note>
Reference-contexts: A sequential machine is an implementation of a finite state machine (FSM) in digital logic and delays <ref> [Kohavi, 1978] </ref>. The sequential machine formalism separates combinational logic from memory elements. In a sequential machine implementation of an FSM, the memory elements contain the current state of the FSM. Combinational logic is used to define the state transitions and the outputs of the machine.
Reference: [Krogh and Hertz, 1992] <author> Krogh, A. and Hertz, J. </author> <year> (1992). </year> <title> A simple weight decay can improve generalization. </title> <editor> In [Moody et al., </editor> <year> 1992], </year> <pages> pages 950-957. </pages>
Reference-contexts: During training (and testing), before introduction of a new string, the values of all the delays were set to 0. Online back propagation [Rumelhart et al., 1986] with a learning rate of 0.25 and momentum of 0.25 was used for training. Weight decay <ref> [Krogh and Hertz, 1992] </ref> with a weight decay parameter of 0.0001 was used. A selective updating scheme was applied whereby weights were updated in an online fashion, but only if the absolute error on the current training sample was greater than 0.2.
Reference: [Lang et al., 1990] <author> Lang, K., Waibel, A., and Hinton, G. </author> <year> (1990). </year> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3(1) </volume> <pages> 23-44. </pages>
Reference-contexts: An NNFIR (Neural Network Finite Impulse Response [Wan, 1993]; also known as a Time Delay Neural Network or TDNN <ref> [Lang et al., 1990] </ref>) is similar to a multi-layer perceptron 2 in that all connections feed forward.
Reference: [Lin et al., 1992] <author> Lin, D., Dayhoff, J., and Ligomenides, P. </author> <year> (1992). </year> <title> A learning algorithm for adaptive time-delays in a temporal neural network. </title> <type> Technical Report TR 92-59, </type> <institution> Systems Research Center, University of Maryland, College Park, Maryland. </institution>
Reference-contexts: To our knowledge, there has been no work on adapting the number of delays, but techniques for adapting the number of time steps between delays have been developed [Pearlmutter, 1989], <ref> [Lin et al., 1992] </ref>, [Day and Davenport, 1993]. 3 3 Description of IDNN Architecture A natural restriction of the general NNFIR topology is the class of NNFIR architectures which have delays only on the input units. We call these input delayed neural networks (IDNNs).
Reference: [Moody et al., 1992] <editor> Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors (1992). </editor> <booktitle> Advances in Neural Information Processing Systems 4 Proceedings of the 1991 Conference, </booktitle> <address> Denver, Colorado. </address> <publisher> Morgan Kaufmann Publishers : San Francisco, </publisher> <address> CA. </address>
Reference: [Pearlmutter, 1989] <author> Pearlmutter, B. A. </author> <year> (1989). </year> <title> Learning state space trajectories in recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 263-269. </pages>
Reference-contexts: To our knowledge, there has been no work on adapting the number of delays, but techniques for adapting the number of time steps between delays have been developed <ref> [Pearlmutter, 1989] </ref>, [Lin et al., 1992], [Day and Davenport, 1993]. 3 3 Description of IDNN Architecture A natural restriction of the general NNFIR topology is the class of NNFIR architectures which have delays only on the input units. We call these input delayed neural networks (IDNNs).
Reference: [Pollack, 1991] <author> Pollack, J. B. </author> <year> (1991). </year> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> 7(2/3):227-252. 
Reference-contexts: 1 Introduction In the neural network language induction literature, induction of finite state automata is commonly thought of as the domain of recurrent network architectures [Cleeremans et al., 1989] [Elman, 1991] <ref> [Pollack, 1991] </ref> [Giles et al., 1992] [Watrous and Kuhn, 1992]. However, recent work [Giles et al., 1994] has shown that a restricted class of recurrent nets can learn a subclass of finite state automata called finite-memory automata.
Reference: [Rice, 1988] <author> Rice, J. A. </author> <year> (1988). </year> <title> Mathematical Statistics and Data Analysis. </title> <publisher> Brooks/Cole Publishing Company, </publisher> <address> Monterey, California. </address> <month> 19 </month>
Reference-contexts: The mean performance of the NNFIR architecture across the entire curve is significantly better than that of the IDNN architecture as shown by a two-way analysis of variance (ANOVA) <ref> [Rice, 1988] </ref> (M S arch = 0:2749, M S error = 0:0027, F (1; 456) = 101:427, p &lt; 0:001) 2 . This result supports our hypothesis that the general NNFIR architecture is better at learning a DMM whose mapping function contains repeated terms.
Reference: [Rumelhart et al., 1986] <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. and McClel-land, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1, chapter 8. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: Likewise, the network contains full connections between the hidden layer and the output. During training (and testing), before introduction of a new string, the values of all the delays were set to 0. Online back propagation <ref> [Rumelhart et al., 1986] </ref> with a learning rate of 0.25 and momentum of 0.25 was used for training. Weight decay [Krogh and Hertz, 1992] with a weight decay parameter of 0.0001 was used.
Reference: [Waibel et al., 1989] <author> Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang, K. </author> <year> (1989). </year> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 37(3) </volume> <pages> 328-339. </pages>
Reference-contexts: As such these automata can be implemented by a class of sequential machines termed Definite Memory Machines (DMMs). Structurally these DMMs resemble time-delay neural networks (TDNNs). TDNNs were originally defined <ref> [Waibel et al., 1989] </ref>, to include feedforward nets with both input time-delays and internal time-delays. Like TDNNs, DMMs make no distinction between these two forms of time delays either. In the literature [Wan, 1993], a TDNN is also referred to as an NNFIR (Neural Network Finite Impulse Response) architecture.
Reference: [Wan, 1993] <author> Wan, E. A. </author> <year> (1993). </year> <title> Time series prediction by using a connectionist network with internal delay lines. </title> <editor> In Weigend, A. S. and Gershenfeld, N. A., editors, </editor> <title> Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison Wesley. </publisher>
Reference-contexts: Structurally these DMMs resemble time-delay neural networks (TDNNs). TDNNs were originally defined [Waibel et al., 1989], to include feedforward nets with both input time-delays and internal time-delays. Like TDNNs, DMMs make no distinction between these two forms of time delays either. In the literature <ref> [Wan, 1993] </ref>, a TDNN is also referred to as an NNFIR (Neural Network Finite Impulse Response) architecture. For clarity we define the subclass which has only time delays on the inputs as an Input Delay Neural Network (IDNN). <p> If there are p inputs to the network, then the first p nodes are used as input units 8i p; y i = u i where u i is the i th network input. An NNFIR (Neural Network Finite Impulse Response <ref> [Wan, 1993] </ref>; also known as a Time Delay Neural Network or TDNN [Lang et al., 1990]) is similar to a multi-layer perceptron 2 in that all connections feed forward. <p> Clearly then, the set of functions computable by the NNFIRs must include all those computable by the IDNNs. It is less intuitive but also easy to show that the set of functions computable by any IDNN includes all those computable by the general NNFIR class <ref> [Wan, 1993] </ref> 1 . Figure 2 and the discussion below presents a construction which results in removing all the internal delay lines from an NNFIR network and replacing them with an increased number of delays on the network inputs, resulting in an IDNN network which computes the same output function. <p> We have already seen that the NNFIR and IDNN classes are functionally equivalent. This implies that NNFIRs implement DMMs as well. On the other hand, if the inputs and outputs are allowed to take on real values, problems not representable by a finite state automata can be effectively handled <ref> [Wan, 1993] </ref>. Still, the finite history maintained by the NNFIR architecture precludes the representation of some regular languages. For example, the language 10* requires that the information that a 1 has been seen to be maintained across a potentially infinite number of 0s. <p> When an error value reaches the beginning of the buffer it serves as the error for the node which drives the associated delay line. See <ref> [Wan, 1993] </ref> for a detailed description of the general algorithm. With the exception of the difference of this more complicated training method, all details of the NNFIR training were identical to that of the IDNN training. The results of the simulation are presented in figure 9.
Reference: [Watrous and Kuhn, 1992] <author> Watrous, R. and Kuhn, G. </author> <year> (1992). </year> <title> Induction of finite state languages using second-order recurrent networks. </title> <editor> In [Moody et al., </editor> <year> 1992], </year> <pages> pages 309-316. </pages>
Reference-contexts: 1 Introduction In the neural network language induction literature, induction of finite state automata is commonly thought of as the domain of recurrent network architectures [Cleeremans et al., 1989] [Elman, 1991] [Pollack, 1991] [Giles et al., 1992] <ref> [Watrous and Kuhn, 1992] </ref>. However, recent work [Giles et al., 1994] has shown that a restricted class of recurrent nets can learn a subclass of finite state automata called finite-memory automata. In this paper, we show that feedforward-only architectures can represent and learn a class of automata, DeBruijn automata.
References-found: 18

