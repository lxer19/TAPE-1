URL: http://www.cs.washington.edu:80/homes/gretta/research/paper.ps
Refering-URL: http://www.cs.washington.edu:80/homes/gretta/research/markov.html
Root-URL: http://www.cs.washington.edu
Title: Potentials and Limitations of Fault-Based Markov Prefetching for Virtual Memory Pages  
Abstract: This paper examines fault-based prefetching of virtual memory pages using Markov prediction of future page access. A fault-based approach accumulates knowledge only at page fault times, and is thus efficient and easy to implement within the operating system. We compare several alternative algorithms for fault-based prediction. As well, we develop a simple model relating the performance of prefetching to the inter-fault time, the fetch time, and the prediction accuracy. Our measurement and modelling results show that fault-based prediction using a one-level Markov net can achieve high prediction accuracy; however, we also show that the potential for speedup is severely limited in current technology. 
Abstract-found: 1
Intro-found: 1
Reference: [Aho et al. 71] <author> A. B. Aho, P. J. Denning, and J. D. Ullman. </author> <title> Principles of optimal page replacement. </title> <journal> Journal of the ACM, </journal> <volume> 18(1), </volume> <month> January </month> <year> 1971. </year>
Reference-contexts: Their theoretical work also assumes that as many pages as are desired can be prefetched simultaneously. Recent work by Liberatore [Liberatore 99] studies the validity of first-order Markov chains as a model for page reference patterns; this model was proposed by <ref> [Aho et al. 71] </ref> and studied by various authors [Franklin & Gupta 74, Karlin et al. 92]. Liberatore shows that first-order Markov chains are not a statistically valid model for instruction page traces.
Reference: [Alexander & Kedem 96] <author> T. Alexander and G. Kedem. </author> <title> Distributed prefetch-buffer/cache design for high performance memory systems. </title> <booktitle> In Proceedings of HPCA-2 Conference, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: One Markov-based predictor attempts to predetermine control flow at branch sites [Chen et al. 96], while others prefetch data from the off-chip to the on-chip cache or from DRAM to SRAM on the chip <ref> [Joseph & Grunwald 97, Alexander & Kedem 96] </ref>. Palmer and Zdonik train an associative memory to recognize access patterns [Palmer & Zdonik 91] and Salem computes first-order statistics for prediction [Salem 91].
Reference: [Baer & Sager 76] <author> J.-L. Baer and G. R. Sager. </author> <title> Dynamic improvement of locality in virtual memory systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-2(1), </volume> <month> March </month> <year> 1976. </year>
Reference-contexts: It is interesting to note, however, that a very basic Markov-like predictor was analyzed for memory prefetching as early as 1976 <ref> [Baer & Sager 76] </ref>. However, with the addition of a new network-memory layer to the memory hierarchy, applying imperfect prediction and prefetching to the virtual memory system has become a viable avenue of research. <p> If no two of the last three strides match, no prediction is issued. 3. MajStride-Seq. The same as MajStride, except in cases where MajStride issues no pre diction, predict Sequential. 4. Baer: Based on <ref> [Baer & Sager 76] </ref>, this algorithm exploits temporal locality in access patterns. It keeps a lookup table of all page numbers in the address space; on fault f n , table [f n1 ] is set to f n , and table [f n ] is predicted.
Reference: [Boden et al. 95] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, and W.-K. Su. Myrinet: </author> <title> a gigabit per second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1), </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: We show that adaptive fault-based prefetching based on a one-level Markov net can achieve high prediction accuracy for some classes of scientific applications. 1 Our study also focuses on high-speed LAN environments, because the combination of newer network technologies (e.g., gigabit-per-second networks such as Myrinet <ref> [Boden et al. 95] </ref>) and cooperative network memory systems [Dahlin et al. 94, Feeley et al. 95, Voelker et al. 98] offers a new opportunity for prefetching. <p> As a simple experiment, we ran the wave5 benchmark on the PGMS system with speculative Markov prefetching. The Markov predictor had a prediction accuracy of 80% for wave5. We ran the experiment on a small network of 600 Mhz DEC Alpha 21164 processors, connected by a 1Gb/sec Myrinet <ref> [Boden et al. 95] </ref> switched network running with Trapeze software and firmware [Yocum et al. 97] to accelerate page transfers. A page fault from a remote memory in PGMS on this configuration takes slightly less than 200 sec. In this environment, we measured only a 2% speedup for wave5.
Reference: [Cao et al. 95] <author> P. Cao, E. Felten, A. Karlin, and K. Li. </author> <title> A study of integrated prefetching and caching strategies. </title> <booktitle> In Proc. of the ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Some of these studies ignore the problem of prediction by providing future knowledge of the reference stream to the predictor <ref> [Cao et al. 95, Kimbrel et al. 96] </ref> or to the virtual memory system [Cao et al. 96] in advance. The most commonly implemented contemporary prefetching systems use sequential prefetching only, possibly with strides of greater than one.
Reference: [Cao et al. 96] <author> P. Cao, E. Felten, A. Karlin, and K. Li. </author> <title> Implementation and performance of integrated application-controlled file caching, prefetching, and disk scheduling. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 14(4), </volume> <month> November </month> <year> 1996. </year>
Reference-contexts: Some of these studies ignore the problem of prediction by providing future knowledge of the reference stream to the predictor [Cao et al. 95, Kimbrel et al. 96] or to the virtual memory system <ref> [Cao et al. 96] </ref> in advance. The most commonly implemented contemporary prefetching systems use sequential prefetching only, possibly with strides of greater than one.
Reference: [Carey et al. 93] <author> M. J. Carey, D. J. Dewitt, and J. F. Naughton. </author> <title> The OO7 benchmark. </title> <booktitle> In Proceedings of the ACM SIGMOD Internation Conference on Management of Data, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: OO7 is a program that uses a large mapped file (100MB in our experiments): it is a parameterized object-oriented database benchmark <ref> [Carey et al. 93] </ref>. Websim is a simple proxy cache simulator for processing HTTP web traces that uses large in-core data structures. All of the programs above were executed on Unix and their behavior measured on-line. these experiments, the programs were running in 75% of their full memory requirement.
Reference: [Chen et al. 96] <author> I.-C. K. Chen, J. T. Coffey, and T. N. Mudge. </author> <title> Analysis of branch prediction via data compression. </title> <booktitle> In Proceedings of the Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VII), </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: There has also been a large body of work on more advanced prediction schemes, though few of these have been aimed at prefetching between memory and secondary storage. One Markov-based predictor attempts to predetermine control flow at branch sites <ref> [Chen et al. 96] </ref>, while others prefetch data from the off-chip to the on-chip cache or from DRAM to SRAM on the chip [Joseph & Grunwald 97, Alexander & Kedem 96].
Reference: [Curewitz et al. 93] <author> K. Curewitz, P. Krishnan, and J. S. Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> SIGMOD, </booktitle> <year> 1993. </year> <month> 14 </month>
Reference-contexts: Madhyastha and Reed classify access patterns by training a hidden Markov model [Mad-hyastha & Reed 97]. The interesting idea of using data compression techniques for prefetching has been studied theoretically and evaluated empirically in a database setting <ref> [Vitter & Krishnan 3 96, Curewitz et al. 93] </ref>. They show that any optimal character-by-character data compressor can be converted to a prefetcher with optimal fault rate assuming references are generated by a probabilistic finite automaton.
Reference: [Dahlin et al. 94] <author> M. Dahlin, R. Wang, T. Anderson, and D. Patterson. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In Proc. of the Conf. on Operating Systems Design and Implementation. USENIX, </booktitle> <month> November </month> <year> 1994. </year>
Reference: [Feeley et al. 95] <author> M. Feeley, W. Morgan, F. Pighin, A. Karlin, H. Levy, and C. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> In Proc. of the 15th ACM Sympoisum on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference: [Franklin & Gupta 74] <author> M. Franklin and R. Gupta. </author> <title> Computation of page fault probability from program transition diagrams. </title> <journal> Communications of the ACM, </journal> <volume> 17(4), </volume> <month> April </month> <year> 1974. </year>
Reference-contexts: Recent work by Liberatore [Liberatore 99] studies the validity of first-order Markov chains as a model for page reference patterns; this model was proposed by [Aho et al. 71] and studied by various authors <ref> [Franklin & Gupta 74, Karlin et al. 92] </ref>. Liberatore shows that first-order Markov chains are not a statistically valid model for instruction page traces. For these traces, he shows that predictions based on the last two requested pages achieve better prediction rates than the first order Markov model.
Reference: [Griffioen & Appleton 96] <author> J. Griffioen and R. Appleton. </author> <title> The design, implementation, and evaluation of a predictive file caching scheme. In University of Kentucky, </title> <type> Technical Report CS-264-96, </type> <month> June </month> <year> 1996. </year>
Reference-contexts: Palmer and Zdonik train an associative memory to recognize access patterns [Palmer & Zdonik 91] and Salem computes first-order statistics for prediction [Salem 91]. In the context of file systems, a number of speculative prefetching and prediction schemes have been proposed and evaluated <ref> [Griffioen & Appleton 96, Tait & Duchamp 90] </ref>. Madhyastha and Reed classify access patterns by training a hidden Markov model [Mad-hyastha & Reed 97].
Reference: [Joseph & Grunwald 97] <author> D. Joseph and D. Grunwald. </author> <title> Prefetching using markov predictors. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: One Markov-based predictor attempts to predetermine control flow at branch sites [Chen et al. 96], while others prefetch data from the off-chip to the on-chip cache or from DRAM to SRAM on the chip <ref> [Joseph & Grunwald 97, Alexander & Kedem 96] </ref>. Palmer and Zdonik train an associative memory to recognize access patterns [Palmer & Zdonik 91] and Salem computes first-order statistics for prediction [Salem 91].
Reference: [Karlin et al. 92] <author> A. R. Karlin, S. J. Phillips, and P. Raghavan. </author> <title> Markov paging. </title> <booktitle> In Proc. of the 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Recent work by Liberatore [Liberatore 99] studies the validity of first-order Markov chains as a model for page reference patterns; this model was proposed by [Aho et al. 71] and studied by various authors <ref> [Franklin & Gupta 74, Karlin et al. 92] </ref>. Liberatore shows that first-order Markov chains are not a statistically valid model for instruction page traces. For these traces, he shows that predictions based on the last two requested pages achieve better prediction rates than the first order Markov model.
Reference: [Kimbrel et al. 96] <author> T. Kimbrel, A. Tomkins, R. Patterson, B. Bershad, P. Cao, E. Felten, G. Gib-son, A. Karlin, and K. Li. </author> <title> A trace-drive comparison of algorithms for parallel prefetching and caching. </title> <booktitle> In Proc. of the 2nd Symp. on Operating Systems Design and Implementation. USENIX, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Some of these studies ignore the problem of prediction by providing future knowledge of the reference stream to the predictor <ref> [Cao et al. 95, Kimbrel et al. 96] </ref> or to the virtual memory system [Cao et al. 96] in advance. The most commonly implemented contemporary prefetching systems use sequential prefetching only, possibly with strides of greater than one.
Reference: [Kotz & Ellis 93] <author> D. Kotz and C. Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1), </volume> <month> January </month> <year> 1993. </year>
Reference-contexts: Because prefetching only overlaps computation and fetch latency without reducing overall transfer time, the most successful sequential prefetchers have used parallel disk systems to increase overall disk bandwidth <ref> [Kotz & Ellis 93] </ref>. There has also been a large body of work on more advanced prediction schemes, though few of these have been aimed at prefetching between memory and secondary storage.
Reference: [Liberatore 99] <author> V. Liberatore. </author> <title> Empirical investigation of the markov reference model. </title> <booktitle> Proceedings of ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1999. </year>
Reference-contexts: They show that any optimal character-by-character data compressor can be converted to a prefetcher with optimal fault rate assuming references are generated by a probabilistic finite automaton. Their theoretical work also assumes that as many pages as are desired can be prefetched simultaneously. Recent work by Liberatore <ref> [Liberatore 99] </ref> studies the validity of first-order Markov chains as a model for page reference patterns; this model was proposed by [Aho et al. 71] and studied by various authors [Franklin & Gupta 74, Karlin et al. 92].
Reference: [Madhyastha & Reed 97] <author> T. Madhyastha and D. Reed. </author> <title> Input/output access pattern classification using hidden markov chains. </title> <booktitle> In IOPADS, </booktitle> <year> 1997. </year>
Reference: [Mowry et al. 96] <author> T. Mowry, A. Demke, and O. Krieger. </author> <title> Automatic compiler-inserted I/O prefetching for out-of-core applications. </title> <booktitle> In Proc. of the 2nd Symp. on Operating Systems Design and Implementation. USENIX, </booktitle> <month> October </month> <year> 1996. </year>
Reference: [Palmer & Zdonik 91] <author> M. Palmer and S. Zdonik. </author> <title> Fido: A cache that learns to fetch. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large DataBases, </booktitle> <year> 1991. </year>
Reference-contexts: Palmer and Zdonik train an associative memory to recognize access patterns <ref> [Palmer & Zdonik 91] </ref> and Salem computes first-order statistics for prediction [Salem 91]. In the context of file systems, a number of speculative prefetching and prediction schemes have been proposed and evaluated [Griffioen & Appleton 96, Tait & Duchamp 90].
Reference: [Patterson et al. 95] <author> R. Patterson, G. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proc. of the 15th Symp. on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference: [Salem 91] <author> K. Salem. </author> <title> Adaptive prefetching for disk buffers. In CESDIS, </title> <institution> Goddard Space Flight Center, TR-91-46, </institution> <year> 1991. </year>
Reference-contexts: Palmer and Zdonik train an associative memory to recognize access patterns [Palmer & Zdonik 91] and Salem computes first-order statistics for prediction <ref> [Salem 91] </ref>. In the context of file systems, a number of speculative prefetching and prediction schemes have been proposed and evaluated [Griffioen & Appleton 96, Tait & Duchamp 90]. Madhyastha and Reed classify access patterns by training a hidden Markov model [Mad-hyastha & Reed 97].
Reference: [Tait & Duchamp 90] <author> C. Tait and D. Duchamp. </author> <title> Detection and exploitation of file working sets. </title> <institution> In Columbia University, </institution> <type> Technical Report CUCS-050-90, </type> <year> 1990. </year>
Reference-contexts: Palmer and Zdonik train an associative memory to recognize access patterns [Palmer & Zdonik 91] and Salem computes first-order statistics for prediction [Salem 91]. In the context of file systems, a number of speculative prefetching and prediction schemes have been proposed and evaluated <ref> [Griffioen & Appleton 96, Tait & Duchamp 90] </ref>. Madhyastha and Reed classify access patterns by training a hidden Markov model [Mad-hyastha & Reed 97].
Reference: [Vitter & Krishnan 96] <author> J. S. Vitter and P. Krishnan. </author> <title> Optimal prefetching via data compression. </title> <journal> Journal of the ACM, </journal> <volume> 43(5), </volume> <month> September </month> <year> 1996. </year> <month> 15 </month>
Reference: [Voelker et al. 98] <author> G. Voelker, E. Anderson, T. Kimbrel, M. Feeley, J. Chase, A. Karlin, and H. Levy. </author> <title> Implementing cooperative prefetching and caching in a globally-managed memory system. </title> <booktitle> In Proc. of the ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: While prefetching has been implemented in the context of a network-memory system, this implementation used full future knowledge as a basis for prefetching <ref> [Voelker et al. 98] </ref>. Our prefetching system differs from all of these schemes, in that it is adaptable, efficient, easily implementable, and operates on virtual memory pages. <p> The optimal improvement occurs when r=f , as it (nearly) does in 4.1 Performance in a Global-Memory Environment In order to test the ease of implementing fault-based prefetching and the prediction ability of the model, we obtained Voelker et al.'s prefetching global memory system <ref> [Voelker et al. 98] </ref> (PGMS) and modified it to use a one-level Markov net for speculatively prefetching virtual memory pages at fault time. The PGMS system supports remote paging of file system and virtual memory blocks from the memories of other nodes on the network.
Reference: [Yocum et al. 97] <author> K. Yocum, J. Chase, A. Gallatin, and A. R. Lebeck. </author> <title> Cut-through delivery in trapeze: An exercise in low-latency messaging. </title> <booktitle> In Proceedings of the IEEE International Symposium on High Performance Distributed Computing, </booktitle> <month> August </month> <year> 1997. </year> <month> 16 </month>
Reference-contexts: When network memory is used as backing store instead of disk, the cost of transferring (or prefetching) a page over a gigabit network is low perhaps 50 times lower than the cost of a disk fetch <ref> [Yocum et al. 97] </ref>. <p> The Markov predictor had a prediction accuracy of 80% for wave5. We ran the experiment on a small network of 600 Mhz DEC Alpha 21164 processors, connected by a 1Gb/sec Myrinet [Boden et al. 95] switched network running with Trapeze software and firmware <ref> [Yocum et al. 97] </ref> to accelerate page transfers. A page fault from a remote memory in PGMS on this configuration takes slightly less than 200 sec. In this environment, we measured only a 2% speedup for wave5.
References-found: 27

