URL: http://www.cs.toronto.edu/~tamer/publications/cvpr98.ps.gz
Refering-URL: http://www.cs.toronto.edu/~tamer/publications/publications.html
Root-URL: http://www.cs.toronto.edu
Email: e-mail: ftamer|dtg@cs.toronto.edu  
Title: Stereo and Color Analysis for Dynamic Obstacle Avoidance  
Author: Tamer F. Rabie and Demetri Terzopoulos 
Address: 10 King's College Road, Toronto, Ontario, M5S 3G4  
Affiliation: Department of Computer Science, University of Toronto  
Abstract: We develop a vision system for highly mobile autonomous agents that is capable of dynamic obstacle avoidance. We demonstrate the robust performance of the system in artificial animals with directable, foveated eyes, situated in physics-based virtual worlds. Through active perception, each agent controls its eyes and body by continuously analyzing photorealistic binocular retinal image streams. The vision system computes stereo disparity and segments looming targets in the low-resolution visual periphery while controlling eye movements to track an object fixated in the high-resolution fovea. It matches segmented targets against mental models of colored objects of interest in order to decide whether the segmented objects are harmless or represent dangerous obstacles. The latter are localized, enabling the artificial animal to exercise the sen-sorimotor control necessary to avoid collision. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Arakawa and M. Etoh. </author> <title> An integration algorithm for stereo, motion and color in real-time applications. </title> <journal> IEICE Transactions on Information and Systems, </journal> <volume> E78-D(12):1615 1620, </volume> <month> December </month> <year> 1995. </year>
Reference-contexts: Color and stereo cues have recently been integrated together with motion cues to implement a real-time passive stereo system that can de tect and identify moving objects for application to surveil-lance and human-computer interaction <ref> [1] </ref>. Disparity and color cues have also been combined to improve the focus of attention and recognition capabilities of an active vision system [12].
Reference: [2] <author> R. </author> <title> Bajcsy. Active perception. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 76(8):9961005, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Animals are active observers of their environment [11]. This fact has inspired a trend in computer vision popularly known as active vision <ref> [2, 3, 26] </ref>. The recently proposed animat vision paradigm offers a new approach to developing biomimetic active vision systems and experimenting with them [28]. <p> A boundary condition of zero disparity at image borders is applied. Also a zero disparity condition is applied to locations where no match is possible, such as across constant intensity areas. The disparity range used lies within <ref> [ 2 ; 2 ] </ref>.
Reference: [3] <author> D. Ballard. </author> <title> Animate vision. </title> <journal> Artificial Intelligence, </journal> <volume> 48:57 86, </volume> <year> 1991. </year>
Reference-contexts: 1 Introduction Animals are active observers of their environment [11]. This fact has inspired a trend in computer vision popularly known as active vision <ref> [2, 3, 26] </ref>. The recently proposed animat vision paradigm offers a new approach to developing biomimetic active vision systems and experimenting with them [28].
Reference: [4] <author> D.H. Ballard and L.E. Wixson. </author> <title> Object recognition using steerable filters at multiple scales. </title> <booktitle> In Proc. IEEE Workshop on Qualitative Vision, </booktitle> <pages> pages 2 10, </pages> <address> Los Alamitos, CA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Steerable filters, first developed by Freeman and Adelson [10], have been recently used for estimation of scene motion [14] and for object recognition <ref> [4] </ref> and stereopsis [16]. Simoncelli and Freeman have recently introduced a multi-scale, multi-orientation steerable filter image decomposition framework called the Steerable Pyramid [24] which we use as a front-end for our stereo algorithm. It has the advantage of producing feature descriptions that are both translation- and rotation-invariant.
Reference: [5] <author> M. Campani, A. Giachetti, and V. Torre. </author> <title> Optic flow and autonomous navigation. </title> <journal> Perception, </journal> <volume> 24:253267, </volume> <year> 1995. </year>
Reference-contexts: Disparity and color cues have also been combined to improve the focus of attention and recognition capabilities of an active vision system [12]. Recent work involving autonomous mobile robot systems have used single image cues for obstacle detection and avoidance such as stereo disparity [6], optical flow <ref> [5] </ref>, visual looming [15], peripheral optical flow [8], divergence of image flow and time-to-contact [7], and appearance based models of color and shape [21]. Shigang et. al. [23] have recently proposed a method for autonomous robot navigation along routes described by landmarks based on range and color information.
Reference: [6] <author> Y.C. Cho and H.S. Cho. </author> <title> A stereo vision-based obstacle detecting method for mobile robot navigation. </title> <journal> Robotica, </journal> <volume> 12(3):203 216, </volume> <month> May - June </month> <year> 1994. </year>
Reference-contexts: Disparity and color cues have also been combined to improve the focus of attention and recognition capabilities of an active vision system [12]. Recent work involving autonomous mobile robot systems have used single image cues for obstacle detection and avoidance such as stereo disparity <ref> [6] </ref>, optical flow [5], visual looming [15], peripheral optical flow [8], divergence of image flow and time-to-contact [7], and appearance based models of color and shape [21].
Reference: [7] <author> D. Coombs, M. Herman, T. Hong, and M. Nashman. </author> <title> Real-time obstacle avoidance using central flow divergence and peripheral flow. </title> <booktitle> In Proc. Fifth Inter. Conf. Computer Vision 156 166 180 190 (ICCV'95), </booktitle> <pages> pages 276 283, </pages> <publisher> MIT, </publisher> <address> Cambridge, MA, </address> <month> June 20 - 23 </month> <year> 1995. </year>
Reference-contexts: Recent work involving autonomous mobile robot systems have used single image cues for obstacle detection and avoidance such as stereo disparity [6], optical flow [5], visual looming [15], peripheral optical flow [8], divergence of image flow and time-to-contact <ref> [7] </ref>, and appearance based models of color and shape [21]. Shigang et. al. [23] have recently proposed a method for autonomous robot navigation along routes described by landmarks based on range and color information. The following sections describe our dynamic obstacle recognition and avoidance algorithms.
Reference: [8] <author> D. Coombs and K. Roberts. Bee-bot: </author> <title> Using peripheral optical flow to avoid obstacles. </title> <booktitle> Proc. SPIE Intelligent Robots and Computer Vision XI, </booktitle> <volume> 1825:714 721, </volume> <year> 1992. </year>
Reference-contexts: In fact, a great deal of mobility can be supported by low resolution peripheral vision, freeing the small, high resolution visual area to attend to important matters during navigation <ref> [8] </ref>. Spatially nonuniform retinal imaging provides opportunities for increased computational efficiency through economization of photoreceptors and focus of attention, but it forces the visual system to solve problems that do not generally arise with a uniform field of view. <p> Recent work involving autonomous mobile robot systems have used single image cues for obstacle detection and avoidance such as stereo disparity [6], optical flow [5], visual looming [15], peripheral optical flow <ref> [8] </ref>, divergence of image flow and time-to-contact [7], and appearance based models of color and shape [21]. Shigang et. al. [23] have recently proposed a method for autonomous robot navigation along routes described by landmarks based on range and color information.
Reference: [9] <author> D. Fleet, A. Jepson, and M. Jenkin. </author> <title> Phase-based disparity measurement. CVGIP: Image Understanding, </title> <address> 53:198 210, </address> <year> 1991. </year>
Reference-contexts: To assure stable performance, area-based stereo algorithms need suitably chosen correlation measures and a sufficiently large patch size, which is a computationally expensive process. Other methods extract local Fourier phases of left and right images and the phase difference at each location is used to estimate disparity <ref> [22, 17, 9] </ref>. Several approaches take into consideration available biological and neurophysiological data about the human visual system [19, 22, 16]. There is biological evidence that the pattern of light projected on the human retina is sampled and spatially filtered.
Reference: [10] <author> W.T. Freeman and E.H. Adelson. </author> <title> The design and use of steerable filters. </title> <journal> IEEE Trans. PAMI, </journal> <volume> 13(9):891 906, </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: Steerable filter is a term used to describe a class of spatial filters in which a filter of arbitrary orientation is synthesized as a linear combination of a set of basis filters. Steerable filters, first developed by Freeman and Adelson <ref> [10] </ref>, have been recently used for estimation of scene motion [14] and for object recognition [4] and stereopsis [16]. Simoncelli and Freeman have recently introduced a multi-scale, multi-orientation steerable filter image decomposition framework called the Steerable Pyramid [24] which we use as a front-end for our stereo algorithm. <p> The number of basis filters that are needed for steering the filter is n + 1 for an n th order filter. We use third-order filters, thus requiring four basis filters oriented at 0 ffi ; 45 ffi ; 90 ffi , and 135 ffi <ref> [10] </ref>. Fig. 4 (a) shows these four spatial basis filters (B i ) which form a steerable basis set; any orientation of this filter can be written as a linear combination of the basis filters. Fig. 4 (b) shows the two low-pass filters used to construct the pyramid.
Reference: [11] <author> J. J. Gibson. </author> <title> The Ecological Approach to Visual Perception. </title> <publisher> Houghton Mifflin, </publisher> <address> Boston, MA, </address> <year> 1979. </year>
Reference-contexts: 1 Introduction Animals are active observers of their environment <ref> [11] </ref>. This fact has inspired a trend in computer vision popularly known as active vision [2, 3, 26]. The recently proposed animat vision paradigm offers a new approach to developing biomimetic active vision systems and experimenting with them [28].
Reference: [12] <author> W.E.L. Grimson, A.L. Ratan, G. Klanderman, and A. O'Donnell. </author> <title> An active visual attention system to play 'where's waldo'. </title> <booktitle> In Proc. of the Image Understanding Workshop, </booktitle> <volume> volume 2, </volume> <pages> pages 1059 1065, </pages> <address> Monterey, CA, 13 - 16 Nov. </address> <year> 1994. </year>
Reference-contexts: Disparity and color cues have also been combined to improve the focus of attention and recognition capabilities of an active vision system <ref> [12] </ref>. Recent work involving autonomous mobile robot systems have used single image cues for obstacle detection and avoidance such as stereo disparity [6], optical flow [5], visual looming [15], peripheral optical flow [8], divergence of image flow and time-to-contact [7], and appearance based models of color and shape [21].
Reference: [13] <author> B.K.P. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The displacement is computed as a translational offset in the retinotopic coordinate system by a least squares minimization of the optical flow between successive image frames <ref> [13] </ref>. The optical flow stabilization method is robust only for small displacements between frames. Consequently, when the displacement of the target between frames is large enough that the method is likely to produce bad estimates, the foveation module is invoked to re-detect and re-foveate the target as described earlier. <p> color cues, the algorithms enable the animat to navigate through its virtual environment fixating and tracking a reference target in the fovea while avoiding obstacles that appear in its low resolution visual periphery. 3.1 Stereo Analysis Classical stereo analysis deals with the correspondence problem with two basic techniques; area-based methods <ref> [18, 13] </ref> and feature-based methods [19, 13]. Both types of stereo algorithms have computational problems. For example, in feature-based stereo algorithms the intensity data is first converted to a set of features assumed to be a more stable image property than raw intensities. <p> the animat to navigate through its virtual environment fixating and tracking a reference target in the fovea while avoiding obstacles that appear in its low resolution visual periphery. 3.1 Stereo Analysis Classical stereo analysis deals with the correspondence problem with two basic techniques; area-based methods [18, 13] and feature-based methods <ref> [19, 13] </ref>. Both types of stereo algorithms have computational problems. For example, in feature-based stereo algorithms the intensity data is first converted to a set of features assumed to be a more stable image property than raw intensities.
Reference: [14] <author> C.L. Huang and Y.T. Chen. </author> <title> Motion estimation method using a 3D steerable filter. Image and Vision Computing, </title> <address> 13(1):2132, </address> <year> 1995. </year>
Reference-contexts: Steerable filters, first developed by Freeman and Adelson [10], have been recently used for estimation of scene motion <ref> [14] </ref> and for object recognition [4] and stereopsis [16]. Simoncelli and Freeman have recently introduced a multi-scale, multi-orientation steerable filter image decomposition framework called the Steerable Pyramid [24] which we use as a front-end for our stereo algorithm.
Reference: [15] <author> K. Joarder and D. Raviv. </author> <title> A new method to calculate looming for autonomous obstacle avoidance. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf. (CVPR'94), </booktitle> <pages> pages 777 780, </pages> <year> 1994. </year>
Reference-contexts: Recent work involving autonomous mobile robot systems have used single image cues for obstacle detection and avoidance such as stereo disparity [6], optical flow [5], visual looming <ref> [15] </ref>, peripheral optical flow [8], divergence of image flow and time-to-contact [7], and appearance based models of color and shape [21]. Shigang et. al. [23] have recently proposed a method for autonomous robot navigation along routes described by landmarks based on range and color information.
Reference: [16] <author> D.G. Jones and J. Malik. </author> <title> A computational framework for determining stereo correspondence from a set of linear spatial filters. </title> <booktitle> In Proc. Euro. Conf. Computer Vision (ECCV'92), </booktitle> <pages> pages 395 410, </pages> <address> Portofino, Italy, </address> <year> 1992. </year>
Reference-contexts: Other methods extract local Fourier phases of left and right images and the phase difference at each location is used to estimate disparity [22, 17, 9]. Several approaches take into consideration available biological and neurophysiological data about the human visual system <ref> [19, 22, 16] </ref>. There is biological evidence that the pattern of light projected on the human retina is sampled and spatially filtered. <p> Steerable filters, first developed by Freeman and Adelson [10], have been recently used for estimation of scene motion [14] and for object recognition [4] and stereopsis <ref> [16] </ref>. Simoncelli and Freeman have recently introduced a multi-scale, multi-orientation steerable filter image decomposition framework called the Steerable Pyramid [24] which we use as a front-end for our stereo algorithm. It has the advantage of producing feature descriptions that are both translation- and rotation-invariant.
Reference: [17] <author> K. Langley, T.J. Atherton, R.G. Wilson, and M.H.E. Lar-combe. </author> <title> Vertical and horizontal disparities from phase. </title> <booktitle> In Proc. Euro. Conf. Computer Vision (ECCV'90), </booktitle> <pages> pages 315 325, </pages> <year> 1990. </year>
Reference-contexts: To assure stable performance, area-based stereo algorithms need suitably chosen correlation measures and a sufficiently large patch size, which is a computationally expensive process. Other methods extract local Fourier phases of left and right images and the phase difference at each location is used to estimate disparity <ref> [22, 17, 9] </ref>. Several approaches take into consideration available biological and neurophysiological data about the human visual system [19, 22, 16]. There is biological evidence that the pattern of light projected on the human retina is sampled and spatially filtered.
Reference: [18] <author> B.D. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> Proc. Image Understanding Workshop, </booktitle> <pages> pages 121130, </pages> <year> 1981. </year>
Reference-contexts: color cues, the algorithms enable the animat to navigate through its virtual environment fixating and tracking a reference target in the fovea while avoiding obstacles that appear in its low resolution visual periphery. 3.1 Stereo Analysis Classical stereo analysis deals with the correspondence problem with two basic techniques; area-based methods <ref> [18, 13] </ref> and feature-based methods [19, 13]. Both types of stereo algorithms have computational problems. For example, in feature-based stereo algorithms the intensity data is first converted to a set of features assumed to be a more stable image property than raw intensities.
Reference: [19] <author> D. Marr and T. Poggio. </author> <title> A computational theory of human stereo vision. </title> <journal> Proc. R. Soc. Lond. B., </journal> <volume> 204:301 328, </volume> <year> 1979. </year>
Reference-contexts: the animat to navigate through its virtual environment fixating and tracking a reference target in the fovea while avoiding obstacles that appear in its low resolution visual periphery. 3.1 Stereo Analysis Classical stereo analysis deals with the correspondence problem with two basic techniques; area-based methods [18, 13] and feature-based methods <ref> [19, 13] </ref>. Both types of stereo algorithms have computational problems. For example, in feature-based stereo algorithms the intensity data is first converted to a set of features assumed to be a more stable image property than raw intensities. <p> Other methods extract local Fourier phases of left and right images and the phase difference at each location is used to estimate disparity [22, 17, 9]. Several approaches take into consideration available biological and neurophysiological data about the human visual system <ref> [19, 22, 16] </ref>. There is biological evidence that the pattern of light projected on the human retina is sampled and spatially filtered.
Reference: [20] <author> T.F. Rabie and D. Terzopoulos. </author> <title> Motion and color analysis for animat perception. </title> <booktitle> In Proc. Thirteenth National Conf. on Artificial Intelligence (AAAI'96), </booktitle> <pages> pages 1090 1097, </pages> <address> Portland, Oregon, </address> <month> August 4-8 </month> <year> 1996. </year>
Reference-contexts: In the present work, the fish animat serves as an autonomous mobile robot inhabiting a photorealistic, dynamic environment. Our new navigation algorithms significantly enhance the prototype animat vision system implemented in prior work <ref> [28, 20, 27] </ref>. They support more robust vision-guided navigation, including obstacle recognition and avoidance.
Reference: [21] <author> G. Salgian and D.H. Ballard. </author> <title> Visual routines for autonomous driving. </title> <booktitle> In Proc. Sixth Inter. Conf. Computer Vision (ICCV'98), </booktitle> <address> Bombay, India, </address> <month> January </month> <year> 1998. </year>
Reference-contexts: Recent work involving autonomous mobile robot systems have used single image cues for obstacle detection and avoidance such as stereo disparity [6], optical flow [5], visual looming [15], peripheral optical flow [8], divergence of image flow and time-to-contact [7], and appearance based models of color and shape <ref> [21] </ref>. Shigang et. al. [23] have recently proposed a method for autonomous robot navigation along routes described by landmarks based on range and color information. The following sections describe our dynamic obstacle recognition and avoidance algorithms.
Reference: [22] <author> T. Sanger. </author> <title> Stereo disparity computation using gabor filters. </title> <journal> Biological Cybernetics, </journal> <volume> 59:405 418, </volume> <year> 1988. </year>
Reference-contexts: To assure stable performance, area-based stereo algorithms need suitably chosen correlation measures and a sufficiently large patch size, which is a computationally expensive process. Other methods extract local Fourier phases of left and right images and the phase difference at each location is used to estimate disparity <ref> [22, 17, 9] </ref>. Several approaches take into consideration available biological and neurophysiological data about the human visual system [19, 22, 16]. There is biological evidence that the pattern of light projected on the human retina is sampled and spatially filtered. <p> Other methods extract local Fourier phases of left and right images and the phase difference at each location is used to estimate disparity [22, 17, 9]. Several approaches take into consideration available biological and neurophysiological data about the human visual system <ref> [19, 22, 16] </ref>. There is biological evidence that the pattern of light projected on the human retina is sampled and spatially filtered.
Reference: [23] <author> L. Shigang, A. Ochi, and S. Tsuji. </author> <title> Route description by landmarks. </title> <booktitle> In Proc. of the Intelligent Vehicles '95 Symposium, </booktitle> <pages> pages 454 459, </pages> <address> Detroit, MI, </address> <month> 25 - 26 September </month> <year> 1995. </year>
Reference-contexts: Shigang et. al. <ref> [23] </ref> have recently proposed a method for autonomous robot navigation along routes described by landmarks based on range and color information. The following sections describe our dynamic obstacle recognition and avoidance algorithms.
Reference: [24] <author> E.P. Simoncelli and W.T. Freeman. </author> <title> The steerable pyramid: A flexible architecture for multi-scale derivative computation. </title> <booktitle> In IEEE Inter. Conf. on Image Processing, </booktitle> <pages> pages 444 447, </pages> <address> Washington, DC, </address> <month> October 23-26 </month> <year> 1995. </year>
Reference-contexts: Steerable filters, first developed by Freeman and Adelson [10], have been recently used for estimation of scene motion [14] and for object recognition [4] and stereopsis [16]. Simoncelli and Freeman have recently introduced a multi-scale, multi-orientation steerable filter image decomposition framework called the Steerable Pyramid <ref> [24] </ref> which we use as a front-end for our stereo algorithm. It has the advantage of producing feature descriptions that are both translation- and rotation-invariant. Our disparity estimation algorithm starts by decomposing the left and right images into steerable pyramid representations. <p> Fig. 4 (b) shows the two low-pass filters used to construct the pyramid. Typically, L 0 (w) is chosen to be L 1 (w=2) in the frequency domain <ref> [24] </ref>. Fig. 5 shows an example of a three-level steerable pyramid for a single orientation for an image acquired by the animat's right eye.
Reference: [25] <author> M. Swain and D. Ballard. </author> <title> Color indexing. </title> <journal> Inter. J. Computer Vision, </journal> <volume> 7:11 32, </volume> <year> 1991. </year>
Reference-contexts: To detect and localize any target that may be imaged in the low resolution periphery of its retinas, the animat vision system of the fish employs an improved version of a color indexing algorithm proposed by Swain <ref> [25] </ref>. 1 Since each model object has a unique color histogram signature, it can be detected in the retinal image by histogram intersection and localized by histogram backprojection. 2.3 Saccadic Eye Movements When a target is detected in the visual periphery, the eyes will saccade to the angular offset of the <p> To localize a detected obstacle accurately, the exact region of support of this obstacle must be properly segmented out from the original segmentation obtained above. To tackle this non-trivial problem, we make use of Swain's color histogram backprojection methods <ref> [25] </ref>. Briefly, histogram backprojection gives large weights to pixel locations in the image whose color histogram closely resembles the color histogram of the model.
Reference: [26] <author> M.J. Swain and M.A. Stricker. </author> <title> Promising directions in active vision. </title> <journal> Inter. J. Computer Vision, </journal> <volume> 11(2):109 126, </volume> <year> 1993. </year>
Reference-contexts: 1 Introduction Animals are active observers of their environment [11]. This fact has inspired a trend in computer vision popularly known as active vision <ref> [2, 3, 26] </ref>. The recently proposed animat vision paradigm offers a new approach to developing biomimetic active vision systems and experimenting with them [28].
Reference: [27] <author> D. Terzopoulos. </author> <title> Modeling living systems for computer vision. </title> <booktitle> In Proc. Int. Joint Conf. Artificial Intelligence (IJ-CAI'95), </booktitle> <pages> pages 10031013, </pages> <year> 1995. </year>
Reference-contexts: In the present work, the fish animat serves as an autonomous mobile robot inhabiting a photorealistic, dynamic environment. Our new navigation algorithms significantly enhance the prototype animat vision system implemented in prior work <ref> [28, 20, 27] </ref>. They support more robust vision-guided navigation, including obstacle recognition and avoidance.
Reference: [28] <author> D. Terzopoulos and T.F. Rabie. </author> <title> Animat vision: Active vision in artificial animals. </title> <journal> Videre: Journal of Computer Vision Research, </journal> <volume> 1(1):219, </volume> <month> September </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Animals are active observers of their environment [11]. This fact has inspired a trend in computer vision popularly known as active vision [2, 3, 26]. The recently proposed animat vision paradigm offers a new approach to developing biomimetic active vision systems and experimenting with them <ref> [28] </ref>. Rather than allow the limitations of available robot hardware to hamper research, an-imat vision prescribes the use of virtual robots that take the form of realistic artificial animals, or animats, situated in physics-based virtual worlds. <p> In the present work, the fish animat serves as an autonomous mobile robot inhabiting a photorealistic, dynamic environment. Our new navigation algorithms significantly enhance the prototype animat vision system implemented in prior work <ref> [28, 20, 27] </ref>. They support more robust vision-guided navigation, including obstacle recognition and avoidance. <p> review the animat vision system in the next section before presenting, in the subsequent sections, our new work on integrating stereo disparity and color analysis for animat navigation and perception. 2 A Prototype Animat Vision System The basic functionality of the animat vision system, which is described in detail in <ref> [28] </ref>, starts with binocular perspective projection of the color 3D world onto the animat's 2D retinas. Retinal imaging is accomplished by photorealistic graphics rendering of the world from the an-imat's point of view. This projection respects occlusion relationships among objects. <p> The details of the improved algorithm are presented in <ref> [28] </ref>. l = 0 l = 1 l = 2 l = 3 l = 0 l = 1 l = 2 l = 3 Left eye Right eye (b) Composited retinal images (borders of composited component images are shown in white). <p> The corresponding segmented pixels in the right eye image give the actual segmentation of the color objects. The color histogram of this segmentation is intersected with the color histogram of the mental models of stored obstacles, using the color methods described in <ref> [28] </ref>. A match indicates that this segment contains an obstacle; no match indicates a false alarm and the animat continues in its current path. To localize a detected obstacle accurately, the exact region of support of this obstacle must be properly segmented out from the original segmentation obtained above.
Reference: [29] <author> D. Terzopoulos, X. Tu, and R. Grzeszczuk. </author> <title> Artificial fishes: Autonomous locomotion, perception, behavior, and learning in a simulated physical world. </title> <journal> Artificial Life, </journal> <volume> 1(4):327 351, </volume> <year> 1994. </year>
Reference-contexts: Building upon the animat vision paradigm, the stereo and color based motor control algorithms that we propose in this paper are implemented and evaluated within artificial fishes in a virtual marine world (Fig. 1). The fish animats are the result of research in the domain of artificial life (see <ref> [29] </ref> for the details). In the present work, the fish animat serves as an autonomous mobile robot inhabiting a photorealistic, dynamic environment. Our new navigation algorithms significantly enhance the prototype animat vision system implemented in prior work [28, 20, 27]. <p> Up/down turn motor commands are issued to the fish's pectoral fins, with an above-threshold positive OE P interpreted as up and negative as down. The motor controllers are explained in <ref> [29] </ref>.
Reference: [30] <author> R.A. Young. </author> <title> Simulation of human retinal function with the Gaussian derivative model. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf. (CVPR'86), </booktitle> <pages> pages 564 569, </pages> <year> 1986. </year>
Reference-contexts: Very early in cortical visual processing, receptive fields become oriented and are well ap proximated by linear spatial filters, with impulse response functions that are similar to partial derivatives of a Gaussian function <ref> [30] </ref>. Our animat vision approach for estimating stereo disparity draws ideas from early visual processing in the primate cortex. We implement the receptive fields as steerable spatial filters that process the input images.
References-found: 30

