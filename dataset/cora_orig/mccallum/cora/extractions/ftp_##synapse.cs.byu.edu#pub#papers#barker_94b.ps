URL: ftp://synapse.cs.byu.edu/pub/papers/barker_94b.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: GENERALIZATION BY CONTROLLED EXPANSION OF EXAMPLES  
Author: Cory Barker Tony Martinez 
Address: Provo, Utah 84602  Provo, Utah 84602  
Affiliation: Computer Science Department Brigham Young University  Computer Science Department Brigham Young University  
Note: In Proceedings of the 7th International Symposium on Artificial Intelligence, pp. 142-149, 1994.  
Abstract: SG (Specific to General) is a learning system that derives general rules from specific examples. SG learns incrementally with good speed and generalization. The SG network is built of many simple nodes that adapt to the problem being learned. Learning is done without requiring user adjustment of sensitive parameters and noise is tolerated with graceful degradation in performance. Nodes learn important features in the input space and then monitor the ability of the features to predict output values. Learning is O(n log n) for each example, where n is the number of nodes in the network, and the number of inputs and output values are treated as constants. An enhanced network topology reduces time complexity to O(log n). Empirical results show that the model gives good generalization and that learning converges in a small number of training passes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ackley, D. H., Hinton, D. E., & Sejnowski, T. J. </author> <year> (1985). </year> <title> A Learning Algorithm for Boltzmann Machines. </title> <journal> Cognitive Science, </journal> <volume> 9, </volume> <pages> 147-169. </pages>
Reference-contexts: The system described in this work uses symbolic representations and generalization rules often found in symbolic machine learning [3, 4, 6]. The system uses a parallel network of simple nodes and the system makes incremental changes to its state as is commonly found in neural networks <ref> [1, 7, 8] </ref>. The new system described in this work provides: Good generalization. Good learning speed. Incremental learning. Graceful degradation in the presence of noise. Ease of use. A massively parallel architectural model. Inductive learning systems extract general rules from specific examples.
Reference: [2] <author> Barker, J. C. & Martinez, T. R. </author> <year> (1993). </year> <title> Generalization by Controlled Intersection of Examples. </title> <booktitle> Proceedings of the Sixth Australian Joint Conference on Artificial Intelligence. </booktitle>
Reference-contexts: In feature-intersection, nodes create new candidate features by computing the intersection of Generalization by Controlled Expansion of Examples their current feature and the feature defined by the training example. Details are given in <ref> [2] </ref>. In feature-generalization, nodes create new candidate features by dropping single input/value pairs from their current feature. Details are given in the next section. After the training example is learned, nodes in the network that are not needed delete themselves.
Reference: [3] <author> Michalski, R. S. </author> <year> (1983). </year> <title> A Theory and Methodology of Inductive Learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 111-161. </pages>
Reference-contexts: INTRODUCTION This paper describes a system for inductive machine learning. The system described in this work uses symbolic representations and generalization rules often found in symbolic machine learning <ref> [3, 4, 6] </ref>. The system uses a parallel network of simple nodes and the system makes incremental changes to its state as is commonly found in neural networks [1, 7, 8]. The new system described in this work provides: Good generalization. Good learning speed. Incremental learning.
Reference: [4] <author> Mitchell, T. M. </author> <year> (1982). </year> <title> Generalization as Search. </title> <journal> Artificial Intelligence, </journal> <volume> 18, </volume> <pages> 203-226 </pages>
Reference-contexts: INTRODUCTION This paper describes a system for inductive machine learning. The system described in this work uses symbolic representations and generalization rules often found in symbolic machine learning <ref> [3, 4, 6] </ref>. The system uses a parallel network of simple nodes and the system makes incremental changes to its state as is commonly found in neural networks [1, 7, 8]. The new system described in this work provides: Good generalization. Good learning speed. Incremental learning.
Reference: [5] <author> Murphy, P. M. & Aha, D. W. </author> <year> (1992). </year> <title> U C I Repository of machine learning databases. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference-contexts: Node 6 has no competition for creation of a new node. Node 7 is created with counters initialized from Nodes 1 and 6. The final network is shown in Figure 6. RESULTS SG was tested using 10 data sets obtained from UC Irvine <ref> [5] </ref>; breast cancer diagnosis, chess endgame, hepatitis diagnosis, iris plant classification, LED digit recognition, mushroom classification, soybean disease diagnosis, tic-tac-toe endgame, 1984 House voting records, and animal classification. Both learning models (intersection and generalization) were tested on each problem. The results given here are averages over ten runs.
Reference: [6] <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: INTRODUCTION This paper describes a system for inductive machine learning. The system described in this work uses symbolic representations and generalization rules often found in symbolic machine learning <ref> [3, 4, 6] </ref>. The system uses a parallel network of simple nodes and the system makes incremental changes to its state as is commonly found in neural networks [1, 7, 8]. The new system described in this work provides: Good generalization. Good learning speed. Incremental learning.
Reference: [7] <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. </title> <journal> Psychological Review, </journal> <volume> 65, </volume> <pages> 386-408. </pages>
Reference-contexts: The system described in this work uses symbolic representations and generalization rules often found in symbolic machine learning [3, 4, 6]. The system uses a parallel network of simple nodes and the system makes incremental changes to its state as is commonly found in neural networks <ref> [1, 7, 8] </ref>. The new system described in this work provides: Good generalization. Good learning speed. Incremental learning. Graceful degradation in the presence of noise. Ease of use. A massively parallel architectural model. Inductive learning systems extract general rules from specific examples.
Reference: [8] <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning Internal Representations by Error Propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland (Eds.) </editor> <title> Pa ra lle l Di str ibu te d Processing (Vol. </title> <type> 1). </type> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The system described in this work uses symbolic representations and generalization rules often found in symbolic machine learning [3, 4, 6]. The system uses a parallel network of simple nodes and the system makes incremental changes to its state as is commonly found in neural networks <ref> [1, 7, 8] </ref>. The new system described in this work provides: Good generalization. Good learning speed. Incremental learning. Graceful degradation in the presence of noise. Ease of use. A massively parallel architectural model. Inductive learning systems extract general rules from specific examples.
References-found: 8

