URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-90-941/CS-TR-90-941.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-90-941/
Root-URL: http://www.cs.wisc.edu
Title: On the Complexity of the Policy Improvement Algorithm for Markov Decision Processes  
Author: Mary Melekopoglou Anne Condon 
Address: 1210 West Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin Madison  
Abstract: We consider the complexity of the policy improvement algorithm for Markov decision processes. We show that four variants of the algorithm require exponential time in the worst case. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Condon. </author> <title> The Complexity of Stochastic Games. </title> <journal> Information and Computation, </journal> <note> to appear, 1990. Also available as Technical Report Number 863, </note> <institution> Computer Sciences Department, University of Wisconsin-Madison. </institution>
Reference: [2] <author> C. Derman. </author> <title> Finite State Markov Decision Processes. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference: [3] <author> J. Gill. </author> <title> The Computational Complexity of Probabilistic Turing Machines. </title> <journal> SIAM Journal on Computing, </journal> <volume> 6 </volume> <pages> 675-695, </pages> <year> 1977. </year>
Reference: [4] <author> Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> M.I.T. Press, </publisher> <year> 1960. </year>
Reference-contexts: 1. Introduction Finding an optimal policy in a Markov decision process is a classical problem in optimization theory. Although the problem is solvable in polynomial time using linear programming (Howard <ref> [4] </ref>, Khachian [7]), in practice, the policy improvement algorithm is often used. We show that four natural variants of this algorithm require exponential time in the worst case. A stationary Markov decision process consists of a finite number of states, which includes an initial state s 0 . <p> The decision d incurs a cost c (s,d). A policy S is a mapping from states to decisions. The cost of a policy can be measured in different ways (see Howard <ref> [4] </ref>). The discounted cost with discount factor b is the expectation of the sum over all t of c (s t, S (s t ))b t . The total cost is the expectation of the sum over all t of c (s t, S (s t )). <p> Repeat this until no state is switchable. At that point, an optimal policy is reached. Since the number of policies is at most exponential in the number of states, the algorithm halts within exponential time. See Howard <ref> [4] </ref> for a proof of correctness of this algorithm. We describe this algorithm in more detail in the next section. There are many versions of this algorithm, depending on how the states to be switched are selected at a step of the algorithm.
Reference: [5] <author> R. J. Jeroslow. </author> <title> The Simplex Algorithm with the Pivot Rule of Maximizing Criterion Improvement. </title> <journal> Discrete Math., </journal> <volume> 4 </volume> <pages> 367-378, </pages> <year> 1973. </year>
Reference: [6] <author> D. S. Johnson, C. H. Papadimitriou and M. Yannakakis. </author> <title> How Easy is Local Search? Journal on Computer and System Sciences, </title> <booktitle> 37 </booktitle> <pages> 79-100, </pages> <year> 1988. </year>
Reference: [7] <author> L. G. Khachiyan. </author> <title> A Polynomial algorithm in linear programming. </title> <journal> Soviet Math Dokl., </journal> <volume> 20 </volume> <pages> 191-194, </pages> <year> 1979. </year>
Reference-contexts: 1. Introduction Finding an optimal policy in a Markov decision process is a classical problem in optimization theory. Although the problem is solvable in polynomial time using linear programming (Howard [4], Khachian <ref> [7] </ref>), in practice, the policy improvement algorithm is often used. We show that four natural variants of this algorithm require exponential time in the worst case. A stationary Markov decision process consists of a finite number of states, which includes an initial state s 0 .
Reference: [8] <author> V. Klee and G. Minty. </author> <title> How Good is the Simplex Algorithm? Inequalities III, </title> <editor> O. Shisha, </editor> <publisher> Academic Press, </publisher> <address> New York, 159-175, </address> <year> 1979. </year>
Reference: [9] <author> C. H. Papadimitriou, A. A. Sch"affer and M. Yannakakis. </author> <title> On the Complexity of Local Search. </title> <booktitle> Proceedings of the 22nd Annual Symposium on the Theory of Computing (STOC), </booktitle> <pages> 438-445, </pages> <year> 1990. </year>
Reference: [10] <author> H. J. M. Peters and O. J. Vrieze. </author> <title> Surveys in game theory and related topics, CWI Tract 39. </title> <publisher> Centrum voor Wiskunde en Informatica, </publisher> <address> Amsterdam, </address> <year> 1987. </year>
Reference: [11] <author> L. S. Shapley. </author> <title> Stochastic Games. </title> <booktitle> Proceedings of the National Academy of Sciences, U.S.A, </booktitle> <volume> 39: </volume> <pages> 1095-1100, </pages> <year> 1953. </year>
Reference: [12] <author> C. A. Tovey. </author> <title> Low Order Polynomial Bounds on the Expected Performance of Local Improvement Algorithms. </title> <journal> Mathematical Programming, </journal> <volume> 35(2): </volume> <pages> 193-224, </pages> <year> 1986. </year> - -- 
Reference-contexts: It would also be interesting to obtain results on the average case performance of the policy improvement algorithm, on reasonable distributions of inputs. The work of Tovey <ref> [12] </ref>, may be a useful start in this direction. A select procedure widely used in practice is one in which all switchable states are switched at each iteration. Does this algorithm require exponential time in the worst case? - --
References-found: 12

