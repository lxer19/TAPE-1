URL: http://www.cs.umn.edu/Users/dept/users/sycho/cho.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/sycho/
Root-URL: http://www.cs.umn.edu
Email: E-mail: sangyeun.cho@acm.org  
Title: Decoupling Local Variable Accesses in a Wide-Issue Superscalar Processor  
Author: Sangyeun Cho, Pen-Chung Yew and Gyungho Lee 
Address: Yongin-City, Korea Minneapolis, MN 55455 San Antonio, TX 78249  
Affiliation: MCU Team, System LSI Div. Dept. of Comp. Sci. and Eng. Division of Engineering Samsung Electronics Co. University of Minnesota Univ. of Texas at San Antonio  
Abstract: Providing adequate data bandwidth is extremely important for a wide-issue superscalar processor to achieve its full performance potential. Adding a large number of ports to a data cache, however, becomes increasingly inefficient and can add to the hardware complexity significantly. This paper takes an alternative or complementary approach for providing more data bandwidth, called the data-decoupled architecture. The approach, with support from the compiler and/or hardware, partitions the memory stream into two independent streams early in the processor pipeline, and feeds each stream to a separate memory access queue and cache. Under this model, the paper studies the potential of decoupling memory accesses to program's local variables that are allocated on the run-time stack. Using a set of integer and floating-point programs from the SPEC95 benchmark suite, it is shown that local variable accesses constitute a large portion of all the memory references, while their reference space is very small, averaging around 7 words per (static) procedure. To service local variable accesses quickly, two optimizations, fast data forwarding and access combining, are proposed and studied. Some of the important design parameters, such as the cache size, the number of cache ports, and the degree of access combining, are studied based on simulations. The potential performance of the proposed scheme is measured using various configurations, and it is concluded that the scheme can become a viable alternative to building a single multi-ported data cache. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Bergner, P. Dahl, D. Engebretsen, and M. O'Keefe. </author> <title> Spill Code Minimization via Interference Region Spilling, </title> <booktitle> Proc. of the 1997 ACM SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. 287 295. </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Memory accesses to these variables, usually indexed by stack pointer ($sp), can constitute a large fraction of overall memory references [11]. For example, spill codes can produce a significant number of memory references at run time, as many as 20% of all the executed instructions <ref> [1] </ref>. Even though spill codes could be eliminated or reduced by increasing the number of (architected) registers and/or by using a more sophisticated register allocation scheme, such attempts are restricted by the current technological trends; Aggressive ILP optimizations often increase register pressure and could introduce extra spill codes. <p> Chow and Hennessy [7] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 7, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [2] <author> P. Briggs, K. D. Cooper, K. Kennedy, and L. Torczon. </author> <title> Coloring Heuristics for Register Allocation, </title> <booktitle> Proc. of the 1989 ACM SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. 275 284. </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Chow and Hennessy [7] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 7, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [3] <author> D. Burger and T. M. Austin. </author> <title> The SimpleScalar Tool Set, </title> <note> Version 2.0, Computer Sciences Department Technical Report, No. 1342, </note> <institution> Univ. of Wisconsin, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: benefit from the unrolling optimization or software pipelining due to high register pressure, may use such program optimizations based on a new cost model. 3 Experimental Setup 3.1 Simulator and machine model We develop and use a cycle-accurate execution-driven simulator derived from the sim-outorder simulator in the SimpleScalar tool set <ref> [3] </ref>. The machine model used in the experiments is a superscalar processor that supports out-of-order issue and execution, based on the Register Update Unit (RUU) [24]. The RUU scheme 6 Two such functions have been found in the programs studied load-core () and dumpcore () in 124.m88ksim.
Reference: [4] <author> G. J. Chaitin. </author> <title> Register Allocation and Spilling via Graph Coloring, </title> <booktitle> Proc. of the 1982 ACM SIGPLAN Symp. on Compiler Construction, </booktitle> <pages> pp. 98 105, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: Chow and Hennessy [7] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 7, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [5] <author> S. Cho, P.-C. Yew, and G. Lee. </author> <title> Decoupling Local Variable Accesses in a Wide-Issue Superscalar Processor, </title> <type> Technical Report #98-020, </type> <institution> Dept. of Computer Sci. and Eng., Univ. of Minnesota, </institution> <month> May </month> <year> 1998. </year>
Reference: [6] <author> S. Cho, P.-C. Yew, and G. Lee. </author> <title> Access Region Locality for High-Bandwidth Processor Memory System Design, </title> <type> Technical Report #99-004, </type> <institution> Dept. of Computer Sci. and Eng., Univ. of Minnesota, </institution> <month> Feb. </month> <year> 1999. </year>
Reference-contexts: First, decoded memory access instructions should be partitioned into independent streams before they enter the instruction window, as shown in Figure 1 (b). Either run-time or compile-time information on per-reference access type is needed. When run-time speculation <ref> [6, 15, 30, 17] </ref> is used for the classification, verification and recovery actions are required to handle mispredictions. <p> Extracting classification information at compile time, on the other hand, can simplify the hardware design, while putting more burden on the compiler. Using a hybrid of both compile-time and run-time information can be more flexible and cost-effective than using either one alone <ref> [6] </ref>. 3 Second, each partitioned stream should contain an adequate amount of workload to justify the scheduling and possible communication overhead. Moreover, different types of memory references should be interleaved evenly for the approach to be effective. <p> Using a simple 1-bit hardware predictor storing the previous access region of these small number of instructions results in pointer. about 99.9% of all the dynamic memory references correctly classified into local and non-local accesses <ref> [6] </ref>. Therefore, this paper assumes that a processor can accurately separate the local accesses from others with such hardware and software techniques. <p> Compared to these approaches, data decoupling requires much simpler mechanism for dynamic classification a small hardware table and simple instruction decoding logic <ref> [6] </ref>. The technique proposed in this paper can be implemented with the above techniques together. In fact, it may expose more opportunities for dynamic dependence speculation methods.
Reference: [7] <author> F. C. Chow and J. L. Hennessy. </author> <title> The Priority-Based Coloring Approach to Register Allocation, </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 12:4, </volume> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: Among current microprocessors, Sun UltraSparc employs a special register file structure called register window to reduce the cost of a procedure call/return [29]. Chow and Hennessy <ref> [7] </ref> categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics [4, 2, 7, 1] try to efficiently assign a set of hard registers to the live ranges. <p> Chow and Hennessy [7] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference. Register allocation techniques with various heuristics <ref> [4, 2, 7, 1] </ref> try to efficiently assign a set of hard registers to the live ranges. Increasing the number of registers or using a sophisticated register allocation scheme will cut down the number of memory references in the first category above.
Reference: [8] <author> D. Ditzel and R. McLellan. </author> <title> Register Allocation for Free: The C Machine Stack Cache, </title> <booktitle> Proc. of the Symp. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 48 56, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: To service the local variable accesses efficiently, we study two optimizations fast data forwarding and access combining. Although there have been efforts to optimize local variable accesses with hardware support in the past <ref> [8, 12, 26] </ref>, little work has been done to study their performance impact in the context of a wide-issue superscalar processor with a multi-ported data cache. <p> A locally declared variable whose storage class is static is not local in our definition. programs studied. The results under 99% percentile are shown to ease reading. Ditzel and McLellan <ref> [8] </ref> report similar results. compatibility, as exemplified by the Intel's x86 architecture, may disallow increasing the size of a register file. tions in a set of SPEC95 programs [27]. 5 A large fraction of memory references are to local variables, with an average of 30% of loads and 48% of stores <p> Floating-point programs produced similar numbers also. The results suggest that if a separate cache is used to hold the local variables, it need not be large to obtain a high hit rate. In fact, this has been the motivation for some previous work <ref> [8, 12, 26, 29] </ref>. The high frequency of local variable accesses and their strong locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Moreover, identifying local variables in the stack frames is relatively easy for hardware or compiler. 2.2.2 Architectural support To service local variable accesses efficiently, a specialized hardware organization to simulate the run-time stack may appear attractive <ref> [8, 12, 26] </ref>. However, we use a more general cache design called the local variable cache (LVC) in the framework of our data-decoupled architecture. This approach has two advantages; First, the LVC is a conventional cache and can leverage the most efficient current design. <p> Alternatively, the processor can assume those accesses indexed by $sp (or frame pointer, $fp) as local variable accesses <ref> [8] </ref>. Not all local accesses, however, may be indexed by $sp or $fp; When the address of a local variable is taken to index through the data structure, $sp is not used. <p> It is worth noting that the recent 21264 processor [13] has incorporated a 64 KB L1 data cache, eight times larger than that of its predecessor [9]. 5 Related Work The idea of optimizing accesses to local variables on run-time stack is not new. Ditzel and McLellan <ref> [8] </ref> studied a transpar ent data buffer as a close mapping of the run-time stack, called the stack cache. The stack cache is effectively a large register file to simulate the run-time stack that replaces the general register file.
Reference: [9] <author> J. Edmondson et al. </author> <title> Internal Organization of the Alpha 21164, a 300-MHz, 64-Bit, Quad-Issue, CMOS RISC Microprocessor, </title> <journal> Digital Technical Journal, </journal> <volume> Volume 7, Number 1, </volume> <year> 1995. </year>
Reference-contexts: Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. For example, DEC 21264 provides a two-ported data cache by running the cache twice as fast as the processor clock [13], DEC 21164, the predecessor of 21264, uses a replicated data cache <ref> [9] </ref>, and MIPS R10000 implements a two-way interleaved data cache [33]. Each design, however, is either costly to implement, and/or has significant drawbacks. The time-division multiplexing does not scale beyond a certain number of ports (seemingly two). <p> It is worth noting that the recent 21264 processor [13] has incorporated a 64 KB L1 data cache, eight times larger than that of its predecessor <ref> [9] </ref>. 5 Related Work The idea of optimizing accesses to local variables on run-time stack is not new. Ditzel and McLellan [8] studied a transpar ent data buffer as a close mapping of the run-time stack, called the stack cache.
Reference: [10] <author> EGCS Project. </author> <note> http://egcs.cygnus.com. </note>
Reference-contexts: Instruction mixes of these programs, in terms of memory and non-memory instructions, are shown in Figure 2. All the programs were compiled using EGCS 7 version 1.1b <ref> [10] </ref> at the -O3 optimization level with loop unrolling. Either train or test input is used in most cases, with some data set modification to control the simulation time.
Reference: [11] <author> J. Emer and D. Clark. </author> <title> A Characterization of Processor Performance in the VAX-11/780, </title> <booktitle> Proc. of the 11th Int'l Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: Although not visible to a programmer, additional local variables are generated by a compiler for saving/restoring registers, passing arguments, and register spilling. Memory accesses to these variables, usually indexed by stack pointer ($sp), can constitute a large fraction of overall memory references <ref> [11] </ref>. For example, spill codes can produce a significant number of memory references at run time, as many as 20% of all the executed instructions [1]. <p> These studies aimed primarily at reducing the impact of a procedure call/return on the processor performance, motivated by an observation that programs written in a high-level language tend to have many procedure calls and returns <ref> [11] </ref>, and that a function call is the most costly source language statement [20].
Reference: [12] <author> M. J. Flynn and L. W. Hoevel. </author> <title> Execution Architecture: The DEL-tran Experiment, </title> <journal> IEEE Trans. on Computers, C-32(2): </journal> <volume> 156 175, </volume> <month> Feb. </month> <year> 1983. </year>
Reference-contexts: To service the local variable accesses efficiently, we study two optimizations fast data forwarding and access combining. Although there have been efforts to optimize local variable accesses with hardware support in the past <ref> [8, 12, 26] </ref>, little work has been done to study their performance impact in the context of a wide-issue superscalar processor with a multi-ported data cache. <p> Floating-point programs produced similar numbers also. The results suggest that if a separate cache is used to hold the local variables, it need not be large to obtain a high hit rate. In fact, this has been the motivation for some previous work <ref> [8, 12, 26, 29] </ref>. The high frequency of local variable accesses and their strong locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Moreover, identifying local variables in the stack frames is relatively easy for hardware or compiler. 2.2.2 Architectural support To service local variable accesses efficiently, a specialized hardware organization to simulate the run-time stack may appear attractive <ref> [8, 12, 26] </ref>. However, we use a more general cache design called the local variable cache (LVC) in the framework of our data-decoupled architecture. This approach has two advantages; First, the LVC is a conventional cache and can leverage the most efficient current design. <p> The stack cache is effectively a large register file to simulate the run-time stack that replaces the general register file. The contour buffer proposed by Flynn and Hoevel <ref> [12] </ref> in their Directly Executed Languages model, is a programmer-addressable buffer that is used in conjunction with the run-time stack in memory.
Reference: [13] <author> L. Gwennap. </author> <title> Digital 21264 Sets New Standard, </title> <type> Microprocessor Report, Volume 10, </type> <note> Issue 14, </note> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. For example, DEC 21264 provides a two-ported data cache by running the cache twice as fast as the processor clock <ref> [13] </ref>, DEC 21164, the predecessor of 21264, uses a replicated data cache [9], and MIPS R10000 implements a two-way interleaved data cache [33]. Each design, however, is either costly to implement, and/or has significant drawbacks. The time-division multiplexing does not scale beyond a certain number of ports (seemingly two). <p> Further optimizations are possible for the LVAQ and the LVC. Two such techniques to improve the local variable accesses are introduced: * Fast data forwarding. In recent superscalar processors <ref> [33, 13] </ref>, data is forwarded from a store to a later load of the same address in the LSQ. This data forwarding enables faster loads without accessing the data cache. There is another opportunity to perform an even faster forwarding in the LVAQ. <p> The ROB and the LSQ effectively form the instruction window of the processor. The primary on-chip data cache is 32 KB in size and 2-way set-associative, and has 2-cycle hit time as in some recent machines <ref> [33, 13] </ref>. The 512 KB L2 cache, either on-chip or off-chip, has a 12-cycle hit latency. Both caches are lock-up free. When data decoupling is used, a direct-mapped 2 KB LVC is employed. The line size of the caches is 32 Bytes. <p> It is worth noting that the recent 21264 processor <ref> [13] </ref> has incorporated a 64 KB L1 data cache, eight times larger than that of its predecessor [9]. 5 Related Work The idea of optimizing accesses to local variables on run-time stack is not new.
Reference: [14] <author> M. D. Hill. </author> <title> A Case for Direct-Mapped Caches, </title> <journal> IEEE Computer, pp. </journal> <volume> 25 40, </volume> <month> Dec. </month> <year> 1988. </year>
Reference-contexts: For the rest of the experiments, a 2 KB, direct-mapped LVC with one-cycle hit latency is used. 9 We prefer this design to a 4 KB LVC or a set-associative LVC, because a small direct-mapped cache is likely to have an access time advantage when a fast clock is used <ref> [14] </ref>. Furthermore, adding additional ports to this small LVC is much cheaper than to a large data cache like the one used in our study (32 KB).
Reference: [15] <author> M. H. Lipasti and J. P. Shen. </author> <title> Superspeculative Microarchitecture for Beyond AD 2000, </title> <journal> IEEE Computer, pp. </journal> <volume> 59 66, </volume> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: For example, for a processor to sustain ten instructions per cycle (IPC), the memory subsystem should provide a minimum bandwidth of four references per cycle, or more, to prevent excessive queuing delays, assuming that about 40% of all instructions are loads and stores <ref> [15] </ref>. A straightforward approach for increasing memory bandwidth is to implement a multi-ported data cache [25]. There are a number of techniques to provide multiple cache ports: ideal multi-porting, time-division multiplexing, replicating the cache, and interleaving [21]. <p> are summarized in Section 6. 2 Data-Decoupled Architecture 2.1 Concept of data decoupling To extract and exploit more parallelism, a future superscalar processor will establish a wide instruction window that consists of a large number of reservation stations, from which instructions are steered to a set of pipelined functional units <ref> [19, 15] </ref>. Building such a processor, unfortunately, poses many great challenges; Especially, the hardware complexity 1 of the logic that identifies and issues ready instructions from a pool of reservation stations becomes an increasingly severe impediment to a faster clock rate [18]. <p> First, decoded memory access instructions should be partitioned into independent streams before they enter the instruction window, as shown in Figure 1 (b). Either run-time or compile-time information on per-reference access type is needed. When run-time speculation <ref> [6, 15, 30, 17] </ref> is used for the classification, verification and recovery actions are required to handle mispredictions. <p> This is a very critical issue for the future wide-issue processor proposals <ref> [15, 19, 23] </ref>; They put more pressure on the data cache bandwidth as they aggressively speculate on control and register values, and use high-bandwidth instruction caches [34, 22]. <p> There are dynamic techniques to decouple a portion of data references and service them using separate, specialized functional units. Lipasti introduced a notion called load stream partitioning in his Superflow processor model <ref> [15] </ref>, which partitions loads into multiple streams based on their run-time behavior, and sends them to disjoint functional units for processing. The functional units used include a constant verification unit, a queue for load/store folding, a stream buffer/prefetch engine, and a conventional data cache.
Reference: [16] <author> A. Moshovos, S. E. Breach, T. N. Vijaykumar, and G. S. Sohi. </author> <title> Dynamic Speculation and Synchronization of Data Dependences, </title> <booktitle> Proc. of the 24th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 181 193, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Such reduction in hardware complexity can lead to a shorter clock cycle time [18]. Second, dividing the data stream into smaller streams can open up new opportunities to optimize each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding <ref> [16, 17, 30] </ref> can be tailored to each stream for higher efficiency.
Reference: [17] <author> A. Moshovos and G. S. Sohi. </author> <title> Streamlining Inter-operation Memory Communication via Data Dependence Prediction, </title> <booktitle> Proc. of the 30th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 235 245, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: Such reduction in hardware complexity can lead to a shorter clock cycle time [18]. Second, dividing the data stream into smaller streams can open up new opportunities to optimize each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding <ref> [16, 17, 30] </ref> can be tailored to each stream for higher efficiency. <p> First, decoded memory access instructions should be partitioned into independent streams before they enter the instruction window, as shown in Figure 1 (b). Either run-time or compile-time information on per-reference access type is needed. When run-time speculation <ref> [6, 15, 30, 17] </ref> is used for the classification, verification and recovery actions are required to handle mispredictions. <p> The functional units used include a constant verification unit, a queue for load/store folding, a stream buffer/prefetch engine, and a conventional data cache. Techniques to detect dependent memory access instructions and explicitly synchronize and forward data between them have been proposed <ref> [30, 17] </ref>. They provide a dynamic technique to detect a producer operation and a consumer operation within the instruction window, and try to forward the data in a special buffer before the effective addresses are calculated, without accessing the cache memory.
Reference: [18] <author> S. Palacharla, N. P. Jouppi, and J. E. Smith. </author> <title> Complexity-Effective Superscalar Processors, </title> <booktitle> Proc. of the 24th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 206 218, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Instead, existing cache designs with a smaller number of ports can be used. Further, the network and the control logic for orchestrating memory accesses between a large number of reservation stations and cache ports become simpler. Such reduction in hardware complexity can lead to a shorter clock cycle time <ref> [18] </ref>. Second, dividing the data stream into smaller streams can open up new opportunities to optimize each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding [16, 17, 30] can be tailored to each stream for higher efficiency. <p> Building such a processor, unfortunately, poses many great challenges; Especially, the hardware complexity 1 of the logic that identifies and issues ready instructions from a pool of reservation stations becomes an increasingly severe impediment to a faster clock rate <ref> [18] </ref>. The situation is exacerbated when there are multiple functional units of the same type, such as identical integer ALUs, because extra time may be needed for arbitration. <p> Alternatively, it can copy a reference into both the memory access queues to eliminate any communication between them; In this case, the wrongly inserted copy in LSQ or LVAQ will be killed at a later time. pler hardware is a valid goal <ref> [18] </ref>. This is a very critical issue for the future wide-issue processor proposals [15, 19, 23]; They put more pressure on the data cache bandwidth as they aggressively speculate on control and register values, and use high-bandwidth instruction caches [34, 22].
Reference: [19] <author> Y. N. Patt, S. J. Patel, D. H. Friendly, and J. Stark. </author> <title> One Billion Transistors, One Uniprocessor, One Chip, </title> <journal> IEEE Computer, pp. </journal> <volume> 51 57, </volume> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: are summarized in Section 6. 2 Data-Decoupled Architecture 2.1 Concept of data decoupling To extract and exploit more parallelism, a future superscalar processor will establish a wide instruction window that consists of a large number of reservation stations, from which instructions are steered to a set of pipelined functional units <ref> [19, 15] </ref>. Building such a processor, unfortunately, poses many great challenges; Especially, the hardware complexity 1 of the logic that identifies and issues ready instructions from a pool of reservation stations becomes an increasingly severe impediment to a faster clock rate [18]. <p> This is a very critical issue for the future wide-issue processor proposals <ref> [15, 19, 23] </ref>; They put more pressure on the data cache bandwidth as they aggressively speculate on control and register values, and use high-bandwidth instruction caches [34, 22].
Reference: [20] <author> D. A. Patterson and C. H. Sequin. </author> <title> A VLSI RISC, </title> <journal> IEEE Computer, pp. </journal> <volume> 8 21, </volume> <month> Sept. </month> <year> 1982. </year>
Reference-contexts: These studies aimed primarily at reducing the impact of a procedure call/return on the processor performance, motivated by an observation that programs written in a high-level language tend to have many procedure calls and returns [11], and that a function call is the most costly source language statement <ref> [20] </ref>. Unlike the previous approaches, the technique proposed in this paper does not require processor intervention or complex algorithms to manage the buffer, which were mandated in the previous techniques to deal with buffer overflow/underflow and context switches.
Reference: [21] <author> J. A. Rivers, G. S. Tyson, E. S. Davidson, and T. M. Austin. </author> <title> On High-Bandwidth Data Cache Design for Multi-Issue Processors, </title> <booktitle> Proc. of the 30th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 46 56, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: A straightforward approach for increasing memory bandwidth is to implement a multi-ported data cache [25]. There are a number of techniques to provide multiple cache ports: ideal multi-porting, time-division multiplexing, replicating the cache, and interleaving <ref> [21] </ref>. Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. <p> Designing an effective multi-ported cache has been a continuing topic of active research <ref> [25, 31, 32, 21] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [22] <author> E. Rotenberg, S. Bennet, and J. E. Smith. </author> <title> Trace Cache: a Low Latency Approach to High Bandwidth Instruction Fetching, </title> <booktitle> Proc. of the 29th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 24 34, </pages> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: This is a very critical issue for the future wide-issue processor proposals [15, 19, 23]; They put more pressure on the data cache bandwidth as they aggressively speculate on control and register values, and use high-bandwidth instruction caches <ref> [34, 22] </ref>. Under such conditions, the proposed approach can have a performance advantage by providing more data bandwidth than a conventional technique at the same level of hardware complexity.
Reference: [23] <author> E. Rotenberg, Q. Jacobson, and J. E. Smith. </author> <title> Trace Processors, </title> <booktitle> Proc. of the 30th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 138 148, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: This is a very critical issue for the future wide-issue processor proposals <ref> [15, 19, 23] </ref>; They put more pressure on the data cache bandwidth as they aggressively speculate on control and register values, and use high-bandwidth instruction caches [34, 22].
Reference: [24] <author> G. S. Sohi. </author> <title> Instruction Issue Logic for High-Performance, Interruptible, Multiple Functional Unit, Pipelined Computers, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(3):349 359, </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: The machine model used in the experiments is a superscalar processor that supports out-of-order issue and execution, based on the Register Update Unit (RUU) <ref> [24] </ref>. The RUU scheme 6 Two such functions have been found in the programs studied load-core () and dumpcore () in 124.m88ksim. These functions use over 11K words of stack space, by allocating a huge C structure and a buffer. A compiler can easily recognize these memory references as local.
Reference: [25] <author> G. S. Sohi and M. Franklin. </author> <title> High-Bandwidth Data Memory Systems for Superscalar Processors, </title> <booktitle> Proc. of the Fourth Int'l Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 53 62, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Furthermore, the ability to provide the execution core with adequate (cache) memory bandwidth becomes extremely critical for the next generations of wide-issue processors <ref> [25, 31] </ref>. For example, for a processor to sustain ten instructions per cycle (IPC), the memory subsystem should provide a minimum bandwidth of four references per cycle, or more, to prevent excessive queuing delays, assuming that about 40% of all instructions are loads and stores [15]. <p> A straightforward approach for increasing memory bandwidth is to implement a multi-ported data cache <ref> [25] </ref>. There are a number of techniques to provide multiple cache ports: ideal multi-porting, time-division multiplexing, replicating the cache, and interleaving [21]. Except for the very expensive ideal multi-porting, these techniques have been incorporated in recent superscalar processors. <p> Designing an effective multi-ported cache has been a continuing topic of active research <ref> [25, 31, 32, 21] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [26] <author> T. J. Stanley and R. G. Wedig. </author> <title> A Performance Analysis of Automatically Managed Top of Stack Buffers, </title> <booktitle> Proc. of the 14th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 272 281, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: To service the local variable accesses efficiently, we study two optimizations fast data forwarding and access combining. Although there have been efforts to optimize local variable accesses with hardware support in the past <ref> [8, 12, 26] </ref>, little work has been done to study their performance impact in the context of a wide-issue superscalar processor with a multi-ported data cache. <p> Floating-point programs produced similar numbers also. The results suggest that if a separate cache is used to hold the local variables, it need not be large to obtain a high hit rate. In fact, this has been the motivation for some previous work <ref> [8, 12, 26, 29] </ref>. The high frequency of local variable accesses and their strong locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Moreover, identifying local variables in the stack frames is relatively easy for hardware or compiler. 2.2.2 Architectural support To service local variable accesses efficiently, a specialized hardware organization to simulate the run-time stack may appear attractive <ref> [8, 12, 26] </ref>. However, we use a more general cache design called the local variable cache (LVC) in the framework of our data-decoupled architecture. This approach has two advantages; First, the LVC is a conventional cache and can leverage the most efficient current design. <p> The contour buffer proposed by Flynn and Hoevel [12] in their Directly Executed Languages model, is a programmer-addressable buffer that is used in conjunction with the run-time stack in memory. Stanley and Wedig <ref> [26] </ref> proposed three buffer management algorithms for a Top of Stack (TOS) buffer, which is a register file designed to cache the top elements of the stack.
Reference: [27] <institution> The Standard Performance Evaluation Corporation, </institution> <note> http://www.specbench.org. </note>
Reference-contexts: The results under 99% percentile are shown to ease reading. Ditzel and McLellan [8] report similar results. compatibility, as exemplified by the Intel's x86 architecture, may disallow increasing the size of a register file. tions in a set of SPEC95 programs <ref> [27] </ref>. 5 A large fraction of memory references are to local variables, with an average of 30% of loads and 48% of stores in the programs studied. Over 60% of loads and 80% of stores are local variable accesses in 147.vortex. <p> Important parameters of the base machine model are summarized in Table 1. 3.2 Benchmark programs We use eight integer and four floating-point programs from the SPEC95 benchmark suite <ref> [27] </ref>, whose inputs and instruction counts are described in Table 2. Instruction mixes of these programs, in terms of memory and non-memory instructions, are shown in Figure 2. All the programs were compiled using EGCS 7 version 1.1b [10] at the -O3 optimization level with loop unrolling.
Reference: [28] <author> Y. Tamir and C. H. Sequin. </author> <title> Strategies for Managing the Register File in RISC, </title> <journal> IEEE Trans. on Computers, C-32(11): </journal> <volume> 977 989, </volume> <month> Nov. </month> <year> 1983. </year>
Reference-contexts: A direct-mapped LVC with four ports is used for measurement. as discussed previously. Furthermore, most of the programs have a call depth of four or five routines <ref> [28] </ref>. The line size of the LVC, being it 32 or 64 Bytes, has a negligible effect on the hit rate when the LVC size is larger than or equal to 2 KB.
Reference: [29] <author> M. Tremblay, B. Joy, and K. Shin. </author> <title> A Three Dimensional Register File for Superscalar Processors, </title> <booktitle> Proc. of the 28th Annual Hawaii Int'l Conf. on Systems Sciences, </booktitle> <publisher> IEEE CS Press, </publisher> <year> 1995. </year>
Reference-contexts: Floating-point programs produced similar numbers also. The results suggest that if a separate cache is used to hold the local variables, it need not be large to obtain a high hit rate. In fact, this has been the motivation for some previous work <ref> [8, 12, 26, 29] </ref>. The high frequency of local variable accesses and their strong locality motivate us to consider decoupling and servicing the local variable accesses separately. <p> Among current microprocessors, Sun UltraSparc employs a special register file structure called register window to reduce the cost of a procedure call/return <ref> [29] </ref>. Chow and Hennessy [7] categorize memory traffic into five types of references after register allocation unallocated references, global scalars, save/restore memory references, a required stack reference, and a computed reference.
Reference: [30] <author> G. Tyson and T. M. Austin. </author> <title> Improving the Accuracy and Performance of Memory Communication Through Renaming, </title> <booktitle> Proc. of the 30th Annual Int'l Symp. on Microarchitecture, </booktitle> <pages> pp. 218 227, </pages> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: Such reduction in hardware complexity can lead to a shorter clock cycle time [18]. Second, dividing the data stream into smaller streams can open up new opportunities to optimize each with specialized techniques. For instance, various speculative techniques on data dependence and forwarding <ref> [16, 17, 30] </ref> can be tailored to each stream for higher efficiency. <p> First, decoded memory access instructions should be partitioned into independent streams before they enter the instruction window, as shown in Figure 1 (b). Either run-time or compile-time information on per-reference access type is needed. When run-time speculation <ref> [6, 15, 30, 17] </ref> is used for the classification, verification and recovery actions are required to handle mispredictions. <p> The functional units used include a constant verification unit, a queue for load/store folding, a stream buffer/prefetch engine, and a conventional data cache. Techniques to detect dependent memory access instructions and explicitly synchronize and forward data between them have been proposed <ref> [30, 17] </ref>. They provide a dynamic technique to detect a producer operation and a consumer operation within the instruction window, and try to forward the data in a special buffer before the effective addresses are calculated, without accessing the cache memory.
Reference: [31] <author> K. M. Wilson, K. Olukotun, and M. Rosenblum. </author> <title> Increasing Cache Port Efficiency for Dynamic Superscalar Microprocessors, </title> <booktitle> Proc. of the 23th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 147 157, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Furthermore, the ability to provide the execution core with adequate (cache) memory bandwidth becomes extremely critical for the next generations of wide-issue processors <ref> [25, 31] </ref>. For example, for a processor to sustain ten instructions per cycle (IPC), the memory subsystem should provide a minimum bandwidth of four references per cycle, or more, to prevent excessive queuing delays, assuming that about 40% of all instructions are loads and stores [15]. <p> This technique will be beneficial when there are many local store-reload pairs within a small section of code, such as spill codes generated by a compiler. * Access combining <ref> [31] </ref>. When a program or a program region contains many local variable accesses, the number of LVC ports can become a performance bottleneck. In fact, a procedure call/return generates bursty stack accesses for saving/restoring registers and passing parameters. <p> Designing an effective multi-ported cache has been a continuing topic of active research <ref> [25, 31, 32, 21] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [32] <author> K. M. Wilson and K. Olukotun. </author> <title> Designing High Bandwidth On-Chip Caches, </title> <booktitle> Proc. of the 24th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. 121 132, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Designing an effective multi-ported cache has been a continuing topic of active research <ref> [25, 31, 32, 21] </ref>. These studies have focused on increasing the efficiency of cache ports by adding a small buffer, or understanding tradeoffs of various strategies in terms of cost and performance under specific processor models. The data-decoupled architecture is orthogonal to the data cache design techniques.
Reference: [33] <author> K. C. Yeager. </author> <title> The MIPS R10000 Superscalar Microprocessor, </title> <journal> IEEE Micro, </journal> <volume> Volume 16, Number 2, </volume> <pages> pp. 28 40, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: For example, DEC 21264 provides a two-ported data cache by running the cache twice as fast as the processor clock [13], DEC 21164, the predecessor of 21264, uses a replicated data cache [9], and MIPS R10000 implements a two-way interleaved data cache <ref> [33] </ref>. Each design, however, is either costly to implement, and/or has significant drawbacks. The time-division multiplexing does not scale beyond a certain number of ports (seemingly two). <p> The MIPS R10000 processor, for example, partitions the window into an integer queue, a floating-point queue, and an address queue, based on the instruction type <ref> [33] </ref>. The data-decoupled architecture further partitions the instruction window for data memory accesses, and provides a separate cache for each partitioned window. An example of a pipelined, two-way data-decoupled architecture is depicted in Figure 1 (b). <p> Further optimizations are possible for the LVAQ and the LVC. Two such techniques to improve the local variable accesses are introduced: * Fast data forwarding. In recent superscalar processors <ref> [33, 13] </ref>, data is forwarded from a store to a later load of the same address in the LSQ. This data forwarding enables faster loads without accessing the data cache. There is another opportunity to perform an even faster forwarding in the LVAQ. <p> L1 D-cache 2-way set-assoc. 32 KB. 2-cycle hit time. L2 D-cache 4-way. 512 KB. 12-cycle access time. Memory 50-cycle access time. Fully interleaved. I-cache Perfect I-cache with 1 cycle latency. Br. prediction Perfect. Inst. latencies Same as those of MIPS R10000 <ref> [33] </ref>. Table 1. The base machine model. Decode and commit widths are the same as the issue width. uses a reorder buffer (ROB) to automatically perform register renaming and hold the results of pending instructions. <p> The ROB has 128 entries and the LSQ has 64 entries, which are derived from the MIPS R10000 implementation <ref> [33] </ref>. The ROB and the LSQ effectively form the instruction window of the processor. The primary on-chip data cache is 32 KB in size and 2-way set-associative, and has 2-cycle hit time as in some recent machines [33, 13]. <p> The ROB and the LSQ effectively form the instruction window of the processor. The primary on-chip data cache is 32 KB in size and 2-way set-associative, and has 2-cycle hit time as in some recent machines <ref> [33, 13] </ref>. The 512 KB L2 cache, either on-chip or off-chip, has a 12-cycle hit latency. Both caches are lock-up free. When data decoupling is used, a direct-mapped 2 KB LVC is employed. The line size of the caches is 32 Bytes.
Reference: [34] <author> T.-Y. Yeh, D. T. Marr, and Y. N. Patt. </author> <title> Increasing the Instruction Fetch Rate via Multiple Branch Prediction and a Branch Address Cache, </title> <booktitle> Proc. of the 7th Int'l Conf. on Supercomputing, </booktitle> <pages> pp. 67 76, </pages> <month> July </month> <year> 1993. </year> <title> and 102.swim under various (N+M) configurations. Re sults of other programs can be found in [5]. </title>
Reference-contexts: This is a very critical issue for the future wide-issue processor proposals [15, 19, 23]; They put more pressure on the data cache bandwidth as they aggressively speculate on control and register values, and use high-bandwidth instruction caches <ref> [34, 22] </ref>. Under such conditions, the proposed approach can have a performance advantage by providing more data bandwidth than a conventional technique at the same level of hardware complexity.
References-found: 34

