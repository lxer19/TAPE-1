URL: http://www.is.cs.cmu.edu/papers/multimodal/ICASSP97/97ICASSP-rainer.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.multimodal.publications.html
Root-URL: 
Email: stiefel@ira.uka.de, yang+@cs.cmu.edu  
Title: GAZE TRACKING FOR MULTIMODAL HUMAN-COMPUTER INTERACTION  
Author: Rainer Stiefelhagen and Jie Yang 
Address: USA  
Affiliation: Interactive Systems Laboratories University of Karlsruhe Germany, Carnegie Mellon University  
Abstract: This paper discusses the problem of gaze tracking and its applications to multimodal human-computer interaction. The function of a gaze tracking system can be either passive or active. For example, a system can identify user's message target by monitoring the user's gaze, or the user could use his gaze to directly control an application or launch actions. We have developed a real-time gaze tracking system that estimates the 3D position and rotation (Pose) of a user's head. We demonstrate the applications of the gaze tracker to human-computer interaction by two examples. The first example shows that gaze tracker can help speech recognition systems by switching language model and grammar based on user's gaze information. The second example illustrates the combination of the gaze tracker and a speech recognizer to view a panorama image. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Matsu'ura, Y. Masai, J. Iwasaki, S. Tanaka, S., H. Kamio, and T. Nitta, </author> <title> "A multimodal, </title> <booktitle> keyword-based spoken dia-logue system - MultiksDial," Proc. </booktitle> <address> ICASSP'94 (Adelaide, Australia, </address> <month> April </month> <year> 1994), </year> <journal> Vol. </journal> <volume> 2, </volume> <pages> pp. </pages> <month> II/33-36. </month>
Reference-contexts: 1. INTRODUCTION Multimodal human-computer interaction has received much attention recently. Several researchers have studied the effectiveness of multimodal human-computer interaction <ref> [1, 2, 3] </ref>. Multimodal interfaces benefit from the redundancy, naturalness and flexibility that arise from exploiting alternate and complementary communication cues.
Reference: [2] <author> S. Nakagawa and J.X. Zhang, </author> <title> "An input interface with speech and touch screen," </title> <journal> Trans. Inst. Elec. Eng. Jpn. C (Japan), </journal> <volume> Vol. 114-C, No. 10, </volume> <month> Oct. </month> <year> 1994, </year> <pages> pp. 1009-1017. </pages>
Reference-contexts: 1. INTRODUCTION Multimodal human-computer interaction has received much attention recently. Several researchers have studied the effectiveness of multimodal human-computer interaction <ref> [1, 2, 3] </ref>. Multimodal interfaces benefit from the redundancy, naturalness and flexibility that arise from exploiting alternate and complementary communication cues.
Reference: [3] <author> H. Ando, Y. Kitahara, and N. Hataoka, </author> <title> "Evaluation of mul-timodal interface using spoken language and pointing ges-ture on interior design system," </title> <booktitle> Proc. </booktitle> <address> ICSLP'94 (Yokohama, Japan, </address> <month> Sept. </month> <journal> 1994), </journal> <volume> Vol. 2, </volume> <pages> pp. 567-570. </pages>
Reference-contexts: 1. INTRODUCTION Multimodal human-computer interaction has received much attention recently. Several researchers have studied the effectiveness of multimodal human-computer interaction <ref> [1, 2, 3] </ref>. Multimodal interfaces benefit from the redundancy, naturalness and flexibility that arise from exploiting alternate and complementary communication cues.
Reference: [4] <author> M.T. Vo, R. Houghton, J. Yang, U. Bub, U. Meier, A. Waibel, and P. Duchnowski, </author> <title> "Multimodal Learning Interfaces," </title> <booktitle> Proc. ARPA SLT Workshop 95 (Austin, </booktitle> <address> Texas, </address> <month> Jan. </month> <year> 1995). </year>
Reference-contexts: Our research efforts at the Interactive Systems Laboratories (Carnegie Mellon University and University of Karlsruhe) are focused on producing a sensible and useful user interface to support the multimodal human-computer interaction. Some of our initial works along this line have been reported in previous publications <ref> [4, 5] </ref>. While multimodal interfaces offer greater flexibility and robustness, they have still been largely pen- or voice-based, user activated, and operate in settings where headsets, helmets, suits, buttons or other constraining devices are required.
Reference: [5] <author> A. Waibel, M.T. Vo, P. Duchnowski, and S. Manke, </author> <title> "Multimodal Interfaces," </title> <booktitle> Artificial Intelligence Review, Special Volume on Integration of Natural Language and Vision Processing, </booktitle> <editor> McKevitt, P. (Ed.), </editor> <volume> Vol. 10, </volume> <pages> Nos. 3-4, </pages> <year> 1995. </year>
Reference-contexts: Our research efforts at the Interactive Systems Laboratories (Carnegie Mellon University and University of Karlsruhe) are focused on producing a sensible and useful user interface to support the multimodal human-computer interaction. Some of our initial works along this line have been reported in previous publications <ref> [4, 5] </ref>. While multimodal interfaces offer greater flexibility and robustness, they have still been largely pen- or voice-based, user activated, and operate in settings where headsets, helmets, suits, buttons or other constraining devices are required.
Reference: [6] <author> D. A. Simon, M. Hebert, T. Kanade. </author> <title> Real-time 3-D Pose Estimation Using a High-Speed Range Sensor. </title> <booktitle> International Conference of Robotics and Automation Proceedings, </booktitle> <address> May '94, San Diego. </address>
Reference-contexts: A real-time gaze tracker is a prerequisite for tracking user's gaze. There have been several approaches to compute the gaze of a person. Hardware-intensive and/or intrusive methods, where the user has to wear special headgear, or methods that use expensive hardware such as radar-range- finder <ref> [6] </ref>. Recently, there have been proposed non intrusive gaze trackers using mainly software.
Reference: [7] <author> Andrew Gee and Robert Cipolla. </author> <title> Non-Intrusive Gaze Tracking for Human-Computer Interaction. </title> <booktitle> Proc. Mechatronics and Machine Vision in Practise, p. </booktitle> <pages> 112-117, </pages> <address> Toowoomba, Australia, </address> <year> 1994 </year>
Reference-contexts: Hardware-intensive and/or intrusive methods, where the user has to wear special headgear, or methods that use expensive hardware such as radar-range- finder [6]. Recently, there have been proposed non intrusive gaze trackers using mainly software. For example, Cipolla & Gee <ref> [7] </ref> developed a system to track the rotation and position of the head by finding correspondences between facial feature points and corresponding points in a model of the head, using a weak perspective projection.
Reference: [8] <author> Stiefelhagen, R., Yang, J., Waibel, A.: </author> <title> A Model-Based Gaze Tracking System. </title> <booktitle> Proc. of the IEEE International Symposia on Intelligence and Systems, p. </booktitle> <pages> 304-310, </pages> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: However, the system has to be initialized manually because the system cannot locate the face and the facial feature points automatically. We have developed a non-intrusive model-based gaze- tracking system <ref> [8, 9] </ref>. The system estimates the 3-D pose of a user's head by tracking as few as six facial feature points. The system locates a human face using a statistical color- model without any mark on the face. <p> The second example illustrates the combination of the gaze tracker and a speech recognizer to view a panorama image. 2. A REAL-TIME GAZE TRACKER In this section we briefly describe how to track gaze in real- time <ref> [8, 9] </ref>. In our system we are estimating the gaze of the user by computing the pose of his head. <p> The average distance in x- and in y-direction of manually marked feature locations and the automatically found locations was between two and three pixels. The system runs with around 20 frames per second. See <ref> [8, 9] </ref> for complete results. 3. APPLICATIONS TO MULTIMODAL INTERFACES Although gaze tracking techniques have existed for a long time, most applications of these techniques have been in psychological research for probing into subjects' perceptual or cognitive processes.
Reference: [9] <author> Stiefelhagen, R. </author> <title> Gaze Tracking for Multimodal Human Computer Interaction. </title> <institution> University of Karlsruhe, </institution> <year> 1996. </year> <note> Available at http://werner.ira.uka.de/ISL.multimodal.publications.html. </note>
Reference-contexts: However, the system has to be initialized manually because the system cannot locate the face and the facial feature points automatically. We have developed a non-intrusive model-based gaze- tracking system <ref> [8, 9] </ref>. The system estimates the 3-D pose of a user's head by tracking as few as six facial feature points. The system locates a human face using a statistical color- model without any mark on the face. <p> The second example illustrates the combination of the gaze tracker and a speech recognizer to view a panorama image. 2. A REAL-TIME GAZE TRACKER In this section we briefly describe how to track gaze in real- time <ref> [8, 9] </ref>. In our system we are estimating the gaze of the user by computing the pose of his head. <p> The average distance in x- and in y-direction of manually marked feature locations and the automatically found locations was between two and three pixels. The system runs with around 20 frames per second. See <ref> [8, 9] </ref> for complete results. 3. APPLICATIONS TO MULTIMODAL INTERFACES Although gaze tracking techniques have existed for a long time, most applications of these techniques have been in psychological research for probing into subjects' perceptual or cognitive processes.
Reference: [10] <author> D. F. DeMenthon and L. S. Davis. </author> <title> Model based object pose in 25 lines of code. </title> <editor> In G. Sandini, editor, </editor> <booktitle> Computer Vision - ECCV 92, Proceedings Second European Conference on Computer Vision, </booktitle> <address> Santa Margherita Ligure, </address> <month> May </month> <year> 1992, </year> <pages> pages 335 - 343. </pages> <publisher> Springer Verlag, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: To compute the pose from these 3D to 2D correspondences we used the POSIT algorithm, recently proposed by DeMen- thon and Davis <ref> [10] </ref>. In order to compute the pose, the facial features must k 0 = 30 k 1 = 32 k 2 = 34 be searched and tracked in the camera image.
Reference: [11] <author> J. Yang and A. Waibel, </author> <title> "A real-time face tracker," </title> <booktitle> Proceedings of WACV'96 (Sarasota, </booktitle> <address> Florida, USA), </address> <pages> pp. 142-147, </pages> <year> 1996. </year>
Reference-contexts: Searching the Face To find and track the face, we use a statistical color-model consisting of a two-dimensional Gaussian distribution of normalized face colors <ref> [11] </ref>. The input image is searched for pixels with face colors and the largest connected region of face-colored pixels in the camera-image is considered as the region of the face.
Reference: [12] <author> A. H. Gee and R. Cipolla, </author> <title> Fast Visual Tracking by Temporal Consensus. </title> <type> Technical Report CUED/F-INFENG/TR207, </type> <institution> University of Cambridge, </institution> <month> February </month> <year> 1995 </year>
Reference-contexts: At the same time, we use a most consistent subset of 2D to 3D point-correspondences to compute the pose, instead of using all found points. To find a best subset we investigated two methods proposed by Gee & Cipolla <ref> [12] </ref>: Sample consensus tracking and temporal continuity tracking. Using the first method, the subset is chosen that leads to the best back- projection of model-points into the image-plane. Using the second method, the subset that leads to the pose implying the smoothest motion is chosen as the best subset. <p> Experimental Results To evaluate the system we compared the output of the gaze tracker on some pre-recorded image sequences to the results obtained by labelling the facial features manually. The best results were obtained using the temporal continuity method <ref> [12] </ref>, where we achieved rotation errors as low as 5 degrees for rotation around the x- and y-axis and as low as 1 degree for rotation around the z-axi.
Reference: [13] <author> T. Zeppenfeld, M. Finke, K. Ries, and A. Waibel, </author> <title> "Recognition of conversitional telephone speech using the janus speech engine," </title> <booktitle> Proc. of ICASSP'97. </booktitle>
Reference-contexts: User's gaze can reflect his/her attention. In order to increase reliability, we can use voice commands to confirm selections. We have developed an interface to demonstrate the concept. We use gaze and voice to switch language models for Janus III recognition engine <ref> [13] </ref>. The Janus III system is at present specific to discourse domains of common interest, and supports spontaneously uttered human- to-human speech. Janus III was designed to be a speech recognition research tool.
References-found: 13

