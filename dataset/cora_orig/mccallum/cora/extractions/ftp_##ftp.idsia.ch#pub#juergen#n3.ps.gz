URL: ftp://ftp.idsia.ch/pub/juergen/n3.ps.gz
Refering-URL: http://www.idsia.ch/~juergen/topics.html
Root-URL: 
Title: A FIXED SIZE STORAGE O(n 3 TIME COMPLEXITY LEARNING ALGORITHM FOR FULLY RECURRENT CONTINUALLY RUNNING
Author: Jurgen Schmidhuber 
Address: Arcisstr. 21, 8000 Munchen 2, Germany  
Affiliation: Institut fur Informatik Technische Universitat Munchen  
Abstract: The RTRL algorithm for fully recurrent continually running networks (Robinson and Fallside, 1987)(Williams and Zipser, 1989) requires O(n 4 ) computations per time step, where n is the number of non-input units. I describe a method suited for on-line learning which computes exactly the same gradient and requires fixed-size storage of the same order but has an average time complexity 1 per time step of O(n 3 ).
Abstract-found: 1
Intro-found: 1
Reference: <author> Gherrity, M. </author> <year> (1989). </year> <title> A learning algorithm for analog fully recurrent neural networks. </title> <booktitle> In IEEE/INNS International Joint Conference on Neural Networks, San Diego, </booktitle> <volume> volume 1, </volume> <pages> pages 643-644. </pages>
Reference-contexts: Since it is O (n) times faster than RTRL, it should be preferred. Following the argumentation in (Williams and Peng, 1990), continuous time versions of BPTT and RTRL (Pearlmutter, 1989) <ref> (Gherrity, 1989) </ref> can serve as a basis for a correspondingly efficient continuous time version of the algorithm presented here (by means of Euler discretization). Many typical environments produce input sequences that have both local and more global temporal structure. For instance, input sequences are often hierarchically organized (e.g. speech).
Reference: <author> Pearlmutter, B. A. </author> <year> (1989). </year> <title> Learning state space trajectories in recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 263-269. </pages>
Reference-contexts: Since it is O (n) times faster than RTRL, it should be preferred. Following the argumentation in (Williams and Peng, 1990), continuous time versions of BPTT and RTRL <ref> (Pearlmutter, 1989) </ref> (Gherrity, 1989) can serve as a basis for a correspondingly efficient continuous time version of the algorithm presented here (by means of Euler discretization). Many typical environments produce input sequences that have both local and more global temporal structure.
Reference: <author> Pineda, F. J. </author> <year> (1990). </year> <title> Time dependent adaptive neural networks. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 710-718. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: l (t) = t=1 @w ij (t ) Furthermore we define e k (t) = d k (t) x k (t) if k 2 T (t) and 0 otherwise; 2 Pineda has described another recurrent net algorithm which, as he states, "has some of the worst features of both algorithms" <ref> (Pineda, 1990) </ref>.
Reference: <author> Robinson, A. J. and Fallside, F. </author> <year> (1987). </year> <title> The utility driven dynamic error propagation network. </title> <type> Technical Report CUED/F-INFENG/TR.1, </type> <institution> Cambridge University Engineering Department. </institution>
Reference-contexts: Such an algorithm is the RTRL-algorithm <ref> (Robinson and Fallside, 1987) </ref>(Williams and Zipser, 1989). It requires only fixed-size storage of the order O (n 3 ) but is computationally expensive: It requires O (n 4 ) operations per time step 2 . The algorithm described herein requires O (n 3 ) storage, too.
Reference: <author> Schmidhuber, J. H. </author> <year> (1991). </year> <title> Adaptive decomposition of time. </title> <editor> In Kohonen, T., Makisara, K., Simula, O., and Kangas, J., editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 909-914. </pages> <publisher> Elsevier Science Publishers B.V., North-Holland. </publisher>
Reference-contexts: Many typical environments produce input sequences that have both local and more global temporal structure. For instance, input sequences are often hierarchically organized (e.g. speech). In such cases, sequence-composing algorithms <ref> (Schmidhuber, 1991) </ref> (Schmidhuber, 1992) can provide superior alternatives to pure gradient-based algorithms. 1 ACKNOWLEDGEMENTS Thanks to Mike Mozer, Bernd Schurmann, and Daniel Prelinger for providing useful comments on an earlier draft of this paper. 5
Reference: <author> Schmidhuber, J. H. </author> <year> (1992). </year> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 234-242. </pages>
Reference-contexts: Many typical environments produce input sequences that have both local and more global temporal structure. For instance, input sequences are often hierarchically organized (e.g. speech). In such cases, sequence-composing algorithms (Schmidhuber, 1991) <ref> (Schmidhuber, 1992) </ref> can provide superior alternatives to pure gradient-based algorithms. 1 ACKNOWLEDGEMENTS Thanks to Mike Mozer, Bernd Schurmann, and Daniel Prelinger for providing useful comments on an earlier draft of this paper. 5
Reference: <author> Williams, R. J. </author> <year> (1989). </year> <title> Complexity of exact gradient computation algorithms for recurrent neural networks. </title> <type> Technical Report Technical Report NU-CCS-89-27, </type> <institution> Boston: Northeastern University, College of Computer Science. </institution>
Reference: <author> Williams, R. J. and Peng, J. </author> <year> (1990). </year> <title> An efficient gradient-based algorithm for on-line training of recurrent network trajectories. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 491-501. </pages>
Reference-contexts: INTRODUCTION There are two basic methods for performing steepest descent in fully recurrent networks with n non-input units and m = O (n) input units. `Back propagation through time' (BPTT) (e.g. <ref> (Williams and Peng, 1990) </ref>) requires potentially unlimited storage in proportion to the length of the longest training sequence but needs only O (n 2 ) computations per time step. BPTT is the method of choice if training sequences are known to have less than O (n) time steps. <p> This cuts the average time complexity per time step to O (n 3 ). THE ALGORITHM The notation will be similar to the notation of <ref> (Williams and Peng, 1990) </ref>. U is the set of indices k such that at the discrete time step t the quantity x k (t) is the output of a non-input unit k in the network. <p> One way to visualize the w ij (t) is to consider them as weights of connections to the t-th non-input layer of a feed-forward network constructed by `unfolding' the recurrent network in time (e.g. <ref> (Williams and Peng, 1990) </ref>). A training sequence with s + 1 time steps starts at time step 0 and ends at time step s. The algorithm below is of interest if s &gt;> n (otherwise it is preferable to use BPTT). <p> CONCLUDING REMARKS Like the RTRL-algorithm the method needs a fixed amount of storage of the order O (n 3 ). Like the RTRL-algorithm (but unlike the methods described in <ref> (Williams and Peng, 1990) </ref> and (Zipser, 1989)) the algorithm computes the exact gradient. Since it is O (n) times faster than RTRL, it should be preferred. Following the argumentation in (Williams and Peng, 1990), continuous time versions of BPTT and RTRL (Pearlmutter, 1989) (Gherrity, 1989) can serve as a basis for <p> Like the RTRL-algorithm (but unlike the methods described in <ref> (Williams and Peng, 1990) </ref> and (Zipser, 1989)) the algorithm computes the exact gradient. Since it is O (n) times faster than RTRL, it should be preferred. Following the argumentation in (Williams and Peng, 1990), continuous time versions of BPTT and RTRL (Pearlmutter, 1989) (Gherrity, 1989) can serve as a basis for a correspondingly efficient continuous time version of the algorithm presented here (by means of Euler discretization).
Reference: <author> Williams, R. J. and Zipser, D. </author> <year> (1989). </year> <title> Experimental analysis of the real-time recurrent learning algorithm. </title> <journal> Connection Science, </journal> <volume> 1(1) </volume> <pages> 87-111. </pages>
Reference: <author> Williams, R. J. and Zipser, D. </author> <year> (1992). </year> <title> Gradient-based learning algorithms for recurrent networks and their computational complexity. In Back-propagation: Theory, Architectures and Applications. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Zipser, D. </author> <year> (1989). </year> <title> A subgrouping strategy that reduces learning complexity and speeds up learning in recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1(4) </volume> <pages> 552-558. </pages>
Reference-contexts: CONCLUDING REMARKS Like the RTRL-algorithm the method needs a fixed amount of storage of the order O (n 3 ). Like the RTRL-algorithm (but unlike the methods described in (Williams and Peng, 1990) and <ref> (Zipser, 1989) </ref>) the algorithm computes the exact gradient. Since it is O (n) times faster than RTRL, it should be preferred.
References-found: 11

