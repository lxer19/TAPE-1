URL: http://www.cs.wustl.edu/cs/techreports/1994/wucs-94-15.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Title: Strategies for the Parallel Training of Simple Recurrent Neural Networks  
Author: Peter J. McCann and Barry L. Kalman 
Date: June, 1994  
Pubnum: WUCS-94-15  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Coffman, E.G., Garey, M.R., Johnson, D.S, and Tarjan, R.E. </author> <year> (1980). </year> <title> Performance Bounds for Level-Oriented Two-Dimensional Packing Algorithms. </title> <journal> SIAM Journal on Computers 9, </journal> <pages> 808-826. </pages>
Reference-contexts: The problem of finding an assignment of jobs to processors so that 5 is minimized is known to be NP-complete [2], and so we use a variant of the First-Fit Decreasing-Height (FFDH) heuristic analyzed in <ref> [1] </ref>. We statically allocate the sequences to processors according to the following algorithm: First, sort the sequences in order of non-increasing length. <p> It is straightforward to show that this algorithm performs at least as well as the FFDH algorithm. It is proven in <ref> [1] </ref> that the length of a schedule resulting from FFDH will have length at most (R + 1) OPT + max (len (i)) where OPT represents the length of the optimal schedule. After sequences have been assigned to processors, we need to make some additional modifications to the sequential code.
Reference: [2] <author> Coffman, E.G., ed. </author> <year> (1976). </year> <title> Computer and Job Shop Scheduling Theory, </title> <address> New York: </address> <publisher> John Wiley. </publisher> <pages> 12 </pages>
Reference-contexts: The problem of finding an assignment of jobs to processors so that 5 is minimized is known to be NP-complete <ref> [2] </ref>, and so we use a variant of the First-Fit Decreasing-Height (FFDH) heuristic analyzed in [1]. We statically allocate the sequences to processors according to the following algorithm: First, sort the sequences in order of non-increasing length.
Reference: [3] <author> Elman, J.L. </author> <year> (1990). </year> <title> Finding Structure in Time. </title> <booktitle> Cognitive Science 14, </booktitle> <address> 179--211. </address>
Reference-contexts: It takes an exceptionally large amount of computer time to train these types of networks because of the added complexity of the derivative calculations. In this work, we focus on one type of recurrent network, Elman's Simple Recurrent Network <ref> [3] </ref>, and we present two ways to distribute the gradient computation. Our first parallel algorithm distributes the network over processing elements. It distributes the most computationally intense part of the gradient calculation during each pattern presentation.
Reference: [4] <author> Kalman, B.L., and Kwasny, Stan C. </author> <year> (1993). </year> <title> TRAINREC: A System for Training Feedforward and Simple Recurrent Networks Efficiently and Correctly. </title> <type> Technical Report WUCS-93-26, </type> <institution> St. Louis: Department of Computer Science, Washington University. </institution>
Reference-contexts: These weights fully and directly connect the input and context units to the output units. It has been determined experimentally that these connections allow for faster network convergence. They provide an alternate set of parameters for the linearly separable, or perceptron, portion of the problem. See <ref> [4] </ref> for a more complete discussion of the rationale for these connections.
Reference: [5] <author> Kalman, B.L., and Kwasny, Stan C. </author> <year> (1992). </year> <title> Why Tanh: Choosing a Sig-moidal Function. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (Baltimore 1992), </booktitle> <volume> vol. IV, </volume> <pages> 578-581. </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference-contexts: Our sigmoidal function is the hyperbolic tangent with a 3 2 coefficient on the sum of inputs. See <ref> [5] </ref> for a detailed derivation of this coefficient.
Reference: [6] <author> Kalman, B.L., and Kwasny, Stan C. </author> <year> (1991). </year> <title> A Superior Error Function for Training Neural Nets. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (Seattle 1991), </booktitle> <volume> vol. II, </volume> <pages> 49-52. </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference-contexts: While the results presented here are applicable to any choice of error function, we should point out that the typical squared error function is not very well suited to variables on a finite domain such as those produced by the output of a neural network. See <ref> [6] </ref> for a complete treatment of the choice of error function. For calculating the gradient, we need to take a derivative of our error function with respect to each of the parameters of the network.
Reference: [7] <author> Kramer, A., and Sangiovanni-Vincentelli, A. </author> <year> (1989). </year> <title> Efficient Parallel Learning Algorithms for Neural Networks. </title> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <editor> ed. D.S. Touretzky, </editor> <address> 40-48. San Mateo: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: 1 Introduction There has been some work done in the area of parallel neural network training algorithms <ref> [7] </ref>, [10], but very little of it has focused on the training of recurrent networks. It takes an exceptionally large amount of computer time to train these types of networks because of the added complexity of the derivative calculations.
Reference: [8] <author> Sejnowski, T.J., and Rosenberg, C.R. </author> <year> (1987). </year> <title> Parallel Networks that Learn to Pronounce English Text. </title> <booktitle> Complex Systems 1, </booktitle> <pages> 145-168. </pages>
Reference-contexts: A message-passing implementation would, in addition, require duplication and update of the w ij and b i values in the local memory of each node. The test case used was a recurrent version of the NETtalk experiment performed by Sejnowski and Rosenberg <ref> [8] </ref>. We used an architecture consisting of 103 input units, 20 hidden layer units, 10 feedback units, 2 units in a second hidden layer, and 27 outputs. <p> The use of feedback in this network allowed us to eliminate the "look-behind" portion of the input layer while still achieving results comparable to <ref> [8] </ref>. This problem was especially amenable to this form of parallelism because it involved a large number of short training sequences, i.e., one sequence for each training word.
Reference: [9] <author> Steck, J. E., McMillin, B., Krishnamurthy, K., and Leninger, G.G. </author> <year> (1993). </year> <title> Parallel Implementation of a Recursive Least-Squares Neural Network Training Method on the Intel iPSC/2. </title> <journal> Journal of Parallel and Distributed Processing 18, </journal> <pages> 89-93. </pages>
Reference: [10] <author> Wu, C.H., and Tsai, J.H. </author> <year> (1992). </year> <title> Concurrent Asynchronous Learning Algorithms for Massively Parallel Recurrent Neural Networks. </title> <journal> Journal of Parallel and Distributed Computing 14, </journal> <pages> 345-353. 13 </pages>
Reference-contexts: 1 Introduction There has been some work done in the area of parallel neural network training algorithms [7], <ref> [10] </ref>, but very little of it has focused on the training of recurrent networks. It takes an exceptionally large amount of computer time to train these types of networks because of the added complexity of the derivative calculations.
References-found: 10

