URL: ftp://ftp.cs.rutgers.edu/pub/farach/ParDict.ps.Z
Refering-URL: http://www.cs.rutgers.edu/~farach/index.html
Root-URL: http://www.cs.rutgers.edu
Title: Optimal Parallel Dictionary Matching and Compression (Extended Abstract)  
Author: Martin Farach S. Muthukrishnan 
Date: April 26, 1995  
Affiliation: Rutgers University  
Pubnum: DIMACS  
Abstract: Emerging applications in multi-media and the Human Genome Project require storage and searching of large databases of strings a task for which parallelism seems the only hope. In this paper, we consider the parallelism in some of the fundamental problems in compressing strings and in matching large dictionaries of patterns against texts. We present the first work-optimal algorithms for these well-studied problems including the classical dictionary matching problem, optimal compression with a static dictionary and the universal data compression with dynamic dictionary of Lempel and Ziv. All our algorithms are randomized and they are of the Las Vegas type. Furthermore, they are fast, working in time logarithmic in the input size. Additionally, our algorithms seem suitable for a distributed implementation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. De Agostino. </author> <title> P-complete problems in data compression. </title> <journal> Theoretical Computer Science, </journal> <volume> 127 </volume> <pages> 181-186, </pages> <year> 1984. </year>
Reference-contexts: Compression Schemes A wide variety of compression schemes exist in the literature [25]. Amongst the most powerful, and the ones that will be addressed in this paper, are the so-called dictionary schemes. These schemes can generally be described as follows. Suppose the prefix S <ref> [1; i] </ref> of string S has been compressed. Then replace a prefix of S [i + 1; n] with a reference to some word from a "dictionary". If this word is of length k, then we will have compressed the prefix S [1; i + k]. <p> Suppose the prefix S [1; i] of string S has been compressed. Then replace a prefix of S [i + 1; n] with a reference to some word from a "dictionary". If this word is of length k, then we will have compressed the prefix S <ref> [1; i + k] </ref>. Two issues arise: what is the dictionary, and how do we pick the best word from this dictionary so as to minimize the number of references in such a parsing. The well-known LZ1 compression scheme of Lempel and Ziv [20] makes the following choices. <p> LZ1 is known to give better compressions in practice; for example, see Unix compress and gnuzip. Nonetheless, LZ2 is implemented in practice because of the simplicity of its sequential implementation. Curiously, while we provide optimal work RNC algorithm for LZ1 compression, LZ2 is P-Complete <ref> [1] </ref> (hence unlikely to have (R)NC algorithms). Versions of our algorithms seem suitable for dis tributed implementation on a network of workstations [24]. In fact, in this setting, we can conclude from Communication Complexity that even checking equality of strings requires randomization for efficiency [29]. <p> Formally, the dictionary matching problem is as follows. A dictionary D = fP 1 ; : : : ; P k g such that P k i=1 jP i j = d is given for preprocessing. Queries of the form T <ref> [1; n] </ref> are presented and the output is M [1; n] such that M [i] = j if T [i; i + jP j j 1] = P j , that is, if P j matches at T [i], and no longer pattern from D also matches at T [i] 2 <p> A dictionary D = fP 1 ; : : : ; P k g such that P k i=1 jP i j = d is given for preprocessing. Queries of the form T <ref> [1; n] </ref> are presented and the output is M [1; n] such that M [i] = j if T [i; i + jP j j 1] = P j , that is, if P j matches at T [i], and no longer pattern from D also matches at T [i] 2 In what follows, let ^ D be the concatenation <p> Outline of Step 1. We perform this in two substeps. Fix L, an integer to be determined later. Step 1A. We determine S [i], for each location i = kL, for integer k 2 <ref> [1; n=L] </ref>. This substring is specified as an edge (u; v) and a length l. This is done as in [5]. We briefly sketch the approach there. We first construct a separator decomposition of the suffix tree of ^ D. <p> Step 1B. We now compute S [i] for all positions which are not a multiple of L. We process each window T [kL (L 1); : : : ; kL] independently, for integer k 2 <ref> [1; n=L] </ref>. Given S [i], we show how to compute S [i 1] via a procedure called ExtendLeft procedure. Starting with S [kL] and applying ExtendLeft repeatedly, we can compute S [i] for all i within the window under consideration. To implement ExtendLeft we make the following observation. <p> Following that, we can replace the n length text by a string of length O (n log jj) and apply Theorem 3.1. 3.4 Checking the Pattern Matches We are given, for each i 2 <ref> [1; n] </ref>, a pointer M [i] to suffix tree T D of dictionary D which is claimed to represent the longest pattern prefix from D that matches at T [i]. The problem is to check the correctness of the claimed matches. <p> If so, we can check if the match at j is consistent with the overlapping sub-string given by M [A [i]] over the same positions. For example, suppose M [i] = P k 1 <ref> [1; L [i] </ref>]; M [j] = P k 2 [1; L [j]] and i = A [j]. Then we check to make sure that P k 2 [1; L [j]] = P k 1 [j i; j i + L [i]]. Such a check takes constant time, by Lemma 2.6. <p> If so, we can check if the match at j is consistent with the overlapping sub-string given by M [A [i]] over the same positions. For example, suppose M [i] = P k 1 [1; L [i]]; M [j] = P k 2 <ref> [1; L [j] </ref>] and i = A [j]. Then we check to make sure that P k 2 [1; L [j]] = P k 1 [j i; j i + L [i]]. Such a check takes constant time, by Lemma 2.6. <p> For example, suppose M [i] = P k 1 [1; L [i]]; M [j] = P k 2 <ref> [1; L [j] </ref>] and i = A [j]. Then we check to make sure that P k 2 [1; L [j]] = P k 1 [j i; j i + L [i]]. Such a check takes constant time, by Lemma 2.6. Now we consider only dominating matches, since, by the previous step, if the dominating matches are correct, so are the dominated matches. <p> For each dominating match i, we find the next dominating match N [i] via all nearest ones (See Lemma 2.4). For each dominating i, we do the following. Let j = N [i]; M [i] = P k 1 <ref> [1; L [i] </ref>] and M [j] = P k 2 [1; L [j]]. Then we check to make sure that P k 2 [1; L [i] j + i 1] = P k 1 [j i; L [i]]. Once again, such a check takes constant time, by Lemma 2.6. <p> For each dominating match i, we find the next dominating match N [i] via all nearest ones (See Lemma 2.4). For each dominating i, we do the following. Let j = N [i]; M [i] = P k 1 [1; L [i]] and M [j] = P k 2 <ref> [1; L [j] </ref>]. Then we check to make sure that P k 2 [1; L [i] j + i 1] = P k 1 [j i; L [i]]. Once again, such a check takes constant time, by Lemma 2.6. <p> For each dominating i, we do the following. Let j = N [i]; M [i] = P k 1 <ref> [1; L [i] </ref>] and M [j] = P k 2 [1; L [j]]. Then we check to make sure that P k 2 [1; L [i] j + i 1] = P k 1 [j i; L [i]]. Once again, such a check takes constant time, by Lemma 2.6. Lemma 3.4 If an array M passes the above tests, then it is correct. <p> Proof: We need to show only that for dominating matches u &lt; v &lt; w, if u is consistent with v and v is consistent with w, then v is consistent with w. Let M [u] = P k u <ref> [1; L [u] </ref>]; M [v] = P k v [1; L [v]]; M [w] = P k w [1; L [w]]. Then P k w [1; L [u] w + u 1] = P k v [w v; L [u] v + u 1]P k u [w u; L [u]]. 4 <p> Proof: We need to show only that for dominating matches u &lt; v &lt; w, if u is consistent with v and v is consistent with w, then v is consistent with w. Let M [u] = P k u [1; L [u]]; M [v] = P k v <ref> [1; L [v] </ref>]; M [w] = P k w [1; L [w]]. <p> Let M [u] = P k u [1; L [u]]; M [v] = P k v [1; L [v]]; M [w] = P k w <ref> [1; L [w] </ref>]. Then P k w [1; L [u] w + u 1] = P k v [w v; L [u] v + u 1]P k u [w u; L [u]]. 4 Dynamic Compression 4.1 Compression We wish to compute the LZ1 parsing of a string S [1; n]. <p> Let M [u] = P k u <ref> [1; L [u] </ref>]; M [v] = P k v [1; L [v]]; M [w] = P k w [1; L [w]]. Then P k w [1; L [u] w + u 1] = P k v [w v; L [u] v + u 1]P k u [w u; L [u]]. 4 Dynamic Compression 4.1 Compression We wish to compute the LZ1 parsing of a string S [1; n]. <p> Then P k w [1; L [u] w + u 1] = P k v [w v; L [u] v + u 1]P k u [w u; L [u]]. 4 Dynamic Compression 4.1 Compression We wish to compute the LZ1 parsing of a string S <ref> [1; n] </ref>. <p> Recall that the LZ1 parsing is defined as follows: If we have parsed S <ref> [1; i] </ref>, then the next phrase to be parsed is the longest substring of S which starts at both S [i + 1], and at some S [j], for j &lt; i + 1, that is, we must find the pair (j; k) such that S [i + 1; i + <p> If we ever encounter a new character ff, e.g. when we parse the first character in the string, then we output (ff; 0) and advance one character 3 . Suppose that for each i 2 <ref> [1; n] </ref>, we have a pair M [i] = (j; k) such that j &lt; i, S [i; i + k] = S [j; j + k] and k is maximized. <p> For each i, let A [i] = v if v is the deepest ancestor of l i such that L [v] 6= i. Notice that L [A [i]] &lt; i. The importance of these functions is summarized as follows: Lemma 4.1 8i 2 <ref> [1; n] </ref>; M [i] = (L [A [i]]; jA [i]j). Proof: First, by the observations above, L [A [i]] &lt; i, and so the L [A [i]]th suffix is to the left of the ith suffix. <p> By Lemma 2.7, we can therefore compute L within the desired time bounds. Theorem 4.2 A string of length n can be compressed via LZ1 in O (log n) expected time and with O (n) expected work. 4.2 Uncompression Given C <ref> [1; m] </ref>, and LZ1 parsing of a string of length n, we we will construct the uncompressed string T [1; n]. First, we do a prefix sum on the lengths of the blocks in C -taking the special blocks of the form (ff; 0) to have length 1. <p> Theorem 4.2 A string of length n can be compressed via LZ1 in O (log n) expected time and with O (n) expected work. 4.2 Uncompression Given C [1; m], and LZ1 parsing of a string of length n, we we will construct the uncompressed string T <ref> [1; n] </ref>. First, we do a prefix sum on the lengths of the blocks in C -taking the special blocks of the form (ff; 0) to have length 1. <p> We place a 1 at each i in an array B [i] if i is the beginning of a block in T . B [i] = 0 otherwise. Now, for each i 2 <ref> [1; n] </ref> we compute which block it is in, that is, if B [i] = 0, we set B [i] = j such that j &lt; i, B [j] = 1 and j is maximized. This is done by Lemma 2.4. <p> Compression For each position, i, let M [i] be the length of the longest pattern prefix from D that occurs at T [i], that is, if M [i] = j, then there is some k such that P k 2 D and T [i; i + j] = P k <ref> [1; j] </ref>: Similarly, M [i] is the length of the longest matching pattern in a dictionary with the prefix property. Let G = ([1; n]; E) be a directed graph such that (i; i + k) 2 E if k M [i] + 1. <p> Proof: Let M 0 [i] be the prefix maximum (see Lemma 2.3) of array M [i]. For each i, let L [i] = minfjjM 0 [j] &gt; ig. Then, (x; y) 2 E d if L [y] = x. So we must find the rank of each i 2 <ref> [1; n] </ref> in M 0 . If r (i) is the rank of i, then L [i] = r (i) + 1. The rank of each i can be computed in constant time and linear work. All steps are therefore executable within the desired time bounds, thus finishing the proof.
Reference: [2] <author> S. De Agostino and J. Storer. </author> <title> Parallel algorithms for optimal compression using dictionaries with the prefix property. </title> <booktitle> Proc. of the 2nd IEEE Data Compression Conference, </booktitle> <pages> pages 52-61, </pages> <year> 1992. </year>
Reference-contexts: The previously known best algorithm for this problem takes time O (log 2 n) and O (n 3 log 2 n) work, or alternately, takes time O (log n) and O (n 4 log n) work <ref> [2] </ref> after (d log d) work for preprocessing. Dynamic Dictionary Compression: We present the first known optimal algorithm for LZ1 compression as well for its uncompression. Our algorithm constructs the LZ1 compression of a given string of length n in O (log n) time and O (n) work. <p> For instance, the dictionary matching algorithm in [22] does not rely on suffix trees at all. The bottleneck in the previosuly known best bounds for static optimal compression <ref> [2] </ref> lies in computing shortest paths in graphs. Dictionary Matching. <p> In particular, we show that it suffices to consider certain dominating dictionary references to determine the optimal parsing (See Section 4). In contrast, previous approaches to this problem have relied on applying a general purpose shortest-paths routine <ref> [2] </ref>; this step is work-inefficient due to the well-known transitive closure bottleneck [16]. Dynamic Dictionary Compression. LZ1 [20] and LZ2 [30] are two well-known dynamic compression schemes. LZ1 is known to give better compressions in practice; for example, see Unix compress and gnuzip.
Reference: [3] <author> A.V. Aho and M.J. Cora-sick. </author> <title> Efficient string matching. </title> <journal> Communications of the ACM, </journal> <volume> 18(6) </volume> <pages> 333-340, </pages> <year> 1975. </year>
Reference-contexts: These two areas of study have an intimately linked history and are amongst the most intensively studied problems in Computer Science. (For compression see e.g. [25] and for dictionary matching see e.g. <ref> [3, 18, 22, 5, 4] </ref>). In this paper, we present the first work-optimal algorithms for these problems in a parallel setting. Furthermore, all of our algorithms are fast, working in time logarithmic in the input size. Compression Schemes A wide variety of compression schemes exist in the literature [25]. <p> On this model too, the work bounds of our algorithm are optimal. For the special case when the alphabet size is polynomial in the input size, the classical algorithm of <ref> [3] </ref> for dictionary matching can be implemented with randomization in O (n) sequential time and space. For this case, we obtain a suboptimal algorithm: our algorithm has an O (log log d) extra factor in the dictionary and text processing work (with no penalties on time). <p> The bottleneck in the previosuly known best bounds for static optimal compression [2] lies in computing shortest paths in graphs. Dictionary Matching. Within two years of the discovery of the classical linear time string matching algorithm due to Knuth, Morris and Pratt [19], Aho and Corasick <ref> [3] </ref> designed a linear time (hence, optimal) algorithm for dictionary matching by generalizing the finite automaton construction in [19] to a set of strings. In the mid-eighties, Galil [12] and Vishkin [27] designed the first work-optimal string matching algorithms, which have since been extended significantly [28, 13, 9]. <p> In the mid-eighties, Galil [12] and Vishkin [27] designed the first work-optimal string matching algorithms, which have since been extended significantly [28, 13, 9]. However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach of <ref> [3] </ref> is inherently sequential. Recent progress on parallel dictionary matching [4, 5, 18, 22] based on alternate techniques has only yielded suboptimal bounds.
Reference: [4] <author> A. Amir and M. Farach. </author> <title> Adaptive dictionary matching. </title> <booktitle> Proc. of the 32nd IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 760-766, </pages> <year> 1991. </year>
Reference-contexts: These two areas of study have an intimately linked history and are amongst the most intensively studied problems in Computer Science. (For compression see e.g. [25] and for dictionary matching see e.g. <ref> [3, 18, 22, 5, 4] </ref>). In this paper, we present the first work-optimal algorithms for these problems in a parallel setting. Furthermore, all of our algorithms are fast, working in time logarithmic in the input size. Compression Schemes A wide variety of compression schemes exist in the literature [25]. <p> However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach of [3] is inherently sequential. Recent progress on parallel dictionary matching <ref> [4, 5, 18, 22] </ref> based on alternate techniques has only yielded suboptimal bounds. Our work-optimal results for dictionary matching are obtained by judiciously combining ideas from [5] and [22] to solve a generalization of dictionary matching, called, dictionary substring matching (see Section 3).
Reference: [5] <author> A. Amir, M. Farach, and Y. Matias. </author> <title> Efficient randomized dictionary matching algorithms. </title> <booktitle> Proc. of 3rd Combinatorial Pattern Matching Conference, </booktitle> <pages> pages 259-272, </pages> <address> 1992. Tucson, Arizona. </address>
Reference-contexts: These two areas of study have an intimately linked history and are amongst the most intensively studied problems in Computer Science. (For compression see e.g. [25] and for dictionary matching see e.g. <ref> [3, 18, 22, 5, 4] </ref>). In this paper, we present the first work-optimal algorithms for these problems in a parallel setting. Furthermore, all of our algorithms are fast, working in time logarithmic in the input size. Compression Schemes A wide variety of compression schemes exist in the literature [25]. <p> However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach of [3] is inherently sequential. Recent progress on parallel dictionary matching <ref> [4, 5, 18, 22] </ref> based on alternate techniques has only yielded suboptimal bounds. Our work-optimal results for dictionary matching are obtained by judiciously combining ideas from [5] and [22] to solve a generalization of dictionary matching, called, dictionary substring matching (see Section 3). <p> As in the case of [19], the finite automaton based approach of [3] is inherently sequential. Recent progress on parallel dictionary matching [4, 5, 18, 22] based on alternate techniques has only yielded suboptimal bounds. Our work-optimal results for dictionary matching are obtained by judiciously combining ideas from <ref> [5] </ref> and [22] to solve a generalization of dictionary matching, called, dictionary substring matching (see Section 3). We crucially employ a novel data structural primitive that we call the nearest colored ancestors problem on trees. <p> Fix L, an integer to be determined later. Step 1A. We determine S [i], for each location i = kL, for integer k 2 [1; n=L]. This substring is specified as an edge (u; v) and a length l. This is done as in <ref> [5] </ref>. We briefly sketch the approach there. We first construct a separator decomposition of the suffix tree of ^ D. Then we trace down from the root starting from each of the desired text locations independently. <p> All time bounds stated below are expected time bounds that hold with high probability. Also, the bound below do not include that for constructing the suffix tree for T D from Lemma 2.1 [11]. Step 1. From the bounds in <ref> [5] </ref> it follows that Step 1A can be done in O (log d) time and O (n log d=L) work. The precomputation required can be done in O (log d) time and O (d) work.
Reference: [6] <author> O. Berkman, D. Breslauer, Z. Galil, B. Schieber, and U. Vishkin. </author> <title> Highly parallelizable problems. </title> <booktitle> Proc. of the 21st Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 309-319, </pages> <year> 1989. </year>
Reference: [7] <author> O. Berkman and U. Vishkin. </author> <title> Recursive *-tree parallel data-structure. </title> <booktitle> In Proc. of the 30th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 196-202, </pages> <year> 1989. </year>
Reference: [8] <author> P.C.P. Bhatt, K. Diks, T. Hagerup, V.C. Prasad, T. Raznik, and S. Saxena. </author> <title> Improved deterministic parallel integer sorting. </title> <booktitle> Information and Computation, </booktitle> <pages> pages 29-47, </pages> <year> 1991. </year>
Reference-contexts: An important task in our algorithm is the construction of a suffix tree - this has the well-known bottleneck of parallel integer sorting for which the best known algorithm is suboptimal by an O (log log d) factor in this case <ref> [8] </ref>. The work bound of our algorithm improves on the previously best algorithm [22] unless the alphabet size is superexponential in m. In what follows, we assume that the alphabet size is constant, as is standard in the literature on compression techniques [25].
Reference: [9] <author> R. Cole, M. Crochemore, Z. Galil, L. Gasieniec, K. Park, S. Muthukrishnan, H. Ramesh, and W. Rytter. </author> <title> Optimally fast parallel algorithms for preprocessing and pattern matching in one and two dimensions. </title> <booktitle> Proc. of the 34th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 248-258, </pages> <year> 1993. </year>
Reference-contexts: In the mid-eighties, Galil [12] and Vishkin [27] designed the first work-optimal string matching algorithms, which have since been extended significantly <ref> [28, 13, 9] </ref>. However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach of [3] is inherently sequential. Recent progress on parallel dictionary matching [4, 5, 18, 22] based on alternate techniques has only yielded suboptimal bounds.
Reference: [10] <author> M. Crochemore and W. Rytter. </author> <title> Efficient parallel algorithms to test square-freeness and factorize strings. </title> <journal> Information Processing Letters, </journal> <volume> 38 </volume> <pages> 57-60, </pages> <year> 1991. </year>
Reference-contexts: Here we make the standard assumption [23] that n is known. The previously known best algorithms perform O (n log n) work for compression <ref> [23, 10] </ref> as well as for uncompression [23]. 1.2 Technical Contribution The suffix tree of a string (defined in Section 2) is a versatile data structure in string processing. All our algorithms crucially rely on a recently discovered algorithm for constructing the suffix tree of a string [11].
Reference: [11] <author> M. Farach and S. Muthukrishnan. </author> <title> An optimal, logarithmic time, randomized parallel suffix tree contruction algorithm. </title> <type> Technical report, </type> <institution> DIMACS, </institution> <year> 1995. </year>
Reference-contexts: All our algorithms crucially rely on a recently discovered algorithm for constructing the suffix tree of a string <ref> [11] </ref>. <p> All time bounds stated below are expected time bounds that hold with high probability. Also, the bound below do not include that for constructing the suffix tree for T D from Lemma 2.1 <ref> [11] </ref>. Step 1. From the bounds in [5] it follows that Step 1A can be done in O (log d) time and O (n log d=L) work. The precomputation required can be done in O (log d) time and O (d) work. <p> Here is the size of the alphabet set. These work bounds are optimal. Proof: First we use the randomized renaming procedure in <ref> [11] </ref> that remaps the symbols into the range 1 jj in O (log n log jj) time and O (n log jj) work in the comparison model.
Reference: [12] <author> Z. Galil. </author> <title> Optimal parallel algorithms for string matching. </title> <booktitle> In Proc. of the 16th Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 240-248, </pages> <year> 1984. </year>
Reference-contexts: In the mid-eighties, Galil <ref> [12] </ref> and Vishkin [27] designed the first work-optimal string matching algorithms, which have since been extended significantly [28, 13, 9]. However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach of [3] is inherently sequential.
Reference: [13] <author> Z. Galil. </author> <title> A constant-time optimal parallel string-matching algorithm. </title> <booktitle> Proc. of the 24th Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 69-76, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In the mid-eighties, Galil [12] and Vishkin [27] designed the first work-optimal string matching algorithms, which have since been extended significantly <ref> [28, 13, 9] </ref>. However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach of [3] is inherently sequential. Recent progress on parallel dictionary matching [4, 5, 18, 22] based on alternate techniques has only yielded suboptimal bounds.
Reference: [14] <author> H. Gazit. </author> <title> An optimal randomized parallel algorithm for finding connected components in a graph. </title> <booktitle> In Proc. of the 27th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 492-501, </pages> <year> 1986. </year>
Reference-contexts: Lemma 2.2 ( <ref> [14] </ref>) Given a graph on n vertices and m edges, its connected components can be determined in O (log n) time and O (m) work.
Reference: [15] <author> M. E. Gonzalez-Smith and J. A. Storer. </author> <title> Parallel algorithms for data compression. </title> <journal> Journal of the ACM, </journal> <pages> pages 344-373, </pages> <year> 1985. </year>
Reference: [16] <author> R. M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Vol. A, </booktitle> <pages> pages 869-941. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: In particular, we show that it suffices to consider certain dominating dictionary references to determine the optimal parsing (See Section 4). In contrast, previous approaches to this problem have relied on applying a general purpose shortest-paths routine [2]; this step is work-inefficient due to the well-known transitive closure bottleneck <ref> [16] </ref>. Dynamic Dictionary Compression. LZ1 [20] and LZ2 [30] are two well-known dynamic compression schemes. LZ1 is known to give better compressions in practice; for example, see Unix compress and gnuzip. Nonetheless, LZ2 is implemented in practice because of the simplicity of its sequential implementation.
Reference: [17] <author> R.M. Karp and M.O. Rabin. </author> <title> Efficient randomized pattern-matching algorithms. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 31 </volume> <pages> 249-260, </pages> <year> 1987. </year>
Reference-contexts: We first construct a separator decomposition of the suffix tree of ^ D. Then we trace down from the root starting from each of the desired text locations independently. The key is that string comparison along the edges and separators are done using fingerprints <ref> [17] </ref>. Step 1B. We now compute S [i] for all positions which are not a multiple of L. We process each window T [kL (L 1); : : : ; kL] independently, for integer k 2 [1; n=L].
Reference: [18] <author> Z. M. Kedem, G. M. Landau, and K. V. Palem. </author> <title> Optimal parallel suffix-prefix matching algorithm and application. </title> <booktitle> 1st Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <volume> 6 </volume> <pages> 388-398, </pages> <year> 1989. </year>
Reference-contexts: These two areas of study have an intimately linked history and are amongst the most intensively studied problems in Computer Science. (For compression see e.g. [25] and for dictionary matching see e.g. <ref> [3, 18, 22, 5, 4] </ref>). In this paper, we present the first work-optimal algorithms for these problems in a parallel setting. Furthermore, all of our algorithms are fast, working in time logarithmic in the input size. Compression Schemes A wide variety of compression schemes exist in the literature [25]. <p> However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach of [3] is inherently sequential. Recent progress on parallel dictionary matching <ref> [4, 5, 18, 22] </ref> based on alternate techniques has only yielded suboptimal bounds. Our work-optimal results for dictionary matching are obtained by judiciously combining ideas from [5] and [22] to solve a generalization of dictionary matching, called, dictionary substring matching (see Section 3).
Reference: [19] <author> D.E. Knuth, J.H. Morris, and V.R. Pratt. </author> <title> Fast pattern matching in strings. </title> <journal> SIAM Journal on Computing, </journal> <volume> 6 </volume> <pages> 323-350, </pages> <year> 1977. </year>
Reference-contexts: The bottleneck in the previosuly known best bounds for static optimal compression [2] lies in computing shortest paths in graphs. Dictionary Matching. Within two years of the discovery of the classical linear time string matching algorithm due to Knuth, Morris and Pratt <ref> [19] </ref>, Aho and Corasick [3] designed a linear time (hence, optimal) algorithm for dictionary matching by generalizing the finite automaton construction in [19] to a set of strings. <p> Dictionary Matching. Within two years of the discovery of the classical linear time string matching algorithm due to Knuth, Morris and Pratt <ref> [19] </ref>, Aho and Corasick [3] designed a linear time (hence, optimal) algorithm for dictionary matching by generalizing the finite automaton construction in [19] to a set of strings. In the mid-eighties, Galil [12] and Vishkin [27] designed the first work-optimal string matching algorithms, which have since been extended significantly [28, 13, 9]. However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach <p> the finite automaton construction in <ref> [19] </ref> to a set of strings. In the mid-eighties, Galil [12] and Vishkin [27] designed the first work-optimal string matching algorithms, which have since been extended significantly [28, 13, 9]. However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach of [3] is inherently sequential. Recent progress on parallel dictionary matching [4, 5, 18, 22] based on alternate techniques has only yielded suboptimal bounds.
Reference: [20] <author> A. Lempel and J. Ziv. </author> <title> On the complexity of finite sequences. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 22 </volume> <pages> 75-81, </pages> <year> 1976. </year>
Reference-contexts: Two issues arise: what is the dictionary, and how do we pick the best word from this dictionary so as to minimize the number of references in such a parsing. The well-known LZ1 compression scheme of Lempel and Ziv <ref> [20] </ref> makes the following choices. The dictionary is all substrings S [x; y] of S such that x i. <p> In contrast, previous approaches to this problem have relied on applying a general purpose shortest-paths routine [2]; this step is work-inefficient due to the well-known transitive closure bottleneck [16]. Dynamic Dictionary Compression. LZ1 <ref> [20] </ref> and LZ2 [30] are two well-known dynamic compression schemes. LZ1 is known to give better compressions in practice; for example, see Unix compress and gnuzip. Nonetheless, LZ2 is implemented in practice because of the simplicity of its sequential implementation.
Reference: [21] <author> S. Muthukrishnan. </author> <title> A time and space efficient algorithm for dynamic method look-up in object oriented programming languages. </title> <type> manuscript, </type> <year> 1995. </year>
Reference-contexts: We crucially employ a novel data structural primitive that we call the nearest colored ancestors problem on trees. The sequential version of our solution for this problem has already been applied successfully in compilers for Object Oriented Programming Languages <ref> [21] </ref>. Also, interestingly, we have devised a very fast procedure that checks the output of the basic Monte Carlo dictionary matching algorithm: therefore, our dictionary matching algorithm is of the Las Vegas type. Static Dictionary Compression. <p> The second is the structural observation that the extra leaves can be considered in groups of ranges, which reduces our problem to the Van Emde-Boas structure. This algorithm has already been used for dynamic method look-up in object oriented programming languages like the smalltalk <ref> [21] </ref>. 3.3 Results In this section, we state the complexity bounds for each step in our dictionary matching algorithm. All time bounds stated below are expected time bounds that hold with high probability.
Reference: [22] <author> S. Muthukrishnan and K. Palem. </author> <title> Highly efficient parallel dictionary matching. </title> <booktitle> 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year>
Reference-contexts: These two areas of study have an intimately linked history and are amongst the most intensively studied problems in Computer Science. (For compression see e.g. [25] and for dictionary matching see e.g. <ref> [3, 18, 22, 5, 4] </ref>). In this paper, we present the first work-optimal algorithms for these problems in a parallel setting. Furthermore, all of our algorithms are fast, working in time logarithmic in the input size. Compression Schemes A wide variety of compression schemes exist in the literature [25]. <p> The preprocessing and matching work bounds are clearly optimal. The previously known work bound for dictionary processing and text matching in this case are O (d log m) and O (n log m), respectively <ref> [22] </ref>. A natural model in string matching is the comparison model, commonly referred to as the case when the alphabet size is unbounded. <p> The work bound of our algorithm improves on the previously best algorithm <ref> [22] </ref> unless the alphabet size is superexponential in m. In what follows, we assume that the alphabet size is constant, as is standard in the literature on compression techniques [25]. <p> For instance, the dictionary matching algorithm in <ref> [22] </ref> does not rely on suffix trees at all. The bottleneck in the previosuly known best bounds for static optimal compression [2] lies in computing shortest paths in graphs. Dictionary Matching. <p> However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach of [3] is inherently sequential. Recent progress on parallel dictionary matching <ref> [4, 5, 18, 22] </ref> based on alternate techniques has only yielded suboptimal bounds. Our work-optimal results for dictionary matching are obtained by judiciously combining ideas from [5] and [22] to solve a generalization of dictionary matching, called, dictionary substring matching (see Section 3). <p> Recent progress on parallel dictionary matching [4, 5, 18, 22] based on alternate techniques has only yielded suboptimal bounds. Our work-optimal results for dictionary matching are obtained by judiciously combining ideas from [5] and <ref> [22] </ref> to solve a generalization of dictionary matching, called, dictionary substring matching (see Section 3). We crucially employ a novel data structural primitive that we call the nearest colored ancestors problem on trees. <p> That completes the description of Step 2A. Step 2B. We will now compute M [i]. For this, we precompute, for each pattern prefix, its longest prefix which is a pattern. This is done by modifying a similar step in <ref> [22] </ref>. Following that, we can look up M [i] directly. 3.2 Data Structures The Nearest Colored Ancestor Problem. <p> Step 2. Step 2A can be done in O (1) time and O (1) work per text location. The preprocessing can be done in O (log d) time and O (d) work. &gt;From the results in <ref> [22] </ref>, the preprocessing for Step 2B can be done in O (log d) time and O (d) work. The text processing in Step 2B can be done in O (log fl d) time and O (d) work.
Reference: [23] <author> M. Naor. </author> <title> String matching with preprocessing of text and pattern. </title> <booktitle> Proc. of 18th International Colloquium on Automata Languages and Programming, </booktitle> <pages> pages 739-750, </pages> <year> 1991. </year>
Reference-contexts: Also, given the LZ1 compressed version of a string of length n, our algorithm reconstructs the string in O (log n) time and O (n) work. Here we make the standard assumption <ref> [23] </ref> that n is known. The previously known best algorithms perform O (n log n) work for compression [23, 10] as well as for uncompression [23]. 1.2 Technical Contribution The suffix tree of a string (defined in Section 2) is a versatile data structure in string processing. <p> Here we make the standard assumption [23] that n is known. The previously known best algorithms perform O (n log n) work for compression <ref> [23, 10] </ref> as well as for uncompression [23]. 1.2 Technical Contribution The suffix tree of a string (defined in Section 2) is a versatile data structure in string processing. All our algorithms crucially rely on a recently discovered algorithm for constructing the suffix tree of a string [11]. <p> Here we make the standard assumption <ref> [23] </ref> that n is known. The previously known best algorithms perform O (n log n) work for compression [23, 10] as well as for uncompression [23]. 1.2 Technical Contribution The suffix tree of a string (defined in Section 2) is a versatile data structure in string processing. All our algorithms crucially rely on a recently discovered algorithm for constructing the suffix tree of a string [11].
Reference: [24] <author> M. Papadipoyli. </author> <title> A distributed dictionary matching implementation. </title> <year> 1994. </year>
Reference-contexts: Curiously, while we provide optimal work RNC algorithm for LZ1 compression, LZ2 is P-Complete [1] (hence unlikely to have (R)NC algorithms). Versions of our algorithms seem suitable for dis tributed implementation on a network of workstations <ref> [24] </ref>. In fact, in this setting, we can conclude from Communication Complexity that even checking equality of strings requires randomization for efficiency [29]. Thus the randomization in our algorithms seems well justified. Section 2 contains a review of results we use.
Reference: [25] <author> J. Storer. </author> <title> Data compression: methods and theory. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland, </address> <year> 1988. </year>
Reference-contexts: In this paper, we consider parallelism in some of the fundamental problems in compressing strings, and in matching large dictionaries of patterns against texts. These two areas of study have an intimately linked history and are amongst the most intensively studied problems in Computer Science. (For compression see e.g. <ref> [25] </ref> and for dictionary matching see e.g. [3, 18, 22, 5, 4]). In this paper, we present the first work-optimal algorithms for these problems in a parallel setting. Furthermore, all of our algorithms are fast, working in time logarithmic in the input size. <p> In this paper, we present the first work-optimal algorithms for these problems in a parallel setting. Furthermore, all of our algorithms are fast, working in time logarithmic in the input size. Compression Schemes A wide variety of compression schemes exist in the literature <ref> [25] </ref>. Amongst the most powerful, and the ones that will be addressed in this paper, are the so-called dictionary schemes. These schemes can generally be described as follows. Suppose the prefix S [1; i] of string S has been compressed. <p> Such dictionaries are said to have the prefix property. The question of how a match is selected now becomes problematical. The greedy heuristic of always choosing the longest match need not give optimal compression. A variety of other sub-optimal heuristics (Longest Fragment First, etc.) have been proposed <ref> [25] </ref>. In this paper, we will only consider optimal parsing. One of the crucial subtask for static dictionary compression is to find matches from a dictionary of patterns at some locations in the text. Of course this is simply the dictionary matching problem which is defined below. <p> The work bound of our algorithm improves on the previously best algorithm [22] unless the alphabet size is superexponential in m. In what follows, we assume that the alphabet size is constant, as is standard in the literature on compression techniques <ref> [25] </ref>. However, the bounds we quote below can be easily modified to include other cases in a manner analogous to the dictio nary matching case above. Optimal Static Compression: We give a work optimal algorithm for static dictionary compression.
Reference: [26] <author> P. van Emde Boas, R. Kaas, and E. Zijlstra. </author> <title> Design and implementation of an efficient priority queue. </title> <journal> Math. Systems Theory, </journal> <volume> 10 </volume> <pages> 99-127, </pages> <year> 1977. </year>
Reference-contexts: In what follows, we show how the preprocessing work can be reduced to O ( all real skeleton trees R c jR c j) = O (n + C) while taking O (log log n) time for each query. To achieve this, we use the van Emde Boas' result <ref> [26] </ref>. For any c 2 C, associate with each node in the naive skeleton tree T c , the group of out-leaves which are its descendant but which are not the descendant of any other internal node in T c .
Reference: [27] <author> U. Vishkin. </author> <title> Optimal parallel pattern matching in strings. </title> <booktitle> Proc. of 12th International Colloquium on Automata Languages and Programming, </booktitle> <publisher> Springer LNCS, </publisher> <pages> pages 91-113, </pages> <year> 1985. </year>
Reference-contexts: In the mid-eighties, Galil [12] and Vishkin <ref> [27] </ref> designed the first work-optimal string matching algorithms, which have since been extended significantly [28, 13, 9]. However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach of [3] is inherently sequential.
Reference: [28] <author> U. Vishkin. </author> <title> Deterministic sampling anew technique for fast pattern matching. </title> <journal> SIAM Journal on Computing, </journal> <volume> 20 </volume> <pages> 303-314, </pages> <year> 1991. </year>
Reference-contexts: In the mid-eighties, Galil [12] and Vishkin [27] designed the first work-optimal string matching algorithms, which have since been extended significantly <ref> [28, 13, 9] </ref>. However, a work-optimal algorithm for dictionary matching has remained elusive. As in the case of [19], the finite automaton based approach of [3] is inherently sequential. Recent progress on parallel dictionary matching [4, 5, 18, 22] based on alternate techniques has only yielded suboptimal bounds.
Reference: [29] <author> A. C. C. Yao. </author> <title> Some complexity questions related to distributed computing. </title> <booktitle> Proc. of the 11th Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 209-213, </pages> <year> 1979. </year>
Reference-contexts: Versions of our algorithms seem suitable for dis tributed implementation on a network of workstations [24]. In fact, in this setting, we can conclude from Communication Complexity that even checking equality of strings requires randomization for efficiency <ref> [29] </ref>. Thus the randomization in our algorithms seems well justified. Section 2 contains a review of results we use.
Reference: [30] <author> J. Ziv and A. Lempel. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-23(3):337-343, </volume> <year> 1977. </year>
Reference-contexts: In contrast, previous approaches to this problem have relied on applying a general purpose shortest-paths routine [2]; this step is work-inefficient due to the well-known transitive closure bottleneck [16]. Dynamic Dictionary Compression. LZ1 [20] and LZ2 <ref> [30] </ref> are two well-known dynamic compression schemes. LZ1 is known to give better compressions in practice; for example, see Unix compress and gnuzip. Nonetheless, LZ2 is implemented in practice because of the simplicity of its sequential implementation.
References-found: 30

