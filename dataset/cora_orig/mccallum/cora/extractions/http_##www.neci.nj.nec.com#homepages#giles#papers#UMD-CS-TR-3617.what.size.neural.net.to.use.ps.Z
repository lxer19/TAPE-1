URL: http://www.neci.nj.nec.com/homepages/giles/papers/UMD-CS-TR-3617.what.size.neural.net.to.use.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Title: What Size Neural Network Gives Optimal Generalization? Convergence Properties of Backpropagation  
Author: Steve Lawrence ;fl C. Lee Giles yz Ah Chung Tsoi 
Keyword: Local Minima, Generalization, Committees, Ensembles, Convergence, Backpropagation, Smoothness, Network Size, Problem Complexity, Function Approximation, Curse of Dimensionality.  
Note: June  http://www.neci.nj.nec.com/homepages/lawrence Also with the  
Address: 4 Independence Way, Princeton, NJ 08540  St. Lucia 4072, Australia  College Park, MD 20742.  
Affiliation: NEC Research Institute,  Department of Electrical and Computer Engineering University of Queensland,  Institute for Advanced Computer Studies, University of Maryland,  
Email: flawrence,actg@elec.uq.edu.au, giles@research.nj.nec.com  
Phone: 1  2  
Web: http://www.neci.nj.nec.com/homepages/giles.html  
Date: 1996 (Revised August 1996)  
Abstract: Technical Report UMIACS-TR-96-22 and CS-TR-3617 Institute for Advanced Computer Studies University of Maryland College Park, MD 20742 Abstract One of the most important aspects of any machine learning paradigm is how it scales according to problem size and complexity. Using a task with known optimal training error, and a pre-specified maximum number of training updates, we investigate the convergence of the backpropagation algorithm with respect to a) the complexity of the required function approximation, b) the size of the network in relation to the size required for an optimal solution, and c) the degree of noise in the training data. In general, for a) the solution found is worse when the function to be approximated is more complex, for b) oversized networks can result in lower training and generalization error in certain cases, and for c) the use of committee or ensemble techniques can be more beneficial as the level of noise in the training data is increased. For the experiments we performed, we do not obtain the optimal solution in any case. We further support the observation that larger networks can produce better training and generalization error using a face recognition example where a network with many more parameters than training points generalizes better than smaller networks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. </author> <year> (1989), </year> <title> `The Vapnik-Chervonenkis dimension: Information versus complexity in learning', </title> <booktitle> Neural Computation 1(3), </booktitle> <pages> 312317. </pages>
Reference: <author> Akaike, H. </author> <year> (1970), </year> <title> `Statistical predictor identification', </title> <journal> Annals of the Institute for Statistical Mathematics 22, 203217. </journal>
Reference: <author> Akaike, H. </author> <year> (1973), </year> <title> Information theory and an extension of the maximum likelihood principle, </title> <editor> in B. N. Petrov and F. Csaki, eds, </editor> <booktitle> `Proceeding 2nd International Symposium on Information Theory', </booktitle> <publisher> Akademia Kiado, Budapest, </publisher> <pages> pp. 267281. </pages>
Reference: <author> Akaike, H. </author> <year> (1974), </year> <title> `A new look at the statistical model identification', </title> <journal> IEEE Transactions on Automatic Control 19, </journal> <volume> 716723. </volume>
Reference: <author> Amari, S. </author> <year> (1995), </year> <title> Learning and statistical inference, </title> <editor> in M. A. Arbib, ed., </editor> <title> `The Handbook of Brain Theory and Neural Networks', </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <pages> pp. 522526. </pages>
Reference-contexts: There are several theories for determining the optimal network size e.g. the NIC (Network Information Criterion) <ref> (Amari, 1995) </ref> which is a generalization of the AIC (Akaike Information Criterion) (Akaike, 1973; Akaike, 1974) widely used in statistical inference, the generalized final prediction error 4 (GPE) as proposed by Moody (1992), and the Vapnik-Chervonenkis (VC) dimension (Maass, 1995; Abu-Mostafa, 1989; Bartlett, 1993) which is a measure of the expressive
Reference: <author> Back, A. </author> <year> (1992), </year> <title> New Techniques for Nonlinear System Identification: A Rapprochement Between Neural Networks and Linear Systems, PhD thesis, Department of Electrical Engineering, University of Queensland. 31 1 Hidden Node 2 Hidden Nodes 3 Hidden Nodes 4 Hidden Nodes nodes is increased from 1 to 4. is a uniformly distributed random variable between -0.25 and 0.25. The network with two hidden nodes provides the best generalization performance larger networks resulted in worse generalization due to overfitting. </title>
Reference: <author> Baldi, P. and Hornik, K. </author> <year> (1988), </year> <title> `Neural networks and principal component analysis: Learning from examples without local minima', </title> <booktitle> Neural Networks 2(1), </booktitle> <pages> 5358. </pages>
Reference-contexts: In practice, however, other features of the error surface such as ravines and plateaus <ref> (Baldi and Hornik, 1988) </ref> can present difficulty for optimisation. For example, the two error functions shown in figure 1 (from (Gori, 1996)) do not have local minima. However, the function on the left is expected to be more difficult to optimise with gradient descent.
Reference: <author> Barron, A. </author> <year> (1991), </year> <title> Complexity regularization with application to artificial neural networks, </title> <editor> in G. Roussas, ed., </editor> <title> `Non-parametric Functional Estimation and Related Topics', </title> <publisher> Kluwer Academic Publishers, Dordrecht, The Nether-lands, </publisher> <pages> pp. 561576. </pages>
Reference: <author> Barron, A. </author> <year> (1992), </year> <title> Neural net approximation, </title> <booktitle> in `Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems', </booktitle> <institution> Yale University, </institution> <address> New Haven, CT, </address> <pages> pp. 6972. </pages>
Reference: <author> Bartlett, P. </author> <year> (1993), </year> <title> `Vapnik-Chervonenkis dimension bounds for two-and three-layer networks', </title> <booktitle> Neural Computation 5(3), </booktitle> <pages> 371373. </pages>
Reference: <author> Bartlett, P. </author> <year> (1996), </year> <title> The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network, </title> <type> Technical report, </type> <institution> Australian National University. </institution>
Reference: <author> Baum, E. and Haussler, D. </author> <year> (1989), </year> <title> `What size net gives valid generalization?', </title> <booktitle> Neural Computation 1(1), </booktitle> <pages> 151160. </pages>
Reference: <author> Bengio, Y., ed. </author> <year> (1996), </year> <title> Neural Networks for Speech and Sequence Recognition, Thomson. 32 Bourlard, </title> <editor> H. and Morgan, N. </editor> <year> (1994), </year> <title> Connnectionist Speech Recognition: A Hybrid Approach, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA. </address>
Reference-contexts: Generalization refers to how well a model performs on unseen data, i.e. the model is trained using a given training set and generalization corresponds to the expected performance of the model on new patterns. Mathematically, the goal of MLP training can be formulated as minimization of a cost function <ref> (Bengio, 1996) </ref>: E true = Z e (f (x; w); d)p (x; d) dxdd (5) 27 where e is a local cost function, f is the function implemented by the MLP, x is the input to the model, d is the desired output of the model, w corresponds to the weights <p> The objective of training is to optimise the parameters w such that E true is minimised: ^ w = argmin w x;d e (f (x; w); d)p (x; d) dxdd (6) E true is the generalization error <ref> (Bengio, 1996) </ref>, i.e. the expected performance of the MLP on new patterns randomly chosen from p (x; d). In practice p (x; d) is not known. <p> Instead, a training set T = fx p ; d p g N p given, where N p is the number of patterns, and an approximation of E true is minimised which is called the empirical error (Vapnik, 1982) or training error <ref> (Bengio, 1996) </ref>: E = p=1 The quadratic and relative entropy cost functions are examples of such an error function. A very important question is how well a model trained to minimise E generalises (i.e. how low E true is).
Reference: <author> Breiman, L. </author> <year> (1994), </year> <title> `Discussion of neural networks and related methods for classification', </title> <journal> Journal of the Royal Statistical Society B 56(3), </journal> <volume> 409456. </volume>
Reference: <author> Cardell, N. S., Joerding, W. and Li, Y. </author> <year> (1994), </year> <title> `Why some feedforward networks cannot learn some polynomials', </title> <booktitle> Neural Computation 6(4), </booktitle> <pages> 761766. </pages>
Reference-contexts: This is typically a 10 Large weights do not always correspond to target functions which are not smooth, for example this is not the case when fitting the function sech (x) using two tanh sigmoids <ref> (Cardell, Joerding and Li, 1994) </ref> (because sech (x) = lim d!0 (tanh (x + d) tanh (x))=d, i.e. the weights become indefinitely large as the approximation improves). 13 K=1 networks used for creating these plots had the topology 20:10:1. The rows correspond to the first five inputs of these networks.
Reference: <author> Caruana, R. </author> <year> (1993), </year> <title> Generalization vs. Net Size, </title> <booktitle> Neural Information Processing Systems, Tutorial, </booktitle> <address> Denver, </address> <publisher> CO. </publisher>
Reference-contexts: For a network with more hidden layer neurons, this interference effect is expected to be more pronounced. Caruana presented a tutorial at NIPS 93 <ref> (Caruana, 1993) </ref> with generalization results on a variety of problems as the size of the networks was varied from too small to too large.
Reference: <author> Cohn, D. and Tesauro, G. </author> <year> (1992), </year> <title> `How tight are the Vapnik-Chervonenkis bounds?', </title> <booktitle> Neural Computation 4(2), </booktitle> <volume> 249 269. </volume>
Reference-contexts: There is very little published computational experience of the NIC, or the GPE. Their evaluation is prohibitively expensive for large networks. VC bounds have been calculated for various network types <ref> (Cohn and Tesauro, 1992) </ref>. Early VC-dimension work handles only the case of discrete outputs. For the case of real valued outputs, a more general notion of a dimension is required.
Reference: <author> Crane, R., Fefferman, C., Markel, S. and Pearson, J. </author> <year> (1995), </year> <title> Characterizing neural network error surfaces with a sequential quadratic programming algorithm, in `Machines That Learn', </title> <address> Snowbird. </address>
Reference-contexts: The task is as follows and is very similar to the procedure used in <ref> (Crane et al., 1995) </ref>: 1.
Reference: <author> Dadson, J. </author> <year> (1996), </year> <title> `Article in comp.ai.neural-nets, message id: </title> <address> 4t19h0$ko8@sjx-ixn5.ix.netcom.com, </address> <month> 23 July'. </month>
Reference: <author> Drucker, H., Cortes, C., Jackel, L., Le Cun, Y. and Vapnik, V. </author> <year> (1994), </year> <title> `Boosting and other ensemble methods', </title> <booktitle> Neural Computation 6, </booktitle> <pages> 12891301. </pages>
Reference: <author> Friedman, J. </author> <year> (1995), </year> <title> `Introduction to computational learning and statistical prediction', </title> <booktitle> Tutorial Presented at Neural Information Processing Systems, </booktitle> <address> Denver, </address> <publisher> CO. </publisher>
Reference-contexts: The regression, f (x) is a hypersurface in R n . If f (x) is arbitrarily complex and unknown then dense samples are required to approximate the function accurately. However, it is hard to obtain dense samples in high dimensions. This is the curse of dimensionality 11 <ref> (Friedman, 1995) </ref>. The relationship between the sampling density and the number of points required is N 1 n (Friedman, 1995) where n is the dimensionality of the input space and N is the number of points. <p> However, it is hard to obtain dense samples in high dimensions. This is the curse of dimensionality 11 <ref> (Friedman, 1995) </ref>. The relationship between the sampling density and the number of points required is N 1 n (Friedman, 1995) where n is the dimensionality of the input space and N is the number of points. <p> As such, we can see that the problem is not so much the dimensionality, but the complexity of the function (high dimensional functions typically have the potential to be more complex) <ref> (Friedman, 1995) </ref>, i.e. the curse of dimensionality essentially says that in high dimensions, the less data points we have, the simpler the function has to be in order to represent it accurately.
Reference: <author> Geman, S., Bienenstock, E. and Doursat, R. </author> <year> (1992), </year> <title> `Neural networks and the bias/variance dilemma', </title> <booktitle> Neural Computation 4(1), </booktitle> <pages> 158. </pages>
Reference-contexts: Cooper, 1993). This section investigates the effect of using committees as the amount of noise added to the training data increases. A simple weighted ensemble of networks is used. Consider the bias/variance dilemma as in <ref> (Geman, Bienenstock and Doursat, 1992) </ref> where the MSE may be decomposed into bias and variance components: MSE bias = (E D [f (x)] E [yjx]) 2 (1) MSE variance = E D fi (2) where E D represents the expectation with respect to a training set, D, and f (x) is <p> This is the well known bias/variance tradeoff <ref> (Geman et al., 1992) </ref> in the underfitting case, the MLP estimator produces estimates which have high bias but low variance (an estimator is said to be biased if, on average, the estimated value is different to the expected value).
Reference: <author> Gori, M. </author> <year> (1996), </year> <title> `An introduction to computational suspiciousness', </title> <booktitle> Seminar presented at the University of Queens-land, </booktitle> <address> Brisbane, Australia. </address>
Reference-contexts: In practice, however, other features of the error surface such as ravines and plateaus (Baldi and Hornik, 1988) can present difficulty for optimisation. For example, the two error functions shown in figure 1 (from <ref> (Gori, 1996) </ref>) do not have local minima. However, the function on the left is expected to be more difficult to optimise with gradient descent.
Reference: <author> Gori, M. and Tesi, A. </author> <year> (1992), </year> <title> `On the problem of local minima in backpropagation', </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 14(1), </journal> <volume> 7686. </volume>
Reference: <author> Hamey, L. </author> <year> (1995), </year> <title> Analysis of the error surface of the XOR network with two hidden nodes, </title> <type> Technical Report 95/167C, </type> <institution> Macquarie University, </institution> <address> Sydney, Australia. </address>
Reference-contexts: Although neither of these functions contains local minima, the function on the left is expected to be less suitable for gradient descent optimisation due to the flat regions. 3 Prior Work The error surface of very small networks has been characterized previously, e.g. for an XOR network <ref> (Hamey, 1995) </ref>. However, practical networks often contain hundreds or thousands of weights 2 and, in general, theoretical and empirical results on small networks do not scale up to large networks. One reason may be attributed to the interference effect in the training process.
Reference: <author> Hassibi, B. and Stork, D. G. </author> <year> (1993), </year> <title> Second order derivatives for network pruning: Optimal Brain Surgeon, </title> <editor> in C. </editor> <publisher> L. </publisher>
Reference-contexts: The learning rate was 0.05. 6 There are many ways of controlling generalization, e.g. a) early stopping, b) weight decay or weight elimination, and c) pruning e.g. OBD (optimal brain damage) (Le Cun, Denker and Solla, 1990) and OBS (optimal brain surgeon) <ref> (Hassibi and Stork, 1993) </ref>. 6 6 Simulation Results Results for varying the network size, the training set size, the function complexity, and the amount of noise added to the training data are presented in the following sections. 6.1 Network Size This section investigates the training and generalization behavior of the networks <p> There are many solutions which fit the training data well that will not generalize well. Yet, contrary to what might be expected, the results indicate that it is possible for oversized networks to provide better generalization. Successive pruning and retraining of a larger network <ref> (Hassibi and Stork, 1993) </ref> may arrive at a network with similar size to the smaller networks here but with improved training and generalization error. 6.
Reference: <editor> Giles, S. J. Hanson and J. D. Cowan, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 5, </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Hassoun, M., Cherkassky, V., Hanson, S., Oja, E., Sarle, W. and Sudjianto, A. </author> <year> (1996), </year> <title> Panel on neural networks and statistical models, </title> <booktitle> in `International Conference on Neural Networks, ICNN 96', IEEE, </booktitle> <address> Piscataway, NJ, </address> <pages> pp. 18 22. </pages>
Reference-contexts: A conclusion is that backpropagation can result in the underutilisation of network resources in certain cases (i.e. some parameters may be ineffective or only partially effective due to sub-optimal convergence). Since reporting the work contained in this paper, S. Hanson has stated <ref> (Hassoun, Cherkassky, Hanson, Oja, Sarle and Sudjianto, 1996) </ref>: Whether in the language of approximation theory (overfitting etc.) or statistical estimation (bias vs. variance) it is clear that too many parameters in some nonparametric models can be grievous, however with many Neural Networks, more parameters can actually improve things. and Such [phenomena]
Reference: <author> Haykin, S. </author> <year> (1994), </year> <title> Neural Networks, A Comprehensive Foundation, </title> <publisher> Macmillan, </publisher> <address> New York, NY. </address>
Reference: <author> Hecht-Nielsen, R. </author> <year> (1990), </year> <title> Neurocomputing, </title> <publisher> Addison Wesley, </publisher> <address> New York. </address>
Reference: <author> Jacobs, R. </author> <year> (1995), </year> <title> `Methods for combining experts' probability assessments', </title> <booktitle> Neural Computation 7, </booktitle> <pages> 867888. </pages>
Reference: <author> Kohonen, T. </author> <year> (1995), </year> <title> Self-Organizing Maps, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany. </address>
Reference-contexts: The test set contained a different set of 5 images per person. A small window was stepped over the images and the image samples at each point were quantized using a two dimensional self-organizing map <ref> (Kohonen, 1995) </ref>. The outputs of the self-organizing map for each image sample were used as the inputs to the MLP . A subset of the images is shown in figure 6. In each case, the networks were trained for 25,000 updates.
Reference: <author> Kolmogorov, A. </author> <year> (1957), </year> <title> `On the representation of continuous functions of several variables by superpositions of continuous functions of one variable and addition', </title> <type> Dokl 114, 679681. </type>
Reference: <author> Krogh, A. and Vedelsby, J. </author> <year> (1995), </year> <title> Neural network ensembles, cross validation, and active learning, </title> <editor> in G. Tesauro, D. Touretzky and T. Leen, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 7, </volume> <publisher> The MIT Press, </publisher> <pages> pp. 231238. </pages>
Reference: <author> Krose, B. and van der Smagt, P., </author> <title> eds (1993), An Introduction to Neural Networks, </title> <type> fifth edn, </type> <institution> University of Amsterdam. </institution>
Reference: <author> Kffurkova, V. </author> <year> (1991), </year> <title> `Kolmogorov's theorem is relevant', </title> <booktitle> Neural Computation 3(4), </booktitle> <pages> 617622. </pages>
Reference: <author> Kffurkova, V. </author> <year> (1995), </year> <title> Kolmogorov's theorem, </title> <editor> in M. A. Arbib, ed., </editor> <title> `The Handbook of Brain Theory and Neural Networks', </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <pages> pp. </pages> <note> 501502. 33 Le Cun, </note> <author> Y. </author> <year> (1993), </year> <title> `Efficient learning and second order methods', </title> <booktitle> Tutorial presented at Neural Information Processing Systems 5. </booktitle>
Reference: <author> Le Cun, Y., Denker, J. and Solla, S. </author> <year> (1990), </year> <title> Optimal Brain Damage, </title> <editor> in D. Touretzky, ed., </editor> <booktitle> `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. </volume> <pages> 2, </pages> <address> (Denver 1989), </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <pages> pp. 598605. </pages>
Reference-contexts: The learning rate was 0.05. 6 There are many ways of controlling generalization, e.g. a) early stopping, b) weight decay or weight elimination, and c) pruning e.g. OBD (optimal brain damage) <ref> (Le Cun, Denker and Solla, 1990) </ref> and OBS (optimal brain surgeon) (Hassibi and Stork, 1993). 6 6 Simulation Results Results for varying the network size, the training set size, the function complexity, and the amount of noise added to the training data are presented in the following sections. 6.1 Network Size
Reference: <author> Maass, W. </author> <year> (1995), </year> <title> Vapnik-Chervonenkis dimension of neural networks, </title> <editor> in M. A. Arbib, ed., </editor> <title> `The Handbook of Brain Theory and Neural Networks', </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <pages> pp. 522526. </pages>
Reference-contexts: Early VC-dimension work handles only the case of discrete outputs. For the case of real valued outputs, a more general notion of a dimension is required. Such a pseudo-dimension can be defined by considering a loss function which measures the deviation of predictions from the target values <ref> (Maass, 1995) </ref>. VC bounds are likely to be too conservative because they provide generalization guarantees simultaneously for any probability distribution and any training algorithm. The computation of VC bounds for practical networks is difficult.
Reference: <author> McInerny, J., Haines, K., Biafore, S. and Hecht-Nielsen, R. </author> <year> (1989), </year> <title> Back propagation error surfaces can have local minima, </title> <booktitle> in `International Joint Conference on Neural Networks', </booktitle> <volume> Vol. </volume> <pages> 2, </pages> <address> (Washington 1989), </address> <publisher> IEEE, </publisher> <address> New York, p. </address> <month> 627. </month>
Reference: <author> Minsky, M. and Papert, S. </author> <year> (1988), </year> <title> Perceptrons, expanded version of the original 1969 edn, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Moody, J. </author> <year> (1992), </year> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems, </title> <editor> in J. Moody, S. J. Hanson and R. P. Lippmann, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 4, </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> pp. 847854. </pages>
Reference: <author> Muller, K., Finke, M., Schulten, K., Murata, N. and Amari, S. </author> <year> (1996), </year> <title> `A numerical study on learning curves in stochastic multi-layer feed-forward networks', Neural Computation p. </title> <publisher> in press. </publisher>
Reference: <author> Naftaly, U., Intrator, N. and Horn, D. </author> <year> (1995), </year> <title> Training single networks for optimal ensemble performance, </title> <editor> in T. </editor> <publisher> L. </publisher>
Reference-contexts: For a multilayer perceptron, there is another variance term due to convergence to local minima which can be reduced using ensembles, and the effect of this reduction is greater if the individual networks have larger variance (see <ref> (Naftaly, Intrator and Horn, 1995) </ref>). Increasing noise levels, and the resulting poorer convergence, may induce this condition. Therefore, it is expected that the ensemble technique may be more beneficial as the noise level is increased.
Reference: <editor> Gerald Tesauro, David Touretzky, ed., </editor> <booktitle> `Advances in Neural Information Processing Systems 7', </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Perrone, M. P. and Cooper, L. N. </author> <year> (1993), </year> <title> When networks disagree: Ensemble method for neural networks, </title> <editor> in R. J. Mammone, ed., </editor> <title> `Neural Networks for Speech and Image processing', </title> <publisher> Chapman-Hall. </publisher>
Reference: <author> Press, W., Teukolsky, S., Vetterling, W. and Flannery, B. </author> <year> (1992), </year> <title> Numerical Recipes, second edn, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference: <author> Renals, S., Morgan, N., Cohen, M. and Franco, H. </author> <year> (1992), </year> <title> Connectionist probability estimation is the Decipher speech recognition system, </title> <booktitle> in `Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing', </booktitle> <pages> pp. 601604. </pages>
Reference: <author> Ripley, B. </author> <year> (1995), </year> <title> `Statistical ideas for selecting network architectures', Invited Presentation, </title> <booktitle> Neural Information Processing Systems 8. </booktitle>
Reference-contexts: NIC relies on a single well-defined minimum to the fitting function and can be unreliable when there are several local minima <ref> (Ripley, 1995) </ref>. There is very little published computational experience of the NIC, or the GPE. Their evaluation is prohibitively expensive for large networks. VC bounds have been calculated for various network types (Cohn and Tesauro, 1992). Early VC-dimension work handles only the case of discrete outputs.
Reference: <author> Rivlin, T. </author> <year> (1969), </year> <title> An Introduction to the Approximation of Functions, </title> <publisher> Blaisdell Publishing Company, </publisher> <address> Waltham, Mas-sachusetts. </address>
Reference: <author> Robinson, A. </author> <year> (1994), </year> <title> `An application of recurrent nets to phone probability estimation', </title> <journal> IEEE Transactions on Neural Networks 5(2), </journal> <volume> 298305. </volume>
Reference: <author> Saad, D. and Solla, S. </author> <year> (1995), </year> <title> `Exact solution for on-line learning in multilayer neural networks', </title> <journal> Physical Review Letters 74, </journal> <volume> 43374340. </volume>
Reference-contexts: The use of random target networks in this fashion has been referred to as the student teacher problem <ref> (Saad and Solla, 1995) </ref>. Motivated by this work, a very similar technique is used in this paper in order to evaluate the quality of the local minima which are found using backpropagation as a function of various parameters.
Reference: <author> Saad, D. and Solla, S. </author> <year> (1996), </year> <title> Learning from corrupted examples in multilayer networks, </title> <type> Technical Report NCRG/96/019, </type> <institution> Aston University, UK. </institution>
Reference: <author> Sarle, W. </author> <year> (1996), </year> <title> `Comp.ai.neural-nets frequently asked questions', </title> <publisher> ftp://rtfm.mit.edu/pub/usenet/news.- answers/ai-faq/neural-nets. </publisher>
Reference-contexts: Specific rules, such as those mentioned above, are not commonly believed to be accurate <ref> (Sarle, 1996) </ref>. However, the stipulation that the number of parameters must be less than the number of examples is typically believed to be true for common datasets. The results here indicate that this is not always the case. Face Recognition Example This section presents results on real data. <p> A conclusion is that backpropagation can result in the underutilisation of network resources in certain cases (i.e. some parameters may be ineffective or only partially effective due to sub-optimal convergence). Since reporting the work contained in this paper, S. Hanson has stated <ref> (Hassoun, Cherkassky, Hanson, Oja, Sarle and Sudjianto, 1996) </ref>: Whether in the language of approximation theory (overfitting etc.) or statistical estimation (bias vs. variance) it is clear that too many parameters in some nonparametric models can be grievous, however with many Neural Networks, more parameters can actually improve things. and Such [phenomena]
Reference: <author> Sartori, M. A. and Antsaklis, P. </author> <year> (1991), </year> <title> `A simple method to derive bounds on the size and to train multilayer neural networks', </title> <journal> IEEE Transactions on Neural Networks 2, </journal> <volume> 467471. </volume>
Reference: <author> Slomka, S. </author> <year> (1996), </year> <type> `Personal communication'. </type>
Reference: <author> Vapnik, V. </author> <year> (1982), </year> <title> Estimation of Dependencies Based on Empirical Data, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: In practice p (x; d) is not known. Instead, a training set T = fx p ; d p g N p given, where N p is the number of patterns, and an approximation of E true is minimised which is called the empirical error <ref> (Vapnik, 1982) </ref> or training error (Bengio, 1996): E = p=1 The quadratic and relative entropy cost functions are examples of such an error function. A very important question is how well a model trained to minimise E generalises (i.e. how low E true is).
Reference: <author> Vapnik, V. </author> <year> (1995), </year> <title> The Nature of Statistical Learning Theory, </title> <publisher> Springer. </publisher> <address> 34 Werbos, P. </address> <year> (1974), </year> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences, </title> <type> PhD thesis, </type> <institution> Harvard University. </institution>
Reference: <author> Wolpert, D. H. and Macready, W. G. </author> <year> (1995), </year> <title> No free lunch theorems for search, </title> <type> Technical Report SFI-TR-95-02-010, </type> <institution> The Santa Fe Institute. </institution>
Reference-contexts: The No Free Lunch theorem <ref> (Wolpert and Macready, 1995) </ref> shows that, if we do not make any assumptions regarding the target function, no algorithm performs better than any other on average. In other words, we need to make assumptions.
Reference: <author> Yu, X.-H. </author> <year> (1992), </year> <title> `Can backpropagation error surface not have local minima', </title> <journal> IEEE Transactions on Neural Networks 3, </journal> <volume> 10191021. </volume>
Reference-contexts: not used in order to illustrate this case. 2 Local Minima It has been shown that the error surface of a backpropagation network with one hidden layer and t 1 hidden units has no local minima, if the network is trained with an arbitrary set containing t different inputs 1 <ref> (Yu, 1992) </ref>. In practice, however, other features of the error surface such as ravines and plateaus (Baldi and Hornik, 1988) can present difficulty for optimisation. For example, the two error functions shown in figure 1 (from (Gori, 1996)) do not have local minima.
References-found: 60

