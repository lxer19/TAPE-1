URL: http://www.aic.nrl.navy.mil/papers/1995/AIC-95-014.ps.Z
Refering-URL: http://www.aic.nrl.navy.mil/papers/1995/
Root-URL: 
Email: email: gref@aic.nrl.navy.mil  
Title: ROBOT LEARNING WITH PARALLEL GENETIC ALGORITHMS ON NETWORKED COMPUTERS  
Author: John J. Grefenstette 
Keyword: Parallel genetic algorithms, robot learning  
Address: Washington, DC 20375-5337  
Affiliation: Navy Center for Applied Research in AI Naval Research Laboratory  
Abstract: This work explores the use of machine learning methods for extracting knowledge from simulations of complex systems. In particular, we use genetic algorithms to learn rule-based strategies used by autonomous robots. The evaluation of a given strategy may require several executions of a simulation to produce a meaningful estimate of the quality of the strategy. As a consequence, the evaluation of a single individual in the genetic algorithm requires a fairly substantial amount of computation. Such a system suggests the sort of large-grained parallelism that is available on a network of workstations. We describe an implementation of a parallel genetic algorithm, and present case studies of the resulting speedup on two robot learning tasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cohoon, J.P., S.U. Hedge, W.N. Martin and D. </author> <title> Richards (1987). Punctuated equilibria: A parallel genetic algorithm. </title> <booktitle> Proc. Second Intl. Conf. on Genetic Algorithms (pp 148-154), </booktitle> <editor> J. Grefenstette (Ed.), </editor> <address> Hillsdale, NJ: </address> <publisher> Erlbaum Associates. </publisher>
Reference: <editor> Davis, L. D. (Ed.) </editor> <booktitle> (1991). Handbook of Genetic Algorithms. </booktitle> <address> Boston: </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference: <author> Finkel, R. A. </author> <year> (1987). </year> <title> Large-grain paralellism Three case studies. In The Characteristics of Parallel Algorithms. </title> <publisher> L. </publisher>
Reference-contexts: The rest of this section discusses our approach to speeding up the learning process through the exploitation of parallel processing. Master Process The parallel version of SAMUEL uses a master/worker paradigm that is best suited for large-grain parallelism <ref> (Finkel, 1987) </ref>. The master process performs most of the functions associated with the genetic algorithm. The worker processes are responsible for the evaluation of the individual strategies. Pseudo-code for the master and worker process are shown in Figures 3 and 4. <p> parallel algorithm, one definition of speedup is: S (p,N ) = T (1, N) _______ where T (p,N ) is the time required to compute a problem of size N on p processors. 1 Our parallel genetic algorithm is a __________ ________ 1 This definition is also termed rough speedup <ref> (Finkel, 1987) </ref>, since another definition of speedup is to compare the parallel algorithm to the parallel generate-and-test algorithm, in which N possible solutions are generated on each iteration (i.e., N is the popu - lation size).
Reference: <editor> H. Jamieson, D. B. Gannon, R. J. Douglas (eds.), Cam-bridge, </editor> <address> MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Grefenstette, J. J. </author> <year> (1981). </year> <title> Parallel adaptive algorithms for function optimization. </title> <type> Technical Report CS-81-19, </type> <institution> Computer Science Department, Vanderbilt University, Nash-ville, TN. </institution>
Reference: <author> Grefenstette, J. J. </author> <year> (1990). </year> <title> Genetic algorithms and their applications. </title> <booktitle> In The Encyclopedia of Computer Science and Technology 21 (Supplement 6), </booktitle> <editor> Kent, A. and Williams, J. G. (Eds.), </editor> <address> New York: </address> <publisher> Marcel Dekker. </publisher>
Reference-contexts: This paper focuses on running genetic algorithms on networks of loosely-coupled workstations. The next section briefly describes our specific genetic learning system, SAMUEL. SAMUEL SAMUEL is a system that learns strategies, or sets of decision rules, using genetic algorithms <ref> (Grefenstette, Ramsey and Schultz, 1990) </ref>. SAMUEL maintains a population of competing strategies, as shown in Figure 2. Strategies are evaluated by running a simulation of the intended task environment in which the learning agent uses the given strategy to perform a task.
Reference: <author> Grefenstette, J. J. </author> <year> (1991). </year> <title> Lamarckian learning in multi-agent environments. </title> <booktitle> Proceedings of the Fourth International Conference of Genetic Algorithms (pp 303-310). </booktitle> <address> San Diego, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The worker process evaluates a strategy using a three phase process, as shown in Figure 4. First, the given strategy is tested by executing E episodes of problem solving in the simulated environment. Second, Lamarckian learning operators are applied to augment the strategy's rule set based on its experiences <ref> (Grefenstette, 1991) </ref>. For example, suppose that a general rule (i.e., a rule that matches many situations) with a history of mediocre performance happens to fire during a high performance episode.
Reference: <author> Grefenstette, J. J. </author> <year> (1992). </year> <title> The evolution of strategies for multi-agent environments. </title> <booktitle> Adaptive Behavior 1(1), </booktitle> <pages> 65-90. </pages>
Reference-contexts: Our approach uses genetic algorithms to explore the space of alternative strategies for intelligent autonomous agents in a simulated environment. The learned strategies can then be validated on an operational system. We have also developed an approach <ref> (Grefenstette and Ramsey, 1992) </ref> that permits the learning system to modify the simulation based on its experience with the operational environment. We are currently testing these methods in design of intelligent autonomous mobile robots (Grefenstette, 1994; Schultz, 1994). <p> The tracker does not initially have any information concerning the relationship between its behavior and its probability of detection by the target. In particular, the tracker does not know the detection radius or the range of speeds that the target might assume. For further details, see <ref> (Grefenstette and Ramsey, 1992) </ref>. The task is broken down into individual tracking episodes. Each episode begins with a random placement of the two robots, and lasts for up to 30 time steps, or until the tracker is detected.
Reference: <author> Grefenstette, J. J. </author> <year> (1993). </year> <title> Introduction to the special track on genetic algorithms. </title> <booktitle> IEEE Expert, </booktitle> <publisher> IEEE Press, </publisher> <month> October, </month> <year> 1993. </year>
Reference: <author> Grefenstette, J. J. </author> <year> (1994). </year> <title> Evolutionary algorithms in robot - ics, </title> <booktitle> Robotics and Manufacturing: Recent Trends in Research, Education and Applications, v5. Proc. Fifth International Symposium on Robotics and Manufacturing, </booktitle> <volume> ISRAM 94, </volume> <editor> Jamshedi, M. and Nguyen, C. (Eds.), </editor> <address> 65-72, </address> <publisher> ASME Press: </publisher> <address> New York. </address>
Reference: <author> Grefenstette, J. J. and C. L. </author> <title> Ramsey (1992). An Approach to Anytime Learning. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> (pp 189-195), </pages> <address> D. </address>
Reference-contexts: Our approach uses genetic algorithms to explore the space of alternative strategies for intelligent autonomous agents in a simulated environment. The learned strategies can then be validated on an operational system. We have also developed an approach <ref> (Grefenstette and Ramsey, 1992) </ref> that permits the learning system to modify the simulation based on its experience with the operational environment. We are currently testing these methods in design of intelligent autonomous mobile robots (Grefenstette, 1994; Schultz, 1994). <p> The tracker does not initially have any information concerning the relationship between its behavior and its probability of detection by the target. In particular, the tracker does not know the detection radius or the range of speeds that the target might assume. For further details, see <ref> (Grefenstette and Ramsey, 1992) </ref>. The task is broken down into individual tracking episodes. Each episode begins with a random placement of the two robots, and lasts for up to 30 time steps, or until the tracker is detected.
Reference: <editor> Sleeman and P. Edwards (Eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Grefenstette, J. J., C. L. Ramsey and A. C. </author> <title> Schultz (1990). Learning sequential decision rules using simulation models and competition. </title> <booktitle> Machine Learning 5(4), </booktitle> <pages> 355-381. </pages>
Reference-contexts: This paper focuses on running genetic algorithms on networks of loosely-coupled workstations. The next section briefly describes our specific genetic learning system, SAMUEL. SAMUEL SAMUEL is a system that learns strategies, or sets of decision rules, using genetic algorithms <ref> (Grefenstette, Ramsey and Schultz, 1990) </ref>. SAMUEL maintains a population of competing strategies, as shown in Figure 2. Strategies are evaluated by running a simulation of the intended task environment in which the learning agent uses the given strategy to perform a task.
Reference: <author> Muehlenbein, H., M. Schomisch, and J. </author> <title> Born (1991). The parallel genetic algorithm as function optimizer. </title> <journal> Parallel Computing, </journal> <volume> 16, </volume> <year> 1991. </year>
Reference: <author> Pettey, C. B., M. R. Leuze, J. J. </author> <title> Grefenstette (1987). A parallel genetic algorithm. </title> <booktitle> Proc. Second Intl. Conf. on Genetic Algorithms (pp 155-161), </booktitle> <editor> J. Grefenstette (Ed.), </editor> <address> Hillsdale, NJ: </address> <publisher> Erlbaum Associates. </publisher>
Reference: <author> Schultz, A. C. </author> <year> (1994). </year> <title> Learning robot behaviors using genetic algorithms. </title> <booktitle> Intelligent Automation and Soft Computing: Trends in Research, Development, </booktitle> <editor> and Applica - tions, Jamshedi, M. and Nguyen, C. (Eds.), </editor> <address> 607-612, </address> <publisher> TSI Press: </publisher> <address> Albuquerque. </address>
Reference: <author> Schultz, A. C. and Grefenstette, J. J. </author> <year> (1992). </year> <title> Using a genetic algorithm to learn behaviors for autonomous vehicles. </title> <booktitle> Proc. American Institute of Aeronautics and Astronautics Guidance, Navigation and Control Conference, Hilton Head, SC: AIAA, </booktitle> <pages> 739-749. </pages>
Reference: <author> Tanese, R. </author> <year> (1987). </year> <title> Parallel genetic algorithms for a hypercube. </title> <booktitle> Proc. Second Intl. Conf. on Genetic Algorithms (pp 177-183), </booktitle> <editor> J. Grefenstette (Ed.), </editor> <address> Hillsdale, NJ: </address> <publisher> Erlbaum Associates. </publisher>
Reference: <author> Whitley, D. and T. </author> <title> Starkweather (1990). Genitor II: A distributed genetic algorithm. </title> <journal> J. Exp. Theor. Artif. Intell., </journal> <volume> 2, </volume> <pages> 189-214. </pages>
References-found: 19

