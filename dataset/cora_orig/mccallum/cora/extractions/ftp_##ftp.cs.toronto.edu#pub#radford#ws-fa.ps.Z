URL: ftp://ftp.cs.toronto.edu/pub/radford/ws-fa.ps.Z
Refering-URL: http://www.cs.toronto.edu/~radford/ws-fa.abstract.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: radford@stat.utoronto.ca  dayan@ai.mit.edu  
Title: Factor Analysis Using Delta-Rule Wake-Sleep Learning  
Author: Radford M. Neal Peter Dayan 
Date: 24 July 1996  
Address: Toronto  
Affiliation: Department of Statistics and Department of Computer Science University of  Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  
Abstract: Technical Report No. 9607, Department of Statistics, University of Toronto We describe a linear network that models correlations between real-valued visible variables using one or more real-valued hidden variables a factor analysis model. This model can be seen as a linear version of the Helmholtz machine, and its parameters can be learned using the wake-sleep method, in which learning of the primary generative model is assisted by a recognition model, whose role is to fill in the values of hidden variables based on the values of visible variables. The generative and recognition models are jointly learned in wake and sleep phases, using just the delta rule. This learning procedure is comparable in simplicity to Oja's version of Hebbian learning, which produces a somewhat different representation of correlations in terms of principal components. We argue that the simplicity of wake-sleep learning makes factor analysis a plau sible alternative to Hebbian learning as a model of activity-dependent cortical plasticity.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barlow, H. B. </author> <title> (1989) Unsupervised learning, </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 295-311. </pages>
Reference-contexts: Note, however, that it implicitly assumes that all information is equally important. Maximizing information transfer seems less compelling as a goal for subsequent levels of processing, once sensory signals have reached cortex. Several other computational goals have been suggested from this stage upwards, including factorial coding <ref> (Barlow 1989) </ref>, sparsification (Olshausen and Field 1995), and various methods for encouraging the cortex to respect reasonable invari-ances, such as translation or scale invariance for visual processing (Li and Atick 1994). <p> This amounts to forcing the factors to be independent, or factorial, which has itself long been suggested as a goal for early cortical processing <ref> (Barlow 1989) </ref>, and has generally been assumed in non-linear versions of the Helmholtz machine.
Reference: <author> Baudry, M. and Davis, J. L. </author> <title> (1994) Long-term Potentiation: A Debate of Current Issues, </title> <booktitle> Volume 2 Cambridge, </booktitle> <address> Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference: <author> Chou, K. C.,Willsky, A. S. and Benveniste, A. </author> <title> (1994) Multiscale recursive estimation, data fusion, and regularization, </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 39, </volume> <pages> 464-478. </pages>
Reference: <author> Chou, K. C., Willsky, A. S. and Nikoukhah, R. </author> <title> (1994) Multiscale systems, Kalman filters, and Riccati equations, </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 39, </volume> <pages> 479-492. </pages>
Reference: <author> Collingridge, G. L. and Bliss, T. V. </author> <year> (1987) </year> <month> NMDA receptors: </month> <title> Their role in long-term potentiation, </title> <booktitle> Trends in Neurosciences, </booktitle> <volume> 10, </volume> <pages> 288-293. </pages>
Reference: <author> Dayan, P. and Hinton, G. E. </author> <title> (1996) Varieties of Helmholtz Machine, Neural Networks, </title> <publisher> in press. </publisher>
Reference-contexts: An algorithm that correctly performs stochastic gradient descent in the recognition parameters using a correct cost function does exist <ref> (Dayan and Hinton 1996) </ref>, but unfortunately, it involves reinforcement learning methods for which convergence is usually extremely slow. We have nevertheless derived two convergence results.
Reference: <author> Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, </author> <title> R.S. (1995) The Helmholtz machine, </title> <journal> Neural Computation, </journal> <volume> 7, </volume> <pages> 229-904. </pages>
Reference-contexts: One possible extension is to use a mixture of factor analysers (perhaps implemented using an EM-style algorithm). This has been applied successfully in the domain of character recognition <ref> (Hinton, Revow, and Dayan 1995) </ref>, although mixtures of fac 18 tor analysis models did not perform much better than mixtures of principal component mod-els.
Reference: <author> Dempster, A. P., Laird, N. M, and Rubin, D. B. </author> <title> (1977) Maximum likelihood from incomplete data via the EM algorithm, </title> <journal> Proceedings of the Royal Statistical Society, </journal> <volume> B 39, </volume> <pages> 1-38. </pages>
Reference-contexts: In the next section, we describe the factor analysis model. Section 3 explains the wake-sleep algorithm and its roots in the expectation-maximisation (EM) algorithm <ref> (Dempster, Laird, and Rubin 1977) </ref>. Section 4 presents experimental evidence that factor analysis models can indeed be learned by the wake-sleep method, though there appear to be some situations in which the algorithm does not operate perfectly.
Reference: <author> Everitt, B. S. </author> <title> (1984) An Introduction to Latent Variable Models, </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> F oldiak, P. </author> <title> (1989) Adaptive network for optimal linear feature extraction, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Washington, DC, I, </address> <month> 401-405. </month> <title> Grenander, U, (1976-1981) Lectures in Pattern Theory I, II and III: Pattern Analysis, Pattern Synthesis and Regular Structures, </title> <publisher> Berlin: Springer-Verlag. 21 Hasselmo, </publisher> <editor> M. E. and Bower, J. M. </editor> <booktitle> (1993) Acetylcholine and memory, Trends in Neurosciences, </booktitle> <volume> 16, </volume> <pages> 218-222. </pages>
Reference: <author> Hinton, G. E., Dayan, P., Frey, B., and Neal, R. M. </author> <title> (1995) The wake-sleep algorithm for self-organizing neural networks, </title> <journal> Science, </journal> <volume> 268, </volume> <pages> 1158-1160. </pages>
Reference-contexts: For--mally, however, the learning that occurs during sleep corresponds to minimizing a Kullback-Leibler divergence in which the two distributions appear in the wrong order, and in which the distribution over visible variables is that produced by the generative model rather than the external world <ref> (Hinton et al. 1995) </ref>. An algorithm that correctly performs stochastic gradient descent in the recognition parameters using a correct cost function does exist (Dayan and Hinton 1996), but unfortunately, it involves reinforcement learning methods for which convergence is usually extremely slow. We have nevertheless derived two convergence results. <p> One possible extension is to use a mixture of factor analysers (perhaps implemented using an EM-style algorithm). This has been applied successfully in the domain of character recognition <ref> (Hinton, Revow, and Dayan 1995) </ref>, although mixtures of fac 18 tor analysis models did not perform much better than mixtures of principal component mod-els. <p> Extracting more than second order structure requires using non-linearities somewhere in the system. The price to be paid is that there are no guarantees that the recognition distribution will be tractable, and approximations will typically have to be made. In this realm, the Helmholtz machine <ref> (Hinton et al. 1995) </ref>, mean field methods for Bayesian networks (Saul et al. 1995; Jaakkola et al. 1995), and the sparse coding proposal of Olshausen and Field (1995) show promise, though their worth has yet to be proven empirically. 5.3 Implications for activity-dependent plasticity A main motivation for the original development
Reference: <author> Hinton, G. E., Revow, M. and Dayan, P. </author> <title> (1995) Recognizing handwritten digits using mixtures of linear models, </title> <editor> in G. Tesauro, D. S. Touretzky, and T. K. Leen (editors), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 7, </volume> <pages> 1015-1022. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: For--mally, however, the learning that occurs during sleep corresponds to minimizing a Kullback-Leibler divergence in which the two distributions appear in the wrong order, and in which the distribution over visible variables is that produced by the generative model rather than the external world <ref> (Hinton et al. 1995) </ref>. An algorithm that correctly performs stochastic gradient descent in the recognition parameters using a correct cost function does exist (Dayan and Hinton 1996), but unfortunately, it involves reinforcement learning methods for which convergence is usually extremely slow. We have nevertheless derived two convergence results. <p> One possible extension is to use a mixture of factor analysers (perhaps implemented using an EM-style algorithm). This has been applied successfully in the domain of character recognition <ref> (Hinton, Revow, and Dayan 1995) </ref>, although mixtures of fac 18 tor analysis models did not perform much better than mixtures of principal component mod-els. <p> Extracting more than second order structure requires using non-linearities somewhere in the system. The price to be paid is that there are no guarantees that the recognition distribution will be tractable, and approximations will typically have to be made. In this realm, the Helmholtz machine <ref> (Hinton et al. 1995) </ref>, mean field methods for Bayesian networks (Saul et al. 1995; Jaakkola et al. 1995), and the sparse coding proposal of Olshausen and Field (1995) show promise, though their worth has yet to be proven empirically. 5.3 Implications for activity-dependent plasticity A main motivation for the original development
Reference: <author> Hinton, G. E. and Sejnowski, T. J. </author> <title> (1986) Learning and relearning in Boltzmann machines, </title> <editor> in D. E. Rumelhart, J. L. McClelland, </editor> <booktitle> and the PDP research group, Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, </booktitle> <address> Cambridge, Massachusetts: </address> <publisher> MIT Press, </publisher> <pages> 282-317. </pages>
Reference-contexts: Such balanced operation of the two phases is not essential for convergence to the correct solution, however. This contrasts with the wake and sleep phases of learning in Boltzmann Machines <ref> (Hinton and Sejnowski 1986) </ref>, in which the two phases must be exactly balanced for the learning to follow an appropriate gradient. 3.4 Wake-sleep learning for multiple-factor models A Helmholtz machine with more than one hidden factor can be trained using the wake-sleep algorithm in much the same way as described above
Reference: <author> Hinton, G. E. and Zemel, R. S. </author> <year> (1994) </year> <month> Autoencoders, </month> <title> minimum description length and Helmholtz free energy, </title> <editor> in J. D. Cowan, G. Tesauro and J. Alspector (editors), </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 3-10. </pages>
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <title> (1991) Adaptive mixtures of local experts, </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 79-87. </pages>
Reference-contexts: One could also augment such a mixture model by allowing the mixing proportions to vary under the control of a gating network <ref> (Jacobs, Jordan, Nowlan, and Hinton 1991) </ref>. Another possibility is to build a hierarchical model (Ghahramani and Hinton, personal communication; Rao and Ballard 1995).
Reference: <author> Jolliffe, I. T. </author> <title> (1986) Principal Component Analysis, </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Although there are some special circumstances in which factor analysis is equivalent to principal component analysis, the techniques are in general quite different <ref> (Jolliffe 1986) </ref>. Loosely speaking, principal component analysis pays attention to both variance and covari-ance, whereas factor analysis looks only at covariance.
Reference: <author> J oreskog, K. G. </author> <title> (1967) Some contributions to maximum likelihood factor analysis, </title> <journal> Psychome-trika, </journal> <volume> 32, </volume> <pages> 443-482. </pages>
Reference-contexts: In the 1960's, computationally feasible algorithms were developed for performing factor analysis by the statistically attractive method of maximum likelihood <ref> (J oreskog 1967, 1969, 1977) </ref>. In maximum likelihood learning, the parameters of the model are chosen so as to maximize the probability density assigned by the model to the data that were observed (the likelihood).
Reference: <author> J oreskog, K. G. </author> <title> (1969) A general approach to confirmatory maximum likelihood factor analysis, </title> <journal> Psychometrika, </journal> <volume> 34, </volume> <pages> 183-202. </pages>
Reference: <author> J oreskog, K. G. </author> <title> (1977) Factor analysis by least-squares and maximum-likelihood methods, </title> <editor> in K. Enslein, A. Ralston, and H. S. Wilf (editors), </editor> <title> Statistical Methods for Digital Computers (volume 3 of Mathematical Methods for Digital Computers), </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Krim, H., Willsky, A. S. and Karl, W.C. </author> <title> (1994) Multiresolution models for random fields and their use in statistical image processing, </title> <booktitle> Proceedings of 1994 Workshop on Information Theory and Statistics, </booktitle> <address> New York: </address> <publisher> IEEE, </publisher> <pages> 56. </pages>
Reference: <author> Li, Z. and Atick, J. J. </author> <title> (1994) Toward a theory of the striate cortex, </title> <journal> Neural Computation, </journal> <volume> 6, </volume> <pages> 127-146. </pages>
Reference-contexts: Several other computational goals have been suggested from this stage upwards, including factorial coding (Barlow 1989), sparsification (Olshausen and Field 1995), and various methods for encouraging the cortex to respect reasonable invari-ances, such as translation or scale invariance for visual processing <ref> (Li and Atick 1994) </ref>.
Reference: <author> Linsker, R. </author> <title> (1986) From basic network principles to neural architecture, </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 83, </volume> <pages> 7508-7512; 8390-8394; 8779-9783. </pages>
Reference: <author> Linsker, R. </author> <title> (1988) Self-organization in a perceptual network, </title> <journal> Computer, </journal> <volume> 21, </volume> <pages> 105-128. </pages>
Reference: <author> Luettgen, M. R. and Willsky, </author> <title> A.S. (1995) Likelihood calculation for a class of multiscale stochastic models, with application to texture discrimination, </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 4, </volume> <pages> 194-207. </pages>
Reference: <author> Miller, K. D. </author> <title> (1994) A model for the development of simple cell receptive fields and the ordered arrangement of orientation columns through activity-dependent competition between ON- and OFF-center inputs, </title> <journal> Journal of Neuroscience, </journal> <volume> 14, </volume> <pages> 409-441. </pages> <note> 22 Miller, </note> <author> K. D., Keller, J. B. and Stryker, M. P. </author> <title> (1989) Ocular dominance column development: Analysis and simulation, </title> <journal> Science, </journal> <volume> 245, </volume> <pages> pp 605-615. </pages>
Reference: <author> Montague, P. R. and Sejnowski, T. J. </author> <title> (1994) The predictive brain: Temporal coincidence and temporal order in synaptic learning mechanisms, </title> <booktitle> Learning and Memory, </booktitle> <volume> 1, </volume> <pages> 1-33. </pages>
Reference-contexts: Rules equivalent to the delta rule are conventional in classical conditioning (Rescorla and Wagner 1972; Sutton and Barto, 1981) and have also been suggested as underlying cortical plasticity <ref> (Montague and Sejnowski 1994) </ref>. Of course, the wake-sleep learning rule requires two phases of activation, with different connections being primarily responsible for driving the cells in each phase. Although there is some suggestive evidence of this (Hasselmo and Bower 1993), further work on the implementational microcircuitry is clearly required.
Reference: <author> Mumford, D. </author> <title> (1994) Neuronal architectures for pattern-theoretic problems, </title> <editor> in C. Koch and J. Davis (editors), </editor> <title> Large-Scale Theories of the Cortex, </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <pages> 125-152. </pages>
Reference: <author> Olshausen, B. A. and Field, D. J. </author> <title> (1996) Sparse coding of natural images produces localized, oriented, bandpass receptive fields, Nature, </title> <publisher> in press. </publisher>
Reference: <author> Oja, E. </author> <title> (1989) Neural networks, principal components, and subspaces, </title> <journal> International Journal of Neural Systems, </journal> <volume> 1, </volume> <pages> 61-8. </pages>
Reference: <author> Oja, E. </author> <title> (1992) Principal components, minor components, and linear neural networks, </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 927-35. </pages>
Reference: <author> Oja, E. and Karhunen, J. </author> <title> (1985) On stochastic approximation of the eigenvectors and eigen-values of the expectation of a random matrix, </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 106, </volume> <pages> 69-84. </pages>
Reference: <author> Plumbley, </author> <title> M.D. (1993) Efficient information transfer and anti-Hebbian neural networks, </title> <booktitle> Neural Networks, </booktitle> <volume> 6, </volume> <pages> 823-833. </pages>
Reference-contexts: Under the simplifying assumption that the distribution of the inputs is Gaussian, setting the output of a unit to the projection of its input onto the first principal component of the input covariance matrix conveys as much information as possible on average <ref> (see also Plumbley 1993) </ref>. This goal seems reasonable for the very early stages of sensory processing, where information bottlenecks such as the optic nerve may plausibly be present. Note, however, that it implicitly assumes that all information is equally important.
Reference: <author> Rao, P. N. R. and Ballard, D. H. </author> <title> (1995) Dynamic model of visual memory predicts neural response properties in the visual cortex, </title> <type> Technical report 95.4, </type> <institution> Department of Computer Science, Rochester, </institution> <address> New York. </address>
Reference: <author> Rescorla, R. A. and Wagner, </author> <title> A.R. (1972) A theory of Pavlovian conditioning: The effectiveness of reinforcement and non-reinforcement, </title> <editor> in A. H. Black and W. F. Prokasy (editors), </editor> <title> Classical Conditioning II: </title> <booktitle> Current Research and Theory, </booktitle> <pages> pp 64-69. </pages> <address> New York: Appleton-Century-Crofts. </address>
Reference: <author> Rubin, D. B. and Thayer, D. T. </author> <title> (1982) EM algorithms for ML factor analysis, </title> <journal> Psychometrika, </journal> <volume> 47, </volume> <pages> 69-76. </pages>
Reference-contexts: Just as it is usually computationally more efficient to implement principal components analysis using a standard matrix technique such as singular-value decomposition rather than by using Hebbian learning, factor analysis is probably better implemented on a computer using either EM <ref> (Rubin and Thayer 1982) </ref> or the second order Newton methods of J oreskog (1967, 1969, 1977) than by the wake-sleep algorithm.
Reference: <author> Sanger, T. D. </author> <title> (1989) Optimal unsupervised learning in a single-layer linear feedforward neural network, </title> <booktitle> Neural Networks, </booktitle> <volume> 2, </volume> <pages> 459-473. </pages> <editor> von der Malsburg, C. </editor> <title> (1973) Self-organization of orientation sensitive cells in the striate cortex, </title> <journal> Kybernetic, </journal> <volume> 14, </volume> <pages> 85-100. </pages> <editor> von der Malsburg, C. and Willshaw D. J. </editor> <title> (1977) How to label nerve cells so that they can interconnect in an ordered fashion, </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 74, </volume> <pages> 5176-5178. </pages>
Reference: <author> Willshaw, D. J. and von der Malsburg, C. </author> <title> (1976) How patterned neural connections can be set up by self-organisation, </title> <journal> Proceedings of the Royal Society of London B, </journal> <volume> 194, </volume> <pages> 431-445. </pages>
Reference: <author> Willshaw, D. J. and von der Malsburg, C. </author> <title> (1979) A marker induction mechanism for the establishment of ordered neural mappings: its application to the retinotectal problem, </title> <journal> Philosophical Transactions of the Royal Society B, </journal> <volume> 287, </volume> <pages> pp 203-243. </pages>
Reference: <author> Wyatt, J. L., Jr and Elfadel, I. M. </author> <title> (1985) Time-domain solutions of Oja's equations, </title> <journal> Neural Computation, </journal> <volume> 7, </volume> <pages> 915-22. 23 </pages>
References-found: 39

