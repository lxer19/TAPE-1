URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/thrun.KONNAI.planning.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/full.html
Root-URL: http://www.cs.cmu.edu
Email: e-mail: st@gmdzi.uucp, al@gmdzi.uucp  
Title: ADAPTIVE LOOK-AHEAD PLANNING problem of finding good initial plans is solved by the use of
Author: Sebastian Thrun yz Knut M"oller Alexander Linden 
Note: The  
Address: D-5205 St. Augustin, Postfach 1240, F.R.G.  D-5300 Bonn, R"omerstr. 164, F.R.G.  
Affiliation: German National Research Center for Computer Science  University of Bonn Department of Computer Science  
Abstract: We present a new adaptive connectionist planning method. By interaction with an environment a world model is progressively constructed using the backpropagation learning algorithm. The planner constructs a look-ahead plan by iteratively using this model to predict future reinforcements. Future reinforcement is maximized to derive suboptimal plans, thus determining good actions directly from the knowledge of the model network (strategic level). This is done by gradient descent in action space. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. W. Anderson. </author> <title> Learning and problem solving with multilayer connectionist systems. </title> <type> Technical Report COINS TR 86-50, </type> <institution> Dept. of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1986. </year>
Reference-contexts: Planning concerns the synthesization of a sequence of actions to achieve a specific goal. Connectionist approaches so far rely on an associative mapping, which selects good actions given environmental state descriptions. These are based on the interaction of a world model and an action generating network <ref> [1, 2, 6, 10, 11, 14, 15, 20] </ref>. We recognize three major problems with these approaches: 1. Since no explicit consideration of the future is made, future effects of actions must be directly encoded into this world model mapping, thus model learning becomes complicated. 2. <p> Since the reinforcement is an unstructured overall signal, the problem with reinforcement learning tasks is the assignment of particular reinforcements to particular actions in the past. This problem is called the temporal credit assignment problem. Many approaches use a network which solves this problem directly <ref> [1, 11, 17] </ref>. This network, the controller network, learns to generate actions that optimize the whole future reinforcement. Obviously the quality of actions depends strongly on future actions. Thus if the controller generates an action, future actions are implicitly contained in this decision. <p> This implies the assumption that the effect of a certain action to the reinforcement occurs in the next N time steps similar assumptions are also made in <ref> [1, 17] </ref>. N can be an arbitrary number the computational costs of the optimization steps are linear in N . Moreover, N can be determined at planning time dynamically. The World and the World Model Planning is a hypothetical process, in which future states and future actions are involved. <p> The training of this world model is a system identification task. In this paper, we use a multilayer differentiable, non-recurrent connectionist network for modeling the world (c.f. figure 1). This network is trained by backpropagation to predict the behavior of the world <ref> [1, 2, 6, 10, 14, 20] </ref>. Formally, the world considered in this paper is defined as a mapping, which maps an action vector ~a (t) with a current state ~s (t) to a subsequent state ~s (t+1) and reinforcement ~r (t+1). Before training, the mapping of the world is unknown. <p> This network is trained in a supervised manner (e.g. with backpropagation) to compute the resulting action of the training procedure from the current state. The experience network is similar to the control network described in <ref> [1, 17] </ref>, but it is used in a different way. Its output is optimized by our planning procedure before it is given to the world. The advantage of using an experience network is that the time-consuming planning procedure is shifted gradually to the experience in the experience network. <p> On control problems usually many solutions exist for achieving a certain goal. E.g. inverse kinematics are often characterized by infinitely many solutions with different properties. Many connectionist approaches reduce this one-to-many mapping to a one-to-one mapping <ref> [1, 17, 11, 2, 6, 10, 11] </ref>, since the action generator, the control network, is a mathematical function. Therefore often a marginal constraint like smoothness etc. is also optimized. The planning procedure, as it is presented above, is able to perform one-to-many mappings as well.
Reference: [2] <author> A. G. Barto. </author> <title> Connectionist learning for control: An overview. </title> <type> Technical Report COINS TR 89-89, </type> <institution> Dept. of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: Planning concerns the synthesization of a sequence of actions to achieve a specific goal. Connectionist approaches so far rely on an associative mapping, which selects good actions given environmental state descriptions. These are based on the interaction of a world model and an action generating network <ref> [1, 2, 6, 10, 11, 14, 15, 20] </ref>. We recognize three major problems with these approaches: 1. Since no explicit consideration of the future is made, future effects of actions must be directly encoded into this world model mapping, thus model learning becomes complicated. 2. <p> While the model network is an essential part of the training of the action network, the learning of the latter lags behind that of the former. In this paper we present a new connectionist planning procedure. Our model network learns one-step predictions by observing the environmental mapping <ref> [2, 6, 11] </ref>. In this way training information is immediately available and model learning is easier and faster. With such a model network a look-ahead plan is constructed and subsequently optimized. We demonstrate the performance of this planning procedure through simulations on a target tracking problem. <p> The training of this world model is a system identification task. In this paper, we use a multilayer differentiable, non-recurrent connectionist network for modeling the world (c.f. figure 1). This network is trained by backpropagation to predict the behavior of the world <ref> [1, 2, 6, 10, 14, 20] </ref>. Formally, the world considered in this paper is defined as a mapping, which maps an action vector ~a (t) with a current state ~s (t) to a subsequent state ~s (t+1) and reinforcement ~r (t+1). Before training, the mapping of the world is unknown. <p> On control problems usually many solutions exist for achieving a certain goal. E.g. inverse kinematics are often characterized by infinitely many solutions with different properties. Many connectionist approaches reduce this one-to-many mapping to a one-to-one mapping <ref> [1, 17, 11, 2, 6, 10, 11] </ref>, since the action generator, the control network, is a mathematical function. Therefore often a marginal constraint like smoothness etc. is also optimized. The planning procedure, as it is presented above, is able to perform one-to-many mappings as well.
Reference: [3] <author> J. L. Elman. </author> <title> Finding structure in time. </title> <type> Technical Report CRL Technical Report 8801, </type> <institution> Center for Research in Language, University of California, </institution> <address> San Diego, </address> <year> 1988. </year>
Reference-contexts: The Feed-Forward Algorithm for Gradient Search in Action Space As mentioned above the environment is modeled by a non-recurrent multilayer backpropagation network. This restriction is sufficient for our simulation results the extension of the algorithm to recurrent networks <ref> [3, 4, 5, 9, 13, 14, 15, 21] </ref> is straightforward and shown in [18]. The external input of the world model network is a state vector ~s (t) and an action vector ~a (t).
Reference: [4] <author> M. Gherrity. </author> <title> A learning algorithm for analog, fully recurrent neural networks. </title> <booktitle> In Proceedings of the First International Joint Conference on Neural Networks, </booktitle> <address> Washington, DC, San Diego, </address> <year> 1989. </year> <journal> IEEE, IEEE TAB Neural Network Committee. </journal>
Reference-contexts: The Feed-Forward Algorithm for Gradient Search in Action Space As mentioned above the environment is modeled by a non-recurrent multilayer backpropagation network. This restriction is sufficient for our simulation results the extension of the algorithm to recurrent networks <ref> [3, 4, 5, 9, 13, 14, 15, 21] </ref> is straightforward and shown in [18]. The external input of the world model network is a state vector ~s (t) and an action vector ~a (t).
Reference: [5] <author> M. I. Jordan. </author> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the Conference on Cognitive Science, </booktitle> <year> 1986. </year>
Reference-contexts: The Feed-Forward Algorithm for Gradient Search in Action Space As mentioned above the environment is modeled by a non-recurrent multilayer backpropagation network. This restriction is sufficient for our simulation results the extension of the algorithm to recurrent networks <ref> [3, 4, 5, 9, 13, 14, 15, 21] </ref> is straightforward and shown in [18]. The external input of the world model network is a state vector ~s (t) and an action vector ~a (t).
Reference: [6] <author> M. I. Jordan. </author> <title> Generic constraints on unspecified target constraints. </title> <booktitle> In Proceedings of the First International Joint Conference on Neural Networks, </booktitle> <address> Washington, DC, San Diego, </address> <year> 1989. </year> <journal> IEEE, IEEE TAB Neural Network Committee. </journal>
Reference-contexts: Planning concerns the synthesization of a sequence of actions to achieve a specific goal. Connectionist approaches so far rely on an associative mapping, which selects good actions given environmental state descriptions. These are based on the interaction of a world model and an action generating network <ref> [1, 2, 6, 10, 11, 14, 15, 20] </ref>. We recognize three major problems with these approaches: 1. Since no explicit consideration of the future is made, future effects of actions must be directly encoded into this world model mapping, thus model learning becomes complicated. 2. <p> While the model network is an essential part of the training of the action network, the learning of the latter lags behind that of the former. In this paper we present a new connectionist planning procedure. Our model network learns one-step predictions by observing the environmental mapping <ref> [2, 6, 11] </ref>. In this way training information is immediately available and model learning is easier and faster. With such a model network a look-ahead plan is constructed and subsequently optimized. We demonstrate the performance of this planning procedure through simulations on a target tracking problem. <p> The training of this world model is a system identification task. In this paper, we use a multilayer differentiable, non-recurrent connectionist network for modeling the world (c.f. figure 1). This network is trained by backpropagation to predict the behavior of the world <ref> [1, 2, 6, 10, 14, 20] </ref>. Formally, the world considered in this paper is defined as a mapping, which maps an action vector ~a (t) with a current state ~s (t) to a subsequent state ~s (t+1) and reinforcement ~r (t+1). Before training, the mapping of the world is unknown. <p> On control problems usually many solutions exist for achieving a certain goal. E.g. inverse kinematics are often characterized by infinitely many solutions with different properties. Many connectionist approaches reduce this one-to-many mapping to a one-to-one mapping <ref> [1, 17, 11, 2, 6, 10, 11] </ref>, since the action generator, the control network, is a mathematical function. Therefore often a marginal constraint like smoothness etc. is also optimized. The planning procedure, as it is presented above, is able to perform one-to-many mappings as well.
Reference: [7] <author> J. Kindermann and A. Linden. </author> <title> Inversion of neural nets. </title> <journal> Journal of Parallel Computing, </journal> <note> 1990. (to appear). </note>
Reference-contexts: Thus we obtain a prediction for all reinforcements ~r pred (t+1); ~r pred (t+2); :::; ~r pred (t+N ) in this look-ahead window. For improving the actions with respect to the predicted reinforcement we use a gradient descent algorithm in action space <ref> [7, 8, 19] </ref>, which will be derived in detail in the next section. It computes the gradients of the reinforcement with respect to the plan, which give us the information, how to change the plan in small steps in order to improve its performance.
Reference: [8] <author> A. Linden and J. Kindermann. </author> <title> Inversion of multilayer nets. </title> <booktitle> In Proceedings of the First International Joint Conference on Neural Networks, </booktitle> <address> Washington, DC, San Diego, 1989. </address> <publisher> IEEE. </publisher>
Reference-contexts: Thus we obtain a prediction for all reinforcements ~r pred (t+1); ~r pred (t+2); :::; ~r pred (t+N ) in this look-ahead window. For improving the actions with respect to the predicted reinforcement we use a gradient descent algorithm in action space <ref> [7, 8, 19] </ref>, which will be derived in detail in the next section. It computes the gradients of the reinforcement with respect to the plan, which give us the information, how to change the plan in small steps in order to improve its performance.
Reference: [9] <author> M. C. Mozer. </author> <title> A focused backpropagation algorithm for temporal pattern recognition. </title> <type> Technical Report CRG-TR-88-3, </type> <institution> Depts. of Psychology and Computer Science, University of Toronto, Toronto, </institution> <month> Jun </month> <year> 1988. </year>
Reference-contexts: The Feed-Forward Algorithm for Gradient Search in Action Space As mentioned above the environment is modeled by a non-recurrent multilayer backpropagation network. This restriction is sufficient for our simulation results the extension of the algorithm to recurrent networks <ref> [3, 4, 5, 9, 13, 14, 15, 21] </ref> is straightforward and shown in [18]. The external input of the world model network is a state vector ~s (t) and an action vector ~a (t).
Reference: [10] <author> P. Munro. </author> <title> A dual backpropagation scheme for scalar-reward learning. </title> <booktitle> In Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 165-176, </pages> <address> Hillsdale, NJ, 1987. </address> <publisher> Cognitive Science Society, Lawrence Erlbaum. </publisher>
Reference-contexts: Planning concerns the synthesization of a sequence of actions to achieve a specific goal. Connectionist approaches so far rely on an associative mapping, which selects good actions given environmental state descriptions. These are based on the interaction of a world model and an action generating network <ref> [1, 2, 6, 10, 11, 14, 15, 20] </ref>. We recognize three major problems with these approaches: 1. Since no explicit consideration of the future is made, future effects of actions must be directly encoded into this world model mapping, thus model learning becomes complicated. 2. <p> The training of this world model is a system identification task. In this paper, we use a multilayer differentiable, non-recurrent connectionist network for modeling the world (c.f. figure 1). This network is trained by backpropagation to predict the behavior of the world <ref> [1, 2, 6, 10, 14, 20] </ref>. Formally, the world considered in this paper is defined as a mapping, which maps an action vector ~a (t) with a current state ~s (t) to a subsequent state ~s (t+1) and reinforcement ~r (t+1). Before training, the mapping of the world is unknown. <p> On control problems usually many solutions exist for achieving a certain goal. E.g. inverse kinematics are often characterized by infinitely many solutions with different properties. Many connectionist approaches reduce this one-to-many mapping to a one-to-one mapping <ref> [1, 17, 11, 2, 6, 10, 11] </ref>, since the action generator, the control network, is a mathematical function. Therefore often a marginal constraint like smoothness etc. is also optimized. The planning procedure, as it is presented above, is able to perform one-to-many mappings as well.
Reference: [11] <author> D. Nguyen and B. Widrow. </author> <title> The truck backer-upper: An example of self-learning in neural networks. </title> <booktitle> In Proceedings of the First International Joint Conference on Neural Networks, </booktitle> <address> Washington, DC, San Diego, </address> <year> 1989. </year> <journal> IEEE, IEEE TAB Neural Network Committee. </journal>
Reference-contexts: Planning concerns the synthesization of a sequence of actions to achieve a specific goal. Connectionist approaches so far rely on an associative mapping, which selects good actions given environmental state descriptions. These are based on the interaction of a world model and an action generating network <ref> [1, 2, 6, 10, 11, 14, 15, 20] </ref>. We recognize three major problems with these approaches: 1. Since no explicit consideration of the future is made, future effects of actions must be directly encoded into this world model mapping, thus model learning becomes complicated. 2. <p> While the model network is an essential part of the training of the action network, the learning of the latter lags behind that of the former. In this paper we present a new connectionist planning procedure. Our model network learns one-step predictions by observing the environmental mapping <ref> [2, 6, 11] </ref>. In this way training information is immediately available and model learning is easier and faster. With such a model network a look-ahead plan is constructed and subsequently optimized. We demonstrate the performance of this planning procedure through simulations on a target tracking problem. <p> Since the reinforcement is an unstructured overall signal, the problem with reinforcement learning tasks is the assignment of particular reinforcements to particular actions in the past. This problem is called the temporal credit assignment problem. Many approaches use a network which solves this problem directly <ref> [1, 11, 17] </ref>. This network, the controller network, learns to generate actions that optimize the whole future reinforcement. Obviously the quality of actions depends strongly on future actions. Thus if the controller generates an action, future actions are implicitly contained in this decision. <p> On control problems usually many solutions exist for achieving a certain goal. E.g. inverse kinematics are often characterized by infinitely many solutions with different properties. Many connectionist approaches reduce this one-to-many mapping to a one-to-one mapping <ref> [1, 17, 11, 2, 6, 10, 11] </ref>, since the action generator, the control network, is a mathematical function. Therefore often a marginal constraint like smoothness etc. is also optimized. The planning procedure, as it is presented above, is able to perform one-to-many mappings as well.
Reference: [12] <author> N. J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1982. </year>
Reference-contexts: Introduction Undoubtedly planning is an important and powerful concept in problem solving <ref> [12] </ref>. Planning concerns the synthesization of a sequence of actions to achieve a specific goal. Connectionist approaches so far rely on an associative mapping, which selects good actions given environmental state descriptions.
Reference: [13] <author> B. A. Pearlmutter. </author> <title> Learning state space trajectories in recurrent neural networks. </title> <type> Technical Report CMU-CS-88-191, </type> <institution> Carnegie Mellon University, </institution> <year> 1988. </year>
Reference-contexts: The Feed-Forward Algorithm for Gradient Search in Action Space As mentioned above the environment is modeled by a non-recurrent multilayer backpropagation network. This restriction is sufficient for our simulation results the extension of the algorithm to recurrent networks <ref> [3, 4, 5, 9, 13, 14, 15, 21] </ref> is straightforward and shown in [18]. The external input of the world model network is a state vector ~s (t) and an action vector ~a (t).
Reference: [14] <author> A. J. Robinson. </author> <title> Dynamic Error Propagation Networks. </title> <type> PhD thesis, </type> <institution> Cambridge University Engineering Dept., </institution> <address> Cambridge, UK, </address> <month> February </month> <year> 1989. </year>
Reference-contexts: Planning concerns the synthesization of a sequence of actions to achieve a specific goal. Connectionist approaches so far rely on an associative mapping, which selects good actions given environmental state descriptions. These are based on the interaction of a world model and an action generating network <ref> [1, 2, 6, 10, 11, 14, 15, 20] </ref>. We recognize three major problems with these approaches: 1. Since no explicit consideration of the future is made, future effects of actions must be directly encoded into this world model mapping, thus model learning becomes complicated. 2. <p> The training of this world model is a system identification task. In this paper, we use a multilayer differentiable, non-recurrent connectionist network for modeling the world (c.f. figure 1). This network is trained by backpropagation to predict the behavior of the world <ref> [1, 2, 6, 10, 14, 20] </ref>. Formally, the world considered in this paper is defined as a mapping, which maps an action vector ~a (t) with a current state ~s (t) to a subsequent state ~s (t+1) and reinforcement ~r (t+1). Before training, the mapping of the world is unknown. <p> The Feed-Forward Algorithm for Gradient Search in Action Space As mentioned above the environment is modeled by a non-recurrent multilayer backpropagation network. This restriction is sufficient for our simulation results the extension of the algorithm to recurrent networks <ref> [3, 4, 5, 9, 13, 14, 15, 21] </ref> is straightforward and shown in [18]. The external input of the world model network is a state vector ~s (t) and an action vector ~a (t).
Reference: [15] <author> A. J. Robinson and F. Fallside. </author> <title> Dynamic reinforcement driven error propagation networks with application to game playing. </title> <booktitle> to be presented at the Eleventh Annual Conference of the Cognitive Science Society, </booktitle> <address> Ann Arbor, </address> <year> 1989. </year>
Reference-contexts: Planning concerns the synthesization of a sequence of actions to achieve a specific goal. Connectionist approaches so far rely on an associative mapping, which selects good actions given environmental state descriptions. These are based on the interaction of a world model and an action generating network <ref> [1, 2, 6, 10, 11, 14, 15, 20] </ref>. We recognize three major problems with these approaches: 1. Since no explicit consideration of the future is made, future effects of actions must be directly encoded into this world model mapping, thus model learning becomes complicated. 2. <p> The Feed-Forward Algorithm for Gradient Search in Action Space As mentioned above the environment is modeled by a non-recurrent multilayer backpropagation network. This restriction is sufficient for our simulation results the extension of the algorithm to recurrent networks <ref> [3, 4, 5, 9, 13, 14, 15, 21] </ref> is straightforward and shown in [18]. The external input of the world model network is a state vector ~s (t) and an action vector ~a (t).
Reference: [16] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. </booktitle> <volume> Vol. I + II. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This is done by propagating the error back through the network using the backpropagation algorithm <ref> [16, 18] </ref>. Adaptive Look-Ahead-Planning The planning procedure presented in this paper is an approximation procedure, which starts with a initial plan and improves this plan stepwise by gradient descent in order to maximize the next N reinforcements. Let us assume we have such an initial N -step look-ahead plan. <p> One way of computing the gradients of E r with respect to the actions is using backpropagation through the spatial unfolded time-structure <ref> [16, 19] </ref>. Since no dynamical determination of the plan length N is possible by using this backpropagation-in-time technique, we will derive a pure feed-forward algorithm for computing these gradients.
Reference: [17] <author> R. S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1984. </year>
Reference-contexts: Since the reinforcement is an unstructured overall signal, the problem with reinforcement learning tasks is the assignment of particular reinforcements to particular actions in the past. This problem is called the temporal credit assignment problem. Many approaches use a network which solves this problem directly <ref> [1, 11, 17] </ref>. This network, the controller network, learns to generate actions that optimize the whole future reinforcement. Obviously the quality of actions depends strongly on future actions. Thus if the controller generates an action, future actions are implicitly contained in this decision. <p> This implies the assumption that the effect of a certain action to the reinforcement occurs in the next N time steps similar assumptions are also made in <ref> [1, 17] </ref>. N can be an arbitrary number the computational costs of the optimization steps are linear in N . Moreover, N can be determined at planning time dynamically. The World and the World Model Planning is a hypothetical process, in which future states and future actions are involved. <p> This network is trained in a supervised manner (e.g. with backpropagation) to compute the resulting action of the training procedure from the current state. The experience network is similar to the control network described in <ref> [1, 17] </ref>, but it is used in a different way. Its output is optimized by our planning procedure before it is given to the world. The advantage of using an experience network is that the time-consuming planning procedure is shifted gradually to the experience in the experience network. <p> On control problems usually many solutions exist for achieving a certain goal. E.g. inverse kinematics are often characterized by infinitely many solutions with different properties. Many connectionist approaches reduce this one-to-many mapping to a one-to-one mapping <ref> [1, 17, 11, 2, 6, 10, 11] </ref>, since the action generator, the control network, is a mathematical function. Therefore often a marginal constraint like smoothness etc. is also optimized. The planning procedure, as it is presented above, is able to perform one-to-many mappings as well.
Reference: [18] <author> S. Thrun. </author> <title> A general feed-forward algorithm for gradient-descent in neural networks. </title> <note> Technical Report In press, </note> <institution> GMD, </institution> <address> Sankt Augustin, FRG, </address> <year> 1990. </year>
Reference-contexts: This is done by propagating the error back through the network using the backpropagation algorithm <ref> [16, 18] </ref>. Adaptive Look-Ahead-Planning The planning procedure presented in this paper is an approximation procedure, which starts with a initial plan and improves this plan stepwise by gradient descent in order to maximize the next N reinforcements. Let us assume we have such an initial N -step look-ahead plan. <p> This restriction is sufficient for our simulation results the extension of the algorithm to recurrent networks [3, 4, 5, 9, 13, 14, 15, 21] is straightforward and shown in <ref> [18] </ref>. The external input of the world model network is a state vector ~s (t) and an action vector ~a (t). Both state and action vector are the external input ~ I (t) of the model network; for all non-input units this external input is 0.
Reference: [19] <author> S. Thrun and A. Linden. </author> <title> Inversion in time. </title> <booktitle> In Proceedings of the EURASIP Workshop on Neural Networks, </booktitle> <address> Sesimbra, Portugal, </address> <month> February 15-17. EURASIP, </month> <year> 1990. </year>
Reference-contexts: Thus we obtain a prediction for all reinforcements ~r pred (t+1); ~r pred (t+2); :::; ~r pred (t+N ) in this look-ahead window. For improving the actions with respect to the predicted reinforcement we use a gradient descent algorithm in action space <ref> [7, 8, 19] </ref>, which will be derived in detail in the next section. It computes the gradients of the reinforcement with respect to the plan, which give us the information, how to change the plan in small steps in order to improve its performance. <p> One way of computing the gradients of E r with respect to the actions is using backpropagation through the spatial unfolded time-structure <ref> [16, 19] </ref>. Since no dynamical determination of the plan length N is possible by using this backpropagation-in-time technique, we will derive a pure feed-forward algorithm for computing these gradients.
Reference: [20] <author> P. J. Werbos. </author> <title> Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-17:7-19, </volume> <year> 1987. </year>
Reference-contexts: Planning concerns the synthesization of a sequence of actions to achieve a specific goal. Connectionist approaches so far rely on an associative mapping, which selects good actions given environmental state descriptions. These are based on the interaction of a world model and an action generating network <ref> [1, 2, 6, 10, 11, 14, 15, 20] </ref>. We recognize three major problems with these approaches: 1. Since no explicit consideration of the future is made, future effects of actions must be directly encoded into this world model mapping, thus model learning becomes complicated. 2. <p> The training of this world model is a system identification task. In this paper, we use a multilayer differentiable, non-recurrent connectionist network for modeling the world (c.f. figure 1). This network is trained by backpropagation to predict the behavior of the world <ref> [1, 2, 6, 10, 14, 20] </ref>. Formally, the world considered in this paper is defined as a mapping, which maps an action vector ~a (t) with a current state ~s (t) to a subsequent state ~s (t+1) and reinforcement ~r (t+1). Before training, the mapping of the world is unknown.
Reference: [21] <author> R. J. Williams and D. Zipser. </author> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <type> Technical Report ICS Report 8805, </type> <institution> Institute for Cognitive Science, University of California, </institution> <address> San Diego, CA, </address> <year> 1988. </year>
Reference-contexts: The Feed-Forward Algorithm for Gradient Search in Action Space As mentioned above the environment is modeled by a non-recurrent multilayer backpropagation network. This restriction is sufficient for our simulation results the extension of the algorithm to recurrent networks <ref> [3, 4, 5, 9, 13, 14, 15, 21] </ref> is straightforward and shown in [18]. The external input of the world model network is a state vector ~s (t) and an action vector ~a (t).
References-found: 21

