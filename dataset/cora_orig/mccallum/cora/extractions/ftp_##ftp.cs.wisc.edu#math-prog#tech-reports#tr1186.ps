URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/tr1186.ps
Refering-URL: http://www.cs.wisc.edu/~olvi/olvi.html
Root-URL: 
Title: Misclassification Minimization  
Author: O. L. Mangasarian 
Date: Revised April 6, 1994  
Abstract: The problem of minimizing the number of misclassified points by a plane, attempting to separate two point sets with intersecting convex hulls in n-dimensional real space, is formulated as a linear program with equilibrium constraints (LPEC). This general LPEC can be converted to an exact penalty problem with a quadratic objective and linear constraints. A Frank-Wolfe-type algorithm is proposed for the penalty problem that terminates at a stationary point or a global solution. Novel aspects of the approach include: (i) A linear complementarity formulation of the step function that "counts" misclassifications, (ii) Exact penalty formulation without boundedness, nondegeneracy or constraint qualification assumptions, (iii) An exact solution extraction from the sequence of minimizers of the penalty function for a finite value of the penalty parameter for the general LPEC and an explicitly exact solution for the LPEC with uncoupled constraints, and (iv) A parametric quadratic programming formulation of the LPEC associated with the misclassification minimization problem.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Anandalingam and T.L. Friesz. </author> <title> Hierarchical optimization: An introduction. </title> <journal> Annals of Operations Research, </journal> <volume> 34 </volume> <pages> 1-11, </pages> <year> 1992. </year>
Reference-contexts: This LPEC is an important special case of mathematical programs with equilibrium constraints (MPEC) <ref> [11, 1] </ref> studied comprehensively recently in [13]. Section 3 deals with methods for solving a general LPEC.
Reference: [2] <author> G. Anandalingam and D.J. White. </author> <title> A solution method for the linear static stackelberg problem using penalty functions. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 35(10) </volume> <pages> 1170-1173, </pages> <year> 1990. </year>
Reference-contexts: In Section 4 we specialize to an LPEC with uncoupled equilibrium constraints (LPUEC) (32) which covers the misclassification minimization problem (12). Theorem 4.1 gives an explicitly exact penalty solution of this problem without any boundedness assumption that was required for the less general Stackelberg problem <ref> [2] </ref>. We propose Algorithm 4.2 as a finite stepless partial Frank-Wolfe algorithm for solving this problem. Its finite termination to an exact solution or a stationary point is established in Theorem 4.3. <p> This is an improvement over the stepless full Frank-Wolfe algorithm proposed for the linear Stackelberg problem, a special case of (32), with an exact penalty also, but where each linear program was solved completely <ref> [2] </ref>. <p> of the linear programming subproblems is preferable especially when one is far from a solution point or stationary point, and the linear subproblems are merely crude surrogates for the nonlinear problem. (iii) No boundedness of the feasible region is needed for exactness of the penalty function, as was required in <ref> [2] </ref>, for the linear Stackelberg problem and in [13] for the MPEC, nor a constraint qualification as needed in [13].
Reference: [3] <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67, </pages> <address> Amsterdam, 1992. </address> <publisher> North Holland. </publisher>
Reference-contexts: However, the linear programming formulation [4] obtains an approximate separating plane that minimizes some norm of the distances of misclassified points to the approximate separating plane. Although this approach has been quite successful in important real world applications [18, 21] and in the training of neural networks <ref> [3] </ref>, the approximate separating plane does not minimize the number of misclassified points, as do some machine learning approaches [19]. In neural networks [12], misclas sification minimization is achieved by using the sigmoid error function 1 1 + e ffx , with a positive ff, to approximate a step function.
Reference: [4] <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: When the convex hulls of A and B are disjoint, a single linear program <ref> [14, 4] </ref> or the classical iterative perceptron algorithm [20, 9] will obtain a separating plane in a finite number of steps. In the general and usual case of intersecting convex hulls, the perceptron algorithm merely obtains a bounded sequence of iterates [7]. <p> In the general and usual case of intersecting convex hulls, the perceptron algorithm merely obtains a bounded sequence of iterates [7]. However, the linear programming formulation <ref> [4] </ref> obtains an approximate separating plane that minimizes some norm of the distances of misclassified points to the approximate separating plane. <p> This is motivated by the fact that when ffi = 0, problem (40) reduces precisely to problem (2.11) of <ref> [4] </ref> which is guaranteed to generate a non-null w that will minimize the average of the distances of misclassified points (assuming k A i k 2 = 1; k B j k 2 = 1; i = 1; : : : ; m; j = 1; : : :; k).
Reference: [5] <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization & Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: Therefore, it remains to prescribe an algorithm for solving the penalty problem (18) for ff sufficiently large. We propose a Frank-Wolfe algorithm [10, 6] for solving (18) similar to that employed in <ref> [5] </ref>. This approach was quite successful in solving the nonconvex bilinear separability problem on many test cases [5]. For completeness we give the algorithm here. <p> Therefore, it remains to prescribe an algorithm for solving the penalty problem (18) for ff sufficiently large. We propose a Frank-Wolfe algorithm [10, 6] for solving (18) similar to that employed in <ref> [5] </ref>. This approach was quite successful in solving the nonconvex bilinear separability problem on many test cases [5]. For completeness we give the algorithm here. <p> 0 5 z P (z j ; ff)z * Stop if 5 z P (z j ; ff)(z j z j ) = 0 * z j+1 = (1 j )z j + j z i ; where j 2 arg min 01 Convergence of this algorithm is established in <ref> [5] </ref> without any convexity assumption on P (z; ff): We state this convergence result without proof here. <p> Our experience with partial solution of the linear programming subproblems for the closely related bilinear programming problem <ref> [5] </ref> leads us to believe that partial solution of the linear programming subproblems is preferable especially when one is far from a solution point or stationary point, and the linear subproblems are merely crude surrogates for the nonlinear problem. (iii) No boundedness of the feasible region is needed for exactness of <p> The ideas are similar to those of Algorithm 2.1 <ref> [5] </ref> for separable bilinear programs.
Reference: [6] <author> C. Berge and A. Ghouila-Houri. </author> <title> Programming, Games and Transportation Networks. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1965. </year>
Reference-contexts: Corollary 3.3 is useful in obtaining an exact solution of LPEC (13) by monitoring repeated bases for ff ff; and using (27)-(28) to get the desired solution. Therefore, it remains to prescribe an algorithm for solving the penalty problem (18) for ff sufficiently large. We propose a Frank-Wolfe algorithm <ref> [10, 6] </ref> for solving (18) similar to that employed in [5]. This approach was quite successful in solving the nonconvex bilinear separability problem on many test cases [5]. For completeness we give the algorithm here.
Reference: [7] <author> H.D. Block and S.A. Levin. </author> <title> On the boundedness of an iterative procedure for solving a system of linear inequalities. </title> <journal> Proceedings of the American Mathematical Society, </journal> <volume> 26 </volume> <pages> 229-235, </pages> <year> 1970. </year> <month> 12 </month>
Reference-contexts: In the general and usual case of intersecting convex hulls, the perceptron algorithm merely obtains a bounded sequence of iterates <ref> [7] </ref>. However, the linear programming formulation [4] obtains an approximate separating plane that minimizes some norm of the distances of misclassified points to the approximate separating plane.
Reference: [8] <author> J.E. Dennis and R.B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Non--linear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: Our proposed procedure is to solve (40) by Algorithm 4.2 for increasing values of ffi until f (ffi) = 0. A one dimensional secant method <ref> [8] </ref> for finding a nonnegative root of f (ffi) = 0 may be helpful.
Reference: [9] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: When the convex hulls of A and B are disjoint, a single linear program [14, 4] or the classical iterative perceptron algorithm <ref> [20, 9] </ref> will obtain a separating plane in a finite number of steps. In the general and usual case of intersecting convex hulls, the perceptron algorithm merely obtains a bounded sequence of iterates [7].
Reference: [10] <author> M. Frank and P. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year>
Reference-contexts: Corollary 3.3 is useful in obtaining an exact solution of LPEC (13) by monitoring repeated bases for ff ff; and using (27)-(28) to get the desired solution. Therefore, it remains to prescribe an algorithm for solving the penalty problem (18) for ff sufficiently large. We propose a Frank-Wolfe algorithm <ref> [10, 6] </ref> for solving (18) similar to that employed in [5]. This approach was quite successful in solving the nonconvex bilinear separability problem on many test cases [5]. For completeness we give the algorithm here. <p> is bounded below on S 0 by assumption, and x (M x + N y + q) 0 on S 0 ; it follows that the quadratic penalty function P ((x; y); ff) has a minimum solution (x (ff); y (ff)) in the polyhedral set S 0 for ff 0 <ref> [10] </ref>. <p> finite algorithm for solving LPUEC (32) that terminates either at a global solution or a point satisfying the minimum principle necessary optimality condition [15] for min (x;y)2S 0 The algorithm consists of partially solving a succession of linear programs, and as such can be considered as a stepless Frank-Wolfe algorithm <ref> [10] </ref>. The ideas are similar to those of Algorithm 2.1 [5] for separable bilinear programs. <p> Note that problem (40) is solvable for each ffi 2 [0; 1), because its quadratic objective is bounded below on its nonempty feasible region <ref> [10] </ref>. Furthermore f (ffi), the nonnegative minimum value of (40), is a nonincreasing function of ffi that decreases to zero at ffi = ffi; for some ffi 0, which is the global minimum of problem (12).
Reference: [11] <author> P.T. Harker and J.-S. Pang. </author> <title> Existence of optimal solutions to mathematical programs with equilibrium constraints. </title> <journal> Operations Research Letters, </journal> <volume> 7 </volume> <pages> 61-64, </pages> <year> 1988. </year>
Reference-contexts: This LPEC is an important special case of mathematical programs with equilibrium constraints (MPEC) <ref> [11, 1] </ref> studied comprehensively recently in [13]. Section 3 deals with methods for solving a general LPEC.
Reference: [12] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: Although this approach has been quite successful in important real world applications [18, 21] and in the training of neural networks [3], the approximate separating plane does not minimize the number of misclassified points, as do some machine learning approaches [19]. In neural networks <ref> [12] </ref>, misclas sification minimization is achieved by using the sigmoid error function 1 1 + e ffx , with a positive ff, to approximate a step function. Hence, minimization of the sum of distances of misclassified points by a linear program is merely a surrogate for misclassification minimization. <p> We skip the algebra that shows this, but give an intuitive justification of this curious fact. This may also explain why optimization algorithms, including the classical backpropagation algorithm of neural networks <ref> [12] </ref>, may be slow when applied directly to the misclassification error function of (6) or the equivalent formulation (12), instead of the parametric quadratic minimization (40) proposed below. Suppose that the plane wx = fl does not pass through any of the points of either set A or B.
Reference: [13] <author> Z.-Q. Luo, J.-S. Pang, D. Ralph, and S.-Q. Wu. </author> <title> Exact penalization and stationarity conditions of mathematical programs with equilibrium constraints. </title> <type> Technical Report 275, </type> <institution> Communications Research Laboratory, McMaster University, Hamilton, </institution> <address> Ontario, Hamilton, Ontario L8S 4K1, Canada, </address> <year> 1993. </year>
Reference-contexts: This LPEC is an important special case of mathematical programs with equilibrium constraints (MPEC) [11, 1] studied comprehensively recently in <ref> [13] </ref>. Section 3 deals with methods for solving a general LPEC. We convert our LPEC to an exact penalty problem and show (Theorem 3.2) that for a finite value of the penalty parameter an exact solution of the LPEC is contained in a minimizer of the penalty problem. <p> s (v Bw + efl e) = 0 v 0 (12) We note that problem (12) is a linear program with equilibrium (linear complementarity) constraints (LPEC) and is a special case of the more general mathematical program with equilibrium constraints (MPEC) studied in detail by Luo, Pang, Ralph and Wu <ref> [13] </ref>. Being linear, our problem is endowed with some features not possessed by the more general MPECs, principally exactness of a penalty formulation without boundedness of the feasible region and without assuming nondegen eracy. <p> when one is far from a solution point or stationary point, and the linear subproblems are merely crude surrogates for the nonlinear problem. (iii) No boundedness of the feasible region is needed for exactness of the penalty function, as was required in [2], for the linear Stackelberg problem and in <ref> [13] </ref> for the MPEC, nor a constraint qualification as needed in [13]. <p> and the linear subproblems are merely crude surrogates for the nonlinear problem. (iii) No boundedness of the feasible region is needed for exactness of the penalty function, as was required in [2], for the linear Stackelberg problem and in <ref> [13] </ref> for the MPEC, nor a constraint qualification as needed in [13].
Reference: [14] <author> O.L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference-contexts: When the convex hulls of A and B are disjoint, a single linear program <ref> [14, 4] </ref> or the classical iterative perceptron algorithm [20, 9] will obtain a separating plane in a finite number of steps. In the general and usual case of intersecting convex hulls, the perceptron algorithm merely obtains a bounded sequence of iterates [7]. <p> empty vector will be denoted by ;. 2 Misclassification Minimization as a Linear Program with Equi librium Constraint (LPEC) When the point sets A and B, represented by the m fi n and k fi n matrices A and B respectively have disjoint convex hulls, they can be strictly separated <ref> [14] </ref> by a plane wx = fl (1) in R n where w is some weight vector constituting the normal to the separating plane and fl locates the plane relative to the origin.
Reference: [15] <author> O.L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1969. </year>
Reference-contexts: We now give a finite algorithm for solving LPUEC (32) that terminates either at a global solution or a point satisfying the minimum principle necessary optimality condition <ref> [15] </ref> for min (x;y)2S 0 The algorithm consists of partially solving a succession of linear programs, and as such can be considered as a stepless Frank-Wolfe algorithm [10]. The ideas are similar to those of Algorithm 2.1 [5] for separable bilinear programs. <p> S 0 : For ff ff for some ff &gt; 0; Algorithm 4.2 terminates in a finite number of steps at a solution of LPUEC (32) or at a point (x i 1 ; x i+1 1 ; y i 2 ) that satisfies the minimum principle necessary optimality condition <ref> [15] </ref> for min P ((x; y); ff) : 1 ; x 2 ); (y 1 ; y i 1 ; x i+1 1 ; y i 2 ); (y i+1 (39) Proof If for some i; P ((x i+1 ; y i+1 ); ff) 6&lt; P ((x i ; y i
Reference: [16] <author> O.L. Mangasarian. </author> <title> Characterization of linear complementarity problems as linear programs. </title> <journal> Mathematical Programming Study, </journal> <volume> 7 </volume> <pages> 74-87, </pages> <year> 1978. </year>
Reference-contexts: 2 R ` 1 satisfy the following Karush-Kuhn-Tucker conditions for (18) 0 @ y 1 A ? B (M + M T ) N M T M N 0 C 0 @ y 1 A + B c d q C The linear complementarity problem (20) has a vertex solution <ref> [16, Lemma 2] </ref>, and hence a basic solution, for each ff &gt; 0: Since (20) has a finite number of bases, it follows that on the set fffjff ffg, 5 for some ff &gt; 0; only a finite number of basic solutions of (20) appear infinitely often, and no other basic
Reference: [17] <author> O.L. Mangasarian. </author> <title> Some applications of penalty functions in mathematical programming. </title> <editor> In R. Conti, E. De Giorgi, and F. Giannessi, editors, </editor> <booktitle> Optimization and Related Fields, </booktitle> <pages> pages 307-329. </pages> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1986. </year> <note> Lecture Notes in Mathematics 1190. </note>
Reference-contexts: : : : ; B ` ; (21) correspond to these basic solutions and such that 2 4 y (ff) 3 5 = B i 6 c d q 7 5 ; for some i 2 f1; : : :; `g; ff ff (22) It follows from Theorem 2.8 of <ref> [17] </ref> by letting ff ! 1; that " 0 0 := x (1) # " 1 2 2 4 0 3 5 ; for some i 2 f1; : : :; `g; (23) solve LPEC (13). <p> This establishes (17) with " b i = B i B i # 6 c 0 7 " 0 0 as given by (23): To establish (19) we employ Theorem 2.8 <ref> [17] </ref> again which states that lim ffx (ff)(M x (ff) + N y (ff) + q) = 0 (24) Since x (ff)(M x (ff) + N y (ff) + q) = ff 2 + 0 g i + x i where g i := M a i + N b i <p> ((x; y); ff): We now establish exactness of the penalty problem min (x;y)2S 0 P ((x; y); ff): Since S 0 has a finite number of vertices, one vertex, say (x; y) will satisfy (x; y) 2 arg vertex min (x;y)2S 0 for some ff &gt; 0: By Theorem 2.5 <ref> [17] </ref> we have that x (M x + N y + q) = 0 Consequently, for all (x; y) 2 S and ff ff cx + dy = P ((x; y); ff) P ((x; y); ff) = cx + dy Hence (x; y) solves LPUEC (32).
Reference: [18] <author> O.L. Mangasarian, R. Setiono, </author> <title> and W.H. Wolberg. Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Large-Scale Numerical Optimization, </booktitle> <pages> pages 22-31, </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1990. </year> <title> SIAM. </title> <booktitle> Proceedings of the Workshop on Large-Scale Numerical Optimization, </booktitle> <institution> Cornell University, </institution> <address> Ithaca, New York, </address> <month> October 19-20, </month> <year> 1989. </year>
Reference-contexts: However, the linear programming formulation [4] obtains an approximate separating plane that minimizes some norm of the distances of misclassified points to the approximate separating plane. Although this approach has been quite successful in important real world applications <ref> [18, 21] </ref> and in the training of neural networks [3], the approximate separating plane does not minimize the number of misclassified points, as do some machine learning approaches [19].
Reference: [19] <author> S. Murthy, S. Kasif, S. Salzberg, and R. Beigel. </author> <title> OC1: Randomized induction of oblique decision trees. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 322-327, </pages> <address> Cambridge, MA 02142, 1993. </address> <publisher> The AAAI Press/The MIT Press. </publisher>
Reference-contexts: Although this approach has been quite successful in important real world applications [18, 21] and in the training of neural networks [3], the approximate separating plane does not minimize the number of misclassified points, as do some machine learning approaches <ref> [19] </ref>. In neural networks [12], misclas sification minimization is achieved by using the sigmoid error function 1 1 + e ffx , with a positive ff, to approximate a step function.
Reference: [20] <author> F. Rosenblatt. </author> <title> The perceptron-a perceiving and recognizing automaton. </title> <type> Technical Report 85-460-1, </type> <institution> Cornell Aeronautical Laboratory, </institution> <address> Ithaca, New York, </address> <month> January </month> <year> 1957. </year>
Reference-contexts: When the convex hulls of A and B are disjoint, a single linear program [14, 4] or the classical iterative perceptron algorithm <ref> [20, 9] </ref> will obtain a separating plane in a finite number of steps. In the general and usual case of intersecting convex hulls, the perceptron algorithm merely obtains a bounded sequence of iterates [7].
Reference: [21] <author> W. H. Wolberg and O.L. Mangasarian. </author> <title> Multisurface method of pattern separation for medical diagnosis applied to breast cytology. </title> <booktitle> Proceedings of the National Academy of Sciences,U.S.A., </booktitle> <volume> 87 </volume> <pages> 9193-9196, </pages> <year> 1990. </year>
Reference-contexts: However, the linear programming formulation [4] obtains an approximate separating plane that minimizes some norm of the distances of misclassified points to the approximate separating plane. Although this approach has been quite successful in important real world applications <ref> [18, 21] </ref> and in the training of neural networks [3], the approximate separating plane does not minimize the number of misclassified points, as do some machine learning approaches [19].
References-found: 21

