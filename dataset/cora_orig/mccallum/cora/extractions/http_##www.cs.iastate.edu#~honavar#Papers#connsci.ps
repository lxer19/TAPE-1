URL: http://www.cs.iastate.edu/~honavar/Papers/connsci.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/honavar.html
Root-URL: 
Title: BRAINSTRUCTURED CONNECTIONIST NETWORKS THAT PERCEIVE AND LEARN  
Author: Vasant Honavar and Leonard Uhr 
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: This paper specifies the main features of Brain-like, Neuronal, and Connectionist models; argues for the need for, and usefulness of, appropriate successively larger brain-like structures; and examines parallel-hierarchical Recognition Cone models of perception from this perspective, as examples of such structures. The anatomy, physiology, behavior, and development of the visual system are briefly summarized to motivate the architecture of brain-structured networks for perceptual recognition. Results are presented from simulations of carefully pre-designed Recognition Cone structures that perceive objects (e.g., houses) in digitized photographs. A framework for perceptual learning is introduced, including mechanisms for generation-discovery (feedback-guided growth of new links and nodes, subject to brain-like constraints (e.g., local receptive fields, global convergence-divergence). The information processing transforms discovered through generation are fine-tuned by feedback-guided reweight-ing of links. Some preliminary results are presented of brain-structured networks that learn to recognize simple objects (e.g., letters of the alphabet, cups, apples, bananas) through feedback-guided generation and reweighting. These show large improvements over networks that either lack brain-like structure or/and learn by reweighting of links alone. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abu-Mostafa, Y., </author> <title> ``Connectivity and Entropy,'' </title> <booktitle> Advances in Neural Information Processing systems, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference: <author> Barto, A. G. and Anandan, P., </author> <title> ``Pattern recognizing stochastic learning automata,'' </title> <journal> IEEE Trans. Systems, Man and Cybernetics, </journal> <volume> vol. 15, </volume> <pages> pp. 360-375, </pages> <year> 1985. </year>
Reference: <author> DeYoe, E. A. and Van Essen, D. C., </author> <title> ``Concurrent processing streams in monkey visual cortex,'' </title> <journal> Trends in Neuroscience, </journal> <volume> vol. </volume> <pages> 11-5, pp. 219-226, </pages> <month> May </month> <year> 1988. </year> - <note> 18 - Dyer, </note> <author> C. R., </author> <title> ``Multiscale image understanding,'' in Parallel Computer Vision, </title> <editor> ed. L. Uhr, </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference: <author> Feldman, J. A. and Ballard, D. H., </author> <title> ``Connectionist models and their properties,'' </title> <journal> Cognitive Science, </journal> <volume> vol. 6, </volume> <pages> pp. 205-264, </pages> <year> 1982. </year>
Reference: <author> Greenough, W. T. and Bailey, C. H., </author> <title> ``The anatomy of a memory: convergence of results across a diversity of tests,'' </title> <journal> Trends in Neuroscience, </journal> <volume> vol. 11, </volume> <pages> pp. 142-147, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Neuroanatomi-cal and physiological evidence suggests that the plasticity of information processing structures involves both changes in existing connections (analogous to reweighting of links in a connectionist network) as well growth of new connections in response to environmental input practically throughout life <ref> (Greenough, 1988) </ref>. Development of certain visual processes (e.g., binocular fusion) relies heavily on the availability of certain kinds of stimuli (e.g., similar patterns to both eyes) during appropriate stages of development.
Reference: <author> Hebb, D. O., </author> <title> The Organization of Behavior, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1949. </year>
Reference-contexts: investigated, including the development of good sub-nets that decide whether to further re-weight or to generate, the optimal number of nodes in a node-cluster, whether to make nodes within a cluster compete, and whether and how to combine generation with other reweighting rules e.g., some variant of the Hebbian rule <ref> (Hebb, 1949) </ref>. Generation is also being used to learn speech, boolean as well as continuous functions, and simple arithmetic. Conclusions Connectionist networks built from simple neuron-like units arranged in brain-like topologies can be constructed to yield relatively good perceptual recognition of complex real-world objects (e.g., houses) in large (512x512) images.
Reference: <author> Hinton, G. E., </author> <title> ``Connectionist learning procedures,'' </title> <type> Technical Report CMU-CS-87-115, </type> <institution> Computer Science Department, Carnegie Mellon University, Pitts-burgh, Pennsylvania, </institution> <year> 1987a. </year>
Reference-contexts: Several algorithms for changing weights associated with the links are available <ref> (Hinton, 1987a) </ref>. A learning scheme for [3] that employs a mechanism for activity-dependent, feedback-guided generation of new links is described in (Honavar, 1987; Honavar, 1988). <p> This form of learning has been studied widely in connectionist systems, and several reweighting algorithms are available <ref> (Hinton, 1987a) </ref>. We use one that is quite similar to the error back-propagation algorithm (Rumelhart, 1986b) (see below). The Need for the Capability to Generate New Transforms The input to the network represents a certain encoding of the environment.
Reference: <author> Hinton, G. E., </author> <title> ``Learning translation-invariant recognition in a massively parallel network,'' </title> <booktitle> in PARLE: Parallel Architectures and Languages, Europe. Lecture Notes in Computer Science, </booktitle> <editor> ed. G. Goos and J. Hartmanis, </editor> <publisher> Springer-Verlag , Berlin , 1987b. </publisher>
Reference: <author> Hochberg, J., </author> <title> Perception, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1978. </year>
Reference-contexts: In this context, anatomical and physiological evidence for a phased development of the visual pathways from the retina to successively deeper cortical layers is tantalizing. Further, there is some evidence for the gestalt principles of perceptual organization <ref> (Hochberg, 1978) </ref> e.g., proximity (image features that are relatively close together tend to be grouped together) and similarity (image segments that have similar brightness, color, or texture tend to be grouped together) among very young infants.
Reference: <author> Holland, J. H., Holyoak, K. J., Nisbett, R. E., and Thagard, P. R., </author> <title> Induction: Processes of Inference, Learning, and Discovery, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference: <author> Honavar, V. and Uhr, L., </author> <title> ``Recognition Cones: A neuronal architecture for perception and learning,'' </title> <type> Computer Sciences Technical Report #717, </type> <institution> University of Wisconsin-Madison, Madison, WI, </institution> <year> 1987. </year>
Reference: <author> Honavar, V. and Uhr, L., </author> <title> ``A network of neuron-like units that learns to perceive by generation as well as reweighting of its links,'' </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <editor> ed. G. E. Hin-ton, T. J. Sejnowski and D. S. Touretzky, </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: This is accomplished entirely by simple networks of neuron-like units, through local computations performed incrementally following each training presentation. An implementation of such structures is described elsewhere <ref> (Honavar, 1988) </ref>. Details of a particular implementation are unimportant, since a variety of local computations of this kind can be performed by appropriate microcircuits (e.g., counters, comparators) built from neuron-like units.
Reference: <author> Honavar, V. and Uhr, L., </author> <title> ``Generation, Local Receptive Fields, and Global Convergence Improve Perceptual Learning in Connectionist Networks,'' </title> <booktitle> in Proceedings of the 1989 International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989a </year> . 
Reference: <author> Honavar, V., </author> <title> ``Perceptual development and learning: From behavioral, neurophysiological, and morphological evidence to computational models,'' </title> <type> Computer Sciences Technical Report #818, </type> <institution> University of Wisconsin-Madison, Madison, WI, </institution> <year> 1989b. </year>
Reference-contexts: See (Kuffler, 1984; Van Essen, 1985; Uhr, 1986b; Zeki, 1988; Livingstone, 1988; DeYoe, 1988) for more details on vision, and <ref> (Honavar, 1989b) </ref> for a review of of perceptual development and learning and implications for modeling. The purpose of this overview is to motivate the architecture of Recognition Cones as an example of systems that structure networks of neuron-like units into successively larger brain-like modules.
Reference: <author> Hubel, D. H., </author> <title> ``Explorations of the primary visual cortex, 1955-1978,'' </title> <journal> Nature , vol. </journal> <volume> 299, </volume> <pages> pp. 515-524, </pages> <year> 1982. </year>
Reference: <author> Kuffler, S. W., Nicholls, J. G., and Martin, A. R., </author> <title> From Neuron To Brain, </title> <publisher> Sinauer Associates Inc., </publisher> <address> Sunderland, Massachusetts, </address> <year> 1984. </year>
Reference-contexts: Characteristically, processes that connect cells in different layers within an area run for the most part, perpendicular to the surface of the cortex. In contrast, the majority of lateral processes are short. Lateral connections between areas are made by axons that run in bundles through the white matter <ref> (Kuffler, 1984) </ref>. In the monkey, the inputs from the two eyes remain segregated in V1 (like in the LGN), giving rise to the so called ocular dominance columns, columns of cells that respond to stimulation from one eye but not the other (Hubel, 1982).
Reference: <author> Li, Z. N. and Uhr, L., </author> <title> ``Pyramid vision using key features to integrate image-driven bottom-up and model-driven top-down processes,'' </title> <journal> Systems, Man and Cybernetics, </journal> <volume> vol. 17, </volume> <pages> pp. </pages> <month> 250-263 , </month> <year> 1987. </year>
Reference-contexts: Simulations that combine data-driven, bottom-up processing where many feature-detecting transform are applied in parallel with model-driven, top-down processes which are activated when certain transforms respond to the image with sufficiently high weights <ref> (Li, 1987) </ref> recognize complex real-world objects such as windows, shutters, doors, houses, etc. from digitized (grey values range from 0 to 255), high resolution (512512) TV images of outdoor scenes.
Reference: <author> Livingstone, M. and Hubel, D., </author> <title> ``Segregation of form, color, movement, and depth: anatomy, </title> <journal> physiology, and perception,'' Science, </journal> <volume> vol. 240, </volume> <pages> pp. 740-749, </pages> <year> 1988. </year>
Reference-contexts: Cells sensitive to simple features such as oriented edges, colors, have been identified in V1. The orientation selective cells are anatomically seggregated from the color selective cells <ref> (Livingstone, 1988) </ref>. There are also cells that respond optimally to specific combinations of simple features e.g., oriented edge separating patches of different colors.
Reference: <author> Marr, D., </author> <title> Vision, </title> <editor> W. H. </editor> <publisher> Freeman and Co., </publisher> <address> New York, NY, </address> <year> 1982. </year>
Reference: <editor> Michalski, R. S., Carbonell, J. G., and Mitchell, T. M. (eds.), </editor> <booktitle> Machine Learning An Artificial Intelligence Approach, </booktitle> <volume> vol. 1, </volume> <publisher> Tioga , Palo Alto, </publisher> <address> California, </address> <year> 1983. </year>
Reference: <author> Minsky, M. and Papert, S., </author> <title> Perceptrons: An Introduction to Computational Geometry, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1969. </year>
Reference-contexts: The Need for the Capability to Generate New Transforms The input to the network represents a certain encoding of the environment. A single layer of neuron-like units computing fuzzy transforms over this encoding is com-binatorially explosive and not always sufficient to produce the desired input-output mapping <ref> (Minsky, 1969) </ref>. Internal representations that capture non-linear relationships between features in the input encoding must be created to overcome this problem.
Reference: <author> Perret, D. I., Mistlin, A. J., and Chitty, A. J., </author> <title> ``Visual neurones responsive to faces,'' </title> <booktitle> Trends in Neurosci-ence, </booktitle> <volume> vol. </volume> <pages> 10-9, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: However, cells that respond in a very specific manner to extremely complex stimuli e.g., hands, faces, or even a specific face have been found by a number of researchers in the monkey temporal cortex <ref> (Perret, 1987) </ref>. Each cell responds to several stimuli, but optimal response is obtained for only a small subset of those stimuli. Similarly, several cells respond to each stimulus.
Reference: <author> Rosenfeld, A., Hummel, R. A., and Zucker, S. W., </author> <title> ``Scene labeling by relaxation operations,'' </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 6, </volume> <pages> pp. 430-440, </pages> <year> 1976. </year>
Reference: <author> Rosenfeld, A., </author> <title> ``Pyramids: multiresolution image analysis,'' </title> <booktitle> Proceedings of the Third Scandinavian Conference on Image Analysis, </booktitle> <pages> pp. 23-28, </pages> <year> 1983. </year>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and McClelland, </author> <note> J. </note>
Reference: <author> L., </author> <title> ``A general framework for parallel distributed processing,'' in Parallel Distributed Processing vol. 1: Explorations into the Microstructure of Cognition, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986a. </year>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J., </author> <title> ``Learning internal representations by error propagation,'' </title> <booktitle> in Parallel Distributed Processing vol. 1: Foundations, </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1986b. </year>
Reference-contexts: This form of learning has been studied widely in connectionist systems, and several reweighting algorithms are available (Hinton, 1987a). We use one that is quite similar to the error back-propagation algorithm <ref> (Rumelhart, 1986b) </ref> (see below). The Need for the Capability to Generate New Transforms The input to the network represents a certain encoding of the environment. <p> Networks that learn by reweighting alone run the risk of getting stuck in a local minimum <ref> (Rumelhart, 1986b) </ref> of the error surface. Generation provides a potential means of getting out (by modifying the error surface as a result of generated transforms).
Reference: <editor> Rumelhart, D. E., </editor> <booktitle> ``Parallel Distributed Processing,'' Plenary Lecture given at the IEEE International Conference on Neural Networks, </booktitle> <address> San Diego, </address> <year> 1988. </year>
Reference: <author> Sejnowski, T. J., Koch, C., and Churchland, P. S., </author> - <title> 19 - ``Computational neuroscience,'' </title> <journal> Science, </journal> <volume> vol. 240, </volume> <pages> pp. 1299-1305, </pages> <year> 1988. </year>
Reference-contexts: This approach to machine perception differs significantly in philosophy from that in which intelligent behavior is realized in computer vision programs with little reference to underlying neural mechanisms. It also differs from realistic brain models <ref> (Sejnowski, 1988) </ref>, wherein a neural net model of a specific portion of the brain is built and simulated, as faithfully as possible, based on the current neurophysiological data, and explanations for, and predictions of, neural phenomena that might take place in that part of the brain are advanced. <p> As do most simplifying brain models <ref> (Sejnowski, 1988) </ref> that are today called neuronal or connectionist, it uses basic units reminiscent of simplified neurons to build the larger system. But it also employs successively larger brain-like structures, in order to give the system the power needed for real-world perception. <p> In the hierarchy of brain models, they occupy a place between realistic models and simplifying models <ref> (Sejnowski, 1988) </ref>, capturing some aspects of both.
Reference: <author> Shapley, R. and Perry, V. H., </author> <title> ``Cat and monkey retinal ganglion cells,'' </title> <booktitle> Trends in Neuroscience, </booktitle> <pages> pp. 229-235, </pages> <year> 1986. </year>
Reference-contexts: The retinal ganglion cells appear to enhance differences and emphasize spatial as well as temporal gradients in the dynamic input image in the information conveyed to the cortex <ref> (Shapley, 1986) </ref>. About a million ganglion cells carry signals from the retina via one layer of synapses in the lateral geni-culate to the primary visual area of the cortex, retaining the retinotopic nature of the map, much like the original image.
Reference: <author> Smolensky, P., </author> <title> ``On the proper treatment of connectionism,'' </title> <journal> Behavioral and Brain Sciences, </journal> <volume> vol. 11, </volume> <pages> pp. 1-23, </pages> <year> 1988. </year>
Reference: <author> Sterling, P., Freed, M., and Smith, R. G., </author> <booktitle> ``Microcircuitry and functional architecture of the cat retina,'' Trends in Neuroscience, </booktitle> <pages> pp. 186-198, </pages> <year> 1986. </year>
Reference-contexts: The other cell types - 4 - respond to illumination or darkness with relatively slow, graded potentials found in most neurons only in the synaptic regions. The rich system of retinal interconnections and its functional significance is only beginning to be understood <ref> (Sterling, 1986) </ref>. The orderly, layered organization suggests that the visual information processing is carried out in hierarchically arranged levels, going from one functionally related group of cells to the next.
Reference: <author> Torras, C., </author> <title> ``Relaxation and neural learning: Points of convergence and divergence,'' </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. </volume> <publisher> (In Press), </publisher> <year> 1989. </year>
Reference: <author> Uhr, L., </author> <title> ``Layered recognition cone networks that preprocess, classify, and describe,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 21, </volume> <pages> pp. 758-768, </pages> <year> 1972. </year>
Reference: <author> Uhr, L., </author> <title> Pattern Recognition, Learning and Thought, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1973. </year>
Reference: <author> Uhr, L. and Douglass, R., </author> <title> ``A parallel-serial recognition cone system for perception,'' </title> <journal> Pattern Recognition, </journal> <volume> vol. 11, </volume> <pages> pp. 29-40, </pages> <year> 1979. </year>
Reference-contexts: such programs have demonstrated the capability to identify hand-printed letters (with gaps, small distortions and other forms of noise) as well as stylized hand-drawn sketches of place settings consisting of plates, spoons, knives and forks, and also some of the major structural features (e.g., doors, roofs, windows) of photographed houses <ref> (Uhr, 1979) </ref>.
Reference: <author> Uhr, L., </author> <title> ``Pyramid multi-computer structures, and augmented pyramids,'' in Computing Structures for Image Processing, </title> <editor> ed. M. J. B. Duff, </editor> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1983. </year>
Reference: <author> Uhr, L., </author> <title> ``Multiple image and multi-modal augmented pyramid networks,'' in Intermediate Level Image Processing, </title> <editor> ed. M. J. B. Duff, </editor> <publisher> Academic Press, </publisher> <address> Lon-don, </address> <year> 1986. </year>
Reference: <author> Uhr, L., </author> <title> ``Toward a computational information processing model of object perception,'' </title> <type> Computer Sciences Technical Report #651, </type> <institution> University of Wisconsin-Madison, Madison, WI, </institution> <year> 1986. </year>
Reference: <author> Uhr, L., </author> <title> ``Highly parallel, hierarchical, recognition cone perceptual structures,'' in Parallel Computer Vision, </title> <editor> ed. L. Uhr, </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference: <author> Van Essen, D. C., </author> <title> ``Functional organization of the primate visual cortex,'' in Cerebral Cortex, </title> <editor> ed. </editor> <publisher> A. </publisher>
Reference: <editor> Peters and E. G. Jones, </editor> <volume> vol. 3, </volume> <publisher> Plenum, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference: <author> Zeki, S. and Shipp, S., </author> <title> ``The functional logic of cortical connections,'' </title> <journal> Nature, </journal> <volume> vol. 335, </volume> <pages> pp. 311-317, </pages> <year> 1988. </year>
Reference-contexts: The significance of these different areas is that they provide for abstraction, enhancement, and integration of information from specific visual submodalities (e.g., color, motion, shape). From area to area there is much variation in the structural and functional properties of cells <ref> (Zeki, 1988) </ref>, as well as the relative thickness of different layers. Characteristically, processes that connect cells in different layers within an area run for the most part, perpendicular to the surface of the cortex. In contrast, the majority of lateral processes are short.
References-found: 43

