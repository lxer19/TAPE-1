URL: http://www.cs.washington.edu/homes/ladner/cachesort.ps
Refering-URL: http://www.cs.washington.edu/homes/ladner/papers.html
Root-URL: 
Email: lamarca@parc.xerox.com ladner@cs.washington.edu  
Title: The Influence of Caches on the Performance of Sorting  
Author: Anthony LaMarca Richard E. Ladner 
Address: 3333 Coyote Hill Road Palo Alto CA 94304  Seattle, WA 98195  
Affiliation: Xerox PARC  Department of Computer Science and Engineering University of Washington  
Abstract: We investigate the effect that caches have on the performance of sorting algorithms both experimentally and analytically. To address the performance problems that high cache miss penalties introduce we restructure mergesort, quicksort, and heapsort in order to improve their cache locality. For all three algorithms the improvement in cache performance leads to a reduction in total execution time. We also investigate the performance of radix sort. Despite the extremely low instruction count incurred by this linear time sorting algorithm, its relatively poor cache performance results in worse overall performance than the efficient comparison based sorting algorithms. For each algorithm we provide an analysis that closely predicts the number of cache misses incurred by the algorithm.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Blelloch, C. Plaxton, C. Leiserson, S Smith, B. Maggs, and M. Zagha. </author> <title> A comparison of sorting algorithms for the connection machine. </title> <booktitle> In Proceedings of the 3rd ACM Symposium on Parallel Algorithms & Architecture, </booktitle> <pages> pages 3-16, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: To fix this inefficiency, a single multi-partition pass can be used to divide the full set into a number of subsets which are likely to be cache sized or smaller. Multi-partitioning is used in parallel sorting algorithms to divide a set into subsets for multiple processors <ref> [1, 13] </ref> in order to quickly balance the load. We choose the number of pivots so that the number of subsets larger than the cache is small with sufficiently high probability.
Reference: [2] <author> S. Carlsson. </author> <title> An optimal algorithm for deleting the root of a heap. </title> <journal> Information Processing Letters, </journal> <volume> 37(2) </volume> <pages> 117-120, </pages> <year> 1991. </year>
Reference-contexts: In addition, we employ 11 a standard optimization of using a sentinel at the end of the heap to eliminate a comparison per level which reduces instruction count. The literature contains a number of other optimizations that reduce the number of comparisons performed for both adds and removes <ref> [4, 2, 9] </ref>, but in practice these increase the total number of instructions executed and do not improve performance. For this reason, we do not include them in our base heapsort. 6.2 Memory Optimizations To this base heapsort algorithm, we now apply memory optimizations in order to further improve performance.
Reference: [3] <author> D. Clark. </author> <title> Cache performance of the VAX-11/780. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(1) </volume> <pages> 24-37, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction Since the introduction of caches, main memory has continued to grow slower relative to processor cycle times. The time to service a cache miss to memory has grown from 6 cycles for the Vax 11/780 to 120 for the AlphaServer 8400 <ref> [3, 6] </ref>. Cache miss penalties have grown to the point where good overall performance cannot be achieved without good cache performance.
Reference: [4] <author> J. De Graffe and W. Kosters. </author> <title> Expected heights in heaps. </title> <journal> BIT, </journal> <volume> 32(4) </volume> <pages> 570-579, </pages> <year> 1992. </year>
Reference-contexts: In addition, we employ 11 a standard optimization of using a sentinel at the end of the heap to eliminate a comparison per level which reduces instruction count. The literature contains a number of other optimizations that reduce the number of comparisons performed for both adds and removes <ref> [4, 2, 9] </ref>, but in practice these increase the total number of instructions executed and do not improve performance. For this reason, we do not include them in our base heapsort. 6.2 Memory Optimizations To this base heapsort algorithm, we now apply memory optimizations in order to further improve performance.
Reference: [5] <author> W. Feller. </author> <title> An Introduction to Probability Theory and its Applications. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1971. </year>
Reference-contexts: Feller shows that if k points are placed randomly in a range of length 1, the chance of a resulting subrange being of size x or greater is exactly (1 x) k <ref> [5, Vol. 2, Pg. 22] </ref>. Let n be the total number of keys, B the number of keys per cache block, and C the capacity of the cache in blocks. In multi-quicksort we partition the input array into 3n=(BC) pieces, requiring (3n=(BC)) 1 pivots.
Reference: [6] <author> D. Fenwick, D. Foley, W. Gist, S. VanDoren, and D. Wissell. </author> <title> The AlphaServer 8000 series: High-end server platform development. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1) </volume> <pages> 43-65, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Since the introduction of caches, main memory has continued to grow slower relative to processor cycle times. The time to service a cache miss to memory has grown from 6 cycles for the Vax 11/780 to 120 for the AlphaServer 8400 <ref> [3, 6] </ref>. Cache miss penalties have grown to the point where good overall performance cannot be achieved without good cache performance.
Reference: [7] <author> Robert W. Floyd. </author> <title> Treesort 3. </title> <journal> Communications of the ACM, </journal> <volume> 7(12):701, </volume> <year> 1964. </year>
Reference-contexts: In 1965 Floyd proposed an improved technique for building a heap with better average case performance and a worst case of O (n) steps <ref> [7] </ref>. Williams' base algorithm with Floyd's improvement is still the most prevalent heapsort variant in use. 6.1 Base Algorithm As a base heapsort algorithm, we follow the recommendations of algorithm textbooks and use an array implementation of a binary heap constructed using Floyd's method.
Reference: [8] <author> E. H. </author> <title> Friend. </title> <journal> Journal of the ACM, </journal> <volume> 3:152, </volume> <year> 1956. </year>
Reference-contexts: The first pass accumulates counts of the number of keys with each radix. The counts are used to determine the offsets in the keys of each radix in the destination array. The second pass moves the source array to the destination array according to the offsets. Friend <ref> [8] </ref> suggested an improvement to reduce the number of passes over the source array, by accumulating the counts for the (i + 1)-st iteration at the same time as moving keys during the i-th iteration. This requires a second count array of size 2 r .
Reference: [9] <author> G. Gonnet and J. Munro. </author> <title> Heaps on heaps. </title> <journal> SIAM Journal of Computing, </journal> <volume> 15(4) </volume> <pages> 964-971, </pages> <year> 1986. </year>
Reference-contexts: In addition, we employ 11 a standard optimization of using a sentinel at the end of the heap to eliminate a comparison per level which reduces instruction count. The literature contains a number of other optimizations that reduce the number of comparisons performed for both adds and removes <ref> [4, 2, 9] </ref>, but in practice these increase the total number of instructions executed and do not improve performance. For this reason, we do not include them in our base heapsort. 6.2 Memory Optimizations To this base heapsort algorithm, we now apply memory optimizations in order to further improve performance.
Reference: [10] <author> J. Hennesey and D. Patterson. </author> <title> Computer Architecture A Quantitative Approach, Second Edition. </title> <publisher> Mor-gan Kaufman Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1996. </year>
Reference-contexts: Accordingly, our design techniques will attempt to improve both the temporal and spatial locality of the sorting algorithms. Cache misses are often categorized into compulsory, capacity, and conflict misses <ref> [10] </ref>. Compulsory misses are those that occur when a block is first accessed and brought into the cache. Capacity misses are those caused by the fact that more blocks are accessed than can fit all at one time in the cache. <p> Throughout this study we have assumed that a block of contiguous pages in the virtual address space map to a block of contiguous pages in the cache. This is only guaranteed to be true when caches are virtually indexed rather than physically indexed <ref> [10] </ref>. Unfortunately, the caches on all five of our test machines are physically indexed.
Reference: [11] <author> C. A. R. Hoare. </author> <title> Quicksort. </title> <journal> Computer Journal, </journal> <volume> 5 </volume> <pages> 10-15, </pages> <year> 1962. </year>
Reference-contexts: Sorting is a fundamental task and hundreds of sorting algorithms have been developed. In this paper we explore the potential performance gains that cache-conscious design offers in understanding and improving the performance of four popular sorting algorithms: mergesort 1 , quicksort <ref> [11] </ref>, heapsort [24], and fl A preliminary version of this paper appears in Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, 370-379, 1997. 1 Knuth [15] traces mergesort back to card sorting machines of the 1930s. 1 2 radix sort 2 . <p> This number is negligible for any practical values of n, B, and C. 5 Quicksort Quicksort is an in-place divide-and-conquer sorting algorithm considered by most to be the fastest comparison-based sorting algorithm when the set of keys fit in memory <ref> [11] </ref>. In quicksort, a key from the set is chosen as the pivot, and all other keys in the set are partitioned around this pivot.
Reference: [12] <author> F. E. Holberton. </author> <booktitle> In Symosium on Automatic Programming, </booktitle> <pages> pages 34-39, </pages> <year> 1952. </year>
Reference-contexts: Algorithms which sort in this manner are known as mergesort algorithms, and there are both recursive and iterative variants <ref> [12, 15] </ref>. 4.1 Base Algorithm For a base algorithm, we chose an iterative mergesort [15] since it is both easy to implement and is very amenable to traditional optimization techniques.
Reference: [13] <author> Li Hui and K. C. Sevcik. </author> <title> Parallel sorting by overpartitioning. </title> <booktitle> In Proceedings of the 6th ACM Symposium on Parallel Algorithms & Architecture, </booktitle> <pages> pages 46-56, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: To fix this inefficiency, a single multi-partition pass can be used to divide the full set into a number of subsets which are likely to be cache sized or smaller. Multi-partitioning is used in parallel sorting algorithms to divide a set into subsets for multiple processors <ref> [1, 13] </ref> in order to quickly balance the load. We choose the number of pivots so that the number of subsets larger than the cache is small with sufficiently high probability.
Reference: [14] <author> D. B. Johnson. </author> <title> Priority queues with update and finding minimum spanning trees. </title> <journal> Information Processing Letters, </journal> <volume> 4, </volume> <year> 1975. </year>
Reference-contexts: In addition we have shown that two other optimizations reduce the number of cache misses incurred by the remove-min operation [17, 16]. The first optimization is to replace the traditional binary heap with a d-heap <ref> [14] </ref> where each non-leaf node has d children instead of two. The fanout d is chosen so that exactly d keys fit in a cache block.
Reference: [15] <author> D. E. Knuth. </author> <title> The Art of Computer Programming, vol III Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: the potential performance gains that cache-conscious design offers in understanding and improving the performance of four popular sorting algorithms: mergesort 1 , quicksort [11], heapsort [24], and fl A preliminary version of this paper appears in Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, 370-379, 1997. 1 Knuth <ref> [15] </ref> traces mergesort back to card sorting machines of the 1930s. 1 2 radix sort 2 . Mergesort, quicksort, and heapsort are all comparison based sorting algorithms while radix sort is not. <p> Improving an algorithm's overall performance may require increasing the number of instructions executed while, at the same time, reducing the number of cache misses. Consequently, cache-conscious design of algorithms is required to achieve the best performance. 2 Knuth <ref> [15] </ref> traces the radix sorting method to the Hollerith sorting machine that was first used to assist the 1890 United States census. 3 2 Caches In order to speed up memory accesses, small high speed memories called caches are placed between the processor and the main memory. <p> Algorithms which sort in this manner are known as mergesort algorithms, and there are both recursive and iterative variants <ref> [12, 15] </ref>. 4.1 Base Algorithm For a base algorithm, we chose an iterative mergesort [15] since it is both easy to implement and is very amenable to traditional optimization techniques. <p> Algorithms which sort in this manner are known as mergesort algorithms, and there are both recursive and iterative variants [12, 15]. 4.1 Base Algorithm For a base algorithm, we chose an iterative mergesort <ref> [15] </ref> since it is both easy to implement and is very amenable to traditional optimization techniques. The standard iterative mergesort makes 5 dlog 2 ne passes over the array, where the i-th pass merges sorted subarrays of length 2 i1 into sorted subarrays of length 2 i . <p> To fix this inefficiency in the second phase, we employ a multi-way merge similar to those used in external sorting (Knuth devotes a section of his book to techniques for multi-merging <ref> [15, Sec. 5.4.1] </ref>). In multi-mergesort we replace the final dlog 2 (n=(BC=2))e merge passes of tiled mergesort with a single pass that merges all of the pieces together at once. This single pass makes use of a memory-optimized heap to hold the heads of the lists being multi-merged [16]. <p> The wobble in the instruction count curve for the base mergesort is due to the final copy that may need to take place depending on whether the final merge wrote into the source array or the auxiliary array <ref> [15] </ref>. When the set size is smaller than the cache, the multi-mergesort executes the same number of instructions as the tiled mergesort. Beyond that size, the multimerge is performed and this graph shows the increase it causes in the 6 instruction count. <p> We then have the recurrence M (n) = B 1 n1 X (M (i) + M (n i 1)) if n &gt; BC and M (n) = 0 if n BC. Using standard techniques <ref> [15] </ref> this recurrence solves to M (n) = B n + 1 ) + O ( n for n &gt; BC. The first correction we make is undercounting the misses that are incurred when the subproblem first reaches size BC. <p> The predictions match the simulation results surprisingly well considering the simplifying assumptions made in the analysis. 7 Radix Sort Radix sort is the most important non-comparison based sorting algorithm used today. Knuth <ref> [15] </ref> traces the radix sort suitable for sorting in the main memory of a computer to a Master's thesis of Seward, 1954 [21].
Reference: [16] <author> A. LaMarca. </author> <title> Caches and algorithms. </title> <type> Ph.D. Dissertation, </type> <institution> University of Washington, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: In multi-mergesort we replace the final dlog 2 (n=(BC=2))e merge passes of tiled mergesort with a single pass that merges all of the pieces together at once. This single pass makes use of a memory-optimized heap to hold the heads of the lists being multi-merged <ref> [16] </ref>. The multi-merge introduces several complications to the algorithm and significantly increases the dynamic instruction count. <p> For this reason, we do not include them in our base heapsort. 6.2 Memory Optimizations To this base heapsort algorithm, we now apply memory optimizations in order to further improve performance. Our previous results <ref> [17, 16] </ref> show that William's repeated-adds algorithm [24] for building a binary heap incurs fewer cache misses than Floyd's method. In addition we have shown that two other optimizations reduce the number of cache misses incurred by the remove-min operation [17, 16]. <p> Our previous results <ref> [17, 16] </ref> show that William's repeated-adds algorithm [24] for building a binary heap incurs fewer cache misses than Floyd's method. In addition we have shown that two other optimizations reduce the number of cache misses incurred by the remove-min operation [17, 16]. The first optimization is to replace the traditional binary heap with a d-heap [14] where each non-leaf node has d children instead of two. The fanout d is chosen so that exactly d keys fit in a cache block.
Reference: [17] <author> A. LaMarca and R. E. Ladner. </author> <title> The influence of caches on the performance of heaps. </title> <journal> Journal of Experimental Algorithmics, </journal> <volume> Vol 1, Article 4, </volume> <year> 1996. </year>
Reference-contexts: For this reason, we do not include them in our base heapsort. 6.2 Memory Optimizations To this base heapsort algorithm, we now apply memory optimizations in order to further improve performance. Our previous results <ref> [17, 16] </ref> show that William's repeated-adds algorithm [24] for building a binary heap incurs fewer cache misses than Floyd's method. In addition we have shown that two other optimizations reduce the number of cache misses incurred by the remove-min operation [17, 16]. <p> Our previous results <ref> [17, 16] </ref> show that William's repeated-adds algorithm [24] for building a binary heap incurs fewer cache misses than Floyd's method. In addition we have shown that two other optimizations reduce the number of cache misses incurred by the remove-min operation [17, 16]. The first optimization is to replace the traditional binary heap with a d-heap [14] where each non-leaf node has d children instead of two. The fanout d is chosen so that exactly d keys fit in a cache block. <p> For 4,096,000 keys, the memory-tuned heapsort sorts 81% faster than the base heapsort. 6.4 Analysis For n BC heapsort, as an in-place sorting algorithm takes 1=B misses per key. For n &gt; BC we adopt an analysis technique, collective analysis that we used in a previous paper <ref> [17] </ref>. Collective analysis is an analytical framework for predicting the cache performance of algorithms when the algorithm's memory access behavior can be approximated using independent stochastic processes. As part of the analysis, the cache is divided into regions that are assumed to be accessed uniformly.
Reference: [18] <author> A. Lebeck and D. Wood. </author> <title> Cache profiling and the spec benchmarks: a case study. </title> <journal> Computer, </journal> <volume> 27(10) </volume> <pages> 15-26, </pages> <month> Oct </month> <year> 1994. </year>
Reference-contexts: A program exhibits spatial locality if there is good chance that subsequently accessed data items are located near each other in memory. Most programs tend to exhibit both kinds of locality and typical hit ratios are greater than 90% <ref> [18] </ref>. With a 90% hit ratio, cutting the number of cache misses in half has the effect of raising hit ratio to 95%. <p> The second optimization is to align the heap array in memory so that all d children lie on the same cache block. This optimization reduces what Lebeck and Wood refer to as alignment misses <ref> [18] </ref>. Our memory-optimized heapsort dynamically chooses between Williams' repeated-adds method and Floyd's method for building a heap. If the heap is larger than the cache and Williams' method can offer a reduction in cache misses, it is chosen over Floyd's method.
Reference: [19] <author> C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. Alphasort: </author> <title> a RISC machine sort. </title> <booktitle> In 1994 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 233-242, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Interestingly, many of the design techniques are not particularly new. Some have already been used in optimizing compilers, in algorithms which use external storage devices, and in parallel algorithms. Similar techniques have also been used successfully in the development of the cache-efficient Alphasort algorithm <ref> [19] </ref>. As mentioned earlier we focus on three measures of performance: instruction count, cache misses, and overall performance in terms of execution time. All of the dynamic instruction counts and cache simulation results were measured using Atom [22].
Reference: [20] <author> R. Sedgewick. </author> <title> Implementing quicksort programs. </title> <journal> Communications of the ACM, </journal> <volume> 21(10) </volume> <pages> 847-857, </pages> <month> October </month> <year> 1978. </year>
Reference-contexts: The simple recursive quicksort is a simple, elegant algorithm and can be expressed in less than twenty lines of code. 5.1 Base Algorithm An excellent study of fast implementations of quicksort was conducted by Sedgewick, and we use the optimized quicksort he develops as our base algorithm <ref> [20] </ref>. Among the optimizations that Sedgewick suggests is one that sorts small subsets using a faster sorting method [20]. <p> than twenty lines of code. 5.1 Base Algorithm An excellent study of fast implementations of quicksort was conducted by Sedgewick, and we use the optimized quicksort he develops as our base algorithm <ref> [20] </ref>. Among the optimizations that Sedgewick suggests is one that sorts small subsets using a faster sorting method [20]. He suggests that, rather than sorting these in the natural course of the quicksort recursion, all the small unsorted subarrays be left unsorted until the very end, at which time they are sorted using insertion sort in a single final pass over the entire array.
Reference: [21] <author> H. H. Seward. </author> <type> Masters Thesis, </type> <institution> M.I.T. Digital Computer Laboratory Report R-232, </institution> <year> 1954. </year> <month> 18 </month>
Reference-contexts: Knuth [15] traces the radix sort suitable for sorting in the main memory of a computer to a Master's thesis of Seward, 1954 <ref> [21] </ref>.
Reference: [22] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the 1994 ACM Symposium on Programming Languages Design and Implementation, </booktitle> <pages> pages 196-205. </pages> <publisher> ACM, </publisher> <year> 1994. </year>
Reference-contexts: As mentioned earlier we focus on three measures of performance: instruction count, cache misses, and overall performance in terms of execution time. All of the dynamic instruction counts and cache simulation results were measured using Atom <ref> [22] </ref>. Atom is a toolkit developed by DEC for instrumenting program executables on Alpha workstations. Dynamic instruction counts are obtained by inserting an increment to a counter after each instruction executed by the algorithm.
Reference: [23] <author> G. Taylor, P. Davies, and M. Farmwald. </author> <title> The TBL slice:a low-cost high-speed address translation mechanism. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 355-363, </pages> <year> 1990. </year>
Reference-contexts: Fortunately, some operating systems, such as Digital Unix, have virtual to physical page mapping polices that attempt to map pages so that blocks of memory nearby in the virtual address space do not conflict in the cache <ref> [23] </ref>. Unlike the heapsort algorithms, tiled mergesort relies heavily on the assumption that a cache-sized block of pages do not conflict in the cache. As a result, the speedup of tiled mergesort relies heavily on the quality of the operating system's page mapping decisions.
Reference: [24] <author> J. W. Williams. </author> <title> Heapsort. </title> <journal> Communications of the ACM, </journal> <volume> 7(6) </volume> <pages> 347-348, </pages> <year> 1964. </year>
Reference-contexts: Sorting is a fundamental task and hundreds of sorting algorithms have been developed. In this paper we explore the potential performance gains that cache-conscious design offers in understanding and improving the performance of four popular sorting algorithms: mergesort 1 , quicksort [11], heapsort <ref> [24] </ref>, and fl A preliminary version of this paper appears in Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, 370-379, 1997. 1 Knuth [15] traces mergesort back to card sorting machines of the 1930s. 1 2 radix sort 2 . <p> The total is approximately 4 (5) misses per key which closely matches the misses reported in Figure 2. 6 Heapsort While heaps are used for a variety of purposes, they were first proposed by Williams as part of the heapsort algorithm <ref> [24] </ref>. The heapsort algorithm sorts by first building a heap containing all of the keys and then removing them one at a time in sorted order. Using an array implementation of a heap results in an straightforward in-place sorting algorithm. <p> For this reason, we do not include them in our base heapsort. 6.2 Memory Optimizations To this base heapsort algorithm, we now apply memory optimizations in order to further improve performance. Our previous results [17, 16] show that William's repeated-adds algorithm <ref> [24] </ref> for building a binary heap incurs fewer cache misses than Floyd's method. In addition we have shown that two other optimizations reduce the number of cache misses incurred by the remove-min operation [17, 16]. <p> Our heapsort algorithm goes through two phases: the build-heap phase and the remove phase. For the build-heap phase, recall that William's method for building the heap simply puts each key at the bottom of the heap and percolates it up to its proper place <ref> [24] </ref>. We pessimistically assume that all of these adds percolate to the root and that only the most recent leaf-to-root path is in the cache.

References-found: 24

