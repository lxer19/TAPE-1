URL: http://iacoma.cs.uiuc.edu/iacoma-papers/cedarnet.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Title: The Performance of the Cedar Multistage Switching Network  
Author: Josep Torrellas and Zheng Zhang 
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: While multistage switching networks for vector multiprocessors have been studied extensively, detailed evaluations of their performance are rare. Indeed, analytical models, simulations with pseudo-synthetic loads, studies focused on average-value parameters, and measurements of networks disconnected from the machine, all provide limited information. In this paper, instead, we present an in-depth empirical analysis of a multistage switching network in a realistic setting: we use hardware probes to examine the performance of the omega network of the Cedar shared-memory machine executing real applications. The machine is configured with 16 vector processors. The analysis suggests that the performance of multistage switching networks is limited by traffic non-uniformities. We identify two major non-uniformities that degrade Cedar's performance and are likely to slow down other networks too. The first one is the contention caused by the return messages in a vector access as they converge from the memories to one processor port. This traffic convergence penalizes vector reads and, more importantly, causes tree saturation. The second non-uniformity is the uneven contention delays induced by even a relatively fair scheme to resolve message collisions. Based on our observations, we argue that intuitive optimizations for multistage switching networks may not be cost-effective. Instead, we suggest changes to increase the network bandwidth at the root of the traffic convergence tree and to delay traffic convergence up until the final stages of the network. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. B. Andrews. </author> <title> A Hardware Tracing Facility for a Multiprocessing Supercomputer. </title> <type> Technical Report 1009, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: Note that link connections are rotated so that, in most cases, a given processor-processor path is given different priorities in different switches. This makes the network more fair. 2.2 Experimental Setup To perform our measurements, we use a trace-collecting hardware performance monitor <ref> [1] </ref>. The monitor has 16 trace buffers that can store over a million entries each. Each entry contains the address referenced, the time stamp, the type of reference (vector/scalar read or write, or test&set), and other information for a total of 64 bits.
Reference: [2] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Time Total Exec. Time Network Total Exec. Time Latency (%) (%) (%) (Cycles) (%) Flo52 22.4 13.5 11.7 17.2 18.7 Trfd 17.6 24.3 20.0 18.3 23.3 Ocean 14.6 19.4 12.7 21.5 35.0 Bdna 10.3 19.2 12.0 20.5 31.8 Club benchmarks <ref> [2] </ref> running on 16 processors. The versions used are highly vectorized and are close to the best that a parallelizing compiler can perform [3].
Reference: [3] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> The Cedar Fortran Project. </title> <type> Technical Report 1262, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The versions used are highly vectorized and are close to the best that a parallelizing compiler can perform <ref> [3] </ref>. All the data are allocated in Cedar's shared memory, except for private loop index variables and other local variables, which are allocated in a private, faster memory closer to the processors. Practical reasons limit the length of the code section that we can trace.
Reference: [4] <author> A. Gottlieb, R. Grishman, C. Kruskal, K. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer Designing an MIMD Shared Memory Parallel Computer. </title> <journal> In IEEE Trans. on Computers, </journal> <pages> pages 175-189, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: It is known that the performance of computer systems is determined by bursty activities that are difficult to predict without detailed empirical observations. While networks are no exception to this rule, measuring such systems is costly and difficult. Therefore, many studies have been based on analytical models <ref> [4, 6, 11] </ref> or on simulations [5, 14, 15, 17, 18], usually using pseudo-synthetic loads. While these methods supply useful information, they do not provide the complete picture. <p> The conclusion, therefore, is that under this bursty traffic, it is worth having a completely fair collision resolution scheme. 6 Related Work Many authors have used analytical models to predict the performance of multistage switching networks <ref> [4, 6, 11] </ref>. Turner and Veidenbaum [18] used simulations to study vector machines of up to 512 CPUs, focusing on average-value parameters like speedup, efficiency, or latency. Turner [17] simulated architectural variations like different memory speeds or amount of buffering. <p> Smith and Taylor [14] pointed out that non-blocking buffers substan tially increase performance. Our results agree. Smith and Taylor [15] further discussed how vector disorder degrades performance. In this paper, we showed that, under bursty traffic, relatively fair switches create such disorder. Got-tlieb et al. <ref> [4] </ref> described a network that supports combining. Pfister and Norton [12] analyzed memory hot spots and tree saturation in the forward network. Noakes et al. [10] presented measurements of average-value parameters in a real machine. We use a hardware monitor to go beyond averages and record irregularities.
Reference: [5] <author> E. D. Granston, S. W. Turner, and A. V. Veiden-baum. </author> <title> Design and Analysis of a Scalable Shared-memory System with Support for Burst Traffic. 16th ISCA Workshop on Cache and Interconnect Architectures in Multiprocessors, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: While networks are no exception to this rule, measuring such systems is costly and difficult. Therefore, many studies have been based on analytical models [4, 6, 11] or on simulations <ref> [5, 14, 15, 17, 18] </ref>, usually using pseudo-synthetic loads. While these methods supply useful information, they do not provide the complete picture. <p> Pfister and Norton [12] analyzed memory hot spots and tree saturation in the forward network. Noakes et al. [10] presented measurements of average-value parameters in a real machine. We use a hardware monitor to go beyond averages and record irregularities. Based on analytical models, Granston et al. <ref> [5] </ref> and McAuliffe [9] point out that the fan-in of read bursts causes performance degradation in the reverse network. However, no detailed characterization or solutions are provided. 7 Conclusion A realistic analysis of a multistage switching network suggests that the performance of these networks is limited by traffic non-uniformities.
Reference: [6] <author> C. Kruskal and M. Snir. </author> <title> The Performance of Multistage Interconnection Networks for Multiprocessors. </title> <journal> In IEEE Trans. on Computers, </journal> <pages> pages 1091-98, </pages> <month> De-cember </month> <year> 1983. </year>
Reference-contexts: It is known that the performance of computer systems is determined by bursty activities that are difficult to predict without detailed empirical observations. While networks are no exception to this rule, measuring such systems is costly and difficult. Therefore, many studies have been based on analytical models <ref> [4, 6, 11] </ref> or on simulations [5, 14, 15, 17, 18], usually using pseudo-synthetic loads. While these methods supply useful information, they do not provide the complete picture. <p> The conclusion, therefore, is that under this bursty traffic, it is worth having a completely fair collision resolution scheme. 6 Related Work Many authors have used analytical models to predict the performance of multistage switching networks <ref> [4, 6, 11] </ref>. Turner and Veidenbaum [18] used simulations to study vector machines of up to 512 CPUs, focusing on average-value parameters like speedup, efficiency, or latency. Turner [17] simulated architectural variations like different memory speeds or amount of buffering.
Reference: [7] <author> D. Kuck et al. </author> <title> The Cedar System and an Initial Performance Study. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 213-224, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In this paper, we present an in-depth empirical analysis of a multistage switching network in a vector multiprocessor. We use hardware probes to monitor the omega network [8] of the Cedar shared-memory machine <ref> [7] </ref> executing real applications. The machine is configured with 16 CPUs. We examine each individual queue, switch element, and link in the network. The analysis suggests that the performance of multistage switching networks is limited by traffic non-uniformities. <p> Cedar is a shared-memory multiprocessor developed at the Center for Supercomputing Research and Development, University of Illinois <ref> [7] </ref>. The machine has 32 Alliant FX/8 vector processors. Limited performance monitoring hardware, however, forces us to disable 16 of them. Processors do not stall on writes; however, they stall with two pending scalar reads. Messages in a vector access are pipelined.
Reference: [8] <author> D. H. Lawrie. </author> <title> Access and Alignment of Data in an Array Processor. </title> <journal> In IEEE Transactions on Computers, </journal> <pages> pages 1145-1155, </pages> <month> December </month> <year> 1975. </year>
Reference-contexts: In this paper, we present an in-depth empirical analysis of a multistage switching network in a vector multiprocessor. We use hardware probes to monitor the omega network <ref> [8] </ref> of the Cedar shared-memory machine [7] executing real applications. The machine is configured with 16 CPUs. We examine each individual queue, switch element, and link in the network. The analysis suggests that the performance of multistage switching networks is limited by traffic non-uniformities. <p> All processors share 64 Mbytes of memory organized in 32 memory modules interleaved on a 64-bit word basis. In addition, processors have local memory where they store private data like loop index variables. The processors and memories are connected via a forward and a reverse omega network <ref> [8] </ref> (Figure 1). The cycle time of the networks is 85 ns. For our system, the total bandwidth of the processors and memo ries is equal: each processor can issue one read message to the network every two cycles, while memory modules have a 4-cycle cycle time.
Reference: [9] <author> K. McAuliffe. </author> <title> Analysis of Cache Memories in Highly Parallel Systems. </title> <type> Technical Report 269, </type> <institution> Department of Computer Science, </institution> <address> New York University, </address> <month> May </month> <year> 1986. </year>
Reference-contexts: Noakes et al. [10] presented measurements of average-value parameters in a real machine. We use a hardware monitor to go beyond averages and record irregularities. Based on analytical models, Granston et al. [5] and McAuliffe <ref> [9] </ref> point out that the fan-in of read bursts causes performance degradation in the reverse network. However, no detailed characterization or solutions are provided. 7 Conclusion A realistic analysis of a multistage switching network suggests that the performance of these networks is limited by traffic non-uniformities.
Reference: [10] <author> M. Noakes, D. Wallach, and W. Dally. </author> <title> The J-Machine Multicomputer: An Architectural Evaluation. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 224-235, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: by the National Science Foundation under grants NSF RIA MIP 93-08098, NSF MIP 93-07910, and NSF MIP 89-20891; a NASA ICLASS grant; and grant 1-1-28028 from the Univ. of Illinois Research Board. instead, have measured real networks while focusing on high-level average-value parameters like round trip time, bandwidth, or utilization <ref> [10] </ref>. Some other studies have been performed with the network disconnected from the machine. Again, the information provided is not complete. As a result, researchers still lack a complete picture of how multistage switching networks perform in real systems and how to compare their performance [13]. <p> All these numbers are average values; peak values can be much larger. In the past, it has been known that completely unfair fixed-priority schemes degrade performance. For example, Noakes et al. pointed out the back pressure on the low-priority links of the J-machine <ref> [10] </ref>. <p> In this paper, we showed that, under bursty traffic, relatively fair switches create such disorder. Got-tlieb et al. [4] described a network that supports combining. Pfister and Norton [12] analyzed memory hot spots and tree saturation in the forward network. Noakes et al. <ref> [10] </ref> presented measurements of average-value parameters in a real machine. We use a hardware monitor to go beyond averages and record irregularities. Based on analytical models, Granston et al. [5] and McAuliffe [9] point out that the fan-in of read bursts causes performance degradation in the reverse network.
Reference: [11] <author> J. H. Patel. </author> <title> Performance of Processor-Memory Interconnections for Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 30(10) </volume> <pages> 771-780, </pages> <month> October </month> <year> 1981. </year>
Reference-contexts: It is known that the performance of computer systems is determined by bursty activities that are difficult to predict without detailed empirical observations. While networks are no exception to this rule, measuring such systems is costly and difficult. Therefore, many studies have been based on analytical models <ref> [4, 6, 11] </ref> or on simulations [5, 14, 15, 17, 18], usually using pseudo-synthetic loads. While these methods supply useful information, they do not provide the complete picture. <p> The conclusion, therefore, is that under this bursty traffic, it is worth having a completely fair collision resolution scheme. 6 Related Work Many authors have used analytical models to predict the performance of multistage switching networks <ref> [4, 6, 11] </ref>. Turner and Veidenbaum [18] used simulations to study vector machines of up to 512 CPUs, focusing on average-value parameters like speedup, efficiency, or latency. Turner [17] simulated architectural variations like different memory speeds or amount of buffering.
Reference: [12] <author> G. Pfister and A. Norton. </author> <title> 'Hot Spot' Contention and Combining in Multistage Interconnection Networks. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 790-797, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Indeed, it causes the blocking cycles (Confl Block and Que1 Block) and a fraction of the remaining queuing cycles approximately equal to Confl Block/(all conflict cycles). Overall, the saturation effect that we measured is similar to the one caused by a hot spot <ref> [12] </ref>. However, the solution often proposed for hot spots, namely message combining, does not work for this kind of saturation. 4.1.2 Vector Reads Suffer the Most from the Traffic Convergence A second consequence of the traffic convergence problem is that vector reads are slowed down relative to other messages. <p> This selective slow down of messages several stages before an unfair switch we call ripple saturation because it looks like a propagating wave. Before finishing the discussion on the network's performance problems, we consider memory hot spots <ref> [12] </ref>. In our system, we did not find hot spots for three reasons. First, scheduling overheads and random delays prevent processors in a parallel Do loop from accessing mem ory in lockstep. <p> Our results agree. Smith and Taylor [15] further discussed how vector disorder degrades performance. In this paper, we showed that, under bursty traffic, relatively fair switches create such disorder. Got-tlieb et al. [4] described a network that supports combining. Pfister and Norton <ref> [12] </ref> analyzed memory hot spots and tree saturation in the forward network. Noakes et al. [10] presented measurements of average-value parameters in a real machine. We use a hardware monitor to go beyond averages and record irregularities.
Reference: [13] <author> Panel Session. </author> <title> Benchmarking Interconnects. In Hot Interconnects I Symposium, </title> <month> August </month> <year> 1993. </year>
Reference-contexts: Some other studies have been performed with the network disconnected from the machine. Again, the information provided is not complete. As a result, researchers still lack a complete picture of how multistage switching networks perform in real systems and how to compare their performance <ref> [13] </ref>. In this paper, we present an in-depth empirical analysis of a multistage switching network in a vector multiprocessor. We use hardware probes to monitor the omega network [8] of the Cedar shared-memory machine [7] executing real applications. The machine is configured with 16 CPUs.
Reference: [14] <author> J. E. Smith and W. R. Taylor. </author> <title> Accurate Modeling of Interconnection Networks in Vector Supercomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <pages> pages 264-273, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: While networks are no exception to this rule, measuring such systems is costly and difficult. Therefore, many studies have been based on analytical models [4, 6, 11] or on simulations <ref> [5, 14, 15, 17, 18] </ref>, usually using pseudo-synthetic loads. While these methods supply useful information, they do not provide the complete picture. <p> For the first optimization, we use a switch proposed in <ref> [14] </ref>. Each switch input is fanned out to as many input buffers as there are outputs in the switch (Figure 8-(a)). Therefore, switches in S3 and S4 have 8 and 4 2-entry buffers per input respectively. <p> Hence, any 4-element stride-one vector access accesses memories sharing no switch in S3. This address assignment is similar to Cray Y/MP's <ref> [14] </ref> and has negligible cost. The processor stall time with this new interleaving and the optimized network interface discussed above is shown in the Av+Interl bars of Figure 7 (Av+Interl stands for available with new interleaving). We see that the new interleaving makes a difference. <p> Turner [17] simulated architectural variations like different memory speeds or amount of buffering. Partly because of the relatively low memory bandwidth of his system, the best optimization was large memory input buffers. Smith and Taylor <ref> [14] </ref> pointed out that non-blocking buffers substan tially increase performance. Our results agree. Smith and Taylor [15] further discussed how vector disorder degrades performance. In this paper, we showed that, under bursty traffic, relatively fair switches create such disorder. Got-tlieb et al. [4] described a network that supports combining.
Reference: [15] <author> J. E. Smith and W. R. Taylor. </author> <title> Characterizing Memory Performance in Vector Multiprocessors. </title> <booktitle> In Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 35-44, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: While networks are no exception to this rule, measuring such systems is costly and difficult. Therefore, many studies have been based on analytical models [4, 6, 11] or on simulations <ref> [5, 14, 15, 17, 18] </ref>, usually using pseudo-synthetic loads. While these methods supply useful information, they do not provide the complete picture. <p> Partly because of the relatively low memory bandwidth of his system, the best optimization was large memory input buffers. Smith and Taylor [14] pointed out that non-blocking buffers substan tially increase performance. Our results agree. Smith and Taylor <ref> [15] </ref> further discussed how vector disorder degrades performance. In this paper, we showed that, under bursty traffic, relatively fair switches create such disorder. Got-tlieb et al. [4] described a network that supports combining. Pfister and Norton [12] analyzed memory hot spots and tree saturation in the forward network.
Reference: [16] <author> J. Torrellas and Z. Zhang. </author> <title> The Performance of the Cedar Multistage Switching Network. </title> <type> Technical report, </type> <institution> Center for Supercomputing Research and Development, </institution> <year> 1994. </year>
Reference-contexts: While neither of these two properties is necessarily present in all networks, they are not uncommon in existing designs. Our experiments show that contention does delay a message in a vector read 1.6 cycles more or 70% more than one in a vector write <ref> [16] </ref>. This is unfortunate, given that low read delays are more critical to performance than low write delays. 4.2 Collision Resolution Should Be Completely Fair The second non-uniformity that limits the performance of the Cedar network is the uneven distribution of contention delays observed in a given queue stage. <p> Memory hot spots, therefore, may be relatively rare for machines with few processors like ours. We present some evidence in <ref> [16] </ref>. 5 Optimizing the Performance of the Network While in the previous section we have exposed the performance problems of the Cedar network, in this section we optimize the network cost-effectively. The goal is to reduce the processor stall time induced by messages traversing the network and memory.
Reference: [17] <author> S. Turner. </author> <title> Shared Memory and Interconnection Network Performance for Vector Multiprocessors. M.S. </title> <type> thesis. Technical Report 876, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: While networks are no exception to this rule, measuring such systems is costly and difficult. Therefore, many studies have been based on analytical models [4, 6, 11] or on simulations <ref> [5, 14, 15, 17, 18] </ref>, usually using pseudo-synthetic loads. While these methods supply useful information, they do not provide the complete picture. <p> Turner and Veidenbaum [18] used simulations to study vector machines of up to 512 CPUs, focusing on average-value parameters like speedup, efficiency, or latency. Turner <ref> [17] </ref> simulated architectural variations like different memory speeds or amount of buffering. Partly because of the relatively low memory bandwidth of his system, the best optimization was large memory input buffers. Smith and Taylor [14] pointed out that non-blocking buffers substan tially increase performance. Our results agree.
Reference: [18] <author> S. Turner and A. Veidenbaum. </author> <title> Performance of a Shared-Memory System for Vector Multiprocessors. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 315-325, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: While networks are no exception to this rule, measuring such systems is costly and difficult. Therefore, many studies have been based on analytical models [4, 6, 11] or on simulations <ref> [5, 14, 15, 17, 18] </ref>, usually using pseudo-synthetic loads. While these methods supply useful information, they do not provide the complete picture. <p> The conclusion, therefore, is that under this bursty traffic, it is worth having a completely fair collision resolution scheme. 6 Related Work Many authors have used analytical models to predict the performance of multistage switching networks [4, 6, 11]. Turner and Veidenbaum <ref> [18] </ref> used simulations to study vector machines of up to 512 CPUs, focusing on average-value parameters like speedup, efficiency, or latency. Turner [17] simulated architectural variations like different memory speeds or amount of buffering.
Reference: [19] <author> C. Wu and T. Feng. </author> <title> Tutorial: Interconnection Networks for Parallel and Distributed Processing. </title> <booktitle> The Computer Society of the IEEE, </booktitle> <year> 1984. </year>
Reference-contexts: 1 Introduction Few topics in computer systems have attracted more attention than multistage switching networks for multiprocessors <ref> [19] </ref>. One reason is their impact on machine performance. Indeed, processor communication is a fundamental yet intrinsically expensive activity; networks should support it efficiently. A second reason is the many network variations that are interesting. Each of them satisfies different cost/performance requirements, ranging from those of busses to crossbars.
References-found: 19

