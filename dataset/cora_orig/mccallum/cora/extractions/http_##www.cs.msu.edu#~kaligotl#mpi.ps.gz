URL: http://www.cs.msu.edu/~kaligotl/mpi.ps.gz
Refering-URL: http://www.cs.msu.edu/~kaligotl/papers.html
Root-URL: http://www.cs.msu.edu
Title: Study of MPICH MPI Library Implementation  
Author: Chakrapani Kaligotla Instructor: Professor Lionel M. Ni 
Date: CPS 822 (Spring 1995):  May 5, 1995  
Affiliation: Department of Computer Science Michigan State University  Parallel Processing Computer Systems  
Abstract: Message-Passing Interface (MPI) is an emerging standard for parallel programming on distributed-memory parallel computing platforms. With an aim to efficiently implement MPI standard on a Network of Workstations connected by a high speed network like ATM and FDDI, we started looking for MPI implementations currently available in public domain which we can use as a starting point for our implementation. MPICH, an implementation of MPI standard by Argonne National Laboratory, was chosen because it was claimed to be complete with respect to the standard, with good performance. This paper presents the results of study and performance evaluation of the implementation of the point-to-point routines of MPICH library on a cluster of DEC Alpha workstations. It was found that derived datatypes were implemented very inefficiently. Latency for short messages was high due to high software overhead and throughput for long messages was low due to excessive memory allocation system calls and memory-to-memory copying. Implementing MPI over another existing message passing system like P4, though easy, was found to affect performance significantly. Several alternative approaches are identified to solve these performance related problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> MPI Forum, </author> <title> MPI : A Message-Passing Interface Standard, </title> <year> 1994. </year>
Reference-contexts: Together with a base address, a datatype specifies a communication buffer. General datatypes are built up hierarchically from simpler components. There are four basic constructors for datatypes, namely the contiguous, vector, indexed and structure constructors. Refer to <ref> [1] </ref> for more information on these four constructors of derived datatypes. 3 Architecture of the MPICH This section gives a layered overview of the MPICH design. Figure 1 (a) shows a complete layered protocol stack starting from the user application down to the communication hardware.
Reference: [2] <author> W. Gropp and E. Lusk, </author> <title> "MPICH ADI implementation reference manual,", </title> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> Jan. </month> <year> 1995. </year> <title> [3] "MPICH MPI source code and associated documentation," </title> <type> Version 1.0.8, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: Code using point-to-point routines. #endif Table 1 gives a list of currently defined entry points. The entry points with an asterisk are optional. If they are defined, MPI will use them, otherwise these routines are emulated in software by MPI. For information on these routines refer to <ref> [2] </ref>, [3].
Reference: [4] <author> W. Gropp and B. Smith, </author> <title> "Chameleon parallel programming tools users manual," </title> <type> Tech. Rep. </type> <institution> ANL-93/23, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: These locking primitives are used in a multi-threaded environment for maintaining the integrity of the message queues. The current design of ADI is still experimental and as the design is evolving more entry points are being added. Originally ADI was implemented over Chameleon <ref> [4] </ref> i.e, Chameleon was used as the com munication device. Only a small set of Chameleon calls were used for implementing the ADI. The ADI was implemented in such a way that it is portable to several popular message-passing systems (communication devices) like P4, PVM, INTEL NX and IBM's EUI.
Reference: [5] <author> R. Butler and E. Lusk, </author> <title> "User's guide to the p4 parallel programming system," </title> <type> Tech. Rep. </type> <institution> TM-ANL-92/17, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: The Chameleon ADI ported onto P4 is called ch p4. Table 2 lists the set of P4 routines used by ch p4. See <ref> [5] </ref> for more information on these routines. It is quite interesting to see that such a portable ADI can be implemented (though at the expense of performance, see section 6) using only a small set of calls provided by the underlying communication devices.
Reference: [6] <author> W. Gropp and E. Lusk, </author> <title> "An abstract device definition to support the implementation of a high-level point-to-point message-passing interface," </title> <type> Tech. Rep. </type> <institution> MCS-P342-1193, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: It is quite interesting to see that such a portable ADI can be implemented (though at the expense of performance, see section 6) using only a small set of calls provided by the underlying communication devices. For a detailed description of the design and implementation of ADI refer to <ref> [6] </ref>, [3].
Reference: [7] <author> N. Nupairoj and L. M. Ni, </author> <title> "Performance evaluation of some MPI implementations on workstation clusters," </title> <type> Tech. Rep. </type> <institution> MSU-CPS-ACS-94, Michigan State University, Department of Computer Science, </institution> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Performance of the blocking point-to-point MPI routines is discussed next. Some of the performance related problems with the current implementation are identified and alternative approaches for improving the performance are suggested. See <ref> [7] </ref> for a comparative performance evaluation of several MPI implementations available in public domain. 6.1 Benchmark Programs This section describes the programs used for benchmarking the performance of MPI point-to-point routines. 6.1.1 Ping The purpose of Ping benchmark is to measure the effective bandwidth of MPI by sending messages from one
Reference: [8] <author> H. Franke, </author> <title> "MPI-F : An MPI implementation for IBM SP-1/SP-2," </title> <type> Tech. Rep. Version 1.39, </type> <institution> IBM T. J. Watson Research Center, </institution> <month> Feb. </month> <year> 1995. </year> <month> 23 </month>
Reference-contexts: This evaluation computes the sequence of displacements specified by the datatype expression and gets (puts) simultaneously the elements at these displacements from (into) the communication buffer. MPI-F, the implementation of MPI standard on IBM SP-1/SP-2, uses the second approach <ref> [8] </ref>. 7 Conclusions and Future Work This study has analyzed in detail the algorithms used by MPICH for implementing point-to-point MPI routines. Benchmarks were run to identify performance bottlenecks. It was found that the current implementation of derived datatypes is very inefficient.
References-found: 7

