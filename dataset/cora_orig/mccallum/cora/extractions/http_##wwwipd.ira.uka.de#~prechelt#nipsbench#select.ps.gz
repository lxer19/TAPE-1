URL: http://wwwipd.ira.uka.de/~prechelt/nipsbench/select.ps.gz
Refering-URL: http://wwwipd.ira.uka.de/~prechelt/NIPS_bench.html
Root-URL: 
Title: Statistical Ideas for Selecting Network Architectures  
Author: B. D. Ripley 
Degree: Professor  
Address: Oxford, UK  
Affiliation: of Applied Statistics, University of Oxford  
Abstract: Choosing the architecture of a neural network is one of the most important problems in making neural networks practically useful, but accounts of applications usually sweep these details under the carpet. How many hidden units are needed? Should weight decay be used, and if so how much? What type of output units should be chosen? And so on. We address these issues within the framework of statistical theory for model This paper is principally concerned with architecture selection issues for feed-forward neural networks (also known as multi-layer perceptrons). Many of the same issues arise in selecting radial basis function networks, recurrent networks and more widely. These problems occur in a much wider context within statistics, and applied statisticians have been selecting and combining models for decades. Two recent discussions are [4, 5]. References [3, 20, 21, 22] discuss neural networks from a statistical perspective. choice, which provides a number of workable approximate answers.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Besag, J., Green, P., Higdon, D. and Mengersen, K. </author> <title> (1995) Bayesian computation and stochastic systems. </title> <journal> Statistical Science 1995. </journal>
Reference: [2] <author> Bishop, C. </author> <title> Improving the generalization properties of radial basis function neural networks. </title> <booktitle> Neural Computation 1991; 3 </booktitle> <pages> 579-588. </pages>
Reference-contexts: The two interact, as for moderate values of the number of hidden units has very little effect on the fitted function. (We might do better to use a direct regularization penalty, such as the R (f 00 ) 2 used in spline smoothing, see <ref> [2] </ref>.) How do we choose the degree of smoothness from the data alone? 2 Cross-validation, NIC and p eff Cross-validation ([27]) is perhaps the most widely used method to select architectural parameters, but the term is often mis-used in the neural networks field.
Reference: [3] <author> Cheng, B, and Titterington, D. M. </author> <title> Neural networks: a review from a statistical perspective (with discussion). </title> <booktitle> Statistical Science 1994; 9 </booktitle> <pages> 2-54. </pages>
Reference: [4] <author> Draper, D. </author> <title> Assessment and propagation of model uncertainty (with discussion). </title> <journal> Journal of the Royal Statistical Society series B 1995; 57 </journal> <pages> 45-97. </pages>
Reference: [5] <author> Gelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B. </author> <title> Bayesian Data Analysis. </title> <publisher> Chapman & Hall, </publisher> <address> New York, </address> <year> 1995. </year>
Reference: [6] <author> Geman, S., Bienenstock, E. and Doursat, R. </author> <title> Neural networks and the bias/variance dilemma. </title> <booktitle> Neural Computation 1992; 4 </booktitle> <pages> 1-58. </pages>
Reference-contexts: The effect of weight decay is to reduce the variability of the fit, at the cost of bias, since the fitted curve will be smoother than the true curve. Model selection is choosing a point on the bias/variance compromise (in the terminology of <ref> [6] </ref>). Thus it is surprising that the standard errors in Figure 1 (b) are tighter than those in Figure 3 (a). In fact the former are optimistic: the local linearization breaks down too quickly, and in any case does not take into account the other local minima.
Reference: [7] <author> Huber, P. J. </author> <title> The behavior of maximum likelihood estimates under nonstandard conditions. In: Le Cam, </title> <editor> L. M. and Neyman, J. </editor> <booktitle> (eds) Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability. </booktitle> <institution> University of California Press, Berkeley, </institution> <year> 1967, </year> <pages> 1 221-233 </pages>
Reference-contexts: E @ @ T and K = Var @ Here the averages are done under the true distribution, 0 is the `least false' parameter (the best possible fit to the true distribution) and g is the log density plus the regularization divided by n. (This follows from standard statistical theory; <ref> [7, 24] </ref>. Without regularization p fl = p.) These quantities can be estimated from the derivatives and the Hessian evaluated at the fit to the training set, replacing 0 by b and expectations by averages over the training set.
Reference: [8] <author> Jeffreys, H. </author> <title> Theory of Probability. Third edition. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1961. </year>
Reference-contexts: Only the two hidden-unit model shows bias. 3 Bayesian model selection In the Bayesian framework ([5]) it has been established for many years (at least since the 1948 edition of <ref> [8] </ref>) that the way to choose models is via the Bayes factor, (ratios of) the probability density of the data given the model. We have a prior density p () over the parameters (weights) which must be integrable.
Reference: [9] <author> Lincoln, W. P. and Skrzypek, J. </author> <title> Synergy of clustering multiple backpropagation networks. </title> <editor> In: Touretzky, D. S. </editor> <booktitle> (ed) Advances in Neural Information Processing Systems 2. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990, </year> <pages> pp. 650-657. </pages>
Reference-contexts: His suggestion was to take a convex combination of the predictions of several models, using cross-validation to choose the weights of the combination. Averaging (often unweighted) has often been proposed for neural networks (e.g. <ref> [9, 19] </ref>). There is little argument that model averaging is a good idea for predictions (and neural networks have almost no explanatory purpose). The main argument against is computational; for many pattern recognition problems it is difficult to get one network to predict fast enough, let alone many.
Reference: [10] <author> Madigan, D. and Raftery, A. E. </author> <title> Model selection and accounting for model uncertainty in graphical models using Occam's window. </title> <journal> Journal of the American Statistical Association 1994; 89 </journal> <pages> 1535-1546. </pages>
Reference-contexts: But if it is computationally feasible, it has been a long established practice in applied statistics to average over a few models. The Bayesian view averages over all models, even those which do not fit well, and this has been applied ([26]), but Madigan & Raftery in <ref> [10] </ref> advocate an Occam's window to exclude all but models with good fits, and to exclude models which subsume others that are included. Thus only the simplest models which fit well are included in their averaging process.
Reference: [11] <author> Madigan, D. and York, J. </author> <title> Bayesian graphical models for discrete data. </title> <type> Technical report 239, </type> <institution> Department of Statistics, University of Washington, </institution> <year> 1993. </year>
Reference-contexts: Markov chain Monte Carlo methods ([1]) have been used to explore the model space and so compute Bayes factors, for example in <ref> [26, 11] </ref>. 4 Model averaging Bayesians have long known that the logical endpoint of their theory is to average the predictions of their models, using the Bayes factors as weights, and this has become computationally feasible recently ([26, 10, 4]).
Reference: [12] <author> Moody, J. E. </author> <title> Note on generalization, regularization and architecture selection in nonlinear learning systems. </title> <booktitle> In First IEEE-SP Workshop on Neural Networks in Signal Processing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1991, </year> <pages> pp. 1-10. </pages>
Reference-contexts: The presence of local minima causes difficulties; we can attempt to track local minima by starting at a solution to the full problem ([14, 15]), but removing one outlying point can change dramatically the structure of the local minima. References <ref> [28, 12, 13, 16, 17, 18] </ref> take another approach, a generalization of Akaike's AIC criterion.
Reference: [13] <author> Moody, J. E. </author> <title> The effective number of parameters: an analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In: Moody, J. E., Hanson, S. J. and Lippmann, R. P. </editor> <booktitle> (eds) Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992, </year> <pages> pp. 847-854. </pages>
Reference-contexts: The presence of local minima causes difficulties; we can attempt to track local minima by starting at a solution to the full problem ([14, 15]), but removing one outlying point can change dramatically the structure of the local minima. References <ref> [28, 12, 13, 16, 17, 18] </ref> take another approach, a generalization of Akaike's AIC criterion.
Reference: [14] <author> Moody, J. and Utans, J. </author> <title> Principled architecture selection for neural networks: Application to corporate bond rating prediction. </title> <editor> In: Moody, J. E., Hanson, S. J. and Lippmann, R. P. </editor> <booktitle> (eds) Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992, </year> <pages> pp. 683-690. </pages>
Reference: [15] <author> Moody, J. and Utans, J. </author> <title> Architecture selection strategies for neural networks: Application to corporate bond rating prediction. In: Refenes, A.-P. (ed) Neural Networks in the Capital Markets. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1995, </year> <pages> pp. 277-300. </pages>
Reference: [16] <author> Murata, N., Yoshizawa, S. and Amari, S. </author> <title> A criterion for determining the number of parameters in an artificial neural network model. </title> <editor> In: Kohonen, T., Makisara, K., Simula, O. and Kangas, J. </editor> <booktitle> (eds) Artificial Neural Networks. </booktitle> <publisher> North Holland, </publisher> <address> Ams-terdam, </address> <year> 1991, </year> <pages> pp. 9-14. </pages>
Reference-contexts: The presence of local minima causes difficulties; we can attempt to track local minima by starting at a solution to the full problem ([14, 15]), but removing one outlying point can change dramatically the structure of the local minima. References <ref> [28, 12, 13, 16, 17, 18] </ref> take another approach, a generalization of Akaike's AIC criterion.
Reference: [17] <author> Murata, N., Yoshizawa, S. and Amari, S. </author> <title> Learning curves, model selection and complexity of neural networks. </title> <editor> In: Hanson, S. J., Cowan, J. D. and Giles, C. L. </editor> <booktitle> (eds) Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993, </year> <pages> pp. 607-614. </pages>
Reference-contexts: The presence of local minima causes difficulties; we can attempt to track local minima by starting at a solution to the full problem ([14, 15]), but removing one outlying point can change dramatically the structure of the local minima. References <ref> [28, 12, 13, 16, 17, 18] </ref> take another approach, a generalization of Akaike's AIC criterion.
Reference: [18] <author> Murata, N., Yoshizawa, S. and Amari, S. </author> <title> Network information criterion determining the number of hidden units for artificial neural network models. </title> <booktitle> IEEE Transactions on Neural Networks 1994; 5 </booktitle> <pages> 865-872. </pages>
Reference-contexts: The presence of local minima causes difficulties; we can attempt to track local minima by starting at a solution to the full problem ([14, 15]), but removing one outlying point can change dramatically the structure of the local minima. References <ref> [28, 12, 13, 16, 17, 18] </ref> take another approach, a generalization of Akaike's AIC criterion.
Reference: [19] <author> Perrone, M. P. and Cooper, L. N. </author> <title> When networks disagree: Ensemble methods for hybrid neural networks. </title> <editor> In: Mammone, R. J. </editor> <title> (ed) Artificial Neural Networks for Speech and Vision. </title> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1993, </year> <pages> pp. 126-142. </pages>
Reference-contexts: His suggestion was to take a convex combination of the predictions of several models, using cross-validation to choose the weights of the combination. Averaging (often unweighted) has often been proposed for neural networks (e.g. <ref> [9, 19] </ref>). There is little argument that model averaging is a good idea for predictions (and neural networks have almost no explanatory purpose). The main argument against is computational; for many pattern recognition problems it is difficult to get one network to predict fast enough, let alone many.
Reference: [20] <author> Ripley, B. D. </author> <title> Statistical aspects of neural networks. </title> <note> In Barndorff-Nielsen, </note> <author> O. E., Jensen, J. L. and Kendall, W. S. </author> <title> (eds) Networks and Chaos Statistical and Probabilistic Aspects. </title> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1993, </year> <pages> pp. 40-123 </pages>
Reference: [21] <author> Ripley, B. D. </author> <title> Neural networks and related methods for classification (with discussion). </title> <journal> Journal of the Royal Statistical Society series B 1994; 56 </journal> <pages> 409-456 </pages>
Reference: [22] <author> Ripley, B. D. </author> <title> Neural networks and flexible regression and discrimination. </title> <editor> In: Mardia, K. V. (ed) Statistics and Images. Carfax, Abingdon, </editor> <year> 1994, </year> <pages> pp. </pages> <booktitle> 39-57 (Advances in Applied Statistics 2). </booktitle>
Reference: [23] <author> Ripley, B. D. </author> <title> Flexible non-linear approaches to classification. </title> <note> In: </note> <author> Cherkassky, V., Friedman, J. H. and Wechsler, H. </author> <title> (eds) From Statistics to Neural Networks. Theory and Pattern Recognition Applications. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1994, </year> <pages> pp. 105-126. </pages>
Reference: [24] <author> Ripley, B. D. </author> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1995. </year>
Reference-contexts: E @ @ T and K = Var @ Here the averages are done under the true distribution, 0 is the `least false' parameter (the best possible fit to the true distribution) and g is the log density plus the regularization divided by n. (This follows from standard statistical theory; <ref> [7, 24] </ref>. Without regularization p fl = p.) These quantities can be estimated from the derivatives and the Hessian evaluated at the fit to the training set, replacing 0 by b and expectations by averages over the training set.
Reference: [25] <author> Seber, G. A. F. and Wild, C. J. </author> <title> Nonlinear Regression. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: The limits shown are 2 standard errors, obtained from the theory of non-linear regression (based on local linearization: <ref> [25] </ref>) since this is a non-linear regression problem.
Reference: [26] <author> Stewart, L. </author> <title> Hierarchical Bayesian analysis using Monte Carlo integration: computing posterior distributions when there are many possible models. </title> <booktitle> The Statistician 1987; 36 </booktitle> <pages> 211-219. </pages>
Reference-contexts: Markov chain Monte Carlo methods ([1]) have been used to explore the model space and so compute Bayes factors, for example in <ref> [26, 11] </ref>. 4 Model averaging Bayesians have long known that the logical endpoint of their theory is to average the predictions of their models, using the Bayes factors as weights, and this has become computationally feasible recently ([26, 10, 4]).
Reference: [27] <author> Stone, M. </author> <title> Cross-validatory choice and assessment of statistical predictions (with discussion). </title> <journal> Journal of the Royal Statistical Society series B 1974; 36 </journal> <pages> 111-147. </pages>
Reference: [28] <author> Stone, M. </author> <title> An asymptotic equivalence of choice of model by cross-validation and Akaike's criterion. </title> <journal> Journal of the Royal Statistical Society series B 1977; 39 </journal> <pages> 44-47. </pages>
Reference-contexts: The presence of local minima causes difficulties; we can attempt to track local minima by starting at a solution to the full problem ([14, 15]), but removing one outlying point can change dramatically the structure of the local minima. References <ref> [28, 12, 13, 16, 17, 18] </ref> take another approach, a generalization of Akaike's AIC criterion. <p> Moody's p eff is p fl , and this version of AIC is called NIC. The underlying idea of NIC/AIC is to estimate the deviance for a test set of size n, compensating for the fact that the weights were chosen to fit the training set. Stone in <ref> [28] </ref> proved (but did not actually state) that for large training sets LOOCV and NIC are equivalent, and this seems to work well in practice. Thus NIC gives the effect of LOOCV at much less computational effort.
Reference: [29] <author> Wahba, G. and Wold, S. </author> <title> A completely automatic French curve. </title> <booktitle> Communications in Statistics 1975; 4 </booktitle> <pages> 1-17. </pages>
Reference: [30] <author> Wolpert, D. H. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks 1992; 5 </booktitle> <pages> 241-259. </pages>
Reference-contexts: This is feasible with a coarse grid for . It also insists that we average over the predictions from local minima, computing the weights from (2). Model averaging has been discussed in non-Bayesian contexts, for example Wolpert's <ref> [30] </ref> stacked generalization. It seems to have been missed that this was part of Stone's first paper on cross-validation ([27], pp. 126-7). His suggestion was to take a convex combination of the predictions of several models, using cross-validation to choose the weights of the combination.
References-found: 30

