URL: http://www.cs.cornell.edu/skeshav/doc/92/2-04.ps
Refering-URL: http://www.cs.cornell.edu/skeshav/papers.html
Root-URL: http://www.cs.brown.edu/
Email: keshav@research.att.com  
Title: Flow Control in High-Speed Networks with Long Delays  
Author: Srinivasan Keshav 
Address: 600 Mountain Ave., Murray Hill, NJ 07974.  
Affiliation: AT&T Bell Laboratories,  
Abstract: The flow control component of a transport layer protocol regulates the natural data transmission rate of an application to match the service rate offered by the network. In this paper, we study the problems that arise when a flow control protocol has to deal with long delays in receiving information about network state, and has large amounts of data transmitted but unacknowledged. We describe three representative flow control protocols, and study their behavior on a suite of three benchmark scenarios. Our simulations indicate that in networks with large delays, protocols that are insensitive to the state of the network, or that require multiple round trip times to attain the optimal transmission rate, will perform poorly. The packet-pair protocol, which avoids these problems, is shown to perform well under rather adverse conditions. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Caceres, P. B. Danzig, S. Jamin and D. J. Mitzel, </author> <title> Characteristics of Application Conversations in TCP/IP Wide-Area Internetworks, </title> <booktitle> Proc. ACM SigComm 1991, </booktitle> <month> September </month> <year> 1991. </year>
Reference-contexts: All lines have zero propagation delay, unless otherwise marked. Sources are assumed to always have data to send, and are meant to model large file transfers (FTP protocol). The packet size is 1000 bytes. This roughly corresponds to the measured mean value 570 bytes in the DARPA Internet <ref> [1] </ref>. All the sources are assumed to start sending data at the same time - this makes throughput comparisions easy to make, and does not qualitatively alter our results.
Reference: 2. <author> D. D. Clark, M. L. Lambert and L. Zhang, NETBLT: </author> <title> A Bulk Data Transfer Protocol, </title> <institution> RFC-998, Network Working Group, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: Similar rate-based flow control protocols such as NETBLT <ref> [2] </ref> and the delay-based congestion avoidance scheme [7] allows users to increase and decrease their sending rates in response to changes monitored in the acknowledgment stream (instead of packet losses).
Reference: 3. <author> A. Demers, S. Keshav and S. Shenker, </author> <title> Analysis and Simulation of a Fair Queueing Algorithm, </title> <journal> Journal of Internetworking Research and Experience, </journal> <month> September </month> <year> 1990, </year> <pages> 3-26;. </pages> <note> also Proc. ACM SigComm, Sept. 1989, pp 1-12.. </note>
Reference-contexts: Then we compare three flow control protocols: (a) Generic, (b) BSD4.3-Tahoe operating system's TCP flow control scheme, and (c) PP, the packet-pair flow control scheme. They are evaluated in conjunction with two different packet scheduling disciplines: First Come First Served (FCFS) and Fair Queueing FQ <ref> [3] </ref>. We briefly describe each scheme, and then compare their behavior on some benchmark networks. 2. The flow control problem The data stream from a user has application-specific dynamics. <p> The second factor has been extensively studied in recent work <ref> [3, 16] </ref>, and so we shall not consider it in any detail. Instead, we explore the first issue through simulations of some flow control protocols (in conjunction with scheduling disciplines). Similar studies have also been presented recently by [9, 17]. 3. <p> The FCFS discipline is standard in most current networks, and has been extensively analyzed. FQ is a simple extension to round-robin with per-conversation data queues that allows for variable sized packets <ref> [3] </ref>. 4. Simulation results This section presents a simulation results for a suite of three benchmark scenarios. The simulations were performed using the REAL simulator [11], using the methodology detailed in [13], and summarized below.
Reference: 4. <author> A. G. Fraser, </author> <title> Designing a Public Data Network, </title> <journal> IEEE Communications Magazine, </journal> <month> October </month> <year> 1991, </year> <pages> 31-35. </pages>
Reference-contexts: The three potential bottlenecks can lead to bottleneck migration, and large discrepancies in monitoring the bottleneck service rate. It is generally accepted that a switch should have at least a bandwidth-delay product worth of buffers to be shared amongst the conversations sending data through that switch <ref> [4, 5] </ref>. Here, the minimum round trip propagation delay is 12 seconds, and the bottleneck bandwidth is 10 packets/s. Thus, 120 switch buffers are provided, as 120 is the bandwidth-delay product. Recall that in our simulations buffers are not reserved.
Reference: 5. <author> E. L. Hahne, C. R. Kalmanek and S. P. Morgan, </author> <title> Fairness and Congestion Control on a Large ATM Data Network with Dynamically Adjustable Windows, </title> <booktitle> 13th International Teletraffic Congress, </booktitle> <address> Copenhagen , June 1991. </address>
Reference-contexts: In this discussion we have assumed that no bandwidth or buffers are reserved at a switch. If buffer reservations can be made, then the problem changes its character, and solutions such as those described in Reference <ref> [5] </ref> become applicable. Other intermediate buffer management schemes that combine sharing and reservation are described in [8]. However, since we restrict ourselves to connectionless networks, such schemes are beyond the scope of this paper. <p> The three potential bottlenecks can lead to bottleneck migration, and large discrepancies in monitoring the bottleneck service rate. It is generally accepted that a switch should have at least a bandwidth-delay product worth of buffers to be shared amongst the conversations sending data through that switch <ref> [4, 5] </ref>. Here, the minimum round trip propagation delay is 12 seconds, and the bottleneck bandwidth is 10 packets/s. Thus, 120 switch buffers are provided, as 120 is the bandwidth-delay product. Recall that in our simulations buffers are not reserved.
Reference: 6. <author> V. Jacobson, </author> <title> Congestion Avoidance and Control, </title> <booktitle> Proc. ACM SigComm 1988, </booktitle> <month> August </month> <year> 1988, </year> <pages> 314-329. </pages>
Reference-contexts: The first protocol, that we call `generic' represents the first attempt in designing flow control algorithms, and is relatively inflexible in its response to changes in the network state [19, 23]. The second protocol, the Jacobson-Karels modifications to TCP (JK) <ref> [6] </ref>. is an attempt to modify the basic framework of the `generic' protocol to make it sensitive to network state. The first two protocols do not depend on the choice of scheduling discipline at the intermediate queueing points in the network. <p> Rather than using that code directly in our simulations, we choose to model the JK algorithm by adding many of the congestion control ideas found in that code, such as adjustable windows, better timeout calculations, and fast retransmit, to our generic flow control algorithm <ref> [6, 10] </ref>. Similar rate-based flow control protocols such as NETBLT [2] and the delay-based congestion avoidance scheme [7] allows users to increase and decrease their sending rates in response to changes monitored in the acknowledgment stream (instead of packet losses).
Reference: 7. <author> R. Jain, </author> <title> A Delay-based Approach for Congestion Avoidance in Interconnected Heterogeneous Computer Networks, </title> <journal> Computer Communications Review, </journal> <month> October </month> <year> 1989, </year> <pages> 56-71. </pages>
Reference-contexts: Similar rate-based flow control protocols such as NETBLT [2] and the delay-based congestion avoidance scheme <ref> [7] </ref> allows users to increase and decrease their sending rates in response to changes monitored in the acknowledgment stream (instead of packet losses). The idea is that a slowed down acknowledgement rate implicitly signals congestion, and triggers a reduction in the source's sending rate.
Reference: 8. <author> F. Kamoun and L. Kleinrock, </author> <title> Analysis of Finite Storage in </title>
Reference-contexts: If buffer reservations can be made, then the problem changes its character, and solutions such as those described in Reference [5] become applicable. Other intermediate buffer management schemes that combine sharing and reservation are described in <ref> [8] </ref>. However, since we restrict ourselves to connectionless networks, such schemes are beyond the scope of this paper. In high speed networks, a second source of problems is the presence of other users, who might be able to inject data at high speeds into some access link.
References-found: 8

