URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15740-f96/OldFiles/public/doc/Blelloch.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15740-f96/OldFiles/public/doc/
Root-URL: http://www.cs.cmu.edu
Email: Blelloch  
Title: Programming Parallel Algorithms language-based performance model that uses work and depth rather than a machine-based
Author: Guy E. 
Keyword: Data-parallel, parallel algorithms, supercomputers, nested parallelism, PRAM model, parallel programming languages, collection-oriented languages.  
Note: A  
Abstract: Note: This paper is also available on the World-Wide Web at: http://web.scandal.cs.cmu.edu/www/cacm.html. The Web version is interactive and allows the reader to try the parallel algorithms presented in this paper. In the past 20 years there has been tremendous progress in developing and analyzing parallel algorithms. Researchers have developed efficient parallel algorithms to solve most problems for which efficient sequential solutions are known. Although some of these algorithms are efficient only in a theoretical framework, many are quite efficient in practice or have key ideas that have been used in efficient implementations. This research on parallel algorithms has not only improved our general understanding of parallelism, but in several cases has led to improvements in sequential algorithms. Unfortunately there has been less success in developing good languages for programming parallel algorithms, particularly languages that are well suited for teaching and prototyping algorithms. There has been a large gap between languages that are too low level, requiring specification of many details that obscure the meaning of the algorithm, and languages that are too high-level, making the performance implications of various constructs unclear. In sequential computing many standard languages such as C or Pascal do a reasonable job of bridging this gap, but in parallel languages building such a bridge has been significantly more difficult. Our research involves developing a parallel language that is useful for teaching as well as for implementing parallel algorithms. To achieve this, an important goal has been to develop a language that allows high-level descriptions of parallel algorithms while also having a well understood mapping onto a performance model (i.e. bridges the gap). Based on our research, we believe that the following two features are important for achieving this goal: 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in <ref> [1, 2, 3, 4] </ref>g; adds the two sequences elementwise returning [4, -2, -6, 9]. The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. <p> HPF 1.0 has some limited support for nested calls and future versions are likely to have significantly better support. 8 function factorial (n) = if (n == 1) then 1 else n*factorial (n-1); and then apply it over the elements of a sequence as in ffactorial (i) : i in <ref> [3, 1, 7] </ref>g; which returns the sequence [6, 1, 5040]. In addition to the parallelism supplied by apply-to-each, Nesl provides a set of functions on sequences, each of which can be implemented in parallel. <p> nested calls and future versions are likely to have significantly better support. 8 function factorial (n) = if (n == 1) then 1 else n*factorial (n-1); and then apply it over the elements of a sequence as in ffactorial (i) : i in [3, 1, 7]g; which returns the sequence <ref> [6, 1, 5040] </ref>. In addition to the parallelism supplied by apply-to-each, Nesl provides a set of functions on sequences, each of which can be implemented in parallel. For example the function sum adds the elements of a sequence, and the function reverse reverses the elements of a sequence. <p> For example, executing a plus-scan on the sequence <ref> [3, 5, 3, 1, 6] </ref> returns [0, 3, 8, 11, 12]. This can be implemented as shown in Figure 11. The algorithm works by elementwise adding the odd and even elements and recursively solving the problem on these sums.
Reference: [2] <author> Alfred V. Aho and Jeffrey D. Ullman. </author> <booktitle> Foundations of Computer Science. </booktitle> <publisher> Computer Science Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Such a language-based performance model specifies the costs of the primitive instructions, and a set of rules for composing costs across program expressions. The use of language-based models is certainly not new. Aho and Ullman, in their popular introductory text book "Foundations of Computer Science" <ref> [2] </ref>, define such a model for deriving running times of sequential algorithms. The approach allows them to discuss the running time of the algorithms without introducing a machine model. A similar approach can be taken to define a model based on work and depth. <p> This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in <ref> [1, 2, 3, 4] </ref>g; adds the two sequences elementwise returning [4, -2, -6, 9]. The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter.
Reference: [3] <author> Arvind, Rishiyur S. Nikhil, and Keshav K. Pingali. I-structures: </author> <title> Data structures for parallel computing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 598-632, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: This ability to operate in parallel over sets of data is often referred to as data-parallelism [15], and languages based on it are often referred to as data-parallel languages or collection-oriented languages [24]. We note that many parallel languages have data-parallel features in conjunction with other forms of parallelism <ref> [10, 3, 12, 18] </ref>. Before we come to the rash conclusion that data-parallel languages are the panacea for programming parallel algorithms, we make a distinction between flat and nested data-parallel languages. <p> All elements of a sequence must be of the same type, and sequence indices are zero based (a [0] extracts the first element of the sequence a). The main data-parallel construct is apply-to-each, which uses a set-like notation. For example, the expression fa * a : a in <ref> [3, -4, -9, 5] </ref>g; squares each elements of the sequence [3, -4, -9, 5] returning the sequence [9, 16, 81, 25]. This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. <p> The main data-parallel construct is apply-to-each, which uses a set-like notation. For example, the expression fa * a : a in <ref> [3, -4, -9, 5] </ref>g; squares each elements of the sequence [3, -4, -9, 5] returning the sequence [9, 16, 81, 25]. This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in <p> For example, the expression fa * a : a in <ref> [3, -4, -9, 5] </ref>g; squares each elements of the sequence [3, -4, -9, 5] returning the sequence [9, 16, 81, 25]. This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in [1, 2, 3, 4]g; adds the two sequences elementwise returning [4, -2, -6, 9]. <p> each elements of the sequence <ref> [3, -4, -9, 5] </ref> returning the sequence [9, 16, 81, 25]. This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in [1, 2, 3, 4]g; adds the two sequences elementwise returning [4, -2, -6, 9]. The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa * a : a in [3, -4, -9, 5] | a <p> This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in <ref> [1, 2, 3, 4] </ref>g; adds the two sequences elementwise returning [4, -2, -6, 9]. The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. <p> + b : a in <ref> [3, -4, -9, 5] </ref>; b in [1, 2, 3, 4]g; adds the two sequences elementwise returning [4, -2, -6, 9]. The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa * a : a in [3, -4, -9, 5] | a &gt; 0g; can be read: "in parallel, for each a in the sequence [3, -4, -9, 5] such that a is greater than 0, square a". It returns the sequence [9, 25]. The elements that remain maintain their relative order. <p> The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa * a : a in <ref> [3, -4, -9, 5] </ref> | a &gt; 0g; can be read: "in parallel, for each a in the sequence [3, -4, -9, 5] such that a is greater than 0, square a". It returns the sequence [9, 25]. The elements that remain maintain their relative order. This filtering was used in the Quicksort example. <p> HPF 1.0 has some limited support for nested calls and future versions are likely to have significantly better support. 8 function factorial (n) = if (n == 1) then 1 else n*factorial (n-1); and then apply it over the elements of a sequence as in ffactorial (i) : i in <ref> [3, 1, 7] </ref>g; which returns the sequence [6, 1, 5040]. In addition to the parallelism supplied by apply-to-each, Nesl provides a set of functions on sequences, each of which can be implemented in parallel. <p> For example, executing a plus-scan on the sequence <ref> [3, 5, 3, 1, 6] </ref> returns [0, 3, 8, 11, 12]. This can be implemented as shown in Figure 11. The algorithm works by elementwise adding the odd and even elements and recursively solving the problem on these sums. <p> For example, executing a plus-scan on the sequence [3, 5, 3, 1, 6] returns <ref> [0, 3, 8, 11, 12] </ref>. This can be implemented as shown in Figure 11. The algorithm works by elementwise adding the odd and even elements and recursively solving the problem on these sums. The result of the recursive call is then used to generate all the prefix sums.
Reference: [4] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Although the models are well suited for such theoretical analysis, they are not a convenient model for programming parallel algorithms. Vector Machine Models: The first programmable machine model based on work and depth was the Vector Random Access Machine (VRAM) <ref> [4] </ref>. The VRAM model is a sequential random-access machine (RAM) extended with a set of instructions that operate on vectors (see Figure 3). Each location of the memory contains a whole vector, and the vectors can vary in size during 3 the computation. <p> In flat data-parallel languages a function can be applied in parallel over a set of values, but the function itself must be sequential. In nested data-parallel languages <ref> [4] </ref> any function can be applied over a set of values, including parallel functions. For example, the summation of each row of the matrix mentioned above could itself execute in parallel using a tree sum. <p> This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in <ref> [1, 2, 3, 4] </ref>g; adds the two sequences elementwise returning [4, -2, -6, 9]. The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. <p> The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in [1, 2, 3, 4]g; adds the two sequences elementwise returning <ref> [4, -2, -6, 9] </ref>. The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. <p> We furthermore argue that language-based models seem to be the most reasonable way to define a programming model based on work and depth. A full implementation of Nesl is currently available on the Web. The compiler is based on a technique called flattening nested parallelism <ref> [4] </ref>, and compiles to an intermediate language called Vcode. Benchmark results for this implementation for the Connection Machines CM-2 and CM-5 and the Cray C90 are described in [6].
Reference: [5] <author> Guy E. Blelloch. NESL: </author> <title> A nested data-parallel language (version 2.6). </title> <type> Technical Report CMU-CS-93-129, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: The importance of allowing nesting in data-parallel languages 7 has also been observed by others [13]. However, most existing data-parallel languages, such as High Performance Fortran (HPF) [14] or C* [21], do not have direct support for such nesting. 2 2.1 Nesl This paper uses Nesl <ref> [5] </ref> as an example of a nested data-parallel language. This section gives an overview of the language and Section 3 gives several examples of parallel algorithms described and analyzed with Nesl. <p> All elements of a sequence must be of the same type, and sequence indices are zero based (a [0] extracts the first element of the sequence a). The main data-parallel construct is apply-to-each, which uses a set-like notation. For example, the expression fa * a : a in <ref> [3, -4, -9, 5] </ref>g; squares each elements of the sequence [3, -4, -9, 5] returning the sequence [9, 16, 81, 25]. This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. <p> The main data-parallel construct is apply-to-each, which uses a set-like notation. For example, the expression fa * a : a in <ref> [3, -4, -9, 5] </ref>g; squares each elements of the sequence [3, -4, -9, 5] returning the sequence [9, 16, 81, 25]. This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in <p> For example, the expression fa * a : a in <ref> [3, -4, -9, 5] </ref>g; squares each elements of the sequence [3, -4, -9, 5] returning the sequence [9, 16, 81, 25]. This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in [1, 2, 3, 4]g; adds the two sequences elementwise returning [4, -2, -6, 9]. <p> each elements of the sequence <ref> [3, -4, -9, 5] </ref> returning the sequence [9, 16, 81, 25]. This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in [1, 2, 3, 4]g; adds the two sequences elementwise returning [4, -2, -6, 9]. The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa * a : a in [3, -4, -9, 5] | a <p> + b : a in <ref> [3, -4, -9, 5] </ref>; b in [1, 2, 3, 4]g; adds the two sequences elementwise returning [4, -2, -6, 9]. The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa * a : a in [3, -4, -9, 5] | a &gt; 0g; can be read: "in parallel, for each a in the sequence [3, -4, -9, 5] such that a is greater than 0, square a". It returns the sequence [9, 25]. The elements that remain maintain their relative order. <p> The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. For example fa * a : a in <ref> [3, -4, -9, 5] </ref> | a &gt; 0g; can be read: "in parallel, for each a in the sequence [3, -4, -9, 5] such that a is greater than 0, square a". It returns the sequence [9, 25]. The elements that remain maintain their relative order. This filtering was used in the Quicksort example. <p> For each pair (i,v) the value v is inserted into position i of the destination sequence. For example write ([0, 0, 0, 0, 0, 0, 0, 0], [(4,-2),(2,5),(5,9)]); inserts the -2, 5 and 9 into the sequence at locations 4, 2 and 5, respectively, returning <ref> [0, 0, 5, 0, -2, 9, 0, 0] </ref> . If an index is repeated, then one value is written non-deterministically. For readers familiar with the variants of the PRAM model, we note that the write function is analogous to an "arbitrary" concurrent write. <p> Nested parallelism is supplied in Nesl by allowing sequences to be nested and allowing parallel functions to be used in an apply-to-each. For example, we could apply the sum function in parallel over a nested sequence, as in fsum (a) : a in [[2,3], [8,3,9], [7]]g which would return <ref> [5, 20, 7] </ref>. Here there is parallelism both within each sum and across the sums. <p> For example, executing a plus-scan on the sequence <ref> [3, 5, 3, 1, 6] </ref> returns [0, 3, 8, 11, 12]. This can be implemented as shown in Figure 11. The algorithm works by elementwise adding the odd and even elements and recursively solving the problem on these sums.
Reference: [6] <author> Guy E. Blelloch, Siddhartha Chatterjee, Jonathan C. Hardwick, Jay Sipelstein, and Marco Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 4-14, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: nested calls and future versions are likely to have significantly better support. 8 function factorial (n) = if (n == 1) then 1 else n*factorial (n-1); and then apply it over the elements of a sequence as in ffactorial (i) : i in [3, 1, 7]g; which returns the sequence <ref> [6, 1, 5040] </ref>. In addition to the parallelism supplied by apply-to-each, Nesl provides a set of functions on sequences, each of which can be implemented in parallel. For example the function sum adds the elements of a sequence, and the function reverse reverses the elements of a sequence. <p> For example, executing a plus-scan on the sequence <ref> [3, 5, 3, 1, 6] </ref> returns [0, 3, 8, 11, 12]. This can be implemented as shown in Figure 11. The algorithm works by elementwise adding the odd and even elements and recursively solving the problem on these sums. <p> The compiler is based on a technique called flattening nested parallelism [4], and compiles to an intermediate language called Vcode. Benchmark results for this implementation for the Connection Machines CM-2 and CM-5 and the Cray C90 are described in <ref> [6] </ref>.
Reference: [7] <author> Guy E. Blelloch and John Greiner. </author> <title> Parallelism in sequential functional languages. </title> <booktitle> In Proceedings of the Symposium on Functional Programming and Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: HPF 1.0 has some limited support for nested calls and future versions are likely to have significantly better support. 8 function factorial (n) = if (n == 1) then 1 else n*factorial (n-1); and then apply it over the elements of a sequence as in ffactorial (i) : i in <ref> [3, 1, 7] </ref>g; which returns the sequence [6, 1, 5040]. In addition to the parallelism supplied by apply-to-each, Nesl provides a set of functions on sequences, each of which can be implemented in parallel. <p> Nested parallelism is supplied in Nesl by allowing sequences to be nested and allowing parallel functions to be used in an apply-to-each. For example, we could apply the sum function in parallel over a nested sequence, as in fsum (a) : a in [[2,3], [8,3,9], [7]]g which would return <ref> [5, 20, 7] </ref>. Here there is parallelism both within each sum and across the sums. <p> The work and depth for a function call and for scalar primitives are 1 each. The costs of the Nesl functions on sequences are summarized in Table 7. We note that the performance rules can be more precisely defined using an operational semantics <ref> [7] </ref>. As an example of composing work and depth, consider evaluating the expression e = ffactorial (n) : n in ag where a = [3,1,5,2].
Reference: [8] <author> Guy E. Blelloch and Jonathan C. Hardwick. </author> <title> Class notes: Programming parallel algorithms. </title> <type> Technical Report CMU-CS-93-115, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: For example, executing a plus-scan on the sequence [3, 5, 3, 1, 6] returns <ref> [0, 3, 8, 11, 12] </ref>. This can be implemented as shown in Figure 11. The algorithm works by elementwise adding the odd and even elements and recursively solving the problem on these sums. The result of the recursive call is then used to generate all the prefix sums.
Reference: [9] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <year> 1974. </year> <month> 17 </month>
Reference-contexts: In fact, the complexities are often referred to as T 1 and T 1 . In practice, however, we want to know the running time for some fixed number of processors. A simple but important result of Brent <ref> [9] </ref> showed that knowing the two limits is good enough to place reasonable bounds on running time for any fixed number of processors. <p> The main data-parallel construct is apply-to-each, which uses a set-like notation. For example, the expression fa * a : a in [3, -4, -9, 5]g; squares each elements of the sequence [3, -4, -9, 5] returning the sequence <ref> [9, 16, 81, 25] </ref>. This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. <p> The apply-to-each can be used over multiple sequences. The expression fa + b : a in [3, -4, -9, 5]; b in [1, 2, 3, 4]g; adds the two sequences elementwise returning <ref> [4, -2, -6, 9] </ref>. The apply-to-each construct also provides the ability to subselect elements of a sequence based on a filter. <p> For example fa * a : a in [3, -4, -9, 5] | a &gt; 0g; can be read: "in parallel, for each a in the sequence [3, -4, -9, 5] such that a is greater than 0, square a". It returns the sequence <ref> [9, 25] </ref>. The elements that remain maintain their relative order. This filtering was used in the Quicksort example. Any function, whether primitive or user defined, may be applied to each element of a sequence. <p> For each pair (i,v) the value v is inserted into position i of the destination sequence. For example write ([0, 0, 0, 0, 0, 0, 0, 0], [(4,-2),(2,5),(5,9)]); inserts the -2, 5 and 9 into the sequence at locations 4, 2 and 5, respectively, returning <ref> [0, 0, 5, 0, -2, 9, 0, 0] </ref> . If an index is repeated, then one value is written non-deterministically. For readers familiar with the variants of the PRAM model, we note that the write function is analogous to an "arbitrary" concurrent write.
Reference: [10] <author> K. Mani Chandy and Jayadev Misra. </author> <title> Parallel Program Design: A Foundation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: This ability to operate in parallel over sets of data is often referred to as data-parallelism [15], and languages based on it are often referred to as data-parallel languages or collection-oriented languages [24]. We note that many parallel languages have data-parallel features in conjunction with other forms of parallelism <ref> [10, 3, 12, 18] </ref>. Before we come to the rash conclusion that data-parallel languages are the panacea for programming parallel algorithms, we make a distinction between flat and nested data-parallel languages.
Reference: [11] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press and McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: All the code is shown in Figure 11. These algorithms give more demonstration of the conciseness of nested data parallel constructs. We use the standard recursive version for the fast Fourier transform (FFT) <ref> [11] </ref>. The second argument w is a sequence of the same length as a containing all the complex nth roots of unity. The FFT is called recursively on the odd and even elements of a. The results are then combined using cadd and cmult (complex addition and multiplication). <p> For example, executing a plus-scan on the sequence [3, 5, 3, 1, 6] returns <ref> [0, 3, 8, 11, 12] </ref>. This can be implemented as shown in Figure 11. The algorithm works by elementwise adding the odd and even elements and recursively solving the problem on these sums. The result of the recursive call is then used to generate all the prefix sums. <p> A variation of Quicksort can be used to find the k th smallest element of a sequence <ref> [11] </ref>. This algorithm only calls itself recursively on the set of elements containing the result. Here we consider a parallel version of this algorithm. After selecting the lesser elements, if #lesser is greater than k, then the k th smallest element must belong to that set.
Reference: [12] <author> John T. Feo, David C. Cann, and Rodney R. Oldehoeft. </author> <title> A Report on the Sisal Language Project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10(4) </volume> <pages> 349-366, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: This ability to operate in parallel over sets of data is often referred to as data-parallelism [15], and languages based on it are often referred to as data-parallel languages or collection-oriented languages [24]. We note that many parallel languages have data-parallel features in conjunction with other forms of parallelism <ref> [10, 3, 12, 18] </ref>. Before we come to the rash conclusion that data-parallel languages are the panacea for programming parallel algorithms, we make a distinction between flat and nested data-parallel languages. <p> For example, executing a plus-scan on the sequence [3, 5, 3, 1, 6] returns <ref> [0, 3, 8, 11, 12] </ref>. This can be implemented as shown in Figure 11. The algorithm works by elementwise adding the odd and even elements and recursively solving the problem on these sums. The result of the recursive call is then used to generate all the prefix sums.
Reference: [13] <author> Phil Hatcher, Walter F. Tichy, and Michael Philippsen. </author> <booktitle> A critique of the programming language C fl . Communications of the ACM, </booktitle> <volume> 35(6) </volume> <pages> 21-24, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: In particular, nested parallelism can be used to implement nested loops and divide-and-conquer algorithms in parallel (five out of the seven algorithms described in this paper use nesting in a crucial way). The importance of allowing nesting in data-parallel languages 7 has also been observed by others <ref> [13] </ref>. However, most existing data-parallel languages, such as High Performance Fortran (HPF) [14] or C* [21], do not have direct support for such nesting. 2 2.1 Nesl This paper uses Nesl [5] as an example of a nested data-parallel language.
Reference: [14] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: The importance of allowing nesting in data-parallel languages 7 has also been observed by others [13]. However, most existing data-parallel languages, such as High Performance Fortran (HPF) <ref> [14] </ref> or C* [21], do not have direct support for such nesting. 2 2.1 Nesl This paper uses Nesl [5] as an example of a nested data-parallel language. This section gives an overview of the language and Section 3 gives several examples of parallel algorithms described and analyzed with Nesl.
Reference: [15] <author> W. Daniel Hillis and Guy L. Steele Jr. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12), </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: This ability to operate in parallel over sets of data is often referred to as data-parallelism <ref> [15] </ref>, and languages based on it are often referred to as data-parallel languages or collection-oriented languages [24]. We note that many parallel languages have data-parallel features in conjunction with other forms of parallelism [10, 3, 12, 18].
Reference: [16] <author> Joseph JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Mass., </address> <year> 1992. </year>
Reference-contexts: The depth represents the best possible running time, assuming an ideal machine with an unlimited number of processors. Work and depth have been used informally for many years to describe the performance of parallel algorithms [23], especially when teaching them <ref> [17, 16] </ref>. The claim is that it is easier to describe, think about, and analyze algorithms in terms of work and depth rather than in terms of running time on a processor-based model (a model based on P processors). <p> The main data-parallel construct is apply-to-each, which uses a set-like notation. For example, the expression fa * a : a in [3, -4, -9, 5]g; squares each elements of the sequence [3, -4, -9, 5] returning the sequence <ref> [9, 16, 81, 25] </ref>. This can be read: "in parallel, for each a in the sequence [3, -4, -9, 5], square a". The apply-to-each can be used over multiple sequences. <p> It is hard to state the average case time since it depends on the distribution of the inputs. Other parallel algorithms for the convex-hull problem run in D = O (log n), and W = O (n) in the worst case <ref> [16] </ref>, but have larger constants. 3.4 Three Other Algorithms We conclude our examples with brief discussions of three other algorithms: the fast Fourier transform (FFT), the scan operation (all prefix sums), and an algorithm for finding the k th smallest element of a set. <p> In particular it suggests that we move away from using theoretical performance models based on machines to models based on languages. As mentioned in the paper, some reference works already informally analyze parallel algorithms in terms of work and depth before mapping them onto a PRAM <ref> [17, 16] </ref>. We suggest that the extra step is taken of formalizing a model based on work and depth. With this formal model the PRAM can be cut out of the loop, directly mapping the model onto more realistic machines.
Reference: [17] <author> Richard M. Karp and Vijaya Ramachandran. </author> <title> Parallel algorithms for shared memory machines. </title> <editor> In J. Van Leeuwen, editor, </editor> <title> Handbook of Theoretical Computer Science|Volume A: Algorithms and Complexity. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: The depth represents the best possible running time, assuming an ideal machine with an unlimited number of processors. Work and depth have been used informally for many years to describe the performance of parallel algorithms [23], especially when teaching them <ref> [17, 16] </ref>. The claim is that it is easier to describe, think about, and analyze algorithms in terms of work and depth rather than in terms of running time on a processor-based model (a model based on P processors). <p> As such, the models have been used for many years to study various theoretical aspects of parallelism, for example to prove that certain problems are hard to solve in parallel (see <ref> [17] </ref> for an overview). Although the models are well suited for such theoretical analysis, they are not a convenient model for programming parallel algorithms. Vector Machine Models: The first programmable machine model based on work and depth was the Vector Random Access Machine (VRAM) [4]. <p> In particular it suggests that we move away from using theoretical performance models based on machines to models based on languages. As mentioned in the paper, some reference works already informally analyze parallel algorithms in terms of work and depth before mapping them onto a PRAM <ref> [17, 16] </ref>. We suggest that the extra step is taken of formalizing a model based on work and depth. With this formal model the PRAM can be cut out of the loop, directly mapping the model onto more realistic machines.
Reference: [18] <author> Peter H. Mills, Lars S. Nyland, Jan F. Prins, John H. Reif, and Robert A. Wagner. </author> <title> Prototyping parallel and distributed programs in Proteus. </title> <type> Technical Report UNC-CH TR90-041, </type> <institution> Computer Science Department, University of North Carolina, </institution> <year> 1990. </year>
Reference-contexts: This ability to operate in parallel over sets of data is often referred to as data-parallelism [15], and languages based on it are often referred to as data-parallel languages or collection-oriented languages [24]. We note that many parallel languages have data-parallel features in conjunction with other forms of parallelism <ref> [10, 3, 12, 18] </ref>. Before we come to the rash conclusion that data-parallel languages are the panacea for programming parallel algorithms, we make a distinction between flat and nested data-parallel languages.
Reference: [19] <author> Robin Milner, Mads Tofte, and Robert Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: The ideas, however, can clearly be used in other languages. Nesl is loosely based on ML <ref> [19] </ref>, a language with a powerful type system, and on SETL [22], a language designed for concisely expressing sequential algorithms. As with ML, Nesl is mostly functional (has only limited forms of side effects), but this feature is tangential to the points made in this paper.
Reference: [20] <author> Franco P. Preparata and Michael I. Shamos. </author> <title> Computational Geometry|An Introduction. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Nested parallelism is supplied in Nesl by allowing sequences to be nested and allowing parallel functions to be used in an apply-to-each. For example, we could apply the sum function in parallel over a nested sequence, as in fsum (a) : a in [[2,3], [8,3,9], [7]]g which would return <ref> [5, 20, 7] </ref>. Here there is parallelism both within each sum and across the sums. <p> This example shows another use of nested parallelism for divide-and-conquer algorithms. The algorithm we use is a parallel Quickhull <ref> [20] </ref>, so named because of its similarity to the Quicksort algorithm. As with Quicksort, the strategy is to pick a "pivot" element, split the data based on the pivot, and recurse on each of the split sets.
Reference: [21] <author> J. R. Rose and G. L. Steele Jr. </author> <title> C*: An extended C language for data parallel programming. </title> <booktitle> In Proceedings Second International Conference on Supercomputing, </booktitle> <volume> Vol. 2, </volume> <pages> pages 2-16, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: The importance of allowing nesting in data-parallel languages 7 has also been observed by others [13]. However, most existing data-parallel languages, such as High Performance Fortran (HPF) [14] or C* <ref> [21] </ref>, do not have direct support for such nesting. 2 2.1 Nesl This paper uses Nesl [5] as an example of a nested data-parallel language. This section gives an overview of the language and Section 3 gives several examples of parallel algorithms described and analyzed with Nesl.
Reference: [22] <author> J. T. Schwartz, R. B. K. Dewar, E. Dubinsky, and E. Schonberg. </author> <title> Programming with Sets: An Introduction to SETL. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: The ideas, however, can clearly be used in other languages. Nesl is loosely based on ML [19], a language with a powerful type system, and on SETL <ref> [22] </ref>, a language designed for concisely expressing sequential algorithms. As with ML, Nesl is mostly functional (has only limited forms of side effects), but this feature is tangential to the points made in this paper. Nesl supports data-parallelism by means of operations on sequences|one dimensional arrays.
Reference: [23] <author> Yossi Shiloach and Uzi Vishkin. </author> <title> An O(n 2 log n) parallel Max-Flow algorithm. </title> <journal> J. Algorithms, </journal> <volume> 3 </volume> <pages> 128-146, </pages> <year> 1982. </year>
Reference-contexts: The depth represents the best possible running time, assuming an ideal machine with an unlimited number of processors. Work and depth have been used informally for many years to describe the performance of parallel algorithms <ref> [23] </ref>, especially when teaching them [17, 16]. The claim is that it is easier to describe, think about, and analyze algorithms in terms of work and depth rather than in terms of running time on a processor-based model (a model based on P processors).
Reference: [24] <author> Jay Sipelstein and Guy E. Blelloch. </author> <booktitle> Collection-oriented languages. Proceedings of the IEEE, </booktitle> <volume> 79(4) </volume> <pages> 504-523, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This ability to operate in parallel over sets of data is often referred to as data-parallelism [15], and languages based on it are often referred to as data-parallel languages or collection-oriented languages <ref> [24] </ref>. We note that many parallel languages have data-parallel features in conjunction with other forms of parallelism [10, 3, 12, 18]. Before we come to the rash conclusion that data-parallel languages are the panacea for programming parallel algorithms, we make a distinction between flat and nested data-parallel languages.

Reference: [s:e] <editor> Return integer sequence from s to e. </editor> <address> (e - s) 1 </address>

References-found: 25

