URL: ftp://ftp.cs.rochester.edu/pub/papers/ai/97.tr646.Learning_to_solve_multiple_goals.ps.gz
Refering-URL: http://www.cs.rochester.edu/trs/ai-trs.html
Root-URL: 
Title: Learning to Solve Multiple Goals  
Author: by Jonas Karlsson Professor Dana H. Ballard 
Degree: Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Supervised by  
Date: 1997  
Address: Rochester, New York  
Affiliation: Department of Computer Science The College Arts and Sciences University of Rochester  
Abstract-found: 0
Intro-found: 1
Reference: [ Agre, 1988 ] <author> Agre, Philip E. </author> <year> 1988. </year> <title> The Dynamic Structure of Everyday Life. </title> <type> Ph.D. Dissertation, </type> <institution> MIT Artificial Intelligence Lab. </institution> <type> (Tech Report No. 1085). </type>
Reference-contexts: However, sometimes hidden state can be useful to an agent, eliminating irrelevant detail from the state description (This case is described as passive abstraction by Agre <ref> [ Agre, 1988 ] </ref> ).In our modular approach for example, we deliberately introduce hidden state in order to reduce the size of the state-space each module must learn in.
Reference: [ Arbib and House, 1987 ] <author> Arbib, Michael A. and House, Donald H. </author> <year> 1987. </year> <title> Depth and detours:an essay on visually guided behavior. </title> <editor> In Arbib, M. and Hanson, A., editors 1987, </editor> <title> Vision, Brain, and Cooperative Computation. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference-contexts: The greatest mass has also been used in solutions in other problems. Using potential fields for path planning [ Khatib, 1986 ] , <ref> [ Arbib and House, 1987 ] </ref> , uses 22 the same techniques of gathering utilities, or potentials, from different sources (obstacles and goal locations) and then summing them. Using the greatest mass technique is one possible way the agent can merge policies, rather than simply selecting one to execute.
Reference: [ Arkin, 1990 ] <author> Arkin, Ronald C. </author> <year> 1990. </year> <title> Integrating behavioral, perceptual, and world knowledge in reactive navigation. </title> <booktitle> Robotics and Autonomous Systems 6 </booktitle> <pages> 105-122. </pages>
Reference-contexts: While this work illustrates the benefit of using prior domain knowledge encoded as reward functions, simply using reinforcement learning to learn how to arbitrate between behaviors is probably not feasible in most complex domains. Arkin proposes an arbitration scheme similar to our greatest mass technique <ref> [ Arkin, 1990 ] </ref> . Each behavior is defined as a schema, implemented as a potential field. These fields can be easily added to each other resulting in a field taking information from all behaviors into account. Thus the arbitration scheme is a simple vector sum.
Reference: [ Barto et al., 1983 ] <author> Barto, Andrew G.; Sutton, Richard S.; and Anderson, Charles W. </author> <year> 1983. </year> <title> Neuron-like elements that can solve difficult learning control problems. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics SMC-13(5):834-846. </journal>
Reference-contexts: Animals often can not be directly instructed on how to perform a task, but can be taught by administering feedback that is pleasurable or painful, depending on the animal's performance. As an intelligent control mechanism, reinforcement learning is also referred to as "learning with a critic" <ref> [ Barto et al., 1983 ] </ref> . For both biological and artificial systems the basic idea is to give the agent only some indication of how well it is performing or has performed a given task, without communicating the exact nature of the task or how it might be solved.
Reference: [ Brooks and Connell, 1986 ] <author> Brooks, Rodney A. and Connell, Jonathan H. </author> <year> 1986. </year> <title> Asynchronous distributed control system for a mobile robot. </title> <booktitle> SPIE 727 </booktitle> <pages> 77-84. </pages>
Reference-contexts: Also, low-level behaviors need not be concerned with high-level information but can focus on tasks such as "move forward 1 meter". The information needed at each level is therefore limited, making the search-space smaller. Brooks' subsumption architecture is a hierarchical behavior-based approach <ref> [ Brooks and Connell, 1986 ] </ref> , [ Brooks, 1985 ] . Lower levels control "instinctive" behaviors and higher levels control tasks that are regarded as more abstract. In a mobile robot for example, a low level behavior might be obstacle avoidance, while a high level behavior controls path planning.
Reference: [ Brooks, 1985 ] <author> Brooks, Rodney A. </author> <year> 1985. </year> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation RA-2(1). </journal>
Reference-contexts: Also, low-level behaviors need not be concerned with high-level information but can focus on tasks such as "move forward 1 meter". The information needed at each level is therefore limited, making the search-space smaller. Brooks' subsumption architecture is a hierarchical behavior-based approach [ Brooks and Connell, 1986 ] , <ref> [ Brooks, 1985 ] </ref> . Lower levels control "instinctive" behaviors and higher levels control tasks that are regarded as more abstract. In a mobile robot for example, a low level behavior might be obstacle avoidance, while a high level behavior controls path planning.
Reference: [ Dayan, 1992 ] <author> Dayan, </author> <title> Peter 1992. The convergenece of TD() for general . Machine Learning 8. </title>
Reference-contexts: TD () has been further generalized (and the generalization shown to converge) to include cases where information used in learning is not limited to immediately adjacent time steps, but can stem from any arbitrary state <ref> [ Dayan, 1992 ] </ref> . The approach has also been applied successfully in some domains, most notably in a backgammon player [ Tesauro, 1992 ] . However, the TD () algorithm still depends on the presence on a final outcome.
Reference: [ Feldman, 1962 ] <author> Feldman, D. </author> <year> 1962. </year> <title> Contributions to the two-armed bandit problem. </title> <journal> Annals of Math. Statist. </journal> <volume> 33(2) </volume> <pages> 847-856. </pages>
Reference-contexts: This strategy of gradually diminishing exploration is in accordance with what is known about so called "two-armed bandit" problems <ref> [ Feldman, 1962 ] </ref> , for which it has been demonstrated that while it is good to explore initially, as time passes it is better to select the action that is thought to be optimal with an increasing frequency. system.
Reference: [ Foulser et al., 1990 ] <author> Foulser, David E.; Li, Ming; and Yang, </author> <title> Qiang 1990. Theory and algorithms for plan merging. </title> <type> Technical report, </type> <institution> University of Waterloo. </institution>
Reference-contexts: This may result in executing actions that are suboptimal to each local module, but have a high global utility. Foulser, Li, and Yang, have studied this type of policy combination, using traditional, non-linear plans as their representation <ref> [ Foulser et al., 1990 ] </ref> . However, their work has been concentrated on the problem of finding the set of merging operations, that produce the optimal combined plan. This assumes that it is already known what parts of the plans can be merged, and how that is accomplished.
Reference: [ Garey and Johnson, 1979 ] <author> Garey, Michael R. and Johnson, David S. </author> <year> 1979. </year> <title> Computers and Intractability. </title> <publisher> W.H&gt; Freeman and Co. </publisher> <pages> 99 </pages>
Reference-contexts: The simple grid world example is a good example of the type of multi-goal domain for which our modular approach to reinforcement learning is intended. The problem is equivalent to the NP-complete Hamiltonian Path problem, meaning that learning the optimal solution is impractically expensive for large problem sizes <ref> [ Garey and Johnson, 1979 ] </ref> . However, the sub-goals are clearly identifiable, and the associated reward functions and sensory inputs can be decomposed into components relevant to individual sub-goals only. Furthermore, in grid world, we can be satisfied with an approximate solution.
Reference: [ Haddawy and Hanks, 1990 ] <author> Haddawy, Peter and Hanks, </author> <title> Steve 1990. Issues in decision-theoretic planning: Symbolic goals and numeric utilities. In DARPA workshop on Innovative approaches to planning, scheduling, and control. </title> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: In the field of decision theory and planning, there is a growing amount of research on how to properly assign utility to goals, whether they be goals of achievement, avoidance, or maintenance <ref> [ Haddawy and Hanks, 1990 ] </ref> . Given a simple goal, it is fairly straightforward to define a reward function leading to the proper behavior. For an achievement goal for example, the reward function returns a positive value in the goal state, and 0 in all other states.
Reference: [ Humphrys, 1996 ] <author> Humphrys, </author> <title> Mark 1996. Action selection methods using reinforcement learning. </title> <booktitle> In "Proceedings of the Fourth International Conference on the Simulation of Adaptive Behavior",. </booktitle>
Reference-contexts: In more complex domains with reward functions allowing for partial satisfaction of goals (or progress measurement) and where there is much delayed reinforcement, the neural network approach is not applicable. Humphrys has developed an approach similar to ours, using a fixed arbitration scheme, and learning the individual sub-tasks <ref> [ Humphrys, 1996 ] </ref> . Modules limit their inputs and have individual reward functions that are separate from the global reward used as an evaluation metric.
Reference: [ Jacobs and Jordan, 1991 ] <author> Jacobs, Robert A. and Jordan, Michael I. </author> <year> 1991. </year> <title> A competitive modular connectionist architecture. </title> <editor> In Lippmann, R. P.; Moody, J.; and Touretzky, D. S., editors 1991, </editor> <booktitle> Advances in Neural information processing systems 3. </booktitle> <publisher> Morgan Kaufmann publishers, Inc. </publisher>
Reference-contexts: Jacobs and Jordan have developed a neural network architecture that learns not only the arbitration, but how to decompose the task into sub-tasks, and learn each individual sub-task as well <ref> [ Jacobs and Jordan, 1991 ] </ref> . Arbitration is done by a "gating network" that selects the output of one of a set of "expert networks" assigned to sub-tasks. For any given state, the gating network selects the expert network that provides the best solution.
Reference: [ Johnson, 1990 ] <author> Johnson, David S. </author> <year> 1990. </year> <title> Local optimization and the traveling salesman problem. </title> <editor> In Paterson, M. S., editor 1990, </editor> <booktitle> Lecture notes in computer science: Automata, languages and programming. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: As the size of the grid and the number of goal locations increases, the learning time using standard reinforcement learning becomes prohibitive. The arrows indicate the path learned by the modular policy. the Hamiltonian path problem <ref> [ Johnson, 1990 ] </ref> . A drawback to the grid world domain is that all goals are goals of achievement. There are no negative sources of rewards, or states the agent must learn to avoid. Therefore, an arbitration strategy designed to learn a nearest neighbor solution would necessarily perform well. <p> This is not surprising when one realizes that nearest neighbor is a good approximation algorithm for TSP, achieving the optimal tour within a constant factor in Euclidian spaces, and shown experimentally to do extremely well on cases with large numbers of cities <ref> [ Johnson, 1990 ] </ref> . Even outside the grid world, one might be able to generalize the problem of achieving multiple goals to a variant of TSP. If all goals are goals of achievement, the problem might be viewed as finding the shortest path in state-space to all goal states.
Reference: [ Jordan and Jacobs, 1993 ] <author> Jordan, Michael I. and Jacobs, Robert A. </author> <year> 1993. </year> <title> Hierarchical mixtures of experts and the em algorithm. </title> <type> Technical Report 9301, </type> <institution> MIT Computational Cognitive Science. </institution>
Reference-contexts: This approach has also been extended hierarchically, with gating networks at the lowest level feeding into a higher level of networks <ref> [ Jordan and Jacobs, 1993 ] </ref> . This approach has been applied successfully to various classification schemes, as well as robot control. A significant difference between this approach and our modular reinforcement learning is the way the task is decomposed.
Reference: [ Kaelbling, 1989 ] <author> Kaelbling, Leslie P. </author> <year> 1989. </year> <title> A formal framework for learning in embedded systems. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <pages> 350-353. </pages>
Reference: [ Khatib, 1986 ] <author> Khatib, O. </author> <year> 1986. </year> <title> Real-time obstacle avoidance for manipulators and mobile robots. </title> <journal> International Journal of Robotics Research 5(1). </journal>
Reference-contexts: However, the more complex the task, the less likely it is that the state-space is Euclidian, so nearest neighbor might no longer be a good approximation algorithm. The greatest mass has also been used in solutions in other problems. Using potential fields for path planning <ref> [ Khatib, 1986 ] </ref> , [ Arbib and House, 1987 ] , uses 22 the same techniques of gathering utilities, or potentials, from different sources (obstacles and goal locations) and then summing them.
Reference: [ Lin, 1993a ] <author> Lin, </author> <month> Long-Ji </month> <year> 1993a. </year> <title> Reinforcement learning for Robots using neural networks. </title> <type> Ph.D. Dissertation, </type> <institution> Carneige Mellon. </institution>
Reference-contexts: It does not seem to be extensible towards domains with a more complex reward function, allowing partial goal-satisfaction. Another approach to using learned sub-tasks as building blocks for learning higher level tasks is discussed in <ref> [ Lin, 1993a ] </ref> . A robot must learn to recharge its battery by finding a battery charger in another room. The state-space is large and continuous, thus making the problem too difficult for an agent not decomposing the task in some way.
Reference: [ Lin, 1993b ] <author> Lin, </author> <month> Long-Ji </month> <year> 1993b. </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie Mellon, School of Computer Science. </institution>
Reference-contexts: simulated worlds have been given reward functions encoding the information that desirable states are those where the sensors detect simple features, such as: the presence of cheese [ McCallum, 1992 ] , a particular color block in the agent's gripper [ Whitehead, 1991 ] , or some other distinctive marker <ref> [ Lin, 1993b ] </ref>[ Tenenberg et al., 1992 ] .
Reference: [ Maes and Brooks, 1990a ] <author> Maes, Pattie and Brooks, Rodney A. </author> <year> 1990a. </year> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings of AAAI 90. </booktitle>
Reference-contexts: to decide which RAP to execute may not work well where the sub-task interact with each other and penalizes behaviors which call several sub-RAPs (since they are placed at the end of the queue). 17 Maes and Brooks designed a six-legged robot that learned the arbitration strategy for its behaviors <ref> [ Maes and Brooks, 1990a ] </ref> . Their architecture contains a set of binary perceptual conditions, a set of behaviors, a positive feedback generator, and a negative feedback generator. Each behavior has a precondition list testing the status of a set of perceptual conditions.
Reference: [ Maes and Brooks, 1990b ] <author> Maes, Pattie and Brooks, Rodney A. </author> <year> 1990b. </year> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings of AAAI-90. </booktitle> <pages> 796-802. </pages>
Reference-contexts: The designers of the walking robot Genghis constructed a reward function that encoded the knowledge that keeping a touch sensor on the robot's "belly" from being activated, and keeping a motion sensor on the robot's tail activated, was the most desirable state <ref> [ Maes and Brooks, 1990b ] </ref> .
Reference: [ Maes, 1989 ] <author> Maes, </author> <title> Pattie 1989. The dynamics of action selection. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <pages> 100 </pages>
Reference-contexts: It might not be possible to use the simple priority ordering described above to control the sequence of sub-task if the decomposition is more complex than three behaviors, or the exact sequence is unknown. In <ref> [ Maes, 1989 ] </ref> , Maes describes an architecture that allows for more complex types of interactions between modules. Modules are arranged as nodes in a small network.
Reference: [ Maes, 1992 ] <author> Maes, </author> <title> Pattie 1992. Learning behavior networks form experience. </title> <editor> In Varelda, Francisco J. and Bourgin, Paul, editors 1992, </editor> <booktitle> Toward a Practise of Autonomous Systems: Proceedings of the First European conference on Artificial Life. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: However, a large amount of domain knowledge is necessary to determine how sub-task interfere with each other so that the network can be constructed. In later work, Maes has designed methods by which the topology of the network of links is learned, based on past experience <ref> [ Maes, 1992 ] </ref> . Statistical methods are used to correlate actions with results corresponding to preconditions of other modules. Positive correlations lead to links being created and negative ones lead to the deletion of links.
Reference: [ Mahadevan and Connell, 1990 ] <author> Mahadevan, Sridhar and Connell, </author> <title> Jonathan 1990. Automatic programming of behavior-based robots using reinforcement learning. </title> <type> Research Report RC 16359, </type> <institution> IBM T.J. Watson Research Center. </institution>
Reference-contexts: Since it is known in advance what the sequence of sub-tasks is, how to design such an arbitration strategy will also usually be known. Mahadevan and Connell use a subsumption based design for an agent learning to find a box and push it against a wall <ref> [ Mahadevan and Connell, 1990 ] </ref> . The task is divided into finding a box to push, pushing it against a wall, and "unwedging" from the wall. Each task is assigned to a module consisting of a transfer function and an applicability condition.
Reference: [ Mataric, 1994 ] <author> Mataric, </author> <title> Maja 1994. Reward functions for accelerated learning. </title> <booktitle> In The Proceedings of the Eleventh International Machine Learning Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: In contrast, Mataric uses a reinforcement learning approach to learn how to coordinate behaviors, allowing the use of more complex reward functions <ref> [ Mataric, 1994 ] </ref> . The domain consists of a workspace of multiple robots gathering pucks 18 and transporting them to a home location. At any given time the agent can chose between three behaviors: search, disperse, and home. <p> Thus, though kicking the trash might be more efficient, the reward function is biased against any solution that does not involve picking up the trash first. Increasing the complexity of the reward function can clearly speed up learning 30 <ref> [ Mataric, 1994 ] </ref> . The rewards can be used to guide the agent in what is deemed to be a desirable direction towards the goal, thereby limiting the amount of random exploration that must be undertaken before a solution is found.
Reference: [ McCallum, 1992 ] <author> McCallum, R. </author> <title> Andrew 1992. First results with utile distinction memory for reinforcement learning. </title> <type> Technical Report 446, </type> <institution> University of Rochester Computer Science Dept. </institution>
Reference-contexts: Similarly, agents in simulated worlds have been given reward functions encoding the information that desirable states are those where the sensors detect simple features, such as: the presence of cheese <ref> [ McCallum, 1992 ] </ref> , a particular color block in the agent's gripper [ Whitehead, 1991 ] , or some other distinctive marker [ Lin, 1993b ][ Tenenberg et al., 1992 ] .
Reference: [ McCallum, 1996 ] <author> McCallum, Andrew Kachites 1996. </author> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> Ph.D. Dissertation, </type> <institution> University of Rochester. </institution>
Reference-contexts: Since this inconsistency can hamper learning, it is usually combated by attempting to gather more information about the state [ Whitehead, 1991 ] , or using some form of short term memory <ref> [ McCallum, 1996 ] </ref> .
Reference: [ Minsky, 1954 ] <author> Minsky, Marvin L. </author> <year> 1954. </year> <title> Theory of Neural-Analog Reinforcement Systems and Its Application to The Brain-Model Problem. </title> <type> Ph.D. Dissertation, </type> <institution> Princeton University. </institution>
Reference-contexts: We briefly review Q-learning here, and a more in-depth description is available in [ Watkins, 1989 ] . Reinforcement learning draws much of its inspiration from conditioning mechanisms observed in animals subjected to positive and negative stimuli <ref> [ Minsky, 1954 ] </ref> . Animals often can not be directly instructed on how to perform a task, but can be taught by administering feedback that is pleasurable or painful, depending on the animal's performance.
Reference: [ Peng and Williams, 1992 ] <author> Peng, Jing and Williams, Ronald J. </author> <year> 1992. </year> <title> Efficient learning and planning within the dyna framework. </title> <booktitle> In From Animals to An-imats:Proceedings of the Second International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: given no prior knowledge about the expected results of actions, but attempts to learn the range of possible results through experimentation in the domain. 3.3 The reward function Often researchers describing their model of agent/world interaction, depict the reinforcement signal as coming from the environment or world to the agent <ref> [ Peng and Williams, 1992 ] </ref>[ Watkins, 1989 ] . In a biological system this may be true. The brain translates different stimuli into pleasant or unpleasant sensations depending on the current task and context.
Reference: [ Sacerdoti, 1974 ] <author> Sacerdoti, E. D. </author> <year> 1974. </year> <title> Planning in a hierarchy of abstraction spaces. </title> <booktitle> Artificial Intelligence 5(2) </booktitle> <pages> 115-135. </pages>
Reference: [ Sacerdoti, 1975 ] <author> Sacerdoti, E. D. </author> <year> 1975. </year> <title> The non-linear nature of plans. </title> <booktitle> In IJCAI-4. </booktitle> <pages> 206-214. </pages>
Reference-contexts: Once the sub-goal has been achieved, the calling module is resumed, with the total reward received by achieving the sub-task used as the reward for executing the invoke i action. This approach mirrors the concept of abstraction in classical planning ( [ Sacer-doti, 1974 ] , <ref> [ Sacerdoti, 1975 ] </ref> , [ Tenenberg, 1988 ] ), where information is limited in higher levels using lower levels as building blocks. Though the approach appears simple, creating elemental building blocks out of frequently used sub-goals, the difficulty lies in identifying appropriate sub-tasks.
Reference: [ Samuel, 1963 ] <author> Samuel, A. L. </author> <year> 1963. </year> <title> Some studies in machine learning using the game of checkers. </title> <editor> In Feigenbaum, E. and Feldman, J., editors 1963, </editor> <booktitle> Computers and Thought. </booktitle> <publisher> Krieger, </publisher> <address> Malabar, FL. </address> <pages> 71-105. </pages>
Reference-contexts: One early application of an incremental approach was Samuel's checker player, which adjusted its evaluation of the current board state based on the evaluation of the board layout after the move <ref> [ Samuel, 1963 ] </ref> . Sutton formalized the approach, terming it TD (), and proved its convergence to an optimal solution [ Sutton, 1988 ] .
Reference: [ Seidenfeld, 1985 ] <author> Seidenfeld, </author> <month> Teddy </month> <year> 1985. </year> <title> Calibration, coherence, and scoring rules. </title> <booktitle> Philosophy of Science </booktitle> (52):274-294. 
Reference-contexts: The implicit "meta-goal" of a reinforcement learning agent is to maximize some measure of the reward. In order for this goal to be meaningful, there must be some finite time limit after which this measure is assessed <ref> [ Seidenfeld, 1985 ] </ref> . A correct reward function therefore, must rank all possible trajectories through the state-space of a length corresponding to the finite time limit in order of desirability.
Reference: [ Singh, 1992 ] <author> Singh, </author> <month> Satinder </month> <year> 1992. </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <booktitle> Machine Learning 8 </booktitle> <pages> 323-339. 101 </pages>
Reference-contexts: If learning to achieve the sub-tasks is desired in addition to 15 learning the structure of the network, it seems likely that the task would become intractable with even only a small number of modules. Singh proposes an architecture that reuses learned elemental tasks to learn arbitrary sequential tasks <ref> [ Singh, 1992 ] </ref> . Elemental tasks are learned using Q-learning and are defined to accomplish goals of achievement, with only one goal state. A positive reward is given in the goal state of an elemental task and nowhere else. Composite tasks consist of a sequence of elemental tasks.
Reference: [ Spector and Hendler, 1990 ] <author> Spector, Lee and Hendler, </author> <title> James 1990. Knowledge strata: Reactive planning with a multi-level architecture. </title> <type> Technical Report 2564, </type> <institution> Institute for Advanced Computer Studies, Dept of Computer Science and Systems Research Center, University of Maryland. </institution>
Reference-contexts: Since abstraction is not taken advantage of, the hierarchical organization brings little advantage and may be too limiting for some tasks. In contrast, Spector and Hendler discuss a hierarchical approach called "knowledge strata" <ref> [ Spector and Hendler, 1990 ] </ref> that makes explicit use of the hierarchy. There can be several modules on any level of the hierarchy, each operating in parallel. Modules communicate with each other, and with the levels immediately above and below, through separate storage locations called blackboards.
Reference: [ Sutton, 1988 ] <author> Sutton, Richard S. </author> <year> 1988. </year> <title> Learning to predict by the method of temporal differences. </title> <booktitle> Machine Learning 3(1) </booktitle> <pages> 9-44. </pages>
Reference-contexts: However, incremental approaches, where the learning can be made at each time step, before the sequence is complete and the outcome known, have been shown to be more efficient <ref> [ Sutton, 1988 ] </ref> . Using an incremental approach, the learning system can't compare the output given the current state with the desired future outcome, but rather compares it with the output corresponding to the next state. <p> Sutton formalized the approach, terming it TD (), and proved its convergence to an optimal solution <ref> [ Sutton, 1988 ] </ref> . TD () has been further generalized (and the generalization shown to converge) to include cases where information used in learning is not limited to immediately adjacent time steps, but can stem from any arbitrary state [ Dayan, 1992 ] .
Reference: [ Sutton, 1990 ] <author> Sutton, Richard S. </author> <year> 1990. </year> <title> Integrated architectures for learning, planning, and reactingbased on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufman Publishers. </publisher>
Reference-contexts: In both cases care must also be taken to not under-estimate the Q-values, since that 97 would also lead to poor performance. It may also be possible to use an internal simulation of the world, similar to that used by the DYNA system <ref> [ Sutton, 1990 ] </ref> , to arrive at Q-values of an appropriate scale. The ideal extension to the modular approach would be an algorithm that could detect the global states in which the modular approximation failed, and then learned in the monolithic state-space for those states only.
Reference: [ Tenenberg et al., 1992 ] <author> Tenenberg, Josh; Karlsson, Jonas; and Whitehead, </author> <title> Steven 1992. Learning via task decomposition. </title> <booktitle> In Proceedings of the Second International Conference on Simulation of Adaptive Behavior: From Animals to Animats. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: [ Tenenberg, 1988 ] <author> Tenenberg, Josh D. </author> <year> 1988. </year> <title> Abstraction in planning. </title> <type> Ph.D. Dissertation, </type> <institution> University of Rochester. </institution>
Reference-contexts: This approach mirrors the concept of abstraction in classical planning ( [ Sacer-doti, 1974 ] , [ Sacerdoti, 1975 ] , <ref> [ Tenenberg, 1988 ] </ref> ), where information is limited in higher levels using lower levels as building blocks. Though the approach appears simple, creating elemental building blocks out of frequently used sub-goals, the difficulty lies in identifying appropriate sub-tasks.
Reference: [ Tesauro, 1992 ] <author> Tesauro, G. </author> <year> 1992. </year> <title> Practical issues in temporal difference learning. </title> <booktitle> Machine Learning 8 </booktitle> <pages> 257-277. </pages>
Reference-contexts: The approach has also been applied successfully in some domains, most notably in a backgammon player <ref> [ Tesauro, 1992 ] </ref> . However, the TD () algorithm still depends on the presence on a final outcome. Thus, it is mostly suitable for tasks with a single goal of achievement.
Reference: [ Thrun, 1992 ] <author> Thrun, </author> <title> Sebastian 1992. Efficient exploration in reinforcement learning. </title> <type> Technical Report CMU-CS-92-102, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: Clearly this is not possible in practical applications. Typically however, a finite number of steps suffice, and the learning rate and exploration rate are both decayed, so that after a certain time point the agent is not searching for new solutions <ref> [ Thrun, 1992 ] </ref> . It is possible therefore, that an optimal solution is not found by an agent that limits its exploration and learning. <p> We have left the issue of exploration largely unexamined. Exploration is done by executing a random action with some probability, instead of the action recommended by the policy. There are much better exploration strategies available for Q-learning (see <ref> [ Thrun, 1992 ] </ref> for an overview) that could possibly be adapted to the modular approach. For example, counter-based approaches, that try to guide the exploration towards actions which have been executed less frequently could also be used in the modular system.
Reference: [ Watkins and Dayan, 1992 ] <author> Watkins, Christopher and Dayan, </author> <note> Peter 1992. Q-learning. Machine Learning 8. </note>
Reference-contexts: By also assuming that the agent employs an exploration strategy that occasionally deviates from the policy determined by the Q-values (thereby allowing the discovery of new solutions), and an infinite number of time steps, Q-learning can be shown to converge to an optimal policy <ref> [ Watkins and Dayan, 1992 ] </ref> . The two last assumptions are necessary to ensure that once a solution is found, the agent continues to attempt to improve it. Without exploration, the agent would stay with the first solution found, which typically has a lot of room for improvement.
Reference: [ Watkins, 1989 ] <author> Watkins, </author> <title> Chris 1989. Learning from delayed rewards. </title> <type> Ph.D. Dissertation, </type> <institution> Cambridge University. </institution>
Reference-contexts: reward function can be decomposed into a set of independent reward functions, each of which evaluates a different sub-task. * An approximate solution must be acceptable, since the optimal solution is unattainable. 1.1 Statement of thesis This thesis defines a modular extension to the reinforcement learning algorithm known as Q-learning <ref> [ Watkins, 1989 ] </ref> . By using modules, a complex task can be decomposed into a set of independent sub-tasks. Each module is devoted to learning one sub-task, and all modules learn independently and in parallel. <p> Though there are several reinforcement learning algorithms available, we have based our method on Watkins' Q-learning algorithm. We briefly review Q-learning here, and a more in-depth description is available in <ref> [ Watkins, 1989 ] </ref> . Reinforcement learning draws much of its inspiration from conditioning mechanisms observed in animals subjected to positive and negative stimuli [ Minsky, 1954 ] .
Reference: [ Whitehead and Ballard, 1989 ] <author> Whitehead, Steven D. and Ballard, Dana H. </author> <year> 1989. </year> <title> A role for anticipation in reactive systems that learn. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Ithaca, NY. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Many researchers recognize that for any real artificial agent implementation, there is no convenient reward function in the world inserting appropriate 28 values into the agent. The reward function is thus depicted as a part of the agent that translates the observed state into a reward value <ref> [ Whitehead and Ballard, 1989 ] </ref>[ Kaelbling, 1989 ] . That is, part of the agent's a priori knowledge is the ability to recognize states and assign a value to them. A reinforcement learning agent must be provided with its reward function from an external source, typically a human designer.
Reference: [ Whitehead et al., 1992 ] <author> Whitehead, Steven D.; Karlsson, Jonas; and Tenenberg, </author> <title> Josh 1992. Learning multiple goal behavior via task decomposition and dynamic policy merging. In Robot Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: As we described above, adding sub-tasks can cause the size of the search space to increase dramatically. In <ref> [ Whitehead et al., 1992 ] </ref> we illustrate the utility of the modular approach in a simple grid world domain. The domain is an m fi m grid, with n cells designated as goal locations, as shown in Figure 1.2 (a).
Reference: [ Whitehead, 1991 ] <author> Whitehead, Steven D. </author> <year> 1991. </year> <title> Reinforcement Learning for the Adaptive Control of Perception and Action. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Rochester, Rochester, </institution> <address> NY. </address> <month> 102 </month>
Reference-contexts: Similarly, agents in simulated worlds have been given reward functions encoding the information that desirable states are those where the sensors detect simple features, such as: the presence of cheese [ McCallum, 1992 ] , a particular color block in the agent's gripper <ref> [ Whitehead, 1991 ] </ref> , or some other distinctive marker [ Lin, 1993b ][ Tenenberg et al., 1992 ] . <p> It can be shown that in domains with certain properties (such as having reversible actions), the time to find the goal state in the first trial is exponential in the number of states in the domain <ref> [ Whitehead, 1991 ] </ref> . To a large extent, the long search time cannot be avoided. Given that the agent has no domain knowledge, it must explore every state in order to guarantee that the optimal solution is found. <p> Since this inconsistency can hamper learning, it is usually combated by attempting to gather more information about the state <ref> [ Whitehead, 1991 ] </ref> , or using some form of short term memory [ McCallum, 1996 ] .
Reference: [ Wixson, 1991 ] <author> Wixson, Lambert E. </author> <year> 1991. </year> <title> Scaling reinforcement learning techniques via modularity. </title> <editor> In Birnbaum, Lawrence E. and Collins, Gregg C., editors 1991, </editor> <booktitle> Machine Learning:Proceedings of the eighth International workshop. </booktitle> <publisher> Mor-gan Kaufman. </publisher>
Reference-contexts: We are considering domains where solutions are unknown and must be learned. Approaches such as the subsumption architecture and knowledge strata that require modules to be programmed to, in effect, modify each other's solutions would therefore be difficult to apply. Wixson <ref> [ Wixson, 1991 ] </ref> proposes a mechanism to use a hierarchical organization of behaviors in reinforcement learning. The hierarchy is created by defining modules completely responsible for a subset of both inputs and outputs.
References-found: 47

