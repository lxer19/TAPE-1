URL: http://www.isi.edu/~pedro/Misc/CSCI599/papers/raw.ps
Refering-URL: http://www.isi.edu/~pedro/Misc/CSCI599/papers/presentations.html
Root-URL: http://www.isi.edu
Title: The Raw Compiler Project  
Author: A. Agarwal, S. Amarasinghe, R. Barua, M. Frank, W. Lee, V. Sarkar, D. Srikrishna, M. Taylor 
Web: http://cag-www.lcs.mit.edu/raw  
Address: Cambridge, MA 02139  
Affiliation: MIT Laboratory for Computer Science  
Abstract: Compilers today are capable of inferring detailed information about program parallelism and analyzing whole-program behavior. However, the traditional interface between the compiler and the processor, as defined by the instruction set architecture (ISA), is unable to communicate much of the compiler knowledge to the processor. The approach taken by modern processors such as superscalars is to incorporate purely run-time algorithms in their hardware to perform analyses and optimizations such as detection of instruction-level parallelism. However, these complex hardware implementations can only exploit a small fraction of the parallelism information available to the compiler. The Raw architecture developed at MIT aims to maximally utilize the compiler by fully exposing the hardware and by delegating the hardware's control completely to the software system. The Raw microprocessor, a set of simple RISC-like processor tiles interconnected with a high-speed 2D mesh network, does not provide hardware implementations for any of the complex algorithms found in conventional microprocessors. Instead, the compiler and the run-time software system fully orchestrate the Raw hardware resources, and they implement run-time analyses and optimizations tailored to the need of each individual application. This novel approach provides many opportunities and challenges for the Raw compiler and run-time system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, M. Ganapathi, and S. W. K. Tjiang. </author> <title> Code generation using tree matching and dynamic programming. </title> <journal> ACM TOPLAS, </journal> <volume> 11(4), </volume> <month> October </month> <year> 1989. </year>
Reference-contexts: It thus provides improved computation density and is more suitable for stream-based multimedia applications. One of the most difficult tasks of the compiler will be to identify the regions of code well suited to configurable implementation. We are exploring the possibility of using tree-pattern matching techniques <ref> [1, 10] </ref> to assist in identifying these regions.
Reference: [2] <author> J. Auslander, M. Philipose, C. Chambers, S. J. Eggers, and B. N. Bershad. </author> <title> Fast, effective dynamic compilation. </title> <booktitle> In Proc. PLDI '96, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Finally, the run-time system has to provide support for performance monitoring and debugging. Because the requested features (e.g., a breakpoint) may change while the program is running, the run-time system needs to support dynamic code scheduling <ref> [2, 5, 6, 15] </ref>. Because the run-time system inserts code only where it is needed, the user will not need to pay any additional overhead for features they are not currently using. The overall performance of the Raw system will depend on a number of factors.
Reference: [3] <author> J. Babb, M. Frank, V. Lee, E. Waingold, R. Barua, M. Taylor, J. Kim, S. Devabhaktuni, and A. Agarwal. </author> <title> The raw benchmark suite: Computation structures for general purpose computing. </title> <booktitle> In IEEE Symposium on Field-Programmable Custom Computing Machines, </booktitle> <address> Napa Valley, CA, </address> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: There is also a small amount of bit-level and control logic to support special bit-level and conditional operations. A Raw processor is thus coarser than a traditional FPGA-based computer <ref> [3] </ref>, which evolved from structures better suited for random hardware glue logic than datapath oriented computation. Compared to a FPGA computer, the Raw configurable logic has a smaller configuration state, better propagation delays for common operations, lower routing requirements, and smaller area per ALU-like operation.
Reference: [4] <author> J. Babb, R. Tessier, and A. Agarwal. </author> <title> Virtual Wires: Overcoming Pin Limitations in FPGA-based Logic Emulators. </title> <booktitle> In Proceedings IEEE Workshop on FPGA-based Custom Computing Machines, </booktitle> <pages> pages 142-151, </pages> <address> Napa, CA, </address> <month> April </month> <year> 1993. </year> <note> IEEE. Also as MIT/LCS TM-491, </note> <month> January </month> <year> 1993. </year>
Reference-contexts: We are exploring the possibility of using tree-pattern matching techniques [1, 10] to assist in identifying these regions. Once these regions are identified, hardware compilation algorithms can be employed to compile the critical program patterns into the configurable hardware <ref> [4] </ref>. 4.5 Code generation Because the Raw compiler is hardware-omniscient, its code generation phase is more involved than that of a traditional compiler.
Reference: [5] <author> B. Cmelik and D. Keppel. Shade: </author> <title> A Fast Instruction-Set Simulator for Execution Profiling. </title> <booktitle> In Proc. 1994 ACM SIGMETRICS Conference, </booktitle> <pages> pages 128-137, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Finally, the run-time system has to provide support for performance monitoring and debugging. Because the requested features (e.g., a breakpoint) may change while the program is running, the run-time system needs to support dynamic code scheduling <ref> [2, 5, 6, 15] </ref>. Because the run-time system inserts code only where it is needed, the user will not need to pay any additional overhead for features they are not currently using. The overall performance of the Raw system will depend on a number of factors.
Reference: [6] <author> D. R. Engler. </author> <title> VCODE: A retargetable, extensible, very fast dynamic code generation system. </title> <booktitle> In Proc. PLDI '96, </booktitle> <pages> pages 160-170, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Finally, the run-time system has to provide support for performance monitoring and debugging. Because the requested features (e.g., a breakpoint) may change while the program is running, the run-time system needs to support dynamic code scheduling <ref> [2, 5, 6, 15] </ref>. Because the run-time system inserts code only where it is needed, the user will not need to pay any additional overhead for features they are not currently using. The overall performance of the Raw system will depend on a number of factors.
Reference: [7] <author> R. W. et al. </author> <title> SUIF: A Parallelizing and Optimizing Research Compiler. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(12) </volume> <pages> 31-37, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: See Figure 3 for how the compiler outputs relate to the architecture. 5 Compiler Implementation We are using the SUIF compiler infrastructure to implement the Raw compiler. SUIF has many features which make it an ideal framework for the Raw compiler project <ref> [7] </ref>. It provides a single tool under which all compiler development, from whole program analysis for resource allocation 9 to switch and tile code generation, can be performed. We plan to leverage on many aggressive analyses such as data-flow analysis and pointer alias analyses available for the SUIF infrastructure.
Reference: [8] <author> J. A. Fisher. </author> <title> Very Long Instruction Word Architectures and the ELI-512. </title> <booktitle> In Proc. 10th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 140-150, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: There have been numerous attempts to use compiler and run-time software technology to eliminate the complex algorithms from hardware. A prominent example of this approach is the VLIW processor <ref> [8] </ref>. However, most of these approaches had concentrated on off-loading individual algorithms and optimizations from hardware to software. We are developing a novel approach that fully exposes the low-level details of the hardware architecture to the compiler.
Reference: [9] <author> M. W. Hall, S. P. Amarasinghe, B. R. Murphy, S. Liao, and M. S. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <address> San Diego, CA, </address> <month> Dec. </month> <year> 1995. </year> <month> 11 </month>
Reference-contexts: The Raw compiler needs to perform whole program analysis to obtain the run-time resource requirements and the availability of fine-grain and coarse-grain parallelism <ref> [9] </ref>. 4.2 Exploitation of fine-grain parallelism Raw has the flexibility to explore many forms of parallelism. It can support both the SIMD and MIMD programming models used by conventional multiprocessors. In fact, its low latency network allows it to attain speedup for a bigger set of such applications.
Reference: [10] <author> T. A. Proebsting. </author> <title> Simple and efficient BURS table generation. </title> <booktitle> Proceedings of the ACM SIG--PLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <address> San Fransico, California., </address> <month> June </month> <year> 1992. </year>
Reference-contexts: It thus provides improved computation density and is more suitable for stream-based multimedia applications. One of the most difficult tasks of the compiler will be to identify the regions of code well suited to configurable implementation. We are exploring the possibility of using tree-pattern matching techniques <ref> [1, 10] </ref> to assist in identifying these regions.
Reference: [11] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> Pitman, London and The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year> <booktitle> In the series, Research Monographs in Parallel and Distributed Computing. </booktitle>
Reference-contexts: The partitioning and scheduling framework in the compiler builds on the work reported in <ref> [11, 16] </ref>. In addition, the compiler can turn certain loops into inter-tile pipelines which leverage the low cost of communication to achieve maximum throughputs. When both the static and dynamic networks are used in the same application, we need to consider the issues which arise due to their interaction.
Reference: [12] <author> D. J. Scales, K. Gharachorloo, and C. A. Thekkath. </author> <title> Shasta: A Low Overhead, Software-Only Approach for Supporting Fine-Grain Shared Memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 174-185, </pages> <address> Cambridge, Massachusetts, </address> <month> October 1-5, </month> <year> 1996. </year>
Reference-contexts: For example, Raw can do caching in software. The run-time system manages the memory hierarchy by checking each memory access and providing the current mapping of the requested address. As with other systems that use software to manage the memory system <ref> [12, 13, 14] </ref>, the compiler has two responsibilities. It must insert code to perform these checks at each memory reference, and it must optimize away checks that can be statically determined to be redundant. Inevitably, the baseline cost for software caching is higher than that for hardware caching.
Reference: [13] <author> I. Schoinas, B. Falsafi, A. Lebeck, S. Reinhardt, J. Larus, and D. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proc. ASPLOS VI, </booktitle> <pages> pages 297-306, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: For example, Raw can do caching in software. The run-time system manages the memory hierarchy by checking each memory access and providing the current mapping of the requested address. As with other systems that use software to manage the memory system <ref> [12, 13, 14] </ref>, the compiler has two responsibilities. It must insert code to perform these checks at each memory reference, and it must optimize away checks that can be statically determined to be redundant. Inevitably, the baseline cost for software caching is higher than that for hardware caching.
Reference: [14] <author> R. Wahbe, S. Lucco, T. Anderson, and S. Graham. </author> <title> Efficient Software-Based Fault Isolation. </title> <booktitle> In Proc. Fourteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 203-216, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: For example, Raw can do caching in software. The run-time system manages the memory hierarchy by checking each memory access and providing the current mapping of the requested address. As with other systems that use software to manage the memory system <ref> [12, 13, 14] </ref>, the compiler has two responsibilities. It must insert code to perform these checks at each memory reference, and it must optimize away checks that can be statically determined to be redundant. Inevitably, the baseline cost for software caching is higher than that for hardware caching.
Reference: [15] <author> R. Wahbe, S. Lucco, and S. L. Graham. </author> <title> Practical Data Breakpoints: </title> <booktitle> Design and Implementation. In Proc. PLDI '93, </booktitle> <pages> pages 1-12, </pages> <month> June 23-25, </month> <year> 1993. </year>
Reference-contexts: Finally, the run-time system has to provide support for performance monitoring and debugging. Because the requested features (e.g., a breakpoint) may change while the program is running, the run-time system needs to support dynamic code scheduling <ref> [2, 5, 6, 15] </ref>. Because the run-time system inserts code only where it is needed, the user will not need to pay any additional overhead for features they are not currently using. The overall performance of the Raw system will depend on a number of factors.
Reference: [16] <author> T. Yang and A. Gerasoulis. </author> <title> DSC: Scheduling parallel tasks on an unbounded number of processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(9) </volume> <pages> 951-967, </pages> <year> 1994. </year> <month> 12 </month>
Reference-contexts: The partitioning and scheduling framework in the compiler builds on the work reported in <ref> [11, 16] </ref>. In addition, the compiler can turn certain loops into inter-tile pipelines which leverage the low cost of communication to achieve maximum throughputs. When both the static and dynamic networks are used in the same application, we need to consider the issues which arise due to their interaction.
References-found: 16

