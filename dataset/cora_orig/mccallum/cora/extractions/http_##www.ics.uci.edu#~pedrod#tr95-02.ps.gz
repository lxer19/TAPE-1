URL: http://www.ics.uci.edu/~pedrod/tr95-02.ps.gz
Refering-URL: http://www.ics.uci.edu/~pedrod/
Root-URL: 
Abstract: The RISE 2.0 System: A Case Study in Multistrategy Learning Pedro Domingos pedrod@ics.uci.edu Technical Report 95-2 January 9, 1995 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference-contexts: As a re sult, research in this field has blossomed, and several mature approaches to inductive learning are now available to the practitioner. These include induction of decision trees (Quinlan, 1986), rule induction (Michalski, 1983), instance-based learning <ref> (Aha et al, 1991) </ref>, Bayesian classification (Buntine, 1989), back-propagation (Rumelhart et al, 1986), and genetic algorithms (Booker et al, 1989). Empirical comparison of these different approaches in a variety of application do mains has shown that each performs best in some, but not all, domains. <p> Another issue in IBL methods is their sensitivity to noise. Incorrect instances are liable to create an area around them where new examples will also be misclassi fied. Several methods have been successfully introduced to deal with this problem. IB3 <ref> (Aha et al, 1991) </ref> retains only reliable instances, reliability being judged by the instance's classification performance over a "probation period." PEBLS (Cost and Salzberg, 1993) assign weights to instances, making their apparent distance to new examples increase with their misclassification rate. <p> With multiple instances of each class, the frontier will be composed of a number of hyperplanar sections, and can thus become quite complex even with few instances <ref> (Aha et al, 1991) </ref>. The introduction of weights further increases this complexity, turning the hyperplanes into hyperquadrics (Cost and Salzberg, 1993). Another extension to the basic IBL paradigm consists in using the k nearest neighbors for classification, instead of just the nearest one (Duda and Hart, 1973).
Reference: <author> Belew, R. K, McInerney, J., & Schraudolph, N. N. </author> <year> (1992). </year> <title> Evolving networks: Using the genetic algorithm with connectionist learning. </title> <editor> In C. G. Langton, C. Taylor, J. D. Farmer, & S. Rasmussen (Eds.), </editor> <booktitle> Artificial Life II. </booktitle> <address> Redwood City, CA: </address> <publisher> Addison-Wesley. </publisher> <editor> 24 Booker, L. B, Goldberg, D. E., & Holland, J. H. </editor> <year> (1989). </year> <title> Classifier systems and genetic algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 40, </volume> <pages> 235-282. </pages>
Reference-contexts: Several induction al gorithms proposed in the literature can be seen as empirical multi-strategy learners, but combining different paradigms from RISE's: decision trees and rules (Quinlan, 1987), decision trees and perceptrons (Utgoff, 1989), rules and Bayesian classification (Smyth et al, 1990), back-propagation and genetic algorithms <ref> (Belew et al, 1992) </ref>, etc. In form, the most similar system to RISE in the literature is EACH (Salzberg, 1991), which produces and uses hyperrectangles generalized from specific instances.
Reference: <author> Brodley, C. E. </author> <year> (1993). </year> <title> Addressing the selective superiority problem: Automatic algorithm/model class selection. </title> <booktitle> Proceedings of the Tenth International Con ference on Machine Learning (pp. </booktitle> <pages> 17-24). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Empirical comparison of these different approaches in a variety of application do mains has shown that each performs best in some, but not all, domains. This has been termed the "selective superiority" problem <ref> (Brodley, 1993) </ref>, and presents a dilemma to the knowledge engineer approaching a new task: which induction paradigm should be used? One solution is to try each one in turn, and use cross-validation to choose the one that appears to perform best. <p> Unfortunately, the success of this approach has so far been moderate. The re sulting algorithms are prone to be cumbersome, and often achieve accuracies that lie 1 between those of their parents, instead of matching the highest <ref> (e.g. Brodley, 1993) </ref>. Here a theoretical question arises. <p> In 10 of those domains this is true with a confidence level of 95% or higher, using a one-tailed paired t test (see Tables 5 and 6). This stands in contrast to the results achieved by e.g. MCS <ref> (Brodley, 1993) </ref>, and shows that a significant synergy can be obtained by combining multiple empirical learning approaches. 22 7 Discussion The empirical study just described leads to the conclusion that RISE represents a significant advance in the state-of-the-art in empirical learning. <p> It differs from RISE in many ways, however: it is applicable only in purely numerical domains, is an incremental algorithm, never drops attributes, uses different heuristics and search strategies, allows only nested hyperrectangles as opposed to arbitrary in tersecting ones, always prefers the most specific hyperrectangle, etc. MCS <ref> (Brodley, 1993) </ref> has the most similar aims to RISE's, but uses an entirely different approach (applying meta-knowledge to detect when one algorithm should be applied instead of another), and generally achieved accuracies between those of the individual algo rithms.
Reference: <author> Buntine, W. </author> <year> (1989). </year> <title> Learning classification rules using Bayes. </title> <booktitle> Proceedings of the Sixth International Conference on Machine Learning (pp. </booktitle> <pages> 94-98). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As a re sult, research in this field has blossomed, and several mature approaches to inductive learning are now available to the practitioner. These include induction of decision trees (Quinlan, 1986), rule induction (Michalski, 1983), instance-based learning (Aha et al, 1991), Bayesian classification <ref> (Buntine, 1989) </ref>, back-propagation (Rumelhart et al, 1986), and genetic algorithms (Booker et al, 1989). Empirical comparison of these different approaches in a variety of application do mains has shown that each performs best in some, but not all, domains.
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> Megainduction: A test flight. </title> <booktitle> Proceedings of the Eighth In ternational Conference on Machine Learning (pp. </booktitle> <pages> 589-604). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: RISE has not been opti mized, however, and several important components of the system are amenable to such optimization. Beyond that, windowing and other sampling techniques can be used without expected loss in accuracy <ref> (Catlett, 1991) </ref>. RISE's rule sets are also not as compact as those produced by C4.5 or CN2, although as reported earlier there may be a trade-off between simplicity and accuracy.
Reference: <author> Clark, P., & Boswell, R. </author> <year> (1991). </year> <title> Rule induction with CN2: Some recent improve ments. </title> <booktitle> Proceedings of the Sixth European Working Session on Learning (pp. </booktitle> <pages> 151-163). </pages> <address> London: </address> <publisher> Pitman. </publisher>
Reference-contexts: This measure tends to the uncorrected accuracy when the rule has strong statistical support, i.e. when it covers many examples, but tends to 1=C, i.e. "maximum ignorance," when it covers few. It is used in recent versions of CN2 <ref> (Clark and Boswell, 1991) </ref>. Classification of a new example is performed by matching each rule against it, and selecting those it satisfies. If there is only one, its class is assigned to the example. <p> The other is to let the different rules vote, and select the class receiving the most votes. Recent versions of CN2 attach to each rule the number of examples of each class that it covers, and use these numbers as votes at classification time <ref> (Clark and Boswell, 1991) </ref>. The use of unordered rules has been found to generally achieve higher accuracy (Clark and Boswell, 1991), and also has the advantage of greater comprehensibility, since in a decision list each rule body is implicitly conjoined with the negations of all those that precede it. <p> Recent versions of CN2 attach to each rule the number of examples of each class that it covers, and use these numbers as votes at classification time <ref> (Clark and Boswell, 1991) </ref>. The use of unordered rules has been found to generally achieve higher accuracy (Clark and Boswell, 1991), and also has the advantage of greater comprehensibility, since in a decision list each rule body is implicitly conjoined with the negations of all those that precede it. In rule induction algorithms that do not deal with noise (e.g. <p> The policy 17 was adopted of using the number of observed values or 10, whichever was lowest. Ad hoc variation of this value produced no appreciable change in results. A recent version of CN2 (6.1) was used, one incorporating Laplace accuracy and the use of unordered rules <ref> (Clark and Boswell, 1991) </ref>. Again, all default settings were used. To gauge its position in the overall spectrum of induction methods, RISE was also compared with a decision tree learner, C4.5 (Quinlan, 1993a).
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283. </pages>
Reference-contexts: The AQ series of algorithms (Michalski et al, 1986) uses apparent accuracy, i.e. the accuracy of the rule on the training set: H (N ; N ) = N + N The CN2 system <ref> (Clark and Niblett, 1989) </ref> originally used entropy gain (Quinlan, 1986). The problem with both these measures, however, is that they tend to favor overly specific rules, because they attain their maximum value with a rule covering a single example. <p> CN2 is O (BE 2 A 2 ), where B is the beam size, an integer-valued internal parameter of the algorithm. (Note that the computations in <ref> (Clark and Niblett, 1989) </ref> are only for the basic step of the algorithm, which is embed ded in loops that may run O (EA) in the worst case.) Thus RISE's worst-case time complexity is comparable to that of CN2, and to those of of similar rule and decision tree induction systems. <p> will be seen below, the pattern of results is identical in the two groups, and testing on more domains produces a higher level of confidence in the results.) A representative of each of RISE's parent approaches was used: PEBLS for IBL (Cost and Salzberg, 1993), and CN2 for rule induction <ref> (Clark and Niblett, 1989) </ref>. Note that PEBLS is a state-of-the art system, as opposed to the skeleton nearest-neighbor implementations typically used in empirical comparisons. PEBLS 2.1's inability to deal with missing values was overcome by grafting onto it an approach similar to the one selected for RISE (see previous section).
Reference: <author> Cost, S., & Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 57-78. </pages>
Reference-contexts: Aha et al, 1991) use a simple overlap metric: ffi (x i ; x j ) = 0 if i = j 1 otherwise (3) This measure is obviously less informative than its numeric counterpart, and its use has often been found to lead to poor performance <ref> (Cost and Salzberg, 1993) </ref>. A more insightful alternative, first proposed by Stanfill and Waltz in 1986, consists of considering two symbolic values to be similar if they make similar predictions, i.e. if they correlate similarly with the class feature. <p> Several methods have been successfully introduced to deal with this problem. IB3 (Aha et al, 1991) retains only reliable instances, reliability being judged by the instance's classification performance over a "probation period." PEBLS <ref> (Cost and Salzberg, 1993) </ref> assign weights to instances, making their apparent distance to new examples increase with their misclassification rate. Given two instances of opposite classes, the frontier between classes induced by them is a hyperplane perpendicular to the line connecting the two instances, and bisecting it. <p> With multiple instances of each class, the frontier will be composed of a number of hyperplanar sections, and can thus become quite complex even with few instances (Aha et al, 1991). The introduction of weights further increases this complexity, turning the hyperplanes into hyperquadrics <ref> (Cost and Salzberg, 1993) </ref>. Another extension to the basic IBL paradigm consists in using the k nearest neighbors for classification, instead of just the nearest one (Duda and Hart, 1973). <p> Two versions were compared: no weights, and each rule weighted by the inverse of its Laplace accuracy, leading unreliable rules to appear farther from new instances. This is similar to the weighting scheme used in PEBLS <ref> (Cost and Salzberg, 1993) </ref>, but slightly more sophisticated because the Laplace correction is used. Note that this correction is indeed justified when dealing with individual rules. No significant difference was observed. <p> Values of k = 1 and k = 2 (see Eq. 4) were tried, each combined with values of r = 1 and r = 2 (see Eq. 7). No appreciable difference was observed, confirming previous results <ref> (Cost and Salzberg, 1993) </ref>. The default values for RISE are k = 1 and r = 2 (Euclidean distance). * Treatment of numeric values. <p> performed on only the remaining 15 domains, but as will be seen below, the pattern of results is identical in the two groups, and testing on more domains produces a higher level of confidence in the results.) A representative of each of RISE's parent approaches was used: PEBLS for IBL <ref> (Cost and Salzberg, 1993) </ref>, and CN2 for rule induction (Clark and Niblett, 1989). Note that PEBLS is a state-of-the art system, as opposed to the skeleton nearest-neighbor implementations typically used in empirical comparisons.
Reference: <author> Cover, T. M., & Hart, P. E. </author> <year> (1967). </year> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13, </volume> <pages> 21-27. </pages>
Reference: <author> DeGroot, M. H. </author> <year> (1986). </year> <title> Probability and Statistics (Second Edition). </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The sign test then measures how unlikely the number of +'s observed is according to the binomial distribution, under the null hypothesis that there is no difference, i.e. P (+) = 1 2 <ref> (DeGroot, 1986) </ref>. (A draw adds 0.5 to the binomial variable. <p> Intuitively the magnitude of the differences should also be taken into account, but not so much that domains where there are naturally higher variations acquire undue weight. The Wilcoxon test is a more sensitive procedure that addresses these issues <ref> (DeGroot, 1986) </ref>. It takes into account the ranks of the magnitudes, though not their exact values, i.e. the largest difference counts more than the second largest one and so forth, but it does not matter by how much.
Reference: <author> Domingos, P. </author> <year> (1994). </year> <title> The RISE system: Conquering without separating. </title> <booktitle> Submitted to the Sixth IEEE International Conference on Tools with Artificial Intelligence. </booktitle>
Reference-contexts: This methodol ogy is implemented in the RISE 2.0 system (Rule Induction from a Set of Exemplars; an earlier version, RISE 1.0, is described in <ref> (Domingos, 1994) </ref>). One of its basic fea tures is that rules and instances are treated uniformly; no distinction is made between 7 the two.
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: The introduction of weights further increases this complexity, turning the hyperplanes into hyperquadrics (Cost and Salzberg, 1993). Another extension to the basic IBL paradigm consists in using the k nearest neighbors for classification, instead of just the nearest one <ref> (Duda and Hart, 1973) </ref>. The class assigned is then that of the majority of those k neighbors, or alternatively the most voted one, with a neighbor's vote decreasing with its distance from the test example.
Reference: <author> Golding, A. R., & Rosenbloom, P. S. </author> <year> (1991). </year> <title> Improving rule-based systems through case-based reasoning. </title> <booktitle> Proceedings of the Ninth National Conference on Artifi cial Intelligence (pp. </booktitle> <pages> 22-27). </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 63-91. </pages>
Reference-contexts: Essentially all the datasets in the UCI repository that didn't fall under one of these restrictions were included in the study. Table 4 lists these datasets in alphabetical order of code-name, and summarizes some of their main characteristics. Domains included in the listing of empirical results in <ref> (Holte, 1993) </ref> are referred to by the same codes.
Reference: <author> Kolodner, J. </author> <year> (1993). </year> <title> Case-Based Reasoning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. 25 Michalski, </publisher> <editor> R. S., & Tecuci, G. (Eds.) </editor> <year> (1994). </year> <title> Machine Learning: A Multistrategy Approach. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: often dispose easily of irrelevant attributes. (The two paradigms also share a number of characteristics, of course, most notably the assumption that the instance space contains large continuous regions of constant class membership|the similarity hypothesis.) Instances and rules also form the basis of two competing approaches to reasoning: case-based reasoning <ref> (Kolodner, 1993) </ref> and the rule-based reasoning more often found in expert systems.
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 111-161. </pages>
Reference-contexts: As a re sult, research in this field has blossomed, and several mature approaches to inductive learning are now available to the practitioner. These include induction of decision trees (Quinlan, 1986), rule induction <ref> (Michalski, 1983) </ref>, instance-based learning (Aha et al, 1991), Bayesian classification (Buntine, 1989), back-propagation (Rumelhart et al, 1986), and genetic algorithms (Booker et al, 1989). Empirical comparison of these different approaches in a variety of application do mains has shown that each performs best in some, but not all, domains.
Reference: <author> Michalski, R. S., Mozetic, I., Hong, J., & Lavrac, N. </author> <year> (1986). </year> <title> The multi-purpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 1041-1045). </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The choice of evaluation heuristic H (N , N ) is of some importance to the al gorithm's performance. H (N , N ) should be an increasing function of N , and a decreasing function of N . The AQ series of algorithms <ref> (Michalski et al, 1986) </ref> uses apparent accuracy, i.e. the accuracy of the rule on the training set: H (N ; N ) = N + N The CN2 system (Clark and Niblett, 1989) originally used entropy gain (Quinlan, 1986). <p> However, this simplification was accompanied by a small overall decrease in accuracy, and post-processing is thus not the default in RISE. These observations stand in contrast to studies on decision tree learners (Quin lan, 1987b) and the AQ15 rule induction system <ref> (Michalski et al, 1986) </ref>, where simplification was (surprisingly) accompanied by an increase in accuracy. <p> However, for domains of very large size, and/or when comprehensibility is paramount, a system like C4.5 will still be the first choice. The RISE approach should be seen in the context of previous work in induc tive learning and related areas. The AQ15 system <ref> (Michalski et al, 1986) </ref> employed best-match classification, but used a more primitive overlap-based distance measure. Golding and Rosenbloom (1991) designed a system that gainfully combined case based and rule-based reasoning, but it did not learn, and was hindered by the use of different representations for cases and rules.
Reference: <author> Mitchell, T. M. </author> <year> (1980). </year> <title> The need for biases in learning generalizations. </title> <type> Technical re port. </type> <institution> New Brunswick, NJ: Rutgers University, Computer Science Department. </institution>
Reference-contexts: Brodley, 1993). Here a theoretical question arises. It is well known that no induction algorithm can be the best in all possible domains; each algorithm contains an explicit or implicit bias <ref> (Mitchell, 1980) </ref> that leads it to prefer certain generalizations over others, and it will be successful only insofar as this bias matches the characteristics of the application domain.
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1990). </year> <title> Efficient induction of logic programs. </title> <booktitle> Proceedings of the First Workshop on Algorithmic Learning Theory (pp. </booktitle> <pages> 368-381). </pages> <address> Tokyo: </address> <publisher> Ohmsha. </publisher>
Reference: <author> Murphy, P., & Pazzani, M. </author> <year> (1994). </year> <title> Exploring the decision forest: An empirical investigation of Occam's razor in decision tree induction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 257-275. </pages>
Reference: <author> Murphy, P. M., & Aha, D. W. </author> <year> (1992). </year> <title> UCI repository of machine learning databases. Machine-readable data repository. </title> <address> Irvine, CA: </address> <institution> University of California, Depart ment of Information and Computer Science. </institution>
Reference-contexts: All domains were drawn from the UCI repository <ref> (Murphy and Aha, 1992) </ref>. Reasons for excluding datasets in this repository from the study were: * Some datasets are inappropriate for this type of algorithm.
Reference: <author> Niblett, T. </author> <year> (1987). </year> <title> Constructing decision trees in noisy domains. </title> <booktitle> Proceedings of the Second European Working Session on Learning (pp. </booktitle> <pages> 67-78). </pages> <address> London: </address> <publisher> Pitman. </publisher>
Reference-contexts: The problem with both these measures, however, is that they tend to favor overly specific rules, because they attain their maximum value with a rule covering a single example. This can be overcome by use of the Laplace correction <ref> (Niblett, 1987) </ref>: N + 1 (6) Table 1: General structure of rule induction algorithms. Input: ES is the training set. Procedure Rule Induction (ES) Let RS = ;. For each class C Let = fe 2 ES j Class (E) = Cg.
Reference: <author> Ourston, D., & Mooney, R. J. </author> <year> (1994). </year> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66, </volume> <pages> 273-309. </pages>
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 71-99. </pages>
Reference-contexts: Finally, if more than one rule covers the example, two strategies are possible. One is to order the rules into a "decision list," and select only the first rule that fires <ref> (Pagallo and Haussler, 1990) </ref>. The other is to let the different rules vote, and select the class receiving the most votes.
Reference: <author> Pazzani, M., & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 57-94. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: As a re sult, research in this field has blossomed, and several mature approaches to inductive learning are now available to the practitioner. These include induction of decision trees <ref> (Quinlan, 1986) </ref>, rule induction (Michalski, 1983), instance-based learning (Aha et al, 1991), Bayesian classification (Buntine, 1989), back-propagation (Rumelhart et al, 1986), and genetic algorithms (Booker et al, 1989). <p> The AQ series of algorithms (Michalski et al, 1986) uses apparent accuracy, i.e. the accuracy of the rule on the training set: H (N ; N ) = N + N The CN2 system (Clark and Niblett, 1989) originally used entropy gain <ref> (Quinlan, 1986) </ref>. The problem with both these measures, however, is that they tend to favor overly specific rules, because they attain their maximum value with a rule covering a single example.
Reference: <author> Quinlan, J. R. </author> <year> (1987a). </year> <title> Generating production rules from decision trees. </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence (pp. 304 307). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: C4.5RULES, the version of C4.5 that converts fully-sprouted decision trees into production rules and then prunes the latter, was chosen because this approach has been observed to achieve the highest accuracy <ref> (Quinlan, 1987a) </ref>, as well as being the one most directly comparable to RISE. The default classifier (always choosing the most frequent class) was also included in the study to provide a baseline.
Reference: <author> Quinlan, J. R. </author> <year> (1987b). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man Machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages> <note> 26 Quinlan, </note> <author> J. R. </author> <year> (1993a). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1993b). </year> <title> Combining instance-based and model-based learning. </title> <booktitle> Pro ceedings of the Tenth International Conference on Machine Learning (pp. 236 243). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Riesbeck, C. K., & Schank, R. C. </author> <year> (1989). </year> <title> Inside Case-Based Reasoning. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: In recent years, case-based reasoning has gained popularity as an alternative to rule systems, but its proponents recognize that there is a wide spectrum between specific cases and the very general rules typically used <ref> (Riesbeck and Schank, 1989) </ref>, one that has so far been left largely unexplored. This report describes and evaluates the RISE 2.0 algorithm, an approach to in ductive learning that produces knowledge bases spanning this entire spectrum.
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition (Vol. 2). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: As a re sult, research in this field has blossomed, and several mature approaches to inductive learning are now available to the practitioner. These include induction of decision trees (Quinlan, 1986), rule induction (Michalski, 1983), instance-based learning (Aha et al, 1991), Bayesian classification (Buntine, 1989), back-propagation <ref> (Rumelhart et al, 1986) </ref>, and genetic algorithms (Booker et al, 1989). Empirical comparison of these different approaches in a variety of application do mains has shown that each performs best in some, but not all, domains. <p> The default classifier (always choosing the most frequent class) was also included in the study to provide a baseline. Back-propagation <ref> (Rumelhart et al, 1986) </ref>, although a widely-used learning algorithm, was left out because its need for extensive fine-tuning and very long running times would make a large-scale study of this type difficult to carry out.
Reference: <author> Salzberg, S. </author> <year> (1991). </year> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 251-276. </pages>
Reference-contexts: In form, the most similar system to RISE in the literature is EACH <ref> (Salzberg, 1991) </ref>, which produces and uses hyperrectangles generalized from specific instances.
Reference: <author> Schaffer, C. </author> <year> (1994). </year> <title> A conservation law for generalization performance. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 259-265). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Further, recent results <ref> (Schaffer, 1994) </ref> show that performance over the set of all possible domains is subject to a "conservation law": if one algorithm is better than another in some domains, then there are necessarily other domains in which this relationship is reversed.
Reference: <author> Smyth, P. R., Goodman, M., & Higgins, C. </author> <year> (1990). </year> <title> A hybrid rule-based/Bayesian classifier. </title> <booktitle> Proceedings of the Seventh European Conference on Artificial Intelli gence (pp. </booktitle> <pages> 610-615). </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Several induction al gorithms proposed in the literature can be seen as empirical multi-strategy learners, but combining different paradigms from RISE's: decision trees and rules (Quinlan, 1987), decision trees and perceptrons (Utgoff, 1989), rules and Bayesian classification <ref> (Smyth et al, 1990) </ref>, back-propagation and genetic algorithms (Belew et al, 1992), etc. In form, the most similar system to RISE in the literature is EACH (Salzberg, 1991), which produces and uses hyperrectangles generalized from specific instances.
Reference: <author> Stanfill, C., & Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29, </volume> <pages> 1213-1228. </pages>
Reference: <author> Towell, G. G., Shavlik, J. W., & Noordewier, M. O. </author> <year> (1990). </year> <title> Refinement of approx imate domain theories by knowledge-based artificial neural networks. </title> <booktitle> Proceed ings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 861-866). </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Utgoff, P. E. </author> <year> (1989). </year> <title> Perceptron trees: A case study in hybrid concept representa tions. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 377-391. </pages>
Reference-contexts: Several induction al gorithms proposed in the literature can be seen as empirical multi-strategy learners, but combining different paradigms from RISE's: decision trees and rules (Quinlan, 1987), decision trees and perceptrons <ref> (Utgoff, 1989) </ref>, rules and Bayesian classification (Smyth et al, 1990), back-propagation and genetic algorithms (Belew et al, 1992), etc. In form, the most similar system to RISE in the literature is EACH (Salzberg, 1991), which produces and uses hyperrectangles generalized from specific instances.
References-found: 37

