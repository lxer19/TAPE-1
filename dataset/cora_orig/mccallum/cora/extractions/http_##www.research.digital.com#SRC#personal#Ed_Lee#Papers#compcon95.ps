URL: http://www.research.digital.com/SRC/personal/Ed_Lee/Papers/compcon95.ps
Refering-URL: http://www.research.digital.com/SRC/personal/Ed_Lee/Petal/petal.html
Root-URL: http://www.research.digital.com
Email: eklee@src.dec.com  
Title: Highly-Available, Scalable Network Storage  
Author: Edward K. Lee 
Affiliation: Digital Equipment Corporation Systems Research Center  
Abstract: The ideal storage system is always available, is incrementally expandable, scales in performance as new components are added, and requires no management. Existing storage systems are far from this ideal. The recent introduction of low-cost, scalable, high-performance networks allows us to re-examine the way we build storage systems and to investigate storage architectures that bring us closer to the ideal storage system. This document examines some of the issues and ideas in building such storage systems and describes our first scalable storage prototype. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Luis-Felipe Cabrera and Darrel D. E. Long. Swift: </author> <title> Using distributed disk striping to provide high I/O data rates. </title> <journal> ACM Computing Systems, </journal> <volume> 4 </volume> <pages> 405-436, </pages> <month> Fall </month> <year> 1991. </year>
Reference: [2] <author> Pei Cao, Swee Boon Lim, Shivakumar Venkataraman, and John Wilkes. </author> <title> The TickerTAIP parallel RAID architecture. </title> <booktitle> In Proc. International Symposium on Computer Architecture, </booktitle> <pages> pages 52-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The main problem with this arrangement is the availability of the system. Any single disk failure will result in data loss and any single server failure will result in data unavailability. In theory, you can implement a distributed RAID 5 using storage servers <ref> [2] </ref>. However, in practice, the complexity of the additional synchronization required for RAID 5 systems in comparison to a replication-based redundancy scheme is greatly amplified in a distributed environment.
Reference: [3] <author> Peter M. Chen, Edward K. Lee, Ann L. Drapeau, Ken Lutz, Ethan L. Miller, Srinivasan Seshan, Ken Shirriff, David A. Patterson, and Randy H. Katz. </author> <title> Performance and design evaluation of the RAID-II storage server. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 2(3) </volume> <pages> 243-260, </pages> <month> July </month> <year> 1994. </year>
Reference: [4] <author> Peter M. Chen, Edward K. Lee, Garth A. Gibson, Randy H. Katz, and David A. Patterson. </author> <title> Raid: High-performance, reliable secondary storage. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(2) </volume> <pages> 145-185, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Products such as redundant disk arrays and logical volume managers often simplify the management of centralized storage systems by automatically balancing capacity and performance across disks, and by tolerating and automatically recovering from some component failures <ref> [4] </ref>. Most such products, however, do not support large-scale distributed environments. They cannot balance capacity or performance across multiple server nodes and cannot tolerate server, network, or site failures. <p> Each letter represents a block of data stored in the storage system. The figure shows that the first block of data, D0, is stored on storage server 0, the second block of data, D1, is stored on storage server 1, and so on. This effectively implements a RAID 0 <ref> [4] </ref>, using storage servers instead of disks. The main problem with this arrangement is the availability of the system. Any single disk failure will result in data loss and any single server failure will result in data unavailability.
Reference: [5] <author> Joy Foglesong, George Richmond, Loellyn Cassel, Ca-role Hogan, John Kordas, and Michael Nemanic. </author> <title> The Livermore distributed storage: Implementation and experiences. </title> <type> Technical Report UCRL-102663, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> May </month> <year> 1990. </year>
Reference: [6] <author> John H. Hartman and John K. Ousterhout. </author> <title> Zebra: A striped network file system. </title> <booktitle> In Proc. USENIX File Systems Workshop, </booktitle> <pages> pages 71-78, </pages> <month> May </month> <year> 1992. </year>
Reference: [7] <author> Hui-I Hsiao and David J. DeWitt. Chained declus-tering: </author> <title> A new availability strategy for multiprocessor database machines. </title> <type> Technical Report CS TR 854, </type> <institution> University of Wisconsin, Madison, </institution> <month> June </month> <year> 1989. </year>
Reference: [8] <author> A. L. Narasimha Reddy and Prithviraj Banerjee. </author> <title> Gracefully degradable disk arrays. </title> <booktitle> Proc. International Symposium on Fault-Tolerant Computing, </booktitle> <month> June </month> <year> 1991. </year>
Reference: [9] <author> Michael D. Schroeder, Andrew D. Birrell, Michael Burrows, Hal Murray, Roger M. Needham, Thomas L. Rodeheffer, Edwin H. Satterthwaite, and Charles P. Thacker. Autonet: </author> <title> a high-speed, self-configuring local area network using point-to-point links. </title> <type> Technical Report SRC-59, </type> <institution> Digital Equipment Corporation, Systems Research Center, </institution> <address> 130 Lytton Ave., Palo Alto, CA 94301-1044, </address> <month> April </month> <year> 1990. </year> <month> 6 </month>
Reference-contexts: Appropriately partitioning the servers over two or more sites should allow the system to survive site failures, but this has not been tested. The prototype currently does not support virtual volumes. The prototypes is implemented using Alpha/OSF workstations and the AN1 <ref> [9] </ref> (similar to switched FDDI) network. Due to the current lack of spare disks in our computing environment, we have thus far simulated the disks. Our first prototype uses the chained declustered data placement described previously.
References-found: 9

