URL: ftp://archive.cis.ohio-state.edu/pub/neuroprose/Thesis/zhang.rl4jss.ps.Z
Refering-URL: http://www.ics.uci.edu/~mlearn/MLlist/v10/4.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Reinforcement Learning for Job-Shop Scheduling  
Author: by Wei Zhang 
Degree: A DISSERTATION submitted to  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  
Date: April 16, 1996 Commencement June 1996  
Note: Presented  
Affiliation: Oregon State University  
Abstract-found: 0
Intro-found: 1
Reference: [ Aarts, 1989 ] <author> E. H. L. Aarts. </author> <title> Simulated Annealing and Boltzmann Machines: A Stochastic Approach to Combinatorial Optimization and Neural Computing. </title> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference-contexts: The search was performed based on the evaluation of the "textures" of the constraints. These textures measure the properties of the constraint diagram such as contention and reliance of variables on values, providing useful heuristics for variable and value ordering. The second paradigm is using simulated annealing <ref> [ Kirkpatrick et al., 1983, Van Laarhoven, 1988, Aarts, 1989 ] </ref> . Rather than regular hill climbing, simulated annealing allows some downhill moves with a certain probability; therefore, it comes up with a higher chance of finding the global optimal solution.
Reference: [ Abramson, 1991 ] <author> D. Abramson. </author> <title> Constructing school timetables using simulated annealing: Sequential and parallel algorithms. </title> <journal> Management Science, </journal> <volume> 37(1) </volume> <pages> 98-113, </pages> <year> 1991. </year>
Reference: [ Albus, 1981 ] <author> J. S. Albus. </author> <title> Brain, Behavior, and Robotics. </title> <publisher> Byte Books, </publisher> <year> 1981. </year>
Reference-contexts: Popular representations include various types of neural networks, CMAC (Cerebellar Model Articulation computer) <ref> [ Albus, 1981, Sutton, 1996 ] </ref> , decision trees, and local memory-based representations such as nearest neighbor methods. Generalization is important and useful for reinforcement learning in many situations as follows: 38 CHAPTER 2. REINFORCEMENT LEARNING & STATE-SPACE SEARCH * The state space is often very large.
Reference: [ Aytug et al., 1994 ] <author> H. Aytug, G. J. Koehler, and J. L. Snowdon. </author> <title> Genetic learning of dynamic scheduling within a simulation environment. </title> <journal> Computer and Operations Research, </journal> <volume> 21(8) </volume> <pages> 909-926, </pages> <year> 1994. </year>
Reference-contexts: experimentally that optimal rules for simple scheduling problems could be discovered by GAs. [ Davis, 1985 ] focused attention on the order-dependent nature of job-shop scheduling, and then [ Cleveland and Smith, 1989 ] and [ Whitley et al., 1989 ] explored new GA operators to handle these order considerations. <ref> [ Aytug et al., 1994 ] </ref> presented an approach of using GAs to learn composite dispatching rules as scheduling knowledge based on some "base" dispatching rules such as FCFS (first come first serve), EDD (earliest due date), and MST (minimum slack time) rules.
Reference: [ Baird, 1995 ] <author> L. Baird. </author> <title> Residual algorithm: reinforcement learning with function approximation. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 30-37. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: SCALE UP: GENERALIZATION 39 important source of the failures is due to a systematic overestimation of the value function by the "max" operation according to the definition of the value function. Each Bellman backup could convert zero-mean "function approximation noise" into a positive-mean noise. [ Tsitsiklis, 1995 ] , <ref> [ Baird, 1995 ] </ref> and [ Boyan and Moore, 1995 ] showed some simple examples where value function errors grow arbitrarily large when function approximation with reinforcement learning is used. <p> Gordon, 1995 ] showed in theory that several methods such as nearest-neighbor will not diverge. [ Sutton, 1996 ] reported that the unstable phenomena that appeared in [ Boyan and Moore, 1995 ] 's examples can be resolved using an on-line Sarsa learning approach based on a sparse-coarse-coded CMAC representation. <ref> [ Baird, 1995 ] </ref> presented the residual algorithm which employs a combined current gradient and "residual gradient" to update weights in a neural network. The residual gradient is the current gradient subtracted by the discounted previous gradient. This algorithm can be stable but slow.
Reference: [ Baker, 1974 ] <author> K. R. Baker. </author> <title> Introduction to Sequencing and Scheduling. </title> <publisher> Wiley, </publisher> <year> 1974. </year>
Reference: [ Baluja and Caruana, 1995 ] <author> S. Baluja and R. Caruana. </author> <title> Removing the genetics from the standard genetic algorithm. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 38-46. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: and specially-customized mechanisms which are not suited for GAs used for adaptation in dynamic environments. [ Lang, 1995 ] described "an experiment that cast doubt not only on the relative efficiency of genetic programming, but also on the validity of the story about the value of the high-fitness gene pool". <ref> [ Baluja and Caruana, 1995 ] </ref> and [ Baluja, 1995 ] found that GA's population-based search mechanism and the "crossover" operator (which have been the keys of GAs) are not particularly helpful.
Reference: [ Baluja, 1995 ] <author> S. Baluja. </author> <title> An empirical comparison of seven iterative and evolutionary function optimization heuristics. </title> <type> Tech report, </type> <institution> cmu-cs-95-193, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1995. </year>
Reference-contexts: for GAs used for adaptation in dynamic environments. [ Lang, 1995 ] described "an experiment that cast doubt not only on the relative efficiency of genetic programming, but also on the validity of the story about the value of the high-fitness gene pool". [ Baluja and Caruana, 1995 ] and <ref> [ Baluja, 1995 ] </ref> found that GA's population-based search mechanism and the "crossover" operator (which have been the keys of GAs) are not particularly helpful.
Reference: [ Barto et al., 1983 ] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuronlike element that can solve difficult learning control problem. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846, </pages> <year> 1983. </year>
Reference-contexts: RTDP which updates one value per step actually is on-line on-policy TD learning with learning rate ff = 1. A general method for applying TD learning for control problems is the adaptive heuristic critic (AHC) algorithm <ref> [ Barto et al., 1983 ] </ref> . AHC is a learning analog of policy iteration in which TD learning is employed to compute the value function for a policy.
Reference: [ Barto et al., 1995 ] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1) </volume> <pages> 81-138, </pages> <year> 1995. </year>
Reference-contexts: Active learning is often performed along with a problem-solving process. By this, we simultaneously search solutions to a problem and gain search control knowledge. This learning paradigm is called on-line learning. 2.3.1 Real-Time Dynamic Programming Real-time dynamic programming (RTDP) <ref> [ Barto et al., 1995 ] </ref> is an active on-line learning approach to compute the value function. In contrast to standard value iteration methods that systematically compute the value function, RTDP employs a controller to explore the state space. <p> REINFORCEMENT LEARNING & STATE-SPACE SEARCH when a restart criterion is satisfied. This criterion can be "terminate a trial with a fixed amount of time" or "terminate when an absorbing state is reached". The trial-based RTDP algorithm is shown in Figure 2.6. <ref> [ Barto et al., 1995 ] </ref> presented the convergence proof of this algorithm. A comparable and closely related work to RTDP is learning real-time A fl , LRTA fl [ Korf, 1990 ] .
Reference: [ Bellman, 1957 ] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year> <note> 160 BIBLIOGRAPHY </note>
Reference-contexts: THE THESIS: REINFORCEMENT LEARNING FOR SCHEDULING 13 However, to make EBL more useful for scheduling, further development is needed to improve its ability to approach optimality. 1.3 The Thesis: Reinforcement Learning for Scheduling A growing number of researchers have been developing learning methods based on dynamic programming <ref> [ Bellman, 1957, Bertsekas, 1987 ] </ref> for solving stochastic optimal control problems, leading to the coming of current reinforcement learning (RL) [ Sutton, 1990, Sutton, 1991, Watkins, 1989, Watkins and Dayan, 1992, Barto et al., 1995 ] . Briefly, reinforcement learning algorithms learn policies for optimal control problems through "trial-and-error". <p> fl (s) = max Q fl (s; a): (2:11) Putting these two parts together, we have: V fl (s) = max 2 X P a (s; s 0 )V fl (s 0 ) 5 : (2:12) Equation 2.12 is called the optimality equation or Bellman equation due to Bellman's contribution <ref> [ Bellman, 1957 ] </ref> . This equation explicitly tells whether a value function is optimal. A value function is optimal if and only if the lefthand side of the equation is equal to the righthand side over all states.
Reference: [ Bertsekas and Tsitsiklis, 1989 ] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference: [ Bertsekas, 1987 ] <author> D. P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: THE THESIS: REINFORCEMENT LEARNING FOR SCHEDULING 13 However, to make EBL more useful for scheduling, further development is needed to improve its ability to approach optimality. 1.3 The Thesis: Reinforcement Learning for Scheduling A growing number of researchers have been developing learning methods based on dynamic programming <ref> [ Bellman, 1957, Bertsekas, 1987 ] </ref> for solving stochastic optimal control problems, leading to the coming of current reinforcement learning (RL) [ Sutton, 1990, Sutton, 1991, Watkins, 1989, Watkins and Dayan, 1992, Barto et al., 1995 ] . Briefly, reinforcement learning algorithms learn policies for optimal control problems through "trial-and-error". <p> Under this stopping criterion, the resulting policy is called the *-optimal policy. There have been a number of variations to the standard value iteration algorithm for accelerating convergence. These variations include the Gauss-Seidel method, the Jacobi method, and the asynchronous method <ref> [ Bertsekas, 1987, Puterman, 1994 ] </ref> , which permits different value update orders and updating rules. Different variations win in different situations. Convergence of these variations have all been proved. <p> This algorithm is also a pseudo-polynomial algorithm under a fixed discount factor. As variations of the modified policy iteration algorithm, we can use other update orders such as Gauss-Seidel steps instead to evaluate the current policy. 2.2.4 Adaptive State Aggregation The adaptive state aggregation algorithm <ref> [ Bertsekas, 1987 ] </ref> differs from the modified policy iteration in that rather than using multiple value updates in the policy evaluation phase, it employs a state aggregation method to evaluate the current policy.
Reference: [ Biegal and Davern, 1990 ] <author> J. Biegal and J. Davern. </author> <title> Genetic algorithms and job shop scheduling. </title> <journal> Computer and Industrial Engineering, </journal> <volume> 19 </volume> <pages> 81-91, </pages> <year> 1990. </year>
Reference-contexts: Finally, there has been a large amount of research using genetic algorithms (GAs) [ Goldberg, 1989 ] . Many of the projects showed the success of their applications, including <ref> [ Biegal and Davern, 1990 ] </ref> , [ Syswerda, 1991 ] , [ Fang et al., 1993 ] , [ Uckun et al., Oct 1993 ] , [ Lee and Piramuthu, Oct 1994 ] , and [ Nordstrom and Tufekci, 1994 ] .
Reference: [ Boyan and Moore, 1995 ] <author> J. A. Boyan and A. W. Moore. </author> <title> Generalization in reinforcement learning: safely approximating the value function. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <address> San Mateo, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Each Bellman backup could convert zero-mean "function approximation noise" into a positive-mean noise. [ Tsitsiklis, 1995 ] , [ Baird, 1995 ] and <ref> [ Boyan and Moore, 1995 ] </ref> showed some simple examples where value function errors grow arbitrarily large when function approximation with reinforcement learning is used. Both [ Tsitsiklis, 1995 ] and [ Boyan and Moore, 1995 ] have suggested using actual values as in the TD () algorithm when = 1 <p> convert zero-mean "function approximation noise" into a positive-mean noise. [ Tsitsiklis, 1995 ] , [ Baird, 1995 ] and <ref> [ Boyan and Moore, 1995 ] </ref> showed some simple examples where value function errors grow arbitrarily large when function approximation with reinforcement learning is used. Both [ Tsitsiklis, 1995 ] and [ Boyan and Moore, 1995 ] have suggested using actual values as in the TD () algorithm when = 1 to overcome this problem. <p> Fortunately, many pitfalls can be avoided by a careful design of reinforcement learning systems. [ Gordon, 1995 ] showed in theory that several methods such as nearest-neighbor will not diverge. [ Sutton, 1996 ] reported that the unstable phenomena that appeared in <ref> [ Boyan and Moore, 1995 ] </ref> 's examples can be resolved using an on-line Sarsa learning approach based on a sparse-coarse-coded CMAC representation. [ Baird, 1995 ] presented the residual algorithm which employs a combined current gradient and "residual gradient" to update weights in a neural network.
Reference: [ Bradtke, 1993 ] <author> S. J. Bradtke. </author> <title> Reinforcement learning applied to linear quadratic regulation. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 295-302, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J. H. Friedman, R. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth Internatinal Group, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: Otherwise the network would only learn Boolean output functions). Table 4.2 shows the weights for all features by linear regression based on sample S 100 . The most important five features presented by the model are F18, F15, F13, F19, and F11. Regression trees: Regression trees <ref> [ Breiman et al., 1984 ] </ref> are another popular model for uncovering the structure in continuous data. Regression tree methods usually produce a function very different from ones produced by classical statistical models such as linear and logistic regression models.
Reference: [ Cassandra et al., 1994 ] <author> A. R. Cassandra, L. P. Kaelbling, and M. L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1023-1028, </pages> <year> 1994. </year>
Reference: [ Chapman and Kaelbling, 1991 ] <author> D. Chapman and L. P. Kaelbling. </author> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 726-731, </pages> <year> 1991. </year>
Reference-contexts: A nice result about the neural network approach is that 40 CHAPTER 2. REINFORCEMENT LEARNING & STATE-SPACE SEARCH TD () as originally presented in [ Sutton, 1988 ] has been well combined with the backpropagation method. Other noticeable generalization methods include the tree-based G-learning algorithm <ref> [ Chapman and Kaelbling, 1991 ] </ref> , the kd-tree method [ Moore, 1990, Moore, 1991 ] , and the parti-game algorithm [ Moore, 1994 ] . 2.5 Improving Learning Performance Reinforcement learning has been observed to be slow in its convergence to an optimal or near-optimal policy.
Reference: [ Chapman, 1989 ] <author> D. Chapman. </author> <title> Penguins can make cake. </title> <journal> AI Magazine, </journal> <volume> 10 </volume> <pages> 45-50, </pages> <year> 1989. </year>
Reference-contexts: A comparable and closely related work to RTDP is learning real-time A fl , LRTA fl [ Korf, 1990 ] . RTDP generalizes LRTA fl to stochastic problems and includes the option of updating values of many states at a time. RTDP also relates to "universal planning" <ref> [ Chapman, 1989, Ginsberg, 1989, Schoppers, 1987, Schoppers, 1989 ] </ref> . RTDP can be employed to learn a partial policy for a problem with a fixed starting state. In this situation, some exploration strategy must be applied for the learner in order to make it visit various states.
Reference: [ Cichosz and Mulawka, 1995 ] <author> P. Cichosz and J. J. Mulawka. </author> <title> Fast and efficient reinforcement learning with truncated temporal difference learning. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 99-107, </pages> <year> 1995. </year>
Reference-contexts: The replacing trace differs the accumulating trace in that it sets e t (s) = 1 when s is revisited. Implementing the eligibility trace is computationally expensive. <ref> [ Cichosz and Mulawka, 1995 ] </ref> presented the truncated TD learning method to overcome this problem. For undiscounted problems, when = 1, TD () is the same as supervised learning which uses "actual values" as output for updating state-values.
Reference: [ Cleveland and Smith, 1989 ] <author> G. A. Cleveland and S. F. Smith. </author> <title> Using genetic algorithms to schedule flow shop release. </title> <booktitle> In Proceedings of the the Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 160-169. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: optimization methods for scheduling, there is some work in using GAs to learn scheduling knowledge. [ Hilliard et al., 1988 ] showed experimentally that optimal rules for simple scheduling problems could be discovered by GAs. [ Davis, 1985 ] focused attention on the order-dependent nature of job-shop scheduling, and then <ref> [ Cleveland and Smith, 1989 ] </ref> and [ Whitley et al., 1989 ] explored new GA operators to handle these order considerations. [ Aytug et al., 1994 ] presented an approach of using GAs to learn composite dispatching rules as scheduling knowledge based on some "base" dispatching rules such as FCFS
Reference: [ Clouse and Utgoff, 1992 ] <author> J. A. Clouse and P. E. Utgoff. </author> <title> A teaching method for reinforcement learning. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 92-101. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference: [ Coffman, Jr., 1976 ] <author> E. G. Coffman, Jr. </author> <title> Computer and Job-shop Scheduling Theory. </title> <publisher> Wiley, </publisher> <year> 1976. </year> <note> BIBLIOGRAPHY 161 </note>
Reference: [ Crites and Barto, 1996 ] <author> R. H. Crites and A. G. Barto. </author> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <address> Cambridge, MA, 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Tesauro (1992) applied the TD () algorithm using a neural network to approximate the value function. The resulting TD-gammon system is now far better than any other computer program and competitive with the best human players. <ref> [ Crites and Barto, 1996 ] </ref> presented a multi-agent Q-learning neural network approach to successfully construct a policy for elevator control. The work reported here presents further encouraging results on a combination of back-propagation neural networks and TD learning.
Reference: [ Davis, 1985 ] <author> L. Davis. </author> <title> Job shop scheduling with genetic algorithm. </title> <booktitle> In Proceedings of the an International Conference on Genetic Algorithms and Their Applications, </booktitle> <pages> pages 136-140, </pages> <year> 1985. </year>
Reference-contexts: While GAs are mostly applied as optimization methods for scheduling, there is some work in using GAs to learn scheduling knowledge. [ Hilliard et al., 1988 ] showed experimentally that optimal rules for simple scheduling problems could be discovered by GAs. <ref> [ Davis, 1985 ] </ref> focused attention on the order-dependent nature of job-shop scheduling, and then [ Cleveland and Smith, 1989 ] and [ Whitley et al., 1989 ] explored new GA operators to handle these order considerations. [ Aytug et al., 1994 ] presented an approach of using GAs to learn
Reference: [ Dayan and Hinton, 1993 ] <author> P. Dayan and G. E. Hinton. </author> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Dayan and Sejnowski, 1994 ] <author> P. Dayan and T. J. Sejnowski. </author> <title> TD() converges with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14(3) </volume> <pages> 295-302, </pages> <year> 1994. </year>
Reference: [ Dayan, 1992 ] <author> P. Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> 8(3/4):341-362, 1992. 
Reference-contexts: REINFORCEMENT LEARNING & STATE-SPACE SEARCH Here, s t is the current state. Initially, e 0 (s) is set to 0 for every state s. [ Sutton, 1988 ] and <ref> [ Dayan, 1992 ] </ref> proved the convergence of TD () for general . Recently, [ Singh and Sutton, 1996 ] presented a new type of trace, the replacing eligibility trace, and showed that it performs better than the above accumulating trace.
Reference: [ Deale et al., 1994 ] <author> M. Deale, M. Yvanovich, D. Schnitzius, D. Kautz, M. Carpenter, M. Zweben, G. Davis, and B. Daun. </author> <title> The space shuttle ground processing scheduling system. </title> <editor> In M. Zweben and M. S. Fox, editors, </editor> <title> Intelligent Scheduling, </title> <booktitle> chapter 15, </booktitle> <pages> pages 423-449. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1994. </year>
Reference-contexts: The optimization criterion may also vary. Because of the mathematical similarity, 1.1. JOB-SHOP SCHEDULING METHODOLOGIES 3 the terminology of job-shop scheduling is adopted in many other scheduling domains such as transportation scheduling and spacecraft scheduling. Space shuttle operations scheduling is dynamic in nature. <ref> [ Deale et al., 1994 ] </ref> stated that ... Weather plays a major role in the transportation of the orbiter from California to KSC, and on a number of occasions, the orbiters have been damaged as a result of the weather as they cross the country.
Reference: [ Dean and Lin, 1995 ] <author> T. Dean and S.-H. Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1121-1127. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference: [ DeJong and Mooney, 1986 ] <author> G. DeJong and R. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1986. </year>
Reference-contexts: An operator strength function evaluates each operator based on how successful that operator has been in the past. Another general technique is to learn new "macro operators" by composing sequences of original operators <ref> [ Mitchell et al., 1986, DeJong and Mooney, 1986 ] </ref> . The learned macro operators are added to the set of operators considered by the problem solver, and they allow it to take "big steps" in the search space. <p> This is not realistic for a complex scheduling task. Ideally for scheduling, we need to have some mechanism that can incrementally generate training patterns for learning scheduling strategies. 1.2.2 Explanation-Based Learning Explanation-based learning (EBL) <ref> [ Mitchell et al., 1986, DeJong and Mooney, 1986 ] </ref> is a knowledge-based analytical learning methodology. EBL has mostly been applied to the speedup learning problem. It has been developed to learn search control knowledge in the forms of meta-level rules as well as to learn new macro operators.
Reference: [ DeJong, 1992 ] <author> K. DeJong. </author> <title> Genetic algorithms are NOT function optimizers. </title> <editor> In Whitley, editor, </editor> <booktitle> FOGA-2 Foundations of Genetic Algorithms-2, </booktitle> <pages> pages 5-17. </pages> <publisher> Mor-gan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: JOB-SHOP SCHEDULING METHODOLOGIES 7 work for general problems. Furthermore, it is not yet clear whether and how much the GA mechanisms benefit the search process. Recently, many researchers raised questions about GA mechanisms and showed negative experimental results. <ref> [ DeJong, 1992 ] </ref> claims that GAs are not a function optimizer and that typical GAs for optimization often use different and specially-customized mechanisms which are not suited for GAs used for adaptation in dynamic environments. [ Lang, 1995 ] described "an experiment that cast doubt not only on the relative
Reference: [ Dell'Amico, 1993 ] <author> M. Dell'Amico. </author> <title> Applying tabu-search to the job-shop scheduling problem. </title> <journal> Annals of Operations Research, </journal> <volume> 41 </volume> <pages> 231-252, </pages> <year> 1993. </year>
Reference-contexts: Tabu search uses tabu "restriction" and "aspiration" criteria to prevent search from cycling for a better local search. It also uses "intensification" and "diversification" mechanisms to determine when and how to jump search to another area. The work of applying tabu search to scheduling includes <ref> [ Dell'Amico, 1993 ] </ref> , [ Icmeli and Erenguc, 1994 ] , [ Hubscher and Glover, 1994 ] , and [ Hooker and Natraj, 1995 ] .
Reference: [ Denardo, 1982 ] <author> E. V. Denardo. </author> <title> Dynamic Programming Models and Applications. </title> <publisher> Prentice-Hall, Inc, </publisher> <year> 1982. </year>
Reference-contexts: Each decision rule specifies an action for each state. A stationary policy is a policy in which a single decision rule is applied over time. The solution of an infinite-stage problem is a stationary policy. An important property of MDPs is captured by the following turnpike theorem <ref> [ Denardo, 1982 ] </ref> : There exists a threshold n 0 such that an optimal policy for the infinite time horizon fl 1 is also an optimal decision rule for a more than n 0 step future. <p> REINFORCEMENT LEARNING & STATE-SPACE SEARCH Linear programming is a general technique. Its most popular algorithm is the simplex method, which runs very fast for most problems, although its worst case complexity is exponential to the size of a problem. <ref> [ Denardo, 1982 ] </ref> pointed out that the simplex method performs exactly the same as policy iteration: it makes the same sequence of policy substitutions (pivots) as policy iteration does. Like policy iteration, linear programming is only good in solving small MDPs.
Reference: [ Dietterich and Flann, 1995 ] <author> T. D. Dietterich and N. S. Flann. </author> <title> Explanation-based learning and reinforcement learning: A unified view. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 176-184. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference: [ Fang et al., 1993 ] <author> H. L. Fang, P. Ross, and D. Corne. </author> <title> A promising genetic algorithm approach to job-shop scheduling, rescheduling, and open-shop scheduling problems. </title> <booktitle> In Proceedings of the the Fifth International Conference on Genetic Algorithms, </booktitle> <year> 1993. </year> <note> 162 BIBLIOGRAPHY </note>
Reference-contexts: Finally, there has been a large amount of research using genetic algorithms (GAs) [ Goldberg, 1989 ] . Many of the projects showed the success of their applications, including [ Biegal and Davern, 1990 ] , [ Syswerda, 1991 ] , <ref> [ Fang et al., 1993 ] </ref> , [ Uckun et al., Oct 1993 ] , [ Lee and Piramuthu, Oct 1994 ] , and [ Nordstrom and Tufekci, 1994 ] . GAs mimic the process of natural evolution in searching a solution space for optimal solutions.
Reference: [ Fox and Zweben, 1995 ] <author> M. S. Fox and M. </author> <title> Zweben. Constraint directed search: Theory and practice. </title> <booktitle> Totorial MA4, Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: For most complex real-world problems, we must perform actions correctly throughout the whole search process in order to achieve success. It has been clear that early decisions are crucial in scheduling. <ref> [ Fox and Zweben, 1995 ] </ref> states that in order to make good schedules, it is very important to make correct repairs in the early stages of scheduling. They points out that this has been the major reason that backtracking-based scheduling systems fail to construct schedules efficiently.
Reference: [ Fox, 1983 ] <author> M. S. Fox. </author> <title> Constraint-Directed Search: A Case Study of Job-Shop Scheduling. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1983. </year>
Reference-contexts: Many efforts have been made in developing effective heuristics in the OR framework. Accordingly, such OR methods have been more connected to AI methods. AI began its attack on the scheduling problem in early 1980s <ref> [ Fox, 1983 ] </ref> , aiming at developing approaches capable of solving general and large-scale real-world scheduling problems. The AI methodology basically is knowledge-based search or heuristic search. <p> By performing constraint consistency check and propagation, these techniques may greatly narrow the possible assignments of values to each variable, therefore, reducing the search space. The methods that follow this paradigm included ISIS, OPIS, and Micro-Boss. ISIS developed by Mark Fox and colleagues <ref> [ Fox, 1983, Fox, 1994 ] </ref> first demonstrated AI technologies in scheduling. One of the most important contribution of this 1.1. JOB-SHOP SCHEDULING METHODOLOGIES 5 work is that ISIS provided approaches to dealing with the full range of constraints and objectives encountered in the manufacturing domain.
Reference: [ Fox, 1994 ] <author> M. S. Fox. </author> <title> ISIS: A retrospective. </title> <editor> In M. Zweben and M. S. Fox, editors, </editor> <title> Intelligent Scheduling, </title> <booktitle> chapter 1, </booktitle> <pages> pages 3-28. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1994. </year>
Reference-contexts: By performing constraint consistency check and propagation, these techniques may greatly narrow the possible assignments of values to each variable, therefore, reducing the search space. The methods that follow this paradigm included ISIS, OPIS, and Micro-Boss. ISIS developed by Mark Fox and colleagues <ref> [ Fox, 1983, Fox, 1994 ] </ref> first demonstrated AI technologies in scheduling. One of the most important contribution of this 1.1. JOB-SHOP SCHEDULING METHODOLOGIES 5 work is that ISIS provided approaches to dealing with the full range of constraints and objectives encountered in the manufacturing domain.
Reference: [ French, 1982 ] <author> S. </author> <title> French. Sequencing and Scheduling. </title> <publisher> Wiley, </publisher> <year> 1982. </year>
Reference: [ Garey and Johnson, 1979 ] <author> M. R. Garey and D. S. Johnson. </author> <title> Computer and Intractability: A Guide to the Theory of NP-Completeness. </title> <publisher> Freeman and Co, </publisher> <year> 1979. </year>
Reference: [ Ginsberg, 1989 ] <author> M. L. Ginsberg. </author> <title> Universal planning: an (almost) universally bad idea. </title> <journal> AI Magazine, </journal> <volume> 10(4) </volume> <pages> 40-44, </pages> <year> 1989. </year>
Reference-contexts: A comparable and closely related work to RTDP is learning real-time A fl , LRTA fl [ Korf, 1990 ] . RTDP generalizes LRTA fl to stochastic problems and includes the option of updating values of many states at a time. RTDP also relates to "universal planning" <ref> [ Chapman, 1989, Ginsberg, 1989, Schoppers, 1987, Schoppers, 1989 ] </ref> . RTDP can be employed to learn a partial policy for a problem with a fixed starting state. In this situation, some exploration strategy must be applied for the learner in order to make it visit various states.
Reference: [ Glover, 1990 ] <author> F. Glover. </author> <title> Tabu search: A tutorial. </title> <journal> Interfaces, </journal> <volume> 20(4) </volume> <pages> 74-94, </pages> <year> 1990. </year>
Reference-contexts: The resulting scheduling system is able to provide an efficient and flexible facility for scheduling space shuttle ground operations. It is in regular use at the Kennedy Space Center. The next noticeable search paradigm is tabu search <ref> [ Glover, 1990 ] </ref> . Tabu search performs hill climbing except it makes use of a tabu list to force the search away from solutions selected for recent iterations to escape the local maxima. The tabu 6 CHAPTER 1. INTRODUCTION list records both local and large-step search information.
Reference: [ Goldberg, 1989 ] <author> D. E. Goldberg. </author> <title> Genetic Algorithms in Search Optimization and Machine Learning. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Several researchers have shown that tabu search has the potential to outperform simulated annealing for scheduling and optimization problems [ Lourenco and Zwijnenburg, 1995, Woodruff, 1994 ] . Finally, there has been a large amount of research using genetic algorithms (GAs) <ref> [ Goldberg, 1989 ] </ref> . <p> Solutions are improved over "generations" of the mating and offspring bearing process on the "population" (the set of current solutions) until the solutions are near optimal. Another power of GAs is that they can be applied to learn problem-solving knowledge <ref> [ Montana and Davis, 1989, Goldberg, 1989 ] </ref> .
Reference: [ Gordon, 1995 ] <author> G. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 261-268. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Both [ Tsitsiklis, 1995 ] and [ Boyan and Moore, 1995 ] have suggested using actual values as in the TD () algorithm when = 1 to overcome this problem. Fortunately, many pitfalls can be avoided by a careful design of reinforcement learning systems. <ref> [ Gordon, 1995 ] </ref> showed in theory that several methods such as nearest-neighbor will not diverge. [ Sutton, 1996 ] reported that the unstable phenomena that appeared in [ Boyan and Moore, 1995 ] 's examples can be resolved using an on-line Sarsa learning approach based on a sparse-coarse-coded CMAC representation.
Reference: [ Grefenstette, 1988 ] <author> J. J Grefenstette. </author> <title> Credit assignment in rule discorvery systems based on genetic algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 225-245, </pages> <year> 1988. </year>
Reference-contexts: An exact policy explicitly tells what action should be chosen in a state; while an generalized policy can be in a form of operator selection/rejection rules [ Laird et al., 1986, Minton, 1990 ] or operator strength functions <ref> [ Grefenstette, 1988 ] </ref> . Operator selection/rejection rules are "meta-level" rules that are evaluated to decide which "base-level" rule or operator should be applied next. An operator strength function evaluates each operator based on how successful that operator has been in the past. <p> Samuel's program learned an evaluation function based on the difference of estimates between successive board positions. Other earlier TD related works include Holland's bucket brigade and profit-sharing plan [ Holland, 1986 ] and Grefenstette's RUDI system <ref> [ Grefenstette, 1988 ] </ref> . 2.3.3 Sarsa and Q-learning The Sarsa algorithm is a variation of T D () [ Rummery and Niranjan, 1994, Sutton, 1996, Singh and Sutton, 1996 ] .
Reference: [ Hilliard et al., 1988 ] <author> M. R. Hilliard, G. E. Piepens, M. Palmer, and D. J. Kejitan. </author> <title> Machine learning application to job shop scheduling. </title> <booktitle> In Proceedings of the the fiest International Conference on AI and Expert Systems, </booktitle> <pages> pages 728-733. </pages> <publisher> ACM Press, </publisher> <year> 1988. </year>
Reference-contexts: Another power of GAs is that they can be applied to learn problem-solving knowledge [ Montana and Davis, 1989, Goldberg, 1989 ] . While GAs are mostly applied as optimization methods for scheduling, there is some work in using GAs to learn scheduling knowledge. <ref> [ Hilliard et al., 1988 ] </ref> showed experimentally that optimal rules for simple scheduling problems could be discovered by GAs. [ Davis, 1985 ] focused attention on the order-dependent nature of job-shop scheduling, and then [ Cleveland and Smith, 1989 ] and [ Whitley et al., 1989 ] explored new GA
Reference: [ Holland, 1986 ] <author> J. H. Holland. </author> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. S. Michal-ski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: Samuel's program learned an evaluation function based on the difference of estimates between successive board positions. Other earlier TD related works include Holland's bucket brigade and profit-sharing plan <ref> [ Holland, 1986 ] </ref> and Grefenstette's RUDI system [ Grefenstette, 1988 ] . 2.3.3 Sarsa and Q-learning The Sarsa algorithm is a variation of T D () [ Rummery and Niranjan, 1994, Sutton, 1996, Singh and Sutton, 1996 ] .
Reference: [ Hooker and Natraj, 1995 ] <author> J. N. Hooker and N. R. Natraj. </author> <title> Solving a general routing and scheduling problem by chain decompostion and tabu search. </title> <journal> Transportation Science, </journal> <volume> 29(1) </volume> <pages> 30-44, </pages> <year> 1995. </year>
Reference-contexts: It also uses "intensification" and "diversification" mechanisms to determine when and how to jump search to another area. The work of applying tabu search to scheduling includes [ Dell'Amico, 1993 ] , [ Icmeli and Erenguc, 1994 ] , [ Hubscher and Glover, 1994 ] , and <ref> [ Hooker and Natraj, 1995 ] </ref> . Several researchers have shown that tabu search has the potential to outperform simulated annealing for scheduling and optimization problems [ Lourenco and Zwijnenburg, 1995, Woodruff, 1994 ] .
Reference: [ Howard, 1960 ] <author> R. Howard. </author> <title> Dynamic Programming and Markov Decision Problem. </title> <publisher> The MIT Press, </publisher> <address> Mass, </address> <year> 1960. </year>
Reference: [ Hubscher and Glover, 1994 ] <author> R. Hubscher and F. Glover. </author> <title> Applying tabu search with influential diversification to multiprocessor scheduling. </title> <journal> Computer and Operations Research, </journal> <volume> 21(8) </volume> <pages> 877-884, </pages> <year> 1994. </year> <note> BIBLIOGRAPHY 163 </note>
Reference-contexts: It also uses "intensification" and "diversification" mechanisms to determine when and how to jump search to another area. The work of applying tabu search to scheduling includes [ Dell'Amico, 1993 ] , [ Icmeli and Erenguc, 1994 ] , <ref> [ Hubscher and Glover, 1994 ] </ref> , and [ Hooker and Natraj, 1995 ] . Several researchers have shown that tabu search has the potential to outperform simulated annealing for scheduling and optimization problems [ Lourenco and Zwijnenburg, 1995, Woodruff, 1994 ] .
Reference: [ Icmeli and Erenguc, 1994 ] <author> O. Icmeli and S. S. Erenguc. </author> <title> A tabu search procedure for the resource constrainted project scheduling problem with discounted cash flows. </title> <journal> Computer and Operations Research, </journal> <volume> 21(8) </volume> <pages> 841-854, </pages> <year> 1994. </year>
Reference-contexts: It also uses "intensification" and "diversification" mechanisms to determine when and how to jump search to another area. The work of applying tabu search to scheduling includes [ Dell'Amico, 1993 ] , <ref> [ Icmeli and Erenguc, 1994 ] </ref> , [ Hubscher and Glover, 1994 ] , and [ Hooker and Natraj, 1995 ] . Several researchers have shown that tabu search has the potential to outperform simulated annealing for scheduling and optimization problems [ Lourenco and Zwijnenburg, 1995, Woodruff, 1994 ] .
Reference: [ Jaakkola et al., 1994 ] <author> T. Jaakkola, M. I. Jordan, and S. P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1185-1201, </pages> <year> 1994. </year>
Reference: [ Kaelbling et al., 1996 ] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement learning: A servey. </title> <note> Journal of AI Research (to appear), </note> <year> 1996. </year>
Reference: [ Kaelbling, 1990 ] <author> L. P. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <type> PhD thesis, </type> <institution> Stan-ford University, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: Other information-directed exploration methods include the interval estimation method <ref> [ Kaelbling, 1990 ] </ref> , the exploration bonus in Dyna [ Sutton, 1990 ] , and exploration based on a priority queue [ Moore and Atkeson, 1993 ] .
Reference: [ Kaelbling, 1993 ] <author> L. P. Kaelbling. </author> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference: [ Karmarkar, 1984 ] <author> N. Karmarkar. </author> <title> A new polynomial time algorithm for linear programming. </title> <journal> Combinatorica, </journal> <volume> 4(4) </volume> <pages> 373-385, </pages> <year> 1984. </year>
Reference-contexts: It proves that MDPs are solvable in polynomial time in N; M , and B, independent of the discount factor. This result is due to [ Khachian, 1979 ] and <ref> [ Karmarkar, 1984 ] </ref> , who presented algorithms for solving rational linear programs in time polynomial in the number of variables and constraints as well as in the number of bits used to represent the coefficients. Real-world applications involve large MDPs.
Reference: [ Khachian, 1979 ] <author> L. G. </author> <title> Khachian. A polynomial algorithm for linear programming. </title> <journal> Soviet Math Dokl., </journal> <volume> 20 </volume> <pages> 191-194, </pages> <year> 1979. </year>
Reference-contexts: Nonetheless, the most striking feature about linear programming for MDPs is that it comes up with so far the best computational complexity of MDPs. It proves that MDPs are solvable in polynomial time in N; M , and B, independent of the discount factor. This result is due to <ref> [ Khachian, 1979 ] </ref> and [ Karmarkar, 1984 ] , who presented algorithms for solving rational linear programs in time polynomial in the number of variables and constraints as well as in the number of bits used to represent the coefficients. Real-world applications involve large MDPs.
Reference: [ Kirkpatrick et al., 1983 ] <author> S. Kirkpatrick, C. D. Galatt Jr., and M. P. Vecchi. </author> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680, </pages> <year> 1983. </year>
Reference-contexts: The search was performed based on the evaluation of the "textures" of the constraints. These textures measure the properties of the constraint diagram such as contention and reliance of variables on values, providing useful heuristics for variable and value ordering. The second paradigm is using simulated annealing <ref> [ Kirkpatrick et al., 1983, Van Laarhoven, 1988, Aarts, 1989 ] </ref> . Rather than regular hill climbing, simulated annealing allows some downhill moves with a certain probability; therefore, it comes up with a higher chance of finding the global optimal solution.
Reference: [ Klopf, 1972 ] <author> A. H. Klopf. </author> <title> Brain function and adaptive systems|a heterostatic theory. </title> <type> Tech report afcrl-72-0164, </type> <institution> Air Force Cambridge Research Lab., Bedford, </institution> <address> MA, </address> <year> 1972. </year>
Reference: [ Korf, 1990 ] <author> R. E. Korf. </author> <title> Real-time heuristic search. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 189-211, </pages> <year> 1990. </year>
Reference-contexts: The trial-based RTDP algorithm is shown in Figure 2.6. [ Barto et al., 1995 ] presented the convergence proof of this algorithm. A comparable and closely related work to RTDP is learning real-time A fl , LRTA fl <ref> [ Korf, 1990 ] </ref> . RTDP generalizes LRTA fl to stochastic problems and includes the option of updating values of many states at a time. RTDP also relates to "universal planning" [ Chapman, 1989, Ginsberg, 1989, Schoppers, 1987, Schoppers, 1989 ] .
Reference: [ Laird et al., 1986 ] <author> J. E. Laird, P. S. Rosenbloom, and A. Newell. </author> <title> Chunking in soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference-contexts: Search control knowledge can be also learned in the form of policies. An exact policy explicitly tells what action should be chosen in a state; while an generalized policy can be in a form of operator selection/rejection rules <ref> [ Laird et al., 1986, Minton, 1990 ] </ref> or operator strength functions [ Grefenstette, 1988 ] . Operator selection/rejection rules are "meta-level" rules that are evaluated to decide which "base-level" rule or operator should be applied next. <p> EBL has mostly been applied to the speedup learning problem. It has been developed to learn search control knowledge in the forms of meta-level rules as well as to learn new macro operators. EBL has been integrated into several problem-solving architectures including SOAR <ref> [ Laird et al., 1986 ] </ref> and Prodigy [ Minton, 1990 ] . When these architectures need 12 CHAPTER 1. INTRODUCTION to choose an operator to apply to some state, they consult a (learned) set of control rules that recommend which operator to apply.
Reference: [ Lang et al., 1990 ] <author> K. J. Lang, A. H. Waibel, and G. E. Hinton. </author> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 33-43, </pages> <year> 1990. </year>
Reference-contexts: However, hand-engineering increases the cost of creating a new application and reduces the autonomy of the learning system. Therefore, we wish to develop a method that can automatically learn good input features. The time-delay neural network <ref> [ Lang et al., 1990, LeCun et al., 1989 ] </ref> has proved to be very effective in learning good position-independent features in visual- and 5.2. NETWORK ARCHITECTURE 141 speech-recognition tasks.
Reference: [ Lang, 1995 ] <author> K. J. Lang. </author> <title> Hill climbing beats genetic search on a boolean circuit synthesis problem of koza's. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 340-343. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Recently, many researchers raised questions about GA mechanisms and showed negative experimental results. [ DeJong, 1992 ] claims that GAs are not a function optimizer and that typical GAs for optimization often use different and specially-customized mechanisms which are not suited for GAs used for adaptation in dynamic environments. <ref> [ Lang, 1995 ] </ref> described "an experiment that cast doubt not only on the relative efficiency of genetic programming, but also on the validity of the story about the value of the high-fitness gene pool". [ Baluja and Caruana, 1995 ] and [ Baluja, 1995 ] found that GA's population-based search
Reference: [ LeCun et al., 1989 ] <author> Y. LeCun, B. Boser, J. S. Deniker, and D. Henderson et al. </author> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 541-551, </pages> <year> 1989. </year>
Reference-contexts: However, hand-engineering increases the cost of creating a new application and reduces the autonomy of the learning system. Therefore, we wish to develop a method that can automatically learn good input features. The time-delay neural network <ref> [ Lang et al., 1990, LeCun et al., 1989 ] </ref> has proved to be very effective in learning good position-independent features in visual- and 5.2. NETWORK ARCHITECTURE 141 speech-recognition tasks.
Reference: [ Lee and Piramuthu, Oct 1994 ] <author> C. Lee and S. Piramuthu. </author> <title> Global job shop scheduling with genetic algorithm and machine learning. </title> <address> ORSA/TIMES Detroit, </address> <month> Oct, </month> <year> 1994. </year> <note> 164 BIBLIOGRAPHY </note>
Reference-contexts: Many of the projects showed the success of their applications, including [ Biegal and Davern, 1990 ] , [ Syswerda, 1991 ] , [ Fang et al., 1993 ] , [ Uckun et al., Oct 1993 ] , <ref> [ Lee and Piramuthu, Oct 1994 ] </ref> , and [ Nordstrom and Tufekci, 1994 ] . GAs mimic the process of natural evolution in searching a solution space for optimal solutions. A GA combines the principle of "survival-and-the-fittest" with randomized information exchange.
Reference: [ Lin, 1991 ] <author> Longji Lin. </author> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 781-786, </pages> <year> 1991. </year>
Reference-contexts: A number of forms of teaching methods have been developed for reinforcement learning [ Lin, 1991, Lin, 1992, Whitehead, 1991, Utgoff and Clouse, 1991, 42 CHAPTER 2. REINFORCEMENT LEARNING & STATE-SPACE SEARCH Clouse and Utgoff, 1992 ] . <ref> [ Lin, 1991 ] </ref> provided an approach by giving the learner lessons, which are human-generated sequences of experiences in (s; a; r; s 0 ) quadruples that begin with an initial state and proceed to a goal state. <p> The guidance can be either taken on-line or taken in advance as prior knowledge. Several researchers have reported that teaching methods combined with some random exploration can scale very efficiently to solve difficult problems <ref> [ Lin, 1991, Whitehead, 1991 ] </ref> . 2.5.3 Experience Replay As discussed earlier, generalization with neural networks has been the most popular methodology for solving large-scale problems.
Reference: [ Lin, 1992 ] <author> Longji Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> machine learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year>
Reference-contexts: The residual gradient is the current gradient subtracted by the discounted previous gradient. This algorithm can be stable but slow. Lin <ref> [ Lin, 1992, Lin, 1993 ] </ref> demonstrated that reinforcement learning with neural networks is able to learn to accomplish various tasks for a simulated robot. <p> The experience obtained by the trial-and-error procedure is often costly, and some experiences may only appear rarely. To effectively train a neural network, experiences should be reused in an effective way. <ref> [ Lin, 1992 ] </ref> suggested replaying experiences that follow the current policy. The work reported in this thesis replays good experiences. 2.6.
Reference: [ Lin, 1993 ] <author> Longji Lin. </author> <title> Reinforcement Learning for Robots using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: The residual gradient is the current gradient subtracted by the discounted previous gradient. This algorithm can be stable but slow. Lin <ref> [ Lin, 1992, Lin, 1993 ] </ref> demonstrated that reinforcement learning with neural networks is able to learn to accomplish various tasks for a simulated robot.
Reference: [ Littman et al., 1995a ] <author> M. L. Littman, A. R. Cassandra, and L. P. Kaelbling. </author> <title> Learning policies for partially observable environments: scaling up. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <year> 1995. </year>
Reference: [ Littman et al., 1995b ] <author> M. L. Littman, T. L. Dean, and L. P. Kaelbling. </author> <title> On the complexity of solving Markov decision problems. </title> <booktitle> In Proceedings of the the 11th Annual Conference of Uncertainty and AI (UAI-95), </booktitle> <year> 1995. </year>
Reference-contexts: Furthermore, it was proved that for fixed fl &lt; 1, the value iteration algorithm takes time polynomial in M; N , and B <ref> [ Tseng, 1990, Littman et al., 1995b ] </ref> , which is pseudo-polynomial (because it relies on B). In the worst case the number of iterations grows polynomially in 1=(1 fl), so the convergence could be very slow when the discount factor is close to 1. <p> This algorithm is also a pseudo-polynomial algorithm under a fixed discount factor. 2.2.5 Linear Programming The problem of computing the optimal value function for an infinite-horizon MDP can be formulated as a linear program as follows <ref> [ Littman et al., 1995b ] </ref> .
Reference: [ Lourenco and Zwijnenburg, 1995 ] <author> H. R. Lourenco and M. Zwijnenburg. </author> <title> Combining the large-step optimization with tabu-search: Application to the job-shop scheduling problem. </title> <booktitle> Presented in the first AI and OR workshop, </booktitle> <address> Timberline, Oregon, </address> <year> 1995. </year>
Reference-contexts: Several researchers have shown that tabu search has the potential to outperform simulated annealing for scheduling and optimization problems <ref> [ Lourenco and Zwijnenburg, 1995, Woodruff, 1994 ] </ref> . Finally, there has been a large amount of research using genetic algorithms (GAs) [ Goldberg, 1989 ] .
Reference: [ Minton et al., 1992 ] <author> S. Minton, M. D. Johnson, A. B. Philips, and P. Laird. </author> <title> Minimizing conflicts: a heuristic repair method for constraint satisfaction and scheduling problem. </title> <journal> Artificial Intelligence, </journal> <volume> 58 </volume> <pages> 161-205, </pages> <year> 1992. </year>
Reference-contexts: The constructive methods seem more straightforward for scheduling. A repair method can also perform efficiently in making new schedules. Simulated annealing, tabu search and GAs are generally practiced under a complete schedule search space. Some evaluation on a complete schedule, such as the number of violations <ref> [ Minton et al., 1992 ] </ref> and the current schedule length, can be very useful for searching the problem space effectively. With such a good objective evaluation, methods like simulated annealing can often perform very well in quickly finding high-quality solutions.
Reference: [ Minton, 1990 ] <author> S. Minton. </author> <title> Quantitative results concerning the utility of explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 363-392, </pages> <year> 1990. </year>
Reference-contexts: Search control knowledge can be also learned in the form of policies. An exact policy explicitly tells what action should be chosen in a state; while an generalized policy can be in a form of operator selection/rejection rules <ref> [ Laird et al., 1986, Minton, 1990 ] </ref> or operator strength functions [ Grefenstette, 1988 ] . Operator selection/rejection rules are "meta-level" rules that are evaluated to decide which "base-level" rule or operator should be applied next. <p> It has been developed to learn search control knowledge in the forms of meta-level rules as well as to learn new macro operators. EBL has been integrated into several problem-solving architectures including SOAR [ Laird et al., 1986 ] and Prodigy <ref> [ Minton, 1990 ] </ref> . When these architectures need 12 CHAPTER 1. INTRODUCTION to choose an operator to apply to some state, they consult a (learned) set of control rules that recommend which operator to apply. <p> One shortcoming of EBL is that there is no performance guarantee on the learning process. EBL does not learn optimal or near-optimal problem-solving strategies [ Di-etterich and Flann, 1995 ] . Another problem with EBL is the utility problem <ref> [ Minton, 1990 ] </ref> . Although learning provides a problem solver with better knowledge of search, the system may slow down dramatically as more complex control knowledge is learned, because of the cost of evaluating the control knowledge. The EBL methodology has been applied in a scheduling domain.
Reference: [ Mitchell et al., 1983 ] <author> T. M. Mitchell, P. E. Utgoff, and R. B. Banerji. </author> <title> Learning by experimentation: Acquiring and refining problem-solving heuristics. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (Vol I). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: The most impressive works include Samual's checker-playing program [ Samuel, 1959 ] and the LEX system <ref> [ Mitchell et al., 1983 ] </ref> . Applying supervised learning to learn search control knowledge requires having knowledge about the correct solution paths. An analysis of the traces provides positive and negative instances for generating rules to control search.
Reference: [ Mitchell et al., 1986 ] <author> T. M. Mitchell, R. M. Keller, and S. t. Kedar-Cabelli. </author> <title> Explanation-based generalization: a unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1986. </year>
Reference-contexts: An operator strength function evaluates each operator based on how successful that operator has been in the past. Another general technique is to learn new "macro operators" by composing sequences of original operators <ref> [ Mitchell et al., 1986, DeJong and Mooney, 1986 ] </ref> . The learned macro operators are added to the set of operators considered by the problem solver, and they allow it to take "big steps" in the search space. <p> This is not realistic for a complex scheduling task. Ideally for scheduling, we need to have some mechanism that can incrementally generate training patterns for learning scheduling strategies. 1.2.2 Explanation-Based Learning Explanation-based learning (EBL) <ref> [ Mitchell et al., 1986, DeJong and Mooney, 1986 ] </ref> is a knowledge-based analytical learning methodology. EBL has mostly been applied to the speedup learning problem. It has been developed to learn search control knowledge in the forms of meta-level rules as well as to learn new macro operators.
Reference: [ Mittenthal et al., 1993 ] <author> J. Mittenthal, M. Raghavachari, and A. Rana. </author> <title> A hybrid simulated annealing approach for single machine scheduling problems with non-regular penalty functions. </title> <journal> Computer and Operations Research, </journal> <volume> 20(2) </volume> <pages> 103-111, </pages> <year> 1993. </year>
Reference: [ Montana and Davis, 1989 ] <author> D. Montana and L. Davis. </author> <title> Training feedforward neural networks using genetic algorithms. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 762-767, </pages> <year> 1989. </year>
Reference-contexts: Solutions are improved over "generations" of the mating and offspring bearing process on the "population" (the set of current solutions) until the solutions are near optimal. Another power of GAs is that they can be applied to learn problem-solving knowledge <ref> [ Montana and Davis, 1989, Goldberg, 1989 ] </ref> .
Reference: [ Moore and Atkeson, 1993 ] <author> A. W. Moore and C. G. Atkeson. </author> <title> Prioritized sweeping: reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 103-130, </pages> <year> 1993. </year> <note> BIBLIOGRAPHY 165 </note>
Reference-contexts: Most reinforcement learning researchers have been focused on learning in this type of environment, coming up with a number of important reinforcement learning methods such as model-free method Q-learning [ Watkins, 1989, Watkins and Dayan, 1992 ] and model-based methods Dyna [ Sutton, 1990 ] and prioritized sweeping <ref> [ Moore and Atkeson, 1993 ] </ref> . Recently, a number of researchers are also concerned about developing reinforcement learning methods for non-Markov domains, particularly for partially observable stochastic domains, yielding several important results [ Cassandra et al., 1994, Littman et al., 1995a, Parr and Russell, 1995 ] . <p> Other information-directed exploration methods include the interval estimation method [ Kaelbling, 1990 ] , the exploration bonus in Dyna [ Sutton, 1990 ] , and exploration based on a priority queue <ref> [ Moore and Atkeson, 1993 ] </ref> .
Reference: [ Moore, 1990 ] <author> A. W. Moore. </author> <title> Efficient Memory-Based Learning for Robot Control. </title> <type> PhD thesis, </type> <institution> University of Cambridge, UK, </institution> <year> 1990. </year>
Reference-contexts: REINFORCEMENT LEARNING & STATE-SPACE SEARCH TD () as originally presented in [ Sutton, 1988 ] has been well combined with the backpropagation method. Other noticeable generalization methods include the tree-based G-learning algorithm [ Chapman and Kaelbling, 1991 ] , the kd-tree method <ref> [ Moore, 1990, Moore, 1991 ] </ref> , and the parti-game algorithm [ Moore, 1994 ] . 2.5 Improving Learning Performance Reinforcement learning has been observed to be slow in its convergence to an optimal or near-optimal policy.
Reference: [ Moore, 1991 ] <author> A. W. Moore. </author> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state-spaces. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 333-337, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: REINFORCEMENT LEARNING & STATE-SPACE SEARCH TD () as originally presented in [ Sutton, 1988 ] has been well combined with the backpropagation method. Other noticeable generalization methods include the tree-based G-learning algorithm [ Chapman and Kaelbling, 1991 ] , the kd-tree method <ref> [ Moore, 1990, Moore, 1991 ] </ref> , and the parti-game algorithm [ Moore, 1994 ] . 2.5 Improving Learning Performance Reinforcement learning has been observed to be slow in its convergence to an optimal or near-optimal policy.
Reference: [ Moore, 1994 ] <author> A. W. Moore. </author> <title> The parti-game algorithm for variable resolution reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other noticeable generalization methods include the tree-based G-learning algorithm [ Chapman and Kaelbling, 1991 ] , the kd-tree method [ Moore, 1990, Moore, 1991 ] , and the parti-game algorithm <ref> [ Moore, 1994 ] </ref> . 2.5 Improving Learning Performance Reinforcement learning has been observed to be slow in its convergence to an optimal or near-optimal policy.
Reference: [ Nordstrom and Tufekci, 1994 ] <author> A. L. Nordstrom and S. Tufekci. </author> <title> A genetic algorithm for the talent scheduling problem. </title> <journal> Computer and Operations Research, </journal> <volume> 21(8) </volume> <pages> 927-940, </pages> <year> 1994. </year>
Reference-contexts: Many of the projects showed the success of their applications, including [ Biegal and Davern, 1990 ] , [ Syswerda, 1991 ] , [ Fang et al., 1993 ] , [ Uckun et al., Oct 1993 ] , [ Lee and Piramuthu, Oct 1994 ] , and <ref> [ Nordstrom and Tufekci, 1994 ] </ref> . GAs mimic the process of natural evolution in searching a solution space for optimal solutions. A GA combines the principle of "survival-and-the-fittest" with randomized information exchange.
Reference: [ Ogbu and Smith, 1990 ] <author> F. A. Ogbu and D. K. Smith. </author> <title> The application of the simulated annealing algorithm to the solution of the n/m/c max flowshop problem. </title> <journal> Computer and Operations Research, </journal> <volume> 17(3) </volume> <pages> 243-253, </pages> <year> 1990. </year>
Reference: [ Parr and Russell, 1995 ] <author> R. Parr and S. Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1088-1094, </pages> <year> 1995. </year>
Reference: [ Pomerleau, 1991 ] <author> D. A. Pomerleau. </author> <title> Efficient training of artificial neural networks for autonomous navigation. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 88-97, </pages> <year> 1991. </year>
Reference-contexts: A move involves a small cost; this tends to discourage to walk back and forth in a pair of states. To solve large problems, we use overlapping Gaussian ranges <ref> [ Pomerleau, 1991 ] </ref> as an input representation. Overlapping Gaussian ranges can be considered as an extension of the CMAC representation. While the CMAC uses a boolean value to tell whether a region is considered by a feature, overlapping Gaussian ranges smooth values in the boundary of the region. <p> LEARNING WITH HAND-ENGINEERED FEATURES 4.1.3 Network Architecture and Training Procedure To represent the value function, we trained feed-forward networks having 40 sigmoidal hidden units and 8 sigmoidal output units. The 8 output units encode the predicted RDF using the technique of overlapping Gaussian ranges <ref> [ Pomerleau, 1991 ] </ref> as follows. Each output unit represents one assigned RDF value, v j (j = 1; : : : ; 8). For the artificial problems, these RDF values are v 1 = 0:8; v 2 = 1:0; : : : ; v 8 = 2:2. <p> H2 has no adjustable parameters. H3 has 40 hidden units fully connected to H2, for a total of 720 parameters. The output layer has 8 units fully connected to H3 and encoding the predicted RDF using the technique of overlapping Gaussian ranges <ref> [ Pomerleau, 1991 ] </ref> . The output layer has 328 (= 8 (40 + 1)) parameters. Therefore, this net has a total of 1123 parameters. All units in H1, H3, and the output layer use sigmoidal transfer functions. 5.2. NETWORK ARCHITECTURE 143 144 CHAPTER 5.
Reference: [ Puterman and Shin, 1978 ] <author> M. L. Puterman and M. C. Shin. </author> <title> Modified policy iteration algorithm for discounted Markov decision problem. </title> <journal> Management Science, </journal> <volume> 24(11) </volume> <pages> 1127-1137, </pages> <year> 1978. </year>
Reference: [ Puterman, 1990 ] <author> M. L. Puterman. </author> <title> Markov decision process. </title> <editor> In D. P. Heyman and M. J. Sobel, editors, </editor> <booktitle> Handbooks in Operations Research and Management Science: Stochastic Models, chapter 8, </booktitle> <pages> pages 331-434. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference: [ Puterman, 1994 ] <author> M. L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <year> 1994. </year>
Reference-contexts: Under this stopping criterion, the resulting policy is called the *-optimal policy. There have been a number of variations to the standard value iteration algorithm for accelerating convergence. These variations include the Gauss-Seidel method, the Jacobi method, and the asynchronous method <ref> [ Bertsekas, 1987, Puterman, 1994 ] </ref> , which permits different value update orders and updating rules. Different variations win in different situations. Convergence of these variations have all been proved. <p> )V (s 0 ) (b) (Partial policy evaluation) Repeat for m iterations (m is determined by the evaluation rule) i. for s 2 S simultaneously set V (s) = R E (s; (s)) + fl s 0 2S P (s) (s; s 0 )V (s 0 ) in later iterations). <ref> [ Puterman, 1994 ] </ref> claims that modified policy iteration always converges more quickly than value iteration, because it avoids maximization over the set of actions at each pass. This algorithm is also a pseudo-polynomial algorithm under a fixed discount factor.
Reference: [ Rich and Knight, 1991 ] <author> E. Rich and K. Knight. </author> <booktitle> Artificial Intelligence. </booktitle> <publisher> McGraw-Hill, Inc., </publisher> <address> second edition, </address> <year> 1991. </year>
Reference-contexts: A number of search paradigms rely on an evaluation function as the heuristic to guide search. These search paradigms includes best-first search, mini-max game tree search, and various forms of greedy search and real-time search <ref> [ Russell and Norvig, 1995, Rich and Knight, 1991 ] </ref> . Search control knowledge can be also learned in the form of policies.
Reference: [ Ross, 1983 ] <author> S. M. Ross. </author> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic, </publisher> <address> New York, </address> <year> 1983. </year>
Reference: [ Rummery and Niranjan, 1994 ] <author> G. A. Rummery and M. Niranjan. </author> <title> On-line Q-learning using connectionist systems. </title> <type> Tech report cued/f-infeng/td 166, </type> <institution> Cambridge University, Engineering Dept, </institution> <year> 1994. </year>
Reference: [ Russell and Norvig, 1995 ] <author> S. Russell and P. Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <address> New Jersey, </address> <year> 1995. </year> <note> 166 BIBLIOGRAPHY </note>
Reference-contexts: A number of search paradigms rely on an evaluation function as the heuristic to guide search. These search paradigms includes best-first search, mini-max game tree search, and various forms of greedy search and real-time search <ref> [ Russell and Norvig, 1995, Rich and Knight, 1991 ] </ref> . Search control knowledge can be also learned in the form of policies.
Reference: [ Samuel, 1959 ] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3 </volume> <pages> 211-229, </pages> <year> 1959. </year>
Reference-contexts: While supervised learning was designed to solve various kinds of prediction problems, it can also be applied to learn an evaluation function and meta-level rules as search control knowledge to improve the efficiency of a problem solver. The most impressive works include Samual's checker-playing program <ref> [ Samuel, 1959 ] </ref> and the LEX system [ Mitchell et al., 1983 ] . Applying supervised learning to learn search control knowledge requires having knowledge about the correct solution paths. An analysis of the traces provides positive and negative instances for generating rules to control search. <p> It is worthwhile to mention that Samuel's checker-playing program <ref> [ Samuel, 1959 ] </ref> probably was the first system to use a kind of TD learning method. Samuel's program learned an evaluation function based on the difference of estimates between successive board positions.
Reference: [ Schoppers, 1987 ] <author> M. J. Schoppers. </author> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1039-1046, </pages> <year> 1987. </year>
Reference-contexts: A comparable and closely related work to RTDP is learning real-time A fl , LRTA fl [ Korf, 1990 ] . RTDP generalizes LRTA fl to stochastic problems and includes the option of updating values of many states at a time. RTDP also relates to "universal planning" <ref> [ Chapman, 1989, Ginsberg, 1989, Schoppers, 1987, Schoppers, 1989 ] </ref> . RTDP can be employed to learn a partial policy for a problem with a fixed starting state. In this situation, some exploration strategy must be applied for the learner in order to make it visit various states.
Reference: [ Schoppers, 1989 ] <author> M. J. Schoppers. </author> <title> In defense of reaction plans as caches. </title> <journal> AI Magazine, </journal> <volume> 10 </volume> <pages> 51-60, </pages> <year> 1989. </year>
Reference-contexts: A comparable and closely related work to RTDP is learning real-time A fl , LRTA fl [ Korf, 1990 ] . RTDP generalizes LRTA fl to stochastic problems and includes the option of updating values of many states at a time. RTDP also relates to "universal planning" <ref> [ Chapman, 1989, Ginsberg, 1989, Schoppers, 1987, Schoppers, 1989 ] </ref> . RTDP can be employed to learn a partial policy for a problem with a fixed starting state. In this situation, some exploration strategy must be applied for the learner in order to make it visit various states.
Reference: [ Schraudolph et al., 1994 ] <author> N. Schraudolph, P. Dayan, and T. Sejnowski. </author> <title> Using TD() to learn an evaluation function for the game of go. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Schwartz, 1993 ] <author> A. Schwartz. </author> <title> A reinforcement learning method for maximizing undiscounted reward. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: For applications where discounting does not apply, the value function can adopt the form of the expected average future reinforcement over time: V (s) = E lim P k k : (2:9) Solving MDPs with this type of value function is generally more complicated <ref> [ Schwartz, 1993, Tadepalli and Ok, 1994 ] </ref> . The optimal value function, noted as V fl , is a function that maximizes the expected total future reward over all states. A policy that achieves this objective is an optimal policy, noted fl .
Reference: [ Singh and Sutton, 1996 ] <author> S. P. Singh and R. S. Sutton. </author> <title> Reinforcement learning with replacing eligibility trace. </title> <note> Machine Learning (to appear), </note> <year> 1996. </year>
Reference-contexts: REINFORCEMENT LEARNING & STATE-SPACE SEARCH Here, s t is the current state. Initially, e 0 (s) is set to 0 for every state s. [ Sutton, 1988 ] and [ Dayan, 1992 ] proved the convergence of TD () for general . Recently, <ref> [ Singh and Sutton, 1996 ] </ref> presented a new type of trace, the replacing eligibility trace, and showed that it performs better than the above accumulating trace. The replacing trace differs the accumulating trace in that it sets e t (s) = 1 when s is revisited. <p> Having loops not only is inefficient but also may cause troubles. When using the accumulating eligibility trace, the value function can be over-estimated when a loop is encountered <ref> [ Singh and Sutton, 1996 ] </ref> . If we can not completely avoid loops, we should employ the replacing eligibility trace to obtain more reliable results. * parameter: The standard TD () algorithm updates values strictly on-line. The parameter is important to the efficiency of the learning process.
Reference: [ Smith and Ow, 1986 ] <author> S. F. Smith and P. S. </author> <title> Ow. Constructing and maintaining detailed production plans: Inverstigations into the development of knowledge-based factory scheduling systems. </title> <journal> AI magazine, </journal> <volume> 7(4) </volume> <pages> 45-61, </pages> <year> 1986. </year>
Reference: [ Smith et al., 1986 ] <author> S. F. Smith, P. S. Ow, C. Le Pape, B. McLarn, and N. Muscet-tola. </author> <title> Integrating mupltiple scheduling perspectives to generate detailed production plans. </title> <booktitle> In Proceedings 1986 SME Conference o AI in Manufacturing, </booktitle> <pages> pages 123-137, </pages> <year> 1986. </year>
Reference: [ Smith, 1994 ] <author> S. F. Smith. OPIS: </author> <title> A metholology and architecture for reactive scheduling. </title> <editor> In M. Zweben and M. S. Fox, editors, </editor> <title> Intelligent Scheduling, </title> <booktitle> chapter 2, </booktitle> <pages> pages 29-66. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1994. </year>
Reference: [ Sutton, 1988 ] <author> R. S. Sutton. </author> <title> Learning to predict by the method of temporal difference. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: TD learning provides a fundamental idea for reinforcement learning. Learning is aimed at reducing the difference between the estimated values of a pair of adjacent states (s; s 0 ). A simple TD update, which is noted as TD (0) <ref> [ Sutton, 1988 ] </ref> , works as follows: V (s) := V (s) + ff [R (s; a; s 0 ) + flV (s 0 ) V (s)] ; (2:22) where ff is the learning rate. <p> REINFORCEMENT LEARNING & STATE-SPACE SEARCH Here, s t is the current state. Initially, e 0 (s) is set to 0 for every state s. <ref> [ Sutton, 1988 ] </ref> and [ Dayan, 1992 ] proved the convergence of TD () for general . Recently, [ Singh and Sutton, 1996 ] presented a new type of trace, the replacing eligibility trace, and showed that it performs better than the above accumulating trace. <p> Among various different types of function approximators, neural networks appear most popular in application. A nice result about the neural network approach is that 40 CHAPTER 2. REINFORCEMENT LEARNING & STATE-SPACE SEARCH TD () as originally presented in <ref> [ Sutton, 1988 ] </ref> has been well combined with the backpropagation method. <p> Five goals for learning to walk. Policy Correctness Mean Prediction Error Goal-1 &gt; 0.9 &lt; 0:02 Goal-2 1 &lt; 0:02 Goal-3 1 &lt; 0:01 Goal-4 1 &lt; 0:002 Goal-5 1 &lt; 0:001 over all states along the sequence. Weight update is performed after each sequence. Under this condition, <ref> [ Sutton, 1988 ] </ref> proves that TD (1) is exactly the same as using the actual final value as the output value. Let us consider how to learn the value function given that we have obtained the best trajectory.
Reference: [ Sutton, 1990 ] <author> R. S. Sutton. </author> <title> Integrated architecture for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Senventh International Workshop on Machine Learning, </booktitle> <pages> pages 216-224. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Most reinforcement learning researchers have been focused on learning in this type of environment, coming up with a number of important reinforcement learning methods such as model-free method Q-learning [ Watkins, 1989, Watkins and Dayan, 1992 ] and model-based methods Dyna <ref> [ Sutton, 1990 ] </ref> and prioritized sweeping [ Moore and Atkeson, 1993 ] . <p> Other information-directed exploration methods include the interval estimation method [ Kaelbling, 1990 ] , the exploration bonus in Dyna <ref> [ Sutton, 1990 ] </ref> , and exploration based on a priority queue [ Moore and Atkeson, 1993 ] .
Reference: [ Sutton, 1991 ] <author> R. S. Sutton. </author> <title> Planning by incremental dynamic programming. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 353-357. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [ Sutton, 1996 ] <author> R. S. Sutton. </author> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <address> Cambridge, MA, 1996. </address> <publisher> MIT Press. BIBLIOGRAPHY 167 </publisher>
Reference-contexts: Popular representations include various types of neural networks, CMAC (Cerebellar Model Articulation computer) <ref> [ Albus, 1981, Sutton, 1996 ] </ref> , decision trees, and local memory-based representations such as nearest neighbor methods. Generalization is important and useful for reinforcement learning in many situations as follows: 38 CHAPTER 2. REINFORCEMENT LEARNING & STATE-SPACE SEARCH * The state space is often very large. <p> When applied to reinforcement learning, these methods may encounter several pitfalls. The key problem is that in reinforcement learning based on dynamic programming or temporal difference learning, the estimates for the value function are built on the basis of other estimates. <ref> [ Sutton, 1996 ] </ref> stated: Because the estimates are imperfect, and because they in turn are used as the targets for other estimates, it seems possible that the ultimate result might be very poor estimates, or even divergence. <p> Fortunately, many pitfalls can be avoided by a careful design of reinforcement learning systems. [ Gordon, 1995 ] showed in theory that several methods such as nearest-neighbor will not diverge. <ref> [ Sutton, 1996 ] </ref> reported that the unstable phenomena that appeared in [ Boyan and Moore, 1995 ] 's examples can be resolved using an on-line Sarsa learning approach based on a sparse-coarse-coded CMAC representation. [ Baird, 1995 ] presented the residual algorithm which employs a combined current gradient and "residual
Reference: [ Syswerda, 1991 ] <author> G. Syswerda. </author> <title> Schedule optimization using genetic algorithms. </title> <editor> In L. Davis, editor, </editor> <booktitle> Handbook of Genetic Algorithms, </booktitle> <pages> pages 332-349. </pages> <publisher> Van Nostrand, </publisher> <year> 1991. </year>
Reference-contexts: Finally, there has been a large amount of research using genetic algorithms (GAs) [ Goldberg, 1989 ] . Many of the projects showed the success of their applications, including [ Biegal and Davern, 1990 ] , <ref> [ Syswerda, 1991 ] </ref> , [ Fang et al., 1993 ] , [ Uckun et al., Oct 1993 ] , [ Lee and Piramuthu, Oct 1994 ] , and [ Nordstrom and Tufekci, 1994 ] .
Reference: [ Tadepalli and Ok, 1994 ] <author> P. Tadepalli and D. </author> <title> Ok. Discounting considered harmful: A comparison of reinforcement learning methods in automatic guided vehicle scheduling. </title> <booktitle> In Proceedings of Robot Learning workshop, </booktitle> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: For applications where discounting does not apply, the value function can adopt the form of the expected average future reinforcement over time: V (s) = E lim P k k : (2:9) Solving MDPs with this type of value function is generally more complicated <ref> [ Schwartz, 1993, Tadepalli and Ok, 1994 ] </ref> . The optimal value function, noted as V fl , is a function that maximizes the expected total future reward over all states. A policy that achieves this objective is an optimal policy, noted fl .
Reference: [ Tesauro, 1992 ] <author> G. J. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> 8(3/4):257-277, 1992. 
Reference-contexts: An important research problem is to make reinforcement learning algorithms work with more compact representations. The research described in this dissertation is motivated and greatly encouraged by the success of the TD-gammon system developed by Tesauro <ref> [ Tesauro, 1992 ] </ref> . Tesauro showed that the reinforcement learning algorithm TD () could learn to play world-class backgammon. His TD-gammon program is now far better than any other computer program and competitive with the best human players. <p> However, the length of schedules varies depending on the number of tasks and the complexity of their temporal and resource constraints. In previous applications of reinforcement learning, such as Tesauro's <ref> [ Tesauro, 1992 ] </ref> work on backgammon, the representation was exact: every possible state had a unique representation. However, in scheduling this is not possible: we must employ some form of abstract input representation.
Reference: [ Thrun and Schwartz, 1993 ] <author> S. Thrun and A. Schwartz. </author> <title> Issues in using approximation for reinforcement learning. </title> <booktitle> In Proceedings of the Fourth Connectionist Models Summer School, </booktitle> <address> Hillsdale, NJ, 1993. </address> <publisher> Lawrence Erlbaum Publisher. </publisher>
Reference-contexts: In theory, reinforcement learning algorithms converge to optimality only when lookup-tables are used. <ref> [ Thrun and Schwartz, 1993 ] </ref> pointed out that an 2.4. SCALE UP: GENERALIZATION 39 important source of the failures is due to a systematic overestimation of the value function by the "max" operation according to the definition of the value function.
Reference: [ Thrun, 1992 ] <author> S. Thrun. </author> <title> The role of exploration in learning control. </title> <editor> In D. A. White and D. A. Sofge, editors, </editor> <title> Handbook of Intelligent Control: Neural, Fuzzy and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: The other class is more information-directed, named "directed" exploration, which memorizes information about the learning process and utilizes this information for directing exploration. <ref> [ Thrun, 1992 ] </ref> compared the following directed exploration methods: * counter-based exploration: count the number of visits to each state and go to less explored states. * recency-based exploration: similar to the eligibility trace in Sutton's TD (). * error-based exploration: calculate the change of the value estimates and choose <p> For a deterministic problem, while learning time with the random-walk based exploration is expected to scale exponentially in the size of state space in order to come up with a complete policy [ Whitehead, 1991 ] , most directed exploration methods only involve polynomial time <ref> [ Thrun, 1992 ] </ref> . Evidently, for small problems directed exploration is more efficient. However, the directed exploration methods have difficulties to scale up for larger problems because these methods generally allocate one counter of information for each state or state-action pair.
Reference: [ Tseng, 1990 ] <author> P. Tseng. </author> <title> Solving H-horizon, stationary Markov decision problems in time proportional to log(h). </title> <journal> Operations Research Letters, </journal> <volume> 9(5) </volume> <pages> 287-297, </pages> <year> 1990. </year>
Reference-contexts: Furthermore, it was proved that for fixed fl &lt; 1, the value iteration algorithm takes time polynomial in M; N , and B <ref> [ Tseng, 1990, Littman et al., 1995b ] </ref> , which is pseudo-polynomial (because it relies on B). In the worst case the number of iterations grows polynomially in 1=(1 fl), so the convergence could be very slow when the discount factor is close to 1.
Reference: [ Tsitsiklis, 1994 ] <author> J. Tsitsiklis. </author> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3) </volume> <pages> 185-202, </pages> <year> 1994. </year>
Reference: [ Tsitsiklis, 1995 ] <author> J. Tsitsiklis. </author> <title> Counterexample to temporal difference learning. </title> <journal> Neural Computation, </journal> <volume> 7(2) </volume> <pages> 270-279, </pages> <year> 1995. </year>
Reference-contexts: SCALE UP: GENERALIZATION 39 important source of the failures is due to a systematic overestimation of the value function by the "max" operation according to the definition of the value function. Each Bellman backup could convert zero-mean "function approximation noise" into a positive-mean noise. <ref> [ Tsitsiklis, 1995 ] </ref> , [ Baird, 1995 ] and [ Boyan and Moore, 1995 ] showed some simple examples where value function errors grow arbitrarily large when function approximation with reinforcement learning is used. Both [ Tsitsiklis, 1995 ] and [ Boyan and Moore, 1995 ] have suggested using actual <p> Each Bellman backup could convert zero-mean "function approximation noise" into a positive-mean noise. <ref> [ Tsitsiklis, 1995 ] </ref> , [ Baird, 1995 ] and [ Boyan and Moore, 1995 ] showed some simple examples where value function errors grow arbitrarily large when function approximation with reinforcement learning is used. Both [ Tsitsiklis, 1995 ] and [ Boyan and Moore, 1995 ] have suggested using actual values as in the TD () algorithm when = 1 to overcome this problem.
Reference: [ Uckun et al., Oct 1993 ] <author> S. Uckun, S. Bagchi, K. Kawamura, and Y. Miyabe. </author> <title> Managing genetic search in job shop scheduling. </title> <journal> IEEE Expert, </journal> <pages> pages 15-24, </pages> <month> Oct, </month> <year> 1993. </year>
Reference-contexts: Finally, there has been a large amount of research using genetic algorithms (GAs) [ Goldberg, 1989 ] . Many of the projects showed the success of their applications, including [ Biegal and Davern, 1990 ] , [ Syswerda, 1991 ] , [ Fang et al., 1993 ] , <ref> [ Uckun et al., Oct 1993 ] </ref> , [ Lee and Piramuthu, Oct 1994 ] , and [ Nordstrom and Tufekci, 1994 ] . GAs mimic the process of natural evolution in searching a solution space for optimal solutions. A GA combines the principle of "survival-and-the-fittest" with randomized information exchange.
Reference: [ Utgoff and Clouse, 1991 ] <author> P. E. Utgoff and J. A. Clouse. </author> <title> Two kinds of training information for evaluation function learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 596-600, </pages> <year> 1991. </year>
Reference-contexts: To speed learning further, the quadruples are presented to the system in temporally backward order. <ref> [ Utgoff and Clouse, 1991 ] </ref> 's approach allows the learner to query an expert as to the correct action to take when it has low confidence in its own decisions. [ Whitehead, 1991 ] presented two methods.
Reference: [ Van Laarhoven et al., 1992 ] <author> P. J. M. Van Laarhoven, E. H. L. Aarts, and J. K. Lenstra. </author> <title> Job shop scheduling by simulated annealing. </title> <journal> Operations Research, </journal> <volume> 40 </volume> <pages> 113-125, </pages> <year> 1992. </year>
Reference: [ Van Laarhoven, 1988 ] <author> P. J. M. Van Laarhoven. </author> <title> Theoretical and Computational Aspects of Simulated Annealing. </title> <type> PhD thesis, </type> <institution> Erasmus University, Rotterdam, Nether-lands, </institution> <year> 1988. </year>
Reference-contexts: The search was performed based on the evaluation of the "textures" of the constraints. These textures measure the properties of the constraint diagram such as contention and reliance of variables on values, providing useful heuristics for variable and value ordering. The second paradigm is using simulated annealing <ref> [ Kirkpatrick et al., 1983, Van Laarhoven, 1988, Aarts, 1989 ] </ref> . Rather than regular hill climbing, simulated annealing allows some downhill moves with a certain probability; therefore, it comes up with a higher chance of finding the global optimal solution.
Reference: [ Watkins and Dayan, 1992 ] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> 8(3/4):279-292, 1992. 
Reference-contexts: Most reinforcement learning researchers have been focused on learning in this type of environment, coming up with a number of important reinforcement learning methods such as model-free method Q-learning <ref> [ Watkins, 1989, Watkins and Dayan, 1992 ] </ref> and model-based methods Dyna [ Sutton, 1990 ] and prioritized sweeping [ Moore and Atkeson, 1993 ] .
Reference: [ Watkins, 1989 ] <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year> <note> 168 BIBLIOGRAPHY </note>
Reference-contexts: Most reinforcement learning researchers have been focused on learning in this type of environment, coming up with a number of important reinforcement learning methods such as model-free method Q-learning <ref> [ Watkins, 1989, Watkins and Dayan, 1992 ] </ref> and model-based methods Dyna [ Sutton, 1990 ] and prioritized sweeping [ Moore and Atkeson, 1993 ] . <p> In other words, it is a learning method to approximate the long-term probabilities of a Markov chain. TD (0) is a special case of a more sophisticated learning algorithm named Q-learning <ref> [ Watkins, 1989 ] </ref> |a learning algorithm that learns a policy without knowing the action model. TD (0) works for a model with one action, while Q-learning works for an arbitrary number of actions. Due to the convergence proof of Q-learning provided by [ Watkins, 1989 ] , TD (0) also <p> a more sophisticated learning algorithm named Q-learning <ref> [ Watkins, 1989 ] </ref> |a learning algorithm that learns a policy without knowing the action model. TD (0) works for a model with one action, while Q-learning works for an arbitrary number of actions. Due to the convergence proof of Q-learning provided by [ Watkins, 1989 ] , TD (0) also converges for predicting the value function. 2.3. SCALE UP: LEARNING PARTIAL POLICIES 35 Nonetheless, TD learning also works in control problems to produce an optimal policy.
Reference: [ Whitehead, 1991 ] <author> S. D. Whitehead. </author> <title> A complexity analysis of cooperative mechanisms in reinforcement learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 607-613, </pages> <year> 1991. </year>
Reference-contexts: For a deterministic problem, while learning time with the random-walk based exploration is expected to scale exponentially in the size of state space in order to come up with a complete policy <ref> [ Whitehead, 1991 ] </ref> , most directed exploration methods only involve polynomial time [ Thrun, 1992 ] . Evidently, for small problems directed exploration is more efficient. <p> To speed learning further, the quadruples are presented to the system in temporally backward order. [ Utgoff and Clouse, 1991 ] 's approach allows the learner to query an expert as to the correct action to take when it has low confidence in its own decisions. <ref> [ Whitehead, 1991 ] </ref> presented two methods. One is "Learning with an External Critic", where an external agent provides an immediate critic to the learner for its action. This critic is used to learn a "biasing" function over state-action pairs. <p> The guidance can be either taken on-line or taken in advance as prior knowledge. Several researchers have reported that teaching methods combined with some random exploration can scale very efficiently to solve difficult problems <ref> [ Lin, 1991, Whitehead, 1991 ] </ref> . 2.5.3 Experience Replay As discussed earlier, generalization with neural networks has been the most popular methodology for solving large-scale problems.
Reference: [ Whitley et al., 1989 ] <author> D. Whitley, T. Starkweather, and D. Fuquay. </author> <title> Scheduling problems and traveling salesmen: the genetic edge recombination operator. </title> <booktitle> In Proceedings of the the Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 133-140. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: work in using GAs to learn scheduling knowledge. [ Hilliard et al., 1988 ] showed experimentally that optimal rules for simple scheduling problems could be discovered by GAs. [ Davis, 1985 ] focused attention on the order-dependent nature of job-shop scheduling, and then [ Cleveland and Smith, 1989 ] and <ref> [ Whitley et al., 1989 ] </ref> explored new GA operators to handle these order considerations. [ Aytug et al., 1994 ] presented an approach of using GAs to learn composite dispatching rules as scheduling knowledge based on some "base" dispatching rules such as FCFS (first come first serve), EDD (earliest due
Reference: [ Williams and Baird, 1993 ] <author> R J. Williams and L. C. Baird. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Tech report nu-ccs-93-14, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA, </address> <year> 1993. </year>
Reference-contexts: Initialize V (s) arbitrarily 2. Repeat until stopping criterion is satisfied (a) Perform Bellman update, for all s 2 S simultaneously set V (s) := max a2A (s) R E (s; a) + fl s 0 2S P a (s; s 0 )V (s 0 ) 2*fl=(1 fl) <ref> [ Williams and Baird, 1993 ] </ref> . Under this stopping criterion, the resulting policy is called the *-optimal policy. There have been a number of variations to the standard value iteration algorithm for accelerating convergence.
Reference: [ Woodruff, 1994 ] <author> D. L. Woodruff. </author> <title> Simulated annealing and tabu search: Lessons from a line search. </title> <journal> Computer and Operations Research, </journal> <volume> 21(8) </volume> <pages> 823-840, </pages> <year> 1994. </year>
Reference-contexts: Several researchers have shown that tabu search has the potential to outperform simulated annealing for scheduling and optimization problems <ref> [ Lourenco and Zwijnenburg, 1995, Woodruff, 1994 ] </ref> . Finally, there has been a large amount of research using genetic algorithms (GAs) [ Goldberg, 1989 ] .
Reference: [ Zhang and Dietterich, 1995 ] <author> W. Zhang and T. Dietterich. </author> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In IJCAI-95, </booktitle> <pages> pages 1114-1120, </pages> <year> 1995. </year>
Reference-contexts: In general, multiple updates with a small learning rate work better than a single update with a big learning rate. Chapter 4 Learning with Hand-Engineered Features This chapter is organized as follows. We first demonstrate the main results presented in the <ref> [ Zhang and Dietterich, 1995 ] </ref> paper. We next describe an earlier preliminary study that was conducted to choose good features for describing schedules and to design the network architecture.
Reference: [ Zhang and Dietterich, 1996 ] <author> W. Zhang and T. Dietterich. </author> <title> High-performance job-shop scheduling with a time-delay TD() network. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <address> Cambridge, MA, 1996. </address> <publisher> MIT Press. </publisher>
Reference: [ Zweben et al., 1992 ] <author> M. Zweben, E. Davis, B. Daun, E. Drascher, M. Deale, and M. Eskey. </author> <title> Learning to improve constraint-based scheduling. </title> <journal> Artificial Intelligence, </journal> <volume> 58 </volume> <pages> 271-296, </pages> <year> 1992. </year>
Reference-contexts: Although learning provides a problem solver with better knowledge of search, the system may slow down dramatically as more complex control knowledge is learned, because of the cost of evaluating the control knowledge. The EBL methodology has been applied in a scheduling domain. For example, <ref> [ Zweben et al., 1992 ] </ref> developed an EBL method named plausible explanation-based learning (PEBL) to overcome the utility problem and applied it to solving the space shuttle payload processing problem. In PEBL, the concept description is generalized by a set of examples rather than by a single example. <p> Under the problem configuration of ART-2, there might not be domain-specific aspects that require using different scheduling strategies. We conducted another experiment to test for domain-specific knowledge as follows. We tested on the scheduling problems generated according to the mission component method <ref> [ Zweben et al., 1992 ] </ref> . By this method, a mission (job) is composed 4.7. SUMMARY 137 of tasks and constraints from a generic flow with various "options". A "suite" of scheduling problems is considered to be a domain.
Reference: [ Zweben et al., 1994 ] <author> M. Zweben, B. Daun, and M. </author> <title> Deal. Scheduling and rescheduling with iterative repair. </title> <editor> In M. Zweben and M. S. Fox, editors, </editor> <title> Intelligent Scheduling, </title> <booktitle> chapter 8, </booktitle> <pages> pages 241-255. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year> <pages> APPENDIX </pages>
Reference-contexts: This paradigm has been applied in many scheduling applications [ Ogbu and Smith, 1990, Abramson, 1991, Van Laarhoven et al., 1992, Zweben et al., 1992, Mittenthal et al., 1993 ] . One most prominent work was Zweben and colleague's Space Shuttle scheduling application <ref> [ Zweben et al., 1994 ] </ref> . The resulting scheduling system is able to provide an efficient and flexible facility for scheduling space shuttle ground operations. It is in regular use at the Kennedy Space Center. The next noticeable search paradigm is tabu search [ Glover, 1990 ] . <p> DOMAIN: THE TASK, SCHEDULING, AND LEARNING Other types of space shuttle scheduling problems were also discussed in <ref> [ Zweben et al., 1994 ] </ref> . Other practical optimization criteria considered in this domain include minimizing weekend work in a schedule and minimizing schedule perturbation for rescheduling. A more sophisticated specification of the domain would include state constraints.
References-found: 129

