URL: http://www.wi.leidenuniv.nl/home/joost/eann.ps.gz
Refering-URL: http://www.wi.leidenuniv.nl/home/joost/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: bas@cs.ruu.nl  
Title: Interactive Segmentation of Three-dimensional Medical Images (Extended abstract)  
Author: S. Haring J.N. Kok 
Address: P.O.Box 80.089, 3508 TB Utrecht, The Netherlands  
Affiliation: Department of Computer Science, Utrecht University,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Each voxel is then represented as a pattern of real-valued numbers. For the classification of these patterns we apply a novel self-organizing network that is both an extension of a vector quantization network <ref> [1] </ref> and a self-organizing version of the radial-basis network [2]. The network adapts itself such that it estimates the probability density functions underlying the various objects in the image. These estimations are then used for classification.
Reference: [2] <author> J. Moody and C.J. Darken. </author> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference-contexts: Each voxel is then represented as a pattern of real-valued numbers. For the classification of these patterns we apply a novel self-organizing network that is both an extension of a vector quantization network [1] and a self-organizing version of the radial-basis network <ref> [2] </ref>. The network adapts itself such that it estimates the probability density functions underlying the various objects in the image. These estimations are then used for classification.
Reference: [3] <author> J.J. Koenderink and A.J van Doorn. </author> <title> The structure of images. </title> <journal> Biological Cybernetics, </journal> <volume> 50 </volume> <pages> 363-370, </pages> <year> 1984. </year>
Reference-contexts: This calls for a multiresolution (or multiscale) observation of the image. An elegant way (in terms of a number of mathematical constraints) of downsampling the resolution of I (~x) is as follows <ref> [3] </ref>: I (~x; ) = G (~x; ) I (~x ); (1) where G (~x; ) is the normalized Gaussian function with a scale , and denotes the convolution operator. I (~x; ) has been called a scale space. Equation (1) follows from the diffusion equation.
Reference: [4] <author> L. M. J. Florack, B. M. ter Haar Romeny, J. J. Koenderink, and M. A. Viergever. </author> <title> Scale and the differential structure of images. </title> <journal> Image and Vision Computing, </journal> <volume> 10(6) </volume> <pages> 376-388, </pages> <note> July/August 1992. Special Issue: Information Processing in Medical Imaging. </note>
Reference-contexts: Syntactical structure is determined by the differential structure at various scales; to ensure that the observed structure is invariant under rotation we apply specific combinations of derivatives <ref> [4] </ref>.
Reference: [5] <author> E. Parzen. </author> <title> On estimation of a probability density function and mode. </title> <journal> Annual Mathematical Statistics, </journal> <volume> 33 </volume> <pages> 1065-1076, </pages> <year> 1962. </year>
Reference-contexts: The Bayes rule yields theoretically the optimal classifier. The obvious problem is, however, to find the functions P (! i j~p ). A standard method is Parzen window estimation <ref> [5] </ref>; it produces an estimate of P (~p j! i ) which may straightforwardly be transformed to P (! i j~p ). In this method each pattern is seen as a Gaussian distribution whose mean equals the pattern position and whose standard deviation (or scale) has some arbitrary value.
Reference: [6] <author> S. Haring and J.N. Kok. </author> <title> Adaptive scaling of codebook vectors. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms, </booktitle> <year> 1995. </year>
Reference-contexts: By choosing the parameters fi and fl we can tune the algorithm such that the correct estimate of a normal probability distribution is a stable point of the algorithm <ref> [6] </ref>. After a number of iterations the average of all Gaussians in the network is an estimate of P (~p j! i ). Note that the network was only trained with patterns from class ! i .
References-found: 6

