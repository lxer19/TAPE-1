URL: http://www.ai.mit.edu/people/sayan/webPub/1st.ps
Refering-URL: http://www.ai.mit.edu/people/sayan/pub.html
Root-URL: 
Email: sayan@cs.columbia.edu nayar@cs.columbia.edu  
Phone: 2  
Title: Object Recognition and Pose Estimation in Eigenspace Using a RBF Network 1  
Author: Shayan Mukherjee and Shree K. Nayar 
Web: 76-92-C-0007.  
Note: 1 This research was conducted at the Center for Research in  It was supported in part by the David and Lucile Packard Fellowship and in part by ARPA Contract No. DACA  
Date: November, 1993  
Address: New York, NY 10027 USA  New York, N.Y. 10027  
Affiliation: Department of Computer Science Columbia University  Intelligent Systems, Department of Computer Science, Columbia University.  Department of Applied Physics, Columbia University,  
Pubnum: CUCS-040-93  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. J. Besl and R. C. Jain, </author> <title> "Three-Dimensional Object Recognition," </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 17, No. 1, </volume> <pages> pp. 75-145, </pages> <year> 1985. </year>
Reference-contexts: This task can be extremely difficult depending on the shape, reflectance properties, and pose of the object, and the lighting conditions of the environment. The appropriate technique to perform this task depends upon the above parameters. For a survey of several object recognition techniques see Besl and Jain <ref> [1] </ref>. We are interested in vison systems that recognize three-dimensional objects from their appearance in two-dimensional brightness images. To recognize an object, a model of the object must be stored in memory. It is desirable for these models to be acquired automatically, without human assistance.
Reference: [2] <author> S. Edelman, H. Bultoff, D. Weinshall, </author> <title> "Stimulus Familiarity Determines Recognition Strategy for Novel 3D Objects," </title> <journal> A.I. </journal> <note> Memo No.1138, </note> <institution> Cambridge : Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1989. </year>
Reference-contexts: So, our recognition system must acquire these object models. This means the system learns the objects it is to recognize. In short, we are interested in systems that learn and recognize three-dimensional objects from their two-dimensional images. An interesting aside is that recent psychophysical studies [3] <ref> [2] </ref> indicate that the human vision system represents objects with a set of two-dimensional views rather than one three-dimensional view, that is the human visual system learns three dimensional objects from their two-dimensional images.
Reference: [3] <author> M. Tarr and S. Pinker, </author> <title> "Mental Rotation and Orientation-Dependence in Shape Recog nition," </title> <journal> Cognitive Psychology, Vo. </journal> <volume> 21, </volume> <pages> pp. 233-282, </pages> <year> 1989. </year>
Reference-contexts: So, our recognition system must acquire these object models. This means the system learns the objects it is to recognize. In short, we are interested in systems that learn and recognize three-dimensional objects from their two-dimensional images. An interesting aside is that recent psychophysical studies <ref> [3] </ref> [2] indicate that the human vision system represents objects with a set of two-dimensional views rather than one three-dimensional view, that is the human visual system learns three dimensional objects from their two-dimensional images.
Reference: [4] <author> T. Poggio and S. Edelman, </author> <title> "A network that learns to recognize three-dimensional ob jects," </title> <journal> Nature, </journal> <volume> Vol. 343, </volume> <pages> pp. 263-266, </pages> <year> 1990. </year>
Reference-contexts: There currently exist a handful of computer vision systems <ref> [4] </ref> [5] [6] [7] that have this ability. Poggio and Edelman [4] have used a three-layer network, a regularization network, to learn three-dimensional stick figures from images of these figures taken at multiple viewpoints. <p> There currently exist a handful of computer vision systems <ref> [4] </ref> [5] [6] [7] that have this ability. Poggio and Edelman [4] have used a three-layer network, a regularization network, to learn three-dimensional stick figures from images of these figures taken at multiple viewpoints.
Reference: [5] <author> S. Edelman and D. Weinshall, </author> <title> "A self-organizing multiple-view representation of 3D objects," </title> <journal> Biological Cybernetics, </journal> <volume> Vol. 64, </volume> <pages> pp. 209-219, </pages> <year> 1991. </year>
Reference-contexts: There currently exist a handful of computer vision systems [4] <ref> [5] </ref> [6] [7] that have this ability. Poggio and Edelman [4] have used a three-layer network, a regularization network, to learn three-dimensional stick figures from images of these figures taken at multiple viewpoints.
Reference: [6] <author> M. A. Turk and A. P. Pentland, </author> <title> "Face Recognition using Eigenfaces," </title> <booktitle> Proc. of IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 586-591, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: There currently exist a handful of computer vision systems [4] [5] <ref> [6] </ref> [7] that have this ability. Poggio and Edelman [4] have used a three-layer network, a regularization network, to learn three-dimensional stick figures from images of these figures taken at multiple viewpoints.
Reference: [7] <author> H. Murase and S. K. Nayar, </author> <title> "Learning and Recognition of 3D Objects from Appearance," </title> <booktitle> Proc. of IEEE Workshop on Qualitative Vision, </booktitle> <pages> pp. 39-50, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: There currently exist a handful of computer vision systems [4] [5] [6] <ref> [7] </ref> that have this ability. Poggio and Edelman [4] have used a three-layer network, a regularization network, to learn three-dimensional stick figures from images of these figures taken at multiple viewpoints. <p> Once the network learns the figures, the network will recognize the figure in an image and estimates its pose, even if the image is taken from a novel viewpoint. Murase and Nayar <ref> [7] </ref> developed a system that uses principal component analysis to a create a compact representation of object appearance parameterized by pose and illumination. Once this compact representation, subspace, is created, object recognition and parameter estimation is performed in the subspace. <p> Unlike Poggio and Edelman's network, recognition and pose estimation is performed for real objects instead of stick figures. This system also alleviates some problems that arise in the system developed by Murase and Nayar. In the vision system proposed by Murase and Nayar <ref> [7] </ref>, a compact representation of object appearance parameterized by pose and illumination is introduced. In this paper, we ignore the illumination parameter, since we are only interested in estimating the pose of the object in an image.
Reference: [8] <author> T. Poggio and F. Girosi, </author> <title> "Networks for Approximation and Learning," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 78, </volume> <pages> pp. 1481-1497, </pages> <year> 1990. </year>
Reference-contexts: The search algorithm can only be performed on discrete points. We introduce a technique that constructs a continuous representation of the manifold and uses this representation to find parameter values along the manifold. A class of three-layer networks, regularization networks, have been developed by Poggio and Girosi <ref> [8] </ref> that take discrete input-output values and approximate a continuous function from these discrete values. We replace the resampled manifold representation and search with a network that performs an input-output mapping, from a point in universal eigenspace to its pose and recognition parameters. <p> W fl are the optimal parameter values of the approximating function. There exist standard formulations of the approximating function, F (W; x) <ref> [8] </ref>. <p> In addition, we decided not to minimize the error functional with respect to certain parameters. This was done to reduce the number of computations required in optimization. We first set initial parameter estimates for the center vectors, t j . The standard approach <ref> [8] </ref> is setting each t j to a particular x i . In our network, as in almost all RBF networks, there are more input vectors than center vectors. So we must choose which input vector, x i , a particular center vector, t j , is set to.
Reference: [9] <author> W. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling, </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: jjx i t j jj 2 = 2 dH [F o (W o ;x)] = 2 i=1 jjx i t j jj 2 = 2 dH [F o (W o ;x)] 4 j i=1 k=1 jjx i t j jj 2 = 2 A conjugate gradient algorithm [Press et. al. <ref> [9] </ref>] can be used to find the parameter values that satisfy the above conditions. There are two problems with the conjugate gradient descent algorithm. The first problem is that the algorithm is computationally intensive. <p> The other option is to use methods that avoid the problem of local minima. Simulated annealing and hybrid methods such as conjugate gradient descent with a stochastic term are two examples <ref> [9] </ref>. However, these methods are extremely computationally intensive. The most attractive solution was using a conjugate gradient algorithm with good initial parameter estimates. In addition, we decided not to minimize the error functional with respect to certain parameters. This was done to reduce the number of computations required in optimization.
Reference: [10] <author> S.A. Nene and S. K. Nayar, </author> <title> "Binary Search Through Multiple Dimensions," </title> <type> Technical Report CUCS-01-94, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> January, </month> <year> 1994. </year>
Reference-contexts: An image of the object to be recognized is projected onto a point in universal eigenspace. We search for the point on the resampled manifolds closest to this point. The search algorithm <ref> [10] </ref> currently implemented is based on binary search. Once this point is found, the object recognized is determined by which manifold the point lies upon. The pose estimated, for that object, is the pose parameter of that point. <p> The performance in each of these areas is compared to that of a binary search <ref> [10] </ref> and an exhaustive search. We show that the network's performance is comparable or superior to that of both searches. An image of the object to be recognized is projected onto a point in eigenspace. <p> The pose estimated is the pose parameter of this point. For the exhaustive search, the closest point is found by comparing the distance between this point and every point on the resampled manifolds. The binary search algorithm <ref> [10] </ref> also finds the point on the resampled manifolds for which the distance measure is minimum. However, complicated data structures are used to facilitate the search, making it much more efficient than the exhaustive search.
Reference: [11] <author> E. Oja, </author> <title> Subspace methods of Pattern Recognition, </title> <publisher> Research Studies Press, </publisher> <address> Hertfordshire, </address> <year> 1983. </year>
Reference-contexts: For each object, a large image set is created by taking images of an object at various poses. This large image set is then compressed using the Karhunen-Loeve transform <ref> [11] </ref> to obtain a subspace. The eigenvectors of the correlation matrix of this image set are computed. The eigenvectors, corresponding to the largest eigenvalues, make up the dimensions of this subspace or eigenspace. When this subspace is computed using image sets of all objects, it is called the universal eigenspace.
Reference: [12] <author> M. P. </author> <title> Do Carmo, Differential Geometry of Curves and Surfaces, </title> <publisher> Prentice-Hall, </publisher> <address> New Jersey, </address> <year> 1976. </year>
Reference-contexts: The resulting analytic function, f , approximated from these sets, performs the desired mapping. An input-output mapping from R n to R m is identical to graphs in R n fi R m . This can be expressed as the following proposition <ref> [12] </ref>: If f : U 7! R m is a differentiable function in an open set U of R n , then the graph of f , that is, the subset R n+m given by (x; f (x)) is a manifold.
Reference: [13] <author> F. Girosi, M. Jones, and T. Poggio, </author> <title> "Priors, Stabilizers and Basis Functions: from regularization to radial, tensor, and additive splines," </title> <journal> A.I. </journal> <note> Memo No.1430, </note> <institution> Cambridge : Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: For such networks the user would only have to specify the maximum value allowed for the error functional. The network parameters would then be set by this analytic method. The parameters should be optimal. The relation between GRBF networks and spline interpolation is an ongoing research area <ref> [13] </ref>. Splines are traditionally thought of as weighted polynomials of varying degrees. The weights of these polynomials are set by solving a set of linear equations. A more powerful and general approach is considering splines as a superposition of weighted basis functions. <p> Some numerical tests on how different GRBF networks approximate a set of functions are addressed in a paper by Girosi, Jones and Poggio <ref> [13] </ref>. However, their results are hard to generalize upon. A general and systematic approach to these numerical tests is needed. The tests should be addressed in terms of function spaces, error functionals, and types of discontinuities in the mapping/representation (C o , C 1 , and C 2 ).
Reference: [14] <author> C. K. Chui, </author> <title> An Introduction to Wavelets, </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1992. </year>
Reference-contexts: What we need is a transform technique similar to Fourier transforms except using basis functions that are local in span in both spatial and frequency domains. In addition, these basis functions must either approximate Gaussians or be derived from Gaussians. The Integral Wavelet transform <ref> [14] </ref> allows use to create a variety of basis functions. Basis functions that are local in both frequency and spatial domain can be constructed. These functions can then be shifted and dilated in the spatial domain.
Reference: [15] <author> L. C. Andrews and B. K. Shivamoggi, </author> <title> Integral Transforms for Engineers and Applied Mathematicians, </title> <publisher> Macmillan Publishing Company, </publisher> <address> NY, NY, </address> <year> 1988. </year>
Reference-contexts: The basis functions used in our network are Gaussians. The functions have three parameters, span, center vector, and weight. In addition, the number of basis functions is a parameter. We need to introduce an Integral transform technique <ref> [15] </ref> that allows us to do with Gaussians what Fourier transforms allow us to do with sines and cosines. The problem with Fourier transforms is that the basis functions are global in spatial domain and delta functions in frequency domain.
Reference: [16] <author> M. Unser, A. Aldroubi, and M. Eden, </author> <title> "Polynomial Splines and Wavelets A Signal Processing Perspective," Wavelets ATutorial in Theory and Applications, </title> <editor> C. K. Chui editor, </editor> <booktitle> pp. </booktitle> <pages> 91-122, </pages> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Basis functions that are local in both frequency and spatial domain can be constructed. These functions can then be shifted and dilated in the spatial domain. This corresponds 13 to shifting the centers and varying the spans of the Gaussians in our GRBF network. Unser and Aldroubi <ref> [16] </ref> have presented a framework for representing signals using spline basis functions and wavelets. They also discuss a fast implementation of this technique using filter banks. This technique can be extended to tune Gaussian GRBF networks.
Reference: [17] <author> M. Unser, A. Aldroubi, M. Eden, </author> <title> "B-Spline Signal Processing: Part I-Theory Part, Part II- Efficient Design and Applications," </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> Vol. 41, </volume> <pages> pp. 821-848, </pages> <year> 1993. </year>
Reference-contexts: The weights, spans, and positions of the splines are obtained using the filtering algorithms <ref> [17] </ref> implemented by Unser and Aldroubi. This corresponds directly to the weights, spans, and positions of Gaussian basis functions approximated by high order splines. The issue of dimensionality complicates matters slightly, since the proposed method has not been implemented for more than two dimensions.
Reference: [18] <author> S. Edelman and T. Poggio, </author> <title> "Models of Object Recognition," </title> <booktitle> Current Opinion in Neu robiology, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 270-273, </pages> <year> 1991. </year>
Reference-contexts: For a multidimensional implementation, the basis functions need not be radial since the spans are different in different dimensions. This type of network has many interesting similarities with low-level vision processes in biological systems <ref> [18] </ref> [19] as well as neural processes in the temporal cortex of primates [20] [21]. A vast set of functions can be used as basis functions in regularization networks. Mic-chelli [22] states the criteria that a function must fulfill to be used as a basis function in regularization networks.
Reference: [19] <author> B. M. T. H. Romeny and L. Florack, </author> <title> "A Multiscale Geometric Model of Human Vision," The Perception of Visual Information, </title> <editor> W. R. Hendee and P. N. T. Welles editors, </editor> <booktitle> pp. </booktitle> <pages> 73-114, </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: For a multidimensional implementation, the basis functions need not be radial since the spans are different in different dimensions. This type of network has many interesting similarities with low-level vision processes in biological systems [18] <ref> [19] </ref> as well as neural processes in the temporal cortex of primates [20] [21]. A vast set of functions can be used as basis functions in regularization networks. Mic-chelli [22] states the criteria that a function must fulfill to be used as a basis function in regularization networks.
Reference: [20] <author> T. Poggio, </author> <title> "A Theory of How the Brain Might Work," </title> <booktitle> Cold Spring Harbor Symposia on Quantative Biology, </booktitle> <volume> Vol. LV, </volume> <pages> pp. 899-910, </pages> <year> 1990. </year>
Reference-contexts: For a multidimensional implementation, the basis functions need not be radial since the spans are different in different dimensions. This type of network has many interesting similarities with low-level vision processes in biological systems [18] [19] as well as neural processes in the temporal cortex of primates <ref> [20] </ref> [21]. A vast set of functions can be used as basis functions in regularization networks. Mic-chelli [22] states the criteria that a function must fulfill to be used as a basis function in regularization networks.
Reference: [21] <author> D. I. Perret, E. T. Rolls, and W. Chan, </author> <title> "Visual Neurones Responsive to Faces in the Monkey Temporal Cortex," </title> <journal> Experimental Brain Research, </journal> <volume> Vol. 47, </volume> <pages> pp. 329-342, </pages> <year> 1982. </year>
Reference-contexts: For a multidimensional implementation, the basis functions need not be radial since the spans are different in different dimensions. This type of network has many interesting similarities with low-level vision processes in biological systems [18] [19] as well as neural processes in the temporal cortex of primates [20] <ref> [21] </ref>. A vast set of functions can be used as basis functions in regularization networks. Mic-chelli [22] states the criteria that a function must fulfill to be used as a basis function in regularization networks.
Reference: [22] <author> C. A. Micchelli, </author> <title> "Interpolation of Scattered Data: Distance Matrices and Conditionally Positive Definite Functions," Constructive Approximation, </title> <journal> Vol. </journal> <volume> 2, </volume> <pages> pp. 11-22, </pages> <year> 1986. </year>
Reference-contexts: We formulate F (W; x) as a linear combination of suitable basis functions <ref> [22] </ref>. This formulation can easily be implemented as a network. When these basis functions are radially symmetric, the network implemented is called a Radial Basis Function (RBF) network. The weight parameters, c m , are optimized in RBF networks. <p> This type of network has many interesting similarities with low-level vision processes in biological systems [18] [19] as well as neural processes in the temporal cortex of primates [20] [21]. A vast set of functions can be used as basis functions in regularization networks. Mic-chelli <ref> [22] </ref> states the criteria that a function must fulfill to be used as a basis function in regularization networks. It would be interesting to see how different basis functions perform the input-output mapping on different data sets. <p> The box function does not satisfy Micchelli's <ref> [22] </ref> conditions for basis functions in regularization networks. However, it has been shown that a network using the box function does learn input-output mappings [25]. <p> The tests should be addressed in terms of function spaces, error functionals, and types of discontinuities in the mapping/representation (C o , C 1 , and C 2 ). This type of numerical approach might lead us directly from which of Micchelli's conditions <ref> [22] </ref> a basis function satisfies to how well it reconstructs different representations of data. A possible analytical approach to this problem is using information-based complexity techniques. This basically consists of extending the work of Boult [28] to these regularization networks.
Reference: [23] <author> J. S. Albus, </author> <title> "A new approach to manipulator control: The cerebellar model articulation controller (CMAC)," </title> <journal> Trans. ASME, J. Dynamic Syst. Meas. Contr., </journal> <volume> Vol. 97, </volume> <pages> pp. 220-227, </pages> <year> 1975. </year>
Reference-contexts: in most applications; First order fi-spline: to examine the effect of a non-smooth basis function in regularization networks; Seventh order fi-spline: to examine the accuracy of the approximation of a Gaus sian with high order fi-splines; Box function: to relate regularization networks with the cerebellar model articu lation controller (CMAC) <ref> [23] </ref> and Kanerva's Sparse Distributed Memory (SDM) model [24]. The box function does not satisfy Micchelli's [22] conditions for basis functions in regularization networks. However, it has been shown that a network using the box function does learn input-output mappings [25].
Reference: [24] <author> P. Kanerva, </author> <title> Sparse Distributed Memory, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: the effect of a non-smooth basis function in regularization networks; Seventh order fi-spline: to examine the accuracy of the approximation of a Gaus sian with high order fi-splines; Box function: to relate regularization networks with the cerebellar model articu lation controller (CMAC) [23] and Kanerva's Sparse Distributed Memory (SDM) model <ref> [24] </ref>. The box function does not satisfy Micchelli's [22] conditions for basis functions in regularization networks. However, it has been shown that a network using the box function does learn input-output mappings [25].
Reference: [25] <author> Y. Wong and A. Sideris, </author> <title> "Learning Convergence in the Cerebellar Model Articulation Controller," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Vol. 3, No. 1, </volume> <pages> pp. 115-121, </pages> <year> 1992. </year>
Reference-contexts: The box function does not satisfy Micchelli's [22] conditions for basis functions in regularization networks. However, it has been shown that a network using the box function does learn input-output mappings <ref> [25] </ref>. A technique to combine these basis functions to find the optimally tuned network may be possible. * Representation Issues : The way that input and output values are represented makes a significant difference in the accuracy and size of regularization networks.
Reference: [26] <author> Y. Wong, </author> <title> `How Gaussian Radial Basis Functions Work," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Vol. 1, No. 2, </volume> <pages> pp. </pages> <month> 133-137 </month> <year> 1991. </year>
Reference-contexts: The output of the object module is a vector y o that consists of C o and o . Gaussian GRBF networks have difficulty performing accurate input-output mappings when these mappings are discontinuous <ref> [26] </ref>. It is for this reason that we use sin ( o ) and 6 cos ( o ) as output parameters, instead of directly using o . There is a discontinuity in o between 360 o and 0 o . <p> The more basis functions used, the more accurate the input-output mapping, but the number of mapping computations also increases. It is known that, for a given degree of accuracy, the number of basis functions required depends upon the smoothness of the manifold <ref> [26] </ref>. In our implementation, the GRBF in each object module uses 18 basis functions. We arrived at this number empirically.
Reference: [27] <author> T. E. Boult, </author> <title> "Optimal Algorithms: Tools for Mathematical Modeling," </title> <journal> Journal of Com plexity, </journal> <volume> Vol. 3, </volume> <pages> pp. 183-200, </pages> <year> 1987. </year> <month> 18 </month>
Reference-contexts: Choosing an optimal representation is very similar to the problem addressed by Boult <ref> [27] </ref> in choosing optimal models. He discusses choosing an optimal model set from 15 different model sets based on error functionals for the respective model sets.
Reference: [28] <author> T. E. Boult, </author> <title> "Information Based Complexity in Non-Linear Equations and Computer Vision," </title> <type> Phd. Thesis, </type> <institution> Department of Computer Science, Columbia University. </institution> <month> 19 </month>
Reference-contexts: This type of numerical approach might lead us directly from which of Micchelli's conditions [22] a basis function satisfies to how well it reconstructs different representations of data. A possible analytical approach to this problem is using information-based complexity techniques. This basically consists of extending the work of Boult <ref> [28] </ref> to these regularization networks. He used information based complexity to select what function space is better suited for different types of spline interpolators.
References-found: 28

