URL: http://drl.cs.uiuc.edu/pubs/chen-thesis.ps
Refering-URL: http://drl.cs.uiuc.edu/pubs/chen-thesis.html
Root-URL: http://www.cs.uiuc.edu
Title: AUTOMATIC PARALLEL I/O PERFORMANCE OPTIMIZATION IN PANDA  
Author: BY YING CHEN 
Degree: THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Date: 1995  
Address: 1993 M.S., University of Illinois,  1998 Urbana, Illinois  
Affiliation: B.C.S., University of Minnesota,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Acharya, M. Uysal, R. Bennett, A. Mendelson, M. Beynon, J.K. Hollingsworth, J. Saltz, and A. Sussman. </author> <title> Tuning the performance of I/O intensive parallel applications. </title> <booktitle> In Proceedings of the Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 15-27, </pages> <address> Philadelphia, May 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Jovian does not directly address issues of efficient interconnect utilization and load balancing. Working with real I/O intensive applications inspired the Jovian developers to rethink some of the design decisions made in Jovian, resulting in Jovian-2 <ref> [1] </ref>. Jovian-2 has no collective I/O interface, and uses a simple POSIX lio listio () interface for data accesses. Application I/O requests are assumed to be large and no request coalescing is done.
Reference: [2] <author> A. Alexandrov, M. Ionescu, K. Schauser, and C. Scheiman. LogGP: </author> <title> Incorporating long messages into the LogP model-One step closer towards a realistic model for parallel computation. </title> <booktitle> In Proceedings of the 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1995. </year>
Reference-contexts: T sd rv (n; i; l) is the duration from the time when i sends out a message of length n to the time when l receives n. To model T sd rv , we can borrow the LogGP <ref> [2] </ref> communication model and adapt it to fit our environment. The LogGP communication model is an extension of a widely used communication model for small messages, LogP [28]. LogGP extends LogP to handle both large and small message transfers. <p> We borrow the LogGP concept directly to model T sd rv as shown below: T sd rv (n; i; l) = L + o + (n 1)G + o: (3.15) Here, L; o; and G are LogGP model parameters as defined in <ref> [2] </ref>. <p> Dealing with a small number of parameters can simplify the performance analysis and keep researchers focused on the major performance factors without worrying about unnecessary details. The LogP [28] and LogGP <ref> [2] </ref> models were motivated by the elegance of the BSP model. LogP uses four machine-independent model parameters to predict parallel computation. The model is well suited for small message communications.
Reference: [3] <author> J. M. Anderson, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Processing, </booktitle> <year> 1995. </year>
Reference-contexts: Our model-based approach is similar to the attribute-managed approach; however, the completely different target systems undergoing optimization make it necessary to devise drastically different automatic techniques. Some researchers are also investigating automatic performance optimization using compiler techniques. [10] presented performance optimization for an out-of-core application using compile time analysis. <ref> [3] </ref> presented several techniques for array computations. These compile-time based optimization techniques can be quite effective under certain conditions.
Reference: [4] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <booktitle> Machine Learning, </booktitle> <pages> pages 319-342, </pages> <year> 1988. </year>
Reference-contexts: Thus the given description described by the language presented in Chapter 4 will contain missing information, or inaccurate information if users only provide some guesses. One way to solve this problem is by using a pattern language <ref> [4] </ref> approach which can help identify whether an actual request sequence fits the anticipated request sequence defined by a pattern language. For the requests that match the patterns defined by the language, 134 Panda can still use the selected optimal settings for them; otherwise, Panda can use the defaults.
Reference: [5] <editor> Adaptive Simulated Annealing (ASA). ftp.alumni.caltech.edu: /pub/ingber/ASA-shar, </editor> <year> 1993. </year>
Reference-contexts: On the other hand, if too optimistic values are used, the temperature could decrease at a high speed, causing the annealing to prematurely terminate at a local optimum. The detailed explanations of these parameters and their mathematical development can be found in <ref> [5, 45] </ref>. We focus on Temperature Ratio Scale and Temperature Anneal Scale in this thesis, since our performance study shows that the optimization time is not very sensitive to change in Cost Parameter Scale Ratio settings. <p> The modified parameter settings are listed in Table 5.2 and other parameter settings are listed in <ref> [5] </ref>. The optimizations were carried out on the I/O nodes. Currently, in Panda the automatic optimizations can be done either sequentially on one I/O node or in parallel on all the I/O nodes. <p> We used most of the default ASA parameter settings with a few exceptions. The modified parameter settings are listed in Table 5.2 and other parameter settings are listed in <ref> [5] </ref>. With this set of ASA parameter settings, both search methods selected the same optimal Panda parameter settings. In this benchmark, we let the Panda optimizer select the optimal settings for array disk layouts only, while holding other Panda parameter settings constant.
Reference: [6] <author> F.E. Bassow. </author> <title> IBM AIX Parallel I/O File System: Installation, Administration, and Use. </title> <institution> IBM, Kingston, </institution> <address> N.Y., </address> <month> May </month> <year> 1995. </year> <title> Document Number SH34-6065-00. </title>
Reference-contexts: Blindly applying the settings for one system to another can result in poor I/O performance. File system-dependent parameters Traditional sequential UNIX file systems do not support complicated file layouts; however, some advanced parallel file systems (e.g., PIOFS <ref> [6] </ref>, VESTA [26], Galley [67]) superimpose a 2- or 3-dimensional structure on the byte vectors, to 22 reflect the array distribution. The application programmer can define a particular file layout to store his/her output data. <p> Each PIOFS server had 3 GB local SCSI disk with 8 MB/s bandwidth; hence, total PIOFS capacity is 24 GB. PIOFS distributes files across multiple PIOFS servers <ref> [6] </ref>. Each file consists of a set of cells; each cell is stored on a particular server node, and the default number of cells is the number of PIOFS servers. Cells are striped across the PIOFS servers in a round-robin fashion. <p> We explained the effect of the number of I/O nodes in the previous section. The file layout and file system mode effects are discussed at length in [11] and <ref> [6] </ref>, so we do not discuss them in this thesis. <p> More recent parallel file systems intend to remove such drawbacks by providing more complex file structures to user applications. The VESTA parallel file system [26] and its commercial version, PIOFS <ref> [6] </ref>, impose a two dimensional structure on files. A parallel file can contain a set of subfiles, and a subfile contains multiple "cells", each a linear sequence of bytes. User applications can choose to access different portions of files by specifying different views of the same file.
Reference: [7] <author> J. L. Bell and G. S. Patterson, Jr. </author> <title> Data organization in large numerical computations. </title> <journal> The Journal of Supercomputing, </journal> <volume> 1(1), </volume> <year> 1987. </year>
Reference-contexts: Array chunking has been in use in main memory for decades <ref> [7] </ref> as a means of handling out-of-core computations, and more recently as a way of decomposing arrays for computation on parallel platforms.
Reference: [8] <author> K.P. Bennett, M.C. Ferris, and Y.E. Ioannidis. </author> <title> A genetic algorithm for database query optimization. </title> <type> Technical Report CS-TR-91-1004, </type> <institution> University of Wisconsin at Madison, </institution> <year> 1991. </year> <month> 136 </month>
Reference-contexts: The performance results presented there suggest that with carefully selected simulated annealing parameter settings, the simulated annealing can be effectively used to identify optimal solutions in a relatively short time. <ref> [8] </ref> presented the use of randomized search algorithm, genetic algorithms, to optimize complex database queries.
Reference: [9] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20, </pages> <institution> Mississippi State, MS, </institution> <address> October 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: As pointed out in Chapter 2, the efficient use of communication bandwidth can be extremely important on certain platforms for certain workload. To provide robust performance for a wide range of conditions, careful optimization in communication systems is needed. Jovian <ref> [9] </ref> is a parallel I/O library for SPMD-style applications with a collective I/O interface. It assumes a loosely synchronous computation model where all processors engage in a collective I/O operation and the computation is closely synchronized.
Reference: [10] <author> R. Bordawekar, A. Choudhary, and J. Ramanujam. </author> <title> Automatic optimization of communication in compiling out-of-core stencil codes. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <pages> pages 366-373, </pages> <address> Philadelphia, PA, May 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Our model-based approach is similar to the attribute-managed approach; however, the completely different target systems undergoing optimization make it necessary to devise drastically different automatic techniques. Some researchers are also investigating automatic performance optimization using compiler techniques. <ref> [10] </ref> presented performance optimization for an out-of-core application using compile time analysis. [3] presented several techniques for array computations. These compile-time based optimization techniques can be quite effective under certain conditions.
Reference: [11] <author> Rajesh Bordawekar. </author> <title> Implementation of collective I/O in the Intel Paragon Parallel File System: Initial experiences. </title> <booktitle> In Proceedings of 11th ACM International Conference on Supercomputing, </booktitle> <pages> pages 20-27. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1997. </year>
Reference-contexts: On Intel Parallel File System (PFS), user applications can choose different file modes when accessing PFS files. Different modes are designed for different I/O patterns or applications with special I/O requirements, e.g., shared or independent file accesses by multiple processors. <ref> [11] </ref> discussed at length the significant impact of different file access modes on performance, and we do not discuss them in this thesis. On PFS, the file access mode is one of the Panda parameters to be tuned. <p> We explained the effect of the number of I/O nodes in the previous section. The file layout and file system mode effects are discussed at length in <ref> [11] </ref> and [6], so we do not discuss them in this thesis.
Reference: [12] <author> E. Borowsky, R. Golding, A. Merchant, E. Shriver, M. Spasojevic, and J. Wilkes. </author> <title> Eliminating storage headaches through self-management. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <address> Seattle, WA, </address> <year> 1996. </year>
Reference-contexts: The increasing complexity in storage systems suggests that automatically choosing storage system configurations for target workloads without human intervention is a promising direction. The self-managing and self-configuring storage system project aims at developing techniques to relieve users from storage system management headaches <ref> [12] </ref>. An attribute-managed storage system approach [38] was proposed to automatically adapt the system to dynamically changing 123 workloads and select appropriate storage system configuration for the end user. Detailed storage device models [81] were developed to help make optimization decisions.
Reference: [13] <author> G.E.P. Box, W.G. Hunter, and J.S. Hunter. </author> <title> Statistics for experimenters. </title> <publisher> John Wiley and Sons, </publisher> <year> 1978. </year>
Reference-contexts: Otherwise, this approach can be costly and may not be able to find a set of proper parameter settings. In the future, we plan to investigate the use of a more general experiment design methodology, factorial experiments <ref> [13, 29] </ref>, which requires only a fraction of the total number of different combinations of parameter settings to be tried to select the proper ASA parameter settings in the future. The tuning process is repeated until either the goal is reached or a pre-defined maximum number of iterations is reached.
Reference: [14] <author> M.J. Box, D. Davies, </author> <title> and W.H. Swann. Non-linear optimization techniques. In ICI Monograph No. </title> <type> 5, </type> <institution> Edinburgh, </institution> <address> 1969. </address> <publisher> Oliver & Boyd. </publisher>
Reference-contexts: These algorithms can be effective. However, for a large complex system with nonlinear interdependent parameters, such algorithms are extremely difficult to devise. Numerical algorithms such as Newton's method, steepest descent method <ref> [14] </ref>, and direct search methods [41] are also inappropriate since the Panda objective function is not continuous and differentiable. We have considered several randomized search algorithms, i.e., simulated annealing (SA) [49] and genetic algorithms (GAs) [37, 40].
Reference: [15] <author> P. Brezany, Michael Gernt, Piyush Mehotra, and Hans Zima. </author> <title> Concurrent file operations in High Performance FORTRAN. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 230-237. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: Taken all together, these studies suggest that there is no single caching or prefetching strategy that can work well in general. To be successful, automatic techniques are needed to select proper policies without human intervention. The VIPIOS <ref> [15] </ref> project has designed a parallel I/O system to go with Vienna Fortran, with integrated compiler support for I/O. The runtime system and compiler cooperate to store array data in alternative orders in a file. Flexible data distributions on disk can be chosen by users, as in Panda.
Reference: [16] <author> C. Calvin and L. Colombet. </author> <title> Performance evaluation and modeling of collective communications on Cray-T3D. </title> <journal> Parallel Computing, </journal> <volume> 22(10) </volume> <pages> 1413-1427, </pages> <year> 1996. </year>
Reference-contexts: The widespread use of MPI message passing libraries has motivated researchers to develop analytical performance models for communication network and message passing systems. <ref> [16] </ref> presented a model for the PVM message passing library on the Cray T3D with a focus on collective communications.
Reference: [17] <author> P. Chen and D. Patterson. </author> <title> A new approach to I/O performance evaluation|self-scaling I/O benchmarks, predicted I/O performance. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Santa Clara, CA, </address> <month> May </month> <year> 1993. </year> <note> ACM Press. 137 </note>
Reference-contexts: The performance validation results in [75] show that this approach provides very accurate performance predictions for relatively complex applications on sequential platforms. Several efforts have evaluated I/O performance for different types of workloads on sequential platforms. <ref> [17] </ref> proposed a self-scaling benchmark that uses parameterized workloads for I/O performance evaluation. Their work addresses issues such as scalability of I/O benchmarks to future machines and different workloads. They use a model with five parameters to characterize all workloads, which might not adequately characterize workloads with special characteristics.
Reference: [18] <author> Y. Chen, Y. Cho, S. Kuo, K.E. Seamons, M. Subramaniam, and M. Winslett. </author> <title> Server--directed input and output in Panda: A commodity-parts approach to high-performance I/O. </title> <note> Submitted for publication, </note> <year> 1997. </year>
Reference-contexts: master client that the operation has completed. 11 2.3 Panda performance parameters Although the file system bandwidth utilization is an important performance factor for collective I/O operations, and Panda's SDIO strategy can ensure long, sequential file system accesses that lead to high file system bandwidth utilization, our previous performance studies <ref> [19, 18] </ref> show that often the file system is not the only system bottleneck. <p> Our implementation of the idea at the user level shows that such support can be done without expensive modifications to the underlying file systems and provides ease-of-use and application portability. <ref> [18] </ref> compares SDIO and DDIO in detail and points out several advantages of implementing at the user application level. 6.2 Performance modeling The complex behavior of a parallel I/O system makes empirical performance analysis extremely difficult, yet an abstract characterization is essential to help analyzing system performance and can be used
Reference: [19] <author> Y. Chen, I. Foster, J. Nieplocha, and M. Winslett. </author> <title> Optimizing collective I/O performance on parallel computers: A multisystem study. </title> <booktitle> In Proceedings of 11th ACM International Conference on Supercomputing, </booktitle> <pages> pages 28-35. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1997. </year>
Reference-contexts: master client that the operation has completed. 11 2.3 Panda performance parameters Although the file system bandwidth utilization is an important performance factor for collective I/O operations, and Panda's SDIO strategy can ensure long, sequential file system accesses that lead to high file system bandwidth utilization, our previous performance studies <ref> [19, 18] </ref> show that often the file system is not the only system bottleneck. <p> In these experiments, we hand-selected disk layouts (n fi n), where n is the number of compute nodes used, based on our observations from our previous empirical study <ref> [19] </ref>.
Reference: [20] <author> Y. Chen, M. Winslett, Y. Cho, and S. Kuo. </author> <title> Speeding up automatic parallel I/O performance optimization in Panda. </title> <note> Submitted for publication, </note> <year> 1997. </year>
Reference: [21] <author> Y. Chen, M. Winslett, S. Kuo, Y. Cho, M. Subramaniam, and K.E. Seamons. </author> <title> Performance modeling for the Panda array I/O library. </title> <booktitle> In Proceedings of Supercomputing '96. </booktitle> <publisher> ACM Press and IEEE Computer Society Press, </publisher> <month> November </month> <year> 1996. </year>
Reference-contexts: In this section we show an example of using the performance model to find high-quality I/O plans. More examples can be found in <ref> [21] </ref>. of each I/O node separately. The figure shows the predicted elapsed time of the startup and intermediate phases on each server for the cosmology benchmark experiments. In particular, these experiments write 2048 arrays of size 64 KB or 128 KB using different numbers of clients and servers. <p> The platform specific characteristics can be obtained by the system microbenchmarking approach. Chapter 3 section 3.2.1 describes the types of microbenchmarks built for Panda. 60 The measurements taken in the microbenchmark are used in the Panda performance model <ref> [21] </ref> to predict Panda performance on a specific platform. In Panda, each array belongs to one or more array groups. Most I/O requests are applied to the entire array group instead of to individual arrays, which is convenient for applications that input or output multiple arrays simultaneously. <p> We have built and validated an analytical model for Panda, which predicts the cost of Panda I/O operation accurately under different system conditions <ref> [21] </ref>. However, the cost of predicting the performance of a particular combination of parameter settings ("model evaluation") can be high, resulting in high overall optimization cost.
Reference: [22] <author> Y. Chen, M. Winslett, K.E. Seamons, S. Kuo, Y. Cho, and M. Subramaniam. </author> <title> Scalable message passing in Panda. </title> <booktitle> In Proceedings of the Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 109-121, </pages> <address> Philadelphia, May 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: These considerations give rise to the use of the server-orchestrated strategy whenever appropriate. In Chapter 4, we present a set of rules used to determine proper communication strategies for different situations. A comparative performance study <ref> [22] </ref> between Panda and DRA [64] also provides more evidence on how different communication strategies can impact the I/O performance on different platforms. The array disk layout, disk unit size, and the communication strategy are the most important parameters in Panda. <p> The speed of writes in the cosmology experiments is limited by the speed of the assembly of the disk units, and the speed of writes in the baseline experiments is limited by the speed of the underlying file system. Based on this analysis, <ref> [22] </ref> devised a new technique to reduce disk unit assembly cost and showed how it improved performance. (left) and the baseline benchmark (right). 3.2.3 Choosing high-quality parameter settings using the performance model Identifying potential performance bottlenecks and simplifying performance analysis is important to Panda developers, but a more significant use of
Reference: [23] <author> Y. Cho, M. Winslett, M. Subramaniam, Y. Chen, S. Kuo, and K.E. Seamons. </author> <title> Exploiting local data in parallel array I/O on a practical network of workstations. </title> <booktitle> In Proceedings of the Fifth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1 - 13, </pages> <address> San Jose, CA, </address> <month> November </month> <year> 1997. </year> <note> ACM Press. </note>
Reference-contexts: In this thesis, we discuss how different communication strategies can affect Panda performance for different types of I/O workloads. <ref> [23] </ref> presents an other communication strategy devised for an FDDI-connected HP workstation cluster. <p> In such an architecture, not only the number of I/O nodes impacts the performance; which nodes are selected as I/O nodes is also a vital issue. <ref> [23] </ref> presented an algorithm to select optimal I/O nodes to reduce the communication traffic in the part-time I/O architecture in a workstation cluster environment. The foregoing discussion suggests that the platform and underlying file system must be considered when choosing the number of I/O nodes. <p> The performance validation results presented in section 3.2 also back up this claim. However, when the potential message contention is high, to improve the performance prediction accuracy, ffit should be modeled in more detail. <ref> [23] </ref> presents a Panda model that takes into account of the potential message contention on an an HP-Cluster using an FDDI ring as its interconnection network. <p> For the baseline benchmark, the communication traffic is relatively light, the message contention is low, and hence the overall model is more accurate. To improve the prediction accuracy, we need to model the potential message contention in more details. <ref> [23] </ref> presents such an example in the context of workstation cluster environment with an FDDI-ring interconnect. 3.2.2 Identifying potential performance bottlenecks using the performance model To help Panda developers with performance analysis, we have instrumented the Panda simulator that takes values for the model parameters and outputs the predicted values of <p> Chapter 3 presented an analytical model for Panda and demonstrated how such a model can be used to help with performance analysis in an IBM SP-like execution environment. <ref> [23] </ref> customized the Panda model on an FDDI-connected HP-cluster with an emphasis on the modeling of the communication subsystem in Panda, the ultimate performance bottleneck on the HP-cluster. [61] also presented several analytical models developed to help compare the potential performance gains of different enhancements to DDIO for fine-grained data distributions.
Reference: [24] <author> M. Clement, M. Steed, and P. Crandall. </author> <title> Network performance modeling for PVM clusters. </title> <booktitle> In Proceedings of Supercomputing '96. </booktitle> <publisher> ACM Press and IEEE Computer Society Press, </publisher> <month> November </month> <year> 1996. </year>
Reference-contexts: The study of the model parameters such as the size of the message communicated and the number of processors involved greatly simplifies the performance analysis tasks and helps identify potential performance bottlenecks. <ref> [24] </ref> presents an automated analysis system that uses an application-independent model for predicting the impact of ATM on the execution time of iterative parallel applications.
Reference: [25] <author> M. J. Clement and M. Quinn. </author> <title> Analytical performance prediction on multicomputers. </title> <booktitle> In Proceedings of Supercomputing 1993, </booktitle> <address> Portland, OR, 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The validation of this approach show that such a modeling technique can accurately model storage system behavior, and the model can be used to automate storage system management tasks, such as choosing proper storage system configurations for the target workloads. 6.2.2 Parallel system modeling <ref> [25] </ref> developed an analytical performance model to predict the performance of applications on parallel platforms and help programmers make better use of the power of multicomputers.
Reference: [26] <author> P.F. Corbett and D.G. Feitelson. </author> <title> The Vesta parallel file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(3) </volume> <pages> 225-264, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: A traditional way to achieve scalable performance in a computer system is to develop a system that incorporate different strategies designed specifically for different situations, and then give users control over the choice of proper strategies <ref> [26, 67, 77, 42] </ref>. This approach can be effective when the decision making tasks are simple and the computer system under control is not very complex. <p> Blindly applying the settings for one system to another can result in poor I/O performance. File system-dependent parameters Traditional sequential UNIX file systems do not support complicated file layouts; however, some advanced parallel file systems (e.g., PIOFS [6], VESTA <ref> [26] </ref>, Galley [67]) superimpose a 2- or 3-dimensional structure on the byte vectors, to 22 reflect the array distribution. The application programmer can define a particular file layout to store his/her output data. <p> As discussed in Chapter 2, direct use of such interfaces by scientific programmers typically results in inefficient resource utilization and slow response time. More recent parallel file systems intend to remove such drawbacks by providing more complex file structures to user applications. The VESTA parallel file system <ref> [26] </ref> and its commercial version, PIOFS [6], impose a two dimensional structure on files. A parallel file can contain a set of subfiles, and a subfile contains multiple "cells", each a linear sequence of bytes. <p> To speed up file accesses and provide sufficient flexibility, many state-of-the-art parallel file systems support fairly complex I/O strategies, such as strided access patterns typical of collective I/O [67], new caching and prefetching mechanisms [68, 42], a variety of file access modes [70], and numerous file layouts <ref> [26] </ref>. Such design suggests that performance optimization by hand-tuning can be extremely difficult; without a higher-level parallel I/O facility to help select proper parallel file system configurations for the target workloads and execution environment, system resource utilization can be poor.
Reference: [27] <author> Phyllis E. Crandall, R.A. Aydt, A.A. Chien, and Daniel A. Reed. </author> <title> Input/output characteristics of scalable parallel applications. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, December 1995. </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 138 </pages>
Reference-contexts: We agree with the Jovian developers on the importance of a high-level description of the I/O requests and system configuration. I/O characterization studies <ref> [27, 82] </ref> show that caching and prefetching can make a significant difference in performance. As discussed in Chapter 2, we have not needed special caching or prefetching to obtain high performance for collective I/O, although it may be needed for other types of parallel I/O.
Reference: [28] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. Schauser, and E. Santos. </author> <title> LogP: Towards a realistic model of parallel computations. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: To model T sd rv , we can borrow the LogGP [2] communication model and adapt it to fit our environment. The LogGP communication model is an extension of a widely used communication model for small messages, LogP <ref> [28] </ref>. LogGP extends LogP to handle both large and small message transfers. <p> Dealing with a small number of parameters can simplify the performance analysis and keep researchers focused on the major performance factors without worrying about unnecessary details. The LogP <ref> [28] </ref> and LogGP [2] models were motivated by the elegance of the BSP model. LogP uses four machine-independent model parameters to predict parallel computation. The model is well suited for small message communications.
Reference: [29] <author> O.L. Davies. </author> <title> The design and analysis of industrial experiments. Longman Group Limited, </title> <booktitle> 2nd edition, </booktitle> <year> 1978. </year>
Reference-contexts: Otherwise, this approach can be costly and may not be able to find a set of proper parameter settings. In the future, we plan to investigate the use of a more general experiment design methodology, factorial experiments <ref> [13, 29] </ref>, which requires only a fraction of the total number of different combinations of parameter settings to be tried to select the proper ASA parameter settings in the future. The tuning process is repeated until either the goal is reached or a pre-defined maximum number of iterations is reached.
Reference: [30] <author> Juan Miguel del Rosario, R. Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In Proceedings of the IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <address> Newport Beach, CA, </address> <year> 1993. </year> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference: [31] <author> D. Dewitt, N. Kabra, J. Luo, J.M. Patel, and J.B. Yu. </author> <title> Client-server Paradise. </title> <booktitle> In Proceedings of the 20th VLDB Conference, </booktitle> <address> Santiago, Chile, </address> <year> 1994. </year>
Reference-contexts: The problems of scientific array data access have received little attention from the database community. Exceptions are the Panda project itself, and a small effort within the POSTGRES project to incorporate optimal chunk distributions for read-intensive arrays [76]. In addition, the Paradise project <ref> [31] </ref> incorporates support for chunked 2D arrays, coupled with compression, an option also explored in Panda [79]. The Scalable I/O Initiative (http://www.cacr.caltech.edu/SIO/) is motivated by the need for parallel I/O support on parallel platforms. This project involves several institutions from academia, government, and industry.
Reference: [32] <author> P. Dibble, M. Scott, and C.S. Ellis. </author> <title> Bridge: A high-performance file system for parallel processors. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computer Systems, </booktitle> <pages> pages 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Researchers have been focusing on the following major research issues: the design of the parallel file system interface, the parallel file layouts, and parallel file access strategies. The early designed parallel file systems provide Unix-like interfaces to access data. The Bridge file system <ref> [32] </ref> is one such example; Bridge also supports more complex interfaces for user applications to access local file systems on each I/O node. Intel CFS [70] and its successor PFS also support a linear file structure.
Reference: [33] <author> D.G. Feitelson, P.F. Corbett, and J. Prost. </author> <title> Performance of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 150-158, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: User applications can choose to access different portions of files by specifying different views of the same file. With proper implementation, such support can improve I/O performance significantly as shown in Chapter 2 and <ref> [33] </ref>. However, the design of such interfaces, if placed at the file system level, can be rather inflexible. For instance, using VESTA's 2D file structure, it is difficult to represent irregular data. Such I/O interfaces can also be quite complex, which make them hard for scientists to master.
Reference: [34] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: In Panda, array data can be distributed in memory using HPF's BLOCK, CYCLIC (k), and * directives <ref> [34] </ref> along each dimension; on disk, BLOCK, and * directives are permitted for each dimension of the array. 8 A layout determines the logical neighbors of each compute node.
Reference: [35] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <address> Portland, OR, 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Section 2.5 summarizes these studies. 2.1 Collective I/O Scientific applications on parallel platforms typically operate on multidimensional arrays that are distributed across multiple processors. Previous I/O characterization studies (e.g., <ref> [71, 35] </ref>) have shown that the resulting computations often carry out closely synchronized collective I/O operations, in which many processors work in parallel to accomplish conceptually simple I/O requests, such as reading or writing an array from or to disk.
Reference: [36] <author> G.R. Ganger and Y.N. Patt. </author> <title> The process-flow model: Examining I/O performance from the system's point of view. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Santa Clara, CA, May 1993. </address> <publisher> ACM Press. </publisher>
Reference: [37] <author> D.E. Goldberg. </author> <title> Genetic algorithms in search, optimization and machine learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year> <month> 139 </month>
Reference-contexts: Numerical algorithms such as Newton's method, steepest descent method [14], and direct search methods [41] are also inappropriate since the Panda objective function is not continuous and differentiable. We have considered several randomized search algorithms, i.e., simulated annealing (SA) [49] and genetic algorithms (GAs) <ref> [37, 40] </ref>. We have experimented with both SA and GAs, and we found that SA in general performs better than GAs. In this thesis, we present the use of SA to solve our automatic performance optimization problem. SA is a probabilistic hill-climbing optimization algorithm. <p> Applying other optimization algorithms. Modeling the automatic parameter setting selection problem as a multivariant nonlinear optimization problem allows one to experiment with a number of optimization algorithms. Currently, we have considered three algorithms, the ASA algorithm, genetic algorithms <ref> [37] </ref>, and the exhaustive search method. The ASA algorithm is the most suitable approach. However, this does not mean that SA is the only algorithm that is suitable to solve our problem. Other optimization algorithms, such as a combination of simulated annealing and genetic algorithms, are also very appealing.
Reference: [38] <author> R. Golding, E. Shriver, T. Sullivan, and J. Wilkes. </author> <title> Attribute-managed storage. </title> <booktitle> In Work--shop on Modeling and Specification of I/O, </booktitle> <year> 1995. </year>
Reference-contexts: The increasing complexity in storage systems suggests that automatically choosing storage system configurations for target workloads without human intervention is a promising direction. The self-managing and self-configuring storage system project aims at developing techniques to relieve users from storage system management headaches [12]. An attribute-managed storage system approach <ref> [38] </ref> was proposed to automatically adapt the system to dynamically changing 123 workloads and select appropriate storage system configuration for the end user. Detailed storage device models [81] were developed to help make optimization decisions.
Reference: [39] <author> G. Graefe. </author> <title> Query evaluation techniques for large databases. </title> <journal> Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <year> 1993. </year>
Reference-contexts: Profiling has also been used in many other contexts, such as those in [53]. 122 Despite the lack of automatic optimization work in the parallel I/O world, automatic per-formance optimization is not uncommon in many other research areas, such as database and operating system research. <ref> [39] </ref> provides a fairly complete discussion of many query processing and optimization techniques used for database systems and the design of query optimizers in database systems. [84] compared several optimization algorithms used to optimize large join queries.
Reference: [40] <author> J. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> The University of Michigan Press, </publisher> <address> Ann Arbor, Michigan, </address> <year> 1975. </year>
Reference-contexts: Numerical algorithms such as Newton's method, steepest descent method [14], and direct search methods [41] are also inappropriate since the Panda objective function is not continuous and differentiable. We have considered several randomized search algorithms, i.e., simulated annealing (SA) [49] and genetic algorithms (GAs) <ref> [37, 40] </ref>. We have experimented with both SA and GAs, and we found that SA in general performs better than GAs. In this thesis, we present the use of SA to solve our automatic performance optimization problem. SA is a probabilistic hill-climbing optimization algorithm.
Reference: [41] <author> R. Hooke and T.A. Jeeves. </author> <title> "Direct Search" solution of numerical and statistical problems. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 8 </volume> <pages> 212-229, </pages> <year> 1961. </year>
Reference-contexts: These algorithms can be effective. However, for a large complex system with nonlinear interdependent parameters, such algorithms are extremely difficult to devise. Numerical algorithms such as Newton's method, steepest descent method [14], and direct search methods <ref> [41] </ref> are also inappropriate since the Panda objective function is not continuous and differentiable. We have considered several randomized search algorithms, i.e., simulated annealing (SA) [49] and genetic algorithms (GAs) [37, 40].
Reference: [42] <author> J. Huber, C.L. Elford, D.A. Reed, A.A. Chien, </author> <title> and D.S. Blumenthal. PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, July 1995. </address> <publisher> ACM Press. </publisher>
Reference-contexts: A traditional way to achieve scalable performance in a computer system is to develop a system that incorporate different strategies designed specifically for different situations, and then give users control over the choice of proper strategies <ref> [26, 67, 77, 42] </ref>. This approach can be effective when the decision making tasks are simple and the computer system under control is not very complex. <p> To speed up file accesses and provide sufficient flexibility, many state-of-the-art parallel file systems support fairly complex I/O strategies, such as strided access patterns typical of collective I/O [67], new caching and prefetching mechanisms <ref> [68, 42] </ref>, a variety of file access modes [70], and numerous file layouts [26]. <p> PPFS <ref> [42] </ref>, an application-level parallel I/O library, was also developed to examine techniques to improve data caching and prefetching, and currently possesses an arsenal of different strategies.
Reference: [43] <author> L. Ingber. </author> <title> Statistical mechanics aids to calculating term structure models. </title> <journal> Physics Review, </journal> <volume> A 42(12) </volume> <pages> 7057-7064, </pages> <year> 1990. </year>
Reference-contexts: The ASA method, however, takes into such considerations and generates more efficient annealing schedules for a given problem. The ASA method has been successfully used in many disciplines, such as combat analysis [47], finance <ref> [43] </ref>, 124 and neuroscience [44]. [46] discussed the ASA method and presented the parameters that can be used to tune the performance of ASA for particular problems, together with some guidelines on how to tune the ASA parameters for a particular problem.
Reference: [44] <author> L. Ingber. </author> <title> Statistical mechanics of neocortical interactions: A scaling paradigm applied to electroencephalography. </title> <journal> Physics Review, </journal> <volume> A 44(6) </volume> <pages> 4017-4060, </pages> <year> 1991. </year>
Reference-contexts: The ASA method, however, takes into such considerations and generates more efficient annealing schedules for a given problem. The ASA method has been successfully used in many disciplines, such as combat analysis [47], finance [43], 124 and neuroscience <ref> [44] </ref>. [46] discussed the ASA method and presented the parameters that can be used to tune the performance of ASA for particular problems, together with some guidelines on how to tune the ASA parameters for a particular problem.
Reference: [45] <author> L. Ingber. </author> <title> Simulated annealing: Practice versus theory. </title> <journal> Mathl. Compu. Modelling, </journal> <volume> 18(11) </volume> <pages> 29-57, </pages> <year> 1993. </year>
Reference-contexts: Many early SA algorithms randomly sample an infinite solution space and do not consider the importance and sensitivities of different parameters, hence resulting in long optimization times that may make the approach impractical to use in real systems. The Adaptive Simulated Annealing (ASA) algorithm, developed by Lester Ingber <ref> [45] </ref>, takes into consideration the finite solution space and different sensitivities of the parameters and uses an importance-sampling technique to reduce the annealing time. The ASA algorithm has been embedded in the ASA software package, which We have integrated into Panda. <p> In some SA algorithms, these three parameters are combined into one parameter; however, there 80 are several advantages of separating them as done in ASA. These advantages together with the mathematical explanations can be found in <ref> [45] </ref>. We do not discuss them in this thesis. Temperature Ratio Scale controls the ratio of the initial and final temperatures. For the same initial temperature, the larger the value for Temperature Ratio Scale is, the lower the final temperature must be. <p> On the other hand, if too optimistic values are used, the temperature could decrease at a high speed, causing the annealing to prematurely terminate at a local optimum. The detailed explanations of these parameters and their mathematical development can be found in <ref> [5, 45] </ref>. We focus on Temperature Ratio Scale and Temperature Anneal Scale in this thesis, since our performance study shows that the optimization time is not very sensitive to change in Cost Parameter Scale Ratio settings.
Reference: [46] <author> L. Ingber. </author> <title> Adaptive simulated annealing (ASA): Lessons learned. </title> <journal> Control and Cybernetics, </journal> <volume> 25 </volume> <pages> 33-54, </pages> <year> 1996. </year>
Reference-contexts: The ASA method, however, takes into such considerations and generates more efficient annealing schedules for a given problem. The ASA method has been successfully used in many disciplines, such as combat analysis [47], finance [43], 124 and neuroscience [44]. <ref> [46] </ref> discussed the ASA method and presented the parameters that can be used to tune the performance of ASA for particular problems, together with some guidelines on how to tune the ASA parameters for a particular problem.
Reference: [47] <author> L. Ingber, H. Fujio, and M.F. Wehner. </author> <title> Mathematical comparison of combat computer models to exercise data. </title> <journal> Mathl. Comput. Modelling, </journal> <volume> 15(1) </volume> <pages> 65-90, </pages> <year> 1991. </year>
Reference-contexts: The ASA method, however, takes into such considerations and generates more efficient annealing schedules for a given problem. The ASA method has been successfully used in many disciplines, such as combat analysis <ref> [47] </ref>, finance [43], 124 and neuroscience [44]. [46] discussed the ASA method and presented the parameters that can be used to tune the performance of ASA for particular problems, together with some guidelines on how to tune the ASA parameters for a particular problem.
Reference: [48] <author> Y. Ioannidis and E. Wong. </author> <title> Query optimization by simulated annealing. </title> <booktitle> In Proceedings of the 1987 ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 9-22, </pages> <year> 1987. </year>
Reference-contexts: The performance results showed that the simple iterative improvement is superior to all other methods when the amount of time allowed to perform optimization is small. However, as the optimization time increases, simulated annealing becomes the winner. <ref> [48] </ref> also showed how to use simulated annealing to optimize recursive queries.
Reference: [49] <author> S. Kirkpatrick, C.D. Gelatt Jr., </author> <title> and M.P. Vecchi. Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220(4598) </volume> <pages> 671-680, </pages> <year> 1983. </year> <month> 140 </month>
Reference-contexts: Numerical algorithms such as Newton's method, steepest descent method [14], and direct search methods [41] are also inappropriate since the Panda objective function is not continuous and differentiable. We have considered several randomized search algorithms, i.e., simulated annealing (SA) <ref> [49] </ref> and genetic algorithms (GAs) [37, 40]. We have experimented with both SA and GAs, and we found that SA in general performs better than GAs. In this thesis, we present the use of SA to solve our automatic performance optimization problem. SA is a probabilistic hill-climbing optimization algorithm. <p> There is no support for unanticipated access patterns. On the other hand, Panda's combination of the rule-based and simulated annealing approach is more general. Simulated annealing has been widely used in many different disciplines. <ref> [49] </ref> presented examples of using the SA method to find optimal wiring for computer chips. [59] showed how the SA method can be used to do large-dimensional path integrals in the area of statistical physics. These early simulated annealing methods typically use Boltzmann annealing (BA) which was discussed in [85].
Reference: [50] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(1) </volume> <pages> 41-74, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: Several major collective I/O strategies (a variant of two-phase I/O (TPIO) [61] and disk-directed I/O (DDIO) <ref> [50] </ref> address this problem by ensuring long sequential reads and writes at each I/O node whenever possible. <p> Several research groups have been investigating techniques from many aspects, e.g., programming language support, compiler support, runtime libraries, operating system, parallel file systems, and high-performance communication software. Panda's SDIO is motivated by David Kotz's disk-directed I/O (DDIO) <ref> [50] </ref> which was originally proposed to work at the file system level.
Reference: [51] <author> D. Kotz and C.S. Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):140-145, January and February 1993. </note>
Reference-contexts: ENWRICH [72] was developed to study compute node caching possibilities with a simple UNIX-like I/O interface; the conclusion was that a small amount of cache could significantly improve interconnect and disk utilization with that type of appli 117 cation interface. <ref> [51] </ref> examined a variety of caching strategies on I/O nodes. PPFS [42], an application-level parallel I/O library, was also developed to examine techniques to improve data caching and prefetching, and currently possesses an arsenal of different strategies.
Reference: [52] <author> D. Kotz and N. Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <address> Washington, DC, November 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The resulting computations often carry out collective I/O operations, where all the processors used by an application are closely synchronized and periodically cooperate in conceptually simple I/O operations, such as reading or writing an entire array that is distributed across multiple processors <ref> [71, 52] </ref>. The efficient implementation of such collective requests is a challenging problem: If care is not taken, a conceptually simple request can result in a long and inefficient sequence of low-level operations.
Reference: [53] <author> Masayuki Kuba, Constantine D. Polychronopoulos, and Kyle Gallivan. </author> <title> The synergetic effect of compiler, architecture, and manual optimizations on the performance of CFD on multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Such an approach may adapt to variations of I/O access patterns from run to run better than the profiling based approach. Profiling has also been used in many other contexts, such as those in <ref> [53] </ref>. 122 Despite the lack of automatic optimization work in the parallel I/O world, automatic per-formance optimization is not uncommon in many other research areas, such as database and operating system research. [39] provides a fairly complete discussion of many query processing and optimization techniques used for database systems and the
Reference: [54] <author> S. Kuo, M. Winslett, Y. Chen, Y. Cho, M. Subramaniam, and K. Seamons. </author> <title> Application experience with Panda. </title> <booktitle> In Proceedings of the 8th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1997. </year>
Reference-contexts: Second, a new set of system parameters needs to be identified and included in the system, such as the number of I/O nodes. With these prerequisites, we can apply the same optimization strategies to select optimal parameter values. Data migration has recently received attention in the parallel I/O community <ref> [54] </ref>, since this is the most practical and urgent problem that scientists face on parallel platforms. The current data migration strategies are not only slow, hard-to-use, but also nonportable. Panda researchers have been investigating approaches to solve this problem by working with several real applications. [54] presented several strategies used to <p> in the parallel I/O community <ref> [54] </ref>, since this is the most practical and urgent problem that scientists face on parallel platforms. The current data migration strategies are not only slow, hard-to-use, but also nonportable. Panda researchers have been investigating approaches to solve this problem by working with several real applications. [54] presented several strategies used to migrate data to tertiary storage systems and concluded that multiple data migration strategies can be used and the data migration strategies must be integrated into the parallel I/O system.
Reference: [55] <author> S. Kuo, M. Winslett, Y. Chen, Y. Cho, M. Subramaniam, and K.E. Seamons. </author> <title> Parallel input/output with heterogeneous disks. </title> <booktitle> In Proceedings of the 9th International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 79-90, </pages> <address> Olympia, Washington, July 1997. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: In a homogeneous execution environment, Panda supports a round-robin disk chunk assignment strategy across all I/O nodes by default. In a heterogeneous execution environment, where different I/O nodes have different I/O capabilities, a round-robin chunk assignment may not balance the workload well. <ref> [55] </ref> presented several disk chunk to I/O node assignment strategies in such execution environments. We do not discuss them here, since homogeneous execution environment is the focus of this thesis.
Reference: [56] <author> T.M. Madhyasta, C.L. Elford, and D.A. Reed. </author> <title> Optimizing input/output using adaptive file system policies. </title> <booktitle> In Proceedings of the Fifth NASA Goddard Conference on Mass Storage Systems, </booktitle> <pages> pages II:493-514, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: Dynamically changing I/O patterns can make it difficult to select proper caching and prefetching policies, and this is the current focus of the PPFS project <ref> [56, 57] </ref>. Taken all together, these studies suggest that there is no single caching or prefetching strategy that can work well in general. To be successful, automatic techniques are needed to select proper policies without human intervention. <p> The model is well suited for small message communications. LogGP extends LogP to include modeling of large message communications. 6.3 Automatic performance optimization A recent effort has focused on automatically selecting efficient file system caching and prefetch-ing policies in PPFS using two different I/O access pattern classification approaches. In <ref> [56] </ref> and [57], a trained neural network is used to recognize the application I/O access patterns based on a pre-defined classification of patterns. A Hidden-Markov model is used to detect the application I/O access patterns automatically. <p> In our experience, there are other important parameters for which it is impractical to attempt to predetermine optimal settings for particular situations; the results in section 5 indicate that disk layout is one such parameter. Thus the approaches in <ref> [56, 57] </ref>, as well as our own rule-based approach, need to be combined with other, more general paradigms such as simulated annealing to provide a general, portable, and robust solution. The approaches taken by PPFS and Panda can be mutually beneficial.
Reference: [57] <author> T.M. Madhyasta and D.A. Reed. </author> <title> Intelligent, adaptive file system policy selection. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 172-179. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: Dynamically changing I/O patterns can make it difficult to select proper caching and prefetching policies, and this is the current focus of the PPFS project <ref> [56, 57] </ref>. Taken all together, these studies suggest that there is no single caching or prefetching strategy that can work well in general. To be successful, automatic techniques are needed to select proper policies without human intervention. <p> LogGP extends LogP to include modeling of large message communications. 6.3 Automatic performance optimization A recent effort has focused on automatically selecting efficient file system caching and prefetch-ing policies in PPFS using two different I/O access pattern classification approaches. In [56] and <ref> [57] </ref>, a trained neural network is used to recognize the application I/O access patterns based on a pre-defined classification of patterns. A Hidden-Markov model is used to detect the application I/O access patterns automatically. <p> In our experience, there are other important parameters for which it is impractical to attempt to predetermine optimal settings for particular situations; the results in section 5 indicate that disk layout is one such parameter. Thus the approaches in <ref> [56, 57] </ref>, as well as our own rule-based approach, need to be combined with other, more general paradigms such as simulated annealing to provide a general, portable, and robust solution. The approaches taken by PPFS and Panda can be mutually beneficial.
Reference: [58] <author> D.A. Menasce and L.A. Barroso. </author> <title> A methodology for performance evaluation of parallel applications on multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 180-189, </pages> <year> 1992. </year>
Reference-contexts: The researchers there used a microbenchmarking approach to measure the costs of the core computation modules and then use these measurements as the basic constructs to predict the performance of the overall applications. This is similar to our approach to modeling the com 120 putational cost components in Panda. <ref> [58] </ref> presented an analytic methodology for measuring performance of parallel applications running on shared memory machines. The approach is similar to our phase-based approach in which the overall performance is modeled by seperate models for individual computational phases.
Reference: [59] <author> N. Metropolis, A.W. Rosenblush, M.N. Rosenbluth, A.H. Teller, and E. Teller. </author> <title> Equation of state calculations by fast computing machines. </title> <journal> Journal of Chem. Phys., </journal> <volume> 21(6) </volume> <pages> 1087-1092, </pages> <year> 1953. </year>
Reference-contexts: There is no support for unanticipated access patterns. On the other hand, Panda's combination of the rule-based and simulated annealing approach is more general. Simulated annealing has been widely used in many different disciplines. [49] presented examples of using the SA method to find optimal wiring for computer chips. <ref> [59] </ref> showed how the SA method can be used to do large-dimensional path integrals in the area of statistical physics. These early simulated annealing methods typically use Boltzmann annealing (BA) which was discussed in [85].
Reference: [60] <author> Jason A. Moore, Philip J. Hatcher, and Michael J. Quinn. </author> <title> Efficient data-parallel files via automatic mode detection. </title> <booktitle> In Proceedings of the Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1-14, </pages> <address> Philadelphia, May 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: These compile-time based optimization techniques can be quite effective under certain conditions. However, if the I/O pattern depends on run-time application inputs, the compile-time analysis may not offer much help. <ref> [60] </ref> worked on integrating I/O into languages and showed how such techniques are applied to a virtual-processor-oriented language. Several machine-independent modes are used to support both high performance and generality for common access patterns.
Reference: [61] <author> Jason A. Moore and Michael J. Quinn. </author> <title> Enhancing disk-directed I/O for fine-grained redistribution of file data. </title> <journal> Parallel Computing, </journal> <volume> 23(4), </volume> <year> 1997. </year>
Reference-contexts: Several major collective I/O strategies (a variant of two-phase I/O (TPIO) <ref> [61] </ref> and disk-directed I/O (DDIO) [50] address this problem by ensuring long sequential reads and writes at each I/O node whenever possible. <p> for Panda and demonstrated how such a model can be used to help with performance analysis in an IBM SP-like execution environment. [23] customized the Panda model on an FDDI-connected HP-cluster with an emphasis on the modeling of the communication subsystem in Panda, the ultimate performance bottleneck on the HP-cluster. <ref> [61] </ref> also presented several analytical models developed to help compare the potential performance gains of different enhancements to DDIO for fine-grained data distributions. The models developed there also focused on the communication subsystem. <p> The integrated system can narrow the gap between Panda developers and the application developers so that Panda can provide high performance. To help understand the performance issues with fine-grained redistribution of file data on parallel platforms, <ref> [61] </ref> proposed several enhancements to DDIO. Several analytical models were developed to help compare the potential performance gains of different enhancements under different situations.
Reference: [62] <author> Steven A. Moyer and V. S. Sunderam. </author> <title> PIOUS: a scalable parallel I/O system for distributed computing environments. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 71-78, </pages> <year> 1994. </year>
Reference-contexts: Such design suggests that performance optimization by hand-tuning can be extremely difficult; without a higher-level parallel I/O facility to help select proper parallel file system configurations for the target workloads and execution environment, system resource utilization can be poor. PIOUS <ref> [62] </ref> is a parallel file system architecture for a network computing environment. The focus of the PIOUS project is to provide transaction-based protocols for data access. Under such an approach, I/O operations are enclosed in transactions, which are carried out transparently to users.
Reference: [63] <author> MPI: </author> <title> A message-passing interface standard. Message Passing Interface Forum, </title> <year> 1994. </year>
Reference-contexts: To model the communication costs T send and T sd rv , we examine the communication mechanisms used in Panda in more detail. Panda 2.0.5 uses standard blocking MPI message passing <ref> [63] </ref>. As shown in section 3.1.3, T send (n; i) is used in several formulas. The message lengths n in those occurences of T send (n; i) are typically very small, since those are the schema, requests or quit messages. Such messages are at most several hundred bytes.
Reference: [64] <author> J. Nieplocha and I. Foster. </author> <title> Disk resident arrays: An array-oriented I/O library for out-of-core computations. </title> <booktitle> In Proceedings of Frontiers '96 of Massively Parallel Computing Symposium, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: These considerations give rise to the use of the server-orchestrated strategy whenever appropriate. In Chapter 4, we present a set of rules used to determine proper communication strategies for different situations. A comparative performance study [22] between Panda and DRA <ref> [64] </ref> also provides more evidence on how different communication strategies can impact the I/O performance on different platforms. The array disk layout, disk unit size, and the communication strategy are the most important parameters in Panda. <p> We used a (n fi 1) disk layout (n is the number of I/O nodes used) in these experiments. As shown in the figure, when 8 I/O nodes are used, Panda achieved above 40 MB/s PIOFS bandwidth, which is comparable to the results reported elsewhere <ref> [64, 88] </ref>. <p> We used a (n fi 1) disk layout (n is the number of I/O nodes used) in these experiments. As shown in the figure, when 8 I/O nodes are used, Panda achieved above 40 MB/s PIOFS bandwidth, which is comparable to the results reported elsewhere [64, 88]. In <ref> [64] </ref>, the authors have reported close to 40 MB/s I/O performance when using an array-oriented parallel I/O library, DRA, on PIOFS on the same IBM SP. [88] has reported more than 30 MB/s when using PASSION on PIOFS for an astrophysics application. previously, Panda has chosen PIOFS's dynamic file partitioning to <p> Like Panda, Disk Resident Arrays (DRA) <ref> [64] </ref> is a parallel array-oriented I/O library targeted at scientific applications performing array I/O operations. DRA is built on top of the Global Array library [65], and provides a shared memory programming model, while Panda 115 adopts a distributed program model.
Reference: [65] <author> J. Nieplocha, R.J. Harrison, and R.J. Littlefield. </author> <title> Global arrays: A portable "shared-memory" programming model for distributed memory computers. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 340-349, </pages> <year> 1994. </year>
Reference-contexts: Like Panda, Disk Resident Arrays (DRA) [64] is a parallel array-oriented I/O library targeted at scientific applications performing array I/O operations. DRA is built on top of the Global Array library <ref> [65] </ref>, and provides a shared memory programming model, while Panda 115 adopts a distributed program model. Both Panda and DRA provide an array-oriented high--level interface to user applications. A variation of the server push/pull strategy is used in DRA to form long, sequential requests whenever possible.
Reference: [66] <author> N. Nieuwejaar and D. Kotz. </author> <title> The Galley parallel file system. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <pages> pages 374-381, </pages> <address> Philadelphia, PA, May 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Such I/O interfaces can also be quite complex, which make them hard for scientists to master. We believe that an additional higher level parallel I/O interface, providing simpler and more abstract interfaces to user applications, is necessary to best utilize such file systems. Galley <ref> [66] </ref> is an experimental parallel file system developed at Dartmouth College. It addresses the problem of parallel file system interface by providing a three dimensional file structure. A parallel file contains a set of subfiles.
Reference: [67] <author> N. Nieuwejaar and D. Kotz. </author> <title> The Galley parallel file system. </title> <journal> Parallel Computing, </journal> <volume> 23(4) </volume> <pages> 447-476, </pages> <year> 1997. </year>
Reference-contexts: A traditional way to achieve scalable performance in a computer system is to develop a system that incorporate different strategies designed specifically for different situations, and then give users control over the choice of proper strategies <ref> [26, 67, 77, 42] </ref>. This approach can be effective when the decision making tasks are simple and the computer system under control is not very complex. <p> Blindly applying the settings for one system to another can result in poor I/O performance. File system-dependent parameters Traditional sequential UNIX file systems do not support complicated file layouts; however, some advanced parallel file systems (e.g., PIOFS [6], VESTA [26], Galley <ref> [67] </ref>) superimpose a 2- or 3-dimensional structure on the byte vectors, to 22 reflect the array distribution. The application programmer can define a particular file layout to store his/her output data. <p> To speed up file accesses and provide sufficient flexibility, many state-of-the-art parallel file systems support fairly complex I/O strategies, such as strided access patterns typical of collective I/O <ref> [67] </ref>, new caching and prefetching mechanisms [68, 42], a variety of file access modes [70], and numerous file layouts [26].
Reference: [68] <author> B. Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: For example, a naive implementation of collective I/O will generate read and write requests too small to obtain peak performance from most UNIX implementations. Buffering imperfections typically plague parallel file systems <ref> [68] </ref>. Some systems prefetch the wrong data; others don't prefetch at all. With naive implementations of collective I/O, a write of a partial disk block will often require an expensive read of the block, followed by the requested write. <p> To speed up file accesses and provide sufficient flexibility, many state-of-the-art parallel file systems support fairly complex I/O strategies, such as strided access patterns typical of collective I/O [67], new caching and prefetching mechanisms <ref> [68, 42] </ref>, a variety of file access modes [70], and numerous file layouts [26].
Reference: [69] <author> R. Hugo Patterson, G.A. Gibson, Eka Ginting, Daniel Stodolsky, and Jim Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on 142 Operating Systems Principles, </booktitle> <pages> pages 79-95, </pages> <address> Copper Mountain, CO, </address> <month> December </month> <year> 1995. </year> <note> ACM Press. </note>
Reference-contexts: If a file system were designed to provide effective support for collective I/O operations, an interface that allowed hints about the appropriate prefetching and write-behind strategies could eliminate this problem. Using hints to guide the selection of caching/prefetching policies has been studied in <ref> [69] </ref>. Note that the array disk layout and disk unit size have subtle, yet highly tightly-coupled relationships. For a given I/O request, the cost for gathering/scattering one disk unit on each I/O node depends on the array disk layouts chosen. <p> Their performance results show that genetic algorithms can effectively identify high quality query execution plans, and the selected plans are in general comparable to or better than those generated by other methods. <ref> [69] </ref> presented techniques used to automatically make file caching and prefetching decisions based on the hints of access patterns. Users can specify any of the four predetermined access patterns as a hint through the file system interface.
Reference: [70] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <address> Monterey, CA, March 1989. </address> <publisher> Golden Gate Enterprises, </publisher> <address> Los Altos, CA. </address>
Reference-contexts: The early designed parallel file systems provide Unix-like interfaces to access data. The Bridge file system [32] is one such example; Bridge also supports more complex interfaces for user applications to access local file systems on each I/O node. Intel CFS <ref> [70] </ref> and its successor PFS also support a linear file structure. User applications are provided with several modes to access files, such as using shared or independent file pointers across all compute nodes. The data read/written is striped across multiple I/O servers using a system-wide default striping 113 unit. <p> To speed up file accesses and provide sufficient flexibility, many state-of-the-art parallel file systems support fairly complex I/O strategies, such as strided access patterns typical of collective I/O [67], new caching and prefetching mechanisms [68, 42], a variety of file access modes <ref> [70] </ref>, and numerous file layouts [26]. Such design suggests that performance optimization by hand-tuning can be extremely difficult; without a higher-level parallel I/O facility to help select proper parallel file system configurations for the target workloads and execution environment, system resource utilization can be poor.
Reference: [71] <author> J.T. Poole. </author> <title> Preliminary survey of I/O intensive applications. </title> <type> Technical Report CCSF-38, </type> <institution> Scalable I/O Initiative, Caltech Concurrent Supercomputing Facilities, Caltech, </institution> <year> 1994. </year>
Reference-contexts: The resulting computations often carry out collective I/O operations, where all the processors used by an application are closely synchronized and periodically cooperate in conceptually simple I/O operations, such as reading or writing an entire array that is distributed across multiple processors <ref> [71, 52] </ref>. The efficient implementation of such collective requests is a challenging problem: If care is not taken, a conceptually simple request can result in a long and inefficient sequence of low-level operations. <p> Section 2.5 summarizes these studies. 2.1 Collective I/O Scientific applications on parallel platforms typically operate on multidimensional arrays that are distributed across multiple processors. Previous I/O characterization studies (e.g., <ref> [71, 35] </ref>) have shown that the resulting computations often carry out closely synchronized collective I/O operations, in which many processors work in parallel to accomplish conceptually simple I/O requests, such as reading or writing an array from or to disk.
Reference: [72] <author> A. Purakayastha, C.S. Ellis, and D. Kotz. </author> <title> ENWRICH: a compute-processor write caching scheme for parallel file systems. </title> <booktitle> In Proceedings of the Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 55-68, </pages> <address> Philadelphia, May 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: I/O characterization studies [27, 82] show that caching and prefetching can make a significant difference in performance. As discussed in Chapter 2, we have not needed special caching or prefetching to obtain high performance for collective I/O, although it may be needed for other types of parallel I/O. ENWRICH <ref> [72] </ref> was developed to study compute node caching possibilities with a simple UNIX-like I/O interface; the conclusion was that a small amount of cache could significantly improve interconnect and disk utilization with that type of appli 117 cation interface. [51] examined a variety of caching strategies on I/O nodes.
Reference: [73] <author> D.A. Reed, R.A. Aydt, R.J. Noe, P.C. Roth, K.A. Shields, B. Schwartz, </author> <title> and L.F. Tavera. Scalable performance analysis: The Pablo performance analysis environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference. IEEE Computing Society, </booktitle> <year> 1993. </year>
Reference-contexts: The approach is similar to our phase-based approach in which the overall performance is modeled by seperate models for individual computational phases. The Pablo performance analysis environment <ref> [73] </ref> can be used to study the I/O characteristics of user applications. The integration of this type of user application level performance analysis tool with Panda performance models can greatly help users to understand Panda behavior and also help Panda understand the application's characteristics and requirements.
Reference: [74] <author> E. Rich and K. Knight. </author> <booktitle> Artificial intelligence, chapter 3. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1991. </year>
Reference-contexts: No general techniques were proposed to select proper ASA parameter settings. We devised an adaptive incremental tuning strategy to select proper ASA annealing scales in this thesis. Our constraint-based approach to reducing the parameter solution space has some commonalities as well as some fundamental differences with the constraint satisfaction <ref> [74] </ref> technique. Constraint satisfaction is a search technique that identifies the optimal solutions for a given optimization problem by imposing constraints alone. Identifying optimal solutions is viewed as discovering some point in the solution space that satisfies a given set of constraints.
Reference: [75] <author> R.H. Saavedra-Barrera, A.J. Smith, and E. Miya. </author> <title> Machine characterization based on an abstract high-level language machine. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12), </volume> <year> 1989. </year>
Reference-contexts: We use a platform microbenchmarking approach to extract such information. We have built a platform microbenchmark suite to extract platform specific information for Panda. This approach has also been adopted by others for predicting application performance on both parallel and sequential platforms <ref> [90, 75] </ref>. 50 The Panda platform microbenchmark Our benchmark suite measures characteristics of the file system, message passing system, and memory system. <p> Despite the lack of performance modeling efforts in the parallel I/O community, performance analysis and modeling of computation intensive applications for sequential and parallel platforms has been well studied in the past. We discuss such work below. 6.2.1 Sequential system modeling <ref> [75] </ref> describes an approach to predict the performance of arbitrary Fortran programs by measuring the performance of a given system on a Fortran abstract machine in a microbenchmark suite. The Fortran abstract machine measures the performance of a set of basic Fortran lan 119 guage constructs. <p> The Fortran abstract machine measures the performance of a set of basic Fortran lan 119 guage constructs. This approach is similar to our method of obtaining the values for Panda computational cost components. The performance validation results in <ref> [75] </ref> show that this approach provides very accurate performance predictions for relatively complex applications on sequential platforms. Several efforts have evaluated I/O performance for different types of workloads on sequential platforms. [17] proposed a self-scaling benchmark that uses parameterized workloads for I/O performance evaluation. <p> performance of an parallel application is similar to our approach, however, they emphasized the prediction of the performance of parallel applications, while we focus on both system performance predictions as well as using the model to carry out the performance analysis tasks. [90] extended the abstract machine approach described in <ref> [75] </ref> to parallel machines. The researchers there used a microbenchmarking approach to measure the costs of the core computation modules and then use these measurements as the basic constructs to predict the performance of the overall applications.
Reference: [76] <author> S. Sarawagi and M. Stonebraker. </author> <title> Efficient organization of large multidimensional arrays. </title> <booktitle> In Proceedings of the 10th International Conference on Data Engineering, </booktitle> <pages> pages 328-336, </pages> <year> 1994. </year>
Reference-contexts: The problems of scientific array data access have received little attention from the database community. Exceptions are the Panda project itself, and a small effort within the POSTGRES project to incorporate optimal chunk distributions for read-intensive arrays <ref> [76] </ref>. In addition, the Paradise project [31] incorporates support for chunked 2D arrays, coupled with compression, an option also explored in Panda [79]. The Scalable I/O Initiative (http://www.cacr.caltech.edu/SIO/) is motivated by the need for parallel I/O support on parallel platforms. This project involves several institutions from academia, government, and industry.
Reference: [77] <author> K.E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, December 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The efficient implementation of such collective requests is a challenging problem: If care is not taken, a conceptually simple request can result in a long and inefficient sequence of low-level operations. Panda (URL http://drl.cs.uiuc.edu/panda/) <ref> [80, 77] </ref> is an I/O library motivated by the needs of high-performance SPMD scientific applications that must input and output multidimensional arrays on distributed memory parallel platforms or networks of workstations. Panda supports collective I/O operations and provides application developers with a simple and portable high-level interface for array I/O. <p> A traditional way to achieve scalable performance in a computer system is to develop a system that incorporate different strategies designed specifically for different situations, and then give users control over the choice of proper strategies <ref> [26, 67, 77, 42] </ref>. This approach can be effective when the decision making tasks are simple and the computer system under control is not very complex. <p> Several major collective I/O strategies (a variant of two-phase I/O (TPIO) [61] and disk-directed I/O (DDIO) [50] address this problem by ensuring long sequential reads and writes at each I/O node whenever possible. In Panda, we adopted a server-directed I/O strategy (SDIO) <ref> [77] </ref>. 2.2 Server-directed I/O in Panda Given that we wish the I/O nodes to do long sequential accesses to disk, but a compute node typically needs bits of data that are scattered all over the disk, one approach is to adopt a server push/pull paradigm. <p> We use 1 MB as the disk unit size on each I/O node in our experiments. Values for the computational cost components were obtained using the methods described in section 3.1.4. 51 To validate the model, we predicted Panda 2.0.5's performance on the baseline benchmark <ref> [77] </ref> we used for Panda 2.0 and also on a "cosmology benchmark" that abstracts the I/O behavior of an application that simulates black holes. In both cases, the experiments measure the performance of Panda's high-level array I/O routines that write n arrays.
Reference: [78] <author> K.E. Seamons and M. Winslett. </author> <title> Physical schemas for large multidimensional arrays in scientific computing applications. </title> <booktitle> In Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 218-227, </pages> <month> September </month> <year> 1994. </year> <month> 143 </month>
Reference-contexts: Panda has been used as a testbed to address issues of ease of use, application portability, and above all, high performance collective I/O. For multidimensional arrays, collective I/O performance can often be further improved if array on-disk storage layouts <ref> [78] </ref>, such as chunked array layouts, can be used to store arrays on disk, rather than always using traditional row-major or column-major order storage format.
Reference: [79] <author> K.E. Seamons and M. Winslett. </author> <title> A data management approach for handling large com-pressed arrays in high performance computing. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 119-128, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Exceptions are the Panda project itself, and a small effort within the POSTGRES project to incorporate optimal chunk distributions for read-intensive arrays [76]. In addition, the Paradise project [31] incorporates support for chunked 2D arrays, coupled with compression, an option also explored in Panda <ref> [79] </ref>. The Scalable I/O Initiative (http://www.cacr.caltech.edu/SIO/) is motivated by the need for parallel I/O support on parallel platforms. This project involves several institutions from academia, government, and industry. The Scalable I/O Initiative is an application-driven 118 project.
Reference: [80] <author> Kent E. Seamons and Marianne Winslett. </author> <title> Multidimensional array I/O in Panda 1.0. </title> <journal> Journal of Supercomputing, </journal> <volume> 10(2) </volume> <pages> 191-211, </pages> <year> 1996. </year>
Reference-contexts: The efficient implementation of such collective requests is a challenging problem: If care is not taken, a conceptually simple request can result in a long and inefficient sequence of low-level operations. Panda (URL http://drl.cs.uiuc.edu/panda/) <ref> [80, 77] </ref> is an I/O library motivated by the needs of high-performance SPMD scientific applications that must input and output multidimensional arrays on distributed memory parallel platforms or networks of workstations. Panda supports collective I/O operations and provides application developers with a simple and portable high-level interface for array I/O.
Reference: [81] <author> E. Shriver. </author> <title> Performance Modeling for Realistic Storage Devices. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, </institution> <address> New York University, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: Under their approach, performance on a set of representative workloads is measured, and the performance of an unmeasured workload can be extrapolated from the measurements. Our phase and component based approach is similar to the composite device modeling approach to develop realistic storage device models <ref> [81] </ref>. Under this approach, a complex storage system is divided into multiple system components, e.g., disk caches, controllers, etc. Individual queuing models are developed for individual system components and then aggregated to form the overall storage system model. <p> An attribute-managed storage system approach [38] was proposed to automatically adapt the system to dynamically changing 123 workloads and select appropriate storage system configuration for the end user. Detailed storage device models <ref> [81] </ref> were developed to help make optimization decisions. The dynamic workload assignment problem is formulated into a multiple-knapsack, multiple-constraint optimization problem to allow experimentation with a number of optimization algorithms.
Reference: [82] <author> E. Smirni, R.A. Aydt, A.A. Chien, and D.A. Reed. </author> <title> I/O requirements of scientific applications: An evolutionary view. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 49-59, </pages> <address> Syracuse, NY, 1996. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: We agree with the Jovian developers on the importance of a high-level description of the I/O requests and system configuration. I/O characterization studies <ref> [27, 82] </ref> show that caching and prefetching can make a significant difference in performance. As discussed in Chapter 2, we have not needed special caching or prefetching to obtain high performance for collective I/O, although it may be needed for other types of parallel I/O.
Reference: [83] <author> Mahesh Subramaniam. </author> <title> Efficient implementation of server-directed I/O. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, University of Illinois, </institution> <month> June </month> <year> 1996. </year>
Reference-contexts: The work presented in this thesis uses Panda as the testbed, and focuses on a dedicated I/O node architecture, where each node is either dedicated to application computation or I/O, even though Panda currently also supports several other system architectures as described in <ref> [83] </ref>. The methodology described in this thesis is applicable to 1 other Panda system architectures as well. The dedicated I/O nodes are called "Panda servers" and the compute nodes are called "Panda clients".
Reference: [84] <author> A. Swami and A. Gupta. </author> <title> Optimization of large join queries. </title> <booktitle> In Proceedings of the 1988 ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 8-17, </pages> <address> Chicago, IL, </address> <year> 1988. </year>
Reference-contexts: work in the parallel I/O world, automatic per-formance optimization is not uncommon in many other research areas, such as database and operating system research. [39] provides a fairly complete discussion of many query processing and optimization techniques used for database systems and the design of query optimizers in database systems. <ref> [84] </ref> compared several optimization algorithms used to optimize large join queries. The algorithms studied include an iterative improvement method, simulated annealing, a perturbation walk, and quasi-random sampling.
Reference: [85] <author> H. Szu and R. </author> <title> Hartley. Fast simulated annealing. </title> <journal> Physics Review, A 122(3-4):157-162, </journal> <year> 1987. </year>
Reference-contexts: These early simulated annealing methods typically use Boltzmann annealing (BA) which was discussed in <ref> [85] </ref>. BA samples infinite parameter solution spaces and ignores the fact that in many real-world problems, different parameters have different sensitivities, and hence can be quite inefficient for certain problems. The ASA method, however, takes into such considerations and generates more efficient annealing schedules for a given problem.
Reference: [86] <author> R. Thakur and A. Choudhary. </author> <title> Runtime support for out-of-core parallel programs. </title> <editor> In R. Jain, J. Werth, and J.C. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, volume 362 of The Kluwer International Series in Engineering and Computer Science, chapter 6, </booktitle> <pages> pages 147-165. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Current implementations of PASSION are all on parallel file systems (CFS, PFS, PIOFS). PASSION has also been extended to handle out-of-core I/O requests <ref> [86] </ref>. Like Panda, PASSION supports array-oriented I/O accesses directly. However, no special support in flexible array storage format on disk is considered. Arrays are assumed to be stored in a fixed row-major or column-major format on disk.
Reference: [87] <author> R. Thakur, A. Choudhary, R. Bordawekar, S. More, and S. Kuditipudi. </author> <title> PASSION: Optimized I/O for parallel applications. </title> <journal> IEEE Computer, </journal> <volume> 29(6) </volume> <pages> 70-78, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: A properly selected disk layout can also reduce the amount of extra data read and written for an out-of-core request. This is because Panda handles out-of-core requests via a strategy similar to that used in PASSION <ref> [87] </ref>. Under this strategy, Panda servers always work on one array disk chunk at a time. <p> The addition to DRA of Panda-style facilities to correct load imbalance and choose data buffer size, as well as carefully selected communication mechanisms to improve internal parallelism, would probably lead to high performance under a wider range of system conditions. PASSION <ref> [87] </ref> is a runtime parallel I/O library developed in the Syracuse University. PASSION embodies the "two-phase I/O" (TPIO) strategy for handling collective I/O requests. TPIO performs collective I/O in two phases.
Reference: [88] <author> R. Thakur, E. Lusk, and W. Gropp. </author> <title> I/O characterization of a portable astrophysics application on the IBM SP and Intel Paragon. </title> <type> Technical Report MCS-P534-0895, </type> <institution> Argonne National Laboratory, </institution> <month> August </month> <year> 1995. </year> <note> Revised October 1995. 144 </note>
Reference-contexts: We used a (n fi 1) disk layout (n is the number of I/O nodes used) in these experiments. As shown in the figure, when 8 I/O nodes are used, Panda achieved above 40 MB/s PIOFS bandwidth, which is comparable to the results reported elsewhere <ref> [64, 88] </ref>. <p> In [64], the authors have reported close to 40 MB/s I/O performance when using an array-oriented parallel I/O library, DRA, on PIOFS on the same IBM SP. <ref> [88] </ref> has reported more than 30 MB/s when using PASSION on PIOFS for an astrophysics application. previously, Panda has chosen PIOFS's dynamic file partitioning to store arrays in PIOFS files. 29 Under this strategy, the array disk chunks written by one I/O node are mapped to a subfile stored on one
Reference: [89] <author> L.G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-11, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: This approach can also be used to carry out a cost-benefit analysis that can be used to determine whether an investment in ATM equipment is justified for a particular workstation cluster environment. 121 The bulk-synchronous parallel model (BSP) <ref> [89] </ref> is a well-known model for parallel appli-cations. It models the latency and bandwidth of parallel communication systems and allows modeling of asynchronous processing of multiple processors with only a few system parameters.
Reference: [90] <author> S. J. Worley and A. J. Smith. </author> <title> Microbenchmarking and performance prediction for parallel computers. </title> <type> Technical Report CSD-95-873, </type> <institution> Dept. of Computer Science, University of California at Berkeley, </institution> <year> 1995. </year> <month> 145 </month>
Reference-contexts: We use a platform microbenchmarking approach to extract such information. We have built a platform microbenchmark suite to extract platform specific information for Panda. This approach has also been adopted by others for predicting application performance on both parallel and sequential platforms <ref> [90, 75] </ref>. 50 The Panda platform microbenchmark Our benchmark suite measures characteristics of the file system, message passing system, and memory system. <p> The approach they took to model the performance of an parallel application is similar to our approach, however, they emphasized the prediction of the performance of parallel applications, while we focus on both system performance predictions as well as using the model to carry out the performance analysis tasks. <ref> [90] </ref> extended the abstract machine approach described in [75] to parallel machines. The researchers there used a microbenchmarking approach to measure the costs of the core computation modules and then use these measurements as the basic constructs to predict the performance of the overall applications.
References-found: 90

