URL: http://www.isi.edu/acal/tech-reports/1993/tr-93-10.ps.Z
Refering-URL: http://www.isi.edu/acal/tech-reports/index.html
Root-URL: http://www.isi.edu
Title: Automatic Design of Computer Instruction Sets  
Author: by Bruce Kester Holmer Professor Alvin M. Despain, Co-Chair Professor David E. Culler, Co-Chair Professor Randy H. Katz Professor William H. Miller 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge:  
Date: 1983  1993  
Address: Wisconsin, Madison)  
Note: 1979 Ph.D.  
Affiliation: B.A. (University of Kansas)  (University of  
Abstract-found: 0
Intro-found: 1
Reference: [AAK74] <author> A. M. Abd-Alla and David C. Karlgaard. </author> <title> Heuristic synthesis of mi-croprogrammed computer architecture. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-23(8):802-807, </volume> <month> August </month> <year> 1974. </year>
Reference-contexts: two good starting points for the application of subroutinization are the programs expressed in RTL (register transfer language) or a set of instructions that is minimal yet complete. 2.2.2 Dynamic Microprogramming and Vertical Migration In the 1970's the availability of writable microcode started research into dynamic microprogramming and vertical migration <ref> [AAK74, RA78, SvD78] </ref>. The basic idea is to move often-used functions, program loops, or instruction sequences to writable microcode memory. Each program could then have its own specialized instructions optimized to improve the program's performance.
Reference: [AGH + 84] <author> P. Aigrain, S. L. Graham, R. R. Henry, M. K. McKusick, and E. Pelegri-Llopart. </author> <title> Experience with a Graham-Glanville code generator. </title> <booktitle> In Proceedings of the SIG-PLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 13-24, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Most current techniques used for constructing retargetable code generators can be divided into three types: (1) Graham-Glanville systems which rely on LR parsing of expression trees written in prefix form <ref> [AGH + 84] </ref>, (2) Twig and BURS systems which make use of fast pattern matching of expression trees [AGT89, PLG88], and (3) peephole-based systems which use naive code generators and extensive peephole optimization using symbolic simulation [DF84]. 5 The problem has not been proven NP-complete, but no polynomial algorithm is known.
Reference: [AGT89] <author> Alfred V. Aho, Mahadevan Ganapathi, and Steven W. K. Tjiang. </author> <title> Code generation using tree matching and dynamic programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 491-516, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: When common subexpressions are allowed, then generating optimal code is NP-hard [AJU77]. Although the conditions under which optimality is guaranteed for a linear algorithm are restrictive, a version of the Aho and Johnson algorithm is used for practical code generation <ref> [AGT89] </ref>. One of the assumptions in Aho and Johnson's work was that the instructions were executed sequentially. <p> Most current techniques used for constructing retargetable code generators can be divided into three types: (1) Graham-Glanville systems which rely on LR parsing of expression trees written in prefix form [AGH + 84], (2) Twig and BURS systems which make use of fast pattern matching of expression trees <ref> [AGT89, PLG88] </ref>, and (3) peephole-based systems which use naive code generators and extensive peephole optimization using symbolic simulation [DF84]. 5 The problem has not been proven NP-complete, but no polynomial algorithm is known. 31 Peephole Optimization One of the last stages in the process of compilation of a program into machine
Reference: [AJ76] <author> A. V. Aho and S. C. Johnson. </author> <title> Optimal code generation for expression trees. </title> <journal> Journal of the ACM, </journal> <volume> 23(3) </volume> <pages> 488-501, </pages> <month> July </month> <year> 1976. </year>
Reference-contexts: There are, however, simple cases, which are of practical interest, for which optimal code can be produced. Aho and Johnson showed how to generate optimal code for expression trees (no common subexpressions) on multiregister machines <ref> [AJ76] </ref>. In this case opti 4 Equivalence of simple programs is solvable. For example, simple looping programs with only one level of nesting of loops are solvable.
Reference: [AJU77] <author> A. V. Aho, S. C. Johnson, and J. D. Ullman. </author> <title> Code generation for expressions with common subexpressions. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 146-160, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: But equivalence of programs with loops nested two or more levels are unsolvable [Tsi70]. 30 mal code can be produced in linear time using a dynamic programming algorithm. When common subexpressions are allowed, then generating optimal code is NP-hard <ref> [AJU77] </ref>. Although the conditions under which optimality is guaranteed for a linear algorithm are restrictive, a version of the Aho and Johnson algorithm is used for practical code generation [AGT89]. One of the assumptions in Aho and Johnson's work was that the instructions were executed sequentially.
Reference: [AS93] <author> Peter M. Athanas and Harvey F. Silverman. </author> <title> Processor reconfiguration through instruction-set metamorphosis. </title> <journal> Computer, </journal> <volume> 26(3), </volume> <month> March </month> <year> 1993. </year>
Reference-contexts: Recent developments in vertical migration have been spurred by the emergence of field programmable gate arrays (FPGAs). A recent article outlines the use of FPGAs to implement often used functions (subroutines) <ref> [AS93] </ref>. Functions suitable for migration, however, must have well-defined and limited input and output sizes. 2.2.3 Bose|High Level Language Instruction Set Design A widely held belief during the 1970's was that computer architectures should be customized for high level languages (HLLs). Several studies focused on deriving HLL instruction sets.
Reference: [BCM + 70] <author> G. Bell, R. Cady, H McFarland, B. DeLagi, J. O'Laughlin, R. Noonan, and W. Wulf. </author> <title> A new architecture for mini-computers: The DEC PDP-11. </title> <booktitle> In AFIPS Conference Proceedings, </booktitle> <year> 1970. </year> <note> Reprinted in Computer Structures: Principles and Examples, </note> <editor> Siewiorek, Bell, </editor> <booktitle> and Newell, </booktitle> <pages> pages 649-661. 255 </pages>
Reference-contexts: We have used the conventional approach in this design: first a basic ISP was adopted and then incremental design modifications were made (based on the results of the benchmarks)." Bell, et. al., A New Architecture for Mini-Computers: The DEC PDP-11, AFIPS Conference Proceedings, 1970. <ref> [BCM + 70] </ref> 2.1 A Brief History of Instruction Set Design Not only now, but in the early years of digital computers, computer design was constrained by the cost, reliability, and complexity of computer hardware. <p> since the multiple registers were assigned to specific functions and had inherent idiosyncrasies to score well on the benchmarks; the machine did not perform well for programs other than those used in the benchmark test; and finally, compilers which took advantage of the machine appeared to be difficult to write. <ref> [BCM + 70] </ref> This episode illustrates some potential pitfalls in instruction set design. For the most part they can be avoided by developing the compiler and instruction set simultaneously.
Reference: [BD84] <author> Pradip Bose and Edward S. Davidson. </author> <title> Design of instruction set architectures for support of high-level languages. </title> <booktitle> In 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 198-206, </pages> <year> 1984. </year>
Reference-contexts: Functions suitable for migration, however, must have well-defined and limited input and output sizes. 2.2.3 Bose|High Level Language Instruction Set Design A widely held belief during the 1970's was that computer architectures should be customized for high level languages (HLLs). Several studies focused on deriving HLL instruction sets. Bose <ref> [BD84] </ref> tackled this problem by specifying a large set of transformations that can be used in various combinations to convert a HLL program into an instruction set. The transformations include aspects of compilation, lexical and semantic analysis, along with transformations on program data representation, storage allocation, and instruction format. <p> Symmetric operations are unaffected, but an operation like subtract requires the creation of a new instruction. The code resulting from applying this transformation to the left column of Several major problems have prevented Bose's proposal from becoming a useful tool. The examples and results give in [Bos83] and <ref> [BD84] </ref> were all derived by hand. Providing an interactive or automated tool for applying the transformations was prevented by the difficulty of defining the attribute-grammar formally.
Reference: [Ben86] <author> J. P. Bennett. </author> <title> Automated design of an instruction set for BCPL. </title> <type> Technical Report 93, </type> <institution> University of Cambridge, Computer Laboratory, </institution> <year> 1986. </year>
Reference-contexts: After applying the three transformations to the original Mesa instruction set, the final instruction set gave an overall 12% reduction in code size for the programs studied. 17 2.2.5 Bennett|Automating the Mesa Transformations The Mesa instruction set study was done by hand. Bennett <ref> [Ben86, Ben88] </ref> automated this process and applied it to generate a byte-code instruction set for BCPL. The steps in Bennett's technique are: (1) Design a canonical instruction set. This step is not automated, and the expertise of the designer is required.
Reference: [Ben88] <author> J. P. Bennett. </author> <title> A Methodology for Automated Design of Computer Instruction Sets. </title> <type> PhD thesis, </type> <institution> University of Cambridge, Computer Laboratory, </institution> <year> 1988. </year> <note> Also available as Technical Report 129. </note>
Reference-contexts: After applying the three transformations to the original Mesa instruction set, the final instruction set gave an overall 12% reduction in code size for the programs studied. 17 2.2.5 Bennett|Automating the Mesa Transformations The Mesa instruction set study was done by hand. Bennett <ref> [Ben86, Ben88] </ref> automated this process and applied it to generate a byte-code instruction set for BCPL. The steps in Bennett's technique are: (1) Design a canonical instruction set. This step is not automated, and the expertise of the designer is required. <p> The techniques for generating these instructions are outlined in Table 2.7. Specialization and Generalization. An instruction in the current instruction set can be specialized or generalized to form a new instruction. Sweet [SS82] and Bennett <ref> [Ben88] </ref> use two specific types of specialization: reduction of operand size and making an operand 2 A fine distinction, but necessary for greedy algorithms, which only accept transformations that improve the metric, or for stochastic optimization algorithms, which require well picked transformations to achieve good convergence. 22 A microcoded implementation of <p> This substitution is valid only when the cycle time is not altered by changes in the instruction set. Cycle count was used as the primary metric for vertical migration (Section 2.2.2). * Static code size. This metric is popular when memory is limited <ref> [FSLB77, SS82, Bos83, Ben88] </ref>. Reduction in code size allows more sophisticated programs to fit in a limited amount of memory.
Reference: [BG89] <author> David Bernstein and Izidor Gertner. </author> <title> Scheduling expressions on a pipelined processor with a maximal delay of one cycle. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(1) </volume> <pages> 57-66, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Theoretical work has shown that the scheduling problem (rearrangement of instructions after code generation) can be solved optimally in polynomial time if the pipeline stalls are no more than one cycle <ref> [BG89] </ref>. The theoretic results for code generation tell us that producing optimal code is very difficult in all but the restricted cases.
Reference: [BGvN46] <author> Arthur W. Burks, Herman H. Goldstine, and John von Neumann. </author> <title> Preliminary discussion of the logical design of an electronic computing instrument. </title> <type> Technical report, </type> <institution> for U. S. Army Ordnance Department, </institution> <year> 1946. </year> <title> Reprinted in Bell and Newell, Computer Structures: Readings and Examples, </title> <publisher> McGraw-Hill, </publisher> <year> 1971. </year>
Reference-contexts: The original designers were well aware that instruction selection depends on <ref> [BGvN46, Mau73] </ref>: * the character of the computations, * the frequency of the operations, * the ease of programming operations not implemented in hardware, * the hardware required by the builtin operations, and * the required performance.
Reference: [BJR89] <author> David Bernstein, Jeffrey M. Jaffe, and Michael Rodeh. </author> <title> Scheduling arithmetic and load operations in parallel with no spilling. </title> <journal> SIAM Journal on Computing, </journal> <volume> 18(6) </volume> <pages> 1098-1127, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: One of the assumptions in Aho and Johnson's work was that the instructions were executed sequentially. If one allows parallel execution of instructions, say a load instruction in parallel with an arithmetic instruction, then optimal code generation appears to be even more difficult <ref> [BJR89] </ref>. 5 For pipelined machines, pipeline stalls may be caused by data dependencies between instructions. For example, many present day machines stall the pipeline when a memory load is immediately followed by an instruction that uses the data to be loaded.
Reference: [Bos83] <author> Pradip Bose. </author> <title> Instruction Set Design for Support of High-Level Languages. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1983. </year>
Reference-contexts: These six grammar transformations are a subset of those given in <ref> [Bos83] </ref>. in the grammar. When these lexemes are scanned by the interpreter, their order does not match the order in which their semantic actions are performed. For example, interpreting "3 + 2" requires that the lexeme for 2 be read before the addition is done. <p> Symmetric operations are unaffected, but an operation like subtract requires the creation of a new instruction. The code resulting from applying this transformation to the left column of Several major problems have prevented Bose's proposal from becoming a useful tool. The examples and results give in <ref> [Bos83] </ref> and [BD84] were all derived by hand. Providing an interactive or automated tool for applying the transformations was prevented by the difficulty of defining the attribute-grammar formally. <p> Changes in the instruction set features or definition generally affects many instructions simultaneously. There have been at least two previous studies of techniques in this category <ref> [Han68, Bos83] </ref> that have already been discussed in Sections 2.2.1 and 2.2.3. 2.3.4 Constructive, Feature-Based Techniques This classification encompasses techniques that construct an instruction set from its features but without any evaluation of intermediate results or alternatives. <p> This substitution is valid only when the cycle time is not altered by changes in the instruction set. Cycle count was used as the primary metric for vertical migration (Section 2.2.2). * Static code size. This metric is popular when memory is limited <ref> [FSLB77, SS82, Bos83, Ben88] </ref>. Reduction in code size allows more sophisticated programs to fit in a limited amount of memory. <p> Furthermore, processor chips now include fast instruction cache, and if the cache is small, reducing the number of instruction words fetched will make the cache more effective. * Number of data memory references. Bose used this metric as a measure of execution time <ref> [Bos83] </ref>. Subsequent work [HF89], however, has shown that data traffic is mostly a function of the number of registers provided by the architecture and the register allocation strategy used by the compiler. It has little dependence on the instruction set. * Number of words transferred between memory and processor. <p> It has little dependence on the instruction set. * Number of words transferred between memory and processor. This is the sum of the previous two metrics, and has been used as an implementation independent estimate of execution time <ref> [FSLB77, Bos83] </ref>. 1 There are other metrics that have been suggested which are not listed here (see, for example, Bose [Bos83]). <p> This is the sum of the previous two metrics, and has been used as an implementation independent estimate of execution time [FSLB77, Bos83]. 1 There are other metrics that have been suggested which are not listed here (see, for example, Bose <ref> [Bos83] </ref>). These metrics deal with issues arising in high level language computers which directly interpret the program in hardware, whereas this work always employs a compiler. 45 * Number of words transferred among internal registers. This metric has been used as an implementation independent estimate of execution time [FSLB77]. <p> For stack based instruction sets this relates to the execution stack height <ref> [Bos83] </ref>. For all computers, it represents the approximate amount of dynamic state required to execute the program. It is not clear whether this metric is useful since it is mostly a function of the algorithms used in the program and the compiler optimizations done.
Reference: [CR89] <author> Raul Camposano and Wolfgang Rosenstiel. </author> <title> Synthesizing circuits from behavioral descriptions. </title> <journal> IEEE Transactions on Computer-aided Design of Integrated Circuits and Systems, </journal> <volume> 8(2) </volume> <pages> 171-180, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: 1 There are also important multi-cycle instructions that contain loops, for example, block memory copy, string match, vector operations, and pointer dereferencing (heavily used in Prolog). 9.2.2 Integration with High-Level Synthesis Tools High level synthesis is the generation of a data path and control from an algorithm or instruction set <ref> [RW88, THK + 83, Des88, CR89] </ref>. High level synthesis programs take an algorithm written in a high level language and convert it to a microarchitecture and microcode that satisfies the user's constraints on speed, power, and manufacturing cost.
Reference: [Das79] <author> Subrata Dasgupta. </author> <title> The organization of microprogram stores. </title> <journal> Computing Surveys, </journal> <volume> 11(1) </volume> <pages> 39-65, </pages> <month> March </month> <year> 1979. </year>
Reference-contexts: Reducing the width of the microinstruction can be accomplished by encoding of microinstruction fields or by using the same field for different functions (as long as those functions are never needed in the same microinstruction) <ref> [Das79] </ref>. Drastic reduction in the microinstruction width results in vertical microinstructions in which there is room to encode only one or a few parallel microoperations. The single-cycle constant-width instructions found in recently designed computers (RISCs) have strong similarities with vertical microcode.
Reference: [DDP85] <author> T. P. Dobry, A. M. Despain, and Y. N. Patt. </author> <title> Performance studies of a Prolog machine architecture. </title> <booktitle> In 12th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1985. </year> <month> 256 </month>
Reference-contexts: Kursawe [Kur87] shows how the popular Warren abstract machine (WAM) [War83] can be derived from pure Prolog. Although instruction sets derived from a HLL are primarily used in compilers as an intermediate language, they have also been used in microcoded processors (an example for Prolog is the PLM <ref> [DDP85] </ref>). Bennett's proposed methodology for deriving byte-encoded instruction sets (Section 2.2.5) also begins with a HLL. The final two ways of selecting the initial instruction set are based on the idea of a space (or universe) of instructions.
Reference: [Des88] <author> Alvin M. Despain. </author> <title> The design system (ASP) of the Aquarius project. </title> <booktitle> In 2nd International Workshop on VLSI Design, </booktitle> <month> December </month> <year> 1988. </year>
Reference-contexts: 1 There are also important multi-cycle instructions that contain loops, for example, block memory copy, string match, vector operations, and pointer dereferencing (heavily used in Prolog). 9.2.2 Integration with High-Level Synthesis Tools High level synthesis is the generation of a data path and control from an algorithm or instruction set <ref> [RW88, THK + 83, Des88, CR89] </ref>. High level synthesis programs take an algorithm written in a high level language and convert it to a microarchitecture and microcode that satisfies the user's constraints on speed, power, and manufacturing cost.
Reference: [DF84] <author> Jack W. Davidson and Christopher W. Fraser. </author> <title> Code selection through object code optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(4) </volume> <pages> 505-526, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: Graham-Glanville systems which rely on LR parsing of expression trees written in prefix form [AGH + 84], (2) Twig and BURS systems which make use of fast pattern matching of expression trees [AGT89, PLG88], and (3) peephole-based systems which use naive code generators and extensive peephole optimization using symbolic simulation <ref> [DF84] </ref>. 5 The problem has not been proven NP-complete, but no polynomial algorithm is known. 31 Peephole Optimization One of the last stages in the process of compilation of a program into machine code is peephole optimization. <p> Research on automatically constructed peephole optimizers has shown that they allow the code generator to be simplified without degrading the quality of the final code. Considerable case analysis can be avoided in the code generator because the peephole stage allows the code generator to produce naive code <ref> [DF84] </ref>. Recall that the argument for regularity in instruction sets was based on the fact that regularity simplifies the case analysis in code generation. But the success of automatically generated peephole optimizers allows one to relax the regularity principle as long as some subset of the instruction set is regular.
Reference: [DF87] <author> Jack W. Davidson and Christopher W. Fraser. </author> <title> Automatic inference and fast interpretation of peephole optimization rules. </title> <journal> Software|Practice and Experience, </journal> <volume> 17(11) </volume> <pages> 801-812, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Most of the raw materials for these tools result as a byproduct of the instruction set generation process. For example, David-son and Fraser showed how to automatically derive a fast peephole optimizer from a slow one <ref> [DF87] </ref>. Following their method, the search program used for recompilation can be modified to be a peephole optimizer for the new instruction set. Thus, given the benchmark programs, a search program is used to find optimizations. Each optimization found is then added to a peephole rule database.
Reference: [DFK + 92] <author> William J. Dally, J. A. Stuart Fiske, John S. Keen, Richard A. Lethin, Michael D. Noakes, Peter R. Nuth, Roy E. Davison, and Gregory A. Fyler. </author> <title> The message-driven processor: A multicomputer processing node with efficient mechanisms. </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 23-39, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Effective parallel processing also has needs that may be better supported by new instructions. For example, the J-machine has instructions that provide primitives for communication, synchronization, and naming <ref> [DFK + 92] </ref>. As CAD and compiler tools improve there may come a time when special purpose computers can be designed in such a way that the instruction set need not be known by the application programmer.
Reference: [Dit81] <author> D. R. Ditzel. </author> <title> Reflections on the high-level language SYMBOL computer system. </title> <journal> Computer, </journal> <volume> 14(7) </volume> <pages> 55-66, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: In the late 60's and early 70's high-level instruction sets were seen as a way to reduce rising software costs by reducing the gap between high-level languages and computer instruction sets <ref> [Mye78, Dit81] </ref>. For these computers there was almost a one-to-one mapping between tokens in the source code and machine instructions executed by the computer.
Reference: [Ell86] <author> John R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: For instruction generation, any of the standard microcode compaction techniques can be used: exhaustive (or branch-and-bound) search, list scheduling [LDSM80], trace scheduling <ref> [Fis81, Ell86] </ref>, and percolation scheduling [Nic85a, Nic85b]. Composition results in a single, larger instruction whereas compaction breaks down the block of rearranged microoperations into two or more instructions. It appears that rearrangement of microoperations into several new instructions has not been used before as a tool for instruction set generation.
Reference: [Fis81] <author> Joseph A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(7):478-490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: For instruction generation, any of the standard microcode compaction techniques can be used: exhaustive (or branch-and-bound) search, list scheduling [LDSM80], trace scheduling <ref> [Fis81, Ell86] </ref>, and percolation scheduling [Nic85a, Nic85b]. Composition results in a single, larger instruction whereas compaction breaks down the block of rearranged microoperations into two or more instructions. It appears that rearrangement of microoperations into several new instructions has not been used before as a tool for instruction set generation.
Reference: [Fly83] <author> Michael J. Flynn. </author> <title> Towards better instruction sets. </title> <booktitle> In 16th Annual Workshop on Microprogramming and Microarchitecture (MICRO-16), </booktitle> <pages> pages 3-8, </pages> <year> 1983. </year>
Reference-contexts: Reduction of the serial component increases the predictability of finding the next instruction and allows more overlap in instruction execution. * Compile time. If the instruction set is harder to compile to, then the compile time increases <ref> [Fly83] </ref>. This metric is questionable since the instruction set affects mostly just the code generation which is only part of the entire compilation process. Also, the compilation time is primarily a function of the number and types of optimizations used.
Reference: [FSB77] <author> S. H. Fuller, H. S. Stone, and W. E. Burr. </author> <title> Initial selection and screening of the CFA candidate computer architectures. </title> <booktitle> In AFIPS Conference Proceedings, </booktitle> <volume> volume 46, </volume> <month> June </month> <year> 1977. </year> <title> Abstracted in Computer Structures: Principles and Examples, Siewiorek, </title> <journal> Bell, and Newell, </journal> <pages> pages 49-56. </pages>
Reference-contexts: One also tends to forget about other instruction sequences that occur in programs. The Army-Navy Computer Family Architecture (CFA) Project (1970's) attempted to establish a methodology by which computers could be compared <ref> [FSB77, FSLB77] </ref>. An initial screening phase used absolute criteria representing the minimum state of the art in instruction set design. These criteria included support for virtual memory, memory protection, floating-point, interrupts, traps, and multiprocessing. The benchmarking phase used small kernel-type test programs of not more than 200 machine instructions.
Reference: [FSLB77] <author> S. H. Fuller, P. Shaman, D. Lamb, and W. E. Burr. </author> <title> Evaluation of computer architectures via test programs. </title> <booktitle> In AFIPS Conference Proceedings, </booktitle> <volume> volume 46, </volume> <month> June </month> <year> 1977. </year> <title> Abstracted in Computer Structures: Principles and Examples, Siewiorek, </title> <journal> Bell, and Newell, </journal> <pages> pages 57-60. 257 </pages>
Reference-contexts: One also tends to forget about other instruction sequences that occur in programs. The Army-Navy Computer Family Architecture (CFA) Project (1970's) attempted to establish a methodology by which computers could be compared <ref> [FSB77, FSLB77] </ref>. An initial screening phase used absolute criteria representing the minimum state of the art in instruction set design. These criteria included support for virtual memory, memory protection, floating-point, interrupts, traps, and multiprocessing. The benchmarking phase used small kernel-type test programs of not more than 200 machine instructions. <p> This substitution is valid only when the cycle time is not altered by changes in the instruction set. Cycle count was used as the primary metric for vertical migration (Section 2.2.2). * Static code size. This metric is popular when memory is limited <ref> [FSLB77, SS82, Bos83, Ben88] </ref>. Reduction in code size allows more sophisticated programs to fit in a limited amount of memory. <p> It has little dependence on the instruction set. * Number of words transferred between memory and processor. This is the sum of the previous two metrics, and has been used as an implementation independent estimate of execution time <ref> [FSLB77, Bos83] </ref>. 1 There are other metrics that have been suggested which are not listed here (see, for example, Bose [Bos83]). <p> These metrics deal with issues arising in high level language computers which directly interpret the program in hardware, whereas this work always employs a compiler. 45 * Number of words transferred among internal registers. This metric has been used as an implementation independent estimate of execution time <ref> [FSLB77] </ref>. Because performance heavily depends on the implementation, including the number and types of internal buses, this metric is not a good predictor of execution time. * Average or maximum number of words used to represent the state of the computation.
Reference: [GK92] <author> T. Granlund and R. Kenner. </author> <title> Eliminating branches using a superoptimizer and the GNU C compiler. </title> <booktitle> In SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 341-52, </pages> <year> 1992. </year> <journal> Available as Sigplan Notices, </journal> <volume> volume 27, number 7, </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: The use of a naive code generator and extensive peephole optimization seems to be ideally suited as the way to produce compilers for automatically generated instruction sets. Superoptimizers Exhaustive search for finding optimal code sequences has been used to study the replacement of short conditional branches with straight-line code <ref> [Mas87, GK92] </ref>. This code transformation lengthens the basic blocks and helps avoid pipeline stalls due to branches. Superoptimization begins with a subset of the instruction set, and iterative deepening search is used to generate all possible programs using this subset. <p> Checking for equivalence can be done in at least two ways. The first possibility has been used by "superoptimizers" <ref> [Mas87, GK92] </ref> and works by checking for equivalent final states when the code segment is executed using randomly selected initial data values. If the final states match for all sets of random input data, then it is very likely that the two pieces of code are equivalent.
Reference: [GS87] <author> D. Gifford and A. Spector. </author> <title> Case study: IBM's system 360|370 architecture. </title> <journal> Communications of the ACM, </journal> <volume> 30(4) </volume> <pages> 292-307, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: The benchmarks where selected to approximate the type of programs typically run by the customer. The designers of the IBM System 360 looked at instruction traces from previous architectures in order to determine the relative importance of the various instructions <ref> [GS87] </ref>. However, since so much of the design was changing they could not rely exclusively on these traces. They also hand-coded common instruction sequences (called kernels) that were from 4 to 40 instructions long. These kernels allowed the benefit of an instruction to be measured accurately.
Reference: [Han68] <author> Frederick M. Haney. </author> <title> Using a Computer to Design Computer Instruction Sets. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1968. </year>
Reference-contexts: There is currently no theory of instruction sets, although there have been attempts to construct them [Mau66], and there has also been an attempt to have a computer program design an instruction set <ref> [Han68] </ref>. <p> The software-hardware level will be set solely to optimize the performance within the cost constraints. 2.2 Automating Instruction Set Design 2.2.1 Haney|The Generalized Instruction Set One of the first studies into automatic instruction set design was by Haney <ref> [Han68, Han69] </ref>. He attacked the problem by creating an instruction set design system (ISDS) that is based on a model of computer instruction sets, the generalized instruction set (GIS). Instructions in GIS consist of an operation and one or more features. <p> Some groups are, in effect, smaller than others because 1 This example appears in [Han69] with different values for general registers and extra operations. The numbers given here are based on the detailed results given in <ref> [Han68] </ref>. <p> The numbers given here are based on the detailed results given in [Han68]. The numbers from [Han69] are correct, but are counted twice for extra operations and five times for general registers. 10 Table 2.1: Example Input for ISDS <ref> [Han68, Han69] </ref> * A cost/value matrix: Instruction Feature Cost Value General registers 0 50 Indirect addressing 0 20 Indexing 10 10 Index register update 0 10 Extra operations 0 2 Partial word addressing 0 1 * A cost constraint of 10. * A set of 17 required operations for fixed point, <p> This problem could be avoided if instruction operations are defined as a composition of several fine-grain operations. Then the number of representable instructions can be very large without requiring a large number of primitive operations. Haney <ref> [Han68, page 46] </ref> outlines two additional ways of constructing instruction sets. Both are based on modifying an existing instruction set using a process he calls sub routinization. Subroutinization involves finding frequent compound operations and making 12 them into an instruction, provided the operands fit into the instruction word size. <p> Changes in the instruction set features or definition generally affects many instructions simultaneously. There have been at least two previous studies of techniques in this category <ref> [Han68, Bos83] </ref> that have already been discussed in Sections 2.2.1 and 2.2.3. 2.3.4 Constructive, Feature-Based Techniques This classification encompasses techniques that construct an instruction set from its features but without any evaluation of intermediate results or alternatives.
Reference: [Han69] <author> F. M. Haney. </author> <title> ISDS|a program that designs computer instruction sets. </title> <booktitle> In Fall Joint Computer Conference, </booktitle> <pages> pages 575-580, </pages> <year> 1969. </year>
Reference-contexts: The software-hardware level will be set solely to optimize the performance within the cost constraints. 2.2 Automating Instruction Set Design 2.2.1 Haney|The Generalized Instruction Set One of the first studies into automatic instruction set design was by Haney <ref> [Han68, Han69] </ref>. He attacked the problem by creating an instruction set design system (ISDS) that is based on a model of computer instruction sets, the generalized instruction set (GIS). Instructions in GIS consist of an operation and one or more features. <p> As shown in Table 2.2, instruction features and operations are added, largest valued first, to the partial instruction set. Note that related operations are collected into groups, and operations are added as a group. Some groups are, in effect, smaller than others because 1 This example appears in <ref> [Han69] </ref> with different values for general registers and extra operations. The numbers given here are based on the detailed results given in [Han68]. The numbers from [Han69] are correct, but are counted twice for extra operations and five times for general registers. 10 Table 2.1: Example Input for ISDS [Han68, Han69] <p> Some groups are, in effect, smaller than others because 1 This example appears in <ref> [Han69] </ref> with different values for general registers and extra operations. The numbers given here are based on the detailed results given in [Han68]. The numbers from [Han69] are correct, but are counted twice for extra operations and five times for general registers. 10 Table 2.1: Example Input for ISDS [Han68, Han69] * A cost/value matrix: Instruction Feature Cost Value General registers 0 50 Indirect addressing 0 20 Indexing 10 10 Index register update 0 10 Extra operations <p> The numbers given here are based on the detailed results given in [Han68]. The numbers from [Han69] are correct, but are counted twice for extra operations and five times for general registers. 10 Table 2.1: Example Input for ISDS <ref> [Han68, Han69] </ref> * A cost/value matrix: Instruction Feature Cost Value General registers 0 50 Indirect addressing 0 20 Indexing 10 10 Index register update 0 10 Extra operations 0 2 Partial word addressing 0 1 * A cost constraint of 10. * A set of 17 required operations for fixed point,
Reference: [Hay89] <author> R. Haygood. </author> <title> A Prolog benchmark suite for Aquarius. </title> <type> Technical Report UCB/CSD 89/509, </type> <institution> Computer Science Division, University of California, Berke-ley, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: (f0008ea0),[reg (0),reg (27),reg (28),reg (29), reg (30),reg (31)]), live (l (f0008ea3),[reg (2),reg (24),reg (27),reg (28), reg (29),reg (30),reg (31)]), l (f0008e9f), btgne (3,reg (0),l (f0008ea6)), l (f0008ea0), ldd (reg (0),reg (1)/reg (2)), l (f0008ea2), jmp (l (f000a2b5)), This figure shows a portion of the execution trace for the times10 benchmark <ref> [Hay89] </ref>. Analysis of the trace determines the variables that are live at each of the exit points on the code segment. The symbolic state transitions for the code segment are given in Figure 7.3. <p> This figure shows a code segment which ends with a jump register instruction. This code segment is from the chat_parser benchmark <ref> [Hay89] </ref>. [[cond/eq (reg (0),'20000000'), opc/f000b294, reg (23)/reg (23), reg (28)/reg (28), reg (30)/reg (30)]]. a jump register Instruction This figure shows the symbolic state transition for the code segment given in 115 [live (l (f000b607),[reg (27),reg (28),reg (29),reg (30)]), live (l (f000bd91),[reg (3),reg (27),reg (28),reg (29),reg (30)]), l (f000b606), jmpr (reg <p> This figure gives a code segment that begins with a jump register instruction. This code segment is from the chat_parser benchmark <ref> [Hay89] </ref>. a given jump register instruction can have many destinations during the execution of a program. If the jump register instruction (jmpr) and its delay slot are the last instructions in the code segment, then the jmpr gives rise to a single term in the symbolic state. <p> This figure shows a code segment that uses a branch to a jump because of limitations in the branch instruction (swt). This code segment is from the prover benchmark <ref> [Hay89] </ref>. instruction is a three-way branch based on the tag of a value in a register. The two branch offsets allow only forward branches, but the code in this example needs to branch backwards for one of the destinations. <p> Although the control model supports multi-cycle instructions, the results in this chapter assume single-cycle instructions. 8.2 Benchmarks Four benchmarks are used as input: chat_parser, meta_qsort, prover, and simple_analyzer <ref> [Hay89] </ref>. Basic statistics for these benchmarks are given in Table 8.1. These benchmarks are used because they are the benchmarks used in the instruction set studies for the VLSI-BAM [HSC + 90].
Reference: [HF89] <author> Jerome C. Huck and Michael J. Flynn. </author> <title> Analyzing Computer Architectures. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1989. </year>
Reference-contexts: Furthermore, processor chips now include fast instruction cache, and if the cache is small, reducing the number of instruction words fetched will make the cache more effective. * Number of data memory references. Bose used this metric as a measure of execution time [Bos83]. Subsequent work <ref> [HF89] </ref>, however, has shown that data traffic is mostly a function of the number of registers provided by the architecture and the register allocation strategy used by the compiler. It has little dependence on the instruction set. * Number of words transferred between memory and processor. <p> For example, some algorithms increase memory use to achieve a lower execution time. * Number of instruction objects fetched. This metric is a measure of the number of instruction fragment decodes necessary <ref> [HF89, page 62] </ref>. When given as a ratio to the number of instructions fetched, it is a measure of the amount of serial interpretation built into the instruction set. For example, some instruction set encodings require the interpretation of one field before another field can be found and interpreted.
Reference: [HJG + 82] <author> J. Hennessy, N. Jouppi, J. Gill, F. Baskett, A. Strong, T. Gross, C. Rowen, and J. Leonard. </author> <title> The MIPS machine. </title> <booktitle> In Spring Compcon, </booktitle> <pages> pages 2-7, </pages> <year> 1982. </year>
Reference-contexts: Performance was disappointing since this strict one-to-one correspondence resulted in the absence of instructions that could have been useful in optimizing for common special cases. 8 Beginning in the late 70's, researchers <ref> [Rad82, PS82, HJG + 82] </ref> began to study instruction set design by using an iterative process wherein benchmark programs were compiled for a proposed architecture and simulated. Results from the simulations prompted adjustments to the instruction set, and the process was repeated (see Figure 1.1, page 2).
Reference: [HJP + 83] <author> John Hennessy, Norman Jouppi, Steven Przybylski, Christopher Rowen, and Thomas Gross. </author> <title> Design of a high performance VLSI processor. </title> <booktitle> In Third Caltech Conference on VLSI, </booktitle> <pages> pages 33-54, </pages> <year> 1983. </year>
Reference-contexts: Each instruction, therefore, should be useful to the compiler. In the Stanford MIPS project, the person who proposed a new instruction had to modify the compiler to use the new instruction and collect statistics on performance and instruction use <ref> [HJP + 83] </ref>. <p> Unfortunately, determining the hardware cost of an instruction that does not fit into the model of other instructions in the architecture is at best an ad hoc process." Hennessy, et. al., Design of a High Performance VLSI Processor, Third Caltech Conference on VLSI, 1983. <ref> [HJP + 83] </ref> The success of automating instruction set design depends on finding good metrics for measuring the quality of one instruction set against another.
Reference: [Hol89] <author> Bruce K. Holmer. </author> <title> A detailed description of the VLSI-PLM instruction set: A WAM based processor for Prolog. </title> <type> Technical Report UCB/CSD 90/610, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: ! mem (H) unify variable Y 3 2 tvar@H ! mem (E+disp) H+1 ! H tvar@H ! mem (H) unify void 2 1 H+1 ! H imm ! mem (H) unify constant 1 1 H+1 ! H The RTL in this table is adapted from the pseudo code given in <ref> [Hol89] </ref>. It is assumed that Ai and Xj are in registers, and Yj is in memory and is accessed using an offset from E. This table assumes that the arguments of the instructions are dereferenced, there are no unsafe variables, and the unification flag is set to write-mode.
Reference: [HSC + 90] <author> Bruce K. Holmer, Barton Sano, Michael Carlton, Peter Van Roy, Ralph Hay-good, William R. Bush, Alvin M. Despain, Joan M. Pendleton, and Tep Do-bry. </author> <title> Fast Prolog with an extended general purpose architecture. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 282-291, </pages> <year> 1990. </year> <month> 258 </month>
Reference-contexts: these same instruction sets be the best suited? Instruction sets for each microar-chitecture, simple or complex, can be derived and compared. 4 1.4 Contributions This dissertation grew out of my experience with the design of the VLSI-BAM, a general purpose microprocessor with extensions to support high performance execution of Prolog <ref> [HSC + 90] </ref>. <p> At a later stage in the design of the VLSI-BAM, the compiler was stable enough to compile large programs. This allowed us to determine the performance benefit of individual instructions or sets of instructions dependent on Prolog-specific architectural features <ref> [HSC + 90] </ref>. This dissertation presents the results of this work on the VLSI-BAM and expands upon it. <p> an example application domain. 34 Chapter 3 Optimal Data Structure Creation in Prolog To introduce the new method of instruction set design, an example will be used that occurred during the design of the VLSI-BAM, a general purpose microprocessor with extensions for the support of high performance execution of Prolog <ref> [HSC + 90] </ref>. During the instruction set design of the VLSI-BAM, our research group had proposed several instructions for the support of data structure creation, 1 but had no good way, at the time, to judge between the choices. <p> data structure would be weighted in accordance with its frequency of occurrence during execution 36 of actual Prolog programs, but equal-weighted benchmarks are employed for illustration. 3.2 Model of Data Path The model for the data path that will be used is based on the data path of the VLSI-BAM <ref> [HSC + 90] </ref>. In order to study the effects of the data path on the instruction set, several features of the data path model are parameterized and their values will be varied in the experiments presented below. <p> The derived instruction sets will include arithmetic, branch, and jump instructions. In Chapter 6, a flexible control model is introduced and will allow conditional instructions which can possibly replace instruction idioms containing conditional branches (for example, unify immediate in <ref> [HSC + 90] </ref>). <p> For microprocessor implementations of an instruction set, one can quantify the amount of VLSI area needed to support the microarchitectural features required by an instruction or group of instructions. See [Pen85] and <ref> [HSC + 90] </ref> for the analysis of special language-oriented features added to a VLSI chip to support special instructions. * Power consumption. A computer system's power consumption is primarily a function of the implementation. One of the large power consumers is the memory system. <p> The second cycle can be hidden as long as the instruction following the store does not use the data cache. This cache interface is the one used for the VLSI-BAM <ref> [HSC + 90] </ref>. More aggressive cache interface designs could pipeline cache access and remove the stall cycle between consecutive store and memory operations. <p> var ptr H+1 ! H The RTL in [Mar88] has been modified slightly to reflect the assumptions that Ai and Xj are in registers, and Yj is in memory and is accessed using an offset from E. 82 The selection of the structure creation instructions in the VLSI-BAM instruction set <ref> [HSC + 90] </ref> was influenced by preliminary versions of the results given in this chapter. The VLSI-BAM processor's data path is very similar to the 32121 data path. <p> The other extreme is to decode the opcode at each stage into only those control signals needed by that stage. In the design of the VLSI-BAM <ref> [HSC + 90] </ref> the data stationary control model was used. There is, however, a unique twist to its implementation which allows greater flexibility in pipeline control. <p> The pipelines for the opcodes are staggered so that a row of boxes represents operations that occur at the same time. 92 6.3 Advantages of the Opcode Pipeline Control Model The internal opcode control model is actually used in several special and general purpose microprocessors <ref> [PKB + 86, HSC + 90, Pen90] </ref>. It is easy to implement, uses little chip area, and does not lengthen the processor cycle time. Its main advantage over other control models is that a richer instruction set is possible for very little extra cost. <p> Note that if one uses as input the data path which implements the instruction set used for tracing, then there will most likely be a strong resemblance between the generated instruction set and the original one. 3 See <ref> [HSC + 90] </ref> for a summary of the VLSI-BAM instruction set. 97 [live (l (f0008ea0),[reg (0),reg (27),reg (28),reg (29), reg (30),reg (31)]), live (l (f0008ea3),[reg (2),reg (24),reg (27),reg (28), reg (29),reg (30),reg (31)]), l (f0008e9f), btgne (3,reg (0),l (f0008ea6)), l (f0008ea0), ldd (reg (0),reg (1)/reg (2)), l (f0008ea2), jmp (l (f000a2b5)), <p> The segmentation process described here does not suffer from the same problems that kernels entail (see page 7). Because the 7 Instructions containing tight loops are well known, for example the VLSI-BAM pointer dereference instruction, dref <ref> [HSC + 90] </ref>. <p> The goal is to compare an automatically generated instruction set with the hand-crafted instruction set for the VLSI-BAM processor <ref> [HSC + 90] </ref>. This instruction set is chosen because it has been implemented as a VLSI chip and details of the implementation are available allowing the automatically generated instruction set to be based on the same data path. <p> Basic statistics for these benchmarks are given in Table 8.1. These benchmarks are used because they are the benchmarks used in the instruction set studies for the VLSI-BAM <ref> [HSC + 90] </ref>. The compiler used is the Aquarius Prolog compiler [VR90, VRD92]|a highly optimizing compiler that can automatically determine the data types of many variables in the Prolog code to yield smaller and faster code. The compiler has been targeted for several machines: VLSI-BAM, MIPS, Sparc, and MC68020. <p> Considerable effort is expended to ensure that the result of macro expansion and peephole optimization produces efficient code. In previous work, this method was used to calculate the performance contribution of each specialized VLSI-BAM instruction <ref> [HSC + 90] </ref>. Because the resulting code uses a subset of the VLSI-BAM instruction set, the VLSI-BAM instruction level simulator can be used to measure the performance. <p> This decision to not include pusht in the instruction set agrees with previous 171 VLSI-BAM instruction set studies that showed that pusht contributed less than 1% to the performance of the four benchmarks used <ref> [HSC + 90] </ref>. Arithmetic Instructions The arithmetic instructions are given in Table 8.7. Most of the operations are included because they are required by the completeness code segments. Only addition supports the use of an immediate value as one of the operand sources.
Reference: [Jai91] <author> Raj Jain. </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: Often there is a large number of application programs so one is forced to use a representative subset. Selection of this subset is often done in an ad hoc manner. One systematic technique for chosing a representative set of programs is clustering <ref> [Jai91, pages 83-91] </ref>, but it is seldom used because of its computational expense. 105 Benchmark selection has the potential of skewing the results of the instruction set generation. This can be good or bad depending on the goal.
Reference: [Joh79] <author> Stephen C. Johnson. </author> <title> A 32-bit processor design. </title> <type> Technical Report Computer Science technical report number 80, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1979. </year>
Reference-contexts: The compiler could be modified to make use of more registers and simulations using the proposed architecture could show if more registers could be profitably used. Johnson did such an experiment <ref> [Joh79] </ref> in which he used an iterative technique of proposing a machine, writing a compiler, and measuring the results to propose a better machine. The cycle was repeated several times.
Reference: [Joh91] <author> Mike Johnson. </author> <title> Superscalar Microprocessor Design. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Much of this work builds on established instruction sets. But as technology and programs change there is a periodic need to reevaluate instruction sets. Perhaps there are better instruction sets to support instruction-level parallelism for either superscalar or long-word implementations (see, for example, <ref> [Joh91] </ref>, section 12.1). Effective parallel processing also has needs that may be better supported by new instructions. For example, the J-machine has instructions that provide primitives for communication, synchronization, and naming [DFK + 92].
Reference: [Kan89] <author> Gerry Kane. </author> <title> MIPS RISC architecture. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: In the Stanford MIPS project, the person who proposed a new instruction had to modify the compiler to use the new instruction and collect statistics on performance and instruction use [HJP + 83]. This design approach was later refined by the MIPS company into the 1% rule <ref> [Kan89, page 1.18] </ref>: "any instruction added for performance reasons had to provide a verifiable 1% performance gain over a range of applications or else the instruction was rejected." Research results indicated that instruction sets could be simple and yet provide efficient support for high-level languages.
Reference: [KGV83] <author> S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. </author> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220(4598) </volume> <pages> 671-680, </pages> <month> 13 May </month> <year> 1983. </year>
Reference-contexts: Only occasionally does a segment require explicit compilation using the trial instruction set. Other techniques could be used to decide when to add or delete an instruction. One such possibility would be to use a stochastic method like simulated annealing <ref> [KGV83] </ref>. 149 tag@Ri ! mem (Ri); Rj ! Ri tag@(Ri+disp) ! mem (Ri); Rj ! Ri tag@Ri ! mem (Rj) tag@(Ri+disp) ! mem (Rj) tag@(Ri+disp) ! mem (Rj); Ri ! Rk Q Q Q @ @ This figure shows the same subsumption relations as Figure 7.24.
Reference: [Knu73] <author> Donald E. Knuth. </author> <title> The Art of Computer Programming, Sorting and Searching, volume 3. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Because the list of lifetime intervals is very long and cannot be sorted in core, an external sort is necessary. The UNIX sort utility could not be used because its intermediate files are not compressed, and so an external sort routine was written that is based on replacement selection <ref> [Knu73, pages 254-256] </ref>. The external files are compressed using squash and compress, and only one merge of the external files is required because the input data are nearly sorted. The sorted birth and death data must then be transformed to associate register liveness with instruction addresses.
Reference: [Kog77] <author> Peter M. Kogge. </author> <title> The microprogramming of pipelined processors. </title> <booktitle> In 4th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 63-69, </pages> <year> 1977. </year>
Reference-contexts: so this chapter presents a control model which provides a flexible basis for inventing new instructions, yet is constrained enough to prevent these instructions from lengthening the cycle time. 6.1 General Models There are two basic ways of controlling a pipelined data path: time stationary control and data stationary control <ref> [Kog77, Kog81] </ref>. These two extremes of control structure are shown schematically in Figure 6.1. In the center of the diagram is a simple pipelined data path showing pipeline latches between four stages: register read, arithmetic operation, memory operation, and register write. <p> The decision between using time stationary or data stationary control must be made sometime during the design process. This decision is based on tradeoffs in design flexibility, hardware complexity, and layout. The designer's familiarity with previous designs can also be an important consideration. Kogge <ref> [Kog77] </ref> points out that time stationary control has typically been used for conventional microprogrammed processors (today these 84 The register files at the beginning and end of the pipeline are the same register file, and it is duplicated to make the diagram more readable.
Reference: [Kog81] <author> Peter M. Kogge. </author> <title> The Architecture of Pipelined Computers. </title> <publisher> McGraw-Hill Book Company, </publisher> <year> 1981. </year>
Reference-contexts: The single-cycle constant-width instructions found in recently designed computers (RISCs) have strong similarities with vertical microcode. There are, however, several differences between vertical microinstructions and RISC instructions: the RISC instructions must provide for state restoration after virtual memory faults, and the RISC implementations are implemented with data-stationary control <ref> [Kog81] </ref>. 6 Because of the similarity of fixed-length instructions and vertical microinstructions, the instruction set design techniques presented in this dissertation are applicable to the design of vertical microinstruction formats. <p> so this chapter presents a control model which provides a flexible basis for inventing new instructions, yet is constrained enough to prevent these instructions from lengthening the cycle time. 6.1 General Models There are two basic ways of controlling a pipelined data path: time stationary control and data stationary control <ref> [Kog77, Kog81] </ref>. These two extremes of control structure are shown schematically in Figure 6.1. In the center of the diagram is a simple pipelined data path showing pipeline latches between four stages: register read, arithmetic operation, memory operation, and register write.
Reference: [Kur87] <author> P. Kursawe. </author> <title> How to invent a Prolog machine. </title> <journal> New Generation Computing, </journal> <volume> 5, </volume> <year> 1987. </year>
Reference-contexts: Kursawe <ref> [Kur87] </ref> shows how the popular Warren abstract machine (WAM) [War83] can be derived from pure Prolog. Although instruction sets derived from a HLL are primarily used in compilers as an intermediate language, they have also been used in microcoded processors (an example for Prolog is the PLM [DDP85]). <p> Use of traps for the uncommon case is a valid design alternative, but will not be considered in this dissertation. Specialization based on compile-time knowledge of operand types has been applied to Prolog unification by Kursawe <ref> [Kur87] </ref> and Van Roy [VR89]. Kursawe assumed only trivial type analysis and obtained a WAM-like instruction set. Van Roy assumed extensive type analysis and obtained an instruction set with finer granularity. Decomposition. An instruction in the current instruction set can be decomposed into two or more new instructions.
Reference: [Lar93] <author> James R. Larus. </author> <title> Efficient program tracing. </title> <journal> Computer, </journal> <volume> 26(5) </volume> <pages> 52-61, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Software tracing can be done in many ways, but the most popular two alternatives are: (1) trapping after each instruction and recording information about the instruction address, and (2) rewriting the object code to install extra code to produce the trace (one such system is QPT <ref> [Lar93] </ref>). Hardware monitors use either built-in logic for recording execution events or use probes to observe behavior on a bus. Simulation is a convenient way to gather information, especially for a processor that has not been physically built.
Reference: [LDSM80] <author> David Landskov, Scott Davidson, Bruce Shriver, and Patrick W. Mallett. </author> <title> Local microcode compaction techniques. </title> <journal> Computing Surveys, </journal> <volume> 12(3) </volume> <pages> 261-294, </pages> <month> Septem-ber </month> <year> 1980. </year>
Reference-contexts: For instruction generation, any of the standard microcode compaction techniques can be used: exhaustive (or branch-and-bound) search, list scheduling <ref> [LDSM80] </ref>, trace scheduling [Fis81, Ell86], and percolation scheduling [Nic85a, Nic85b]. Composition results in a single, larger instruction whereas compaction breaks down the block of rearranged microoperations into two or more instructions.
Reference: [Lee89] <author> Ruby B. Lee. </author> <title> Precision architecture. </title> <journal> Computer, </journal> <volume> 22(1) </volume> <pages> 78-91, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Usually annulling is applied to branch delay slot instructions, but the mechanism can be used by any instruction. An example of a commercial machine that uses annulling with instructions other than branches is the HP Precision Architecture <ref> [Lee89] </ref>. All arithmetic instructions in the HP-PA have the option of conditionally annulling the following instruction. This allows short if-then-else code to be compiled without branch instructions. <p> These do not show up in the final instruction set because they are not generally useful enough for the benchmark programs. This is not to say that combination of annulling with arithmetic and memory instructions is not useful. For example, the HP Precision Architecture (HP-PA) <ref> [Lee89] </ref> incorporates annulling with all arithmetic instructions. This 170 type of annulling is different from the annulling used here.
Reference: [Lun77] <author> A. Lunde. </author> <title> Empirical evaluation of some features of instruction set processor architectures. </title> <journal> Communications of the ACM, </journal> <volume> 20(3) </volume> <pages> 143-153, </pages> <month> March </month> <year> 1977. </year>
Reference-contexts: In addition, my technique makes use of decomposition and compaction (Section 2.3.1) to allow new, but simple, instructions to be generated. 2.4.2 Empirical Studies of Instruction Set Use There have been many empirical studies of instruction set usage. Two of the most important early studies where done by Lunde <ref> [Lun77] </ref> and Shustek [Shu78]. Both involved tracing a large number of programs written in a variety of languages (including assembly). Instruction opcode, instruction addressing mode, and register usage statistics were collected. In addition, typical sequences of code were found.
Reference: [Mar88] <author> Andre Marien. </author> <title> An optimal intermediate code for structure creation in a WAM-based Prolog implementation. </title> <booktitle> In Proceedings of the International Computer Science Conference '88, </booktitle> <pages> pages 229-236, </pages> <month> December </month> <year> 1988. </year> <month> 259 </month>
Reference-contexts: There are deficiencies in the PLM's (WAM's) support of structure creation and several researchers have proposed remedies to these problems. Marien <ref> [Mar88, MD91] </ref> proposed an abstract instruction set for Prolog structure creation which is given in Table 5.8. Close inspection shows that much of the RTL is the same as that given in Table 5.7 for the PLM, but there are some major differences. <p> The push double register/tagged address (7984682) and push double with displacement (1873476) instructions allow optimizations which combine two consecutive push instructions into a single-cycle instruction. 81 Table 5.8: Abstract Instruction Set for Structure Creation <ref> [Mar88] </ref> Ri ! Rk put xval mem (E+disp) ! Rk put yval tvar@H ! mem (H) put xvar tvar@H ! Rk tvar@H ! Rl H+1 ! H tvar@H ! mem (H) put yvar tvar@H ! mem (E+disp) tvar@H ! Rk H+1 ! H tlst@H ! Rk put heap list ptr tstr@H <p> ! mem (H) h push cte/h push functor H+1 ! H tlst@(H+disp) ! mem (H) h push heap list ptr H+1 ! H tstr@(H+disp) ! mem (H) h push heap struct ptr H+1 ! H tvar@(H+disp) ! mem (H) h push heap var ptr H+1 ! H The RTL in <ref> [Mar88] </ref> has been modified slightly to reflect the assumptions that Ai and Xj are in registers, and Yj is in memory and is accessed using an offset from E. 82 The selection of the structure creation instructions in the VLSI-BAM instruction set [HSC + 90] was influenced by preliminary versions of
Reference: [Mas87] <author> Henry Massalin. </author> <title> Superoptimizer|a look at the smallest program. </title> <booktitle> In Second Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS II), </booktitle> <pages> pages 122-126, </pages> <year> 1987. </year>
Reference-contexts: The use of a naive code generator and extensive peephole optimization seems to be ideally suited as the way to produce compilers for automatically generated instruction sets. Superoptimizers Exhaustive search for finding optimal code sequences has been used to study the replacement of short conditional branches with straight-line code <ref> [Mas87, GK92] </ref>. This code transformation lengthens the basic blocks and helps avoid pipeline stalls due to branches. Superoptimization begins with a subset of the instruction set, and iterative deepening search is used to generate all possible programs using this subset. <p> Checking for equivalence can be done in at least two ways. The first possibility has been used by "superoptimizers" <ref> [Mas87, GK92] </ref> and works by checking for equivalent final states when the code segment is executed using randomly selected initial data values. If the final states match for all sets of random input data, then it is very likely that the two pieces of code are equivalent. <p> The search process used to construct new code sequences is not complete in the sense that it can find every possible equivalent code sequence. 11 The search does not consider the possibility of using unusual sequences of (apparently) unrelated arithmetic operations. For example, in the original superoptimizer paper <ref> [Mas87] </ref> the sign and maximum functions can be calculated using straight-line code consisting of arithmetic operations that manipulate the carry bit.
Reference: [Mau66] <author> Ward Douglas Maurer. </author> <title> A theory of computer instructions. </title> <journal> Journal of the ACM, </journal> <volume> 13(2) </volume> <pages> 226-235, </pages> <month> April </month> <year> 1966. </year>
Reference-contexts: There is currently no theory of instruction sets, although there have been attempts to construct them <ref> [Mau66] </ref>, and there has also been an attempt to have a computer program design an instruction set [Han68].
Reference: [Mau73] <author> J. W. Mauchly. </author> <title> Preparation of problems for edvac-type machines. In The Origins of Digital Computers, </title> <booktitle> Selected Papers, </booktitle> <pages> pages 365-369. </pages> <year> 1973. </year>
Reference-contexts: The original designers were well aware that instruction selection depends on <ref> [BGvN46, Mau73] </ref>: * the character of the computations, * the frequency of the operations, * the ease of programming operations not implemented in hardware, * the hardware required by the builtin operations, and * the required performance.
Reference: [MD91] <author> Andre Marien and Bart Demoen. </author> <title> A new scheme for unification in the WAM. </title> <booktitle> In Proceedings of the ILPS, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: There are deficiencies in the PLM's (WAM's) support of structure creation and several researchers have proposed remedies to these problems. Marien <ref> [Mar88, MD91] </ref> proposed an abstract instruction set for Prolog structure creation which is given in Table 5.8. Close inspection shows that much of the RTL is the same as that given in Table 5.7 for the PLM, but there are some major differences.
Reference: [Mye78] <editor> Glenford J. Myers. </editor> <booktitle> Advances in Computer Architecture. </booktitle> <publisher> John Wiley & Sons, </publisher> <year> 1978. </year>
Reference-contexts: In the late 60's and early 70's high-level instruction sets were seen as a way to reduce rising software costs by reducing the gap between high-level languages and computer instruction sets <ref> [Mye78, Dit81] </ref>. For these computers there was almost a one-to-one mapping between tokens in the source code and machine instructions executed by the computer.
Reference: [Nic85a] <author> Alexandru Nicolau. </author> <title> Percolation scheduling: A parallel compilation technique. </title> <type> Technical Report TR 85-678, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1985. </year>
Reference-contexts: For instruction generation, any of the standard microcode compaction techniques can be used: exhaustive (or branch-and-bound) search, list scheduling [LDSM80], trace scheduling [Fis81, Ell86], and percolation scheduling <ref> [Nic85a, Nic85b] </ref>. Composition results in a single, larger instruction whereas compaction breaks down the block of rearranged microoperations into two or more instructions. It appears that rearrangement of microoperations into several new instructions has not been used before as a tool for instruction set generation.
Reference: [Nic85b] <author> Alexandru Nicolau. </author> <title> Uniform parallelism exploitation in ordinary programs. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 614-618, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: For instruction generation, any of the standard microcode compaction techniques can be used: exhaustive (or branch-and-bound) search, list scheduling [LDSM80], trace scheduling [Fis81, Ell86], and percolation scheduling <ref> [Nic85a, Nic85b] </ref>. Composition results in a single, larger instruction whereas compaction breaks down the block of rearranged microoperations into two or more instructions. It appears that rearrangement of microoperations into several new instructions has not been used before as a tool for instruction set generation.
Reference: [Ope91] <author> Open Software Foundation. </author> <title> OSF architecture-neutral distribution format rationale. </title> <type> Technical Report OSF-ANDF-RD-1, </type> <institution> Open Software Foundation, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: Better instruction sets may be passed up because of the need to maintain compatibility with previous computers and software. There have been efforts to diminish the need for instruction set compatibility. One current 47 effort is ANDF (Architecture Neutral Distribution Format) <ref> [Ope91] </ref>. This distribution format is a type of compiler intermediate language that can be translated into the appropriate machine code. Because the intermediate language is distributed, the same program can hypothetically run on any computer system for which a translator has been written.
Reference: [Pen85] <author> J. M. Pendleton. </author> <title> A Design Methodology for VLSI Processors. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <month> November </month> <year> 1985. </year>
Reference-contexts: For microprocessor implementations of an instruction set, one can quantify the amount of VLSI area needed to support the microarchitectural features required by an instruction or group of instructions. See <ref> [Pen85] </ref> and [HSC + 90] for the analysis of special language-oriented features added to a VLSI chip to support special instructions. * Power consumption. A computer system's power consumption is primarily a function of the implementation. One of the large power consumers is the memory system. <p> Internal opcodes allow easy implementation of multicycle instructions or instructions which dynamically change as they execute (conditional execution instructions). The internal opcode control model was first described by Pendleton <ref> [Pen85, PKB + 86] </ref>. The internal opcode control model is illustrated in Figure 6.2. The process of injecting internal opcodes into the normal flow of external opcodes is accomplished using multiplexors and read-only-memories (ROMs) 1 at each stage of the opcode pipeline.
Reference: [Pen90] <author> Joan M. Pendleton. </author> <title> Private communication about the design of a new SPARC implementation, </title> <year> 1990. </year>
Reference-contexts: The pipelines for the opcodes are staggered so that a row of boxes represents operations that occur at the same time. 92 6.3 Advantages of the Opcode Pipeline Control Model The internal opcode control model is actually used in several special and general purpose microprocessors <ref> [PKB + 86, HSC + 90, Pen90] </ref>. It is easy to implement, uses little chip area, and does not lengthen the processor cycle time. Its main advantage over other control models is that a richer instruction set is possible for very little extra cost.
Reference: [PH90] <author> David A. Patterson and John L. Hennessy. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year> <month> 260 </month>
Reference-contexts: generate an initial instruction set, and then an iterative, instruction-based technique can be used to arrive at the final instruction set. 2.4 Related Work 2.4.1 General Principles of Instruction Set Design Textbook presentations of instruction set design usually begin with outlining the broad design choices that need to be made <ref> [PH90, pages 89-137] </ref> [SBN82, pages 65-78]. <p> set is, the fewer the number of special cases that need to be considered by the compiler during code generation. * Simplify tradeoffs among alternatives. "One of the toughest jobs a compiler writer has is figuring out what instruction sequence will be best for every segment of code that arises" <ref> [PH90, page 121] </ref>. <p> Examples include subroutine call instructions that support only one parameter passing mechanism, or a loop instruction that supports only certain types of initialization, test, and increment. * Provide instructions that bind quantities known at compile time. One should avoid having instructions computing information already known at compile time <ref> [PH90, page 121] </ref>. Using the example of generic addition (see Figure 2.3), if the compiler can deduce the types of the operands, then it should be provided with more specific instructions such as INT_ADD or REAL_ADD.
Reference: [PKB + 86] <author> J. Pendleton, S. Kong, E. Brown, F. Dunlap, C. Marino, D. Ungar, D. Patterson, and D. Hodges. </author> <title> A 32-bit microprocessor for Smalltalk. </title> <journal> IEEE Journal of Solid State Circuits, </journal> <volume> SC-21(5):741-749, </volume> <month> October </month> <year> 1986. </year>
Reference-contexts: Internal opcodes allow easy implementation of multicycle instructions or instructions which dynamically change as they execute (conditional execution instructions). The internal opcode control model was first described by Pendleton <ref> [Pen85, PKB + 86] </ref>. The internal opcode control model is illustrated in Figure 6.2. The process of injecting internal opcodes into the normal flow of external opcodes is accomplished using multiplexors and read-only-memories (ROMs) 1 at each stage of the opcode pipeline. <p> The pipelines for the opcodes are staggered so that a row of boxes represents operations that occur at the same time. 92 6.3 Advantages of the Opcode Pipeline Control Model The internal opcode control model is actually used in several special and general purpose microprocessors <ref> [PKB + 86, HSC + 90, Pen90] </ref>. It is easy to implement, uses little chip area, and does not lengthen the processor cycle time. Its main advantage over other control models is that a richer instruction set is possible for very little extra cost.
Reference: [PLG88] <author> E. Pelegri-Llopart and S. L. Graham. </author> <title> Optimal code generation for expression trees: An application of BURS theory. </title> <booktitle> In Conference Record of the Fifteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 294-308, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Most current techniques used for constructing retargetable code generators can be divided into three types: (1) Graham-Glanville systems which rely on LR parsing of expression trees written in prefix form [AGH + 84], (2) Twig and BURS systems which make use of fast pattern matching of expression trees <ref> [AGT89, PLG88] </ref>, and (3) peephole-based systems which use naive code generators and extensive peephole optimization using symbolic simulation [DF84]. 5 The problem has not been proven NP-complete, but no polynomial algorithm is known. 31 Peephole Optimization One of the last stages in the process of compilation of a program into machine
Reference: [PS82] <author> D. A. Patterson and C. H. Sequin. </author> <title> A VLSI RISC. </title> <journal> Computer, </journal> <volume> 15(9) </volume> <pages> 8-18, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: Performance was disappointing since this strict one-to-one correspondence resulted in the absence of instructions that could have been useful in optimizing for common special cases. 8 Beginning in the late 70's, researchers <ref> [Rad82, PS82, HJG + 82] </ref> began to study instruction set design by using an iterative process wherein benchmark programs were compiled for a proposed architecture and simulated. Results from the simulations prompted adjustments to the instruction set, and the process was repeated (see Figure 1.1, page 2).
Reference: [RA78] <author> Tomlinson G. Rauscher and Ashok K. Agrawala. </author> <title> Dynamic problem-oriented redefinition of computer architecture via microprogramming. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(11):1006-1014, </volume> <month> November </month> <year> 1978. </year>
Reference-contexts: two good starting points for the application of subroutinization are the programs expressed in RTL (register transfer language) or a set of instructions that is minimal yet complete. 2.2.2 Dynamic Microprogramming and Vertical Migration In the 1970's the availability of writable microcode started research into dynamic microprogramming and vertical migration <ref> [AAK74, RA78, SvD78] </ref>. The basic idea is to move often-used functions, program loops, or instruction sequences to writable microcode memory. Each program could then have its own specialized instructions optimized to improve the program's performance.
Reference: [Rad82] <author> G. Radin. </author> <title> The 801 minicomputer. </title> <booktitle> In Proceedings Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 39-47, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Performance was disappointing since this strict one-to-one correspondence resulted in the absence of instructions that could have been useful in optimizing for common special cases. 8 Beginning in the late 70's, researchers <ref> [Rad82, PS82, HJG + 82] </ref> began to study instruction set design by using an iterative process wherein benchmark programs were compiled for a proposed architecture and simulated. Results from the simulations prompted adjustments to the instruction set, and the process was repeated (see Figure 1.1, page 2).
Reference: [RLTS92] <author> B. R. Rau, M. Lee, P. P. Tirumalai, and M. S. Schlansker. </author> <title> Register allocation for software pipelined loops. </title> <booktitle> In SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 283-299, </pages> <year> 1992. </year> <journal> Available as Sigplan Notices, </journal> <volume> volume 27, number 7, </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: Admittedly, this is not satisfactory, but is required because of the loop inside dref. Future work will investigate the generation of instructions that contain loops. One possible avenue of research would be the use of modulo scheduling <ref> [RLTS92] </ref>. 7.8.6 Heuristics to Reduce Redundant Operations If one blindly finds all possible compilations for a given code segment, then some compilations will involve unnecessary register movement or redundant operations not required by other compilations with the same performance.
Reference: [RW88] <author> Deborah W. Runner and Erwin H. Warshawsky. </author> <title> Synthesizing Ada's ideal machine mate. </title> <booktitle> VLSI Systems Design, </booktitle> <pages> pages 30-39, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: 1 There are also important multi-cycle instructions that contain loops, for example, block memory copy, string match, vector operations, and pointer dereferencing (heavily used in Prolog). 9.2.2 Integration with High-Level Synthesis Tools High level synthesis is the generation of a data path and control from an algorithm or instruction set <ref> [RW88, THK + 83, Des88, CR89] </ref>. High level synthesis programs take an algorithm written in a high level language and convert it to a microarchitecture and microcode that satisfies the user's constraints on speed, power, and manufacturing cost.
Reference: [Sam89a] <author> A. Dain Samples. Mache: </author> <title> No-loss trace compaction. </title> <type> Technical Report UCB/CSD 89/446, </type> <institution> University of California, Berkeley, Computer Science Division, </institution> <year> 1989. </year>
Reference-contexts: For example, the trace produced by the chat_parser benchmark is over 90 megabytes. Use of a compression utility like the Unix compress program reduces this size down to approximately 20 megabytes. To achieve a larger compression ratio, Samples' technique for compressing trace data <ref> [Sam89a, Sam89b] </ref> is used. In Samples' technique, the trace is represented using differences between data values. The differences can often be expressed in fewer bytes, and, in addition, the string of differences is more likely to have repeating patterns which compress can then encode with greater efficiency.
Reference: [Sam89b] <author> A. Dain Samples. Mache: </author> <title> No-loss trace compaction. </title> <booktitle> In Proceedings of the Sig-metrics International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 89-97, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: For example, the trace produced by the chat_parser benchmark is over 90 megabytes. Use of a compression utility like the Unix compress program reduces this size down to approximately 20 megabytes. To achieve a larger compression ratio, Samples' technique for compressing trace data <ref> [Sam89a, Sam89b] </ref> is used. In Samples' technique, the trace is represented using differences between data values. The differences can often be expressed in fewer bytes, and, in addition, the string of differences is more likely to have repeating patterns which compress can then encode with greater efficiency.
Reference: [SBN82] <author> D. P. Siewiorek, C. G. Bell, and A. Newell. </author> <title> Computer Structures: Principles and Examples. </title> <publisher> McGraw-Hill, </publisher> <year> 1982. </year> <month> 261 </month>
Reference-contexts: instruction set, and then an iterative, instruction-based technique can be used to arrive at the final instruction set. 2.4 Related Work 2.4.1 General Principles of Instruction Set Design Textbook presentations of instruction set design usually begin with outlining the broad design choices that need to be made [PH90, pages 89-137] <ref> [SBN82, pages 65-78] </ref>.
Reference: [Shu78] <author> Leonard J. Shustek. </author> <title> Analysis and Performance of Computer Instruction Sets. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1978. </year>
Reference-contexts: Two of the most important early studies where done by Lunde [Lun77] and Shustek <ref> [Shu78] </ref>. Both involved tracing a large number of programs written in a variety of languages (including assembly). Instruction opcode, instruction addressing mode, and register usage statistics were collected. In addition, typical sequences of code were found.
Reference: [Smi82] <author> Alan Jay Smith. </author> <title> Cache memories. </title> <journal> Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <month> Septem-ber </month> <year> 1982. </year>
Reference-contexts: When this is done, however, the metric given by Equation 4.5 does not exactly coincide with the intent of the 1% rule stated by Kane (page 8). The metric would accept a new instruction if the average benefit for the instruction is 1%. Kane's 2 Using Figures 25-28 in <ref> [Smi82] </ref>, one can get a rough upper bound on the relationship between code density and instruction cache performance.
Reference: [SS82] <author> Richard E. Sweet and James G. Sandman, Jr. </author> <title> Empirical analysis of the Mesa instruction set. </title> <booktitle> In Proceedings Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 158-166, </pages> <year> 1982. </year>
Reference-contexts: Perhaps the advances in compiler theory since the time of Bose's work could make this a more tractable problem. 2.2.4 Sweet|Revising the Mesa Instruction Set In a very practical study Sweet revised the Mesa instruction set to improve code density on a large set of production codes <ref> [SS82] </ref>. The instruction set was byte encoded, each instruction consisted of a byte opcode and a set of variable-size operands. <p> The instruction addition and replacement transformations require that one or more new instructions be generated. The techniques for generating these instructions are outlined in Table 2.7. Specialization and Generalization. An instruction in the current instruction set can be specialized or generalized to form a new instruction. Sweet <ref> [SS82] </ref> and Bennett [Ben88] use two specific types of specialization: reduction of operand size and making an operand 2 A fine distinction, but necessary for greedy algorithms, which only accept transformations that improve the metric, or for stochastic optimization algorithms, which require well picked transformations to achieve good convergence. 22 A <p> This substitution is valid only when the cycle time is not altered by changes in the instruction set. Cycle count was used as the primary metric for vertical migration (Section 2.2.2). * Static code size. This metric is popular when memory is limited <ref> [FSLB77, SS82, Bos83, Ben88] </ref>. Reduction in code size allows more sophisticated programs to fit in a limited amount of memory.
Reference: [STN + 87] <author> V. P. Srini, J. V. Tam, T. M. Nguyen, Y. N. Patt, A. M. Despain, M. Moll, and D. Ellsworth. </author> <title> A CMOS chip for Prolog. </title> <booktitle> In Proceedings of the International Conference on Computer Design, </booktitle> <month> October </month> <year> 1987. </year>
Reference-contexts: The Warren Abstract Machine (WAM) [War83] is the basis for many commonly used abstract instruction sets for Prolog compilers. Variations of the WAM have also been used as the instruction set for microcoded Prolog processors. An example of one of these processors is the VLSI-PLM <ref> [STN + 87] </ref>. The subset of its instruction set which supports structure creation is given in Table 5.7. It is important to point out that the actual PLM instructions are sometimes much more complex than the RTL given. For example, the unify instructions support both structure creation and general unification.
Reference: [SvD78] <author> John Stockenberg and Andries van Dam. </author> <title> Vertical migration for performance enhancement in layered hardware/firmware/software systems. </title> <journal> Computer, </journal> <volume> 11(5) </volume> <pages> 35-50, </pages> <month> May </month> <year> 1978. </year>
Reference-contexts: two good starting points for the application of subroutinization are the programs expressed in RTL (register transfer language) or a set of instructions that is minimal yet complete. 2.2.2 Dynamic Microprogramming and Vertical Migration In the 1970's the availability of writable microcode started research into dynamic microprogramming and vertical migration <ref> [AAK74, RA78, SvD78] </ref>. The basic idea is to move often-used functions, program loops, or instruction sequences to writable microcode memory. Each program could then have its own specialized instructions optimized to improve the program's performance.
Reference: [THK + 83] <author> D. E. Thomas, C. Y. Hitchcock, T. J. Kowalski, J. V. Rajan, and R. A. Walker. </author> <title> Automated data path synthesis. </title> <journal> Computer, </journal> <volume> 21 </volume> <pages> 59-70, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: 1 There are also important multi-cycle instructions that contain loops, for example, block memory copy, string match, vector operations, and pointer dereferencing (heavily used in Prolog). 9.2.2 Integration with High-Level Synthesis Tools High level synthesis is the generation of a data path and control from an algorithm or instruction set <ref> [RW88, THK + 83, Des88, CR89] </ref>. High level synthesis programs take an algorithm written in a high level language and convert it to a microarchitecture and microcode that satisfies the user's constraints on speed, power, and manufacturing cost.
Reference: [Tho90] <author> Ken Thompson. </author> <title> A new C compiler. </title> <booktitle> In Proceedings of the Summer 1990 UKUUG Conference, </booktitle> <pages> pages 41-51, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: One consequence of this is that determining the best instruction set is also extremely difficult, and in a sense justifies the computational effort required by the design techniques presented in this dissertation. Practical Code Generation Although some compilers still have hand-written code generators <ref> [Tho90] </ref>, the work presented here will be aimed towards retargetable code generators, since this is the logical choice to use with an automated instruction set design system.
Reference: [Tsi70] <author> D. Tsichritzis. </author> <title> The equivalence problem of simple programs. </title> <journal> Journal of the ACM, </journal> <volume> 17(4) </volume> <pages> 729-738, </pages> <month> October </month> <year> 1970. </year>
Reference-contexts: In this case opti 4 Equivalence of simple programs is solvable. For example, simple looping programs with only one level of nesting of loops are solvable. But equivalence of programs with loops nested two or more levels are unsolvable <ref> [Tsi70] </ref>. 30 mal code can be produced in linear time using a dynamic programming algorithm. When common subexpressions are allowed, then generating optimal code is NP-hard [AJU77].
Reference: [Ung87] <author> D. M. Ungar. </author> <title> The Design and Evaluation of a High Performance Smalltalk System. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Even if the compiler cannot deduce the operand types, then one may still want to 23 use INT_ADD if this instruction has been modified to trap when its operands are not integers. Unger shows how this principle can be applied to an instruction set specialized to support Smalltalk <ref> [Ung87] </ref>. Use of traps for the uncommon case is a valid design alternative, but will not be considered in this dissertation. Specialization based on compile-time knowledge of operand types has been applied to Prolog unification by Kursawe [Kur87] and Van Roy [VR89].
Reference: [VR89] <author> Peter Van Roy. </author> <title> An intermediate language to support Prolog's unification. </title> <editor> In Lusk and Overbeek, editors, </editor> <booktitle> Proceedings of the North American Conference on Logic Programming. </booktitle> <publisher> MIT Press, </publisher> <month> October </month> <year> 1989. </year>
Reference-contexts: Use of traps for the uncommon case is a valid design alternative, but will not be considered in this dissertation. Specialization based on compile-time knowledge of operand types has been applied to Prolog unification by Kursawe [Kur87] and Van Roy <ref> [VR89] </ref>. Kursawe assumed only trivial type analysis and obtained a WAM-like instruction set. Van Roy assumed extensive type analysis and obtained an instruction set with finer granularity. Decomposition. An instruction in the current instruction set can be decomposed into two or more new instructions.
Reference: [VR90] <author> Peter L. Van Roy. </author> <title> Can Logic Programming Execute as Fast as Imperative Programming? PhD thesis, </title> <institution> University of California, Berkeley, </institution> <month> December </month> <year> 1990. </year> <note> Available as U. C. Berkeley Computer Science Division technical report no. UCB/CSD 90/600. 262 </note>
Reference-contexts: Basic statistics for these benchmarks are given in Table 8.1. These benchmarks are used because they are the benchmarks used in the instruction set studies for the VLSI-BAM [HSC + 90]. The compiler used is the Aquarius Prolog compiler <ref> [VR90, VRD92] </ref>|a highly optimizing compiler that can automatically determine the data types of many variables in the Prolog code to yield smaller and faster code. The compiler has been targeted for several machines: VLSI-BAM, MIPS, Sparc, and MC68020.
Reference: [VRD92] <author> Peter Van Roy and Alvin M. Despain. </author> <title> High-performance logic programming with the Aquarius Prolog compiler. </title> <journal> Computer, </journal> <volume> 25(1) </volume> <pages> 54-68, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Using the 21011 instruction set, Benchmark 25 compiles to a repeated sequence of create pointer, push, and store instructions: 2 The environment pointer, E, points to the procedure activation record. In the BAM Prolog execution model <ref> [VRD92] </ref>, the return address and old environment pointer is stored just below E. <p> Basic statistics for these benchmarks are given in Table 8.1. These benchmarks are used because they are the benchmarks used in the instruction set studies for the VLSI-BAM [HSC + 90]. The compiler used is the Aquarius Prolog compiler <ref> [VR90, VRD92] </ref>|a highly optimizing compiler that can automatically determine the data types of many variables in the Prolog code to yield smaller and faster code. The compiler has been targeted for several machines: VLSI-BAM, MIPS, Sparc, and MC68020.
Reference: [War83] <author> D. H. D. Warren. </author> <title> An abstract Prolog instruction set. </title> <type> Technical Report 309, </type> <institution> SRI International, </institution> <month> October </month> <year> 1983. </year>
Reference-contexts: Kursawe [Kur87] shows how the popular Warren abstract machine (WAM) <ref> [War83] </ref> can be derived from pure Prolog. Although instruction sets derived from a HLL are primarily used in compilers as an intermediate language, they have also been used in microcoded processors (an example for Prolog is the PLM [DDP85]). <p> The Warren Abstract Machine (WAM) <ref> [War83] </ref> is the basis for many commonly used abstract instruction sets for Prolog compilers. Variations of the WAM have also been used as the instruction set for microcoded Prolog processors. An example of one of these processors is the VLSI-PLM [STN + 87].
Reference: [Wul81] <author> W. A. Wulf. </author> <booktitle> Compilers and computer architecture. Computer, </booktitle> <volume> 14(7) </volume> <pages> 41-47, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Compiler writers have expressed their ideas on the characteristics of a good in struction set. These characteristics can be summarized by the following principles: * Regularity. "If something is done in one way in one place, it ought to be done the same way everywhere" <ref> [Wul81] </ref>. Related to this principle is the characteristic of being able to describe data types, addressing modes, and operations independently. Also, it should be possible to compose any combination of type, addressing mode, and operation in an instruction. <p> are to compile the same block of code, the harder the case analysis is to determine the best sequence to use." * Provide primitives, not solutions. "It is far better to provide good primitives from which solutions to code generation problems can be synthesized than to provide the solutions themselves" <ref> [Wul81] </ref>. If one provides too much semantic content to an instruction, then the use of the instruction is restricted to limited contexts.
References-found: 86

