URL: ftp://ftp.cs.rutgers.edu/pub/kudenko/papers/aaai98.ps
Refering-URL: http://athos.rutgers.edu/~kudenko/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: lastname@cs.rutgers.edu  
Title: Feature Generation for Sequence Categorization  
Author: Daniel Kudenko and Haym Hirsh 
Address: Piscataway, NJ 08855  
Affiliation: Department of Computer Science Rutgers University  
Abstract: The problem of sequence categorization is to generalize from a corpus of labeled sequences procedures for accurately labeling future unlabeled sequences. The choice of representation of sequences can have a major impact on this task, and in the absence of background knowledge a good representation is often not known and straightforward representations are often far from optimal. We propose a feature generation method (called FGEN) that creates Boolean features that check for the presence or absence of heuristically selected collections of subsequences. We show empirically that the representation computed by FGEN improves the accuracy of two commonly used learning systems (C4.5 and Ripper) when the new features are added to existing representations of sequence data. We show the superiority of FGEN across a range of tasks selected from three domains: DNA sequences, Unix command sequences, and English text. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cohen, W., and Kudenko, D. </author> <year> 1997. </year> <title> Transferring and retraining learned information filters. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence. </booktitle>
Reference: <author> Cohen, W. </author> <year> 1995. </year> <title> Fast effective rule induction. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle>
Reference-contexts: We evaluated FGEN representations on a wide range of sequence categorization tasks: English text, DNA sequences, and Unix command sequences. The evaluation was done by comparing the accuracy of C4.5 (Quinlan 1994) and Ripper <ref> (Cohen 1995) </ref> using straightforward sequence representations to the ac curacy of those learners using FGEN's features ap-pended to these representations. Our results show that FGEN's features often improved the accuracy of the respective learning algorithms, and in those cases when they didn't, had only modest impact.
Reference: <author> Cohen, W. </author> <year> 1996. </year> <title> Learning rules that classify e-mail. </title> <booktitle> In Proceedings of the 1996 AAAI Spring Symposium on Machine Learning in Information Access. </booktitle>
Reference: <author> Davison, B., and Hirsh, H. </author> <year> 1997. </year> <title> Toward an adaptive command line interface. </title> <booktitle> In Seventh International Conference on Human-Computer Interaction. </booktitle>
Reference-contexts: 40 to 90. 6 One of the users forms the tar 5 The choice of sequence length 100 was arbitrary, as is the case for the book sequence dataset and the Unix command sequence dataset. 6 The superset of this data has been collected for experiments in Unix command prediction <ref> (Davison & Hirsh 1997) </ref>. get class (a different one for each dataset) while the other two form the background class. English Text Newsgroup topic spotting: We generated three datasets by extracting postings from three different newsgroups: comp.os.ms-windows, rec.games.- backgammon, and sci.classics.
Reference: <author> Hirsh, H., and Kudenko, D. </author> <year> 1997. </year> <title> Representing sequences in description logics. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: In other words, F 1 contains less restrictions than F 2 . FGEN features can be efficiently implemented using a data structure based on suffix trees, which has been described in detail in <ref> (Hirsh & Kudenko 1997) </ref>. The FGEN algorithm The input to FGEN are pre-classified training exam ples. The output is a set of features as described above, which can be used to map each training and test ex ample into a Boolean feature vector. <p> Although this method will not always choose the optimal step. These feature simplification operations can be computed efficiently on the data structure for FGEN features, which is based on suffix trees <ref> (Hirsh & Kudenko 1997) </ref>. 3 The number of features is in the order of jj n , where is the alphabet and n is the length of the subsequences underlying the representation. 4 For tractability reasons we restricted n to 5 or less. representation, it will yield the better performance on <p> 40 to 90. 6 One of the users forms the tar 5 The choice of sequence length 100 was arbitrary, as is the case for the book sequence dataset and the Unix command sequence dataset. 6 The superset of this data has been collected for experiments in Unix command prediction <ref> (Davison & Hirsh 1997) </ref>. get class (a different one for each dataset) while the other two form the background class. English Text Newsgroup topic spotting: We generated three datasets by extracting postings from three different newsgroups: comp.os.ms-windows, rec.games.- backgammon, and sci.classics.
Reference: <author> Hirsh, H., and Noordewier, M. </author> <year> 1994. </year> <title> Using background knowledge to improve inductive learning of DNA sequences. </title> <booktitle> In Proceedings IEEE Conference on AI for Applications. </booktitle>
Reference-contexts: For example, one could choose to create a feature for every possible subsequence up to a certain length k (i.e., make a feature for all n-grams for n ranging from 1 to k). Unfortunately, such straightforward, albeit general, representations need not perform well. For example, Hirsh and Noordewier <ref> (Hirsh & Noordewier 1994) </ref> have shown that the choice of features for DNA sequence categorization problems can have a substantial impact on the accuracy of feature-based learners. Their features, based on domain knowledge, performed dramatically better than a straightforward positional encoding of data commonly used on this problem.
Reference: <author> Loewenstern, D.; Berman, H.; and Hirsh, H. </author> <year> 1998. </year> <title> Maximum a posteriori classification of DNA structure from sequence information. </title> <booktitle> In Proceedings of PSB-98. </booktitle>
Reference-contexts: 1.7 3.4 0.9 News 1 13.8 9.4 13.4 9.6 News 2 8.8 10.3 9.7 9.1 News 3 17.8 14.4 12.4 13.5 Talks 1 10.1 10.2 15.4 15.7 Talks 2 5.3 5.1 4.1 4.2 Talks 3 4.0 4.0 4.7 4.7 Talks 4 7.8 3.9 5.1 5.5 Books 23.8 20.3 16.2 12.7 <ref> (Loewenstern, Berman, & Hirsh 1998) </ref>. Hidden Markov Models require an initial model, which is usually based on domain knowledge. Lllama has been designed mainly with biological sequences in mind.
Reference: <author> Norton, S. W. </author> <year> 1994. </year> <title> Learning to recognize promoter sequences in E.coli. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence. </booktitle>
Reference: <author> Quinlan, J. R. </author> <year> 1994. </year> <title> C4.5: programs for machine learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We evaluated FGEN representations on a wide range of sequence categorization tasks: English text, DNA sequences, and Unix command sequences. The evaluation was done by comparing the accuracy of C4.5 <ref> (Quinlan 1994) </ref> and Ripper (Cohen 1995) using straightforward sequence representations to the ac curacy of those learners using FGEN's features ap-pended to these representations. Our results show that FGEN's features often improved the accuracy of the respective learning algorithms, and in those cases when they didn't, had only modest impact.
Reference: <author> Rabiner, L. </author> <year> 1989. </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> In Proceedings IEEE. </booktitle>
Reference-contexts: Note that the number of features generated by FGEN never exceeded 20 on any dataset. This is remarkable, given the large size of the space of potential features. Related Work Previous work in sequence categorization includes Hidden Markov Models <ref> (Rabiner 1989) </ref>, learning of finite automata, and entropy-based induction (Lllama) 7 This data was previously used in a previous study on e-mail filtering (Cohen 1996; Cohen & Kudenko 1997). 8 The book texts have been taken from the Calgary Corpus, a collection of files for compression benchmarks.
Reference: <author> Saitta, L. </author> <year> 1996. </year> <title> Representation change in machine learning. </title> <journal> AI Communications 9 </journal> <pages> 14-20. </pages>
Reference-contexts: An overview of the area of constructive induction, which deals with the automatic generation of representations for learning, is presented in <ref> (Saitta 1996) </ref>. We are not aware of any constructive induction research for sequence categorization. Final Remarks We presented a feature generation method for sequence categorization that creates boolean features based on minimum subsequence frequencies.
Reference: <author> Salzberg, S. </author> <year> 1995. </year> <title> Locating protein coding regions in human DNA using a decision tree algorithm. </title> <journal> Journal of Computational Biology 2(3). </journal>
Reference: <author> Schaffer, C. </author> <year> 1993. </year> <title> Selecting a classifier method by cross-validation. </title> <booktitle> Machine Learning 13. </booktitle>
Reference-contexts: of features is in the order of jj n , where is the alphabet and n is the length of the subsequences underlying the representation. 4 For tractability reasons we restricted n to 5 or less. representation, it will yield the better performance on average, as has been shown in <ref> (Schaffer 1993) </ref>. Afterwards, the training and test data is represented in the winning format and the error rate with this representation is reported.
References-found: 13

