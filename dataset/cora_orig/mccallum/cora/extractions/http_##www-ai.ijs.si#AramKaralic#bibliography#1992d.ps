URL: http://www-ai.ijs.si/AramKaralic/bibliography/1992d.ps
Refering-URL: http://www-ai.ijs.si/AramKaralic/bibliography/1992d.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: aram.karalic@ijs.si  
Title: Linear Regression in Regression Tree Leaves  
Author: Aram Karalic 
Date: June 1992  
Address: Jamova 39 61111 Ljubljana Slovenia  
Affiliation: Jozef Stefan Institute  
Abstract: The advantage of using linear regression in the leaves of a regression tree is analysed in the paper. It is carried out how this modification affects the construction, pruning and interpretation of a regression tree. The modification is tested on artificial and real-life domains where its impact on classification error and stability of the induced trees is considered. The results show that the modification is beneficial, as it leads to smaller classification errors of induced regression trees. The Bayesian approach to estimation of class distributions is used in all experiments.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Breiman, L., Friedman, J.H., Olshen, R.A., & Stone, C.J.: </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth Int. Group, </publisher> <address> Belmont, California, USA, </address> <year> 1984. </year>
Reference-contexts: At that point, a class value is computed according to 1 the function labelling the leaf. This class value represents an answer of the tree and is assigned to the example. In the basic CART algorithm <ref> [1] </ref> the class value in the leaves of the induced regression tree is estimated as a constant function. This definition of the leaves is extended in the paper. The extension allows function in the leaf, which predicts value of the class, to be linear function of continuous attributes. <p> A regression tree actually implements a function y (x 1 ; x 2 ; : : : ; x n ) of n continuous or discrete attributes x 1 ; x 2 ; : : : ; x n . More about regression trees can be found in <ref> [1] </ref>. The algorithm for regression tree construction belongs to TDIDT family of algorithms. These algorithms split example set, representing the node of a tree, into two subsets, from which they recursively form subtrees.
Reference: [2] <author> Cestnik, B.: </author> <title> Estimating probabilities: A Crucial Task in Machine Learning, </title> <booktitle> Proceedings of ECAI-90 , Stockholm, </booktitle> <address> Sweden, </address> <year> 1990. </year>
Reference-contexts: Some experiments are then performed. According to the results of experiments, prior value, or prior hypothesis, is modified as to obtain posterior value of the estimated quantity. This process can be incrementally repeated to improve the estimation. Bayesian approach to estimating probabilities was introduced in <ref> [2] </ref>. Bayesian estimate of class distribution of example set E is the combination of initial (prior) class distribution and class distribution of E. Weight, and thus influence, of the initial class distribution is regulated with parameter m.
Reference: [3] <author> Cestnik, B., & Bratko, I.: </author> <title> On Estimating Probabilities in Tree Pruning, </title> <booktitle> Proceedings of EWSL-91 , Porto, </booktitle> <address> Portugal, </address> <year> 1991. </year>
Reference-contexts: Select the best tree in the sequence of pruned trees. We used m = 0 during the tree-construction phase for the similar reasons as described in <ref> [3] </ref>; namely: we consider the tree merely as a different form of learning examples and want to preserve all the information present in the learning example set.
Reference: [4] <author> Cestnik, B., Kononenko, I., & Bratko, I.: </author> <title> ASSISTANT 86: A Knowledge-Elicitation Tool for Sophisticated Users, Progress in Machine Learning, </title> <editor> ed. by Bratko, I. and Lavrac, N., </editor> <publisher> Sigma Press, </publisher> <address> Wilmslow, </address> <year> 1987. </year>
Reference-contexts: A part of a regression tree is shown on Figure 1. The trees are constructed by algorithms from the TDIDT family of algorithms. Two well known representatives of this family are ID3 [13] and ASSISTANT <ref> [11, 4] </ref> algorithms. Once built, regression tree is interpreted similarly as a classification tree. When classifying an example by the regression tree, the interpretation begins at the root of the tree.
Reference: [5] <author> Cestnik, B., & Urbancic, T.: </author> <title> Prediction of Fresh Concrete Properties with Artificial Intelligence Methods, </title> <type> Technical Report, </type> <institution> IJS DP-5963, Jozef Stefan Institute, Ljubl-jana, Slovenia, </institution> <note> 1990 (in Slovene). </note>
Reference-contexts: The domain is described in more detail in <ref> [5] </ref>. The results of the experiments are summarised in six graphs, depicted in Figure 3, each containing the results obtained in one domain. 6 Discussion and Conclusions We wanted to find out whether local linear regression was beneficial.
Reference: [6] <author> Ferguson, </author> <title> G.A.: Statistical Analysis in Psychology and Education, </title> <publisher> McGraw-Hill, </publisher> <address> Lon-don, United Kingdom, </address> <year> 1966. </year>
Reference-contexts: The answer can be (partially) found by looking at graphs on Figure 3. The results of the t-test of significance for correlated samples <ref> [6] </ref> for the hypothesis: "The classification error when using local linear regression is smaller than the error without using local linear regression." are presented in the Table 1.
Reference: [7] <author> Filipic, B., Junkar, M., Bratko, I., & Karalic, A.: </author> <title> An Application of Machine Learning to a Metal-Working Process, </title> <booktitle> Proceedings of ITI-91 , Cavtat, </booktitle> <address> Croatia, </address> <note> 1991 (in press). </note>
Reference-contexts: Roughness of the workpiece is to be determined from the properties of the sound produced during the process of steel grinding. 123 examples were available, described in terms of three attributes. Detailed descrip tion of the domain together with the previous work can be found in <ref> [7, 9] </ref>. * SPIN: This domain deals with the prediction of the cotton yarn strength from the properties of spinning material mixture. The mixture is described in terms of 10 attributes. There were only 18 learning examples available for the experiments.
Reference: [8] <author> Good, I.J.: </author> <title> The Estimation of Probabilities, </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, Massachusetts, USA, </address> <year> 1965. </year>
Reference-contexts: squared error of the tree is normalised by the mean squared error of the predictor which always predicts the mean value of the training example set. 3 Bayesian Approach to Tree-Structured Regression Bayesian approach to estimating class distribution in the tree-structured regression is based on Good's notion of Bayesian analysis <ref> [8] </ref> and was introduced in [10]. Here, let us briefly summarise main characteristics of Bayesian approach. Basic idea is that one initially assumes a prior value of estimated quantity. Some experiments are then performed.
Reference: [9] <author> Junkar, M., Filipic, B., & Bratko, I.: </author> <title> Identifying the grinding process by means of inductive machine learning, </title> <booktitle> Preprints of the first CIRP Workshop of the Intelligent Manufacturing Systems, </booktitle> <address> Budapest, Hungary, </address> <year> 1991. </year>
Reference-contexts: Roughness of the workpiece is to be determined from the properties of the sound produced during the process of steel grinding. 123 examples were available, described in terms of three attributes. Detailed descrip tion of the domain together with the previous work can be found in <ref> [7, 9] </ref>. * SPIN: This domain deals with the prediction of the cotton yarn strength from the properties of spinning material mixture. The mixture is described in terms of 10 attributes. There were only 18 learning examples available for the experiments.
Reference: [10] <author> Karalic, A., & Cestnik, B.: </author> <title> The Bayesian Approach to Tree-Structured Regression, </title> <booktitle> Proceedings of ITI-91 , Cavtat, </booktitle> <address> Croatia, </address> <note> 1991 (in press). </note>
Reference-contexts: is normalised by the mean squared error of the predictor which always predicts the mean value of the training example set. 3 Bayesian Approach to Tree-Structured Regression Bayesian approach to estimating class distribution in the tree-structured regression is based on Good's notion of Bayesian analysis [8] and was introduced in <ref> [10] </ref>. Here, let us briefly summarise main characteristics of Bayesian approach. Basic idea is that one initially assumes a prior value of estimated quantity. Some experiments are then performed.
Reference: [11] <author> Kononenko, I.: </author> <title> The Development of the Inductive Learning System Assistant , M.Sc. </title> <type> Thesis, </type> <institution> Edvard Kardelj University, Ljubljana, Slovenia, </institution> <note> 1985 (in Slovene). </note>
Reference-contexts: A part of a regression tree is shown on Figure 1. The trees are constructed by algorithms from the TDIDT family of algorithms. Two well known representatives of this family are ID3 [13] and ASSISTANT <ref> [11, 4] </ref> algorithms. Once built, regression tree is interpreted similarly as a classification tree. When classifying an example by the regression tree, the interpretation begins at the root of the tree.
Reference: [12] <author> Niblett, T. & Bratko, I.: </author> <title> Learning Decision Rules in Noisy Domains, Development in Expert Systems, </title> <editor> (ed. Bramer, M.), </editor> <publisher> Cambridge University Press, </publisher> <year> 1986. </year>
Reference-contexts: during the construction of a regression tree with enabled local linear regression is use of impurity measure (3) instead of impurity measure (2) . 4.2 Local Linear Regression During Post-Pruning of a Regres sion Tree For post-pruning of the generated regression trees an algorithm based on the Niblett-Bratko post-pruning method <ref> [12] </ref> was used. Pruning is based on the idea that for every node an estimation of its classification error on test examples is made. In each node the algorithm makes an estimate of static error (e s ) and backed-up error (e b ).
Reference: [13] <author> Quinlan, J.R.: </author> <title> Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> Vol. 1, </volume> <year> 1986. </year>
Reference-contexts: A part of a regression tree is shown on Figure 1. The trees are constructed by algorithms from the TDIDT family of algorithms. Two well known representatives of this family are ID3 <ref> [13] </ref> and ASSISTANT [11, 4] algorithms. Once built, regression tree is interpreted similarly as a classification tree. When classifying an example by the regression tree, the interpretation begins at the root of the tree.
Reference: [14] <author> Stjepanovic, Z., & Jezernik, A.: </author> <title> A Contribution to the Prediction of Cotton Yarn Properties Using Artificial Intelligence, </title> <booktitle> Preprints of the first CIRP Workshop of the Intelligent Manufacturing Systems, </booktitle> <address> Budapest, Hungary, </address> <year> 1991. </year> <month> 11 </month>
Reference-contexts: The mixture is described in terms of 10 attributes. There were only 18 learning examples available for the experiments. Details of the domain and the previous work can are described in <ref> [14] </ref>. * ZRMK: In this domain properties of fresh concrete are predicted from the data describing input materials and mix proportions. 254 examples described in terms of 7 ten continuous and four discrete attributes were available. The domain is described in more detail in [5].
References-found: 14

