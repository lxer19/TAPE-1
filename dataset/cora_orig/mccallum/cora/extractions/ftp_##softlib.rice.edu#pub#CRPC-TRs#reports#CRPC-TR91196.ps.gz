URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR91196.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Evaluation of Compiler Optimizations for Fortran D on MIMD Distributed-Memory Machines  
Author: Seema Hiranandani Ken Kennedy Chau-Wen Tseng 
Note: Center for Research on Parallel Computation  In Proceedings of the 1992 International Conference on Supercomputing, Washington DC,  
Date: November 1991  July 1992.  
Address: 91196  P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: CRPC-TR  Rice University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of For tran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: For instance, the Intel iPSC/860 requires approximately 95 sec to send one byte versus .4 sec for each additional byte [6]. The following optimizations seek to reduce T start by combining or eliminating messages. Message vectorization Message vectorization uses the results of data dependence analysis <ref> [1, 28] </ref> to combine element messages into vectors. It first calculates commlevel, the level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence. This determines the outermost loop where element messages resulting from the same array reference may be legally combined [3, 15]. <p> In comparison, coarse-grain pipelining strip-mines the k loop by a factor B, then interchanges the itera-tor loop kk outside the j loop. This allows communication for B iterations to be vectorized at the j loop. The legality of loop interchange and strip-mine is determined exactly as for shared-memory programs <ref> [1, 25, 28] </ref>. The Fortran D compiler first permutes loops in memory order to exploit data locality on individual processors [24], then applies coarse-grain pipelining to adjust the degree of pipeline parallelism. 4.4 Reducing Storage Most optimizations increase the amount of temporary storage required by the program.
Reference: [2] <author> F. Andre, J. Pazat, and H. Thomas. </author> <title> Pandore: A system to manage data distribution. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: By using dependence analysis, the Fortran D compiler can detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal [29], Id Nouveau [32]), all parallel loops (Al [36], Arf [37], Kali [26]), parallel functions (C*/Dataparallel C [17, 33], Dino [34]), parallel code blocks (Oxygen [35], Pandore <ref> [2] </ref>), or parallel array operations (CM Fortran [7], Paragon [10]). The Fortran D compiler is similar to Callahan & Kennedy [9] and Superb [15, 38], but applies analysis and optimization up front before code generation, rather than inserting guards and element-wise messages then optimizing via program transformations and partial evaluation.
Reference: [3] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: It first calculates commlevel, the level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence. This determines the outermost loop where element messages resulting from the same array reference may be legally combined <ref> [3, 15] </ref>. Vectorized nonlocal accesses are represented as RSDs and stored at the loop at commlevel. They eventually generate messages at loop headers for loop-carried RSDs and in the loop body for loop-independent RSDs. <p> Dynamic Data Decomposition Other computations contain parallelism, but are partitioned by the "owner computes" rule in a way that causes sequential execution. In these cases dynamic data decomposition may be used to temporarily change the ownership of data during program execution, exposing parallelism by internalizing cross-processor dependences <ref> [3] </ref>. For instance, consider the two substitution phases in the Alternating Direction Implicit (ADI) integration example in Figure 10. The computation wavefront only crosses one spatial dimension in each phase. A fixed column or row data distribution would result in one parallel and one sequential phase. <p> Ongoing work to provide environmental support for automatic data decomposition and static performance estimation will also enhance the usefulness of the Fortran D compiler <ref> [3, 4, 19, 23] </ref>. 10 Acknowledgements The authors wish to thank Mary Hall, Uli Kremer, and Kathryn M c Kinley for their assistance on this paper, as well as Joel Saltz, Robert Schnabel and Robert Weaver for several enlightening discussions.
Reference: [4] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 [6]. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs <ref> [4, 19, 23] </ref>. Accurate static estimates of communication and computation are also needed by the compiler to calculate block sizes for coarse-grain pipelining. Dynamic data decomposition The previous sections show how parallel computation time can be estimated for pipelined computations. <p> Ongoing work to provide environmental support for automatic data decomposition and static performance estimation will also enhance the usefulness of the Fortran D compiler <ref> [3, 4, 19, 23] </ref>. 10 Acknowledgements The authors wish to thank Mary Hall, Uli Kremer, and Kathryn M c Kinley for their assistance on this paper, as well as Joel Saltz, Robert Schnabel and Robert Weaver for several enlightening discussions.
Reference: [5] <author> S. Benkner, B. Chapman, and H. Zima. </author> <title> Vienna Fortran 90. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Most distributed-memory compilers reduce communication overhead by extracting communication out of user-specified parallel regions (e.g., loops, code blocks, array operations, procedures). Fortran D is similar to Superb and Vienna Fortran <ref> [5] </ref> in that it vectorizes messages using data dependence information, which can extract communication even out of sequential regions such as those found in ADI or SOR. Crystal and Id Nouveau identify parallelism automatically and vectorize messages using the single assignment semantics of their high-level functional languages.
Reference: [6] <author> S. Bokhari. </author> <title> Complete exchange on the iPSC-860. </title> <type> ICASE Report 91-4, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: For most MIMD distributed-memory machines, the cost to send the first byte is significantly higher than the cost for additional bytes. For instance, the Intel iPSC/860 requires approximately 95 sec to send one byte versus .4 sec for each additional byte <ref> [6] </ref>. The following optimizations seek to reduce T start by combining or eliminating messages. Message vectorization Message vectorization uses the results of data dependence analysis [1, 28] to combine element messages into vectors. <p> Messages are later inserted in front of the loop accessing ZA. Collective communication Message overhead can also be reduced by utilizing fast collective communication, such as broadcast, all-to-all, or transpose, instead of generating individual messages <ref> [6, 29] </ref>. Collective communication opportunities are recognized by comparing the subscript expression of each distributed dimension in the rhs with the aligned dimension in the lhs reference. <p> Fortunately this is relatively true for the small block sizes that are selected. More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 <ref> [6] </ref>. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs [4, 19, 23]. Accurate static estimates of communication and computation are also needed by the compiler to calculate block sizes for coarse-grain pipelining.
Reference: [7] <author> M. Bromley, S. Heller, T. McNerney, and G. Steele, Jr. </author> <title> For tran at ten gigaflops: The Connection Machine convolution compiler. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: In the following sections we describe each optimization and provide motivating examples using a small selection of scientific program kernels adapted from the Livermore Kernels and finite-difference algorithms [30]. They contain stencil computations and reductions, techniques commonly used by scientific programmers to solve partial differential equations (PDEs) <ref> [7, 13] </ref>. For clarity we ignore boundary conditions and use constant loop bounds and machine size in the examples, though this is not required by the optimizations. <p> compiler can detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal [29], Id Nouveau [32]), all parallel loops (Al [36], Arf [37], Kali [26]), parallel functions (C*/Dataparallel C [17, 33], Dino [34]), parallel code blocks (Oxygen [35], Pandore [2]), or parallel array operations (CM Fortran <ref> [7] </ref>, Paragon [10]). The Fortran D compiler is similar to Callahan & Kennedy [9] and Superb [15, 38], but applies analysis and optimization up front before code generation, rather than inserting guards and element-wise messages then optimizing via program transformations and partial evaluation.
Reference: [8] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Tor czon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: To evaluate the Fortran D programming model, we are implementing a prototype compiler in the context of the Para-Scope programming environment <ref> [8] </ref>. Previous work described algorithms for partitioning data and computation in the Fortran D compiler, as well as its optimization and validation strategy [21]. Internal representations, program analysis, message vectorization, pipelining, and code generation algorithms were presented elsewhere [20].
Reference: [9] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The first step is partitioning the data and computation across processors. The second is introducing communication to maintain the semantics of the program. A simple compilation technique known as run-time resolution yields code that explicitly calculates the ownership and communication for each reference at run time <ref> [9, 32, 38] </ref>, but resulting programs are likely to execute significantly slower than the original sequential code. p 4 p 2 p 4 p 2 p 4 p 2 DECOMPOSITION D (N,N) REAL A (N,N) ALIGN A (I,J) with D (J-2,I+3) DISTRIBUTE D (:,BLOCK) DISTRIBUTE D (CYCLIC,:) ffl Fortran D Program <p> Alignment and distribution statements are used to calculate the array section owned by each processor. 3) Partition computation The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes values of data it owns <ref> [9, 32, 38] </ref>. The left-hand side (lhs) of each assignment statement in a loop nest is used to calculate the set of loop iterations that cause a processor to assign to local data. <p> The Fortran D compiler is similar to Callahan & Kennedy <ref> [9] </ref> and Superb [15, 38], but applies analysis and optimization up front before code generation, rather than inserting guards and element-wise messages then optimizing via program transformations and partial evaluation.
Reference: [10] <author> C. Chase, A. Cheung, A. Reeves, and M. Smith. </author> <title> Paragon: A parallel programming environment for scientific applications using communication structures. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal [29], Id Nouveau [32]), all parallel loops (Al [36], Arf [37], Kali [26]), parallel functions (C*/Dataparallel C [17, 33], Dino [34]), parallel code blocks (Oxygen [35], Pandore [2]), or parallel array operations (CM Fortran [7], Paragon <ref> [10] </ref>). The Fortran D compiler is similar to Callahan & Kennedy [9] and Superb [15, 38], but applies analysis and optimization up front before code generation, rather than inserting guards and element-wise messages then optimizing via program transformations and partial evaluation.
Reference: [11] <author> S. Chatterjee, G. Blelloch, and M. Zagha. </author> <title> Scan primitives for vector computers. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Scans are similar but perform parallel-prefix operations instead. A sum scan would return the sums of all the prefixes of an array. Scans may be used to solve a number of computations in scientific codes, including linear recurrences and tridiagonal systems <ref> [11, 27] </ref>. The Fortran D compiler applies dependence analysis to recognize reductions and scans. If the reduction or scan accesses data in a manner that sequentializes computation across processors, the Fortran D compiler may parallelize it by relaxing the "owner computes" rule and providing methods to combine partial results. <p> do j = 1, 25 X (i,j) = F 1 (X (i,j),X (i-1,j),A (i),B (i)) redistribute-row-to-col (X) do j = 2, 100 X1 (i,j) = F 2 (X (i,j),X (i,j-1),A (i),B (i)) redistribute-col-to-row (X1) end preceding processors are combined locally and used as a basis for computing local prefix sums <ref> [11] </ref>. Dynamic Data Decomposition Other computations contain parallelism, but are partitioned by the "owner computes" rule in a way that causes sequential execution. In these cases dynamic data decomposition may be used to temporarily change the ownership of data during program execution, exposing parallelism by internalizing cross-processor dependences [3].
Reference: [12] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kre mer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Both irregular and dynamic data decomposition are supported. Some sample data alignment and distributions are shown in Figure 1, the complete language is described in detail elsewhere <ref> [12] </ref>. 3 Fortran D Compiler There are two major steps in compiling Fortran D for MIMD distributed-memory machines. The first step is partitioning the data and computation across processors. The second is introducing communication to maintain the semantics of the program.
Reference: [13] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: In the following sections we describe each optimization and provide motivating examples using a small selection of scientific program kernels adapted from the Livermore Kernels and finite-difference algorithms [30]. They contain stencil computations and reductions, techniques commonly used by scientific programmers to solve partial differential equations (PDEs) <ref> [7, 13] </ref>. For clarity we ignore boundary conditions and use constant loop bounds and machine size in the examples, though this is not required by the optimizations. <p> Partitioning computation Most scientific applications are completely parallel in either a synchronous or loosely synchronous manner <ref> [13] </ref>. If this can be determined by the compiler, partitioning the computation using the "owner computes" rule yields a fully parallel program. To successfully exploit parallelism in these basic cases, the compiler must be able to intelligently partition the work at compile-time.
Reference: [14] <author> E. Gabber, A. Averbuch, and A. Yehudai. </author> <title> Experience with a portable parallelizing Pascal compiler. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Crystal and Id Nouveau identify parallelism automatically and vectorize messages using the single assignment semantics of their high-level functional languages. Crystal pioneered the strategy of identifying collective communication opportunities. Aspar [22] and P 3 C <ref> [14] </ref> extract communication from parallel loops and rely on portable run-time libraries to support collective communication and reductions. CM Fortran extracts communications from array operations and handles reductions expressed as array intrinsics. Dino programs can be significantly improved through iteration reordering and pipelining [31].
Reference: [15] <author> M. Gerndt. </author> <title> Updating distributed variables in local compu tations. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: For RSDs representing array elements contiguous to the local array section, the compiler reserves storage using overlaps created by extending the local array bounds <ref> [15] </ref>. <p> It first calculates commlevel, the level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence. This determines the outermost loop where element messages resulting from the same array reference may be legally combined <ref> [3, 15] </ref>. Vectorized nonlocal accesses are represented as RSDs and stored at the loop at commlevel. They eventually generate messages at loop headers for loop-carried RSDs and in the loop body for loop-independent RSDs. <p> The Fortran D compiler is similar to Callahan & Kennedy [9] and Superb <ref> [15, 38] </ref>, but applies analysis and optimization up front before code generation, rather than inserting guards and element-wise messages then optimizing via program transformations and partial evaluation. Most distributed-memory compilers reduce communication overhead by extracting communication out of user-specified parallel regions (e.g., loops, code blocks, array operations, procedures).
Reference: [16] <author> M. Gupta and P. Banerjee. </author> <title> Compile-time estimation of com munication costs on multicomputers. </title> <booktitle> In Proceedings of the 6th International Parallel Processing Symposium, </booktitle> <address> Beverly Hills, CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Dino programs can be significantly improved through iteration reordering and pipelining [31]. Id Nouveau applies message pipelining and recognizes reductions. Kali performs iteration reordering for individual parallel loops and suggests using array transposition for ADI integration. Gupta & Banerjee estimate collective communication and pipelining costs in Parafrase-2 <ref> [16] </ref>. 9 Conclusions A usable yet efficient machine-independent parallel programming model is needed to make large-scale parallel machines useful for scientific programmers. We believe that Fortran D, one of the first data-placement languages, can provide such a portable data-parallel programming model.
Reference: [17] <author> P. Hatcher, M. Quinn, A. Lapadula, B. Seevers, R. Ander son, and R. Jones. </author> <title> Data-parallel programming on MIMD computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 377-383, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: By using dependence analysis, the Fortran D compiler can detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal [29], Id Nouveau [32]), all parallel loops (Al [36], Arf [37], Kali [26]), parallel functions (C*/Dataparallel C <ref> [17, 33] </ref>, Dino [34]), parallel code blocks (Oxygen [35], Pandore [2]), or parallel array operations (CM Fortran [7], Paragon [10]).
Reference: [18] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of inter procedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Regular section descriptors (RSDs) are built for the sections of data to be communicated. They compactly represent rectangular array sections and their higher dimension analogs <ref> [18] </ref>. 6) Manage storage The compiler collects the extent and type of nonlocal data accesses represented by RSDs to calculate the storage required for nonlocal data.
Reference: [19] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 [6]. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs <ref> [4, 19, 23] </ref>. Accurate static estimates of communication and computation are also needed by the compiler to calculate block sizes for coarse-grain pipelining. Dynamic data decomposition The previous sections show how parallel computation time can be estimated for pipelined computations. <p> Ongoing work to provide environmental support for automatic data decomposition and static performance estimation will also enhance the usefulness of the Fortran D compiler <ref> [3, 4, 19, 23] </ref>. 10 Acknowledgements The authors wish to thank Mary Hall, Uli Kremer, and Kathryn M c Kinley for their assistance on this paper, as well as Joel Saltz, Robert Schnabel and Robert Weaver for several enlightening discussions.
Reference: [20] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler opti mizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Previous work described algorithms for partitioning data and computation in the Fortran D compiler, as well as its optimization and validation strategy [21]. Internal representations, program analysis, message vectorization, pipelining, and code generation algorithms were presented elsewhere <ref> [20] </ref>. The principal contribution of this paper is to introduce, classify, and evaluate (both empirically and analytically) new and existing compiler optimizations in a unified framework. The rest of this paper briefly reviews the Fortran D language and compiler before presenting each optimization, followed by empirical and analytical evaluations. <p> We present a brief overview of the Fortran D compilation algorithm below. The details are discussed elsewhere <ref> [20, 21] </ref>: 1) Analyze program The Fortran D compiler performs scalar dataflow analysis, symbolic analysis, and dependence testing to determine the type and level of all data dependences [25]. 2) Partition data The compiler analyzes Fortran D data decomposition specifications to determine the decomposition of each array in a program. <p> Calls to copy routines are inserted during code generation to pack noncontiguous data into message buffers, but is not needed for this example since the data is contiguous. Finally, explicit send and recv statements are placed at loops headers to communicate nonlocal data <ref> [20] </ref>. Message aggregation Message coalescing ensures that each data value is sent to a processor only once. In comparison, message aggregation ensures that only one message is sent to each processor, possibly at the expense of extra buffering. <p> To successfully exploit parallelism in these basic cases, the compiler must be able to intelligently partition the work at compile-time. The Fortran D compiler achieves this through loop bounds reduction and guard introduction <ref> [20, 21] </ref>. An exception to the "owner computes" rule must be made for private variables, scalars or arrays that are only defined and used in the same loop iteration. Since private variables are usually replicated, naive compilation would cause their computation to be performed on all processors. <p> Since private variables are usually replicated, naive compilation would cause their computation to be performed on all processors. The Fortran D compiler needs to recognize these private variables and partition their computation based on where their values are used <ref> [20] </ref>. Compile-time partitioning of parallel computations is key to any reasonable compilation system, and should not really be considered an optimization. Cross-processor dependences point out sequential components of the computation that cross processor boundaries. <p> The degree of pipeline parallelism depends on how soon each processor is able to begin work after its predecessor starts. The Fortran D compiler can distinguish pipelined computations from fully parallel computations by discovering cross-processor loops|loops that cause computation wave-fronts to sweep across processor boundaries <ref> [20] </ref>. The compiler finds cross-processor loops as follows. First, it considers all pairs of array references that cause loop-carried true dependences. If non-identical subscript expressions occur in a distributed dimension of the array, all loop index variables appearing in the subscript expressions belong to cross-processor loops.
Reference: [21] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: To evaluate the Fortran D programming model, we are implementing a prototype compiler in the context of the Para-Scope programming environment [8]. Previous work described algorithms for partitioning data and computation in the Fortran D compiler, as well as its optimization and validation strategy <ref> [21] </ref>. Internal representations, program analysis, message vectorization, pipelining, and code generation algorithms were presented elsewhere [20]. The principal contribution of this paper is to introduce, classify, and evaluate (both empirically and analytically) new and existing compiler optimizations in a unified framework. <p> We present a brief overview of the Fortran D compilation algorithm below. The details are discussed elsewhere <ref> [20, 21] </ref>: 1) Analyze program The Fortran D compiler performs scalar dataflow analysis, symbolic analysis, and dependence testing to determine the type and level of all data dependences [25]. 2) Partition data The compiler analyzes Fortran D data decomposition specifications to determine the decomposition of each array in a program. <p> To successfully exploit parallelism in these basic cases, the compiler must be able to intelligently partition the work at compile-time. The Fortran D compiler achieves this through loop bounds reduction and guard introduction <ref> [20, 21] </ref>. An exception to the "owner computes" rule must be made for private variables, scalars or arrays that are only defined and used in the same loop iteration. Since private variables are usually replicated, naive compilation would cause their computation to be performed on all processors.
Reference: [22] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An au tomatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Crystal and Id Nouveau identify parallelism automatically and vectorize messages using the single assignment semantics of their high-level functional languages. Crystal pioneered the strategy of identifying collective communication opportunities. Aspar <ref> [22] </ref> and P 3 C [14] extract communication from parallel loops and rely on portable run-time libraries to support collective communication and reductions. CM Fortran extracts communications from array operations and handles reductions expressed as array intrinsics. Dino programs can be significantly improved through iteration reordering and pipelining [31].
Reference: [23] <author> K. Kennedy and U. Kremer. </author> <title> Automatic data alignment and distribution for loosely synchronous problems in an interactive programming environment. </title> <type> Technical Report TR91-155, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 [6]. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs <ref> [4, 19, 23] </ref>. Accurate static estimates of communication and computation are also needed by the compiler to calculate block sizes for coarse-grain pipelining. Dynamic data decomposition The previous sections show how parallel computation time can be estimated for pipelined computations. <p> Ongoing work to provide environmental support for automatic data decomposition and static performance estimation will also enhance the usefulness of the Fortran D compiler <ref> [3, 4, 19, 23] </ref>. 10 Acknowledgements The authors wish to thank Mary Hall, Uli Kremer, and Kathryn M c Kinley for their assistance on this paper, as well as Joel Saltz, Robert Schnabel and Robert Weaver for several enlightening discussions.
Reference: [24] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: This allows communication for B iterations to be vectorized at the j loop. The legality of loop interchange and strip-mine is determined exactly as for shared-memory programs [1, 25, 28]. The Fortran D compiler first permutes loops in memory order to exploit data locality on individual processors <ref> [24] </ref>, then applies coarse-grain pipelining to adjust the degree of pipeline parallelism. 4.4 Reducing Storage Most optimizations increase the amount of temporary storage required by the program.
Reference: [25] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: We present a brief overview of the Fortran D compilation algorithm below. The details are discussed elsewhere [20, 21]: 1) Analyze program The Fortran D compiler performs scalar dataflow analysis, symbolic analysis, and dependence testing to determine the type and level of all data dependences <ref> [25] </ref>. 2) Partition data The compiler analyzes Fortran D data decomposition specifications to determine the decomposition of each array in a program. <p> In comparison, coarse-grain pipelining strip-mines the k loop by a factor B, then interchanges the itera-tor loop kk outside the j loop. This allows communication for B iterations to be vectorized at the j loop. The legality of loop interchange and strip-mine is determined exactly as for shared-memory programs <ref> [1, 25, 28] </ref>. The Fortran D compiler first permutes loops in memory order to exploit data locality on individual processors [24], then applies coarse-grain pipelining to adjust the degree of pipeline parallelism. 4.4 Reducing Storage Most optimizations increase the amount of temporary storage required by the program.
Reference: [26] <author> C. Koelbel and P. Mehrotra. </author> <title> Programming data parallel algorithms on distributed memory machines using Kali. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Where this is not the case, iteration reordering may be applied to change the order of program execution, subject to dependence constraints. This allows loop iterations accessing only local data to be separated and placed between send and recv statements to hide T transit <ref> [26] </ref>. We demonstrate how the Fortran D compiler finds local loop iterations for the Jacobi algorithm in Figure 7. First, communication analysis calculates [2:99,0] and [2:99,26] to represent nonlocal accesses to array B. These accesses are caused by the references B (j,i-1) and B (j,i+1). <p> By applying dynamic data decomposition using collective communication routines to change the array decomposition after each phase, the Fortran D compiler can internalize the computation wavefront in both phases, allowing processors to execute in parallel without communication <ref> [26] </ref>. However, dynamic data decomposition is only applicable when there are full dimensions of parallelism available in the computation. For instance, it cannot be used to exploit parallelism for SOR or Livermore 23 in Figure 12, because the computation wavefront crosses both spatial dimensions. <p> By using dependence analysis, the Fortran D compiler can detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal [29], Id Nouveau [32]), all parallel loops (Al [36], Arf [37], Kali <ref> [26] </ref>), parallel functions (C*/Dataparallel C [17, 33], Dino [34]), parallel code blocks (Oxygen [35], Pandore [2]), or parallel array operations (CM Fortran [7], Paragon [10]).
Reference: [27] <author> P. Kogge and H. Stone. </author> <title> A parallel algorithm for the efficient solution of a general class of recurrence equations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-22(8):786-793, </volume> <month> August </month> <year> 1973. </year>
Reference-contexts: Scans are similar but perform parallel-prefix operations instead. A sum scan would return the sums of all the prefixes of an array. Scans may be used to solve a number of computations in scientific codes, including linear recurrences and tridiagonal systems <ref> [11, 27] </ref>. The Fortran D compiler applies dependence analysis to recognize reductions and scans. If the reduction or scan accesses data in a manner that sequentializes computation across processors, the Fortran D compiler may parallelize it by relaxing the "owner computes" rule and providing methods to combine partial results.
Reference: [28] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: For instance, the Intel iPSC/860 requires approximately 95 sec to send one byte versus .4 sec for each additional byte [6]. The following optimizations seek to reduce T start by combining or eliminating messages. Message vectorization Message vectorization uses the results of data dependence analysis <ref> [1, 28] </ref> to combine element messages into vectors. It first calculates commlevel, the level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence. This determines the outermost loop where element messages resulting from the same array reference may be legally combined [3, 15]. <p> In comparison, coarse-grain pipelining strip-mines the k loop by a factor B, then interchanges the itera-tor loop kk outside the j loop. This allows communication for B iterations to be vectorized at the j loop. The legality of loop interchange and strip-mine is determined exactly as for shared-memory programs <ref> [1, 25, 28] </ref>. The Fortran D compiler first permutes loops in memory order to exploit data locality on individual processors [24], then applies coarse-grain pipelining to adjust the degree of pipeline parallelism. 4.4 Reducing Storage Most optimizations increase the amount of temporary storage required by the program.
Reference: [29] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient pro grams for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Messages are later inserted in front of the loop accessing ZA. Collective communication Message overhead can also be reduced by utilizing fast collective communication, such as broadcast, all-to-all, or transpose, instead of generating individual messages <ref> [6, 29] </ref>. Collective communication opportunities are recognized by comparing the subscript expression of each distributed dimension in the rhs with the aligned dimension in the lhs reference. <p> By using dependence analysis, the Fortran D compiler can detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal <ref> [29] </ref>, Id Nouveau [32]), all parallel loops (Al [36], Arf [37], Kali [26]), parallel functions (C*/Dataparallel C [17, 33], Dino [34]), parallel code blocks (Oxygen [35], Pandore [2]), or parallel array operations (CM Fortran [7], Paragon [10]).
Reference: [30] <author> F. McMahon. </author> <title> The Livermore Fortran Kernels: A com puter test of the numerical performance range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1986. </year>
Reference-contexts: Figure 3 lists Fortran D compiler optimizations in each category. In the following sections we describe each optimization and provide motivating examples using a small selection of scientific program kernels adapted from the Livermore Kernels and finite-difference algorithms <ref> [30] </ref>. They contain stencil computations and reductions, techniques commonly used by scientific programmers to solve partial differential equations (PDEs) [7, 13]. For clarity we ignore boundary conditions and use constant loop bounds and machine size in the examples, though this is not required by the optimizations.
Reference: [31] <author> D. Olander and R. Schnabel. </author> <title> Preliminary experience in de veloping a parallel thin-layer Navier Stokes code and implications for parallel language design. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Aspar [22] and P 3 C [14] extract communication from parallel loops and rely on portable run-time libraries to support collective communication and reductions. CM Fortran extracts communications from array operations and handles reductions expressed as array intrinsics. Dino programs can be significantly improved through iteration reordering and pipelining <ref> [31] </ref>. Id Nouveau applies message pipelining and recognizes reductions. Kali performs iteration reordering for individual parallel loops and suggests using array transposition for ADI integration.
Reference: [32] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The first step is partitioning the data and computation across processors. The second is introducing communication to maintain the semantics of the program. A simple compilation technique known as run-time resolution yields code that explicitly calculates the ownership and communication for each reference at run time <ref> [9, 32, 38] </ref>, but resulting programs are likely to execute significantly slower than the original sequential code. p 4 p 2 p 4 p 2 p 4 p 2 DECOMPOSITION D (N,N) REAL A (N,N) ALIGN A (I,J) with D (J-2,I+3) DISTRIBUTE D (:,BLOCK) DISTRIBUTE D (CYCLIC,:) ffl Fortran D Program <p> Alignment and distribution statements are used to calculate the array section owned by each processor. 3) Partition computation The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes values of data it owns <ref> [9, 32, 38] </ref>. The left-hand side (lhs) of each assignment statement in a loop nest is used to calculate the set of loop iterations that cause a processor to assign to local data. <p> The same optimizations can also hide T copy , the message copy time, by using nonblocking messages. Message pipelining Message pipelining inserts a send for each nonlocal reference as soon as it is defined <ref> [32] </ref>. The recv is placed immediately before the value is used. Any computation performed between the definition and use of the value can then help hide T transit . Unfortunately, message pipelining prevents optimizations such as message vec-torization, resulting in significantly greater total communication cost. <p> By using dependence analysis, the Fortran D compiler can detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal [29], Id Nouveau <ref> [32] </ref>), all parallel loops (Al [36], Arf [37], Kali [26]), parallel functions (C*/Dataparallel C [17, 33], Dino [34]), parallel code blocks (Oxygen [35], Pandore [2]), or parallel array operations (CM Fortran [7], Paragon [10]).
Reference: [33] <author> J. Rose and G. Steele, Jr. </author> <title> C fl : An extended C language for data parallel programming. </title> <editor> In L. Kartashev and S. Karta-shev, editors, </editor> <booktitle> Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> Santa Clara, CA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: By using dependence analysis, the Fortran D compiler can detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal [29], Id Nouveau [32]), all parallel loops (Al [36], Arf [37], Kali [26]), parallel functions (C*/Dataparallel C <ref> [17, 33] </ref>, Dino [34]), parallel code blocks (Oxygen [35], Pandore [2]), or parallel array operations (CM Fortran [7], Paragon [10]).
Reference: [34] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: By using dependence analysis, the Fortran D compiler can detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal [29], Id Nouveau [32]), all parallel loops (Al [36], Arf [37], Kali [26]), parallel functions (C*/Dataparallel C [17, 33], Dino <ref> [34] </ref>), parallel code blocks (Oxygen [35], Pandore [2]), or parallel array operations (CM Fortran [7], Paragon [10]).
Reference: [35] <author> R. Ruhl and M. Annaratone. </author> <title> Parallelization of fortran code on distributed-memory parallel processors. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: By using dependence analysis, the Fortran D compiler can detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal [29], Id Nouveau [32]), all parallel loops (Al [36], Arf [37], Kali [26]), parallel functions (C*/Dataparallel C [17, 33], Dino [34]), parallel code blocks (Oxygen <ref> [35] </ref>, Pandore [2]), or parallel array operations (CM Fortran [7], Paragon [10]).
Reference: [36] <author> P.-S. Tseng. </author> <title> A parallelizing compiler for distributed mem ory parallel computers. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: By using dependence analysis, the Fortran D compiler can detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal [29], Id Nouveau [32]), all parallel loops (Al <ref> [36] </ref>, Arf [37], Kali [26]), parallel functions (C*/Dataparallel C [17, 33], Dino [34]), parallel code blocks (Oxygen [35], Pandore [2]), or parallel array operations (CM Fortran [7], Paragon [10]).
Reference: [37] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: By using dependence analysis, the Fortran D compiler can detect and exploit parallelism automatically, without requiring the user to specify single assignment (Crystal [29], Id Nouveau [32]), all parallel loops (Al [36], Arf <ref> [37] </ref>, Kali [26]), parallel functions (C*/Dataparallel C [17, 33], Dino [34]), parallel code blocks (Oxygen [35], Pandore [2]), or parallel array operations (CM Fortran [7], Paragon [10]).
Reference: [38] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: The first step is partitioning the data and computation across processors. The second is introducing communication to maintain the semantics of the program. A simple compilation technique known as run-time resolution yields code that explicitly calculates the ownership and communication for each reference at run time <ref> [9, 32, 38] </ref>, but resulting programs are likely to execute significantly slower than the original sequential code. p 4 p 2 p 4 p 2 p 4 p 2 DECOMPOSITION D (N,N) REAL A (N,N) ALIGN A (I,J) with D (J-2,I+3) DISTRIBUTE D (:,BLOCK) DISTRIBUTE D (CYCLIC,:) ffl Fortran D Program <p> Alignment and distribution statements are used to calculate the array section owned by each processor. 3) Partition computation The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes values of data it owns <ref> [9, 32, 38] </ref>. The left-hand side (lhs) of each assignment statement in a loop nest is used to calculate the set of loop iterations that cause a processor to assign to local data. <p> The Fortran D compiler is similar to Callahan & Kennedy [9] and Superb <ref> [15, 38] </ref>, but applies analysis and optimization up front before code generation, rather than inserting guards and element-wise messages then optimizing via program transformations and partial evaluation. Most distributed-memory compilers reduce communication overhead by extracting communication out of user-specified parallel regions (e.g., loops, code blocks, array operations, procedures).
References-found: 38

