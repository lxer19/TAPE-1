URL: http://www.cs.cmu.edu/~yiming/papers.yy/aij98.ps
Refering-URL: http://www.cs.cmu.edu/~yiming/publications.html
Root-URL: 
Email: fyiming,jgc,ralf,refg@cs.cmu.edu  
Title: Translingual Information Retrieval: Learning from Bilingual Corpora (AI Journal special issue: Best of IJCAI-97)  
Author: Yiming Yang, Jaime G. Carbonell, Ralf D. Brown, Robert E. Frederking 
Keyword: Information retrieval Translingual or Cross-Language IR Statistical learning Corpus-based methods Generalized Vector Space Model  
Address: 5000 Forbes Avenue, Pittsburgh, PA 15213 USA  
Affiliation: Language Technologies Institute, School of Computer Science Carnegie Mellon University  
Abstract: Translingual information retrieval (TLIR) consists of providing a query in one language and searching document collections in one or more different languages. This paper introduces new TLIR methods and reports on comparative TLIR experiments with these new methods and with previously reported ones in a realistic setting. Methods fall into two categories: query translation and statistical-IR approaches establishing translingual associations. The results show that using bilingual corpora for automated extraction of term equivalences in context outperforms dictionary-based methods. Translingual versions of the Generalized Vector Space Model (GVSM) and Latent Semantic Indexing (LSI) also perform well, as does translingual pseudo relevance feedback (PRF) and Example-Based Term-in-context Translation (EBT). All showed relatively small performance loss between monolingual and translingual versions, ranging between 87% to 101% of monolingual IR performance. Query translation based on a general machine-readable bilingual dictionary heretofore the most popular method did not match the performance of other, more sophisticated methods. Also, the previous very high LSI results in the literature based on "mate-finding"were superseded by more realistic relevance-based evaluations; LSI performance proved comparable to that of other statistical corpus-based methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Lisa Ballesteros and Bruce Croft. </author> <title> Phrasal translation and query expansion techniques for cross-language information retrieval. </title> <booktitle> In 20th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'97), </booktitle> <pages> pages 85-91, </pages> <year> 1997. </year>
Reference-contexts: 1 Introduction Translingual information retrieval (TLIR) has begun to receive considerable attention in recent years with the increased accessibility of ever-more-diverse on-line international text collections, including centrally the World Wide Web. In spite of recent TLIR work <ref> [20, 12, 16, 23, 1, 24, 9] </ref>, evaluations of different TLIR techniques on realistic retrieval tasks are rare. This paper reports our evaluation of the results of both newly developed TLIR techniques and re-implementations of previously reported techniques. <p> We investigated three approaches: 1. Dictionary-based Term Translation Lookup each query term in a general-purpose bilingual dictionary, and use all its possible translations. This is a form of query expansion upon translation. Other forms of dictionary-based query translation methods have been reported before <ref> [8, 16, 1] </ref>, and our results reported in section 5 are consistent with the dictionary-translation literature. 2. Corpus-based Term Translation Use a sentence-aligned bilingual training corpus to find the terms that co-occur in context across languages, thus creating a corpus-based term-equivalence matrix. <p> Our primary interest in PRF is to effectively cross the language barrier in translingual retrieval. Adapting PRF (and RF) to translingual retrieval is natural if a bilingual corpus is available <ref> [7, 1] </ref>. That is, once the top-ranking documents are retrieved for a query in the source language, their translation mates (the corresponding documents in the target language) can be used to form the query in the target language. Figure 1 illustrates the data flow for translingual RF and PRF.
Reference: [2] <author> Ralf D. Brown. </author> <title> Automatically-Extracted Thesauri for Cross-Language IR: When Better is Worse. </title> <booktitle> In Proceedings of the First Workshop on Computational Terminology (COMPUTERM'98), </booktitle> <month> August </month> <year> 1998. </year>
Reference-contexts: EBT, in contrast, performed much better at AVGP=0.49, slightly better than ML-VSM. The two major reasons for the improvement of EBT over DICT and GLOSS are term frequency information and context-specific term translation (including an inherent query expansion described further in <ref> [2] </ref>), both derived automatically from the bilingual corpus.
Reference: [3] <author> R.D. Brown. </author> <title> Example-Based Machine Translation in the Pangloss System. </title> <booktitle> In Proceedings of the Sixteenth International Conference on Computation Linguistics, </booktitle> <pages> pages 169-174, </pages> <year> 1996. </year>
Reference-contexts: Only general-purpose dictionary translation (called DICT or GLOSS below) and corpus-based term translation (called EBT below, for Example-Based Term translation) are further described. All three MT-based methods used variations of the Pangloss Example-Based Machine Translation engine (PanEBMT) <ref> [3] </ref>. In general, EBMT systems [3, 18] use a large corpus of example pairs of previously translated sentences in order to find close matches and translations of words and phrases in context. <p> Only general-purpose dictionary translation (called DICT or GLOSS below) and corpus-based term translation (called EBT below, for Example-Based Term translation) are further described. All three MT-based methods used variations of the Pangloss Example-Based Machine Translation engine (PanEBMT) [3]. In general, EBMT systems <ref> [3, 18] </ref> use a large corpus of example pairs of previously translated sentences in order to find close matches and translations of words and phrases in context. <p> Should a term in one language co-occur with several terms in the other language with sufficient frequency to pass the conditional probability threshold, all are stored as candidate translations. The corpus-based term translation techniques are discussed in greater detail in <ref> [4, 3] </ref>. This method has the nice property that adjusting the filtering thresholds allows us to tune a tradeoff: stricter thresholds prevent spurious translations, but significantly reduce the possible translations; more lenient thresholds produce better yields, at the cost of allowing more spurious translations.
Reference: [4] <author> R.D. Brown. </author> <title> Automated Dictionary Extraction for "Knowledge-Free" Example-Based Translation. </title> <booktitle> In Proceedings of the Seventh International Conference on Theoretical and Methodological Issues in Machine Translation, </booktitle> <year> 1997. </year>
Reference-contexts: Instead, we developed the term-in-corpus-context translation method. 2.1 Example-based Term Translation (EBT) In order to create domain-specific or corpus-specific bilingual dictionaries automatically, we start from a large sentence-aligned bilingual corpus and generate a large thresholded term co-occurrence table <ref> [4] </ref>. The result was used as the dictionary for corpus-based (example-based) term substitution. Co-occurrence dictionary generation is performed in two phases: First the co-occurrence matrix (indexed by source-language words on one axis and target-language words on the other) is generated. <p> Should a term in one language co-occur with several terms in the other language with sufficient frequency to pass the conditional probability threshold, all are stored as candidate translations. The corpus-based term translation techniques are discussed in greater detail in <ref> [4, 3] </ref>. This method has the nice property that adjusting the filtering thresholds allows us to tune a tradeoff: stricter thresholds prevent spurious translations, but significantly reduce the possible translations; more lenient thresholds produce better yields, at the cost of allowing more spurious translations.
Reference: [5] <author> C. Buckley, G. Salton, J. Allan, and A. Singhal. </author> <title> Automatic query expansion using smart: </title> <booktitle> Trec 3. In Overview of the Third Text REtrieval Conference (TREC-3), </booktitle> <pages> pages 69-80, </pages> <year> 1995. </year> <month> 18 </month>
Reference: [6] <author> J. G. Carbonell. </author> <title> New Approaches to Machine Translation. </title> <booktitle> In Proceedings of the conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages, </booktitle> <address> Hamilton, NY, </address> <year> 1985. </year>
Reference-contexts: Let us consider the pros and cons of each approach: * Translation Accuracy Both human and machine translation <ref> [6, 19] </ref> require context to achieve accuracy. Translating isolated words in a query is unreliable, largely due to unresolved lexical ambiguity.
Reference: [7] <author> J.G. Carbonell, Y. Yang, R.E. Frederking, R. Brown, Y. Geng, and D. Lee. </author> <title> Translingual information retrieval: A comparative evaluation. </title> <booktitle> In Proceedings of IJCAI-97, </booktitle> <address> Nagoya, Japan, </address> <year> 1997. </year> <note> (Distinguished paper award). </note>
Reference-contexts: Our primary interest in PRF is to effectively cross the language barrier in translingual retrieval. Adapting PRF (and RF) to translingual retrieval is natural if a bilingual corpus is available <ref> [7, 1] </ref>. That is, once the top-ranking documents are retrieved for a query in the source language, their translation mates (the corresponding documents in the target language) can be used to form the query in the target language. Figure 1 illustrates the data flow for translingual RF and PRF. <p> Given these methods, empirical validation is important. For monolingual retrieval, performance improvement of GVSM over VSM was observed on small collections [27]; sometimes, improvement of LSI over VSM was observed, but not always [10]. Until the work described in this paper and its previous version <ref> [7] </ref>, a comparison between GVSM and LSI in either monolingual or translingual retrieval has not been made. 4 Corpus and Query Preparations In order to conduct an empirical evaluation, our first task was to prepare a bilingual corpus for translin-gual experimentation. <p> We also conducted a different evaluation without using human relevance judgments of the same methods, instead relying on the degree of overlap between documents retrieved monolingually and their translation-mates retrieved translingually. Such an evaluation, as reported in our previous paper <ref> [7] </ref>, may not be as informative, but is helpful when human relevance judgements are not available. 3 We expanded the SMART Spanish stop word list so that its coverage is equivalent to the English one, resulting in a somewhat longer list because of irregular inflections for Spanish auxiliary verbs. 11 For <p> If the user were willing to provide true relevance judgements, full relevance feedback should exhibit higher absolute performance, both for monolingual and translingual retrieval. The early experiments reported in <ref> [7, 29] </ref> used the entire test set of 1121 documents, rather than dividing into validation and blind-test subsets reported in Table 1 and described in the previous paragraph. The results of the earlier experiments were similar, but slightly lower overall.
Reference: [8] <author> M. Davis and T. Dunning. </author> <title> A trec evaluation of query translation methods for multi-lingual text retrieval. </title> <booktitle> In The 4th Text Retrieval Conference (TREC-4), </booktitle> <year> 1996. </year>
Reference-contexts: We investigated three approaches: 1. Dictionary-based Term Translation Lookup each query term in a general-purpose bilingual dictionary, and use all its possible translations. This is a form of query expansion upon translation. Other forms of dictionary-based query translation methods have been reported before <ref> [8, 16, 1] </ref>, and our results reported in section 5 are consistent with the dictionary-translation literature. 2. Corpus-based Term Translation Use a sentence-aligned bilingual training corpus to find the terms that co-occur in context across languages, thus creating a corpus-based term-equivalence matrix.
Reference: [9] <author> Mark Davis and William Ogden. Quilt: </author> <title> Implementing a large-scale cross-language text retrieval system. </title> <booktitle> In 20th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'97), </booktitle> <pages> pages 92-98, </pages> <year> 1997. </year>
Reference-contexts: 1 Introduction Translingual information retrieval (TLIR) has begun to receive considerable attention in recent years with the increased accessibility of ever-more-diverse on-line international text collections, including centrally the World Wide Web. In spite of recent TLIR work <ref> [20, 12, 16, 23, 1, 24, 9] </ref>, evaluations of different TLIR techniques on realistic retrieval tasks are rare. This paper reports our evaluation of the results of both newly developed TLIR techniques and re-implementations of previously reported techniques.
Reference: [10] <author> S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> In J Amer Soc Inf Sci 1, </journal> <volume> 6, </volume> <pages> pages 391-407, </pages> <year> 1990. </year>
Reference-contexts: total vocabulary by less than 2000 words, the additional vocabulary consists primarily of inflected forms of the most frequent words. 3 IR-based Methods for TLIR We extended three monolingual retrieval methods to translingual retrieval: Pseudo-Relevance Feedback (PRF)[5], the Generalized Vector Space Model (GVSM)[27], and the Latent Semantic Indexing (LSI) approach <ref> [10] </ref>. In each case, a translingual semantic correspondence between queries and documents is established based on a document-aligned bilingual training corpus, without requiring bilingual dictionaries or machine translation. <p> Fortunately, we found it possible to significantly reduce this complexity by aggressively removing non-influential elements from the transformed document vectors without sacrificing retrieval performance, as shown in our previous work [28] and in the empirical results of this study (Section 5.4). 3.3 Latent Semantic Indexing Latent Semantic Indexing <ref> [10] </ref> (LSI) is a one-step extension of GVSM. <p> Which model best represents the semantic space of documents and queries is a scientifically challenging question. Given these methods, empirical validation is important. For monolingual retrieval, performance improvement of GVSM over VSM was observed on small collections [27]; sometimes, improvement of LSI over VSM was observed, but not always <ref> [10] </ref>.
Reference: [11] <editor> Ed. DK Harman. </editor> <booktitle> Overview of the Third Text REtrieval Conference (TREC-3). </booktitle> <institution> US Government Printing Office, </institution> <address> Washington, DC, </address> <year> 1995. </year>
Reference: [12] <author> S.T. Dumais, </author> <title> T.K. Landauer, and M.L. Littman. Automatic cross-linguistic information retrieval using latent semantic indexing. </title> <booktitle> In SIGIR'96 Workshop On Cross-Linguistic Information Retrieval, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction Translingual information retrieval (TLIR) has begun to receive considerable attention in recent years with the increased accessibility of ever-more-diverse on-line international text collections, including centrally the World Wide Web. In spite of recent TLIR work <ref> [20, 12, 16, 23, 1, 24, 9] </ref>, evaluations of different TLIR techniques on realistic retrieval tasks are rare. This paper reports our evaluation of the results of both newly developed TLIR techniques and re-implementations of previously reported techniques. <p> Hence for both this reason and the above, document translation is preferable in principle. In fact, preliminary findings by Dumais et al <ref> [12] </ref> support this line of reasoning. * Practicality Many document collections are very large. Most are searched remotely. Some are proprietary; individual documents may be read or down-loaded, but the entire collection may not be copied or translated. <p> The dimensions in U are linear combinations of documents, while the dimensions in A are individual documents. The translingual LSI model <ref> [12] </ref> is similar to the model for monolingual LSI, except that a bilingual document corpus is needed for training instead of a monolingual corpus. <p> At least one occurrence of each word is output, even if the proportion rounds to zero. 5.5 Mate finding LSI was first extended from MLIR to TLIR at Bellcore <ref> [17, 12] </ref>, including the "fold-in" process mentioned earlier. However, the evaluation was unorthodox due to their lack of a bilingual corpus with queries and relevance judgements (such as the UNICEF corpus we prepared).
Reference: [13] <author> R. Frederking, S. Nirenburg, D. Farwell, S. Helmreich, E. Hovy, K. Knight, S. Beale, C. Domashnev, D. Attardo, D. Grannes, and R. Brown. </author> <title> Integrating Translations from Multiple Sources within the Pangloss Mark III Machine Translation System. </title> <booktitle> In Proceedings of the first conference of the Association for Machine Translation in the Americas, </booktitle> <address> Columbia, MD, </address> <year> 1994. </year>
Reference-contexts: The down side of the dictionary inversion is that most inflected English words are not found in the dictionary at all. 2.3 Manual Glossary (GLOSS) In addition to the Spanish-English Collins dictionary, we also had hand-built glossaries from the Pan-gloss project available <ref> [13] </ref>. As with the Collins dictionary, these were created for Spanish-to-English translation, so we inverted the glossaries and extracted the single-word English entries. The extracted entries were then added to the inverted Collins dictionary to form the translation dictionary used as the basis of the GLOSS method.
Reference: [14] <author> David Graff and Rebecca Finch. </author> <title> Multilingual Text Resources at the Linguistic Data Consortium. </title> <booktitle> In Proceedings of the 1994 ARPA Human Language Technology Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: In general, EBMT systems [3, 18] use a large corpus of example pairs of previously translated sentences in order to find close matches and translations of words and phrases in context. The PanEBMT parallel corpus was derived primarily from the Spanish and English portions of the UN Multilingual Corpus <ref> [14] </ref>, with an admixture of texts from the Pan-American Health Organization and ARPA MT evaluations. The total corpus contains some 685,000 sentence pairs about 250 megabytes after duplicated Spanish sentences have been removed. <p> Three separate training corpora were used to generate corpus-based thesauri: the full 250 megabytes of aligned Spanish-English text available to PanEBMT (consisting almost entirely of text from the UN Multilingual Corpus <ref> [14] </ref>), a 33-megabyte contiguous subset thereof, and a 12-megabyte corpus consisting of the training texts from our experimental corpus (described in Section 4) and the non-UN portions of the PanEBMT corpus. <p> The large UN Multilingual Corpus (about 500 megabytes of data per language) <ref> [14] </ref> from the Linguistic Data Consortium was available to us, but the original UN corpus is a heterogeneous mixture of many types of documents.
Reference: [15] <author> W. Hersh, C. Buckley, T.J. Leone, and D. Hickman. Ohsumed: </author> <title> an interactive retrieval evaluation and new large text collection for research. </title> <booktitle> In 17th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'94), </booktitle> <pages> pages 192-201, </pages> <year> 1994. </year>
Reference-contexts: Both positive and negative evidence was found in empirical studies with respect to the effect of PRF on retrieval accuracy <ref> [15, 25] </ref>. As discussed in section 5, we also found PRF cuts both ways, depending somewhat on how the queries were formulated originally. Our primary interest in PRF is to effectively cross the language barrier in translingual retrieval.
Reference: [16] <author> D.A. Hull and G. Grefenstette. </author> <title> Querying across languages: a dictionary-based approach to multilingual information retrieval. </title> <booktitle> In 19th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'96), </booktitle> <pages> pages 49-57, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Translingual information retrieval (TLIR) has begun to receive considerable attention in recent years with the increased accessibility of ever-more-diverse on-line international text collections, including centrally the World Wide Web. In spite of recent TLIR work <ref> [20, 12, 16, 23, 1, 24, 9] </ref>, evaluations of different TLIR techniques on realistic retrieval tasks are rare. This paper reports our evaluation of the results of both newly developed TLIR techniques and re-implementations of previously reported techniques. <p> We investigated three approaches: 1. Dictionary-based Term Translation Lookup each query term in a general-purpose bilingual dictionary, and use all its possible translations. This is a form of query expansion upon translation. Other forms of dictionary-based query translation methods have been reported before <ref> [8, 16, 1] </ref>, and our results reported in section 5 are consistent with the dictionary-translation literature. 2. Corpus-based Term Translation Use a sentence-aligned bilingual training corpus to find the terms that co-occur in context across languages, thus creating a corpus-based term-equivalence matrix.
Reference: [17] <author> T. Landauer and M. Littman. </author> <title> Fully Automatic Cross-Linguage Document Retrieval using Latent Semantic Indexing. </title> <booktitle> In Proceedings of the 6th OED Conference on Text Research, </booktitle> <pages> pages 31-38, </pages> <year> 1990. </year>
Reference-contexts: At least one occurrence of each word is output, even if the proportion rounds to zero. 5.5 Mate finding LSI was first extended from MLIR to TLIR at Bellcore <ref> [17, 12] </ref>, including the "fold-in" process mentioned earlier. However, the evaluation was unorthodox due to their lack of a bilingual corpus with queries and relevance judgements (such as the UNICEF corpus we prepared).
Reference: [18] <author> M. Nagao. </author> <title> A Framework of a Mechanical Translation between Japanese and English by Analogy Principle. </title> <editor> In A. Elithorn and R. Banerji (eds), editors, </editor> <booktitle> Artificial and Human Intelligence. </booktitle> <publisher> NATO Publications, </publisher> <year> 1984. </year>
Reference-contexts: Only general-purpose dictionary translation (called DICT or GLOSS below) and corpus-based term translation (called EBT below, for Example-Based Term translation) are further described. All three MT-based methods used variations of the Pangloss Example-Based Machine Translation engine (PanEBMT) [3]. In general, EBMT systems <ref> [3, 18] </ref> use a large corpus of example pairs of previously translated sentences in order to find close matches and translations of words and phrases in context.
Reference: [19] <author> S. Nirenburg, J. G. Carbonell, M. Tomita, and K. Goodman. </author> <title> Knowledge-Based Machine Translation. </title> <publisher> Morgan Kaufmann Inc, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Let us consider the pros and cons of each approach: * Translation Accuracy Both human and machine translation <ref> [6, 19] </ref> require context to achieve accuracy. Translating isolated words in a query is unreliable, largely due to unresolved lexical ambiguity.
Reference: [20] <author> G. Salton. </author> <title> Automatic Processing of Foreign Language Documents. </title> <journal> Journal of American Society for Information Sciences, </journal> <volume> 21 </volume> <pages> 187-194, </pages> <year> 1970. </year>
Reference-contexts: 1 Introduction Translingual information retrieval (TLIR) has begun to receive considerable attention in recent years with the increased accessibility of ever-more-diverse on-line international text collections, including centrally the World Wide Web. In spite of recent TLIR work <ref> [20, 12, 16, 23, 1, 24, 9] </ref>, evaluations of different TLIR techniques on realistic retrieval tasks are rare. This paper reports our evaluation of the results of both newly developed TLIR techniques and re-implementations of previously reported techniques.
Reference: [21] <author> G. Salton. </author> <title> Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Pennsylvania, </address> <year> 1989. </year>
Reference-contexts: All these methods, PRF, GVSM and LSI, are variants of the vector space model (VSM) which was initially developed by Salton and is a fundamental paradigm in monolingual text retrieval <ref> [21] </ref>. To allow clear theoretical comparison of these IR-based methods, let us define the notation for VSM. <p> A term is typically weighted by T F fl IDF , i.e., the product of within-document term frequency (TF) and the Inverted Document Frequency (IDF) of the term <ref> [21] </ref>. 3.1 Pseudo-Relevance Feedback Pseudo-relevance feedback (PRF) (aka "local feedback") is a variation of the classic relevance feedback (RF)[22]. Relevance feedback is a query expansion technique which adds terms in the relevant documents found in a initial retrieval to the query, and uses the expanded query for further retrieval. <p> The 11-point average precision is the interpolated average of precision values when thresholding at recall levels of 0%, 10%, ..., 100%. For further details of the interpolated averaging method, refer to <ref> [21] </ref>. For brevity we refer simply to "average precision" or AVGP. In the monolingual retrieval experiments, we optimized each method with respect to its performance on the UNICEF corpus using the human relevance judgements on the 30 queries and the 550 validation documents. <p> The parameter values for each method which produced optimal performance in monolingual retrieval were also found optimal or nearly optimal in our translingual experiments (Section5.4). We implemented all the monolingual and translingual methods using components of the publicly-available SMART retrieval engine <ref> [21] </ref>, including indexing, stemming, TF*IDF-based word weighting and stop-word elimination in both languages 3 . This common infrastructure enabled us to factor out extraneous variables from our experiments. For the monolingual VSM baseline, we ran SMART without relevance feedback (SMART.basic). 5.3 Primary results recall/precision curves for TLIR methods.
Reference: [22] <author> G. Salton and C. Buckley. </author> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of American Society for Information Sciences, </journal> <volume> 41 </volume> <pages> 288-297, </pages> <year> 1990. </year>
Reference: [23] <author> P. Sheridan and J.P. Ballerini. </author> <title> Experiments in multilingual information retrieval using the spider system. </title> <booktitle> In 19th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'96), </booktitle> <pages> pages 58-65, </pages> <year> 1996. </year> <month> 19 </month>
Reference-contexts: 1 Introduction Translingual information retrieval (TLIR) has begun to receive considerable attention in recent years with the increased accessibility of ever-more-diverse on-line international text collections, including centrally the World Wide Web. In spite of recent TLIR work <ref> [20, 12, 16, 23, 1, 24, 9] </ref>, evaluations of different TLIR techniques on realistic retrieval tasks are rare. This paper reports our evaluation of the results of both newly developed TLIR techniques and re-implementations of previously reported techniques. <p> Wong et al proposed an alternative, namely the "generalized vector space model" (GVSM) [27], also referred to as "the dual space" <ref> [23] </ref> which uses word combinations (or individual documents) to form the basis instead of individual terms.
Reference: [24] <author> P. Sheridan, M. Wechsler, and P. Schauble. </author> <title> Cross-language speech retrieval: establishing a baseline performance. </title> <booktitle> In 20th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'97), </booktitle> <pages> pages 99-107, </pages> <year> 1997. </year>
Reference-contexts: 1 Introduction Translingual information retrieval (TLIR) has begun to receive considerable attention in recent years with the increased accessibility of ever-more-diverse on-line international text collections, including centrally the World Wide Web. In spite of recent TLIR work <ref> [20, 12, 16, 23, 1, 24, 9] </ref>, evaluations of different TLIR techniques on realistic retrieval tasks are rare. This paper reports our evaluation of the results of both newly developed TLIR techniques and re-implementations of previously reported techniques.
Reference: [25] <author> Padmini Srinivasan. </author> <title> Optimal document-indexing vobabulary for MEDLINE. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 32(5) </volume> <pages> 503-514, </pages> <year> 1996. </year>
Reference-contexts: Both positive and negative evidence was found in empirical studies with respect to the effect of PRF on retrieval accuracy <ref> [15, 25] </ref>. As discussed in section 5, we also found PRF cuts both ways, depending somewhat on how the queries were formulated originally. Our primary interest in PRF is to effectively cross the language barrier in translingual retrieval.
Reference: [26] <author> S.K.M. Wong, W. Ziarko, V.V. Raghavan, and P.C.N. Wong. </author> <booktitle> On modeling of information retrieval concepts in vector space. In ACM Transaction of Database Systems, </booktitle> <volume> number 2, </volume> <pages> pages 299-321, </pages> <year> 1987. </year>
Reference-contexts: Empirical studies showed somewhat better performance of GVSM over conventional VSM when using binary term weighting (a value of one for terms present, and zero for terms absent), while the comparison is inconclusive if more advanced term weighting was used in VSM <ref> [26] </ref>. Comparison of GVSM with PRF and LSI has not been 5 carried out previously either in the monolingual retrieval literature or in the new TLIR literature. Our major focus here is a novel adaptation of GVSM to translingual retrieval.
Reference: [27] <author> S.K.M. Wong, W. Ziarko, and P.C.N. Wong. </author> <title> Generalized vector space model in information retrieval. </title> <booktitle> In ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'85), </booktitle> <pages> pages 18-25, </pages> <year> 1985. </year>
Reference-contexts: Wong et al proposed an alternative, namely the "generalized vector space model" (GVSM) <ref> [27] </ref>, also referred to as "the dual space" [23] which uses word combinations (or individual documents) to form the basis instead of individual terms. <p> Which model best represents the semantic space of documents and queries is a scientifically challenging question. Given these methods, empirical validation is important. For monolingual retrieval, performance improvement of GVSM over VSM was observed on small collections <ref> [27] </ref>; sometimes, improvement of LSI over VSM was observed, but not always [10].
Reference: [28] <author> Y. Yang. </author> <title> Noise reduction in a statistical approach to text categorization. </title> <booktitle> In Proceedings of the 18th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'95), </booktitle> <pages> pages 256-263, </pages> <year> 1995. </year>
Reference-contexts: This can be expensive for very large applications. Fortunately, we found it possible to significantly reduce this complexity by aggressively removing non-influential elements from the transformed document vectors without sacrificing retrieval performance, as shown in our previous work <ref> [28] </ref> and in the empirical results of this study (Section 5.4). 3.3 Latent Semantic Indexing Latent Semantic Indexing [10] (LSI) is a one-step extension of GVSM.
Reference: [29] <author> Y. Yang, R.E. Frederking, J.G. Carbonell, R. Brown, Y. Geng, and D. Lee. </author> <title> Bilingual-corpus based approaches to translingual information retrieval. </title> <booktitle> In Proceedings of MULSAIC-97, </booktitle> <address> Nagoya, Japan, </address> <year> 1997. </year>
Reference-contexts: If the user were willing to provide true relevance judgements, full relevance feedback should exhibit higher absolute performance, both for monolingual and translingual retrieval. The early experiments reported in <ref> [7, 29] </ref> used the entire test set of 1121 documents, rather than dividing into validation and blind-test subsets reported in Table 1 and described in the previous paragraph. The results of the earlier experiments were similar, but slightly lower overall.
Reference: [30] <author> Y. Yang and J.P. Pedersen. </author> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In The Fourteenth International Conference on Machine Learning, </booktitle> <pages> pages 412-420, </pages> <year> 1997. </year> <month> 20 </month>
Reference-contexts: It remains to be seen if a larger dictionary would improve performance, as rare words tend to have a rather small effect on overall performance in related tasks <ref> [30] </ref> (corroborated by our own experience with corpus-derived dictionaries on this task). Due to the way in which our Spanish-English dictionary was built, inverting it provides the benefit of additional query expansion in some cases.
References-found: 30

