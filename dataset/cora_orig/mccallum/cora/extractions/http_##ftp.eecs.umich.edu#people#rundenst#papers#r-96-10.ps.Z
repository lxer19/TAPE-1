URL: http://ftp.eecs.umich.edu/people/rundenst/papers/r-96-10.ps.Z
Refering-URL: http://ftp.eecs.umich.edu/people/rundenst/papers/
Root-URL: http://www.eecs.umich.edu
Email: Email: hibino@eecs.umich.edu  
Title: Extending and Evaluating Visual Information Seeking for Video Data  
Author: Stacie Hibino 
Keyword: Video analysis, dynamic queries, temporal query filters, interactive visualizations.  
Address: 1301 Beal Avenue, Ann Arbor, MI 48109-2122 USA  
Affiliation: EECS Department, Software Systems Research Laboratory The University of Michigan,  
Note: [To appear in CHI96 Conference Companion as a a summary for the CHI96 doctoral consortium.]  
Abstract: Extending and adapting the visual information seeking paradigm for video analysis would empower casual users to explore temporal, spatial, and motion relationships between video objects and events. Several extensions are required to accomplish this: extensions to dynamic queries to specify multiple subsets, customized temporal, spatial, and motion query filters, and the design of new spatio-temporal visualizations to highlight these relationships. In my thesis research, I am working on these extensions by combining a new multimedia visual query language with spatio-temporal visualizations into an integrated MultiMedia Visual Information Seeking (MMVIS) environment. This research summary describes my overall approach, research goals, and evaluation plan. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Ahlberg, C., Williamson, C., & Shneiderman, B. </author> <year> (1992). </year> <title> Dynamic Queries for Information Exploration: An Implementation and Evaluation. </title> <booktitle> CHI'92 Conference Proceedings. </booktitle> <publisher> NY:ACM Press, </publisher> <pages> pp. 619-626). </pages>
Reference-contexts: This approach has been shown to aid users in locating information, as well as for searching for trends and exceptions to trendsand to accomplish such tasks more efficiently than through traditional forms-based methods <ref> [1] </ref>.
Reference: 2. <author> Ahlberg, C., & Shneiderman, B. </author> <year> (1994). </year> <title> Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays. C H I ' 9 4 Conference Proceedings. </title> <publisher> NY:ACM Press, </publisher> <pages> pp. 619-626. </pages>
Reference-contexts: INTRODUCTION Visual Information Seeking (VIS) is a framework for information exploration where users filter data through direct manipulation of dynamic query filters <ref> [2] </ref>. A visualization of the results is dynamically updated as users adjust a query filter, thus allowing them to incrementally specify and refine their queries. In this way, users also see the direct correlation between adjusting parameter values and the corresponding changes in the visualization of results.
Reference: 3. <author> Harrison, B.L., Owen, R., & Baecker, R.M. </author> <year> (1994). </year> <title> Timelines: An Interactive System for the Collection of Visualization of Temporal Data. </title> <booktitle> Proc. of Graphics Interface '94. Canadian Information Processing Society. </booktitle>
Reference-contexts: analysis, users would be empowered to explore various relationships (e.g., temporal relationships such as how often different types of events start or end at the same time) in a way that was not previously possible through other traditional means (e.g., timelines for temporal analysis) or other video analysis approaches (e.g., <ref> [3, 5] </ref>). EXTENDING VIS FOR VIDEO DATA I have identified several extensions to the original VIS framework that are necessary to adapt VIS for the analysis of video data.
Reference: 4. <author> Hibino, S. & Rundensteiner, E. </author> <title> (in press). A Visual Query Language for Temporal Analysis of Video Data. The Design and Implementation of Multimedia Database Systems (K. </title> <editor> Nwosu, Ed.), </editor> <address> NY: </address> <publisher> Kluwer Books. </publisher>
Reference-contexts: In MMVIS, we provide subset query palettes (i.e., duplicate sets of query filters placed on palettes) for selecting multiple subsets. We have designed specialized temporal query filters <ref> [4] </ref>, and have done some preliminary work on spatial and motion query filters. We have focused initial visualization work on temporal visualizations that cluster temporal relationships. The integrated MultiMedia Visual Information Seeking (MMVIS) environment currently supports the features listed above. <p> Researchers can use subset query palettes to select two subsets: A) talking and nonverbal events and B) DRs. They can then explore various relationships between members of these subsets using the specialized relationship query filters. Our temporal query filters form a temporal visual query language (TVQL) <ref> [4] </ref> and are presented to the user on a single palette (see Figure 1, Temporal Query palette). Keeping within the VIS paradigm, the visualization of results are dynamically updated as users specify the subsets as well as the temporal and/or spatial relationships.
Reference: 5. <author> Mackay, W. E. </author> <year> (1989). </year> <title> EVA: An experimental video annotator for symbolic analysis of video data. </title> <journal> SIGCHI Bulletin, </journal> <volume> 21(2), </volume> <pages> 68-71. </pages>
Reference-contexts: analysis, users would be empowered to explore various relationships (e.g., temporal relationships such as how often different types of events start or end at the same time) in a way that was not previously possible through other traditional means (e.g., timelines for temporal analysis) or other video analysis approaches (e.g., <ref> [3, 5] </ref>). EXTENDING VIS FOR VIDEO DATA I have identified several extensions to the original VIS framework that are necessary to adapt VIS for the analysis of video data.
References-found: 5

