URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:flexibility.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~dfk/papers/flexibility.html
Root-URL: http://www.cs.dartmouth.edu
Email: fdfk,nilsg@cs.dartmouth.edu  
Title: Flexibility and Performance of Parallel File Systems  
Author: David Kotz Nils Nieuwejaar 
Date: February 14, 1996  
Address: Hanover, NH 03755  
Affiliation: Department of Computer Science Dartmouth College  
Web: URL ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:flexibility.ps.Z  
Note: Copyright 1996 by David Kotz. ACM Operating Systems Review 30(2), April 1996, pp. 63-73. Available at  
Abstract: Many scientific applications for high-performance multiprocessors have tremendous I/O requirements. As a result, the I/O system is often the limiting factor of application performance. Several new parallel file systems have been developed in recent years, each promising better performance for some class of parallel applications. As we gain experience with parallel computing, and parallel file systems in particular, it becomes increasingly clear that a single solution does not suit all applications. For example, it appears to be impossible to find a single appropriate interface, caching policy, file structure, or disk management strategy. Furthermore, the proliferation of file-system interfaces and abstractions make application portability a significant problem. We propose that the traditional functionality of parallel file systems be separated into two components: a fixed core that is standard on all platforms, encapsulating only primitive abstractions and interfaces, and a set of high-level libraries to provide a variety of abstractions and application-programmer interfaces (APIs). We think of this approach as the "RISC" of parallel file-system design. We present our current and next-generation file systems as examples of this structure. Their features, such as a three-dimensional file structure, strided read and write interfaces, and I/O-node programs, are specifically designed with the flexibility and performance necessary to sup port a wide range of applications.
Abstract-found: 1
Intro-found: 1
Reference: [BP88] <author> Andrea J. Borr and Franco Putzolu. </author> <title> High performance SQL through low-level system integration. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 342-349, </pages> <year> 1988. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [KE93b, KE93a, PGG + 95], two-phase I/O [dBC93], disk-directed I/O [Kot94], compute-node caching [PEK96], chunking [SW94], compression [SW95], filtering <ref> [Kot95, BP88] </ref>, and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems. <p> Incoming data can be filtered in a data-dependent way, passing only the necessary data on to the compute node, saving network bandwidth and compute-node memory <ref> [Kot95, BP88] </ref>. Blocks can be moved directly between I/O nodes, for example, to rearrange blocks between disks during a copy or permutation operation, without passing through compute nodes. Format conversion, compression, and decompression are also possible. <p> Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function [CBZ95, SG90, Gra95]. Some distributed database systems execute part of the SQL query in the server rather than the client, to reduce client-server traffic <ref> [BP88] </ref>. Hatcher and Quinn hint that allowing user code to run on nCUBE I/O nodes would be a good idea [HQ91]. 7 Status Galley runs on the IBM SP-2 and on workstation clusters [NK96a], and has so far been extremely successful [NK96b].
Reference: [BSP + 95] <author> Brian Bershad, Stefan Savage, Przemys law Pardyak, Emin Gun Sirer, Marc E. Fiuczynski, David Becker, Craig Chambers, and Susan Eggers. </author> <title> Extensibility, safety and performance in the SPIN operating system. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 267-284, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: All three of these systems provide the application programmer some control over the parallel file system, primarily by selecting existing policies from the built-in alternatives. Galley2 promotes the use of application-selected code on the I/O nodes. Several operating systems can download user code into the kernel <ref> [Gai72, LCC94, BSP + 95] </ref>. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function [CBZ95, SG90, Gra95].
Reference: [CBH + 94] <author> Alok Choudhary, Rajesh Bordawekar, Michael Harry, Rakesh Krishnaiyer, Ravi Ponnusamy, Tarvinder Singh, and Rajeev Thakur. </author> <title> PASSION: parallel and scalable software for input-output. </title> <type> Technical Report SCCS-636, </type> <institution> ECE Dept., NPAC and CASE Center, Syracuse University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Some languages or libraries would provide a traditional read-write abstraction; others (probably with compiler support) would provide transparent out-of-core data structures; still others may provide persistent objects. Some libraries may be designed for particular application classes like computational chemistry [FN96] or to support a particular language <ref> [CC94, CBH + 94] </ref>. Finally, some compilers and programmers may choose to generate application-specific code using the core interface directly. The concept of I/O libraries is not new; the C stdio library and the C++ iostreams library are common examples, both layered above the "core" kernel interface.
Reference: [CBZ95] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Techniques for reducing consistency-related communication in distributed shared-memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-243, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Several operating systems can download user code into the kernel [Gai72, LCC94, BSP + 95]. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function <ref> [CBZ95, SG90, Gra95] </ref>. Some distributed database systems execute part of the SQL query in the server rather than the client, to reduce client-server traffic [BP88].
Reference: [CC94] <author> Thomas H. Cormen and Alex Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Some languages or libraries would provide a traditional read-write abstraction; others (probably with compiler support) would provide transparent out-of-core data structures; still others may provide persistent objects. Some libraries may be designed for particular application classes like computational chemistry [FN96] or to support a particular language <ref> [CC94, CBH + 94] </ref>. Finally, some compilers and programmers may choose to generate application-specific code using the core interface directly. The concept of I/O libraries is not new; the C stdio library and the C++ iostreams library are common examples, both layered above the "core" kernel interface.
Reference: [CFH + 95] <author> Peter Corbett, Dror Feitelson, Yarson Hsu, Jean-Pierre Prost, Marc Snir, Sam Fineberg, Bill Nitzberg, Bernard Traversat, and Parkson Wong. </author> <title> MPI-IO: a parallel file I/O interface for MPI. </title> <type> Technical Report NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1995. </year> <note> Version 0.3. </note>
Reference-contexts: This diversity of current systems, particularly of the application-programmer's interface (API), also makes it difficult to write portable applications. Nearly every file system mentioned above has its own API. A standard interface is being developed, MPI-IO <ref> [CFH + 95] </ref>, but even that interface is appropriate only for a certain class of applications. 2 Solution We believe that flexibility is needed for performance. An application programmer should be able to choose the interfaces and abstractions that work best for that application.
Reference: [CFP + 95] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, George S. Almasi, Sandra Johnson Baylor, Anthony S. Bolmarcich, Yarsun Hsu, Julian Satran, Marc Snir, Robert Colao, Brian Herr, Joseph Kavaky, Thomas R. Morgan, and Anthony Zlotek. </author> <title> Parallel file systems for the IBM SP computers. </title> <journal> IBM Systems Journal, </journal> <pages> pages 222-248, </pages> <year> 1995. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS [Pie89], nCUBE [DdR92], OSF/PFS [Roy93], sfs [LIN + 93], Vesta/PIOFS <ref> [CFP + 95] </ref>, HFS [KS96], PIOUS [MS94], RAMA [MK95], PPFS [HER + 95], Scotch [GSC + 95], and Galley [NK96a, NK96b]. <p> We are currently porting several application libraries on top of Galley, including a traditional striped-file library, Panda [SCJ + 95], Vesta <ref> [CFP + 95] </ref>, and SOLAR [TG96]. We are also using Galley to investigate policies for managing multi-application workloads. We are building a simulator for Galley2, to evaluate some of the key ideas, and a full implementation, to experiment with real applications.
Reference: [CK93] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dart-mouth Institute for Advanced Graduate Studies. </institution> <note> Revised as Dartmouth PCS-TR93-188 on 9/20/94. </note>
Reference-contexts: It is important that applications be able to choose the interface and policies that work best for them, and for application programmers to have control over I/O <ref> [WGRW93, CK93] </ref>. This diversity of current systems, particularly of the application-programmer's interface (API), also makes it difficult to write portable applications. Nearly every file system mentioned above has its own API.
Reference: [dBC93] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [KE93b, KE93a, PGG + 95], two-phase I/O <ref> [dBC93] </ref>, disk-directed I/O [Kot94], compute-node caching [PEK96], chunking [SW94], compression [SW95], filtering [Kot95, BP88], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems.
Reference: [dC94] <author> Juan Miguel del Rosario and Alok Choudhary. </author> <title> High performance I/O for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Scientific applications are increasingly dependent on multiprocessor computers to satisfy their computational needs. Many scientific applications, however, also use tremendous amounts of data <ref> [dC94] </ref>: input data collected from satellites or seismic experiments, checkpointing output, and visualization output. Worse, some applications manipulate data sets too large to fit in main memory, requiring either explicit or implicit virtual memory support.
Reference: [DdR92] <author> Erik DeBenedictis and Juan Miguel del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Proceedings of the Eleventh Annual IEEE International Phoenix Conference on Computers and Communications, </booktitle> <pages> pages 0117-0124, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes. In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS [Pie89], nCUBE <ref> [DdR92] </ref>, OSF/PFS [Roy93], sfs [LIN + 93], Vesta/PIOFS [CFP + 95], HFS [KS96], PIOUS [MS94], RAMA [MK95], PPFS [HER + 95], Scotch [GSC + 95], and Galley [NK96a, NK96b].
Reference: [Dib90] <author> Peter C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes. In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS <ref> [Dib90] </ref>, CFS [Pie89], nCUBE [DdR92], OSF/PFS [Roy93], sfs [LIN + 93], Vesta/PIOFS [CFP + 95], HFS [KS96], PIOUS [MS94], RAMA [MK95], PPFS [HER + 95], Scotch [GSC + 95], and Galley [NK96a, NK96b].
Reference: [FN96] <author> Ian Foster and Jarek Nieplocha. </author> <title> ChemIO: High-performance I/O for computational chemistry applications. </title> <note> WWW http://www.mcs.anl.gov/chemio/, February 1996. </note>
Reference-contexts: Some languages or libraries would provide a traditional read-write abstraction; others (probably with compiler support) would provide transparent out-of-core data structures; still others may provide persistent objects. Some libraries may be designed for particular application classes like computational chemistry <ref> [FN96] </ref> or to support a particular language [CC94, CBH + 94]. Finally, some compilers and programmers may choose to generate application-specific code using the core interface directly.
Reference: [Gai72] <author> R. Stockton Gaines. </author> <title> An operating system based on the concept of a supervisory computer. </title> <journal> Communications of the ACM, </journal> <volume> 15(3) </volume> <pages> 150-156, </pages> <month> March </month> <year> 1972. </year>
Reference-contexts: All three of these systems provide the application programmer some control over the parallel file system, primarily by selecting existing policies from the built-in alternatives. Galley2 promotes the use of application-selected code on the I/O nodes. Several operating systems can download user code into the kernel <ref> [Gai72, LCC94, BSP + 95] </ref>. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function [CBZ95, SG90, Gra95].
Reference: [GM94] <author> James Gosling and Henry McGilton. </author> <title> The Java language: A white paper. Sun Microsystems, </title> <year> 1994. </year>
Reference-contexts: for requesting I/O to and from buffers? * message-passing: what is the best interface for I/O-node programs to communicate with the compute nodes, and with each other? * What is the appropriate mechanism to support I/O-node programs? We are considering three alternatives: processes, threads within a safe language like Java <ref> [GM94] </ref> or Python 3 , and threads running sandboxed code [WLAG93].
Reference: [Gra95] <author> Robert S. Gray. </author> <title> Agent Tcl: A transportable agent system. </title> <booktitle> In Proceedings of the CIKM Workshop on Intelligent Information Agents, Fourth International Conference on Information and Knowledge Management (CIKM 95), </booktitle> <address> Baltimore, Maryland, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Several operating systems can download user code into the kernel [Gai72, LCC94, BSP + 95]. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function <ref> [CBZ95, SG90, Gra95] </ref>. Some distributed database systems execute part of the SQL query in the server rather than the client, to reduce client-server traffic [BP88].
Reference: [GSC + 95] <author> Garth A. Gibson, Daniel Stodolsky, Pay W. Chang, William V. Courtwright II, Chris G. Demetriou, Eka Ginting, Mark Holland, Qingming Ma, LeAnn Neal, R. Hugo Patterson, Jiawen Su, Rachad Youssef, and Jim Zelenka. </author> <title> The Scotch parallel storage systems. </title> <booktitle> In Proceedings of 40th IEEE Computer Society International Conference (COMPCON 95), </booktitle> <pages> pages 403-410, </pages> <address> San Francisco, </address> <month> Spring </month> <year> 1995. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS [Pie89], nCUBE [DdR92], OSF/PFS [Roy93], sfs [LIN + 93], Vesta/PIOFS [CFP + 95], HFS [KS96], PIOUS [MS94], RAMA [MK95], PPFS [HER + 95], Scotch <ref> [GSC + 95] </ref>, and Galley [NK96a, NK96b]. Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [KE93b, KE93a, PGG + 95], two-phase I/O [dBC93], disk-directed I/O [Kot94], compute-node caching [PEK96], chunking [SW94], compression [SW95], filtering [Kot95, BP88], and so forth.
Reference: [HER + 95] <author> Jay Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumen-thal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS [Pie89], nCUBE [DdR92], OSF/PFS [Roy93], sfs [LIN + 93], Vesta/PIOFS [CFP + 95], HFS [KS96], PIOUS [MS94], RAMA [MK95], PPFS <ref> [HER + 95] </ref>, Scotch [GSC + 95], and Galley [NK96a, NK96b]. <p> Galley permits, but does not enforce, a building-block approach to library design; other approaches are possible. Finally, the Hurricane operating system does not dedicate nodes to I/O, so it is not unusual for application code to run on "I/O" nodes. The Portable Parallel File System (PPFS) <ref> [HER + 95] </ref> is a testbed for experimenting with parallel file-system issues. It includes many alternative policies for declustering, caching, prefetching, and consistency control, and allows application programmers to select appropriate policies for their needs. It also supports user-defined declustering patterns through an upcall function.
Reference: [HQ91] <author> Philip J. Hatcher and Michael J. Quinn. C*-Linda: </author> <title> A programming environment with multiple data-parallel modules and parallel I/O. </title> <booktitle> In Proceedings of the Twenty-Fourth Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 382-389, </pages> <year> 1991. </year>
Reference-contexts: Some distributed database systems execute part of the SQL query in the server rather than the client, to reduce client-server traffic [BP88]. Hatcher and Quinn hint that allowing user code to run on nCUBE I/O nodes would be a good idea <ref> [HQ91] </ref>. 7 Status Galley runs on the IBM SP-2 and on workstation clusters [NK96a], and has so far been extremely successful [NK96b]. We are currently porting several application libraries on top of Galley, including a traditional striped-file library, Panda [SCJ + 95], Vesta [CFP + 95], and SOLAR [TG96].
Reference: [KE93a] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):140-145, January and February 1993. </note>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching <ref> [KE93b, KE93a, PGG + 95] </ref>, two-phase I/O [dBC93], disk-directed I/O [Kot94], compute-node caching [PEK96], chunking [SW94], compression [SW95], filtering [Kot95, BP88], and so forth.
Reference: [KE93b] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching <ref> [KE93b, KE93a, PGG + 95] </ref>, two-phase I/O [dBC93], disk-directed I/O [Kot94], compute-node caching [PEK96], chunking [SW94], compression [SW95], filtering [Kot95, BP88], and so forth.
Reference: [Kot94] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [KE93b, KE93a, PGG + 95], two-phase I/O [dBC93], disk-directed I/O <ref> [Kot94] </ref>, compute-node caching [PEK96], chunking [SW94], compression [SW95], filtering [Kot95, BP88], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems. <p> We refer to all of these choices as "application-selected code." There are many reasons to allow application-selected code on the I/O node. Application-specific optimizations can be applied to I/O-node caching and prefetching. Mechanisms like disk-directed I/O <ref> [Kot94] </ref> can be implemented, using application-specific data-distribution information. File data can be distributed among memories according to a data-dependent mapping function, for example, in applications with a data-dependent decomposition of unstructured data [Kot95].
Reference: [Kot95] <author> David Kotz. </author> <title> Expanding the potential for disk-directed I/O. </title> <booktitle> In Proceedings of the 1995 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 490-495, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [KE93b, KE93a, PGG + 95], two-phase I/O [dBC93], disk-directed I/O [Kot94], compute-node caching [PEK96], chunking [SW94], compression [SW95], filtering <ref> [Kot95, BP88] </ref>, and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems. <p> Application-specific optimizations can be applied to I/O-node caching and prefetching. Mechanisms like disk-directed I/O [Kot94] can be implemented, using application-specific data-distribution information. File data can be distributed among memories according to a data-dependent mapping function, for example, in applications with a data-dependent decomposition of unstructured data <ref> [Kot95] </ref>. Incoming data can be filtered in a data-dependent way, passing only the necessary data on to the compute node, saving network bandwidth and compute-node memory [Kot95, BP88]. <p> Incoming data can be filtered in a data-dependent way, passing only the necessary data on to the compute node, saving network bandwidth and compute-node memory <ref> [Kot95, BP88] </ref>. Blocks can be moved directly between I/O nodes, for example, to rearrange blocks between disks during a copy or permutation operation, without passing through compute nodes. Format conversion, compression, and decompression are also possible.
Reference: [Kot96] <author> David Kotz. </author> <title> Introduction to multiprocessor I/O architecture. </title> <editor> In Ravi Jain, John Werth, and J. C. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <month> January </month> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Fortunately, it is now possible to configure most parallel systems with sufficient I/O hardware <ref> [Kot96] </ref>. Most of today's parallel computers interconnect tens or hundreds of processor nodes, each of which has a processor and memory, with a high-speed network. Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes.
Reference: [KS96] <author> Orran Krieger and Michael Stumm. </author> <title> HFS: A performance-oriented flexible file system based on building-block compositions. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 95-108, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS [Pie89], nCUBE [DdR92], OSF/PFS [Roy93], sfs [LIN + 93], Vesta/PIOFS [CFP + 95], HFS <ref> [KS96] </ref>, PIOUS [MS94], RAMA [MK95], PPFS [HER + 95], Scotch [GSC + 95], and Galley [NK96a, NK96b]. <p> Indeed, it seems that no one interface or structure will be appropriate for all parallel applications; for maximum performance, flexibility of the underlying system is critical <ref> [KS96] </ref>. It is important that applications be able to choose the interface and policies that work best for them, and for application programmers to have control over I/O [WGRW93, CK93]. This diversity of current systems, particularly of the application-programmer's interface (API), also makes it difficult to write portable applications. <p> The tricky part might be dynamic linking of sandboxed code. 3. what is the overhead? 6 Related work The Hurricane File System (HFS) <ref> [KS96] </ref>, a parallel file system for the Hector multiprocessor, is also designed with the philosophy that flexibility is critical for performance. Indeed, their results clearly demonstrate the tremendous performance impact of choosing the right file structure and 3 http://www.python.org/ management policies for the application's access pattern.
Reference: [LCC94] <author> Chao Hsien Lee, Meng Chang Chen, and Ruei Chuan Chang. </author> <title> HiPEC: High performance external virtual memory caching. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 153-164, </pages> <year> 1994. </year>
Reference-contexts: All three of these systems provide the application programmer some control over the parallel file system, primarily by selecting existing policies from the built-in alternatives. Galley2 promotes the use of application-selected code on the I/O nodes. Several operating systems can download user code into the kernel <ref> [Gai72, LCC94, BSP + 95] </ref>. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function [CBZ95, SG90, Gra95].
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes. In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS [Pie89], nCUBE [DdR92], OSF/PFS [Roy93], sfs <ref> [LIN + 93] </ref>, Vesta/PIOFS [CFP + 95], HFS [KS96], PIOUS [MS94], RAMA [MK95], PPFS [HER + 95], Scotch [GSC + 95], and Galley [NK96a, NK96b].
Reference: [MK95] <author> Ethan L. Miller and Randy H. Katz. </author> <title> RAMA: Easy access to a high-bandwidth massively parallel file system. </title> <booktitle> In Proceedings of the 1995 Winter USENIX Conference, </booktitle> <pages> pages 59-70, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS [Pie89], nCUBE [DdR92], OSF/PFS [Roy93], sfs [LIN + 93], Vesta/PIOFS [CFP + 95], HFS [KS96], PIOUS [MS94], RAMA <ref> [MK95] </ref>, PPFS [HER + 95], Scotch [GSC + 95], and Galley [NK96a, NK96b].
Reference: [MPI94] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <address> 1.0 edition, </address> <month> May 5 </month> <year> 1994. </year> <note> http://www.mcs.anl.gov/Projects/mpi/standard.html. </note>
Reference-contexts: While the implementation of the core is platform dependent, and provided by the platform vendor, its interface is standard across all platforms. This approach has proven successful with the MPI message-passing standard <ref> [MPI94] </ref>. Application programmers may then choose from a variety of different languages and libraries, 1 We avoid the term "kernel," as the core may be comprised of user-level libraries, server daemons, and kernel code. to select one that best fits the application's needs.
Reference: [MS94] <author> Steven A. Moyer and V. S. Sunderam. </author> <title> PIOUS: a scalable parallel I/O system for distributed computing environments. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 71-78, </pages> <year> 1994. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS [Pie89], nCUBE [DdR92], OSF/PFS [Roy93], sfs [LIN + 93], Vesta/PIOFS [CFP + 95], HFS [KS96], PIOUS <ref> [MS94] </ref>, RAMA [MK95], PPFS [HER + 95], Scotch [GSC + 95], and Galley [NK96a, NK96b].
Reference: [NK96a] <author> Nils Nieuwejaar and David Kotz. </author> <title> The Galley parallel file system. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <month> May </month> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS [Pie89], nCUBE [DdR92], OSF/PFS [Roy93], sfs [LIN + 93], Vesta/PIOFS [CFP + 95], HFS [KS96], PIOUS [MS94], RAMA [MK95], PPFS [HER + 95], Scotch [GSC + 95], and Galley <ref> [NK96a, NK96b] </ref>. Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [KE93b, KE93a, PGG + 95], two-phase I/O [dBC93], disk-directed I/O [Kot94], compute-node caching [PEK96], chunking [SW94], compression [SW95], filtering [Kot95, BP88], and so forth. <p> In the second, with the tentative name Galley2, we go a step further and allow user code to run on the I/O nodes. The next two sections discuss each file system in more detail. 3 The Galley Parallel File System Our current parallel file system, Galley <ref> [NK96a, NK96b] </ref>, looks like Figure 1b. A more detailed picture is shown in Figure 2. The core file system includes servers that run on the I/O nodes and a tiny interface library that runs on the compute nodes. The I/O-node servers manage file-system metadata, I/O-node caching, and disk scheduling. <p> More information about Galley is available on the WWW 2 and in forthcoming papers <ref> [NK96a, NK96b] </ref>. 4 The Galley2 Parallel File System Our next-generation file system, which we so far call "Galley2" for lack of a better name, goes beyond Galley to allow application control over I/O-node activities. <p> Hatcher and Quinn hint that allowing user code to run on nCUBE I/O nodes would be a good idea [HQ91]. 7 Status Galley runs on the IBM SP-2 and on workstation clusters <ref> [NK96a] </ref>, and has so far been extremely successful [NK96b]. We are currently porting several application libraries on top of Galley, including a traditional striped-file library, Panda [SCJ + 95], Vesta [CFP + 95], and SOLAR [TG96]. We are also using Galley to investigate policies for managing multi-application workloads.
Reference: [NK96b] <author> Nils Nieuwejaar and David Kotz. </author> <title> Performance of the Galley parallel file system. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 83-94, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS [Pie89], nCUBE [DdR92], OSF/PFS [Roy93], sfs [LIN + 93], Vesta/PIOFS [CFP + 95], HFS [KS96], PIOUS [MS94], RAMA [MK95], PPFS [HER + 95], Scotch [GSC + 95], and Galley <ref> [NK96a, NK96b] </ref>. Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [KE93b, KE93a, PGG + 95], two-phase I/O [dBC93], disk-directed I/O [Kot94], compute-node caching [PEK96], chunking [SW94], compression [SW95], filtering [Kot95, BP88], and so forth. <p> In the second, with the tentative name Galley2, we go a step further and allow user code to run on the I/O nodes. The next two sections discuss each file system in more detail. 3 The Galley Parallel File System Our current parallel file system, Galley <ref> [NK96a, NK96b] </ref>, looks like Figure 1b. A more detailed picture is shown in Figure 2. The core file system includes servers that run on the I/O nodes and a tiny interface library that runs on the compute nodes. The I/O-node servers manage file-system metadata, I/O-node caching, and disk scheduling. <p> To allow application libraries to support these patterns efficiently, the Galley interface supports both structured (e.g., strided and nested strided) and unstructured read and write requests. This interface leads to dramatically better performance <ref> [NK96b] </ref>. "core" file system that attempts to serve all applications. In our Galley File System, we shrink the core to leave the API and many of the parallel features to an application-selectable library. <p> More information about Galley is available on the WWW 2 and in forthcoming papers <ref> [NK96a, NK96b] </ref>. 4 The Galley2 Parallel File System Our next-generation file system, which we so far call "Galley2" for lack of a better name, goes beyond Galley to allow application control over I/O-node activities. <p> Hatcher and Quinn hint that allowing user code to run on nCUBE I/O nodes would be a good idea [HQ91]. 7 Status Galley runs on the IBM SP-2 and on workstation clusters [NK96a], and has so far been extremely successful <ref> [NK96b] </ref>. We are currently porting several application libraries on top of Galley, including a traditional striped-file library, Panda [SCJ + 95], Vesta [CFP + 95], and SOLAR [TG96]. We are also using Galley to investigate policies for managing multi-application workloads.
Reference: [NKP + 95] <author> Nils Nieuwejaar, David Kotz, Apratim Purakayastha, Carla Schlatter Ellis, and Michael Best. </author> <title> File-access characteristics of parallel scientific workloads. </title> <type> Technical Report PCS-TR95-263, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1995. </year> <note> Submitted to IEEE TPDS. </note>
Reference-contexts: The structure of parallel files, beyond the fact that they are collections of local files, is completely determined by library code. Multiple applications wishing to use the same parallel files must maintain a mutually agreed structure, by convention. In an extensive characterization of parallel scientific applications <ref> [NKP + 95] </ref>, we found that many applications access files in small pieces, typically in a regular "strided" pattern. To allow application libraries to support these patterns efficiently, the Galley interface supports both structured (e.g., strided and nested strided) and unstructured read and write requests.
Reference: [PAB + 95] <author> Calton Pu, Tito Autrey, Andrew Black, Charles Consel, Crispin Cowan, Jon Inouye, Lakshmi Kethana, Jonathan Walpole, and Ke Zhang. </author> <title> Optimistic incremental specialization: Streamlining a commercial operating system. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 314-324, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Again, we expect most applications to choose from pre-defined libraries, but we also encourage use of application-specific code written by application programmers, generated automatically by compilers, or generated at run time <ref> [PAB + 95] </ref>. We refer to all of these choices as "application-selected code." There are many reasons to allow application-selected code on the I/O node. Application-specific optimizations can be applied to I/O-node caching and prefetching. Mechanisms like disk-directed I/O [Kot94] can be implemented, using application-specific data-distribution information.
Reference: [PEK96] <author> Apratim Purakayastha, Carla Schlatter Ellis, and David Kotz. </author> <title> ENWRICH: a compute-processor write caching scheme for parallel file systems. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 55-68, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [KE93b, KE93a, PGG + 95], two-phase I/O [dBC93], disk-directed I/O [Kot94], compute-node caching <ref> [PEK96] </ref>, chunking [SW94], compression [SW95], filtering [Kot95, BP88], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems.
Reference: [PGG + 95] <author> R. Hugo Patterson, Garth A. Gibson, Eka Ginting, Daniel Stodolsky, and Jim Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-95, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching <ref> [KE93b, KE93a, PGG + 95] </ref>, two-phase I/O [dBC93], disk-directed I/O [Kot94], compute-node caching [PEK96], chunking [SW94], compression [SW95], filtering [Kot95, BP88], and so forth. <p> Unlike Galley, however, there is no clearly defined lower-level interface to which programmers may write new high-level libraries. Unlike Galley2, it does not allow application-selected code (beyond that already included in PPFS) to execute on the I/O nodes. In the Transparent Informed Prefetching (TIP) system <ref> [PGG + 95] </ref> an application provides a set of hints about its future accesses to the file system. The file system uses these hints to make intelligent caching and prefetching decisions. While this technique can lead to better performance through better prefetching, it only affects prefetching and caching behavior.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160. </pages> <publisher> Golden Gate Enterprises, </publisher> <address> Los Altos, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes. In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS <ref> [Pie89] </ref>, nCUBE [DdR92], OSF/PFS [Roy93], sfs [LIN + 93], Vesta/PIOFS [CFP + 95], HFS [KS96], PIOUS [MS94], RAMA [MK95], PPFS [HER + 95], Scotch [GSC + 95], and Galley [NK96a, NK96b].
Reference: [Roy93] <author> Paul J. Roy. </author> <title> Unix file access and caching in a multicomputer environment. </title> <booktitle> In Proceedings of the Usenix Mach III Symposium, </booktitle> <pages> pages 21-37, </pages> <year> 1993. </year>
Reference-contexts: Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes. In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [Dib90], CFS [Pie89], nCUBE [DdR92], OSF/PFS <ref> [Roy93] </ref>, sfs [LIN + 93], Vesta/PIOFS [CFP + 95], HFS [KS96], PIOUS [MS94], RAMA [MK95], PPFS [HER + 95], Scotch [GSC + 95], and Galley [NK96a, NK96b].
Reference: [SCJ + 95] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: We are currently porting several application libraries on top of Galley, including a traditional striped-file library, Panda <ref> [SCJ + 95] </ref>, Vesta [CFP + 95], and SOLAR [TG96]. We are also using Galley to investigate policies for managing multi-application workloads. We are building a simulator for Galley2, to evaluate some of the key ideas, and a full implementation, to experiment with real applications.
Reference: [SG90] <author> James W. Stamos and David K. Gifford. </author> <title> Remote execution. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(4) </volume> <pages> 537-565, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Several operating systems can download user code into the kernel [Gai72, LCC94, BSP + 95]. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function <ref> [CBZ95, SG90, Gra95] </ref>. Some distributed database systems execute part of the SQL query in the server rather than the client, to reduce client-server traffic [BP88].
Reference: [SW94] <author> K. E. Seamons and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array I/O. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [KE93b, KE93a, PGG + 95], two-phase I/O [dBC93], disk-directed I/O [Kot94], compute-node caching [PEK96], chunking <ref> [SW94] </ref>, compression [SW95], filtering [Kot95, BP88], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems.
Reference: [SW95] <author> K. E. Seamons and M. Winslett. </author> <title> A data management approach for handling large compressed arrays in high performance computing. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 119-128, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [KE93b, KE93a, PGG + 95], two-phase I/O [dBC93], disk-directed I/O [Kot94], compute-node caching [PEK96], chunking [SW94], compression <ref> [SW95] </ref>, filtering [Kot95, BP88], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems.
Reference: [TG96] <author> Sivan Toledo and Fred G. Gustavson. </author> <title> The design and implementation of SOLAR, a portable library for scalable out-of-core linear algebra computations. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 28-40, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: We are currently porting several application libraries on top of Galley, including a traditional striped-file library, Panda [SCJ + 95], Vesta [CFP + 95], and SOLAR <ref> [TG96] </ref>. We are also using Galley to investigate policies for managing multi-application workloads. We are building a simulator for Galley2, to evaluate some of the key ideas, and a full implementation, to experiment with real applications.
Reference: [WGRW93] <author> David Womble, David Greenberg, Rolf Riesen, and Stephen Wheat. </author> <title> Out of core, out of mind: Practical parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-16, </pages> <institution> Mississippi State University, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: It is important that applications be able to choose the interface and policies that work best for them, and for application programmers to have control over I/O <ref> [WGRW93, CK93] </ref>. This diversity of current systems, particularly of the application-programmer's interface (API), also makes it difficult to write portable applications. Nearly every file system mentioned above has its own API.
Reference: [WLAG93] <author> Robert Wahbe, Steven Lucco, Thomas E. Anderson, and Susan L. Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 203-216, </pages> <year> 1993. </year>
Reference-contexts: is the best interface for I/O-node programs to communicate with the compute nodes, and with each other? * What is the appropriate mechanism to support I/O-node programs? We are considering three alternatives: processes, threads within a safe language like Java [GM94] or Python 3 , and threads running sandboxed code <ref> [WLAG93] </ref>.
References-found: 45

