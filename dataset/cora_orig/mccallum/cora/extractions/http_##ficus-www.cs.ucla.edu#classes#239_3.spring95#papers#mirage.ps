URL: http://ficus-www.cs.ucla.edu/classes/239_3.spring95/papers/mirage.ps
Refering-URL: http://ficus-www.cs.ucla.edu/project-members/reiher/CS239_spring96.html
Root-URL: http://www.cs.ucla.edu
Title: Mirage A Kernel Implementation of Distributed Shared Memory on a Network of Personal Computers  Error corrected  
Author: Brett D. Fleisch Randall L. Hyde Niels Christian Juul 
Address: Riverside CA 92521-0304  Riverside  Riverside  Denmark  
Affiliation: Department of Computer Science University of California  Department of Computer Science, University of California,  Department of Computer Science, University of California,  Department of Computer Science, University of Copenhagen,  University of Copenhagen.  
Note: version:  This research is sponsored by a Joint Study with IBM Corporation, NSF CDA09209405.  Work done during PostDoc employment with UCR, also supported by the Danish Natural Science Research Council. Released simultaneously as DIKU-rapport 94/13 by  
Pubnum: UCR-CS-94-2  
Email: (brett@cs.ucr.edu).  (rhyde@cs.ucr.edu).  (ncjuul@diku.dk).  
Date: (Original released: April 1994)  June 1994  
Abstract-found: 0
Intro-found: 1
Reference: [Accetta 86] <author> M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> Mach: A new kernel foundation for UNIX development. </title> <booktitle> In Proceedings of the Summer 1986 USENIX Conference, USENIX, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: Nevertheless, loosely coupled distributed systems provide the potential to scale, economically, to very large configurations using commodity hardware. In the past, operating system designers have exploited the similarity between network packets and messages in the design of loosely coupled distributed systems <ref> [Accetta 86] </ref>. However, some researchers have observed that the message passing approach may not be well suited for tightly coupled processors that access shared memory [Li 86b, Li 90, Bisiani 90, Bennett 90b, Ramachandran 88, Fleisch 89b].
Reference: [Bell 89a] <author> T. Bell, J. Cleary, and I. Witten. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, USA, </address> <year> 1989. </year>
Reference: [Bell 89b] <author> T. Bell, I. H. Witten, and J. G. Cleary. </author> <title> Modeling for text compression. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(4) </volume> <pages> 557-589, </pages> <month> December </month> <year> 1989. </year>
Reference: [Bennett 89] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Shared Memory for Distributed Memory Multiprocessors. </title> <type> Technical Report COMP TR89-91, </type> <institution> Rice University, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: A time window mechanism is employed to avoid possible thrashing, to facilitate performance tuning, and to provide a locking mechanism. Our performance optimizations, as described in this paper, are not pursued in IVY. 6.2 Munin Munin is a distributed shared memory system developed at Rice University <ref> [Bennett 89, Carter 93] </ref>. Munin is distinct in that it uses type-specific coherence mechanisms. These mechanisms are part of the run-time system and permit annotations from the user to specify the coherence mechanism to be used for each object.
Reference: [Bennett 90a] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The authors based their design decisions on the results of a study of sharing and synchronization behavior in a variety of shared memory parallel programs <ref> [Bennett 90a] </ref>. The approach used in Munin is the antithesis of what we are doing in Mirage + . The Mirage + experiment is to provide transparency to the System V IPC application designer concerning DSM application design.
Reference: [Bennett 90b] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the 1990 Conf. Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168-176, </pages> <publisher> ACM Press, </publisher> <address> New York, NY, USA, </address> <year> 1990. </year>
Reference: [Bisiani 90] <author> Roberto Bisiani and Mosur Ravishankar. </author> <title> PLUS: A Distributed Shared-Memory System. </title> <type> Technical Report, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1990. </year>
Reference: [Brinch Hansen 73] <author> Per Brinch Hansen. </author> <booktitle> Operating Systems Principles. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, USA, </address> <month> July </month> <year> 1973. </year>
Reference-contexts: A coherent implementation is one in which a store to an address is always visible to all subsequent load operations of the same address, independent of the machine location where the load occurs. Higher level synchronization primitives, such as semaphores or monitors <ref> [Brinch Hansen 73, Dijkstra 72, Hoare 74] </ref> may be used by applications that require additional consistency guarantees. For example, we use the UNIX System V semaphore interface in many of our (distributed) applications. Our past work on Mirage focused on a time-based locking approach to DSM.
Reference: [Carter 89] <author> John B. Carter and Willy Zwaenepoel. </author> <title> Optimistic implementation of bulk data transfer protocols. </title> <booktitle> In International Conference on Measurement and Modeling of Computer Systems, Proceedings in: Performance Evaluation Review Volume 17(1), </booktitle> <pages> pages 61-6, </pages> <address> Berkeley, CA, USA, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Vertical optimization of the protocol layers is a well proven technique to achieve better performance [Schroeder 90, Tanenbaum 92]. Our design constraints were, however, to build the DSM system on top of the communication layers without spending considerable time optimizing communication subsystems written by others. With high-level packet blasting <ref> [Zwaenepoel 85, Carter 89] </ref>, we expected the total time for a remote page fault would improve significantly. The savings using high-level packet blasting instead of explicitly handshaking each packet, is not only due to the removal of explicit acknowledgments, but also due to increased parallelism during communication.
Reference: [Carter 93] <author> John B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, Texas, USA, </institution> <month> September </month> <year> 1993. </year> <month> 24 </month>
Reference-contexts: A time window mechanism is employed to avoid possible thrashing, to facilitate performance tuning, and to provide a locking mechanism. Our performance optimizations, as described in this paper, are not pursued in IVY. 6.2 Munin Munin is a distributed shared memory system developed at Rice University <ref> [Bennett 89, Carter 93] </ref>. Munin is distinct in that it uses type-specific coherence mechanisms. These mechanisms are part of the run-time system and permit annotations from the user to specify the coherence mechanism to be used for each object.
Reference: [Daley 68] <author> R. C. Daley and J. B. Dennis. </author> <title> Virtual memory processes and sharing in Multics. </title> <journal> Com--munications of the ACM, </journal> <volume> 11(5) </volume> <pages> 306-311, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: The TCF portion of the AIX product has since been discontinued. 2 2 The Mirage Model of Distributed Shared Memory The new DSM system, like Mirage [Fleisch 89b], uses a paged segmentation scheme <ref> [Daley 68, Den-ning 70] </ref>. In our model, processes create shared memory segments by specifying the size of the segment, name, and access protection. Processes locate and attach segments into their virtual memory address space by name.
Reference: [Denning 70] <author> Peter J. Denning. </author> <title> Virtual memory. </title> <journal> ACM Computing Surveys, </journal> <volume> 2(3) </volume> <pages> 153-189, </pages> <month> September </month> <year> 1970. </year>
Reference: [Dijkstra 72] <author> E. W. Dijkstra. </author> <title> Hierarchical Ordering of Sequential Processes. </title> <publisher> Academic Press, </publisher> <address> New York, NY, USA, </address> <year> 1972. </year>
Reference-contexts: A coherent implementation is one in which a store to an address is always visible to all subsequent load operations of the same address, independent of the machine location where the load occurs. Higher level synchronization primitives, such as semaphores or monitors <ref> [Brinch Hansen 73, Dijkstra 72, Hoare 74] </ref> may be used by applications that require additional consistency guarantees. For example, we use the UNIX System V semaphore interface in many of our (distributed) applications. Our past work on Mirage focused on a time-based locking approach to DSM.
Reference: [Fleisch 89a] <author> Brett D. Fleisch. </author> <title> Distributed Shared Memory in a Loosely Coupled Environment. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of California, </institution> <address> Los Angeles, CA, USA, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: In this paper, we focus on performance issues related to DSM page size and the underlying DSM support structure. This work is based on our previous DSM system called Mirage <ref> [Fleisch 89b, Fleisch 89a] </ref>. Mirage is a DSM facility implemented entirely in the kernel of a UNIX-based operating system [Popek 81, Walker 83]. Here we examine our work to adapt Mirage for personal computers with a larger page size.
Reference: [Fleisch 89b] <author> Brett D. Fleisch and Gerald J. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, published in Operating Systems Review 23(5) Special Issue, </booktitle> <pages> pages 211-223, </pages> <publisher> ACM SIGOPS, ACM Press, </publisher> <address> The Wigwam, Litchfield Park, Arizona, USA, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: In this paper, we focus on performance issues related to DSM page size and the underlying DSM support structure. This work is based on our previous DSM system called Mirage <ref> [Fleisch 89b, Fleisch 89a] </ref>. Mirage is a DSM facility implemented entirely in the kernel of a UNIX-based operating system [Popek 81, Walker 83]. Here we examine our work to adapt Mirage for personal computers with a larger page size. <p> Here we examine our work to adapt Mirage for personal computers with a larger page size. We call the new system Mirage + . 1.1 Goals and Overview Our prior work <ref> [Fleisch 89b] </ref> focused on operating system extensions to support DSM, performance of synthetic applications which exercise DSM, examination of supporting algorithms and protocols, and performance optimizations using a time-based locking approach. 2 Although experiences with Mirage were encouraging, there were a number of concerns that motivated our port to a new <p> The TCF portion of the AIX product has since been discontinued. 2 2 The Mirage Model of Distributed Shared Memory The new DSM system, like Mirage <ref> [Fleisch 89b] </ref>, uses a paged segmentation scheme [Daley 68, Den-ning 70]. In our model, processes create shared memory segments by specifying the size of the segment, name, and access protection. Processes locate and attach segments into their virtual memory address space by name. <p> receive the second and third packet as 3.2 msec, the first packet does not benefit from any concurrency, and the last packet must reply with a high-level acknowledgment for all four packets. 4.5 Comparision of Remote Page Fault Costs We have compared the results with previous published results for Mirage <ref> [Fleisch 89b] </ref> running on a set of three VAX 11/750s. The comparison is shown as Table 3. <p> On one site, the observed behavior with respect to the yield call, as well as the time spent per iteration of the access pattern, are similar to the observations reported for Mirage <ref> [Fleisch 89b] </ref>. The total run of the simulation contains 789 iterations, where each iteration has a shared memory access pattern as illustrated in Figure 5.
Reference: [Fleisch 93] <author> Brett D. Fleisch, Randall L. Hyde, and Niels Christian Juul. </author> <title> Moving Distributed Shared Memory to the Personal Computer: The Mirage+ Experience. </title> <type> Technical Report UCR-CS-93-6, </type> <institution> Department of Computer Science, University of California, </institution> <address> Riverside, CA, USA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: This research was sponsored by a Joint Study with IBM Corporation and is currently sponsored by NSF CCR-9209405. Juul was also supported by the Danish Natural Science Research Council. A preliminary version of this paper, with early results, has been released as UC, Riverside Technical Report UCR-CS-93-6 <ref> [Fleisch 93] </ref>.
Reference: [Hoare 74] <author> C. A. R. Hoare. </author> <title> Monitors: An operating system structuring concept. </title> <journal> Communications of the ACM, </journal> <volume> 17(10) </volume> <pages> 545-57, </pages> <month> October </month> <year> 1974. </year>
Reference-contexts: A coherent implementation is one in which a store to an address is always visible to all subsequent load operations of the same address, independent of the machine location where the load occurs. Higher level synchronization primitives, such as semaphores or monitors <ref> [Brinch Hansen 73, Dijkstra 72, Hoare 74] </ref> may be used by applications that require additional consistency guarantees. For example, we use the UNIX System V semaphore interface in many of our (distributed) applications. Our past work on Mirage focused on a time-based locking approach to DSM.
Reference: [Hyde 94] <author> Randall L. Hyde and Brett D. Fleisch. </author> <title> An Analysis of Degenerate Sharing and False Coherence. </title> <type> Technical Report UCR-CS-94-1, </type> <institution> Department of Computer Science, University of California, </institution> <address> Riverside, CA, USA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: False sharing phenomena arises from the nature of applications. Dynamic detection and attribution of false sharing remains a hard, important research problem. This issue is beyond the scope of this paper. We present a precise definition of false sharing and our work in the area in a separate paper <ref> [Hyde 94] </ref>. 5.2 Application Descriptions To check the viability of compression before investing extensive work in it, we constructed three applications which access memory using common access patterns: EachN, UptoN, and RandN. Each of these applications starts with a page containing all zeros and writes uncompressable values to the page.
Reference: [Li 86a] <author> Kai Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> September </month> <year> 1986. </year>
Reference-contexts: We focus on operating system implementations of DSM and do not discuss related hardware DSM systems and language/compiler implementations of DSM. Li <ref> [Li 86a, Li 88] </ref> experimented with a coherent shared virtual memory system on a loosely coupled multiprocessor, the Apollo Domain system [Nelson 84]. Shared data is paged between processors, some of which have copies of the virtual address space pages.
Reference: [Li 86b] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <booktitle> In Proceedings 5th ACM SIGACT-SIGOPS Symposium of Principles of Distributed Computing, </booktitle> <pages> pages 229-239, </pages> <publisher> ACM Press, </publisher> <address> Canada, </address> <month> August </month> <year> 1986. </year>
Reference: [Li 88] <author> Kai Li. IVY: </author> <title> a shared virtual memory system for parallel computing. </title> <booktitle> In Proceedings 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 94-101, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: We focus on operating system implementations of DSM and do not discuss related hardware DSM systems and language/compiler implementations of DSM. Li <ref> [Li 86a, Li 88] </ref> experimented with a coherent shared virtual memory system on a loosely coupled multiprocessor, the Apollo Domain system [Nelson 84]. Shared data is paged between processors, some of which have copies of the virtual address space pages.
Reference: [Li 90] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 54-64, </pages> <month> May </month> <year> 1990. </year>
Reference: [Metcalfe 76] <author> R. M. Metcalfe and D. R. Boggs. </author> <title> Ethernet: distributed packet switching for local computer networks. </title> <journal> Communications of the ACM, </journal> <volume> 19(7) </volume> <pages> 395-403, </pages> <year> 1976. </year>
Reference-contexts: The systems are connected by a 10 Mbps Ethernet <ref> [Metcalfe 76] </ref> using Ungermann-Bass NICps/2 network adapters (technology circa 1987) without on-board caching.
Reference: [Minnich 89] <author> Ronald G. Minnich and David J. Farbar. </author> <title> The Mether system: Distributed shared memory for SunOS 4.0. </title> <booktitle> In Proceedings of the Summer 1989 USENIX Conference, </booktitle> <pages> pages 51-60, </pages> <publisher> USENIX, </publisher> <address> Baltimore, Maryland, USA, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: For example, the Matrix Multiply shows only a 4% improvement with annotated 21 objects. However, other applications realized better than 50% execution time improvement. 6.3 Mether Mether <ref> [Minnich 89] </ref> is a DSM system that supports inconsistent memory, leaving the responsibility for enforcing consistency to user-defined protocols. Mether operates on a cluster of SUN SPARCStations connected with Ethernet and running SunOS 4.0. DSM under Mether is implemented through modifications to the NSF file system.
Reference: [Nelson 84] <author> David L. Nelson and Paul J. Leach. </author> <title> The architecture and applications of the Apollo Domain. </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> 58-66, </pages> <month> April </month> <year> 1984. </year> <month> 25 </month>
Reference-contexts: We focus on operating system implementations of DSM and do not discuss related hardware DSM systems and language/compiler implementations of DSM. Li [Li 86a, Li 88] experimented with a coherent shared virtual memory system on a loosely coupled multiprocessor, the Apollo Domain system <ref> [Nelson 84] </ref>. Shared data is paged between processors, some of which have copies of the virtual address space pages. The model assumes a write-invalidate DSM system where ownership of pages can vary from processor to processor either statically or dynamically.
Reference: [Nelson 91] <author> M. Nelson. </author> <title> The Data Compression Book. M & T Books, </title> <address> Redwood City, CA, USA, </address> <year> 1991. </year>
Reference: [Nitzberg 91] <author> B. Nitzberg and V. Lo. </author> <title> Distributed shared memory: A survey of issues and algorithms. </title> <journal> IEEE Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference: [Popek 81] <author> G. Popek, B. Walker, J. Chow, D. Edwards, C. Kline, G. Rudisin, and G. Thiel. </author> <title> Locus: A network transparent, high reliability distributed system. </title> <booktitle> In Proceedings of the Eigth ACM Symposium on Operating Systems Principles, published in Operating Systems Review 15, </booktitle> <pages> pages 169-177, </pages> <publisher> ACM SIGOPS, ACM Press, </publisher> <address> Pacific Grove, CA, USA, </address> <month> December </month> <year> 1981. </year>
Reference-contexts: In this paper, we focus on performance issues related to DSM page size and the underlying DSM support structure. This work is based on our previous DSM system called Mirage [Fleisch 89b, Fleisch 89a]. Mirage is a DSM facility implemented entirely in the kernel of a UNIX-based operating system <ref> [Popek 81, Walker 83] </ref>. Here we examine our work to adapt Mirage for personal computers with a larger page size. <p> a time-based locking approach. 2 Although experiences with Mirage were encouraging, there were a number of concerns that motivated our port to a new research platform: 1) the hardware running the Mirage prototype (VAX 11/750s) was obsolete, 2) Mirage was built on an early version of the Locus operating system <ref> [Popek 81, Walker 83] </ref> that operated only on VAXs, 3) we needed a testbed where we could examine the scalability of our system beyond three machines, and 4) we needed a new platform to address the issue of reliability in our future work.
Reference: [Raita 87] <author> T. Raita and J. Teuhola. </author> <title> Predictive text compression by hashing. </title> <booktitle> In Proceedings of the 10th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 223-233, </pages> <publisher> ACM, </publisher> <address> New Orleans, USA, </address> <month> June </month> <year> 1987. </year>
Reference: [Ramachandran 88] <author> Umakishore Ramachandran, Mustaque Ahamad, and M. Yousef A. Khalidi. </author> <title> Unifying Synchronization and Data Transfer in Maintaining Coherence of Distributed Shared Memory. </title> <type> Technical Report GIT-ICS-88/23, </type> <institution> Georgia Institute of Technology, </institution> <address> Atlanta, GA, USA, </address> <month> June </month> <year> 1988. </year>
Reference: [Schroeder 90] <author> Michael D. Schroeder and Michael Burrows. </author> <title> Performance of Firefly RPC. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The low-level communication subsystem is responsible for transmitting the programmer's packet to the destination site. The messages are sequenced, ordered, reliable, and possibly combined with other messages when sent to the destination. Vertical optimization of the protocol layers is a well proven technique to achieve better performance <ref> [Schroeder 90, Tanenbaum 92] </ref>. Our design constraints were, however, to build the DSM system on top of the communication layers without spending considerable time optimizing communication subsystems written by others.
Reference: [Storer 88] <author> J. Storer. </author> <title> Data Compression: Methods and Theory. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland, USA, </address> <year> 1988. </year>
Reference: [Tanenbaum 92] <author> Andrew S. Tanenbaum. </author> <title> Modern Operating Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, USA, </address> <year> 1992. </year>
Reference-contexts: The low-level communication subsystem is responsible for transmitting the programmer's packet to the destination site. The messages are sequenced, ordered, reliable, and possibly combined with other messages when sent to the destination. Vertical optimization of the protocol layers is a well proven technique to achieve better performance <ref> [Schroeder 90, Tanenbaum 92] </ref>. Our design constraints were, however, to build the DSM system on top of the communication layers without spending considerable time optimizing communication subsystems written by others.
Reference: [Thekkath 93] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For three packets, there must be about 0.9 msec latency per short message sent one-way for the system to show improvement. As Thekkath and Levy <ref> [Thekkath 93] </ref> point out, bandwidths are improving dramatically while 6 1.0+2.6=3.6, taking advantage of concurrency between the sending and the receiving even when no delay is present. 12 Average cost of remote page fault under varying network latencies. latencies are not.
Reference: [Walker 83] <author> Bruce Walker, Gerald Popek, Robert English, Charles Kline, and Greg Thiel. </author> <title> The LOCUS distributed operating system. </title> <booktitle> In Proceedings of the Nineth ACM Symposium on Operating Systems Principles, published in Operating Systems Review 17(5), </booktitle> <pages> pages 49-70, </pages> <publisher> ACM SIGOPS, ACM Press, </publisher> <address> Bretton Woods, NH, USA, </address> <month> October </month> <year> 1983. </year>
Reference-contexts: In this paper, we focus on performance issues related to DSM page size and the underlying DSM support structure. This work is based on our previous DSM system called Mirage [Fleisch 89b, Fleisch 89a]. Mirage is a DSM facility implemented entirely in the kernel of a UNIX-based operating system <ref> [Popek 81, Walker 83] </ref>. Here we examine our work to adapt Mirage for personal computers with a larger page size. <p> a time-based locking approach. 2 Although experiences with Mirage were encouraging, there were a number of concerns that motivated our port to a new research platform: 1) the hardware running the Mirage prototype (VAX 11/750s) was obsolete, 2) Mirage was built on an early version of the Locus operating system <ref> [Popek 81, Walker 83] </ref> that operated only on VAXs, 3) we needed a testbed where we could examine the scalability of our system beyond three machines, and 4) we needed a new platform to address the issue of reliability in our future work.
Reference: [Williams 90] <author> R. Williams. </author> <title> Adaptive Data Compression. </title> <publisher> Kluwer Books, Norwell, </publisher> <address> Ma, USA, </address> <year> 1990. </year>
Reference: [Ziv 77] <author> J. Ziv and A. Lempel. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 23(3) </volume> <pages> 337-343, </pages> <month> May </month> <year> 1977. </year>
Reference: [Zwaenepoel 85] <author> Willy Zwaenepoel. </author> <title> Protocols for Large Data Transfers over Local Networks. </title> <type> Technical Report COMP TR85-23, </type> <institution> Department of Computer Science, Rice University, Houston, Texas, USA, </institution> <month> July </month> <year> 1985. </year> <month> 26 </month>
Reference-contexts: Vertical optimization of the protocol layers is a well proven technique to achieve better performance [Schroeder 90, Tanenbaum 92]. Our design constraints were, however, to build the DSM system on top of the communication layers without spending considerable time optimizing communication subsystems written by others. With high-level packet blasting <ref> [Zwaenepoel 85, Carter 89] </ref>, we expected the total time for a remote page fault would improve significantly. The savings using high-level packet blasting instead of explicitly handshaking each packet, is not only due to the removal of explicit acknowledgments, but also due to increased parallelism during communication.
References-found: 38

