URL: http://pattie.www.media.mit.edu/people/pattie/alife-journal.ps
Refering-URL: http://www.cs.berkeley.edu/~davidp/cs294-writeup.html
Root-URL: 
Email: pattie@media.mit.edu  
Title: Modeling Adaptive Autonomous Agents  
Author: Pattie Maes 
Address: 20 Ames Street Rm 305 Cambridge, MA 02139  
Affiliation: MIT Media-Laboratory  
Abstract: One category of researchers in artificial life is concerned with modeling and building so-called adaptive autonomous agents. Autonomous agents are systems that inhabit a dynamic, unpredictable environment in which they try to satisfy a set of time-dependent goals or motivations. Agents are said to be adaptive if they improve their competence at dealing with these goals based on experience. Autonomous agents constitute a new approach to the study of artificial intelligence (AI) which is highly inspired by biology, in particular ethology, the study of animal behavior. Research in autonomous agents has brought about a new wave of excitement into the field of AI. This paper reflects on the state of the art of this new approach. It attempts to extract its main ideas, evaluates what contributions have been made so far and identifies its current limitations and open problems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agre P.E., </author> <title> The Dynamic structure of Everyday Life, </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: He notes that the complexity of the ant's behavior is more a reflection of the complexity of the environment than of its own internal complexity. He muses that one could think that this is true for human behavior. Many years later, Agre <ref> [1] </ref> showed how behavior as complex as goal-directed action sequences can be modeled as an emergent property 1 Situatedness in time cuts both ways: it also means that the agent has to react in a timely fashion and be able to deal with interrupts. 7 of the interaction dynamics between a <p> The representations within one module are often of a less propositional, objective and declarative nature than those employed in traditional AI. For example they might index objects according to the features and properties that make them significant to the task at hand <ref> [1] </ref> rather than the identities of the objects. They can be of a numeric, procedural [44] or analog nature. Often a lot of task-specific "problem solving" is performed in the perception part of a particular competence [57] [14] [2]. <p> Examples of such architectures are the Subsumption Architecture [10] and the architectures reported in [15] (a minimalist version of the Subsumption Architecture) <ref> [1] </ref> [14] [4] and others.
Reference: [2] <author> Ballard D.H., </author> <title> Reference Frames for Animate Vision, </title> <booktitle> Proceedings of IJCAI-89 conference, </booktitle> <address> Detroit, </address> <year> 1989. </year> <month> 31 </month>
Reference-contexts: They can be of a numeric, procedural [44] or analog nature. Often a lot of task-specific "problem solving" is performed in the perception part of a particular competence [57] [14] <ref> [2] </ref>. Decentralized Control Structure Traditional AI adopts a sequential organization of the different modules within the system. The modules take turns being "active" or processing and changing 10 the internal representations. Perception and inference first update the internal model (beliefs and goals).
Reference: [3] <author> Bates, J., Loyall B. & Reilly W., </author> <title> Broad Agents, </title> <booktitle> Proceedings of the AAAI Spring Symposium on Integrated Intelligent Architectures, Stanford, CA. Available in SIGART Bulletin, </booktitle> <volume> Vol. 2 (4), </volume> <pages> pp 38-40, </pages> <year> 1991. </year>
Reference-contexts: This paper presents a more general perspective. It argues that the autonomous agent approach is appropriate for the class of problems that require a system to autonomously fulfill several goals in a dynamic, unpredictable environment. This includes applications such as virtual actors in interactive training and entertainment systems <ref> [3] </ref> [39], interface agents [40] [53], process scheduling [42], and so on. Wilson's account [70] focuses on a scientific methodology for research in autonomous agents, while Meyer [46] aims to give an overview of the research performed so far.
Reference: [4] <author> Beer R., Chiel H. & Sterling L., </author> <title> A Biological Perspective on Autonomous Agent Design, In: Designing Autonomous Agents: Theory and Practice from Biology to Engineering and Back, edited by P. Maes, </title> <publisher> MIT Press/Bradford Books, </publisher> <year> 1990. </year>
Reference-contexts: Examples of such architectures are the Subsumption Architecture [10] and the architectures reported in [15] (a minimalist version of the Subsumption Architecture) [1] [14] <ref> [4] </ref> and others.
Reference: [5] <author> Beer R., </author> <title> A Dynamical Systems Perspective on Autonomous Agents, </title> <type> Technical Report CES-92-11, </type> <institution> Department of Computer Engineering and Science, Case Western Reserve University, </institution> <year> 1992. </year>
Reference-contexts: One consequence is that we need a better understanding of the particular characteristics of an environment. If we want to be able to understand or prove aspects about the resulting performance of autonomous agents, we have to model the agent as well as its environment <ref> [5] </ref> [23]. Another consequence is that we need better models of the interaction dynamics between an agent (or components of the agent) and its environment. 2. Simple interaction dynamics between the different components within an agent can lead to emergent structure or emergent functionality. <p> Kaelbling and Rosenschein offer a logical model [24], while Beer <ref> [5] </ref>, Kiss [28] and Steels [58] have started approaching this problem from a dynamical systems perspective. <p> Some first steps towards a theory of emergent functionality have been proposed, using tools from complex dynamics [58] [28] <ref> [5] </ref>. However, so far the proposed theories have only been applicable to very simple toy examples. There is tension inherent in the agent approach that is as of now unresolved. Research in autonomous agents has adopted very task-driven, pragmatic solutions.
Reference: [6] <author> Belew R., </author> <title> Evolution, Learning and Culture: Computational Metaphors for Adaptive Algorithms, </title> <institution> UCSD Computer Science and Engineering Department, </institution> <type> CSE Technical Report #CS89-156, </type> <year> 1989. </year>
Reference-contexts: what discretization or what subdivision of the continuous space of possible actions is appropriate for the environment and the goals at hand. * We need to understand better what the role of learning is and how it interacts with other adaptive phenomena like cultural learning and adaptation through evolution (see <ref> [6] </ref>). We need to better understand what "building blocks" evolution could provide that could facilitate learning (e.g. provide a built-in bias for learning, or built-in specialized structures, etc). * Finally, most of the approaches taken have been inspired by behaviorism and comparative psychology, rather than ethology.
Reference: [7] <editor> Blumberg B., Action-Selection in Hamsterdam: </editor> <title> Lessons from Ethology, Submitted to: </title> <booktitle> the Third International Conference on the Simulation of Adaptive Behavior, </booktitle> <address> Brighton, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: A final category of action selection models proposes a more hierarchical organization of the different actions or competence modules. Examples of such architectures are Agar [65], Hamsterdam <ref> [7] </ref>, Rosenblatt and Payton's work [52], which is a more sophisticated version of the Subsumption Architecture and Tyrrell's work [67]. Most of these architectures are closely inspired by models of animal behavior stemming from ethologists such as Lorenz and Tinbergen. <p> Typically these architectures organize actions in a hierarchy that ranges from high-level "modes" or activities via mid-level composite actions to detailed, primitive actions. Only the primitive actions are actually executable. Tyrrell [67] and Blumberg <ref> [7] </ref> both have demonstrated that when scaling the problem to more complex agents that have many different goals and actions, it is desirable to have more structure (than that present in flat networks) that may help decide which actions are relevant. <p> Typically only one module at a time determines what command is sent to an actuator. There is no way for the outputs of multiple modules to be combined. Some proposals for solutions to this problem are presented in [52] and <ref> [7] </ref>). * All of the above architectures are completely decentralized and do not keep any central state. As a result they may suffer from the lack of what Minsky would call a "B-brain" [47].
Reference: [8] <author> Booker L., </author> <title> Classifier Systems that Learn Internal World Models, </title> <journal> Machine Learning Journal, </journal> <volume> Volume 1, Number 2,3, </volume> <year> 1988. </year>
Reference-contexts: Independent of this, the agent learns (or infers) what the importance or value of taking certain actions in certain situations is. Interesting combinations of these three types of architectures exist. For example, some systems combine learning of an action policy with learning of a model <ref> [8] </ref> [61]. As is the case with action selection models, many of the architectures proposed have been inspired by theories of animal learning. <p> A second category of architectures for learning agents are based on classifier systems [20]. In particular Wilson [69] and Booker <ref> [8] </ref> have studied how classifier systems can be used to build adaptive autonomous agents. These architectures can be viewed a special case of reinforcement learning systems. That is, again, the agent attempts to learn how it can optimize the reward it receives for taking certain actions in certain situations.
Reference: [9] <author> Braitenberg V., </author> <title> Vehicles: Experiments in Synthetic Psychology, </title> <publisher> MIT Press/Bradford Books, </publisher> <year> 1984. </year>
Reference-contexts: This idea is inspired by the field of ethology. Ethologists have stressed that an animal's behavior can only be understood (and only makes sense) in the context of the particular environment it inhabits. Braitenberg, a cybernetician, also convincingly illustrated a similar idea in his book "Vehicles" <ref> [9] </ref>. Finally, in AI, Simon [55] referred to the same idea when he discussed the example of an ant on the beach. He notes that the complexity of the ant's behavior is more a reflection of the complexity of the environment than of its own internal complexity.
Reference: [10] <author> Brooks R.A., </author> <title> A Robust Layered Control System for a Mobile Robot, </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-2, </volume> <month> April </month> <year> 1986. </year>
Reference-contexts: In contrast, an agent is viewed as a set of competence modules (often also called behaviors) <ref> [10] </ref>. These modules are responsible for a particular small task-oriented competence. Each of the modules is directly connected to its relevant sensors and actuators. Modules interface to one another via extremely simple messages rather than a common representation of beliefs, and so on. <p> However, some simple arbitration method is included in order to select or fuse multiple conflicting actuator commands (commands of different modules might be mutually exclusive). This arbitration network might be a winner-take-all network, as in [33] or a hardcoded priority scheme as in <ref> [10] </ref>. Because of its distributed operation, an agent is typically able to react quickly to changes in the environment or changes in the goals of the system. Goal-directed Activity is an Emergent Property Traditional AI models activity as the result of a "deliberative thinking" process. <p> The execution module executes this plan while possibly checking at certain points whether things are going as predicted. If not, control is returned to the planner. An adaptive autonomous agent for the same task could be constructed in the following way (as inspired by <ref> [10] </ref>). <p> This robot does not plan a course of action. However, from an observer's point of view it will appear to operate in a systematic, rational way. Brooks <ref> [10] </ref> [11] has argued convincingly, in writing and in demonstrations, which of the two above robots will be more successful at dealing with the task in a robust and reliable way 2 . <p> A number of architectures have been proposed that require the designer of the agent to solve the action selection problem from scratch for every agent that is built. Examples of such architectures are the Subsumption Architecture <ref> [10] </ref> and the architectures reported in [15] (a minimalist version of the Subsumption Architecture) [1] [14] [4] and others.
Reference: [11] <author> Brooks R.A., </author> <title> Intelligence without Reason, </title> <booktitle> Computers and Thought lecture, Proceedings of IJCAI-91, </booktitle> <address> Sidney, Australia, </address> <year> 1991. </year>
Reference-contexts: Finally, the term "animat approach" (shorthand for "artificial animal"), which was coined by Wilson [69], is also frequently used. Several people have given definitions and written overviews of research in Autonomous Agents, among others Brooks <ref> [11] </ref>, Wilson [70] and Meyer [46]. There are several reasons for giving it yet another try. First of all many researchers are still skeptical about the approach. Some claim that it isn't very different from what they have been doing all along. <p> Others are still not convinced that the approach is founded and scientific. 1 A second reason is that this account is different from the papers listed above. Brooks, being one of the main originators of this new approach, presents a picture which is restricted to robotic forms of intelligence <ref> [11] </ref>. This paper presents a more general perspective. It argues that the autonomous agent approach is appropriate for the class of problems that require a system to autonomously fulfill several goals in a dynamic, unpredictable environment. <p> A complete intelligent system is always part of some environment; it is situated in some space. This implies that there is less of a need for modeling, because the "world is its own best model" <ref> [11] </ref>. The environment can also be used as an external memory, for example, for reminding the system which tasks still have to be performed and which ones it already did perform [59]. The environment usually has particular characteristics that can be exploited by the system. <p> In some systems, the evolution towards increasingly more sophisticated and more adaptive behavior is simulated by the programmer, e.g. by incrementally adding more structure to existing successful systems <ref> [11] </ref>. Other systems employ learning by the individual [38] [35] [69] [18] [25] [62]. In almost all cases, the system concentrates on learning new information (or behavior) from its environment, rather than on reformulating information it already has. <p> This robot does not plan a course of action. However, from an observer's point of view it will appear to operate in a systematic, rational way. Brooks [10] <ref> [11] </ref> has argued convincingly, in writing and in demonstrations, which of the two above robots will be more successful at dealing with the task in a robust and reliable way 2 . <p> For example, as long as the robot manages to find the recharging station before its battery dies, as well as make sufficient progress towards its more task-specific goal of surveying the offices, it is considered an acceptable solution, even if it does not always follow optimal paths. Brooks <ref> [11] </ref> refers to this latter criterion as "adequacy". McFarland [45] takes this last point even further. He argues that one should use an ecological approach to evaluate agent behavior: if an agent fills a market niche, then that agent is considered successful.
Reference: [12] <author> Brooks R.A., </author> <title> Challenges for Complete Creature Architectures, </title> <booktitle> In: From Animals to Animats, Proceedings of the First International Conference on the Simulation of Adaptive Behavior, </booktitle> <editor> edited by Meyer J.-A. & Wilson S.W., </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1991. </year>
Reference-contexts: Agent research is founded on the belief that shifting into the "interaction" domain as opposed to the "component" domain will make it easier to solve the problem of building intelligent systems. This idea also applies at several different levels <ref> [12] </ref>: 1. Interaction dynamics between an agent and its environment can lead to emergent structure or emergent functionality. This idea is inspired by the field of ethology.
Reference: [13] <editor> Brooks R.A., </editor> <booktitle> Artificial Life and Real Robots, In: Toward a Practice of Autonomous Systems, Proceedings of the First European Conference on Artificial Life, edited by Varela, </booktitle> <editor> F. & Bourgine P., </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1992. </year>
Reference-contexts: This is especially so in the case of hand-built networks, because no support is given to the designer of an agent for building the complicated arbitration network that will govern its behavior. The most obvious solution to be investigated is to either evolve <ref> [13] </ref> or learn and adapt the network [35] based on experience. However, few experiments along these lines have been performed so far. 20 * Related to this, not enough effort has been put into making pieces of agent networks "reusable" within other agents.
Reference: [14] <author> Chapman D., </author> <title> Vision, Instruction and Action, </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: They can be of a numeric, procedural [44] or analog nature. Often a lot of task-specific "problem solving" is performed in the perception part of a particular competence [57] <ref> [14] </ref> [2]. Decentralized Control Structure Traditional AI adopts a sequential organization of the different modules within the system. The modules take turns being "active" or processing and changing 10 the internal representations. Perception and inference first update the internal model (beliefs and goals). <p> Examples of such architectures are the Subsumption Architecture [10] and the architectures reported in [15] (a minimalist version of the Subsumption Architecture) [1] <ref> [14] </ref> [4] and others. <p> They can get stuck in loops or deadlock situations (i.e. keep activating the same actions even though they have proven not to result in any change of state). * Most of the above architectures (apart from <ref> [14] </ref>) have a narrow-minded view of the relationship between perception and action.
Reference: [15] <author> Connell J., </author> <title> Minimalist Mobile Robotics, </title> <publisher> Academic Press, </publisher> <year> 1990. </year> <month> 32 </month>
Reference-contexts: A number of architectures have been proposed that require the designer of the agent to solve the action selection problem from scratch for every agent that is built. Examples of such architectures are the Subsumption Architecture [10] and the architectures reported in <ref> [15] </ref> (a minimalist version of the Subsumption Architecture) [1] [14] [4] and others.
Reference: [16] <author> Deneubourg J.L., Goss S., Franks N., Sendova-Franks A., Detrain C., Chre--tien L., </author> <title> The Dynamics of Collective Sorting: Robot-like Ants and Ant-like Robots, </title> <booktitle> In: From Animals to Animats, Proceedings of the First International Conference on the Simulation of Adaptive Behavior, </booktitle> <editor> edited by Meyer J.-A. & Wilson S.W., </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1991. </year>
Reference-contexts: The action selection behavior is an emergent property of some activation/inhibition dynamics among the primitive components of the system. 3. Interaction dynamics between the component agents of a social system can lead to emergent structure or functionality. Deneubourg <ref> [16] </ref> [17] describes how social insects following simple local rules can produce emergent complexity such as a path to a food source, food foraging trees, etc. Malone's collection of autonomous bidding systems addresses the complicated task of process-processor allocation [42].
Reference: [17] <author> Deneubourg J.L., Theraulaz G. & Beckers R., </author> <title> Swarm-Made Architectures, In: Toward a Practice of Autonomous Systems, </title> <booktitle> Proceedings of the First European Conference on Artificial Life, edited by F.J. </booktitle> <editor> Varela & P. Bourgine, </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1992. </year>
Reference-contexts: The action selection behavior is an emergent property of some activation/inhibition dynamics among the primitive components of the system. 3. Interaction dynamics between the component agents of a social system can lead to emergent structure or functionality. Deneubourg [16] <ref> [17] </ref> describes how social insects following simple local rules can produce emergent complexity such as a path to a food source, food foraging trees, etc. Malone's collection of autonomous bidding systems addresses the complicated task of process-processor allocation [42].
Reference: [18] <author> Drescher G.L., </author> <title> Made-Up Minds: A Constructivist Approach to Artificial Intelligence, </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: In some systems, the evolution towards increasingly more sophisticated and more adaptive behavior is simulated by the programmer, e.g. by incrementally adding more structure to existing successful systems [11]. Other systems employ learning by the individual [38] [35] [69] <ref> [18] </ref> [25] [62]. In almost all cases, the system concentrates on learning new information (or behavior) from its environment, rather than on reformulating information it already has. The learning algorithms are implemented in a distributed way: typically a similar learning algorithm runs in different competence modules. <p> Related to the idea of learning is that of redundancy: often the system has multiple modules for a particular competence. Experience sorts out which of these modules implements the competence in a more reliable way and should thus be preferred [38] [50] <ref> [18] </ref>. Systems built using all of the above principles (task-oriented modules, task-specific solutions, de-emphasized representations, decentralized control, etc) tend to demonstrate more adaptive and robust behavior. <p> Some of the architectures proposed allow for learning of new "composite" actions or composite competence modules [41] <ref> [18] </ref>. They allow the agent to independently learn composite modules as well as the arbitration network for these composite modules. The different architectures proposed can be grouped in three classes: reinforcement learning systems, classifier systems and model learners. <p> Model Builders. A final class of agents that learn from experience actually learn a causal model of their actions, rather than a policy map <ref> [18] </ref> [38] [50]. Drescher's model, which was inspired by Piaget's theories of development in infants, is probably the most sophisticated example. The agent builds up a probabilistic model of what the effects are of taking an action in a certain situation. <p> Such a combination of (i) a set of conditions, (ii) a primitive (or composite) action and (iii) a set of expected results (and probabilities for these results), is called a schema <ref> [18] </ref>, or a module or behavior [38]. Model learning architectures deal with the three subproblems defined above in the following way: 1. Action Selection Mechanism. Agents built using these architectures can deal with time varying, multiple, 27 explicit goals. <p> Often this value assignment process favors modules that prove to be more reliable. It may even trade off reliability of a sequence of actions for length of a sequence of actions leading to the goals. Typically a spreading activation process [38] or simple marker propagation process <ref> [18] </ref> is used to assign these values given some goals and sensor data. 2. Learning Method. Whenever a particular action is taken, the agent monitors what changes happen in the environment. It uses this information to learn correlations between particular conditions-action pairs and certain results.
Reference: [19] <author> Foner L., </author> <title> Paying Attention to What's Important: Using Focus of Attention to Improve Unsupervised Learning. Submitted to: </title> <booktitle> the Third International Conference on the Simulation of Adaptive Behavior, </booktitle> <address> Brighton, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Exploration Strategy. The exploration strategy used in these architectures varies. Drescher's system, while demonstrating sophisticated learning, has an extremely simple exploration strategy, namely a random one. His agent basically does not do anything else but learn by performing random experiments. Foner <ref> [19] </ref> discusses how Drescher's agent can be made to learn much faster and to learn more relevant knowledge by adopting a smarter experimentation strategy as well as a focus of attention mechanism. <p> The computational complexity of all of the learning systems discussed is too big to be practically useful to build com plex agents that solve real problems. * One reason this is the case is that very few algorithms have incorporated interesting attention mechanisms. For example, Foner <ref> [19] </ref> demonstrates that incorporating attention mechanisms such as spatial locality can improve the tractability of learning from experience in a significant way.
Reference: [20] <author> Holland J.H., </author> <title> Escaping Brittleness: the Possibilities of General-Purpose Learning Algorithms applied to Parallel Rule-Based Systems, In: Machine Learning, an Artificial Intelligence Approach, Volume II, edited by R.S. </title> <editor> Michalski, J.G. Carbonell & T.M. Mitchell, </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: A second category of architectures for learning agents are based on classifier systems <ref> [20] </ref>. In particular Wilson [69] and Booker [8] have studied how classifier systems can be used to build adaptive autonomous agents. These architectures can be viewed a special case of reinforcement learning systems.
Reference: [21] <author> Holland J.H., </author> <title> The Optimal Allocation of Trials (Chapter 5), </title> <booktitle> in: Adaption in Natural and Artificial Systems, </booktitle> <publisher> MIT Press/Bradford Books, </publisher> <year> 1992. </year>
Reference-contexts: The problem of learning from experience: How can an agent improve its performance over time based on its experience? How can it decide when to "exploit" its current best action, versus "exploring" other actions so as to possibly discover better ways of achieving its goals <ref> [21] </ref>? How can it incorporate the feedback from the world into its internal behavior-producing structures? How can it correct "wrong" or ineffective behavior-producing structures? etc. Section 6 discusses both of these problems in more detail.
Reference: [22] <author> Horswill I., </author> <title> Characterizing Adaptation by Constraint, In: Toward a Practice of Autonomous Systems, </title> <booktitle> Proceedings of the First European Conference on Artificial Life, edited by F.J. </booktitle> <editor> Varela & P. Bourgine, </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1992. </year>
Reference-contexts: The environment usually has particular characteristics that can be exploited by the system. For example, offices consist of vertical walls and horizontal floors, doors typically are of a particular size, etc. These "habitat constraints" can be exploited by the system, making its task much easier <ref> [22] </ref>. 3. An intelligent system is not only situated in space, but also in time. This implies that the system can develop itself so as to become better at its task, 6 if time and the particular task permit (through learning from experience).
Reference: [23] <author> Horswill I., </author> <title> Specialization of Perceptual Processes, </title> <type> PhD thesis, </type> <institution> AI Laboratory, MIT, </institution> <year> 1993. </year>
Reference-contexts: One consequence is that we need a better understanding of the particular characteristics of an environment. If we want to be able to understand or prove aspects about the resulting performance of autonomous agents, we have to model the agent as well as its environment [5] <ref> [23] </ref>. Another consequence is that we need better models of the interaction dynamics between an agent (or components of the agent) and its environment. 2. Simple interaction dynamics between the different components within an agent can lead to emergent structure or emergent functionality.
Reference: [24] <author> Kaelbling L.P. & Rosenschein S., </author> <title> Action and Planning in Embedded Agents, In: Designing Autonomous Agents: Theory and Practice from Biology to Engineering and Back, edited by P. Maes, </title> <publisher> MIT Press/Bradford Books, </publisher> <year> 1990. </year>
Reference-contexts: Compiled, Flat Networks. A second class of architectures attempts to facilitate the construction of agents by automating the process of designing the arbitration circuitry among competence modules. Examples of such architectures are the Rex/Gaps system <ref> [24] </ref>, Behavior Networks [33] and Teleo-Reactive Trees [49]. These architectures require the designer to specify in a particular formalism what the goals of the agent are, how goals can be reduced to other goals or to actions and what the different modules/actions are and their conditions and expected effects. <p> A compiler analyzes this specification and generates a circuit that will implement the desired goal-seeking behavior. In Kaelbling's and Rosenschein's work <ref> [24] </ref>, the types of goals and sensors that can be dealt with are restricted to booleans. <p> Kaelbling and Rosenschein offer a logical model <ref> [24] </ref>, while Beer [5], Kiss [28] and Steels [58] have started approaching this problem from a dynamical systems perspective.
Reference: [25] <author> Kaelbling L.P., </author> <title> An Adaptable Mobile Robot, In: Toward a Practice of Autonomous Systems, </title> <booktitle> Proceedings of the First European Conference on Artificial Life, edited by F.J. </booktitle> <editor> Varela & P. Bourgine, </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1992. </year>
Reference-contexts: In some systems, the evolution towards increasingly more sophisticated and more adaptive behavior is simulated by the programmer, e.g. by incrementally adding more structure to existing successful systems [11]. Other systems employ learning by the individual [38] [35] [69] [18] <ref> [25] </ref> [62]. In almost all cases, the system concentrates on learning new information (or behavior) from its environment, rather than on reformulating information it already has. The learning algorithms are implemented in a distributed way: typically a similar learning algorithm runs in different competence modules.
Reference: [26] <author> Kaelbling L.P., </author> <title> Learning in Embedded Systems, </title> <publisher> MIT Press, </publisher> <year> 1993. </year> <month> 33 </month>
Reference-contexts: Reinforcement Learning. The idea of reinforcement learning [61] [62] <ref> [26] </ref> is the following.
Reference: [27] <author> Kaelbling L.P., </author> <title> Learning to Achieve Goals, </title> <booktitle> In: Proceedings of IJCAI-93, the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference: [28] <author> Kiss G., </author> <title> Autonomous Agents, AI and Chaos Theory, </title> <booktitle> In: From Animals to Animats, Proceedings of the First International Conference on the Simulation of Adaptive Behavior, </booktitle> <editor> edited by Meyer J.-A. & Wilson S.W., </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1991. </year>
Reference-contexts: Kaelbling and Rosenschein offer a logical model [24], while Beer [5], Kiss <ref> [28] </ref> and Steels [58] have started approaching this problem from a dynamical systems perspective. <p> Some first steps towards a theory of emergent functionality have been proposed, using tools from complex dynamics [58] <ref> [28] </ref> [5]. However, so far the proposed theories have only been applicable to very simple toy examples. There is tension inherent in the agent approach that is as of now unresolved. Research in autonomous agents has adopted very task-driven, pragmatic solutions.
Reference: [29] <author> Kleinrock L. & Nilsson A., </author> <title> On Optimal Scheduling Algorithms for Time-Shared Systems, </title> <journal> Journal of the ACM, </journal> <volume> 28, 3, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: Once that schedule has been produced, the processing jobs can actually be sent to the different machines that they have been assigned to. This centralized way of solving the problem is present in the majority of the earlier 14 work in this area <ref> [29] </ref>. Among others, Malone has proposed a different solution to this problem [42], that one could call more "agent-based". In his Enterprise system, each of the machines in the network is autonomous and in charge of its own work load. The system is based on the metaphor of a market.
Reference: [30] <author> Koza J.R., </author> <title> Evolution and Co-evolution of Computer Programs to Control Independently-Acting Agents, </title> <booktitle> In: From Animals to Animats, Proceedings of the First International Conference on the Simulation of Adaptive Behavior, </booktitle> <editor> edited by Meyer J.-A. & Wilson S.W., </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1991. </year>
Reference-contexts: Instead, some sort of evolution-based learning at the species level might be able to deal with the long term adaptation required <ref> [30] </ref>.
Reference: [31] <author> Lin L.-J., </author> <title> Reinforcement Learning for Robots using Neural Networks, </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: ([27] attempts to overcome this problem), (iii) for realistic applications, the size of the state space (or the number of situation-action pairs) is so large that learning takes too much time to be practical (as a result, researchers have started developing algorithms that can generalize over the state space [41] <ref> [31] </ref>), (iv) learning only happens "at the fringe" of the state space (only when a reward is received can the system start learning about the sequence of actions leading to that reward), as a result it takes a lot of time to learn long action sequences ([61] attempts to deal with
Reference: [32] <author> Littman M., </author> <title> An Optimization-Based Categorization of Reinforcement Learning Environments. </title> <booktitle> In: From Animals to Animats 2, Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <editor> edited by Meyer J.-A., Roitblat H.L. & Wilson S.W., </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1993. </year>
Reference-contexts: More specifically such a problem class is defined in terms of particular characteristics of the agent's resources (e.g. memory, sensors, compute power), and particular characteristics of the tasks and environment [64] <ref> [32] </ref>. 3 Guiding Principles The study of Adaptive Autonomous Agents is grounded in two important insights. These serve as "guiding principles" for the research performed: * Looking at complete systems changes the problems often in a favorable way. * Interaction dynamics can lead to emergent complexity. <p> Todd and Wilson [64] and Littman <ref> [32] </ref> have started to build a taxonomy of 17 environments and taxonomy of agents that will provide a more profound basis for comparing different proposals. 6.1.2 Progress Made The different models for action selection in an autonomous agent that have been proposed differ in the way they deal with the following <p> Instead, some sort of evolution-based learning at the species level might be able to deal with the long term adaptation required [30]. Todd and Wilson [64] and <ref> [32] </ref> present some first steps towards a taxonomy of environments and agents that may make comparisons more meaningful. 6.2.2 Progress Made All of the architectures that have been proposed in the literature assume that the agent has a set of primitive actions or competence modules.
Reference: [33] <author> Maes P., </author> <title> Situated Agents Can Have Goals, In: Designing Autonomous Agents: Theory and Practice from Biology to Engineering and Back, edited by P. Maes, </title> <publisher> MIT Press/Bradford Books, </publisher> <year> 1990. </year>
Reference-contexts: Neither one of these modules is primarily "responsible" for the wall following behavior. It is their interaction dynamics that makes the robot follow walls reliably. In Maes' networks <ref> [33] </ref>, none of the component modules is responsible for action selection. The action selection behavior is an emergent property of some activation/inhibition dynamics among the primitive components of the system. 3. Interaction dynamics between the component agents of a social system can lead to emergent structure or functionality. <p> Often the system explores multiple solutions in parallel, so that as soon as certain variables change, 8 the system is able to switch to an alternative way of doing things. For example, in Maes' system <ref> [33] </ref> several sequences of actions are evaluated in parallel, the best one determining the behavior of the agent. <p> None of the modules is "in control" of other modules. However, some simple arbitration method is included in order to select or fuse multiple conflicting actuator commands (commands of different modules might be mutually exclusive). This arbitration network might be a winner-take-all network, as in <ref> [33] </ref> or a hardcoded priority scheme as in [10]. Because of its distributed operation, an agent is typically able to react quickly to changes in the environment or changes in the goals of the system. <p> Compiled, Flat Networks. A second class of architectures attempts to facilitate the construction of agents by automating the process of designing the arbitration circuitry among competence modules. Examples of such architectures are the Rex/Gaps system [24], Behavior Networks <ref> [33] </ref> and Teleo-Reactive Trees [49]. These architectures require the designer to specify in a particular formalism what the goals of the agent are, how goals can be reduced to other goals or to actions and what the different modules/actions are and their conditions and expected effects. <p> In most of the work, except for <ref> [33] </ref>, these types of architectures produce agents with implicit, fixed (not time-varying) goals. However, in contrast with the previous class of architectures, the goals are explicit in the designer's formal specification of the agent. <p> However, in contrast with the previous class of architectures, the goals are explicit in the designer's formal specification of the agent. This implies that the agent's circuitry has to be resynthesized if the agent should fulfill a different set of goals. Maes <ref> [33] </ref> [37] proposes an architecture with explicit, time-varying goals.
Reference: [34] <author> Maes, P., </author> <title> Designing Autonomous Agents: Theory and Practice from Biology to Engineering and Back, </title> <publisher> MIT Press/Bradford Books, </publisher> <year> 1990. </year>
Reference: [35] <author> Maes P. & Brooks R.A., </author> <title> Learning to Coordinate Behaviors, </title> <booktitle> Proceedings of AAAI-90, </booktitle> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: In some systems, the evolution towards increasingly more sophisticated and more adaptive behavior is simulated by the programmer, e.g. by incrementally adding more structure to existing successful systems [11]. Other systems employ learning by the individual [38] <ref> [35] </ref> [69] [18] [25] [62]. In almost all cases, the system concentrates on learning new information (or behavior) from its environment, rather than on reformulating information it already has. The learning algorithms are implemented in a distributed way: typically a similar learning algorithm runs in different competence modules. <p> The most obvious solution to be investigated is to either evolve [13] or learn and adapt the network <ref> [35] </ref> based on experience. However, few experiments along these lines have been performed so far. 20 * Related to this, not enough effort has been put into making pieces of agent networks "reusable" within other agents. <p> Almost all of these architectures neglect the issue of learning from experience (except for <ref> [35] </ref> [38]). This means that agents built using these architectures are only adaptive in a very restricted sense: they are able to deal with unexpected situations (opportunities, contingencies). However, these agents do not learn from environment feedback.
Reference: [36] <author> Maes P., </author> <title> Adaptive Action Selection, </title> <booktitle> Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1991. </year>
Reference-contexts: Even though this is ultimately true, it is not particularly useful as a means for comparing different proposals for agent architectures. Tyrrell [67] compares several action selection proposals, but he does so with respect to one particular benchmark environment and task. Maes <ref> [36] </ref> and Wil-son [70] have argued that it is not possible to decide that one action selection model is better than another one unless one also mentions what the particular characteristics are of the environment, the task and the agent.
Reference: [37] <author> Maes, P., </author> <title> A Bottom-Up Mechanism for Behavior-Selection in an Artificial Creature, </title> <booktitle> In: From Animals to Animats, Proceedings of the First International Conference on the Simulation of Adaptive Behavior, </booktitle> <editor> edited by Meyer J.-A. & Wilson S.-W., </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1991. </year> <month> 34 </month>
Reference-contexts: However, in contrast with the previous class of architectures, the goals are explicit in the designer's formal specification of the agent. This implies that the agent's circuitry has to be resynthesized if the agent should fulfill a different set of goals. Maes [33] <ref> [37] </ref> proposes an architecture with explicit, time-varying goals. The arbitration network that is compiled has an explicit representation of the goals of the agent and these goals can have intensities that vary over time (e.g. hunger level for an artificial animal, or motivation to recharge the battery of a robot).
Reference: [38] <author> Maes P., </author> <title> Learning Behavior Networks from Experience, In: Toward a Prac--tice of Autonomous Systems, </title> <booktitle> Proceedings of the First European Conference on Artificial Life, edited by F.J. </booktitle> <editor> Varela & P. Bourgine, </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1992. </year>
Reference-contexts: In some systems, the evolution towards increasingly more sophisticated and more adaptive behavior is simulated by the programmer, e.g. by incrementally adding more structure to existing successful systems [11]. Other systems employ learning by the individual <ref> [38] </ref> [35] [69] [18] [25] [62]. In almost all cases, the system concentrates on learning new information (or behavior) from its environment, rather than on reformulating information it already has. The learning algorithms are implemented in a distributed way: typically a similar learning algorithm runs in different competence modules. <p> Related to the idea of learning is that of redundancy: often the system has multiple modules for a particular competence. Experience sorts out which of these modules implements the competence in a more reliable way and should thus be preferred <ref> [38] </ref> [50] [18]. Systems built using all of the above principles (task-oriented modules, task-specific solutions, de-emphasized representations, decentralized control, etc) tend to demonstrate more adaptive and robust behavior. <p> Almost all of these architectures neglect the issue of learning from experience (except for [35] <ref> [38] </ref>). This means that agents built using these architectures are only adaptive in a very restricted sense: they are able to deal with unexpected situations (opportunities, contingencies). However, these agents do not learn from environment feedback. <p> Model Builders. A final class of agents that learn from experience actually learn a causal model of their actions, rather than a policy map [18] <ref> [38] </ref> [50]. Drescher's model, which was inspired by Piaget's theories of development in infants, is probably the most sophisticated example. The agent builds up a probabilistic model of what the effects are of taking an action in a certain situation. <p> Such a combination of (i) a set of conditions, (ii) a primitive (or composite) action and (iii) a set of expected results (and probabilities for these results), is called a schema [18], or a module or behavior <ref> [38] </ref>. Model learning architectures deal with the three subproblems defined above in the following way: 1. Action Selection Mechanism. Agents built using these architectures can deal with time varying, multiple, 27 explicit goals. <p> Often this value assignment process favors modules that prove to be more reliable. It may even trade off reliability of a sequence of actions for length of a sequence of actions leading to the goals. Typically a spreading activation process <ref> [38] </ref> or simple marker propagation process [18] is used to assign these values given some goals and sensor data. 2. Learning Method. Whenever a particular action is taken, the agent monitors what changes happen in the environment. <p> Foner [19] discusses how Drescher's agent can be made to learn much faster and to learn more relevant knowledge by adopting a smarter experimentation strategy as well as a focus of attention mechanism. In Maes' architecture <ref> [38] </ref>, the exploration strategy is also more goal-oriented: the agent biases its experimentation towards actions that show promise to contribute to the goals.
Reference: [39] <author> Maes P., </author> <year> 1993, </year> <title> ALIVE: An Artificial Life Interactive Video Environment, </title> <booktitle> Visual Proceedings of the Siggraph-93 Conference, </booktitle> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: This paper presents a more general perspective. It argues that the autonomous agent approach is appropriate for the class of problems that require a system to autonomously fulfill several goals in a dynamic, unpredictable environment. This includes applications such as virtual actors in interactive training and entertainment systems [3] <ref> [39] </ref>, interface agents [40] [53], process scheduling [42], and so on. Wilson's account [70] focuses on a scientific methodology for research in autonomous agents, while Meyer [46] aims to give an overview of the research performed so far. <p> Finally, agents can inhabit simulated physical environments. An example of such an agent could be a "synthetic actor" in a computer animated world. Combinations of these three types of agents may exist. For example, in the ALIVE interactive environment <ref> [39] </ref>, the animated (virtual) agents employ real sensors (namely a camera), to decide how to react to a person's movements and gestures.
Reference: [40] <author> Maes P. & Kozierok R., </author> <title> Learning Interface Agents, </title> <booktitle> Proceedings of AAAI-93, the Eleventh National Conference on Artificial Intelligence, </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: It argues that the autonomous agent approach is appropriate for the class of problems that require a system to autonomously fulfill several goals in a dynamic, unpredictable environment. This includes applications such as virtual actors in interactive training and entertainment systems [3] [39], interface agents <ref> [40] </ref> [53], process scheduling [42], and so on. Wilson's account [70] focuses on a scientific methodology for research in autonomous agents, while Meyer [46] aims to give an overview of the research performed so far. <p> For other systems these might be other simple competences, like reacting in a market system by simple bidding and buying behaviors [42] or executing a simple software routine in the case of an interface agent <ref> [40] </ref>. 2. Traditional AI has focussed on "closed" systems that have no direct interaction with the problem domain about which they encode knowledge and solve problems. Their connection with the environment is very controlled and indirect through a human operator. <p> Therefore there is no need for the agent to figure everything out by itself. For example, a mobile robot could use the strategy of closely following a person passing by, in order to achieve the competence of navigating in an office environment without bumping into things. Maes and Kozierok <ref> [40] </ref> report on some experiments in which interface agents learned to perform certain tasks by observing and imitating users. As a consequence of the above ideas, autonomous agent research has concentrated on modeling systems within their context. <p> Instead, an adaptive autonomous "interface agent" can be built as follows <ref> [40] </ref>. Several competence modules are constructed that are experts (or try to become experts) about a small aspect of the task. For example, one module might be responsible for invoking a particular command (like "lpr") at a particular moment.
Reference: [41] <author> Mahadevan S. & Connell, J., </author> <title> Automatic Programming of Behavior-Based Robots using Reinforcement Learning, </title> <booktitle> In: Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Some of the architectures proposed allow for learning of new "composite" actions or composite competence modules <ref> [41] </ref> [18]. They allow the agent to independently learn composite modules as well as the arbitration network for these composite modules. The different architectures proposed can be grouped in three classes: reinforcement learning systems, classifier systems and model learners. <p> scratch ([27] attempts to overcome this problem), (iii) for realistic applications, the size of the state space (or the number of situation-action pairs) is so large that learning takes too much time to be practical (as a result, researchers have started developing algorithms that can generalize over the state space <ref> [41] </ref> [31]), (iv) learning only happens "at the fringe" of the state space (only when a reward is received can the system start learning about the sequence of actions leading to that reward), as a result it takes a lot of time to learn long action sequences ([61] attempts to deal
Reference: [42] <author> Malone T.W., Fikes R.E., Grant K.R., </author> & <title> Howard M.T., Enterprise: A Market-like Task Scheduler for Distributed Computing Environments, In: The Ecology of Computation, edited by B. Huberman, </title> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: It argues that the autonomous agent approach is appropriate for the class of problems that require a system to autonomously fulfill several goals in a dynamic, unpredictable environment. This includes applications such as virtual actors in interactive training and entertainment systems [3] [39], interface agents [40] [53], process scheduling <ref> [42] </ref>, and so on. Wilson's account [70] focuses on a scientific methodology for research in autonomous agents, while Meyer [46] aims to give an overview of the research performed so far. <p> Typically the competences are lower-level competences. For a robot these are competences such as locomotion, navigation, keeping the battery charged, collecting objects, etc. For other systems these might be other simple competences, like reacting in a market system by simple bidding and buying behaviors <ref> [42] </ref> or executing a simple software routine in the case of an interface agent [40]. 2. Traditional AI has focussed on "closed" systems that have no direct interaction with the problem domain about which they encode knowledge and solve problems. <p> Deneubourg [16] [17] describes how social insects following simple local rules can produce emergent complexity such as a path to a food source, food foraging trees, etc. Malone's collection of autonomous bidding systems addresses the complicated task of process-processor allocation <ref> [42] </ref>. Finally, anthropologists have studied how different concepts and complex methods for solving problems are gradually shaped through social interaction among different people [59] [54]. What is important is that such emergent complexity is often more robust, flexible and fault-tolerant than programmed, top-down organized complexity. <p> For example, in Maes' system [33] several sequences of actions are evaluated in parallel, the best one determining the behavior of the agent. Also in Malone's system <ref> [42] </ref> several mappings of processes to machines can be viewed as being explored in parallel. 4 Characteristics of Agent Architectures Many of the architectures for autonomous agents that have been proposed have characteristics in common. <p> This centralized way of solving the problem is present in the majority of the earlier 14 work in this area [29]. Among others, Malone has proposed a different solution to this problem <ref> [42] </ref>, that one could call more "agent-based". In his Enterprise system, each of the machines in the network is autonomous and in charge of its own work load. The system is based on the metaphor of a market.
Reference: [43] <author> Mataric M.J., </author> <title> Behavioral Synergy without Explicit Integration, In: </title> <journal> Special Issue of SIGART on Integrated Cognitive Architectures, </journal> <volume> Volume 2, Number 4, </volume> <year> 1991. </year>
Reference-contexts: Simple interaction dynamics between the different components within an agent can lead to emergent structure or emergent functionality. For example, Mataric's wall-following robot does not have a single component to which the expertise of wall-following can be attributed <ref> [43] </ref>. One module is responsible for steering the robot towards the wall when the distance to the wall is above some threshold while another module is responsible for steering the robot away from the wall when the distance is below some threshold.
Reference: [44] <author> Mataric, M.J., </author> <title> Integration of Representation Into Goal-Driven Behavior-Based Robots, </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> Vol. 8, No. 3, </volume> <month> June </month> <year> 1992, </year> <pages> 304-312. </pages>
Reference-contexts: For example they might index objects according to the features and properties that make them significant to the task at hand [1] rather than the identities of the objects. They can be of a numeric, procedural <ref> [44] </ref> or analog nature. Often a lot of task-specific "problem solving" is performed in the perception part of a particular competence [57] [14] [2]. Decentralized Control Structure Traditional AI adopts a sequential organization of the different modules within the system.
Reference: [45] <author> McFarland D., </author> <title> What it Means for a Robot Behavior to be Adaptive, </title> <booktitle> In: From Animals to Animats, Proceedings of the First International Conference on the Simulation of Adaptive Behavior, </booktitle> <editor> edited by Meyer J.-A. & Wilson S.-W., </editor> <publisher> MIT Press/Bradford Books, </publisher> <year> 1991. </year>
Reference-contexts: Brooks [11] refers to this latter criterion as "adequacy". McFarland <ref> [45] </ref> takes this last point even further. He argues that one should use an ecological approach to evaluate agent behavior: if an agent fills a market niche, then that agent is considered successful.
Reference: [46] <author> Meyer J.-A. & Guillot A., </author> <title> Simulation of Adaptive Behavior in Animats: Review and Prospects, </title> <booktitle> In: From Animals to Animats, Proceedings of the First International Conference on the Simulation of Adaptive Behavior, </booktitle> <editor> edited by Meyer J.-A. & Wilson S.-W., </editor> <publisher> MIT Press/Bradford Books 1991. </publisher>
Reference-contexts: Finally, the term "animat approach" (shorthand for "artificial animal"), which was coined by Wilson [69], is also frequently used. Several people have given definitions and written overviews of research in Autonomous Agents, among others Brooks [11], Wilson [70] and Meyer <ref> [46] </ref>. There are several reasons for giving it yet another try. First of all many researchers are still skeptical about the approach. Some claim that it isn't very different from what they have been doing all along. <p> This includes applications such as virtual actors in interactive training and entertainment systems [3] [39], interface agents [40] [53], process scheduling [42], and so on. Wilson's account [70] focuses on a scientific methodology for research in autonomous agents, while Meyer <ref> [46] </ref> aims to give an overview of the research performed so far. Finally, a third reason is that, since the approach has been around for a number of years now, it is time to perform a critical evaluation. This paper discusses the basic problems of research in adaptive autonomous agents.
Reference: [47] <author> Minsky M., </author> <title> The Society of Mind, </title> <publisher> Simon and Schuster, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Part of the reason for this more pragmatic approach is a pessimistic vision about whether it is possible at all to come up with a general solution to the vision problem, a general solution to the planning problem, etc, a view also expressed by Minsky <ref> [47] </ref>. <p> Some proposals for solutions to this problem are presented in [52] and [7]). * All of the above architectures are completely decentralized and do not keep any central state. As a result they may suffer from the lack of what Minsky would call a "B-brain" <ref> [47] </ref>. They can get stuck in loops or deadlock situations (i.e. keep activating the same actions even though they have proven not to result in any change of state). * Most of the above architectures (apart from [14]) have a narrow-minded view of the relationship between perception and action.
Reference: [48] <author> Nilsson N., </author> <title> Shakey the Robot, SRI A.I. </title> <note> Center Technical Note 323, </note> <year> 1984. </year>
Reference-contexts: Its task requires that it navigate from room to room. The traditional AI version of this robot could work in a similar way to Shakey <ref> [48] </ref>. The perception module processes the different sensor data and integrates them into a representation of the environment. It attempts to update this model as often as possible.
Reference: [49] <author> Nilsson N.J., </author> <title> Toward Agent Programs with Circuit Semantics, </title> <institution> Department of Computer Science, Report number STAN-CS-92-1412, Stanford University, </institution> <month> January </month> <year> 1992. </year> <month> 35 </month>
Reference-contexts: Compiled, Flat Networks. A second class of architectures attempts to facilitate the construction of agents by automating the process of designing the arbitration circuitry among competence modules. Examples of such architectures are the Rex/Gaps system [24], Behavior Networks [33] and Teleo-Reactive Trees <ref> [49] </ref>. These architectures require the designer to specify in a particular formalism what the goals of the agent are, how goals can be reduced to other goals or to actions and what the different modules/actions are and their conditions and expected effects.
Reference: [50] <author> Payton D.W., Keirsey D., Krozel J. & Rosenblatt K., </author> <title> Do Whatever Works: </title>
Reference-contexts: Related to the idea of learning is that of redundancy: often the system has multiple modules for a particular competence. Experience sorts out which of these modules implements the competence in a more reliable way and should thus be preferred [38] <ref> [50] </ref> [18]. Systems built using all of the above principles (task-oriented modules, task-specific solutions, de-emphasized representations, decentralized control, etc) tend to demonstrate more adaptive and robust behavior. <p> Model Builders. A final class of agents that learn from experience actually learn a causal model of their actions, rather than a policy map [18] [38] <ref> [50] </ref>. Drescher's model, which was inspired by Piaget's theories of development in infants, is probably the most sophisticated example. The agent builds up a probabilistic model of what the effects are of taking an action in a certain situation.
References-found: 50

