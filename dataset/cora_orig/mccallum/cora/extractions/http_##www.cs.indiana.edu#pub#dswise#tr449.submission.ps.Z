URL: http://www.cs.indiana.edu/pub/dswise/tr449.submission.ps.Z
Refering-URL: http://www.cs.indiana.edu/pub/dswise/
Root-URL: http://www.cs.indiana.edu
Email: jfrens, dswise@cs.indiana.edu  
Title: Auto-blocking Matrix-Multiplication or Tracking BLAS3 Performance with Source Code (Extended Abstract)  
Author: Jeremy D. Frens and David S. Wise 
Date: December 1996  
Address: Bloomington, Indiana 474054101, USA  
Affiliation: Computer Science Department, Indiana University  
Abstract: An elementary, machine-independent, recursive algorithm for matrix multiplication C+=A*B provides implicit blocking at every level of the memory hierarchy and tests out faster than classically optimal code, tracking hand-coded BLAS3 routines. Proof of concept is demonstrated by racing the in-place algorithm against manufacturer's hand-tuned BLAS3 routines; it can win. The recursive code bifurcates naturally at the top level into independent block-oriented processes, that each writes to a disjoint and contiguous region of memory. Experience has shown that the indexing vastly improves the patterns of memory access at all levels of the memory hierarchy, independently of the sizes of caches or pages and without ad hoc programming. It also exposed a weakness in SGI's C compilers that merrily unroll loops for the super-scalar R8000 processor, but don't analogously unfold the base case of the most elementary recursions. Such deficiencies might divert programmers from this class of recursive algorithms. Categories and subject descriptors: G.1.3 [Numerical Analysis]: Numerical Linear Algebralinear systems; E.1 [Data Structures]: Arrays; D.4.2 [Operating Systems]: Storage Managementsegmentation, swapping, virtual memory; B.3.2 [Memory Structures]: Design Stylesprimary memory; F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problemscomputations on matrices; G.4 [Mathematical Software]: Algorithm analysis. General Term: Algorithms. Additional Key Words and Phrases: storage management, indexing, quadtrees, swapping, cache misses, paging.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. W. Burton & J. G. Kollias. </author> <title> Comment on `The explicit quad tree as a structure for computer graphics.' </title> <journal> Comput. J. </journal> <volume> 26, </volume> <month> 2 (May </month> <year> 1983), </year> <month> 188. </month>
Reference-contexts: Ignoring the offset, this definition provides a zero-based, level-order indexing 1 across a matrix's tree [12, p. 350, 401], as illustrated in Figure 4 <ref> [3, 1] </ref>. It prompts several observations. * Every block/subtree is indexed consecutively at each of its levels. The scalars in any subblock, as terminal nodes at the same level in some subtree, are therefore indexed consecutively.
Reference: [2] <author> D. Cann. </author> <title> Retire FORTRAN? : a debate rekindled. </title> <journal> Comm. </journal> <note> ACM 35, 8 (August 1992) pp. 8189. </note>
Reference-contexts: So it is suited to both local and distributed processing. With these features as the fabric of classic FORTRAN, these results, forty years later, argue again <ref> [2] </ref> to replace it in its core applications.
Reference: [3] <author> J. Cohen & M. Roth. </author> <title> On the implementation of Strassen's fast multiplication algorithm. </title> <journal> Acta Infor-mat. </journal> <volume> 6, </volume> <month> 4 (August </month> <year> 1976), </year> <month> 341355. </month>
Reference-contexts: Ignoring the offset, this definition provides a zero-based, level-order indexing 1 across a matrix's tree [12, p. 350, 401], as illustrated in Figure 4 <ref> [3, 1] </ref>. It prompts several observations. * Every block/subtree is indexed consecutively at each of its levels. The scalars in any subblock, as terminal nodes at the same level in some subtree, are therefore indexed consecutively.
Reference: [4] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, & T. von Eicken. </author> <title> LogP: a practical model of parallel computation. </title> <journal> Comm. ACM 39, </journal> <month> 11 (November </month> <year> 1996), </year> <month> 7885. </month>
Reference-contexts: A different control structure, recursion, and another array representation, quadtree decomposition, are shown to yield elegant, machine-independent code that produces better run-time performance across the board: under both unipro-cessing and multiprocessing, and independently of the parameters of memory layering like latency, size of cache lines, and page size <ref> [4] </ref>. So it is suited to both local and distributed processing. With these features as the fabric of classic FORTRAN, these results, forty years later, argue again [2] to replace it in its core applications.
Reference: [5] <author> J. W. Demmel & N. J. Higham. </author> <title> Stability of block algorithms with fast Level 3 BLAS. </title> <journal> ACM Trans. Math. </journal> <volume> Software 18, </volume> <month> 3 (September </month> <year> 1992), </year> <month> 274291. </month>
Reference-contexts: Experiments on the algorithm expose weaknesses in production compilers: recursion unfolding versus loop unrolling, and advantages relating to process scheduling under usual runtime systems. It is well known that linear systems are best solved by algorithms that decompose matrices into blocks <ref> [5, 6] </ref>.
Reference: [6] <author> J. Dongarra, J. DuCroz, S. Hammarling, & R. Hanson. </author> <title> An extended set of FORTRAN basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Softw. </journal> <volume> 14 (1988), </volume> <pages> 117. </pages>
Reference-contexts: 1 Introduction This paper revisits matrix algebra, specifically multiplication, to explore an algorithm nearly as simple as traditional ones, and certainly better for hierarchical memory. We present a simple recursive algorithm and a matrix representation suited to it that have outperformed hand-optimized BLAS3 matrix multiplication <ref> [6] </ref>, currently offering the best performance available. Experiments on the algorithm expose weaknesses in production compilers: recursion unfolding versus loop unrolling, and advantages relating to process scheduling under usual runtime systems. It is well known that linear systems are best solved by algorithms that decompose matrices into blocks [5, 6]. <p> Experiments on the algorithm expose weaknesses in production compilers: recursion unfolding versus loop unrolling, and advantages relating to process scheduling under usual runtime systems. It is well known that linear systems are best solved by algorithms that decompose matrices into blocks <ref> [5, 6] </ref>.
Reference: [7] <author> P. C. Fischer & R. L. Probert. </author> <title> Storage reorganization techniques for matrix computation in a paging environment. </title> <journal> Comm. ACM 22, </journal> <month> 7 (July </month> <year> 1979), </year> <month> 405415. </month>
Reference-contexts: The anomaly is invisible in Figure 11. 6 Analysis This is the first analysis to explainas supported by experiments over the years <ref> [13, 7] </ref>that matrices should be partitioned into square blocks, independently of the size and shape of the problem, for swapping across boundaries in a layered memory.
Reference: [8] <author> J. Frens & D. S. Wise. </author> <title> Matrix inversion using quadtrees implemented in GOFER. </title> <type> Technical Report 433, </type> <institution> Computer Science Dept., </institution> <note> Indiana University (May 1995). </note>
Reference-contexts: All this is generic code that contrasts with the competing BLAS3 routines that have been polished over decades to maximal performance on each architecture. A functional programmer's model for matrix multiplication uses mapping functions over quadruples <ref> [18, 8] </ref> to decompose matrix problems into square blocks. It is not too different from Figure 5 which exposes the blockwise algorithm, but obfuscates both the functional syntax and the sparse-matrix algebra that motivated this representation.
Reference: [9] <author> K. A. Gallivan, R. J. Plemmons, & A. H. Sameh. </author> <title> Parallel Algorithms for dense linear algebra. </title> <note> SIAM Review 32, </note> <month> 1 (March </month> <year> 1990), </year> <note> 54135. Reprinted in Gallivan et al. Parallel Algorithms for Matrix Computation, Philadelphia, SIAM (1990), 182. </note>
Reference-contexts: Figure 1), it is well known that only two cache misses are necessary between the innermost steps. Optimal block sizes seem to depend on all of l; m; n; p; q; r; we seek good values for the last three, independent of the first three. The six familiar algorithms <ref> [9, p. 19] </ref> for the product (two using inner, two middle, and two outer-products), correspond to each of the six permutations of the three for-loops in Figure 1.
Reference: [10] <author> G. H. Golub & C. F. Van Loan. </author> <title> Matrix Computations 2nd edition. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore (1989). 16 REFERENCES </address>
Reference-contexts: If a third or fourth processor were available, then it would be committed in another bifurcation at the next level of the quadtree. If we would allow temporary storage to the algorithm, then this is the case where Strassen's recurrence <ref> [15, 10, x1.3.8] </ref> applies best (except on sparse or tiny matrices). Otherwise, there are seven cases where padding might unbalance a top-down parallel dispatch, classified by the bottom sketches in Figure 6.
Reference: [11] <author> N. J. Higham. </author> <title> Exploiting fast matrix multiplication within the Level 3 BLAS. </title> <journal> ACM Trans. Math. </journal> <volume> Software 16, </volume> <month> 4 (December </month> <year> 1990), </year> <month> 352368. </month>
Reference-contexts: Programmers using a four megabyte on-board cache should consider square, terminal nodes of order 418, where loops are no longer troubled by cache misses. We have not tested the attractive hybrid composed of Strassen's recurrence and this one; stability <ref> [11] </ref> and in-place constraints must be set aside. However, for the balanced, dense matrix (Case 0) Strassen's original 18-addition presentation also bifurcates nicely into 3-and-3 parallel block multiplications, followed by 1 serially.
Reference: [12] <author> D. E. Knuth. </author> <title> The Art of Computer Programming I, Fundamental Algorithms (2nd ed.), </title> <address> Reading, MA, </address> <publisher> Addison-Wesley, </publisher> <year> (1973). </year>
Reference-contexts: Ignoring the offset, this definition provides a zero-based, level-order indexing 1 across a matrix's tree <ref> [12, p. 350, 401] </ref>, as illustrated in Figure 4 [3, 1]. It prompts several observations. * Every block/subtree is indexed consecutively at each of its levels. The scalars in any subblock, as terminal nodes at the same level in some subtree, are therefore indexed consecutively.
Reference: [13] <author> A. C. McKellar & E. G. Coffman, Jr. </author> <title> Organizing matrices and matrix operations for paged-memory systems Comm. </title> <booktitle> ACM 12, </booktitle> <month> 3 (March </month> <year> 1969), </year> <month> 153165. </month>
Reference-contexts: The anomaly is invisible in Figure 11. 6 Analysis This is the first analysis to explainas supported by experiments over the years <ref> [13, 7] </ref>that matrices should be partitioned into square blocks, independently of the size and shape of the problem, for swapping across boundaries in a layered memory.
Reference: [14] <author> J. Spiess. </author> <title> Untersuchungen des Zeitgewinns durch neue Algorithmen zur Matrix-Multiplikation. </title> <booktitle> Computing 17, 1 (1976), </booktitle> <pages> 2336. </pages>
Reference-contexts: That suggests a hybrid scheme in which our indexing and recursion is used at the highest, unbalanced levels (Cases 17) and Strassen's is used on the intermediate, balanced subproblems. Ours again becomes appropriate for the tiny blocks <ref> [14] </ref> that fit in cache. Figure 4's indexing, of course, is used throughout any recurrenceof these two or of other algorithms. The underlying matrix representation is critical to these results.
Reference: [15] <author> V. Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numer. Math. </journal> <volume> 13 (1969), </volume> <pages> 354356. </pages>
Reference-contexts: It is not too different from Figure 5 which exposes the blockwise algorithm, but obfuscates both the functional syntax and the sparse-matrix algebra that motivated this representation. As observed elsewhere <ref> [17, 15] </ref>, the quadrants of the answer can provide a partitioning into 4, 16, 64, . . . processes to compute the answer independently of one, another. 4.1 Balanced parallel multiplication 5 int offset; Scalar *A_matrix, *B_matrix, *C_matrix; void naive_multiply (Matrix a, Matrix b, Matrix c) - offset = a.offset; A_matrix <p> If a third or fourth processor were available, then it would be committed in another bifurcation at the next level of the quadtree. If we would allow temporary storage to the algorithm, then this is the case where Strassen's recurrence <ref> [15, 10, x1.3.8] </ref> applies best (except on sparse or tiny matrices). Otherwise, there are seven cases where padding might unbalance a top-down parallel dispatch, classified by the bottom sketches in Figure 6.
Reference: [16] <author> G. L. Steele, Jr. </author> <title> Debunking the expensive procedure call myth, or Procedure call implementations considered harmful, or LAMBDA: the ultimate GOTO. </title> <booktitle> ACM77: Proc. 1977 Annual Conf., </booktitle> <address> New York, </address> <note> ACM (1977), 153162. </note>
Reference-contexts: Management for the instruction cache ought to be handled by compilers, targeted as they are to specific hardware. Unfortunately, these compilers do a poor job with ordinary function linkage, stacking when it is unnecessary, and not unfolding recursions at all. (Nothing new there <ref> [16] </ref>!) So C's macro facility was used to unfold the base case manually (within constraints of the instruction cache, as a good compiler would). A typical result of unfolding appears in Figure 8.
Reference: [17] <author> D. S. Wise. </author> <title> Representing matrices as quadtrees for parallel processors (extended abstract). </title> <journal> ACM SIGSAM Bulletin 18, </journal> <month> 3 (August </month> <year> 1984), </year> <month> 2425. </month>
Reference-contexts: Although additional storage may be available at distributed processors, it uses none, except for local variables in a recursion stack of depth lg n for order n matricesconstant space for all practical purposes. Four insights underlie the algorithms. First is the quadtree decomposition of matrices <ref> [17] </ref>, as well as the algorithms that manipulate them using recursive descent [18]. Second is a familiar indexing, newly applied to the map of matrices onto the address space. <p> This restriction is relaxed later with negligible overhead; the timing curves are smooth. Definition 1 <ref> [17] </ref> A complete matrix has index 0. <p> It is not too different from Figure 5 which exposes the blockwise algorithm, but obfuscates both the functional syntax and the sparse-matrix algebra that motivated this representation. As observed elsewhere <ref> [17, 15] </ref>, the quadrants of the answer can provide a partitioning into 4, 16, 64, . . . processes to compute the answer independently of one, another. 4.1 Balanced parallel multiplication 5 int offset; Scalar *A_matrix, *B_matrix, *C_matrix; void naive_multiply (Matrix a, Matrix b, Matrix c) - offset = a.offset; A_matrix
Reference: [18] <author> D. S. Wise. </author> <title> Undulant block elimination and integer-preserving matrix inversion. </title> <type> Technical Report 418, </type> <institution> Computer Science Department, Indiana University (revised, </institution> <month> August </month> <year> 1995). </year>
Reference-contexts: Four insights underlie the algorithms. First is the quadtree decomposition of matrices [17], as well as the algorithms that manipulate them using recursive descent <ref> [18] </ref>. Second is a familiar indexing, newly applied to the map of matrices onto the address space. Next is a decomposition of the usual eight recursive, quadrant multiplications into two parallel streams, balancing computational loads when the factors have known padding (east or south) with zeroes. <p> All this is generic code that contrasts with the competing BLAS3 routines that have been polished over decades to maximal performance on each architecture. A functional programmer's model for matrix multiplication uses mapping functions over quadruples <ref> [18, 8] </ref> to decompose matrix problems into square blocks. It is not too different from Figure 5 which exposes the blockwise algorithm, but obfuscates both the functional syntax and the sparse-matrix algebra that motivated this representation.
References-found: 18

