URL: http://www.cs.wustl.edu/cs/techreports/1992/wucs-92-28.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Email: sg@cs.wustl.edu  mkearns@research.att.com  
Title: On the Complexity of Teaching  
Author: Sally A. Goldman Michael J. Kearns 
Date: August 11, 1992  
Address: St. Louis, MO 63130  Murray Hill, NJ 07974  
Affiliation: Department of Computer Science Washington University  AT&T Bell Laboratories  
Pubnum: WUCS-92-28  
Abstract: While most theoretical work in machine learning has focused on the complexity of learning, recently there has been increasing interest in formally studying the complexity of teaching. In this paper we study the complexity of teaching by considering a variant of the on-line learning model in which a helpful teacher selects the instances. We measure the complexity of teaching a concept from a given concept class by a combinatorial measure we call the teaching dimension. Informally, the teaching dimension of a concept class is the minimum number of instances a teacher must reveal to uniquely identify any target concept chosen from the class. fl A preliminary version of this paper appeared in the Proceedings of the Fourth Annual Workshop on Computational Learning Theory, pages 303-314. August 1991. Most of this research was carried out while both authors were at MIT Laboratory for Computer Science with support provided by ARO Grant DAAL03-86-K-0171, DARPA Contract N00014-89-J-1988, NSF Grant CCR-88914428, and a grant from the Siemens Corporation. S. Goldman is currently supported in part by a G.E. Foundation Junior Faculty Grant and NSF Grant CCR-9110108. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: Their work suggests an interesting variation on our teaching model in which the teaching sequence is only required to eliminate those hypotheses that are not "close" to the target. Such a model would use a PAC-style [19] success criterion for the learner versus the exact-identification-style <ref> [1] </ref> criterion that we have used here. In other related work, Anthony, Brightwell, Cohen, and Shawe-Taylor [2] define the specification number of a concept c 2 C to be the cardinality of the smallest sample for which only c is consistent with the sample.
Reference: [2] <author> Martin Anthony, Graham Brightwell, Dave Cohen, and John Shawe-Taylor. </author> <title> On exact specification by examples. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 311-318. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: Such a model would use a PAC-style [19] success criterion for the learner versus the exact-identification-style [1] criterion that we have used here. In other related work, Anthony, Brightwell, Cohen, and Shawe-Taylor <ref> [2] </ref> define the specification number of a concept c 2 C to be the cardinality of the smallest sample for which only c is consistent with the sample. Thus the specification number is just the length of the optimal teaching sequence for c. <p> More recently, Anthony, et al. <ref> [2] </ref> have considered the problem of computing an optimal teaching sequence when it is known that every instance is a positive example of exactly three concepts from C.
Reference: [3] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. War-muth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Informally, the teaching dimension of a concept class is the minimum number of instances a teacher must reveal to uniquely identify any target concept chosen from the class. We show that this new dimension measure is fundamentally different from the Vapnik-Chervonenkis dimension <ref> [3, 11, 20] </ref> and the dimension measure of Natara-jan [13]. <p> The Vapnik-Chervonenkis dimension of C, denoted vcd (C), is defined to be the smallest d for which no set of d + 1 points is shattered by C. Blumer et al. <ref> [3] </ref> have shown that this combinatorial measure of a concept class characterizes the number of examples required for learning any concept in the class under the distribution-free or PAC model of Valiant [19]. In the next section we briefly discuss some related work. <p> Blumer et al. <ref> [3] </ref> have shown that this combinatorial measure of a concept class exactly characterizes (modulo dependencies on the accuracy and confidence parameters) the number of examples required for learning under the distribution-free or PAC model of Valiant [19]. We now compare the teaching dimension to the VC dimension.
Reference: [4] <author> V. Chvatal. </author> <title> A greedy heuristic for the set covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4(3) </volume> <pages> 233-235, </pages> <year> 1979. </year>
Reference-contexts: Corollary 9 The optimal teaching sequence problem is N P-hard even if it is known that each instance in X is a positive example for at most three concepts from C. While it is N P-complete to compute a minimum set covering, Chvatal <ref> [4] </ref> proves that the greedy algorithm (which is a polynomial-time algorithm) computes a cover that is within a logarithmic factor of the minimum cover.
Reference: [5] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: The question is then: Is there a teaching sequence for c fl of length k or less? We now show that this problem is equivalent to the minimum cover problem. (See Garey and Johnson <ref> [5] </ref> for a formal description of the minimum cover problem.) Theorem 8 The optimal teaching sequence problem is equivalent to a minimum cover problem in which there are jCj 1 objects to be covered and jXj sets from which to form the covering. <p> By giving a reduction to exact cover by 3-sets, they show that even in this restricted situation the problem of computing an optimal teaching sequence is N P-hard. Since, the set covering problem is known to be N P-complete even when all sets have size at most three <ref> [5] </ref>, the following corollary to Theorem 8 immediately follows. Corollary 9 The optimal teaching sequence problem is N P-hard even if it is known that each instance in X is a positive example for at most three concepts from C.
Reference: [6] <author> Sally A. Goldman, Michael J. Kearns, and Robert E. Schapire. </author> <title> Exact identification of circuits using fixed points of amplification functions. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 193-202, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Note, however, that the optimal teaching sequence is allowed to vary with the target concept, as opposed to the universal identification sequences of Goldman, Kearns and Schapire <ref> [6] </ref>. Finally, we define the Vapnik-Chervonenkis dimension [20]. Let X be the instance space, and C be a concept class over X. A finite set Y X is shattered by C if fc " Y j c 2 Cg = 2 Y . <p> The fundamental philosophical difference between their work and ours is that we do not assume that the teacher knows the algorithm used by the learner. The work of Goldman, Kearns and Schapire <ref> [6] </ref>, in which they described a technique for exactly identifying certain classes of read-once formulas from random examples, is also related to our work. They defined a universal identification sequence for a 6 concept class C as a single instance sequence that distinguishes every concept c 2 C. <p> Finally, we suggest the following open problems. It would be quite informative to determine whether large and powerful classes (such as polynomial-sized monotone circuits) have polynomial teaching dimensions. Potentially the technique of Goldman, Kearns, and Schapire <ref> [6] </ref> may be useful in solving this problem. Another good area of research is to study the time complexity of computing optimal teaching sequences.
Reference: [7] <author> Sally A. Goldman, Ronald L. Rivest, and Robert E. Schapire. </author> <title> Learning binary relations and total orders. </title> <type> Technical Report MIT/LCS/TM-413, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> May </month> <year> 1990. </year> <note> A preliminary version is available in Proceedings of the Thirtieth Annual Symposium on Foundations of Computer Science, pages 46-51, </note> <year> 1989. </year>
Reference-contexts: 1 Introduction While most theoretical work in machine learning has focused on the complexity of learning, recently there has been some work on the complexity of teaching <ref> [7, 8, 13, 16, 17] </ref>. In this paper we study the complexity of teaching by considering a variant of the on-line learning model in which a helpful teacher selects the instances (this is the teacher-directed learning model of Goldman, Rivest, and Schapire [7]). <p> In this paper we study the complexity of teaching by considering a variant of the on-line learning model in which a helpful teacher selects the instances (this is the teacher-directed learning model of Goldman, Rivest, and Schapire <ref> [7] </ref>). We measure the complexity of teaching a concept from a given concept class by a combinatorial measure we call the teaching dimension. <p> In Section 6 we summarize our results and discuss some open problems. 4 3 Related Work Our work is directly motivated by the teacher-directed learning model of Goldman, Rivest and Schapire <ref> [7] </ref>. In fact, the teaching dimension of a concept class is equal to the optimal mistake bound under teacher-directed learning (which considers the worst-case mistake bound over all consistent learners). <p> Furthermore, the teaching dimension is an upper bound for the optimal mistake bound since no mistakes could be made after the target concept has been exactly identified. Thus Goldman et al. <ref> [7] </ref> have computed exact bounds for the teaching dimension for binary relations and total orders. Independently, Shinohara and Miyano [18] introduced a notion of teachability that shares the same basic framework as our work. <p> This gives a contradiction. Thus an algorithm that achieves exact identification using membership queries provides an upper bound on the teaching dimension. As noted earlier, since the teaching dimension is equivalent to the optimal mistake bound under teacher-directed learning, the results of Goldman et al. <ref> [7] </ref> give tight bounds on the teaching dimension for binary relations and total orders.
Reference: [8] <author> Sally Ann Goldman. </author> <title> Learning Binary Relations, Total Orders, and Read-once Formulas. </title> <type> PhD thesis, </type> <institution> MIT Dept. of Electrical Engineering and Computer Science, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: 1 Introduction While most theoretical work in machine learning has focused on the complexity of learning, recently there has been some work on the complexity of teaching <ref> [7, 8, 13, 16, 17] </ref>. In this paper we study the complexity of teaching by considering a variant of the on-line learning model in which a helpful teacher selects the instances (this is the teacher-directed learning model of Goldman, Rivest, and Schapire [7]).
Reference: [9] <author> S. Goldwasser, S. Goldwasser, and C. Rackoff. </author> <title> The knowledge complexity of interactive proofs. </title> <booktitle> In 26th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 291-304, </pages> <month> October </month> <year> 1985. </year> <month> 28 </month>
Reference-contexts: To avoid collusion between the teacher and learner, they consider the interaction between the teacher and learner as a modified prover-verifier session <ref> [9] </ref> in which the learner and teacher can collude, but no adversarial teacher can cause the learner to output an hypothesis inconsistent with the sample.
Reference: [10] <author> Jeffrey Jackson and Andrew Tomkins. </author> <title> A computational model of teaching. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 319-326. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: So a concept class is teachable by examples if the teaching dimension is polynomially bounded. The primary focus of their work is to establish a relationship between learnability and teachability 1 . Recently, Jackson and Tomkins <ref> [10] </ref> have considered a variation of our teaching model in which they can study teacher/learner pairs in which the teacher chooses examples tailored to a particular learner.
Reference: [11] <author> Nathan Linial, Yishay Mansour, and Ronald L. Rivest. </author> <title> Results on learnability and the Vapnik-Chervonenkis dimension. </title> <booktitle> In 29th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 120-129, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Informally, the teaching dimension of a concept class is the minimum number of instances a teacher must reveal to uniquely identify any target concept chosen from the class. We show that this new dimension measure is fundamentally different from the Vapnik-Chervonenkis dimension <ref> [3, 11, 20] </ref> and the dimension measure of Natara-jan [13].
Reference: [12] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexamples. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 262-267, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: for the smallest j for which y j is 1. 5.3 Orthogonal Rectangles in f0; 1; ; n 1g d We now consider the concept class of orthogonal rectangles in f0; 1; ; n1g d . (This is the same as the class box d n of Maass and Turan <ref> [12] </ref>.) Theorem 14 4 For the concept class C d of orthogonal rectangles in f0; 1; ; n1g d : td (C d ) = 2 + 2d: Proof: We build the following teaching sequence T .
Reference: [13] <author> B. K. Natarajan. </author> <title> On learning Boolean functions. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 296-304, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: 1 Introduction While most theoretical work in machine learning has focused on the complexity of learning, recently there has been some work on the complexity of teaching <ref> [7, 8, 13, 16, 17] </ref>. In this paper we study the complexity of teaching by considering a variant of the on-line learning model in which a helpful teacher selects the instances (this is the teacher-directed learning model of Goldman, Rivest, and Schapire [7]). <p> We show that this new dimension measure is fundamentally different from the Vapnik-Chervonenkis dimension [3, 11, 20] and the dimension measure of Natara-jan <ref> [13] </ref>. While we show that there is a concept class C for which the teaching dimension is jCj 1, we prove that in such cases there is one "hard-to-teach" concept that when removed yields a concept class that has a teaching dimension of one. <p> Observe that a universal identification sequence is always a teaching sequence (modulo different labelings) for any concept in the class, and thus their work provides an upper bound on the teaching dimension for the concept classes they considered. Finally, Natarajan <ref> [13] </ref> defines a dimension measure for concept classes of Boolean functions that measures the complexity of a concept class by the length of the shortest example sequence for which the target concept is the unique most specific concept consistent with the sample. <p> Finally, we use the naive algorithm of Observation 2 to eliminate the remaining functions using at most jCj 2 d additional examples. 4.2 Natarajan's Dimension Measure In this section we compare the teaching dimension to the following dimension measure defined by Natarajan <ref> [13] </ref> for concept classes of Boolean functions: 11 nd (C) = min 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; : For all c 2 C; there exists a labeled sample S c of cardinality d such that c is consistent with S c and for all
Reference: [14] <author> Ronald L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: Let V n = fv 1 ; v 2 ; . . . ; v n g be a set of n Boolean variables. Let the instance space X n = f0; 1g n . The class C n of monotone decision lists <ref> [14] </ref> is defined as follows. A concept c 2 C n is a list L = h (y 1 ; b 1 ); . . . (y ` ; b ` )i where each y i 2 V n and each b i 2 f0; 1g.
Reference: [15] <author> Kathleen Romanik. </author> <title> Approximate testing and learnability. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 327-332. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: In our results presented here, the learner and teacher sometimes share such information, but we do not provide a formal characterization of what kind of information can be shared in this manner. The work of Romanik and Smith <ref> [16, 15] </ref> on testing geometric objects shares similarities with our work. They propose a testing problem that involves specifying for a given target concept a set of test points that can be used to determine if a tested object is equivalent to the target. <p> show the following d negative instances: for each dimension, give the neighboring point (unless the given point is on the border of the space f0; 1; ; n 1g d in the given dimension) just outside the box in that dimension as a negative instance. (See 4 Romanik and Smith <ref> [16, 15] </ref> independently obtained this result for the special case that d = 2. 20 rectangles in f0; 1; ; n1g d for n = 20 and d = 2. with T , thus to prove that T is a teaching sequence we need only show that it is the only
Reference: [16] <author> Kathleen Romanik and Carl Smith. </author> <title> Testing geometric objects. </title> <type> Technical Report UMIACS-TR-90-69, </type> <institution> University of Maryland College Park, Department of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: 1 Introduction While most theoretical work in machine learning has focused on the complexity of learning, recently there has been some work on the complexity of teaching <ref> [7, 8, 13, 16, 17] </ref>. In this paper we study the complexity of teaching by considering a variant of the on-line learning model in which a helpful teacher selects the instances (this is the teacher-directed learning model of Goldman, Rivest, and Schapire [7]). <p> In our results presented here, the learner and teacher sometimes share such information, but we do not provide a formal characterization of what kind of information can be shared in this manner. The work of Romanik and Smith <ref> [16, 15] </ref> on testing geometric objects shares similarities with our work. They propose a testing problem that involves specifying for a given target concept a set of test points that can be used to determine if a tested object is equivalent to the target. <p> show the following d negative instances: for each dimension, give the neighboring point (unless the given point is on the border of the space f0; 1; ; n 1g d in the given dimension) just outside the box in that dimension as a negative instance. (See 4 Romanik and Smith <ref> [16, 15] </ref> independently obtained this result for the special case that d = 2. 20 rectangles in f0; 1; ; n1g d for n = 20 and d = 2. with T , thus to prove that T is a teaching sequence we need only show that it is the only
Reference: [17] <author> Steven Salzberg, Arthur Delcher, David Heath, and Simon Kasif. </author> <title> Learning with a helpful teacher. </title> <booktitle> In 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 705-711, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction While most theoretical work in machine learning has focused on the complexity of learning, recently there has been some work on the complexity of teaching <ref> [7, 8, 13, 16, 17] </ref>. In this paper we study the complexity of teaching by considering a variant of the on-line learning model in which a helpful teacher selects the instances (this is the teacher-directed learning model of Goldman, Rivest, and Schapire [7]). <p> Thus the specification number is just the length of the optimal teaching sequence for c. Their paper studies several aspects of the specification number with an emphasis on determining the specification numbers of hypotheses in the set of linearly separable Boolean functions. Salzberg, Delcher, Heath and Kasif <ref> [17] </ref> have also considered a model of learning with a helpful teacher. Their model requires the teacher to present the shortest example sequence so that any learner using a particular algorithm (namely, the nearest-neighbor algorithm) learns the target concept.
Reference: [18] <author> Ayumi Shinohara and Satoru Miyano. </author> <title> Teachability in computational learning. </title> <journal> New Generation Computing, </journal> <volume> 8 </volume> <pages> 337-347, </pages> <year> 1991. </year>
Reference-contexts: Thus Goldman et al. [7] have computed exact bounds for the teaching dimension for binary relations and total orders. Independently, Shinohara and Miyano <ref> [18] </ref> introduced a notion of teachability that shares the same basic framework as our work. In particular, they consider a notion of teachability in which a concept class is teachable by examples if there exists a polynomial size sample under which all consistent learners will exactly identify the target concept. <p> Thus the VC dimension can be arbitrarily smaller than the teaching dimension. Furthermore, we note that the concept class used in the proof of Lemma 1 has the largest possible teaching dimension. Observation 2 For any concept class C, td (C) jCj 1. 2 Independently, Shinohara and Miyano <ref> [18] </ref> give a construction of a class of concepts that is PAC-learnable but not teachable by examples. 8 x 0 x 1 x 2 x n2 x n1 x n x n+1 x n+lg n1 c 1 + + . . . c n1 + + + + Proof: Each concept <p> It is easily seen that an optimal teaching sequence directly corresponds to an optimal set covering with jCj 1 objects and jXj sets. 13 We note that Shinohara and Miyano <ref> [18] </ref> independently obtained the similar re-sult that computing an optimal teaching sequence (what they call the minimum key problem) is N P-complete by giving a reduction from the hitting set problem. <p> Theorem 11 3 For the concept class C n of monotone monomials over n variables td (C n ) = min (r + 1; n) where r is the number of relevant variables. 3 Shinohara and Miyano <ref> [18] </ref> independently showed that the teaching dimension of monotone monomials over n variables is at most n. 15 Proof: We begin by exhibiting a teaching sequence of length min (r + 1; n). First present a positive example in which all relevant variables are 1 and the rest are 0.
Reference: [19] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Blumer et al. [3] have shown that this combinatorial measure of a concept class characterizes the number of examples required for learning any concept in the class under the distribution-free or PAC model of Valiant <ref> [19] </ref>. In the next section we briefly discuss some related work. In Section 4 we compare the teaching dimension to both the Vapnik-Chervonenkis dimension and Natarajan's dimension measure. <p> Their work suggests an interesting variation on our teaching model in which the teaching sequence is only required to eliminate those hypotheses that are not "close" to the target. Such a model would use a PAC-style <ref> [19] </ref> success criterion for the learner versus the exact-identification-style [1] criterion that we have used here. <p> Blumer et al. [3] have shown that this combinatorial measure of a concept class exactly characterizes (modulo dependencies on the accuracy and confidence parameters) the number of examples required for learning under the distribution-free or PAC model of Valiant <ref> [19] </ref>. We now compare the teaching dimension to the VC dimension.
Reference: [20] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year> <month> 29 </month>
Reference-contexts: Informally, the teaching dimension of a concept class is the minimum number of instances a teacher must reveal to uniquely identify any target concept chosen from the class. We show that this new dimension measure is fundamentally different from the Vapnik-Chervonenkis dimension <ref> [3, 11, 20] </ref> and the dimension measure of Natara-jan [13]. <p> Note, however, that the optimal teaching sequence is allowed to vary with the target concept, as opposed to the universal identification sequences of Goldman, Kearns and Schapire [6]. Finally, we define the Vapnik-Chervonenkis dimension <ref> [20] </ref>. Let X be the instance space, and C be a concept class over X. A finite set Y X is shattered by C if fc " Y j c 2 Cg = 2 Y .
References-found: 20

