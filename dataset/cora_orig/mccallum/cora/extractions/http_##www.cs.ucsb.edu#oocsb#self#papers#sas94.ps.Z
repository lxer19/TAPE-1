URL: http://www.cs.ucsb.edu/oocsb/self/papers/sas94.ps.Z
Refering-URL: http://www.cs.ucsb.edu/oocsb/self/papers/sas94.html
Root-URL: http://www.cs.ucsb.edu
Email: agesen@cs.stanford.edu  
Title: Constraint-Based Type Inference and Parametric Polymorphism  
Author: Ole Agesen 
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Abstract: Constraint-based analysis is a technique for inferring implementation types. Traditionally it has been described using mathematical formalisms. We explain it in a different and more intuitive way as a ow problem. The intuition is facilitated by a direct correspondence between run-time and analysis-time concepts. Precise analysis of polymorphism is hard; several algorithms have been developed to cope with it. Focusing on parametric polymorphism and using the ow perspective, we analyze and compare these algorithms, for the first time directly characterizing when they succeed and fail. Our study of the algorithms lead us to two conclusions. First, designing an algorithm that is either efficient or precise is easy, but designing an algorithm that is efficient and precise is hard. Second, to achieve efficiency and precision simultaneously, the analysis effort must be actively guided towards the areas of the program with the highest pay-off. We define a general class of algorithms that do this: the adaptive algorithms. The two most powerful of the five algorithms we study fall in this class. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Agesen, O., L. Bak, C. Chambers, B.W. Chang, U. Hlzle, J. Maloney, R.B. Smith, D. Ungar, M. Wolczko. </author> <title> How to use Self 3.0 & The Self 3.0 Programmer's Reference Manual. 1993. Sun Microsystems Laboratories, </title> <type> 2550 Garcia Avenue, </type> <institution> Mountain View, </institution> <address> CA 94043, USA. </address> <note> Available by anonymous ftp from self.stanford.edu or www: http://self.stanford.edu/. </note>
Reference-contexts: Second, data polymorphism is the ability to store objects of different types in a variable or slot. Link objects forming a heterogeneous list of integers, oats, and strings, exhibit data polymorphism because their contents slots contain objects of several types. Throughout this paper we rely on the Self system <ref> [1, 24] </ref> for examples. Still, our results are language independent, and we could equally well have chosen any other polymorphic language, even one which is not object-oriented.
Reference: 2. <author> Agesen, O., J. Palsberg, and M.I. Schwartzbach, </author> <title> Type Inference of Self: Analysis of Objects with Dynamic and Multiple Inheritance. </title> <booktitle> In ECOOP '93, Seventh European Conference on Object-Oriented Programming. 1993. </booktitle> <address> Kaiserslautern. </address> <publisher> Springer-Verlag (LNCS 707). </publisher>
Reference-contexts: The latter is remarkable since object-oriented dynamic dispatch and inheritance interacts with parametric poly-morphism by restricting the zeroth argument (receiver) of a method to be an object which inherits the method. In <ref> [2] </ref> it is shown that the interaction can be resolved by doing lookups at type inference time. The approach precisely handles multiple and dynamic inheritance, and generalizes to multiple dispatch (multi-methods). <p> At the conclusion of this step, the type variables will constitute the nodes in a directed Table 1. The five algorithms we analyze. Name Section Reference Adaptive? Basic algorithm 3.1 [16] No 1-level expansion algorithm 3.2 [16] No p-level expansion algorithm 3.3 [19] No Hash function algorithm 3.5 <ref> [2] </ref> Yes Iterative algorithm 3.6 [20] Yes 5 graph. The edges will be constraints. A constraint is the type-inference-time equivalent of a run-time data ow. For example, if the target program executes the assignment x:=y there is a data ow from y to x. <p> Fig. 3 shows the constraint for the clone primitive. Scheme has an if expression which is illustrated in Fig. 4. Basic data types such as integers and oats and their operators should also be taken into account. More examples are found in <ref> [2, 15, 16, 20] </ref>. When a constraint is added, more propagation becomes possible. The reverse also holds: propagation makes new constraints necessary. <p> It can almost be argued that this abstraction is necessary, since even moderately sized programs generate constraints by the thousands <ref> [2, 20] </ref>. 3.1.2 Relation Between Run Time and Type Inference Time In constraint-based type inference there is a direct correspondence between program execution concepts and type inference concepts. We have already seen two examples: object @ object type and data ow @ constraint. <p> We believe their numbers are indicative, if not in absolute terms at least relatively. (The p=k=0 algorithm applied to and coded in Self is roughly 100 times faster for the 3+4 example; see also <ref> [2] </ref> which presents extensive measurements of a more advanced algorithm applied to Self programs). 3.4 Introduction to Adaptive Algorithms Precise type inference must avoid mixing types when mapping run time to type inference time. Types mix if two incompatible activation records are represented by the same template. <p> They can thereby avoid the unfavorable trade-off between efficiency and precision that, e.g., the p-level expansion algorithm exhibited. We are now ready to study two specific adaptive algorithms. 3.5 Hash Function Algorithm The hash function algorithm was the first adaptive algorithm to be developed <ref> [2] </ref>. It dramatically improved precision and efficiency over the non-adaptive algorithms. As a result, useful types could be inferred for many Self programs for the first time. Today the hash function algorithm appears to be outperformed by the algorithm described in Section 3.6. <p> Without a precise handling of polymorphic arguments, the type inference algorithm will lose information when analyzing the double-dispatching code, since it is unable to keep distinct types separate when they appear as arguments. To address these shortcomings, the algorithm described in <ref> [2] </ref> supplements the hash function with a small number of additional rules. The rules specify that templates should never be shared for a certain small group of methods, including the double-dispatching methods, ifTrue:False:, and a few others.
Reference: 3. <author> Agesen, O. and D. Ungar, </author> <title> Sifting Out the Gold: Delivering Compact Applications from an Object-Oriented Exploratory Programming Environment. </title> <note> To be presented at OOPSLA94. </note>
Reference-contexts: 1 Introduction Static analysis of programs is gaining importance: it is the core of any optimizing or parallelizing compiler [20]. Other programming tools based on static analysis are also emerging: <ref> [3] </ref> describes a tool that can extract compact applications from integrated programming environments by analyzing an image to identify a small but sufficient set of objects for a given application. Constraint-based analysis is a general technique for computing implementation types (or representation/concrete types). Implementation types are sets of classes.
Reference: 4. <author> Bondorf, A., </author> <title> Automatic Autoprojection of Higher Order Recursive Equations. </title> <booktitle> In Science of Computer Programming, </booktitle> <pages> 17(1-3), </pages> <year> 1991, </year> <pages> p. 3-34. </pages>
Reference-contexts: A further comparison of the two systems, and a proof that the ow-based approach in some sense is more powerful, can be found in [18]. Closer to the constraint-based analysis described here, is Sestofts closure analysis [21, 22] which Bondorf reviews in <ref> [4] </ref>. The analysis computes, for each application point in the target program, a superset of the closures that may be applied at that point. Closure analysis for higher-order functional languages face similar difficulties as inference of implementation types for object-oriented programs.
Reference: 5. <author> Chambers, C., D. Ungar, and E. Lee, </author> <title> An Efficient Implementation of SELF, a Dynamically-Typed Object-Oriented Language Based on Prototypes. </title> <booktitle> In OOPSLA '89, Object-Oriented Programming Systems, Languages and Applications. 1989. </booktitle> <address> New Orleans, LA. </address> <booktitle> Also published in Lisp and Symbolic Computation 4(3), </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <month> June, </month> <year> 1991. </year>
Reference-contexts: Thus, two uses of a given method can share a template only if they provide the same receiver object, ensuring that the type of self is a singleton set (this is similar to customization <ref> [5] </ref>). This improves type inference precision in two ways. First, inherited methods are reanalyzed in the context of every object that inherits them. Second, sends to self, which tend to be common, can be analyzed more precisely because a single target can be found.
Reference: 6. <author> Chien, A.A., V. Karamcheti, and J. Plevyak, </author> <title> The Concert System Compiler and Run time Support for Efficient, Fine-Grained Concurrent Object-Oriented Programs. </title> <institution> Department of Computer Science, University of Illinois Urbana-Champaign, </institution> <type> Technical Report UIUCDCS-R-93-1815, </type> <year> 1993. </year>
Reference-contexts: affect the robustness of the type inference algorithm, making it sensitive to how certain things in the Self world are coded. 3.6 The Iterative Algorithm Plevyak and Chien [20] developed an iterative adaptive type inference algorithm and applied it to the Concurrent Aggregates language in the Illinois Concert System language <ref> [6] </ref>, a dynamically typed, single inheritance, object-oriented language. Their algorithm improved the precision for both parametric and data polymorphism, but we will only consider the former. The iterative algorithm gains precision over the basic algorithm by repeatedly inferring types for the target program.
Reference: 7. <author> Consel, C., </author> <title> Polyvariant Binding-Time Analysis For Applicative Languages, </title> <address> ACM-PEPM93, Copenhagen, Denmark. </address>
Reference-contexts: In the object-oriented case, the challenge is to accurately track objects that carry along methods from their creation points to points they are sent messages. A distinction has been made between monovariant and polyvariant analysis by people working on partial evaluation, see Consel <ref> [7] </ref>. In monovariant analysis each function is analyzed once, analogously to the basic algorithm described in Section 3.1.
Reference: 8. <author> Cousot, P. and R. Cousot, </author> <title> Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints. </title> <booktitle> In Symposium on Principles of Programming Languages. </booktitle> <year> 1977. </year> <month> 23 </month>
Reference-contexts: The table also specifies which algorithms are adaptive (defined in Section 3.4). 3.1 The Basic Algorithm The basic algorithm for constraint-based analysis, described in [16], essentially simulates a general execution of the target program. In this regard it is like abstract interpretation <ref> [8, 23] </ref>. We describe the basic algorithm in some detail here to present a data ow perspective on constraint-based analysis. Constraint-based type inference is a three step process. Below we describe each step as the basic algorithm executes it.
Reference: 9. <author> Graver, J.O. and R.E. Johnson, </author> <title> A Type System for Smalltalk. </title> <booktitle> In Seventeenth Symposium on Principles of Programming Languages. 1990. </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: For brevity, when the context disambiguates, we usually drop the distinction between objects and object types and simply write types as true, false-. The idea that sets of objects (or classes) form a useful notion of type was developed by Graver and Johnson <ref> [9, 13] </ref>. 3 Type Inference Algorithms We are now ready to analyze the basic type inference algorithm and the four improvements. We present the algorithms in order of increasing power and precision. Table 1 1. This relation must be refined to analyze data polymorphism precisely.
Reference: 10. <author> Hall, M.W. and K. Kennedy, </author> <title> Efficient Call Graph Analysis. </title> <journal> ACM Letters on Programming Languages and Systems, 1992. </journal> <volume> 1(3) p. </volume> <pages> 227-242. </pages>
Reference-contexts: Implementation types are perhaps most useful as a basis for further analysis. For instance, they facilitate call graph analysis which is important because many classical optimization algorithms require call graphs. Hall and Kennedy give an algorithm for computing call graphs <ref> [10] </ref>. They emphasize efficiency rather than generality so the algorithm does not handle assignable procedure-valued variables or dynamic dispatch. Thus, while useful for analyzing Fortran, it is not effective on object-oriented programs where control ow and data ow are coupled through ubiquitous use of dynamically dispatched message sends.
Reference: 11. <author> Hense, </author> <title> A.V., Polymorphic Type Inference for a Simple Object Oriented Programming Language With State. </title> <type> Tech. </type> <note> Bericht Nr. A 20/90 (Technical Report), </note> <institution> Universitt des Saarlandes, </institution> <year> 1990. </year>
Reference-contexts: Consequently, if the max: method is used by two different sends, types are inferred for it twice. This improvement 5 enabled them to type small test programs that had previously proven too hard for other type inference algorithms such as <ref> [11] </ref>. We express the idea by creating several templates from each method in the program; in contrast, the basic algorithm creates only one template per method. Specifically, the improved algorithm connects each send S that may invoke a given method M to its own M-template.
Reference: 12. <author> Hlzle, U. and D. Ungar, </author> <title> Optimizing Dynamically-Dispatched Calls with Run-Time Type Feedback. </title> <note> To appear in PLDI94, </note> <month> June </month> <year> 1994. </year>
Reference-contexts: Inlining allows a method to be compiled in a specific context, hopefully improving the generated code (elimination of the call overhead is often less significant than optimizations made possible by compiling in a more specific context <ref> [12] </ref>). 3.2.1 When Does it Work? Below we show that the 1-level expansion is only a slim improvement over the basic algorithm, but first we give an example where the expansion actually helps: b1 ifTrue: [horse] False: [donkey]. b2 ifTrue: [tiger] False: [jaguar].
Reference: 13. <author> Johnson, R.E., </author> <title> Type-Checking Smalltalk. </title> <booktitle> In OOPSLA '86 Object-Oriented Programming Systems, Languages and Applications. </booktitle> <year> 1986. </year>
Reference-contexts: For brevity, when the context disambiguates, we usually drop the distinction between objects and object types and simply write types as true, false-. The idea that sets of objects (or classes) form a useful notion of type was developed by Graver and Johnson <ref> [9, 13] </ref>. 3 Type Inference Algorithms We are now ready to analyze the basic type inference algorithm and the four improvements. We present the algorithms in order of increasing power and precision. Table 1 1. This relation must be refined to analyze data polymorphism precisely.
Reference: 14. <author> Milner, R., </author> <title> A Theory of Type Polymorphism in Programming. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 17, </volume> <year> 1978, </year> <pages> p. 348-375. </pages>
Reference-contexts: Static analysis of functional programs has been based on inference of principal types where an untyped program is annotated with type declarations that are derived 20 over a grammar of type expressions <ref> [14] </ref>. Wand has shown how inference of principal types can be formulated as a constraint problem [25]. Wands constraints have a different interpretation than the constraints found in this paper. His constraints are over the domain of type expressions, allowing him to build type expressions using unification.
Reference: 15. <author> Oxhj, N., J. Palsberg, and M.I. Schwartzbach, </author> <title> Making Type Inference Practical. </title> <booktitle> In ECOOP '92, Sixth European Conference on Object-Oriented Programming. 1992. </booktitle> <address> Utrecht, The Netherlands. </address> <publisher> Springer-Verlag (LNCS 615). </publisher>
Reference-contexts: Fig. 3 shows the constraint for the clone primitive. Scheme has an if expression which is illustrated in Fig. 4. Basic data types such as integers and oats and their operators should also be taken into account. More examples are found in <ref> [2, 15, 16, 20] </ref>. When a constraint is added, more propagation becomes possible. The reverse also holds: propagation makes new constraints necessary. <p> The car and cdr functions in LISP, the new method in Smalltalk, and iter-ators on user-defined data structures in any language are prime examples. 3.2 The 1-Level Expansion Algorithm Palsberg, Schwartzbach, and Oxhj acknowledged that high-precision type inference requires avoiding the mixing of types <ref> [15, 16] </ref>. To this end they proposed retyping each method for every send invoking it. Consequently, if the max: method is used by two different sends, types are inferred for it twice. <p> These proposals parallel the algorithms for parametric polymorphism: 22 there is the basic algorithm (which does nothing particular), there is the 1-level expansion where the expansion is on object types rather than methods <ref> [15] </ref> and its generalization to p levels [19]. An adaptive algorithm, the data polymorphism equivalent of the iterative algorithm, has also been developed [20]. Based on the poor performance of the non-adaptive algorithms on parametric polymorphism, we do not hold much faith in non-adaptive algorithms for data polymorphism.
Reference: 16. <author> Palsberg, J. and M.I. Schwartzbach, </author> <title> Object-Oriented Type Inference. </title> <booktitle> In OOPSLA '91, ACM SIGPLAN Sixth Annual Conference on Object-Oriented Programming Systems, Languages and Applications. 1991. </booktitle> <address> Phoenix, Arizona. </address>
Reference-contexts: To resolve the coupling, we need implementation types, since only by knowing possible classes of the receiver can we determine which method (s) a send may invoke. In First International Static Analysis Symposium, SAS94, Springer-Verlag LNCS 864. 2 The basic approach to constraint-based analysis was described in <ref> [16] </ref> for a simple object-oriented language. The algorithm has significant deficiencies when analyzing polymorphic code. There are two kinds of polymorphism. First, parametric polymor-phism is the ability of routines to be invoked on arguments of several types. <p> The table also specifies which algorithms are adaptive (defined in Section 3.4). 3.1 The Basic Algorithm The basic algorithm for constraint-based analysis, described in <ref> [16] </ref>, essentially simulates a general execution of the target program. In this regard it is like abstract interpretation [8, 23]. We describe the basic algorithm in some detail here to present a data ow perspective on constraint-based analysis. Constraint-based type inference is a three step process. <p> At the conclusion of this step, the type variables will constitute the nodes in a directed Table 1. The five algorithms we analyze. Name Section Reference Adaptive? Basic algorithm 3.1 <ref> [16] </ref> No 1-level expansion algorithm 3.2 [16] No p-level expansion algorithm 3.3 [19] No Hash function algorithm 3.5 [2] Yes Iterative algorithm 3.6 [20] Yes 5 graph. The edges will be constraints. A constraint is the type-inference-time equivalent of a run-time data ow. <p> At the conclusion of this step, the type variables will constitute the nodes in a directed Table 1. The five algorithms we analyze. Name Section Reference Adaptive? Basic algorithm 3.1 <ref> [16] </ref> No 1-level expansion algorithm 3.2 [16] No p-level expansion algorithm 3.3 [19] No Hash function algorithm 3.5 [2] Yes Iterative algorithm 3.6 [20] Yes 5 graph. The edges will be constraints. A constraint is the type-inference-time equivalent of a run-time data ow. <p> Fig. 3 shows the constraint for the clone primitive. Scheme has an if expression which is illustrated in Fig. 4. Basic data types such as integers and oats and their operators should also be taken into account. More examples are found in <ref> [2, 15, 16, 20] </ref>. When a constraint is added, more propagation becomes possible. The reverse also holds: propagation makes new constraints necessary. <p> The car and cdr functions in LISP, the new method in Smalltalk, and iter-ators on user-defined data structures in any language are prime examples. 3.2 The 1-Level Expansion Algorithm Palsberg, Schwartzbach, and Oxhj acknowledged that high-precision type inference requires avoiding the mixing of types <ref> [15, 16] </ref>. To this end they proposed retyping each method for every send invoking it. Consequently, if the max: method is used by two different sends, types are inferred for it twice. <p> Second, sends to self, which tend to be common, can be analyzed more precisely because a single target can be found. Palsberg and Schwartzbach obtained the same benefits by expanding away inheritance from the target program <ref> [16] </ref>. The second component of each hash value is the send itself. Thus, different sends, which often supply different types of arguments, connect to different templates. This is essentially an implementation of the 1-level expansion.
Reference: 17. <author> Palsberg, J. and M.I. Schwartzbach, </author> <title> Polyvariant Analysis of the Untyped Lambda Calculus. </title> <type> Technical Report, </type> <institution> Daimi PB-386, Computer Science Department, Aarhus University, Den-mark, </institution> <year> 1992. </year>
Reference-contexts: Phillips and Shepard implemented this algorithm for Smalltalk [19]. In addition to p, which controls how precisely parametric polymorphism is analyzed, their algorithm has a second integer parameter, k, that affects analysis of data polymorphism. The p-level expansion algorithm is also discussed in <ref> [17] </ref> where it is applied to the untyped lambda calculus. We consider the p-level expansion unrealistic. The calamity is that the size of the transformed program is exponential in p, effectively ruling out even moderately high values of p. <p> His analysis is parameterized by a function that determines the degree of polyvariance. In this regard it is similar to the hash function algorithm. In <ref> [17] </ref> a constraint-based closure analysis of the untyped lambda calculus is given. On top of the closure analysis a binding time analysis and a safety analysis is defined. The latter determines where type checks must be inserted in the target program to Table 3.
Reference: 18. <author> Palsberg, J. and M.I. Schwartzbach, </author> <title> Safety Analysis versus Type Inference for Partial Types. </title> <journal> Information Processing Letters, </journal> <volume> 43, </volume> <year> 1992, </year> <pages> p. 175-180. </pages>
Reference-contexts: Our constraints capture data ow between different run-time entities. A further comparison of the two systems, and a proof that the ow-based approach in some sense is more powerful, can be found in <ref> [18] </ref>. Closer to the constraint-based analysis described here, is Sestofts closure analysis [21, 22] which Bondorf reviews in [4]. The analysis computes, for each application point in the target program, a superset of the closures that may be applied at that point.
Reference: 19. <author> Phillips, G. and T. Shepard, </author> <title> Static Typing Without Explicit Types. </title> <note> Submitted for publication. </note> <institution> Dept. of Electrical and Computer Engineering, Royal Military College of Canada, </institution> <address> King-ston, Ontario, Canada, </address> <year> 1994. </year>
Reference-contexts: At the conclusion of this step, the type variables will constitute the nodes in a directed Table 1. The five algorithms we analyze. Name Section Reference Adaptive? Basic algorithm 3.1 [16] No 1-level expansion algorithm 3.2 [16] No p-level expansion algorithm 3.3 <ref> [19] </ref> No Hash function algorithm 3.5 [2] Yes Iterative algorithm 3.6 [20] Yes 5 graph. The edges will be constraints. A constraint is the type-inference-time equivalent of a run-time data ow. For example, if the target program executes the assignment x:=y there is a data ow from y to x. <p> Phillips and Shepard implemented this algorithm for Smalltalk <ref> [19] </ref>. In addition to p, which controls how precisely parametric polymorphism is analyzed, their algorithm has a second integer parameter, k, that affects analysis of data polymorphism. The p-level expansion algorithm is also discussed in [17] where it is applied to the untyped lambda calculus. <p> These proposals parallel the algorithms for parametric polymorphism: 22 there is the basic algorithm (which does nothing particular), there is the 1-level expansion where the expansion is on object types rather than methods [15] and its generalization to p levels <ref> [19] </ref>. An adaptive algorithm, the data polymorphism equivalent of the iterative algorithm, has also been developed [20]. Based on the poor performance of the non-adaptive algorithms on parametric polymorphism, we do not hold much faith in non-adaptive algorithms for data polymorphism.
Reference: 20. <author> Plevyak, J. and A.A. Chien, </author> <title> Incremental Inference of Concrete Types, </title> <institution> Department of Computer Science, University of Illinois Urbana-Champaign, </institution> <type> Technical Report UIUCDCS-R-93-1829, </type> <year> 1993. </year>
Reference-contexts: 1 Introduction Static analysis of programs is gaining importance: it is the core of any optimizing or parallelizing compiler <ref> [20] </ref>. Other programming tools based on static analysis are also emerging: [3] describes a tool that can extract compact applications from integrated programming environments by analyzing an image to identify a small but sufficient set of objects for a given application. <p> The five algorithms we analyze. Name Section Reference Adaptive? Basic algorithm 3.1 [16] No 1-level expansion algorithm 3.2 [16] No p-level expansion algorithm 3.3 [19] No Hash function algorithm 3.5 [2] Yes Iterative algorithm 3.6 <ref> [20] </ref> Yes 5 graph. The edges will be constraints. A constraint is the type-inference-time equivalent of a run-time data ow. For example, if the target program executes the assignment x:=y there is a data ow from y to x. <p> Fig. 3 shows the constraint for the clone primitive. Scheme has an if expression which is illustrated in Fig. 4. Basic data types such as integers and oats and their operators should also be taken into account. More examples are found in <ref> [2, 15, 16, 20] </ref>. When a constraint is added, more propagation becomes possible. The reverse also holds: propagation makes new constraints necessary. <p> It can almost be argued that this abstraction is necessary, since even moderately sized programs generate constraints by the thousands <ref> [2, 20] </ref>. 3.1.2 Relation Between Run Time and Type Inference Time In constraint-based type inference there is a direct correspondence between program execution concepts and type inference concepts. We have already seen two examples: object @ object type and data ow @ constraint. <p> Perhaps the 1-level expansion algorithm works reasonably well on real programs, but just not on artificial examples that are constructed to show its weakness? Again, the answer is no. Polymorphic call chains of depth greater than one are common in real programs <ref> [20] </ref>: all it takes to create such a chain is one method receiving a polymorphic argument and passing it to another method. In the Self system the most compelling example is the method ifTrue:. <p> The computational cost of never sharing these templates is affordable since the methods are small and few. However, the rules affect the robustness of the type inference algorithm, making it sensitive to how certain things in the Self world are coded. 3.6 The Iterative Algorithm Plevyak and Chien <ref> [20] </ref> developed an iterative adaptive type inference algorithm and applied it to the Concurrent Aggregates language in the Illinois Concert System language [6], a dynamically typed, single inheritance, object-oriented language. Their algorithm improved the precision for both parametric and data polymorphism, but we will only consider the former. <p> Typically, though, it seems that 5-7 iterations suffice. The problem with the latter alternative is that a fix-point may never be reached when analyzing recursive routines; see <ref> [20] </ref> for a discussion of termination issues. Assuming that a fix-point will be reached, the resulting algorithm has the nice property of creating enough templates to avoid mixing of types, but no more. <p> An adaptive algorithm, the data polymorphism equivalent of the iterative algorithm, has also been developed <ref> [20] </ref>. Based on the poor performance of the non-adaptive algorithms on parametric polymorphism, we do not hold much faith in non-adaptive algorithms for data polymorphism. It is not surprising then, that the good results presented in [20] were obtained using an algorithm which is adaptive on both parametric and data polymorphism. <p> adaptive algorithm, the data polymorphism equivalent of the iterative algorithm, has also been developed <ref> [20] </ref>. Based on the poor performance of the non-adaptive algorithms on parametric polymorphism, we do not hold much faith in non-adaptive algorithms for data polymorphism. It is not surprising then, that the good results presented in [20] were obtained using an algorithm which is adaptive on both parametric and data polymorphism. We are currently working on a new adaptive algorithm. It has the potential to be more precise than the algorithms surveyed here, but its main advantage is that it is non-iterative.
Reference: 21. <author> Sestoft, P., </author> <title> Replacing Function Parameters by Global Variables, M.Sc. </title> <type> thesis 88-7-2, </type> <institution> DIKU, University of Copenhagen, Denmark, </institution> <year> 1988. </year>
Reference-contexts: Our constraints capture data ow between different run-time entities. A further comparison of the two systems, and a proof that the ow-based approach in some sense is more powerful, can be found in [18]. Closer to the constraint-based analysis described here, is Sestofts closure analysis <ref> [21, 22] </ref> which Bondorf reviews in [4]. The analysis computes, for each application point in the target program, a superset of the closures that may be applied at that point. Closure analysis for higher-order functional languages face similar difficulties as inference of implementation types for object-oriented programs.
Reference: 22. <author> Sestoft, P., </author> <title> Replacing Function Parameters by Global Variables. </title> <booktitle> Proceedings of the Fourth International Conference on Functional Programming and Computer Architecture, </booktitle> <address> London, UK, p. 39-53, </address> <publisher> ACM Press, </publisher> <month> September </month> <year> 1989. </year>
Reference-contexts: Our constraints capture data ow between different run-time entities. A further comparison of the two systems, and a proof that the ow-based approach in some sense is more powerful, can be found in [18]. Closer to the constraint-based analysis described here, is Sestofts closure analysis <ref> [21, 22] </ref> which Bondorf reviews in [4]. The analysis computes, for each application point in the target program, a superset of the closures that may be applied at that point. Closure analysis for higher-order functional languages face similar difficulties as inference of implementation types for object-oriented programs.
Reference: 23. <author> Shivers, O., </author> <title> Control-Flow Analysis of Higher-Order Languages, </title> <type> Ph.D. thesis. </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA 15213. </address> <year> 1991. </year>
Reference-contexts: The table also specifies which algorithms are adaptive (defined in Section 3.4). 3.1 The Basic Algorithm The basic algorithm for constraint-based analysis, described in [16], essentially simulates a general execution of the target program. In this regard it is like abstract interpretation <ref> [8, 23] </ref>. We describe the basic algorithm in some detail here to present a data ow perspective on constraint-based analysis. Constraint-based type inference is a three step process. Below we describe each step as the basic algorithm executes it.
Reference: 24. <author> Ungar, D. and R.B. Smith, </author> <title> SELF: The Power of Simplicity. </title> <journal> Lisp and Symbolic Computing, </journal> <volume> 4(3), </volume> <publisher> Kluwer Academic Publishers, </publisher> <month> June </month> <year> 1991. </year> <booktitle> Originally published in OOPSLA 87, Object-Oriented Programming Systems, Languages and Applications, p. </booktitle> <pages> 227-241, </pages> <year> 1987. </year>
Reference-contexts: Second, data polymorphism is the ability to store objects of different types in a variable or slot. Link objects forming a heterogeneous list of integers, oats, and strings, exhibit data polymorphism because their contents slots contain objects of several types. Throughout this paper we rely on the Self system <ref> [1, 24] </ref> for examples. Still, our results are language independent, and we could equally well have chosen any other polymorphic language, even one which is not object-oriented.
Reference: 25. <author> Wand, M., </author> <title> A Simple Algorithm and Proof for Type Inference. </title> <note> Fundamenta Informaticae, X, 1987, </note> <author> p. </author> <month> 115-121. </month>
Reference-contexts: Static analysis of functional programs has been based on inference of principal types where an untyped program is annotated with type declarations that are derived 20 over a grammar of type expressions [14]. Wand has shown how inference of principal types can be formulated as a constraint problem <ref> [25] </ref>. Wands constraints have a different interpretation than the constraints found in this paper. His constraints are over the domain of type expressions, allowing him to build type expressions using unification. Our constraints capture data ow between different run-time entities.
References-found: 25

