URL: ftp://ftp.cs.caltech.edu/tr/cs-tr-98-01.ps.Z
Refering-URL: ftp://ftp.cs.caltech.edu/tr/INDEX.html
Root-URL: http://www.cs.caltech.edu
Title: Combining Multilayer Networks to Combine Learning  
Author: Eric Bax 
Date: March 28, 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Y. S. </author> <title> Abu-Mostafa, </title> <type> personal communication. </type>
Reference-contexts: backpropogation followed by a dot product. @E = @nn () 4.3 Hybrid Proportioning The endpoints of the binary search can be set to = 0 on the left and = 1 on the right, but a more robust search results from combining a coarse grid search to identify regions of <ref> [0; 1] </ref> containing local minima with a precise binary search for a minimum in each of the regions. We refer to this process as hybrid proportioning. The grid search examines 2 [0; 1] at regular intervals to find regions with error decreasing on the left and increasing on the right. (These <p> on the right, but a more robust search results from combining a coarse grid search to identify regions of <ref> [0; 1] </ref> containing local minima with a precise binary search for a minimum in each of the regions. We refer to this process as hybrid proportioning. The grid search examines 2 [0; 1] at regular intervals to find regions with error decreasing on the left and increasing on the right. (These regions contain local minima.) So the grid search evaluates @E @ for each 2 f0; 4; 24; : : : ; 1g. <p> After an interval, the training is interrupted, and the process of matching, combining, duplicating, and training is repeated. This process models the multicomputer training scheme with data parallelism that was discussed earlier in this paper. The test uses HUF matching. Both tests employ the twin networks framework <ref> [1] </ref>. Weights are drawn at random to form a teacher network. The teacher network's function is the target function. To create data, inputs are drawn at random and fed to the teacher network to create target outputs. <p> The teacher network's weights are drawn independently and uniformly at random from [10; 10]. The student network weights are drawn from [0:1; 0:1]. The data set inputs were drawn from <ref> [1; 1] </ref> 10 . Each data set has twice as many examples as the number of weights in each network. <p> To explore the dependence of the matching methods on both training sets being drawn from the same distribution, a test was run in which the train 23 ing data set inputs were drawn uniformly at random from separate halves of the input space <ref> [1; 1] </ref> 10 . The inputs for D a are drawn uniformly at random from [1; 0] fi [1; 1] 9 , and the inputs for D b are drawn uniformly at random from [0; 1] fi [1; 1] 9 . <p> The inputs for D a are drawn uniformly at random from <ref> [1; 0] </ref> fi [1; 1] 9 , and the inputs for D b are drawn uniformly at random from [0; 1] fi [1; 1] 9 . The test and matching set inputs are drawn uniformly at random from the whole space. <p> sets being drawn from the same distribution, a test was run in which the train 23 ing data set inputs were drawn uniformly at random from separate halves of the input space <ref> [1; 1] </ref> 10 . The inputs for D a are drawn uniformly at random from [1; 0] fi [1; 1] 9 , and the inputs for D b are drawn uniformly at random from [0; 1] fi [1; 1] 9 . The test and matching set inputs are drawn uniformly at random from the whole space. <p> The inputs for D a are drawn uniformly at random from [1; 0] fi [1; 1] 9 , and the inputs for D b are drawn uniformly at random from <ref> [0; 1] </ref> fi [1; 1] 9 . The test and matching set inputs are drawn uniformly at random from the whole space. The results with nonhomogeneous training data are similar to those with homogeneous training data. <p> inputs were drawn uniformly at random from separate halves of the input space <ref> [1; 1] </ref> 10 . The inputs for D a are drawn uniformly at random from [1; 0] fi [1; 1] 9 , and the inputs for D b are drawn uniformly at random from [0; 1] fi [1; 1] 9 . The test and matching set inputs are drawn uniformly at random from the whole space. The results with nonhomogeneous training data are similar to those with homogeneous training data. <p> After matching, the weights of the network series can be extrapolated to form a network for future action. to create parameterized networks. A 10-10-10 teacher network's weights were drawn uniformly at random from [10; 10]. Then a perturbation network's weights were drawn at random from <ref> [1; 1] </ref>. One network was trained on data generated by adding the perturbation network to the teacher network. Another network was trained on data generated by subtracting the perturbation network from the teacher network. The trained networks were matched by the HUF algorithm. <p> The results are averaged over 100 tests. Each test used the twin networks framework. Each network had two layers of weights, with ten input units, ten 41 hidden units, and ten output units. Each data set input was drawn at random from <ref> [1; 1] </ref>. Each test followed the procedure: 1. Create a teacher network by drawing each weight at random from [10; 10]. 2. Create two student networks by drawing each of their weights at random from [0:1; 0:1]. 3. <p> For each scheme, the results of a single training session are shown. Identical in-sample data sets, test data sets, and initial networks are used for all schemes. The twin networks framework <ref> [1] </ref> is employed, with a 10-10-10 teacher network with weights drawn uniformly at random from [-10,10]. The 10-10-10 student networks' initial weights are drawn uniformly at random from [-0.1,0.1]. The learning rate is 0.01. <p> The test uses the twin networks framework <ref> [1] </ref>, with 10-10-10 networks. Each teacher network weight is drawn independently and uniformly at random from [10; 10]. Each initial student network weight is drawn independently and uniformly at random from [0:1; 0:1]. All data set inputs are drawn uniformly at random from [1; 1] 10 . <p> Each teacher network weight is drawn independently and uniformly at random from [10; 10]. Each initial student network weight is drawn independently and uniformly at random from [0:1; 0:1]. All data set inputs are drawn uniformly at random from <ref> [1; 1] </ref> 10 . Each of the training sets D a and D b contains 660 examples, and the test data set contains 2200 examples. The gradient combination test trains a single student on D a [ D b .
Reference: [2] <author> Zehra Cataltepe and Malik Magdon-Ismail, </author> <type> personal communication. </type>
Reference-contexts: Hence, the matching data set outputs need not be known. If the input distribution is known, then inputs x can be generated at random to enhance the matching data set. So the accuracy of the matching need not be limited by a scarcity of in-sample data <ref> [2] </ref>. 3.3.3 Complexity Each evaluation of u hi (x) requires O (Ln 2 ) time, where n maxfn 0 : : : n L g, since the evaluation consists of multiplying the matrices W 1 : : : W L by vectors. <p> As in the HUF algorithm, the matching data set outputs y 2 D are not used by the BNF algorithm. Thus, if the input distribution is known, then the matching data can be enhanced with randomly drawn inputs <ref> [2] </ref>. 3.4.3 Complexity Each bottleneck network function evaluation requires O (Ln 2 ) time. The algorithm executes O (Ln 2 jDj) evaluations, so it has time complexity O (L 2 n 4 jDj).
Reference: [3] <author> K. M. Chandy and J. Misra, </author> <title> Parallel Program Design, A Foundation. </title> <publisher> (Addison-Wesley, </publisher> <year> 1988) </year>
Reference-contexts: Multicomputer Model and Notation We use the model of multicomputer computation presented by Van de Velde [18], with some changes in notation. (The model is only outlined here; for more 45 detail, see the books on concurrent computation by Van de Velde [18] and on UNITY by Chandy and Misra <ref> [3] </ref>.) The model consists of sequential processes which communicate via channels. Each channel is unidirectional, i.e. it carries messages from a specific process to another specific process. Also, each channel preserves message order, i.e. messages sent on the same channel are received in the order in which they are sent.
Reference: [4] <author> A. M. Chen, H. Lu and R. Hecht-Nielsen, </author> <title> On the geometry of feedforward neural network error surfaces, </title> <booktitle> Neural Computation 5 (1993) 910-927. </booktitle>
Reference-contexts: Chen, Lu, and Hecht-Nielson <ref> [4] </ref> present a method to convert a given network to a canonical form such that a pair of networks execute the same function if and only if their canonical forms agree weight by weight. The following procedure converts a network to this canonical form.
Reference: [5] <author> H. W. Kuhn, </author> <title> The Hungarian method for the assignment problem, </title> <institution> Naval Res. Logist. Quart. </institution> <month> 2 </month> <year> (1955) </year> <month> 83-97. </month>
Reference-contexts: The overall similarity s ij is the maximum of similarity and flipped similarity. The permutation that gives a best matching corresponds to an independent set of elements in S with maximum sum. Therefore, the permutation matrix P can be found by applying Kuhn's Hungarian algorithm <ref> [5, 7] </ref> to the matrix S. The algorithm returns a set of element positions containing one position in each row and one position in each column. To produce the permutation matrix P , set the elements in these positions to 1, and set all other elements to 0.
Reference: [6] <author> B. M. E. Moret and H. D. Shapiro, </author> <title> Algorithms from P to NP, Volume I. </title> <publisher> (The Benjamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, CA, </address> <year> 1990) </year>
Reference-contexts: For example, binary search <ref> [6] </ref> can be used to find local minima of the error by finding such that @E @ = 0.
Reference: [7] <author> C. H. Papadimitriou and K. Steiglitz, </author> <title> Combinatorial Optimization, Algorithms and Complexity (Prentice-Hall, </title> <publisher> Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982) </year> <month> 433-448. </month>
Reference-contexts: The overall similarity s ij is the maximum of similarity and flipped similarity. The permutation that gives a best matching corresponds to an independent set of elements in S with maximum sum. Therefore, the permutation matrix P can be found by applying Kuhn's Hungarian algorithm <ref> [5, 7] </ref> to the matrix S. The algorithm returns a set of element positions containing one position in each row and one position in each column. To produce the permutation matrix P , set the elements in these positions to 1, and set all other elements to 0.
Reference: [8] <author> H. Paugam-Moisy, </author> <title> Optimal speedup conditions for a parallel backpropagation algorithm, </title> <booktitle> Lecture Notes in Computer Science 634 (1992) 719-724. </booktitle>
Reference-contexts: Thus, network combination is a useful tool for genetic algorithms and other training schemes in which learning is combined at intervals. Iterated network combination can increase the speed of network training on message-passing multicomputers. First, consider the following scheme to learn with batch mode backpropogation using data parallelism <ref> [8, 9, 16, 19, 20] </ref>. The data is distributed among the processors, and there is a copy of the network in each processor. Concurrently, each processor calculates @E=@w for its share of the data.
Reference: [9] <author> D. A. Pomerleau, G. L. Gusciora, D. S. Touretzky and H. T. Kung, </author> <title> Neural network simulation at Warp speed: How we got 17 million connections per second, </title> <booktitle> IEEE Internat. Conf. on Neural Networks, </booktitle> <address> San Diego, CA, </address> <year> 1988, </year> <pages> pp. 143-150. </pages>
Reference-contexts: Thus, network combination is a useful tool for genetic algorithms and other training schemes in which learning is combined at intervals. Iterated network combination can increase the speed of network training on message-passing multicomputers. First, consider the following scheme to learn with batch mode backpropogation using data parallelism <ref> [8, 9, 16, 19, 20] </ref>. The data is distributed among the processors, and there is a copy of the network in each processor. Concurrently, each processor calculates @E=@w for its share of the data.
Reference: [10] <author> W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, </author> <title> Numerical Recipes in C, </title> <publisher> Second Edition (Cambridge University Press, </publisher> <address> New York, NY, </address> <year> 1992) </year> <month> 379-393. </month>
Reference-contexts: Each epoch consists of sequential mode backpropogation, with step size 21 0.01. All of the networks in the tests have 2 layers, with 10 input units, 10 hidden units, and 10 output units. The teacher network's weights are drawn independently and uniformly at random from <ref> [10; 10] </ref>. The student network weights are drawn from [0:1; 0:1]. The data set inputs were drawn from [1; 1] 10 . Each data set has twice as many examples as the number of weights in each network. <p> A series of networks can be trained on progressive sliding windows of the data. After matching, the weights of the network series can be extrapolated to form a network for future action. to create parameterized networks. A 10-10-10 teacher network's weights were drawn uniformly at random from <ref> [10; 10] </ref>. Then a perturbation network's weights were drawn at random from [1; 1]. One network was trained on data generated by adding the perturbation network to the teacher network. Another network was trained on data generated by subtracting the perturbation network from the teacher network. <p> Each network had two layers of weights, with ten input units, ten 41 hidden units, and ten output units. Each data set input was drawn at random from [1; 1]. Each test followed the procedure: 1. Create a teacher network by drawing each weight at random from <ref> [10; 10] </ref>. 2. Create two student networks by drawing each of their weights at random from [0:1; 0:1]. 3. Create a test data set of 2200 examples by feeding random input vectors to the teacher network to produce corresponding output vectors. 4. <p> The test uses the twin networks framework [1], with 10-10-10 networks. Each teacher network weight is drawn independently and uniformly at random from <ref> [10; 10] </ref>. Each initial student network weight is drawn independently and uniformly at random from [0:1; 0:1]. All data set inputs are drawn uniformly at random from [1; 1] 10 . <p> So proportioning is network training in an n-dimensional subspace of weight space. If n is small enough, then fancier minimization methods than gradient descent become feasible, including New ton's method and its variations <ref> [10] </ref>. 77 Chapter 10 Combining Networks to Improve Generalization Can we achieve better out-of-sample performance by training several networks and combining them than by training a single network? The tests detailed here show that combination improves generalization on a set of credit data. <p> In each run, a random problem is generated using the twin networks procedure. Each network has 10 input units, 10 hidden units, and 10 output units. The teacher network weights are drawn randomly from <ref> [10; 10] </ref>. The student network initial weights are drawn from [0:1; 0:1]. The weights of each student network are drawn independently. Each run uses a test data set with 2200 examples (10 examples per weight.) The student network data sets are created independently. <p> The data sets are generated using the twin networks framework, with 10-10-10 networks. The test runs each use the same training and test data sets, generated by a teacher network with weights drawn uniformly at random from <ref> [10; 10] </ref>. The training data set has 8 examples per network parameter (1760 examples), and the test data set has 10 examples per network parameter (2200 examples). Each test run uses the same initial network weights, drawn uniformly at random from [0:1; 0:1].
Reference: [11] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> Learning internal representations by error propogation, </title> <booktitle> Ch.8 in Parallel Distributed Processing, Volume 1: Foundations. </booktitle> <publisher> (The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986) </year> <month> 124 </month>
Reference-contexts: The quantity @E @ can be computed using the chain rule. @E = @nn () @nn () (4.1) The first term on the RHS is the gradient of the error with respect to the weights of a network, which can be computed by backpropogation <ref> [11] </ref>. <p> initial network * D the subset of the in-sample data stored in the process * intervals thenumber of training intervals to be performed * epochs thenumber of training epochs to be performed in each interval * the backpropogation multiplier to be used in network updates w := w ffiE ffiw <ref> [11] </ref>. The value of intervals must be the same in both process' function calls. We assume that the values of epochs and are also the same in both processes. In each interval, each process trains its network for several epochs. <p> The following function computes the error gradient with respect to the weights of network nn. Each process computes the gradient with respect its data; then the gradient is summed over the processes. The function backprop (nn; D) is an implementation of the backpropogation procedure <ref> [11] </ref>. The function sum net pair sums network weight vectors over processes. <p> First, backpropogation <ref> [11] </ref> is used to find the gradient over the data within each process.
Reference: [12] <author> N. Shamir, D. Saad and E. Marom, </author> <title> Neural net pruning based on functional behavior of neurons, </title> <note> International Journal of Neural Systems 4 (1993) 143-158. </note>
Reference-contexts: The function of hidden unit i in layer h is: u hi (x) [tanh (W h (: : : tanh (W 1 x t 1 ) : : :) t h )] i (3.1) Shamir, Saad, and Marom have used hidden unit functions for pruning and genetic recombination <ref> [12, 13, 14] </ref>. The matching algorithm must identify functional correspondences among hidden units in the two networks being matched.
Reference: [13] <author> N. Shamir, D. Saad and E. Marom, </author> <title> Preserving the diversity of a genetically evolving population of nets using the functional behavior of neurons, </title> <booktitle> Complex Systems 7 (1993) 327-346 </booktitle>
Reference-contexts: The function of hidden unit i in layer h is: u hi (x) [tanh (W h (: : : tanh (W 1 x t 1 ) : : :) t h )] i (3.1) Shamir, Saad, and Marom have used hidden unit functions for pruning and genetic recombination <ref> [12, 13, 14] </ref>. The matching algorithm must identify functional correspondences among hidden units in the two networks being matched.
Reference: [14] <author> N. Shamir, D. Saad and E. Marom, </author> <title> Using the functional behavior of neurons for genetic recombination in neural nets training, </title> <booktitle> Complex Systems 7 (1993) 445-467 </booktitle>
Reference-contexts: The function of hidden unit i in layer h is: u hi (x) [tanh (W h (: : : tanh (W 1 x t 1 ) : : :) t h )] i (3.1) Shamir, Saad, and Marom have used hidden unit functions for pruning and genetic recombination <ref> [12, 13, 14] </ref>. The matching algorithm must identify functional correspondences among hidden units in the two networks being matched.
Reference: [15] <author> J. Sill, </author> <title> Monotonicity hints, </title> <note> to appear in NIPS 9 </note>
Reference-contexts: I received the data from Joseph Sill, who in turn received the data from the machine learning database repository maintained by UC-Irvine. The following information about the credit data is drawn directly from Joseph Sill's paper on monotonicity hints <ref> [15] </ref>. "The credit card task is to predict whether or not an applicant will default. For each of the 690 applicant case histories, the database contains 690 features describing the applicant plus the class label indicating whether or not a default ultimately occurred.
Reference: [16] <author> A. Singer, </author> <title> Implementations of artificial neural networks on the Connection Machine, </title> <note> Parallel Computing 14 (1990) 305-315. </note>
Reference-contexts: Thus, network combination is a useful tool for genetic algorithms and other training schemes in which learning is combined at intervals. Iterated network combination can increase the speed of network training on message-passing multicomputers. First, consider the following scheme to learn with batch mode backpropogation using data parallelism <ref> [8, 9, 16, 19, 20] </ref>. The data is distributed among the processors, and there is a copy of the network in each processor. Concurrently, each processor calculates @E=@w for its share of the data.
Reference: [17] <author> H. J. Sussman, </author> <title> Uniqueness of the weights for minimal feedforward nets with a given input-output map, </title> <booktitle> Neural Networks 5 (1992) 589-593. </booktitle>
Reference-contexts: This process 5 multiplies by 1 every weight in row k of W h , position k of t h , and column k of W h+1 . 3.2 Canonical Form Sussman <ref> [17] </ref> has shown that permutations and sign flips are the only invariances for irreducible 2 layer neural networks with hyperbolic tangent sigmoids, i.e. if two such networks execute the same function, then there is a sequence of permutations and sign flips that converts one network to the other.
Reference: [18] <author> E. F. Van de Velde, </author> <title> Concurrent Scientific Computing (***Publisher?***) </title>
Reference-contexts: Then we consider a relaxation of our implementation which sacrifices robustness for speed. Finally, we test some of the combination schemes considered in this section. 8.1 Multicomputer Model and Notation We use the model of multicomputer computation presented by Van de Velde <ref> [18] </ref>, with some changes in notation. (The model is only outlined here; for more 45 detail, see the books on concurrent computation by Van de Velde [18] and on UNITY by Chandy and Misra [3].) The model consists of sequential processes which communicate via channels. <p> the combination schemes considered in this section. 8.1 Multicomputer Model and Notation We use the model of multicomputer computation presented by Van de Velde <ref> [18] </ref>, with some changes in notation. (The model is only outlined here; for more 45 detail, see the books on concurrent computation by Van de Velde [18] and on UNITY by Chandy and Misra [3].) The model consists of sequential processes which communicate via channels. Each channel is unidirectional, i.e. it carries messages from a specific process to another specific process.
Reference: [19] <author> M. Witbrock and M. Zagha, </author> <title> An implementation of backpropagation learning on GF11, a large SIMD parallel computer, </title> <note> Parallel Computing 14 (1990) 329-346. </note>
Reference-contexts: Thus, network combination is a useful tool for genetic algorithms and other training schemes in which learning is combined at intervals. Iterated network combination can increase the speed of network training on message-passing multicomputers. First, consider the following scheme to learn with batch mode backpropogation using data parallelism <ref> [8, 9, 16, 19, 20] </ref>. The data is distributed among the processors, and there is a copy of the network in each processor. Concurrently, each processor calculates @E=@w for its share of the data.
Reference: [20] <author> X. Zhang, M. McKenna, J. P. Mesirov, and D. L. Waltz, </author> <title> The backpropagation algorithm on grid and hypercube architectures, </title> <note> Parallel Computing 14 (1990) 317-327. </note>
Reference-contexts: Thus, network combination is a useful tool for genetic algorithms and other training schemes in which learning is combined at intervals. Iterated network combination can increase the speed of network training on message-passing multicomputers. First, consider the following scheme to learn with batch mode backpropogation using data parallelism <ref> [8, 9, 16, 19, 20] </ref>. The data is distributed among the processors, and there is a copy of the network in each processor. Concurrently, each processor calculates @E=@w for its share of the data.
References-found: 20

