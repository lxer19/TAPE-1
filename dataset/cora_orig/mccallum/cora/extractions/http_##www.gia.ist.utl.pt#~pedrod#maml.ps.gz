URL: http://www.gia.ist.utl.pt/~pedrod/maml.ps.gz
Refering-URL: http://www.gia.ist.utl.pt/~pedrod/
Root-URL: 
Email: pedrod@gia.ist.utl.pt  
Title: How to Get a Free Lunch: A Simple Cost Model for Machine Learning Applications  
Author: Pedro Domingos 
Address: Lisbon 1096, Portugal  
Affiliation: Artificial Intelligence Group Instituto Superior Tecnico  
Abstract: This paper proposes a simple cost model for machine learning applications based on the notion of net present value. The model extends and unifies the models used in (Pazzani et al., 1994) and (Masand & Piatetsky-Shapiro, 1996). It attempts to answer the question "Should a given machine learning system now in the prototype stage be fielded?" The model's inputs are the system's confusion matrix, the cash flow matrix for the application, the cost per decision, the one-time cost of deploying the system, and the rate of return on investment. Like Provost and Fawcett's (1997) ROC convex hull method, the present model can be used for decision-making even when its input variables are not known exactly. Despite its simplicity, it has a number of non-trivial consequences. For example, under it the "no free lunch" theorems of learning theory no longer apply. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Berger, J. O. </author> <year> 1985. </year> <title> Statistical Decision Theory and Bayesian Analysis. </title> <address> New York, NY: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Brealey, R. A., and Myers, S. C. </author> <year> 1996. </year> <title> Principles of Corporate Finance. </title> <address> New York, NY: </address> <publisher> McGraw-Hill, 5th edition. </publisher>
Reference-contexts: It now needs to decide whether to deploy this system in the field. A Cost Model Deploying a machine learning system is an investment decision like any other, and it can be made in the standard way: deploy the system if its net present value (NPV) is positive <ref> (Brealey & Myers 1996) </ref>. The NPV of an investment is the sum of the cash flows it generates, discounted by the rate of return. <p> We have also assumed that r is the same for the current solution and for the learning system, but in general it may differ, if the two have different risk characteristics. A common measure of risk is the variance of returns <ref> (Brealey & Myers 1996) </ref>. Lower variance will lead to a lower return being demanded. Thus, N P V M may be greater than N P V L , even if the learning system makes on average worse decisions than the current procedure, if it reduces the variance in cash flows.
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: Theorem 2 implies that machine learning researchers can be more optimistic about their work than previously assumed. Contrasted with Theorem 1, it also illustrates how abstract results can be misleading. Related Work CART <ref> (Breiman et al. 1984) </ref> was one of the first learning systems to allow explicitly incorporating cost considerations. However, its method of variable misclassi-fication costs only works for two classes, and Pazzani et al. (1994) found that in practice CART's altered priors method was not very effective in reducing mis-classification costs.
Reference: <author> Chan, P.; Stolfo, S.; and Wolpert, D., eds. </author> <year> 1996. </year> <title> Proceedings of the AAAI-96 Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms. </title> <address> Portland, OR: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Henrion, M.; Breese, J.; and Horvitz, E. </author> <year> 1992. </year> <title> Decision analysis and expert systems. </title> <journal> AI Magazine 12 </journal> <pages> 64-91. </pages>
Reference-contexts: In order to preserve generality, we have not coupled the cost model with any particular type of induced model (e.g., decision trees or neural networks). However, if the induced model is an explicit representation of a probability distribution (e.g, a Bayesian network <ref> (Henrion, Breese, & Horvitz 1992) </ref>), the inclusion of attribute-dependent cost and confusion information is conceptually straightforward. On the other hand, it may be computationally hard, requiring approximations (e.g., (Horvitz, Suermondt, & Cooper 1989)) and/or careful management of computation (e.g., (Horvitz 1997)).
Reference: <author> Horvitz, E.; Suermondt, H.; and Cooper, G. </author> <year> 1989. </year> <title> Bounded conditioning: Flexible inference for decisions under scarce resources. </title> <booktitle> In Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 182-193. </pages> <address> Windsor, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, if the induced model is an explicit representation of a probability distribution (e.g, a Bayesian network (Henrion, Breese, & Horvitz 1992)), the inclusion of attribute-dependent cost and confusion information is conceptually straightforward. On the other hand, it may be computationally hard, requiring approximations (e.g., <ref> (Horvitz, Suermondt, & Cooper 1989) </ref>) and/or careful management of computation (e.g., (Horvitz 1997)). Many classification models also have implicitly associated probability models (e.g., the leaf probabilities in decision trees), and using them may be computationally easier.
Reference: <author> Horvitz, E. </author> <year> 1990. </year> <title> Computation and Action Under Bounded Resources. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA. </address>
Reference: <author> Horvitz, E. </author> <year> 1997. </year> <title> Models of continual computation. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> 286-293. </pages> <address> Providence, RI: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: On the other hand, it may be computationally hard, requiring approximations (e.g., (Horvitz, Suermondt, & Cooper 1989)) and/or careful management of computation (e.g., <ref> (Horvitz 1997) </ref>). Many classification models also have implicitly associated probability models (e.g., the leaf probabilities in decision trees), and using them may be computationally easier. In the future we would like to apply the proposed model to specific areas (e.g., banking, database marketing, health care).
Reference: <author> Howard, R. A. </author> <year> 1966. </year> <title> Information value theory. </title> <journal> IEEE Transactions on Systems Science and Cybernetics 2 </journal> <pages> 22-26. </pages>
Reference: <author> Keeney, R. L., and Raiffa, H. </author> <year> 1976. </year> <title> Decisions with Multiple Objectives: Preferences and Value TradeOffs. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: If the utility of a decision is equated with the cash flow it generates, Equation 2 minus the D term corresponds to computing the expected utility of the system's decision <ref> (Keeney & Raiffa 1976) </ref>.
Reference: <author> Masand, B., and Piatetsky-Shapiro, G. </author> <year> 1996. </year> <title> A comparison of approaches for maximizing business payoff of prediction models. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 195-201. </pages> <address> Portland, OR: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Matheus, C. J.; Piatetsky-Shapiro, G.; and McNeill, D. </author> <year> 1996. </year> <title> Selecting and reporting what is interesting: The KEFIR application to healthcare data. </title> <editor> In Fayyad, U. M.; Piatetsky-Shapiro, G.; Smyth, P.; and Uthurusamy, R., eds., </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher> <pages> 495-515. </pages>
Reference: <editor> Michalski, R. S., and Tecuci, G., eds. </editor> <year> 1994. </year> <title> Machine Learning: A Multistrategy Approach. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Nakhaeizadeh, G., and Schnabl, A. </author> <year> 1997. </year> <title> Development of multi-criteria metrics for evaluation of data mining algorithms. </title> <booktitle> In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 37-42. </pages> <address> Newport Beach, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Pazzani, M.; Merz, C.; Murphy, P.; Ali, K.; Hume, T.; and Brunk, C. </author> <year> 1994. </year> <title> Reducing misclassification costs. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 217-225. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Piatetsky-Shapiro, G. </author> <year> 1991. </year> <title> Discovery, analysis, and presentation of strong rules. </title> <editor> In Piatetsky-Shapiro, G., and Frawley, W. J., eds., </editor> <booktitle> Knowledge Discovery in Databases. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher> <pages> 229-248. </pages>
Reference: <author> Provost, F., and Fawcett, T. </author> <year> 1997. </year> <title> Analysis and visualization of classifier performance: Comparison under imprecise class and cost distributions. </title> <booktitle> In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 43-48. </pages> <address> Newport Beach, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Our proposal also has this model as a special case. Provost and Fawcett <ref> (Provost & Fawcett 1997) </ref> propose a method for evaluating classifiers in two-class domains in terms of the true positive and false positive rates. Despite its intuitive appeal, this method may be hard to generalize to arbitrary cost matrices, and does not take the deployment and decision-making costs into account.
Reference: <author> Rao, R. B.; Gordon, D.; and Spears, W. </author> <year> 1995. </year> <title> For every action, is there really an equal and opposite reaction? Analysis of the conservation law for generalization performance. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 471-479. </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schaffer, C. </author> <year> 1994. </year> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 259-265. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Silberschatz, A., and Tuzhilin, A. </author> <year> 1995. </year> <title> On subjective measures of interestingness in knowledge discovery. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 275-281. </pages> <address> Montreal, Canada: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Turney, P. </author> <year> 1995. </year> <title> Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree algorithm. </title> <journal> Journal of Artificial Intelligence Research 2 </journal> <pages> 369-409. </pages>
Reference-contexts: The method we propose shares some of the same goals of simple visualization and robustness to imprecise information. Another learning system that allows explicit consideration of costs is described in <ref> (Turney 1995) </ref>. There is a relatively small but growing literature on cost-sensitive learning; see (Turney 1997) for an online bibliography.
Reference: <author> Turney, P. </author> <year> 1997. </year> <title> Cost-sensitive learning bibliography. Online bibliography, </title> <institution> Institute for Information Technology of the National Research Council of Canada, </institution> <address> Ottawa, Canada. http://ai.iit.nrc.ca/bibliographies/- cost-sensitive.html. </address>
Reference-contexts: The method we propose shares some of the same goals of simple visualization and robustness to imprecise information. Another learning system that allows explicit consideration of costs is described in (Turney 1995). There is a relatively small but growing literature on cost-sensitive learning; see <ref> (Turney 1997) </ref> for an online bibliography. Some of the systems referenced in this bibliography take into account that different attributes have different costs of evaluation (e.g., some medical tests are more expensive than others); in our model, this corresponds to having a variable D term (Equation 2).
Reference: <author> Wolpert, D. </author> <year> 1996. </year> <title> The lack of a priori distinctions between learning algorithms. </title> <booktitle> Neural Computation 8 </booktitle> <pages> 1341-1390. </pages>
References-found: 23

