URL: http://www.cs.columbia.edu/robotics/publications/yoshimi-allen.icra96.ps.gz
Refering-URL: http://www.cs.columbia.edu/robotics/publications/publications.html
Root-URL: http://www.cs.columbia.edu
Title: CLOSED-LOOP VISUAL GRASPING AND MANIPULATION  
Author: Billibon H. Yoshimi and Peter Allen 
Affiliation: Department of Computer Science, Columbia University  
Abstract: Sensors play an important role in providing feedback to robotic control systems. Vision can be an effective sensing modality due to its speed, low cost, and flexibility. It can also serve as an external sensor that can provide control information for devices that lack internal sensing. Many robot grippers do not have internal sensors. These grippers can be prone to error as they operate under open loop control and usually require a precise model of the environment to be effective. This paper describes 2 visual control primitives that can be used to provide position control for a sensorless robot system. The primitives use a simple visual tracking and correspondence scheme to provide real-time feedback control in the presence of imprecise camera calibrations. Experimental results are shown for the positioning task of locating, picking up, and inserting a bolt into a nut under visual control. Results are also presented for the visual control of a bolt tightening task. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Allen, A. Timcenko, B. Yoshimi, and P. Michelman. </author> <title> Automated tracking and grasping of a moving object with a robotic hand-eye system. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 9(2) </volume> <pages> 152-165, </pages> <year> 1993. </year>
Reference-contexts: Sharma et al. [8] use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces. Other systems which have used visual control include <ref> [10, 7, 6, 9, 13, 1] </ref>. Our system is specifically aimed at merging vision with grasping by providing visual control of position and contact using sensorless robotic fingers and hands.
Reference: [2] <author> A. Castano and S. Hutchinson. </author> <title> Visual compliance: </title> <journal> Task-directed visual servo control. IEEE Trans. on Robotics and Automation, </journal> <volume> 10(3) </volume> <pages> 334-342, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: They correct for the transform's errors by using a second control scheme which converts the relative error in image space into a real world positioning change. Cas-tano and Hutchinson <ref> [2] </ref> use visual constraint planes to create compliant surfaces for constraint robot movement in the real world. Both Hager et al. [4] and Fed-dema et al. [3] have used Image Jacobian-based control to perform various positioning tasks.
Reference: [3] <author> J. Feddema and C. S. G. Lee. </author> <title> Adaptive image feature prediction and control for visual tracking with a hand-eye coordinated camera. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 20 </volume> <pages> 1172-1183, </pages> <address> Sept./Oct. </address> <year> 1990. </year>
Reference-contexts: Cas-tano and Hutchinson [2] use visual constraint planes to create compliant surfaces for constraint robot movement in the real world. Both Hager et al. [4] and Fed-dema et al. <ref> [3] </ref> have used Image Jacobian-based control to perform various positioning tasks. Sharma et al. [8] use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces.
Reference: [4] <author> G. Hager, W. Chang, and A. Morse. </author> <title> Robot feedback control based on stereo vision: Towards calibration-free hand-eye coordination. </title> <booktitle> In Proc. IEEE Conf. on Robotics and Automation, </booktitle> <volume> volume 4, </volume> <pages> pages 2850-2856, </pages> <year> 1994. </year>
Reference-contexts: Cas-tano and Hutchinson [2] use visual constraint planes to create compliant surfaces for constraint robot movement in the real world. Both Hager et al. <ref> [4] </ref> and Fed-dema et al. [3] have used Image Jacobian-based control to perform various positioning tasks. Sharma et al. [8] use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces.
Reference: [5] <author> N. Hollinghurst and R. Cipolla. </author> <title> Uncalibrated stereo hand-eye coordination. </title> <type> Technical Report CUED/F-INFENG/TR126, </type> <institution> Department of Engineering, University of Cambridge, </institution> <year> 1993. </year>
Reference-contexts: We will show that visual feedback gives many systems the compliance necessary to perform the task and the ability to gauge its performance. Other researchers have built systems that use vision to control robot motion. Hollingshurst and Cipolla <ref> [5] </ref> have developed a system for positioning a gripper above an object in the environment using an affine stereo transform to estimate the object's position.
Reference: [6] <author> A. Koivo and N. Houshangi. </author> <title> Real-time vision feedback for servoing robotic manipulator with self-tuning controller. </title> <journal> IEEE Transactions on System, Man, and Cybernetics, </journal> <volume> 21, No. 1 </volume> <pages> 134-142, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Sharma et al. [8] use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces. Other systems which have used visual control include <ref> [10, 7, 6, 9, 13, 1] </ref>. Our system is specifically aimed at merging vision with grasping by providing visual control of position and contact using sensorless robotic fingers and hands.
Reference: [7] <author> N. Papanikolopoulos, B. Nelson, and P. Khosla. </author> <title> Six degree-of-freedom hand/eye visual tracking with uncertain parameters. </title> <booktitle> In Proc. of IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 174-179, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Sharma et al. [8] use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces. Other systems which have used visual control include <ref> [10, 7, 6, 9, 13, 1] </ref>. Our system is specifically aimed at merging vision with grasping by providing visual control of position and contact using sensorless robotic fingers and hands.
Reference: [8] <author> R. Sharma, J. Herve, and P. Cucka. </author> <title> Analysis of dynamic hand positioning tasks using visual feedback. </title> <type> Technical Report CAR-TR-574, </type> <institution> Center for Auto. Res., University of Maryland, </institution> <year> 1991. </year>
Reference-contexts: Cas-tano and Hutchinson [2] use visual constraint planes to create compliant surfaces for constraint robot movement in the real world. Both Hager et al. [4] and Fed-dema et al. [3] have used Image Jacobian-based control to perform various positioning tasks. Sharma et al. <ref> [8] </ref> use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces. Other systems which have used visual control include [10, 7, 6, 9, 13, 1].
Reference: [9] <author> S. Skaar, W. Brockman, and R. Hanson. </author> <title> Camera-space manipulation. </title> <journal> International Journal of Robotics Research, </journal> <volume> 6(4) </volume> <pages> 20-32, </pages> <month> Winter </month> <year> 1987. </year>
Reference-contexts: Sharma et al. [8] use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces. Other systems which have used visual control include <ref> [10, 7, 6, 9, 13, 1] </ref>. Our system is specifically aimed at merging vision with grasping by providing visual control of position and contact using sensorless robotic fingers and hands.
Reference: [10] <author> T. M. Sobh and R. </author> <title> Bajcsy. Autonomous observation under uncertainty. </title> <booktitle> In Proc. of IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 1792-1798, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Sharma et al. [8] use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces. Other systems which have used visual control include <ref> [10, 7, 6, 9, 13, 1] </ref>. Our system is specifically aimed at merging vision with grasping by providing visual control of position and contact using sensorless robotic fingers and hands.
Reference: [11] <author> K. Suzumori, S. Iikura, and H. Tanaka. </author> <title> Development of a flexible microactuator and its application to robotic mechanisms. </title> <booktitle> In IEEE International Conference of Robotics and Automation, </booktitle> <pages> pages 1622-1627, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The gripper we used is a Toshiba FMA gripper (see figure 3). The gripper is comprised of 4 fingers where each finger is made up of 3 stretch-able, flexible chambers <ref> [11] </ref>. The gripper could be positioned in a variety of different poses by changing the pressure in each of the finger chambers. The original design used binary pressure valves which limited the movement of each finger to only eight possible configurations in a 3 degrees of freedom workspace.
Reference: [12] <author> B. H. Yoshimi. </author> <title> Visual Control of Robotics Tasks. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Other systems which have used visual control include [10, 7, 6, 9, 13, 1]. Our system is specifically aimed at merging vision with grasping by providing visual control of position and contact using sensorless robotic fingers and hands. Details on this research can be found in <ref> [12] </ref>. 2 Experimental system The system pictured in figure 1 shows the major components of the system: a robot, a gripper, and two fixed stereo cameras. The gripper we used is a Toshiba FMA gripper (see figure 3).
Reference: [13] <author> B. H. Yoshimi and P. Allen. </author> <title> Alignment using an uncalibrated camera system. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 411-420, </pages> <year> 1993. </year>
Reference-contexts: Sharma et al. [8] use perceptual 3D surfaces to represent the workspace of the gripper and object and they plan their positioning tasks along these surfaces. Other systems which have used visual control include <ref> [10, 7, 6, 9, 13, 1] </ref>. Our system is specifically aimed at merging vision with grasping by providing visual control of position and contact using sensorless robotic fingers and hands.
References-found: 13

