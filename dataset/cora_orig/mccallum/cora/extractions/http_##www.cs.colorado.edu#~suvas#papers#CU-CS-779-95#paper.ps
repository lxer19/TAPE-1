URL: http://www.cs.colorado.edu/~suvas/papers/CU-CS-779-95/paper.ps
Refering-URL: http://www.cs.colorado.edu/~suvas/Papers.html
Root-URL: http://www.cs.colorado.edu
Email: suvas@cs.colorado.edu  grunwald@cs.colorado.edu  
Title: Exploiting Temporal Locality Using a Dependence Driven Execution  
Author: Suvas Vajracharya Dirk Grunwald 
Keyword: run-time systems, data locality, temporal locality, loop transformations, dependence-driven, coarse-grain dataflow  
Address: Boulder, CO, U.S.A.  Boulder, CO, U.S.A.  
Affiliation: Department of Computer Science University of Colorado  Department of Computer Science University of Colorado  
Abstract: The order in which loop iterations are executed can have a large impact on the number of cache misses that an applications takes. A new loop order that preserves the semantics of the old order but has a better cache data re-uses, improves the performance of that application. Several compiler techniques exists to staticly transform loops such that the order of iterations reduces cache misses. This paper introduces a run-time method to determine the order based on a dependence-driven execution. In a dependence-driven execution, an execution traverses the iteration space by following the dependence arcs between the iterations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.R. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 19(6) </volume> <pages> 233-246, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: We draw upon prior works in dependency analysis which is a well studied problem [3, 4, 5, 17]. Based on loop dependences, optimizing compilers may make following transformations to improve locality: * Loop Interchange: Loop Interchange <ref> [17, 1] </ref> swaps an inner loop with an outer loop.
Reference: [2] <author> David F. Bacon, Susan L. Graham, and Oliver J. Sharp. </author> <title> Compiler transformations for high-performance computing. </title> <type> Technical Report CSD-93-781, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1993. </year>
Reference-contexts: Skewing traverses the iteration space diagonally in waves. As an example, consider the hyperbolic 1d PDE. Figure 2 shows the dependences and the iteration order before any transformations. skewing and blocking transformation with a block size of two. A good survey of compiler transformations can be found in <ref> [2] </ref>. 3 Dependence Driven Execu tion In this section, we describe the Def-Use-Descriptor-Environment (Dude). In Dude, the loop order of a nested loop is determined during run-time based on data dependence information collected during compile-time.
Reference: [3] <author> U. Banerjee. </author> <title> Speedup of Ordinary programs. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> Oct </month> <year> 1979. </year>
Reference-contexts: The difference between J and I, J I is called the dependence distance vector. The set of distance vectors make up the data dependences that determine the allowable reordering transformation. We draw upon prior works in dependency analysis which is a well studied problem <ref> [3, 4, 5, 17] </ref>. Based on loop dependences, optimizing compilers may make following transformations to improve locality: * Loop Interchange: Loop Interchange [17, 1] swaps an inner loop with an outer loop.
Reference: [4] <author> U. Banerjee. </author> <title> Dependence analysis for supercomputing. </title> <publisher> Kluwer Academic Publisher, </publisher> <address> Boston, Massachusetts, </address> <year> 1988. </year>
Reference-contexts: The difference between J and I, J I is called the dependence distance vector. The set of distance vectors make up the data dependences that determine the allowable reordering transformation. We draw upon prior works in dependency analysis which is a well studied problem <ref> [3, 4, 5, 17] </ref>. Based on loop dependences, optimizing compilers may make following transformations to improve locality: * Loop Interchange: Loop Interchange [17, 1] swaps an inner loop with an outer loop.
Reference: [5] <author> U. Banerjee. </author> <title> An introduction to a formal theory of dependence analysis. </title> <journal> J. Supercomp., </journal> <volume> 2(2) </volume> <pages> 133-149, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The difference between J and I, J I is called the dependence distance vector. The set of distance vectors make up the data dependences that determine the allowable reordering transformation. We draw upon prior works in dependency analysis which is a well studied problem <ref> [3, 4, 5, 17] </ref>. Based on loop dependences, optimizing compilers may make following transformations to improve locality: * Loop Interchange: Loop Interchange [17, 1] swaps an inner loop with an outer loop.
Reference: [6] <author> Forest Baskett. </author> <title> Keynote address. </title> <booktitle> In International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The trends in technology continue to increase the gap between CPU speeds and DRAM speeds. An estimate <ref> [6] </ref> shows that the performance of single-chip microprocessors are improving at a rate of 80% annually, while DRAM speeds are improving at a rate of only 5-10% per year [6] [11]. <p> 1 Introduction The trends in technology continue to increase the gap between CPU speeds and DRAM speeds. An estimate <ref> [6] </ref> shows that the performance of single-chip microprocessors are improving at a rate of 80% annually, while DRAM speeds are improving at a rate of only 5-10% per year [6] [11]. Burger et al. [8] predict that in future microprocessor, techniques to tolerate latency such as pre-fetching and multi-threading may successfully reduce latencies only to expose limitations in memory bandwidth.
Reference: [7] <author> Lance Berc, Sanjay Ghemawat, Monika Hen-zinger, Shun-Tak Leung, Mitch Lichtenberg, Dick Sites, Mark Vandevoorde, Carl Wald-spurger, and Bill Weihl. </author> <title> Digital continuous profiling infrastructure. </title> <booktitle> In Work-in-progress Session of OSDI Symposium, </booktitle> <year> 1996. </year>
Reference-contexts: To determine where the cycles (cache miss, branch mispredict, useful computation, etc.) were spent, we used the Digital Continuous Profiling Infrastructure (DCPI) <ref> [7] </ref> available on Alpha platforms. DCPI runs on the background and unobtrusively generates profile data for the applications running on the machines by sampling hardware performance counters available on the Alphas.
Reference: [8] <author> Douglas C. Burger, James R. Goodman, and Alain Kagi. </author> <title> The declining effectiveness of dynamic caching for general-purpose microprocessors. </title> <type> Technical Report 1261, </type> <institution> University of Wisconsin-Madison, Computer Science Dept., </institution> <year> 1995. </year>
Reference-contexts: An estimate [6] shows that the performance of single-chip microprocessors are improving at a rate of 80% annually, while DRAM speeds are improving at a rate of only 5-10% per year [6] [11]. Burger et al. <ref> [8] </ref> predict that in future microprocessor, techniques to tolerate latency such as pre-fetching and multi-threading may successfully reduce latencies only to expose limitations in memory bandwidth.
Reference: [9] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support fo shared memory parallel computing. </title> <journal> ACM. Trans on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: This object-oriented model is based on AWESIME [10] and the Chores <ref> [9] </ref> run-time systems. The following is a list of objects in Dude: * Data Descriptor: Data Descriptors describe a subsection of the data space. For example, a matrix can be divided into submatrices with each submatrix being defined by a data descriptor.
Reference: [10] <author> Dirk Grunwald. </author> <title> A users guide to awesime: An object oriented parallel programming and simulation system. </title> <type> Technical Report CU-CS-552-91, </type> <institution> University of Colorado, Boulder, </institution> <year> 1991. </year>
Reference-contexts: This object-oriented model is based on AWESIME <ref> [10] </ref> and the Chores [9] run-time systems. The following is a list of objects in Dude: * Data Descriptor: Data Descriptors describe a subsection of the data space. For example, a matrix can be divided into submatrices with each submatrix being defined by a data descriptor.
Reference: [11] <author> J.L. Hennessy and D.A. Patterson. </author> <title> Computer Architecture: a Quantitative Approach. </title> <address> Morgan-Kaufman, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction The trends in technology continue to increase the gap between CPU speeds and DRAM speeds. An estimate [6] shows that the performance of single-chip microprocessors are improving at a rate of 80% annually, while DRAM speeds are improving at a rate of only 5-10% per year [6] <ref> [11] </ref>. Burger et al. [8] predict that in future microprocessor, techniques to tolerate latency such as pre-fetching and multi-threading may successfully reduce latencies only to expose limitations in memory bandwidth.
Reference: [12] <author> M.S. Lam, E.E. Rothberg, and M.E. Wolf. </author> <title> The cache performance and optimization of blocked algorithm. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Optimizing compilers will apply this transformation to improve memory locality if the interchange reduces the array access stride. * Blocking (or Tiling): Blocking <ref> [18, 12, 16] </ref> takes advantage of applications with spatial locality by traversing a rectangle of iteration space at a time.
Reference: [13] <author> S. Levialdi. </author> <title> On shrinking binary picture patterns. </title> <journal> Communications of the ACM, </journal> <volume> 15(1) </volume> <pages> 7-10, </pages> <month> January </month> <year> 1972. </year>
Reference-contexts: If future machines continue to increase the memory access costs relative to cpu cycles cost, the static method will give superior performance on some applications that have perfectly nested loop structures. 4.3 Component Labeling Levialdi's algorithm <ref> [13] </ref> for component labeling is used in image processing to detect connected components of a picture. It involves a series of phases, each phase consisting of changing a 1-pixel to a 0-pixel if its upper, left, and upper-left neighbors are 0-pixels.
Reference: [14] <author> Hanan Samet. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: In a dependence-driven execution, the memory locality of the entire execution of the nested-loop is sensitive to the order in which the initial unconstrained Iterates are loaded. For applications that have block memory access patterns, the system loads the Iterates in Morton order <ref> [14] </ref>. After the initial Iterates has been loaded, the system scheduler pops off an Iterate in LIFO queue and applies the main operator to the data descriptor for that Iterate.
Reference: [15] <author> Suvas Vajracharya and Dirk Grunwald. </author> <title> Dependence-driven run-time system. </title> <booktitle> In Proceedings of Language and Compilers for Parallel Computing, </booktitle> <pages> pages 168-176, </pages> <year> 1996. </year>
Reference-contexts: Dependence information in the form of symbols allows the system to interpret the dependence during run-time. This interpretation gives the system a greater flexibility. Elsewhere <ref> [15] </ref>, we discussed the scalability and the performance of a dependence-driven execution on a multiprocessor. In this paper, we show that performance improvements are attainable on an uniprocessor as well. <p> Since run-time optimization add instructions that were not in the original source code, the cost of the overheads should be less than the cost of cache misses that were avoided. 3. to increase parallelism in loops. Since we have discussed this at some length in <ref> [15] </ref>, we will not discuss it further here. 1 From here on we will say user to refer to the compiler or the programmer To achieve these goals, we have taken a object-oriented approach: data, loops and iterations are composable objects that can be put together to describe complex general loops.
Reference: [16] <author> Michael Edward Wolf. </author> <title> Improving locality and parallelism in nested loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Optimizing compilers will apply this transformation to improve memory locality if the interchange reduces the array access stride. * Blocking (or Tiling): Blocking <ref> [18, 12, 16] </ref> takes advantage of applications with spatial locality by traversing a rectangle of iteration space at a time.
Reference: [17] <author> M.J. Wolfe. </author> <title> Optimizing supercompilers for supercomputers. </title> <type> PhD thesis, </type> <institution> Univ. Illinois, Urbana, </institution> <month> April </month> <year> 1987. </year> <type> Rep. 329. </type>
Reference-contexts: The difference between J and I, J I is called the dependence distance vector. The set of distance vectors make up the data dependences that determine the allowable reordering transformation. We draw upon prior works in dependency analysis which is a well studied problem <ref> [3, 4, 5, 17] </ref>. Based on loop dependences, optimizing compilers may make following transformations to improve locality: * Loop Interchange: Loop Interchange [17, 1] swaps an inner loop with an outer loop. <p> We draw upon prior works in dependency analysis which is a well studied problem [3, 4, 5, 17]. Based on loop dependences, optimizing compilers may make following transformations to improve locality: * Loop Interchange: Loop Interchange <ref> [17, 1] </ref> swaps an inner loop with an outer loop. <p> In some cases, skewing <ref> [17] </ref> can be applied to enable blocking transformation. Skewing traverses the iteration space diagonally in waves. As an example, consider the hyperbolic 1d PDE. Figure 2 shows the dependences and the iteration order before any transformations. skewing and blocking transformation with a block size of two.
Reference: [18] <author> M.J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proc. of Supercomputing 89, </booktitle> <pages> pages 655-664, </pages> <month> Nov </month> <year> 1989. </year>
Reference-contexts: Optimizing compilers will apply this transformation to improve memory locality if the interchange reduces the array access stride. * Blocking (or Tiling): Blocking <ref> [18, 12, 16] </ref> takes advantage of applications with spatial locality by traversing a rectangle of iteration space at a time.
References-found: 18

