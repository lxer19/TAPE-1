URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/gamma-nnsp97/gamma-nnsp97-a4.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/gamma-nnsp97/
Root-URL: http://www.neci.nj.nec.com
Phone: 2  
Title: The Gamma MLP Using Multiple Temporal Resolutions for Improved Classification  
Author: J. Principe, L. Giles, N. Morgan, E. Wilson. Steve Lawrence Andrew D. Back Ah Chung Tsoi C. Lee Giles 
Address: 4 Independence Way, Princeton, NJ 08540  NSW 2522 Australia  
Affiliation: 1 NEC Research Institute,  The Institute of Physical and Chemical Research (RIKEN), Japan 3 Faculty of Informatics, University of Wollongong,  
Note: Appears in IEEE Workshop on Neural Networks for Signal Processing VII, Eds.  pp. 362367. IEEE Press, 1997. Copyright IEEE.  
Abstract: We have previously introduced the Gamma MLP which is defined as an MLP with the usual synaptic weights replaced by gamma filters and associated gain terms throughout all layers. In this paper we apply the Gamma MLP to a larger scale speech phoneme recognition problem, analyze the operation of the network, and investigate why the Gamma MLP can perform better than alternatives. The Gamma MLP is capable of employing multiple temporal resolutions (the temporal resolution is defined here, as per de Vries and Principe, as the number of parameters of freedom (i.e. the number of tap variables) per unit of time in the gamma memory this is equal to the gamma memory parameter as detailed in the paper). Multiple temporal resolutions may be advantageous for certain problems, e.g. different resolutions may be optimal for extracting different features from the input data. For the problem in this paper, the Gamma MLP is observed to use a large range of temporal resolutions. In comparison, TDNN networks typically use only a single temporal resolution. Further motivation for the Gamma MLP is related to the curse of dimensionality and the ability of the Gamma MLP to trade off temporal resolution for memory depth, and therefore increase memory depth without increasing the dimensionality of the network. The IIR MLP is a more general version of the Gamma MLP however the IIR MLP performs poorly for the problem in this paper. Investigation suggests that the error surface of the Gamma MLP is more suitable for gradient descent training than the error surface of the IIR MLP. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.D. </author> <title> Back. New Techniques for Nonlinear System Identification: A Rapprochement Between Neural Networks and Linear Systems. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, University of Queensland, </institution> <year> 1992. </year>
Reference-contexts: Additionally, the Gamma MLP can contain gamma memory structures in all layers of the network. Other motivation for the Gamma MLP can be seen with comparison to TDNN, FIR MLP and IIR MLP (An MLP where the weights are replaced by IIR filters and optional gain terms <ref> [1] </ref>) models. In comparison to the TDNN and FIR MLP models, the Gamma MLP may provide improved performance because it allows temporal resolution to be traded for memory depth, i.e. for a system of given dimensionality, the Gamma MLP can employ filters with a greater memory depth. <p> The Gamma MLP is therefore a special case of the IIR MLP <ref> [1] </ref>. The motivation behind the inclusion of the gain term is discussed in section 5. A separate parameter is used for each filter. Gradient descent update equations for the Gamma MLP are given in [7].
Reference: [2] <author> A.D. Back and A.C. Tsoi. </author> <title> FIR and IIR synapses, a new neural network architecture for time series modeling. </title> <booktitle> Neural Computation, </booktitle> <address> 3(3):375385, </address> <year> 1991. </year>
Reference-contexts: For &lt; 1 the gamma filter may be considered as a low pass filter. For = 1, the memory is a tapped delay line corresponding to the memory structure in an FIR MLP (An MLP where the weights are replaced by FIR filters and optional gain terms <ref> [2] </ref>) or a TDNN.
Reference: [3] <author> Etienne Barnard, R.A. Cole, and L. Hou. </author> <title> Location and classification of plosive constants using expert knowledge and neural-net classifiers. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 84 Supp 1:S60, </volume> <year> 1988. </year>
Reference-contexts: The frequencies of the forty classes varies significantly, and it was found that all models had a tendency to ignore the rarer phonemes <ref> [3] </ref> due to biases inherent in the neural network architecture and training algorithm. We therefore employed a scaling technique whereby weight updates are scaled on a class by class basis.
Reference: [4] <author> H.A. Bourlard and N. Morgan. </author> <title> Connnectionist Speech Recognition: A Hybrid Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1994. </year>
Reference-contexts: Each succeeding frame overlapped with the preceding frame by 10 ms. 9 PLP coefficients plus the signal power were extracted and used as features describing each frame of data. The difference between the current and previous frames was added to the input vectors, as is commonly done <ref> [4] </ref>. Periods of silence before and after the sentences were reduced to two frames in order to limit any skew of the results caused by a disproportionate percentage of silence frames. The models had 40 outputs corresponding to the 40 phonemes 4 .
Reference: [5] <author> B. de Vries and J.C. Principe. </author> <title> The Gamma model a new neural network for temporal processing. Neural Networks, </title> <address> 5(4):565576, </address> <year> 1992. </year>
Reference-contexts: The use of gamma filters as a memory structure at the input of an otherwise stan-dard MLP network was proposed by de Vries and Principe <ref> [5] </ref>. The gamma filter, a special case of an IIR filter, is designed to retain the uncoupling of memory depth to the number of parameters provided by IIR filters, but to have simple stability conditions. <p> The output of a neuron in a multilayer perceptron is computed using 3 y l k = i P N l1 ki y i . The addition of short term memory with delays was consid ered by de Vries and Principe <ref> [5] </ref>: y l k = f i=0 j=0 g l i (t j) where g l kij (t) = ki t j1 e l ki t , j = 1; 2; : : : ; K. <p> The depth of the memory is controlled by , and K is the order of the filter. For the discrete time case, de Vries and Principe <ref> [5] </ref> obtain the following recurrence relation: z j (t) = x (t); j = 0 (1) where x (t) is the filter input and z j (t) are the filter outputs. For &lt; 1 the gamma filter may be considered as a low pass filter. <p> neuron i in layer l 1, y l 0 = 1 (bias), and f is commonly a sigmoid function. 3 The Gamma MLP 3.1 Motivation The focused gamma network which uses the gamma memory as a preprocessing layer for a standard MLP has been proposed by de Vries and Principe <ref> [5] </ref>. This network allows for the use of only one temporal resolution per input. However, it may be desirable to use multiple temporal resolutions (e.g. different resolutions may be optimal for extracting different features or for classifying different phonemes).
Reference: [6] <author> J.H. Friedman. </author> <title> Introduction to computational learning and statistical prediction. </title> <booktitle> Tutorial Presented at Neural Information Processing Systems, </booktitle> <address> Denver, CO, </address> <year> 1995. </year>
Reference-contexts: However, taking into account greater context typically leads to larger models. The amount of training data required for accurate estimation of class distributions can increase significantly when the input dimensionality increases (cf. the curse of dimensionality <ref> [6] </ref>) 1 .
Reference: [7] <author> Steve Lawrence, A.C. Tsoi, and A.D. </author> <title> Back. The Gamma MLP for speech phoneme recognition. </title> <editor> In D. Touretzky, M. Mozer, and M. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 785791. </pages> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: N l (neuron index), l = 0; 1; :::; L (layer), and z l kij j i=0 = 1; w l kij j i=0 = 1 (bias). 2 A Gamma MLP is defined as a multilayer perceptron where every synapse contains a gamma filter and a gain term (introduced in <ref> [7] </ref>), as shown in the definition above. The Gamma MLP is therefore a special case of the IIR MLP [1]. The motivation behind the inclusion of the gain term is discussed in section 5. A separate parameter is used for each filter. <p> The Gamma MLP is therefore a special case of the IIR MLP [1]. The motivation behind the inclusion of the gain term is discussed in section 5. A separate parameter is used for each filter. Gradient descent update equations for the Gamma MLP are given in <ref> [7] </ref>.
Reference: [8] <author> Kai-Fu Lee and Hsiao-Wuen Hon. </author> <title> Speaker-independent phone recognition using hidden Markov models. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 37(11):16411648, </volume> <year> 1989. </year>
Reference-contexts: The tanh activation function was used. A search then converge learning rate schedule was used with an initial learning rate of 0.1 for the parameters and 0.2 for all other parameters. 4 The TIMIT allophones were converted to the standard 40 phoneme set <ref> [8] </ref>. 4.2 Results Results are presented for frame level phoneme recognition, i.e. for each frame the recognizer predicts the current phoneme.
Reference: [9] <author> J.C. Principe, B. de Vries, and P. Oliveira. </author> <title> The gamma filter a new class of adaptive IIR filters with restricted feedback. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 41:649 656, </volume> <year> 1993. </year>
Reference-contexts: The length of the impulse response is related to the memory depth 2 of a system, and hence IIR filters allow a greater memory depth than FIR filters of the same order. However, IIR filters are not widely used in adaptive signal processing <ref> [9] </ref>. <p> For &lt; 1 the gamma memory structure implements a tapped dispersive delay line where the degree of dispersion is controlled by . de Vries and Principe <ref> [9] </ref> define the temporal resolution, R, of a gamma memory structure as the number of parameters of freedom (i.e. the number of tap variables) per unit of time in the filter memory: R = K=D = where D is the memory depth of the structure (the temporal mean value of the
Reference: [10] <author> J.C. Principe, J.M. Kuo, and S. Celebi. </author> <title> An analysis of the Gamma memory in dynamic neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2):331337, </volume> <year> 1994. </year>
Reference-contexts: a gamma memory structure as the number of parameters of freedom (i.e. the number of tap variables) per unit of time in the filter memory: R = K=D = where D is the memory depth of the structure (the temporal mean value of the impulse response of the last tap) <ref> [10] </ref>: D = K=. When = 1, the memory depth is equal to the order of the memory, K. The memory depth increases when &lt; 1, and the temporal resolution decreases, i.e. the gamma memory can trade resolution for memory depth.
Reference: [11] <author> J.J. Shynk. </author> <title> Adaptive IIR filtering. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 421, </pages> <year> 1989. </year>
Reference-contexts: However, IIR filters are not widely used in adaptive signal processing [9]. This may be attributed to the fact that a) there may be instability during training and b) the gradient descent training procedures are not guaranteed to locate the global optimum in the possibly non-convex error surface <ref> [11] </ref>. 1 Additionally, increases in the complexity of the desired target function may make gradient descent optimization more difficult training algorithms may take longer to converge or become stuck in local minima or plateaus which are increasingly poor compared to the global optimum. 2 A greater memory depth implies that the
Reference: [12] <author> L. Yaeger, R. Lyon, and B. Webb. </author> <title> Effective training of a neural network character classifier for word recognition. </title> <editor> In M.C. Mozer, M.I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The amount of scaling is varied using a control parameter, c s , from none (c s = 0) to scaling according to the prior probabilities of the classes (c s = 1). Yaeger et al. have recently introduced a very similar technique which they call frequency balancing <ref> [12] </ref>. Reporting results in terms of the percentage of correct classifications can be misleading when the frequency of the individual classes varies significantly (e.g. a relatively low error rate may be achieved by a network which ignores low frequency classes).
References-found: 12

