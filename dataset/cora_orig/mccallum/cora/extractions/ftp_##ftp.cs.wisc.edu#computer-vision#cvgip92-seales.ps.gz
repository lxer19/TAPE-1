URL: ftp://ftp.cs.wisc.edu/computer-vision/cvgip92-seales.ps.gz
Refering-URL: http://www.cs.wisc.edu/computer-vision/pubs.html
Root-URL: 
Email: seales@ms.uky.edu  dyer@cs.wisc.edu  seales@sophia.inria.fr  
Title: Viewpoint From Occluding Contour  
Author: W. Brent Seales Charles R. Dyer 
Date: September 1992:  
Note: Address until  
Address: Lexington, Kentucky 40536  Madison, Wisconsin 53706  2004 Route des Lucioles, B.P. 109 06561 Valbonne Cedex, France Internet:  
Affiliation: Computer Science Dept University of Kentucky  Computer Science Dept University of Wisconsin  INRIA Sophia-Antipolis,  
Abstract: In this paper we present the geometry and the algorithms for organizing a viewer-centered representation of the occluding contour of polyhedra. The contour is computed from a polyhedral boundary model as it would appear under orthographic projection into the image plane from every viewpoint on the view sphere. Using this representation, we show how to derive constraints on regions in viewpoint space from the relationship between detected image features and our precomputed contour model. Such constraints are based on both qualitative (viewpoint extent) and quantitative (angle measurements and relative geometry) information that has been precomputed about how the contour appears in the image plane as a set of projected curves and T-junctions from self-occlusion. The results we show from an experimental system demonstrate that features of the occluding contour can be computed in a model-based framework, and and their geometry constrains the viewpoints from which a model will project to a set of occluding contour features in an image. 
Abstract-found: 1
Intro-found: 1
Reference: [BC82] <author> R. Bolles and R. Cain. </author> <title> Recognizing and locating partially visible objects: The local-feature-focus method. </title> <journal> Int. J. Robotics Research, </journal> <volume> 1(3) </volume> <pages> 57-82, </pages> <year> 1982. </year>
Reference-contexts: The relative position of the occluding contour to the oriented T-junction was used to further constrain potential correspondences. This is similar in style to the local feature focus method <ref> [BC82] </ref>. A measure of the degree of correspondence was 29 determined by using a least squares distance measure between predicted contour fragments and image contours.
Reference: [BESS89] <author> K. Bowyer, D. Eggert, J. Stewman, and L. Stark. </author> <title> Developing the aspect graph representation for use in image understanding. </title> <booktitle> Proc. Image Understanding Workshop, </booktitle> <pages> pages 831-849, </pages> <year> 1989. </year>
Reference: [BWR90] <author> J. B. Burns, R. Weiss, and E. M. Riseman. </author> <title> View variation of point set and line segment features. </title> <booktitle> Proc. Image Understanding Workshop, </booktitle> <pages> pages 650-659, </pages> <year> 1990. </year>
Reference-contexts: In general, there is a locus of viewpoints within an EE-event patch that can produce a T-junction that exactly matches an image T-junction. The variation in the geometry of an EE-event depends on the size of the viewpoint region where it occurs <ref> [BWR90] </ref>. In other words, the solution for the model-to-image correspondence matrix for a T-junction is not unique given only the two model segments and the image T-junction. We can, however, approximate the geometry of the EE-event by chosing a single, representative viewpoint.
Reference: [CB90] <author> R. Cipolla and A. Blake. </author> <title> The dynamic analysis of apparent contours. </title> <booktitle> Proc. 3rd Int. Conf. Computer Vision, </booktitle> <pages> pages 616-623, </pages> <year> 1990. </year>
Reference: [EB89] <author> D. Eggert and K. Bowyer. </author> <title> Computing the orthographic projection aspect graph of solids of revolution. </title> <booktitle> Proc. IEEE Workshop on Interpretation of 3D Scenes, </booktitle> <pages> pages 102-108, </pages> <year> 1989. </year>
Reference: [FH86] <author> O. Faugeras and M. Hebert. </author> <title> The representation, recognition, and locating of 3-D objects. </title> <journal> Int. J. Robotics Research, </journal> <volume> 5(3) </volume> <pages> 27-52, </pages> <year> 1986. </year>
Reference-contexts: Features such as T-junctions and curvature extrema on the contour are interesting in that they persist over large portions of the space of viewpoints despite the continuous change in the 3D surface points generating them. Our approach avoids difficult numerical problems by relying on the linearity of polyhedra <ref> [FH86] </ref>. We define here the sets of 3D points called the rim, the visible rim and the occluded rim. These definitions for smooth surfaces motivate our approximation of them for the polyhedral model. Let p 2 S be a point on a smooth, oriented surface in IR 3 .
Reference: [GM90] <author> Z. Gigus and J. Malik. </author> <title> Computing the aspect graph for line drawings of polyhedral objects. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intell., </journal> <volume> 12(2) </volume> <pages> 113-122, </pages> <year> 1990. </year>
Reference: [Gri90] <author> W. E. L. </author> <title> Grimson. Object recognition by computer: The role of geometric constraints. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: For example, in the case of iterative methods [Low87], a starting viewpoint can be obtained from a constrained viewpoint set. Parameter space methods [TM87] that can avoid a costly search of the entire space of transformations become much more efficient. The search of an interpretation tree <ref> [Gri90] </ref> is more efficient when there are global constraints on the possible solutions. A constrained viewpoint region can help aspect graph methods [BESS89,KP90,Ike87] address the problem of how to select a few aspects to test from a large number of potential aspects. 32
Reference: [Ike87] <author> K. </author> <title> Ikeuchi. Generating an interpretation tree from a CAD model for 3D object recognition in bin-picking tasks. </title> <journal> Int. J. Computer Vision, </journal> <pages> pages 145-165, </pages> <year> 1987. </year>
Reference: [KD91] <author> Kiriakos N. Kutulakos and Charles R. Dyer. </author> <title> Recovering shape by purposive viewpoint adjustment. </title> <type> Technical Report 1035, </type> <institution> University of Wisconsin Madi-son, </institution> <month> August </month> <year> 1991. </year>
Reference: [Koe84] <author> J. J. Koenderink. </author> <title> What does the occluding contour tell us about solid shape? Perception, </title> <booktitle> 13 </booktitle> <pages> 321-330, </pages> <year> 1984. </year>
Reference-contexts: The projection mapping generates occluding contours, and the opacity of solid shape causes the creation of T-junctions in the image plane. The arrangement of these contours and T-junctions is directly related to the 3D properties of the shape and provides strong information for recognition <ref> [Koe84] </ref>. Object-centered object models do not explicitly represent the properties of the occluding contour since the occluding contour is not generated by any specific set of object features. Viewer-centered models of 3D shape are better suited to encode the viewpoint-dependent properties of the occluding contour.
Reference: [Koe90] <author> J. J. Koenderink. </author> <title> Solid Shape. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: 1 Introduction The occluding contour is a prominent part of what is seen in the image of a projected three-dimensional shape. For example, human observers can obtain information about 3D surface shape and orientation from the contour alone <ref> [Koe90] </ref>. This paper reports our work toward recovering a viewpoint estimate of a model from the occluding contour in a model-based vision system. First, we examine an approach to modeling the features of the occluding contour of polyhedra.
Reference: [KP89] <author> D. J. Kriegman and J. Ponce. </author> <title> Computing exact aspect graphs of curved objects: Solids of revolution. </title> <booktitle> Proc. IEEE Workshop on Interpretation of 3D Scenes, </booktitle> <pages> pages 116-122, </pages> <year> 1989. </year>
Reference: [KP90] <author> D. J. Kriegman and J. Ponce. </author> <title> On recognizing and positioning curved 3D objects from image contours. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intell., </journal> <volume> 12(12) </volume> <pages> 1127-1137, </pages> <year> 1990. </year>
Reference-contexts: In particular, the locus of points that lie on the folds and creases of the multi-dimensional rim surface are of interest since they are formed by the self-occlusion of the model. Krieg-man and Ponce <ref> [KP90] </ref> have used parametric patches under a weak perspective viewing model in order to formulate equations for the occluding contour of surfaces of revolution. The equations that describe the self-occlusion of the occluding boundary of a torus under orthographic projection is an 8th degree polynomial containing over 170 terms. <p> The CC-event is the T-junction that is produced by the projection of two smooth 18 surfaces. The difficulty in explicitly representing T-junctions for smooth surfaces is the numerical complexity of the locus of surface points that define the viewing directions where T-junctions occur <ref> [KP90] </ref>. For polyhedra, we assemble the EE-events into piecewise CC-event structures in order to provide an approximation of the smooth T-junction geometry.
Reference: [KvD79] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The internal representation of solid shape with respect to vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year> <month> 33 </month>
Reference: [Low87] <author> D. G. Lowe. </author> <title> Three-dimensional object recognition from single two-dimensional images. </title> <journal> Artificial Intell., </journal> <volume> 31 </volume> <pages> 355-395, </pages> <year> 1987. </year>
Reference-contexts: The problem of constraining viewpoint for a particular model is often considered a subproblem of model-based 3D object recognition <ref> [Low87] </ref>. In general, the model-based approach is to select a model M and the corresponding viewpoint V that will produce a projection that best matches the image data. For each model, the best viewpoint V is selected from a space of all possible viewpoints. <p> An exact solution for viewpoint is not found, but rather a constrained region of viewpoints is found that accounts for the T-junction and occluding contour data. Given a tightly-constrained set of viewpoints, an exact solution can be found using, for example, an iterative method <ref> [Low87] </ref>. Furthermore, a contour correspondence is found, not an exact edge-to-edge correspondence. The two model edges aligned with the T-junction in Because of the symmetry of the torus there are many close solutions that can be found. The solution viewpoint displayed is the center of the final solution viewpoint region. <p> The constrained viewpoint region that is computed using the the occluding contour can be used together with other methods for exact viewpoint recovery that must assume a starting point or an initial, approximate solution. For example, in the case of iterative methods <ref> [Low87] </ref>, a starting viewpoint can be obtained from a constrained viewpoint set. Parameter space methods [TM87] that can avoid a costly search of the entire space of transformations become much more efficient.
Reference: [Mal87] <author> J. Malik. </author> <title> Interpreting line drawings of curved objects. </title> <journal> Int. J. Computer Vision, </journal> <volume> 1 </volume> <pages> 73-103, </pages> <year> 1987. </year>
Reference-contexts: Finally, we present implementation results and make several observations about the methods. 3.1 Contour Geometry and Organization Smooth opaque shapes without surface discontinuities generate only T-junctions, smooth occluding contours, and contour terminals in the image plane <ref> [Mal87] </ref>. We assume here that the polyhedral model is an approximation of a smooth shape. It can then be assumed that none of the polyhedral edges are true surface discontinuities. In general, surface discontinuities can be treated without difficulty, but for simplicity the discussion here is restricted to smooth surfaces.
Reference: [PD86] <author> W. H. Plantinga and C. R. Dyer. </author> <title> An algorithm for constructing the aspect graph. </title> <booktitle> Proc. IEEE Symp. Foundations of Computer Science, </booktitle> <pages> pages 123-131, </pages> <year> 1986. </year>
Reference: [PD90] <author> W. H. Plantinga and C. R. Dyer. </author> <title> Visibility, occlusion, and the aspect graph. </title> <journal> Int. J. Computer Vision, </journal> <volume> 5(2) </volume> <pages> 137-160, </pages> <year> 1990. </year>
Reference-contexts: These results indicate that the rim appearance representation saves significant time and space over the asp (and hence the aspect graph as well because the aspect graph contains nodes and transitions for all changes in the projected line drawing <ref> [PD90] </ref>) while preserving the completeness of the exact appearance of the occluding contour at all viewpoints. 15 Model No. Faces Total No. No. <p> A set of EE-event hypersurfaces that are adjacent in aspect space (viewpoint fi image plane) establishes a piecewise representation for a single T-junction between two occluding contours. This piecewise-connected set of hypersurfaces approximates a single contour-contour event (CC-event). 1 Aspect space and its properties are fully developed in <ref> [PD90] </ref>. 17 between the edges in the rightmost view. At the transition viewpoint the T-junction curve intersects the crease between the two edges that share a vertex. The rim surface for a polyhedron is a piecewise collection of individual rim surfaces for the edges.
Reference: [Pla88] <author> W. H. Plantinga. </author> <title> The Asp: A Continuous, Viewer-centered Object Representation for Computer Vision. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of Wisconsin-Madison, </institution> <year> 1988. </year>
Reference-contexts: The space has been termed aspect space <ref> [Pla88] </ref>. Consider a surface patch S IR 3 . <p> X 4 : fi 1 ( ~ X 1 ) + (1 fi 1 )( ~ X 2 ) (6) where fi 1 = V (( ~ X 2 ~ X 1 ) fi ( ~ X 4 ~ X 3 )) with the viewpoint V expressed in Cartesian coordinates <ref> [Pla88] </ref>. This curve of intersection is specified in the boundary representation of the rim surface and is exactly the image location of the T-junction between the two edges. Figure 3 shows the curve where the two rim surfaces intersect for a fixed value of the viewpoint parameter . <p> Find EE-events between edges that are part of the rim. (a) Modify the structure for each rim edge that is occluded by another. This builds a boundary representation (BREP) with algebraic boundaries specified by the equations for the EV-events and EEE-events <ref> [Pla88] </ref>. (b) Organize the final BREP for each edge according to its global visibility. The global visibility test must be made to determine if the edge is fully visible or completely occluded. 3. Connect the individual BREP's together spatially and in viewpoint. <p> Since the algorithm must compute the potential intersection in the image plane of each rim edge with every other, the construction time is bounded by O (n 5 ). These complexity bounds are the same as those for constructing the asp, an intermediate representation for computing the aspect graph <ref> [Pla88] </ref>. As with the asp, pathological polyhedra such as picket fences and grids can achieve the worst-case behavior. <p> By the definition of the polyhedral rim, an edge only forms T-junctions for viewpoints where that edge is on the rim. The image coordinates of a projected T-junction for two edges are represented directly as a function of the viewing direction V and the endpoints of the segments <ref> [Pla88] </ref>. As shown in Figure 6 (b), a viewpoint is modeled as a point (; ) on the unit sphere. The orthographic projective transformation for this sphere of viewpoints is rotation by (; ) and then orthographic projection in the direction of the z-axis.
Reference: [RDW88] <author> W. Richards, B. Dawson, and D. Whittington. </author> <title> Encoding contour shape by curvature extrema, </title> <address> pages 83-98. </address> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1988. </year>
Reference: [SD91] <author> W. B. Seales and C. R. Dyer. </author> <title> Constrained viewpoint from occluding contour. </title> <booktitle> Proc. IEEE Workshop on Directions in Automated CAD-based Vision, </booktitle> <month> June </month> <year> 1991. </year>
Reference: [Sea91] <author> W. B. Seales. </author> <title> Appearance models of three-dimensional shape for machine vision and graphics. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of Wisconsin-Madison, </institution> <month> August </month> <year> 1991. </year>
Reference: [SJ89] <author> T. Sripradisvarakul and R. Jain. </author> <title> Generating aspect graphs for curved objects. </title> <booktitle> Proc. Workshop on Interpretation of 3D Scenes, </booktitle> <pages> pages 109-115, </pages> <year> 1989. </year>
Reference-contexts: Others have computed the singularities of the rim surface for the purposes of computing the aspect graph for surfaces of revolution [EB89,KP89] and for parametric surfaces <ref> [SJ89] </ref>. All of the work with smooth model representations suffers from difficult numerical problems. Our approach is to compute a simplified, approximate rim surface that is the result of the piecewise-linear occluding contour of polyhedra.
Reference: [TM87] <author> D. W. Thompson and J. L. Mundy. </author> <title> Three-dimensional model matching from an unconstrained viewpoint. </title> <booktitle> Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 208-220, </pages> <year> 1987. </year>
Reference-contexts: For example, in the case of iterative methods [Low87], a starting viewpoint can be obtained from a constrained viewpoint set. Parameter space methods <ref> [TM87] </ref> that can avoid a costly search of the entire space of transformations become much more efficient. The search of an interpretation tree [Gri90] is more efficient when there are global constraints on the possible solutions.
Reference: [VF91] <author> R. Vaillant and O. Faugeras. </author> <title> Using extremal boundaries for 3D object modeling. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intell., </journal> <year> 1991. </year> <month> 34 </month>
Reference-contexts: This recovered viewpoint region comes from the constraints that have been precomputed about how the occluding contour can appear in the image plane. We assume that geometric information about the models is known, occluding contours can be detected in the image <ref> [VF91] </ref>, and 2 information such as surface normals or texture is not available. The problem of constraining viewpoint for a particular model is often considered a subproblem of model-based 3D object recognition [Low87].
References-found: 26

