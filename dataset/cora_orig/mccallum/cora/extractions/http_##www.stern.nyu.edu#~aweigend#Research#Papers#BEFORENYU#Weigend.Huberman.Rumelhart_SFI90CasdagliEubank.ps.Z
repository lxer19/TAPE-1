URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/Weigend.Huberman.Rumelhart_SFI90CasdagliEubank.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: PREDICTING SUNSPOTS AND EXCHANGE RATES WITH CONNECTIONIST NETWORKS  
Author: Andreas S.Weigend Bernardo A.Huberman David E.Rumelhart 
Note: Proceedings of the 1990 NATO Workshop on Nonlinear Modeling and Forecasting (Santa Fe, NM) Edited by Stephen Eubank and Martin Casdagli. Redwood City, CA: Addison-Wesley (1991)  
Address: Stanford, CA 94305 Palo Alto, CA 94304 Stanford, CA 94305  
Affiliation: Physics Department Dynamics of Computation Psychology Department Stanford University Xerox PARC Stanford University  
Abstract: We investigate the effectiveness of connectionist networks for predicting the future continuation of temporal sequences. The problem of overfitting, particularly serious for short records of noisy data, is addressed by the method of weight-elimination: a term penalizing network complexity is added to the usual cost function in back-propagation. The ultimate goal is prediction accuracy. We analyze two time series. On the benchmark sunspot series, the networks outperform traditional statistical approaches. We show that the network performance does not deteriorate when there are more input units than needed. Weight-elimination also manages to extract some part of the dynamics of the notoriously noisy currency exchange rates and makes the network solution interpretable. 
Abstract-found: 1
Intro-found: 1
Reference: [Aka70] <author> Hirotugo Akaike. </author> <title> Statistical predictor identification. </title> <journal> Ann. Institute of Statistical Mathematics, </journal> <volume> 22 </volume> <pages> 203-217, </pages> <year> 1970. </year>
Reference-contexts: We believe that these heuristics cannot be blindly applied to connectionist networks. An example of such a heuristic is to multiply the fitting error with (N +k)=(N k), where N is the number of data points and k is the number of parameters of the model (Akaike <ref> [Aka70] </ref>). It is not at all clear to what degree such approximations break down for nonlinear models, such as connectionist networks.
Reference: [Bar91] <author> Andrew R. Barron. </author> <title> Universal approximation bounds for superpositions of a sig-moidal function. </title> <type> Technical Report 58, </type> <institution> University of Illinois (Statistics Department), Urbana, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: example, the feat of forecasting the dazzling displays of the chaotic characteristics of the quadratic map, x t = 4x t1 (1 x t1 ), amounts to nothing but approximating a parabola! Given the theoretical results that networks can essentially fit any function (Cybenko [Cyb89], Funahashi [Fun89], White [Whi90], Barron <ref> [Bar91] </ref>), there is not much content in showing that networks can, yes, also fit a parabola. Weigend, Huberman and Rumelhart 2 The problem of overfitting is approached from two angles: by using internal validation and by the method of weight-elimination.
Reference: [BC91] <author> Pierre Baldi and Yves Chauvin. </author> <title> A study of generalization in simple linear networks. </title> <note> Submitted to Neural Computation, </note> <year> 1991. </year>
Reference-contexts: early stopping through internal validation are that * a part of the available training data cannot be used directly for parameter estimation, * the monitored error validation set error often shows multiple minima as a function of training time (even in the simple linear case analyzed by Baldi and Chauvin <ref> [BC91] </ref>), * the specific solution at the stopping points depends strongly on the specific pair of training set and validation set, and * the results are also sensitive on the specific initialization of the parameters.
Reference: [BLL90] <author> William A. Brock, Josef Lakonishok, and Blake LeBaron. </author> <title> Simple technical trading rules and the stochastic properties of stock returns. </title> <type> Technical Report 9022, </type> <institution> Social Systems Research Institute, University of Wisconsin - Madison, </institution> <year> 1990. </year>
Reference-contexts: We obtain an estimate of the distribution of by straightforwardly applying the bootstrap method by Efron [Efr79]. Our use of the bootstrap idea should be contrasted with Brock et al. <ref> [BLL90] </ref> and LeBaron [LeB91]: whereas we bootstrap the correlation coefficient, they bootstrap the time series. 16 We take the network solution from Fig. 9 after 500 epochs, and closely follow Efron [Efr79, p. 465]: 1. <p> We exactly follow the procedure given by Efron [Efr79] to obtain this distribution and then simply read off the probabilities of certain values of for that time series and that solution. Brock et al. <ref> [BLL90] </ref> and LeBaron [LeB91] use the bootstrap idea to generate new time series that embody the null hypothesis. The simple null hypothesis of a random walk, for example, is implemented by resampling the returns of the original series.
Reference: [Bre88] <author> G. Larry Bretthorst. </author> <title> Bayesian Spectrum Analysis and Parameter Estimation, </title> <booktitle> volume 48 of Lecture Notes in Statistics. </booktitle> <publisher> Springer, </publisher> <year> 1988. </year>
Reference-contexts: In the simplest case, a single hyperplane approximates the 12 Given the periodic appearance of the series, one might want to try to approximate the series by sinusoids. Such a Fourier interpolation captures the global structure in time over the entire fitting range. Bretthorst <ref> [Bre88] </ref> carries out a Bayesian Fourier Analysis of the sunspot data. Extending his solution into the future gives poor predictions: almost immediately after the end of the fitting region, the extrapolation yields negative numbers for the sunspots. Weigend, Huberman and Rumelhart 15 data points.
Reference: [Bro91] <author> William A. Brock. </author> <title> Causality, chaos, explanation and prediction in economics and finance. </title> <editor> In J. Casti and A. Karlqvist, editors, </editor> <title> Beyond Belief: Randomness, Prediction, </title> <booktitle> and Explanation in Science, </booktitle> <pages> pages 230-279. </pages> <publisher> CRC Press, </publisher> <address> Boca Raton, FL, </address> <year> 1991. </year>
Reference-contexts: Refreshingly critical remarks on dimensions of chaotic systems are expressed by Ruelle [Rue90]. More specifically on the problem of prediction, see Casdagli [Cas89] and Farmer and Sidorowich [FS89]. Chaos (or its absence) in economic and financial time series is discussed by Brock <ref> [Bro91] </ref>.
Reference: [BW91] <author> Wray L. Buntine and Andreas S. Weigend. </author> <title> Bayesian Back-propagation. </title> <type> NASA/Ames Technical Report, </type> <year> 1991. </year>
Reference-contexts: Since we want this unit to predict the probability that the return is positive, we choose a sigmoidal unit with range (0; 1) and minimize the Weigend, Huberman and Rumelhart 24 cross-entropy error (Golden [Gol88], Buntine and Weigend <ref> [BW91] </ref>), N X fi fl t i denotes the target value and o i the actual output value for day i. <p> Offer the returns of the other currencies of the recent past in addition to the present day. * Combine five such networks, one for each day, to make predictions for every day of the week possible. * Estimate also the error of each forecast, as outlined in Buntine and Weigend <ref> [BW91] </ref>. * Add connections that link the inputs directly with the outputs, skipping the hidden layer. These weights can encode only a linear part of the solution. * Add input units that are given previous forecasting errors, allowing the network to model the response to exogenous shocks or innovations.
Reference: [Cas89] <author> Martin Casdagli. </author> <title> Nonlinear prediction of chaotic time series. </title> <journal> Physica D, </journal> <volume> 35 </volume> <pages> 335-356, </pages> <year> 1989. </year>
Reference-contexts: More general introductions to chaotic dynamics are Schuster [Sch88] and Eubank and Farmer [EF90]. Refreshingly critical remarks on dimensions of chaotic systems are expressed by Ruelle [Rue90]. More specifically on the problem of prediction, see Casdagli <ref> [Cas89] </ref> and Farmer and Sidorowich [FS89]. Chaos (or its absence) in economic and financial time series is discussed by Brock [Bro91].
Reference: [Che90] <author> Peter C. Cheeseman. </author> <title> On finding the most probable model. In Jeff Shrager and Pat Langley, </title> <editor> editors, </editor> <booktitle> Computational Models of Scientific Discovery and Theory Formation, </booktitle> <pages> pages 73-95. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: For our discussion, differences between these approaches are not important. An introduction is given by Cheeseman <ref> [Che90] </ref>.
Reference: [Cyb89] <author> George Cybenko. </author> <title> Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> 2 </volume> <pages> 303-314, </pages> <year> 1989. </year>
Reference-contexts: For example, the feat of forecasting the dazzling displays of the chaotic characteristics of the quadratic map, x t = 4x t1 (1 x t1 ), amounts to nothing but approximating a parabola! Given the theoretical results that networks can essentially fit any function (Cybenko <ref> [Cyb89] </ref>, Funahashi [Fun89], White [Whi90], Barron [Bar91]), there is not much content in showing that networks can, yes, also fit a parabola. Weigend, Huberman and Rumelhart 2 The problem of overfitting is approached from two angles: by using internal validation and by the method of weight-elimination.
Reference: [DN90] <author> Francis X. Diebold and James A. Nason. </author> <title> Nonparametric exchange rate prediction? Journal of International Economics, </title> <booktitle> 28 </booktitle> <pages> 315-332, </pages> <year> 1990. </year>
Reference-contexts: Any simple predictor can easily reduce the in-sample error by twenty per cent immediately yielding disastrous out-of-sample performance, much worse than the random walk prediction that tomorrow's rate is the same as today's. Diebold and Nason <ref> [DN90] </ref> review attempts to forecast foreign exchange rates and conclude that none of them succeeded out-of-sample: the efficient market hypothesis implies that the vicissitudes of financial markets are unpredictable. 5.1 TASK AND ARCHITECTURE If the daily rates really performed a random walk, then the difference from one day to the next
Reference: [EF90] <editor> Stephen G. Eubank and J. Doyne Farmer. </editor> <title> An introduction to chaos and randomness. In Erica Jen, editor, 1989 Lectures in Complex Systems, SFI Studies in the Sciences of Complexity, </title> <journal> Lect. </journal> <volume> Vol. II, </volume> <pages> pages 75-190. </pages> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: More general introductions to chaotic dynamics are Schuster [Sch88] and Eubank and Farmer <ref> [EF90] </ref>. Refreshingly critical remarks on dimensions of chaotic systems are expressed by Ruelle [Rue90]. More specifically on the problem of prediction, see Casdagli [Cas89] and Farmer and Sidorowich [FS89]. Chaos (or its absence) in economic and financial time series is discussed by Brock [Bro91].
Reference: [Efr79] <author> Bradley Efron. </author> <title> Computers and the theory of statistics: Thinking the unthinkable. </title> <journal> SIAM Review (Society for Industrial and Applied Mathematics), </journal> <volume> 21 </volume> <pages> 460-480, </pages> <year> 1979. </year>
Reference-contexts: We obtain an estimate of the distribution of by straightforwardly applying the bootstrap method by Efron <ref> [Efr79] </ref>. Our use of the bootstrap idea should be contrasted with Brock et al. [BLL90] and LeBaron [LeB91]: whereas we bootstrap the correlation coefficient, they bootstrap the time series. 16 We take the network solution from Fig. 9 after 500 epochs, and closely follow Efron [Efr79, p. 465]: 1. <p> Our use of the bootstrap idea should be contrasted with Brock et al. [BLL90] and LeBaron [LeB91]: whereas we bootstrap the correlation coefficient, they bootstrap the time series. 16 We take the network solution from Fig. 9 after 500 epochs, and closely follow Efron <ref> [Efr79, p. 465] </ref>: 1. Use a random number generator to draw 215 pairs independently and with replacement from the set of 215 out-of sample target-output pairs. Such a new sample, called a bootstrap sample, is a subset of the original 215 out-of-sample pairs. 2. <p> We exactly follow the procedure given by Efron <ref> [Efr79] </ref> to obtain this distribution and then simply read off the probabilities of certain values of for that time series and that solution. Brock et al. [BLL90] and LeBaron [LeB91] use the bootstrap idea to generate new time series that embody the null hypothesis.
Reference: [Fou90] <author> Peter V. Foukal. </author> <title> The variable sun. </title> <publisher> Scientific American, </publisher> <address> 262:34, </address> <month> February </month> <year> 1990. </year>
Reference-contexts: They are analyzed in Weigend [Wei91]. Weigend, Huberman and Rumelhart 14 4 SUNSPOTS Sunspots, often larger in diameter than the earth, are dark blotches on the sun. They were first observed around 1610, shortly after the invention of the telescope <ref> [Fou90] </ref>. Yearly averages have been recorded since 1700. The sunspot numbers are defined as k (10g + f ), where g is the number of sunspot groups, f is the number of individual sunspots, and k is used to reduce different telescopes to the same scale (Marple [Mar87]).
Reference: [Fri91] <author> Jerome H. Friedman. </author> <title> Multivariate adaptive regression splines (with discussion). </title> <journal> The Annals of Statistics, </journal> <volume> 19:1:141, </volume> <year> 1991. </year>
Reference-contexts: On iterated predictions into the future, the network performance turns out to be significantly better than most other models. 3 Comparable results were only obtained by multivariate adaptive regression splines (MARS, by Friedman <ref> [Fri91] </ref>, applied to the sunspot series by Lewis and Stevens [LS91]). Furthermore, by increasing the number of inputs to four times the usual size, we also show that networks can ignore irrelevant information. This feature encouraged us to apply networks to the prediction of the notoriously noisy foreign exchange rates. <p> For one and two iterations, both methods perform comparably. When iterated more than twice, however, the network outperforms the WLP model. Recently, Lewis and Stevens [LS91] applied multivariate adaptive regression splines (MARS, Friedman <ref> [Fri91] </ref>) to the sunspot series. We find that the performance of MARS is very similar to the network performance. <p> We found the network performance to be very comparable to the performance obtained with multivariate adaptive regression splines (MARS) by Friedman <ref> [Fri91] </ref>, applied to the sunspot series by Lewis and Stevens [LS91]. In the second example, the notoriously noisy foreign exchange rates series, we defined a sub-task by picking one day (Monday) and one currency (DM vs. US$).
Reference: [FS89] <editor> J. Doyne Farmer and John J. </editor> <title> Sidorowich. Exploiting chaos to predict the future and reduce noise. </title> <editor> In Y. C. Lee, editor, </editor> <title> Evolution, Learning and Cognition. </title> <publisher> World Scientific, </publisher> <year> 1989. </year> <note> Weigend, Huberman and Rumelhart 34 </note>
Reference-contexts: More general introductions to chaotic dynamics are Schuster [Sch88] and Eubank and Farmer [EF90]. Refreshingly critical remarks on dimensions of chaotic systems are expressed by Ruelle [Rue90]. More specifically on the problem of prediction, see Casdagli [Cas89] and Farmer and Sidorowich <ref> [FS89] </ref>. Chaos (or its absence) in economic and financial time series is discussed by Brock [Bro91].
Reference: [Fun89] <author> Ken-ichi Funahashi. </author> <title> On the approximate realization of continuous mappings by neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 183-192, </pages> <year> 1989. </year>
Reference-contexts: For example, the feat of forecasting the dazzling displays of the chaotic characteristics of the quadratic map, x t = 4x t1 (1 x t1 ), amounts to nothing but approximating a parabola! Given the theoretical results that networks can essentially fit any function (Cybenko [Cyb89], Funahashi <ref> [Fun89] </ref>, White [Whi90], Barron [Bar91]), there is not much content in showing that networks can, yes, also fit a parabola. Weigend, Huberman and Rumelhart 2 The problem of overfitting is approached from two angles: by using internal validation and by the method of weight-elimination.
Reference: [Ger89] <author> Neil A. Gershenfeld. </author> <title> An experimentalist's introduction to the observation of dynamical systems. </title> <editor> In Bai-Lin Hao, editor, </editor> <booktitle> Directions in Chaos, </booktitle> <volume> volume 2, </volume> <pages> pages 310-384. </pages> <publisher> World Scientific, </publisher> <year> 1989. </year>
Reference-contexts: This lag space representation is consistent with the short-term predictability of chaotic systems. It is to be contrasted to a stronger approach for non-chaotic systems with long-term predictability, where one usually 4 A clear introduction is given by Gershenfeld <ref> [Ger89] </ref>, new results on embedding are presented by Sauer, Casdagli and Yorke [SCY91]. More general introductions to chaotic dynamics are Schuster [Sch88] and Eubank and Farmer [EF90]. Refreshingly critical remarks on dimensions of chaotic systems are expressed by Ruelle [Rue90].
Reference: [Gol88] <author> Richard M. Golden. </author> <title> A unified framework fo connectionist systems. </title> <journal> Biological Cybernetics, </journal> <volume> 59 </volume> <pages> 109-120, </pages> <year> 1988. </year>
Reference-contexts: Its target value is one when the price goes up and zero otherwise. Since we want this unit to predict the probability that the return is positive, we choose a sigmoidal unit with range (0; 1) and minimize the Weigend, Huberman and Rumelhart 24 cross-entropy error (Golden <ref> [Gol88] </ref>, Buntine and Weigend [BW91]), N X fi fl t i denotes the target value and o i the actual output value for day i.
Reference: [GS81] <author> M. M. Gabr and T. Subba Rao. </author> <title> The estimation and prediction of subset bilinear time series models with applications. </title> <journal> Journal of Time Series Analysis, </journal> <volume> 2 </volume> <pages> 155-171, </pages> <year> 1981. </year>
Reference-contexts: Our evaluation of the network model, however, is carried out by comparison to a nonlinear model, the threshold autoregressive model (TAR) by Tong and Lim [TL80, Ton90]. It has served as a benchmark for Priestley [Pri81, Pri88], for Subba Rao and Gabr <ref> [GS81, SG84] </ref>, for Lewis and Stevens [LS91], for Stokbro [Sto91] and others. The TAR model is globally nonlinear: it consists of two local linear autoregressive models. Tong and Lim found optimal performance for input dimension d = 12. <p> This concludes the comparison with the benchmark model. Weigend, Huberman and Rumelhart 20 Gabr and Subba Rao <ref> [GS81, SG84] </ref> apply a bilinear model 14 to the sunspot data and find an improvement of about 15% over the TAR model, both for single step and iterated predictions. <p> We first applied the networks to the sunspot series. On this noisy real world time series, our networks outperformed the threshold autoregressive model by Tong and Lim [TL80], the bilinear model by Gabr and Subba Rao <ref> [GS81, SG84] </ref>, and the weighted linear predictors by Stokbro [Sto91]. We found the network performance to be very comparable to the performance obtained with multivariate adaptive regression splines (MARS) by Friedman [Fri91], applied to the sunspot series by Lewis and Stevens [LS91].
Reference: [Hin87] <author> Geoffrey E. Hinton. </author> <title> Connectionist learning procedures. </title> <type> Technical Report CMU-CS-87-115 (version 2), </type> <institution> Carnegie-Mellon University, </institution> <year> 1987. </year>
Reference-contexts: Depending on the dynamic range and the number of the units of the preceding layer, w 0 might be given different values for different layers of the network. 11 Variations and alternatives have been developed by Hinton <ref> [Hin87] </ref>, Scalettar and Zee [SZ88], Hanson and Pratt [HP89], Mozer and Smolensky [MS89], Schurmann [Sch89], Ishikawa [Ish90], Le Cun, Denker and Solla, [LDS90], Ji, Snapp and Psaltis [JSP90], and others. They are analyzed in Weigend [Wei91].
Reference: [HP89] <author> Stephen Jose Hanson and Lorien Y. Pratt. </author> <title> Comparing biases for minimal network construction with back-propagation. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1 (NIPS*88), </booktitle> <pages> pages 177-185. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: Depending on the dynamic range and the number of the units of the preceding layer, w 0 might be given different values for different layers of the network. 11 Variations and alternatives have been developed by Hinton [Hin87], Scalettar and Zee [SZ88], Hanson and Pratt <ref> [HP89] </ref>, Mozer and Smolensky [MS89], Schurmann [Sch89], Ishikawa [Ish90], Le Cun, Denker and Solla, [LDS90], Ji, Snapp and Psaltis [JSP90], and others. They are analyzed in Weigend [Wei91]. Weigend, Huberman and Rumelhart 14 4 SUNSPOTS Sunspots, often larger in diameter than the earth, are dark blotches on the sun.
Reference: [Hsi89] <author> David Hsieh. </author> <title> Testing for nonlinear dependence in daily foreign exchange rates. </title> <journal> Journal of Business, </journal> <volume> 62 </volume> <pages> 339-369, </pages> <year> 1989. </year>
Reference-contexts: It is well known (Hsieh <ref> [Hsi89] </ref>) that there is a strong day of the week effect: the dynamics of the market differs for different days of the week.
Reference: [Ish90] <author> Masumi Ishikawa. </author> <title> A structural learning algorithm with forgetting of link weights. </title> <type> Technical Report TR-90-7, </type> <institution> Electrotechnical Laboratory, Life Electronics Research Center, </institution> <address> Tokyo, </address> <year> 1990. </year> <title> Modified version of a paper presented at IJCNN, </title> <address> Washington D.C. </address> <year> 1989. </year>
Reference-contexts: dynamic range and the number of the units of the preceding layer, w 0 might be given different values for different layers of the network. 11 Variations and alternatives have been developed by Hinton [Hin87], Scalettar and Zee [SZ88], Hanson and Pratt [HP89], Mozer and Smolensky [MS89], Schurmann [Sch89], Ishikawa <ref> [Ish90] </ref>, Le Cun, Denker and Solla, [LDS90], Ji, Snapp and Psaltis [JSP90], and others. They are analyzed in Weigend [Wei91]. Weigend, Huberman and Rumelhart 14 4 SUNSPOTS Sunspots, often larger in diameter than the earth, are dark blotches on the sun.
Reference: [JSP90] <author> Chuanyi Ji, Robert R. Snapp, and Demetri Psaltis. </author> <title> Generalizing smoothness constraints from discrete samples. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 188-197, 90. </pages>
Reference-contexts: layer, w 0 might be given different values for different layers of the network. 11 Variations and alternatives have been developed by Hinton [Hin87], Scalettar and Zee [SZ88], Hanson and Pratt [HP89], Mozer and Smolensky [MS89], Schurmann [Sch89], Ishikawa [Ish90], Le Cun, Denker and Solla, [LDS90], Ji, Snapp and Psaltis <ref> [JSP90] </ref>, and others. They are analyzed in Weigend [Wei91]. Weigend, Huberman and Rumelhart 14 4 SUNSPOTS Sunspots, often larger in diameter than the earth, are dark blotches on the sun. They were first observed around 1610, shortly after the invention of the telescope [Fou90].
Reference: [LDS90] <author> Yann Le Cun, John S. Denker, and Sara A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2 (NIPS*89), </booktitle> <pages> pages 598-605. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: the units of the preceding layer, w 0 might be given different values for different layers of the network. 11 Variations and alternatives have been developed by Hinton [Hin87], Scalettar and Zee [SZ88], Hanson and Pratt [HP89], Mozer and Smolensky [MS89], Schurmann [Sch89], Ishikawa [Ish90], Le Cun, Denker and Solla, <ref> [LDS90] </ref>, Ji, Snapp and Psaltis [JSP90], and others. They are analyzed in Weigend [Wei91]. Weigend, Huberman and Rumelhart 14 4 SUNSPOTS Sunspots, often larger in diameter than the earth, are dark blotches on the sun. They were first observed around 1610, shortly after the invention of the telescope [Fou90].
Reference: [LeB91] <author> Blake LeBaron. </author> <title> Technical trading rules and simulated processes for foreign exchange rates. </title> <type> Technical report, </type> <institution> Department of Economics, University of Wisconsin - Madison, </institution> <year> 1991. </year>
Reference-contexts: r denotes the relative variances of DM-returns of the set in comparison to the 15 We thank Blake LeBaron for giving us the data, for the friendly discussions at Santa Fe, for sharing his knowledge about the problem domain, and for sending us a draft of a forthcoming paper, LeBaron <ref> [LeB91] </ref>. <p> We obtain an estimate of the distribution of by straightforwardly applying the bootstrap method by Efron [Efr79]. Our use of the bootstrap idea should be contrasted with Brock et al. [BLL90] and LeBaron <ref> [LeB91] </ref>: whereas we bootstrap the correlation coefficient, they bootstrap the time series. 16 We take the network solution from Fig. 9 after 500 epochs, and closely follow Efron [Efr79, p. 465]: 1. <p> We exactly follow the procedure given by Efron [Efr79] to obtain this distribution and then simply read off the probabilities of certain values of for that time series and that solution. Brock et al. [BLL90] and LeBaron <ref> [LeB91] </ref> use the bootstrap idea to generate new time series that embody the null hypothesis. The simple null hypothesis of a random walk, for example, is implemented by resampling the returns of the original series. <p> Important quantities are boldfaced, medium ones are in normal font, and insignificant ones are italicized. For example, the Canadian Dollar returns are less important than the returns of the other four currencies. This finding, consistent with LeBaron <ref> [LeB91] </ref>, could be due to the strong coupling between the Canadian and US economies. The sign in parenthesis indicates the sign of the weight. The signs of the three previous days DM return are (+), (-) and (+). The magnitude of their weights is similar to the present day return.
Reference: [LF87] <author> Alan S. Lapedes and Robert M. Farber. </author> <title> Nonlinear signal processing using neural networks: prediction and system modelling. </title> <type> Technical Report LA-UR-87-2662, </type> <institution> Los Alamos National Laboratory, </institution> <year> 1987. </year>
Reference-contexts: We use feed-forward networks to predict the continuation of a temporal sequence by extracting knowledge from its past. In distinction to previous connectionist approaches that dealt only with noise free, computer generated time series 2 (Lapedes and Farber <ref> [LF87] </ref>, Moody and Darken [MD89]), we focus on noisy, real world data of limited record length. In this case, the problem of overfitting can become very serious. A priori, it is not clear what network size is required to solve a given problem.
Reference: [LKS91] <author> Yann Le Cun, Ido Kanter, and Sara A. Solla. </author> <title> Second order properties of error surfaces: Learning time and generalization. </title> <editor> In R. P. Lippmann, J. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3 (NIPS*90), </booktitle> <pages> pages 918-924. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: The equi-activation surfaces (on which a hidden unit's activation is constant) are 7 For merely historical reasons, we use sigmoids in the sunspot example and tanh-units for the exchange rates. Tanh-units train faster than sigmoids under conditions given by Le Cun et al. <ref> [LKS91] </ref>. A comparison to radial basis functions is given in Weigend et al. [WHR90].
Reference: [LS91] <author> Peter A. W. Lewis and J. G. Stevens. </author> <title> Nonlinear modeling of time series using multivariate adaptive regression splines (MARS). </title> <journal> Submitted to Journal of the American Statistical Association, </journal> <note> 1991. Weigend, Huberman and Rumelhart 35 </note>
Reference-contexts: On iterated predictions into the future, the network performance turns out to be significantly better than most other models. 3 Comparable results were only obtained by multivariate adaptive regression splines (MARS, by Friedman [Fri91], applied to the sunspot series by Lewis and Stevens <ref> [LS91] </ref>). Furthermore, by increasing the number of inputs to four times the usual size, we also show that networks can ignore irrelevant information. This feature encouraged us to apply networks to the prediction of the notoriously noisy foreign exchange rates. <p> Our evaluation of the network model, however, is carried out by comparison to a nonlinear model, the threshold autoregressive model (TAR) by Tong and Lim [TL80, Ton90]. It has served as a benchmark for Priestley [Pri81, Pri88], for Subba Rao and Gabr [GS81, SG84], for Lewis and Stevens <ref> [LS91] </ref>, for Stokbro [Sto91] and others. The TAR model is globally nonlinear: it consists of two local linear autoregressive models. Tong and Lim found optimal performance for input dimension d = 12. <p> For one and two iterations, both methods perform comparably. When iterated more than twice, however, the network outperforms the WLP model. Recently, Lewis and Stevens <ref> [LS91] </ref> applied multivariate adaptive regression splines (MARS, Friedman [Fri91]) to the sunspot series. We find that the performance of MARS is very similar to the network performance. <p> We found the network performance to be very comparable to the performance obtained with multivariate adaptive regression splines (MARS) by Friedman [Fri91], applied to the sunspot series by Lewis and Stevens <ref> [LS91] </ref>. In the second example, the notoriously noisy foreign exchange rates series, we defined a sub-task by picking one day (Monday) and one currency (DM vs. US$).
Reference: [Mac91] <author> David J. C. MacKay. </author> <title> 1. Bayesian interpolation and 2. A practical Bayesian framework for backprop networks. </title> <note> To be submitted to Neural Computation, </note> <year> 1991. </year>
Reference-contexts: If the goal is to compare different classes of models such as sigmoid networks and radial basis function networks, p (D) cannot be treated as constant any more. This more general case is addressed by MacKay <ref> [Mac91] </ref>. Weigend, Huberman and Rumelhart 13 for different values of . Weights that are merely the result of noise are drawn from the bump centered on zero; they are expected to be small.
Reference: [Mar81] <author> R. Douglas Martin. </author> <title> Robust methods for time series. </title> <editor> In D. F. Findley, editor, </editor> <booktitle> Applied Time Series Analysis II, </booktitle> <pages> pages 683-759. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: For autoregression, this assumption is poor since the same observation first serves as target output, then as input. Robust error models specific for time series prediction are discussed by Martin <ref> [Mar81] </ref>. Weigend, Huberman and Rumelhart 5 Then, given the lag space, primitives, error model and complexity model, all that is left to do is to estimate the parameters of the surface minimizing the sum of the second ingredient (error cost) and the third ingredient (complexity cost).
Reference: [Mar87] <author> S. Lawrence Marple. </author> <title> Digital Spectral Analysis with Applications. </title> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: Yearly averages have been recorded since 1700. The sunspot numbers are defined as k (10g + f ), where g is the number of sunspot groups, f is the number of individual sunspots, and k is used to reduce different telescopes to the same scale (Marple <ref> [Mar87] </ref>). The observations are shown as black squares in Fig. 4. The average time between maxima is 11 years. Note, however, that the time between maxima ranges from 7 to 15 years. 12 The underlying mechanism for sunspot appearances is not exactly known.
Reference: [MD89] <author> John Moody and Christian J. Darken. </author> <title> Fast learning in networks of locally tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference-contexts: We use feed-forward networks to predict the continuation of a temporal sequence by extracting knowledge from its past. In distinction to previous connectionist approaches that dealt only with noise free, computer generated time series 2 (Lapedes and Farber [LF87], Moody and Darken <ref> [MD89] </ref>), we focus on noisy, real world data of limited record length. In this case, the problem of overfitting can become very serious. A priori, it is not clear what network size is required to solve a given problem.
Reference: [MS89] <author> Michael C. Mozer and Paul Smolensky. </author> <title> Using relevance to reduce network size automatically. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 3-16, </pages> <year> 1989. </year>
Reference-contexts: Depending on the dynamic range and the number of the units of the preceding layer, w 0 might be given different values for different layers of the network. 11 Variations and alternatives have been developed by Hinton [Hin87], Scalettar and Zee [SZ88], Hanson and Pratt [HP89], Mozer and Smolensky <ref> [MS89] </ref>, Schurmann [Sch89], Ishikawa [Ish90], Le Cun, Denker and Solla, [LDS90], Ji, Snapp and Psaltis [JSP90], and others. They are analyzed in Weigend [Wei91]. Weigend, Huberman and Rumelhart 14 4 SUNSPOTS Sunspots, often larger in diameter than the earth, are dark blotches on the sun.
Reference: [Pri81] <author> Maurice B. Priestley. </author> <title> Spectral Analysis and Time Series. </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: Our evaluation of the network model, however, is carried out by comparison to a nonlinear model, the threshold autoregressive model (TAR) by Tong and Lim [TL80, Ton90]. It has served as a benchmark for Priestley <ref> [Pri81, Pri88] </ref>, for Subba Rao and Gabr [GS81, SG84], for Lewis and Stevens [LS91], for Stokbro [Sto91] and others. The TAR model is globally nonlinear: it consists of two local linear autoregressive models. Tong and Lim found optimal performance for input dimension d = 12.
Reference: [Pri88] <author> Maurice B. Priestley. </author> <title> Non-linear and Non-stationary Time Series Analysis. </title> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: Our evaluation of the network model, however, is carried out by comparison to a nonlinear model, the threshold autoregressive model (TAR) by Tong and Lim [TL80, Ton90]. It has served as a benchmark for Priestley <ref> [Pri81, Pri88] </ref>, for Subba Rao and Gabr [GS81, SG84], for Lewis and Stevens [LS91], for Stokbro [Sto91] and others. The TAR model is globally nonlinear: it consists of two local linear autoregressive models. Tong and Lim found optimal performance for input dimension d = 12.
Reference: [RHW86] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In David E. Rumelhart, James L. McClelland, and the PDP Research Group, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: In back-propagation learning (Rumelhart et al. <ref> [RHW86] </ref>), this minimization is simply done by gradient descent. Once the surface has been determined, the prediction for the value following a point in lag space is given by the value of the surface above that point in the (x t1 ; x t2 ; :::; x td )-plane. <p> In the next section, we turn to this question of parameter estimation. Particular attention is devoted to the problem of model selection for noisy data. Weigend, Huberman and Rumelhart 9 3 LEARNING 3.1 BACK-PROPAGATION We use the error back-propagation algorithm of Rumelhart et al. <ref> [RHW86] </ref> to train the network: the parameters are changed by gradient descent on the cost surface over the weights and biases. This is numerically very simple.
Reference: [Ris87] <author> Jorma Rissanen. </author> <title> Stochastic complexity. </title> <journal> Journal Royal Statistical Society B, </journal> <volume> 49 </volume> <pages> 223-239 (with discussion: 252-265), </pages> <year> 1987. </year>
Reference-contexts: lead to too large weight changes, corresponding to a too big effect of the last pattern (if updated after each pattern), or to too large overall weight changes (if updated at the end of each epoch). 9 Wallace and Boulton [WB68, WF87] introduce the term Minimum Message Length (MML), Rissanen <ref> [Ris87, Ris89] </ref> uses the term Minimum Description Length (MDL). For our discussion, differences between these approaches are not important. An introduction is given by Cheeseman [Che90].
Reference: [Ris89] <author> Jorma Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <year> 1989. </year>
Reference-contexts: lead to too large weight changes, corresponding to a too big effect of the last pattern (if updated after each pattern), or to too large overall weight changes (if updated at the end of each epoch). 9 Wallace and Boulton [WB68, WF87] introduce the term Minimum Message Length (MML), Rissanen <ref> [Ris87, Ris89] </ref> uses the term Minimum Description Length (MDL). For our discussion, differences between these approaches are not important. An introduction is given by Cheeseman [Che90].
Reference: [Rue90] <author> David Ruelle. </author> <title> Deterministic chaos: the science and the fiction. </title> <journal> Proc. Royal Society London, </journal> <volume> A 427 </volume> <pages> 241-248, </pages> <year> 1990. </year>
Reference-contexts: More general introductions to chaotic dynamics are Schuster [Sch88] and Eubank and Farmer [EF90]. Refreshingly critical remarks on dimensions of chaotic systems are expressed by Ruelle <ref> [Rue90] </ref>. More specifically on the problem of prediction, see Casdagli [Cas89] and Farmer and Sidorowich [FS89]. Chaos (or its absence) in economic and financial time series is discussed by Brock [Bro91].
Reference: [Sch88] <author> Heinz-Georg Schuster. </author> <title> Deterministic Chaos. </title> <publisher> VCH Verlagsgesellschaft, </publisher> <year> 1988. </year>
Reference-contexts: It is to be contrasted to a stronger approach for non-chaotic systems with long-term predictability, where one usually 4 A clear introduction is given by Gershenfeld [Ger89], new results on embedding are presented by Sauer, Casdagli and Yorke [SCY91]. More general introductions to chaotic dynamics are Schuster <ref> [Sch88] </ref> and Eubank and Farmer [EF90]. Refreshingly critical remarks on dimensions of chaotic systems are expressed by Ruelle [Rue90]. More specifically on the problem of prediction, see Casdagli [Cas89] and Farmer and Sidorowich [FS89]. Chaos (or its absence) in economic and financial time series is discussed by Brock [Bro91].
Reference: [Sch89] <author> Bernd Sch urmann. </author> <title> Stability and adaptation in artificial neural systems. </title> <journal> Physical Review A, </journal> <volume> 40 </volume> <pages> 2681-2688, </pages> <year> 1989. </year>
Reference-contexts: on the dynamic range and the number of the units of the preceding layer, w 0 might be given different values for different layers of the network. 11 Variations and alternatives have been developed by Hinton [Hin87], Scalettar and Zee [SZ88], Hanson and Pratt [HP89], Mozer and Smolensky [MS89], Schurmann <ref> [Sch89] </ref>, Ishikawa [Ish90], Le Cun, Denker and Solla, [LDS90], Ji, Snapp and Psaltis [JSP90], and others. They are analyzed in Weigend [Wei91]. Weigend, Huberman and Rumelhart 14 4 SUNSPOTS Sunspots, often larger in diameter than the earth, are dark blotches on the sun.
Reference: [SCY91] <author> Tim Sauer, Martin Casdagli, and James A. Yorke. Embedology. </author> <note> Submitted to Comm. </note> <institution> Math. Phys, </institution> <year> 1991. </year>
Reference-contexts: It is to be contrasted to a stronger approach for non-chaotic systems with long-term predictability, where one usually 4 A clear introduction is given by Gershenfeld [Ger89], new results on embedding are presented by Sauer, Casdagli and Yorke <ref> [SCY91] </ref>. More general introductions to chaotic dynamics are Schuster [Sch88] and Eubank and Farmer [EF90]. Refreshingly critical remarks on dimensions of chaotic systems are expressed by Ruelle [Rue90]. More specifically on the problem of prediction, see Casdagli [Cas89] and Farmer and Sidorowich [FS89].
Reference: [SG84] <author> T. Subba Rao and M. M. Gabr. </author> <title> An Introduction to Bispectral Analysis and Bilinear Time Series Models, </title> <booktitle> volume 24 of Lecture Notes in Statistics. </booktitle> <publisher> Springer, </publisher> <year> 1984. </year>
Reference-contexts: We then apply this method to the time series of sunspots and of foreign exchange rates, and close with some ideas for the future. 3 Specifically, we compare the network model to the threshold autoregressive model by Tong and Lim [TL80], the bilinear model by Subba Rao and Gabr <ref> [SG84] </ref> and the weighted linear predictor model by Stokbro [Sto91]. All models are globally nonlinear. <p> Our evaluation of the network model, however, is carried out by comparison to a nonlinear model, the threshold autoregressive model (TAR) by Tong and Lim [TL80, Ton90]. It has served as a benchmark for Priestley [Pri81, Pri88], for Subba Rao and Gabr <ref> [GS81, SG84] </ref>, for Lewis and Stevens [LS91], for Stokbro [Sto91] and others. The TAR model is globally nonlinear: it consists of two local linear autoregressive models. Tong and Lim found optimal performance for input dimension d = 12. <p> This concludes the comparison with the benchmark model. Weigend, Huberman and Rumelhart 20 Gabr and Subba Rao <ref> [GS81, SG84] </ref> apply a bilinear model 14 to the sunspot data and find an improvement of about 15% over the TAR model, both for single step and iterated predictions. <p> We first applied the networks to the sunspot series. On this noisy real world time series, our networks outperformed the threshold autoregressive model by Tong and Lim [TL80], the bilinear model by Gabr and Subba Rao <ref> [GS81, SG84] </ref>, and the weighted linear predictors by Stokbro [Sto91]. We found the network performance to be very comparable to the performance obtained with multivariate adaptive regression splines (MARS) by Friedman [Fri91], applied to the sunspot series by Lewis and Stevens [LS91].
Reference: [SM90] <author> George Sugihara and Robert M. </author> <month> May. </month> <title> Nonlinear forecasting as a way of distinguishing chaos from measurement error in time series. </title> <journal> Nature, </journal> <volume> 344 </volume> <pages> 734-741, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: The performance does not degrade with input dimension several times larger than necessary: the network ignores irrelevant information. This important insensitivity to the input dimension is an advantage over other prediction methods such as the simplex algorithm employed by Sugihara and May <ref> [SM90] </ref>. 14 In addition to linear autoregression (terms proportional to x ti ), Subba Rao and Gabr allow terms proportional to the forecasting errors * tj as well as terms proportional to the product x tk * tl (bilinear interactions).
Reference: [Sto91] <author> Kurt Stokbro. </author> <title> Predicting chaos with weighted maps. </title> <type> Technical Report 91/10 S, </type> <institution> Nordita, Copenhagen, </institution> <year> 1991. </year>
Reference-contexts: series of sunspots and of foreign exchange rates, and close with some ideas for the future. 3 Specifically, we compare the network model to the threshold autoregressive model by Tong and Lim [TL80], the bilinear model by Subba Rao and Gabr [SG84] and the weighted linear predictor model by Stokbro <ref> [Sto91] </ref>. All models are globally nonlinear. Weigend, Huberman and Rumelhart 3 2 REPRESENTATION 2.1 EMBEDDING We begin our analysis with a discussion of the representation of the time series fx t g, a sequence of measurements of an observable x that were taken at equal time intervals. <p> It has served as a benchmark for Priestley [Pri81, Pri88], for Subba Rao and Gabr [GS81, SG84], for Lewis and Stevens [LS91], for Stokbro <ref> [Sto91] </ref> and others. The TAR model is globally nonlinear: it consists of two local linear autoregressive models. Tong and Lim found optimal performance for input dimension d = 12. <p> On predictions further than one step into the future, our networks outperform the bilinear model on average by 35% in mean squared error. Stokbro <ref> [Sto91] </ref> uses a weighted linear predictor (WLP). In a WLP, each primitive is the product of a first order polynomial and a normalized Gaussian radial basis function. The predictor is the linear superposition of these primitives. <p> We first applied the networks to the sunspot series. On this noisy real world time series, our networks outperformed the threshold autoregressive model by Tong and Lim [TL80], the bilinear model by Gabr and Subba Rao [GS81, SG84], and the weighted linear predictors by Stokbro <ref> [Sto91] </ref>. We found the network performance to be very comparable to the performance obtained with multivariate adaptive regression splines (MARS) by Friedman [Fri91], applied to the sunspot series by Lewis and Stevens [LS91].
Reference: [SZ88] <author> R. Scalettar and A. </author> <title> Zee. Emergence of grandmother memory in feed-forward networks: Learning with noise and forgetfulness. </title> <editor> In D. Waltz and J. Feldman, editors, </editor> <booktitle> Connectionist Models and their Implications, </booktitle> <pages> pages 309-327. </pages> <publisher> Ablex, </publisher> <year> 1988. </year> <note> Weigend, Huberman and Rumelhart 36 </note>
Reference-contexts: Depending on the dynamic range and the number of the units of the preceding layer, w 0 might be given different values for different layers of the network. 11 Variations and alternatives have been developed by Hinton [Hin87], Scalettar and Zee <ref> [SZ88] </ref>, Hanson and Pratt [HP89], Mozer and Smolensky [MS89], Schurmann [Sch89], Ishikawa [Ish90], Le Cun, Denker and Solla, [LDS90], Ji, Snapp and Psaltis [JSP90], and others. They are analyzed in Weigend [Wei91].
Reference: [Tak81] <author> Floris Takens. </author> <title> Detecting strange attractors in turbulence. </title> <editor> In D. A. Rand and L.-S. Young, editors, </editor> <booktitle> Dynamical Systems and Turbulence, volume 898 of Lecture Notes in Mathematics, </booktitle> <pages> pages 366-381. </pages> <publisher> Springer, </publisher> <year> 1981. </year>
Reference-contexts: This standard approach is also called state-space reconstruction in the physics community and tapped delay line in the engineering community. Since it is a regression onto past values of the variable that is to be predicted, statisticians refer to it as autoregressive. For noise-free data, Takens <ref> [Tak81] </ref> showed that this approach constitutes an embedding (in a mathematical sense) that preserves invariances. 4 The optimal window-size d, however, is not supplied with the time series to be predicted. On the one hand, d has to be large enough to resolve ambiguities.
Reference: [TL80] <author> Howell Tong and K. S. Lim. </author> <title> Threshold autoregression, limit cycles and cyclical data. </title> <journal> Journal Royal Statistical Society B, </journal> <volume> 42 </volume> <pages> 245-292, </pages> <year> 1980. </year>
Reference-contexts: We then apply this method to the time series of sunspots and of foreign exchange rates, and close with some ideas for the future. 3 Specifically, we compare the network model to the threshold autoregressive model by Tong and Lim <ref> [TL80] </ref>, the bilinear model by Subba Rao and Gabr [SG84] and the weighted linear predictor model by Stokbro [Sto91]. All models are globally nonlinear. <p> Weigend, Huberman and Rumelhart 15 data points. Such a linear autoregressive model is a linear superposition of past values of the observable. Our evaluation of the network model, however, is carried out by comparison to a nonlinear model, the threshold autoregressive model (TAR) by Tong and Lim <ref> [TL80, Ton90] </ref>. It has served as a benchmark for Priestley [Pri81, Pri88], for Subba Rao and Gabr [GS81, SG84], for Lewis and Stevens [LS91], for Stokbro [Sto91] and others. The TAR model is globally nonlinear: it consists of two local linear autoregressive models. <p> On the sunspot data set, the prediction error for direct multi-step prediction was significantly worse than the error for iterated single-step prediction. In summary, although we took extreme care not to gain any unfair advantage over Tong and Lim <ref> [TL80] </ref> (by taking the same input dimension, using identical data sets, minimizing the same sum of squared errors, etc.), the multi-step predictions were found to be significantly better: on average, the iterated prediction variances of the network were about half the iterated prediction variances of the threshold autoregressive model. <p> Weigend, Huberman and Rumelhart 30 6 SUMMARY We investigated connectionist networks for short-term prediction of time series. We first applied the networks to the sunspot series. On this noisy real world time series, our networks outperformed the threshold autoregressive model by Tong and Lim <ref> [TL80] </ref>, the bilinear model by Gabr and Subba Rao [GS81, SG84], and the weighted linear predictors by Stokbro [Sto91].
Reference: [Ton90] <author> Howell Tong. </author> <title> Non-linear Time Series: a Dynamical System Approach. </title> <publisher> Oxford University Press, </publisher> <year> 1990. </year>
Reference-contexts: Weigend, Huberman and Rumelhart 15 data points. Such a linear autoregressive model is a linear superposition of past values of the observable. Our evaluation of the network model, however, is carried out by comparison to a nonlinear model, the threshold autoregressive model (TAR) by Tong and Lim <ref> [TL80, Ton90] </ref>. It has served as a benchmark for Priestley [Pri81, Pri88], for Subba Rao and Gabr [GS81, SG84], for Lewis and Stevens [LS91], for Stokbro [Sto91] and others. The TAR model is globally nonlinear: it consists of two local linear autoregressive models.
Reference: [Vap82] <author> Vladimir N. Vapnik. </author> <title> Estimation of Dependencies Based on Empirical Data. </title> <publisher> Springer, </publisher> <year> 1982. </year>
Reference-contexts: Two common choices are * likelihood / exp ( 1 2 D 2 t = 2 ) (GAUSS) ! minimize squared differences, P t * likelihood / exp (LAPLACE) ! minimize absolute differences, P As Vapnik <ref> [Vap82] </ref> points out, a Gaussian model is appropriate if the measurements are carried out under fixed conditions, whereas a Laplacian model reflects maximally varying experimental conditions. 5 We here assume the errors to be Gaussian distributed. 3. Complexity model.
Reference: [WB68] <author> Chris S. Wallace and D. M. Boulton. </author> <title> An information measure for classification. </title> <journal> Comp. J., </journal> <volume> 11 </volume> <pages> 185-195, </pages> <year> 1968. </year>
Reference-contexts: Too large learning rates for the exchange rates lead to too large weight changes, corresponding to a too big effect of the last pattern (if updated after each pattern), or to too large overall weight changes (if updated at the end of each epoch). 9 Wallace and Boulton <ref> [WB68, WF87] </ref> introduce the term Minimum Message Length (MML), Rissanen [Ris87, Ris89] uses the term Minimum Description Length (MDL). For our discussion, differences between these approaches are not important. An introduction is given by Cheeseman [Che90].
Reference: [Wei91] <author> Andreas S. Weigend. </author> <title> Connectionist Architectures for Time Series Prediction. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1991. </year> <note> (in preparation). </note>
Reference-contexts: They are analyzed in Weigend <ref> [Wei91] </ref>. Weigend, Huberman and Rumelhart 14 4 SUNSPOTS Sunspots, often larger in diameter than the earth, are dark blotches on the sun. They were first observed around 1610, shortly after the invention of the telescope [Fou90]. Yearly averages have been recorded since 1700. <p> In the language of statistics, this means to extend the nonlinear autoregressive model to a nonlinear autoregressive Weigend, Huberman and Rumelhart 29 moving average (ARMA) model, see Weigend <ref> [Wei91] </ref>. * Finally, leaving the turf of mere academic comparison of forecasting methods, Offer additional information that cannot be derived from the exchange rates, such as interest rates. Change the cost-function to reflect the actual profit.
Reference: [WF87] <author> Chris S. Wallace and P. R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal Royal Statistical Society B, </journal> <volume> 49 </volume> <pages> 240-265 (with discussion), </pages> <year> 1987. </year>
Reference-contexts: Too large learning rates for the exchange rates lead to too large weight changes, corresponding to a too big effect of the last pattern (if updated after each pattern), or to too large overall weight changes (if updated at the end of each epoch). 9 Wallace and Boulton <ref> [WB68, WF87] </ref> introduce the term Minimum Message Length (MML), Rissanen [Ris87, Ris89] uses the term Minimum Description Length (MDL). For our discussion, differences between these approaches are not important. An introduction is given by Cheeseman [Che90].
Reference: [Whi90] <author> Halbert White. </author> <title> Connectionist nonparametric regression: multilayer feedforward networks can learn arbitrary mappings. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 535-549, </pages> <year> 1990. </year>
Reference-contexts: For example, the feat of forecasting the dazzling displays of the chaotic characteristics of the quadratic map, x t = 4x t1 (1 x t1 ), amounts to nothing but approximating a parabola! Given the theoretical results that networks can essentially fit any function (Cybenko [Cyb89], Funahashi [Fun89], White <ref> [Whi90] </ref>, Barron [Bar91]), there is not much content in showing that networks can, yes, also fit a parabola. Weigend, Huberman and Rumelhart 2 The problem of overfitting is approached from two angles: by using internal validation and by the method of weight-elimination.
Reference: [WHR90] <author> Andreas S. Weigend, Bernardo A. Huberman, and David E. Rumelhart. </author> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 193-209, </pages> <year> 1990. </year>
Reference-contexts: These two examplesmedium noise and medium nonlinearity for the sunspots, very high noise and small nonlinearity for the exchange ratescomplement the example of a chaotic time series with little noise for which the network solution was found to be highly nonlinear: in Weigend et al. <ref> [WHR90] </ref>, we analyzed a time series from a computational ecosystem and found that connectionist networks can predict the fraction of agents choosing a certain strategy for hundreds of steps into the future. <p> Tanh-units train faster than sigmoids under conditions given by Le Cun et al. [LKS91]. A comparison to radial basis functions is given in Weigend et al. <ref> [WHR90] </ref>. <p> We analyzed the specific solution of the network that was stopped at point b and subsequently trained with a very small learning rate for a few epochs. (The parameters of the network can be found in Weigend et al. <ref> [WHR90] </ref>.) The main contribution to the first hidden unit comes from x t9 , to the second hidden unit from x t2 , and to the third hidden unit from x t1 . <p> The predictor is the linear superposition of these primitives. Stokbro compares WLP with the network solution on the on the 1921 to to 1946 prediction set that is given in Weigend et al. <ref> [WHR90] </ref>. For one and two iterations, both methods perform comparably. When iterated more than twice, however, the network outperforms the WLP model. Recently, Lewis and Stevens [LS91] applied multivariate adaptive regression splines (MARS, Friedman [Fri91]) to the sunspot series.
Reference: [WRH91] <author> Andreas S. Weigend, David E. Rumelhart, and Bernardo A. Huberman. </author> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In R. P. Lippmann, J. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3 (NIPS*90), </booktitle> <pages> pages 875-882. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: In Weigend et al. <ref> [WRH91] </ref>, we also compare weight-elimination with yet another approach that tries to minimize the number of bits per weight, thereby creating a network that is not too dependent on the precise values of its weights. 3.3 WEIGHT-ELIMINATION In 1987, Rumelhart proposed several methods for finding minimal networks within the framework of
Reference: [Yul27] <author> G. U. Yule. </author> <title> On a method of investigating periodicities in disturbed series with special reference to Wolfer's sunspot numbers. </title> <journal> Philosophical Transactions Royal Society London Ser. A, </journal> <volume> 226 </volume> <pages> 267-298, </pages> <year> 1927. </year>
Reference-contexts: All models are globally nonlinear. Weigend, Huberman and Rumelhart 3 2 REPRESENTATION 2.1 EMBEDDING We begin our analysis with a discussion of the representation of the time series fx t g, a sequence of measurements of an observable x that were taken at equal time intervals. Following Yule <ref> [Yul27] </ref>, we express the present value x t as a function of the previous d values of the time series itself, x t = f (past values) = 8 : (x t1 ; x t2 ; :::; x td ) 7! x t The vector (x t1 ; x t2 ;
References-found: 59

