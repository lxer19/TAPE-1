URL: http://www.cs.cornell.edu/Info/People/amaldi/ijns.ps
Refering-URL: http://www.cs.cornell.edu/Info/Projects/ccop/reports.html
Root-URL: 
Title: Two Constructive Methods for Designing Compact Feedforward Networks of Threshold Units  
Author: Edoardo Amaldi Bertrand Guenin 
Keyword: Feedforward networks, threshold units, constructive algorithms, greedy strategy.  
Abstract: We propose two algorithms for constructing and training compact feedforward networks of linear threshold units. The Shift procedure constructs networks with a single hidden layer while the PTI constructs multilayered networks. The resulting networks are guaranteed to perform any given task with binary or real-valued inputs. The various experimental results reported for tasks with binary and real inputs indicate that our methods compare favorably with alternative procedures deriving from similar strategies, both in terms of size of the resulting networks and of their generalization properties. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Amaldi. </author> <title> On the complexity of training perceptrons. </title> <editor> In T. Kohonen, K. Makisara, O. Sim-ula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 55-60, </pages> <address> Amsterdam, 1991. </address> <publisher> Elsevier science publishers B.V. </publisher>
Reference-contexts: The first one pertains to the computational complexity of training single linear threshold units. The problems of maximizing the number of correct classifications or minimizing the number of errors are not only NP-hard to solve optimally but also to approximate within some constant factors <ref> [1, 4, 6] </ref>. These problems becomes even harder when all input patterns with one of the two desired outputs must be correctly classified, like in the methods proposed in [33, 32, 15]. The second limitation is inherent to the greedy approach. <p> Clearly, for nonlinearly separable tasks, updates never terminate and the sequence of w i does not need to tend towards a weight vector that minimizes the number 3 of errors or maximizes the number of correct classifications. Since it is N P-hard to find such an optimal weight vector <ref> [1] </ref> and even an approximate one that is guaranteed to be a fixed percentage away from the optimum [4, 6, 26, 5], we have to settle for an efficient heuristic with a good average-case behavior.
Reference: [2] <author> E. Amaldi. </author> <title> From finding maximum feasible subsystems of linear systems to feedforward neural network design. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, Swiss Federal Institute of Technology, Lausanne, </institution> <year> 1994. </year>
Reference-contexts: Breaking down the network design problem into that of training a sequence of single units is certainly attractive. However, even if optimum weight vectors were available for each single unit, greedy strategies would not be guaranteed to yield minimum size networks, see <ref> [2] </ref> for several examples. The computational complexity of the overall network design problem clearly accounts for these two limitations. In the sequel we focus on networks with n inputs, h hidden layers and a single output unit. <p> In this work we use the thermal perceptron procedure [21] to find a close-to-optimal weight vector for each single unit. Although other methods could be used, this one compares favorably with the pocket procedure [22] and with some other methods based on least mean squared error or cross-entropy minimization <ref> [19, 21, 2] </ref>. The basic idea is to pay decreasing attention to misclassified input vectors with large total inputs. <p> Specifically, at the beginning of the c th cycle we set t = flt 0 with fl = 1 (c=C). See <ref> [2, 3] </ref> for more details. Since t is decreased to 0, the updates clearly terminate after C cycles. <p> is worth noting that, if the weight connecting the first hidden unit to the output unit was just taken as = min k fv k : z k = 1 ^ b k = 1g, at least four hidden units would be necessary in order to perform the original task <ref> [2] </ref>. In fact, it is easy to see that if optimal weights are found for single units the Shift algorithm yields a network which performs the parity function with n inputs and d n+1 2 e 1 hidden units and direct input-to-output connections. <p> In fact, there is some similarity in the way the tasks for new units are defined in the PTI and in the Offset algorithms <ref> [2] </ref>. In the PTI, however, the output unit does not compute a parity function, more than one hidden layer is constructed, prototypes are considered instead of input vectors and only prototypes belonging to unfaithful classes are included in the task assigned to each new unit.
Reference: [3] <author> E. Amaldi and C. Diderich. </author> <title> On the probabilistic and thermal perceptron training algorithms. </title> <type> Manuscript, </type> <year> 1997. </year>
Reference-contexts: Specifically, at the beginning of the c th cycle we set t = flt 0 with fl = 1 (c=C). See <ref> [2, 3] </ref> for more details. Since t is decreased to 0, the updates clearly terminate after C cycles.
Reference: [4] <author> E. Amaldi and V. Kann. </author> <title> The complexity and approximability of finding maximum feasible subsystems of linear relations. </title> <journal> Theoretical Computer Science, </journal> <volume> 147 </volume> <pages> 181-210, </pages> <year> 1995. </year> <month> 20 </month>
Reference-contexts: The first one pertains to the computational complexity of training single linear threshold units. The problems of maximizing the number of correct classifications or minimizing the number of errors are not only NP-hard to solve optimally but also to approximate within some constant factors <ref> [1, 4, 6] </ref>. These problems becomes even harder when all input patterns with one of the two desired outputs must be correctly classified, like in the methods proposed in [33, 32, 15]. The second limitation is inherent to the greedy approach. <p> Since it is N P-hard to find such an optimal weight vector [1] and even an approximate one that is guaranteed to be a fixed percentage away from the optimum <ref> [4, 6, 26, 5] </ref>, we have to settle for an efficient heuristic with a good average-case behavior. In this work we use the thermal perceptron procedure [21] to find a close-to-optimal weight vector for each single unit.
Reference: [5] <author> E. Amaldi and V. Kann. </author> <title> On the approximability of minimizing nonzero variables and unsatisfied relations in linear systems. </title> <booktitle> Theoretical Computer Science, </booktitle> <year> 1997. </year> <note> To appear. Preliminary version available as ECCC Technical Report 96-15. </note>
Reference-contexts: In <ref> [29, 43, 5] </ref> the problem of designing close-to-minimum size networks in terms of nonzero weights is investigated for single perceptrons, i.e., linear threshold units. <p> For instance, it is NP-hard to find for any task that can be performed by a single perceptron a set of weight values which correctly classifies all input vectors and whose number of nonzero weights exceeds the minimum one by any given constant factor <ref> [43, 5] </ref>. In fact, a stronger nonapproximability bound was established in [5] under a slightly stronger assumption than P 6= NP. These results, which derive from a worst-case analysis, compel us to devise efficient heuristics with good average-case behavior. <p> In fact, a stronger nonapproximability bound was established in <ref> [5] </ref> under a slightly stronger assumption than P 6= NP. These results, which derive from a worst-case analysis, compel us to devise efficient heuristics with good average-case behavior. As is well-known, constructive algorithms have several advantages with respect to methods for training networks with a fixed architecture. <p> Since it is N P-hard to find such an optimal weight vector [1] and even an approximate one that is guaranteed to be a fixed percentage away from the optimum <ref> [4, 6, 26, 5] </ref>, we have to settle for an efficient heuristic with a good average-case behavior. In this work we use the thermal perceptron procedure [21] to find a close-to-optimal weight vector for each single unit.
Reference: [6] <author> S. Arora, L. Babai, J. Stern, and Z. Sweedyk. </author> <title> The hardness of approximate optima in lattices, codes, and systems of linear equations. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 54 </volume> <pages> 317-331, </pages> <year> 1997. </year>
Reference-contexts: The first one pertains to the computational complexity of training single linear threshold units. The problems of maximizing the number of correct classifications or minimizing the number of errors are not only NP-hard to solve optimally but also to approximate within some constant factors <ref> [1, 4, 6] </ref>. These problems becomes even harder when all input patterns with one of the two desired outputs must be correctly classified, like in the methods proposed in [33, 32, 15]. The second limitation is inherent to the greedy approach. <p> Since it is N P-hard to find such an optimal weight vector [1] and even an approximate one that is guaranteed to be a fixed percentage away from the optimum <ref> [4, 6, 26, 5] </ref>, we have to settle for an efficient heuristic with a good average-case behavior. In this work we use the thermal perceptron procedure [21] to find a close-to-optimal weight vector for each single unit.
Reference: [7] <author> E. B. Baum and D. Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1 </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference-contexts: According to Occam's principle, among all networks capable of performing a given task, more compact ones |the ones with the smallest number of free parameters| are more likely to exhibit good generalization (see for instance <ref> [7, 16] </ref>). Therefore considerable attention has been devoted to constructive algorithms that build compact networks in terms of number of weights or of units.
Reference: [8] <author> K. Bennett and J. </author> <title> Blue. Optimal decision trees. </title> <type> Technical Report No 214, </type> <institution> Department of Mathematical Sciences, Rensselaer Polytechnic Institute, </institution> <address> Troy, New York, </address> <year> 1996. </year>
Reference-contexts: The emphasis is on the size of the resulting networks and on their generalization properties. Finally, some concluding remarks and extensions are mentioned in Section 6. 2 Preliminaries 2.1 Global strategies In spite of their particularities, most constructive methods are based on a greedy strategy (see <ref> [9, 8] </ref> for a non-greedy but computationally intensive approach to build decision trees). The global design problem is subdivided into a sequence of local training problems concerning single units.
Reference: [9] <author> K. Bennett and E. J. Bredensteiner. </author> <title> Global tree optimization: A non-greedy decision tree algorithm. </title> <journal> Computing Science and Statistics, </journal> <volume> 26 </volume> <pages> 156-160, </pages> <year> 1994. </year>
Reference-contexts: The emphasis is on the size of the resulting networks and on their generalization properties. Finally, some concluding remarks and extensions are mentioned in Section 6. 2 Preliminaries 2.1 Global strategies In spite of their particularities, most constructive methods are based on a greedy strategy (see <ref> [9, 8] </ref> for a non-greedy but computationally intensive approach to build decision trees). The global design problem is subdivided into a sequence of local training problems concerning single units.
Reference: [10] <author> K. Binder. </author> <title> Monte Carlo methods in statistical Physics. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1979. </year> <booktitle> Topics in Current Physics 7. </booktitle>
Reference-contexts: The results obtained for the 3-or-more clumps problem with n = 25 are reported in Figures 9 and 10 as well as in Table 1. The tasks were generated through a stochastic procedure <ref> [10] </ref> ensuring that the average number of clumps is equal to 2:5. Thus approximately half of the examples are positive and half are negative. In order to evaluate the generalization abilities in terms of the available information, tasks of various sizes are considered.
Reference: [11] <author> A. Blum and R. Rivest. </author> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 117-127, </pages> <year> 1992. </year>
Reference-contexts: Designing a minimum-size network is of course at least as hard as the problem of finding an appropriate set of values for the weights of a given architecture, which is NP-complete <ref> [27, 11] </ref>. However, in practice minimum networks are not required and near-optimum ones that exhibit good generalization properties suffice. The central question is how close to the optimum one should and one can get.
Reference: [12] <author> N. Burgess. </author> <title> A constructive algorithm that converges for real-valued input patterns. </title> <journal> International Journal of Neural Systems, </journal> <volume> 5 </volume> <pages> 59-66, </pages> <year> 1995. </year>
Reference-contexts: Although all algorithms will be described for binary tasks, i.e., a k 2 f0; 1g n , they can be easily extended to deal with real-valued inputs using the stereographic projection as a preprocessing technique [40], see Section 5 and <ref> [12] </ref>. 2.2 Training single units Consider a single linear threshold unit with a weight vector w 2 R n and a threshold w 0 2 R.
Reference: [13] <author> N. Burgess, M. N. Granieri, and S. Patarnello. </author> <title> 3-D object classification: Application of a constructive algorithm. </title> <journal> International Journal of Neural Systems, </journal> <volume> 2 </volume> <pages> 275-282, </pages> <year> 1991. </year>
Reference-contexts: This makes them attractive from a hardware implementation point of view. However, the units cannot be trained in parallel since they are allocated incrementally. Although methods for constructing networks of linear threshold units have been applied so far to several practical problems (see for instance <ref> [41, 13, 30, 32, 15] </ref>), algorithms that yield more compact networks with better generalization properties are needed to deal with larger applications. The paper is organized as follows. <p> In other ones, such as Tiling [36], new units and layers are created downstream of the existing ones so that 2 the network is extended outward. The first category includes methods for neural trees [23, 41] or for cascade architectures <ref> [13] </ref> as well as sequential methods that focus alternatively on input patterns with a certain type of desired output [33, 15]. Procedures that construct networks with a single hidden layer and a predefined output function belong to the same category [19].
Reference: [14] <author> C. Campbell. </author> <title> Constructive learning techniques for designing neural network systems. </title> <editor> In C. T. Leondes, editor, </editor> <title> Neural network systems, techniques and applications. </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: 1 Introduction Since the beginning of the 90's, methods for constructing feedforward networks of threshold units have attracted a considerable interest (see for instance <ref> [14] </ref> and the included references). Constructive algorithms do not only tune the weights of the network but also vary its topology, in other words, they design rather than just train the network.
Reference: [15] <author> C. Campbell and C. Perez Vicente. </author> <title> The target switch algorithm: a constructive learning procedure for feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 1245-1264, </pages> <year> 1995. </year>
Reference-contexts: This makes them attractive from a hardware implementation point of view. However, the units cannot be trained in parallel since they are allocated incrementally. Although methods for constructing networks of linear threshold units have been applied so far to several practical problems (see for instance <ref> [41, 13, 30, 32, 15] </ref>), algorithms that yield more compact networks with better generalization properties are needed to deal with larger applications. The paper is organized as follows. <p> The first category includes methods for neural trees [23, 41] or for cascade architectures [13] as well as sequential methods that focus alternatively on input patterns with a certain type of desired output <ref> [33, 15] </ref>. Procedures that construct networks with a single hidden layer and a predefined output function belong to the same category [19]. <p> Most of the above-mentioned methods yield networks with more than two layers of threshold units, but some actually generate a single hidden layer <ref> [33, 15] </ref>. Constructive algorithms that solve a sequence of Linear Programs (LP) have also been proposed [39, 32, 30]. While the one in [32] is restricted to simple halfspace intersections and neural decision lists, the Multisurface method [30] can deal with any type of task with binary outputs. <p> These problems becomes even harder when all input patterns with one of the two desired outputs must be correctly classified, like in the methods proposed in <ref> [33, 32, 15] </ref>. The second limitation is inherent to the greedy approach. Breaking down the network design problem into that of training a sequence of single units is certainly attractive.
Reference: [16] <author> D. Cohn and G. Tesauro. </author> <title> How tight are the Vapnik-Chervonenkis bounds? Neural Computation, </title> <booktitle> 4 </booktitle> <pages> 249-269, </pages> <year> 1992. </year>
Reference-contexts: According to Occam's principle, among all networks capable of performing a given task, more compact ones |the ones with the smallest number of free parameters| are more likely to exhibit good generalization (see for instance <ref> [7, 16] </ref>). Therefore considerable attention has been devoted to constructive algorithms that build compact networks in terms of number of weights or of units.
Reference: [17] <author> J. Denker, D. Schwartz, B. Wittner, S. Solla, R. Howard, L. Jackel, and J. </author> <title> Hopfield. Large automatic learning, rule extraction, and generalization. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 877-922, </pages> <year> 1987. </year>
Reference-contexts: The central question is how close to the optimum one should and one can get. On one hand, any task with binary outputs can be performed by a a three layer network with a large enough number of hidden units <ref> [17, 45, 46] </ref>. <p> We also compare the refined version of the Shift algorithm with the basic version. 5.2 Generalization The generalization properties of the constructed networks have been tested on two structured problems. The first one is the classical K-or-more clumps problem that is frequently used as a benchmark <ref> [17] </ref>.
Reference: [18] <author> S. E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <type> Technical Report CMU-CS-90-100, </type> <institution> Carnegie Mellon University, </institution> <year> 1990. </year>
Reference-contexts: This is achieved at the simple cost of adding an input dimension. In [40] the stereographic preprocessing has been shown to work well for the famous benchmark problem that consists in separating two interlocking spirals <ref> [18] </ref>. This type of task has a circle-like structure and is well-suited to such a transformation. The question arises as to whether, using such a preprocessing, Shift can perform well on real-world data with real-valued inputs like in the breast cancer database.
Reference: [19] <author> M. Frean. </author> <title> Small Nets and Short Paths. </title> <type> PhD thesis, </type> <institution> Department of Physics, University of Edinburgh, Edinburgh, </institution> <address> Scotland, </address> <year> 1990. </year>
Reference-contexts: Procedures that construct networks with a single hidden layer and a predefined output function belong to the same category <ref> [19] </ref>. The Offset algorithm proposed in [38] and independently in [24] generates networks whose output unit computes a parity function, which can be itself implemented by a two-layer network of threshold units. <p> In this work we use the thermal perceptron procedure [21] to find a close-to-optimal weight vector for each single unit. Although other methods could be used, this one compares favorably with the pocket procedure [22] and with some other methods based on least mean squared error or cross-entropy minimization <ref> [19, 21, 2] </ref>. The basic idea is to pay decreasing attention to misclassified input vectors with large total inputs.
Reference: [20] <author> M. Frean. </author> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209, </pages> <year> 1990. </year>
Reference-contexts: Clearly, the size of the resulting network depends on the global strategy as well as on the effectiveness of the method used locally. Strategies for constructing feedforward networks of threshold units can be subdivided into two major categories. In some algorithms, such as Upstart <ref> [20] </ref>, the first unit is the future output unit and new hidden units are inserted between the input and output layers. In other ones, such as Tiling [36], new units and layers are created downstream of the existing ones so that 2 the network is extended outward. <p> Since t is decreased to 0, the updates clearly terminate after C cycles. Finally, we recall that for any task with binary inputs it is always possible to correctly classify one input vector with a given target and all those with the other target <ref> [20, 33] </ref>. <p> If T 0 is nonlinearly separable, new units are allocated in a single hidden layer until the network correctly classifies all input vectors a k . As in the Upstart <ref> [20] </ref>, we distinguish between two types of misclassications: either the output is wrongly-on, i.e., the output unit is on while the target is b k = 0, or it is wrongly-off, i.e., the output unit is off while b k = 1. <p> Note that this solution has nearly half as many hidden units as the one mentioned in <ref> [20] </ref>. To the best of our knowledge, this very compact solution has not yet been discovered. 6 3.3 Convergence Theorem 3.1 The Shift algorithm constructs a network consistent with any binary task T . <p> See [35] for an to feedforward networks with binary weights. 5 Experimental results Three test problems have been selected in order to compare the performances of the Shift and PTI algorithms to three alternative methods, namely the Upstart <ref> [20] </ref> the Tiling [36] and the Offset [24, 34], which derive from similar strategies. Therefore two important criteria are considered: network size and generalization abilities. Experiments have also been carried out on a real-world problem with real-valued inputs. <p> The Shift algorithm turns out to build networks containing less hidden units and less weights than those produced by Upstart. Moreover, they have a single hidden layer instead of a tree-like topology with a very large number of layers. Not surprisingly, the Upstart variant <ref> [20] </ref> which allows us to contract the tree in a single hidden layer performs sensibly worse than the original method, see Figure 8. <p> Although the case K = 2 has sometimes been considered <ref> [20] </ref>, we take K = 3. Indeed, for K = 2 there are only n (n 1) + 1 possible input vectors with at most 1 clump. If n = 25, like in [20], considering tasks with p = 600 examples of which half of them are negative amounts to select <p> Although the case K = 2 has sometimes been considered <ref> [20] </ref>, we take K = 3. Indeed, for K = 2 there are only n (n 1) + 1 possible input vectors with at most 1 clump. If n = 25, like in [20], considering tasks with p = 600 examples of which half of them are negative amounts to select half of all possible negative examples.
Reference: [21] <author> M. Frean. </author> <title> A "thermal" perceptron learning rule. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 946-957, </pages> <year> 1992. </year> <month> 21 </month>
Reference-contexts: In this work we use the thermal perceptron procedure <ref> [21] </ref> to find a close-to-optimal weight vector for each single unit. Although other methods could be used, this one compares favorably with the pocket procedure [22] and with some other methods based on least mean squared error or cross-entropy minimization [19, 21, 2]. <p> In this work we use the thermal perceptron procedure [21] to find a close-to-optimal weight vector for each single unit. Although other methods could be used, this one compares favorably with the pocket procedure [22] and with some other methods based on least mean squared error or cross-entropy minimization <ref> [19, 21, 2] </ref>. The basic idea is to pay decreasing attention to misclassified input vectors with large total inputs.
Reference: [22] <author> S. I. Gallant. </author> <title> Perceptron-based learning algorithms. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1 </volume> <pages> 179-191, </pages> <year> 1990. </year>
Reference-contexts: In this work we use the thermal perceptron procedure [21] to find a close-to-optimal weight vector for each single unit. Although other methods could be used, this one compares favorably with the pocket procedure <ref> [22] </ref> and with some other methods based on least mean squared error or cross-entropy minimization [19, 21, 2]. The basic idea is to pay decreasing attention to misclassified input vectors with large total inputs.
Reference: [23] <author> M. Golea and M. Marchand. </author> <title> A growth algorithm for neural network decision trees. </title> <journal> Euro-physics Letters, </journal> <volume> 12 </volume> <pages> 205-210, </pages> <year> 1990. </year>
Reference-contexts: In other ones, such as Tiling [36], new units and layers are created downstream of the existing ones so that 2 the network is extended outward. The first category includes methods for neural trees <ref> [23, 41] </ref> or for cascade architectures [13] as well as sequential methods that focus alternatively on input patterns with a certain type of desired output [33, 15]. Procedures that construct networks with a single hidden layer and a predefined output function belong to the same category [19].
Reference: [24] <author> B. </author> <month> Guenin. </month> <institution> Etude d'algorithmes d'apprentissage dans les reseaux de neurones en couches. Diploma Project, Swiss Federal Institute of Technology, Department of Mathematics, Lau-sanne, </institution> <year> 1991. </year>
Reference-contexts: Procedures that construct networks with a single hidden layer and a predefined output function belong to the same category [19]. The Offset algorithm proposed in [38] and independently in <ref> [24] </ref> generates networks whose output unit computes a parity function, which can be itself implemented by a two-layer network of threshold units. Most of the above-mentioned methods yield networks with more than two layers of threshold units, but some actually generate a single hidden layer [33, 15]. <p> See [35] for an to feedforward networks with binary weights. 5 Experimental results Three test problems have been selected in order to compare the performances of the Shift and PTI algorithms to three alternative methods, namely the Upstart [20] the Tiling [36] and the Offset <ref> [24, 34] </ref>, which derive from similar strategies. Therefore two important criteria are considered: network size and generalization abilities. Experiments have also been carried out on a real-world problem with real-valued inputs.
Reference: [25] <author> A. Hajnal, W. Maass, P. Pudlak, M. Szegedy, and G. Turan. </author> <title> Threshold circuits of bounded depth. </title> <booktitle> In Proc. of 28th Ann. IEEE Symp. on Foundations of Comput. Sci., </booktitle> <pages> pages 99-110, </pages> <year> 1987. </year>
Reference-contexts: For threshold units with polynomially bounded weights, there exists even a class of functions that can be computed by a three-layer network with a polynomial number of units but that require an exponential number of units if only a single hidden layer is available <ref> [25] </ref>. The question of whether similar functions do also exist for unbounded weights is an important open question in circuit complexity. 4.1 Description Consider a task T 0 and a network with more than l layers.
Reference: [26] <author> K-U. Hoffgen, H-U. Simon, and K. van Horn. </author> <title> Robust trainability of single neurons. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50 </volume> <pages> 114-125, </pages> <year> 1995. </year>
Reference-contexts: Since it is N P-hard to find such an optimal weight vector [1] and even an approximate one that is guaranteed to be a fixed percentage away from the optimum <ref> [4, 6, 26, 5] </ref>, we have to settle for an efficient heuristic with a good average-case behavior. In this work we use the thermal perceptron procedure [21] to find a close-to-optimal weight vector for each single unit.
Reference: [27] <author> S. J. Judd. </author> <title> Neural network design and the complexity of learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Designing a minimum-size network is of course at least as hard as the problem of finding an appropriate set of values for the weights of a given architecture, which is NP-complete <ref> [27, 11] </ref>. However, in practice minimum networks are not required and near-optimum ones that exhibit good generalization properties suffice. The central question is how close to the optimum one should and one can get.
Reference: [28] <author> M. Kearns and L. Valiant. </author> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <journal> Journal of ACM, </journal> <volume> 41 </volume> <pages> 67-95, </pages> <year> 1994. </year>
Reference-contexts: In <ref> [28] </ref> it is shown that under a cryptographic assumption (namely, if trapdoor functions exist) no polynomial time algorithm can find a feedforward network with a bounded number of layers that performs a given task and that is at most polynomially larger than the minimum possible one, where the size is measured
Reference: [29] <author> J.-H. Lin and J. S. Vitter. </author> <title> Complexity results on learning by neural nets. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 211-230, </pages> <year> 1991. </year>
Reference-contexts: In <ref> [29, 43, 5] </ref> the problem of designing close-to-minimum size networks in terms of nonzero weights is investigated for single perceptrons, i.e., linear threshold units.
Reference: [30] <author> O. L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5 </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: This makes them attractive from a hardware implementation point of view. However, the units cannot be trained in parallel since they are allocated incrementally. Although methods for constructing networks of linear threshold units have been applied so far to several practical problems (see for instance <ref> [41, 13, 30, 32, 15] </ref>), algorithms that yield more compact networks with better generalization properties are needed to deal with larger applications. The paper is organized as follows. <p> Most of the above-mentioned methods yield networks with more than two layers of threshold units, but some actually generate a single hidden layer [33, 15]. Constructive algorithms that solve a sequence of Linear Programs (LP) have also been proposed <ref> [39, 32, 30] </ref>. While the one in [32] is restricted to simple halfspace intersections and neural decision lists, the Multisurface method [30] can deal with any type of task with binary outputs. <p> Constructive algorithms that solve a sequence of Linear Programs (LP) have also been proposed [39, 32, 30]. While the one in [32] is restricted to simple halfspace intersections and neural decision lists, the Multisurface method <ref> [30] </ref> can deal with any type of task with binary outputs. In this paper we focus on strategies in which simple perceptron-like procedures are used to train single linear threshold units. Unlike the LP-based methods that require complex optimization solvers, such algorithms are reasonable candidates for hardware implementation. <p> The nine attributes correspond to nine cellular measurements made microscopically by a surgical oncologist. Measurements are coded as integer values between 1 and 10. The purpose is to separate benign cases from malignant ones. This data set has been used to demonstrate the applicability of the Multisurface method <ref> [30] </ref> and the Linear Programming-trained neural network has been used at the University of Wisconsin Hospitals. To deal with arbitrary task with real-valued inputs a simple preprocessing suffices. <p> When greedy constructive strategies are considered, it is not necessary and even desirable to train each unit optimally. So, sophisticated techniques like Linear Programming, which minimize an objective function that is a surrogate for the number of errors such as in <ref> [30] </ref>, are not required and efficient perceptron variants such as the thermal one are appropriate. This is good news from the hardware implementation point of view.
Reference: [31] <author> O. L. Mangasarian, R. Setiono, and W. H. Wolberg. </author> <title> Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Large-sacle numerical optimization, </booktitle> <pages> pages 22-30. </pages> <publisher> SIAM Publications, </publisher> <year> 1990. </year>
Reference-contexts: For 500 cycles, the resulting networks contain at most 2 additional units. In all our experiments the number of hidden units was never greater than 7. For the sake of comparison, the Multisurface method trained on the first 369 examples generated a network with 7 hidden units <ref> [31] </ref>. The networks built by Shift are thus more compact in terms of units. Nevertheless, they have an additional input unit (due to the stereographic preprocessing) and direct input-to-output connections. In this kind of application, the network ability to correctly classify new cases is of course a major concern. In [31] <p> <ref> [31] </ref>. The networks built by Shift are thus more compact in terms of units. Nevertheless, they have an additional input unit (due to the stereographic preprocessing) and direct input-to-output connections. In this kind of application, the network ability to correctly classify new cases is of course a major concern. In [31] the authors report that the network trained with the Multisurface method on the first 369 examples correctly classified the following 45 cases.
Reference: [32] <author> M. Marchand and M. Golea. </author> <title> On learning simple neural concepts: from halfspace intersections to neural decision lists. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 4 </volume> <pages> 67-85, </pages> <year> 1993. </year>
Reference-contexts: This makes them attractive from a hardware implementation point of view. However, the units cannot be trained in parallel since they are allocated incrementally. Although methods for constructing networks of linear threshold units have been applied so far to several practical problems (see for instance <ref> [41, 13, 30, 32, 15] </ref>), algorithms that yield more compact networks with better generalization properties are needed to deal with larger applications. The paper is organized as follows. <p> Most of the above-mentioned methods yield networks with more than two layers of threshold units, but some actually generate a single hidden layer [33, 15]. Constructive algorithms that solve a sequence of Linear Programs (LP) have also been proposed <ref> [39, 32, 30] </ref>. While the one in [32] is restricted to simple halfspace intersections and neural decision lists, the Multisurface method [30] can deal with any type of task with binary outputs. <p> Most of the above-mentioned methods yield networks with more than two layers of threshold units, but some actually generate a single hidden layer [33, 15]. Constructive algorithms that solve a sequence of Linear Programs (LP) have also been proposed [39, 32, 30]. While the one in <ref> [32] </ref> is restricted to simple halfspace intersections and neural decision lists, the Multisurface method [30] can deal with any type of task with binary outputs. In this paper we focus on strategies in which simple perceptron-like procedures are used to train single linear threshold units. <p> These problems becomes even harder when all input patterns with one of the two desired outputs must be correctly classified, like in the methods proposed in <ref> [33, 32, 15] </ref>. The second limitation is inherent to the greedy approach. Breaking down the network design problem into that of training a sequence of single units is certainly attractive.
Reference: [33] <author> M. Marchand, M. Golea, and P. Rujan. </author> <title> A convergence theorem for sequential learning in two-layer perceptrons. </title> <journal> Europhysics Letters, </journal> <volume> 11 </volume> <pages> 487-492, </pages> <year> 1990. </year>
Reference-contexts: The first category includes methods for neural trees [23, 41] or for cascade architectures [13] as well as sequential methods that focus alternatively on input patterns with a certain type of desired output <ref> [33, 15] </ref>. Procedures that construct networks with a single hidden layer and a predefined output function belong to the same category [19]. <p> Most of the above-mentioned methods yield networks with more than two layers of threshold units, but some actually generate a single hidden layer <ref> [33, 15] </ref>. Constructive algorithms that solve a sequence of Linear Programs (LP) have also been proposed [39, 32, 30]. While the one in [32] is restricted to simple halfspace intersections and neural decision lists, the Multisurface method [30] can deal with any type of task with binary outputs. <p> These problems becomes even harder when all input patterns with one of the two desired outputs must be correctly classified, like in the methods proposed in <ref> [33, 32, 15] </ref>. The second limitation is inherent to the greedy approach. Breaking down the network design problem into that of training a sequence of single units is certainly attractive. <p> Since t is decreased to 0, the updates clearly terminate after C cycles. Finally, we recall that for any task with binary inputs it is always possible to correctly classify one input vector with a given target and all those with the other target <ref> [20, 33] </ref>.
Reference: [34] <author> D. Martinez and D. Esteve. </author> <title> The offset algorithm: Building and learning method for multilayer neural networks. </title> <journal> Europhysics Letters, </journal> <volume> 18 </volume> <pages> 95-100, </pages> <year> 1992. </year>
Reference-contexts: See [35] for an to feedforward networks with binary weights. 5 Experimental results Three test problems have been selected in order to compare the performances of the Shift and PTI algorithms to three alternative methods, namely the Upstart [20] the Tiling [36] and the Offset <ref> [24, 34] </ref>, which derive from similar strategies. Therefore two important criteria are considered: network size and generalization abilities. Experiments have also been carried out on a real-world problem with real-valued inputs.
Reference: [35] <author> E. Mayoraz and F. Aviolat. </author> <title> Constructive training methods for feedforward neural networks with binary units. </title> <journal> International Journal of Neural Systems, </journal> <volume> 7 </volume> <pages> 149-166, </pages> <year> 1996. </year>
Reference-contexts: Finally, note that although the focus in this paper is on constructive algorithms using simple perceptron-like training procedures for single units, it is possible to allocate additional units to break down unfaithful classes in a more natural way at the expense of using more complex local procedures. See <ref> [35] </ref> for an to feedforward networks with binary weights. 5 Experimental results Three test problems have been selected in order to compare the performances of the Shift and PTI algorithms to three alternative methods, namely the Upstart [20] the Tiling [36] and the Offset [24, 34], which derive from similar strategies. <p> In spite of the poor generalization performances of PTI networks, algorithms that generate an arbitrary number of hidden layers are worth pursuing because some functions may have a much more compact representation using a larger number of hidden layers, see Section 4. It is worth noting that in <ref> [35] </ref> the basic idea of the PTI is extended to the deal with feedforward networks with binary weights and the K-similarity problem is considered as a benchmark. Finally, the two algorithms we have presented can be extended to construct networks with multiple outputs.
Reference: [36] <author> M. Mezard and J.-P. Nadal. </author> <title> Learning in feedforward layered networks: the tiling algorithm. </title> <journal> Journal of Physics A: Math. Gen., </journal> <volume> 22 </volume> <pages> 2191-2203, </pages> <year> 1989. </year>
Reference-contexts: Strategies for constructing feedforward networks of threshold units can be subdivided into two major categories. In some algorithms, such as Upstart [20], the first unit is the future output unit and new hidden units are inserted between the input and output layers. In other ones, such as Tiling <ref> [36] </ref>, new units and layers are created downstream of the existing ones so that 2 the network is extended outward. <p> see in Section 5, these differences lead to substantial improvements in terms of network size and of generalization abilities. 4 The PTI algorithm The second algorithm we propose is named PTI (Partial Target Inversion) and it constructs the network layer by layer in a way similar to the Tiling algorithm <ref> [36] </ref>. Algorithms that generate an arbitrary number of hidden layers are worth pursuing because some functions may have a much more compact representation using a larger number of hidden layers. <p> In a way similar to <ref> [36] </ref>, we establish the claim by showing that U l+1 makes a number of errors on T l+1 1 strictly smaller than the number of errors made by U l on T l 1 , see Figure 3. <p> See [35] for an to feedforward networks with binary weights. 5 Experimental results Three test problems have been selected in order to compare the performances of the Shift and PTI algorithms to three alternative methods, namely the Upstart [20] the Tiling <ref> [36] </ref> and the Offset [24, 34], which derive from similar strategies. Therefore two important criteria are considered: network size and generalization abilities. Experiments have also been carried out on a real-world problem with real-valued inputs.
Reference: [37] <author> M. L. Minsky and S. Papert. </author> <title> Perceptrons: An introduction to computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year> <note> Expanded edition. </note>
Reference-contexts: In such a case, we say that the unit performs the task T . For the sake of simplicity, the threshold w 0 is considered as an additional weight corresponding to an additional input that is always on. In the well-known perceptron algorithm <ref> [37] </ref>, the current weight vector is updated as follows w i+1 = w i + i (b k y k )a k ; where i 0 is the increment.
Reference: [38] <author> G. J. Mitchison and R. M. </author> <title> Durbin. Bounds on the learning capacity of some multi-layer networks. </title> <journal> Biological Cybernetics, </journal> <volume> 60 </volume> <pages> 345-356, </pages> <year> 1989. </year> <month> 22 </month>
Reference-contexts: Procedures that construct networks with a single hidden layer and a predefined output function belong to the same category [19]. The Offset algorithm proposed in <ref> [38] </ref> and independently in [24] generates networks whose output unit computes a parity function, which can be itself implemented by a two-layer network of threshold units.
Reference: [39] <author> A. Roy, L. Kim, and S. Mukhopadhyay. </author> <title> A polynomial time algorithm for the construction and training of a class of multi-layer perceptrons. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 535-546, </pages> <year> 1993. </year>
Reference-contexts: Most of the above-mentioned methods yield networks with more than two layers of threshold units, but some actually generate a single hidden layer [33, 15]. Constructive algorithms that solve a sequence of Linear Programs (LP) have also been proposed <ref> [39, 32, 30] </ref>. While the one in [32] is restricted to simple halfspace intersections and neural decision lists, the Multisurface method [30] can deal with any type of task with binary outputs.
Reference: [40] <author> J. Saffery and C. Thorton. </author> <title> Using the stereographic projection as a pre-processing technique for Upstart. </title> <booktitle> In Proc. International Joint Conference on Neural Networks of the IEEE, </booktitle> <volume> volume II, </volume> <pages> pages 441-446, </pages> <year> 1991. </year>
Reference-contexts: Although all algorithms will be described for binary tasks, i.e., a k 2 f0; 1g n , they can be easily extended to deal with real-valued inputs using the stereographic projection as a preprocessing technique <ref> [40] </ref>, see Section 5 and [12]. 2.2 Training single units Consider a single linear threshold unit with a weight vector w 2 R n and a threshold w 0 2 R. <p> Although this is not necessarily true for tasks with real-valued inputs, such tasks can be dealt with using an appropriate preprocessing such as the stereographic projection <ref> [40] </ref>, see Section 5. 3.4 Variants The algorithm described above can be further refined. Suppose that one is about to add a new hidden unit to correct wrongly-off errors. Let v min be the smallest total input v k for which the output unit is wrongly-off. <p> Although this is not necessarily true when real-valued inputs are considered, the problem can be circumvented by using the stereographic projection as a preprocessing technique <ref> [40] </ref>. Indeed, any given point on a hypersphere being linearly separable from any set of other points. This is achieved at the simple cost of adding an input dimension. In [40] the stereographic preprocessing has been shown to work well for the famous benchmark problem that consists in separating two interlocking <p> true when real-valued inputs are considered, the problem can be circumvented by using the stereographic projection as a preprocessing technique <ref> [40] </ref>. Indeed, any given point on a hypersphere being linearly separable from any set of other points. This is achieved at the simple cost of adding an input dimension. In [40] the stereographic preprocessing has been shown to work well for the famous benchmark problem that consists in separating two interlocking spirals [18]. This type of task has a circle-like structure and is well-suited to such a transformation.
Reference: [41] <author> J. A. Sirat and J.-P. Nadal. </author> <title> Neural trees: a new tool for classification. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 1 </volume> <pages> 423-438, </pages> <year> 1990. </year>
Reference-contexts: This makes them attractive from a hardware implementation point of view. However, the units cannot be trained in parallel since they are allocated incrementally. Although methods for constructing networks of linear threshold units have been applied so far to several practical problems (see for instance <ref> [41, 13, 30, 32, 15] </ref>), algorithms that yield more compact networks with better generalization properties are needed to deal with larger applications. The paper is organized as follows. <p> In other ones, such as Tiling [36], new units and layers are created downstream of the existing ones so that 2 the network is extended outward. The first category includes methods for neural trees <ref> [23, 41] </ref> or for cascade architectures [13] as well as sequential methods that focus alternatively on input patterns with a certain type of desired output [33, 15]. Procedures that construct networks with a single hidden layer and a predefined output function belong to the same category [19].
Reference: [42] <author> E. D. Sontag. </author> <title> Feedforward nets for interpolation and classification. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 42 </volume> <pages> 20-48, </pages> <year> 1992. </year>
Reference-contexts: It is worth noting that direct connections between the input units and the output unit can be eliminated. Depending on the threshold value, all initial errors will then be either wrongly-on or wrongly-off. Since allowing direct input-to-output connection sensibly increases the representation power <ref> [42] </ref>, simpler networks without direct connections might be preferred. Thus, there are three main differences between the Shift and the Upstart algorithms. First, a single hidden layer is constructed.
Reference: [43] <author> K. van Horn and T. Martinez. </author> <title> The minimum feature set problem. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7 </volume> <pages> 491-494, </pages> <year> 1994. </year>
Reference-contexts: In <ref> [29, 43, 5] </ref> the problem of designing close-to-minimum size networks in terms of nonzero weights is investigated for single perceptrons, i.e., linear threshold units. <p> For instance, it is NP-hard to find for any task that can be performed by a single perceptron a set of weight values which correctly classifies all input vectors and whose number of nonzero weights exceeds the minimum one by any given constant factor <ref> [43, 5] </ref>. In fact, a stronger nonapproximability bound was established in [5] under a slightly stronger assumption than P 6= NP. These results, which derive from a worst-case analysis, compel us to devise efficient heuristics with good average-case behavior.
Reference: [44] <author> W. H. Wolberg and O. L. Mangasarian. </author> <title> Multisurface method for surface separation for medical diagnosis applied to breast cytology. </title> <booktitle> Proceedings of the National Academy of Science, U.S.A., </booktitle> <volume> 87 </volume> <pages> 9193-9196, </pages> <year> 1990. </year>
Reference-contexts: Also in this case, the PTI algorithm compares very favorably to the Tiling. 5.3 Real-valued inputs To test the performance of Shift on a task with real-valued inputs, we consider the medical diagnosis application studied in <ref> [44] </ref>. The data set fl is relative to breast cancer; it consists of 699 examples from two different classes referring to malignant and benign cases. The nine attributes correspond to nine cellular measurements made microscopically by a surgical oncologist. Measurements are coded as integer values between 1 and 10.
Reference: [45] <author> P. J. Zwietering, E. H. L. Aarts, and J. Wessels. </author> <title> The design and complexity of exact multilayer perceptrons. </title> <journal> International Journal of Neural Systems, </journal> <volume> 2 </volume> <pages> 185-199, </pages> <year> 1991. </year>
Reference-contexts: The central question is how close to the optimum one should and one can get. On one hand, any task with binary outputs can be performed by a a three layer network with a large enough number of hidden units <ref> [17, 45, 46] </ref>.
Reference: [46] <author> P. J. Zwietering, E. H. L. Aarts, and J. Wessels. </author> <title> Exact classification with two-layered perceptrons. </title> <journal> International Journal of Neural Systems, </journal> <volume> 3 </volume> <pages> 143-156, </pages> <year> 1992. </year> <month> 23 </month>
Reference-contexts: The central question is how close to the optimum one should and one can get. On one hand, any task with binary outputs can be performed by a a three layer network with a large enough number of hidden units <ref> [17, 45, 46] </ref>.
References-found: 46

