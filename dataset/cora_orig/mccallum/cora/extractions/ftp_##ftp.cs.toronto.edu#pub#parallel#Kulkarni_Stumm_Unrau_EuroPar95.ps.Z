URL: ftp://ftp.cs.toronto.edu/pub/parallel/Kulkarni_Stumm_Unrau_EuroPar95.ps.Z
Refering-URL: http://www.cs.toronto.edu/~kulki/pubs_abs.html
Root-URL: 
Email: Email: kulki@cs.toronto.edu  
Title: Implementing Flexible Computation Rules with Subexpression-level Loop Transformations  
Author: Dattatraya Kulkarni*, Michael Stumm*, and Ronald C. Unrau** 
Keyword: **Parallel Compiler Development  
Address: Toronto, Toronto, Canada, M5S 1A4  Toronto, Canada, M3C 1V7  
Affiliation: *Department of Computer Science and Department of Electrical Computer Engineering University of  IBM Toronto Laboratory  
Note: Proceedings of the First International EURO-PAR Conference, August 1995, pages 327--338, LNCS 966.  
Abstract: Computation Decomposition and Alignment (CDA) is a new loop transformation framework that extends the linear loop transformation framework and the more recently proposed Computation Alignment frameworks by linearly transforming computations at the granularity of subexpressions. It can be applied to achieve a number of optimization objectives, including the removal of data alignment constraints, the elimination of ownership tests, the reduction of cache conflicts, and improvements in data access locality. In this paper we show how CDA can be used to effectively implement flexible computation rules with the objective of minimizing communication and, whenever possible, eliminating intrinsics that test whether computations need to be executed or not. We describe CDA, show how it can be used to implement flexible computation rules, and present an algorithm for deriving appropriate CDA transformations.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Conference Record of the 14th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 63-76, </pages> <address> Munich, West Germany, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: Instead of transforming the statements as written by the programmer, CDA first partitions the original statements into finer statements and then aligns the statements at this finer granularity. This creates 1 The origins of CA can be traced to loop alignment <ref> [1, 19] </ref>, which is a special case of CA. additional opportunities for optimization. In later sections we show how CDA can be used to implement flexible computation rules. A number of optimization techniques have been proposed that reduce communication by transforming data.
Reference: 2. <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 28, </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: XDP can thus be used to implement variations of owner-computes rule. In this context, dynamic data alignment [11] can be considered as a structured form of implicit data movement. Earlier approaches resorted to static solutions by deriving best data alignments [16] and distributions <ref> [2, 8] </ref> considering global constraints. Chatterjee et al. [5] developed algorithms that derive communication optimal flexible computation rules for a class of expressions.
Reference: 3. <author> V. Bala, J. Ferrante, and L. Carter. </author> <title> Explicit data placement (xdp): A methodology for explicit compile-time representation and optimization of data movement. </title> <booktitle> In Proceedings of the 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <volume> volume 28, </volume> <pages> pages 139-149, </pages> <address> San Diego, CA, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: In later sections we show how CDA can be used to implement flexible computation rules. A number of optimization techniques have been proposed that reduce communication by transforming data. Bala and Ferrante <ref> [3] </ref> proposed the insertion of XDP directives that explicitly move data to transfer ownerships. XDP can thus be used to implement variations of owner-computes rule. In this context, dynamic data alignment [11] can be considered as a structured form of implicit data movement.
Reference: 4. <author> Utpal Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Linear loop transformation is a general technique developed in 1990 that changes the execution order of the iterations <ref> [4, 15, 17, 23] </ref> and can be used, for example, to reduce communication overhead by moving communications to the outer loop levels [17, 23]. However, linear loop transformations are limited in their optimization capabilities, since they leave iterations unchanged as they map the original iteration space onto a new one.
Reference: 5. <author> S. Chatterjee, J.R. Gilbert, , R. Schreiber, and S. Teng. </author> <title> Optimal evaluation of array expressions on massively parallel machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(1) </volume> <pages> 123-156, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: In contrast, flexible computation rules take into account the location of all the data needed for the computation and the cost of communication when deciding where a computation is to be executed <ref> [5] </ref>. The granularity of the computation being mapped is usually at the subexpression level. It is therefore possible to achieve optimal or near optimal computation mappings. However, the code generation is much more complex. Flexible computation rules are also important on shared memory multiprocessors. <p> In this context, dynamic data alignment [11] can be considered as a structured form of implicit data movement. Earlier approaches resorted to static solutions by deriving best data alignments [16] and distributions [2, 8] considering global constraints. Chatterjee et al. <ref> [5] </ref> developed algorithms that derive communication optimal flexible computation rules for a class of expressions. They take the machine topology into account to find optimal mappings of subexpressions onto processors in polynomial time. 3 P-Computes Rules The P-Computes operator, , can be used to specify flexible computation rules. <p> We believe that it is possible to produce near optimal computation rules by first deriving CDA transformations that minimize the number of distinct references, and then employing existing algorithms that derive flexible computation rules <ref> [5] </ref>. For example, previous work by Chatterjee et al. shows that subexpressions can be optimally mapped to processors in polynomial time [5]. However, their results have to be extended if CDA transformations are taken into account. <p> possible to produce near optimal computation rules by first deriving CDA transformations that minimize the number of distinct references, and then employing existing algorithms that derive flexible computation rules <ref> [5] </ref>. For example, previous work by Chatterjee et al. shows that subexpressions can be optimally mapped to processors in polynomial time [5]. However, their results have to be extended if CDA transformations are taken into account.
Reference: 6. <author> P. Feautrier. </author> <title> Dataflow analysis of array and scalar references. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20, </volume> <year> 1991. </year>
Reference-contexts: We represent data flow constraints in the loop with dependence relations [20], and we keep the exact dependence information between each pair of read and write <ref> [6, 18] </ref>. Consider a read reference r in statement S r flow dependent on a write reference w of statement S w .
Reference: 7. <author> HPF Forum. </author> <title> HPF: High performance fortran language specification. </title> <type> Technical report, HPF Forum, </type> <year> 1993. </year>
Reference-contexts: 1 Introduction In a SPMD framework such as HPF <ref> [7] </ref>, data alignments and distributions are usually specified by the user or suggested by some automatic tool such as PARADIGM [8]. Given the data alignments and distributions, the compiler then maps computations to processors using a computation rule. <p> For example, the owner-computes rule is a fixed rule and is used almost exclusively. It maps a statement instance to the processor which owns the lhs (left hand side) data element of the statement <ref> [7] </ref>, even if it would be more efficient to compute the statement on another processor, and it always maps a statement instance in its entirety. Fixed computation rules provide a general schema for computation mapping and hence simplify code generation, especially the insertion of commu-nication code.
Reference: 8. <author> M. Gupta. </author> <title> Automatic data partitioning on distributed memory multicomputers. </title> <type> Technical report, </type> <institution> Dept of computer Science, University of Illinois at Urbana Cham-paign, </institution> <year> 1992. </year>
Reference-contexts: 1 Introduction In a SPMD framework such as HPF [7], data alignments and distributions are usually specified by the user or suggested by some automatic tool such as PARADIGM <ref> [8] </ref>. Given the data alignments and distributions, the compiler then maps computations to processors using a computation rule. The choice of computation rule can have a significant impact on performance, since it affects the amount of communication generated and the number of intrinsics needed in the code. <p> XDP can thus be used to implement variations of owner-computes rule. In this context, dynamic data alignment [11] can be considered as a structured form of implicit data movement. Earlier approaches resorted to static solutions by deriving best data alignments [16] and distributions <ref> [2, 8] </ref> considering global constraints. Chatterjee et al. [5] developed algorithms that derive communication optimal flexible computation rules for a class of expressions.
Reference: 9. <author> W. Kelly and W. Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report UMIACS-TR-92-126, </type> <institution> University of Maryland, </institution> <year> 1992. </year>
Reference-contexts: However, linear loop transformations are limited in their optimization capabilities, since they leave iterations unchanged as they map the original iteration space onto a new one. Over the last three years, Computation Alignment (CA) frameworks have been proposed that extend the capabilities of linear transformations <ref> [9, 12, 21] </ref> by transforming loops at the granularity of statements. 1 By applying a separate transformation to each statement in the loop body they change the execution order of each statement, thus effectively changing the constitution of the iterations. <p> Guard-free code is usually desirable for better performance, but a perfect loop may be desirable in some cases, for instance to avoid non-vector communications or to avoid loop overheads. Algorithms to generate code employing both strategies can be found in the literature <ref> [9, 10, 12, 21, 22] </ref>. 4.3 Aligning the Temporary Arrays In the third stage, each temporary array is data aligned to the array used by the operator to specify the processor onto which the computations are to be mapped.
Reference: 10. <author> W. Kelly, W. Pugh, and E. Rosser. </author> <title> Code generation for multiple mappings. </title> <type> Technical Report UMIACS-TR-94-87, </type> <institution> University of Maryland, </institution> <year> 1994. </year>
Reference-contexts: Guard-free code is usually desirable for better performance, but a perfect loop may be desirable in some cases, for instance to avoid non-vector communications or to avoid loop overheads. Algorithms to generate code employing both strategies can be found in the literature <ref> [9, 10, 12, 21, 22] </ref>. 4.3 Aligning the Temporary Arrays In the third stage, each temporary array is data aligned to the array used by the operator to specify the processor onto which the computations are to be mapped.
Reference: 11. <author> K. Knobe, J.D. Lucas, and W.J. Dally. </author> <title> Dynamic alignment on distributed memory systems. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for Parallel Computers, Vienna, </booktitle> <pages> pages 394-404, </pages> <year> 1992. </year>
Reference-contexts: A number of optimization techniques have been proposed that reduce communication by transforming data. Bala and Ferrante [3] proposed the insertion of XDP directives that explicitly move data to transfer ownerships. XDP can thus be used to implement variations of owner-computes rule. In this context, dynamic data alignment <ref> [11] </ref> can be considered as a structured form of implicit data movement. Earlier approaches resorted to static solutions by deriving best data alignments [16] and distributions [2, 8] considering global constraints. Chatterjee et al. [5] developed algorithms that derive communication optimal flexible computation rules for a class of expressions.
Reference: 12. <author> D. Kulkarni and M. Stumm. </author> <title> Computational alignment: A new, unified program transformation for local and global optimization. </title> <type> Technical Report CSRI-292, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: However, linear loop transformations are limited in their optimization capabilities, since they leave iterations unchanged as they map the original iteration space onto a new one. Over the last three years, Computation Alignment (CA) frameworks have been proposed that extend the capabilities of linear transformations <ref> [9, 12, 21] </ref> by transforming loops at the granularity of statements. 1 By applying a separate transformation to each statement in the loop body they change the execution order of each statement, thus effectively changing the constitution of the iterations. <p> CA transformations have been applied to improve SPMD code in a variety of ways <ref> [12, 22] </ref>. For example, CA may be used to align the statements in the loop body so that all lhs data elements accessed in an iteration are located on the same processor in the hope of eliminating the need for ownership tests. <p> We can now employ CA to separately transform each statement of the new loop body in attempting to eliminate the need for intrinsics <ref> [12, 22] </ref>. Intuitively, the mapping causes a relative movement of the statement instances across iterations. The idea is to move the computations so that those that are mapped to the same processor belong to the same iteration. <p> Guard-free code is usually desirable for better performance, but a perfect loop may be desirable in some cases, for instance to avoid non-vector communications or to avoid loop overheads. Algorithms to generate code employing both strategies can be found in the literature <ref> [9, 10, 12, 21, 22] </ref>. 4.3 Aligning the Temporary Arrays In the third stage, each temporary array is data aligned to the array used by the operator to specify the processor onto which the computations are to be mapped.
Reference: 13. <author> D. Kulkarni and M. Stumm. </author> <title> CDA loop transformations. </title> <booktitle> In Proceedings of Third workshop on languages, compilers and run-time systems for scalable computers, </booktitle> <address> Troy, NY, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: For example, it is important to minimize remote memory accesses on shared memory multiprocessors with non-uniform access times. Moreover, the location of computations can significantly affect cache locality and interference patterns. In this paper, we show how the recently proposed CDA loop transformation framework <ref> [13, 14] </ref> can be used to efficiently implement a flexible computation rule called P-Computes. Section 2 briefly reviews the most important related work. Section 3 introduces the P-Computes rule. <p> For example, CA may be used to align the statements in the loop body so that all lhs data elements accessed in an iteration are located on the same processor in the hope of eliminating the need for ownership tests. Computation Decomposition and Alignment (CDA) <ref> [13] </ref> is a generalization of CA and goes a step further in that it can transform computations of granularity smaller than a statement. <p> The first two stages correspond to CDA, as described in <ref> [13] </ref>. In the first stage, the statements in the loop are decomposed so that statements can be assigned to processors in their entirety. This may require the introduction of new temporary arrays. <p> CDA is a general subexpression-level transformation framework which we applied here only for SPMD code optimization. CDA can be used in several other optimization contexts, for example to remove data alignment constraints, improve locality, eliminate cache conflicts, or reduce register pressure <ref> [13, 14] </ref>. Our current work includes the development of efficient algorithms that derive P-Computes rules and an analysis of their complexity. We are also working on more efficient algorithms to eliminate intrinsics that take intermediate alignments into account while constructing partial alignments.
Reference: 14. <author> D. Kulkarni, M. Stumm, R. Unrau, and W. Li. </author> <title> A generalized theory of linear loop transformations. </title> <type> Technical Report CSRI-317, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: For example, it is important to minimize remote memory accesses on shared memory multiprocessors with non-uniform access times. Moreover, the location of computations can significantly affect cache locality and interference patterns. In this paper, we show how the recently proposed CDA loop transformation framework <ref> [13, 14] </ref> can be used to efficiently implement a flexible computation rule called P-Computes. Section 2 briefly reviews the most important related work. Section 3 introduces the P-Computes rule. <p> CDA is a general subexpression-level transformation framework which we applied here only for SPMD code optimization. CDA can be used in several other optimization contexts, for example to remove data alignment constraints, improve locality, eliminate cache conflicts, or reduce register pressure <ref> [13, 14] </ref>. Our current work includes the development of efficient algorithms that derive P-Computes rules and an analysis of their complexity. We are also working on more efficient algorithms to eliminate intrinsics that take intermediate alignments into account while constructing partial alignments.
Reference: 15. <author> K.G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Deriving good transformations for mapping nested loops on hierarchical parallel machines in polynomial time. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Linear loop transformation is a general technique developed in 1990 that changes the execution order of the iterations <ref> [4, 15, 17, 23] </ref> and can be used, for example, to reduce communication overhead by moving communications to the outer loop levels [17, 23]. However, linear loop transformations are limited in their optimization capabilities, since they leave iterations unchanged as they map the original iteration space onto a new one.
Reference: 16. <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed memory machines. </title> <journal> Journal of parallel and distributed computing, </journal> <volume> 13 </volume> <pages> 213-221, </pages> <year> 1991. </year>
Reference-contexts: XDP can thus be used to implement variations of owner-computes rule. In this context, dynamic data alignment [11] can be considered as a structured form of implicit data movement. Earlier approaches resorted to static solutions by deriving best data alignments <ref> [16] </ref> and distributions [2, 8] considering global constraints. Chatterjee et al. [5] developed algorithms that derive communication optimal flexible computation rules for a class of expressions.
Reference: 17. <author> W. Li and K. Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In Proceedings of the Fifth Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Linear loop transformation is a general technique developed in 1990 that changes the execution order of the iterations <ref> [4, 15, 17, 23] </ref> and can be used, for example, to reduce communication overhead by moving communications to the outer loop levels [17, 23]. However, linear loop transformations are limited in their optimization capabilities, since they leave iterations unchanged as they map the original iteration space onto a new one. <p> Linear loop transformation is a general technique developed in 1990 that changes the execution order of the iterations [4, 15, 17, 23] and can be used, for example, to reduce communication overhead by moving communications to the outer loop levels <ref> [17, 23] </ref>. However, linear loop transformations are limited in their optimization capabilities, since they leave iterations unchanged as they map the original iteration space onto a new one.
Reference: 18. <author> D.E. Maydan, J.L. Hennessy, and M.S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <journal> SIGPLAN Notices, </journal> <volume> 26(6) </volume> <pages> 1-14, </pages> <year> 1991. </year>
Reference-contexts: We represent data flow constraints in the loop with dependence relations [20], and we keep the exact dependence information between each pair of read and write <ref> [6, 18] </ref>. Consider a read reference r in statement S r flow dependent on a write reference w of statement S w .
Reference: 19. <author> D. Padua. </author> <title> Multiprocessors: Discussion of some theoretical and practical problems. </title> <type> PhD thesis, </type> <institution> University of Illinois, Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: Instead of transforming the statements as written by the programmer, CDA first partitions the original statements into finer statements and then aligns the statements at this finer granularity. This creates 1 The origins of CA can be traced to loop alignment <ref> [1, 19] </ref>, which is a special case of CA. additional opportunities for optimization. In later sections we show how CDA can be used to implement flexible computation rules. A number of optimization techniques have been proposed that reduce communication by transforming data.
Reference: 20. <author> W. Pugh. </author> <title> Uniform techniques for loop optimization. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 341-352, </pages> <address> Cologne, Germany, </address> <year> 1991. </year>
Reference-contexts: If computation space CS (S) is transformed by T , then reference matrix R of each reference r in S is changed to RT 1 . We represent data flow constraints in the loop with dependence relations <ref> [20] </ref>, and we keep the exact dependence information between each pair of read and write [6, 18]. Consider a read reference r in statement S r flow dependent on a write reference w of statement S w .
Reference: 21. <author> J. Torres and E. Ayguade. </author> <title> Partitioning the statement per iteration space using non-singular matrices. </title> <booktitle> In Proceedings of 1993 International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: However, linear loop transformations are limited in their optimization capabilities, since they leave iterations unchanged as they map the original iteration space onto a new one. Over the last three years, Computation Alignment (CA) frameworks have been proposed that extend the capabilities of linear transformations <ref> [9, 12, 21] </ref> by transforming loops at the granularity of statements. 1 By applying a separate transformation to each statement in the loop body they change the execution order of each statement, thus effectively changing the constitution of the iterations. <p> Guard-free code is usually desirable for better performance, but a perfect loop may be desirable in some cases, for instance to avoid non-vector communications or to avoid loop overheads. Algorithms to generate code employing both strategies can be found in the literature <ref> [9, 10, 12, 21, 22] </ref>. 4.3 Aligning the Temporary Arrays In the third stage, each temporary array is data aligned to the array used by the operator to specify the processor onto which the computations are to be mapped.
Reference: 22. <author> J. Torres, E. Ayguade, J. Labarta, and M. Valero. </author> <title> Align and distribute-based linear loop transformations. </title> <booktitle> In Proceedings of Sixth Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: CA transformations have been applied to improve SPMD code in a variety of ways <ref> [12, 22] </ref>. For example, CA may be used to align the statements in the loop body so that all lhs data elements accessed in an iteration are located on the same processor in the hope of eliminating the need for ownership tests. <p> We can now employ CA to separately transform each statement of the new loop body in attempting to eliminate the need for intrinsics <ref> [12, 22] </ref>. Intuitively, the mapping causes a relative movement of the statement instances across iterations. The idea is to move the computations so that those that are mapped to the same processor belong to the same iteration. <p> Guard-free code is usually desirable for better performance, but a perfect loop may be desirable in some cases, for instance to avoid non-vector communications or to avoid loop overheads. Algorithms to generate code employing both strategies can be found in the literature <ref> [9, 10, 12, 21, 22] </ref>. 4.3 Aligning the Temporary Arrays In the third stage, each temporary array is data aligned to the array used by the operator to specify the processor onto which the computations are to be mapped.
Reference: 23. <author> M.E. Wolf and M.S. Lam. </author> <title> An algorithmic approach to compound loop transformation. </title> <booktitle> In Proceedings of Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Linear loop transformation is a general technique developed in 1990 that changes the execution order of the iterations <ref> [4, 15, 17, 23] </ref> and can be used, for example, to reduce communication overhead by moving communications to the outer loop levels [17, 23]. However, linear loop transformations are limited in their optimization capabilities, since they leave iterations unchanged as they map the original iteration space onto a new one. <p> Linear loop transformation is a general technique developed in 1990 that changes the execution order of the iterations [4, 15, 17, 23] and can be used, for example, to reduce communication overhead by moving communications to the outer loop levels <ref> [17, 23] </ref>. However, linear loop transformations are limited in their optimization capabilities, since they leave iterations unchanged as they map the original iteration space onto a new one.
References-found: 23

