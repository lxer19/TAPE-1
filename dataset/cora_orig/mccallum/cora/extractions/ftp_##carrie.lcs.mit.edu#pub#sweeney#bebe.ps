URL: ftp://carrie.lcs.mit.edu/pub/sweeney/bebe.ps
Refering-URL: http://carrie.lcs.mit.edu/people/sweeney/bebe.html
Root-URL: 
Email: sweeney@lcs.mit.edu, pmt@ai.mit.edu  
Title: Speech Perception Using Real-Time Phoneme Detection: The BeBe System  
Author: Latanya Sweeney and Patrick Thompson Latanya Sweeney and Patrick Thompson 
Note: Copyright 1997 by  
Address: Cambridge, MA 02139, USA  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: [Allen, 1994] <author> Allen, J. </author> <title> How do humans process and recognize speech? IEEE Transactions on Speech and Audio Processing: </title> <type> 2(4). </type>
Reference-contexts: The recognition of the sounds became automatic. Even though these survey results may not be statistically or psychologically persuasive, they do agree with the findings of Fletcher and his colleagues, who studied the principles behind human speech recognition from 1918 to 1950 at Bell Labs <ref> [Allen, 1994] </ref> and concluded that humans decode speech sounds into independent units at an early stage, before semantic context is used. Diagram 2. A presentation of BeBe 0 at work. <p> As part of their study of human speech recognition, Fletcher and his colleagues proposed a model of human speech recognition that consisted of a cascade of recognition layers, starting with the cochlea <ref> [Allen, 1994] </ref>. As in the BeBe architecture, no feedback is assumed between layers; Fletchers abstractions are similar.
Reference: [Baker, 1975] <author> Baker, J. </author> <title> Stochastic modeling for automatic speech understanding. Speech Recognition, edited by Raj Reddy. </title> <publisher> London: Academic Press: </publisher> <pages> 521-541. </pages>
Reference: [Bourlard, et al., 1996] <author> Bourlard, H., Hermansky, H., and Morgan, N. </author> <title> Towards increasing speech recognition error rates. Speech Communication: </title> <type> 18(3). </type>
Reference-contexts: 1 Introduction The task of speech recognition is to map a digitally encoded signal to a string of words. Over the past 10 years speech recognition technology has advanced dramatically, evolving into 65,000-word vocabulary research systems capable of transcribing naturally spoken sentences on specific topics from any new talker <ref> [Bourlard, et al., 1996] </ref>, but achieving humanlike performance remains distant.
Reference: [Carlson, et al., 1992] <author> Carlson, R. and Glass, J. </author> <title> Vowel classification based on analysis-by synthesis. </title> <booktitle> Proceedings of ICSLP, </booktitle> <pages> 575-578. </pages>
Reference: [Chigier, et al., 1992] <author> Chigier, B. and Leung, H. </author> <title> The effects of signal representations, phonetic classification techniques. </title> <booktitle> Proceedings of ICSLP, </booktitle> <pages> 97-100. </pages>
Reference: [Cole, et al., 1980] <author> Cole, R.A., Rudnicky, A.I., Zue, V.W. and Reddy, </author> <title> D.R. Speech as patterns on paper. Perception and Production of Fluent Speech, edited by R.A. </title> <publisher> Cole. </publisher> <address> Hillsdale: </address> <publisher> Lawrence Erlbaum Associates: </publisher> <pages> 3-50. </pages>
Reference-contexts: The last two phrases are the same utterances except phrase II is at normal speed and III is spoken rapidly. 6 2.3 Phoneme models Acoustic-phonetic recognition involves global reasoning about the identity of phonemes in a digitized representation of a spectrogram <ref> [Cole, et al., 1980; Zue, 1985] </ref>. There are three stages: feature extraction, segmentation and labeling, and word-level recognition. First, the system examines the signal representation for features that describe spectral patterns.
Reference: [Cole, et al., 1992] <author> Cole, R. and Muthusamy, Y. </author> <title> Perceptual studies on vowels excised from continuous speech. </title> <booktitle> Proceedings of ICSLP, </booktitle> <pages> 1091-1094. </pages>
Reference: [Digalakis, et al., 1993] <author> Digalakis, V., Rohlicek, J., and Ostendorf, M. </author> <title> ML estimation of a stochastic linear system with the EM algorithm and its application to speech recognition. </title> <booktitle> IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> 431-442. </pages>
Reference: [Erman, et al., 1980] <author> Erman, L., Hayes-Roth, F., Lesser, V. and Reddy, </author> <title> D.R. The Hearsay-II speech-understanding system: integrating knowledge to resolve uncertainty. </title> <journal> Computing Surveys, </journal> <volume> 12(2), </volume> <pages> 213-251. </pages>
Reference-contexts: Diagram 1 presents an overview of the BeBe architecture which will be discussed in further detail in the Design and Implementation sections. In the area of speech recognition, Hearsay-IIs blackboard architecture <ref> [Erman, et al., 1980] </ref> engages multiple knowledge sources that work in parallel. Adjacent sources communicate with each other using a message center called a blackboard.
Reference: [Forgie, et al., 1959] <author> Forgie, J. And Forgie, C. </author> <title> Results obtained from a vowel recognition computer program. </title> <journal> Journal of Acoustic Society of America, </journal> <volume> 31(11) </volume> <pages> 1480-1489. </pages>
Reference: [Hall, 1961] <author> Hall, R.A. </author> <title> Sound and spelling in English. </title> <address> Philadelphia. </address>
Reference-contexts: Another strategy applies the BeBe architecture at a higher level. A phonetic transcript is sent to a second stage that converts the transcript to poorly spelled English words using sound-to-spelling rules, phoneme-to-grapheme mappings <ref> [Hall, 1961] </ref>, or orthographic rules [Sweeney, 1996]; see Diagram 3. The candidate words from the second stage then go to a spelling corrector which produces the final result.
Reference: [House, 1962] <author> House, </author> <title> A.S. On the learning of speechlike vocabularies. </title> <journal> Journal of Verbal and Learning Verbal Behavior, </journal> <volume> 1, </volume> <pages> 133-143. </pages>
Reference: [Huang, 1991] <author> Huang, C.B. </author> <title> An acoustic and perceptual study of vowel formant trajectories in American English. </title> <institution> Massachusetts Institute of Technology, Research Laboratory of Electronics: </institution> <type> Technical Report 563. 20 </type>
Reference: [Jones, 1993] <author> Jones, </author> <title> E.R. Contemporary College Physics, 2 nd ed., </title> <address> Reading: </address> <publisher> Addison-Wesley: </publisher> <pages> 426. </pages>
Reference-contexts: Spectral slices are developed by passing these segments through a Hamming window to an FFT analyzer. The moduli of the transformed values are approximately weighted according to the frequency response of the human ear <ref> [Jones, 1993] </ref> and smoothed along the time axis by a simple recursive filter. With these steps completed, each detection algorithm has available to it a vector of the sample segment, and vectors of the FFT before and after weighting.
Reference: [Klatt, 1977] <author> Klatt, D. </author> <title> Review of the ARPA Speech Understanding Project. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 62, </volume> <pages> 1345-1366. </pages>
Reference-contexts: Only one system met ARPA SURs original mandate, Carnegie Mellon Universitys Harpy <ref> [Klatt, 1977] </ref>, and the success of Harpys statistical modeling techniques continues to have a profound effect as researchers seek to build larger statistical knowledge bases in an attempt to overcome problems and extend the performance of systems.
Reference: [Kornfeld, 1979] <author> Kornfeld, </author> <title> W.A. Using parallel processing for problem solving. </title> <institution> Massachusetts Institute of Technology, Artificial Intelligence Laboratory: </institution> <note> Memo 561. </note>
Reference-contexts: Although the Scrub System used numerous knowledge sources such as lists of area codes, first names, medical terms and so forth, BeBe has no stored lists and uses no higher-level predictive knowledge. BeBe relies only on its ability to recognize phonemes. In Ether <ref> [Kornfeld, 1979] </ref>, decentralized parallel processing was shown to be an effective alternative, in computational complexity terms, to many kinds of heuristic search strategies that implemented backtracking. Parallelism in Ether, as in BeBe, is design-based and does not necessarily require parallelism in its implementation.
Reference: [Liberman, 1957] <author> Liberman, </author> <title> A.M. The discrimination of speech sounds within and across phoneme boundaries. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 54, </volume> <pages> 358-368. </pages>
Reference: [Lippmann, 1996] <author> Lippmann, R. </author> <title> Recognition by humans and machines: miles to go before we sleep. </title> <journal> Speech Communication: </journal> <volume> 18(3), </volume> <pages> 247-248. </pages>
Reference: [Markowitz, 1996] <author> Markowitz, J. </author> <title> Using speech recognition. Upper Saddle River: </title> <publisher> Prentice-Hall: </publisher> <pages> 129-135 </pages>
Reference-contexts: Most of the speech recognition systems commercially available today are really connected speech systems which require a deliberate pause between each word <ref> [Markowitz, 1996] </ref>. The pause cannot be eliminated since it is used to identify word boundaries. Table 1 shows a phonetic transcript of some American English phrases and their corresponding text with and without word boundaries.
Reference: [Meng, et al., 1991] <author> Meng, H. and Zue, V. </author> <title> Signal representation comparison for phonetic classification. </title> <booktitle> Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> 285-288. </pages>
Reference: [Peterson and Barney, 1952] <author> Peterson, G.E. and Barney, H.L. </author> <title> Control methods used in a study of the vowels. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 24(2), </volume> <pages> 175-184. </pages>
Reference-contexts: The certainty factor can be conceived as a number between 0 and 1. For many of the vowels, we found that simple heuristics concerning typical duration and templated spectral-band densities for formants F1, F2, and F3 <ref> [Peterson and Barney, 1952] </ref> could accurately identify instances. False positives were gated out by requiring a minimum average spectral density (a loudness threshold) and then a minimum ratio of template-band density to average density for suspected vowels (a minimum vowelsignal-to-noise).
Reference: [Pieraccini, et al., 1991] <author> Pieraccini, R., Lee, C., Giachin, E. and Rabiner, L. </author> <title> Complexity reduction in a large vocabulary speech recognizer. </title> <booktitle> IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> 729-732. </pages>
Reference-contexts: Table 1 shows a phonetic transcript of some American English phrases and their corresponding text with and without word boundaries. To combat this problem in continuous speech, many systems use triphones, a phoneme surrounded by contextual information on both sides, to model crossword coarticulation. Researchers at AT&T Bell Laboratories <ref> [Pieraccini, et al., 1991] </ref> reported that when these highly detailed speech units were used the complexity of the overall implementation increased quadratically with the number of units, making a fullsearch implementation at that time totally impractical, if not impossible. 5 Clearly, having a reliable phoneme detector changes the nature of speech
Reference: [Rabiner, et al., 1993] <author> Rabiner, L. and Juang, B. </author> <title> Fundamentals of speech recognition. </title> <address> Englewood Cliffs: </address> <publisher> Prentice-Hall: </publisher> <pages> 392. </pages>
Reference-contexts: The recording environment was an office setting in which background noise included a loud ventilation system and the electronic hum of 10 three computer systems. The sound pressure level was estimated at 63 dBA where a typical office with a single computer is around 45-50 dBA <ref> [Rabiner, et al., 1993] </ref>. Here is a walk through the system. A forwardsliding window identifies 512-sample segments of the audio waveform, nominally sampled at 22050 Hz, monaurally, with 8-bit resolution. Spectral slices are developed by passing these segments through a Hamming window to an FFT analyzer.
Reference: [Reddy, 1966] <author> Reddy, D. </author> <title> An approach to computer speech recognition by direct analysis of the speech wave. </title> <institution> Stanford University, Computer Science Department, </institution> <note> Technical Report C549. </note>
Reference: [Rimmer, 1995] <author> Rimmer, S. </author> <title> Advanced multimedia programming. </title> <address> New York: Windcrest/McGraw-Hill. </address>
Reference: [Sakai, et al., 1962] <author> Sakai, T. and Doshita, S. </author> <title> The phonetic typewriter, </title> <booktitle> information processing. Proceedings IFIP Congress. </booktitle> <address> Munich. </address> <note> Also discussed in [Rabiner, </note> <editor> et al., </editor> <year> 1993]. </year>
Reference: [Schmid, 1996] <author> Schmid, P. </author> <title> Explicit, N-best formant features for segment-based speech recognition. </title> <institution> Oregon Graduate Institute, Department. of Computer Science, </institution> <note> Technical Report CSE-96-TH-003. </note>
Reference: [Schroeder, et al., 1979] <author> Schroeder, M.R., Atal, B.S., and Hall, J.L. </author> <title> Objective measure of certain speech signal degradations based on masking properties of human auditory perception. 21 Frontiers of Speech Communication Research, edited by B. </title> <editor> Lindblom and S. Ohman. </editor> <publisher> London: Academic Press: </publisher> <pages> 217-229. </pages>
Reference-contexts: The magic values 50, 3 and 6 were determined empirically. 11 listing of the /R/ detector that uses this simple strategy. Other detectors work differently, and some could be made adaptive. BeBe 0 detects formants based on fixed templates. A logarithmically varying template based on critical bands <ref> [Schroeder, et al., 1979] </ref>, such as the Bark scale used by Huang [1991], would likely improve detector accuracy and speaker independence in BeBe. BeBe 0 gets robust results with only the fixed-template version of the /R/ detector.
Reference: [Suzuki, et al., 1961] <author> Suzuki, J. and Nakata, K. </author> <title> Recognition of Japanese vowels preliminary to the recognition of speech. </title> <journal> Journal Radio Research Laboratory : 37(8) </journal> <pages> 193-212. </pages>
Reference: [Sweeney, 1996a] <author> Sweeney, L. </author> <title> Replacing Personally-Identifying Information in Medical Records, the Scrub System. </title> <editor> In: Cimino, JJ, ed. </editor> <booktitle> Proceedings, Journal of the American Medical Informatics Association. </booktitle> <address> Washington, DC: Hanley & Belfus, </address> <publisher> Inc, </publisher> 1996 333-337. 
Reference: [Sweeney, 1996b] <author> Sweeney, L. </author> <title> Automatic acquisition of orthographic rules for recognizing and generating spellings. MIT. </title> <note> AI Working Paper. </note>
Reference-contexts: Some consonant-vowel and vowel-consonant transitions, for example, are common and others do not occur at all and such may be related to how humans pronounce and recognize nonsense words <ref> [Sweeney, 1996b] </ref>. 6 Discussion We have presented the BeBe architecture and demonstrated its robustness.
Reference: [Weinstein, et al. 1975] <author> Weinstein, C., McCandless, S., Modshein, L., and Zue, V. </author> <title> A system for acoustic-phonetic analysis of continuous speech. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 23(1), </volume> <pages> 314-327. </pages>
Reference: [Zue, 1985] <author> Zue, V. W. </author> <title> The use of speech knowledge in automatic speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 73(11), </volume> <pages> 1602-1615. </pages>
Reference-contexts: The last two phrases are the same utterances except phrase II is at normal speed and III is spoken rapidly. 6 2.3 Phoneme models Acoustic-phonetic recognition involves global reasoning about the identity of phonemes in a digitized representation of a spectrogram <ref> [Cole, et al., 1980; Zue, 1985] </ref>. There are three stages: feature extraction, segmentation and labeling, and word-level recognition. First, the system examines the signal representation for features that describe spectral patterns.
References-found: 33

