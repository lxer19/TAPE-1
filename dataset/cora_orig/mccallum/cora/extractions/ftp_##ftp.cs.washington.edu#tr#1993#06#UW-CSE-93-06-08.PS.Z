URL: ftp://ftp.cs.washington.edu/tr/1993/06/UW-CSE-93-06-08.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Data Locality On Shared Memory Computers Under Two Programming Models  
Author: Ton A. Ngo Lawrence Snyder 
Address: Seattle, WA 98195  
Affiliation: Dept. of Computer Science and Engineering University of Washington  
Abstract: Data locality is a well-recognized requirement for the development of any parallel application, but the emphasis on data locality varies with the programming model. In this paper, we examine two models at the extreme ends of the spectrum with respect to one general class of parallel machines. We present experiments on shared-memory machines that indicate that programs written in the nonshared-memory programming model generally offer better data locality and thus better performance. We study the LU decomposition problem and a Molecular Dynamics simulation on five shared-memory machines with widely differing architectures, and analyze the effect of the programming models on data locality. The comparison is done through a simple analytical model and measurements on the machines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Anderson and L. Snyder, </author> <title> "A Comparison of Shared and Nonshared Memory Models of Parallel Computation," </title> <booktitle> Proceedings of IEEE, (1991), </booktitle> <volume> 79(4) </volume> <pages> 480-487. </pages>
Reference-contexts: Anderson and Snyder <ref> [1] </ref> analyzed P s and P ns of several common algorithms and found that the shared memory model tends to give overly optimistic performance prediction and, more importantly, can lead to the use of suboptimal algorithms. 3 Methodology 3.1 The Machines Machine Sequent CSRD Cedar Butterfly Kendall Square Stanford DASH model
Reference: [2] <author> C. Ashcraft, </author> <title> "A Taxonomy of Distributed Dense LU Factorization Methods," </title> <journal> Engineering Computing and Analysis Technical Report ECA-TR-161, </journal> <month> (March </month> <year> 1991). </year>
Reference-contexts: the O (n 3 ) of the row update step, this step only constitutes a minor part of the computation; however, it introduces additional serialization into the algorithm. 5 4.2 The parallel algorithms Since LU decomposition is a well studied problem, optimized parallel algorithms are widely available in the literature <ref> [2, 6, 13] </ref>. The computations in the row update step for each iteration are independent and can be parallelized easily. The partial pivoting step is more difficult to parallelize effectively because the parallelism available is small compared to the communication/synchronization required for parallelization.
Reference: [3] <author> G. Byrd and B. Delagi, </author> <title> "A Performance Comparison of Shared Variables versus Message Passing," </title> <booktitle> The Third International Conference on Supercomputing, (1988) Vol. </booktitle> <volume> 1, </volume> <pages> pp. 1-7. </pages>
Reference-contexts: Byrd and Delagi <ref> [3] </ref> used a model of network access cost model to compare P s jC s and P ns jC ns ; they found that P s jC s is much more susceptible to high network latency, which is a likely characteristic of large scale machines.
Reference: [4] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua, </author> <title> "Experience in the Automatic Parallelization of Four Perfect Benchmark Programs," </title> <booktitle> The Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> (August </month> <year> 1991) </year> <month> pp. g1-g19. </month>
Reference: [5] <author> High Performance Fortran Forum, </author> <title> "HPF Language Specification version 1.0," </title> <year> (1993). </year>
Reference: [6] <author> A. Karp, </author> <title> "Programming for Parallelism," </title> <journal> Computer, </journal> <note> (May 1987) pp. 43-56. </note>
Reference-contexts: the O (n 3 ) of the row update step, this step only constitutes a minor part of the computation; however, it introduces additional serialization into the algorithm. 5 4.2 The parallel algorithms Since LU decomposition is a well studied problem, optimized parallel algorithms are widely available in the literature <ref> [2, 6, 13] </ref>. The computations in the row update step for each iteration are independent and can be parallelized easily. The partial pivoting step is more difficult to parallelize effectively because the parallelism available is small compared to the communication/synchronization required for parallelization.
Reference: [7] <institution> Kendall Square Research, </institution> <type> "KSR Technical Summary", </type> <year> (1992). </year>
Reference-contexts: Since there is no hardware coherency mechanism, the machine provides various cache invalidation functions to support software caching. Finally, to avoid hot spots in this memory organization, the global address space is interleaved across all nodes. The KSR-1 employs an AllCache architecture <ref> [7] </ref>: instead of a main memory, each node possesses a large cache that is kept coherent with all other caches through a directory-based scheme. In addition to the main cache, there are also instruction and data subcaches on each node.
Reference: [8] <author> T. LeBlanc, </author> <title> "Shared-Memory versus Message-Passing in a Tightly-Coupled Multiprocessor: A Case Study," </title> <booktitle> International Conference on Parallel Processing, (1986) pp. </booktitle> <pages> 463-466. </pages>
Reference-contexts: Lin and Snyder [11] made similar comparison of P s and P ns on several shared-memory machines using the Jacobi iteration and matrix multiplication: they found that P ns can actually outperform P s . Leblanc <ref> [8] </ref> also compared P s jC s and Sim ns!s (P ns )jC s using Gaussian Elimination (without pivoting) on the BBN Butterfly. He observed that the model should be chosen based on the nature of the application, and that the shared model may encourage too much communication.
Reference: [9] <author> D. Lenoski, et al., </author> <title> "The DASH Prototype: Logic Overhead and Performance," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> (January 1993) Vol 4, No 1, </volume> <pages> pp. 41-61. </pages>
Reference-contexts: While the architecture provides a combining mechanism by servicing a cache miss at the lowest level of the ring hierarchy, the machine used in the experiment only contains one ring; therefore this combining effect is not visible. The Stanford DASH also uses a directory scheme for cache coherency <ref> [9] </ref>; however, it is organized as a collection of bus-based clusters connected in a mesh topology. Each cluster contains 4 processor nodes, a cluster memory and a cluster cache.
Reference: [10] <author> K. Li and P. Hudak, </author> <title> "Memory Coherence in Shared Virtual Memory Systems," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> (November 1989) Vol 7, No 4, </volume> <pages> pp. 463-466. </pages>
Reference-contexts: The time to execute P x on C x is denoted P x jC x . The compiling and runtime support to run programs of model x on computers of type y is Sim x!y . Example of Sim s!ns includes Shared Virtual Memory <ref> [10] </ref>, while Sim ns!s can be trivially implemented by emulating the send/receive operations using the shared memory. <p> The converse to the problem we are considering, Sim s!ns , has also attracted recent interest. An example is the Shared Virtual Memory system proposed by Li and Hudak <ref> [10] </ref>, in which the operating system maintains a consistent cache of memory pages to create an illusion of shared memory in a distributed machine environment.
Reference: [11] <author> C. Lin and L. Snyder, </author> <title> "A Comparison of Programming Models for Shared Memory Multiprocessors," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, (1990), Penn State Vol. II, </booktitle> <pages> pp. 163-170. </pages>
Reference-contexts: By ensuring that the programs perform the same scalar operations, we can attribute the differences in performance to differences in the memory model. 2.2 Related work P s and P ns have also been compared in similar and different contexts. Lin and Snyder <ref> [11] </ref> made similar comparison of P s and P ns on several shared-memory machines using the Jacobi iteration and matrix multiplication: they found that P ns can actually outperform P s .
Reference: [12] <author> F. Andre and T. Priol, </author> <title> "Programming Distributed Memory Parallel Computers without Explicit Message Passing," </title> <address> SHPCC, </address> <year> (1992), </year> <pages> pp. 90-97. </pages>
Reference-contexts: An example is the Shared Virtual Memory system proposed by Li and Hudak [10], in which the operating system maintains a consistent cache of memory pages to create an illusion of shared memory in a distributed machine environment. More specifically, Priol and Lahjomri <ref> [12] </ref> developed a Shared Virtual Memory system on the iPSC/2 and compared Sim s!ns (P s ) in this simulated environment against the native P ns ; they found that the P s tends to have difficulty with the granularity of sharing.
Reference: [13] <author> Y. Robert, </author> <title> The Impact of Vector and Parallel Architectures on the Gaussian Elimination Algorithm, </title> <publisher> Halsted Press (1990). </publisher>
Reference-contexts: the O (n 3 ) of the row update step, this step only constitutes a minor part of the computation; however, it introduces additional serialization into the algorithm. 5 4.2 The parallel algorithms Since LU decomposition is a well studied problem, optimized parallel algorithms are widely available in the literature <ref> [2, 6, 13] </ref>. The computations in the row update step for each iteration are independent and can be parallelized easily. The partial pivoting step is more difficult to parallelize effectively because the parallelism available is small compared to the communication/synchronization required for parallelization. <p> This argument is supported by the fact that the speedup improves uniformly with larger problem sizes; in this case, a more accurate method for measuring speedup would use the largest problem size possible for each number of processors <ref> [13] </ref>. However, the memory requirement as a function of the problem size may limit scaling up the problem size to match the number of processors: the system memory at best increases linearly with the number of processors, but the memory requirement of LU, for instance, is O (n 2 ).
Reference: [14] <author> J. Singh, W. Weber, and A. Gupta, </author> <title> "SPLASH: Stanford Parallel Applications for Shared Memory," </title> <type> report draft, </type> <institution> Department of Computer Science, Stanford University, </institution> <year> (1991). </year>
Reference-contexts: However, such problem size would quickly exceed the storage capacity of the machine. 10 11 12 5 Molecular Dynamics Simulation 5.1 The problem The WATER benchmark from the Stanford SPLASH suite is a simulation of several hundred water molecules in a cubical box in the liquid state at room temperature <ref> [14] </ref>. The program is representative of the n-body problem, in which each body interacts in certain ways with all other bodies in the system. In this case, the simulation computes the forces and potentials among the water molecules to predict various static and dynamic properties of water.
Reference: [15] <author> L. Snyder, </author> <title> "Type Architecture, Shared Memory and the Corollary of Modest Potential," </title> <booktitle> Annual Review of Computer Science, (1986), </booktitle> <volume> Vol. 1, </volume> <booktitle> Annual Review, </booktitle> <publisher> Inc., </publisher> <pages> pp. 289-318. </pages>
Reference: [16] <author> S. Stark, and A. Beris, </author> <title> "LU Decomposition Optimized For A Parallel Computer With A Hierarchical Distributed Memory," </title> <booktitle> The 1991 MPCI Yearly Report: The Attack of the Killer Micros, </booktitle> <month> (March </month> <year> 1991), </year> <pages> pp. 127-132. </pages>
Reference-contexts: Each of these factors constitutes a component in the overall communication cost which increases with the number of processors. It is also possible that the problem sizes used are not large enough. Karp and Boris <ref> [16] </ref> showed that speedup for LU on the Butterfly can be maintained if the problem size is increased proportionally with the number of processors.
References-found: 16

