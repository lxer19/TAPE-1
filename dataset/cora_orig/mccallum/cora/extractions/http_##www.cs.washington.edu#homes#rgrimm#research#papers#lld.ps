URL: http://www.cs.washington.edu/homes/rgrimm/research/papers/lld.ps
Refering-URL: http://www.cs.washington.edu/homes/rgrimm/research/papers.html
Root-URL: 
Email: frgrimm, wchsieh, kaashoekg@lcs.mit.edu  wiebren@cs.vu.nl  
Title: Atomic Recovery Units: Failure Atomicity for Logical Disks  
Author: Robert Grimm Wilson C. Hsieh M. Frans Kaashoek Wiebren de Jonge 
Address: Cambridge, U.S.A.  Amsterdam, The Netherlands  
Affiliation: Laboratory for Computer Science, M.I.T.  Dept. of Mathematics and Computer Science, Vrije Universiteit  
Abstract: Atomic recovery units (ARUs) are a mechanism that allows several logical disk operations to be executed as a single atomic unit with respect to failures. For example, ARUs can be used during file creation to update several pieces of file meta-data atomically. ARUs simplify file systems, as they isolate issues of atomicity within the logical disk system. ARUs are designed as part of the Logical Disk (LD), which provides an interface to disk storage that separates file and disk management by using logical block numbers and block lists. This paper discusses the semantics of concurrent ARUs, as well as the concurrency control they require. A prototype implementation in a log-structured logical disk system is presented and evaluated. The performance evaluation shows that the run-time overhead to support concurrent ARUs is negligible for Read and Write operations, and small but pronounced for file creation (4.0%-7.2%) and deletion (17.9%-20.5%), which mainly manipulate meta-data. The low overhead (when averaged over file creation, writing, reading, and deletion) for concurrent ARUs shows that issues of atomicity can be successfully isolated within the disk system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. D. Chamberlin, M. M. Astrahan, M. W. Blasgen, J. N. Gray, W. F. King, B. G. Lindsay, R. Lorie, J. W. Mehl, T. G. Price, F. Putzolu, P. G. Selinger, M. Schkolnick, D. R. Slutz, I. L. Traiger, B. W. Wade, and R. A. Yost. </author> <title> A history and evaluation of System R. </title> <booktitle> Readings in Database Systems, </booktitle> <pages> pages 54-68, </pages> <year> 1988. </year>
Reference-contexts: Related Work ARUs are unique in that they allow for clearly separated disk management and file management. Most other approaches to failure atomicity provide this support implicitly (as in file systems) or bundle it with transactions. Transactions [8] are commonly used in database systems such as System R <ref> [1] </ref> and Postgres [16]. A successful implementation provides a correct way to group several operations in one larger, atomic unit.
Reference: [2] <author> C. Choa, R. English, D. Jacobson, A. Stepanov, and J. Wilkes. Mime: </author> <title> a high performance parallel storage device with strong recovery guarantees. </title> <type> Technical Report HPL CSP-92-9rev1, </type> <institution> Hewlett Packard, </institution> <year> 1992. </year>
Reference-contexts: Furthermore, as our performance evaluation shows, concurrent atomic recovery units are an efficient solution that removes considerable complexity from the file system. The Mime disk storage architecture <ref> [2] </ref> is based on the Loge [5] disk controller (which uses internal indirection and shadow-paged block updates) and features atomic writes over multiple blocks through the use of visibility groups.
Reference: [3] <author> S. Chutani, O. T. Anderson, M. L. Kazar, B. W. Leverett, W. A. Mason, and R. N. Sidebotham. </author> <title> The Episode file system. </title> <booktitle> In Proceedings of 1992 Winter USENIX, </booktitle> <pages> pages 43 60, </pages> <year> 1992. </year>
Reference-contexts: Soft updates serialize meta-data updates to reflect the correct semantic dependencies as blocks are written to disk, which enables the use of delayed writes, but still uses conventional overwrite semantics. Both the re-implementation of the Cedar File System FSD [9] and the Episode File System <ref> [3] </ref>, use a write-ahead log for all updates to meta-data.
Reference: [4] <author> W. de Jonge, M. F. Kaashoek, and W. C. Hsieh. </author> <title> The log ical disk: A new approach to improving file systems. </title> <booktitle> In Proceedings of the 14th Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-28, </pages> <year> 1993. </year>
Reference-contexts: In particular, clients need to define and implement their own locking mechanisms. ARUs also do not guarantee durability. However, if desired, file systems and databases can easily implement these functions on top of ARUs. ARUs are designed as part of the Logical Disk <ref> [4] </ref>. The Logical Disk (LD) is an interface to disk storage that provides its clients with a virtual representation of disk storage by using a logical name-space for disk blocks. File management and disk management can thus be separated, which removes complex disk optimizations from the file system. <p> To demonstrate the fact that concurrent atomic recovery units at the disk level perform well, we present a prototype implementation. The prototype is based on the log-structured prototype of LD (log-structured logical disk or LLD, see <ref> [4] </ref>), which originally did not support concurrent ARUs. We use the same combination of LLD and the Minix file system [17] as in [4], which shows excellent performance on Write operations (utilizing 85% of the available bandwidth as compared to 13% for the Minix file system by itself) and good performance <p> The prototype is based on the log-structured prototype of LD (log-structured logical disk or LLD, see <ref> [4] </ref>), which originally did not support concurrent ARUs. We use the same combination of LLD and the Minix file system [17] as in [4], which shows excellent performance on Write operations (utilizing 85% of the available bandwidth as compared to 13% for the Minix file system by itself) and good performance on Read operations (depending on the workload). We also use the same experiments as in [4] for our performance comparison, which allows us <p> the Minix file system [17] as in <ref> [4] </ref>, which shows excellent performance on Write operations (utilizing 85% of the available bandwidth as compared to 13% for the Minix file system by itself) and good performance on Read operations (depending on the workload). We also use the same experiments as in [4] for our performance comparison, which allows us a direct evaluation of the effects of concurrent ARUs on run-time behavior. <p> Finally, Section 7 summarizes the experience of designing and implementing concurrent ARUs within LLD. 2. Background: The Logical Disk System ARUs are designed as part of the logical disk system (LD). LD separates disk and file management by providing an abstract interface to disk access <ref> [4] </ref>. It provides its clients with a virtual representation of disk storage through the use of a logical name-space for disk blocks. Blocks are the smallest unit of disk storage in LD, and can be further arranged in larger logical units called lists. <p> The list-table records the first (and last) block of each list. These tables describe the persistent state and are the same as in the initial version of LLD. Figure 3 (which is the same as Figure 2 in <ref> [4] </ref>) illustrates these two tables. It also illustrates the layout of disk storage, which is fragmented into segments. Segments are further divided into two parts, one part that stores the data blocks and one part that stores the segment summary. <p> Furthermore, the log-structured implementation can be replaced the shadow state and the disk layout. This illustration shows the shadow state for ARU i after the logical block k has been allocated as the first block on list j. with a different implementation without changing disk system clients. As in <ref> [4] </ref>, we use a single-threaded Minix file system on top of LLD for our experiments (the combined system is called MinixLLD). Since LLD provides the disk management, most of the disk management code (350 lines) has been deleted from Minix. <p> Both LLD and Minix are linked together in a single user process that accesses the disk through the raw disk interface provided by SunOS. The performance of the original prototype of MinixLLD (without concurrent ARUs) compares favorably with the Minix file system by itself (for detailed performance results see <ref> [4] </ref>). MinixLLD shows excellent performance on Write operations (utilizing 85% of the available bandwidth as compared to 13% for the Minix file system by itself) and good performance on Read operations (depending on the workload). We repeat the micro-benchmarks from [4]. <p> Minix file system by itself (for detailed performance results see <ref> [4] </ref>). MinixLLD shows excellent performance on Write operations (utilizing 85% of the available bandwidth as compared to 13% for the Minix file system by itself) and good performance on Read operations (depending on the workload). We repeat the micro-benchmarks from [4]. The first benchmarks measure small file I/O by creating and writing, then reading and finally deleting 10,000 1-KByte files and 1,000 10-KByte files. The second benchmark measures large file I/O. This test uses a 78.125 MByte file, which is first written sequentially (write1), then read sequentially (read1). <p> Concurrency Overhead The three different versions of LLD which are used to determine the concurrency overhead are listed in Table 1. old stands for the original prototype of LLD, which supports non-concurrent ARUs and is described in <ref> [4] </ref>. new is our prototype implementation of concurrent ARUs within LLD and new, delete is the same prototype implemen old The original version of MinixLLD (with sequential ARUs). new The new version of MinixLLD (with concurrent ARUs). new, delete The new version of MinixLLD with improved file deletion in Minix. <p> The Sprite Log-Structured File System [12, 11] introduced the idea of using a log-like structure to organize disk storage. The original prototype of LLD (without concurrent ARUs) is largely based on the same log-structured design principle; a detailed comparison between Sprite LFS and LLD can be found in <ref> [4] </ref>. While Sprite LFS shares the general design principle with our prototype implementation of concurrent ARUs within LLD, it does not provide any support for a larger grain of operations or embedded transactions, nor does Sprite LFS feature a clear separation between disk and file system.
Reference: [5] <author> R. M. English and A. A. Stepanov. Loge: </author> <title> a self-organizing disk controller. </title> <booktitle> In Proceedings of 1992 Winter USENIX, </booktitle> <pages> pages 237-251, </pages> <year> 1992. </year>
Reference-contexts: Furthermore, as our performance evaluation shows, concurrent atomic recovery units are an efficient solution that removes considerable complexity from the file system. The Mime disk storage architecture [2] is based on the Loge <ref> [5] </ref> disk controller (which uses internal indirection and shadow-paged block updates) and features atomic writes over multiple blocks through the use of visibility groups.
Reference: [6] <author> G. Ganger and Y. Patt. </author> <title> Soft updates: A solution to the meta data update problem in file systems. </title> <type> Technical Report CSE TR-254-05, </type> <institution> University of Michigan Department of Electrical Engineering and Computer Science, </institution> <year> 1995. </year>
Reference-contexts: File system meta-data plays a crucial role in ensuring file system integrity. Conventional file systems, such as the Berkeley FFS [10], use costly synchronous writes to ensure that meta-data updates reach persistent storage. Because of the considerable performance degradation caused by syn chronous writes, Ganger and Patt <ref> [7, 6] </ref> introduce a special form of delayed write, called soft update. Soft updates serialize meta-data updates to reflect the correct semantic dependencies as blocks are written to disk, which enables the use of delayed writes, but still uses conventional overwrite semantics.
Reference: [7] <author> G. R. Ganger and Y. N. Patt. </author> <title> Metadata update performance in file systems. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 49 60, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: File system meta-data plays a crucial role in ensuring file system integrity. Conventional file systems, such as the Berkeley FFS [10], use costly synchronous writes to ensure that meta-data updates reach persistent storage. Because of the considerable performance degradation caused by syn chronous writes, Ganger and Patt <ref> [7, 6] </ref> introduce a special form of delayed write, called soft update. Soft updates serialize meta-data updates to reflect the correct semantic dependencies as blocks are written to disk, which enables the use of delayed writes, but still uses conventional overwrite semantics.
Reference: [8] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Related Work ARUs are unique in that they allow for clearly separated disk management and file management. Most other approaches to failure atomicity provide this support implicitly (as in file systems) or bundle it with transactions. Transactions <ref> [8] </ref> are commonly used in database systems such as System R [1] and Postgres [16]. A successful implementation provides a correct way to group several operations in one larger, atomic unit.
Reference: [9] <author> R. Hagmann. </author> <title> Reimplementing the Cedar file system using logging and group commit. </title> <booktitle> In Proceedings of the 11th Sym posium on Operating Systems Principles, </booktitle> <pages> pages 155-162, </pages> <year> 1987. </year>
Reference-contexts: Soft updates serialize meta-data updates to reflect the correct semantic dependencies as blocks are written to disk, which enables the use of delayed writes, but still uses conventional overwrite semantics. Both the re-implementation of the Cedar File System FSD <ref> [9] </ref> and the Episode File System [3], use a write-ahead log for all updates to meta-data.
Reference: [10] <author> M. K. McKusick, W. N. Joy, S. J. Leffler, and R. S. Fabry. </author> <title> A fast file system for UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-197, </pages> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: Atomic transactions have much stricter semantics than ARUs, are much more heavy-weight than ARUs, and therefore are not appropriate for disk systems. File system meta-data plays a crucial role in ensuring file system integrity. Conventional file systems, such as the Berkeley FFS <ref> [10] </ref>, use costly synchronous writes to ensure that meta-data updates reach persistent storage. Because of the considerable performance degradation caused by syn chronous writes, Ganger and Patt [7, 6] introduce a special form of delayed write, called soft update.
Reference: [11] <author> M. Rosenblum. </author> <title> The Design and Implementation of a Log structured File System. </title> <type> PhD thesis, </type> <institution> University of Califor nia at Berkeley, </institution> <year> 1992. </year> <note> Also availbale as Technical Report UCB/CSD 92/696. </note>
Reference-contexts: Third, it allows for efficient solutions to the I/O bottleneck. Since LD presents a virtual representation of disk storage, it can transparently re-organize the layout of data on disk to reduce access times. The log-structured LD prototype (the log-structured logical disk system or LLD) is modeled after Sprite LFS <ref> [11, 12] </ref>. It divides the disk into large, fixed-size segments that are filled in main memory and written to disk in single disk operations. Segments are divided into two parts. <p> A visibility group will be restored up to the latest barrier point after a failure. Visibility groups address the same problem as ARUs, but are more complex than ARUs, since they support checkpoints and unrolling, whereas ARUs do not. The Sprite Log-Structured File System <ref> [12, 11] </ref> introduced the idea of using a log-like structure to organize disk storage. The original prototype of LLD (without concurrent ARUs) is largely based on the same log-structured design principle; a detailed comparison between Sprite LFS and LLD can be found in [4].
Reference: [12] <author> M. Rosenblum and J. K. Ousterhout. </author> <title> The design and imple mentation of a log-structured file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 26-52, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Third, it allows for efficient solutions to the I/O bottleneck. Since LD presents a virtual representation of disk storage, it can transparently re-organize the layout of data on disk to reduce access times. The log-structured LD prototype (the log-structured logical disk system or LLD) is modeled after Sprite LFS <ref> [11, 12] </ref>. It divides the disk into large, fixed-size segments that are filled in main memory and written to disk in single disk operations. Segments are divided into two parts. <p> A visibility group will be restored up to the latest barrier point after a failure. Visibility groups address the same problem as ARUs, but are more complex than ARUs, since they support checkpoints and unrolling, whereas ARUs do not. The Sprite Log-Structured File System <ref> [12, 11] </ref> introduced the idea of using a log-like structure to organize disk storage. The original prototype of LLD (without concurrent ARUs) is largely based on the same log-structured design principle; a detailed comparison between Sprite LFS and LLD can be found in [4].
Reference: [13] <author> M. Seltzer, K. Bostic, M. K. McKusick, and C. Staelin. </author> <title> An implementation of a log-structured file system for UNIX. </title> <booktitle> In Proceedings of 1993 Winter USENIX, </booktitle> <pages> pages 307-326, </pages> <year> 1993. </year>
Reference-contexts: Her embedded implementation features full transaction support at the file system level. Service disruption because of segment cleaning and poor sequential read performance caused her to redesign the log-structured file system, this time as part of the Berkeley BSD 4.4 release. The resulting file system <ref> [13, 14] </ref> does not support embedded transactions and, by treating the original fast file system and the log-structured file system as alternatives, neglects to specify the semantics of disk access or to introduce more advanced semantics (though it does present alternative optimizations to address different disk workloads). 7.
Reference: [14] <author> M. Seltzer, K. A. Smith, H. Balakrishnan, J. Chang, S. Mc Mains, and V. Padmanabhan. </author> <title> File system logging versus clustering: A performance comparison. </title> <booktitle> In Proceedings of the 1995 USENIX Technical Conference, </booktitle> <pages> pages 325, 249 264, </pages> <year> 1995. </year>
Reference-contexts: Her embedded implementation features full transaction support at the file system level. Service disruption because of segment cleaning and poor sequential read performance caused her to redesign the log-structured file system, this time as part of the Berkeley BSD 4.4 release. The resulting file system <ref> [13, 14] </ref> does not support embedded transactions and, by treating the original fast file system and the log-structured file system as alternatives, neglects to specify the semantics of disk access or to introduce more advanced semantics (though it does present alternative optimizations to address different disk workloads). 7.
Reference: [15] <author> M. I. Seltzer. </author> <title> File System Performance and Transaction Support. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1983. </year>
Reference-contexts: While Sprite LFS shares the general design principle with our prototype implementation of concurrent ARUs within LLD, it does not provide any support for a larger grain of operations or embedded transactions, nor does Sprite LFS feature a clear separation between disk and file system. Seltzer <ref> [15] </ref> presents both a user-level transaction system and an embedded transaction system using the Sprite Log-Structured File System. Her embedded implementation features full transaction support at the file system level.
Reference: [16] <author> M. Stonebraker. </author> <title> The design of the Postgres storage system. </title> <booktitle> Readings in Database Systems, </booktitle> <pages> pages 610-621, </pages> <year> 1988. </year>
Reference-contexts: Most other approaches to failure atomicity provide this support implicitly (as in file systems) or bundle it with transactions. Transactions [8] are commonly used in database systems such as System R [1] and Postgres <ref> [16] </ref>. A successful implementation provides a correct way to group several operations in one larger, atomic unit. It ensures that the effects of concurrent transactions are isolated from each other by some locking mechanism and that the effect on the overall state (e.g., on the database) is persistent.
Reference: [17] <author> A. S. Tanenbaum. </author> <title> Operating Systems: Design and Imple mentation. </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, N.J. 07632, </address> <year> 1987. </year>
Reference-contexts: The prototype is based on the log-structured prototype of LD (log-structured logical disk or LLD, see [4]), which originally did not support concurrent ARUs. We use the same combination of LLD and the Minix file system <ref> [17] </ref> as in [4], which shows excellent performance on Write operations (utilizing 85% of the available bandwidth as compared to 13% for the Minix file system by itself) and good performance on Read operations (depending on the workload).
References-found: 17

