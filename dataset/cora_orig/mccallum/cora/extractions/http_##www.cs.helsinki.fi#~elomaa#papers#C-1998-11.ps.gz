URL: http://www.cs.helsinki.fi/~elomaa/papers/C-1998-11.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~elomaa/
Root-URL: 
Title: Evaluation of Attributes with a High Number of Boundary Points  
Author: Tapio Elomaa and Juho Rousu 
Address: Finland  
Affiliation: University of Helsinki  
Note: Postponing the  
Abstract: Department of Computer Science Series of Publications C Report C-1998-11 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Blum, A. & Langley, P. </author> <year> (1997). </year> <title> Selection of relevant features and examples in machine learning. </title> <booktitle> Artificial Intelligence 97, </booktitle> <pages> 245-271. </pages>
Reference-contexts: 1 Introduction Identifying and eliminating either irrelevant attributes, or features, [17, 5, 19, 18] or untrustworthy training examples [16, 4, 23] prior to classifier construction are techniques used to aid and enhance the induction process (for a comprehensive survey see <ref> [1] </ref>). Such cleaning methods can be heavier than the actual process of building a classifier, which is not in the spirit of efficient induction. Moreover, irreversible decisions to remove attributes or examples are taken.
Reference: [2] <author> Bohanec, M. & Bratko, I. </author> <year> (1994). </year> <title> Trading accuracy for simplicity in decision trees. </title> <booktitle> Machine Learning 15, </booktitle> <pages> 223-250. </pages>
Reference-contexts: As evaluating such an attribute is also time consuming, postponing its evaluation should turn out beneficial in the resulting classifiers quality and speed of classifier construction. We do not want to trade accuracy for efficiency or simplicity <ref> [15, 14, 2] </ref>, but strive to maintain the prediction ability of the resulting decision tree while speeding up the classifier construction by simple and efficient dynamic data processing.
Reference: [3] <author> Breiman, L., Friedman, J., Olshen, R. & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth. </publisher>
Reference-contexts: Such functions include, e.g., balanced gain [20, 9], gain ratio [25, 27], and normalized distance measure [21]. Also, the gini index (of diversity) <ref> [3] </ref> has a very similar formula as IG, and ought to be easy to analyze.
Reference: [4] <author> Brodley, C. & Friedl, M. </author> <year> (1996). </year> <title> Identifying and eliminating mislabeled training instances. </title> <booktitle> In Proc. Thirteenth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: 1 Introduction Identifying and eliminating either irrelevant attributes, or features, [17, 5, 19, 18] or untrustworthy training examples <ref> [16, 4, 23] </ref> prior to classifier construction are techniques used to aid and enhance the induction process (for a comprehensive survey see [1]). Such cleaning methods can be heavier than the actual process of building a classifier, which is not in the spirit of efficient induction.
Reference: [5] <author> Caruana, R. & Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <editor> In W. Cohen & H. Hirsh (eds.), </editor> <booktitle> Machine Learning: Proc. Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction Identifying and eliminating either irrelevant attributes, or features, <ref> [17, 5, 19, 18] </ref> or untrustworthy training examples [16, 4, 23] prior to classifier construction are techniques used to aid and enhance the induction process (for a comprehensive survey see [1]).
Reference: [6] <author> Codrington, C. & Brodley, C. </author> <year> (1997). </year> <title> On the qualitative behavior of impurity-based splitting rules I: The minima-free property. </title> <type> Technical Report 97-5. </type> <institution> Pur-due University, School of Electrical and Computer Engineering. </institution>
Reference-contexts: Codrington and Brodley <ref> [6] </ref> present further studies of the convexity properties of many common attribute evaluation functions. Definition Let a sequence S of examples be sorted by the value of a numerical attribute A.
Reference: [7] <author> Cover, T. & Thomas, J. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons. </publisher>
Reference: [8] <author> Dietterich, T. </author> <title> (forthcoming). Approximate statistical tests for comparing supervised classification learning algorithms. </title> <booktitle> Neural Computation. </booktitle>
Reference-contexts: As the test strategy we use two-fold cross validation testing repeated five times, 5x2cv; it has been observed to be a reliable statistical test in experiments that involve comparison of more than two learning algorithms <ref> [8] </ref>. The average prediction accuracies obtained using the strategies in the 5x2cv test are tabulated in Table 1.
Reference: [9] <author> Elomaa, T. & Rousu, J. </author> <title> (forthcoming). General and efficient multisplitting of numerical attributes. </title> <booktitle> Machine Learning. </booktitle>
Reference-contexts: Even a linear-time method like binarization can require substantial amount of time. This presents a particular problem for learning algorithms that have to manipulate numerical attributes exhaustively; e.g., optimal <ref> [13, 9] </ref> or greedy [11] multisplitters in decision tree learning, rule induction, and nearest neighbor methods. The inconvenience for all attribute selection strategies alike is that the time consumption of attribute selection is dominated by the attributes that are the heaviest ones to evaluate. <p> Our research is motivated by an attempt to relieve the quadratic-time optimal multisplitting <ref> [13, 9] </ref> from its vulnerability with respect to a single anomalous dimension of the data. Even though the methods that we propose can be characterized as 1 lazy evaluation, this is not lazy decision tree learning in the sense of Fried--man, Kohavi and Yun [12]. <p> The basic relationships of these three figures are B V n, but it is 2 the common (mis)conception that B t V t n in real-world data. Recently the relationship of these figures have been studied in detail <ref> [9] </ref> for a large collection of the most commonly used machine learning data sets from the University of California at Irvine data repository [22]. <p> Moreover, only the class distribution of each block needs to be known in order to be able to compute the impurities of partitions defined on boundary points. Gathering examples into blocks can be applied for well-behaved evaluation functions, which include all the most commonly used attribute evaluation functions <ref> [9] </ref>. <p> If a well-behaved evaluation function also has the so-called cumulativity property, 5 the quadratic-time optimal partitioning algorithm of Fulton et al. [13] can be adapted to operate in time that is quadratic in the number of blocks instead of bins <ref> [9] </ref>. 3 Using boundary points as an direct indica tion of attribute relevance Let us study the well-behaved evaluation function average class entropy, ACE. <p> Many other evaluation functions use IG as their building block, which means that from the above analysis of ACE we can obtain bounds for the values of these functions as well. Such functions include, e.g., balanced gain <ref> [20, 9] </ref>, gain ratio [25, 27], and normalized distance measure [21]. Also, the gini index (of diversity) [3] has a very similar formula as IG, and ought to be easy to analyze. <p> In addition, it has other desirable properties <ref> [9] </ref>. 7 4 Utilizing information from preprocessing No matter which partitioning strategy|binarization, greedy or optimal multi-splitting|is used to handle numerical attributes, preprocessing of the data is required. At least the examples have to be sorted. Thereafter, candidate cut points can be evaluated. <p> However, from the preprocessing stage we can also extract, at the low cost of O (mB), the class distributions of blocks. In practice, this preprocessing time has been observed to be negligible with respect to the time required by actual evaluation of candidate partitions <ref> [9] </ref>. These distributions give another possibility to bound (sometimes more tightly) the relevance of an attribute on the basis of boundary points. <p> Most of the domains are well-known; we do not describe them here, for a comprehensive description of their characteristic figures see, e.g., <ref> [9] </ref>. The domain abbreviations are abalone, adult, australian, auto imports, german, glass, letter recognition, mole, page blocks, satellite, segmentation, shuttle, vehicle, waveform, and yeast.
Reference: [10] <author> Fayyad, U. & Irani, K. </author> <year> (1992). </year> <title> On the handling of continuous-valued attributes in decision tree generation. </title> <booktitle> Machine Learning 8, </booktitle> <pages> 87-102. 13 </pages>
Reference-contexts: The inconvenience for all attribute selection strategies alike is that the time consumption of attribute selection is dominated by the attributes that are the heaviest ones to evaluate. Hence, even a single difficult attribute can ruin the efficiency of an otherwise manageable domain. This paper studies how boundary points <ref> [10] </ref> can be utilized to determine the relevance of an attribute in univariate induction. It is shown that an attribute with many boundary points is not relevant for class prediction in univariate induction. <p> Therefore, we can consider the data in a categorized version; such where all examples with an equal attribute value constitute a bin of examples. The number of bins, of course, is the same as the number of distinct values for the attribute, V . Fayyad and Irani's <ref> [10] </ref> analysis of the binarization technique proved that reductions in time consumption can be obtained for the information gain function [24], since only boundary points need to be considered as potential cut points, because optimal (binary) splits always fall on them due to the functions convexity. <p> In the original definition of Fayyad and Irani <ref> [10] </ref> a boundary point was taken to be a value that is strictly in between the values val A (s 1 ) and val A (s 2 ).
Reference: [11] <author> Fayyad, U. & Irani, K. </author> <year> (1993). </year> <title> Multi-interval discretization of continuous--valued attributes for classification learning. </title> <booktitle> In Proc. Thirteenth International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Even a linear-time method like binarization can require substantial amount of time. This presents a particular problem for learning algorithms that have to manipulate numerical attributes exhaustively; e.g., optimal [13, 9] or greedy <ref> [11] </ref> multisplitters in decision tree learning, rule induction, and nearest neighbor methods. The inconvenience for all attribute selection strategies alike is that the time consumption of attribute selection is dominated by the attributes that are the heaviest ones to evaluate.
Reference: [12] <author> Friedman, J., Kohavi, R. & Yun, Y. </author> <year> (1996). </year> <title> Lazy decision trees. </title> <booktitle> In Proc. Thirteenth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: Even though the methods that we propose can be characterized as 1 lazy evaluation, this is not lazy decision tree learning in the sense of Fried--man, Kohavi and Yun <ref> [12] </ref>. We still construct a comprehensive hypothesis in training phase and subsequently use it to classify the instances.
Reference: [13] <author> Fulton, T., Kasif, S. & Salzberg, S. </author> <year> (1995). </year> <title> Efficient algorithms for finding multi-way splits for decision trees. </title> <editor> In A. Prieditis & S. Russell (eds.), </editor> <booktitle> Machine Learning: Proc. Twelfth International Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Even a linear-time method like binarization can require substantial amount of time. This presents a particular problem for learning algorithms that have to manipulate numerical attributes exhaustively; e.g., optimal <ref> [13, 9] </ref> or greedy [11] multisplitters in decision tree learning, rule induction, and nearest neighbor methods. The inconvenience for all attribute selection strategies alike is that the time consumption of attribute selection is dominated by the attributes that are the heaviest ones to evaluate. <p> Our research is motivated by an attempt to relieve the quadratic-time optimal multisplitting <ref> [13, 9] </ref> from its vulnerability with respect to a single anomalous dimension of the data. Even though the methods that we propose can be characterized as 1 lazy evaluation, this is not lazy decision tree learning in the sense of Fried--man, Kohavi and Yun [12]. <p> By using a well-behaved function we may concentrate on boundary points independent of whether the partition arity is limited a priori or not. If a well-behaved evaluation function also has the so-called cumulativity property, 5 the quadratic-time optimal partitioning algorithm of Fulton et al. <ref> [13] </ref> can be adapted to operate in time that is quadratic in the number of blocks instead of bins [9]. 3 Using boundary points as an direct indica tion of attribute relevance Let us study the well-behaved evaluation function average class entropy, ACE.
Reference: [14] <author> Holte, R. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used data sets. </title> <booktitle> Machine Learning 11, </booktitle> <pages> 63-90. </pages>
Reference-contexts: As evaluating such an attribute is also time consuming, postponing its evaluation should turn out beneficial in the resulting classifiers quality and speed of classifier construction. We do not want to trade accuracy for efficiency or simplicity <ref> [15, 14, 2] </ref>, but strive to maintain the prediction ability of the resulting decision tree while speeding up the classifier construction by simple and efficient dynamic data processing.
Reference: [15] <author> Iba, W., Wogulis, J. & Langley, P. </author> <year> (1988). </year> <title> Trading off simplicity and coverage in incremental concept learning. </title> <editor> In J. Laird (ed.), </editor> <booktitle> Proc. Fifth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As evaluating such an attribute is also time consuming, postponing its evaluation should turn out beneficial in the resulting classifiers quality and speed of classifier construction. We do not want to trade accuracy for efficiency or simplicity <ref> [15, 14, 2] </ref>, but strive to maintain the prediction ability of the resulting decision tree while speeding up the classifier construction by simple and efficient dynamic data processing.
Reference: [16] <author> John, G. </author> <year> (1995). </year> <title> Robust decision trees: Removing outliers from data. </title> <booktitle> In Proc. First International Conference on Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: 1 Introduction Identifying and eliminating either irrelevant attributes, or features, [17, 5, 19, 18] or untrustworthy training examples <ref> [16, 4, 23] </ref> prior to classifier construction are techniques used to aid and enhance the induction process (for a comprehensive survey see [1]). Such cleaning methods can be heavier than the actual process of building a classifier, which is not in the spirit of efficient induction.
Reference: [17] <author> Kira, K. & Rendell, L. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <booktitle> In Proc. Ninth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: 1 Introduction Identifying and eliminating either irrelevant attributes, or features, <ref> [17, 5, 19, 18] </ref> or untrustworthy training examples [16, 4, 23] prior to classifier construction are techniques used to aid and enhance the induction process (for a comprehensive survey see [1]).
Reference: [18] <author> Kohavi, R. & John, G. </author> <year> (1997). </year> <title> Wrappers for feature subset selection. </title> <booktitle> Artificial Intelligence 97, </booktitle> <pages> 273-324. </pages>
Reference-contexts: 1 Introduction Identifying and eliminating either irrelevant attributes, or features, <ref> [17, 5, 19, 18] </ref> or untrustworthy training examples [16, 4, 23] prior to classifier construction are techniques used to aid and enhance the induction process (for a comprehensive survey see [1]).
Reference: [19] <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extensions of Relief. </title> <editor> In F. Bergadano & L. De Raedt (eds.), </editor> <booktitle> Proc. Seventh European Conference on Machine Learning. Lecture Notes in Artificial Intelligence 784. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: 1 Introduction Identifying and eliminating either irrelevant attributes, or features, <ref> [17, 5, 19, 18] </ref> or untrustworthy training examples [16, 4, 23] prior to classifier construction are techniques used to aid and enhance the induction process (for a comprehensive survey see [1]).
Reference: [20] <author> Kononenko, I., Bratko, I., & Roskar, E. </author> <year> (1984). </year> <title> Experiments in automatic learning of medical diagnostic rules. </title> <type> Technical Report. </type> <institution> Josef Stefan Institute, Faculty of Electrical Engineering and Computer Science. </institution>
Reference-contexts: Many other evaluation functions use IG as their building block, which means that from the above analysis of ACE we can obtain bounds for the values of these functions as well. Such functions include, e.g., balanced gain <ref> [20, 9] </ref>, gain ratio [25, 27], and normalized distance measure [21]. Also, the gini index (of diversity) [3] has a very similar formula as IG, and ought to be easy to analyze.
Reference: [21] <author> Lopez de Mantaras, R. </author> <year> (1991). </year> <title> A distance-based attribute selection measure for decision tree induction. </title> <booktitle> Machine Learning 6, </booktitle> <pages> 81-92. </pages>
Reference-contexts: Many other evaluation functions use IG as their building block, which means that from the above analysis of ACE we can obtain bounds for the values of these functions as well. Such functions include, e.g., balanced gain [20, 9], gain ratio [25, 27], and normalized distance measure <ref> [21] </ref>. Also, the gini index (of diversity) [3] has a very similar formula as IG, and ought to be easy to analyze.
Reference: [22] <author> Merz, C. J., & Murphy, P. M. </author> <year> (1996). </year> <title> UCI repository of machine learning databases (http://www.ics.uci.edu/~mlearn/MLRepository.html). University of California, </title> <institution> Department Information and Computer Science. </institution> <month> 14 </month>
Reference-contexts: Recently the relationship of these figures have been studied in detail [9] for a large collection of the most commonly used machine learning data sets from the University of California at Irvine data repository <ref> [22] </ref>. It turns out that most typically the number of boundary points in a numerical dimension is at least half of the total number of existing values in the data. The claim V t n is better grounded, and B t n even more so.
Reference: [23] <author> Oates, T. & Jensen, D. </author> <year> (1997). </year> <title> The effects of training set size on decision tree complexity. </title> <editor> In D. Fisher (ed.), </editor> <booktitle> Machine Learning: Proc. Fourteenth International Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction Identifying and eliminating either irrelevant attributes, or features, [17, 5, 19, 18] or untrustworthy training examples <ref> [16, 4, 23] </ref> prior to classifier construction are techniques used to aid and enhance the induction process (for a comprehensive survey see [1]). Such cleaning methods can be heavier than the actual process of building a classifier, which is not in the spirit of efficient induction.
Reference: [24] <author> Quinlan, R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In R. Michalski, J. Carbonell & T. Mitchell (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Tioga. </publisher>
Reference-contexts: The number of bins, of course, is the same as the number of distinct values for the attribute, V . Fayyad and Irani's [10] analysis of the binarization technique proved that reductions in time consumption can be obtained for the information gain function <ref> [24] </ref>, since only boundary points need to be considered as potential cut points, because optimal (binary) splits always fall on them due to the functions convexity. Codrington and Brodley [6] present further studies of the convexity properties of many common attribute evaluation functions. <p> The above calculated absolute minimum value for ACE serves as the basis for an upper bound of the highest obtainable value of the information gain function <ref> [24] </ref>. It is defined as IG i ] S i : H (S)|the entropy of the data set S prior to partitioning it by any of the attributes|is constant with respect to the dimensions of the data.
Reference: [25] <author> Quinlan, R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages>
Reference-contexts: Many other evaluation functions use IG as their building block, which means that from the above analysis of ACE we can obtain bounds for the values of these functions as well. Such functions include, e.g., balanced gain [20, 9], gain ratio <ref> [25, 27] </ref>, and normalized distance measure [21]. Also, the gini index (of diversity) [3] has a very similar formula as IG, and ought to be easy to analyze.
Reference: [26] <author> Quinlan, R. </author> <year> (1988). </year> <title> Decision trees and multivalued attributes. </title> <editor> In J. Hayes, D. Michie & J. Richards (eds.), </editor> <booktitle> Machine Intelligence 11: Logic and the Acquisition of Knowledge. </booktitle> <publisher> Oxford University Press. </publisher>
Reference-contexts: Therefore, H (S) B is an upper bound for information gain of any partition of S. Incidentally, this explains why the information gain function is so eager to favor higher arity partitions of numerical attribute domains and nominal attributes with many potential values <ref> [26] </ref>. Furthermore, we can use this value to obtain an upper bound for the balanced gain. Observe that BG log does not (necessarily) obtain its maximum value when all blocks of the data constitute a partition subset of their own since the denominator log 2 k biases against unnecessary splitting.
Reference: [27] <author> Quinlan, R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann. </publisher> <pages> 15 </pages>
Reference-contexts: We, however, have chosen to follow the common practice that the thresholds defining a partition are taken to be appearing values from the domain of the attribute in question (cf. <ref> [27] </ref>). Both definitions lead to partitions with the same subsets (or intervals). Let us now define a block of examples. It is a concept that facilitates the discovery of all boundary points of a data set. <p> Many other evaluation functions use IG as their building block, which means that from the above analysis of ACE we can obtain bounds for the values of these functions as well. Such functions include, e.g., balanced gain [20, 9], gain ratio <ref> [25, 27] </ref>, and normalized distance measure [21]. Also, the gini index (of diversity) [3] has a very similar formula as IG, and ought to be easy to analyze. <p> Obviously, the above-derived approximations are not either very tight if k t B. We cannot use partitions of arity k as our approximation, since enumerating them requires O (B 2 ) time. 8 5 Empirical evaluation This section presents the results of comparative experiments in which C4.5 algorithm <ref> [27] </ref> changed to do optimal multisplitting for numerical attributes using the balanced gain function is equipped with four different postponing strategies: one which is a combination of the analytical upper bounds and three heuristic postponing strategies. The strategies are * Analytic.
References-found: 27

