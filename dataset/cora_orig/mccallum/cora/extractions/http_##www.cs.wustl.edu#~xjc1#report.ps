URL: http://www.cs.wustl.edu/~xjc1/report.ps
Refering-URL: http://siesta.cs.wustl.edu/~xjc1/
Root-URL: 
Email: JXCHEN@us.oracle.com milind@dworkin.wustl.edu dw@arl.wustl.edu  guru@arl.wustl.edu  
Phone: +1 415 506-8617 +1 314 935 4203 +1 314 935 8563  +1 314 935 7534  
Title: Enhancements to 4.4 BSD UNIX for Efficient Networked Multimedia in Project MARS  
Author: Xin Jane Chen Milind M. Buddhikot Dakang Wu Guru M. Parulkar 
Web: http://dworkin.wustl.edu/ ~milind/MediaServers.html  
Abstract: Cluster based architectures that employ high performance inexpensive Personal Computers (pcs) interconnected by high speed commodity interconnect have been recognized as a cost-effective way of building high performance scalable Multimedia-On-Demand storage servers [1, 3, 4]. Typically, the pcs in these architectures run operating systems such as unix that have traditionally been optimized for interactive computing. They do not provide fast disk-to-network data paths and guaranteed cpu and storage acccess. This paper reports enhancements to the 4.4 bsd unix system carried out to rectify these limitations in the context of our Project Massively-parallel And Real-time Storage (MARS) [3]. We have proposed and implemented the following enhancements to a 4.4 bsd compliant public domain NetBSD unix operating system: (1) A new kernel buffer management system called Multimedia M-buf (mmbuf) which is used to shorten the data path from a storage device to network output device, (2) priority queueing support within the scsi driver to differentiate between real-time and non-real-time streams, and (3) integration of these new os services with a cpu scheduling mechanism called Real Time Upcall [12] and a software disk striping driver called Concatenated Disk (ccd). These enhancements collectively provide quality of service guarantee to multimedia stream connections. Our experimental results demonstrate throughput improvements and QOS guarantees on the data path from the disk to network in a MOD server. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Buddhikot, M., Parulkar, G., M., and Cox, Jerome, Jr., </author> <title> "Design of a Large Scale Multimedia Server," Journal of Computer Networks and ISDN Systems, </title> <publisher> Elsevier (North Holland), </publisher> <pages> pp. 504-524, </pages> <month> Dec </month> <year> 1994. </year>
Reference-contexts: Designing high performance scalable servers and services that support such guarantees has been recognized to be a challenging task. Our project Massively-parallel And Real-time Storage (mars) aims to meet this challenge <ref> [1, 3] </ref>. The interactive mod services developed in our project are based on an innovative cluster based storage server architecture called Massively-parallel And Real-time Storage (mars)[1].
Reference: [2] <author> Buddhikot, M., and Parulkar, G., M., </author> <title> "Efficient Data Layout, Scheduling and Playout Control in MARS," </title> <journal> ACM/Springer Multimedia Systems Journal, pp. </journal> <volume> 199-211, Volume 5, Number 3, </volume> <year> 1997. </year>
Reference: [3] <author> Buddhikot, M., Parulkar, G., and Gopalakrishnan, R., </author> <title> "Scalable Multimedia-On-Demand via World-Wide-Web (WWW) with QOS Guarantees," </title> <booktitle> Proceedings of Sixth International Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <address> NOSSDAV96, Zushi, Japan, </address> <month> April 23-26, </month> <year> 1996. </year>
Reference-contexts: Designing high performance scalable servers and services that support such guarantees has been recognized to be a challenging task. Our project Massively-parallel And Real-time Storage (mars) aims to meet this challenge <ref> [1, 3] </ref>. The interactive mod services developed in our project are based on an innovative cluster based storage server architecture called Massively-parallel And Real-time Storage (mars)[1]. <p> The resulting benefits are minimizing expensive context switches, efficient concurrency control, efficient dispatching of upcalls, and elimination of the need for concurrency control between rtus [12]. Several examples of the effectiveness of rtus in providing excellent qos guarantees for media processing and user-level-protocol processing have been reported in <ref> [12, 3] </ref>. The rtu facility has already been demonstrated to be useful for high performance user level protocol implementations. 4 Performance Evaluation In this section, we will describe the experiments carried out to characterize the performance benefits of our solutions.
Reference: [4] <author> Bernhardt, C., and Biersack, E., </author> <title> "A Scalable Video Server: </title> <booktitle> Architecture, Design and Implementation," In Proceedings of the Real-time Systems Conference, </booktitle> <pages> pp. 63-72, </pages> <address> Paris, France, </address> <month> Jan. </month> <year> 1995. </year>
Reference: [5] <author> Cranor, C., </author> <title> "BSD ATM," Release Notes, </title> <address> Washington University in St. Louis, </address> <month> Jul 3, </month> <year> 1996. </year>
Reference-contexts: Each domain supports several protocols. For example, the inet domain supports tcp, udp and the ip. In our work, we use a newly created domain called Native atm (natm) which supports natm aal5, natm aal0 protocols <ref> [5] </ref>. The os maintains a domain data structure for each domain which in turns maintains a list of supported protocols in the form of a list of protocol switch - protosw structures. Each protosw structure defines the interface functions for a protocol. <p> For all of the experiments described, we used 200MHz Pentium PC with 128 mb ram, eni atm interface, and an Adaptec dual scsi ahc-3940. We ran the version of NetBSD 1.2B bsd kernel with a locally developed eni atm driver <ref> [5] </ref>, enhanced scsi driver, the ccd striping driver, and mmbuf, rtu options. We connected two 9 gb Seagate baracudda scsi disks to the controller. Each disk has a rotational speed of 7200 rpm with internal transfer rate of 80-124 Mbit/sec.
Reference: [6] <author> Druschel, Peter, and Peterson, Larry, "Fbufs: </author> <title> A high-bandwidth cross domain transfer facility," </title> <booktitle> Proceedings of 14 th SOSP, </booktitle> <pages> pp. 1892-202, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Therefore, a fast data path that can provide zero-copy data transfer between storage and network is desirable. Also, the user level application should be able to easily control such a data path. The Container Shipping system [11], the dash ipc [15] and fbufs <ref> [6] </ref> have addressed the problem of minimizing physical data movement across protection domains in an os by employing virtual memory re-mapping techniques. However, none of these projects report implementation of the zero-copy data path between disks and networks.
Reference: [7] <author> Fall, Kevin and Pasquale, Joseph, </author> <title> "Exploiting In-Kernel Data Paths to Improve I/O Throughput and CPU Availability", </title> <booktitle> Proceedings of the USENIX Winter Technical Conference, </booktitle> <address> San Diego, California, </address> <month> January </month> <year> 1993, </year> <pages> pp. 327-333. </pages>
Reference-contexts: However, none of these projects report implementation of the zero-copy data path between disks and networks. Kevin Fall et al <ref> [7, 8] </ref> have designed and implemented a mechanism called Splice in Ultirix 4.2 unix to move data within the kernel between two unix file i/o descriptors (one specifying the source of data and the other sink) without user program intervention.
Reference: [8] <author> Fall, Kevin and Pasquale, Joseph, </author> <title> "Improving Continuous-Media Playback Performance With In-Kernel Data Paths", </title> <booktitle> Proceedings of the IEEE International Conference on Multimedia Computing and Systems (ICMCS), </booktitle> <address> Boston, MA, </address> <month> June </month> <year> 1994, </year> <pages> pp. 100-109 </pages>
Reference-contexts: However, none of these projects report implementation of the zero-copy data path between disks and networks. Kevin Fall et al <ref> [7, 8] </ref> have designed and implemented a mechanism called Splice in Ultirix 4.2 unix to move data within the kernel between two unix file i/o descriptors (one specifying the source of data and the other sink) without user program intervention.
Reference: [9] <author> Kleiman, S., </author> <title> "Design of vnode interface," </title> <booktitle> Proceedings of the USENIX Symposium, </booktitle> <year> 1986. </year>
Reference-contexts: These local file stores employ a common data structure called index node (inode) that maps a file offset to disk locations, and they also, use common name space management code. All file or directory read/write activity is centered around inodes. In the recent past, Network File Systems <ref> [9, 10] </ref> that allow access to local file stores at a remote machine have become commonplace. These remote machines could be running different operating systems and maybe connected to different networks.
Reference: [10] <editor> McKusik, M., et al. </editor> <booktitle> "The Design and Implementation of the 4.4 BSD Operating System," </booktitle> <publisher> Addison Wesley, </publisher> <year> 1996. </year>
Reference-contexts: In a general 4 purpose interactive computing environment, where a large number of small sized i/o operations are common, it has been shown that upto 85 % of the implied disk operations are satisfied out of the buffer cache <ref> [10] </ref>. to describe the buffer's contents including the vnode whose data this buffer holds, the starting offset within the file, the number of bytes contained in the buffer and a pointer to the data area of the buffer. <p> These local file stores employ a common data structure called index node (inode) that maps a file offset to disk locations, and they also, use common name space management code. All file or directory read/write activity is centered around inodes. In the recent past, Network File Systems <ref> [9, 10] </ref> that allow access to local file stores at a remote machine have become commonplace. These remote machines could be running different operating systems and maybe connected to different networks.
Reference: [11] <author> Pasquale, Joseph, Anderson, Eric, and Muller, P. Keith, </author> <title> "Container Shipping: operating system support for i/o intensive applications," </title> <journal> IEEE Computer Magazine, </journal> <volume> 27 (3): </volume> <pages> 84-93, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Therefore, a fast data path that can provide zero-copy data transfer between storage and network is desirable. Also, the user level application should be able to easily control such a data path. The Container Shipping system <ref> [11] </ref>, the dash ipc [15] and fbufs [6] have addressed the problem of minimizing physical data movement across protection domains in an os by employing virtual memory re-mapping techniques. However, none of these projects report implementation of the zero-copy data path between disks and networks.
Reference: [12] <author> Gopal, R., </author> <title> "Efficient Quality of Service Support in Computer Operating systems for High Speed Networking and Multimedia," </title> <type> Doctoral Dissertation, </type> <institution> Washington University in St. Louis, </institution> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: This enhanced driver always gives higher priority to real-time streams and also, provides an easy way to do resource partitioning between different priority streams. We have integrated these new services with a new cpu scheduling mechanism called Real-Time Upcall (rtu) <ref> [12] </ref> and created new system calls for user level application to access these services. 1 We have not included a detailed Related Work section here due to concern for making this paper too long. <p> Guaranteed CPU access: In order to achieve constant rate data transfer between the disks and the network, a user application requires periodic access to cpu to be able to issue periodic stream read () requests. To this end, we employ a novel scheduling mechanism called Real-Time-Upcall (rtu) <ref> [12] </ref>. This mechanism minimizes processor context switching overhead and uses a cooperative (modified Rate Monotonic) scheduling scheme to eliminate the need for locking, thus significantly reducing system call overhead. <p> We believe that though ccd is sub-optimal, it represents a very simple and cost-effective way to build small disk arrays. 3.5 Real Time Upcall (RTU) Real Time Upcalls is a novel mechanism designed and implemented within our research group at Washington University <ref> [12] </ref> to provide guaranteed cpu access to user level and kernel level periodic tasks. 16 rtus are an alternative to real-time periodic threads and have advantages such as low implementation complexity, portability, and efficiency. Figure 16 illustrates the basic concept behind rtus. <p> Figure 16 illustrates the basic concept behind rtus. An rtu is essentially a function in a user program that is invoked periodically in real-time to perform certain activity <ref> [12] </ref>. Various examples of such activities are protocol processing such as tcp, udp, multimedia and bulk data processing, and periodic data retrievals from storage systems. <p> It uses a variant of the Rate Monotonic (rm) scheduling policy. The main feature of this policy is that there is no asynchronous preemption. The resulting benefits are minimizing expensive context switches, efficient concurrency control, efficient dispatching of upcalls, and elimination of the need for concurrency control between rtus <ref> [12] </ref>. Several examples of the effectiveness of rtus in providing excellent qos guarantees for media processing and user-level-protocol processing have been reported in [12, 3]. <p> The resulting benefits are minimizing expensive context switches, efficient concurrency control, efficient dispatching of upcalls, and elimination of the need for concurrency control between rtus [12]. Several examples of the effectiveness of rtus in providing excellent qos guarantees for media processing and user-level-protocol processing have been reported in <ref> [12, 3] </ref>. The rtu facility has already been demonstrated to be useful for high performance user level protocol implementations. 4 Performance Evaluation In this section, we will describe the experiments carried out to characterize the performance benefits of our solutions. <p> Clearly, these measurements indicate that our os enhancements provide qos guarantees and significant improvements in throughput on the data-path from the disks to the network interface. The research contributions described in this paper combined with new cpu scheduling mechanism such as rtus <ref> [12] </ref> makes 4.4 bsd unix a strong candidate for a true multimedia operating system. Acknowledgements First of all, I would like to thank my advisor, Prof. Guru Parulkar and my groupmate Milind Buddhikot for their indispensable help to me on the fulfillment of this project.
Reference: [13] <author> Gopalakrishnan, R., Parulkar, G.M., </author> <title> "A Framework for QoS Guarantees for Multimedia Applications within an End-system," </title> <booktitle> Swiss German Computer Science Society Conf., </booktitle> <year> 1995. </year>
Reference: [14] <author> Gopalakrishnan, R., Parulkar, G.M., </author> <title> "A Real-time Upcall Facility for Protocol Processing with QOS Guarantees," </title> <booktitle> (Poster) ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Copper Mountain, Colorado, </address> <month> Dec. </month> <year> 1995. </year>
Reference: [15] <author> Tzou, Shin-Yan and Anderson, David, </author> <title> "The performance of message-passing using restricted virtual memory re-mapping," </title> <journal> Software Practice and Experience, </journal> <volume> 21(3): </volume> <pages> 251-267, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Therefore, a fast data path that can provide zero-copy data transfer between storage and network is desirable. Also, the user level application should be able to easily control such a data path. The Container Shipping system [11], the dash ipc <ref> [15] </ref> and fbufs [6] have addressed the problem of minimizing physical data movement across protection domains in an os by employing virtual memory re-mapping techniques. However, none of these projects report implementation of the zero-copy data path between disks and networks.
Reference: [16] <author> Thorpe, Jason, </author> <type> Personal Communication, </type> <month> March </month> <year> 1996. </year> <month> 24 </month>
Reference-contexts: Though our current implementation does not implement any such policy, we expect such policies to be fairly easy to incorporate. 3.4 Concatenated disk driver (ccd) The Concatenated Disk Driver (ccd) is a disk striping software developed by Jason Thorpe <ref> [16] </ref>. It allows one or more 4.4 bsd disks or disk partitions of the same or different sizes to be combined into a single virtual disk or a software disk array.
References-found: 16

