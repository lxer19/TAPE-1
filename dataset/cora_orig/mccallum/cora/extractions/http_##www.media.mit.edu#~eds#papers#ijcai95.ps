URL: http://www.media.mit.edu/~eds/papers/ijcai95.ps
Refering-URL: http://www.media.mit.edu/~eds/papers.html
Root-URL: http://www.media.mit.edu
Email: email: eds@media.mit.edu  
Title: Using Musical Knowledge to Extract Expressive Performance Information from Audio Recordings  
Author: Eric D. Scheirer 
Address: E15-401C Cambridge, MA 02140  
Affiliation: MIT Media Laboratory  
Abstract: A computer system is described which performs polyphonic transcription of known solo piano music by using high-level musical information to guide a signal-processing system. This process, which we term expressive performance extraction, maps a digital audio representation of a musical performance to a MIDI representation of the same performance using the score of the music as a guide. Analysis of the accuracy of the system is presented, and its usefulness both as a tool for music-psychology researchers and as an example of a musical-knowledge based signal-processing system is discussed.
Abstract-found: 1
Intro-found: 1
Reference: [ Bilmes, 1993 ] <author> Jeff Bilmes. </author> <title> Timing is of the essence: Perceptual and computational techniques for representing, learning, and reproducing expressive timing in percussive rhythm. </title> <type> Master's thesis, </type> <institution> MIT Media Laboratory, </institution> <year> 1993. </year>
Reference-contexts: We can similarly recreate other sorts of analyses such as those found in [ Palmer, 1989 ] or <ref> [ Bilmes, 1993 ] </ref> by treating the timing variables as random Gaussian variables rather than known values. 2 Depending on which question we want to answer, though, the answers may be less satisfactory for small timing details.
Reference: [ Handel, 1989 ] <author> Stephen Handel. </author> <title> Listening. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Thus, if we are to be able to use this system for understanding timing relationships between melodies and harmony, it must be able to resolve differences at this level of accuracy or finer. 5 ms is generally taken as the threshold of perceptual difference (JND) for musical performance ( <ref> [ Handel, 1989 ] </ref> ); if we wish to be able to reconstruct identical performances, the timing accuracy must be at this level or greater.
Reference: [ Krumhansl, 1991 ] <author> Carol Krumhansl. </author> <title> Cognitive Foundations of Musical Pitch. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1991. </year>
Reference-contexts: It is clear that the human music-cognition system is working with representations of music on many different levels which guide and shape the perception of a particular musical performance. Work such as Krumhansl's tonal hierarchy ( <ref> [ Krumhansl, 1991 ] </ref> ) and Narmour's multi-layered grouping rules ( [ Narmour, 1990 ] , [ Nar-mour, 1993 ] ) show evidence for certain low- and mid-level cognitive representations for musical structure; and syntactic work such as Lerdahl and Jackendoffs' ( [ Ler-dahl and Jackendoff, 1983 ] ), while not
Reference: [ Lerdahl and Jackendoff, 1983 ] <author> Fred Lerdahl and Ray Jackendoff. </author> <title> A Generative Theory of Tonal Music. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference: [ Narmour, 1990 ] <author> Eugene Narmour. </author> <title> The Analysis and Cognition of Basic Melodic Structures. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1990. </year>
Reference-contexts: It is clear that the human music-cognition system is working with representations of music on many different levels which guide and shape the perception of a particular musical performance. Work such as Krumhansl's tonal hierarchy ( [ Krumhansl, 1991 ] ) and Narmour's multi-layered grouping rules ( <ref> [ Narmour, 1990 ] </ref> , [ Nar-mour, 1993 ] ) show evidence for certain low- and mid-level cognitive representations for musical structure; and syntactic work such as Lerdahl and Jackendoffs' ( [ Ler-dahl and Jackendoff, 1983 ] ), while not as well-grounded experimentally, suggest a possible structure for higher levels of
Reference: [ Narmour, 1993 ] <author> Eugene Narmour. </author> <title> The Analysis and Cognition of Melodic Complexity. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1993. </year>
Reference: [ Oppenheimer and Nawab, 1992 ] <author> Alan Oppenheimer and S. Hamid Nawab. </author> <title> Symbolic and Knowledge-Based Signal Processing. </title> <publisher> Prentice-Hall, Inc, </publisher> <year> 1992. </year>
Reference-contexts: Certainly, the "artificial intelligence" component, for understanding and making predictions about the musical signal, would be enormously complex in such a system. Work is currently in progress on a "blackboard system" architecture (see, eg, <ref> [ Oppenheimer and Nawab, 1992 ] </ref> ) for investigation of these issues.
Reference: [ Palmer, 1989 ] <author> Caroline Palmer. </author> <title> Timing in Skilled Music Performance. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <year> 1989. </year>
Reference-contexts: The parameters being extracted are those which are controllable by the expert pianist: velocity, and attack and release timing. <ref> [ Palmer, 1989 ] </ref> suggests certain levels of timing accuracy which can be understood as benchmarks for a system which is to extract note information at a level useful for understanding interpretation. <p> We can similarly recreate other sorts of analyses such as those found in <ref> [ Palmer, 1989 ] </ref> or [ Bilmes, 1993 ] by treating the timing variables as random Gaussian variables rather than known values. 2 Depending on which question we want to answer, though, the answers may be less satisfactory for small timing details.
Reference: [ Papoulis, 1991 ] <author> Athanasios Papoulis. </author> <title> Probability, Random Variables, and Stochastic Processes. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, third edition, </address> <year> 1991. </year>
Reference-contexts: these in turn, and then discuss ways in which the current system could be improved. 4.1 Stochastic Analysis of Music Performance Part of the worth of the sort of variance-of-error study conducted in the Results section is that we can treat extracted data as a stochastic estimator (cf, for example, <ref> [ Papoulis, 1991 ] </ref> ) for the actual performance, and make firm enough assumptions about the distribution of the estimation that we can obtain usable results. It is clear that some aspects of expressive music performance can be readily analyzed within the constraints of the variance in extraction discussed above.
Reference: [ Vercoe, 1984 ] <author> Barry Vercoe. </author> <title> The synthetic performer in the context of live performance. </title> <booktitle> In Proc. Int. Computer Music Conf., </booktitle> <year> 1984. </year>
Reference-contexts: This is generally a solvable problem, though see <ref> [ Vercoe, 1984 ] </ref> for an example. 4.3 Evidence-Integration Systems The evidence integration aspects of the system are the most novel, and at the same time, the least satisfying.
References-found: 10

