URL: http://cosmos.imag.fr/RESEAUX/malek/malek.ijcai95.ps.gz
Refering-URL: http://cosmos.imag.fr/RESEAUX/malek/Publications.html
Root-URL: 
Email: Email: Maria.Malek@imag.fr  Email: Bernard.Amy@imag.fr  
Title: A Preprocessing Model for Integrating CBR and Prototype-Based Neural Networks  
Author: Maria Malek Bernard Amy 
Note: CBR system. 1  
Address: bat. Lifia, 46 ave Felix Viallet, 38031 Grenoble, France  bat. Lifia, 46 ave Felix Viallet, 38031 Grenoble, France  
Affiliation: TIMC-LIFIA-IMAG  LIFIA-IMAG-CNRS  
Abstract: Some important factors that play a major role in determining the performances of a CBR (Case-Based Reasoning) system are the complexity and the accuracy of the retrieval phase. Both flat memory and inductive approaches suffer from serious drawbacks. In the first approach, the search time increases when dealing with large scale memory base, while in the second one the modification of the case memory becomes very complex because of its sophisticated architecture. In this paper, we show how we construct a simple efficient indexing system structure. The idea is to construct a case hierarchy with two levels of memory: the lower level contains cases organised into groups of similar cases, while the upper level contains prototypes. each prototype represents one group of cases. This smaller memory is used during the retrieval phase. Prototype construction is achieved by means of an incremental prototype-based NN (Neural Network). We show that this mode of CBR-NN coupling is a preprocessing one where the neural network serves as an indexing system to the 
Abstract-found: 1
Intro-found: 1
Reference: [ Aamodt and Plaza, 1994 ] <author> A. Aamodt and E. </author> <title> Plaza. Case-based reasoning: Foundational issues, methodological variations, and system approaches. </title> <journal> AICOM, </journal> <volume> 7(1), </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: Given a new input problem, the case retriever identifies the most appropriate cases (in some similarity sense) in the case base and feeds them to the case adapter. The case adapter then examines the retrieved cases and tries to solve the new problem by adapting these cases <ref> [ Aamodt and Plaza, 1994 ] </ref> [ Barletta, 1991 ] [ Riesbeck and Schank, 1989 ] . 1 This research is supported by the MIX ESPRIT Project-9119 It is obvious that more the retrieval phase is efficient more the overall performances of the CBR system is increased.
Reference: [ Alpaydin, 1991 ] <author> E. Alpaydin. </author> <title> Gal : Networks that grow when they learn and shrink when they forget. </title> <type> Technical report, </type> <institution> International Computer Science Institute, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: The first implemented model using this method was described in [ Reilly et al., 1982 ] commercially known as the Nestor Learning System (NLS). An incremental prototype-based neural network which is based on a " Grow and Learn" (Gal) algorithm is described also in <ref> [ Alpaydin, 1991 ] </ref> . In this section, we describe the incremental prototype-based neural network model (ARN2) proposed in [ Azcar-raza and Giacometti, 1991 ] . Figure 1 shows the architecture of the basic model.
Reference: [ Azcarraza and Giacometti, 1991 ] <author> A. Azcarraza and A. Giacometti. </author> <title> A prototype-based incremental network model for classification task. </title> <booktitle> In Neuro-Nimes, </booktitle> <year> 1991. </year>
Reference: [ Bamberger and Goos, 1993 ] <author> S.K. Bamberger and K. Goos. </author> <title> Integration of case-based reasoning and inductive learning methods. </title> <booktitle> In First European Workshop on CBR, </booktitle> <volume> number 1, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: The study of some inductive approaches like ID3/C4 [ Quinlan, 1992 ] have shown that the costs of altering large case bases are very high and lead sometimes to a re-compilation of the complete case base after each alteration <ref> [ Bamberger and Goos, 1993 ] </ref> [ Manago et al., 1993 ] . This shows that using inductive approaches makes retrieval more efficient but it considerably complicates the learning phase because of the sophisticated architecture of the used structure [ Utgoff, 1989 ] .
Reference: [ Barletta, 1991 ] <author> R. Barletta. </author> <title> An introduction to case-based reasoning. </title> <journal> AI Expert, </journal> <month> August </month> <year> 1991. </year>
Reference-contexts: The case adapter then examines the retrieved cases and tries to solve the new problem by adapting these cases [ Aamodt and Plaza, 1994 ] <ref> [ Barletta, 1991 ] </ref> [ Riesbeck and Schank, 1989 ] . 1 This research is supported by the MIX ESPRIT Project-9119 It is obvious that more the retrieval phase is efficient more the overall performances of the CBR system is increased. <p> These approaches aim to construct inductively a model by generalizing from specific examples. They have two advantages <ref> [ Barletta, 1991 ] </ref> : * They can automatically analyze the cases to deter mine the best features for distinguishing them. * The cases are organized for retrieval into a hierarchi-cal structure making the retrieval time a logarithmic function of the number of cases.
Reference: [ Giacometti, 1992 ] <author> A. Giacometti. Modeles Hybrides de l'Expertise. </author> <type> PhD thesis, </type> <institution> Telecom- Paris, </institution> <year> 1992. </year>
Reference-contexts: This situation can be avoided by introducing an uncertain region between two classes in which differentiation is not allowed <ref> [ Giacometti, 1992 ] </ref> . This region presents vectors that cause nearly the same activation to the two prototypes. As a result, the network is unable to learn new cases that falls into this region (we call these cases: boundary cases) (Figure 2).
Reference: [ Hecht-Nielsen, 1990 ] <editor> R. Hecht-Nielsen. Neurocomput-ing. </editor> <publisher> Addison-Wesley publishing Company, </publisher> <year> 1990. </year>
Reference: [ Kolodner, 1993 ] <author> J. Kolodner. </author> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1993. </year>
Reference-contexts: Hence, a particular attention must be paid to the design and the implementation of this phase. In literature, two widely used approaches are distinguished: flat memory systems and shared-features networks (hierarchical organisation of cases) <ref> [ Kolodner, 1993 ] </ref> . In a flat memory system, cases are stored sequentially in a simple list or a file. They are retrieved by applying a matching function sequentially to each case in the list. <p> Experience has shown that PBIS registers generalization accuracies which are comparable to C4.5. In addition, the retrieval time is decreased because of using a parallel memory that contains a few number of prototypes. In addition, using a simple memory hierarchy (two levels of memory) instead of a shared-neural networks <ref> [ Kolodner, 1993 ] </ref> simplifies learning new cases (incremen-tality). This is achieved by modifying one neural unit (prototype) and it is much simpler than the method used in ID5R for example. The similarity measure used now is the Euclidean one.
Reference: [ Malek and Rialle, 1994 ] <author> M. Malek and V. Rialle. </author> <title> A case-based reasoning system applied to neuropathy diagnosis. </title> <booktitle> In Second European Workshop, EWCBR-94, Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: S2 is a situation vector which activates P1 and P2 and which falls into an uncertain region. The network is unable to give a decision for S2 3 The Memory Structure The idea is to construct a simple indexing system that contains two levels of memory <ref> [ Malek and Rialle, 1994 ] </ref> : * Memory that contains prototypical cases * Memory that contains instances or real cases The prototype memory is used during retrieval phase instead of the cases memory in order to decrease retrieval time.
Reference: [ Manago et al., 1993 ] <author> M. Manago, K. Althoff, E. Auriol, R. Traphoner, S. Wess, and N. Conruyt anf F. Maurer. </author> <title> Induction and reasoning from cases. </title> <booktitle> In First European Workshop on CBR, </booktitle> <volume> number 1, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: The study of some inductive approaches like ID3/C4 [ Quinlan, 1992 ] have shown that the costs of altering large case bases are very high and lead sometimes to a re-compilation of the complete case base after each alteration [ Bamberger and Goos, 1993 ] <ref> [ Manago et al., 1993 ] </ref> . This shows that using inductive approaches makes retrieval more efficient but it considerably complicates the learning phase because of the sophisticated architecture of the used structure [ Utgoff, 1989 ] . In this paper, we propose a simple and efficient indexing system.
Reference: [ Medsker and Bailey, 1992 ] <author> L.R. Medsker and D.L. Bai-ley. </author> <title> Models and Guidelines for Integrationg Expert Systems and Neural Networks, </title> <booktitle> chapter 8, </booktitle> <pages> pages 153-171. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1992. </year>
Reference-contexts: to the activated prototype class */ Decrease the influence region of the activated unit to exclude the wrong classified example (differen tiation) Re-learn the example to the network This mode of CBR-NN coupling is a preprocessing one because the neural network serves as an indexing system to the CBR system <ref> [ Medsker and Bailey, 1992 ] </ref> . Let's suppose that a new case is presented to the network, the following retrieval procedure is executed: Algorithm .3 Retrieval Procedure If the new case does not fall into an uncertain region then Consider the network response.
Reference: [ Quinlan, 1986 ] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> (1):81- 106, </volume> <year> 1986. </year>
Reference-contexts: C4.5 [ Quinlan, 1992 ] is an inductive method that generates a decision tree classifier from an initial set of cases. It is a descendent of the ID3 algorithm <ref> [ Quinlan, 1986 ] </ref> . Each leaf in the tree indicates a class (or a prototype). A decision node specifies some test to be carried on a single attribute value.
Reference: [ Quinlan, 1992 ] <author> J.R. Quinlan. </author> <title> C4.5. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Building such a hierarchy needs a lot of time, but it can be used very efficiently during the inference process. Now, adding cases is a complex operation, besides it is hard to keep the network optimal as cases are added. The study of some inductive approaches like ID3/C4 <ref> [ Quinlan, 1992 ] </ref> have shown that the costs of altering large case bases are very high and lead sometimes to a re-compilation of the complete case base after each alteration [ Bamberger and Goos, 1993 ] [ Manago et al., 1993 ] . <p> They have two advantages [ Barletta, 1991 ] : * They can automatically analyze the cases to deter mine the best features for distinguishing them. * The cases are organized for retrieval into a hierarchi-cal structure making the retrieval time a logarithmic function of the number of cases. C4.5 <ref> [ Quinlan, 1992 ] </ref> is an inductive method that generates a decision tree classifier from an initial set of cases. It is a descendent of the ID3 algorithm [ Quinlan, 1986 ] . Each leaf in the tree indicates a class (or a prototype).
Reference: [ Reilly et al., 1982 ] <author> D.L. Reilly, L.N. Cooper, and C. El-baum. </author> <title> A neural model for category learning. </title> <booktitle> Biological Cybermetics, </booktitle> <year> 1982. </year>
Reference-contexts: Each hidden network unit represents a prototype. These prototype units are grouped into their corresponding classes, each class identified by a category label. The first implemented model using this method was described in <ref> [ Reilly et al., 1982 ] </ref> commercially known as the Nestor Learning System (NLS). An incremental prototype-based neural network which is based on a " Grow and Learn" (Gal) algorithm is described also in [ Alpaydin, 1991 ] .
Reference: [ Riesbeck and Schank, 1989 ] <editor> C.K. Riesbeck and R.C. Schank. </editor> <title> Inside Case-Based Reasoning. </title> <publisher> Lawrence Erl-baum Associates, publishers, </publisher> <year> 1989. </year>
Reference-contexts: The case adapter then examines the retrieved cases and tries to solve the new problem by adapting these cases [ Aamodt and Plaza, 1994 ] [ Barletta, 1991 ] <ref> [ Riesbeck and Schank, 1989 ] </ref> . 1 This research is supported by the MIX ESPRIT Project-9119 It is obvious that more the retrieval phase is efficient more the overall performances of the CBR system is increased.
Reference: [ Thrun et al., 1991 ] <author> S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S Dzroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michalski, T. Mitchell, P. Pachowicz, Y. Re-ich H. Vafaie, W. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang. </author> <title> The monk's problems a performance comparison of different learning algorihms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: The algorithm allows at each moment to restructure the whole tree and to expand it if possible. To compare our indexing system to C4.5 and ID5R methods we have chosen the MONK's problems that were used to compare different learning algorithm performances <ref> [ Thrun et al., 1991 ] </ref> .
Reference: [ Utgoff, 1989 ] <author> P.E. Utgoff. </author> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> (4):161-186, 1989. 
Reference-contexts: This shows that using inductive approaches makes retrieval more efficient but it considerably complicates the learning phase because of the sophisticated architecture of the used structure <ref> [ Utgoff, 1989 ] </ref> . In this paper, we propose a simple and efficient indexing system. We construct a case hierarchy of two levels of memory. The lower level contains cases organised into groups of similar cases. The upper level contains prototypes, each of which represents a group of cases. <p> C4.5 is not incremental; this means that one needs to build a new decision tree to learn an additional case. ID5R is an incremental method to build a decision tree <ref> [ Utgoff, 1989 ] </ref> . The idea is to maintain sufficient information to compute the E-score for an attribute at a node, making it possible to change the test attribute at a node to one with the lowest E-score. <p> The generalization accuracies registered by PBIS are not as good as the ones registered by ID5R, but comparable to C4.5. In fact, ID5R and PBIS are both incremental. Adding a new case in ID5R is a complex operation because it leads to a whole restructuration of the generated tree <ref> [ Utgoff, 1989 ] </ref> , whereas in PBIS, this is achieved by simply adding the treated case to the suitable memory zone and by modifying only one unit in the neural network (assimilation, accommodation or differentiation of a prototype). 5 Discussion In this paper we show how prototypes are constructed from
References-found: 17

