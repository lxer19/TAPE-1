URL: ftp://ftp.cs.columbia.edu/reports/reports-1996/cucs-023-96.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1996.html
Root-URL: http://www.cs.columbia.edu
Email: E-mail address: henryk@cs.columbia.edu  
Title: Estimating a Largest Eigenvector by Polynomial Algorithms with a Random Start  
Author: Z. Leyk H. Wozniakowski 
Note: E-mail address: zbigniew.leyk@isc.tamu.edu Supported in part by the National Science Foundation and the Air Force Office of Scientific Research. This work was partly done while the author was visiting the Australian National University in 1994.  
Date: May 20, 1996  
Affiliation: Columbia University Computer Science Department  Institute for Scientific Computation Texas A&M University  Department of Computer Science, Columbia University and Institute of Applied Mathematics, University of Warsaw  
Pubnum: Report CUCS-023-96  
Abstract: In [7] and [8], the power and Lanczos algorithms with random start for estimating the largest eigenvalue of an n fi n large symmetric positive definite matrix were analyzed. In this paper we continue this study by estimating an eigenvector corresponding to the largest eigenvalue. We analyze polynomial algorithms using Krylov information for two error criteria: the randomized error and the randomized residual error. For the randomized error, in contrast to [7] and [8], we prove that it is not possible to get distribution-free bounds. In fact, there exists a matrix for which all polynomial algorithms fail for approximating an eigenvector corresponding to the largest eigenvalue as long as the number of steps is less than n. This shows that the problem of estimating such an eigenvector is much harder than the problem of estimating the largest eigenvalue, and that even the random 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. E. P. Box and M. E. Muller. </author> <title> A Note on the Generation of Random Normal Deviates. </title> <journal> Ann. Math. Statist. </journal> <volume> 29 (1958), </volume> <pages> 610-611. </pages>
Reference-contexts: Since usually we do not have any a priori information about the subspace V (A), it seems natural to select the vector b randomly with uniform distribution over the n dimensional unit sphere. Computationally, this can be done as described, e.g., in <ref> [1] </ref> and [6]. This approach has been taken in [7] and [8] for estimating the largest eigenvalue. In this paper, we study the related problem of estimating a largest eigenvector. We consider polynomial algorithms that use Krylov information with a random start b. <p> We have already mentioned the paper [2], where the same problem is studied for the 3 power algorithm and for the randomized error. We stress that the randomized error in [2] is studied in the L p -norm for arbitrary values of p 2 <ref> [1; +1] </ref>. In particular, the results depend on a relation between multiplicities of the two largest eigenvalue and the parameter p. The results deteriorate for large p. In our paper we consider only the case p = 2. <p> This is definitely the case if the vector b is chosen randomly, say, with uniform distribution on the n-dimensional sphere of radius one. The reader may consult <ref> [1] </ref> and [6, p. 130], where it is explained how such a vector can be generated computationally. Then, in fact, the vector b is not orthogonal to all eigenvectors v i (A) with probability 1. In this paper we assume that the initial vectors b are chosen randomly. <p> Since (Q fl (x)) 0 = i=0 i (x) then by (16) j (Q fl (x)) 0 j ffi i=1 i (x)j C; for any x 2 <ref> [0; 1] </ref>; (17) where C = C (ffi; k; x nk+2 ; : : : ; x n ) but C does not depend on x 2 ; : : : ; x nk+1 and b 1 ; : : : ; b n (due to jb i j ffi). <p> We have Q fl (x j ) = Q fl (1) + (Q fl (~ j )) 0 (x j 1); for ~ j 2 <ref> [0; 1] </ref>: Since Q fl (1) = 1 then by (17) j1 Q fl (x j )j Cj1 x j j C Q fl (x j ) 1 C &gt; 0; for j = 2; : : : ; n k + 1: (18) By (18) it follows that n X <p> The proof for k n is given in the proof of Theorem 5. 2 4 Power Algorithm In this section we present bounds for the power algorithm. The randomized error of the power algorithm is analyzed in [2] in the L p -norm with arbitrary p 2 <ref> [1; +1] </ref>. For completeness, we recall the results for p = 2. <p> Therefore the major part of the proof of Theorem 3.2 from [7] can be also applied for the largest eigenvalue problem. This leads to the following estimate on r 2 k , k 0:824 2 n e 4 (k1)fl + 2 fl 8 ; 8 fl 2 <ref> [0; 1] </ref>: (25) p 0:0803 ln (n (k 1) 8 ) then part (a) is trivial since r k 1.
Reference: [2] <author> Gianna M. Del Corso. </author> <title> Estimating an Eigenvector by the Power Method with a Random Start. </title> <note> Submitted for publication, </note> <year> 1995. </year>
Reference-contexts: For k &lt; n, there are no distribution-free bounds for the randomized error of polynomial algorithms. There are, however, bounds dependent on the distribution of eigenvalues of the matrix A. Such bounds are provided for the power algorithm in <ref> [2] </ref> and they depend on the ratio of the two largest eigenvalues and their multiplicities, see Theorem 2. We provide bounds 6:505 k 1 and n p 1 + 1 : for the Lanczos algorithm in Theorem 5. <p> We also provide asymptotic bounds, as well as bounds dependent on the ratio of the two largest eigenvalues and their multiplicities, for both the power and Lanczos algorithms. In Section 6 we briefly report numerical tests. Since numerical tests for the power algorithm may be found in <ref> [2] </ref>, we restrict ourselves to the Lanczos algorithm. We test the Lanczos algorithm for the tridiagonal matrix with 2's on the main diagonal and 1 on the two codiagonals. <p> The tests seems to indicate that upper bounds of Theorems 4 and 5 may be improved by a factor of order ln (k n). We conclude this introduction by a brief remark on related works. We have already mentioned the paper <ref> [2] </ref>, where the same problem is studied for the 3 power algorithm and for the randomized error. We stress that the randomized error in [2] is studied in the L p -norm for arbitrary values of p 2 [1; +1]. <p> We conclude this introduction by a brief remark on related works. We have already mentioned the paper <ref> [2] </ref>, where the same problem is studied for the 3 power algorithm and for the randomized error. We stress that the randomized error in [2] is studied in the L p -norm for arbitrary values of p 2 [1; +1]. In particular, the results depend on a relation between multiplicities of the two largest eigenvalue and the parameter p. The results deteriorate for large p. <p> Now we take matrices A such that their normalized eigenvalues x i for i 2 <ref> [2; n k + 1] </ref>, tend to 1 . That is, we take 2 (0; 1=C) such that j1 x j j for j = 2; : : : ; n k + 1. <p> The proof for k n is given in the proof of Theorem 5. 2 4 Power Algorithm In this section we present bounds for the power algorithm. The randomized error of the power algorithm is analyzed in <ref> [2] </ref> in the L p -norm with arbitrary p 2 [1; +1]. For completeness, we recall the results for p = 2. Theorem 2 [2] Let u pow be the power algorithm defined by (3). (a) For any k, we have sup e ran (u pow ; A; k) = 1 <p> The randomized error of the power algorithm is analyzed in <ref> [2] </ref> in the L p -norm with arbitrary p 2 [1; +1]. For completeness, we recall the results for p = 2. Theorem 2 [2] Let u pow be the power algorithm defined by (3). (a) For any k, we have sup e ran (u pow ; A; k) = 1 n 10 (b) For any symmetric positive definite matrix A, let p; p &lt; n; and q denote the multiplicities of the two largest
Reference: [3] <author> W. Kahan and B. N. Parlett. </author> <title> How Far Should We Go with the Lanczos Process? in Sparse Matrix Computations eds. </title> <editor> J. Bunch and D. Rose, </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> (1976), </year> <pages> 131-144. </pages>
Reference-contexts: The analysis of convergence of the Lanczos algorithm is more complex and some of it may be found in e.g., <ref> [3, 4, 10, 11, 12, 13, 14, 16] </ref>. Convergence of a polynomial algorithm u (A; b; k) depends on distributions of eigenvalues of the matrix A and of the vector b.
Reference: [4] <author> S. Kaniel. </author> <title> Estimates for Some Computational Techniques in Linear Algebra. </title> <journal> Math. Comp. </journal> <volume> 20 (1966), </volume> <pages> 369-378. </pages>
Reference-contexts: The analysis of convergence of the Lanczos algorithm is more complex and some of it may be found in e.g., <ref> [3, 4, 10, 11, 12, 13, 14, 16] </ref>. Convergence of a polynomial algorithm u (A; b; k) depends on distributions of eigenvalues of the matrix A and of the vector b. <p> It is convenient to start with the randomized residual error. Theorem 4 Let u Lan be the Lanczos algorithm defined by (4). (a) For any symmetric positive definite matrix A, let m denote the number of distinct eigenvalues of A. Then for k m, for k 2 <ref> [4; m 1] </ref>, k 1 6:505 ln (n) ! 2 (b) For any symmetric positive definite matrix A, let p &lt; n be the multiplicity of the largest eigenvalue 1 , and let p+1 and n be the second largest and the smallest eigenvalue of A. <p> Then for k m, and for k 2 <ref> [4; m 1] </ref>, 1 k 1 6:505 1 p+1 ln (n) ! 2 e ran (u Lan ; A; k) 1:61 4 p 1 0 1 ( 1 p+1 )=( 1 n ) q 1 (k1)=2 Proof of Theorem 5 The case k m follows easily from Theorem 4.
Reference: [5] <author> E. Kostlan. </author> <title> Statistical complexity of dominant eigenvector calculation. </title> <editor> J. </editor> <booktitle> of Complexity 7 (1991),. </booktitle> <pages> 371-379. </pages>
Reference-contexts: In this case, the power algorithm is globally convergent and a random start is used to improve efficiency. Still even for n = 2, the problem may be hard, see [15]. Finally, we comment on the two papers <ref> [5] </ref> and [17]. In these two papers, the average case setting for estimating a largest eigenvector is analyzed. That is, it is assumed that matrices are distributed according to some probability measure, and the behaviour of algorithms is analyzed by taking the expectation with respect to matrices.
Reference: [6] <author> D. E. Knuth. </author> <booktitle> The Art of Computer Science, </booktitle> <volume> Vol. 2: </volume> <booktitle> Seminumerical Algorithms. 2nd ed. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA., </address> <year> 1981. </year>
Reference-contexts: Since usually we do not have any a priori information about the subspace V (A), it seems natural to select the vector b randomly with uniform distribution over the n dimensional unit sphere. Computationally, this can be done as described, e.g., in [1] and <ref> [6] </ref>. This approach has been taken in [7] and [8] for estimating the largest eigenvalue. In this paper, we study the related problem of estimating a largest eigenvector. We consider polynomial algorithms that use Krylov information with a random start b. <p> This is definitely the case if the vector b is chosen randomly, say, with uniform distribution on the n-dimensional sphere of radius one. The reader may consult [1] and <ref> [6, p. 130] </ref>, where it is explained how such a vector can be generated computationally. Then, in fact, the vector b is not orthogonal to all eigenvectors v i (A) with probability 1. In this paper we assume that the initial vectors b are chosen randomly.
Reference: [7] <author> J. Kuczynski and H. Wozniakowski. </author> <title> Estimating the largest eigenvalue by the power and Lanczos algorithms with a random start. </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 13 (1992),. </volume> <pages> 1094-1122. </pages>
Reference-contexts: Computationally, this can be done as described, e.g., in [1] and [6]. This approach has been taken in <ref> [7] </ref> and [8] for estimating the largest eigenvalue. In this paper, we study the related problem of estimating a largest eigenvector. We consider polynomial algorithms that use Krylov information with a random start b. <p> For the latter problem, the power algorithm with a random start has a sharp bound roughly ln (n)=k, whereas the Lanczos algorithm with a random start has a bound roughly (ln (n)=k) 2 , see <ref> [7] </ref>. Hence, the problem of estimating a largest eigenvector is much harder than the problem of estimating the largest eigenvalue, and even the random start does not help. The negative result for the randomized error holds for k &lt; n. <p> The only difference is that the factor (1 x i ) 2 is replaced by 1 x i . Repeating step by step the proof of Theorem 3.1 of <ref> [7] </ref> with the extra factor 1 x i , we get r 2 s 8 1 ; fi 2 [0; fi fl ]; (21) where fi fl = 1 2=(k + 2). <p> Then fi 2 (0; fi fl ] for n 8. Obviously fi 2k 1=n 2 . Thus, we have r 2 s 8n 1 k 1:12 k : This proves part (a). Part (b) of Theorem 3 follows easily from the proof of part (b) of Theorem 3.1 in <ref> [7] </ref>. We proceed to prove part (c) of Theorem 3. Recall that p and q are multiplicities of the two largest distinct eigenvalues of A. <p> From (20) we can write as k ! +1, r 2 Z x 2k P p+q i i=1 b 2 p+1 i=p+1 b 2 (db) (1 + o (1)) : The integral above is analyzed in the proof of part (c) of Theorem 3.1 in <ref> [7] </ref>. Applying this analysis we obtain the asymptotic bounds of part (c). This completes the proof of Theorem 3. 2 12 5 Lanczos Algorithm We now proceed to the Lanczos algorithm. The analysis of this algorithm is much more complex and we are able to present only upper bounds. <p> The only difference is that instead of the factor (1 x i ) 2 in the above formulas we have 1 x i . Therefore the major part of the proof of Theorem 3.2 from <ref> [7] </ref> can be also applied for the largest eigenvalue problem. <p> Part (b) directly follows from the proof of part (b) of Theorem 3.2 from <ref> [7] </ref>. 2 We now present upper bounds for the randomized error of the Lanczos algorithm. In view of Theorem 1, these bounds must depend on the eigenvalues of the matrix. Theorem 5 Let u Lan be the Lanczos algorithm defined by (4). <p> Hence, e k 1 x p+1 and all estimates of Theorem 5 follow from the estimates of r k in Theorem 4. 2 6 Numerical Tests We tested several matrices with many pseudo-random starting vectors b. Without loss of generality, see <ref> [7] </ref>, we restricted ourselves only to symmetric tridi-agonal matrices. Vectors b were uniformly distributed over the unit sphere of R n , and they were generated in the same way as in [7]. The tests were performed on a Sun-4/75 workstation with the round-off unit of order 10 16 . <p> Without loss of generality, see <ref> [7] </ref>, we restricted ourselves only to symmetric tridi-agonal matrices. Vectors b were uniformly distributed over the unit sphere of R n , and they were generated in the same way as in [7]. The tests were performed on a Sun-4/75 workstation with the round-off unit of order 10 16 . All calculations were done in double precision using the numerical package Meschach [9]. <p> This termination criterion allowed us to compare our results with 16 those reported in <ref> [7] </ref>.
Reference: [8] <author> J. Kuczynski and H. Wozniakowski. </author> <title> Probabilistic bounds on the extremal eigenvalues and condition number by the Lanczos algorithm. </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 15 (1994), </volume> <pages> 672-691. </pages>
Reference-contexts: Computationally, this can be done as described, e.g., in [1] and [6]. This approach has been taken in [7] and <ref> [8] </ref> for estimating the largest eigenvalue. In this paper, we study the related problem of estimating a largest eigenvector. We consider polynomial algorithms that use Krylov information with a random start b.
Reference: [9] <author> Z. Leyk and D. E. Stewart. Meschach: </author> <title> Matrix Computation in C. </title> <booktitle> Proceedings of the CMA vol. 32, </booktitle> <institution> Australian National University, </institution> <year> 1994, </year> <pages> pp. 1-240. </pages>
Reference-contexts: The tests were performed on a Sun-4/75 workstation with the round-off unit of order 10 16 . All calculations were done in double precision using the numerical package Meschach <ref> [9] </ref>. The purpose of the numerical tests was, in particular, to verify the sharpness of the bounds of Theorem 4 and 5.
Reference: [10] <author> C. C. Paige. </author> <title> The Computation of Eigenvalues and Eigenvectors of Very Large Sparse Matrices. </title> <type> Ph. D. Thesis, </type> <institution> University of London, </institution> <year> 1971. </year> <month> 19 </month>
Reference-contexts: The analysis of convergence of the Lanczos algorithm is more complex and some of it may be found in e.g., <ref> [3, 4, 10, 11, 12, 13, 14, 16] </ref>. Convergence of a polynomial algorithm u (A; b; k) depends on distributions of eigenvalues of the matrix A and of the vector b.
Reference: [11] <author> C. C. Paige. </author> <title> Computational Variants of the Lanczos Method for the Eigen--problem. </title> <journal> J. Inst. Math. Appl. </journal> <volume> 10 (1972), </volume> <pages> 373-381. </pages>
Reference-contexts: The analysis of convergence of the Lanczos algorithm is more complex and some of it may be found in e.g., <ref> [3, 4, 10, 11, 12, 13, 14, 16] </ref>. Convergence of a polynomial algorithm u (A; b; k) depends on distributions of eigenvalues of the matrix A and of the vector b.
Reference: [12] <author> B. N. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, N. J., </address> <year> 1980. </year>
Reference-contexts: The analysis of convergence of the Lanczos algorithm is more complex and some of it may be found in e.g., <ref> [3, 4, 10, 11, 12, 13, 14, 16] </ref>. Convergence of a polynomial algorithm u (A; b; k) depends on distributions of eigenvalues of the matrix A and of the vector b.
Reference: [13] <author> Y. Saad. </author> <title> On the Rates of Convergence of the Lanczos and the Block Lanczos Methods. </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 17 (1980), </volume> <pages> 687-706. </pages>
Reference-contexts: The analysis of convergence of the Lanczos algorithm is more complex and some of it may be found in e.g., <ref> [3, 4, 10, 11, 12, 13, 14, 16] </ref>. Convergence of a polynomial algorithm u (A; b; k) depends on distributions of eigenvalues of the matrix A and of the vector b.
Reference: [14] <author> D. S. Scott. </author> <title> Analysis of the Symmetric Lanczos Process. </title> <type> Ph. D. Thesis, </type> <institution> University of California at Berkeley, Berkeley, CA., </institution> <note> Memorandum NCB/ERLM 78/40, </note> <year> 1978. </year>
Reference-contexts: The analysis of convergence of the Lanczos algorithm is more complex and some of it may be found in e.g., <ref> [3, 4, 10, 11, 12, 13, 14, 16] </ref>. Convergence of a polynomial algorithm u (A; b; k) depends on distributions of eigenvalues of the matrix A and of the vector b.
Reference: [15] <author> M. Shub. </author> <title> The geometry and topology of dynamical systems and algorithms for numerical problems. </title> <booktitle> In Proc. of the 1983 Beijing Symposium on Differential Geometry and Differential Equations, </booktitle> <editor> Ed. Liao Shantao. </editor> <publisher> Science Press, </publisher> <address> Beijing, China, </address> <year> 1986. </year>
Reference-contexts: The results deteriorate for large p. In our paper we consider only the case p = 2. The idea of using a random start for estimating eigenvectors by the power algorithm can be found in <ref> [15] </ref>. The author applies the power algorithm to the exponential matrix exp (A) and analyzes convergence to a (not necessarily largest) eigenvector. In this case, the power algorithm is globally convergent and a random start is used to improve efficiency. <p> In this case, the power algorithm is globally convergent and a random start is used to improve efficiency. Still even for n = 2, the problem may be hard, see <ref> [15] </ref>. Finally, we comment on the two papers [5] and [17]. In these two papers, the average case setting for estimating a largest eigenvector is analyzed.
Reference: [16] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford Univ. Press, </publisher> <address> London and New York, </address> <year> 1965. </year>
Reference-contexts: The analysis of convergence of the Lanczos algorithm is more complex and some of it may be found in e.g., <ref> [3, 4, 10, 11, 12, 13, 14, 16] </ref>. Convergence of a polynomial algorithm u (A; b; k) depends on distributions of eigenvalues of the matrix A and of the vector b.
Reference: [17] <author> P. E. Wright. </author> <title> Statistical complexity of the power method for Markov chains. </title> <editor> J. </editor> <booktitle> of Complexity 5 (1989), </booktitle> <pages> 119-143. 20 </pages>
Reference-contexts: In this case, the power algorithm is globally convergent and a random start is used to improve efficiency. Still even for n = 2, the problem may be hard, see [15]. Finally, we comment on the two papers [5] and <ref> [17] </ref>. In these two papers, the average case setting for estimating a largest eigenvector is analyzed. That is, it is assumed that matrices are distributed according to some probability measure, and the behaviour of algorithms is analyzed by taking the expectation with respect to matrices.
References-found: 17

