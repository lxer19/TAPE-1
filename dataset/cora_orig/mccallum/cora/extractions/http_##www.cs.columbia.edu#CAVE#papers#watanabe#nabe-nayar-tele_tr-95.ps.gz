URL: http://www.cs.columbia.edu/CAVE/papers/watanabe/nabe-nayar-tele_tr-95.ps.gz
Refering-URL: http://www.cs.columbia.edu/CAVE/physics-based-vision.html
Root-URL: http://www.cs.columbia.edu
Title: Telecentric Optics for Constant-Magnification Imaging  
Author: Masahiro Watanabe and Shree K. Nayar 
Date: September, 1995  
Address: New York, N.Y. 10027  
Affiliation: Department of Computer Science Columbia University  
Pubnum: CUCS-026-95  
Abstract: y This research was conducted at the Center for Research in Intelligent Systems, Department of Computer Science, Columbia University. It was supported in part by the Production Engineering Research Laboratory, Hitachi, and in part by the David and Lucile Packard Fellwoship. Masahiro Watanabe is with the Production Engineering Research Laboratory, Hitachi. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Born and E. Wolf. </author> <booktitle> Principles of Optics. </booktitle> <address> London:Permagon, </address> <year> 1965. </year>
Reference-contexts: The magnification problem is eliminated in its entirety by the introduction of an optical configuration, referred to as telecentric optics. Though telecentricity has been known for long in optics <ref> [1, 8] </ref>, it has not been exploited in the realm of computational vision. With this optics, magnification remains constant despite focus changes. The attractive feature of this solution is that commercially available lens (used extensively in machine vision) are easily transformed to telecentric ones by adding an extra aperture. <p> This center lies on the ray R which is radiated from point P and passes through the center of the aperture O and refracted by the lens. This ray is called the principal ray <ref> [1, 8] </ref> 1 Here, focus change is modeled as a translation of the sensor plane. <p> The thin lens is just a special case when the two principal planes <ref> [1, 6] </ref> coincides. Figure 1 is easily modified for the compound lens case, by replacing the thin lens with the two principal planes, U and U 0 , of the compound lens, as shown in Figure 3 (a). <p> One can tell this by looking at the exit pupil position in the specification sheet provided by the manufacturer. If the exit pupil position is at 1, the lens is image-side telecentric <ref> [1, 8] </ref>. An example is Fujinon's H12fi10.5A. But this does not mean that this zoom lens has magnification that is invariant to focus change, because zoom lenses usually change focus by complex movements of some of the lens components.
Reference: [2] <author> V. M. Bove, Jr. </author> <title> Entropy-based depth from focus. </title> <journal> Journal of Optical Society of America A, </journal> <volume> 10 </volume> <pages> 561-566, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [14, 4, 2, 15, 18, 13] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Given that two images are all we have in the case of defocus analysis, the magnification variations prove particularly harmful during depth computation. As a result, most investigators of depth from defocus have been forced to vary aperture size <ref> [14, 4, 2, 15, 18] </ref> rather than the focus setting (for example, the distance between the sensor and the lens) to induce defocus variations between the two images. Aperture size changes have the advantage of not introducing magnification variations.
Reference: [3] <author> T. Darrell and K. Wohn. </author> <title> Pyramid based depth from focus. </title> <booktitle> Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 504-509, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Further, the necessity to change two physical parameters (focus and zoom) simultaneously tends to increase errors caused by backlashes in the lens mechanism and variations in lens distortion. An alternative approach to the magnification problem is a computational one, commonly referred to as image warping. Darrell and Wohn <ref> [3] </ref> proposed the use of warping to correct image shifts due to magnification changes caused by focusing. This method is simple and effective for some applications, but can prove computationally intensive for real-time ones. <p> Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [7, 10, 11, 3, 17, 9] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates.
Reference: [4] <author> J. Ens and P. Lawrence. </author> <title> A matrix based method for determining depth from focus. </title> <booktitle> Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 600-609, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [14, 4, 2, 15, 18, 13] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Given that two images are all we have in the case of defocus analysis, the magnification variations prove particularly harmful during depth computation. As a result, most investigators of depth from defocus have been forced to vary aperture size <ref> [14, 4, 2, 15, 18] </ref> rather than the focus setting (for example, the distance between the sensor and the lens) to induce defocus variations between the two images. Aperture size changes have the advantage of not introducing magnification variations.
Reference: [5] <author> B. K. P. Horn. </author> <title> Focusing. </title> <type> Technical Report Memo 160, </type> <institution> AI Lab., Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, USA, </address> <year> 1968. </year>
Reference-contexts: 1 Introduction The problem of magnification variation due to change in focus setting has been one of the interests to machine vision researchers. The classical approach to solving this problem has been to view it as one of camera calibration <ref> [5, 17] </ref>. Willson and Shafer [17] conducted a careful analysis of the interaction between focus and magnification. They proposed a joint calibration approach that measures the relation between zooming and focusing for a given lens.
Reference: [6] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This above fact results in another remarkable property of the telecentric lens: The image brightness stays constant in a telecentric lens, while, in a conventional lens, brightness decreases as the effective focal length d i increases. This becomes clear by examining the image irradiance equation <ref> [6] </ref>: E = L 4 F e where, E is the irradiance of the sensor plane, L is the radiance of the surface in the direction of the lens and F e is the effective F-number. <p> The thin lens is just a special case when the two principal planes <ref> [1, 6] </ref> coincides. Figure 1 is easily modified for the compound lens case, by replacing the thin lens with the two principal planes, U and U 0 , of the compound lens, as shown in Figure 3 (a).
Reference: [7] <author> R. A. Jarvis. </author> <title> A perspective on range finding techniques for computer vision. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 5(2) </volume> <pages> 122-139, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [7, 10, 11, 3, 17, 9] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates.
Reference: [8] <author> R. Kingslake. </author> <title> Optical System Design. </title> <publisher> Academic Press, </publisher> <year> 1983. </year>
Reference-contexts: The magnification problem is eliminated in its entirety by the introduction of an optical configuration, referred to as telecentric optics. Though telecentricity has been known for long in optics <ref> [1, 8] </ref>, it has not been exploited in the realm of computational vision. With this optics, magnification remains constant despite focus changes. The attractive feature of this solution is that commercially available lens (used extensively in machine vision) are easily transformed to telecentric ones by adding an extra aperture. <p> This center lies on the ray R which is radiated from point P and passes through the center of the aperture O and refracted by the lens. This ray is called the principal ray <ref> [1, 8] </ref> 1 Here, focus change is modeled as a translation of the sensor plane. <p> Straightforward geometrical analysis reveals that the ray of light R 0 from any scene point that passes through the center O 0 of aperture A 0 , i.e. the principal ray, emerges parallel to the optical axis on the image side of the lens <ref> [8] </ref>. Furthermore, this parallel ray is the axis of a cone that includes all light rays radiated by the scene point, passed through by A 0 , and intercepted by the lens. <p> One can tell this by looking at the exit pupil position in the specification sheet provided by the manufacturer. If the exit pupil position is at 1, the lens is image-side telecentric <ref> [1, 8] </ref>. An example is Fujinon's H12fi10.5A. But this does not mean that this zoom lens has magnification that is invariant to focus change, because zoom lenses usually change focus by complex movements of some of the lens components.
Reference: [9] <author> A. Krishnan and N. Ahuja. </author> <title> Range estimation from focus using a non-frontal imaging camera. </title> <booktitle> Proc. of AAAI Conf., </booktitle> <pages> pages 830-835, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [7, 10, 11, 3, 17, 9] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates.
Reference: [10] <author> E. Krotkov. </author> <title> Focusing. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 223-237, </pages> <year> 1987. </year>
Reference-contexts: Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [7, 10, 11, 3, 17, 9] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates.
Reference: [11] <author> S. K. Nayar and Y. Nakagawa. </author> <title> Shape from focus: An effective approach for rough surfaces. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(8) </volume> <pages> 824-831, </pages> <month> August </month> <year> 1994. </year> <month> 19 </month>
Reference-contexts: Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [7, 10, 11, 3, 17, 9] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates.
Reference: [12] <author> S. K. Nayar and M. Watanabe. </author> <title> Passive bifocal vision sensor. </title> <type> Technical Report Technical Report, </type> <note> (in preparation), </note> <institution> Dept. of Computer Science, Columbia University, </institution> <address> New York, NY, USA, </address> <month> Oct </month> <year> 1995. </year>
Reference-contexts: In practice, this can be accomplished by using a beam splitter (or for more than two images, a sequence of beam splitters) behind the telecentric lens <ref> [12] </ref>. In a real-time application, all images can be digitized simultaneously and processed in parallel as in [13]. 3 Aperture Placement Although the discussion on telecentricity in section 2 was based on the thin lens model, the results holds true for compound lenses.
Reference: [13] <author> S. K. Nayar, M. Watanabe, and M. Noguchi. </author> <title> Real-time focus range sensor. </title> <booktitle> Proc. of Intl. Conf. on Computer Vision, </booktitle> <pages> pages 995-1001, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [14, 4, 2, 15, 18, 13] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Further, extensive experimentation is conducted to verify the invariance of magnification to defocus in four telecentric lenses that were derived from 2 commonly used commercial lenses. We have successfully incorporated a telecentric lens into a real-time active range sensor that is based on depth from defocus <ref> [13] </ref>. Here, we demonstrate applications of telecentric optics to passive depth from defocus. <p> In practice, this can be accomplished by using a beam splitter (or for more than two images, a sequence of beam splitters) behind the telecentric lens [12]. In a real-time application, all images can be digitized simultaneously and processed in parallel as in <ref> [13] </ref>. 3 Aperture Placement Although the discussion on telecentricity in section 2 was based on the thin lens model, the results holds true for compound lenses. The thin lens is just a special case when the two principal planes [1, 6] coincides. <p> Fujinon's CF12.5A F/1.4 is an example such a lens, which we converted to telecentric by placing an aperture inside and used to develop a real-time focus range sensor <ref> [13] </ref>. In practice, the exact location where the additional aperture needs to be placed can be determined in the following way. In some cases, the lens manufacturer provides information regarding the front focal position, which is customarily denoted as F in the schematic diagram.
Reference: [14] <author> A. Pentland. </author> <title> A new sense for depth of field. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 9(4) </volume> <pages> 523-531, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [14, 4, 2, 15, 18, 13] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Given that two images are all we have in the case of defocus analysis, the magnification variations prove particularly harmful during depth computation. As a result, most investigators of depth from defocus have been forced to vary aperture size <ref> [14, 4, 2, 15, 18] </ref> rather than the focus setting (for example, the distance between the sensor and the lens) to induce defocus variations between the two images. Aperture size changes have the advantage of not introducing magnification variations.
Reference: [15] <author> G. Surya and M. Subbarao. </author> <title> Depth from defocus by changing camera aperture: A spatial domain approach. </title> <booktitle> Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 61-67, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [14, 4, 2, 15, 18, 13] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Given that two images are all we have in the case of defocus analysis, the magnification variations prove particularly harmful during depth computation. As a result, most investigators of depth from defocus have been forced to vary aperture size <ref> [14, 4, 2, 15, 18] </ref> rather than the focus setting (for example, the distance between the sensor and the lens) to induce defocus variations between the two images. Aperture size changes have the advantage of not introducing magnification variations.
Reference: [16] <author> M. Watanabe and S. K. Nayar. </author> <title> Depth from defocus using a texture invariant operator set. </title> <type> Technical Report Technical Report, </type> <note> (in preparation), </note> <institution> Dept. of Computer Science, Columbia University, </institution> <address> New York, NY, USA, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: Briefly, the algorithm we have used here applies a set of filters, that are sensitive to defocus but invariant to object texture, to two defocused images of a scene. (see <ref> [16] </ref> for details). The filter outputs are used to efficiently compute depth at each pixel, producing a full resolution depth map of the scene. The 25mm lens was used in this experiment. The lens aperture was set to F/8.3.
Reference: [17] <author> R. G. Willson and S. A. Shafer. </author> <title> Modeling and calibration of automated zoom lenses. </title> <type> Technical Report CMU-RI-TR-94-03, </type> <institution> The Robotics Institute, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, USA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The problem of magnification variation due to change in focus setting has been one of the interests to machine vision researchers. The classical approach to solving this problem has been to view it as one of camera calibration <ref> [5, 17] </ref>. Willson and Shafer [17] conducted a careful analysis of the interaction between focus and magnification. They proposed a joint calibration approach that measures the relation between zooming and focusing for a given lens. <p> 1 Introduction The problem of magnification variation due to change in focus setting has been one of the interests to machine vision researchers. The classical approach to solving this problem has been to view it as one of camera calibration [5, 17]. Willson and Shafer <ref> [17] </ref> conducted a careful analysis of the interaction between focus and magnification. They proposed a joint calibration approach that measures the relation between zooming and focusing for a given lens. <p> Depth from focus uses a sequence of images taken by incrementing the focus setting in small steps. For each pixel, the focus setting that maximizes image contrast is determined. This in turn can be used to compute the depth of the corresponding scene point <ref> [7, 10, 11, 3, 17, 9] </ref>. Magnification variations due to defocus, however, cause additional image variations in the form of translations and scalings. Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates.
Reference: [18] <author> Y. Xiong and S. A. Shafer. </author> <title> Moment and hypergeometric filters for high precision computation of focus, stereo and optical flow. </title> <type> Technical Report CMU-RI-TR-94-28, </type> <institution> The Robotics Institute, Carnegie Mellon University, Pittsburg, </institution> <address> PA, USA, </address> <month> September </month> <year> 1994. </year> <month> 20 </month>
Reference-contexts: Estimation of image contrast in the presence of these 1 effects will clearly result in erroneous depth estimates. In contrast to depth from focus, depth from defocus uses only two images with different optical settings <ref> [14, 4, 2, 15, 18, 13] </ref>. As it attempts to compute depth from this minimal number of images, it requires all aspects of the image formation process to be precisely modeled. Since the two images taken correspond to different levels of defocus, they are expected to differ in magnification. <p> Given that two images are all we have in the case of defocus analysis, the magnification variations prove particularly harmful during depth computation. As a result, most investigators of depth from defocus have been forced to vary aperture size <ref> [14, 4, 2, 15, 18] </ref> rather than the focus setting (for example, the distance between the sensor and the lens) to induce defocus variations between the two images. Aperture size changes have the advantage of not introducing magnification variations.
References-found: 18

