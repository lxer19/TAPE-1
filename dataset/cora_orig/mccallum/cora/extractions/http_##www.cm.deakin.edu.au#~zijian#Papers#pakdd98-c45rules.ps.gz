URL: http://www.cm.deakin.edu.au/~zijian/Papers/pakdd98-c45rules.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Email: (zijian@deakin.edu.au)  
Title: Scaling Up the Rule Generation of C4.5  
Author: Zijian Zheng 
Address: Geelong Victoria 3217, Australia  
Affiliation: School of Computing and Mathematics Deakin University,  
Note: To appear in Proceedings of PAKDD'98, Berlin: Springer Verlag  
Abstract: C4.5 is the most well-known inductive learning algorithm. It can be used to build decision trees as well as production rules. Production rules are a very common formalism for representing and using knowledge in many real-world domains. C4.5 generates production rules from raw trees. It has been shown that the set of production rules is usually both simpler and more accurate than the decision tree from which the ruleset was formed. This research shows that generating production rules from pruned trees usually results in significantly simpler rulesets than generating rules from raw trees. This reduction in complexity is achieved without reducing prediction accuracies. Furthermore, the new approach uses significantly less induction time than the latter. This paper uses experiments in a wide variety of natural domains to illustrate these points. It also shows that the new method scales up better than the old one in terms of ruleset size, the number of rules, and learning time when the training set size increases. This is an important characteristic for learning algorithms used for data mining.
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J.: </author> <title> Classification And Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth (1984). </publisher>
Reference: <author> Kohavi, R.: </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann (1995) 1137-1143. </publisher>
Reference: <author> Merz, C.J. and Murphy, </author> <title> P.M.: UCI Repository of Machine Learning Databases [http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, </title> <institution> CA: University of California, Department of Information and Computer Science (1997). </institution>
Reference-contexts: Note that the release 8 (Quinlan, 1996) of the C4.5 package with its default option settings is used. 3.1 Experimental Domains and Methods Forty-two natural domains from the UCI machine learning repository <ref> (Merz and Murphy, 1997) </ref> are used. This test suite covers a wide variety of different domains with respect to dataset size, the number of classes, the number of attributes, and types of attributes.
Reference: <author> Quinlan, J.R.: </author> <title> Generating production rules from decision trees. </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann (1987a) 304-307. </publisher>
Reference-contexts: Section 3 reports experiments to show the advantages of generating rules from pruned trees over generating rules from raw trees. Finally, Section 4 draws conclusions. 2 Generating Rules from Decision Trees C4.5rules <ref> (Quinlan, 1987a) </ref> in the C4.5 package (Quinlan, 1993) generates production rules from decision trees. Given a decision tree, C4.5rules derives a set of production rules based on the same training set used when building the tree. The process consists of two stages.
Reference: <author> Quinlan, J.R.: </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies 27 (1987b) 221-234. </journal>
Reference-contexts: Many inductive learning algorithms build decision trees. C4.5 (Quinlan, 1993) is one of the most commonly used decision tree learning algorithms. It is very efficient. C4.5 first builds a large decision tree (called a raw tree). It, then, applies pessimistic pruning <ref> (Quinlan, 1987b) </ref> to prune the raw tree back to overcome the overfitting problem of the raw tree (Quinlan, 1993). The pruned tree is smaller and often more accurate than the raw tree (Quinlan, 1987b; 1993).
Reference: <author> Quinlan, J.R.: C4.5: </author> <title> Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann (1993). </publisher>
Reference-contexts: 1 Introduction Data mining usually deals with very large datasets. In addition to being highly accurate, the mined knowledge is expected to be easy to understand and use. Moreover, learning algorithms for data mining should be relatively fast. Many inductive learning algorithms build decision trees. C4.5 <ref> (Quinlan, 1993) </ref> is one of the most commonly used decision tree learning algorithms. It is very efficient. C4.5 first builds a large decision tree (called a raw tree). <p> <ref> (Quinlan, 1993) </ref> is one of the most commonly used decision tree learning algorithms. It is very efficient. C4.5 first builds a large decision tree (called a raw tree). It, then, applies pessimistic pruning (Quinlan, 1987b) to prune the raw tree back to overcome the overfitting problem of the raw tree (Quinlan, 1993). The pruned tree is smaller and often more accurate than the raw tree (Quinlan, 1987b; 1993). <p> Both decision trees and production rules are commonly used to represent and process knowledge in data mining. They are understandable if they 1 C4.5 can transform multiple decision trees for the same task into a set of rules <ref> (Quinlan, 1993) </ref>, but we focus on generating rules from a single tree in this paper. 1 are not too large. Production rules are even more commonly used in real-world domains. It is known that the rule generation of C4.5 is relatively slower compared with the tree generation. <p> As a result, some good rules could be eliminated from the search space of the rule generation by pruning the raw tree. However, pruning only deletes tests whose elimination does not significantly reduce the estimated accuracy of the decision tree <ref> (Quinlan, 1993) </ref>. Therefore, we expect that pruned trees should provide a search space not significantly worse than the space provided by the corresponding raw trees for rule generation in terms of prediction accuracy of production rules to be generated. <p> Section 3 reports experiments to show the advantages of generating rules from pruned trees over generating rules from raw trees. Finally, Section 4 draws conclusions. 2 Generating Rules from Decision Trees C4.5rules (Quinlan, 1987a) in the C4.5 package <ref> (Quinlan, 1993) </ref> generates production rules from decision trees. Given a decision tree, C4.5rules derives a set of production rules based on the same training set used when building the tree. The process consists of two stages. First, individual rules are extracted from a decision tree. <p> The reason is as follows. We have mentioned before that the rule generation of C4.5 consists of two steps: creating individual rules from paths of a tree, and finding a subset of rules from rules derived in the first step based on the MDL principle <ref> (Quinlan, 1993) </ref>. The second step is usually more time-consuming than the first step. In the second step, for each class, C4.5 searches for the best subset of rules from those rules that predict this class.
Reference: <author> Quinlan, J.R.: </author> <title> MDL and categorical theories (continued). </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <address> San Francisco, CA: Morgan Kauf-mann (1995) 464-470. </address>
Reference-contexts: Error rate curves exhaustive search. Otherwise, it uses a series of greedy searches <ref> (Quinlan, 1995) </ref>. In the Sick domain, when the training set size is larger than 2400, the number of rules for a single class generated by C4.5rules (p) in the first step is often just less than 10, while it is often just more than 10 for C4.5rules (r).
Reference: <author> Quinlan, J.R.: </author> <title> Improved use of continuous attributes in C4.5. Journal of Artificial Intelligence Research 4 (1996) 77-90. This article was processed using the L a T E X macro package with LLNCS style 12 </title>
Reference-contexts: The computational requirement of building a decision tree, generating rules from a raw tree, and generating rules from a pruned tree is measured using CPU seconds by running C4.5, C4.5rules, and modified C4.5rules respectively on a SUN SPARCstation 5. Note that the release 8 <ref> (Quinlan, 1996) </ref> of the C4.5 package with its default option settings is used. 3.1 Experimental Domains and Methods Forty-two natural domains from the UCI machine learning repository (Merz and Murphy, 1997) are used.
References-found: 8

