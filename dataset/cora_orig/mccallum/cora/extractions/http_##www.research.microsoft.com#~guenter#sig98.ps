URL: http://www.research.microsoft.com/~guenter/sig98.ps
Refering-URL: http://www.research.microsoft.com/~guenter/
Root-URL: http://www.research.microsoft.com
Title: Making Faces  
Author: Brian Guenter Cindy Grimm Daniel Wood Henrique Malvar Fredrick Pighin 
Keyword: CR Categories: I.3.7 [Computer Graphics]: Three Dimensional Graphics and Realism: Animation; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling  
Affiliation: Microsoft Corporation University of Washington  
Abstract: We have created a system for capturing both the three-dimensional geometry and color and shading information for human facial expressions. We use this data to reconstruct photorealistic, 3D animations of the captured expressions. The system uses a large set of sampling points on the face to accurately track the three dimensional deformations of the face. Simultaneously with the tracking of the geometric data, we capture multiple high resolution, registered video images of the face. These images are used to create a texture map sequence for a three dimensional polygonal face model which can then be rendered on standard 3D graphics hardware. The resulting facial animation is surprisingly life-like and looks very much like the original live performance. Separating the capture of the geometry from the texture images eliminates much of the variance in the image data due to motion, which increases compression ratios. Although the primary emphasis of our work is not compression we have investigated the use of a novel method to compress the geometric data based on principal components analysis. The texture sequence is compressed using an MPEG4 video codec. Animations reconstructed from 512x512 pixel textures look good at data rates as low as 240 Kbits per second. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Beier, T., and Neely, S. </author> <title> Feature-based image metamorphosis. </title> <booktitle> In Computer Graphics (SIGGRAPH '92 Proceedings) (July 1992), </booktitle> <editor> E. E. Catmull, Ed., </editor> <volume> vol. 26, </volume> <pages> pp. 35-42. </pages>
Reference-contexts: A texture map generated using this parametrization is shown on the left of Figure 10. We specify a set of line pairs and warp the texture coordinates using the technique described by Beier and Neely <ref> [1] </ref>. This parametrization results in the texture map shown on the right of Figure 10. Only the front of the head is textured with data from the six video streams. Next we create the geometry map containing a mesh location for each texel.
Reference: [2] <author> Bregler, C., Covell, M., and Slaney, M. </author> <title> Video rewrite: Driving visual speech with audio. </title> <booktitle> Computer Graphics 31, </booktitle> <month> 2 (Aug. </month> <year> 1997), </year> <pages> 353-361. </pages>
Reference-contexts: The work of Williams [15] is most similar to ours except that he used a single static texture image of a real person's face and tracked points only in 2D. The work of Bregler et al <ref> [2] </ref> is somewhat less related. They use speech recognition to locate visemes 1 in a video of a person talking and then synthesize new video, based on the original video sequence, for the mouth and jaw region of the face to correspond with synthetic utterances.
Reference: [3] <author> Cassell, J., Pelachaud, C., Badler, N., Steed-man, M., Achorn, B., Becket, T., Douville, B., Prevost, S., and Stone, M. </author> <title> Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. </title> <booktitle> Computer Graphics 28, </booktitle> <month> 2 (Aug. </month> <year> 1994), </year> <pages> 413-420. </pages>
Reference-contexts: Both the time varying texture created from the video streams and the accurate reproduction of the 3D face structure contribute to the believability of the resulting animation. Our system differs from much previous work in facial animation, such as that of Lee [10], Waters [14], and Cas-sel <ref> [3] </ref>, in that we are not synthesizing animations using a physical or procedural model of the face. Instead, we capture facial movements in three dimensions and then replay them. The systems of [10], [14] are designed to make it relatively easy to animate facial expression manually. The system of [3] is <p> Cas-sel <ref> [3] </ref>, in that we are not synthesizing animations using a physical or procedural model of the face. Instead, we capture facial movements in three dimensions and then replay them. The systems of [10], [14] are designed to make it relatively easy to animate facial expression manually. The system of [3] is designed to automatically create a dialog rather than faithfully reconstruct a particular person's facial expression. The work of Williams [15] is most similar to ours except that he used a single static texture image of a real person's face and tracked points only in 2D.
Reference: [4] <author> DeCarlo, D., and Metaxas, D. </author> <title> The integration of optical flow and deformable models with applications to human face shape and motion estimation. </title> <booktitle> Proceedings CVPR (1996), </booktitle> <pages> 231-238. </pages>
Reference-contexts: They do not create a three dimensional face model nor do they vary the expression on the remainder of the face. Since we are only concerned with capturing and reconstructing facial performances out work is unlike that of [5] which attempts to recognize expressions or that of <ref> [4] </ref> which can track only a limited set of facial expressions. An obvious application of this new method is the creation of believable virtual characters for movies and television. Another application is the construction of a flexible type of video compression.
Reference: [5] <author> Essa, I., and Pentland, A. </author> <title> Coding, analysis, interpretation and recognition of facial expressions. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 19, </journal> <volume> 7 (1997), </volume> <pages> 757-763. </pages>
Reference-contexts: They do not create a three dimensional face model nor do they vary the expression on the remainder of the face. Since we are only concerned with capturing and reconstructing facial performances out work is unlike that of <ref> [5] </ref> which attempts to recognize expressions or that of [4] which can track only a limited set of facial expressions. An obvious application of this new method is the creation of believable virtual characters for movies and television. Another application is the construction of a flexible type of video compression.
Reference: [6] <author> Faugeras, O. </author> <title> Three-dimensional computer vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: Each of the six cameras was individually calibrated to determine its intrinsic and extrinsic parameters and to correct for lens distortion. The details of the calibration process are not germane to this paper but the interested reader can find a good overview of the topic in <ref> [6] </ref> as well as an extensive bibliography. We glued 182 dots of six different colors onto the actress' face. The dots were arranged so that dots of the same color were as far apart as possible from each other and followed the contours of the face.
Reference: [7] <author> Fischler, M. A., and Booles, R. C. </author> <title> Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. </title> <journal> Communications of the ACM 24, </journal> <month> 6 (Aug. </month> <year> 1981), </year> <pages> 381-395. </pages>
Reference-contexts: With the large number of fiducials we have placed on the face false matches are also quite likely and these must be detected and removed. We used ray tracing in combination with a RANSAC <ref> [7] </ref> like algorithm to establish fiducial correspondence and to compute accurate 3D dot positions. This algorithm is robust to occlusion and to false matches as well. First, all potential point correspondences between cameras are generated.
Reference: [8] <author> Hoppe, H. </author> <title> Progressive meshes. </title> <booktitle> In SIGGRAPH 96 Conference Proceedings (Aug. </booktitle> <year> 1996), </year> <editor> H. Rushmeier, Ed., </editor> <booktitle> Annual Conference Series, ACM SIGGRAPH, </booktitle> <publisher> Addison Wesley, </publisher> <pages> pp. 99-108. </pages> <address> held in New Orleans, Louisiana, </address> <month> 04-09 August </month> <year> 1996. </year>
Reference-contexts: We map the teeth and tongue onto these polygons when the mouth is open. We reduced the number of polygons in the mesh from approximately 460; 000 to 4800 using Hoppe's simplification method <ref> [8] </ref>. 4.2 Moving the mesh The vertices are moved by a linear combination of the offsets of the nearest dots (refer to Equation 1).
Reference: [9] <author> Horn, B. K. P. </author> <title> Closed-form solution of absolute orientation using unit quaternions. </title> <journal> Journal of the Optical Society of America 4, </journal> <month> 4 (Apr. </month> <year> 1987). </year>
Reference-contexts: We use the method described in <ref> [9] </ref> to find the exact transform, T , between the two sets of dots.
Reference: [10] <author> Lee, Y., Terzopoulos, D., and Waters, K. </author> <title> Realistic modeling for facial animation. </title> <booktitle> Computer Graphics 29, </booktitle> <month> 2 (July </month> <year> 1995), </year> <pages> 55-62. </pages>
Reference-contexts: Both the time varying texture created from the video streams and the accurate reproduction of the 3D face structure contribute to the believability of the resulting animation. Our system differs from much previous work in facial animation, such as that of Lee <ref> [10] </ref>, Waters [14], and Cas-sel [3], in that we are not synthesizing animations using a physical or procedural model of the face. Instead, we capture facial movements in three dimensions and then replay them. The systems of [10], [14] are designed to make it relatively easy to animate facial expression manually. <p> differs from much previous work in facial animation, such as that of Lee <ref> [10] </ref>, Waters [14], and Cas-sel [3], in that we are not synthesizing animations using a physical or procedural model of the face. Instead, we capture facial movements in three dimensions and then replay them. The systems of [10], [14] are designed to make it relatively easy to animate facial expression manually. The system of [3] is designed to automatically create a dialog rather than faithfully reconstruct a particular person's facial expression. <p> However, overall the image is extremely stable. In retrospect, a mesh constructed by hand with the correct geometry and then fit to the cyberware data <ref> [10] </ref> would be much simpler and possibly reduce some of the polygo-nization artifacts. Another implementation artifact that becomes most visible when the head is viewed near profile is that the teeth and tongue appear slightly distorted. This is because we do not use correct 3D models to represent them.
Reference: [11] <author> Pighin, F., Auslander, J., Lishinski, D., Szeliski, R., and Salesin, D. </author> <title> Realistic facial animation using image based 3d morphing. </title> <type> Tech. Report TR-97-01-03, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, Wa, </address> <year> 1997. </year>
Reference-contexts: However, the time required for this task is not critical as the geometry map need only be created once. Next we create preliminary texture maps for frame f one for each camera. This is a modified version of the technique described in <ref> [11] </ref>. To create the texture map for camera c, we begin by deforming the mesh into its frame f position. Then, for each texel, we get its mesh location, (k; fi 1 ; fi 2 ), from the geometry map.
Reference: [12] <author> Sch urmann, J. </author> <title> Pattern Classification: A Unified View of Statistical and Neural Approaches. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: This produces an image of the face with black holes where the fiducials were. The color classifier is a discrete approximation to a nearest neighbor classifier <ref> [12] </ref>. In a nearest neighbor classifier the item to be classified is given the label of the closest item in the training set, which in our case is the color data contained in the color class images.
Reference: [13] <author> Strang. </author> <title> Linear Algebra and its Application. </title> <publisher> HBJ, </publisher> <year> 1988. </year>
Reference-contexts: This re duction in residual error can be great enough to compensate for the overhead bits of the basis vectors. The principal components can be computed using the singular value decomposition (SVD) <ref> [13] </ref>. Efficient implementations of this algorithm are widely available.
Reference: [14] <author> Waters, K. </author> <title> A muscle model for animating three-dimensional facial expression. </title> <booktitle> In Computer Graphics (SIGGRAPH '87 Proceedings) (July 1987), </booktitle> <editor> M. C. Stone, Ed., </editor> <volume> vol. 21, </volume> <pages> pp. 17-24. </pages>
Reference-contexts: Both the time varying texture created from the video streams and the accurate reproduction of the 3D face structure contribute to the believability of the resulting animation. Our system differs from much previous work in facial animation, such as that of Lee [10], Waters <ref> [14] </ref>, and Cas-sel [3], in that we are not synthesizing animations using a physical or procedural model of the face. Instead, we capture facial movements in three dimensions and then replay them. The systems of [10], [14] are designed to make it relatively easy to animate facial expression manually. <p> from much previous work in facial animation, such as that of Lee [10], Waters <ref> [14] </ref>, and Cas-sel [3], in that we are not synthesizing animations using a physical or procedural model of the face. Instead, we capture facial movements in three dimensions and then replay them. The systems of [10], [14] are designed to make it relatively easy to animate facial expression manually. The system of [3] is designed to automatically create a dialog rather than faithfully reconstruct a particular person's facial expression.
Reference: [15] <author> Williams, L. </author> <title> Performance-driven facial animation. </title> <booktitle> Computer Graphics 24, </booktitle> <month> 2 (Aug. </month> <year> 1990), </year> <month> 235-242. </month> <title> to bottom: Face with dots, dots replaced with low frequency skin texture, high frequency skin texture added, </title> <publisher> hue clamped. </publisher>
Reference-contexts: The systems of [10], [14] are designed to make it relatively easy to animate facial expression manually. The system of [3] is designed to automatically create a dialog rather than faithfully reconstruct a particular person's facial expression. The work of Williams <ref> [15] </ref> is most similar to ours except that he used a single static texture image of a real person's face and tracked points only in 2D. The work of Bregler et al [2] is somewhat less related.
References-found: 15

