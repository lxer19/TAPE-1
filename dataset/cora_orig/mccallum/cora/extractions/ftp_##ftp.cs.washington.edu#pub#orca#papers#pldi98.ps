URL: ftp://ftp.cs.washington.edu/pub/orca/papers/pldi98.ps
Refering-URL: http://www.cs.washington.edu/research/zpl/papers/abstracts/pldi98.html
Root-URL: 
Email: fechris,snyderg@cs.washington.edu, lin@cs.utexas.edu  
Title: The Implementation and Evaluation of Fusion and Contraction in Array Languages  
Author: E Christopher Lewis Calvin Lin Lawrence Snyder 
Address: Seattle, WA 98195-2350 USA  Austin, TX 78712 USA  
Affiliation: University of Washington,  University of Texas,  
Abstract: Array languages such as Fortran 90, HPF and ZPL have many benefits in simplifying array-based computations and expressing data parallelism. However, they can suffer large performance penalties because they introduce intermediate arrays|both at the source level and during the compilation process|which increase memory usage and pollute the cache. Most compilers address this problem by simply scalar-izing the array language and relying on a scalar language compiler to perform loop fusion and array contraction. We instead show that there are advantages to performing a form of loop fusion and array contraction at the array level. This paper describes this approach and explains its advantages. Experimental results show that our scheme typically yields runtime improvements of greater than 20% and sometimes up to 400%. In addition, it yields superior memory use when compared against commercial compilers and exhibits comparable memory use when compared with scalar languages. We also explore the interaction between these transformations and communication optimizations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jeanne C. Adams, Walter S. Brainerd, Jeanne T. Martin, Brian T. Smith, and Jerrold L. Wagener. </author> <title> Fortran 90 Handbook. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Array languages such as Fortran 90 (F90) <ref> [1] </ref>, High Performance Fortran (HPF) [13] and ZPL [24] have become important vehicles for expressing data parallelism. Though they simplify the specification of array-based calculations, they also present a potential problem: Large temporary arrays may need to be introduced, either by the programmer or by the compiler.
Reference: [2] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum, R. Fatoohi, S. Fineberg, P. Frederickson, T. Lasinski, R. Schreiber, H. Simon, V. Venkatakrish-nan, and S. Weeratunga. </author> <title> The NAS parallel benchmarks (94). </title> <type> Technical report, RNR Technical Report RNR-94-007, </type> <month> March </month> <year> 1994. </year>
Reference-contexts: Finally, we evaluate how their interaction with communication optimizations effect performance. The benchmark programs we use to evaluate our transformations represent typical parallel array language programs. The SP application and EP kernel belong to the NAS parallel benchmark suite <ref> [2, 3] </ref>. SP solves sets of uncoupled scalar pentadiagonal systems of equations; it is representative of portions of CFD codes. EP generates pairs of Gaussian random deviates, and it is considered "embarrass-ingly parallel." EP characterizes the peak realizable FLOPS of a parallel machine.
Reference: [3] <author> David Bailey, Tim Harris, William Saphir, Rob van der Wi jngaart, Alex Woo, and Maurice Yarrow. </author> <title> The NAS parallel benchmarks 2.0. </title> <type> Technical report, NAS Report NAS-95-020, </type> <month> December </month> <year> 1995. </year>
Reference-contexts: Finally, we evaluate how their interaction with communication optimizations effect performance. The benchmark programs we use to evaluate our transformations represent typical parallel array language programs. The SP application and EP kernel belong to the NAS parallel benchmark suite <ref> [2, 3] </ref>. SP solves sets of uncoupled scalar pentadiagonal systems of equations; it is representative of portions of CFD codes. EP generates pairs of Gaussian random deviates, and it is considered "embarrass-ingly parallel." EP characterizes the peak realizable FLOPS of a parallel machine.
Reference: [4] <author> Steve Carr and Ken Kennedy. </author> <title> Scalar replacement in the presence of conditional control flow. </title> <journal> Software Practice and Experience, </journal> <volume> 24(1) </volume> <pages> 51-77, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: These conditions ensure that all references to x will appear in a single loop nest upon scalarization, and there will be no loop carried dependences due to x. The latter condition may be relaxed when the dependence is along a dimension of the array that is not distributed <ref> [4] </ref>, but here we assume that all dimensions are distributed. 1 This terminology is borrowed from Gao et al. [12], who considered a similar problem. <p> Furthermore, it is unclear what the algorithm does when a potentially contractible array is consumed by multiple loop nests. Our collective scheme performs reversal, interchange and fusion simultaneously to enable contraction. Carr and Kennedy recognized the importance of keeping array values in scalars through scalar replacement <ref> [4] </ref>, which is similar to array contraction in that some array references become scalar references, but array allocation is not eliminated (i.e., memory usage is not reduced).
Reference: [5] <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tseng. </author> <title> Compiler optimizations for improved data locality. </title> <booktitle> In Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> October 1994. San Jose, CA. </address>
Reference-contexts: Their focus is in recognizing the opportunity in a scalar loop nest, while ours is in enabling the opportunity in an array language compiler via statement fusion. Many techniques for improving locality by loop transformations have appeared in the literature <ref> [5, 16, 19, 25] </ref>. Much of this work addresses the issue of managing the conflicting goals of improving locality without sacrificing parallelism.
Reference: [6] <author> B. Chamberlain, S. Choi, E Lewis, C. Lin, L. Snyder, and W. D. Weathersby. Factor-join: </author> <title> A unique approach to compiling array languages for parallel machines. </title> <editor> In David Sehr, Uptal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Proceedings of the Ninth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 481-500. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1996. </year>
Reference-contexts: Our work supports earlier claims that there are performance benefits to performing analyses and transformations at the array level <ref> [6, 21] </ref>. In particular, this paper makes the following contributions. We explain how fusion and contraction can be performed at the array level. We refer to the former as statement fusion because array statements, not loops, are the fused entities.
Reference: [7] <author> Bradford L. Chamberlain, Sung-Eun Choi, E Christopher Lewis, Calvin Lin, Lawrence Snyder, and W. Derrick Weath-ersby. </author> <title> ZPL's WYSIWYG performance model. </title> <booktitle> To appear in Third International Workshop on High-Level Parallel Programming Models and Supportive Environments, </booktitle> <year> 1998. </year>
Reference-contexts: Furthermore, the normal form serves as an effective internal representation when compiling for parallel machines because it makes the alignment of arrays explicit. All array references are perfectly aligned except for vector offsets, so normalized statements will compile to highly efficient parallel code <ref> [7] </ref>. Compiler generated communication primitives need not be normalized because they are not candidates for fusion or contraction.
Reference: [8] <author> Bradford L. Chamberlain, Sung-Eun Choi, E Christopher Lewis, Calvin Lin, Lawrence Snyder, and W. Derrick Weath-ersby. </author> <title> The case for high level parallel programming in ZPL. </title> <note> To appear in IEEE Computational Science and Engineering, </note> <year> 1998. </year>
Reference-contexts: Though we discuss only the relative effect of these transformations, other studies have shown that the ZPL compiler produces code that performs within 10% of hand coded C plus message passing and generally better than HPF <ref> [8, 17, 18, 20] </ref>.
Reference: [9] <author> Sung-Eun Choi and Lawrence Snyder. </author> <title> Quantifying the effect of communication optimizations. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: To achieve efficient parallel execution, compilers must often perform aggressive communication optimizations <ref> [9] </ref>, such as redundancy elimination, message combining and pipelining. In some cases, these communication optimizations are at odds with fusion for contraction.
Reference: [10] <author> W. Crowley, C. P. Hendrickson, and T. I. Luby. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID-17715, </type> <institution> Lawrence Livermore Laboratory, </institution> <year> 1978. </year>
Reference-contexts: EP generates pairs of Gaussian random deviates, and it is considered "embarrass-ingly parallel." EP characterizes the peak realizable FLOPS of a parallel machine. Tomcatv is a SPEC CFP95 benchmark that performs vectorized mesh generation. The Simple code solves hydrodynamics and heat conduction equations by finite difference methods <ref> [10] </ref>. The Fibro application uses mathematical models of biological patterns to simulate the dynamic structure of fibroblasts [11]. We use the Cray T3E, IBM SP-2 and Intel Paragon in our evaluation. The T3E is a distributed shared memory machine, while the other two are message passing distributed memory machines.
Reference: [11] <author> Marios D. Dikaiakos, Calvin Lin, Daphne Manoussaki, and Diana E. Woodward. </author> <title> The portable parallel implementation of two novel mathematical biology algorithms in ZPL. </title> <booktitle> In Ninth International Conference on Supercomputing, </booktitle> <year> 1995. </year>
Reference-contexts: Tomcatv is a SPEC CFP95 benchmark that performs vectorized mesh generation. The Simple code solves hydrodynamics and heat conduction equations by finite difference methods [10]. The Fibro application uses mathematical models of biological patterns to simulate the dynamic structure of fibroblasts <ref> [11] </ref>. We use the Cray T3E, IBM SP-2 and Intel Paragon in our evaluation. The T3E is a distributed shared memory machine, while the other two are message passing distributed memory machines.
Reference: [12] <author> G. Gao, R. Olsen, V. Sarkar, and R. Thekkath. </author> <title> Collective loop fusion for array contraction. </title> <editor> In Uptal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Proceedings of the Fifth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 281-295. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: The latter condition may be relaxed when the dependence is along a dimension of the array that is not distributed [4], but here we assume that all dimensions are distributed. 1 This terminology is borrowed from Gao et al. <ref> [12] </ref>, who considered a similar problem. See Section 6. 3 3 Problem There are two reasons to perform statement fusion: to enable the elimination of arrays by contraction and to improve utilization of the data cache by exploiting inter-statement reuse. <p> They target multiprocessors and exploit pipelining by executing producer and consumer loops on different processors, so they are free to ignore all but flow dependences. Because we instead distribute iteration spaces, preservation of all types of dependences is critical to our solution. Gao et al. <ref> [12] </ref> describe another technique for loop fusion based on a maxflow algorithm. The technique requires its input loop nests to be identically controlled, and it does not perform loop reversal nor interchange to enable additional fusion.
Reference: [13] <author> High Performance Fortran Forum. </author> <title> High Performance For tran Langauge Specification, </title> <note> Version 2.0. </note> <month> January </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Array languages such as Fortran 90 (F90) [1], High Performance Fortran (HPF) <ref> [13] </ref> and ZPL [24] have become important vehicles for expressing data parallelism. Though they simplify the specification of array-based calculations, they also present a potential problem: Large temporary arrays may need to be introduced, either by the programmer or by the compiler.
Reference: [14] <author> Gwan Hwan Hwang, Jenq Keun Lee, and Dz Ching Ju. </author> <title> An array operation synthesis scheme to optimize Fortran 90 programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Thus, these results support our claim that these optimizations for array languages should be performed at the array level. 6 Related Work The problem of optimizing array languages at the array level has recently received attention by others. Hwang et al. describe a scheme for array operation synthesis <ref> [14] </ref>. Multiple instances of element-wise F90 array operations such as MERGE, CSHIFT, and TRANSPOSE are combined into a single operation, reducing data movement and intermediate storage. Their work does not address the inter-statement intermediate array problem except to substitute an intermediate array's use by its definition.
Reference: [15] <author> Dz-Ching Ju. </author> <title> The Optimization and Parallelization of Ar ray Language Programs. </title> <type> PhD thesis, </type> <institution> University of Texas-Austin, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Multiple instances of element-wise F90 array operations such as MERGE, CSHIFT, and TRANSPOSE are combined into a single operation, reducing data movement and intermediate storage. Their work does not address the inter-statement intermediate array problem except to substitute an intermediate array's use by its definition. This statement merge optimization <ref> [15] </ref> enables more operation synthesis, but it is not always possible, and it potentially introduces redundant computation and increases overall program execution time. Roth and Kennedy have independently developed a similar array based data dependence representation for F90, and they describe its use in scalarization [21].
Reference: [16] <author> Ken Kennedy and Kathryn S. McKinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 323-334, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Their focus is in recognizing the opportunity in a scalar loop nest, while ours is in enabling the opportunity in an array language compiler via statement fusion. Many techniques for improving locality by loop transformations have appeared in the literature <ref> [5, 16, 19, 25] </ref>. Much of this work addresses the issue of managing the conflicting goals of improving locality without sacrificing parallelism.
Reference: [17] <author> C. Lin, L. Snyder, R. Anderson, B. Chamberlain, S. Choi, G. Forman, E. Lewis, and W. D. Weathersby. </author> <title> ZPL vs. HPF: A comparison of performance and programming style. </title> <type> Technical Report 95-11-05, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: Though we discuss only the relative effect of these transformations, other studies have shown that the ZPL compiler produces code that performs within 10% of hand coded C plus message passing and generally better than HPF <ref> [8, 17, 18, 20] </ref>.
Reference: [18] <author> Calvin Lin and Lawrence Snyder. </author> <title> SIMPLE performance re sults in ZPL. </title> <editor> In Keshav Pingali, Uptal Banerjee, David Gel-ernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 361-375. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Though we discuss only the relative effect of these transformations, other studies have shown that the ZPL compiler produces code that performs within 10% of hand coded C plus message passing and generally better than HPF <ref> [8, 17, 18, 20] </ref>.
Reference: [19] <author> Naraig Manjikian and Tarek S. Abdelrahman. </author> <title> Fusion of loops for parallelism and locality. </title> <journal> IEEE transactions on parallel and distributed systems, </journal> <volume> 8(2) </volume> <pages> 193-209, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: Their focus is in recognizing the opportunity in a scalar loop nest, while ours is in enabling the opportunity in an array language compiler via statement fusion. Many techniques for improving locality by loop transformations have appeared in the literature <ref> [5, 16, 19, 25] </ref>. Much of this work addresses the issue of managing the conflicting goals of improving locality without sacrificing parallelism.
Reference: [20] <author> Ton A. Ngo. </author> <title> The Role of Performance Models in Paral lel Programming and Languages. </title> <type> PhD thesis, </type> <institution> University of Washington, Department of Computer Science and Engineering, </institution> <year> 1997. </year>
Reference-contexts: Though we discuss only the relative effect of these transformations, other studies have shown that the ZPL compiler produces code that performs within 10% of hand coded C plus message passing and generally better than HPF <ref> [8, 17, 18, 20] </ref>.
Reference: [21] <author> Gerald Roth and Ken Kennedy. </author> <title> Dependence analysis of For tran90 array syntax. </title> <booktitle> In Proceedings of the Int'l Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA'96), </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: Our work supports earlier claims that there are performance benefits to performing analyses and transformations at the array level <ref> [6, 21] </ref>. In particular, this paper makes the following contributions. We explain how fusion and contraction can be performed at the array level. We refer to the former as statement fusion because array statements, not loops, are the fused entities. <p> This statement merge optimization [15] enables more operation synthesis, but it is not always possible, and it potentially introduces redundant computation and increases overall program execution time. Roth and Kennedy have independently developed a similar array based data dependence representation for F90, and they describe its use in scalarization <ref> [21] </ref>. They do not address the fusion for contraction problem. Loop fusion in the context of scalar programming languages such as Fortran 77 is well understood [26]. Though most work only considers pairwise fusion, some research addresses collective loop fusion, as we do.
Reference: [22] <author> Vivek Sarkar and Guang R. Gao. </author> <title> Optimization of array accesses by collective loop transformations. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 194-205, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: They do not address the fusion for contraction problem. Loop fusion in the context of scalar programming languages such as Fortran 77 is well understood [26]. Though most work only considers pairwise fusion, some research addresses collective loop fusion, as we do. Sarkar and Gao <ref> [22] </ref> transform loop nests by loop reversal, interchange and fusion to enable array contraction. They target multiprocessors and exploit pipelining by executing producer and consumer loops on different processors, so they are free to ignore all but flow dependences.
Reference: [23] <author> Zhiyu Shen, Zhiyuan Li, and Pen-Chung Yew. </author> <title> An empirical study of Fortran programs for parallelizing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: If there are e dependences, the running time of lines 6 and 10 is O (e), so Find-Loop-Structure runs in O (n 2 e) time. Because the rank of the arrays, n, is typically very small and effectively constant <ref> [23] </ref>, the algorithm is essentially linear, O (e), in the number of dependences. 5 Evaluation This section evaluates our algorithm for statement fusion and array contraction|as implemented in the ZPL compiler|by comparison to commercial F90/HPF compilers and hand coded C code.
Reference: [24] <author> Lawrence Snyder. </author> <title> Programming Guide to ZPL. </title> <publisher> MIT Press, </publisher> <year> 1998. </year> <note> to appear. </note>
Reference-contexts: 1 Introduction Array languages such as Fortran 90 (F90) [1], High Performance Fortran (HPF) [13] and ZPL <ref> [24] </ref> have become important vehicles for expressing data parallelism. Though they simplify the specification of array-based calculations, they also present a potential problem: Large temporary arrays may need to be introduced, either by the programmer or by the compiler.
Reference: [25] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality opti mizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN'91 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Their focus is in recognizing the opportunity in a scalar loop nest, while ours is in enabling the opportunity in an array language compiler via statement fusion. Many techniques for improving locality by loop transformations have appeared in the literature <ref> [5, 16, 19, 25] </ref>. Much of this work addresses the issue of managing the conflicting goals of improving locality without sacrificing parallelism.
Reference: [26] <author> M. Wolfe. </author> <title> High Performance Compilers for Parallel Com puting. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1996. </year> <month> 10 </month>
Reference-contexts: Data dependences <ref> [26] </ref> represent ordering constraints on statements in a program. A flow or true dependence requires that a variable assignment precede a read to the same variable, and an anti-dependence requires the reverse. An output dependence requires that one assignment to a variable precede another assignment to the same variable. <p> Roth and Kennedy have independently developed a similar array based data dependence representation for F90, and they describe its use in scalarization [21]. They do not address the fusion for contraction problem. Loop fusion in the context of scalar programming languages such as Fortran 77 is well understood <ref> [26] </ref>. Though most work only considers pairwise fusion, some research addresses collective loop fusion, as we do. Sarkar and Gao [22] transform loop nests by loop reversal, interchange and fusion to enable array contraction.
References-found: 26

