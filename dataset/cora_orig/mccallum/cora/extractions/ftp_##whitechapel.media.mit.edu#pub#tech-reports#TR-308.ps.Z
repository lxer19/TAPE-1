URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-308.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: E-mail: drew, bobick@media.mit.edu  
Title: Using Configuration States for the Representation and Recognition of Gesture  
Author: Andrew Wilson Aaron Bobick 
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: Room E15-383, The Media Laboratory Massachusetts Institute of Technology  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 308 Abbreviated version appears in ICCV '95 Abstract A state-based technique for the summarization and recognition of gesture is presented. We define a gesture to be a sequence of states in a measurement or configuration space. For a given gesture, these states are used to capture both the repeatability and variability evidenced in a training set of example trajectories. The states are positioned along a prototype of the gesture, and shaped such that they are narrow in the directions in which the ensemble of examples is tightly constrained, and wide in directions in which a great deal of variability is observed. We develop techniques for computing a prototype trajectory of an ensemble of trajectories, for defining configuration states along the prototype, and for recognizing gestures from an unsegmented, continuous stream of sensor data. The approach is illustrated by application to a range of gesture-related sensory data: the two-dimensional movements of a mouse input device, the movement of the hand measured by a magnetic spatial position and orientation sensor, and, lastly, the changing eigenvector projection coefficients com puted from an image sequence.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Bregler and S. M. Omohundro. </author> <title> Nonlinear image interpolation using surface learning. </title> <editor> In G. Tesauro J. D. Cowan and J. Alspector, editors, </editor> <booktitle> Advances in neural information processing systems 6, </booktitle> <pages> pages 43-50, </pages> <address> San Fransisco, CA, 1994. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Murase and Nayar [9] match changing eigenvector projection coefficients taken from the image data to determine the orientation and illumination angle of the object. Bregler and Omohundro <ref> [1] </ref> learn a surface representing constraints on the image sequence for their nonlinear image interpolation task. Yamato et al. [16] compute a simple region-based statistic from each frame of an image sequence, which is then vector-quantized. Sequences of the discrete symbols are then identified by a trained Hidden Markov Model. <p> This contrasts with a trajectory which is simply a path through configuration space representing some particular motion. A point x in configuration space has a membership to state s i described by the fuzzy membership function s i (x) 2 <ref> [0; 1] </ref>. The states along the gesture should be defined so that all examples of the gesture follow the same sequence of states. That is, the states should fall one after the other along the gesture.
Reference: [2] <author> E. Catmull and R. </author> <title> Rom. A class of local interpolating splines. </title> <editor> In R. Barnhill and R. Riesenfeld, editors, </editor> <booktitle> Computer Aided Geometric Design, </booktitle> <pages> pages 317-326, </pages> <address> San Francisco, 1974. </address> <publisher> Academic Press. </publisher>
Reference-contexts: The prototype for G point is similar. tures (about 40 samples each) and ten pointing gestures (about 70 samples each) were collected. To insure that there were enough points available to compute the prototype curve, each example was upsampled using Catmull-Rom <ref> [2] </ref> splines so that each wave gesture is about 40 samples and each point gesture about 70 samples. The prototype curves for each gesture were computed separately. The membership plots for the prototype wave is shown for each state in Figure 4.
Reference: [3] <author> T.J. Darrell and A.P. Pentland. </author> <title> Space-time gestures. </title> <booktitle> Proc. Comp. Vis. and Pattern Rec., </booktitle> <pages> pages 335-340, </pages> <year> 1993. </year>
Reference-contexts: Rohr [12] smooths a number of example joint angle trajectories to build a representation of one walking cycle parameterized by a pose variable. Others have concentrated on capturing how the output of various image operators change over the course of movement. Darrell and Pentland <ref> [3] </ref> use dynamic time warping 1 to match changing normalized image correlation template scores to learned models. The correlation templates are evenly distributed over the length of the model gesture so that it is always the case that some template has a high match score.
Reference: [4] <author> J. W. Davis and M. Shah. </author> <title> Gesture recognition. </title> <booktitle> Proc. European Conf. Comp. Vis., </booktitle> <pages> pages 331-340, </pages> <year> 1994. </year>
Reference-contexts: Gould and Shah [5] show the analysis of motion trajectories to identify event boundaries. These are recorded in their trajectory primal sketch to be used for motion recognition. Rangarajan et al. [11] demonstrate two-dimensional motion trajectory matching through scale-space. Davis and Shah <ref> [4] </ref> recognize simple hand gestures by matching two-dimensional trajectories made by markers on the fingertips. Rohr [12] smooths a number of example joint angle trajectories to build a representation of one walking cycle parameterized by a pose variable.
Reference: [5] <author> K. Gould and M. Shah. </author> <title> The trajectory primal sketch: a multi-scale scheme for representing motion characteristics. </title> <booktitle> Proc. Comp. Vis. and Pattern Rec., </booktitle> <pages> pages 79-85, </pages> <year> 1989. </year>
Reference-contexts: For example, Polana and Nelson [10] use low level features of motion to recognize periodic motions such as walking. A number of researchers have developed novel representations for motion trajectories that are useful in gesture recognition. Gould and Shah <ref> [5] </ref> show the analysis of motion trajectories to identify event boundaries. These are recorded in their trajectory primal sketch to be used for motion recognition. Rangarajan et al. [11] demonstrate two-dimensional motion trajectory matching through scale-space.
Reference: [6] <author> T. Hastie and W. Stuetzle. </author> <title> Principal curves. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84(406) </volume> <pages> 502-516, </pages> <year> 1989. </year>
Reference-contexts: For ensembles of space curves in metric spaces there are several well known techniques that compute a "principal curve" <ref> [6] </ref> that attempts to minimize distance between each point of each of the trajectories and the nearest point on the principal curve.
Reference: [7] <author> J.S. Lipscomb. </author> <title> A trainable gesture recognizer. </title> <journal> Pattern Recognition, </journal> <volume> 24(9) </volume> <pages> 895-907, </pages> <year> 1991. </year>
Reference-contexts: Sequences of the discrete symbols are then identified by a trained Hidden Markov Model. Lastly, some work has been done in the real time recognition of simple mouse input device gestures. Tew and Gray [14] use dynamic programming to match mouse trajectories to prototype trajectories. Lipscomb <ref> [7] </ref> concentrates on filtering the mouse movement data to obtain robust recognition of similarly filtered models.
Reference: [8] <author> K. V. Mardia, N. M. Ghali, M. Howes T. J. Hainsworth, and N. Sheehy. </author> <title> Techniques for online gesture recognition on workstations. </title> <journal> Image and Vision Computing, </journal> <volume> 11(5) </volume> <pages> 283-294, </pages> <year> 1993. </year>
Reference-contexts: Tew and Gray [14] use dynamic programming to match mouse trajectories to prototype trajectories. Lipscomb [7] concentrates on filtering the mouse movement data to obtain robust recognition of similarly filtered models. Mardia et al. <ref> [8] </ref> compute many features of each trajectory and use a learned decision tree for each gesture to best utilizes the features for recognition. 3 Motivation for a Representation If all the constraints on the motion that make up a gesture were known exactly, recognition would simply be a matter of determining
Reference: [9] <author> H. Murase and S. Nayar. </author> <title> Learning and recognition of 3d objects from appearance. </title> <booktitle> In IEEE 2nd Qualitative Vision Workshop, </booktitle> <address> New York, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The correlation templates are evenly distributed over the length of the model gesture so that it is always the case that some template has a high match score. Murase and Nayar <ref> [9] </ref> match changing eigenvector projection coefficients taken from the image data to determine the orientation and illumination angle of the object. Bregler and Omohundro [1] learn a surface representing constraints on the image sequence for their nonlinear image interpolation task. <p> The first five example waves were used in training. The three eigenvectors with the largest eigenvalues were computed by the Karhunen-Loeve Transform (as in <ref> [15, 9] </ref>) of the training frames, treating each frame as a column of pixel values. The first three eigenvectors accounted for 71% of the variance of the pixel intensity values of the training frames.
Reference: [10] <author> R. Polana and R. Nelson. </author> <title> Low level recognition of human motion. </title> <booktitle> In Proc. of the Workshop on Motion of Non-Rigid and Articulated Objects, </booktitle> <pages> pages 77-82, </pages> <address> Austin, Texas, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Work with very low resolution American Sign Language images by Sperling et al. [13] further supports the notion that in many domains a full geometric reconstruction of the moving object is unnecessary for recognition. For example, Polana and Nelson <ref> [10] </ref> use low level features of motion to recognize periodic motions such as walking. A number of researchers have developed novel representations for motion trajectories that are useful in gesture recognition. Gould and Shah [5] show the analysis of motion trajectories to identify event boundaries.
Reference: [11] <author> K. Rangarajan, W. Allen, and M. Shah. </author> <title> Matching motion trajectories using scale-space. </title> <journal> Pattern Recognition, </journal> <volume> 26(4) </volume> <pages> 595-610, </pages> <year> 1993. </year> <title> points randomly scattered about an arc. Only a fraction of the data points are shown; the distances of these points to the closest points on the curve are indicated. </title>
Reference-contexts: A number of researchers have developed novel representations for motion trajectories that are useful in gesture recognition. Gould and Shah [5] show the analysis of motion trajectories to identify event boundaries. These are recorded in their trajectory primal sketch to be used for motion recognition. Rangarajan et al. <ref> [11] </ref> demonstrate two-dimensional motion trajectory matching through scale-space. Davis and Shah [4] recognize simple hand gestures by matching two-dimensional trajectories made by markers on the fingertips. Rohr [12] smooths a number of example joint angle trajectories to build a representation of one walking cycle parameterized by a pose variable.
Reference: [12] <author> K. Rohr. </author> <title> Towards model-based recognition of human movements in image sequences. Comp. Vis., Graph., </title> <journal> and Img. Proc., </journal> <volume> 59(1) </volume> <pages> 94-115, </pages> <year> 1994. </year>
Reference-contexts: These are recorded in their trajectory primal sketch to be used for motion recognition. Rangarajan et al. [11] demonstrate two-dimensional motion trajectory matching through scale-space. Davis and Shah [4] recognize simple hand gestures by matching two-dimensional trajectories made by markers on the fingertips. Rohr <ref> [12] </ref> smooths a number of example joint angle trajectories to build a representation of one walking cycle parameterized by a pose variable. Others have concentrated on capturing how the output of various image operators change over the course of movement.
Reference: [13] <author> G. Sperling, M. Landy, Y. Cohen, and M. Pavel. </author> <title> Intelligible encoding of ASL image sequences at extremely low information rates. Comp. Vis., Graph., </title> <journal> and Img. Proc., </journal> <volume> 31 </volume> <pages> 335-391, </pages> <year> 1985. </year>
Reference-contexts: Work with very low resolution American Sign Language images by Sperling et al. <ref> [13] </ref> further supports the notion that in many domains a full geometric reconstruction of the moving object is unnecessary for recognition. For example, Polana and Nelson [10] use low level features of motion to recognize periodic motions such as walking.
Reference: [14] <author> A.I. Tew and C.J. Gray. </author> <title> A real-time gesture recognizer based on dynamic programming. </title> <journal> Journal of Biomedical Eng., </journal> <volume> 15 </volume> <pages> 181-187, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Sequences of the discrete symbols are then identified by a trained Hidden Markov Model. Lastly, some work has been done in the real time recognition of simple mouse input device gestures. Tew and Gray <ref> [14] </ref> use dynamic programming to match mouse trajectories to prototype trajectories. Lipscomb [7] concentrates on filtering the mouse movement data to obtain robust recognition of similarly filtered models.
Reference: [15] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: The first five example waves were used in training. The three eigenvectors with the largest eigenvalues were computed by the Karhunen-Loeve Transform (as in <ref> [15, 9] </ref>) of the training frames, treating each frame as a column of pixel values. The first three eigenvectors accounted for 71% of the variance of the pixel intensity values of the training frames.
Reference: [16] <author> J. Yamato, J. Ohya, and K. Ishii. </author> <title> Recognizing human action in time-sequential images using hidden markov model. </title> <booktitle> Proc. Comp. Vis. and Pattern Rec., </booktitle> <pages> pages 379-385, </pages> <year> 1992. </year>
Reference-contexts: Murase and Nayar [9] match changing eigenvector projection coefficients taken from the image data to determine the orientation and illumination angle of the object. Bregler and Omohundro [1] learn a surface representing constraints on the image sequence for their nonlinear image interpolation task. Yamato et al. <ref> [16] </ref> compute a simple region-based statistic from each frame of an image sequence, which is then vector-quantized. Sequences of the discrete symbols are then identified by a trained Hidden Markov Model. Lastly, some work has been done in the real time recognition of simple mouse input device gestures.
References-found: 16

