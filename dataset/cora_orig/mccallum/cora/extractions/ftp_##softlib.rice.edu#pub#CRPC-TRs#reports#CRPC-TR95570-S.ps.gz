URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95570-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: fgagan, saltzg@cs.umd.edu  
Phone: (301)-405-2756  
Title: Interprocedural Compilation of Irregular Applications for Distributed Memory Machines  
Author: Gagan Agrawal and Joel Saltz 
Address: College Park, MD 20742  
Affiliation: UMIACS and Department of Computer Science University of Maryland  
Abstract: Data parallel languages like High Performance Fortran (HPF) are emerging as the architecture independent mode of programming distributed memory parallel machines. In this paper, we present the interprocedural optimizations required for compiling applications having irregular data access patterns, when coded in such data parallel languages. We have developed an Interprocedural Partial Redundancy Elimination (IPRE) algorithm for optimized placement of runtime preprocessing routine and collective communication routines inserted for managing communication in such codes. We also present three new interprocedural optimizations: placement of scatter routines, deletion of data structures and use of coalescing and incremental routines. We then describe how program slicing can be used for further applying IPRE in more complex scenarios. We have done a preliminary implementation of the schemes presented here using the Fortran D compilation system as the necessary infrastructure. We present experimental results from two codes compiled using our system to demonstrate the efficacy of the presented schemes.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal and Joel Saltz. </author> <title> Interprocedural communication optimizations for distributed memory com pilation. </title> <booktitle> In Proceedings of the 7th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 283-299, </pages> <month> August </month> <year> 1994. </year> <note> Also available as University of Maryland Technical Report CS-TR-3264. </note>
Reference-contexts: These schemes are based upon a classical data flow framework called Partial Redundancy Elimination (PRE) [16, 32]. PRE encompasses traditional optimizations like loop invariant code motion and redundant computation elimination. We have developed an Interprocedural Partial Redundancy Elimination framework (IPRE) <ref> [1, 2] </ref> as a basis for performing interprocedural placement. In this paper, we discuss various practical aspects in applying interprocedural partial redundancy elimination for placement of communication and communication preprocessing statements. <p> More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [18, 24]. We have extended an existing intraprocedural partial redundancy scheme to be applied interprocedurally <ref> [1, 2] </ref>. In this section, we describe the functionality of the PRE framework, key data flow properties associated with it and briefly sketch how we have extended an existing intraprocedural scheme interprocedurally. Consider any computation of an expression or a call to a pure function. <p> The data flow properties are computed for the beginning and the end of each edge in the FPR program representation. The details of the data flow analysis required for computing the above properties and then determining placement and deletion based on these has been given elsewhere <ref> [1, 2] </ref>. There are several difficulties in extending the analysis interprocedurally, this includes renaming of influencers across procedure boundaries, saving the calling context of procedures which are called at more than one call sites and further intraprocedural analysis in each procedure to determine final local placement. <p> Second Phase. After the initial pass over all the procedures, we perform the data flow analysis for determining placement. The first step is to generate the full program representation (FPR) using the summary information computed from each procedure <ref> [1] </ref>. The procedure entry nodes are then initialized with the candidates for placement. During the first phase, we have stored the value Global M ax dep, the maximum depth level of any candidate in any procedure.
Reference: [2] <author> Gagan Agrawal, Joel Saltz, and Raja Das. </author> <title> Interprocedural partial redundancy elimination and its application to distributed memory compilation. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 258-269. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1995. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 30, No. </volume> <pages> 6. </pages> <note> Also available as University of Maryland Technical Report CS-TR-3446 and UMIACS-TR-95-42. </note>
Reference-contexts: These schemes are based upon a classical data flow framework called Partial Redundancy Elimination (PRE) [16, 32]. PRE encompasses traditional optimizations like loop invariant code motion and redundant computation elimination. We have developed an Interprocedural Partial Redundancy Elimination framework (IPRE) <ref> [1, 2] </ref> as a basis for performing interprocedural placement. In this paper, we discuss various practical aspects in applying interprocedural partial redundancy elimination for placement of communication and communication preprocessing statements. <p> More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [18, 24]. We have extended an existing intraprocedural partial redundancy scheme to be applied interprocedurally <ref> [1, 2] </ref>. In this section, we describe the functionality of the PRE framework, key data flow properties associated with it and briefly sketch how we have extended an existing intraprocedural scheme interprocedurally. Consider any computation of an expression or a call to a pure function. <p> The data flow properties are computed for the beginning and the end of each edge in the FPR program representation. The details of the data flow analysis required for computing the above properties and then determining placement and deletion based on these has been given elsewhere <ref> [1, 2] </ref>. There are several difficulties in extending the analysis interprocedurally, this includes renaming of influencers across procedure boundaries, saving the calling context of procedures which are called at more than one call sites and further intraprocedural analysis in each procedure to determine final local placement.
Reference: [3] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for structured and block structured applications. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: We have shown in our previous work how communication preprocessing is useful in regular applications in which data distribution, strides and/or loop bounds are not known at compile-time <ref> [3, 5, 4, 36] </ref> or when the number of processors available for the execution of the program varies at runtime [17]. The rest of the paper is organized as follows. In Section 2, we discuss the basic IPRE framework.
Reference: [4] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Efficient runtime support for parallelizing block structured applications. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 158-167. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: We have shown in our previous work how communication preprocessing is useful in regular applications in which data distribution, strides and/or loop bounds are not known at compile-time <ref> [3, 5, 4, 36] </ref> or when the number of processors available for the execution of the program varies at runtime [17]. The rest of the paper is organized as follows. In Section 2, we discuss the basic IPRE framework.
Reference: [5] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> An integrated runtime and compile-time approach for paral lelizing structured and block structured applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1995. To appear. Also available as University of Maryland Technical Report CS-TR-3143 and UMIACS-TR-93-94. </note>
Reference-contexts: We have shown in our previous work how communication preprocessing is useful in regular applications in which data distribution, strides and/or loop bounds are not known at compile-time <ref> [3, 5, 4, 36] </ref> or when the number of processors available for the execution of the program varies at runtime [17]. The rest of the paper is organized as follows. In Section 2, we discuss the basic IPRE framework.
Reference: [6] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1986. </year>
Reference-contexts: A basic block of code in a procedure is a sequence of consecutive statements in a procedure in the flow enters at the beginning and leaves at the end without possibility of branching expect at the end <ref> [6] </ref>. If a candidate is placed at a point p in the program, and if it is available at the point p, then the occurrence of the candidate at the point p is redundant.
Reference: [7] <author> Aart J. C. Bik and Harry A. G. Wijshoff. </author> <title> Annotations for a sparse compiler. </title> <booktitle> In Proceedings of the 8th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [7, 14, 23, 29, 30, 31, 34, 38] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. Recent work has demonstrated that sophisticated compilation techniques can play a crucial role in optimizing performance obtained from irregular codes [14, 24].
Reference: [8] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M.-Y. Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 15-26, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In the procedure Proc A, the scatter operation can be deleted, since this scatter is subsumed by the scatter done later in Proc B. Scatter operations have also been used by distributed memory compilers in compiling regular applications <ref> [8] </ref>. The HPF/Fortran 90D compiler developed at Syracuse University uses scatter operations (called post-comp writes) whenever the subscript in the left hand side array reference is a complex function of the index variable.
Reference: [9] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dy namics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4:187, </volume> <year> 1983. </year>
Reference-contexts: We have used two irregular codes in our study, an Euler solver on an unstructured mesh [13], originally developed at ICASE by Mavriplis et al. and a template taken from CHARMM <ref> [9] </ref>, a molecular dynamics code. We used Intel Paragon at Rice University for performing our experiments. The Euler solver we experimented with performs sweeps over an unstructured mesh inside the time step loop. <p> The second code we considered was a template taken from a molecular dynamics code Charmm <ref> [9, 27] </ref>. The templates we worked with comprised of just 2 procedures, one procedure which computed non-bonded forces between the atoms of the molecules and the other procedure enclosed this procedure in a time step loop.
Reference: [10] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: The total number of nodes in the SuperGraph can get very large and consequently the solution may take much longer time to converge. Several ideas in the design of our representation are similar to the ideas used in Callahan's Program Summary Graph <ref> [10] </ref> and Interprocedural Flow Graph used by Soffa et al. [25]. 8 Conclusions In this paper, we have presented interprocedural optimizations for the compilation of irregular applications on distributed memory machines. In such applications, runtime preprocessing is used to determine the communication required between the processors.
Reference: [11] <author> A. Choudhary, G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, S. Ranka, and J. Saltz. </author> <title> Software support for irregular and loosely synchronous problems. </title> <booktitle> Computing Systems in Engineering, 3(1-4):43-52, 1992. Papers presented at the Symposium on High-Performance Computing for Flight Vehicles, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: In this paper, we discuss the interprocedural analysis and optimizations for compiling irregular applications. Specifically, we concentrate on applications in which data is accessed using indirection arrays. 1 Such codes are common in computational fluid dynamics, molecular dynamics, in Particle In Cell (PIC) problems and in numerical simulations <ref> [11] </ref>. The commonly used approach for compiling irregular applications is the inspector/executor model [29]. Conceptually, an inspector or a communication preprocessing statement analyzes the indirection array to determine the communication required by a data parallel loop. The results of communication preprocessing is then used to perform the communication.
Reference: [12] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the rn programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year> <month> 24 </month>
Reference-contexts: For each such procedure call in the control flow path of candidate, we just examine if any of the variables in the slice are modified by the procedure call <ref> [12] </ref>. If so, we do not consider this candidate for hoisting outside the procedure. When we use slices of the candidates, additional steps are required in final placement of the candidates. In placing the candidate, the entire slice corresponding to candidate is placed.
Reference: [13] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives. </title> <journal> AIAA Journal, </journal> <volume> 32(3) </volume> <pages> 489-496, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: We measure the difference made by performing interprocedural placement of both the communication preprocessing statements and the collective communication statements. We have used two irregular codes in our study, an Euler solver on an unstructured mesh <ref> [13] </ref>, originally developed at ICASE by Mavriplis et al. and a template taken from CHARMM [9], a molecular dynamics code. We used Intel Paragon at Rice University for performing our experiments. The Euler solver we experimented with performs sweeps over an unstructured mesh inside the time step loop.
Reference: [14] <author> Raja Das, Joel Saltz, and Reinhard von Hanxleden. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 152-168. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3076 and UMIACS-TR-93-42. </note>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [7, 14, 23, 29, 30, 31, 34, 38] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. Recent work has demonstrated that sophisticated compilation techniques can play a crucial role in optimizing performance obtained from irregular codes [14, 24]. <p> For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. Recent work has demonstrated that sophisticated compilation techniques can play a crucial role in optimizing performance obtained from irregular codes <ref> [14, 24] </ref>. Thus far, experiences and experimental results reported have been from small code templates. We anticipate that the ability to apply optimizations across procedure boundaries will prove to be extremely important in generating efficient parallel code in large applications. <p> CHAOS/PARTI library provides a rich set of routines for performing the communication preprocessing and optimized communication for such applications [35]. The Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications [26] but has more recently been extended to compile irregular applications <ref> [14, 23] </ref>. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [14, 22]. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed [18, 24]. <p> The Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications [26] but has more recently been extended to compile irregular applications [14, 23]. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication <ref> [14, 22] </ref>. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed [18, 24]. The key idea underlying these schemes is to do the placement so that redundancies are reduced or eliminated. <p> This may not be adequate for performing code motion in several irregular applications, especially the ones in which data is accessed using multiple levels of indirection <ref> [14] </ref>. For such codes, IPRE can be performed by using slices of the call to the candidates. Consider the code given in Figure 7. In the procedure Proc B, the array Q is accessed using array R, which is local within procedure Proc B. <p> For this purpose, we use the notion of program (or procedure) slices. Program Slice. A program (procedure) slice is defined as a program comprising of a set of statements which contribute, either directly or indirectly, to the value of certain variables at a certain point in the program <ref> [14, 15, 37] </ref>. This set of variables and the point in the program is together referred to as the slicing criterion. For our purpose, the slicing criterion used is the set of parameters of the candidate at the point in the program where the candidate is invoked.
Reference: [15] <author> Raja Das, Joel Saltz, Ken Kennedy, and Paul Havlak. </author> <title> Index array flattening through program transformation. </title> <note> Submitted to PLDI '95, </note> <month> November </month> <year> 1994. </year>
Reference-contexts: For this purpose, we use the notion of program (or procedure) slices. Program Slice. A program (procedure) slice is defined as a program comprising of a set of statements which contribute, either directly or indirectly, to the value of certain variables at a certain point in the program <ref> [14, 15, 37] </ref>. This set of variables and the point in the program is together referred to as the slicing criterion. For our purpose, the slicing criterion used is the set of parameters of the candidate at the point in the program where the candidate is invoked.
Reference: [16] <author> D.M. Dhamdhere and H. Patil. </author> <title> An elimination algorithm for bidirectional data flow problems using edge placement. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(2) </volume> <pages> 312-336, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Techniques for performing these optimizations within a single procedure are well developed [18, 24]. The key idea underlying these schemes is to do the placement so that redundancies are reduced or eliminated. These schemes are based upon a classical data flow framework called Partial Redundancy Elimination (PRE) <ref> [16, 32] </ref>. PRE encompasses traditional optimizations like loop invariant code motion and redundant computation elimination. We have developed an Interprocedural Partial Redundancy Elimination framework (IPRE) [1, 2] as a basis for performing interprocedural placement. <p> Partial Redundancy Elimination (PRE) is a unified framework for performing these optimizations intraprocedurally <ref> [16, 32] </ref>. It has been commonly used intraprocedurally for performing optimizations like common subexpression elimination and strength re 2 duction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [18, 24].
Reference: [17] <author> Guy Edjlali, Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Data parallel programming in an adaptive environment. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 827-832. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1995. </year>
Reference-contexts: We have shown in our previous work how communication preprocessing is useful in regular applications in which data distribution, strides and/or loop bounds are not known at compile-time [3, 5, 4, 36] or when the number of processors available for the execution of the program varies at runtime <ref> [17] </ref>. The rest of the paper is organized as follows. In Section 2, we discuss the basic IPRE framework. In Section 3, we present several new optimizations required for compiling irregular applications.
Reference: [18] <author> Manish Gupta, Edith Schonberg, and Harini Srinivasan. </author> <title> A unified data flow framework for optimizing com munication. </title> <booktitle> In Proceedings of Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [14, 22]. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed <ref> [18, 24] </ref>. The key idea underlying these schemes is to do the placement so that redundancies are reduced or eliminated. These schemes are based upon a classical data flow framework called Partial Redundancy Elimination (PRE) [16, 32]. PRE encompasses traditional optimizations like loop invariant code motion and redundant computation elimination. <p> It has been commonly used intraprocedurally for performing optimizations like common subexpression elimination and strength re 2 duction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines <ref> [18, 24] </ref>. We have extended an existing intraprocedural partial redundancy scheme to be applied interprocedurally [1, 2]. In this section, we describe the functionality of the PRE framework, key data flow properties associated with it and briefly sketch how we have extended an existing intraprocedural scheme interprocedurally. <p> This is based upon Call Graph program abstraction and is targeted more towards flow-insensitive interprocedural analysis. Our implementation uses several facilities available from FIAT as part of the Fortran D infrastructure. Partial redundancy elimination was used interprocedurally by Gupta et al. <ref> [18] </ref> for performing communication optimizations. An interesting feature of their work is the available section descriptor, which 22 V 1 : Performance before interprocedural optimizations V 2 : Interprocedural placement of preprocessing stmts. facilitates many other optimizations for regular codes. Hanxleden [24] has developed Give-N-Take, a new communication placement framework.
Reference: [19] <author> Mary Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: We have chosen a concise full program representation, which will allow efficient data flow analysis, while maintaining sufficient precision to allow useful transformations and 4 to ensure safety and correctness of transformations. 2.1 Program Representation In traditional interprocedural analysis, program is abstracted by a call graph <ref> [19, 20] </ref>. In a call graph G = (V; E), V is the set of procedures and directed edge e = (i; j) (e 2 E) represents a call site in which procedure i invokes procedure j.
Reference: [20] <author> Mary Hall, John M Mellor Crummey, Alan Carle, and Rene G Rodriguez. FIAT: </author> <title> A framework for interpro cedural analysis and transformations. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 522-545. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: We have chosen a concise full program representation, which will allow efficient data flow analysis, while maintaining sufficient precision to allow useful transformations and 4 to ensure safety and correctness of transformations. 2.1 Program Representation In traditional interprocedural analysis, program is abstracted by a call graph <ref> [19, 20] </ref>. In a call graph G = (V; E), V is the set of procedures and directed edge e = (i; j) (e 2 E) represents a call site in which procedure i invokes procedure j. <p> This abstraction records any loop (s) enclosing a procedure call. Again, this abstraction does not allow to look for redundant communication preprocessing calls or communication calls in adjacent procedures. Framework for Interprocedural Analysis and Transforms (FIAT) <ref> [20] </ref> has recently been proposed as a general environment for interprocedural analysis. This is based upon Call Graph program abstraction and is targeted more towards flow-insensitive interprocedural analysis. Our implementation uses several facilities available from FIAT as part of the Fortran D infrastructure.
Reference: [21] <author> M.W. Hall, S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <pages> pages 522-534. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1992. </year>
Reference-contexts: First Phase. The initial local compilation phase inserts communication preprocessing and communication statements based upon intraprocedural analysis [26]. This code generation is based upon reaching 18 decomposition analysis <ref> [21] </ref>. Reaching decomposition analysis propagates information about the distribution of arrays from calling procedures to callees. In compiling languages like Fortran D or HPF, the information about data distribution is used by the compiler for determining loop partitioning, communication and to decide upon the appropriate runtime routines to insert. <p> Experiments on hand-parallelization of the entire Charmm code [27, 35] have shown a nearly 20% reduction in the communication time, by using coalescing communication routines. 7 Related Work The only other effort on interprocedural analysis for distributed memory compilation is by Hall et al. <ref> [21] </ref>. They have concentrated on flow-insensitive analysis for regular applications, including management of buffer space and propagation of data distribution and data alignment information across procedure boundaries. In this work, the Augmented Call Graph (ACG) was introduced as a new program abstraction.
Reference: [22] <author> R. v. Hanxleden, K. Kennedy, and J. Saltz. </author> <title> Value-based distributions in Fortran D a preliminary report. </title> <type> Technical Report CRPC-TR93365-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> December </month> <year> 1993. </year> <title> Submitted to Journal of Programming Languages Special Issue on Compiling and Run-Time Issues for Distributed Address Space Machines. </title>
Reference-contexts: The Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications [26] but has more recently been extended to compile irregular applications [14, 23]. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication <ref> [14, 22] </ref>. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed [18, 24]. The key idea underlying these schemes is to do the placement so that redundancies are reduced or eliminated.
Reference: [23] <author> Reinhard v. Hanxleden. </author> <title> Handling irregular problems with Fortran D a preliminary report. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year> <note> Also available as CRPC Technical Report CRPC-TR93339-S. </note>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [7, 14, 23, 29, 30, 31, 34, 38] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. Recent work has demonstrated that sophisticated compilation techniques can play a crucial role in optimizing performance obtained from irregular codes [14, 24]. <p> CHAOS/PARTI library provides a rich set of routines for performing the communication preprocessing and optimized communication for such applications [35]. The Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications [26] but has more recently been extended to compile irregular applications <ref> [14, 23] </ref>. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [14, 22]. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed [18, 24].
Reference: [24] <author> Reinhard von Hanxleden and Ken Kennedy. </author> <title> Give-n-take a balanced code placement framework. </title> <booktitle> In Proceed ings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 107-120. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1994. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 29, No. </volume> <pages> 6. </pages>
Reference-contexts: For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. Recent work has demonstrated that sophisticated compilation techniques can play a crucial role in optimizing performance obtained from irregular codes <ref> [14, 24] </ref>. Thus far, experiences and experimental results reported have been from small code templates. We anticipate that the ability to apply optimizations across procedure boundaries will prove to be extremely important in generating efficient parallel code in large applications. <p> In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [14, 22]. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed <ref> [18, 24] </ref>. The key idea underlying these schemes is to do the placement so that redundancies are reduced or eliminated. These schemes are based upon a classical data flow framework called Partial Redundancy Elimination (PRE) [16, 32]. PRE encompasses traditional optimizations like loop invariant code motion and redundant computation elimination. <p> It has been commonly used intraprocedurally for performing optimizations like common subexpression elimination and strength re 2 duction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines <ref> [18, 24] </ref>. We have extended an existing intraprocedural partial redundancy scheme to be applied interprocedurally [1, 2]. In this section, we describe the functionality of the PRE framework, key data flow properties associated with it and briefly sketch how we have extended an existing intraprocedural scheme interprocedurally. <p> An interesting feature of their work is the available section descriptor, which 22 V 1 : Performance before interprocedural optimizations V 2 : Interprocedural placement of preprocessing stmts. facilitates many other optimizations for regular codes. Hanxleden <ref> [24] </ref> has developed Give-N-Take, a new communication placement framework. This framework extends PRE in several ways, including a notion of early and lazy problems, which is used for performing earliest possible placement of sends and latest possible placement of receive operations. Allowing such asynchronous communication can reduce communication latencies.
Reference: [25] <author> Mary Jean Harrold and Mary Lou Soffa. </author> <title> Efficient computation of interprocedural definition-use chains. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(2) </volume> <pages> 175-204, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Several ideas in the design of our representation are similar to the ideas used in Callahan's Program Summary Graph [10] and Interprocedural Flow Graph used by Soffa et al. <ref> [25] </ref>. 8 Conclusions In this paper, we have presented interprocedural optimizations for the compilation of irregular applications on distributed memory machines. In such applications, runtime preprocessing is used to determine the communication required between the processors.
Reference: [26] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The results of communication preprocessing is then used to perform the communication. CHAOS/PARTI library provides a rich set of routines for performing the communication preprocessing and optimized communication for such applications [35]. The Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications <ref> [26] </ref> but has more recently been extended to compile irregular applications [14, 23]. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [14, 22]. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. <p> By scatter, we mean a routine which, after a data parallel loop, updates the off-processor elements modified by the loop. In distributed memory compilation, a commonly used technique for loop iteration partitioning is owner computes rule <ref> [26] </ref>. In this method, each iteration is executed by the processor which owns the left hand side array reference updated by the iteration. <p> In the third phase, each procedure is visited again, and the decisions made about placement of candidates are actually incorporated in the code for each procedure. First Phase. The initial local compilation phase inserts communication preprocessing and communication statements based upon intraprocedural analysis <ref> [26] </ref>. This code generation is based upon reaching 18 decomposition analysis [21]. Reaching decomposition analysis propagates information about the distribution of arrays from calling procedures to callees.
Reference: [27] <author> Yuan-Shin Hwang, Raja Das, Joel Saltz, Bernard Brooks, and Milan Hodoscek. </author> <title> Parallelizing molecular dynamics programs for distributed memory machines: An application of the CHAOS runtime support library. </title> <institution> Technical Report CS-TR-3374 and UMIACS-TR-94-125, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: The second code we considered was a template taken from a molecular dynamics code Charmm <ref> [9, 27] </ref>. The templates we worked with comprised of just 2 procedures, one procedure which computed non-bonded forces between the atoms of the molecules and the other procedure enclosed this procedure in a time step loop. <p> In the second version (V 2), placement of communication preprocessing statements is optimized interprocedurally. Since this was a relatively small template, no further improvement in performance can be achieved by interprocedural optimization of communication statements. Experiments on hand-parallelization of the entire Charmm code <ref> [27, 35] </ref> have shown a nearly 20% reduction in the communication time, by using coalescing communication routines. 7 Related Work The only other effort on interprocedural analysis for distributed memory compilation is by Hall et al. [21].
Reference: [28] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: High Performance Fortran (HPF) consists of Fortran 90 extensions designed to allow users to specify parallelism and data distributions in a high level manner. The first round of HPF language definition has been completed <ref> [28] </ref> and many commercial HPF compiler development projects are currently underway. Efforts are also underway in the High Performance Fortran Forum to increase the scope of HPF for compiling a wider range of applications.
Reference: [29] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [7, 14, 23, 29, 30, 31, 34, 38] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. Recent work has demonstrated that sophisticated compilation techniques can play a crucial role in optimizing performance obtained from irregular codes [14, 24]. <p> Specifically, we concentrate on applications in which data is accessed using indirection arrays. 1 Such codes are common in computational fluid dynamics, molecular dynamics, in Particle In Cell (PIC) problems and in numerical simulations [11]. The commonly used approach for compiling irregular applications is the inspector/executor model <ref> [29] </ref>. Conceptually, an inspector or a communication preprocessing statement analyzes the indirection array to determine the communication required by a data parallel loop. The results of communication preprocessing is then used to perform the communication.
Reference: [30] <author> V. Kotlyar, K. Pingali, and P. Stodghill. </author> <title> Automatic parallelization of sparse conjugate gradient code: A progress report. </title> <booktitle> In Proceedings of the 8th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [7, 14, 23, 29, 30, 31, 34, 38] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. Recent work has demonstrated that sophisticated compilation techniques can play a crucial role in optimizing performance obtained from irregular codes [14, 24].
Reference: [31] <author> Antonio Lain and Prithviraj Banerjee. </author> <title> Exploiting spatial regularity in irregular iterative applications. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 820-826. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1995. </year>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [7, 14, 23, 29, 30, 31, 34, 38] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. Recent work has demonstrated that sophisticated compilation techniques can play a crucial role in optimizing performance obtained from irregular codes [14, 24].
Reference: [32] <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year> <month> 25 </month>
Reference-contexts: Techniques for performing these optimizations within a single procedure are well developed [18, 24]. The key idea underlying these schemes is to do the placement so that redundancies are reduced or eliminated. These schemes are based upon a classical data flow framework called Partial Redundancy Elimination (PRE) <ref> [16, 32] </ref>. PRE encompasses traditional optimizations like loop invariant code motion and redundant computation elimination. We have developed an Interprocedural Partial Redundancy Elimination framework (IPRE) [1, 2] as a basis for performing interprocedural placement. <p> Partial Redundancy Elimination (PRE) is a unified framework for performing these optimizations intraprocedurally <ref> [16, 32] </ref>. It has been commonly used intraprocedurally for performing optimizations like common subexpression elimination and strength re 2 duction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [18, 24].
Reference: [33] <author> E. Myers. </author> <title> A precise interprocedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 219-230, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Allowing such asynchronous communication can reduce communication latencies. Our work differs significantly since we consider interprocedural optimizations and present several new optimizations. Several different program representations have been used for different flow-sensitive interprocedural problems. Myer has suggested the concept of the SuperGraph <ref> [33] </ref> which is constructed by linking control flow graphs of procedures by inserting edges from call site in the caller to start node in callee. The total number of nodes in the SuperGraph can get very large and consequently the solution may take much longer time to converge.
Reference: [34] <author> Ravi Ponnusamy, Joel Saltz, and Alok Choudhary. </author> <title> Runtime-compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 361-370. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3055 and UMIACS-TR-93-32. </note>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [7, 14, 23, 29, 30, 31, 34, 38] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. Recent work has demonstrated that sophisticated compilation techniques can play a crucial role in optimizing performance obtained from irregular codes [14, 24].
Reference: [35] <author> Shamik D. Sharma, Ravi Ponnusamy, Bongki Moon, Yuan-Shin Hwang, Raja Das, and Joel Saltz. </author> <title> Run-time and compile-time support for adaptive irregular problems. </title> <booktitle> In Proceedings Supercomputing '94, </booktitle> <pages> pages 97-106. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: The results of communication preprocessing is then used to perform the communication. CHAOS/PARTI library provides a rich set of routines for performing the communication preprocessing and optimized communication for such applications <ref> [35] </ref>. The Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications [26] but has more recently been extended to compile irregular applications [14, 23]. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [14, 22]. <p> In the second version (V 2), placement of communication preprocessing statements is optimized interprocedurally. Since this was a relatively small template, no further improvement in performance can be achieved by interprocedural optimization of communication statements. Experiments on hand-parallelization of the entire Charmm code <ref> [27, 35] </ref> have shown a nearly 20% reduction in the communication time, by using coalescing communication routines. 7 Related Work The only other effort on interprocedural analysis for distributed memory compilation is by Hall et al. [21].
Reference: [36] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: We have shown in our previous work how communication preprocessing is useful in regular applications in which data distribution, strides and/or loop bounds are not known at compile-time <ref> [3, 5, 4, 36] </ref> or when the number of processors available for the execution of the program varies at runtime [17]. The rest of the paper is organized as follows. In Section 2, we discuss the basic IPRE framework.
Reference: [37] <author> Mark Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 10 </volume> <pages> 352-357, </pages> <year> 1984. </year>
Reference-contexts: For this purpose, we use the notion of program (or procedure) slices. Program Slice. A program (procedure) slice is defined as a program comprising of a set of statements which contribute, either directly or indirectly, to the value of certain variables at a certain point in the program <ref> [14, 15, 37] </ref>. This set of variables and the point in the program is together referred to as the slicing criterion. For our purpose, the slicing criterion used is the set of parameters of the candidate at the point in the program where the candidate is invoked. <p> Computing Slices. Algorithms for computing a slice, given a slicing criterion, have been presented in the literature <ref> [37] </ref>. We make one important difference in the way slices are computed, since we need to accommodate the fact that some of the statements included in the slice may themselves be candidates for placement.
Reference: [38] <author> Janet Wu, Raja Das, Joel Saltz, Harry Berryman, and Seema Hiranandani. </author> <title> Distributed memory compiler design for sparse problems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(6) </volume> <pages> 737-753, </pages> <month> June </month> <year> 1995. </year> <month> 26 </month>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [7, 14, 23, 29, 30, 31, 34, 38] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. Recent work has demonstrated that sophisticated compilation techniques can play a crucial role in optimizing performance obtained from irregular codes [14, 24].
References-found: 38

