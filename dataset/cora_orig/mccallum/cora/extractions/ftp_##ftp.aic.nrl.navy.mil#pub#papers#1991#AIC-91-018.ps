URL: ftp://ftp.aic.nrl.navy.mil/pub/papers/1991/AIC-91-018.ps
Refering-URL: http://www.aic.nrl.navy.mil/~schultz/papers.html
Root-URL: 
Email: EMAIL: schultz@aic.nrl.navy.mil  
Phone: (202) 767-2684  
Title: Using a Genetic Algorithm to Learn Strategies for Collision Avoidance and Local Navigation  
Author: Alan C. Schultz 
Address: (Code 5514),  DC 20375-5000, U.S.A.  
Affiliation: Navy Center for Applied Research in Artificial Intelligence  Naval Research Laboratory, Washington,  
Abstract: Navigation through obstacles such as mine fields is an important capability for autonomous underwater vehicles. One way to produce robust behavior is to perform projective planning. However, real-time performance is a critical requirement in navigation. What is needed for a truly autonomous vehicle are robust reactive rules that perform well in a wide variety of situations, and that also achieve real-time performance. In this work, SAMUEL, a learning system based on genetic algorithms, is used to learn high-performance reactive strategies for navigation and collision avoidance. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Booker, L. B. </author> <year> (1985). </year> <title> Improving the performance of genetic algorithms in classifier systems. </title> <booktitle> Proceedings of the International Conference Genetic Algorithms and Their Applications (pp. </booktitle> <pages> 80-92). </pages> <address> Pittsburgh, PA. </address>
Reference-contexts: Production System Cycle CPS follows the match/conflict-resolution/ act cycle of traditional production systems. Since there is no guarantee that the current set of rules is in any sense complete, it is important to provide a mechanism for gracefully handling cases in which no rule matches <ref> (Booker, 1985) </ref>. In CPS this is accomplished by assigning each rule a match score equal to the number of conditions it matches. The match set consists of all the rules with the highest current match score.
Reference: <author> De Jong, K. A. </author> <year> (1975). </year> <title> Analysis of the behavior of a class of genetic adaptive systems. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer and Communications Sciences, University of Michigan, </institution> <address> Ann Arbor. </address>
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic algorithms in search, optimization, </title> <booktitle> and machine learning. </booktitle> <address> Reading: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Gordon, D. F. and J. J. </author> <title> Grefenstette (1990). Explanations of empirically derived reactive plans. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Second, it is easier to explain the knowledge learned through experience and to transfer the knowledge to human operators. Third, it is possible to combine several forms of learning in a single system <ref> (Gordon & Grefenstette, 1990) </ref>. Each CPS rule has the form if (and c 1 then (and a 1 . . . a m ) where each c i is a condition on one of the sensors and each action a j specifies a setting for one of the control variables.
Reference: <author> Grefenstette, J. J. </author> <year> (1988). </year> <title> Credit assignment in rule discovery system based on genetic algorithms. </title> <journal> Machine Learning, </journal> <volume> 3(2/3), </volume> <pages> (pp. 225-245). </pages>
Reference-contexts: In general, a given rule may specify conditions for any subset of the sensors and actions for any subset of the control variables. Each rule also has a numeric strength, that serves as a prediction of the rule's utility <ref> (Grefenstette, 1988) </ref>. The methods used to update the rule strengths is described in the section on credit assignment below. <p> It has been shown that the PSP computes a time-weighted estimate of the expected external payoff, and that this estimate is useful for conflict resolution <ref> (Grefenstette, 1988) </ref>. However, conflict resolution should take into account not only the expected payoff associated with each rule, but also some measure of our confidence in that estimate. One way to measure confidence is through the variance associated with the estimated payoff.
Reference: <author> Grefenstette, J. J. </author> <year> (1989). </year> <title> A system for learning control plans with genetic algorithms. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> Fairfax, VA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> (pp. 183-190). </pages>
Reference-contexts: Many genetic algorithms permit recombination within individual rules as a way of creating new rules (Smith, 1980; Schaffer, 1984; Holland, 1986). While such operators are easily defined for SAMUEL's rule language <ref> (Grefenstette, 1989) </ref>, we prefer to use CROSSOVER solely to explore the space of rule combinations, and leave rule modification to other operators (i.e., SPECIALIZE, GENERALIZE, CREEP, MERGE and MUTATION). In SAMUEL, CROSSOVER assigns each rule in two selected parent plans to one of two offspring plans.
Reference: <author> Grefenstette, J. J., C. L. Ramsey, and A. C. </author> <title> Schultz (1990). Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5(4), </volume> <pages> (pp. 355-381). </pages>
Reference-contexts: A Model for Learning from a Simulation Model. Previous studies have illustrated that knowledge learned under simulation is robust and might be applicable to the real world if the simulation is more general (i.e. has more noise, more varied conditions, etc.) than the real world environment <ref> (Ramsey, Schultz and Grefenstette, 1990) </ref>. 3. <p> Second, it is easier to explain the knowledge learned through experience and to transfer the knowledge to human operators. Third, it is possible to combine several forms of learning in a single system <ref> (Gordon & Grefenstette, 1990) </ref>. Each CPS rule has the form if (and c 1 then (and a 1 . . . a m ) where each c i is a condition on one of the sensors and each action a j specifies a setting for one of the control variables. <p> Of course, the new rule is likely to need further modification, and is subject to further competition with the other rules. A third approach is to seed the initial population with existing knowledge <ref> (Schultz and Grefenstette, 1990) </ref>. The rule language of SAMUEL was designed to facilitate the inclusion of available knowledge. In some cases, such as the AUV domain, random behavior will never yield a success and so the adaptive initialization will not specialize the maximally general rule and create new rules. <p> The updated strengths of the rules are also returned to the learning module. Each episode begins with randomly selected initial conditions, and thus represents a single sample of the performance of the plan on the space of all possible initial condition of the world model. <ref> (Grefenstette, Ramsey and Schultz, 1990) </ref> examined several important questions about this sampling procedure and in particular: What if the distribution used for evaluating plans differs from the true distribution of initial conditions that arise in the actual task environment? In summary, this earlier work indicated that it is important for the
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <booktitle> Adaptation in natural and artificial systems. </booktitle> <address> Ann Arbor: </address> <publisher> University Michigan Press. </publisher>
Reference-contexts: A genetic algorithm is used to perform the search. Genetic algorithms are motivated by standard models of heredity and evolution in the field of population genetics, and embody abstractions of the mechanisms of adaptation present in natural systems <ref> (Holland, 1975) </ref>. Briefly, a genetic algorithm simulates the dynamics of population genetics by maintaining a knowledge base of knowledge structures that evolves over time in response to the observed performance of its knowledge structures in their training environment.
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. In R.S. </title> <editor> Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach (Vol. 2). </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Structured nominal sensors are not used in the AUV domain. Finally, pattern sensors can take on binary string values. Conditions on pattern sensors specify patterns over the alphabet -0, 1, #-, as in classifier systems <ref> (Holland, 1986) </ref>. The AUV domain does not use pattern sensors. The right-hand side of each rule specifies a setting for one or more control variables. For the AUV problem, each rule speci fies a setting for the variable turn, and a setting for the variable speed.
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology for inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20(2), </volume> <pages> (pp. 111-161). </pages>
Reference-contexts: The choice of an appropriate learning technique depends on the nature of the performance task and the form of available knowledge. If the performance task is classification, and a large number of training examples are available, then inductive learning techniques <ref> (Michalski, 1983) </ref> can be used to learn classification rules. If there exists an extensive domain theory and a source of expert behavior, then explanation-based methods may be applied (Mitchell, Mahadevan & Steinberg, 1985).
Reference: <author> Mitchell, T. M., S. Mahadevan and L. </author> <title> Steinberg (1985). LEAP: A learning apprentice for VLSI design. </title> <booktitle> Proc. Ninth IJCAI, </booktitle> <pages> (pp. 573-580). </pages> <address> Los Angeles: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If the performance task is classification, and a large number of training examples are available, then inductive learning techniques (Michalski, 1983) can be used to learn classification rules. If there exists an extensive domain theory and a source of expert behavior, then explanation-based methods may be applied <ref> (Mitchell, Mahadevan & Steinberg, 1985) </ref>. Many interesting practical problems that may be amenable to automated learning do not fit either of these models. One such class of problems is the class of sequential decision tasks, such as the AUV task described in this work.
Reference: <author> Ramsey, C. L., Alan C. Schultz and J. J. </author> <title> Grefenstette (1990). Simulation-assisted learning by competition: Effects of noise differences between training model and target environment. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <address> Austin, TX: </address> <publisher> Morgan Kaufmann (pp. </publisher> <pages> 211-215). </pages>
Reference-contexts: A Model for Learning from a Simulation Model. Previous studies have illustrated that knowledge learned under simulation is robust and might be applicable to the real world if the simulation is more general (i.e. has more noise, more varied conditions, etc.) than the real world environment <ref> (Ramsey, Schultz and Grefenstette, 1990) </ref>. 3. <p> The updated strengths of the rules are also returned to the learning module. Each episode begins with randomly selected initial conditions, and thus represents a single sample of the performance of the plan on the space of all possible initial condition of the world model. <ref> (Grefenstette, Ramsey and Schultz, 1990) </ref> examined several important questions about this sampling procedure and in particular: What if the distribution used for evaluating plans differs from the true distribution of initial conditions that arise in the actual task environment? In summary, this earlier work indicated that it is important for the
Reference: <author> Riolo, R. L. </author> <year> (1988). </year> <title> Empirical studies of default hierarchies and sequences of rules in learning classifier systems, </title> <type> Doctoral dissertation, Doctoral dissertation, </type> <institution> Department of Electrical Engineering and Computer Science, University of Michigan, </institution> <address> Ann Arbor. </address>
Reference-contexts: Each possible action receives a bid equal to the strength of the strongest rule in the match set that specifies that action in its right-hand side. Unlike classifier systems in which all members of the match set vote on which action to perform <ref> (Riolo, 1988) </ref>, CPS selects an action using the probability distribution defined by the strength of the (single) bidder for each action. This prevents a large number of low strength rules from combining to suggest an action that is actually associated with low payoff.
Reference: <author> Schaffer, J. D. </author> <year> (1984). </year> <title> Some experiments in machine learning using vector evaluated genetic algorithms, </title> <type> Doctoral dissertation, </type> <institution> Department of Electrical and Biomedical Engineering, Vanderbilt University, Nashville. </institution>
Reference: <author> Schultz, A. C. and J. J. </author> <title> Grefenstette (1990). Improving tactical plans with genetic algorithms. </title> <booktitle> Proceeding of IEEE Conference on Tools for AI 90, </booktitle> <address> Washington, DC: </address> <publisher> IEEE (pp. </publisher> <pages> 328-334). </pages>
Reference-contexts: A Model for Learning from a Simulation Model. Previous studies have illustrated that knowledge learned under simulation is robust and might be applicable to the real world if the simulation is more general (i.e. has more noise, more varied conditions, etc.) than the real world environment <ref> (Ramsey, Schultz and Grefenstette, 1990) </ref>. 3. <p> Of course, the new rule is likely to need further modification, and is subject to further competition with the other rules. A third approach is to seed the initial population with existing knowledge <ref> (Schultz and Grefenstette, 1990) </ref>. The rule language of SAMUEL was designed to facilitate the inclusion of available knowledge. In some cases, such as the AUV domain, random behavior will never yield a success and so the adaptive initialization will not specialize the maximally general rule and create new rules. <p> The updated strengths of the rules are also returned to the learning module. Each episode begins with randomly selected initial conditions, and thus represents a single sample of the performance of the plan on the space of all possible initial condition of the world model. <ref> (Grefenstette, Ramsey and Schultz, 1990) </ref> examined several important questions about this sampling procedure and in particular: What if the distribution used for evaluating plans differs from the true distribution of initial conditions that arise in the actual task environment? In summary, this earlier work indicated that it is important for the
Reference: <author> Wilson, S. W. </author> <year> (1987). </year> <title> Classifier systems and the animat problem. </title> <journal> Machine Learning, </journal> <volume> 2(3), </volume> <pages> (pp. 199-228). </pages>
Reference-contexts: This prevents a large number of low strength rules from combining to suggest an action that is actually associated with low payoff. All rules in the match set that agree with the selected action are said to be active <ref> (Wilson, 1987) </ref>, and will have their strength adjusted according to the credit assignment algorithm described in the next section. After conflict resolution, the control variables are set to the values indicated by the selected actions. 3 The world model is then advanced by one simulation step.
References-found: 16

