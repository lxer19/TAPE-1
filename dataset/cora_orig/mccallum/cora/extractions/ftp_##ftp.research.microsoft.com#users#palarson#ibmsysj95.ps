URL: ftp://ftp.research.microsoft.com/users/palarson/ibmsysj95.ps
Refering-URL: http://www.research.microsoft.com/~palarson/publications.htm
Root-URL: http://www.research.microsoft.com
Title: The CORDS Multidatabase Project  
Author: Gopi K. Attaluri Dexter P. Bradshaw Per -Ake Larson Patrick Martin Avi Silberschatz Jacob Slonim Qiang Zhu 
Note: Contents  
Address: Neil Coburn  
Affiliation: University of Waterloo  University of Waterloo  University of Waterloo  University of Waterloo  Queen's University  University of Texas, Austin  ibm Toronto Laboratory  University of Waterloo  
Abstract: In virtually every organization, data is stored in a variety of ways and managed by different database and file systems. Applications that require data from multiple sources are complex because they must be aware of and deal with the specifics of each data source. They must also perform any data integration needed, for example, joining data from multiple sources. The objective of a multidatabase system is to provide application developers and end users with an integrated view of and a uniform interface to all the required data. The view and the interface should be independent of where the data is stored and how it is managed. cords is a research project focussed on distributed applications. It is a collaborative effort involving ibm and several universities. As part of this project, we are designing and prototyping a multidatabase system. This paper provides an overview of its architecture and describes the approach taken in the following areas: management of catalog information, schema integration, global query optimization, (distributed) transaction management, and interactions with component data sources. The prototype system gives application developers a view of a single relational database system. Currently supported component data sources include several relational database systems, a hierarchical database system, and a network database system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gopi Attaluri and Dexter P. Bradshaw. </author> <title> Architecture for Transaction Management in the CORDS Multidatabase Service. </title> <booktitle> In Proceedings of the 1993 CAS Conference, </booktitle> <pages> pages 873-887, </pages> <address> Toronto, Ontario, Canada, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: A more detailed discussion of the cords-mdbs, and its relationship to the cords distributed transaction services model is given in <ref> [1] </ref>. 7 Conclusion Data integration systems may provide many different levels of service. The simplest systems may provide nothing more than connectivity, that is, the ability for an application to access data stored in multiple database systems.
Reference: [2] <author> Gopi K. Attaluri. </author> <title> Logical Concurrency Control for Large Objects in a Multidatabase System. </title> <booktitle> In Proceedings of the 1993 CAS Conference, </booktitle> <volume> volume II, </volume> <pages> pages 860-872, </pages> <address> Toronto, Ontario, Canada, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: Secondly, determining whether locks on two subobjects conflict is non-trivial because of their non-uniform size. A solution is to model subobjects as one or multidimensional ranges. Ranges can be dynamically indexed to lock or unlock subobjects efficiently. An indexing algorithm for this purpose has been described in <ref> [2] </ref>. 6.7 Multidatabase Transaction Management Subsystem The transaction management subsystem consists of a global transaction manager (gtm), a set of local transaction managers (ltm) at cdss, and multidatabase transaction processing agents (mta) that run as part of the mdbs Agents at cdss.
Reference: [3] <author> Gopi K. Attaluri. </author> <title> An efficient algorithm for dynamic indexing of spatial objects. </title> <year> 1994. </year>
Reference-contexts: The concurrency control and recovery component is the most involved because, for each data object, concurrent operations of logical transactions have to be supported through a single ltm physical transaction. We proposed an lcc scheme in <ref> [3] </ref>. We described the concurrency control and recovery mechanisms and important implementations of a lcc assuming a locking based data source and area-wise granulariza-tion of multidimensional data objects. We are implementing lcc schemes on two types of data (text and multidimensional data) using ObjectStore [37], an object-oriented database system.
Reference: [4] <author> Michael A. Bauer, Neil Coburn, Doreen L. Erickson, Patrick J. Finnigan, James W. Hong, Per -Ake Larson, Jan Pachl, Jacob Slonim, David J. Taylor, and Toby J. Teorey. </author> <title> A Multi-Level Architecture for Distributed Applications. </title> <journal> IBM Systems Journal, </journal> <volume> 33(3) </volume> <pages> 399-425, </pages> <year> 1994. </year>
Reference-contexts: However, simply being able to "get at" the data is not enough. cords is a research project focussed on distributed applications. It is a collaborative effort involving ibm and several universities. More information can be found in <ref> [4] </ref>. As part of this project, we have designed and prototyped an mdbs called the cords-mdbs, that provides an integrated, relational view 2 of multiple heterogeneous database systems. Currently, five data sources are supported: three different relational database systems, a network database system, and a hierarchical database system. <p> some of the catalog information must be globally available so that a request can effectively be serviced by any mdbs server. 3 Architecture and Prototype Implementation Within the cords project, the mdbs acts as one of the Data Services offered by the cords Service Environment (cse) (see Bauer et al. <ref> [5, 4] </ref>). It is designed to offer the full functionality of an existing commercial database management system (dbms). This section presents the overall architecture of the cords-mdbs and briefly describes the current prototype implementation.
Reference: [5] <author> Michael A. Bauer, Neil Coburn, Doreen L. Erickson, Patrick J. Finnigan, James W. Hong, Per -Ake Larson, and Jacob Slonim. </author> <title> An Integrated Architecture for Distributed Applications. </title> <booktitle> In Proceedings of the 1993 CAS Conference, </booktitle> <address> Toronto, Canada, </address> <year> 1993. </year>
Reference-contexts: some of the catalog information must be globally available so that a request can effectively be serviced by any mdbs server. 3 Architecture and Prototype Implementation Within the cords project, the mdbs acts as one of the Data Services offered by the cords Service Environment (cse) (see Bauer et al. <ref> [5, 4] </ref>). It is designed to offer the full functionality of an existing commercial database management system (dbms). This section presents the overall architecture of the cords-mdbs and briefly describes the current prototype implementation.
Reference: [6] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. Addison-Wesley Series in Computer Science. </title> <type> Addison-Wesley, </type> <institution> United States of America, </institution> <year> 1987. </year>
Reference-contexts: The difficulty stems from the fact that local transaction, unknown to the mdbs, may interfere. To ensure the atomicity property of global transactions the mdbs must, in general, use an atomic commit protocol <ref> [6] </ref>. However, an atomic commit protocol is not sufficient to ensure correct global schedules. Local transactions may cause a situation where all local schedules are correct but the global schedule is not. <p> We say that a multidatabase schedule is locally serializable if all the schedules at component data sources in the mdbs are conflict serializable <ref> [6] </ref>. A multidatabase schedule is globally serializable if it is locally serializable, and the relative serialization order of global subtransactions is the same at each data source. Ensuring that schedules are globally serializable is achieved by ensuring that global subtransactions execute in the same relative order at each data source. <p> Global Serializability Schemes This section briefly describes some concurrency control schemes that can be used to ensure global serial-izability given an atomic commitment protocol, such as the two-phase commit (2pc) protocol <ref> [6] </ref>. Note 17 that the assumption of an atomic commitment protocol compromises execution autonomy at component data sources. If all component data sources guarantee locally serializable execution histories the following concurrency control mechanisms guarantee globally serializable multidatabase transaction executions. <p> Recovery specifies that the effects of a global transaction are undone completely after it is aborted, or remain entirely durable after system failures. In general, the global atomicity property is ensured by using an atomic commit protocol <ref> [6] </ref>. Once component data sources are locally recoverable, global atomic commitment also guarantees global recovery. An atomic commitment protocol requires that each participating cds provides a prepared state for each subtransaction. The subtransaction should remain in the prepared state until the coordinator decides whether to commit or abort the transaction.
Reference: [7] <author> D. P. Bradshaw, P. A. Larson, and J. </author> <title> Slonim. Transaction Scheduling in Dynamic Composite Multi-database Systems. </title> <type> Technical Report CS-94-11, </type> <institution> Department of Computer Science, Univertsity of Waterloo, Waterloo, </institution> <address> Ontario, Canada, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Furthermore, nested transactions provide static structural power for composing strict transaction hierarchies while still maintaining global serializability. More detailed discussions on dynamic multidatabase composition, concurrency control, and recovery in the cords-mdbs can be found in <ref> [7, 9] </ref>. 6.3 Global Serializability Generally, conflict serializability is adopted as the correctness criterion for the execution of concurrent transactions at a data source. We say that a multidatabase schedule is locally serializable if all the schedules at component data sources in the mdbs are conflict serializable [6].
Reference: [8] <author> Dexter P. Bradshaw. </author> <title> Open Nested Serializability in Multidatabase Systems. </title> <editor> In M. Bauer, J. Botsford, P. A. Larson, and J. Slonim, editors, </editor> <booktitle> Proceedings of the 1992 CAS Conference, </booktitle> <volume> Volume II, </volume> <pages> pages 93-109, </pages> <address> Toronto, Ontario, Canada, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Therefore, multidatabase systems can be composed arbitrarily and dynamically to resolve any global transaction that spans at least two multidatabase environments. We have shown that guaranteeing multidatabase serializability [32] at multidatabase servers is sufficient for guaranteeing global multidatabase serializability for a dynamic hierarchically composed set of multi-database servers <ref> [8] </ref>. This follows from the property that multidatabase serializability guarantees both atomicity and a consistent ordering of global transactions as they execute from leaf component database sites to the root multidatabase server. However, no sufficiency conditions have been developed for global serializability in arbitrary execution lattices.
Reference: [9] <author> Dexter P. Bradshaw. </author> <title> Composite Multidatabase System Concurrency Control and Recovery. </title> <booktitle> In Proceedings of CASCON'93 Volume II: Distributed Computing, </booktitle> <pages> pages 895-909, </pages> <address> Toronto, Ontario, Canada, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: Furthermore, nested transactions provide static structural power for composing strict transaction hierarchies while still maintaining global serializability. More detailed discussions on dynamic multidatabase composition, concurrency control, and recovery in the cords-mdbs can be found in <ref> [7, 9] </ref>. 6.3 Global Serializability Generally, conflict serializability is adopted as the correctness criterion for the execution of concurrent transactions at a data source. We say that a multidatabase schedule is locally serializable if all the schedules at component data sources in the mdbs are conflict serializable [6].
Reference: [10] <author> Y. Breitbart and A. Silberschatz. </author> <title> Complexity of Global Transaction Management in Multidatabase Systems. </title> <type> Technical Report 198-91, </type> <institution> University of Kentucky, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: If all cdss generate strongly recoverable schedules, then the serial execution of global transactions ensures global serializability. Various concurrency control mechanisms discussed in literature - such as, 2pl, timestamp ordering [20], and optimistic [27], for example can be easily modified to generate strongly recoverable schedules <ref> [10, 41] </ref>. Rigorous Component Data Sources: Some concurrency control mechanisms generate local schedules that are even more restrictive than strongly recoverable schedules, and can, in turn, lead to even more efficient global scheduling schemes. One such concurrency control scheme is the strong-strict 2pl protocol that generates rigorous schedules.
Reference: [11] <author> Yuri Breitbart, Dimitrios Georgakopoulos, Marek Rusinkiewicz, and Abraham Silberschatz. </author> <title> On Rigorous Transaction Scheduling. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(4), </volume> <month> September </month> <year> 1991. </year> <title> Multidatabase Transaction Management. </title> <type> 24 </type>
Reference-contexts: Strongly Recoverable Data Sources: Given some knowledge of the properties of the various local schedules that are generated by the local concurrency control schedulers, effective global concurrency control schemes can be devised. Of particular interest is the notion of a strongly recoverable schedule <ref> [11, 41] </ref>. Strongly recoverable schedulers produce serializable schedules where the order of transaction execution and serialization orders are the same. If all cdss generate strongly recoverable schedules, then the serial execution of global transactions ensures global serializability. <p> One such concurrency control scheme is the strong-strict 2pl protocol that generates rigorous schedules. Rigorous schedulers are strongly recoverable, but moreover, prevent read-write, write-read and write-write conflicts among uncommitted transactions. In <ref> [11] </ref>, it is shown that, if local dbms schedulers are rigorous and the gtm guarantees the atomic commitment of all global transactions, the global schedule is serializable. Other protocols can be easily modified to generate rigorous schedules. <p> For example, basic timestamp ordering can be made rigorous by blocking transactions that either try to read or write data that was previously written by an uncommitted transaction or try to write data that was read by an uncommitted transaction <ref> [11] </ref>. 6.4 Global Atomicity Global atomicity specifies that either all subtransactions of a global transaction run successfully at their local cdss, or they all abort. Recovery specifies that the effects of a global transaction are undone completely after it is aborted, or remain entirely durable after system failures.
Reference: [12] <author> Yuri Breitbart, Avi Silberschatz, and Glenn R. Thompson. </author> <title> Reliable Transaction Management in a Multidatabase System. </title> <editor> In Hector Garcia-Molina and H.V. Jagadish, editors, </editor> <booktitle> Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data, </booktitle> <address> Atlantic City, New Jersey, U.S.A., </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Work on 2pc emulation through the Two-Phase Commit Agent method (2pca) was first published by Wolski and Veijalainen [45, 47]. Alternative schemes for guaranteeing global atomic commitment with one-phase commit resource managers are described by Gray [23] and Breitbart et. al. <ref> [12] </ref>. Work on superdatabases, published by Pu [40], is the only known work on multidatabase composition. A superdatabase is analogous to a multidatabase, and can have other superdatabases as cdss. In this work, superdatabase composition is restricted to a strict static tree.
Reference: [13] <author> CCITT. </author> <note> X.500 Directory Services 1992, </note> <year> 1991. </year>
Reference-contexts: The mdbs Catalog must also support a global naming scheme that can be used to uniquely identify objects on distributed heterogeneous cdss. For these reasons, we decided to implement the prototype mdbs Catalog using an X.500 directory <ref> [13] </ref>. The Catalog module is implemented on top the ean X.500 Directory Service [36], which was extended to provide full transaction support. The Catalog module interacts with the Directory Service via a Directory User Agent (dua) that is responsible for querying the directory.
Reference: [14] <author> C. Chung. DATAPLEX: </author> <title> An Access to Heterogeneous Distributed Databases. </title> <journal> Communications of the ACM, </journal> <volume> 33(1) </volume> <pages> 70-80, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The problem of schema integration in an mdbs has received a great deal of attention and a number of prototype systems which attempt to perform integration have been reported <ref> [14, 18, 35] </ref>. The only commercially available system, to our knowledge, with schema integration facilities is InterViso [17]. All of these systems only address certain aspects of the integration process.
Reference: [15] <author> John R. Corbin. </author> <title> The Art of Distributed Applications. Sun Technical Reference Library. </title> <publisher> Springer-Verlag, </publisher> <address> New York, New York, U.S.A., </address> <year> 1991. </year>
Reference-contexts: Communication Inter-process communication is supported by the mdbs Client and Server libraries. The Client library supports a (draft) version of the Microsoft odbc interface. It translates odbc calls into ibm Distributed Data Management (ddm) [25] messages that it ships via Sun rpc <ref> [15] </ref> or, the Encina Transactional rpc [44] according to the drda protocol. The Server library accepts the rpc calls and translates them back into odbc calls. Schema Integration The schema integration component is an environment to support users during the integration process.
Reference: [16] <author> J.R. Cordy, </author> <title> C.D. Halpern, and E.M. Promislow. TXL: A Rapid Prototyping System for Programming Language Dialects. </title> <journal> Computer Languages, </journal> <volume> 16(1) </volume> <pages> 97-107, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: These tasks include the following: * Translate a local schema (or portion of a local schema) into its corresponding representation in the common data model. The current mdbs View Builder provides a number of translators based on structural transformation <ref> [16] </ref> which translate between relational, entity-relationship, hierarchical, and network schemas.
Reference: [17] <institution> Data Integration Inc., California, U.S.A. </institution> <note> InterViso User's Manual, </note> <year> 1992. </year>
Reference-contexts: The problem of schema integration in an mdbs has received a great deal of attention and a number of prototype systems which attempt to perform integration have been reported [14, 18, 35]. The only commercially available system, to our knowledge, with schema integration facilities is InterViso <ref> [17] </ref>. All of these systems only address certain aspects of the integration process.
Reference: [18] <author> U. Dayal and H. Hwang. </author> <title> View Definition and Generalization for Database Integration in a Multidatabase System. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 10(6) </volume> <pages> 628-644, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: The problem of schema integration in an mdbs has received a great deal of attention and a number of prototype systems which attempt to perform integration have been reported <ref> [14, 18, 35] </ref>. The only commercially available system, to our knowledge, with schema integration facilities is InterViso [17]. All of these systems only address certain aspects of the integration process.
Reference: [19] <author> W. Du, R. Krishnamurthy, and M. C. Shan. </author> <title> Query Optimization in Heterogeneous DBMS. </title> <booktitle> In Proceedings of the Eightenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 277-291, </pages> <address> Vancouver, British Columbia, Canada, </address> <year> 1992. </year>
Reference-contexts: Little research has been reported on query optimization in multidatabase systems. Lu et al. [29, 30] discuss some differences in global query optimization between an mdbs and a traditional distributed database 12 system (ddbs). They also describe a framework for a global query optimizer. Du et al. <ref> [19] </ref> proposed a calibration method for deriving local cost functions. However, the proposed method has several shortcomings. 5.1 Estimating Component Query Cost and Output Size An execution plan produced by the cords-mdbs query optimizer consists of two parts: component queries and an integration plan.
Reference: [20] <author> K. Eswaran, J. Gray, R. Lorie, and I. Traiger. </author> <title> The Notion of Consistency and Predicate Locks in a Database System. </title> <journal> Communications of the ACM, </journal> <month> 19(11), November </month> <year> 1976. </year>
Reference-contexts: Strongly recoverable schedulers produce serializable schedules where the order of transaction execution and serialization orders are the same. If all cdss generate strongly recoverable schedules, then the serial execution of global transactions ensures global serializability. Various concurrency control mechanisms discussed in literature - such as, 2pl, timestamp ordering <ref> [20] </ref>, and optimistic [27], for example can be easily modified to generate strongly recoverable schedules [10, 41]. Rigorous Component Data Sources: Some concurrency control mechanisms generate local schedules that are even more restrictive than strongly recoverable schedules, and can, in turn, lead to even more efficient global scheduling schemes.
Reference: [21] <author> Dimitrios Georgakopoulos, Marek Rusinkiewicz, and Amit Sheth. </author> <title> On Serializability of Multidatabase Transactions Through Forced Local Conflicts. </title> <booktitle> In Proceedings of the Seventh International Conference on Data Engineering, </booktitle> <pages> pages 314-323, </pages> <address> Kobe, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Furthermore, we attempt to extend the algorithms to deal with the more complex issues of multidatabase composition and integrating transaction management schemes for large objects. 6.1 Related Work The work on multidatabase concurrency control focuses on guaranteeing global serializability through rigorous scheduling and forced conflicts <ref> [21] </ref>. Breitbart et. al. [51] completed a comprehensive survey of alternative schemes for guaranteeing global serializability in multidatabase systems. The survey also covers deadlock handling and concurrency control schemes in which either serializability or autonomy constraints are relaxed. <p> If all component data sources guarantee locally serializable execution histories the following concurrency control mechanisms guarantee globally serializable multidatabase transaction executions. Forced Conflicts: One method for guaranteeing global serializability works by forcing direct conflicts among global transactions at component data sources <ref> [21] </ref>. This method uses a special data item, called a ticket, that is maintained at each local site. A single ticket is required for each component site, but tickets at different component data sources are distinct. Only global transactions are allowed to access a ticket.
Reference: [22] <author> J.N. Gray, R.A. Lorie, G.F. Putzolu, and I. Traiger. </author> <title> Granularity of Locks and Degrees of Consistency in a Shared Data Base. </title> <editor> In G.N. Nijssen, editor, </editor> <booktitle> Modeling in Data Base Management Systems, </booktitle> <pages> pages 365-394. </pages> <publisher> North-Holland, </publisher> <year> 1976. </year>
Reference-contexts: Several papers have recognized its importance and highlighted some issues. A survey of issues in mdbs transaction management and non-structured data sources (large data objects and long transactions) can be found in [43]. The multi-granularity locking scheme <ref> [22] </ref> is a well-known concurrency control scheme for hierarchically structured large objects. Variations or extensions to this scheme have been described in [26, 28]. 16 6.2 Multidatabase Composition Current work on mdbss assumes a two-level architecture comprising a monolithic multidatabase server and a collection of component database systems.
Reference: [23] <author> J.N. Gray and A. Reuters. </author> <title> Transaction Processing: Concepts and Techniques. Morgan Kaufmann Series in Data Management Systems. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, U.S.A., </address> <year> 1993. </year>
Reference-contexts: Work on 2pc emulation through the Two-Phase Commit Agent method (2pca) was first published by Wolski and Veijalainen [45, 47]. Alternative schemes for guaranteeing global atomic commitment with one-phase commit resource managers are described by Gray <ref> [23] </ref> and Breitbart et. al. [12]. Work on superdatabases, published by Pu [40], is the only known work on multidatabase composition. A superdatabase is analogous to a multidatabase, and can have other superdatabases as cdss. In this work, superdatabase composition is restricted to a strict static tree.
Reference: [24] <author> International Business Machines Corporation. </author> <title> Distributed Relational Database Architecture Reference, </title> <address> sc26-4651-0 edition, </address> <year> 1990. </year>
Reference-contexts: Application development is simplified if the two database systems support a common interface. Examples of such interfaces are the Microsoft Open Database Connectivity (odbc) [33] suite of functions, the X/Open sql Call Level Interface (cli) [50, 48], and the ibm Distributed Relational Database Architecture (drda) <ref> [24] </ref>. The application is still aware that it is dealing with multiple data sources, but now their interfaces are the same. Integration processing, however, is still the responsibility of the application.
Reference: [25] <institution> International Business Machines Corporation, Rochester, </institution> <address> U.S.A. </address> <booktitle> IBM Distributed Data Management Level 3 Architecture: Implementation Programmer's Guide, </booktitle> <address> SC21-9529, </address> <year> 1990. </year>
Reference-contexts: The amount of prototype code exceeds 200 000 lines. Communication Inter-process communication is supported by the mdbs Client and Server libraries. The Client library supports a (draft) version of the Microsoft odbc interface. It translates odbc calls into ibm Distributed Data Management (ddm) <ref> [25] </ref> messages that it ships via Sun rpc [15] or, the Encina Transactional rpc [44] according to the drda protocol. The Server library accepts the rpc calls and translates them back into odbc calls. Schema Integration The schema integration component is an environment to support users during the integration process.
Reference: [26] <author> Ashok M. Joshi. </author> <title> Adaptive Locking Strategies in a Multi-node Data Sharing Environment. </title> <booktitle> In Proceedings of the Seventeenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 181-191, </pages> <address> Catalonia, Spain, </address> <year> 1991. </year>
Reference-contexts: A survey of issues in mdbs transaction management and non-structured data sources (large data objects and long transactions) can be found in [43]. The multi-granularity locking scheme [22] is a well-known concurrency control scheme for hierarchically structured large objects. Variations or extensions to this scheme have been described in <ref> [26, 28] </ref>. 16 6.2 Multidatabase Composition Current work on mdbss assumes a two-level architecture comprising a monolithic multidatabase server and a collection of component database systems. This centralized view is restrictive and does not scale to include multiple cooperating multidatabase systems.
Reference: [27] <author> H. Kung and J. Robinson. </author> <title> On Optimistic Methods for Concurrency Control. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 6(2), </volume> <month> June </month> <year> 1981. </year>
Reference-contexts: If all cdss generate strongly recoverable schedules, then the serial execution of global transactions ensures global serializability. Various concurrency control mechanisms discussed in literature - such as, 2pl, timestamp ordering [20], and optimistic <ref> [27] </ref>, for example can be easily modified to generate strongly recoverable schedules [10, 41]. Rigorous Component Data Sources: Some concurrency control mechanisms generate local schedules that are even more restrictive than strongly recoverable schedules, and can, in turn, lead to even more efficient global scheduling schemes.
Reference: [28] <author> David Lomet. </author> <title> Private Lock Management. </title> <type> Technical Report CRL 92/9, </type> <institution> Digital Equipment Corporation, Cambridge Research Lab, </institution> <month> November </month> <year> 1992. </year> <month> 25 </month>
Reference-contexts: A survey of issues in mdbs transaction management and non-structured data sources (large data objects and long transactions) can be found in [43]. The multi-granularity locking scheme [22] is a well-known concurrency control scheme for hierarchically structured large objects. Variations or extensions to this scheme have been described in <ref> [26, 28] </ref>. 16 6.2 Multidatabase Composition Current work on mdbss assumes a two-level architecture comprising a monolithic multidatabase server and a collection of component database systems. This centralized view is restrictive and does not scale to include multiple cooperating multidatabase systems.
Reference: [29] <author> H. Lu, B.-C. Ooi, and C.-H. Goh. </author> <title> On global multidatabase query optimization. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 21(4) </volume> <pages> 6-11, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: We ignore file systems and consider only cdss that are database systems. Little research has been reported on query optimization in multidatabase systems. Lu et al. <ref> [29, 30] </ref> discuss some differences in global query optimization between an mdbs and a traditional distributed database 12 system (ddbs). They also describe a framework for a global query optimizer. Du et al. [19] proposed a calibration method for deriving local cost functions.
Reference: [30] <author> H. Lu and M.-C. Shan. </author> <title> On global query optimization in multidatabase systems. </title> <booktitle> In 2nd Int'l Workshop on Research Issues on Data Eng., </booktitle> <pages> page 217, </pages> <address> Tempe, Arizona, USA, </address> <year> 1992. </year>
Reference-contexts: We ignore file systems and consider only cdss that are database systems. Little research has been reported on query optimization in multidatabase systems. Lu et al. <ref> [29, 30] </ref> discuss some differences in global query optimization between an mdbs and a traditional distributed database 12 system (ddbs). They also describe a framework for a global query optimizer. Du et al. [19] proposed a calibration method for deriving local cost functions.
Reference: [31] <author> P. Martin and W. Powley. </author> <title> Database Integration Using Multidatabase Views. </title> <booktitle> In Proceedings of the 1993 CAS Conference, </booktitle> <pages> pages 779-788, </pages> <address> Toronto, Ontario, Canada, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: Details of the prototype system can be found in <ref> [31] </ref>. 5 Global Query Optimization As mentioned, multidatabase query optimization differs from traditional distributed query optimization in two respects: handling data sources with different query processing capabilities and lack of query optimization information at the global level.
Reference: [32] <author> S. Mehrotra, R. Rastogi, Y. Breitbart, H. F. Korth, and A. Silberschatz. </author> <title> Ensuring Transaction Atomicity in Multidatabase Systems. </title> <booktitle> In Proceedings of the 1992 Conference on Principles of Database Systems, </booktitle> <year> 1992. </year>
Reference-contexts: Therefore, multidatabase systems can be composed arbitrarily and dynamically to resolve any global transaction that spans at least two multidatabase environments. We have shown that guaranteeing multidatabase serializability <ref> [32] </ref> at multidatabase servers is sufficient for guaranteeing global multidatabase serializability for a dynamic hierarchically composed set of multi-database servers [8].
Reference: [33] <author> Microsoft, Inc. </author> <title> Microsoft ODBC Application Programmer's Guide, </title> <type> draft edition, </type> <year> 1991. </year>
Reference-contexts: On retrieval, it also has to perform the processing needed to combine data from the two databases. Application development is simplified if the two database systems support a common interface. Examples of such interfaces are the Microsoft Open Database Connectivity (odbc) <ref> [33] </ref> suite of functions, the X/Open sql Call Level Interface (cli) [50, 48], and the ibm Distributed Relational Database Architecture (drda) [24]. The application is still aware that it is dealing with multiple data sources, but now their interfaces are the same.
Reference: [34] <author> J. Elliot B. Moss. </author> <title> Nested Transactions: An Approach to Reliable Distributed Computing. </title> <publisher> MIT Press Series in Information Systems. MIT Press, </publisher> <address> Cambridge, Massachussetts, </address> <year> 1985. </year>
Reference-contexts: This follows from the property that multidatabase serializability guarantees both atomicity and a consistent ordering of global transactions as they execute from leaf component database sites to the root multidatabase server. However, no sufficiency conditions have been developed for global serializability in arbitrary execution lattices. Nested transactions <ref> [34, 42] </ref> provide useful conceptual tools for propagating work among multidatabase servers and encapsulating failure recovery. Nested transactions also furnish two useful properties: intra-transaction parallelism and failure isolation. Unlike flat transactions, nested transaction blocks do not have to execute serially.
Reference: [35] <author> A. Motro. Superviews: </author> <title> Virtual Integration of Multiple Databases. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 13(7) </volume> <pages> 785-798, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The problem of schema integration in an mdbs has received a great deal of attention and a number of prototype systems which attempt to perform integration have been reported <ref> [14, 18, 35] </ref>. The only commercially available system, to our knowledge, with schema integration facilities is InterViso [17]. All of these systems only address certain aspects of the integration process.
Reference: [36] <author> G. Neufeld, B. Brachman, M. Goldberg, and D. Stickings. </author> <title> The EAN X.500 Directory Service. </title> <journal> Journal of Internetworking Research and Experience, </journal> <volume> 3(2) </volume> <pages> 55-82, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: For these reasons, we decided to implement the prototype mdbs Catalog using an X.500 directory [13]. The Catalog module is implemented on top the ean X.500 Directory Service <ref> [36] </ref>, which was extended to provide full transaction support. The Catalog module interacts with the Directory Service via a Directory User Agent (dua) that is responsible for querying the directory. The Directory Service itself is transparently distributed over physically separated entities called Directory System Agents (dsa).
Reference: [37] <institution> Object Design Inc., Burlington, </institution> <address> MA 01803, U.S.A. </address> <note> Reference Manual: ObjectStore Release 3.0 for OS/2 and AIX/xlC 300-320-002 3C, </note> <month> January </month> <year> 1994. </year>
Reference-contexts: We described the concurrency control and recovery mechanisms and important implementations of a lcc assuming a locking based data source and area-wise granulariza-tion of multidimensional data objects. We are implementing lcc schemes on two types of data (text and multidimensional data) using ObjectStore <ref> [37] </ref>, an object-oriented database system. ObjectStore does not employ explicit object or subobject locking, but uses two levels of isolation: page locking and object check-in/check-out. A page lock is implicitly acquired by a transaction when it accesses a part of an object located on that page.
Reference: [38] <institution> Open Software Foundation, </institution> <address> Cambridge, U.S.A. </address> <note> Introduction to OSF DCE, </note> <year> 1991. </year>
Reference-contexts: This structure does not represent our final goal for the architecture of a complete mdbs but it provides a reasonable subset of the functionality of a complete system. The system was built on top of aix and Open Software Foundation (osf) Distributed Computing Environment (dce) <ref> [38] </ref>. The amount of prototype code exceeds 200 000 lines. Communication Inter-process communication is supported by the mdbs Client and Server libraries. The Client library supports a (draft) version of the Microsoft odbc interface.
Reference: [39] <author> G.N. Paulley. </author> <title> Engineering an IMS SQL Gateway. </title> <booktitle> In The 1993 Workshop on Interoperability of Database Systems and Database Applications, Fribourg, </booktitle> <address> Switzerland, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Each agent, therefore, includes a postprocessing engine that handles the processing that cannot be done efficiently by nested-loops. The postprocessing engine must be capable of performing all the normal relational operations. Some additional information on these agents can be found in <ref> [39] </ref>. 4 Schema Integration The schema integration component of an mdbs provides the schema definitions and mappings required to facilitate the global applications of the mdbs. Its main goal is to help identify, and integrate, semantically similar objects in the contributing schemas.
Reference: [40] <author> C. Pu. </author> <title> Superdatabases for Composition of Heterogeneous Databases. </title> <booktitle> In Proceedings of the Fourth International Conference on Data Engineering, </booktitle> <pages> pages 548-555, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Work on 2pc emulation through the Two-Phase Commit Agent method (2pca) was first published by Wolski and Veijalainen [45, 47]. Alternative schemes for guaranteeing global atomic commitment with one-phase commit resource managers are described by Gray [23] and Breitbart et. al. [12]. Work on superdatabases, published by Pu <ref> [40] </ref>, is the only known work on multidatabase composition. A superdatabase is analogous to a multidatabase, and can have other superdatabases as cdss. In this work, superdatabase composition is restricted to a strict static tree. Our work on multidatabase composition focuses on dynamic composition.
Reference: [41] <author> Y. Raz. </author> <title> The Principle of Commit Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers. </title> <type> Technical report, </type> <institution> Digital Equipment Corporation, </institution> <year> 1991. </year>
Reference-contexts: Strongly Recoverable Data Sources: Given some knowledge of the properties of the various local schedules that are generated by the local concurrency control schedulers, effective global concurrency control schemes can be devised. Of particular interest is the notion of a strongly recoverable schedule <ref> [11, 41] </ref>. Strongly recoverable schedulers produce serializable schedules where the order of transaction execution and serialization orders are the same. If all cdss generate strongly recoverable schedules, then the serial execution of global transactions ensures global serializability. <p> If all cdss generate strongly recoverable schedules, then the serial execution of global transactions ensures global serializability. Various concurrency control mechanisms discussed in literature - such as, 2pl, timestamp ordering [20], and optimistic [27], for example can be easily modified to generate strongly recoverable schedules <ref> [10, 41] </ref>. Rigorous Component Data Sources: Some concurrency control mechanisms generate local schedules that are even more restrictive than strongly recoverable schedules, and can, in turn, lead to even more efficient global scheduling schemes. One such concurrency control scheme is the strong-strict 2pl protocol that generates rigorous schedules.
Reference: [42] <author> D. Reed. </author> <title> Naming and Synchronization in a Decentralized Computer System. </title> <type> PhD thesis, </type> <institution> Massachus-setts Institute of Technology, </institution> <year> 1978. </year> <note> Also available as Technical Report MIT-LCS-205, </note> <institution> Massachussetts Institute of Technology, </institution> <year> 1978. </year>
Reference-contexts: This follows from the property that multidatabase serializability guarantees both atomicity and a consistent ordering of global transactions as they execute from leaf component database sites to the root multidatabase server. However, no sufficiency conditions have been developed for global serializability in arbitrary execution lattices. Nested transactions <ref> [34, 42] </ref> provide useful conceptual tools for propagating work among multidatabase servers and encapsulating failure recovery. Nested transactions also furnish two useful properties: intra-transaction parallelism and failure isolation. Unlike flat transactions, nested transaction blocks do not have to execute serially.
Reference: [43] <author> Jacob Slonim, Gopi K. Attaluri, and Per -Ake Larson. </author> <title> Advanced Transaction Systems in the CORDS Multidatabase System Environment. </title> <booktitle> In Proceedings of the Workshop on Next Generation Information Technologies and Systems, </booktitle> <pages> pages 130-146, </pages> <address> Haifa, Israel, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Research on an interoperable environment consisting of structured and non-structured data sources is in the initial stages. Several papers have recognized its importance and highlighted some issues. A survey of issues in mdbs transaction management and non-structured data sources (large data objects and long transactions) can be found in <ref> [43] </ref>. The multi-granularity locking scheme [22] is a well-known concurrency control scheme for hierarchically structured large objects.
Reference: [44] <institution> Transarc Corporation, Pittsburg, Pennsylvania, U.S.A. Encina: Product Overview, </institution> <year> 1991. </year>
Reference-contexts: Communication Inter-process communication is supported by the mdbs Client and Server libraries. The Client library supports a (draft) version of the Microsoft odbc interface. It translates odbc calls into ibm Distributed Data Management (ddm) [25] messages that it ships via Sun rpc [15] or, the Encina Transactional rpc <ref> [44] </ref> according to the drda protocol. The Server library accepts the rpc calls and translates them back into odbc calls. Schema Integration The schema integration component is an environment to support users during the integration process. <p> They are performed in a pipelined data-driven fashion by (internally) buffering intermediate results. Transaction Management Subsystem The transaction manager maintains unique transaction identifiers, guarantees global serializability, and manages distributed global commitment. The cords-mdbs transaction management prototype employs the Encina distributed transaction management toolkit <ref> [44] </ref>, the X/Open xa protocol [49], dce, and the ibm drda protocol. The prototype supports both cdss embedded in a dce environment and those external to dce.
Reference: [45] <author> J. Veijalainen and A. Wolski. </author> <title> The 2PC Agent Method for Transaction Management in Heterogeneous Multidatabases, and its Correctness. </title> <type> Technical Report J-10, </type> <institution> Laboratory for Information Processing, Technical Research Centre of Finland (VTT), Helsinki, Finland, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: The survey also covers deadlock handling and concurrency control schemes in which either serializability or autonomy constraints are relaxed. Work on 2pc emulation through the Two-Phase Commit Agent method (2pca) was first published by Wolski and Veijalainen <ref> [45, 47] </ref>. Alternative schemes for guaranteeing global atomic commitment with one-phase commit resource managers are described by Gray [23] and Breitbart et. al. [12]. Work on superdatabases, published by Pu [40], is the only known work on multidatabase composition. <p> This leads not only to global transactions that are not atomic, but also to incorrect global schedules. 18 Two-Phase Commit Agents In the cords-mdbs, the data sources that do not support the 2pc protocol emulate it through the Two-Phase Commit Agent (2pca) algorithm <ref> [45, 46, 47] </ref>. The 2pca intercepts all transactions from the gtm and passes them to the local transaction manager at the component site. All commands but the prepare message are forwarded.
Reference: [46] <author> Jari Veijalainen and Antoni Wolski. </author> <title> Prepare and Commit Certification for Decentralized Transaction Management in Rigorous Heterogeneous Multidatabases. </title> <booktitle> In Proceedings of the Eight International Conference on Data Engineering, </booktitle> <address> Tempe, Arizona, U.S.A., </address> <month> February </month> <year> 1992. </year> <month> 26 </month>
Reference-contexts: This leads not only to global transactions that are not atomic, but also to incorrect global schedules. 18 Two-Phase Commit Agents In the cords-mdbs, the data sources that do not support the 2pc protocol emulate it through the Two-Phase Commit Agent (2pca) algorithm <ref> [45, 46, 47] </ref>. The 2pca intercepts all transactions from the gtm and passes them to the local transaction manager at the component site. All commands but the prepare message are forwarded.
Reference: [47] <author> Antoni Wolski and Jari Veijalainen. </author> <title> 2PC Agent Method: Achieving Serializability in Presence of Failures in a Heterogeneous Multidatabase. </title> <booktitle> In Proceedings of the IEEE Parbase-90 Conference, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: The survey also covers deadlock handling and concurrency control schemes in which either serializability or autonomy constraints are relaxed. Work on 2pc emulation through the Two-Phase Commit Agent method (2pca) was first published by Wolski and Veijalainen <ref> [45, 47] </ref>. Alternative schemes for guaranteeing global atomic commitment with one-phase commit resource managers are described by Gray [23] and Breitbart et. al. [12]. Work on superdatabases, published by Pu [40], is the only known work on multidatabase composition. <p> This leads not only to global transactions that are not atomic, but also to incorrect global schedules. 18 Two-Phase Commit Agents In the cords-mdbs, the data sources that do not support the 2pc protocol emulate it through the Two-Phase Commit Agent (2pca) algorithm <ref> [45, 46, 47] </ref>. The 2pca intercepts all transactions from the gtm and passes them to the local transaction manager at the component site. All commands but the prepare message are forwarded.
Reference: [48] <author> X/Open. </author> <title> X/Open Portability Guide: Data Management. </title> <publisher> X/Open Company Limited, </publisher> <address> Englewood Cliffs, N.J. 07632, 1st edition, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Application development is simplified if the two database systems support a common interface. Examples of such interfaces are the Microsoft Open Database Connectivity (odbc) [33] suite of functions, the X/Open sql Call Level Interface (cli) <ref> [50, 48] </ref>, and the ibm Distributed Relational Database Architecture (drda) [24]. The application is still aware that it is dealing with multiple data sources, but now their interfaces are the same. Integration processing, however, is still the responsibility of the application.
Reference: [49] <institution> X/Open Company Limited, </institution> <address> Reading, Berkshire, United Kingdom. </address> <booktitle> CAE Specification Distributed Transaction Processing: The XA Specification, </booktitle> <year> 1991. </year>
Reference-contexts: They are performed in a pipelined data-driven fashion by (internally) buffering intermediate results. Transaction Management Subsystem The transaction manager maintains unique transaction identifiers, guarantees global serializability, and manages distributed global commitment. The cords-mdbs transaction management prototype employs the Encina distributed transaction management toolkit [44], the X/Open xa protocol <ref> [49] </ref>, dce, and the ibm drda protocol. The prototype supports both cdss embedded in a dce environment and those external to dce. We first consider cdss embedded in dce and then the changes necessary to support cdss that are not dce compliant. dce Compliant Component Data Sources procedure calls (trpc).
Reference: [50] <author> X/Open Company Ltd., </author> <title> California, U.S.A. Data Management: SQL Call Level Interface (CLI), </title> <year> 1992. </year>
Reference-contexts: Application development is simplified if the two database systems support a common interface. Examples of such interfaces are the Microsoft Open Database Connectivity (odbc) [33] suite of functions, the X/Open sql Call Level Interface (cli) <ref> [50, 48] </ref>, and the ibm Distributed Relational Database Architecture (drda) [24]. The application is still aware that it is dealing with multiple data sources, but now their interfaces are the same. Integration processing, however, is still the responsibility of the application.
Reference: [51] <author> H. Garcia-Molina Y. Breitbart and A. Silberschatz. </author> <title> Overview of Multidatabase Transaction Management. </title> <journal> VLDB Journal, </journal> <volume> 1(2) </volume> <pages> 181-239, </pages> <year> 1992. </year>
Reference-contexts: Furthermore, we attempt to extend the algorithms to deal with the more complex issues of multidatabase composition and integrating transaction management schemes for large objects. 6.1 Related Work The work on multidatabase concurrency control focuses on guaranteeing global serializability through rigorous scheduling and forced conflicts [21]. Breitbart et. al. <ref> [51] </ref> completed a comprehensive survey of alternative schemes for guaranteeing global serializability in multidatabase systems. The survey also covers deadlock handling and concurrency control schemes in which either serializability or autonomy constraints are relaxed.
Reference: [52] <author> Qiang Zhu. </author> <title> Query optimization in multidatabase systems. </title> <booktitle> In Proceedings of the 1992 CAS Conference, </booktitle> <volume> volume II, </volume> <pages> pages 111-27, </pages> <address> Toronto, Canada, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: The challenge is to find "cheap" probing queries that allow estimation with high accuracy. More details about this method can be found in <ref> [52] </ref>. Piggybacking: In addition to exploiting information about intermediate results during the processing of a query, we can also perform additional "side retrievals" on the underlying database during query processing. <p> A utility, called Statistics Collector, is periodically invoked on component databases to collect and update 14 15 the statistics and cost parameters stored in the mdbs catalog by performing probing and sampling queries. For additional discussion of global query optimization in the cords-mdbs, see <ref> [52, 53, 54, 55, 56] </ref>. 6 Managing Multidatabase Transactions In a typical dbms, access to local data is achieved through transactions.
Reference: [53] <author> Qiang Zhu. </author> <title> An Integrated Method of Estimating Selectivities in a Multidatabase System. </title> <booktitle> In Proceedings of the 1993 CAS Conference, </booktitle> <volume> volume II, </volume> <pages> pages 832-47, </pages> <address> Toronto, Canada, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: In this way, statistical information for the data referred to by popular queries can be maintained in the mdbs catalog. Additional information about this method can be found in <ref> [53] </ref>. 5.2 Global Query Optimization Subsystem The optimizer is not yet fully implemented. At compile time, when the cords-mdbs receives an sql query from a global user, the parser in the system checks the syntax and semantics of the query using the schema information stored in the mdbs Catalog. <p> A utility, called Statistics Collector, is periodically invoked on component databases to collect and update 14 15 the statistics and cost parameters stored in the mdbs catalog by performing probing and sampling queries. For additional discussion of global query optimization in the cords-mdbs, see <ref> [52, 53, 54, 55, 56] </ref>. 6 Managing Multidatabase Transactions In a typical dbms, access to local data is achieved through transactions.
Reference: [54] <author> Qiang Zhu and P. A. Larson. </author> <title> Query Optimization Using Fuzzy Set Theory for Multidatabase Systems. </title> <booktitle> In Proceedings of the 1993 CAS Conference, </booktitle> <volume> volume II, </volume> <pages> pages 848-59, </pages> <address> Toronto, Canada, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: A utility, called Statistics Collector, is periodically invoked on component databases to collect and update 14 15 the statistics and cost parameters stored in the mdbs catalog by performing probing and sampling queries. For additional discussion of global query optimization in the cords-mdbs, see <ref> [52, 53, 54, 55, 56] </ref>. 6 Managing Multidatabase Transactions In a typical dbms, access to local data is achieved through transactions.
Reference: [55] <author> Qiang Zhu and P. A. Larson. </author> <title> A query sampling method of estimating local cost parameters in a mul-tidatabase system. </title> <booktitle> In Proceedings of the 10th International Conference on Data Engineering, </booktitle> <address> Houston, Texas, </address> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: During global query optimization, the global optimizer uses the derived cost 13 formulas to estimate the costs of component queries. The cost estimation formulas can be dynamically revised to reflect the changing environment in an mdbs. More details on this method can be found in <ref> [55] </ref>. Probing queries: Carefully designed probing queries are issued on an Agent-cds to directly, or indirectly, retrieve some required local information. <p> A utility, called Statistics Collector, is periodically invoked on component databases to collect and update 14 15 the statistics and cost parameters stored in the mdbs catalog by performing probing and sampling queries. For additional discussion of global query optimization in the cords-mdbs, see <ref> [52, 53, 54, 55, 56] </ref>. 6 Managing Multidatabase Transactions In a typical dbms, access to local data is achieved through transactions.

References-found: 55

