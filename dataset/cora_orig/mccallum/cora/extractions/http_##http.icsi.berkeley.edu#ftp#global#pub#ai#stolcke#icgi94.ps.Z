URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/stolcke/icgi94.ps.Z
Refering-URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/stolcke/
Root-URL: http://http.icsi.berkeley.edu
Email: E-mail: fstolcke,omg@icsi.berkeley.edu  
Title: Inducing Probabilistic Grammars by Bayesian Model Merging  
Author: Andreas Stolcke Stephen Omohundro 
Address: 1947 Center St., Suite 600 Berkeley, CA 94707  
Affiliation: International Computer Science Institute  
Note: To appear in ICGI-94  
Abstract: We describe a framework for inducing probabilistic grammars from corpora of positive samples. First, samples are incorporated by adding ad-hoc rules to a working grammar; subsequently, elements of the model (such as states or nonterminals) are merged to achieve generalization and a more compact representation. The choice of what to merge and when to stop is governed by the Bayesian posterior probability of the grammar given the data, which formalizes a trade-off between a close fit to the data and a default preference for simpler models (`Occam's Razor'). The general scheme is illustrated using three types of probabilistic grammars: Hidden Markov models, class-based n-grams, and stochastic context-free grammars. 
Abstract-found: 1
Intro-found: 1
Reference: <author> ANGLUIN, D., & C. H. SMITH. </author> <year> 1983. </year> <title> Inductive inference: Theory and methods. </title> <journal> ACM Computing Surveys 15.237-269. </journal>
Reference-contexts: Successive merging of states (or state equivalence class construction) is a technique widely used in algorithms for finite-state automata (Hopcroft & Ullman 1979) and automata learning <ref> (Angluin & Smith 1983) </ref>; a recent application to probabilistic finite-state automate is Carrasco & Oncina (1994). Bell et al. (1990) and Ron et al. (1994) describe a method for learning deterministic finite-state models that is in a sense the opposite of the merging approach: successive state splitting.
Reference: <author> BAKER, JAMES K. </author> <year> 1979. </year> <title> Trainable grammars for speech recognition. </title> <booktitle> In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, </booktitle> <editor> ed. by Jared J. Wolf & Dennis H. Klatt, </editor> <address> 547-550, </address> <publisher> MIT, </publisher> <address> Cambridge, Mass. </address>
Reference: <author> BAUM, LEONARD E., TED PETRIE, GEORGE SOULES, & NORMAN WEISS. </author> <year> 1970. </year> <title> A maximization technique occuring in the statistical analysis of probabilistic functions in Markov chains. </title> <journal> The Annals of Mathematical Statistics 41.164-171. </journal>
Reference: <author> BELL, TIMOTHY C., JOHN G. CLEARY, & IAN H. WITTEN. </author> <year> 1990. </year> <title> Text Compression. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice Hall. </publisher>
Reference: <author> BOOTH, TAYLOR L., & RICHARD A. THOMPSON. </author> <year> 1973. </year> <title> Applying probability measures to abstract languages. </title> <journal> IEEE Transactions on Computers C-22.442-450. </journal>
Reference: <author> BROWN, PETER F., VINCENT J. DELLA PIETRA, PETER V. DESOUZA, JENIFER C. LAI, </author> & <title> ROBERT L. </title>
Reference: <author> MERCER. </author> <year> 1992. </year> <title> Class-based n-gram models of natural language. </title> <note> Computational Linguistics 18.467-479. </note>
Reference: <author> BUNTINE, WRAY. </author> <year> 1992. </year> <title> Learning classification trees. </title> <booktitle> In Artificial Intelligence Frontiers in Statistics: AI and Statistics III, </booktitle> <editor> ed. by D. J. Hand. </editor> <publisher> Chapman & Hall. </publisher>
Reference: <author> CARRASCO, RAFAEL C., & JOS E ONCINA, </author> <year> 1994. </year> <title> Learning stochastic regular grammars by means of a state merging method. This volume. </title>
Reference: <author> COOK, CRAIG M., AZRIEL ROSENFELD, & ALAN R. ARONSON. </author> <year> 1976. </year> <title> Grammatical inference by hill climbing. </title> <journal> Information Sciences 10.59-80. </journal>
Reference: <author> DEMPSTER, A. P., N. M. LAIRD, & D. B. RUBIN. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B 34.1-38. </journal>
Reference: <author> GULL, S. F. </author> <year> 1988. </year> <title> Bayesian inductive inference and maximum entropy. In Maximum Entropy and Bayesian Methods in Science and Engineering, Volume 1: Foundations, </title> <editor> ed. by G. </editor> <publisher> J. </publisher>
Reference-contexts: corresponds to the intuition that a larger structure needs to be `picked' from among a larger range of possible equally sized alternatives, thus making each individual choice less probable a priori. * Larger models have more parameters, thus making each particular parameter setting less likely (this is the `Occam factor' <ref> (Gull 1988) </ref>). * After two states have been merged, the effective amount of data per parameter increases (the evidence for the merged substructures is pooled). This shifts and peaks the posterior distributions for those parameters closer to their maximum likelihood settings.
Reference: <author> Erickson & C. R. Smith, </author> <title> 53-74. </title> <publisher> Dordrecht: Kluwer. </publisher>
Reference: <author> HOPCROFT, JOHN E., & JEFFREY D. ULLMAN. </author> <year> 1979. </year> <title> Introduction to Automata Theory, </title> <booktitle> Languages, and Computation. </booktitle> <address> Reading, Mass.: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Successive merging of states (or state equivalence class construction) is a technique widely used in algorithms for finite-state automata <ref> (Hopcroft & Ullman 1979) </ref> and automata learning (Angluin & Smith 1983); a recent application to probabilistic finite-state automate is Carrasco & Oncina (1994).
Reference: <author> HORNING, JAMES JAY. </author> <year> 1969. </year> <title> A study of grammatical inference. </title> <type> Technical Report CS 139, </type> <institution> Computer Science Department, Stanford University, Stanford, </institution> <address> Ca. </address>
Reference: <author> JELINEK, FREDERICK, JOHN D. LAFFERTY, & ROBERT L. MERCER. </author> <year> 1992. </year> <title> Basic methods of probabilistic context free grammars. In Speech Recognition and Understanding. Recent Advances, Trends, and Applications, ed. by Pietro Laface & Renato De Mori, </title> <booktitle> volume F75 of NATO Advanced Sciences Institutes Series, </booktitle> <pages> 345-360. </pages> <address> Berlin: Springer Verlag. </address> <booktitle> Proceedings of the NATO Advanced Study Institute, </booktitle> <address> Cetraro, Italy, </address> <month> July </month> <year> 1990. </year>
Reference: <author> LANGLEY, PAT, </author> <year> 1994. </year> <title> Simplicity and representation change in grammar induction. </title> <note> Unpublished mss. </note>
Reference: <author> OMOHUNDRO, STEPHEN M. </author> <year> 1992. </year> <title> Best-first model merging for dynamic learning and recognition. </title> <type> Technical Report TR-92-004, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> Ca. </address>
Reference-contexts: We also report briefly on some of the applications of the resulting learning algorithms primarily in the area of natural language modeling. 2 Bayesian Model Merging Model merging <ref> (Omohundro 1992) </ref> has been proposed as an efficient, robust, and cognitively plausible method for building probabilistic models in a variety of cognitive domains (e.g., vision).
Reference: <author> ONCINA, JOS E, PEDRO GARC IA, & ENRIQUE VIDAL. </author> <year> 1993. </year> <title> Learning subsequential transducers for pattern recognition interpretation tasks. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 15.448-458. </journal>
Reference: <author> QUINLAN, J. ROSS, & RONALD L. RIVEST. </author> <year> 1989. </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation 80.227-248. </journal>
Reference: <author> RABINER, L. R., & B. H. JUANG. </author> <year> 1986. </year> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine 3.4-16. </journal>
Reference-contexts: We now make these concepts concrete for various types of probabilistic grammar. 3 Model merging applied to probabilistic grammars 3.1 Hidden Markov Models Hidden Markov Models (HMMs) are a probabilistic form of nondeterministic finite-state models <ref> (Rabiner & Juang 1986) </ref>. They allow a particularly straightforward version of the model merging approach. Data incorporation. For each observed sample create a unique path between the initial and final states by assigning a new state to each symbol token in the sample.
Reference: <author> RON, DANA, YORAM SINGER, & NAFTALI TISHBY. </author> <year> 1994. </year> <title> The power of amnesia. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <editor> ed. by Jack Cowan, Gerald Tesauro, & Joshua Alspector. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> SAKAKIBARA, YASUBUMI. </author> <year> 1990. </year> <title> Learning context-free grammars from structural data in polynomial time. </title> <booktitle> Theoretical Computer Science 76.223-242. </booktitle>
Reference: <author> STOLCKE, ANDREAS, </author> <year> 1994. </year> <title> Bayesian Learning of Probabilistic Language Models. </title> <institution> Berkeley, CA: University of California dissertation. , & STEPHEN OMOHUNDRO. </institution> <year> 1994. </year> <title> Best-first model merging for hidden Markov model induction. </title> <type> Technical Report TR-94-003, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA. </address>
Reference-contexts: Finally, the HMM merging algorithm was integrated into the training of a medium-scale spoken language understanding system <ref> (Wooters & Stolcke 1994) </ref>. Here, the algorithm also serves the purpose of inducing multi-pronunciation word models from speech data, but it is now coupled with a separate process that estimates the acoustic emission likelihoods for the HMM states.
Reference: <author> WOLFF, J. G. </author> <year> 1987. </year> <title> Cognitive development as optimisation. In Computational models of learning, </title> <editor> ed. by L. Bolc, </editor> <address> 161-205. Berlin: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> WOOTERS, CHUCK, & ANDREAS STOLCKE. </author> <year> 1994. </year> <title> Multiple-pronunciation lexical modeling in a speaker-independent speech understanding system. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing, </booktitle> <address> Yokohama. </address>
Reference-contexts: Finally, the HMM merging algorithm was integrated into the training of a medium-scale spoken language understanding system <ref> (Wooters & Stolcke 1994) </ref>. Here, the algorithm also serves the purpose of inducing multi-pronunciation word models from speech data, but it is now coupled with a separate process that estimates the acoustic emission likelihoods for the HMM states.
References-found: 26

