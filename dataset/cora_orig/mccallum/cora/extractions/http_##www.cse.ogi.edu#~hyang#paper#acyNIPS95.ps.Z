URL: http://www.cse.ogi.edu/~hyang/paper/acyNIPS95.ps.Z
Refering-URL: http://www.cse.ogi.edu/~hyang/paper.html
Root-URL: http://www.cse.ogi.edu
Email: amari@sat.t.u-tokyo.ac.jp  cia@kamo.riken.go.jp  hhy@koala.riken.go.jp  
Title: A New Learning Algorithm for Blind Signal Separation  
Author: S. Amari A. Cichocki H. H. Yang 
Address: Bunkyo-ku, Tokyo 113, JAPAN  351-01, JAPAN  351-01, JAPAN  
Affiliation: University of Tokyo  Lab. for Artificial Brain Systems FRP, RIKEN Wako-Shi, Saitama,  Lab. for Information Representation FRP, RIKEN Wako-Shi, Saitama,  Lab. for Information Representation, FRP, RIKEN, Wako-shi, Saitama, JAPAN  
Note: In: Advances in Neural Information Processing Systems 8, Editors D. Touretzky, M. Mozer, and M. Hasselmo, pp.757-763, MIT Press, Cambridge MA, 1996.  
Abstract: A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm is verified by computer simulations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amari. </author> <title> Differential-Geometrical Methods in Statistics, </title> <booktitle> Lecture Notes in Statistics vol.28. </booktitle> <publisher> Springer, </publisher> <year> 1985. </year>
Reference-contexts: Note the Kullback-Leibler divergence has some invariant properties from the differential-geometrical point of view <ref> [1] </ref>. <p> The above equation is based on the gradient descent algorithm (10) with the following matrix form: dW = (t) @W From information geometry perspective <ref> [1] </ref>, since the mixing matrix A is non-singular we had better replace the above algorithm by the following natural gradient descent algorithm: dW = (t) @W Applying the previous approximation of the gradient @D @W to (18), we obtain the following algorithm: dW = (t)fI f (y)y T gW (19) which <p> So the natural gradient descent algorithm updates W (t) in the direction of decreasing the dependency D (W ). The information geometry theory <ref> [1] </ref> explains why the natural gradient descent algorithm should be used to minimize the MI. Another on-line learning algorithm for blind separation using recurrent network was proposed in [2]. For this algorithm, the activation function (14) also works well. <p> Example: Assume that the following three unknown sources are mixed by a random mixing matrix A: [s 1 (t); s 2 (t); s 3 (t)] = [n (t); 0:1sin (400t)cos (30t); 0:01sign [sin (500t + 9cos (40t))] where n (t) is a noise source uniformly distributed in the range <ref> [1; +1] </ref>, and s 2 (t) and s 3 (t) are two deterministic source signals. The elements of the mixing matrix A are randomly chosen in [1; +1]. The learning rate is exponentially decreasing to zero as (t) = 250exp (5t). A simulation result is shown in Figure 1. <p> = [n (t); 0:1sin (400t)cos (30t); 0:01sign [sin (500t + 9cos (40t))] where n (t) is a noise source uniformly distributed in the range <ref> [1; +1] </ref>, and s 2 (t) and s 3 (t) are two deterministic source signals. The elements of the mixing matrix A are randomly chosen in [1; +1]. The learning rate is exponentially decreasing to zero as (t) = 250exp (5t). A simulation result is shown in Figure 1. The first three signals denoted by X1, X2 and X3 represent mixing (sensor) signals: x 1 (t), x 2 (t) and x 3 (t).
Reference: [2] <author> S. Amari, A. Cichocki, and H. H. Yang. </author> <title> Recurrent neural networks for blind separation of sources. </title> <booktitle> In Proceedings 1995 International Symposium on Nonlinear Theory and Applications, </booktitle> <volume> volume I, </volume> <pages> pages 37-42, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: The information geometry theory [1] explains why the natural gradient descent algorithm should be used to minimize the MI. Another on-line learning algorithm for blind separation using recurrent network was proposed in <ref> [2] </ref>. For this algorithm, the activation function (14) also works well. In practice, other activation functions such as those proposed in [2]-[6] may also be used in (19). However, the performance of the algorithm for such functions usually depends on the distributions of the sources.
Reference: [3] <author> A. J. Bell and T. J. Sejnowski. </author> <title> An information-maximisation approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 1129-1159, </pages> <year> 1995. </year>
Reference-contexts: 1 INTRODUCTION The problem of blind signal separation arises in many areas such as speech recognition, data communication, sensor signal processing, and medical science. Several neural network algorithms <ref> [3, 5, 7] </ref> have been proposed for solving this problem. The performance of these algorithms is usually affected by the selection of the activation functions for the formal neurons in the networks. However, all activation functions attempted are monotonic and the selections of the activation functions are ad hoc. <p> Although the on-line learning algorithms (16) and (19) look similar to those in <ref> [3, 7] </ref> and [5] respectively, the selection of the activation function in this paper is rational, not ad hoc. The activation function (14) is determined by the ICA. It is a non-monotonic activation function different from those used in [3, 5, 7]. <p> The activation function (14) is determined by the ICA. It is a non-monotonic activation function different from those used in <ref> [3, 5, 7] </ref>. There is a simple way to justify the stability of the algorithm (19). Let Vec () denote an operator on a matrix which cascades the columns of the matrix from the left to the right and forms a column vector.
Reference: [4] <author> J.-F. Cardoso and Beate Laheld. </author> <title> Equivariant adaptive source separation. </title> <note> To appear in IEEE Trans. on Signal Processing, </note> <year> 1996. </year>
Reference-contexts: replace the above algorithm by the following natural gradient descent algorithm: dW = (t) @W Applying the previous approximation of the gradient @D @W to (18), we obtain the following algorithm: dW = (t)fI f (y)y T gW (19) which has the same "equivariant" property as the algorithms developed in <ref> [4, 5] </ref>. Although the on-line learning algorithms (16) and (19) look similar to those in [3, 7] and [5] respectively, the selection of the activation function in this paper is rational, not ad hoc. The activation function (14) is determined by the ICA.
Reference: [5] <author> A. Cichocki, R. Unbehauen, L. Moszczynski, and E. Rummert. </author> <title> A new on-line adaptive learning algorithm for blind separation of source signals. </title> <booktitle> In ISANN94, </booktitle> <pages> pages 406-411, </pages> <address> Taiwan, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: 1 INTRODUCTION The problem of blind signal separation arises in many areas such as speech recognition, data communication, sensor signal processing, and medical science. Several neural network algorithms <ref> [3, 5, 7] </ref> have been proposed for solving this problem. The performance of these algorithms is usually affected by the selection of the activation functions for the formal neurons in the networks. However, all activation functions attempted are monotonic and the selections of the activation functions are ad hoc. <p> replace the above algorithm by the following natural gradient descent algorithm: dW = (t) @W Applying the previous approximation of the gradient @D @W to (18), we obtain the following algorithm: dW = (t)fI f (y)y T gW (19) which has the same "equivariant" property as the algorithms developed in <ref> [4, 5] </ref>. Although the on-line learning algorithms (16) and (19) look similar to those in [3, 7] and [5] respectively, the selection of the activation function in this paper is rational, not ad hoc. The activation function (14) is determined by the ICA. <p> Although the on-line learning algorithms (16) and (19) look similar to those in [3, 7] and <ref> [5] </ref> respectively, the selection of the activation function in this paper is rational, not ad hoc. The activation function (14) is determined by the ICA. It is a non-monotonic activation function different from those used in [3, 5, 7]. <p> The activation function (14) is determined by the ICA. It is a non-monotonic activation function different from those used in <ref> [3, 5, 7] </ref>. There is a simple way to justify the stability of the algorithm (19). Let Vec () denote an operator on a matrix which cascades the columns of the matrix from the left to the right and forms a column vector.
Reference: [6] <author> P. Comon. </author> <title> Independent component analysis, a new concept? Signal Processing, </title> <booktitle> 36 </booktitle> <pages> 287-314, </pages> <year> 1994. </year>
Reference-contexts: The solution W is the matrix which finds all independent components in the outputs. An on-line learning algorithm for W is needed which performs the ICA. It is possible to find such a learning algorithm which minimizes the dependency among the outputs. The algorithm in <ref> [6] </ref> is based on the Edgeworth expansion [8] for evaluating the marginal negentropy. Both the Gram-Charlier expansion [8] and the Edgeworth expansion [8] can be used to approximate probability density functions. We shall use the Gram-Charlier expansion instead of the Edgeworth expansion for evaluating the marginal entropy. <p> We shall use the Gram-Charlier expansion instead of the Edgeworth expansion for evaluating the marginal entropy. We shall explain the reason in section 3. 3 INDEPENDENCE OF SIGNALS The mathematical framework for the ICA is formulated in <ref> [6] </ref>. The basic idea of the ICA is to minimize the dependency among the output components. <p> The minimization of the Kullback-Leibler divergence leads to an ICA algorithm for estimating W in <ref> [6] </ref> where the Edgeworth expansion is used to evaluate the negen-tropy. We use the truncated Gram-Charlier expansion to evaluate the Kullback-Leibler divergence. The Edgeworth expansion has some advantages over the Gram-Charlier expansion only for some special distributions. <p> The ICA is a general principle to design algorithms for blind signal separation. The most difficulties in applying this principle are to evaluate the MI of the outputs and to find a working algorithm which decreases the MI. Different from the work in <ref> [6] </ref>, we use the Gram-Charlier expansion instead of the Edgeworth expansion to calculate the marginal entropy in evaluating the MI. Using the natural gradient method to minimize the MI, we have found an on-line learning algorithm to find a de-mixing matrix.
Reference: [7] <author> C. Jutten and J. Herault. </author> <title> Blind separation of sources, part i: An adaptive algorithm based on neuromimetic architecture. </title> <booktitle> Signal Processing, </booktitle> <volume> 24 </volume> <pages> 1-10, </pages> <year> 1991. </year>
Reference-contexts: 1 INTRODUCTION The problem of blind signal separation arises in many areas such as speech recognition, data communication, sensor signal processing, and medical science. Several neural network algorithms <ref> [3, 5, 7] </ref> have been proposed for solving this problem. The performance of these algorithms is usually affected by the selection of the activation functions for the formal neurons in the networks. However, all activation functions attempted are monotonic and the selections of the activation functions are ad hoc. <p> How should the activation function be determined to minimize the MI? Is it necessary to use monotonic activation functions for blind signal separation? In this paper, we shall answer these questions and give an on-line learning algorithm which uses a non-monotonic activation function selected by the independent component analysis (ICA) <ref> [7] </ref>. <p> Although the on-line learning algorithms (16) and (19) look similar to those in <ref> [3, 7] </ref> and [5] respectively, the selection of the activation function in this paper is rational, not ad hoc. The activation function (14) is determined by the ICA. It is a non-monotonic activation function different from those used in [3, 5, 7]. <p> The activation function (14) is determined by the ICA. It is a non-monotonic activation function different from those used in <ref> [3, 5, 7] </ref>. There is a simple way to justify the stability of the algorithm (19). Let Vec () denote an operator on a matrix which cascades the columns of the matrix from the left to the right and forms a column vector.
Reference: [8] <author> A. Stuart and J. K. </author> <title> Ord. Kendall's Advanced Theory of Statistics. </title> <editor> Edward Arnold, </editor> <year> 1994. </year>
Reference-contexts: An on-line learning algorithm for W is needed which performs the ICA. It is possible to find such a learning algorithm which minimizes the dependency among the outputs. The algorithm in [6] is based on the Edgeworth expansion <ref> [8] </ref> for evaluating the marginal negentropy. Both the Gram-Charlier expansion [8] and the Edgeworth expansion [8] can be used to approximate probability density functions. We shall use the Gram-Charlier expansion instead of the Edgeworth expansion for evaluating the marginal entropy. <p> An on-line learning algorithm for W is needed which performs the ICA. It is possible to find such a learning algorithm which minimizes the dependency among the outputs. The algorithm in [6] is based on the Edgeworth expansion <ref> [8] </ref> for evaluating the marginal negentropy. Both the Gram-Charlier expansion [8] and the Edgeworth expansion [8] can be used to approximate probability density functions. We shall use the Gram-Charlier expansion instead of the Edgeworth expansion for evaluating the marginal entropy. <p> It is possible to find such a learning algorithm which minimizes the dependency among the outputs. The algorithm in [6] is based on the Edgeworth expansion <ref> [8] </ref> for evaluating the marginal negentropy. Both the Gram-Charlier expansion [8] and the Edgeworth expansion [8] can be used to approximate probability density functions. We shall use the Gram-Charlier expansion instead of the Edgeworth expansion for evaluating the marginal entropy. We shall explain the reason in section 3. 3 INDEPENDENCE OF SIGNALS The mathematical framework for the ICA is formulated in [6].
References-found: 8

