URL: http://www.iscs.nus.sg/~liuh/kbs.ps
Refering-URL: 
Root-URL: 
Title: Dimensionality Reduction via Discretization  
Author: Huan Liu and Rudy Setiono 
Keyword: Key Word: Dimensionality Reduction, Discretization, Knowledge Discovery  
Address: Ridge, Singapore 0511  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  
Note: To appear in Knowledge-Based Systems  
Email: fliuh,rudysg@iscs.nus.sg  
Phone: Tel: (+65)772-6563 Fax: (+65)779-4580  
Abstract: The existence of numeric data and large amounts of records in a database pose a challenging task to explicit concepts extraction from the raw data. This paper introduces a method that reduces data vertically and horizontally, keeps the discriminating power of the original data, and paves the way for extracting concepts. The method is based on discretization (vertical reduction) and feature selection (horizontal reduction). The experimental results show that (1) the data can be effectively reduced by the proposed method; (2) the predictive accuracy of a classifier (C4.5) can be improved after data and dimensionality reduction; and (3) the classification rules learned are simpler. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Frawley, G. Piatetsky-Shapiro, and C. Matheus. </author> <title> Knowledge discovery in databases: An overview. </title> <journal> AI Magazine, </journal> <month> Fall </month> <year> 1992. </year>
Reference-contexts: 1 Introduction The wide use of computers brings forth proliferation of databases. Without the aid of computer, little of this flood of raw data will ever be seen and exploited by humans. Knowledge discovery systems in databases <ref> [1] </ref> are designed to analyze the data, find regularities in the data (knowledge), and present it to human with understandable formats. One of the goals of knowledge discovery in databases is to extract explicit concepts from the raw data [2, 1, 3, 4, 5]. <p> Knowledge discovery systems in databases [1] are designed to analyze the data, find regularities in the data (knowledge), and present it to human with understandable formats. One of the goals of knowledge discovery in databases is to extract explicit concepts from the raw data <ref> [2, 1, 3, 4, 5] </ref>. The existence of numeric data and large amounts of records in a database pose a challenging task toward this goal due to the huge data space determined by numeric attributes.
Reference: [2] <editor> N. Cercone and M Tsuchiya. Geust editors, </editor> <title> special issue on learning and discovery in databases. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 5(6), </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: Knowledge discovery systems in databases [1] are designed to analyze the data, find regularities in the data (knowledge), and present it to human with understandable formats. One of the goals of knowledge discovery in databases is to extract explicit concepts from the raw data <ref> [2, 1, 3, 4, 5] </ref>. The existence of numeric data and large amounts of records in a database pose a challenging task toward this goal due to the huge data space determined by numeric attributes.
Reference: [3] <author> J. Han, Y. Cai, and H. Cercone. </author> <title> Knowledge discovery in databases: An attribute oriented approach. </title> <booktitle> In Proceedings of the VLDB conference, </booktitle> <pages> pages 547-559, </pages> <year> 1992. </year>
Reference-contexts: Knowledge discovery systems in databases [1] are designed to analyze the data, find regularities in the data (knowledge), and present it to human with understandable formats. One of the goals of knowledge discovery in databases is to extract explicit concepts from the raw data <ref> [2, 1, 3, 4, 5] </ref>. The existence of numeric data and large amounts of records in a database pose a challenging task toward this goal due to the huge data space determined by numeric attributes.
Reference: [4] <author> C.J. Matheus, P.K. Chan, and G. Piatetsky-Shapiro. </author> <title> Systems for knowledge discovery in databases. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 5(6), </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: Knowledge discovery systems in databases [1] are designed to analyze the data, find regularities in the data (knowledge), and present it to human with understandable formats. One of the goals of knowledge discovery in databases is to extract explicit concepts from the raw data <ref> [2, 1, 3, 4, 5] </ref>. The existence of numeric data and large amounts of records in a database pose a challenging task toward this goal due to the huge data space determined by numeric attributes.
Reference: [5] <editor> G. Piatetsky-Shapiro. Editor, </editor> <title> special isssue on knowledge discovery in databases. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 7(7), </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: Knowledge discovery systems in databases [1] are designed to analyze the data, find regularities in the data (knowledge), and present it to human with understandable formats. One of the goals of knowledge discovery in databases is to extract explicit concepts from the raw data <ref> [2, 1, 3, 4, 5] </ref>. The existence of numeric data and large amounts of records in a database pose a challenging task toward this goal due to the huge data space determined by numeric attributes.
Reference: [6] <author> H. Almuallim and T.G. Dietterich. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, </journal> <volume> 69(1-2):279-305, </volume> <month> November </month> <year> 1994. </year>
Reference-contexts: Removing these duplicates amounts to reducing the amount of data. Hence, the original database, if viewed as a large table, is shorten in its vertical dimension. Feature (attribute) selection is a process to choose relevant attributes among many for a certain problem <ref> [6, 7, 8, 9] </ref>. The selection is accomplished by retaining those attributes having more than one discrete values. Other attributes can be removed. Both discretization and feature selection retain the discriminating power of the processed data.
Reference: [7] <author> U.M. Fayyad and K.B. Irani. </author> <title> The attribute selection problem in decision tree generation. </title> <booktitle> In AAAI-92, Proceedings Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 104-110. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Removing these duplicates amounts to reducing the amount of data. Hence, the original database, if viewed as a large table, is shorten in its vertical dimension. Feature (attribute) selection is a process to choose relevant attributes among many for a certain problem <ref> [6, 7, 8, 9] </ref>. The selection is accomplished by retaining those attributes having more than one discrete values. Other attributes can be removed. Both discretization and feature selection retain the discriminating power of the processed data.
Reference: [8] <author> H. Ragavan and L. Rendell. </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In Machine Learning: Proceedings of the Seventh International Conference, </booktitle> <pages> pages 252-259. </pages> <publisher> Morgan Kaufmann Pub. </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: Removing these duplicates amounts to reducing the amount of data. Hence, the original database, if viewed as a large table, is shorten in its vertical dimension. Feature (attribute) selection is a process to choose relevant attributes among many for a certain problem <ref> [6, 7, 8, 9] </ref>. The selection is accomplished by retaining those attributes having more than one discrete values. Other attributes can be removed. Both discretization and feature selection retain the discriminating power of the processed data.
Reference: [9] <author> N. Wyse, R. Dubes, and A.K. Jain. </author> <title> A critical evaluation of intrinsic dimensionality algorithms. In E.S. </title> <editor> Gelsema and Kanal L.N., editors, </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <pages> pages 415-425. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1980. </year>
Reference-contexts: Removing these duplicates amounts to reducing the amount of data. Hence, the original database, if viewed as a large table, is shorten in its vertical dimension. Feature (attribute) selection is a process to choose relevant attributes among many for a certain problem <ref> [6, 7, 8, 9] </ref>. The selection is accomplished by retaining those attributes having more than one discrete values. Other attributes can be removed. Both discretization and feature selection retain the discriminating power of the processed data.
Reference: [10] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <month> 11 </month>
Reference-contexts: A data and dimensionality reduction (DDR) system is built according to the vertical and horizontal reduction (VHR) method. The experimental results show that (1) the data can be effectively reduced by the VHR method; (2) the predictive accuracy of a 1 classifier (C4.5 <ref> [10] </ref>) can be improved after data and dimensionality reduction; and (3) the classification rules learned are simpler. <p> Given a data set, it is not difficult 1 to compute the number of inconsistencies in the set. For the second aspect, however, 1 The time required is O (n 2 ). 3 a pattern classifier is needed in the experiments. C4.5 <ref> [10] </ref> is chosen because (1) it can handle both numeric and nominal data; (2) it is well known, widely available, and works quite well in many domains. So there is no need to explain it in detail. The output of C4.5 is a decision tree.
Reference: [11] <author> H. Liu and S.T. Tan. X2r: </author> <title> A fast rule generator. </title> <booktitle> In Proceedings of IEEE Inter--national Conference on Systems, Man and Cybernetics. IEEE, </booktitle> <year> 1995. </year> <month> 12 </month>
Reference-contexts: Cancer 350 75 H. Disease 198 173 8 As an example, two sets of rules are presented here to show the point. Rule set A is produced by C4.5, and rule set B is produced by a rule generator that induces rules from a small data set heuristically <ref> [11] </ref>. The accuracies of the two rule sets on the training data are 96% and 99%, on the testing data are 94% and 97% respectively.
References-found: 11

