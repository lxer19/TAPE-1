URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr94/tr94-007.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr94-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: Interruptible Critical Sections  
Author: Theodore Johnson and Krishna Harathi 
Affiliation: Dept. of Computer and Information Science University of Florida  
Abstract: We present a new approach to synchronization on uniprocessors with special applicability to embedded and real-time systems. Existing methods for synchronization in real-time systems are pessimistic, and use blocking to enforce concurrency control. While protocols to bound the blocking of high priority tasks exist, high priority tasks can still be blocked by low priority tasks. In addition, these protocols require a complex interaction with the scheduler. We propose interruptible critical sections (i.e., optimistic synchronization) as an alternative to purely blocking methods. Practical optimistic synchronization requires techniques for writing interruptible critical sections, and system support for detecting critical section access conflicts. We discuss our implementation of an interruptible lock on a system running the pSOS+ real time operating system. Our experimental performance results show that interruptible locks reduce the variance in the response time of the highest priority task with only a small impact on the performance of the low priority tasks. We show how interruptible critical sections can be combined with the Priority Ceiling Protocol, and present an analysis which shows that interruptible locks improve the schedulability of task sets that have high priority tasks with tight deadlines.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. </author> <title> Alemany and E.W. Felton. Performance issues in non-blocking synchronization on shared memory multiprocessors. </title> <booktitle> In Proc. ACM Symp. Principles of Distributed Computing, </booktitle> <year> 1992. </year>
Reference-contexts: The final write commits the updates that are recorded in the buffer by setting a commit flag. Any subsequent process that executes the ICS performs the updates and clears the commit flag. This approach to optimistic synchronization is discussed by Alemany and Felton <ref> [1] </ref> and by Bershad [5].
Reference: [2] <author> T.E. Anderson, B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <year> 1992. </year>
Reference-contexts: We note that the idea of scheduler support for critical sections is well established. In 4.3BSD UNIX, a system call that is interrupted by a signal is restarted using the longjump instruction [19]. Anderson et al. <ref> [2] </ref> argue that the operating system support for parallel threads should recognize that a preempted thread is executing in a critical section, and execute the preempted thread until the thread exits the critical section. <p> Other list operations are similar. struct list elemf data item; struct list elem *forward,*backward; g *head; struct commit record elementf word *lhs,rhsg commit record <ref> [2] </ref> 6 boolean valid insert (elem) list elem *elem list elem *prev,*next interruptible critical sectionf if (valid) instruction=0 while (instruction&lt;2 and commit record [instruction].lhs != NULL) *(commit record [instruction].lhs)=commit record [instruction].rhs valid=FALSE prev=NULL; next=head while (not found position (next)) prev=next; next=next! forward // Found the insertion point elem!forward=next; elem!backward=prev if (prev==NULL)
Reference: [3] <author> T.P. Baker. </author> <title> A stack-based resource allocation policy for realtime processes. </title> <booktitle> In Real Time Systems Symposium, </booktitle> <pages> pages 191-200, </pages> <year> 1990. </year>
Reference-contexts: In addition, blocking for even the duration of one critical section may be excessive. Rajkumar, Sha, and Lehoczky have extended the Priority Ceiling Protocol to work in a multiprocessor system [27]. Blocking-based synchronization algorithms have been extended to work with dynamic-priority schedulers. Baker <ref> [3] </ref> presents a pre-allocation based synchronization algorithm that can manage resources with multiple 1 instances. A task's execution is delayed until the scheduler can guarantee that the task can execute without blocking a higher priority task. <p> Second, dynamic-priority scheduling algorithms are feasible with much higher CPU utilizations than static-priority scheduling algorithms [10], and dynamic-priority schedulers might be required for aperiodic tasks. The simple Priority Ceiling Protocol of Rajkumar, Sha, and Lehoczky [28] can be applied to static-priority schedulers only. The dynamic-priority synchronization protocols <ref> [8, 9, 3] </ref> are complex, and must be closely integrated with the scheduling algorithm. In this paper, we present a different approach to synchronization, one which guarantees that a high-priority task never waits for a low-priority task at a critical section. <p> In such an environment, high priority tasks can enter an ICS without making a system call, thus avoiding the associated overhead. Although an ICS can't reserve resources for a process (but can co-exist with blocking algorithms <ref> [28, 3, 9] </ref> which can be applied), an ICS can be used to communicate with a high-priority device driver. Low priority tasks submit requests to the device driver through the ICS, and the device is serviced by a high priority driver which obtains commands through the ICS. <p> record *getbuf (record **current) buffer *temp temp=*current *current=(*current)!next The procedure to declare that a node is garbage is given by: garbage (record *elem,**g head,**g tail) if (*g tail==NULL) *g tail=elem elem!next=*g head *g head=elem A typical critical section is given by: struct commit record elementf 9 word *lhs,rhsg commit record <ref> [3] </ref> boolean valid Global record *pool critical section () record *current,*g head,*g tail restartablef if (valid) instruction=0 while (instruction&lt;3 and commit record [instruction].lhs != NULL) *(commit record [instruction].lhs)=commit record [instruction].rhs valid=FALSE // Initialize the list pointers current=pool g head=g tail=NULL Compute the modifications to the data structure using the getbuf and
Reference: [4] <author> P.A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: A task calculates its modifications to the shared data structure, then attempts to commit its modification. If a higher priority task previously committed a conflicting modification, the lower priority task fails to commit, and must try again (as in optimistic concurrency control <ref> [4] </ref>). Otherwise, the task succeeds, and continues in its work. The synchronization algorithms are not tied to the scheduling algorithm, simplifying the design of the real-time operating system. A purely optimistic approach to synchronization can starve low priority tasks, leading to poor performance (i.e. low schedulability).
Reference: [5] <author> B. Bershad. </author> <title> Practical considerations for non-blocking concurrent objects. </title> <booktitle> In Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pages 264-273, </pages> <year> 1993. </year>
Reference-contexts: The final write commits the updates that are recorded in the buffer by setting a commit flag. Any subsequent process that executes the ICS performs the updates and clears the commit flag. This approach to optimistic synchronization is discussed by Alemany and Felton [1] and by Bershad <ref> [5] </ref>.
Reference: [6] <author> B.N. Bershad, D.D. Redell, and J.R. Ellis. </author> <title> Fast mutual exclusion for uniprocessors. </title> <booktitle> In 5th Intl. Conference on ASPLOS, </booktitle> <pages> pages 223-232, </pages> <year> 1992. </year>
Reference-contexts: In Section 8, we provide examples of tasks sets that cannot be guaranteed to meet their deadlines using the Priority Ceiling Protocol, but are feasible if interruptible locks are used. 2 Interruptible Critical Sections We build our optimistic synchronization methods on Restartable Atomic Sequences (RAS) <ref> [6] </ref>. A RAS is a section of code that is re-executed from the beginning if a context switch occurs while a process is executing in the code section. The re-execution of a RAS is enforced by the kernel context-switch mechanism. <p> Bershad et al. show that an RAS implementation of an atomic test-and-set has better performance than a hardware test-and-set on many architectures, and is much faster than kernel-level synchronization <ref> [6] </ref>. We note that the idea of scheduler support for critical sections is well established. In 4.3BSD UNIX, a system call that is interrupted by a signal is restarted using the longjump instruction [19]. <p> In addition, Moss and Kohler coded several of the run-time support calls of the Trellis/Owl language so that they could be restarted if interrupted [24]. The simple mechanism described in <ref> [6] </ref> is too crude for our purposes, because there is no guarantee that a conflicting operation was performed when other processes had control of the CPU. The unnecessary re-executions are not a problem for the critical sections described in [6], because those critical sections are very short and a re-execution is <p> The simple mechanism described in <ref> [6] </ref> is too crude for our purposes, because there is no guarantee that a conflicting operation was performed when other processes had control of the CPU. The unnecessary re-executions are not a problem for the critical sections described in [6], because those critical sections are very short and a re-execution is unlikely. In addition, the authors of [6] did not need to consider the predictability 3 required by real time systems. <p> The unnecessary re-executions are not a problem for the critical sections described in <ref> [6] </ref>, because those critical sections are very short and a re-execution is unlikely. In addition, the authors of [6] did not need to consider the predictability 3 required by real time systems. If the critical section execution occupies a large fraction of a time slice, then a context switch is far more likely. <p> * An analysis of interruptible locks in a system of periodic tasks. 3.2 Implementation In the following discussion, we assume that if a process experiences a context switch while executing an ICS, the process re-executes from the start of the ICS when it regains control of the CPU (as in <ref> [6] </ref>). In section 4, we discuss the modification necessary to permit re-execution only when a conflicting operation commits. The modification is minor, but the fully general algorithm would confuse the current discussion. In [31], Turek et al. propose a method for transforming locking data structures into non-blocking data structures.
Reference: [7] <author> H. Tokuda C.D. Locke and E.D. Jensen. </author> <title> A time-driven scheduling model for real-time operating systems. </title> <type> Technical report, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction The scheduling of independent real-time tasks is well understood, as optimal scheduling algorithms have been proposed for periodic and aperiodic tasks on uniprocessor [10, 12] and multiprocessor systems <ref> [11, 7, 23] </ref>. However, if the tasks communicate through shared critical sections, a low-priority task that holds a lock may block a high priority task that requires the lock, causing a priority inversion. In this paper, we present a method for real-time synchronization that avoids priority inversions.
Reference: [8] <author> M.I. Chen and K.J. Lin. </author> <title> Dynamic priority ceiling: A concurrency control protocol for real-time systems. </title> <journal> Real-Time Systems Journal, </journal> <volume> 2(4) </volume> <pages> 325-346, </pages> <year> 1990. </year>
Reference-contexts: A task's execution is delayed until the scheduler can guarantee that the task can execute without blocking a higher priority task. Tripathi and Nirkhe [30], and Faulk and Parnas [13] also discuss pre-allocation based scheduling methods. Chen and Lin <ref> [8] </ref> extend the Priority Ceiling Protocol to permit dynamically-assigned priorities. Chen and Lin [9] extend the protocol in [8] to account for multiple resource instances. Previous approaches to real-time synchronization suffer from several drawbacks. <p> Tripathi and Nirkhe [30], and Faulk and Parnas [13] also discuss pre-allocation based scheduling methods. Chen and Lin <ref> [8] </ref> extend the Priority Ceiling Protocol to permit dynamically-assigned priorities. Chen and Lin [9] extend the protocol in [8] to account for multiple resource instances. Previous approaches to real-time synchronization suffer from several drawbacks. First, a high-priority task might be forced to wait for a low-priority task to complete a critical section. <p> Second, dynamic-priority scheduling algorithms are feasible with much higher CPU utilizations than static-priority scheduling algorithms [10], and dynamic-priority schedulers might be required for aperiodic tasks. The simple Priority Ceiling Protocol of Rajkumar, Sha, and Lehoczky [28] can be applied to static-priority schedulers only. The dynamic-priority synchronization protocols <ref> [8, 9, 3] </ref> are complex, and must be closely integrated with the scheduling algorithm. In this paper, we present a different approach to synchronization, one which guarantees that a high-priority task never waits for a low-priority task at a critical section.
Reference: [9] <author> M.I. Chen and K.J. Lin. </author> <title> A priority ceiling protocol for multiple-instance resources. </title> <booktitle> In Real Time Systems Symposium, </booktitle> <pages> pages 140-149, </pages> <year> 1990. </year>
Reference-contexts: Tripathi and Nirkhe [30], and Faulk and Parnas [13] also discuss pre-allocation based scheduling methods. Chen and Lin [8] extend the Priority Ceiling Protocol to permit dynamically-assigned priorities. Chen and Lin <ref> [9] </ref> extend the protocol in [8] to account for multiple resource instances. Previous approaches to real-time synchronization suffer from several drawbacks. First, a high-priority task might be forced to wait for a low-priority task to complete a critical section. <p> Second, dynamic-priority scheduling algorithms are feasible with much higher CPU utilizations than static-priority scheduling algorithms [10], and dynamic-priority schedulers might be required for aperiodic tasks. The simple Priority Ceiling Protocol of Rajkumar, Sha, and Lehoczky [28] can be applied to static-priority schedulers only. The dynamic-priority synchronization protocols <ref> [8, 9, 3] </ref> are complex, and must be closely integrated with the scheduling algorithm. In this paper, we present a different approach to synchronization, one which guarantees that a high-priority task never waits for a low-priority task at a critical section. <p> In such an environment, high priority tasks can enter an ICS without making a system call, thus avoiding the associated overhead. Although an ICS can't reserve resources for a process (but can co-exist with blocking algorithms <ref> [28, 3, 9] </ref> which can be applied), an ICS can be used to communicate with a high-priority device driver. Low priority tasks submit requests to the device driver through the ICS, and the device is serviced by a high priority driver which obtains commands through the ICS.
Reference: [10] <author> C.L.Liu and W.J. Leyland. </author> <title> Scheduling algorithms for multiprogramming in a hard real-time environment. </title> <journal> JACM, </journal> <volume> 20(1) </volume> <pages> 46-63, </pages> <year> 1973. </year>
Reference-contexts: 1 Introduction The scheduling of independent real-time tasks is well understood, as optimal scheduling algorithms have been proposed for periodic and aperiodic tasks on uniprocessor <ref> [10, 12] </ref> and multiprocessor systems [11, 7, 23]. However, if the tasks communicate through shared critical sections, a low-priority task that holds a lock may block a high priority task that requires the lock, causing a priority inversion. <p> Jeffay [16] discusses the additional feasibility conditions required if tasks have preemption constraints. Second, dynamic-priority scheduling algorithms are feasible with much higher CPU utilizations than static-priority scheduling algorithms <ref> [10] </ref>, and dynamic-priority schedulers might be required for aperiodic tasks. The simple Priority Ceiling Protocol of Rajkumar, Sha, and Lehoczky [28] can be applied to static-priority schedulers only. The dynamic-priority synchronization protocols [8, 9, 3] are complex, and must be closely integrated with the scheduling algorithm.
Reference: [11] <author> S. Davari and S.K. Dhall. </author> <title> An on-line algorithm ofr real-time task allocation. </title> <booktitle> In IEEE Real-Time Systems Symposium, </booktitle> <year> 1986. </year>
Reference-contexts: 1 Introduction The scheduling of independent real-time tasks is well understood, as optimal scheduling algorithms have been proposed for periodic and aperiodic tasks on uniprocessor [10, 12] and multiprocessor systems <ref> [11, 7, 23] </ref>. However, if the tasks communicate through shared critical sections, a low-priority task that holds a lock may block a high priority task that requires the lock, causing a priority inversion. In this paper, we present a method for real-time synchronization that avoids priority inversions.
Reference: [12] <author> M. Dertouzos. </author> <title> Control robotics: The procedural control of physical processes. </title> <booktitle> In Proc. of the IFIP Congress, </booktitle> <year> 1974. </year> <month> 23 </month>
Reference-contexts: 1 Introduction The scheduling of independent real-time tasks is well understood, as optimal scheduling algorithms have been proposed for periodic and aperiodic tasks on uniprocessor <ref> [10, 12] </ref> and multiprocessor systems [11, 7, 23]. However, if the tasks communicate through shared critical sections, a low-priority task that holds a lock may block a high priority task that requires the lock, causing a priority inversion.
Reference: [13] <author> S.R. Faulk and D.L. Parnas. </author> <title> On synchronization in hard real-time systems. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 274-287, </pages> <year> 1988. </year>
Reference-contexts: Baker [3] presents a pre-allocation based synchronization algorithm that can manage resources with multiple 1 instances. A task's execution is delayed until the scheduler can guarantee that the task can execute without blocking a higher priority task. Tripathi and Nirkhe [30], and Faulk and Parnas <ref> [13] </ref> also discuss pre-allocation based scheduling methods. Chen and Lin [8] extend the Priority Ceiling Protocol to permit dynamically-assigned priorities. Chen and Lin [9] extend the protocol in [8] to account for multiple resource instances. Previous approaches to real-time synchronization suffer from several drawbacks.
Reference: [14] <author> M. Herlihy. </author> <title> Apologizing versus asking permission: Optimistic concurrency control for abstract data types. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 15(1) </volume> <pages> 96-124, </pages> <year> 1990. </year>
Reference-contexts: If an operation performs a substantial modification and the number of modifications that an operation commits might vary widely, then a high priority operation might spend a 7 substantial amount of time performing a low priority operation's updates to the data structure. In <ref> [14] </ref>, Herlihy proposes a `shadow-page' method for implementing a non-blocking concurrent data structure. An operation calculates its modifications to the data structure in set of privately allocated (shadow) records, then links its records into the data structure in its decisive instruction. The process is illustrated in Figure 1.
Reference: [15] <author> M. Herlihy. </author> <title> A methodology for implementing highly concurrent data objects. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 15(5) </volume> <pages> 745-770, </pages> <year> 1993. </year>
Reference-contexts: Herlihy <ref> [15] </ref> introduces the idea of non-blocking concurrent objects. An algorithm for a non-blocking object provides the guarantee that one of the processes that accesses the object makes 4 progress in a finite number of steps. <p> paper, we discuss implementational details that do not appear in the previous work, including: * Efficient implementation in a uniprocessor system. * How to perform the bulk of the ICS processing outside of the kernel. * How to share commit buffers among processes. * How to use Herlihy's small-object protocol <ref> [15] </ref> to minimize the number of writes that must be placed in the commit buffer. * How to apply optimistic synchronization to real-time systems. * An analysis of interruptible locks in a system of periodic tasks. 3.2 Implementation In the following discussion, we assume that if a process experiences a context
Reference: [16] <author> K. Jeffay. </author> <title> Analysis of a synchronization and scheduling discipline for real-time tasks with pre-emption constraints. </title> <booktitle> In Real Time Systems Symposium, </booktitle> <pages> pages 295-305, </pages> <year> 1989. </year>
Reference-contexts: If tasks can have delayed release times [20], a high priority task might not be able to block for the duration of a critical section and still be guaranteed to meet its deadlines. Jeffay <ref> [16] </ref> discusses the additional feasibility conditions required if tasks have preemption constraints. Second, dynamic-priority scheduling algorithms are feasible with much higher CPU utilizations than static-priority scheduling algorithms [10], and dynamic-priority schedulers might be required for aperiodic tasks.
Reference: [17] <author> T. Johnson. </author> <title> Interruptible critical sections for real-time systems. </title> <type> Technical Report Electronic TR93-017, </type> <note> Availiable at anonymous ftp site cis.ufl.edu, </note> <institution> University of Florida, Dept. of CIS, </institution> <year> 1993. </year>
Reference-contexts: We present an algorithm of an ICS based on this approach here. We note that one can write an ICS by a rather different approach, the details of which are contained in <ref> [17] </ref>. Every shared concurrent object has a single commit record, and a flag indicating whether the commit record is valid or invalid. When a process starts executing a critical section, it check to see if a previous operation left an unexecuted commit record (the flag is valid).
Reference: [18] <author> M. Joseph and P. Pandya. </author> <title> Finding response times in a real time system. </title> <journal> BCS Computer Journal, </journal> <volume> 29(5) </volume> <pages> 390-395, </pages> <year> 1986. </year>
Reference-contexts: Rate Monotonic scheduling is well studied, and the feasibility of a task set can be exactly characterized. Let r i be the worst cast response time of task i. If the tasks do not access critical sections then, r i is the fixed point of the following recursive equation <ref> [18] </ref>: r i = C i + j&lt;i dr i =T j eC j (1) Unfortunately, it is not always realistic to assume that a task is released for execution at the start of its period.
Reference: [19] <editor> S.J. Le*er, M.K. McKusick, M.J. Karels, and J.S. </editor> <title> Quarterman. The Design and Implementation of the 4.3 BSD UNIX Operating System. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: We note that the idea of scheduler support for critical sections is well established. In 4.3BSD UNIX, a system call that is interrupted by a signal is restarted using the longjump instruction <ref> [19] </ref>. Anderson et al. [2] argue that the operating system support for parallel threads should recognize that a preempted thread is executing in a critical section, and execute the preempted thread until the thread exits the critical section.
Reference: [20] <author> J.P. Lehoczky. </author> <title> Fixed priority scheduling of periodic task sets with arbitrary deadlines. </title> <booktitle> In Proc. IEEE Real-time Systems Symposium, </booktitle> <pages> pages 201-209, </pages> <year> 1990. </year>
Reference-contexts: Mercer and Tokuda [22] note that the blocking of high-priority tasks must be kept to a minimum in order to ensure the responsiveness of the real-time system. If tasks can have delayed release times <ref> [20] </ref>, a high priority task might not be able to block for the duration of a critical section and still be guaranteed to meet its deadlines. Jeffay [16] discusses the additional feasibility conditions required if tasks have preemption constraints. <p> The release of a task can also be blocked by external events, such as the arrival of a message from a communicating task in a distributed system <ref> [20] </ref>. The task set might be subject to release jitter [29], possibly due to tick scheduling or due to waiting for external events. Tindell, Burns, and Wellings show how to modify equation 1 to account for release jitter.
Reference: [21] <author> J.Y.T. Leung and J. Whitehead. </author> <title> On the complexity of fixed-priority scheduling of periodic real-time tasks. Performance Evaluation, </title> <booktitle> 2 </booktitle> <pages> 237-250, </pages> <year> 1982. </year>
Reference-contexts: In each of these cases, the deadline of the task can be less than the task period, perhaps significantly less. If the task deadlines are shorter than the task periods, then the Deadline Monotonic algorithm is the optimal static scheduler <ref> [21] </ref>. If the tasks can access critical sections and thus experience blocking, then the maximum blocking time is added to the above r i value. If the Priority Ceiling Protocol is used, a task will block for at most the duration of one critical section.
Reference: [22] <author> C.W. Mercer and H. Tokuda. </author> <booktitle> Preemptibility in real-time operating systems. In Real Time Systems Symposuim, </booktitle> <pages> pages 78-87, </pages> <year> 1992. </year>
Reference-contexts: Chen and Lin [9] extend the protocol in [8] to account for multiple resource instances. Previous approaches to real-time synchronization suffer from several drawbacks. First, a high-priority task might be forced to wait for a low-priority task to complete a critical section. Mercer and Tokuda <ref> [22] </ref> note that the blocking of high-priority tasks must be kept to a minimum in order to ensure the responsiveness of the real-time system.
Reference: [23] <author> A.K. </author> <title> Mok and M.L. Dertouzos. Multiprocessor on-line scheduling of hard real-time tasks. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 15(12) </volume> <pages> 1497-1506, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction The scheduling of independent real-time tasks is well understood, as optimal scheduling algorithms have been proposed for periodic and aperiodic tasks on uniprocessor [10, 12] and multiprocessor systems <ref> [11, 7, 23] </ref>. However, if the tasks communicate through shared critical sections, a low-priority task that holds a lock may block a high priority task that requires the lock, causing a priority inversion. In this paper, we present a method for real-time synchronization that avoids priority inversions.
Reference: [24] <author> E. Moss and W.H. Kohler. </author> <title> Concurrency features for the trellis/owl language. </title> <booktitle> In European Conference on Object-Oriented Programming, </booktitle> <pages> pages 171-180, </pages> <year> 1987. </year> <note> Appears as Springer-Verlag Computer Science Lecture Note number 276. </note>
Reference-contexts: In addition, Moss and Kohler coded several of the run-time support calls of the Trellis/Owl language so that they could be restarted if interrupted <ref> [24] </ref>. The simple mechanism described in [6] is too crude for our purposes, because there is no guarantee that a conflicting operation was performed when other processes had control of the CPU.
Reference: [25] <author> Inc. Motorola. </author> <title> psos+ rteid-compliant real-time kernel user's manual, </title> <year> 1990. </year>
Reference-contexts: In a typical use of interruptible locks, only one very high priority process will be able to interrupt the lock, so only two commit records are needed. 13 6 Implementation We implemented ICS support in a VMEexec [26] system development environment with a pSOS+ <ref> [25] </ref> real-time, multi-tasking operating system kernel. The VMEexec system consists of a host running on a VMEmodule driven SYSTEM V/68 operating system and a set of VMEmodule target processors running the pSOS+ kernel.
Reference: [26] <institution> Inc. Motorola. </institution> <note> Vmeexec user's guide, second edition, 1990. 24 </note>
Reference-contexts: In a typical use of interruptible locks, only one very high priority process will be able to interrupt the lock, so only two commit records are needed. 13 6 Implementation We implemented ICS support in a VMEexec <ref> [26] </ref> system development environment with a pSOS+ [25] real-time, multi-tasking operating system kernel. The VMEexec system consists of a host running on a VMEmodule driven SYSTEM V/68 operating system and a set of VMEmodule target processors running the pSOS+ kernel.
Reference: [27] <author> R. Rajkumar, L. Sha, and J.P. Lehoczky. </author> <title> Real-time synchronization protocols for multiprocessors. </title> <booktitle> In Real Time Systems Symposium, </booktitle> <year> 1988. </year>
Reference-contexts: However, the tasks must have static priorities in order to apply the Priority Ceiling Protocol. In addition, blocking for even the duration of one critical section may be excessive. Rajkumar, Sha, and Lehoczky have extended the Priority Ceiling Protocol to work in a multiprocessor system <ref> [27] </ref>. Blocking-based synchronization algorithms have been extended to work with dynamic-priority schedulers. Baker [3] presents a pre-allocation based synchronization algorithm that can manage resources with multiple 1 instances. A task's execution is delayed until the scheduler can guarantee that the task can execute without blocking a higher priority task.
Reference: [28] <author> R. Rajkumar, L. Sha, and J.P. Lehoczky. </author> <title> Priority inheritance protocols: An approach to real-time synchronization. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(9) </volume> <pages> 1175-1185, </pages> <year> 1990. </year>
Reference-contexts: However, if the tasks communicate through shared critical sections, a low-priority task that holds a lock may block a high priority task that requires the lock, causing a priority inversion. In this paper, we present a method for real-time synchronization that avoids priority inversions. Rajkumar, Sha, and Lehoczky <ref> [28] </ref> have proposed the Priority Ceiling Protocol (PCP) to minimize the effect of priority inversion. The priority ceiling of a semaphore S is the priority of the highest priority task that will ever lock S. <p> Jeffay [16] discusses the additional feasibility conditions required if tasks have preemption constraints. Second, dynamic-priority scheduling algorithms are feasible with much higher CPU utilizations than static-priority scheduling algorithms [10], and dynamic-priority schedulers might be required for aperiodic tasks. The simple Priority Ceiling Protocol of Rajkumar, Sha, and Lehoczky <ref> [28] </ref> can be applied to static-priority schedulers only. The dynamic-priority synchronization protocols [8, 9, 3] are complex, and must be closely integrated with the scheduling algorithm. <p> In such an environment, high priority tasks can enter an ICS without making a system call, thus avoiding the associated overhead. Although an ICS can't reserve resources for a process (but can co-exist with blocking algorithms <ref> [28, 3, 9] </ref> which can be applied), an ICS can be used to communicate with a high-priority device driver. Low priority tasks submit requests to the device driver through the ICS, and the device is serviced by a high priority driver which obtains commands through the ICS.
Reference: [29] <author> K.W. Tindell, A. Burns, and A.J. Wellings. </author> <title> An extendible approach for analyzing fixed priority hard real-time tasks. Real Time Systems, </title> <booktitle> 6 </booktitle> <pages> 133-151, </pages> <year> 1994. </year>
Reference-contexts: The release of a task can also be blocked by external events, such as the arrival of a message from a communicating task in a distributed system [20]. The task set might be subject to release jitter <ref> [29] </ref>, possibly due to tick scheduling or due to waiting for external events. Tindell, Burns, and Wellings show how to modify equation 1 to account for release jitter. In each of these cases, the deadline of the task can be less than the task period, perhaps significantly less.
Reference: [30] <author> S.K. Tripathi and V. Nirkhe. </author> <title> Pre-scheduling for synchronization in hard real-time systems. </title> <booktitle> In Operating Systems of the '90s and Beyond, Int'l Workshop, </booktitle> <pages> pages 102-108, </pages> <year> 1991. </year> <note> Appears as Springer-Verlag Computer Science Lecture Note number 563. </note>
Reference-contexts: Baker [3] presents a pre-allocation based synchronization algorithm that can manage resources with multiple 1 instances. A task's execution is delayed until the scheduler can guarantee that the task can execute without blocking a higher priority task. Tripathi and Nirkhe <ref> [30] </ref>, and Faulk and Parnas [13] also discuss pre-allocation based scheduling methods. Chen and Lin [8] extend the Priority Ceiling Protocol to permit dynamically-assigned priorities. Chen and Lin [9] extend the protocol in [8] to account for multiple resource instances. Previous approaches to real-time synchronization suffer from several drawbacks.
Reference: [31] <author> J. Turek, D. Shasha, and S. Prakash. </author> <title> Locking without blocking: Making lock based concurrent data structure algorithms nonblocking. </title> <booktitle> In ACM Symp. on Principles of Database Systems, </booktitle> <pages> pages 212-222, </pages> <year> 1992. </year> <pages> 25 26 27 28 </pages>
Reference-contexts: Herlihy provides a method for implementing non-blocking objects that swaps in the new value of the object in a single write. Our methods are similar to an extension of Herlihy's work proposed by Turek, Shasha, and Prakash <ref> [31] </ref>. In the context of real-time synchronization, non-blocking shared objects are desirable because a high priority task will not be blocked by a low priority task. In a uniprocessor system, only one process at a time will access the shared data structures. <p> In section 4, we discuss the modification necessary to permit re-execution only when a conflicting operation commits. The modification is minor, but the fully general algorithm would confuse the current discussion. In <ref> [31] </ref>, Turek et al. propose a method for transforming locking data structures into non-blocking data structures. The key to the transformation is to post a continuation instead of a lock. The continuation contains the modifications that the process intends to perform.
References-found: 31

