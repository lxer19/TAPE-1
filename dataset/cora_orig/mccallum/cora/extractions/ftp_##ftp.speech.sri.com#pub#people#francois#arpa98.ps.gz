URL: ftp://ftp.speech.sri.com/pub/people/francois/arpa98.ps.gz
Refering-URL: http://www.speech.sri.com/people/francois/publications.html
Root-URL: 
Title: DYNAMO: An Algorithm for Dynamic Acoustic Modeling  
Author: Francoise Beaufays, Mitch Weintraub, Yochai Konig 
Address: Menlo Park, CA 94025  
Affiliation: Speech Technology and Research Laboratory SRI International  
Abstract: This paper summarizes part of SRI's effort to improve acoustic modeling in the context of the Large Vocabulary Continuous Speech Recognition (LVCSR) project. It concentrates on two problems that are believed to contribute to the large error rates observed with LVCSR databases: (1) the lack of discriminative power of the speech models in the acoustic space, and (2) the discrepancy between the criterion used to train the models (typically frame-level maximum likelihood) and the task expected from the models (word-level recognition). We address the first issue by searching for features that help in narrowing the model distributions, and by proposing a neural-network-based architecture to combine these features. The neural networks (NNET) are used in association with a set of large Gaussian mixture models (GMM) whose mixture weights are dynamically estimated by the neural networks, for each frame of incoming data. We call the resulting algorithm DYNAMO, for dynamic acoustic modeling. To address the second problem, we propose two discriminative training criteria, both defined at the sentence level. We report preliminary results with the Spanish Callhome database. 
Abstract-found: 1
Intro-found: 1
Reference: [AKC94] <author> A. Andreou, T. Kamm, and J. Cohen. </author> <title> Experiments in vocal tract normalization. </title> <booktitle> In Proc. the CAIP Workshop: Frontiers in Speech Recognition II, </booktitle> <year> 1994. </year>
Reference-contexts: Whereas some of these factors can be efficiently dealt with by explicit modeling (e.g. vocal tract normalization (e.g. <ref> [AKC94] </ref>), pronunciation modeling (e.g. [Slo95, FW97])), many others are left for the acoustic models's multi-modal distributions to model implicitly. This, however, has the well-known result of broad overlapping distributions which often lead to recognition errors.
Reference: [BBdSM86] <author> L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer. </author> <title> Maximum mutual information estimation of hidden markov model parameters for speech recognition. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1986. </year>
Reference-contexts: This indicated that competing models scored higher than the correct model, which confirmed that discriminative training should be used instead. 6. Discriminative Training Criteria Discriminative training of speech models was first introduced by Bahl et al. under the form of Maximum Mutual Information (MMI) estimation <ref> [BBdSM86] </ref>.
Reference: [BBdSM88] <author> L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer. </author> <title> A new algorithm for the estimation of hidden markov model parameters. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> New York, NY, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: Another family of discriminative criteria stems from the motivation of directly optimizing the metric used to evaluate the recognizer, i.e. the word error rate. Bahl et al. proposed the heuristic corrective training procedure in <ref> [BBdSM88] </ref>. Katagiri et al. developed the Generalized Probabilistic Descent method that extends the idea of Bayes optimum classification by introducing smooth classification error functions, and generalizes this framework to the classification of patterns of variable lengths [KLJ91].
Reference: [BdSG + 91] <author> L. R. Bahl, P. V. de Souza, P. S. Gopalakrishnan, D. Na-hamoo, and M.A. Picheny. </author> <title> Context dependent modeling of phones in continuous speech using decision trees. </title> <booktitle> In DARPA Proc. Speech and Natural Language Workshop, </booktitle> <address> Pacific Grove, CA, </address> <month> February </month> <year> 1991. </year>
Reference-contexts: These are the main issues that motivated this work. In the past decade, contextual linguistic features have been widely used in conjunction with decision tree models, and have significantly improved recognition performance (e.g. <ref> [BdSG + 91, YOW94] </ref>). Decision trees, however, make data sharing among different states difficult, and are not well suited to the use of features that are continuous in nature, as opposed to binary. For these reasons, we chose instead to base our models on neural networks.
Reference: [BKM95] <author> H. Bourlard, Y. Konig, and N. Morgan. </author> <title> REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities, applications to transition-based connectionist speech recognition. </title> <type> Technical Report TR-94-064, ICSI, </type> <institution> Berkeley, </institution> <address> CA, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: This architecture thus outputs the likelihoods of the observations. This is in contrast with NNET-HMM hybrids trained for state classi fication [BM90], where the outputs are state posterior probabilities that need to be converted into likelihoods, and with approaches such as REMAP <ref> [BKM95, KBM96] </ref> that estimate global posterior probabilities of word sequences. 4.1. Training of the DYNAMO Models The DYNAMO models are trained in two phases. First, the context-independent phone GMMs are trained with the expectation-maximization (EM) algorithm to maximize the log-likelihood of the training data.
Reference: [BM90] <author> H. Bourlard and N. Morgan. </author> <title> A continuous speech recognition system embedding MLP into HMM. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Infor--mation Processing Systems, </booktitle> <volume> volume 2. </volume> <publisher> Morgan Kauf-mann, </publisher> <year> 1990. </year>
Reference-contexts: This architecture thus outputs the likelihoods of the observations. This is in contrast with NNET-HMM hybrids trained for state classi fication <ref> [BM90] </ref>, where the outputs are state posterior probabilities that need to be converted into likelihoods, and with approaches such as REMAP [BKM95, KBM96] that estimate global posterior probabilities of word sequences. 4.1. Training of the DYNAMO Models The DYNAMO models are trained in two phases.
Reference: [Bri90] <author> J. S. Bridle. </author> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In D. S. Touret-zky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Because the mixture weights for each phone must sum to one, the training of the neural networks is a constrained optimization prob lem. To simplify the training procedure, we chose to hard-wire this constraint in the architecture of the neural networks by using a softmax output layer <ref> [Bri90] </ref>: P g (s) = P where y g (:) is the g th output of the neural network, before the softmax layer. The Gaussians in each phone model can be interpreted as a set of basis functions.
Reference: [Cho90] <author> Y. L. Chow. </author> <title> Maximum mutual information estimation of HMM parameters for continuous speech recognition using the N-best algorithm. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Albuquerque, NM, </address> <year> 1990. </year>
Reference-contexts: Practical implementations of Eq. 9 for continuous speech recognition include the estimation of the denominator with a phone loop model [Mer88], and its approximation by a sum over the hypotheses in an N-best list <ref> [Cho90] </ref>. The first optimization criterion we propose is similar to the N-best list implementation of MMI, but differs in that we augment the N-best list with the correct word sequence, W c .
Reference: [DASW94] <author> L. Deng, M. Aksmanovic, D. Sun, and J. Wu. </author> <title> Speech recognition using hidden markov models with polynomial regression functions as nonstationary states. </title> <journal> IEEE Trans. Speech, Audio Processing, </journal> <volume> 2(4), </volume> <year> 1994. </year>
Reference-contexts: Without embarking in this level of complexity, and following a feature-based approach, we propose to include in the acoustic models time features similar to the time index proposed in <ref> [GN93, DASW94] </ref> and [KM94]. These features don't model correlation but they do alleviate the independence assumption. Our goal here is to explore the usefulness of such knowledge sources as acoustic discriminants, and to propose an efficient and robust architecture to incorporate them in the acoustic models.
Reference: [DMM96] <author> V. V. Digalakis, P. Monaco, and H. Murveit. Genones: </author> <title> Generalized mixture tying in continuous hidden markov model-based speech recognizers. </title> <journal> IEEE Trans. Speech, Audio Processing, </journal> <volume> 4(4), </volume> <month> July </month> <year> 1996. </year>
Reference-contexts: Baseline System and Databases The baseline system for this work is a speaker-independent continuous speech recognition system trained with 75 conversations of Callhome Spanish data and 80 conversations from Callfriend Span-ish. It is based on continuous-density, genonic HMMs <ref> [DMM96] </ref>, and uses a multipass recognition strategy [MBDW93] with a vocabulary of 8K words, non-cross-word acoustic models, and a bigram language model.
Reference: [FW97] <author> M. Finke and A. Waibel. </author> <title> Speaking mode dependent pronunciation modeling in large vocabulary conversational speech recognition. </title> <booktitle> In Proc. Eurospeech, </booktitle> <address> Rhodes, Greece, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: Whereas some of these factors can be efficiently dealt with by explicit modeling (e.g. vocal tract normalization (e.g. [AKC94]), pronunciation modeling (e.g. <ref> [Slo95, FW97] </ref>)), many others are left for the acoustic models's multi-modal distributions to model implicitly. This, however, has the well-known result of broad overlapping distributions which often lead to recognition errors.
Reference: [GN93] <author> H. Gish and K. Ng. </author> <title> A segmental speech model with applications to word spotting. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, volume II, </booktitle> <year> 1993. </year>
Reference-contexts: Without embarking in this level of complexity, and following a feature-based approach, we propose to include in the acoustic models time features similar to the time index proposed in <ref> [GN93, DASW94] </ref> and [KM94]. These features don't model correlation but they do alleviate the independence assumption. Our goal here is to explore the usefulness of such knowledge sources as acoustic discriminants, and to propose an efficient and robust architecture to incorporate them in the acoustic models.
Reference: [HW97] <author> L. P. Heck and M. Weintraub. </author> <title> Handset-dependent background models for robust text-independent speaker recognition. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: In the field of speaker recognition, the use of handset detectors has dramatically decreased recognition error rates by sorting out carbon button from electret handsets <ref> [Rey96, HW97] </ref>. The handset type could also be used as an input to the acoustic modeling algorithms. Another important issue in acoustic modeling is how to capture the dynamics of the speech signal.
Reference: [KBM96] <author> Y. Konig, H. Bourlard, and N. Morgan. </author> <title> REMAP: Experiments with speech recognition. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Atlanta, GA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: This architecture thus outputs the likelihoods of the observations. This is in contrast with NNET-HMM hybrids trained for state classi fication [BM90], where the outputs are state posterior probabilities that need to be converted into likelihoods, and with approaches such as REMAP <ref> [BKM95, KBM96] </ref> that estimate global posterior probabilities of word sequences. 4.1. Training of the DYNAMO Models The DYNAMO models are trained in two phases. First, the context-independent phone GMMs are trained with the expectation-maximization (EM) algorithm to maximize the log-likelihood of the training data.
Reference: [KLJ91] <author> S. Katagiri, C.-H. Lee, and B.-H. Juang. </author> <title> New discriminative training algorithms based on the generalized probabilistic descent method. </title> <booktitle> In Proc. Workshop on Neural Networks for Signal Processing, </booktitle> <year> 1991. </year>
Reference-contexts: Bahl et al. proposed the heuristic corrective training procedure in [BBdSM88]. Katagiri et al. developed the Generalized Probabilistic Descent method that extends the idea of Bayes optimum classification by introducing smooth classification error functions, and generalizes this framework to the classification of patterns of variable lengths <ref> [KLJ91] </ref>.
Reference: [KM94] <author> Y. Konig and N. Morgan. </author> <title> Modeling dynamics in connectionist speech recognition the time index model. </title> <booktitle> In Proc. Intl. Conf. on Speech and Language Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Without embarking in this level of complexity, and following a feature-based approach, we propose to include in the acoustic models time features similar to the time index proposed in [GN93, DASW94] and <ref> [KM94] </ref>. These features don't model correlation but they do alleviate the independence assumption. Our goal here is to explore the usefulness of such knowledge sources as acoustic discriminants, and to propose an efficient and robust architecture to incorporate them in the acoustic models.
Reference: [MBDW93] <author> H. Murveit, J. Butzberger, V. V. Digalakis, and M. Weintraub. </author> <title> Large-vocabulary dictation using SRI's DECIPHER(TM) speech recognition system: </title> <booktitle> Progressive-search techniques. In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages II319:II322, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Baseline System and Databases The baseline system for this work is a speaker-independent continuous speech recognition system trained with 75 conversations of Callhome Spanish data and 80 conversations from Callfriend Span-ish. It is based on continuous-density, genonic HMMs [DMM96], and uses a multipass recognition strategy <ref> [MBDW93] </ref> with a vocabulary of 8K words, non-cross-word acoustic models, and a bigram language model. N-best lists are generated, and rescored with the original acoustic models, a trigram language model, and additional acoustic models such as decision-tree-based cross-word models (DT) or large context-independent phone GMMs (CI). 3.
Reference: [Mer88] <author> B. Merialdo. </author> <title> Phonetic recgnition using hidden markov models and maximum mutual information training. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> New York, NY, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: Practical implementations of Eq. 9 for continuous speech recognition include the estimation of the denominator with a phone loop model <ref> [Mer88] </ref>, and its approximation by a sum over the hypotheses in an N-best list [Cho90]. The first optimization criterion we propose is similar to the N-best list implementation of MMI, but differs in that we augment the N-best list with the correct word sequence, W c .
Reference: [OBB + 97] <author> M. Ostendorf, W. Byrne, M. Bacchiani, M. Finke, A. Gunawardana, K. Ross, S. Roweis, E. Shriberg, D. Talkin, A. Waibel, B. Wheatley, and T. Zeppenfeld. </author> <title> Modeling systematic variations in pronunciation via a language-dependent hidden speaking mode. </title> <type> Technical Report LVCSR Summer Research Workshop, </type> <institution> Johns Hopkins U., </institution> <year> 1997. </year>
Reference-contexts: Decision trees, however, make data sharing among different states difficult, and are not well suited to the use of features that are continuous in nature, as opposed to binary. For these reasons, we chose instead to base our models on neural networks. More recently, Ostendorf et al. <ref> [OBB + 97] </ref> showed that a combination of acoustic and prosodic features could greatly help identifying speech segments that were erroneously recognized (32% predictability improvement for a 10-hour training subset of Switchboard). Similar results were reported by various researchers working on confidence measures for word recognition (e.g. [WBR + 97]).
Reference: [ODK96] <author> M. Ostendorf, V. V. Digalakis, and O. A. Kimball. </author> <title> From HMM's to segment models: A unified view of stochastic modeling for speech recognition. </title> <journal> IEEE Trans. Speech, Audio Processing, </journal> <volume> 4(5), </volume> <year> 1996. </year>
Reference-contexts: Much research has recently been devoted to relaxing the independence assumption imposed by most hidden Markov modeling approaches (HMM) and to modeling the correlation between successive frames of data, leading to the family of so-called segment models <ref> [ODK96] </ref>. Without embarking in this level of complexity, and following a feature-based approach, we propose to include in the acoustic models time features similar to the time index proposed in [GN93, DASW94] and [KM94]. These features don't model correlation but they do alleviate the independence assumption.
Reference: [Rey96] <author> D.A. Reynolds. </author> <title> Mit lincoln laboratory site presentation. In NIST Speaker Recognition Workshop, </title> <address> Linthicum Heights, MD, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: In the field of speaker recognition, the use of handset detectors has dramatically decreased recognition error rates by sorting out carbon button from electret handsets <ref> [Rey96, HW97] </ref>. The handset type could also be used as an input to the acoustic modeling algorithms. Another important issue in acoustic modeling is how to capture the dynamics of the speech signal.
Reference: [RMT86] <author> D.E. Rumelhart, J.L. McClelland, </author> <title> and The PDP Group, </title> <editor> editors. </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1. </volume> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: 5, and taking Eq. 2 into account, we find r Q ' X @~ ' @y j ' ; (6) where ffi j = @y j P j N j P ' ' P j (7) can be backpropagated through the neural network, as in the tradi tional backpropagation algorithm <ref> [RMT86] </ref>. Intuitively, the backpropagation term, ffi, for Gaussian j is large in absolute value if the posterior probability of the Gaussian is very different from its prior probability P j , with both probabilities being functions of the knowledge sources for the current data frame.
Reference: [Slo95] <author> T. Sloboba. </author> <title> Dictionary learning: Performance through consistency. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1995. </year>
Reference-contexts: Whereas some of these factors can be efficiently dealt with by explicit modeling (e.g. vocal tract normalization (e.g. [AKC94]), pronunciation modeling (e.g. <ref> [Slo95, FW97] </ref>)), many others are left for the acoustic models's multi-modal distributions to model implicitly. This, however, has the well-known result of broad overlapping distributions which often lead to recognition errors.
Reference: [WBR + 97] <author> M. Weintraub, F. Beaufays, Z. Rivlin, Y. Konig, and A. Stolcke. </author> <title> Neural network based measures of confidence for word recognition. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: Similar results were reported by various researchers working on confidence measures for word recognition (e.g. <ref> [WBR + 97] </ref>). Presumably, some of these features, which include various measures of speaking rate, SNR, energy, fundamental frequency, stress pattern, and syllable position, could be directly used to disambiguate large acoustic distributions.
Reference: [YOW94] <author> S. J. Young, J. J. Odell, and P. C. Woodland. </author> <title> Tree-based state tying for high accuracy acoustic modelling. </title> <booktitle> In Proc. Human Language Technology Workshop, </booktitle> <pages> pages 307312, </pages> <address> Plainsboro, NJ, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: These are the main issues that motivated this work. In the past decade, contextual linguistic features have been widely used in conjunction with decision tree models, and have significantly improved recognition performance (e.g. <ref> [BdSG + 91, YOW94] </ref>). Decision trees, however, make data sharing among different states difficult, and are not well suited to the use of features that are continuous in nature, as opposed to binary. For these reasons, we chose instead to base our models on neural networks.
References-found: 25

