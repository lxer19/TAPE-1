URL: http://www.cs.indiana.edu/l/www/pub/liu/Inc-TR94.ps.Z
Refering-URL: http://www.cs.indiana.edu/l/www/pub/liu/
Root-URL: http://www.cs.indiana.edu
Title: Systematic Derivation of Incremental Programs  
Author: Yanhong A. Liu Tim Teitelbaum 
Date: August 1994  
Abstract: A systematic approach is given for deriving incremental programs from non-incremental programs written in a standard functional programming language. We exploit a number of program analysis and transformation techniques and domain-specific knowledge, centered around effective utilization of caching, in order to provide a degree of incrementality not otherwise achievable by a generic incremental evaluator.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. Addison-Wesley series in Computer Science. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: The parameter y can be regarded as a change ffix to the input x. Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers <ref> [1, 25, 9, 10] </ref> and transformational programming [27, 39], interactive systems like editors [3, 36] and programming environments [35, 22], and dynamic systems like distributed databases [8, 20] and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34]. <p> Dead code elimination is a traditional optimization <ref> [1, 25] </ref>. <p> Moreover, relevant cache elements with valid context information are chosen to be passed into introduced functions, so that they can be effectively used to incrementalize the computation of corresponding function applications. Last, after the replacements described above, we apply dead code elimination, a traditional optimization technique <ref> [1, 25] </ref>. It is particularly useful here, since replacement changes dependencies between computations, and computations on which no other computations depend are then dead and can be eliminated. Analysis Techniques. To implement the above transformations, several program analysis techniques are needed and summarized here.
Reference: [2] <author> B. Alpern, R. Hoover, B. Rosen, P. Sweeney, and K. Zadeck. </author> <title> Incremental evaluation of computational circuits. </title> <booktitle> In Proceedings of the First Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 32-42, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. Examples are incremental parsing [15, 18], incremental attribute evaluation [36, 49], incremental data-flow analysis [38], incremental circuit evaluation <ref> [2] </ref>, incremental constraint solving [45, 13], etc. The study of dynamic graph algorithms, e.g., [50], can be viewed as falling into this class. Although efforts in this class are directed towards particular incremental algorithms, they apply to a broad class of problems, e.g., any attribute grammar, any circuit, etc.
Reference: [3] <author> R. A. Ballance, S. L. Graham, and M. L. V. D. Vanter. </author> <title> The Pan language-based editing system. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 95-127, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: The parameter y can be regarded as a change ffix to the input x. Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [1, 25, 9, 10] and transformational programming [27, 39], interactive systems like editors <ref> [3, 36] </ref> and programming environments [35, 22], and dynamic systems like distributed databases [8, 20] and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34]. Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes.
Reference: [4] <author> R. S. Bird. </author> <title> Tabulation techniques for recursive programs. </title> <journal> ACM Computing Surveys, </journal> <volume> 12(4) </volume> <pages> 403-417, </pages> <year> 1980. </year>
Reference-contexts: A cache set is augmented, finitely and in a disciplined way, with the help of an auxiliary specializer so that the cached result is utilized effectively under valid context information. The use of a cached result often suggests memoization <ref> [23, 4] </ref>. However, the real power of our approach comes from the effective exploitation of a memoized value under valid context information. The approach to be proposed in Section 6.3 for increasing incrementality by caching auxiliary information can be regarded as a form of smart memoization.
Reference: [5] <author> A. Bondorf and O. Danvy. </author> <title> Automatic autoprojection of recursive equations with global variables and abstract data types. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 16 </volume> <pages> 151-195, </pages> <year> 1991. </year>
Reference-contexts: As occurrence counting analysis helps decide whether an unfolding duplicates computations, it can also help decide whether an unfolding discards computations. Similar solutions are proposed in partial evaluation <ref> [24, 5] </ref>. Note that, even without this technique, the efficiency of our derived programs are guaranteed with the help of time analysis. But in partial evaluation where no time analysis is employed, a transformed program could take exponential time while the original program takes only polynomial time [24].
Reference: [6] <author> R. M. Burstall and J. Darlington. </author> <title> A transformation system for developing recursive programs. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Note that some of the parameters of f 0 0 may be dead and eliminated as discussed in Section 5. 3 obtain a definition of f 0 by the following three steps. First, we unfold <ref> [6] </ref> (also called expand [48]) the application. Second, we incrementalize the unfolded application. <p> Fourth, a global definition set is maintained and used to replace function applications, with corresponding relevant cache elements and valid context information, by applications of introduced functions. Function introduction with generalization and function replacement use the unfold/define/fold scheme <ref> [6] </ref> in a regulated manner so that the transformations are deterministic and the derived programs do not lose termination. Moreover, relevant cache elements with valid context information are chosen to be passed into introduced functions, so that they can be effectively used to incrementalize the computation of corresponding function applications.
Reference: [7] <author> J. Cai and R. Paige. </author> <title> Program derivation by fixed point computation. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 11 </volume> <pages> 197-261, </pages> <month> September </month> <year> 1988/89. </year>
Reference-contexts: N00014-92-J-1973. Authors' address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: fyanhong, ttg@cs.cornell.edu 1 are high level iterators [10], finite differencing of set expressions in SETL [31], optimizing relational database operations [17, 28], incremental fixed point computation <ref> [7] </ref>, differentiation of functional programs in KIDS [39, 40], etc. In most of these works, programs are written in very high-level languages with aggregate data structures, e.g., sets and bags, and fixed rules are offered for transforming aggregate operations into more efficient incremental operations. <p> They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive programs with incremental loop bodies. Such techniques are indispensable as part of an optimizing compiler for languages like SETL or APL <ref> [30, 7] </ref>. The APTS program transformation system [29] has been developed for such purposes. Our technique differs from theirs in that it applies to programs written in a standard language like Lisp.
Reference: [8] <author> S. Ceri, M. A. W. Houtsma, A. M. Keller, and P. Samarati. </author> <title> Achieving incremental consistency among autonomous replicated databases. </title> <journal> IFIP Transactions A [Computer Science and Technology], </journal> <volume> A-25:223-237, </volume> <year> 1993. </year>
Reference-contexts: Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [1, 25, 9, 10] and transformational programming [27, 39], interactive systems like editors [3, 36] and programming environments [35, 22], and dynamic systems like distributed databases <ref> [8, 20] </ref> and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34]. Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes.
Reference: [9] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> 20(11) 850-856, November 1977. 
Reference-contexts: The parameter y can be regarded as a change ffix to the input x. Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers <ref> [1, 25, 9, 10] </ref> and transformational programming [27, 39], interactive systems like editors [3, 36] and programming environments [35, 22], and dynamic systems like distributed databases [8, 20] and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34]. <p> Our work is closest in spirit to the finite differencing techniques of the third class. The name "finite differencing" was originally given by Paige and Koenig [31]. Their work generalizes Cocke's strength reduction <ref> [9] </ref> and provides a convenient framework for implementing a host of transformations including Earley's "iterator inversion" [10]. They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive programs with incremental loop bodies.
Reference: [10] <author> J. Earley. </author> <title> High level iterators and a method for automatically designing data structure representation. </title> <journal> Journal of Computer Languages, </journal> <volume> 1 </volume> <pages> 321-342, </pages> <year> 1976. </year>
Reference-contexts: The parameter y can be regarded as a change ffix to the input x. Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers <ref> [1, 25, 9, 10] </ref> and transformational programming [27, 39], interactive systems like editors [3, 36] and programming environments [35, 22], and dynamic systems like distributed databases [8, 20] and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34]. <p> Examples fl The authors gratefully acknowledge the support of the Office of Naval Research under contract No. N00014-92-J-1973. Authors' address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: fyanhong, ttg@cs.cornell.edu 1 are high level iterators <ref> [10] </ref>, finite differencing of set expressions in SETL [31], optimizing relational database operations [17, 28], incremental fixed point computation [7], differentiation of functional programs in KIDS [39, 40], etc. <p> Our work is closest in spirit to the finite differencing techniques of the third class. The name "finite differencing" was originally given by Paige and Koenig [31]. Their work generalizes Cocke's strength reduction [9] and provides a convenient framework for implementing a host of transformations including Earley's "iterator inversion" <ref> [10] </ref>. They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive programs with incremental loop bodies. Such techniques are indispensable as part of an optimizing compiler for languages like SETL or APL [30, 7].
Reference: [11] <author> D. Eppstein, Z. Galil, G. F. Italiano, and A. Nissenzweig. </author> <title> Sparsification a technique for speeding up dynamic graph algorithms. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on FOCS, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Corresponding to finding auxiliary information, many dynamic algorithms use specially designed data structures to answer queries quickly. Although there is no universally applicable data structure, some apply to a broad class of problems <ref> [11] </ref>. Accommodating such general data structures in our model for deriving incremental programs might help in deciding whether a data structure is applicable to a certain problem.
Reference: [12] <author> J. Field and T. Teitelbaum. </author> <title> Incremental reduction in the lambda calculus. </title> <booktitle> In Proceedings of the ACM '90 Conference on LISP and Functional Programming, </booktitle> <pages> pages 307-322, </pages> <year> 1990. </year>
Reference-contexts: second class, rather than manually developing particular incremental algorithms, application programs are run in a general incremental execution framework so that incremental computation is achieved automatically, e.g., incremental attribute evaluation frameworks [35], incremental computation via function caching [33], formal program manipulations using traditional partial evaluation [43, 42], incremental lambda reduction <ref> [12] </ref>, the change detailing network of INC [51], incremental computation as a program abstraction [16], etc. In this class, often no explicitly incremental version of an application program is derived and run autonomously by a standard evaluator.
Reference: [13] <author> B. N. Freeman-Benson, J. Maloney, and A. Borning. </author> <title> An incremental constraint solver. </title> <journal> Communications of the ACM, </journal> <volume> 33(1) </volume> <pages> 54-63, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. Examples are incremental parsing [15, 18], incremental attribute evaluation [36, 49], incremental data-flow analysis [38], incremental circuit evaluation [2], incremental constraint solving <ref> [45, 13] </ref>, etc. The study of dynamic graph algorithms, e.g., [50], can be viewed as falling into this class. Although efforts in this class are directed towards particular incremental algorithms, they apply to a broad class of problems, e.g., any attribute grammar, any circuit, etc.
Reference: [14] <author> Y. Futamura and K. Nogi. </author> <title> Generalized partial evaluation. </title> <booktitle> In Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 133-151. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: An underlying logic L 0 is used to make inferences based on the facts in an information set. We require that L 0 be compatible with the semantics of the programming language we are using <ref> [14] </ref>, i.e., if two expressions are proved to be equal under L 0 , then they compute the same value. <p> Transformation Techniques. We summarize the major transformation techniques used and emphasize how they are combined to achieve our goal. First, context information is collected for each subcomputation and used to simplify the computation, which mimics the main techniques of generalized partial evaluation <ref> [14] </ref>, where program states are represented symbolically and programs are specialized with the help of a theorem prover. In addition to simplification, context information has another important role in our work, i.e., it serves as keys to cached results and introduced functions for valid replacement to happen.
Reference: [15] <author> C. Ghezzi and D. Mandrioli. </author> <title> Incremental parsing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 58-70, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes. The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. Examples are incremental parsing <ref> [15, 18] </ref>, incremental attribute evaluation [36, 49], incremental data-flow analysis [38], incremental circuit evaluation [2], incremental constraint solving [45, 13], etc. The study of dynamic graph algorithms, e.g., [50], can be viewed as falling into this class.
Reference: [16] <author> R. Hoover. Alphonse: </author> <title> Incremental computation as a programming abstraction. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on PLDI, </booktitle> <pages> pages 261-272, </pages> <address> California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: a general incremental execution framework so that incremental computation is achieved automatically, e.g., incremental attribute evaluation frameworks [35], incremental computation via function caching [33], formal program manipulations using traditional partial evaluation [43, 42], incremental lambda reduction [12], the change detailing network of INC [51], incremental computation as a program abstraction <ref> [16] </ref>, etc. In this class, often no explicitly incremental version of an application program is derived and run autonomously by a standard evaluator. Moreover, any input change to an application program is mapped to whatever the framework can handle, which is fixed for each framework.
Reference: [17] <author> S. Horwitz and T. Teitelbaum. </author> <title> Generating editing environments based on relations and attributes. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 577-608, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: N00014-92-J-1973. Authors' address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: fyanhong, ttg@cs.cornell.edu 1 are high level iterators [10], finite differencing of set expressions in SETL [31], optimizing relational database operations <ref> [17, 28] </ref>, incremental fixed point computation [7], differentiation of functional programs in KIDS [39, 40], etc. In most of these works, programs are written in very high-level languages with aggregate data structures, e.g., sets and bags, and fixed rules are offered for transforming aggregate operations into more efficient incremental operations.
Reference: [18] <author> F. Jalili and J. H. Gallier. </author> <title> Building friendly parsers. </title> <booktitle> In Conference Record of the 9th Annual ACM Symposium on POPL, </booktitle> <pages> pages 196-206, </pages> <address> Albuquerque, New Mexico, </address> <month> January </month> <year> 1982. </year>
Reference-contexts: Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes. The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. Examples are incremental parsing <ref> [15, 18] </ref>, incremental attribute evaluation [36, 49], incremental data-flow analysis [38], incremental circuit evaluation [2], incremental constraint solving [45, 13], etc. The study of dynamic graph algorithms, e.g., [50], can be viewed as falling into this class.
Reference: [19] <author> N. D. Jones, C. K. Gomard, and P. Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: First, time analysis [47, 37] is used when replacing subcomputations by retrievals or replacing function applications by applications of introduced functions. 3 It is a must if we want to guarantee the efficiency of the derived programs. Then, a number of analysis <ref> [19] </ref> are used to assist transforming function applications. Dependence analysis enables us to recognize subcomputations that are possibly computed incrementally, i.e., subcomputations depending on x, and thus avoid introducing functions for function applications that depend only on y, which then helps the derivation procedure terminate.
Reference: [20] <author> E. Levy and A. Silberschatz. </author> <title> Incremental recovery in main memory database systems. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 4(6) </volume> <pages> 529-540, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [1, 25, 9, 10] and transformational programming [27, 39], interactive systems like editors [3, 36] and programming environments [35, 22], and dynamic systems like distributed databases <ref> [8, 20] </ref> and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34]. Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes.
Reference: [21] <author> Y. Liu and T. Teitelbaum. </author> <title> Deriving incremental programs. </title> <type> Technical Report TR 93-1384, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> September (Revised October) </month> <year> 1993. </year>
Reference-contexts: Both works provide only general strategies with no precise procedure to follow, and therefore are much less automatable than ours. A simpler version of our approach for deriving incremental programs was proposed earlier <ref> [21] </ref>. The derivation there consists of four steps: (1) expanding the computation of f 0 (x y), (2) introducing f 0 0 (x; y; r) with the cache parameter r = f 0 (x), (3) replacing subcomputations depending on x by retrievals from r, and (4) eliminating dead computations. <p> Therefore, it can not derive, for example, the functions in Figure 2 from those in Figure 1. The present paper implements an improved version that overcomes this limitation following an idea proposed in <ref> [21] </ref>, where it was called "focusing and switching". The simpler version can be regarded as a special case of the improved version.
Reference: [22] <author> R. Medina-Mora and P. Feiler. </author> <title> An incremental programming environment. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-7(5):472-482, </volume> <month> September </month> <year> 1981. </year>
Reference-contexts: The parameter y can be regarded as a change ffix to the input x. Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [1, 25, 9, 10] and transformational programming [27, 39], interactive systems like editors [3, 36] and programming environments <ref> [35, 22] </ref>, and dynamic systems like distributed databases [8, 20] and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34]. Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes.
Reference: [23] <author> D. Michie. </author> <title> "memo" functions and machine learning. </title> <journal> Nature, </journal> <volume> 218 </volume> <pages> 19-22, </pages> <month> April </month> <year> 1968. </year>
Reference-contexts: A cache set is augmented, finitely and in a disciplined way, with the help of an auxiliary specializer so that the cached result is utilized effectively under valid context information. The use of a cached result often suggests memoization <ref> [23, 4] </ref>. However, the real power of our approach comes from the effective exploitation of a memoized value under valid context information. The approach to be proposed in Section 6.3 for increasing incrementality by caching auxiliary information can be regarded as a form of smart memoization.
Reference: [24] <author> T. Mogensen. </author> <title> Partially static structures in a self-applicable partial evaluator. </title> <booktitle> In Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 325-347. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: As occurrence counting analysis helps decide whether an unfolding duplicates computations, it can also help decide whether an unfolding discards computations. Similar solutions are proposed in partial evaluation <ref> [24, 5] </ref>. Note that, even without this technique, the efficiency of our derived programs are guaranteed with the help of time analysis. But in partial evaluation where no time analysis is employed, a transformed program could take exponential time while the original program takes only polynomial time [24]. <p> Note that, even without this technique, the efficiency of our derived programs are guaranteed with the help of time analysis. But in partial evaluation where no time analysis is employed, a transformed program could take exponential time while the original program takes only polynomial time <ref> [24] </ref>. As a matter of fact, even with this technique, time analysis is still needed in our derivation, since we replace subcomputations by retrievals from a cache result only when we can save time by doing so.
Reference: [25] <editor> S. S. Muchnick and N. D. Jones, editors. </editor> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: The parameter y can be regarded as a change ffix to the input x. Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers <ref> [1, 25, 9, 10] </ref> and transformational programming [27, 39], interactive systems like editors [3, 36] and programming environments [35, 22], and dynamic systems like distributed databases [8, 20] and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34]. <p> Dead code elimination is a traditional optimization <ref> [1, 25] </ref>. <p> Moreover, relevant cache elements with valid context information are chosen to be passed into introduced functions, so that they can be effectively used to incrementalize the computation of corresponding function applications. Last, after the replacements described above, we apply dead code elimination, a traditional optimization technique <ref> [1, 25] </ref>. It is particularly useful here, since replacement changes dependencies between computations, and computations on which no other computations depend are then dead and can be eliminated. Analysis Techniques. To implement the above transformations, several program analysis techniques are needed and summarized here.
Reference: [26] <author> R. Paige. </author> <title> Formal Differentiation: A Program Synthesis Technique, </title> <booktitle> volume 7 of Computer Science. Artificial Intelligence. </booktitle> <publisher> UMI Research Press, </publisher> <address> Ann Arbor, Michigan, </address> <year> 1981. </year> <title> Revision of Ph.D. </title> <address> Thesis New York University, </address> <year> 1979. </year>
Reference-contexts: In the third class, systematic approaches are studied to derive explicitly incremental programs from non-incremental programs using program transformation techniques like finite differencing <ref> [26, 31] </ref>. Examples fl The authors gratefully acknowledge the support of the Office of Naval Research under contract No. N00014-92-J-1973. Authors' address: Department of Computer Science, Cornell University, Ithaca, NY 14853.
Reference: [27] <author> R. Paige. </author> <title> Transformational programming applications to algorithms and systems. </title> <booktitle> In Conference Record of the 10th Annual ACM Symposium on POPL, </booktitle> <pages> pages 73-87, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: The parameter y can be regarded as a change ffix to the input x. Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [1, 25, 9, 10] and transformational programming <ref> [27, 39] </ref>, interactive systems like editors [3, 36] and programming environments [35, 22], and dynamic systems like distributed databases [8, 20] and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34].
Reference: [28] <author> R. Paige. </author> <title> Applications of finite differencing to database integrity control and query/transaction optimization. </title> <editor> In H. Gallaire, J. Minker, and J.-M. Nicolas, editors, </editor> <booktitle> Advances in Database Theory, </booktitle> <volume> Vol. 2, </volume> <pages> pages 171-210. </pages> <publisher> Plenum Press, </publisher> <address> New York, </address> <month> March </month> <year> 1984. </year> <month> 29 </month>
Reference-contexts: N00014-92-J-1973. Authors' address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: fyanhong, ttg@cs.cornell.edu 1 are high level iterators [10], finite differencing of set expressions in SETL [31], optimizing relational database operations <ref> [17, 28] </ref>, incremental fixed point computation [7], differentiation of functional programs in KIDS [39, 40], etc. In most of these works, programs are written in very high-level languages with aggregate data structures, e.g., sets and bags, and fixed rules are offered for transforming aggregate operations into more efficient incremental operations.
Reference: [29] <author> R. Paige. </author> <title> Symbolic finite differencing part I. </title> <booktitle> In Proceedings of the 3rd ESOP, </booktitle> <pages> pages 36-56, </pages> <address> Copenhagen, Denmark, </address> <month> May </month> <year> 1990. </year> <note> Springer-Verlag. LNCS 432. </note>
Reference-contexts: They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive programs with incremental loop bodies. Such techniques are indispensable as part of an optimizing compiler for languages like SETL or APL [30, 7]. The APTS program transformation system <ref> [29] </ref> has been developed for such purposes. Our technique differs from theirs in that it applies to programs written in a standard language like Lisp.
Reference: [30] <author> R. Paige and F. Henglein. </author> <title> Mechanical translation of set theoretic problem specifications into efficient ram code a case study. </title> <journal> Journal of Symbolic Computation, </journal> <volume> 4(2) </volume> <pages> 207-232, </pages> <year> 1987. </year>
Reference-contexts: They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive programs with incremental loop bodies. Such techniques are indispensable as part of an optimizing compiler for languages like SETL or APL <ref> [30, 7] </ref>. The APTS program transformation system [29] has been developed for such purposes. Our technique differs from theirs in that it applies to programs written in a standard language like Lisp.
Reference: [31] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: In the third class, systematic approaches are studied to derive explicitly incremental programs from non-incremental programs using program transformation techniques like finite differencing <ref> [26, 31] </ref>. Examples fl The authors gratefully acknowledge the support of the Office of Naval Research under contract No. N00014-92-J-1973. Authors' address: Department of Computer Science, Cornell University, Ithaca, NY 14853. <p> Examples fl The authors gratefully acknowledge the support of the Office of Naval Research under contract No. N00014-92-J-1973. Authors' address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: fyanhong, ttg@cs.cornell.edu 1 are high level iterators [10], finite differencing of set expressions in SETL <ref> [31] </ref>, optimizing relational database operations [17, 28], incremental fixed point computation [7], differentiation of functional programs in KIDS [39, 40], etc. <p> Our work is closest in spirit to the finite differencing techniques of the third class. The name "finite differencing" was originally given by Paige and Koenig <ref> [31] </ref>. Their work generalizes Cocke's strength reduction [9] and provides a convenient framework for implementing a host of transformations including Earley's "iterator inversion" [10]. They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive programs with incremental loop bodies.
Reference: [32] <author> H. A. Partsch. </author> <title> Specification and Transformation of Programs A Formal Approach to Software Development. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Simplification simplifies the definition of f given the added cache parameter. However, as to how the cache parameter should be used in the simplification to provide incrementality, KIDS provides only the observation that distributive laws can often be applied. The Munich CIP project <ref> [32] </ref> has a strategy for finite differencing that captures similar ideas. It first "defines by a suitable embedding a function f 0 ", and then "derives a recursive version of f 0 using generalized unfold/fold strategy", but provides no special techniques for discovering incrementality.
Reference: [33] <author> W. Pugh and T. Teitelbaum. </author> <title> Incremental computation via function caching. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on POPL, </booktitle> <pages> pages 315-328, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: In the second class, rather than manually developing particular incremental algorithms, application programs are run in a general incremental execution framework so that incremental computation is achieved automatically, e.g., incremental attribute evaluation frameworks [35], incremental computation via function caching <ref> [33] </ref>, formal program manipulations using traditional partial evaluation [43, 42], incremental lambda reduction [12], the change detailing network of INC [51], incremental computation as a program abstraction [16], etc. In this class, often no explicitly incremental version of an application program is derived and run autonomously by a standard evaluator.
Reference: [34] <author> G. Ramalingam and T. Reps. </author> <title> A categorized bibliography on incremental computation. </title> <booktitle> In Conference Record of the 20th Annual ACM Symposium on POPL, </booktitle> <pages> pages 502-510, </pages> <address> Charleston, South Carolina, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: A comprehensive guide to the literature on incremental computation has appeared in <ref> [34] </ref>. Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes. The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. <p> A comprehensive guide to the literature on incremental computation has appeared in <ref> [34] </ref>. Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes. The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. <p> By specializing the general techniques to different applications, we will be able to obtain particular incremental algorithms, particular incremental computation techniques, and particular incremental computation languages. Their applications could include most problems discussed in the literature <ref> [34] </ref>. 28
Reference: [35] <author> T. Reps and T. Teitelbaum. </author> <title> The Synthesizer Generator: A System for Constructing Language-Based Editors. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The parameter y can be regarded as a change ffix to the input x. Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [1, 25, 9, 10] and transformational programming [27, 39], interactive systems like editors [3, 36] and programming environments <ref> [35, 22] </ref>, and dynamic systems like distributed databases [8, 20] and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34]. Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes. <p> In the second class, rather than manually developing particular incremental algorithms, application programs are run in a general incremental execution framework so that incremental computation is achieved automatically, e.g., incremental attribute evaluation frameworks <ref> [35] </ref>, incremental computation via function caching [33], formal program manipulations using traditional partial evaluation [43, 42], incremental lambda reduction [12], the change detailing network of INC [51], incremental computation as a program abstraction [16], etc. <p> We have implemented a prototype system called CACHET for deriving incremental programs based on our approach. The implementation uses the Synthesizer Generator <ref> [35] </ref>, a system for generating language-based editors, and consists of about 14,000 lines of code written in SSL, the Synthesizer Generator language for specifying editors. Source-to-source transformations are operations built in to our editor. Currently, the transformation rules are invoked manually mainly for two reasons.
Reference: [36] <author> T. Reps, T. Teitelbaum, and A. Demers. </author> <title> Incremental context-dependent analysis for language-based editors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 449-477, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: The parameter y can be regarded as a change ffix to the input x. Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [1, 25, 9, 10] and transformational programming [27, 39], interactive systems like editors <ref> [3, 36] </ref> and programming environments [35, 22], and dynamic systems like distributed databases [8, 20] and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34]. Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes. <p> Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes. The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. Examples are incremental parsing [15, 18], incremental attribute evaluation <ref> [36, 49] </ref>, incremental data-flow analysis [38], incremental circuit evaluation [2], incremental constraint solving [45, 13], etc. The study of dynamic graph algorithms, e.g., [50], can be viewed as falling into this class.
Reference: [37] <author> M. Rosendahl. </author> <title> Automatic complexity analysis. </title> <booktitle> In Proceedings of the 4th International Conference on FPCA, </booktitle> <pages> pages 144-156, </pages> <address> London, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: We are only interested in using a cached result if we can save time by doing so. Accordingly, we need a time model T such that T (e) describes the time needed to compute expression e. The function T can be obtained from standard constructions <ref> [47, 37] </ref>. In general, given two expressions e 1 and e 2 , it is not decidable whether e 2 computes faster than e 1 for given values of their variables. <p> It is particularly useful here, since replacement changes dependencies between computations, and computations on which no other computations depend are then dead and can be eliminated. Analysis Techniques. To implement the above transformations, several program analysis techniques are needed and summarized here. First, time analysis <ref> [47, 37] </ref> is used when replacing subcomputations by retrievals or replacing function applications by applications of introduced functions. 3 It is a must if we want to guarantee the efficiency of the derived programs. Then, a number of analysis [19] are used to assist transforming function applications.
Reference: [38] <author> B. G. Ryder and M. C. Paull. </author> <title> Incremental data flow analysis algorithms. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 1-50, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes. The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. Examples are incremental parsing [15, 18], incremental attribute evaluation [36, 49], incremental data-flow analysis <ref> [38] </ref>, incremental circuit evaluation [2], incremental constraint solving [45, 13], etc. The study of dynamic graph algorithms, e.g., [50], can be viewed as falling into this class.
Reference: [39] <author> D. R. Smith. KIDS: </author> <title> A semiautomatic program development system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(9) </volume> <pages> 1024-1043, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The parameter y can be regarded as a change ffix to the input x. Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [1, 25, 9, 10] and transformational programming <ref> [27, 39] </ref>, interactive systems like editors [3, 36] and programming environments [35, 22], and dynamic systems like distributed databases [8, 20] and real-time systems [46]. A comprehensive guide to the literature on incremental computation has appeared in [34]. <p> N00014-92-J-1973. Authors' address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: fyanhong, ttg@cs.cornell.edu 1 are high level iterators [10], finite differencing of set expressions in SETL [31], optimizing relational database operations [17, 28], incremental fixed point computation [7], differentiation of functional programs in KIDS <ref> [39, 40] </ref>, etc. In most of these works, programs are written in very high-level languages with aggregate data structures, e.g., sets and bags, and fixed rules are offered for transforming aggregate operations into more efficient incremental operations. In other work, only high-level strategies are proposed. <p> The technique we propose can be regarded as a principle and a systematic approach, through which incrementalities can be discovered in existing programs written in standard languages. Smith's work in KIDS <ref> [39, 40] </ref> is closely related to ours. KIDS is a semi-automatic program development system that aims to derive efficient programs from high-level specifications [41], as is APTS. Its version of finite differencing was developed for the optimization of its derived functional programs and has two basic operations: abstraction and simplification.
Reference: [40] <author> D. R. Smith. </author> <title> KIDS a knowledge-based software development system. </title> <editor> In M. R. Lowry and R. D. McCartney, editors, </editor> <booktitle> Automating Software Design, chapter 19, </booktitle> <pages> pages 483-514. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1991. </year> <booktitle> Proceedings of the Workshop on Automating Software Design, AAAI '88. </booktitle>
Reference-contexts: N00014-92-J-1973. Authors' address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: fyanhong, ttg@cs.cornell.edu 1 are high level iterators [10], finite differencing of set expressions in SETL [31], optimizing relational database operations [17, 28], incremental fixed point computation [7], differentiation of functional programs in KIDS <ref> [39, 40] </ref>, etc. In most of these works, programs are written in very high-level languages with aggregate data structures, e.g., sets and bags, and fixed rules are offered for transforming aggregate operations into more efficient incremental operations. In other work, only high-level strategies are proposed. <p> The technique we propose can be regarded as a principle and a systematic approach, through which incrementalities can be discovered in existing programs written in standard languages. Smith's work in KIDS <ref> [39, 40] </ref> is closely related to ours. KIDS is a semi-automatic program development system that aims to derive efficient programs from high-level specifications [41], as is APTS. Its version of finite differencing was developed for the optimization of its derived functional programs and has two basic operations: abstraction and simplification.
Reference: [41] <author> D. R. Smith and M. R. Lowry. </author> <title> Algorithm theories and design tactics. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 14 </volume> <pages> 305-321, </pages> <year> 1990. </year>
Reference-contexts: Smith's work in KIDS [39, 40] is closely related to ours. KIDS is a semi-automatic program development system that aims to derive efficient programs from high-level specifications <ref> [41] </ref>, as is APTS. Its version of finite differencing was developed for the optimization of its derived functional programs and has two basic operations: abstraction and simplification. Abstraction of a function f adds an extra cache parameter to f .
Reference: [42] <author> R. S. Sundaresh. </author> <title> Building incremental programs using partial evaluation. </title> <booktitle> In Proceedings of the Symposium on PEPM, </booktitle> <pages> pages 83-93, </pages> <institution> Yale University, </institution> <month> June </month> <year> 1991. </year> <note> Published as SIGPLAN Notices, 26(9). </note>
Reference-contexts: In the second class, rather than manually developing particular incremental algorithms, application programs are run in a general incremental execution framework so that incremental computation is achieved automatically, e.g., incremental attribute evaluation frameworks [35], incremental computation via function caching [33], formal program manipulations using traditional partial evaluation <ref> [43, 42] </ref>, incremental lambda reduction [12], the change detailing network of INC [51], incremental computation as a program abstraction [16], etc. In this class, often no explicitly incremental version of an application program is derived and run autonomously by a standard evaluator. <p> Since our transformational approach is related to partial evaluation in some aspects, it is worthwhile to specially compare it with the work by Sundaresh and Hudak <ref> [43, 42] </ref> in the second class. The common aspect is that both works aim at obtaining incremental computation by transforming non-incremental programs. However, two approaches follows different lines. Their work mostly uses partial evaluation, with extra efforts on partitioning program inputs and combining residual programs.
Reference: [43] <author> R. S. Sundaresh and P. Hudak. </author> <title> Incremental computation via partial evaluation. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on POPL, </booktitle> <pages> pages 1-13, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In the second class, rather than manually developing particular incremental algorithms, application programs are run in a general incremental execution framework so that incremental computation is achieved automatically, e.g., incremental attribute evaluation frameworks [35], incremental computation via function caching [33], formal program manipulations using traditional partial evaluation <ref> [43, 42] </ref>, incremental lambda reduction [12], the change detailing network of INC [51], incremental computation as a program abstraction [16], etc. In this class, often no explicitly incremental version of an application program is derived and run autonomously by a standard evaluator. <p> Since our transformational approach is related to partial evaluation in some aspects, it is worthwhile to specially compare it with the work by Sundaresh and Hudak <ref> [43, 42] </ref> in the second class. The common aspect is that both works aim at obtaining incremental computation by transforming non-incremental programs. However, two approaches follows different lines. Their work mostly uses partial evaluation, with extra efforts on partitioning program inputs and combining residual programs.
Reference: [44] <author> V. F. Turchin. </author> <title> The concept of a supercompiler. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(3) </volume> <pages> 292-325, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Third, in consistence with the strict semantics of our language, we apply simplification and replacement on subcomputations in applicative order, and, moreover, we lift conditions and bindings out of subcomputations. This lifting technique is similar in spirit to the driving transformation by supercompilation <ref> [44] </ref>. It causes relatively drastic reorganization of program structures that helps expose incrementality that is otherwise hidden. Fourth, a global definition set is maintained and used to replace function applications, with corresponding relevant cache elements and valid context information, by applications of introduced functions.
Reference: [45] <author> B. T. Vander Zanden. </author> <title> Incremental constraint satisfaction and its application to graphical interfaces. </title> <type> Ph.D. Thesis TR 88-941, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> October </month> <year> 1988. </year>
Reference-contexts: The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. Examples are incremental parsing [15, 18], incremental attribute evaluation [36, 49], incremental data-flow analysis [38], incremental circuit evaluation [2], incremental constraint solving <ref> [45, 13] </ref>, etc. The study of dynamic graph algorithms, e.g., [50], can be viewed as falling into this class. Although efforts in this class are directed towards particular incremental algorithms, they apply to a broad class of problems, e.g., any attribute grammar, any circuit, etc.
Reference: [46] <author> A. Varma and S. Chalasani. </author> <title> An incremental algorithm for TDM switching assignments in satellite and terrestrial networks. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 10(2) </volume> <pages> 364-377, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Methods of incremental computation have widespread applications, e.g., loop optimizations in optimizing compilers [1, 25, 9, 10] and transformational programming [27, 39], interactive systems like editors [3, 36] and programming environments [35, 22], and dynamic systems like distributed databases [8, 20] and real-time systems <ref> [46] </ref>. A comprehensive guide to the literature on incremental computation has appeared in [34]. Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes. The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes.
Reference: [47] <author> B. Wegbreit. </author> <title> Mechanical program analysis. </title> <journal> Communications of the ACM, </journal> <volume> 18(9) </volume> <pages> 528-538, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: We are only interested in using a cached result if we can save time by doing so. Accordingly, we need a time model T such that T (e) describes the time needed to compute expression e. The function T can be obtained from standard constructions <ref> [47, 37] </ref>. In general, given two expressions e 1 and e 2 , it is not decidable whether e 2 computes faster than e 1 for given values of their variables. <p> It is particularly useful here, since replacement changes dependencies between computations, and computations on which no other computations depend are then dead and can be eliminated. Analysis Techniques. To implement the above transformations, several program analysis techniques are needed and summarized here. First, time analysis <ref> [47, 37] </ref> is used when replacing subcomputations by retrievals or replacing function applications by applications of introduced functions. 3 It is a must if we want to guarantee the efficiency of the derived programs. Then, a number of analysis [19] are used to assist transforming function applications.
Reference: [48] <author> B. Wegbreit. </author> <title> Goal-directed program transformation. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-2(2):69-80, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: Note that some of the parameters of f 0 0 may be dead and eliminated as discussed in Section 5. 3 obtain a definition of f 0 by the following three steps. First, we unfold [6] (also called expand <ref> [48] </ref>) the application. Second, we incrementalize the unfolded application.
Reference: [49] <author> D. Yeh and U. Kastens. </author> <title> Improvements on an incremental evaluation algorithm for ordered attribute grammars. </title> <journal> SIGPLAN Notices, </journal> <volume> 23(12) </volume> <pages> 45-50, </pages> <year> 1988. </year>
Reference-contexts: Despite the relatively diverse categories discussed in [34], most of the work can be divided into three classes. The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. Examples are incremental parsing [15, 18], incremental attribute evaluation <ref> [36, 49] </ref>, incremental data-flow analysis [38], incremental circuit evaluation [2], incremental constraint solving [45, 13], etc. The study of dynamic graph algorithms, e.g., [50], can be viewed as falling into this class.
Reference: [50] <author> D. M. Yellin. </author> <title> Speeding up dynamic transitive closure for bounded degree graphs. </title> <journal> Acta Informatica, </journal> <volume> 30(4) </volume> <pages> 369-384, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The first class includes particular incremental algorithms designed for particular problems dealing with particular input changes. Examples are incremental parsing [15, 18], incremental attribute evaluation [36, 49], incremental data-flow analysis [38], incremental circuit evaluation [2], incremental constraint solving [45, 13], etc. The study of dynamic graph algorithms, e.g., <ref> [50] </ref>, can be viewed as falling into this class. Although efforts in this class are directed towards particular incremental algorithms, they apply to a broad class of problems, e.g., any attribute grammar, any circuit, etc.
Reference: [51] <author> D. M. Yellin and R. E. Strom. INC: </author> <title> A language for incremental computations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 211-236, </pages> <month> April </month> <year> 1991. </year> <month> 30 </month>
Reference-contexts: incremental algorithms, application programs are run in a general incremental execution framework so that incremental computation is achieved automatically, e.g., incremental attribute evaluation frameworks [35], incremental computation via function caching [33], formal program manipulations using traditional partial evaluation [43, 42], incremental lambda reduction [12], the change detailing network of INC <ref> [51] </ref>, incremental computation as a program abstraction [16], etc. In this class, often no explicitly incremental version of an application program is derived and run autonomously by a standard evaluator.
References-found: 51

