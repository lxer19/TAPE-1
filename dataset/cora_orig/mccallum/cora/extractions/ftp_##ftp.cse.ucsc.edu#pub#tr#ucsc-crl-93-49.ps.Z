URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-93-49.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: Automated Termination Analysis for Logic Programs  
Author: by Kirack Sohn Allen Van Gelder Phokion G. Kolaitis Al Kelly Dean 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in  The dissertation of Kirack Sohn is approved:  
Date: December 1993  
Affiliation: University of California Santa Cruz  Computer and Information Sciences  of Graduate Studies and Research  
Abstract-found: 0
Intro-found: 1
Reference: [AB91] <author> K. R. Apt and M. Bezem. </author> <title> Acyclic programs. </title> <journal> New Generation Computing, </journal> <volume> 9 </volume> <pages> 335-363, </pages> <year> 1991. </year>
Reference-contexts: Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues <ref> [AP90, AB91, Dev90] </ref>. These rather theoretical works usually provides manually verifiable criteria for termination. Undecidability of the halting problem, a classical result of theoretical computer science, states that it is undecidable to determine whether or not any program terminates.
Reference: [AP90] <author> K. R. Apt and D. Pedreschi. </author> <title> Studies in pure Prolog: termination. </title> <booktitle> In Proceedings of Esprit symposium on computational logic, </booktitle> <pages> pages 150-176, </pages> <address> Brussels, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues <ref> [AP90, AB91, Dev90] </ref>. These rather theoretical works usually provides manually verifiable criteria for termination. Undecidability of the halting problem, a classical result of theoretical computer science, states that it is undecidable to determine whether or not any program terminates.
Reference: [APP + 89] <author> F. Afrati, C. Papadimitriou, G. Papageorgiou, A. R. Roussou, Y. Sagiv, and J. D. Ullman. </author> <title> On the convergence of query evaluation. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 38(2) </volume> <pages> 341-359, </pages> <year> 1989. </year>
Reference-contexts: This proliferation is mainly motivated by the practical needs for termination analysis, such as in the area of control generation and program verification. A major concern in this case is automation of the termination proof process <ref> [Nai83, UVG88, APP + 89, BS89b, DSVB90, Plu90a, Plu90b, Sag91, SVG91] </ref>. Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues [AP90, AB91, Dev90].
Reference: [BJCB87] <author> M. Bruynooghe, G. Janssens, A. Callebaut, and Demoen B. </author> <title> Abstract interpretation: Towards the global optimisation of Prolog programs. </title> <booktitle> In Proceedings of the 1987 International Symposium on Logic Programming, </booktitle> <publisher> IEEE Press, </publisher> <year> 1987. </year>
Reference-contexts: With this regard, Warren introduced explicit mode declarations to help the compiler produce better code [War77]. But mode declarations must be verified since wrong annotation may introduce subtle errors in program executions. Another approach is to infer mode declarations automatically via abstract interpretation <ref> [Mel87, BJCB87, MU87, DW88] </ref>. Abstract interpretation has been used as standard means for dataflow analysis since it was placed on a solid semantic basis by Cousot and Cousot [CC77] (See [CC92] for the theory and applications to logic programming). <p> In addition, their method could not handle the problem with aliased variables accurately. Bruynooghe et al. <ref> [BJCB87] </ref> suggested multi-passes algorithm (repeat previous call strategy) to resolve aliasing problem, which are considered to be costly if the strategy is applied globally. 1 In their paper, nonground means possibly nonground, which means any 75 Debray and Warren [DW88] presented an algorithm to give a sound and efficient treatment of
Reference: [BS89a] <author> A. Brodsky and Y. Sagiv. </author> <title> Inference of monotonicity constraints in Datalog programs. </title> <booktitle> In Eighth ACM Symposium on Principles of Database Systems, </booktitle> <pages> pages 190-199, </pages> <year> 1989. </year>
Reference-contexts: In Chapter 4, we will describe how to derive those constraints, so-called "interargument constraints" of a predicate. Interargument constraints are essentially a set of constraints which every derivable fact with respect to a predicate satisfies. Research on this topic has recently been studied as a separate task <ref> [BS89a, VG91, BS91] </ref>. In this thesis, the argument sizes of derivable facts with respect to an n-ary predicate are viewed as a set of points in R n , which are approximated by their convex hull. <p> The idea of using interargument constraints (in the form of inequality between two argument positions) was first introduced. Candidates of interargument inequalities sufficient for termination were generated, and then their validity was tested. Brodsky and Sagiv studied inference of interargument constraints called "monotonicity constraints" <ref> [BS89a] </ref> and termination detection [BS89b] in logic programs without function symbols (Datalog programs) as two separate tasks. A monotonicity constraint is a statement that one argument of a predicate is greater than another in all derivable (or given, for EDB) facts for that predicate. <p> Methods to derive ICs have been studied recently in terms of Datalog <ref> [BS89a, BS91] </ref> or logical rules with function symbols [VG91]. In their methods IC is formalized by the least fixpoint of bottom-up inference operator similar to "immediate consequence operator". In [BS91] ICs are captured by a disjunctive union of inequalities between two argument sizes.
Reference: [BS89b] <author> A. Brodsky and Y. Sagiv. </author> <title> On termination of Datalog programs. </title> <booktitle> In First International Conference on Deductive and Object-Oriented Databases, </booktitle> <pages> pages 95-112, </pages> <address> Kyoto, Japan, </address> <year> 1989. </year>
Reference-contexts: This proliferation is mainly motivated by the practical needs for termination analysis, such as in the area of control generation and program verification. A major concern in this case is automation of the termination proof process <ref> [Nai83, UVG88, APP + 89, BS89b, DSVB90, Plu90a, Plu90b, Sag91, SVG91] </ref>. Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues [AP90, AB91, Dev90]. <p> Later, the method was enhanced by Plumer [Plu90a, Plu90b] and Van Gelder and Sohn [SVG91]. Brodsky and Sagiv, who studied termination of Datalog programs, were also main contributors along this line <ref> [BS89b, Sag91] </ref>. 1.2 How It Works In this section, we shall describe the basic structure of termination proof for logic programs. Every termination proof is tantamount to showing the well-foundedness (no infinitely decreasing sequence) of a computation path. <p> The idea of using interargument constraints (in the form of inequality between two argument positions) was first introduced. Candidates of interargument inequalities sufficient for termination were generated, and then their validity was tested. Brodsky and Sagiv studied inference of interargument constraints called "monotonicity constraints" [BS89a] and termination detection <ref> [BS89b] </ref> in logic programs without function symbols (Datalog programs) as two separate tasks. A monotonicity constraint is a statement that one argument of a predicate is greater than another in all derivable (or given, for EDB) facts for that predicate. <p> Sometimes the syntactic transformations described later clarify such situations before termination detection begins. The argument mapping method <ref> [BS89b, BS91] </ref> pays closer attention to unification issues than we do, and has the remarkable property of detecting certain cases where unification fails due to the "occurs check". r 1 : q (X,Y) :- e (X,Y). r 3 : q (X,f (f (Y))) :- p (X,f (Y)). r 5 : p
Reference: [BS91] <author> A. Brodsky and Y. Sagiv. </author> <title> Inference of inequality constraints in logic programs. </title> <booktitle> In Tenth ACM Symposium on Principles of Database Systems, </booktitle> <year> 1991. </year>
Reference-contexts: In Chapter 4, we will describe how to derive those constraints, so-called "interargument constraints" of a predicate. Interargument constraints are essentially a set of constraints which every derivable fact with respect to a predicate satisfies. Research on this topic has recently been studied as a separate task <ref> [BS89a, VG91, BS91] </ref>. In this thesis, the argument sizes of derivable facts with respect to an n-ary predicate are viewed as a set of points in R n , which are approximated by their convex hull. <p> They also gave a sufficient condition for termination on infinite 33 database relations ( Logic programs with function symbols can be simulated by Datalog with infinite database relations). They recently proposed a method to infer inequality constraints between two arguments in programs with function symbols <ref> [BS91] </ref>. Plumer described some extensions for rules that did not satisfy the "uniqueness" property [Plu90a, Plu90b]. His main contribution was that certain constraints involving more than two argument positions could be used. However, he required an "admissibility" property on the rules. Moreover, mutual recursion presented problems for his method. <p> The constraints are obtained by the method in Chapter 4 (or supplied by other external means such as <ref> [VG91, BS91] </ref>). Example 3.7: Consider the adorned logic program perm 3 : r 1 : perm bf ([], []). r 2 : perm bf (P, [X|L]) :- append ffb (E, [X|F], P), append bbf (E, F, P1), perm bf (P1, L). <p> Sometimes the syntactic transformations described later clarify such situations before termination detection begins. The argument mapping method <ref> [BS89b, BS91] </ref> pays closer attention to unification issues than we do, and has the remarkable property of detecting certain cases where unification fails due to the "occurs check". r 1 : q (X,Y) :- e (X,Y). r 3 : q (X,f (f (Y))) :- p (X,f (Y)). r 5 : p <p> Methods to derive ICs have been studied recently in terms of Datalog <ref> [BS89a, BS91] </ref> or logical rules with function symbols [VG91]. In their methods IC is formalized by the least fixpoint of bottom-up inference operator similar to "immediate consequence operator". In [BS91] ICs are captured by a disjunctive union of inequalities between two argument sizes. <p> Methods to derive ICs have been studied recently in terms of Datalog [BS89a, BS91] or logical rules with function symbols [VG91]. In their methods IC is formalized by the least fixpoint of bottom-up inference operator similar to "immediate consequence operator". In <ref> [BS91] </ref> ICs are captured by a disjunctive union of inequalities between two argument sizes. They show undecidability results on interesting questions such as whether a specific procedure for computing ICs computes a finite set of constraints. <p> Overall, the main contribution of this chapter is to provide an efficient method to derive precise ICs for practical logic programs. Example 4.2: With a simple append procedure, we compare our results with others. Sagiv and Brodsky's method <ref> [BS91] </ref> cannot capture the IC we derive here since their IC is in the form of inequalities between two argument positions. Van Gelder's method [VG91] does not converge with this example; his heuristic that "sometimes" works gives the same IC. However, we provide an affine widening that always works. <p> Theorem 4.3: The sequence A 0 ; A 1 ; : : : ; A l is finite. 1 Brodsky and Sagiv studied inference of disjunction of inequalities between two argument positions. With a transformation similar to our recursive transformation, they proved the question on convergence is undecidable <ref> [BS91] </ref> 2 If x T (x) (x T (x)), then x is a postfixpoint (prefixpoint) of T . 64 Proof : It is enough to show the dimension of A i+1 is greater than that of A i for i = 0; : : : ; l 1 since A i <p> It is noted that Van Gelder's recursive transformation does not converge with this example and his heuristic does not work [VG91]. Sagiv and Brodsky's method can infer only inequalities between two argument positions <ref> [BS91] </ref>.
Reference: [CC77] <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Conference Record of the 4th ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1977. </year>
Reference-contexts: Another approach is to infer mode declarations automatically via abstract interpretation [Mel87, BJCB87, MU87, DW88]. Abstract interpretation has been used as standard means for dataflow analysis since it was placed on a solid semantic basis by Cousot and Cousot <ref> [CC77] </ref> (See [CC92] for the theory and applications to logic programming). Early work done by Mellish [Mel81] produced erroneous results since aliasing effects resulting from unification was not considered, which was corrected later [Mel87].
Reference: [CC92] <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation and application to logic programs. </title> <journal> Journal of Logic Programming, </journal> <volume> 13 </volume> <pages> 103-179, </pages> <year> 1992. </year>
Reference-contexts: Another approach is to infer mode declarations automatically via abstract interpretation [Mel87, BJCB87, MU87, DW88]. Abstract interpretation has been used as standard means for dataflow analysis since it was placed on a solid semantic basis by Cousot and Cousot [CC77] (See <ref> [CC92] </ref> for the theory and applications to logic programming). Early work done by Mellish [Mel81] produced erroneous results since aliasing effects resulting from unification was not considered, which was corrected later [Mel87]. <p> We now give a bottom-up abstract interpretation to get success patterns in the form of groundness constraints. Abstract interpretation consists of abstract domain and abstract operations which mimics base semantics faithfully. Hence bottom-up abstract interpretation mimics fixpoint semantics. Formal framework of bottom-up abstract interpretation can be found in <ref> [CC92, MS88] </ref>. Now we define a transformation mimicking immediate consequence operator. 84 Definition 5.6: Suppose we have k predicates whose names are p; q; : : : ; r in an abstract program and p i is i-th clause having the head predicate p and so forth.
Reference: [CD93] <author> M. Codish and B. Demoen. </author> <title> Analysing logic programs using "prop"-ositional logic programs and a magic wand. </title> <editor> In D. Miller, editor, </editor> <booktitle> Logic Programming: Proceedings of the 1993 International Symposium, </booktitle> <pages> pages 114-129, </pages> <address> Cambridge, Massachusetts, 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The ratio was defined in Section 3.9. The input programs are shown in Appendix D. The programs were tested on SUN4 sparc station. The domain PROP was implemented by B. Le Charlier and P. Van Hentenryck [LCVH93] and by M. Codish and B. Demoen <ref> [CD93] </ref>. Our current implementation can take programs in pure Prolog as input. We are working on lifting this limitation. It will be interesting to compare our performance results with their results. Including freeness analysis and developing efficient test of cone equivalence will be future research directions. 86 6.
Reference: [CFW91] <author> G. Cortesi, G. File, and W. Winsborough. </author> <title> PROP revisited: Propositional formula as abstract domain for groundness analysis. </title> <booktitle> In Proceedings of Sixth IEEE Symposium on Logic In Computer Science, </booktitle> <pages> pages 322-327, </pages> <address> Cambridge, Massachusetts, 1991. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Using positive propositional formula (called domain PROP) for groundness analysis has been studied by Marriott and Stndergaard [MS89]. The domain was further studied by Cortesi et al. <ref> [CFW91] </ref>. Our abstract domain is simpler than PROP since we only use positive proposition formula forming so-called a "boolean cone". For such formula, we can easily find a minimal generator set which is a unique minimal representation.
Reference: [CH78] <author> P. Cousot and N. Halbwachs. </author> <title> Automatic discovery of linear restraints among variables of a program. </title> <booktitle> In Conference Record of the 5th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 84-96, </pages> <year> 1978. </year>
Reference-contexts: Requiring the least fixpoint is too much in general; however, postfixpoints 2 are also a correct upper approximation of least fixpoint by Tarski's fixpoint theorem: T (x) x implies lfpT x. The idea of using widening has already been used in the abstract interpretation of programs in procedural languages <ref> [CH78] </ref>. We now describe what we call affine widening. To simplify notations, let us omit subscripts concerning predicate name and suppose there is only one predicate in a program. A i is an affine subspace w.r.t. the predicate. <p> DFS (new neighboring basis, new tableau). Figure B.1: Algorithm to find all extreme points and rays R j = 1. R B (i) = i-th component of A j . The reasoning is we can move from P along R indefinitely. More details can be found in <ref> [VG91, CH78] </ref>. 94 6 x 2 6 t t p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p
Reference: [CLR90] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <booktitle> Introduction to Algorithms, </booktitle> <pages> pages 488-493. </pages> <publisher> MIT Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: By doing the same thing for other rules, we have the predicate dependency graph for the above logical rules, as shown in Figure 3.1. Using algorithms to find SCCs, which can be found in many algorithm textbooks, for example <ref> [CLR90] </ref>, we identify two SCCs, SCC 1 consisting of the predicate s, and SCC 2 consisting of the predicates p, q, r. SCC 1 is linear recursive, while SCC 2 is mutually recursive.
Reference: [CM81] <author> W. F. Clocksin and C. S. Mellish. </author> <title> Programming in Prolog. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year> <month> 103 </month>
Reference-contexts: Technically, an empty clause, that is, a clause with empty head and empty body is denoted . This clause is understood as a contradiction. In examples, we shall use standard Edinburgh-style Prolog syntax <ref> [CM81, SS86] </ref>; Variables are denoted by a character string starting with uppercase letters while constants, function symbols, and predicate symbols are denoted by a character string starting with lower case letters. The syntax [H|T] (equivalent to `.'(H,T)) denotes a list whose head is H and tail is T.
Reference: [Dev90] <author> P. Devienne. </author> <title> Weighted graphs: a tool for studying the halting problem and time complexity in term rewriting systems and logic programming. </title> <journal> Theoretical Computer Science, </journal> <volume> 75(2) </volume> <pages> 157-215, </pages> <year> 1990. </year>
Reference-contexts: Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues <ref> [AP90, AB91, Dev90] </ref>. These rather theoretical works usually provides manually verifiable criteria for termination. Undecidability of the halting problem, a classical result of theoretical computer science, states that it is undecidable to determine whether or not any program terminates.
Reference: [DSD] <author> D. De Schreye and S. Decorte. </author> <title> Termination of logic programs: the never-ending story. </title> <type> preprint. </type>
Reference-contexts: The last few years have nevertheless seen a variety of proposed methods. D. De Schreye and S. Decorte studied and collected in their reference list a total of 53 papers published on this topic since 1988 <ref> [DSD] </ref>. This proliferation is mainly motivated by the practical needs for termination analysis, such as in the area of control generation and program verification. A major concern in this case is automation of the termination proof process [Nai83, UVG88, APP + 89, BS89b, DSVB90, Plu90a, Plu90b, Sag91, SVG91].
Reference: [DSVB90] <author> D. De Schreye, K. Verschaetse, and M. Bruynooghe. </author> <title> A practical technique for detecting non-terminating queries for a restricted class of horn clauses, using directed, weighted graphs. </title> <booktitle> In Proc. 7th Int'l Conf. on Logic Programming, </booktitle> <pages> pages 649-663, </pages> <address> Jerusalem, </address> <year> 1990. </year>
Reference-contexts: This proliferation is mainly motivated by the practical needs for termination analysis, such as in the area of control generation and program verification. A major concern in this case is automation of the termination proof process <ref> [Nai83, UVG88, APP + 89, BS89b, DSVB90, Plu90a, Plu90b, Sag91, SVG91] </ref>. Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues [AP90, AB91, Dev90].
Reference: [DW88] <author> S. K. Debray and D. S. Warren. </author> <title> Automatic mode inference for logic programs. </title> <journal> Journal of Logic Programming, </journal> <volume> 5(3) </volume> <pages> 207-229, </pages> <year> 1988. </year>
Reference-contexts: With this regard, Warren introduced explicit mode declarations to help the compiler produce better code [War77]. But mode declarations must be verified since wrong annotation may introduce subtle errors in program executions. Another approach is to infer mode declarations automatically via abstract interpretation <ref> [Mel87, BJCB87, MU87, DW88] </ref>. Abstract interpretation has been used as standard means for dataflow analysis since it was placed on a solid semantic basis by Cousot and Cousot [CC77] (See [CC92] for the theory and applications to logic programming). <p> Bruynooghe et al. [BJCB87] suggested multi-passes algorithm (repeat previous call strategy) to resolve aliasing problem, which are considered to be costly if the strategy is applied globally. 1 In their paper, nonground means possibly nonground, which means any 75 Debray and Warren <ref> [DW88] </ref> presented an algorithm to give a sound and efficient treatment of aliasing. However their method produces less precise mode information since they use conservative local analysis in which all unsafe instantiations are replaced by any.
Reference: [Gru67] <author> B. Grunbaum. </author> <title> Convex Polytopes. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1967. </year>
Reference-contexts: A graph G = (V ; E ) is one induced by an affine transformation : V = f (v) j v 2 V g (4:15) Definition 4.6: <ref> [Gru67] </ref> Let G = (V; E) be a graph, M a set of vertices of G, and v; u two vertices of G which do not belong to M .
Reference: [Hog90] <author> C. J. </author> <title> Hogger. Essentials of Logic Programming. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Starting with empty interpretation ;, T P arrives at a least fixpoint; however, depending on starting interpretations, there exist numerous fixpoints. The transformation may need infinite iterations before reaching a fixpoint. Details can be found in <ref> [Llo84, Hog90] </ref>. Basically, our algorithm to infer interargument constraints simulates the immediate consequence operator T P . 2.4 Deductive Databases One interpretation of logic is the database interpretation. Here a logic program is regarded as a database.
Reference: [KLTZ83] <author> M. H. Karwan, V. Lofti, J. Telgen, and S. Zionts. </author> <title> Redundancy in Mathematical Programming: A State-of-the-Art Survey. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: The cost of finding an implicit equality is equivalent to that of running a linear program. General techniques for redundancy elimination of linear constraints can be found in <ref> [KLTZ83, LHM89] </ref>. To verify a fixpoint of a recursive transformation T , we need to extract extreme points and rays from points and rays transformed from the constraint set.
Reference: [Kow74] <author> R. A. Kowalski. </author> <title> Predicate logic as a programming language. </title> <booktitle> In Proceedings of IFIP'74, </booktitle> <pages> pages 569-574, </pages> <address> Amsterdam, 1974. </address> <publisher> North-Holland. </publisher>
Reference-contexts: 1. Introduction Logic programming was first introduced in a seminal article of Kowalski in 1974 <ref> [Kow74] </ref>. It has been successfully used as a tool for several areas including compiler writing, expert system design, natural language processing, hardware design, and knowledge-base design for two decades. <p> However, to apply our termination analysis to general logic programs where safety assumption does not hold, groundness analysis is necessary. Chapter 6 discusses further research directions and concludes the thesis. 9 2. Logic Programming Preliminaries Since logic programming was introduced in Kowalski's seminal paper in 1974 <ref> [Kow74] </ref>, logic has been successfully used as a programming language for two decades in various areas of computer sciences, not to mention artificial intelligence and database. Its power stems from simple formalism and rigorous mathematical framework.
Reference: [Kow79] <author> R. A. Kowalski. </author> <title> Algorithm = logic + control. </title> <journal> Communications of the ACM, </journal> <volume> 22 </volume> <pages> 424-431, </pages> <year> 1979. </year>
Reference-contexts: One obvious contribution of termination analysis is to the development of reliable software. Motivations for termination analysis in logic programming, however, arise from more or less different purposes. The well-known Kowalski's formula <ref> [Kow79] </ref> says a logic program can be viewed as Algorithm = Logic + Control. Since the control component of a logic program is independent of its logic component, various control schemes tailored for the applications can be developed.
Reference: [Las90a] <author> J.-L. Lassez. </author> <title> Parametric queries, linear constraints and variable elimination. </title> <booktitle> In Proceedings of DISCO 90, Springer Verlag Lecture Notes in Computer Science, </booktitle> <year> 1990. </year>
Reference-contexts: Contradiction. As there are well-known methods to find extreme rays of convex cones (same as finding extreme points of convex polytopes) <ref> [VG91, Las90a] </ref>, we can find candidates for generators for the corresponding boolean cones by removing redundancy. 5.4 Bottom-Up Groundness Analysis In the preceding sections, we described the relational abstract domain and some useful operations on the domain.
Reference: [Las90b] <author> J.-L. Lassez. </author> <title> Querying constraints. </title> <booktitle> In Ninth ACM Symposium on Principles of Database Systems, </booktitle> <pages> pages 288-298, </pages> <year> 1990. </year>
Reference-contexts: Lassez made essentially the same observation, but in a more circumlocutory fashion <ref> [Las90b] </ref>. <p> 6 6 6 6 6 6 u w fi 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 4 0 ffi ij 0 7 7 7 7 7 7 7 7 (3:8) This set of constraints is very amenable to reduction by Fourier-Motzkin elimination <ref> [Sch86, LHM89, Las90b] </ref>. In this technique a variable is eliminated by "cancelling" all positive occurrences with all negative occurrences, pairwise, creating new rows (with 0 in that variable's column). Then all rows containing a nonzero coefficient for that variable can be eliminated, preserving satisfiability (See Appendix A for details).
Reference: [LCVH93] <author> B. Le Charlier and P. Van Hentenryck. </author> <title> Groundness analysis for prolog: implementation and evaluation of the domain PROP. </title> <booktitle> In Proceedings of Symposium on Partial Evaluation and Semantics-based Program Manipulation, </booktitle> <year> 1993. </year>
Reference-contexts: Some performance measures are listed in Table 5.1. The ratio was defined in Section 3.9. The input programs are shown in Appendix D. The programs were tested on SUN4 sparc station. The domain PROP was implemented by B. Le Charlier and P. Van Hentenryck <ref> [LCVH93] </ref> and by M. Codish and B. Demoen [CD93]. Our current implementation can take programs in pure Prolog as input. We are working on lifting this limitation. It will be interesting to compare our performance results with their results.
Reference: [LHM89] <author> J.-L. Lassez, H. Huynh, and K. McAloon. </author> <title> Simplification and elimination of redundant linear arithmetic constraints. </title> <booktitle> In North American Conf. on Logic Programming, </booktitle> <pages> pages 37-51, </pages> <year> 1989. </year>
Reference-contexts: 6 6 6 6 6 6 u w fi 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 4 0 ffi ij 0 7 7 7 7 7 7 7 7 (3:8) This set of constraints is very amenable to reduction by Fourier-Motzkin elimination <ref> [Sch86, LHM89, Las90b] </ref>. In this technique a variable is eliminated by "cancelling" all positive occurrences with all negative occurrences, pairwise, creating new rows (with 0 in that variable's column). Then all rows containing a nonzero coefficient for that variable can be eliminated, preserving satisfiability (See Appendix A for details). <p> The cost of finding an implicit equality is equivalent to that of running a linear program. General techniques for redundancy elimination of linear constraints can be found in <ref> [KLTZ83, LHM89] </ref>. To verify a fixpoint of a recursive transformation T , we need to extract extreme points and rays from points and rays transformed from the constraint set.
Reference: [Llo84] <author> J. W. Lloyd. </author> <title> Foundations of Logic Programming. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: This chapter introduces the basic concepts of logic programming, which will in turn be used throughout the thesis. Since this chapter serves only a short introduction to logic programming enough to follow the thesis, readers may consult the book by Lloyd <ref> [Llo84] </ref> for details. 2.1 Logic Programs We first give an inductive definition of terms. Constants or variables are terms. <p> Starting with empty interpretation ;, T P arrives at a least fixpoint; however, depending on starting interpretations, there exist numerous fixpoints. The transformation may need infinite iterations before reaching a fixpoint. Details can be found in <ref> [Llo84, Hog90] </ref>. Basically, our algorithm to infer interargument constraints simulates the immediate consequence operator T P . 2.4 Deductive Databases One interpretation of logic is the database interpretation. Here a logic program is regarded as a database.
Reference: [Mel81] <author> C. S. Mellish. </author> <title> The automatic generation of mode declaration for logic programs. </title> <type> Technical Report DAI Research Paper 163, </type> <institution> Department of Artificial Intelligence, University of Edinburgh, </institution> <address> Scotland, </address> <year> 1981. </year>
Reference-contexts: Abstract interpretation has been used as standard means for dataflow analysis since it was placed on a solid semantic basis by Cousot and Cousot [CC77] (See [CC92] for the theory and applications to logic programming). Early work done by Mellish <ref> [Mel81] </ref> produced erroneous results since aliasing effects resulting from unification was not considered, which was corrected later [Mel87].
Reference: [Mel85] <author> C. S. Mellish. </author> <title> Some global optimizations for a prolog compiler. </title> <journal> Journal of Logic Programming, </journal> <volume> 2(1) </volume> <pages> 43-66, </pages> <year> 1985. </year>
Reference-contexts: One of the most attractive features of Prolog is bidirectional use of arguments for input, output, or both; however, in practical programs most procedures do not make this sophisticated use of arguments. Using mode information allows compilers to produce more specific code which results in substantial speedup <ref> [Mel85] </ref>. With this regard, Warren introduced explicit mode declarations to help the compiler produce better code [War77]. But mode declarations must be verified since wrong annotation may introduce subtle errors in program executions. Another approach is to infer mode declarations automatically via abstract interpretation [Mel87, BJCB87, MU87, DW88].
Reference: [Mel87] <author> C. S. Mellish. </author> <title> Abstract interpretation of Prolog programs. </title> <editor> In S. Abramsky and C. Hankin, editors, </editor> <booktitle> Abstract Interpretation of Declarative Languages, </booktitle> <pages> pages 181-198. </pages> <publisher> Ellis Horword, </publisher> <address> Chichester, U.K., </address> <year> 1987. </year> <month> 104 </month>
Reference-contexts: With this regard, Warren introduced explicit mode declarations to help the compiler produce better code [War77]. But mode declarations must be verified since wrong annotation may introduce subtle errors in program executions. Another approach is to infer mode declarations automatically via abstract interpretation <ref> [Mel87, BJCB87, MU87, DW88] </ref>. Abstract interpretation has been used as standard means for dataflow analysis since it was placed on a solid semantic basis by Cousot and Cousot [CC77] (See [CC92] for the theory and applications to logic programming). <p> Early work done by Mellish [Mel81] produced erroneous results since aliasing effects resulting from unification was not considered, which was corrected later <ref> [Mel87] </ref>. Mannila and Ukkonen [MU87] used simple two-valued abstract domains fground; anyg, hence f ree mode 1 cannot be inferred (ground; f ree; any are abstractions of ground terms, free variables, all (ground or nonground) terms, respectively). In addition, their method could not handle the problem with aliased variables accurately.
Reference: [MR80] <author> T. H. Matheiss and D. S. Rubin. </author> <title> A survey and comparison of methods for finding all vertices of convex polyhedral sets. </title> <journal> Mathematics of Operations Research, </journal> <volume> 5 </volume> <pages> 167-185, </pages> <year> 1980. </year>
Reference-contexts: As there is an algorithm to find all extreme points and rays of a polycone in <ref> [MR80, VG91] </ref>, we can effectively compute them (See Appendix B.) The cost of computing all extreme points and rays depends on the number of parameters and constraints.
Reference: [MS88] <author> K. Marriott and H. Stndergaard. </author> <title> Bottom-up abstract interpretation of logic programs. </title> <editor> In S. K. Debray and M. Hermenegildo, editors, </editor> <booktitle> Logic Programming: Proceedings of the Fifth International Conference, </booktitle> <pages> pages 733-748, </pages> <address> Cambridge, Massachusetts, 1988. </address> <publisher> MIT Press. </publisher>
Reference-contexts: We now give a bottom-up abstract interpretation to get success patterns in the form of groundness constraints. Abstract interpretation consists of abstract domain and abstract operations which mimics base semantics faithfully. Hence bottom-up abstract interpretation mimics fixpoint semantics. Formal framework of bottom-up abstract interpretation can be found in <ref> [CC92, MS88] </ref>. Now we define a transformation mimicking immediate consequence operator. 84 Definition 5.6: Suppose we have k predicates whose names are p; q; : : : ; r in an abstract program and p i is i-th clause having the head predicate p and so forth.
Reference: [MS89] <author> K. Marriott and H. Stndergaard. </author> <title> Notes for a tutorial on abstract interpretation of logic programs. </title> <booktitle> North American Conf. on Logic Programming, </booktitle> <year> 1989. </year>
Reference-contexts: Using positive propositional formula (called domain PROP) for groundness analysis has been studied by Marriott and Stndergaard <ref> [MS89] </ref>. The domain was further studied by Cortesi et al. [CFW91]. Our abstract domain is simpler than PROP since we only use positive proposition formula forming so-called a "boolean cone". For such formula, we can easily find a minimal generator set which is a unique minimal representation.
Reference: [MU87] <author> H. Mannila and E. Ukkonen. </author> <title> Flow analysis of Prolog programs. </title> <booktitle> In Proceedings of the 1987 International Symposium on Logic Programming. </booktitle> <publisher> IEEE Press, </publisher> <year> 1987. </year>
Reference-contexts: With this regard, Warren introduced explicit mode declarations to help the compiler produce better code [War77]. But mode declarations must be verified since wrong annotation may introduce subtle errors in program executions. Another approach is to infer mode declarations automatically via abstract interpretation <ref> [Mel87, BJCB87, MU87, DW88] </ref>. Abstract interpretation has been used as standard means for dataflow analysis since it was placed on a solid semantic basis by Cousot and Cousot [CC77] (See [CC92] for the theory and applications to logic programming). <p> Early work done by Mellish [Mel81] produced erroneous results since aliasing effects resulting from unification was not considered, which was corrected later [Mel87]. Mannila and Ukkonen <ref> [MU87] </ref> used simple two-valued abstract domains fground; anyg, hence f ree mode 1 cannot be inferred (ground; f ree; any are abstractions of ground terms, free variables, all (ground or nonground) terms, respectively). In addition, their method could not handle the problem with aliased variables accurately. <p> However their method produces less precise mode information since they use conservative local analysis in which all unsafe instantiations are replaced by any. In this chapter, we study relational abstract domains which describe possible groundness relationships among arguments. Like <ref> [MU87] </ref>, we only analyze groundness, however, our method provides great accuracy with respect to groundness. It is also noted that success of correctness proofs like program termination relies on preciseness of groundness information [UVG88, Plu90b, SVG91].
Reference: [MUVG86] <author> K. Morris, J. D. Ullman, and A. Van Gelder. </author> <title> Design overview of the Nail! system. </title> <booktitle> In Third Int'l Conf. on Logic Programming, </booktitle> <pages> pages 554-568, </pages> <year> 1986. </year>
Reference-contexts: One of main problems in this 3 regard is to generate suitable evaluation strategy. Capture rules were introduced to decide which evaluation strategy is efficiently applicable to a logic program provided with a goal <ref> [UVG85, Ull85, MUVG86] </ref>. A capture rule is a statement of the form: "if the rules satisfy such-and-such conditions, then a good evaluation method is such-and-such." A minimal requirement to apply top-down execution method is the guarantee of termination for any query of interest.
Reference: [Nai83] <author> L. Naish. </author> <title> Automatic generation of control for logic programs. </title> <type> Technical Report 83/6, </type> <institution> Dept. of Computer Science, University of Melbourne, </institution> <address> Melbourne, Australia, </address> <year> 1983. </year>
Reference-contexts: This proliferation is mainly motivated by the practical needs for termination analysis, such as in the area of control generation and program verification. A major concern in this case is automation of the termination proof process <ref> [Nai83, UVG88, APP + 89, BS89b, DSVB90, Plu90a, Plu90b, Sag91, SVG91] </ref>. Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues [AP90, AB91, Dev90]. <p> In order to show termination, we sometimes need to know the relationship among argument sizes of certain predicates. We describe several approaches developed in earlier work. Naish's method <ref> [Nai83] </ref> was based on the measure of "proper subterm", which gives a partial order on logical terms. <p> The final constraints represent a feasibility problem in linear programming. In practice, Fourier-Motzkin elimination is simple and adequate. 3.5 Multiple Bound Arguments In recursive procedures with several bound arguments, some previously developed methods require searching through subsets of bound arguments and/or paths in the dependency graph <ref> [Nai83, Plu90a] </ref>. One of our main motivations in using linear techniques was to short-circuit this complicated and expensive task. Our method seeks a nonnegative linear combination of the bound arguments that suffices for all cases. Example 3.9: The following procedure merges two input lists [VG91].
Reference: [Plu90a] <author> L. Plumer. </author> <title> Termination Proofs for Logic Programs, </title> <booktitle> volume 446 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: This proliferation is mainly motivated by the practical needs for termination analysis, such as in the area of control generation and program verification. A major concern in this case is automation of the termination proof process <ref> [Nai83, UVG88, APP + 89, BS89b, DSVB90, Plu90a, Plu90b, Sag91, SVG91] </ref>. Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues [AP90, AB91, Dev90]. <p> Ullman and Van Gelder were the first who studied termination analysis in deductive database environments with a strong emphasis on automation of the analysis [UVG88]. Later, the method was enhanced by Plumer <ref> [Plu90a, Plu90b] </ref> and Van Gelder and Sohn [SVG91]. Brodsky and Sagiv, who studied termination of Datalog programs, were also main contributors along this line [BS89b, Sag91]. 1.2 How It Works In this section, we shall describe the basic structure of termination proof for logic programs. <p> They recently proposed a method to infer inequality constraints between two arguments in programs with function symbols [BS91]. Plumer described some extensions for rules that did not satisfy the "uniqueness" property <ref> [Plu90a, Plu90b] </ref>. His main contribution was that certain constraints involving more than two argument positions could be used. However, he required an "admissibility" property on the rules. Moreover, mutual recursion presented problems for his method. <p> Let be the vector (P; X; L; E; F; P 1) T . We have x 1 = P , so a and A are: a = 0 A = 1 0 0 0 0 0 3 This example was studied by Plumer <ref> [Plu90a] </ref>. perm bf cannot be shown to terminate by any of the previous methods cited. <p> The final constraints represent a feasibility problem in linear programming. In practice, Fourier-Motzkin elimination is simple and adequate. 3.5 Multiple Bound Arguments In recursive procedures with several bound arguments, some previously developed methods require searching through subsets of bound arguments and/or paths in the dependency graph <ref> [Nai83, Plu90a] </ref>. One of our main motivations in using linear techniques was to short-circuit this complicated and expensive task. Our method seeks a nonnegative linear combination of the bound arguments that suffices for all cases. Example 3.9: The following procedure merges two input lists [VG91]. <p> We suppose no knowledge about z. 44 t J J J J J JJ] &% - '$ &% ? ffi ne ffi et ffi tt ffi nn ff n ff t This example was studied by Plumer <ref> [Plu90a] </ref>, who eliminated mutual recursion by pushing e, n, and t into the third argument of a new predicate parse (L, T, P). Intuitively, such a syntactic change should not make a substantial difference. He had to make further ad hoc assumptions to handle parse.
Reference: [Plu90b] <author> L. Plumer. </author> <title> Termination proofs for logic programs based on predicate inequalities. </title> <booktitle> In Proc. 7th Int'l Conf. on Logic Programming, </booktitle> <pages> pages 634-648, </pages> <address> Jerusalem, </address> <year> 1990. </year>
Reference-contexts: This proliferation is mainly motivated by the practical needs for termination analysis, such as in the area of control generation and program verification. A major concern in this case is automation of the termination proof process <ref> [Nai83, UVG88, APP + 89, BS89b, DSVB90, Plu90a, Plu90b, Sag91, SVG91] </ref>. Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues [AP90, AB91, Dev90]. <p> Ullman and Van Gelder were the first who studied termination analysis in deductive database environments with a strong emphasis on automation of the analysis [UVG88]. Later, the method was enhanced by Plumer <ref> [Plu90a, Plu90b] </ref> and Van Gelder and Sohn [SVG91]. Brodsky and Sagiv, who studied termination of Datalog programs, were also main contributors along this line [BS89b, Sag91]. 1.2 How It Works In this section, we shall describe the basic structure of termination proof for logic programs. <p> They recently proposed a method to infer inequality constraints between two arguments in programs with function symbols [BS91]. Plumer described some extensions for rules that did not satisfy the "uniqueness" property <ref> [Plu90a, Plu90b] </ref>. His main contribution was that certain constraints involving more than two argument positions could be used. However, he required an "admissibility" property on the rules. Moreover, mutual recursion presented problems for his method. <p> In this chapter, we study relational abstract domains which describe possible groundness relationships among arguments. Like [MU87], we only analyze groundness, however, our method provides great accuracy with respect to groundness. It is also noted that success of correctness proofs like program termination relies on preciseness of groundness information <ref> [UVG88, Plu90b, SVG91] </ref>. Let us consider a call p (A, f (A,B)) in which we have no instantiation information on A and B. Hence the goal is abstracted as p (any; any) in the previous cited methods.
Reference: [PS82] <author> C. H. Papadimitriou and K. Steiglitz. </author> <title> Combinatorial Optimization. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: Letting fl be the minimum of the objective function, the implication Eq. 3.3 holds if and only if fl ffi ij . Now consider the dual of the above minimization problem <ref> [PS82, Sch86] </ref>. We use for dual variables u with the same arity as x, v with the same arity as y, and w with the same arity as c. <p> The question is how we can identify extreme rays. The simplex algorithm of linear programming can be modified to find the extreme points and directions of a polycone. Background on this algorithm can be found in Papadimitriou and Steiglitz <ref> [PS82, Ch. 2] </ref>, and elsewhere; we review the essentials briefly. Assume we have a linear programming problem in a standard form, except for the objective function (which may be treated as 0).
Reference: [Roc70] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1970. </year>
Reference-contexts: If gen () = (f ~x 1 ; : : :; ~x n g; f ~y 1 ; : : : ; ~y m g), then aff () &gt; &lt; ~p = i ~x i + j ~y j i = 1 See <ref> [Sch86, Roc70] </ref> for details on polyhedral convex sets. Substitution [x 1 =e 1 ; : : : ; x n =e n ] is defined in an usual way; that is, a variable x i is replaced by a linear arithmetic term e i .
Reference: [Sag91] <author> Y. Sagiv. </author> <title> A termination test for logic programs. </title> <booktitle> In Proceedings of ILPS'91, </booktitle> <pages> pages 160-171, </pages> <address> San Diego, 1991. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This proliferation is mainly motivated by the practical needs for termination analysis, such as in the area of control generation and program verification. A major concern in this case is automation of the termination proof process <ref> [Nai83, UVG88, APP + 89, BS89b, DSVB90, Plu90a, Plu90b, Sag91, SVG91] </ref>. Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues [AP90, AB91, Dev90]. <p> Later, the method was enhanced by Plumer [Plu90a, Plu90b] and Van Gelder and Sohn [SVG91]. Brodsky and Sagiv, who studied termination of Datalog programs, were also main contributors along this line <ref> [BS89b, Sag91] </ref>. 1.2 How It Works In this section, we shall describe the basic structure of termination proof for logic programs. Every termination proof is tantamount to showing the well-foundedness (no infinitely decreasing sequence) of a computation path. <p> The goal r (X, f (f (X))) fails to unify with the head r (f (X), f (X)) of r 8 with occur-check. Sagiv's method can detect termination of such situation <ref> [Sag91] </ref> 4 . 4 This example was given by Sagiv in personal communication 48 Usually, the problem regarding occur-check at one step of resolution can be resolved by predicate splitting. For example, consider a recursive rule: p (X, X) :- p (X, f (X)).
Reference: [Sch86] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Letting fl be the minimum of the objective function, the implication Eq. 3.3 holds if and only if fl ffi ij . Now consider the dual of the above minimization problem <ref> [PS82, Sch86] </ref>. We use for dual variables u with the same arity as x, v with the same arity as y, and w with the same arity as c. <p> 6 6 6 6 6 6 u w fi 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 4 0 ffi ij 0 7 7 7 7 7 7 7 7 (3:8) This set of constraints is very amenable to reduction by Fourier-Motzkin elimination <ref> [Sch86, LHM89, Las90b] </ref>. In this technique a variable is eliminated by "cancelling" all positive occurrences with all negative occurrences, pairwise, creating new rows (with 0 in that variable's column). Then all rows containing a nonzero coefficient for that variable can be eliminated, preserving satisfiability (See Appendix A for details). <p> If gen () = (f ~x 1 ; : : :; ~x n g; f ~y 1 ; : : : ; ~y m g), then aff () &gt; &lt; ~p = i ~x i + j ~y j i = 1 See <ref> [Sch86, Roc70] </ref> for details on polyhedral convex sets. Substitution [x 1 =e 1 ; : : : ; x n =e n ] is defined in an usual way; that is, a variable x i is replaced by a linear arithmetic term e i . <p> Solvability of a linear system can be tested theoretically in polynomial time by Khachiyan's ellipsoid method or Karmarkar's method <ref> [Sch86] </ref>. It also can be tested by exponential time algorithm such as Fourier-Motzkin elimination or the first phase of simplex method. Those polynomial time algorithms have been reported to be impractical unless the input size is substantially large. The exponential-time algorithm will work if we can maintain small input sizes.
Reference: [SS86] <author> L. Sterling and E. Shapiro. </author> <title> The Art of Prolog. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Technically, an empty clause, that is, a clause with empty head and empty body is denoted . This clause is understood as a contradiction. In examples, we shall use standard Edinburgh-style Prolog syntax <ref> [CM81, SS86] </ref>; Variables are denoted by a character string starting with uppercase letters while constants, function symbols, and predicate symbols are denoted by a character string starting with lower case letters. The syntax [H|T] (equivalent to `.'(H,T)) denotes a list whose head is H and tail is T.
Reference: [SU84] <author> Y. Sagiv and J. D. Ullman. </author> <title> Complexity of a top-down capture rule. </title> <type> Technical Report STAN-CS-84-1009, </type> <institution> Stanford University, </institution> <year> 1984. </year>
Reference-contexts: He gave an algorithm determining whether some subset of 32 the bound arguments of each predicate existed such that each recursive call was guaranteed to reduce one or more elements of the subset without changing others. This exponential time algorithm was made semi-polynomial by Sagiv and Ullman <ref> [SU84] </ref>. Ullman and Van Gelder [UVG88] proposed a method to test top-down termination with a view of testing the applicability of top-down capture rules in a knowledge-base system.
Reference: [SVG91] <author> K. Sohn and A. Van Gelder. </author> <title> Termination detection in logic programs using argument sizes. </title> <booktitle> In Tenth ACM Symposium on Principles of Database Systems, </booktitle> <year> 1991. </year>
Reference-contexts: This proliferation is mainly motivated by the practical needs for termination analysis, such as in the area of control generation and program verification. A major concern in this case is automation of the termination proof process <ref> [Nai83, UVG88, APP + 89, BS89b, DSVB90, Plu90a, Plu90b, Sag91, SVG91] </ref>. Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues [AP90, AB91, Dev90]. <p> Ullman and Van Gelder were the first who studied termination analysis in deductive database environments with a strong emphasis on automation of the analysis [UVG88]. Later, the method was enhanced by Plumer [Plu90a, Plu90b] and Van Gelder and Sohn <ref> [SVG91] </ref>. Brodsky and Sagiv, who studied termination of Datalog programs, were also main contributors along this line [BS89b, Sag91]. 1.2 How It Works In this section, we shall describe the basic structure of termination proof for logic programs. <p> Section 4.6 summaries the chapter. 4.2 Basic Concepts Logic programs can be abstracted by the size of terms. Our method does not depend on any specific size definition. In examples we shall use listsize [UVG88] and structural term size <ref> [VG91, SVG91] </ref>. List size and structural term size are defined informally to be the number of edges in the rightmost path and the number of edges, resp., in the tree that represents the ground term. <p> In this chapter, we study relational abstract domains which describe possible groundness relationships among arguments. Like [MU87], we only analyze groundness, however, our method provides great accuracy with respect to groundness. It is also noted that success of correctness proofs like program termination relies on preciseness of groundness information <ref> [UVG88, Plu90b, SVG91] </ref>. Let us consider a call p (A, f (A,B)) in which we have no instantiation information on A and B. Hence the goal is abstracted as p (any; any) in the previous cited methods.
Reference: [Ull85] <author> J. D. Ullman. </author> <title> Implementation of logical query languages for databases. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 10(3) </volume> <pages> 289-321, </pages> <year> 1985. </year>
Reference-contexts: One of main problems in this 3 regard is to generate suitable evaluation strategy. Capture rules were introduced to decide which evaluation strategy is efficiently applicable to a logic program provided with a goal <ref> [UVG85, Ull85, MUVG86] </ref>. A capture rule is a statement of the form: "if the rules satisfy such-and-such conditions, then a good evaluation method is such-and-such." A minimal requirement to apply top-down execution method is the guarantee of termination for any query of interest.
Reference: [Ull89] <author> J. D. Ullman. </author> <title> Principles of Database and Knowledge-base Systems. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1989. </year>
Reference-contexts: Therefore we need to indicate which arguments are used as input. Since there is no precise dichotomy of input/output for arguments, bound arguments (containing no variables) can be viewed as input. We shall use the same notations as in <ref> [Ull89] </ref> for bound-free adornments and rule-goal graphs in the following section.
Reference: [UV88] <author> J. D. Ullman and M. Y. Vardi. </author> <title> The complexity of ordering subgoals. </title> <booktitle> In Proceedings of Principles of Database Systems, </booktitle> <pages> pages 74-81, </pages> <year> 1988. </year>
Reference-contexts: This assumption is not so realistic. The construction may be imprecise since we do not consider the problems with respect to aliased variables. These problems can be resolved by the groundness analysis presented in Chapter 5. The complexity of rule-goal graphs was studied in <ref> [UV88] </ref>.
Reference: [UVG85] <author> J. D. Ullman and A. Van Gelder. </author> <title> Testing applicability of top-down capture rules. </title> <type> Technical Report STAN-CS-85-1046, </type> <institution> Dept. of Computer Science, Stanford University, Stanford, </institution> <address> CA, </address> <month> April </month> <year> 1985. </year> <month> 105 </month>
Reference-contexts: One of main problems in this 3 regard is to generate suitable evaluation strategy. Capture rules were introduced to decide which evaluation strategy is efficiently applicable to a logic program provided with a goal <ref> [UVG85, Ull85, MUVG86] </ref>. A capture rule is a statement of the form: "if the rules satisfy such-and-such conditions, then a good evaluation method is such-and-such." A minimal requirement to apply top-down execution method is the guarantee of termination for any query of interest.
Reference: [UVG88] <author> J. D. Ullman and A. Van Gelder. </author> <title> Efficient tests for top-down termination of logical rules. </title> <journal> Journal of the ACM, </journal> <volume> 35(2) </volume> <pages> 345-373, </pages> <year> 1988. </year>
Reference-contexts: This proliferation is mainly motivated by the practical needs for termination analysis, such as in the area of control generation and program verification. A major concern in this case is automation of the termination proof process <ref> [Nai83, UVG88, APP + 89, BS89b, DSVB90, Plu90a, Plu90b, Sag91, SVG91] </ref>. Another approach concerns the characterization of terminating logic programs. It aims at the treatment of negation as finite failure or the better understanding of decidability issues [AP90, AB91, Dev90]. <p> Ullman and Van Gelder were the first who studied termination analysis in deductive database environments with a strong emphasis on automation of the analysis <ref> [UVG88] </ref>. Later, the method was enhanced by Plumer [Plu90a, Plu90b] and Van Gelder and Sohn [SVG91]. <p> In this section we describe size abstractions of terms to give ordering on ground terms. Size abstraction is a mapping from terms to natural numbers. Ullman and Van Gelder <ref> [UVG88] </ref> were the first to introduce an abstraction function for terms, motivated by the fact that subterm ordering is incapable of handling problems due to local variables. They used listsize abstraction as a basis for the well-founded ordering in a top-down termination analysis, which is defined in the following. <p> So it fails to prove termination of traverse b . In spite of this shortcoming shown in the above example, the basic reason why <ref> [UVG88] </ref> used listsize is that their inference algorithm for interargument constraints can only handle the relationship between two logical variables, each variable representing a listsize of an argument. This restriction was lifted by Plumer who used a "linear norm" as abstraction function. <p> This exponential time algorithm was made semi-polynomial by Sagiv and Ullman [SU84]. Ullman and Van Gelder <ref> [UVG88] </ref> proposed a method to test top-down termination with a view of testing the applicability of top-down capture rules in a knowledge-base system. They introduced a measure of terms called "list size", which corresponds to the number of edges in rightmost path in a tree representation of terms. <p> So we can conclude that termsize ([X,Y|Rs]) &gt; termsize (Us) and termsize ([X,Y|Rs]) &gt; termsize (Vs). This proves termination of mergesort bf . However, it is noticed many published methods to infer disjunctive constraints are practically unusable owing to their higher complexity. The other approach, introduced by <ref> [UVG88] </ref>, is to apply predicate splitting so that termination behavior is better exposed. It is simple to apply the technique and adequate to resolve the problem. The first subgoal split in m 3 does not unify with s 1 . <p> Section 4.6 summaries the chapter. 4.2 Basic Concepts Logic programs can be abstracted by the size of terms. Our method does not depend on any specific size definition. In examples we shall use listsize <ref> [UVG88] </ref> and structural term size [VG91, SVG91]. List size and structural term size are defined informally to be the number of edges in the rightmost path and the number of edges, resp., in the tree that represents the ground term. <p> In this chapter, we study relational abstract domains which describe possible groundness relationships among arguments. Like [MU87], we only analyze groundness, however, our method provides great accuracy with respect to groundness. It is also noted that success of correctness proofs like program termination relies on preciseness of groundness information <ref> [UVG88, Plu90b, SVG91] </ref>. Let us consider a call p (A, f (A,B)) in which we have no instantiation information on A and B. Hence the goal is abstracted as p (any; any) in the previous cited methods.
Reference: [VG91] <author> A. Van Gelder. </author> <title> Deriving constraints among argument sizes in logic programs. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 3, </volume> <booktitle> 1991. Extended abstract appears in Ninth ACM Symposium on Principles of Database Systems, </booktitle> <year> 1990. </year>
Reference-contexts: In Chapter 4, we will describe how to derive those constraints, so-called "interargument constraints" of a predicate. Interargument constraints are essentially a set of constraints which every derivable fact with respect to a predicate satisfies. Research on this topic has recently been studied as a separate task <ref> [BS89a, VG91, BS91] </ref>. In this thesis, the argument sizes of derivable facts with respect to an n-ary predicate are viewed as a set of points in R n , which are approximated by their convex hull. <p> His algorithm was exponential-time, since he had to guess one inequality per predicate and the number of possible guesses was exponential in the number of arguments in a predicate 2 . Van Gelder <ref> [VG91] </ref> investigated the problem of deriving constraints among argument sizes. His method was aimed at finding all constraints in the form of polyhedral convex cones. Thus those constraints can be viewed at least as inequalities among linear combinations of any number of argument positions. <p> The constraints are obtained by the method in Chapter 4 (or supplied by other external means such as <ref> [VG91, BS91] </ref>). Example 3.7: Consider the adorned logic program perm 3 : r 1 : perm bf ([], []). r 2 : perm bf (P, [X|L]) :- append ffb (E, [X|F], P), append bbf (E, F, P1), perm bf (P1, L). <p> One of our main motivations in using linear techniques was to short-circuit this complicated and expensive task. Our method seeks a nonnegative linear combination of the bound arguments that suffices for all cases. Example 3.9: The following procedure merges two input lists <ref> [VG91] </ref>. <p> It is not immediately obvious what these constraints are, but they can be found by the Van Gelder's methods <ref> [VG91] </ref>, and are: 0 = 2 t 1 + t 2 + that is, t 1 2 + t 2 ( is a "slack variable"). Let be the vector (L; T; C; ) T . <p> Methods to derive ICs have been studied recently in terms of Datalog [BS89a, BS91] or logical rules with function symbols <ref> [VG91] </ref>. In their methods IC is formalized by the least fixpoint of bottom-up inference operator similar to "immediate consequence operator". In [BS91] ICs are captured by a disjunctive union of inequalities between two argument sizes. <p> Van Gelder studied a method to derive a single conjunctive set of constraints by taking the convex union of disjunctive sets of constraints as an input for bottom-up inference <ref> [VG91] </ref>. It is often the case that both methods fail to finitely converge. In this chapter, IC with respect to n-ary predicate is captured by a single polyhedral convex set in the nonnegative orthant of R n , called a polycone. <p> In this chapter, IC with respect to n-ary predicate is captured by a single polyhedral convex set in the nonnegative orthant of R n , called a polycone. We formalize the derivation of ICs in a way similar to <ref> [VG91] </ref>. This method is an instance of bottom-up abstract interpretation based on fixpoint semantics, so correctness is guaranteed. We introduce two new techniques to capture ICs in finite time, using affine widening and translativeness property. <p> Example 4.2: With a simple append procedure, we compare our results with others. Sagiv and Brodsky's method [BS91] cannot capture the IC we derive here since their IC is in the form of inequalities between two argument positions. Van Gelder's method <ref> [VG91] </ref> does not converge with this example; his heuristic that "sometimes" works gives the same IC. However, we provide an affine widening that always works. <p> Section 4.6 summaries the chapter. 4.2 Basic Concepts Logic programs can be abstracted by the size of terms. Our method does not depend on any specific size definition. In examples we shall use listsize [UVG88] and structural term size <ref> [VG91, SVG91] </ref>. List size and structural term size are defined informally to be the number of edges in the rightmost path and the number of edges, resp., in the tree that represents the ground term. <p> T P () = (T p 1 (); : : :; T p k ()) (4:8) Natural transformation and recursive transformation are similar to those in <ref> [VG91] </ref>, but more general. Theorem 4.1: A recursive transformation T P is monotone. Proof : The operations involved in T P are essentially intersection, projection, and convex union of polycones. These operations preserve monotonicity. Theorem 4.2: There exists the least fixpoint associated with T P . <p> It is noted that Van Gelder's recursive transformation does not converge with this example and his heuristic does not work <ref> [VG91] </ref>. Sagiv and Brodsky's method can infer only inequalities between two argument positions [BS91]. <p> As there is an algorithm to find all extreme points and rays of a polycone in <ref> [MR80, VG91] </ref>, we can effectively compute them (See Appendix B.) The cost of computing all extreme points and rays depends on the number of parameters and constraints. <p> Section 5.4 presents bottom-up groundness analysis mimicking immediate consequence operator. Section 5.5 summarizes the chapter. The framework similar to our relational bottom-up analysis has been investigated on real arithmetic domain to derive constraints among argument sizes by Van Gelder <ref> [VG91] </ref>. 5.2 Basic Concepts 5.2.1 Abstraction 77 We now describe abstraction of terms, atoms, and clauses. Logic programs can be abstracted by groundness relationships among terms. <p> Contradiction. As there are well-known methods to find extreme rays of convex cones (same as finding extreme points of convex polytopes) <ref> [VG91, Las90a] </ref>, we can find candidates for generators for the corresponding boolean cones by removing redundancy. 5.4 Bottom-Up Groundness Analysis In the preceding sections, we described the relational abstract domain and some useful operations on the domain. <p> DFS (new neighboring basis, new tableau). Figure B.1: Algorithm to find all extreme points and rays R j = 1. R B (i) = i-th component of A j . The reasoning is we can move from P along R indefinitely. More details can be found in <ref> [VG91, CH78] </ref>. 94 6 x 2 6 t t p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p p
Reference: [War77] <author> D. H. D. Warren. </author> <title> Implementing prolog compiling predicate logic programs. </title> <note> Technical Report DAI Research Paper 39 and 40, </note> <institution> Department of Artificial Intelligence, University of Edinburgh, </institution> <address> Scotland, </address> <year> 1977. </year>
Reference-contexts: Using mode information allows compilers to produce more specific code which results in substantial speedup [Mel85]. With this regard, Warren introduced explicit mode declarations to help the compiler produce better code <ref> [War77] </ref>. But mode declarations must be verified since wrong annotation may introduce subtle errors in program executions. Another approach is to infer mode declarations automatically via abstract interpretation [Mel87, BJCB87, MU87, DW88].
References-found: 53

