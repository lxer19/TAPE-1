URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/umsi-98-7.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/
Root-URL: http://www.cs.umn.edu
Title: Diagonal Threshold Techniques in Robust Multi-Level ILU Preconditioners for General Sparse Linear Systems  
Author: Yousef Saad and Jun Zhang 
Keyword: Key words: Incomplete LU factorization, reordering techniques, multi-level preconditioner, Krylov subspace methods, multi-elimination ILU factorization.  
Note: AMS subject classifications: 65F10, 65N06.  
Date: January 22, 1998  
Address: 4-192 EE/CS Building, 200 Union Street S.E., Minneapolis, MN 55455  
Affiliation: Department of Computer Science and Engineering, University of Minnesota,  
Abstract: This paper introduces techniques based on diagonal threshold tolerance when developing multi-elimination and multi-level incomplete LU (ILUM) factorization precondi-tioners for solving general sparse linear systems. Existing heuristics solely based on the adjacency graph of the matrices have been used to find independent sets and are not robust for matrices arising from certain applications in which the matrices may have small or zero diagonals. New heuristic strategies based on the adjacency graph and the diagonal values of the matrices for finding independent sets are introduced. Analytical bounds for the factorization and preconditioned errors are obtained for the case of a two-level analysis. These bounds provide useful information in designing robust ILUM preconditioners. Extensive numerical experiments are conducted in order to compare robustness and efficiency of various heuristic strategies.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> O. Axelsson and P. S. Vassilevski, </author> <title> A survey of multilevel preconditioned iterative methods, </title> <type> BIT 29 (4), </type> <month> 769-793 </month> <year> (1989). </year>
Reference-contexts: Some of these approaches require grid information. Examples of such approaches include the nested recursive two-level factorization and the repeated red-black orderings [2, 6, 9, 22, 33] (see also the survey paper by Axelsson and Vassilevski <ref> [1] </ref>). Other methods require only the adjacency graph of the coefficient matrices [5, 29, 32]. For the repeated red-black ordering approach, a near-optimal bound O (n 0:153 ) for the condition number of the preconditioned matrix has been obtained [21].
Reference: [2] <author> O. Axelsson and V. Eijkhout, </author> <title> The nested recursive two level factorization for nine-point difference matrices, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 12, </volume> <month> 1373-1400 </month> <year> (1991). </year>
Reference-contexts: Some of these approaches require grid information. Examples of such approaches include the nested recursive two-level factorization and the repeated red-black orderings <ref> [2, 6, 9, 22, 33] </ref> (see also the survey paper by Axelsson and Vassilevski [1]). Other methods require only the adjacency graph of the coefficient matrices [5, 29, 32].
Reference: [3] <author> R. E. Bank and J. Xu, </author> <title> The hierarchical basis multigrid method and incomplete LU decomposition, in Seventh International Symposium on Domain Decomposition Methods for Partial Differential Equations, </title> <editor> D. Keyes and J. Xu, eds., </editor> <publisher> AMS, Providence, RI, </publisher> <pages> pp. </pages> <month> 163-173 </month> <year> (1994). </year>
Reference-contexts: There are other algebraic multigrid methods [11, 24, 25, 26] and multigrid methods with matrix dependent treatments [15, 23], as well as multi-level preconditioning techniques based on hierarchical basis or ILU decomposition associated with the finite difference or finite element analysis <ref> [3, 4, 10] </ref>. The ILUM preconditioning technique has been extended to block version (BILUM) in which the pivoting elements are not diagonals, but small block diagonals [8, 32]. For some hard-to-solve problems, the performance of ILUM may be enhanced by the block treatment.
Reference: [4] <author> R. E. Bank and C. Wagner, </author> <title> Multilevel ILU decomposition, </title> <type> Technical Report, </type> <institution> Depart--ment of Mathematics, University of California at San Diego, La Jolla, </institution> <address> CA, </address> <year> 1997. </year>
Reference-contexts: There are other algebraic multigrid methods [11, 24, 25, 26] and multigrid methods with matrix dependent treatments [15, 23], as well as multi-level preconditioning techniques based on hierarchical basis or ILU decomposition associated with the finite difference or finite element analysis <ref> [3, 4, 10] </ref>. The ILUM preconditioning technique has been extended to block version (BILUM) in which the pivoting elements are not diagonals, but small block diagonals [8, 32]. For some hard-to-solve problems, the performance of ILUM may be enhanced by the block treatment.
Reference: [5] <author> E. F. F. Botta, A. van der Ploeg and F. W. Wubs, </author> <title> A fast linear-system solver for large unstructured problems on a shared-memory computer, in Proceedings of the Conference on Algebraic Multilevel Methods with Applications, </title> <editor> O. Axelsson and B. Polman, </editor> <booktitle> eds., </booktitle> <pages> pp. </pages> <month> 105-116 </month> <year> (1996). </year>
Reference-contexts: The ILUM preconditioner has a multi-level structure and offers a good degree of parallelism. Similar preconditioners have been designed and tested in <ref> [5, 32] </ref> to show near grid-independent convergence for certain problems. In a recent report, some of these multi-level preconditioners have been tested and compared favorably with other preconditioned iterative methods and direct methods at least for the Laplace equation [7]. Some of these approaches require grid information. <p> Some of these approaches require grid information. Examples of such approaches include the nested recursive two-level factorization and the repeated red-black orderings [2, 6, 9, 22, 33] (see also the survey paper by Axelsson and Vassilevski [1]). Other methods require only the adjacency graph of the coefficient matrices <ref> [5, 29, 32] </ref>. For the repeated red-black ordering approach, a near-optimal bound O (n 0:153 ) for the condition number of the preconditioned matrix has been obtained [21].
Reference: [6] <author> E. F. F. Botta, A. van der Ploeg, and F. W. Wubs, </author> <title> Nested grids ILU-decomposition (NGILU), </title> <journal> J. Comput. Appl. Math. </journal> <volume> 66, </volume> <month> 515-526 </month> <year> (1996). </year>
Reference-contexts: Some of these approaches require grid information. Examples of such approaches include the nested recursive two-level factorization and the repeated red-black orderings <ref> [2, 6, 9, 22, 33] </ref> (see also the survey paper by Axelsson and Vassilevski [1]). Other methods require only the adjacency graph of the coefficient matrices [5, 29, 32].
Reference: [7] <author> E. F. F. Botta, K. Dekker, Y. Notay, A. van der Ploeg, C. Vuik, F. W. Wubs, and P. M. de Zeeuw, </author> <title> How fast the Laplace equation was solved in 1995, </title> <journal> Appl. Numer. Math. </journal> <volume> 24, </volume> <month> 439-455 </month> <year> (1997). </year>
Reference-contexts: Similar preconditioners have been designed and tested in [5, 32] to show near grid-independent convergence for certain problems. In a recent report, some of these multi-level preconditioners have been tested and compared favorably with other preconditioned iterative methods and direct methods at least for the Laplace equation <ref> [7] </ref>. Some of these approaches require grid information. Examples of such approaches include the nested recursive two-level factorization and the repeated red-black orderings [2, 6, 9, 22, 33] (see also the survey paper by Axelsson and Vassilevski [1]).
Reference: [8] <author> E. F. F. Botta and F. W. Wubs, MRILU: </author> <title> it's the preconditioning that counts, </title> <type> Technical Report W-9703, </type> <institution> Department of Mathematics, University of Groningen, The Nether-lands, </institution> <year> 1997. </year>
Reference-contexts: The ILUM preconditioning technique has been extended to block version (BILUM) in which the pivoting elements are not diagonals, but small block diagonals <ref> [8, 32] </ref>. For some hard-to-solve problems, the performance of ILUM may be enhanced by the block treatment. The heuristic algorithms introduced in [29] to find independent sets ignore the values of the matrix. <p> This seems to suggest that we need to reduce " as the number of levels grows. It has also been suggested to decrease the dropping 5 tolerance t for the coarser levels <ref> [8] </ref>. For general matrices, the magnitudes of the diagonal entries may increase, decrease, or remain the same during the reduction process and so it is preferable to keep " constant unless dropping tolerance t changes.
Reference: [9] <author> C. I. W. Brand, </author> <title> An incomplete-factorization preconditioning using red-black ordering, </title> <journal> Numer. Math. </journal> <volume> 61, </volume> <month> 433-454 </month> <year> (1992). </year>
Reference-contexts: Some of these approaches require grid information. Examples of such approaches include the nested recursive two-level factorization and the repeated red-black orderings <ref> [2, 6, 9, 22, 33] </ref> (see also the survey paper by Axelsson and Vassilevski [1]). Other methods require only the adjacency graph of the coefficient matrices [5, 29, 32].
Reference: [10] <author> T. F. Chan, S. Go, and J. Zou, </author> <title> Multilevel domain decomposition and multigrid methods for unstructured meshes: algorithms and theory, </title> <type> Technical Report 95-24, </type> <institution> Department of Mathematics, University of California at Los Angeles, </institution> <address> Los Angeles, CA, </address> <year> 1995. </year>
Reference-contexts: There are other algebraic multigrid methods [11, 24, 25, 26] and multigrid methods with matrix dependent treatments [15, 23], as well as multi-level preconditioning techniques based on hierarchical basis or ILU decomposition associated with the finite difference or finite element analysis <ref> [3, 4, 10] </ref>. The ILUM preconditioning technique has been extended to block version (BILUM) in which the pivoting elements are not diagonals, but small block diagonals [8, 32]. For some hard-to-solve problems, the performance of ILUM may be enhanced by the block treatment.
Reference: [11] <author> Q. Chang, Y. S. Wong, and H. Fu, </author> <title> On the algebraic multigrid method, </title> <journal> J. Comput. Phys. </journal> <volume> 125, </volume> <month> 279-292 </month> <year> (1996). </year>
Reference-contexts: Other methods require only the adjacency graph of the coefficient matrices [5, 29, 32]. For the repeated red-black ordering approach, a near-optimal bound O (n 0:153 ) for the condition number of the preconditioned matrix has been obtained [21]. There are other algebraic multigrid methods <ref> [11, 24, 25, 26] </ref> and multigrid methods with matrix dependent treatments [15, 23], as well as multi-level preconditioning techniques based on hierarchical basis or ILU decomposition associated with the finite difference or finite element analysis [3, 4, 10].
Reference: [12] <author> A. Chapman, Y. Saad, and L. Wigton, </author> <title> High order ILU preconditioners for CFD problems, </title> <type> Technique Report UMSI 96/14, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1996. </year>
Reference-contexts: These matrices were supplied by H. Simon and other researchers and are generally larger than other sets of matrices. They are all from applications in computational fluid dynamics and have been used as test matrices for high order ILU preconditioners in <ref> [12, 31] </ref>. Table 4 gives a brief description of these matrices. Table 5 lists the number of iterations. Since some of these matrices are very large, we had to adjust some parameters in ILUM and ILUT in order to solve them. These adjustments are also indicated in Table 5.
Reference: [13] <author> E. Chow and Y. Saad, </author> <title> Approximate inverse techniques for block-partitioned matrices, </title> <journal> SIAM J. Sci. Comput. </journal> <volume> 18, </volume> <month> 1657-1675 </month> <year> (1997). </year>
Reference-contexts: In some situations, it is beneficial to scale both columns and rows before constructing the preconditioner <ref> [13] </ref>. However, we did not resort to scaling or permuting for any matrix before computing the preconditioner. <p> The examples tested model the incompressible Navier-Stokes equations. The right bottom sub-block of these matrices may be a zero block <ref> [13] </ref> and many of these matrices have small or zero diagonals and are very hard to solve with standard ILU preconditioners. Table 1 is a simple description of the FIDAP matrices 1 with n being the dimension of the matrix, nz its number of non-zero elements. <p> In both cases, the single-level ILUT performed similarly to ILUM without diagonal threshold tolerance. 1 Matrices available online from the MatrixMarket (http://math.nist.gov/MatrixMarket). Some values for nz in Table 1 are larger than those listed in Table 4.2 of <ref> [13] </ref> and those of the same set in the MatrixMarket because all diagonal elements of the right bottom sub-blocks are kept even though their actual values may be numerically zero. 11 Matrix n nz Matrix n nz EX01 216 4 352 EX04 1 601 32 299 EX21 656 19 144 EX22
Reference: [14] <author> E. Chow and Y. Saad, </author> <title> Experimental study of ILU preconditioners for indefinite matrices, </title> <journal> J. Comput. Appl. Math. </journal> <volume> 86 (2), </volume> <month> 387-414 </month> <year> (1997). </year>
Reference-contexts: The main trade-offs when comparing preconditioners are in their intrinsic efficiency, their parallelism, and their robustness. An experimental study on robustness of some of these general-purpose preconditioners has been conducted in <ref> [14] </ref>. For an introduction to Krylov subspace methods and various preconditioning techniques, see [30]. The `multi-elimination ILU preconditioner' (ILUM), introduced in [29], is based on exploiting the idea of successive independent set orderings and several heuristic algorithms have been suggested to find the independent set. <p> As a result, the inverse of the diagonal matrix with such diagonal entries will contain very large elements and this may cause the resulting preconditioner to be `unstable' <ref> [14] </ref>. Although techniques may be used to invert a near singular diagonal matrix approximately, the resulting preconditioner is sometimes less efficient. A common method for assessing the quality of a preconditioner is to estimate the condition number of the preconditioned system. <p> the inverse of D for the Schur complement (4) and for the block (ED 1 ) in the right-hand side of (3), a small or zero d i in D may cause problems in the process of forming D 1 and may also give rise to instabilities in the LU-solves <ref> [14] </ref>. An obvious strategy to prevent these problems is to choose each d i so that jd i j &gt; " for some tolerance ". This can be combined with the process of finding the independent set. <p> The actual convergence rate of the preconditioned iteration is controlled by the so-called preconditioned error. The preconditioned matrix is L 1 AU 1 = I + L 1 RU 1 : The matrix L 1 RU 1 is the preconditioned error matrix <ref> [14] </ref>, which actually governs the convergence of the preconditioned iteration.
Reference: [15] <author> P. M. de Zeeuw, </author> <title> Matrix-dependent prolongations and restrictions in a blackbox multi-grid solver, </title> <journal> J. Comput. Appl. Math. </journal> <volume> 33, </volume> <month> 1-27 </month> <year> (1990). </year>
Reference-contexts: For the repeated red-black ordering approach, a near-optimal bound O (n 0:153 ) for the condition number of the preconditioned matrix has been obtained [21]. There are other algebraic multigrid methods [11, 24, 25, 26] and multigrid methods with matrix dependent treatments <ref> [15, 23] </ref>, as well as multi-level preconditioning techniques based on hierarchical basis or ILU decomposition associated with the finite difference or finite element analysis [3, 4, 10].
Reference: [16] <author> I. S. Duff, R. G. Grimes and J. G. Lewis, </author> <title> Sparse matrix test problems, </title> <journal> ACM Trans. Math. </journal> <volume> Software 15, </volume> <month> 1-14 </month> <year> (1989). </year>
Reference-contexts: We see that there is no much difference in the CPU time by using different diagonal tolerances if the convergence rate is the same. 4.3 Harwell-Boeing collection The third set of test matrices was taken from the Harwell-Boeing collection <ref> [16, 17] </ref>. (The only exception is that the matrix ADD20 was taken from NIST's MatrixMarket. 2 ) Many of these matrices have been used as test matrices for iterative sparse matrix solvers [29, 32].
Reference: [17] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis, </author> <title> User's Guide for the Harwell-Boeing Sparse Matrix Collection, </title> <type> Technical Report TR/PA/92/86, </type> <institution> CERFACS, Toulouse, France, </institution> <year> 1992. </year>
Reference-contexts: We see that there is no much difference in the CPU time by using different diagonal tolerances if the convergence rate is the same. 4.3 Harwell-Boeing collection The third set of test matrices was taken from the Harwell-Boeing collection <ref> [16, 17] </ref>. (The only exception is that the matrix ADD20 was taken from NIST's MatrixMarket. 2 ) Many of these matrices have been used as test matrices for iterative sparse matrix solvers [29, 32].
Reference: [18] <author> M. Engelman, FIDAP: </author> <title> Examples Manual, Revision 6.0, </title> <booktitle> Fluid Dynamics International, </booktitle> <address> Evanston, IL, </address> <year> 1991. </year>
Reference-contexts: We used Fortran 77 programming language in 64-bit precision. 4.1 FIDAP matrices Our first set of test matrices were extracted from the test problems provided in the FIDAP package <ref> [18] </ref>. The examples tested model the incompressible Navier-Stokes equations. The right bottom sub-block of these matrices may be a zero block [13] and many of these matrices have small or zero diagonals and are very hard to solve with standard ILU preconditioners.
Reference: [19] <author> J. A. George and J. W. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year> <month> 21 </month>
Reference-contexts: Then the corresponding row of C associated with this node is not altered by the reduction process. Proof. This is an immediate consequence of the graph model of Gaussian elimination <ref> [19] </ref>. The reduction process corresponds to eliminating all nodes in S in Gaussian elimination. When a node s in S is eliminated, all its incident edges are removed and only the nodes associated with nodes adjacent to s are altered.
Reference: [20] <author> G. H. Golub and H. A. van der Vorst, </author> <title> Closer to the solution: iterative linear solvers, in The State of the Art in Numerical Analysis, </title> <editor> I. S. Duff and G. A. Watson, eds., </editor> <publisher> Clarendon Press, Oxford, </publisher> <pages> pp. 63-92, </pages> <year> 1997. </year>
Reference-contexts: Such large linear systems are often solved by Krylov subspace methods coupled with a suitable preconditioner [30]. It is widely accepted that the convergence rate of a preconditioned Krylov subspace method is primarily determined by the quality of the preconditioner employed <ref> [20, 30] </ref>. As a result, recent focus has shifted from designing iterative accelerators to constructing efficient preconditioners. fl This work was supported in part by NSF under grant CCR-9618827, and in part by the Minnesota Supercomputer Institute. y E-mail: saad@cs.umn.edu. URL: http://www.cs.umn.edu/~saad. z E-mail: jzhang@cs.umn.edu.
Reference: [21] <author> Y. Notay and Z. Ould Amar, </author> <title> A nearly optimal preconditioning based on recursive red-black orderings, </title> <journal> Numer. Linear Algebra Appl. </journal> <volume> 4, </volume> <month> 369-391 </month> <year> (1997). </year>
Reference-contexts: Other methods require only the adjacency graph of the coefficient matrices [5, 29, 32]. For the repeated red-black ordering approach, a near-optimal bound O (n 0:153 ) for the condition number of the preconditioned matrix has been obtained <ref> [21] </ref>. There are other algebraic multigrid methods [11, 24, 25, 26] and multigrid methods with matrix dependent treatments [15, 23], as well as multi-level preconditioning techniques based on hierarchical basis or ILU decomposition associated with the finite difference or finite element analysis [3, 4, 10].
Reference: [22] <author> Y. Notay and Z. Ould Amar, </author> <title> Incomplete factorization preconditioning may lead to multigrid like speed of convergence, in Advanced Mathematics: Computation and Applications, </title> <editor> A. S. Alekseev and N. S. Bakhvalov, eds., NCC Publisher, </editor> <address> Novosibirsk, Russia, </address> <pages> pp. 435-446, </pages> <year> 1996. </year>
Reference-contexts: Some of these approaches require grid information. Examples of such approaches include the nested recursive two-level factorization and the repeated red-black orderings <ref> [2, 6, 9, 22, 33] </ref> (see also the survey paper by Axelsson and Vassilevski [1]). Other methods require only the adjacency graph of the coefficient matrices [5, 29, 32].
Reference: [23] <author> A. A. Reusken, </author> <title> Multigrid with matrix-dependent transfer operators for convection-diffusion problems, in Multigrid Methods IV, </title> <booktitle> Proceedings of 4th European Multigrid Conference, </booktitle> <publisher> Amsterdam, </publisher> <editor> P.W. Hemker and P. Wesseling, eds., </editor> <publisher> Birkhauser Verlag, Basel, </publisher> <pages> pp. 269-280, </pages> <year> 1994. </year>
Reference-contexts: For the repeated red-black ordering approach, a near-optimal bound O (n 0:153 ) for the condition number of the preconditioned matrix has been obtained [21]. There are other algebraic multigrid methods [11, 24, 25, 26] and multigrid methods with matrix dependent treatments <ref> [15, 23] </ref>, as well as multi-level preconditioning techniques based on hierarchical basis or ILU decomposition associated with the finite difference or finite element analysis [3, 4, 10].
Reference: [24] <author> A. A. Reusken, </author> <title> Approximate cyclic reduction preconditioning, </title> <type> Technical Report RANA 97-02, </type> <institution> Department of Mathematics and Computing Science, Eindhoven University of Technology, </institution> <address> The Netherlands, </address> <year> 1997. </year>
Reference-contexts: Other methods require only the adjacency graph of the coefficient matrices [5, 29, 32]. For the repeated red-black ordering approach, a near-optimal bound O (n 0:153 ) for the condition number of the preconditioned matrix has been obtained [21]. There are other algebraic multigrid methods <ref> [11, 24, 25, 26] </ref> and multigrid methods with matrix dependent treatments [15, 23], as well as multi-level preconditioning techniques based on hierarchical basis or ILU decomposition associated with the finite difference or finite element analysis [3, 4, 10].
Reference: [25] <author> J. W. Ruge and K. Stuben, </author> <title> Efficient solution of finite difference and finite element equations, in Multigrid Methods for Integral and Differential Equations, </title> <editor> (D. J. Paddon and H. Holstein, eds.), </editor> <publisher> Clarendon Press, Oxford, </publisher> <month> 169-212 </month> <year> (1985). </year>
Reference-contexts: Other methods require only the adjacency graph of the coefficient matrices [5, 29, 32]. For the repeated red-black ordering approach, a near-optimal bound O (n 0:153 ) for the condition number of the preconditioned matrix has been obtained [21]. There are other algebraic multigrid methods <ref> [11, 24, 25, 26] </ref> and multigrid methods with matrix dependent treatments [15, 23], as well as multi-level preconditioning techniques based on hierarchical basis or ILU decomposition associated with the finite difference or finite element analysis [3, 4, 10].
Reference: [26] <author> J. W. Ruge and K. Stuben, </author> <title> Algebraic multigrid, </title> <booktitle> in Multigrid Methods, </booktitle> <volume> Vol. 4, </volume> <editor> S. F. Mc-Cormick, ed., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <pages> pp. 73-130, </pages> <year> 1987. </year>
Reference-contexts: Other methods require only the adjacency graph of the coefficient matrices [5, 29, 32]. For the repeated red-black ordering approach, a near-optimal bound O (n 0:153 ) for the condition number of the preconditioned matrix has been obtained [21]. There are other algebraic multigrid methods <ref> [11, 24, 25, 26] </ref> and multigrid methods with matrix dependent treatments [15, 23], as well as multi-level preconditioning techniques based on hierarchical basis or ILU decomposition associated with the finite difference or finite element analysis [3, 4, 10].
Reference: [27] <author> Y. Saad, </author> <title> A flexible inner-outer preconditioned GMRES algorithm, </title> <journal> SIAM J. Sci. Com-put. </journal> <volume> 14, </volume> <month> 461-469 </month> <year> (1993). </year>
Reference-contexts: We used FGMRES (10) as the accelerator for both the inner and outer iterations <ref> [27] </ref>. The outer iteration process was preconditioned by ILUM with diagonal threshold tolerance as discussed in this paper, using 10 levels of reduction. The inner iteration process for solving the last reduced system approximately was preconditioned by a dual-threshold ILUT (t; p) [28].
Reference: [28] <author> Y. Saad, ILUT: </author> <title> a dual threshold incomplete ILU preconditioner, </title> <journal> Numer. Linear Algebra Appl. </journal> <volume> 1, </volume> <month> 387-402 </month> <year> (1994). </year>
Reference-contexts: There are other dropping strategies that control the total number of entries in the LU factors and in the reduced systems, see <ref> [28, 29, 31, 32] </ref>. For an M -matrix, it can be shown that the magnitude of a diagonal element is nonincreasing during the reduction process, i.e., ~a j;j c j;j . This seems to suggest that we need to reduce " as the number of levels grows. <p> The outer iteration process was preconditioned by ILUM with diagonal threshold tolerance as discussed in this paper, using 10 levels of reduction. The inner iteration process for solving the last reduced system approximately was preconditioned by a dual-threshold ILUT (t; p) <ref> [28] </ref>. Unless otherwise stated explicitly, 10 we chose t = 10 4 ; p = 20 for ILUT. The same t was also used as the dropping threshold tol-erance in the construction of ILUM.
Reference: [29] <author> Y. Saad, ILUM: </author> <title> a multi-elimination ILU preconditioner for general sparse matrices, </title> <journal> SIAM J. Sci. Comput. </journal> <volume> 17, </volume> <month> 830-847 </month> <year> (1996). </year>
Reference-contexts: An experimental study on robustness of some of these general-purpose preconditioners has been conducted in [14]. For an introduction to Krylov subspace methods and various preconditioning techniques, see [30]. The `multi-elimination ILU preconditioner' (ILUM), introduced in <ref> [29] </ref>, is based on exploiting the idea of successive independent set orderings and several heuristic algorithms have been suggested to find the independent set. The ILUM preconditioner has a multi-level structure and offers a good degree of parallelism. <p> Some of these approaches require grid information. Examples of such approaches include the nested recursive two-level factorization and the repeated red-black orderings [2, 6, 9, 22, 33] (see also the survey paper by Axelsson and Vassilevski [1]). Other methods require only the adjacency graph of the coefficient matrices <ref> [5, 29, 32] </ref>. For the repeated red-black ordering approach, a near-optimal bound O (n 0:153 ) for the condition number of the preconditioned matrix has been obtained [21]. <p> The ILUM preconditioning technique has been extended to block version (BILUM) in which the pivoting elements are not diagonals, but small block diagonals [8, 32]. For some hard-to-solve problems, the performance of ILUM may be enhanced by the block treatment. The heuristic algorithms introduced in <ref> [29] </ref> to find independent sets ignore the values of the matrix. For matrices arising from certain applications such as finite element methods in computational fluid dynamics on unstructured meshes, some diagonal values may be very small or even zero. <p> Let (v j ; v k ) denote an edge from vertex v j to vertex v k . Recall that an independent set S is a subset of the vertex set V such that no two elements of S are coupled by an equation, 3 see for example, <ref> [29, 30, 32] </ref>. An independent set S is maximal if there is no independent set which strictly includes S, i.e., for any v 2 V , S [ fvg is not independent. <p> The complement of S with respect to V , denoted by S c , clearly always satisfies S [ S c = V . The term independent set is often used to mean a Maximal Independent Set (MIS), see for example <ref> [29, 32] </ref>. Clearly, because of the restrictions on the diagonal elements, the independent sets discussed in this paper are not necessarily maximal. 2.1 Algorithms for independent sets with diagonal threshold In [30], several heuristic algorithms were discussed to find a maximal independent set. <p> We will also consider another heuristic algorithm that searches an independent set with increasing degree traversal and diagonal threshold tolerance <ref> [29] </ref>. <p> There are other locally optimal heuristics that could be used to find an independent set, see <ref> [29] </ref> for a discussion and comparisons. In this paper, we restrict our attention to the above two heuristics. <p> Standard threshold dropping strategies are suggested in <ref> [29, 32] </ref>. In the simplest case, all entries in some blocks whose absolute values are smaller than some threshold tolerance t are dropped, i.e., a j;k is dropped whenever ja j;k j &lt; t: (7) Other dropping strategies are based on the values of the row or column or both. <p> There are other dropping strategies that control the total number of entries in the LU factors and in the reduced systems, see <ref> [28, 29, 31, 32] </ref>. For an M -matrix, it can be shown that the magnitude of a diagonal element is nonincreasing during the reduction process, i.e., ~a j;j c j;j . This seems to suggest that we need to reduce " as the number of levels grows. <p> We found no obvious benefit in changing t during the construction of ILUM or its block variant BILUM <ref> [29, 32] </ref>. Similarly, our tests showed no gains in reducing the diagonal threshold tolerance. <p> The matrices d ED 1 and ^ A 1 result from applying a dropping rule to ED 1 and A 1 , respectively, see <ref> [29] </ref> for details. <p> By visiting the nodes with small degree first, F is likely to be sparser, leading, in general, to a larger independent set. This observation has been confirmed by our numerical results in <ref> [29, 32] </ref>. Although several parameters have an effect on the convergence rate, it is the size of the reduced system and its minimization that seem to draw most of the recent attention. <p> This leads to the development of various blocking strategies and dropping rules [31, 32]. 4 Numerical Experiments Standard implementations of ILUM and BILUM have been described in detail in <ref> [29, 32] </ref>. The iteration process consists of an outer iteration, which is the main preconditioned process to solve the underlying linear system, and an inner iteration, which is the secondary preconditioned process to solve the last reduced system. <p> is the same. 4.3 Harwell-Boeing collection The third set of test matrices was taken from the Harwell-Boeing collection [16, 17]. (The only exception is that the matrix ADD20 was taken from NIST's MatrixMarket. 2 ) Many of these matrices have been used as test matrices for iterative sparse matrix solvers <ref> [29, 32] </ref>. The description of these Harwell-Boeing matrices is listed in Table 6 and the test results are listed in Table 7. Since most of these matrices have large absolute diagonal values, the iteration counts do not vary much. For FS7602 and WATT2, ILUM performed poorly without diagonal threshold tolerance.
Reference: [30] <author> Y. Saad, </author> <title> Iterative Methods for Sparse Linear Systems, </title> <publisher> PWS Publishing Co., </publisher> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction We study reordering techniques in developing efficient multi-level preconditioner to solve general sparse linear system Au = b; (1) where A is an unstructured matrix of order n. Such large linear systems are often solved by Krylov subspace methods coupled with a suitable preconditioner <ref> [30] </ref>. It is widely accepted that the convergence rate of a preconditioned Krylov subspace method is primarily determined by the quality of the preconditioner employed [20, 30]. <p> Such large linear systems are often solved by Krylov subspace methods coupled with a suitable preconditioner [30]. It is widely accepted that the convergence rate of a preconditioned Krylov subspace method is primarily determined by the quality of the preconditioner employed <ref> [20, 30] </ref>. As a result, recent focus has shifted from designing iterative accelerators to constructing efficient preconditioners. fl This work was supported in part by NSF under grant CCR-9618827, and in part by the Minnesota Supercomputer Institute. y E-mail: saad@cs.umn.edu. URL: http://www.cs.umn.edu/~saad. z E-mail: jzhang@cs.umn.edu. <p> The main trade-offs when comparing preconditioners are in their intrinsic efficiency, their parallelism, and their robustness. An experimental study on robustness of some of these general-purpose preconditioners has been conducted in [14]. For an introduction to Krylov subspace methods and various preconditioning techniques, see <ref> [30] </ref>. The `multi-elimination ILU preconditioner' (ILUM), introduced in [29], is based on exploiting the idea of successive independent set orderings and several heuristic algorithms have been suggested to find the independent set. The ILUM preconditioner has a multi-level structure and offers a good degree of parallelism. <p> Let (v j ; v k ) denote an edge from vertex v j to vertex v k . Recall that an independent set S is a subset of the vertex set V such that no two elements of S are coupled by an equation, 3 see for example, <ref> [29, 30, 32] </ref>. An independent set S is maximal if there is no independent set which strictly includes S, i.e., for any v 2 V , S [ fvg is not independent. <p> The term independent set is often used to mean a Maximal Independent Set (MIS), see for example [29, 32]. Clearly, because of the restrictions on the diagonal elements, the independent sets discussed in this paper are not necessarily maximal. 2.1 Algorithms for independent sets with diagonal threshold In <ref> [30] </ref>, several heuristic algorithms were discussed to find a maximal independent set. Some of these strategies use locally optimal heuristics and numerical experiments in [30] suggeste that they have similar performance. We first give the simplest greedy algorithm with diagonal threshold tolerance. Algorithm 2.1 Greedy algorithm 1. <p> the restrictions on the diagonal elements, the independent sets discussed in this paper are not necessarily maximal. 2.1 Algorithms for independent sets with diagonal threshold In <ref> [30] </ref>, several heuristic algorithms were discussed to find a maximal independent set. Some of these strategies use locally optimal heuristics and numerical experiments in [30] suggeste that they have similar performance. We first give the simplest greedy algorithm with diagonal threshold tolerance. Algorithm 2.1 Greedy algorithm 1. <p> We call this technique a restricted IKJ version of Gaussian elimination. The rows associated with the independent set, i.e., rows 1 to m are eliminated using a variant of the IKJ version of Gaussian elimination <ref> [30] </ref>.
Reference: [31] <author> Y. Saad, M. Sosonkina, and J. Zhang, </author> <title> Domain decomposition and multi-level type techniques for general sparse linear systems, </title> <type> Technical Report UMSI 97/244, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1997. </year>
Reference-contexts: There are other dropping strategies that control the total number of entries in the LU factors and in the reduced systems, see <ref> [28, 29, 31, 32] </ref>. For an M -matrix, it can be shown that the magnitude of a diagonal element is nonincreasing during the reduction process, i.e., ~a j;j c j;j . This seems to suggest that we need to reduce " as the number of levels grows. <p> Although several parameters have an effect on the convergence rate, it is the size of the reduced system and its minimization that seem to draw most of the recent attention. This leads to the development of various blocking strategies and dropping rules <ref> [31, 32] </ref>. 4 Numerical Experiments Standard implementations of ILUM and BILUM have been described in detail in [29, 32]. <p> These matrices were supplied by H. Simon and other researchers and are generally larger than other sets of matrices. They are all from applications in computational fluid dynamics and have been used as test matrices for high order ILU preconditioners in <ref> [12, 31] </ref>. Table 4 gives a brief description of these matrices. Table 5 lists the number of iterations. Since some of these matrices are very large, we had to adjust some parameters in ILUM and ILUT in order to solve them. These adjustments are also indicated in Table 5.
Reference: [32] <author> Y. Saad and J. Zhang, BILUM: </author> <title> block versions of multi-elimination and multi-level ILU preconditioner for general sparse linear systems, </title> <type> Technical Report UMSI 97/126, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1997. </year>
Reference-contexts: The ILUM preconditioner has a multi-level structure and offers a good degree of parallelism. Similar preconditioners have been designed and tested in <ref> [5, 32] </ref> to show near grid-independent convergence for certain problems. In a recent report, some of these multi-level preconditioners have been tested and compared favorably with other preconditioned iterative methods and direct methods at least for the Laplace equation [7]. Some of these approaches require grid information. <p> Some of these approaches require grid information. Examples of such approaches include the nested recursive two-level factorization and the repeated red-black orderings [2, 6, 9, 22, 33] (see also the survey paper by Axelsson and Vassilevski [1]). Other methods require only the adjacency graph of the coefficient matrices <ref> [5, 29, 32] </ref>. For the repeated red-black ordering approach, a near-optimal bound O (n 0:153 ) for the condition number of the preconditioned matrix has been obtained [21]. <p> The ILUM preconditioning technique has been extended to block version (BILUM) in which the pivoting elements are not diagonals, but small block diagonals <ref> [8, 32] </ref>. For some hard-to-solve problems, the performance of ILUM may be enhanced by the block treatment. The heuristic algorithms introduced in [29] to find independent sets ignore the values of the matrix. <p> Let (v j ; v k ) denote an edge from vertex v j to vertex v k . Recall that an independent set S is a subset of the vertex set V such that no two elements of S are coupled by an equation, 3 see for example, <ref> [29, 30, 32] </ref>. An independent set S is maximal if there is no independent set which strictly includes S, i.e., for any v 2 V , S [ fvg is not independent. <p> The complement of S with respect to V , denoted by S c , clearly always satisfies S [ S c = V . The term independent set is often used to mean a Maximal Independent Set (MIS), see for example <ref> [29, 32] </ref>. Clearly, because of the restrictions on the diagonal elements, the independent sets discussed in this paper are not necessarily maximal. 2.1 Algorithms for independent sets with diagonal threshold In [30], several heuristic algorithms were discussed to find a maximal independent set. <p> Standard threshold dropping strategies are suggested in <ref> [29, 32] </ref>. In the simplest case, all entries in some blocks whose absolute values are smaller than some threshold tolerance t are dropped, i.e., a j;k is dropped whenever ja j;k j &lt; t: (7) Other dropping strategies are based on the values of the row or column or both. <p> For example, the dropping rule used in <ref> [32] </ref> is relative to the average absolute value of the current row, i.e., an entry a j;k in the U factor ED 1 and in the reduced system A 1 is dropped if ja j;k j &lt; jnz (j)j i 2 nz (j) Diagonal entries in the reduced systems are not <p> There are other dropping strategies that control the total number of entries in the LU factors and in the reduced systems, see <ref> [28, 29, 31, 32] </ref>. For an M -matrix, it can be shown that the magnitude of a diagonal element is nonincreasing during the reduction process, i.e., ~a j;j c j;j . This seems to suggest that we need to reduce " as the number of levels grows. <p> We found no obvious benefit in changing t during the construction of ILUM or its block variant BILUM <ref> [29, 32] </ref>. Similarly, our tests showed no gains in reducing the diagonal threshold tolerance. <p> By visiting the nodes with small degree first, F is likely to be sparser, leading, in general, to a larger independent set. This observation has been confirmed by our numerical results in <ref> [29, 32] </ref>. Although several parameters have an effect on the convergence rate, it is the size of the reduced system and its minimization that seem to draw most of the recent attention. <p> Although several parameters have an effect on the convergence rate, it is the size of the reduced system and its minimization that seem to draw most of the recent attention. This leads to the development of various blocking strategies and dropping rules <ref> [31, 32] </ref>. 4 Numerical Experiments Standard implementations of ILUM and BILUM have been described in detail in [29, 32]. <p> This leads to the development of various blocking strategies and dropping rules [31, 32]. 4 Numerical Experiments Standard implementations of ILUM and BILUM have been described in detail in <ref> [29, 32] </ref>. The iteration process consists of an outer iteration, which is the main preconditioned process to solve the underlying linear system, and an inner iteration, which is the secondary preconditioned process to solve the last reduced system. <p> Although the single-level ILUT used less memory than ILUM in this case, the results do give us some indication of the robustness of the multi-level preconditioned method versus single-level method. Formula (6) was not used in our tests. The construction and application of ILUM preconditioner was described in <ref> [32] </ref>, but here we applied the dropping rules (8) from the first level (the dropping rules were applied starting from the 2nd level only in [32]). For all linear systems, the right-hand side was generated by assuming that the solution is a vector of all ones. <p> Formula (6) was not used in our tests. The construction and application of ILUM preconditioner was described in <ref> [32] </ref>, but here we applied the dropping rules (8) from the first level (the dropping rules were applied starting from the 2nd level only in [32]). For all linear systems, the right-hand side was generated by assuming that the solution is a vector of all ones. The initial guess was a vector of some random numbers. <p> is the same. 4.3 Harwell-Boeing collection The third set of test matrices was taken from the Harwell-Boeing collection [16, 17]. (The only exception is that the matrix ADD20 was taken from NIST's MatrixMarket. 2 ) Many of these matrices have been used as test matrices for iterative sparse matrix solvers <ref> [29, 32] </ref>. The description of these Harwell-Boeing matrices is listed in Table 6 and the test results are listed in Table 7. Since most of these matrices have large absolute diagonal values, the iteration counts do not vary much. For FS7602 and WATT2, ILUM performed poorly without diagonal threshold tolerance.
Reference: [33] <author> A. van der Ploeg, </author> <title> Preconditioning for Sparse Matrices with Applications, </title> <type> Ph.D. thesis, </type> <institution> University of Groningen, </institution> <address> The Netherlands, </address> <year> 1993. </year> <month> 22 </month>
Reference-contexts: Some of these approaches require grid information. Examples of such approaches include the nested recursive two-level factorization and the repeated red-black orderings <ref> [2, 6, 9, 22, 33] </ref> (see also the survey paper by Axelsson and Vassilevski [1]). Other methods require only the adjacency graph of the coefficient matrices [5, 29, 32].
References-found: 33

