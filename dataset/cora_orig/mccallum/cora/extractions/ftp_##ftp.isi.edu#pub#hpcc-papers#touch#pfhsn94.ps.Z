URL: ftp://ftp.isi.edu/pub/hpcc-papers/touch/pfhsn94.ps.Z
Refering-URL: http://www.isi.edu/isi-technical-reports.html
Root-URL: http://www.isi.edu
Email: (touch@isi.edu)  
Title: Protocol Parallelization  
Author: Joseph D. Touch 
Keyword: Keyword Codes: C.2.2, C.2.4, C.5.0 Keywords: Network Protocols, Distributed Systems, Computer System Implementation  
Address: 4676 Admiralty Way, Marina del Rey, CA, 90292-6695, U.S.A.,  
Affiliation: USC Information Sciences Institute,  
Abstract: There is increasing concern about the capability of existing protocols to keep pace with communication rates, as rates approach the gigabit range. This assumes a sort of protocol bottleneck. Many similar bottlenecks are alleviated by the use of parallelism, so one hypothesis is to parallelize protocols. We examine the pros and cons of this hypothesis, and the dimensions to which parallelism might be applied. We distinguish the unique communication issues that result. Our conclusions indicate that conventional parallelism may not be applicable to protocols. New types of parallelism become more significant in this light. These include information parallelism (Parallel Communication) and packet-train parallelism. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bjorkman, M., and Gunningberg, P., </author> <title> Locking Effects in Multiprocessor Implementation of Protocols, </title> <booktitle> Proc. ACM Sigcomm, </booktitle> <month> Oct. </month> <year> 1993, </year> <pages> pp. 74-83. </pages>
Reference-contexts: The processing bottleneck for TCP has been addressed by parallelism [16], <ref> [1] </ref> (both discussed later), even though its performance has been shown not to be the predominant limitation to communication [17], [10]. Other components of the protocol stack have also been parallelized, e.g., via pipelining of the IP check-sum with the data transfer [5]. <p> hdr/time data/time hdr/data low bw low low avg high bw periodic low low avg high bw ASAP high high avg high bw huge packets low high low Low BW High BW / Periodic High BW / ASAP High BW / Big Packets Part Seven Posters 354 Others considered per-packet parallelization <ref> [1] </ref>. They measured parallelism using simulations, and implementations of a multiprocessor x-Kernel implementation with spin-locks. They observed that the parallel processing contends for the shared protocol state (the Connection Control Block).
Reference: [2] <author> Braden, R., </author> <title> Extending TCP for Transactions -- Concepts, </title> <institution> Network Working Group RFC-1379, USC/ISI, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: A critical response indicated that some of these proposed optimizations were harmful, and suggested ow control occur over records, rather than bytes. This indicates a message structure beyond that normally assumed for TCP. A current proposal augments TCP to handle transactions <ref> [2] </ref>, to accommodate some of the record-structure suggested before. This line of research is realizing that bandwidth-delay products are the primary issue (i.e., a question bottleneck), and that large windows dont solve the problem (because they solve answering).
Reference: [3] <author> Cheriton, D.R., VMTP: </author> <title> A Transport Protocol for the Next Generation of Communication Systems, </title> <journal> Computer Communication Review, </journal> <month> Aug. </month> <year> 1986, </year> <pages> pp. 406-415. </pages>
Reference-contexts: 1. INTRODUCTION High-speed networks have brought renewed emphasis on protocol performance optimization. Some optimizations focus on machine-specific implementation tuning [17], or protocol-specific implementation tuning [10]. Others have proposed RISC protocols, removing cumbersome functions from the protocol itself (XTP [4], VMTP <ref> [3] </ref>, etc.). A number of projects have observed that bottlenecks in other disciplines are often alleviated by applying parallelism, either spatial or temporal (i.e., pipelining). Here we investigate the kinds of protocol parallelism possible, summarize the current attempts at parallelization, and make observations about their feasibility and limitations. <p> The remainder of the protocol is implemented as before. State optimizations result in RISC-style lightweight protocols. They implement the cross-product protocols (as above), and remove the non-favored states from the protocol. Examples include VMTP <ref> [3] </ref> and XTP [4]. Implementation optimizations include code tweaking such as has been done on the Cray TCP [17], and generic TCP [10], as well as Integrated Layer Processing (ILP) [5]. They also include optimizing the underlying OS interfaces, such as Jacobsons fast-sockets, and the x-Kernel optimizations [18].
Reference: [4] <author> Chesson, G., et. al., </author> <title> XTP Protocol Definition, Protocol Engines, </title> <publisher> Inc., </publisher> <month> Dec. </month> <year> 1988. </year>
Reference-contexts: 1. INTRODUCTION High-speed networks have brought renewed emphasis on protocol performance optimization. Some optimizations focus on machine-specific implementation tuning [17], or protocol-specific implementation tuning [10]. Others have proposed RISC protocols, removing cumbersome functions from the protocol itself (XTP <ref> [4] </ref>, VMTP [3], etc.). A number of projects have observed that bottlenecks in other disciplines are often alleviated by applying parallelism, either spatial or temporal (i.e., pipelining). Here we investigate the kinds of protocol parallelism possible, summarize the current attempts at parallelization, and make observations about their feasibility and limitations. <p> The remainder of the protocol is implemented as before. State optimizations result in RISC-style lightweight protocols. They implement the cross-product protocols (as above), and remove the non-favored states from the protocol. Examples include VMTP [3] and XTP <ref> [4] </ref>. Implementation optimizations include code tweaking such as has been done on the Cray TCP [17], and generic TCP [10], as well as Integrated Layer Processing (ILP) [5]. They also include optimizing the underlying OS interfaces, such as Jacobsons fast-sockets, and the x-Kernel optimizations [18].
Reference: [5] <author> Clark, D., and Tennenhouse, D., </author> <title> Architectural Considerations for a New Generation of Protocols, </title> <booktitle> Proc. ACM Sigcomm, </booktitle> <month> Sept. </month> <year> 1990, </year> <pages> pp. 200-208. </pages>
Reference-contexts: Other components of the protocol stack have also been parallelized, e.g., via pipelining of the IP check-sum with the data transfer <ref> [5] </ref>. Processing in the OS has also been considered, although most of the focus has been on data transfer issues we denote as answering bottlenecks. Answering is a data transport bottleneck, i.e., bandwidth limitations. This is currently being addressed by parallelizing the internal data paths of workstations. <p> They implement the cross-product protocols (as above), and remove the non-favored states from the protocol. Examples include VMTP [3] and XTP [4]. Implementation optimizations include code tweaking such as has been done on the Cray TCP [17], and generic TCP [10], as well as Integrated Layer Processing (ILP) <ref> [5] </ref>. They also include optimizing the underlying OS interfaces, such as Jacobsons fast-sockets, and the x-Kernel optimizations [18]. Other optimizations address general processing issues, and would benefit conventional applications as well a protocol processing. These include fast context switching and hardware header processing.
Reference: [6] <author> Feldermeier, D., </author> <title> Comparison of Error Control Protocols for High Bandwidth-Delay Product Networks, </title> <booktitle> In participants proceedings, IFIP Workshop on Protocols for High Speed Networks, </booktitle> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Part Seven Posters352 Periodic exchange of state as a protocol mechanism is the basis of the Delta-t pro-tocol [23], and has also been mentioned in TP++ <ref> [6] </ref> and the SNR leaky-bucket protocol [15]. A recent proposal indicated that high bandwidth-delay product environments change bandwidth-bound systems to be latency or server-bound [14]. The sources become message limited (question-bound), and the channel utilization changes only if the message sizes increase.
Reference: [7] <author> Finn, G., </author> <title> An Integration of Network Communication with Workstation Architecture, </title> <journal> ACM Computer Communication Review, V. </journal> <volume> 21, No. 5, </volume> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Answering is a data transport bottleneck, i.e., bandwidth limitations. This is currently being addressed by parallelizing the internal data paths of workstations. The basic idea is to replace the internal bus with a more general topology. The Cambridge Desk Area Network (DAN) [9], and ISIs NetStation <ref> [7] </ref> are examples. One interesting question assumes nearly instantaneous thinking (protocol processing) and answering (bandwidth), and asks, is there anything else? Presume that TCP has infinite window sizes, and can run at 800 Mbps (it can on a CRAY [17]). <p> Presume that we have a NetStation, in which each component of a workstation (disk, RAM, display, etc., <ref> [7] </ref>) can both source and sink at 800 Mbps. What then? This is the question of asking. Latency is the final bottleneck [19]. Ultimately, answers can be given only as fast as questions arrive. <p> This is evidence of conict between the topology of the external network, and the internal backplane communication, an answering bottleneck. Proposed solutions involve moving the network into the backplane, such as in the Cambridge Desktop Area Network (DAN) [9] or ISIs NetStation <ref> [7] </ref>. There are other limitations, asking bottlenecks, as have been recently observed [21]. In a high bandwidth-delay product network, the data source itself becomes a limit to the channel utilization (not enough questions).
Reference: [8] <author> Giarrizzo, D., Kaiserswerth, M., Wicki, T., and Williamson, R., </author> <title> High-Speed Parallel Protocol Implementation, in Protocols for High Speed Networks, Rudin, </title> <editor> H. and Williamson, R. Eds., </editor> <publisher> Elsevier, </publisher> <year> 1989, </year> <pages> pp. 165-180. </pages>
Reference-contexts: The result was 4-way parallelism, with no further decomposition proposed.They concluded that the protocol definitions were not conducive to further parallel decomposition, especially because of the overlap of functions between layers. Other work focused on per-function decomposition of OSI protocols <ref> [8] </ref>. They considered TP-4 and 802.2 LLC protocol decomposition. They decompose their protocols per-layer 4-ways, by allocating 2 processors each for send and receive. Further decomposition follows a dataow diagram of the protocol, where functions are divided among the two processors.
Reference: [9] <author> Hayter, M., and McAuley, D., </author> <title> The Desk Area Network, </title> <journal> ACM Transactions on Operating Systems, </journal> <month> Oct. </month> <year> 1991, </year> <pages> pp. 14-21. </pages>
Reference-contexts: Answering is a data transport bottleneck, i.e., bandwidth limitations. This is currently being addressed by parallelizing the internal data paths of workstations. The basic idea is to replace the internal bus with a more general topology. The Cambridge Desk Area Network (DAN) <ref> [9] </ref>, and ISIs NetStation [7] are examples. One interesting question assumes nearly instantaneous thinking (protocol processing) and answering (bandwidth), and asks, is there anything else? Presume that TCP has infinite window sizes, and can run at 800 Mbps (it can on a CRAY [17]). <p> Protocols exhibits data transport limitations even after avoiding multiple data movement. This is evidence of conict between the topology of the external network, and the internal backplane communication, an answering bottleneck. Proposed solutions involve moving the network into the backplane, such as in the Cambridge Desktop Area Network (DAN) <ref> [9] </ref> or ISIs NetStation [7]. There are other limitations, asking bottlenecks, as have been recently observed [21]. In a high bandwidth-delay product network, the data source itself becomes a limit to the channel utilization (not enough questions).
Reference: [10] <author> Jacobson, V., </author> <title> Congestion Avoidance and Control, </title> <journal> ACM Computer Communication Review, </journal> <month> Oct. </month> <year> 1988, </year> <pages> pp. 314-329. </pages>
Reference-contexts: 1. INTRODUCTION High-speed networks have brought renewed emphasis on protocol performance optimization. Some optimizations focus on machine-specific implementation tuning [17], or protocol-specific implementation tuning <ref> [10] </ref>. Others have proposed RISC protocols, removing cumbersome functions from the protocol itself (XTP [4], VMTP [3], etc.). A number of projects have observed that bottlenecks in other disciplines are often alleviated by applying parallelism, either spatial or temporal (i.e., pipelining). <p> The processing bottleneck for TCP has been addressed by parallelism [16], [1] (both discussed later), even though its performance has been shown not to be the predominant limitation to communication [17], <ref> [10] </ref>. Other components of the protocol stack have also been parallelized, e.g., via pipelining of the IP check-sum with the data transfer [5]. Processing in the OS has also been considered, although most of the focus has been on data transfer issues we denote as answering bottlenecks. <p> This includes thinking optimizations, such as header processing optimizations, state optimizations, implementation optimizations, and augmenting the general Protocol Parallelization 351 processing power of the system. It also includes answering optimizations, includ-ing data transport path optimizations. Header processing optimizations include fast-path optimizations and header prediction <ref> [10] </ref> are specific instances of general cross-product protocol optimizations such as Protocol Bypass [24]. The technique takes the cross product of all protocol functions and layers, factors out the statistically favored states and implements them as special (fast) cases. The remainder of the protocol is implemented as before. <p> State optimizations result in RISC-style lightweight protocols. They implement the cross-product protocols (as above), and remove the non-favored states from the protocol. Examples include VMTP [3] and XTP [4]. Implementation optimizations include code tweaking such as has been done on the Cray TCP [17], and generic TCP <ref> [10] </ref>, as well as Integrated Layer Processing (ILP) [5]. They also include optimizing the underlying OS interfaces, such as Jacobsons fast-sockets, and the x-Kernel optimizations [18]. Other optimizations address general processing issues, and would benefit conventional applications as well a protocol processing.
Reference: [11] <author> Jacobson, V., and Braden, R., </author> <title> TCP Extensions for Long-Delay Paths, </title> <institution> Network Working Group RFC-1072, LBL and USC/Information Sciences Institute, </institution> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: PRIOR WORK Optimized TCP focused on increasing the window size to accommodate higher bandwidth-delay products, so-called long delay <ref> [11] </ref>, high-speed [12], and high-performance [13] TCP. The large windows permitted ow control feedback to occur over byte blocks, rather than bytes themselves, permitting longer answers.
Reference: [12] <author> Jacobson, V., Braden, R., and Zhang, L., </author> <title> TCP Extensions for High-Speed Paths, </title> <institution> Network Working Group RFC-1185, LBL and USC/ISI, </institution> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: PRIOR WORK Optimized TCP focused on increasing the window size to accommodate higher bandwidth-delay products, so-called long delay [11], high-speed <ref> [12] </ref>, and high-performance [13] TCP. The large windows permitted ow control feedback to occur over byte blocks, rather than bytes themselves, permitting longer answers.
Reference: [13] <author> Jacobson, V., Braden, R., and Borman, D., </author> <title> TCP Extensions for High Performance, Network Working Group RFC-1323, </title> <journal> LBL, USC/ISI, and Cray Research, May 1992.IEEE Communications Magazine, </journal> <volume> Vol. 30, No. 4, </volume> <month> April </month> <year> 1992, </year> <pages> pp. </pages> <month> 36-40. </month> <title> Protocol Parallelization 359 </title>
Reference-contexts: PRIOR WORK Optimized TCP focused on increasing the window size to accommodate higher bandwidth-delay products, so-called long delay [11], high-speed [12], and high-performance <ref> [13] </ref> TCP. The large windows permitted ow control feedback to occur over byte blocks, rather than bytes themselves, permitting longer answers. High-performance TCP was specifically designed for high bandwidth-delay environments, observing that the bandwidth-delay product was the issue, rather than the latency or the speed (as in prior proposals) [13]. <p> high-performance <ref> [13] </ref> TCP. The large windows permitted ow control feedback to occur over byte blocks, rather than bytes themselves, permitting longer answers. High-performance TCP was specifically designed for high bandwidth-delay environments, observing that the bandwidth-delay product was the issue, rather than the latency or the speed (as in prior proposals) [13]. A critical response indicated that some of these proposed optimizations were harmful, and suggested ow control occur over records, rather than bytes. This indicates a message structure beyond that normally assumed for TCP.
Reference: [14] <author> Kleinrock, </author> <title> L, The Latency / Bandwidth Tradeoff in Gigabit Networks, </title> <journal> IEEE Communications Magazine, </journal> <volume> Vol. 30, No. 4, </volume> <month> April </month> <year> 1992, </year> <pages> pp. 36-40. </pages>
Reference-contexts: A recent proposal indicated that high bandwidth-delay product environments change bandwidth-bound systems to be latency or server-bound <ref> [14] </ref>. The sources become message limited (question-bound), and the channel utilization changes only if the message sizes increase. The proposal suggests that multiplexing will increase channel utilization, just as multiprocessing increases processor utilization in the presence of I/O communication latency.
Reference: [15] <author> Netravali, Arun N., Roome, W.D., and Sabnani, K., </author> <title> Design and Implementation of a High-Speed Transport Protocol. </title> <journal> IEEE Transactions on Communications V. </journal> <volume> 38, </volume> <month> N.11 (Nov. </month> <year> 1990), </year> <pages> pp. 2010-2024. </pages>
Reference-contexts: Part Seven Posters352 Periodic exchange of state as a protocol mechanism is the basis of the Delta-t pro-tocol [23], and has also been mentioned in TP++ [6] and the SNR leaky-bucket protocol <ref> [15] </ref>. A recent proposal indicated that high bandwidth-delay product environments change bandwidth-bound systems to be latency or server-bound [14]. The sources become message limited (question-bound), and the channel utilization changes only if the message sizes increase.
Reference: [16] <author> Nguyen, C. and Schwartz, M., </author> <title> Reducing the Complexities of TCP for a High Speed Networking Environment, </title> <booktitle> Proc. IEEE Infocom, </booktitle> <month> Mar. </month> <year> 1993, </year> <pages> pp. 1162-1169. </pages>
Reference-contexts: The processing bottleneck for TCP has been addressed by parallelism <ref> [16] </ref>, [1] (both discussed later), even though its performance has been shown not to be the predominant limitation to communication [17], [10]. Other components of the protocol stack have also been parallelized, e.g., via pipelining of the IP check-sum with the data transfer [5]. <p> Some of their results may be an artifact of the communication topology of Transputers, which are 4-way connected. More recent attempts factored TCP/IP as if its functional components were executing on a statically load-balanced compiler <ref> [16] </ref>. At first, they partitioned TCP into three functional components - send, receive, and timer. Their analysis concluded that TCP could be parallelized further by distinguishing between data and control, such that there were four components - send_data, receive_data, control, and timer. <p> Protocols are bandwidth-delay product sensitive [19]. Further, many existing parallelization techniques do not address bandwidth-delay product (the question bottleneck). One exception is <ref> [16] </ref>, but their conclusion is that in order to avoid the problems that bandwidth-delay product causes, they exclude the high-frequency header domain.
Reference: [17] <author> Nicholson, A., Golio, J., Borman, D.A., Young, J., and Roiger, W., </author> <title> High Speed Networking at Cray Research, </title> <journal> ACM Computer Communication Review, V. </journal> <volume> 21, </volume> <editor> N. </editor> <volume> 1, </volume> <month> Jan. </month> <year> 1991, </year> <pages> pp. 99-110. </pages>
Reference-contexts: 1. INTRODUCTION High-speed networks have brought renewed emphasis on protocol performance optimization. Some optimizations focus on machine-specific implementation tuning <ref> [17] </ref>, or protocol-specific implementation tuning [10]. Others have proposed RISC protocols, removing cumbersome functions from the protocol itself (XTP [4], VMTP [3], etc.). A number of projects have observed that bottlenecks in other disciplines are often alleviated by applying parallelism, either spatial or temporal (i.e., pipelining). <p> The processing bottleneck for TCP has been addressed by parallelism [16], [1] (both discussed later), even though its performance has been shown not to be the predominant limitation to communication <ref> [17] </ref>, [10]. Other components of the protocol stack have also been parallelized, e.g., via pipelining of the IP check-sum with the data transfer [5]. Processing in the OS has also been considered, although most of the focus has been on data transfer issues we denote as answering bottlenecks. <p> The Cambridge Desk Area Network (DAN) [9], and ISIs NetStation [7] are examples. One interesting question assumes nearly instantaneous thinking (protocol processing) and answering (bandwidth), and asks, is there anything else? Presume that TCP has infinite window sizes, and can run at 800 Mbps (it can on a CRAY <ref> [17] </ref>). Presume that we have a NetStation, in which each component of a workstation (disk, RAM, display, etc., [7]) can both source and sink at 800 Mbps. What then? This is the question of asking. Latency is the final bottleneck [19]. <p> Other types are neither effective nor feasible (per byte!), because the overhead outweighs any expected gain, or is at least as large as the protocol itself. For each parameter, we consider whether a real bottleneck is being addressed Recall that TCP can run at 800 Mbps <ref> [17] </ref>, however not across a real link. Is the limitation the processing, or the sourced information itself? Is speed a real issue, or is latency? Before we continue, we should reevaluate what the existing bottlenecks of a heavyweight transport protocol are. <p> State optimizations result in RISC-style lightweight protocols. They implement the cross-product protocols (as above), and remove the non-favored states from the protocol. Examples include VMTP [3] and XTP [4]. Implementation optimizations include code tweaking such as has been done on the Cray TCP <ref> [17] </ref>, and generic TCP [10], as well as Integrated Layer Processing (ILP) [5]. They also include optimizing the underlying OS interfaces, such as Jacobsons fast-sockets, and the x-Kernel optimizations [18]. Other optimizations address general processing issues, and would benefit conventional applications as well a protocol processing.
Reference: [18] <author> Peterson, L., Hutchinson, N., et. al., </author> <title> The x-Kernel: A Platform for Accessing Internet Resources, </title> <journal> IEEE Computer, V. </journal> <volume> 23, </volume> <editor> N. </editor> <volume> 5, </volume> <month> May </month> <year> 1990, </year> <pages> pp. 23-33. </pages>
Reference-contexts: Implementation optimizations include code tweaking such as has been done on the Cray TCP [17], and generic TCP [10], as well as Integrated Layer Processing (ILP) [5]. They also include optimizing the underlying OS interfaces, such as Jacobsons fast-sockets, and the x-Kernel optimizations <ref> [18] </ref>. Other optimizations address general processing issues, and would benefit conventional applications as well a protocol processing. These include fast context switching and hardware header processing. The latter is a avor of support processor, other examples of which include FPUs, string processors, and graphic engines.
Reference: [19] <author> Touch, Joseph D., </author> <title> Mirage: A Model for Latency in Communication, </title> <type> Ph.D. dissertation, </type> <institution> Dept. of Computer and Information Science, Univ. of Pennsylva-nia, </institution> <year> 1992. </year> <note> Also available as Dept. of CIS Tech. Report MS-CIS-92-42 / DSL-11. </note>
Reference-contexts: Presume that we have a NetStation, in which each component of a workstation (disk, RAM, display, etc., [7]) can both source and sink at 800 Mbps. What then? This is the question of asking. Latency is the final bottleneck <ref> [19] </ref>. Ultimately, answers can be given only as fast as questions arrive. If the next question depends on the current answer, round trip propagation latency is incurred between asking rounds. Assuming thinking and answering are not the bottlenecks implies asking is. <p> It is here that we believe parallelism is best applied, to the parallelization of possible next questions and thus answers. We call this Parallel Communication, and it is based on parallelization of the information stream <ref> [19] </ref>, [21]. 350 Part Seven Posters 3. PARALLELIZATION ISSUES Parallelization uses replicates to perform the work of a single entity, and involves considering replication dimension, mapping function, scale limitations, replicate interference, overhead, and expected gain. The type of entity replicated is the replication dimension. <p> Nondeterministic multiplexing just pushes the state imprecision problem to the multiplexer control level. The only known solution uses message stream structure (records, branches, and recursion) to permit source anticipation of remote state <ref> [19] </ref>. The message stream carries responses to parallel futures of the remote state. One application which supplies the requisite structure is distributed hypermedia, such as in the WorldWide Web. 5. CURRENT WORK Recent work attempts to parallelize the processing of particular protocols. <p> So the rate issue is a red herring with respect to protocol processing. A protocol does, however, know how many bits are in transit, both in terms of the amount of state required, and the effort required to manage that state. Protocols are bandwidth-delay product sensitive <ref> [19] </ref>. Further, many existing parallelization techniques do not address bandwidth-delay product (the question bottleneck). One exception is [16], but their conclusion is that in order to avoid the problems that bandwidth-delay product causes, they exclude the high-frequency header domain.
Reference: [20] <author> Touch, J.D., and Farber, </author> <title> D.J., Reducing Latency in Communication, letter to the editor in IEEE Communications Magazine, </title> <month> Feb. </month> <year> 1993, </year> <pages> pp. 8-9. </pages>
Reference-contexts: The proposal suggests that multiplexing will increase channel utilization, just as multiprocessing increases processor utilization in the presence of I/O communication latency. A reply to this proposal observed that multiplexing would not so much solve the problem as define it away <ref> [20] </ref>. Deterministic multiplexing is equivalent to lower bandwidth-delay product links. The real issue is protocol state imprecision, induced by high bandwidth-delay products and variance in protocol function (nondeterministic protocols) or in latency (delay variance). Nondeterministic multiplexing just pushes the state imprecision problem to the multiplexer control level.
Reference: [21] <author> Touch, J.D., </author> <title> Parallel Communication, </title> <booktitle> Proc. IEEE Infocom, </booktitle> <month> Mar. </month> <year> 1993, </year> <pages> pp. 505-512. </pages>
Reference-contexts: It is here that we believe parallelism is best applied, to the parallelization of possible next questions and thus answers. We call this Parallel Communication, and it is based on parallelization of the information stream [19], <ref> [21] </ref>. 350 Part Seven Posters 3. PARALLELIZATION ISSUES Parallelization uses replicates to perform the work of a single entity, and involves considering replication dimension, mapping function, scale limitations, replicate interference, overhead, and expected gain. The type of entity replicated is the replication dimension. <p> Other dimensions not commonly considered include per-packet train, and per-information stream. Packet trains are sequences of packets that act as a unit in the protocol; a common example is a fragmentation group. Information streams are alternate packet sequences, such as would occur with breadth-first (BFS) source anticipation <ref> [21] </ref>. BFS source anticipation is parallelizing possible future questions and their answers, in order to keep the communications mechanism occupied between actual questions. Dimensions not considered are per-host address (issue for routers only), and per-application (equivalent to per-protocol). The mapping function helps determine whether a replication dimension is feasible. <p> Proposed solutions involve moving the network into the backplane, such as in the Cambridge Desktop Area Network (DAN) [9] or ISIs NetStation [7]. There are other limitations, asking bottlenecks, as have been recently observed <ref> [21] </ref>. In a high bandwidth-delay product network, the data source itself becomes a limit to the channel utilization (not enough questions).
Reference: [22] <author> Touch, J.D., and Farber, </author> <title> D.J., An Experiment in Latency Reduction, </title> <booktitle> Proc. IEEE Infocom, </booktitle> <month> June. </month> <year> 1994, </year> <pages> pp. 175-181. </pages>
Reference-contexts: current TCP specification are fixed, what will fill these windows? Measurements indicate that network bandwidth-delay products are increasing at a faster rate than that of the end system, so its not clear that file sizes will increase in proportion to network rates, such that a larger window will be usable <ref> [22] </ref>. 4. PRIOR WORK Optimized TCP focused on increasing the window size to accommodate higher bandwidth-delay products, so-called long delay [11], high-speed [12], and high-performance [13] TCP. The large windows permitted ow control feedback to occur over byte blocks, rather than bytes themselves, permitting longer answers.
Reference: [23] <author> Watson, R.W., </author> <title> The Delta-t Transport Protocol: Features and Experience, Protocols for High Speed Networks, </title> <publisher> Elsevier, </publisher> <year> 1989, </year> <pages> pp. 3-17. </pages>
Reference-contexts: Part Seven Posters352 Periodic exchange of state as a protocol mechanism is the basis of the Delta-t pro-tocol <ref> [23] </ref>, and has also been mentioned in TP++ [6] and the SNR leaky-bucket protocol [15]. A recent proposal indicated that high bandwidth-delay product environments change bandwidth-bound systems to be latency or server-bound [14]. The sources become message limited (question-bound), and the channel utilization changes only if the message sizes increase.
Reference: [24] <author> Woodside, C.M., Ravinadran, K., and Franks, R.G., </author> <title> The Protocol Bypass Concept for High Speed OSI Data Transfer, </title> <booktitle> In participants proceedings, IFIP Workshop on Protocols for High Speed Networks, </booktitle> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: It also includes answering optimizations, includ-ing data transport path optimizations. Header processing optimizations include fast-path optimizations and header prediction [10] are specific instances of general cross-product protocol optimizations such as Protocol Bypass <ref> [24] </ref>. The technique takes the cross product of all protocol functions and layers, factors out the statistically favored states and implements them as special (fast) cases. The remainder of the protocol is implemented as before. State optimizations result in RISC-style lightweight protocols.
Reference: [25] <author> Zitterbart, M., </author> <title> High-Speed Protocol Implementations Based on a Multiprocessor Architecture, Protocols for High Speed Networks, Rudin, </title> <editor> H. and Will-iamson, R. Eds., </editor> <publisher> Elsevier, </publisher> <year> 1989, </year> <pages> pp. 151-163. </pages> <note> Part Seven Posters360 </note>
Reference-contexts: These include per-packet and per-function dimensions, both empirically and analytically derived. The results have been disappointing thus far, with a scale limit of 5 parallel processors for most experiments. Early work tried per layer parallelism, a sort of data ow architecture <ref> [25] </ref>. They decomposed TP-4 onto Transputers, where layers 3 and 4 were each assigned one processor for each of send and receive functions.
References-found: 25

