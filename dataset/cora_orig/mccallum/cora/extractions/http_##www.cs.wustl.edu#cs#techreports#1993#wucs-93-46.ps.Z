URL: http://www.cs.wustl.edu/cs/techreports/1993/wucs-93-46.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Email: pwgoldb@cs.sandia.gov  sg@cs.wustl.edu  dmath@cs.wustl.edu  
Title: Learning Unions of Boxes with Membership and Equivalence Queries  
Author: Paul W. Goldberg Sally A. Goldman H. David Mathias 
Note: This research was performed while visiting Washington University. Currently supported by the U.S. Department of Energy under contract DE-AC04-76AL85000. Supported in part by NSF Grant CCR-9110108 and an NSF NYI Grant CCR-9357707.  
Date: May 20, 1994  
Address: MS 1110 P.O. Box 5800 Albuquerque, NM 87185-1110  St. Louis, MO 63130  St. Louis, MO 63130  
Affiliation: Department 1423 Sandia National Laboratories,  Dept. of Computer Science Washington University  Dept. of Computer Science Washington University  
Abstract: We present two algorithms that use membership and equivalence queries to exactly identify the concepts given by the union of s discretized axis-parallel boxes in d-dimensional discretized Euclidean space where each coordinate can have n discrete values. The first algorithm receives at most sd counterexamples and uses time and membership queries polynomial in s and log n for d any constant. Further, all equivalence queries made can be formulated as the union of O(sd log s) axis-parallel boxes. Next, we introduce a new complexity measure that better captures the complexity of a union of boxes than simply the number of boxes and dimensions. Our new measure, , is the number of segments in the target polyhedron where a segment is a maximum portion of one of the sides of the polyhedron that lies entirely inside or entirely outside each of the other halfspaces defining the polyhedron. We then present an improvement of our first algorithm that uses time and queries polynomial in and log n. The hypothesis class used here is decision trees of height at most 2sd. Further we can show that the time and queries used by this algorithm are polynomial in d and log n for s any constant thus generalizing the exact learnability of DNF formulas with a constant number of terms. In fact, this single algorithm is efficient for either s or d constant. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: One such class of geometric concepts is unions of boxes. (By a "box", we mean an axis-aligned hypercuboid. So a box is the set of all points whose Cartesian coordinates satisfy a given set of univariate linear inequalities.) We study this problem under the model of learning with queries <ref> [1] </ref> in which the learner is required to output a final hypothesis that correctly classifies every point in the domain as to whether or not it is inside of one of the target boxes. <p> Observe that the class considered by Frazier et al. is a generalization of the class of DNF formulas in which all variables only appear negated. 3 Definitions The learning model we use in this paper is that of learning with queries developed by An-gluin <ref> [1] </ref>. In this model the learner's goal is to learn exactly how an unknown, Boolean-valued target function f , drawn from concept class C, classifies as positive or negative, all instances from instance space X .
Reference: [2] <author> Peter Auer. </author> <title> On-line learning of rectangles in noisy environments. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 253-261, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: They showed that if the learner was restricted to only make equivalence queries in which each hypothesis was drawn from box d n then (d log n) queries are needed to achieve exact identification [11, 16]. Auer <ref> [2] </ref> improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. <p> Auer <ref> [2] </ref> investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [3] <author> Avrim Blum and Steven Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <booktitle> In Proceedings of the Twenty Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-389, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: it would be interesting to see if S n can be efficiently learned in time polynomial in s and log n for d = O (log s) or in time polynomial in d and log n for s = O (log d) (i.e. a generalization of the Blum and Rudich <ref> [3] </ref> result that O (log n)-term DNF formulas are exactly learnable). Of course, since S n generalizes the class of DNF formulas, it seems very unlikely that one could develop an algorithm that is polynomial in s, log n, and d.
Reference: [4] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Closely related to the problem of learning the union of discretized boxes, is the problem of learning the union of non-discretized boxes in the PAC model [18]. Blumer et al. <ref> [4] </ref> present an algorithm to PAC-learn an s-fold union of boxes in E d by drawing a sufficiently large sample of size m = poly * ; lg 1 , and then performing a greedy covering over the at most em 2d boxes defined by the sample. <p> As discussed by Maass and Turan [16] the task of a concept learning algorithm is to provide a "smart" hy 4 pothesis based on the data available. The results from Blumer et. al <ref> [4] </ref> show that under the PAC model any concise hypothesis that is consistent with the data is "smart enough". In other words, the PAC model provides no suitable basis for distinction among different consistent hypotheses.
Reference: [5] <author> Zhixiang Chen. </author> <title> Learning unions of two rectangles in the plane with equivalence queries. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 243-252. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: Auer shows that box d n is learnable if and only if the fraction of noisy examples is less than 1=(d + 1) and presents an efficient algorithm that handles a noise rate of 1=(2d + 1). More recently, Chen <ref> [5] </ref> gave an algorithm that used equivalence queries to learn general unions of two boxes in the (discretized) plane. The algorithm uses O (log 2 n) equivalence queries, and involves a detailed case analysis of the shapes formed by the two rectangles.
Reference: [6] <author> Zhixiang Chen and Steven Homer. </author> <title> The bounded injury priority method and the learn-ability of unions of rectangles. </title> <type> Unpublished manuscript, </type> <month> May </month> <year> 1994. </year>
Reference-contexts: The hypothesis class of their algorithm is the union of 8s 2 2 rectangles. In work subsequent to that presented here, Chen and Homer <ref> [6] </ref> have improved upon their earlier result by giving an algorithm that learns the union of s boxes in d dimensions using O (k 2 (d+1) d 2 log 2d+1 n) equivalence queries by applying techniques from recursive function theory.
Reference: [7] <author> Zhixiang Chen and Wolfgang Maass. </author> <title> On-line learning of rectangles. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 16-27. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: The best known result for learning the class box d n was provided by the work of Chen and Maass <ref> [7] </ref> in which they gave an algorithm making O (d 2 log n) equivalence queries.
Reference: [8] <author> V. Chvatal. </author> <title> A greedy heuristic for the set covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4(3) </volume> <pages> 233-235, </pages> <year> 1979. </year>
Reference-contexts: Thus our goal is to find the union of as few boxes as possible that "cover" all of the positive regions. We now describe how to formulate this problem as a set covering problem for which we can then use the standard greedy set covering heuristic <ref> [8] </ref> to perform the conversion. The set X of objects to cover will simply contain all positive regions in h. Thus jXj (4s + 1) d . Then the set F of subsets of X will be made as follows.
Reference: [9] <author> Mike Frazier, Sally Goldman, Nina Mishra, and Leonard Pitt. </author> <title> Learning from a consistently ignorant teacher. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: This requirement of selecting a "smart" hypothesis makes the problem of obtaining an efficient algorithm to exactly learn the class S n significantly harder than obtaining the corresponding PAC result. Finally, under a variation of the PAC model in which membership queries can be made, Frazier et al. <ref> [9] </ref> have given an algorithm to PAC-learn the s-fold union of boxes in E d for which each box is entirely contained within the positive quadrant and contains the origin. Furthermore, their algorithm learns this subclass of general unions of boxes in time polynomial in both s and d.
Reference: [10] <author> Steven Homer and Zhixiang Chen. </author> <title> Fast learning unions of rectangles with queries. </title> <type> Unpublished manuscript, </type> <month> July </month> <year> 1993. </year>
Reference-contexts: The algorithm uses O (log 2 n) equivalence queries, and involves a detailed case analysis of the shapes formed by the two rectangles. It does not appear to generalize easily to higher numbers of boxes or dimensions. In work independent of ours, Chen and Homer <ref> [10] </ref> have given an algorithm to learn the union of s rectangles in the plane using O (s 3 log n) queries (both membership and equivalence) and O (s 5 log n) time. The hypothesis class of their algorithm is the union of 8s 2 2 rectangles.
Reference: [11] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: One of the geometric concepts that they studied was the class box d n . They showed that if the learner was restricted to only make equivalence queries in which each hypothesis was drawn from box d n then (d log n) queries are needed to achieve exact identification <ref> [11, 16] </ref>. Auer [2] improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries.
Reference: [12] <author> Philip M. Long and Manfred K. Warmuth. </author> <title> Composite geometric concepts and polyn-momial predictability. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 273-287. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Thus for d constant this algorithm runs in polynomial time. Long and Warmuth <ref> [12] </ref> present an algorithm to PAC-learn this same class by again drawing a sufficiently large sample and constructing a hypothesis that consists of at most s (2d) s boxes consistent with the sample.
Reference: [13] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexamples. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 262-267, </pages> <month> October </month> <year> 1989. </year> <month> 24 </month>
Reference-contexts: An algorithm making O (2 d log n) equivalence queries was given by 2 The final equivalence query is the correct hypothesis, and thus at most sd counterexamples are received. 3 Maass and Turan <ref> [13, 15] </ref>. The best known result for learning the class box d n was provided by the work of Chen and Maass [7] in which they gave an algorithm making O (d 2 log n) equivalence queries.
Reference: [14] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexam-ples and membership queries. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 203-210, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Combining these two methods of analysis, we get the interesting result that this single algorithm is efficient for either s or d constant. 2 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [14, 15, 16] </ref>. One of the geometric concepts that they studied was the class box d n .
Reference: [15] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Algorithms and lower bounds for on-line learning of geometrical concepts. </title> <type> Technical Report IIG-Report 316, </type> <institution> Technische Universitat Graz, TU Graz, Austria, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Combining these two methods of analysis, we get the interesting result that this single algorithm is efficient for either s or d constant. 2 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [14, 15, 16] </ref>. One of the geometric concepts that they studied was the class box d n . <p> An algorithm making O (2 d log n) equivalence queries was given by 2 The final equivalence query is the correct hypothesis, and thus at most sd counterexamples are received. 3 Maass and Turan <ref> [13, 15] </ref>. The best known result for learning the class box d n was provided by the work of Chen and Maass [7] in which they gave an algorithm making O (d 2 log n) equivalence queries.
Reference: [16] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Lower bound methods and separation results for on-line learning models. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 107-145, </pages> <year> 1992. </year>
Reference-contexts: Combining these two methods of analysis, we get the interesting result that this single algorithm is efficient for either s or d constant. 2 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [14, 15, 16] </ref>. One of the geometric concepts that they studied was the class box d n . <p> One of the geometric concepts that they studied was the class box d n . They showed that if the learner was restricted to only make equivalence queries in which each hypothesis was drawn from box d n then (d log n) queries are needed to achieve exact identification <ref> [11, 16] </ref>. Auer [2] improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. <p> So for s constant this yields an efficient PAC algorithm. We note that either of these PAC algorithms can be applied to the class S n giving efficient PAC algorithms for this class for either d constant or s constant. As discussed by Maass and Turan <ref> [16] </ref> the task of a concept learning algorithm is to provide a "smart" hy 4 pothesis based on the data available. The results from Blumer et. al [4] show that under the PAC model any concise hypothesis that is consistent with the data is "smart enough".
Reference: [17] <author> Leonard Pitt and Leslie G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: To illustrate such a distinction, consider the class of k term DNF formulas where k is constant. (This is the special case of S n where n = 2 and s is constant.) Pitt and Valiant <ref> [17] </ref> have shown that the class of k-term DNF formulas are not exactly learnable in polynomial time without membership queries when all hypothesis must 3 For the case of d constant the recent result of Chen and Homer shows that membership queries are not needed when the need not be from
Reference: [18] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year> <month> 25 </month>
Reference-contexts: Closely related to the problem of learning the union of discretized boxes, is the problem of learning the union of non-discretized boxes in the PAC model <ref> [18] </ref>. <p> A positive counterexample x is an instance such that f (x) = 1 and h (x) = 0. Similarly, a negative counterexample is such that f (x) = 0 and h (x) = 1. Another important learning model is the PAC model introduced by Valiant <ref> [18] </ref>. In this model the learner is presented with labeled examples chosen at random according to an unknown, arbitrary distribution D over the instance space. The learner's goal is to output a hypothesis that with high probability, at least (1 ffi), correctly classifies most of the instance space.
References-found: 18

