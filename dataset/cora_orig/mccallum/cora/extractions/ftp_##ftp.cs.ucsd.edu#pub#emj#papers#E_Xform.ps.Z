URL: ftp://ftp.cs.ucsd.edu/pub/emj/papers/E_Xform.ps.Z
Refering-URL: http://www.cs.ucsd.edu/users/emj/
Root-URL: http://www.cs.ucsd.edu
Title: Algebraic Transformations of Objective Functions  
Author: Eric Mjolsness and Charles Garrett 
Keyword: Objective function, Structured neural network, Analog circuit, Transformation of objective, Fixpoint-preserving transformation, Lagrangian dynam ics, Graph-matching neural net, Winner-take-all neural net.  
Date: November 23, 1994  
Affiliation: Department of Computer Science Yale University  
Abstract: Many neural networks can be derived as optimization dynamics for suitable objective functions. We show that such networks can be designed by repeated transformations of one objective into another with the same fixpoints. We exhibit a collection of algebraic transformations which reduce network cost and increase the set of objective functions that are neurally implementable. The transformations include simplification of products of expressions, functions of one or two expressions, and sparse matrix products (all of which may be interpreted as Legendre transformations); also the minimum and maximum of a set of expressions. These transformations introduce new interneurons which force the network to seek a saddle point rather than a minimum. Other transformations allow control of the network dynamics, by reconciling the Lagrangian formalism with the need for fixpoints. We apply the transformations to simplify a number of structured neural networks, beginning with the standard reduction of the winner-take-all network from O(N 2 ) connections to O(N ). Also susceptible are inexact graph-matching, random dot matching, convolutions and coordinate transformations, and sorting. Simulations show that fixpoint-preserving transformations may be applied repeatedly and elaborately, and the example networks still robustly converge. 
Abstract-found: 1
Intro-found: 1
Reference: <editor> Arrow, K. J., Hurwicz, L., and Uzawa, H., editors (1958). </editor> <title> Studies in Linear and Nonlinear Programming. </title> <publisher> Stanford University Press. </publisher>
Reference-contexts: Indeed reversed neurons are a generalization of their analog Lagrange multiplier neurons, which were found earlier in a non-neural context by Arrow <ref> (Arrow et al., 1958) </ref>. Transformations of Objectives 6 Both reversed neurons and Lagrange multiplier neurons act to maximize an objective which other neurons act to minimize. The difference is that Lagrange multiplier neurons must appear linearly in the objective.
Reference: <author> Benes, V. E. </author> <year> (1965). </year> <title> Mathematical Theory of Connecting Networks and Telephone Traffic. </title> <publisher> Academic Press. </publisher> <address> p. </address> <month> 113. </month>
Reference-contexts: Transformations of Objectives 34 B Butterfly Networks B.1 Back-to-back Butterflies By a recursive induction argument <ref> (Benes, 1965) </ref>, any permutation matrix of size N = 2 n (n an integer) can be expressed by setting switches in two back-to-back butterfly networks independently, as shown in Figure 6.
Reference: <author> Courant, R. and Hilbert, D. </author> <year> (1962). </year> <journal> Methods of Mathematical Physics, </journal> <volume> volume II, </volume> <pages> pages 32-39. </pages> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: further reduced using equation (23) and xe y interactions: m Y jX ff j ! ff It has been pointed out to us (Simic, 1989) that the transformations for F (X) and G (X; Y ), and hence all the transformations discussed so far, can be interpreted as Legendre transformations <ref> (Courant and Hilbert, 1962) </ref>. 2.5 Min and Max The minimum or maximum of a set of expressions fX ff g can be implemented using a winner-take-all network in which each expression is represented by a neuron ff which competes with the others and is constrained to lie between 0 and 1.
Reference: <author> Durbin, R. and Willshaw, D. J. </author> <year> (1987). </year> <title> An analogue approach to the travelling salesman problem using an elastic net method. </title> <journal> Nature, </journal> <volume> 326 </volume> <pages> 689-691. </pages>
Reference-contexts: This objective, and the gradual change in K, is quite similar to that of the "elastic net" approach to the Traveling Salesman Problem <ref> (Durbin and Willshaw, 1987) </ref>.
Reference: <author> Feldman, J. A. </author> <year> (1982). </year> <title> Dynamic connections in neural networks. </title> <journal> Biological Cybernetics, </journal> <volume> 46 </volume> <pages> 27-39. </pages>
Reference-contexts: A major problem in neural network research (c.f. <ref> (Feldman, 1982) </ref>) is to reduce the cost of networks which manipulate graphs. Usually (Hopfield and Tank, 1985; Hopfield and Tank, 1986; Mjolsness et al., 1989a) objectives for such problems involve dense matrices of neurons representing all the possible links in a graph.
Reference: <author> Fox, G. C. and Furmansky, W. </author> <year> (1988). </year> <title> Load balancing loosely synchronous problems with a neural network. </title> <type> Technical Report C 3 P 363B, </type> <institution> California Institute of Technology. </institution>
Reference-contexts: A code which allows order reduction to proceed most advantageously is used in the sorting networks of section 3.4. Another approach is used by Fox and Furmansky <ref> (Fox and Furmansky, 1988) </ref>. Their load-balancing network involves binary encoding, but the network evolution is divided into a number of phases in which different classes of neurons are allowed to vary while most neurons are held constant. <p> Then M i 1 i 2 ;j 1 j 2 = k (1) (2) ~ A j 1 ;k (2) subject to obvious constriants on the A's. 2 A partial exception is the load-balancing network of Fox and Furmansky <ref> (Fox and Furmansky, 1988) </ref>, in which the crucial "histogram" may be understood as a set of reversed linear interneurons which simplify their load-balancing objective. But the result is a virtual neural net, not a statically connected circuit.
Reference: <author> Gindi, G., Gmitro, A., and Parthasarathy, K. </author> <year> (1987). </year> <title> Winner-take-all networks and associative memory: Analysis and optical realization. </title> <booktitle> In Proc. of First International Conference on Neural Networks, </booktitle> <volume> volume vol. III, </volume> <pages> pages 607-614. </pages> <publisher> IEEE. </publisher>
Reference: <author> Grossberg, S. </author> <year> (1988). </year> <title> Nonlinear neural networks: </title> <booktitle> Principles, mechanisms, and architectures. Neural Networks, </booktitle> <volume> 1 </volume> <pages> 17-61. </pages>
Reference: <author> Heinbuch, D. V., </author> <title> editor (1988). CMOS3 Cell Library. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: This provides a kind of reuse of neural net design effort which is fundamentally more flexible than the reuse of modules or components as practiced in electronic design, <ref> (Heinbuch, 1988) </ref>, and in neural net learning, e.g. (Mjolsness et al., 1989b). The transformational approach has also proved useful in VLSI design, e.g. transforming a program into a microprocessor (Martin et al., 1989).
Reference: <author> Hopfield, J. J. </author> <year> (1984). </year> <title> Neurons with graded response have collective computational properties like those of two-state neurons. </title> <booktitle> Proceedings of the National Academy of Sciences USA, </booktitle> <volume> vol. 81 </volume> <pages> 3088-3092. </pages>
Reference-contexts: Then the improved circuit is derived from the modified objective. Often it will be clear from the algebra alone that some savings in number of connections will occur, or that a previously non-implementable objective has been transformed to a form (e.g. single-neuron potentials plus polynomial interactions <ref> (Hopfield, 1984) </ref>) whose minimization may be directly implemented as an analog circuit in a given technology. And one interesting class of tranformations can establish detailed control over the state-space trajectory followed during optimization. <p> Utans (Utans et al., 1989)). A related update scheme for the reversed interneuron is = f (s); _s = r @E=@ = r (X s) (18) which is an alternative to direct steepest-ascent-descent. This dynamics has the distinct advantage of a simple interpretation in terms of analog electrical circuits <ref> (Hopfield, 1984) </ref>. For example, F (X) = X (log X 1) requires a special neuron whose transfer function is logarithmic. This can be provided, approximately and within a mildly restricted domain of the input values, in analog VLSI (Sivilotti et al., 1987). <p> ) 1=p (p large) which we replace by a monotonic function thereof, (1=p) P m j~v ~v m j p , possibly adjust ing * and to compensate. (At this point one could take p = 2 to get a quadratic ex pression, and regenerate the content addressable memory of <ref> (Hopfield, 1984) </ref>.) Then using (17d) we find an implementable neural net objective for large p: ^ E CAM (~v; ~) = m;i i v i m *~v ~ h input + (p 1) X j m j p=(p1) + i Transformations of Objectives 16 2.6 Matrices, Graphs, and Pointers One can
Reference: <author> Hopfield, J. J. and Tank, D. W. </author> <year> (1985). </year> <title> `Neural' computation of decisions in optimization problems. </title> <journal> Biological Cybernetics, </journal> <volume> vol. 52 </volume> <pages> 141-152. </pages>
Reference-contexts: Finally, a discussion follows in section 4. 1.1 Reversed Linear Neurons in the WTA Network Consider the ordinary winner-take-all analog neural network. Following <ref> (Hopfield and Tank, 1985) </ref>, such a network can be obtained from the objective E wta (~v) = 2 X v i 1) 2 + c 2 i X (v i ) (c 1 &gt; 0) (1) Transformations of Objectives 4 where (Hopfield, 1984; Grossberg, 1988) (v i ) = dxg 1
Reference: <author> Hopfield, J. J. and Tank, D. W. </author> <year> (1986). </year> <title> Collective computation with continuous variables. </title> <booktitle> In Disordered Systems and Biological Organization, </booktitle> <pages> pages 155-170. </pages> <note> Springer-Verlag. Transformations of Objectives 40 Koch, </note> <author> C., Marroquin, J., and Yuille, A. </author> <year> (1986). </year> <title> Analog "neuronal" networks in early vision. </title> <booktitle> Proceedings of the National Acadamy of Sciences USA, </booktitle> <pages> 83. </pages>
Reference-contexts: Note that for quick descent, r x r is preferred. Despite the saddle point, and despite the potential numerical sensitivity of exponential and logarithmic transfer functions, the network functions well. A second example is the linear programming network of <ref> (Tank and Hopfield, 1986) </ref> which can also be interpreted as an application of transformation (16) with r ! 1 Transformations of Objectives 12 in the dynamics of equation (18). <p> i A i j D ji j : This network approaches a saddle point rather than a minimum, but the r ! 1 version, _u i =r v = u i A i j X D ji v i B j is exactly the network dynamics of equation (17) of <ref> (Tank and Hopfield, 1986) </ref>. 2.4 Interacting Expressions: Reducing G (X; Y ) Until now we have attempted to reduce all interactions to the forms xy and xyz, but those may not be the only cost-effective few-variable interactions allowed by a given physical technology. <p> In using the Runge-Kutta method for solving equations (44) the stepsize had to be t = :0003 near the starting point, even though eventually it could be increased to .003. Transformations of Objectives 24 3.3 Graph Matching and Quadratic Match Consider the following objective for inexact graph-matching <ref> (Hopfield and Tank, 1986) </ref>, c.f. (von der Malsburg and Bienenstock, 1986): E graph = c 1 P fffiij G fffi g ij M ffi M fij +c 2 ff ( i M ffi 1) 2 + c 2 i ( ff M ffi 1) 2 P + ffi (45) where G
Reference: <author> Luenberger, D. G. </author> <year> (1984). </year> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: This follows immediately from the sign of 2 in (5), which eliminates all local minima. Fortunately there are hyperbolic versions of such efficient optimization procedures as the conjugate gradient method; Luenberger <ref> (Luenberger, 1984) </ref> gives two examples. For finite r , is a delayed version of the sum P i v i 1 and although the network dynamics are different from equation (3), the fixed point is the same.
Reference: <author> Martin, A. J., Burns, S. M., Lee, T. K., Borkovic, D., and Hazewindus, P. J. </author> <year> (1989). </year> <title> The design of an asynchronous microprocessor. </title> <booktitle> In Proc. Decennial Caltech Conference on VLSI, </booktitle> <pages> pages 351-373. </pages> <publisher> MIT press. </publisher>
Reference-contexts: The transformational approach has also proved useful in VLSI design, e.g. transforming a program into a microprocessor <ref> (Martin et al., 1989) </ref>. We have shown that there are algebraic transformations of objective functions which not only preserve the set of fixpoints of the resulting analog neural network, but alter the number and nature of interactions requiring physical connections between different neurons.
Reference: <author> Mjolsness, E. </author> <year> (1987). </year> <title> Control of attention in neural networks. </title> <booktitle> In Proc. of First International Conference on Neural Networks, </booktitle> <volume> volume vol. II, </volume> <pages> pages 567-574. </pages> <publisher> IEEE. </publisher>
Reference-contexts: The constants s i = 1=s i = 1 are used to determine whether a neuron attempts to minimize or maximize E and L. If all s i = 1 then dE=dt 0 and equation (34) is a descent dynamics. Another transformation (proposed and subjected to preliminary experiments in <ref> (Mjolsness, 1987) </ref>) can be used to construct a new objective for the control parameters, q, through their effect on the trajectory v (t): E [~v] ! ^ E [~q] = i (@E=@v i )s i _v i + ^ E cost [~q] = dE=dt + ^ E cost [~q]; if all <p> a new objective for the control parameters, q, through their effect on the trajectory v (t): E [~v] ! ^ E [~q] = i (@E=@v i )s i _v i + ^ E cost [~q] = dE=dt + ^ E cost [~q]; if all s i = 1. (35) In <ref> (Mjolsness, 1987) </ref> the s i = 1 version of transformation (35) (but not (34)) was used to introduce a computational "attention mechanism" for neural nets as follows. Suppose we can only afford to simulate R out of N R neurons at a time in a large net.
Reference: <author> Mjolsness, E., Gindi, G., and Anandan, P. </author> <year> (1989a). </year> <title> Optimization in model matching and perceptual organization. </title> <booktitle> Neural Computation, </booktitle> <pages> 1. </pages>
Reference-contexts: We can reduce this to O (N 2 f). Also if one of G or g is variable, with N nodes in the variable graph and m in the constant graph, as in the "Frameville" networks of <ref> (Mjolsness et al., 1989a) </ref>, and both graphs are represented densely, then the number of synapses is reduced from O (N 2 mf) to O (N 2 m + N mf).
Reference: <author> Mjolsness, E. and Miranker, W. </author> <year> (1990). </year> <note> manuscript in preparation. </note>
Reference-contexts: We discuss two algebraic transformations which can be used to introduce detailed control of the dynamics with which an objective is extremized. One transformation, developed by one of the authors in collaboration with W. Miranker <ref> (Mjolsness and Miranker, 1990) </ref>, replaces an objective E with an associated Lagrangian functional to be extremized in a novel way: E [~v] ! L [ _ ~vj~q] = dt K [ _ ~v; ~vj~q] + dt ; ffiL=ffi _v i (t) = 0: (31) Here ~q is a set of control <p> Note that each transformation may have restrictions on its applicability, in addition to the particular form it matches. We will report experiments only with transformations 1.1, 1.2, 2.2, and 5.1 on this list. Experiments with transformations 6.1 and 6.2 will be reported in a later paper <ref> (Mjolsness and Miranker, 1990) </ref>. The rest are still theoretical. These transformations may be iterated, at the expense of creating interactions between the added variables.
Reference: <author> Mjolsness, E., Sharp, D. H., and Alpert, B. K. </author> <year> (1989b). </year> <title> Scaling, </title> <booktitle> machine learning, and genetic neural nets. Advances in Applied Mathematics, </booktitle> <volume> 10 </volume> <pages> 137-163. </pages>
Reference-contexts: optimization of objectives for dynamic neurons with fixed connections, much of the theory may apply also to "learning" considered as the optimization of dynamic connections in an unstructured net (e.g. (Rumelhart et al., 1986b)) or of some smaller set of parameters which indirectly determine the connections in a structured net <ref> (Mjolsness et al., 1989b) </ref>. In the remainder of this section, we will introduce the ideas by rederiving a well-known network simplification: that of the winner-take-all (WTA) network from O (N 2 ) to O (N ) connections. <p> This provides a kind of reuse of neural net design effort which is fundamentally more flexible than the reuse of modules or components as practiced in electronic design, (Heinbuch, 1988), and in neural net learning, e.g. <ref> (Mjolsness et al., 1989b) </ref>. The transformational approach has also proved useful in VLSI design, e.g. transforming a program into a microprocessor (Martin et al., 1989).
Reference: <author> Moody, J. </author> <year> (1989). </year> <title> Optimal architectures and objective functions for associative memory. </title> <type> Technical Report TR668, </type> <institution> Yale University Department of Computer Science. </institution>
Reference-contexts: When it is applied to simplify the simulation of the WTA equations of motion, we arrive at the standard WTA trick. Moody <ref> (Moody, 1989) </ref> independently discovered an objective function equivalent to (5) and first simulated its delayed Hopfield-style equations of motion (see (4)). But he remained unaware that the neuron acts to maximize rather than minimize the objective, driving the system to a saddle point. <p> Using the transformation (28) yields a CAM with one "grand mother neuron" (representative neuron) per memory, and a WTA net among them (closely related to a network described in <ref> (Moody, 1989) </ref>): ^ E CAM (~v; ~) = m;i i v i m *~v ~ h input +C ( X m 1)C 2 =2+ m X 1 (v i ): Another efficient CAM design (Gindi et al., 1987; Moody, 1989) may be derived by applying transformation (29) to the max expression:
Reference: <author> Platt, J. C. and Barr, A. H. </author> <year> (1987). </year> <title> Constrained differential optimization. </title> <editor> In Ander-son, D. Z., editor, </editor> <booktitle> Neural Information Processing Systems. American Institute of Physics. </booktitle>
Reference: <author> Platt, J. C. and Barr, A. H. </author> <year> (1988). </year> <title> Constraint methods for flexible models. </title> <journal> Computer Graphics, </journal> <volume> 22(4). </volume> <booktitle> Proceedings of SIGGRAPH '88. </booktitle>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and McClelland, J. L. </author> <year> (1986a). </year> <title> A general framework for parallel distributed processing. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <pages> pages 73-74. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: For example, one might regard the class of multi-variable polynomials as the "implementable" interactions in a certain technology <ref> (Rumelhart et al., 1986a) </ref>. (In this case the word "interaction" is usually reserved for a multivariable monomial, out of which polynomials are built by addition of objectives.) Then one might use equation (16) to reduce other, far more general interaction objectives to the implementable form.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986b). </year> <title> Learning internal representations by error propagation. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <pages> pages 318-362. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: Although we use the terminology appropriate to the optimization of objectives for dynamic neurons with fixed connections, much of the theory may apply also to "learning" considered as the optimization of dynamic connections in an unstructured net (e.g. <ref> (Rumelhart et al., 1986b) </ref>) or of some smaller set of parameters which indirectly determine the connections in a structured net (Mjolsness et al., 1989b).
Reference: <author> Simic, P. </author> <year> (1989). </year> <type> Personal communication. </type> <note> Transformations of Objectives 41 Sivilotti, </note> <author> M. A., Mahowald, M. A., and Mead, C. A. </author> <year> (1987). </year> <title> Real-time visual computations using analog CMOS processing arrays. </title> <booktitle> In Advanced Research in VLSI: Proceedings of the 1987 Stanford Conference. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: The fixpoint value of is Q at which point the steepest-descent input to v i is =v i = Q A product of expressions could be further reduced using equation (23) and xe y interactions: m Y jX ff j ! ff It has been pointed out to us <ref> (Simic, 1989) </ref> that the transformations for F (X) and G (X; Y ), and hence all the transformations discussed so far, can be interpreted as Legendre transformations (Courant and Hilbert, 1962). 2.5 Min and Max The minimum or maximum of a set of expressions fX ff g can be implemented using
Reference: <author> Tank, D. W. and Hopfield, J. J. </author> <year> (1986). </year> <title> Simple `neural' optimization networks: An a/d converter, signal decision circuit, and a linear programming circuit. </title> <journal> IEEE Transactions on Circuits and Systems, CAS-33. </journal>
Reference-contexts: Note that for quick descent, r x r is preferred. Despite the saddle point, and despite the potential numerical sensitivity of exponential and logarithmic transfer functions, the network functions well. A second example is the linear programming network of <ref> (Tank and Hopfield, 1986) </ref> which can also be interpreted as an application of transformation (16) with r ! 1 Transformations of Objectives 12 in the dynamics of equation (18). <p> i A i j D ji j : This network approaches a saddle point rather than a minimum, but the r ! 1 version, _u i =r v = u i A i j X D ji v i B j is exactly the network dynamics of equation (17) of <ref> (Tank and Hopfield, 1986) </ref>. 2.4 Interacting Expressions: Reducing G (X; Y ) Until now we have attempted to reduce all interactions to the forms xy and xyz, but those may not be the only cost-effective few-variable interactions allowed by a given physical technology. <p> In using the Runge-Kutta method for solving equations (44) the stepsize had to be t = :0003 near the starting point, even though eventually it could be increased to .003. Transformations of Objectives 24 3.3 Graph Matching and Quadratic Match Consider the following objective for inexact graph-matching <ref> (Hopfield and Tank, 1986) </ref>, c.f. (von der Malsburg and Bienenstock, 1986): E graph = c 1 P fffiij G fffi g ij M ffi M fij +c 2 ff ( i M ffi 1) 2 + c 2 i ( ff M ffi 1) 2 P + ffi (45) where G

References-found: 25

