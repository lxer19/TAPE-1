URL: http://drl.cs.uiuc.edu/pubs/super94.ps
Refering-URL: http://drl.cs.uiuc.edu/panda/publications.html
Root-URL: http://www.cs.uiuc.edu
Email: fseamons,winslettg@cs.uiuc.edu  
Title: An Efficient Abstract Interface for Multidimensional Array I/O  
Author: Kent E. Seamons and Marianne Winslett 
Address: Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: Our research seeks to provide scientific programmers with simpler, more abstract interfaces for accessing persistent multidimensional arrays, and to produce advanced i/o libraries supporting more efficient layout alternatives for these arrays on disk and in main memory. We report on our experience to date applying these techniques to applications in computational fluid dynamics in the areas of checkpoint/restart, output data, and visualization. In the applications we have studied, we find that a simple, abstract interface can be used to insulate programmers from physical storage implementation details, while providing improved i/o performance at the same time. For example, we found that the use of "chunked" physical schemas for arrays gave approximately a factor of 10 improvement in time step output performance on the Intel iPSC/860. 
Abstract-found: 1
Intro-found: 1
Reference: [Bell87] <author> J. L. Bell, G. S. Patterson, Jr., </author> <title> Data Organization in Large Numerical Computations, </title> <journal> The Journal of Supercomputing, </journal> <volume> Volume 1, Number 1, </volume> <year> 1987. </year>
Reference-contexts: 1 Introduction Multidimensional arrays are a fundamental data type in scientific computing and are used extensively in Grand Challenge applications <ref> [Bell87] </ref>. Often these arrays are persistent, i.e., they outlive the invocation of the program that created them. It is critical that these applications have easy access to efficient facilities that manage multidimensional array data on secondary storage.
Reference: [Bordawekar93] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudary, </author> <title> Design and Evaluation of Primitives for Parallel I/O, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: Although that performance level is satisfactory for a rare but flexible restart operation, alternative strategies such as those proposed in <ref> [Bordawekar93] </ref> will be important in more read-intensive applications. 2.2 Output data For time dependent (unsteady) flow fields, the flow solver outputs data at selected intervals over time. Time step data are output more frequently than checkpoint data, so performance is more critical for time step data. <p> The Vesta Parallel File System is designed to run on the Vulcan multicomputer at the IBM T. J. Watson Research Center [Corbett93]. It provides a user-definable view of parallel files that is applicable to array data and upon which a library such as ours can be built. <ref> [Bordawekar93] </ref> describes run-time primitives to support a two-phase access strategy for conducting parallel i/o. Such a facility is useful, although it is not needed so far in the write-intensive applications we studied. [Galbreath93] reports on experiences with parallel applications at Argonne National Laboratory.
Reference: [Brezany92] <author> Peter Brezany, Michael Gerndt, Piyush Mehrotra, Hans Zima, </author> <title> Concurrent File Operations in a High Performance FORTRAN, </title> <booktitle> Proceedings of Supercomputing '92, </booktitle> <pages> pages 230-238, </pages> <year> 1992. </year>
Reference-contexts: Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [HPF93, Fox90, Hiranandani92, Brezany92] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data layout and problem decomposition across the processors that is appropriate for the computation. <p> In fact, some of the optimization logic we plan to include in our library may find its way into compilers that, for example, automatically create problem decompositions for out-of-core solvers. <ref> [Brezany92] </ref> reports timings on an Intel iPSC/860 for writing distributed array chunks directly to a file rather than in traditional order. In their work, this is supported at the compiler level.
Reference: [DeWitt92] <author> David DeWitt, Jeffrey Naughton, Dono--van Schneider, S. Seshadri, </author> <title> Practical Skew Handling in Parallel Joins, </title> <booktitle> Proceedings of the 18th VLDB Conference, </booktitle> <year> 1992. </year>
Reference-contexts: HDF will very soon provide support for chunked physical schemas, with our experiments to guide the development. Database researchers have studied the use of parallelism in the context of traditional DBMS operations, such as joins <ref> [DeWitt92] </ref> and range searching [Li92]. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [Maier93]. A few commercial DBMSes support multidimensional arrays, including Interbase, Orion, and Stratum.
Reference: [Corbett93] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, Sandra Johnson Baylor, </author> <title> Parallel Access to Files in the Vesta File System, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year>
Reference-contexts: Our work shows the benefits of storing chunks using real-world applications, and we also illustrate how such a storage structure has benefits to post-processing applications. The Vesta Parallel File System is designed to run on the Vulcan multicomputer at the IBM T. J. Watson Research Center <ref> [Corbett93] </ref>. It provides a user-definable view of parallel files that is applicable to array data and upon which a library such as ours can be built. [Bordawekar93] describes run-time primitives to support a two-phase access strategy for conducting parallel i/o.
Reference: [Crockett89] <author> Thomas W. Crockett, </author> <title> File Concepts for Parallel I/O, </title> <booktitle> Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference: [Fox90] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Tech Report TR 90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [HPF93, Fox90, Hiranandani92, Brezany92] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data layout and problem decomposition across the processors that is appropriate for the computation.
Reference: [French91] <author> James C. French, Terrence W. Pratt, and Mriganka Das, </author> <title> Performance Measurement of a Parallel Input/Output System for the Intel iPSC/2 Hypercube, </title> <booktitle> Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> San Diego, CA, </address> <month> May 21-24, </month> <year> 1991, </year> <pages> pages 178-187. </pages>
Reference: [Globus93] <author> Al Globus, </author> <title> C++ Class Library Data Management for Scientific Visualization, </title> <type> Technical Report RNR-93-006, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, March 3, </address> <year> 1993. </year>
Reference-contexts: Under the first approach, the entire array from each time step is read into memory. The second approach uses memory mapped files and the virtual memory page faulting features of the operating system to read into memory only those pages that the application actually needs <ref> [Globus93] </ref>. We have constructed libraries that support both types of i/o. In the first case, an entire chunked array can be read from disk to a traditionally ordered array in memory.
Reference: [Galbreath93] <author> N. Galbreath, W. Gropp, and D. Levine, </author> <title> Applications-Driven Parallel I/O, </title> <booktitle> Proceeding of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: The original checkpoint/restart facility is implemented to output multiple multidimensional arrays to disk with each processor storing each subar-ray assigned to that processor in a separate file and restarting with the same number of processors. This is a common checkpoint strategy <ref> [Galbreath93] </ref>. The advantages of this approach are simple coding and reasonable performance. The disadvantages are the inconvenience of managing large numbers of files when many processors and/or subarrays are involved in the computation and the limitation of restarting with the same number of processors and processor mesh configuration. <p> Such a facility is useful, although it is not needed so far in the write-intensive applications we studied. <ref> [Galbreath93] </ref> reports on experiences with parallel applications at Argonne National Laboratory. Like ours, and in contrast to many of the efforts discussed above, their work emphasizes the value of abstractions. The interfaces we propose are at a more abstract level than those of [Galbreath93]. <p> far in the write-intensive applications we studied. <ref> [Galbreath93] </ref> reports on experiences with parallel applications at Argonne National Laboratory. Like ours, and in contrast to many of the efforts discussed above, their work emphasizes the value of abstractions. The interfaces we propose are at a more abstract level than those of [Galbreath93]. We decouple the notions of file and array and do not require an array to be stored in a single file.
Reference: [Hiranandani92] <author> S. Hiranandani, K. Kennedy, and C. Tseng, </author> <title> Compiling Fortran D for MIMD Distributed-Memory Machines, </title> <journal> Communications of the ACM, </journal> <volume> Volume 35, Number 8, </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [HPF93, Fox90, Hiranandani92, Brezany92] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data layout and problem decomposition across the processors that is appropriate for the computation.
Reference: [HDF94] <institution> NCSA HDF Reference Manual, </institution> <note> Version 3.3, </note> <institution> National Center for Supercomputing Applications, University of Illinois, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Our experience with scientific applications shows that multidimensional arrays are generally stored on disk using traditional array ordering, whether using file format systems <ref> [HDF94, NetCDF91] </ref>, database management systems, or hand-coded persistence. Traditional array ordering on disk is satisfactory for some applications. It is ideal for applications that need to access an entire persistent array that fits in main memory. <p> Until fairly recently, scientists had only flat files for storing data. Now several file format systems in widespread use support storing multidimensional array data. Hierarchical Data Format (HDF) <ref> [HDF94] </ref> is a self-describing, machine independent file format system developed at NCSA that supports storing multiple objects in a file. HDF was designated by NASA as the preferred file format for EOSDIS version 0.
Reference: [HPF93] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification Version 1.0, </title> <type> Technical Report CRPC-TR92225, CRPC, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [HPF93, Fox90, Hiranandani92, Brezany92] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data layout and problem decomposition across the processors that is appropriate for the computation.
Reference: [Li92] <author> Jianzhong Li, Jaideep Srivastava, Doron Rotem, CMD: </author> <title> A Multidimensional Declus-tering Method for Parallel Database Systems, </title> <booktitle> Proceedings of the 18th VLDB Conference, </booktitle> <year> 1992. </year>
Reference-contexts: HDF will very soon provide support for chunked physical schemas, with our experiments to guide the development. Database researchers have studied the use of parallelism in the context of traditional DBMS operations, such as joins [DeWitt92] and range searching <ref> [Li92] </ref>. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [Maier93]. A few commercial DBMSes support multidimensional arrays, including Interbase, Orion, and Stratum.
Reference: [Maier93] <author> David Maier and Bennet Vance, </author> <title> A Call to Order, </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Principles of Database Systems, </booktitle> <address> Washington D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Database researchers have studied the use of parallelism in the context of traditional DBMS operations, such as joins [DeWitt92] and range searching [Li92]. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk <ref> [Maier93] </ref>. A few commercial DBMSes support multidimensional arrays, including Interbase, Orion, and Stratum. Like the file format systems described earlier, these systems organize arrays in traditional order on disk and provide an interface that supports reading an entire array or subarray into memory from disk.
Reference: [NetCDF91] <author> NetCDF User's Guide, </author> <note> Version 2.0, </note> <institution> Unidata Program Center, University Corporation for Atmospheric Research, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Our experience with scientific applications shows that multidimensional arrays are generally stored on disk using traditional array ordering, whether using file format systems <ref> [HDF94, NetCDF91] </ref>, database management systems, or hand-coded persistence. Traditional array ordering on disk is satisfactory for some applications. It is ideal for applications that need to access an entire persistent array that fits in main memory. <p> Hierarchical Data Format (HDF) [HDF94] is a self-describing, machine independent file format system developed at NCSA that supports storing multiple objects in a file. HDF was designated by NASA as the preferred file format for EOSDIS version 0. NetCDF <ref> [NetCDF91] </ref> is an i/o library that is also used to store scientific information in self-describing, machine independent files. HDF was recently enhanced to support the NetCDF interface. Both HDF and NetCDF organize fixed size arrays using traditional array ordering on disk. They both support unlimited sized arrays along one dimension.
Reference: [Nitzberg92] <author> Bill Nitzberg, </author> <title> Performance of the iPSC/860 Concurrent File System, </title> <type> Technical Report RND-92-020, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: The system consists of 128 compute nodes, 10 i/o nodes with 1 Mb for buffering on each node, and 10 SCSI disks with files striped at the 4 kb block level. <ref> [Nitzberg92] </ref> contains a complete description of the iPSC/860 at NAS. <p> Our non-interleaved arrays are output by interleaving the writes from multiple processors and our interleaved arrays are output by each processor writing contiguously in the file. The opposite terminology was used in previous parallel i/o studies <ref> [Nitzberg92] </ref>, but we prefer our terminology because it adopts a data-centric view and thus applies to the same data file in both parallel and sequential environments. chunk in a separate file, as was done in the original implementation. <p> This allows each i/o node to output a contiguous block of data when the buffer is filled. This characteristic can also be detected in the performance figures reported in <ref> [Nitzberg92] </ref>, where optimal performance for a similar output pattern occurred for data that fit in the buffer and performance degraded when the contiguous data exceeded 10 Mb.
Reference: [Pierce93] <author> Paul Pierce, </author> <title> A Concurrent File System for a Highly Parallel Mass Storage Subsystem, </title> <booktitle> Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, </address> <month> March </month> <year> 1989. </year>
Reference: [Ryan93] <author> J. S. Ryan and S. K. Weeratunga, </author> <title> Parallel Computation of 3-D Navier Stokes Flow-fields for Supersonic Vehicles, AIAA Paper 93-0064, </title> <booktitle> 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <address> Reno, NV. </address> <year> 1993. </year>
Reference-contexts: For checkpoint/restart and output data, we experimented with the i/o routines taken from a flow solver that uses an overset grid method to solve time dependent (unsteady) flow fields. Our primary interests are in the data management aspects of the flow solver. <ref> [Ryan93] </ref> contains algorithmic details of the flow solver. The flow solver is an SPMD style program written in Fortran using explicit message passing.
Reference: [Sarawagi94] <author> Sunita Sarawagi and Michael Stone-braker, </author> <title> Efficient Organization of Large Multidimensional Arrays, </title> <booktitle> Proceedings of the 10th International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1994. </year>
Reference-contexts: There is research in progress to enhance the POSTGRES DBMS to support multidimensional arrays and store multidimensional arrays as chunks <ref> [Sarawagi94] </ref>. The POSTGRES work focuses on read-only applications that can take advantage of the query capabilities of a DBMS, while our work to date has examined write-intensive applications on massively parallel machines and scientific visualization applications.
Reference: [Seamons94] <author> K. E. Seamons and M. Winslett, </author> <title> Physical Schemas for Large Multidimensional Arrays in Scientific Computing Applications, </title> <booktitle> Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <address> Charlottesville, Vir-ginia, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: It is critical that these applications have easy access to efficient facilities that manage multidimensional array data on secondary storage. Our research seeks to provide simpler, more abstract interfaces for i/o of scientific array data <ref> [Seamons94] </ref>, and to produce advanced i/o libraries supporting more efficient layout alternatives for multidimensional arrays on disk and in main memory. Our work targets both massively parallel architectures and also ordinary workstations. <p> The interested reader can examine them on-line at URL http://bunny.cs.uiuc.edu/CADR/winslett/arrays.html or in <ref> [Seamons94] </ref>. 3 Logical contiguity often implies physical contiguity under CFS, because CFS supports allocations of physical extents where possible.
Reference: [Walatka92] <author> P. Walatka, P. Buning, L. Pierce, and P. Elson. </author> <title> PLOT3D User's Manual, </title> <type> NASA Technical Memorandum 101067, </type> <month> July </month> <year> 1992. </year>
Reference-contexts: The answer is that the physical schema is a standard format that has been used for CFD data at NAS for many years <ref> [Walatka92] </ref>. We believe that standards should be set at the level of interfaces, not at the level of physical schemas.
References-found: 22

