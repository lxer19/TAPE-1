URL: http://www.cs.cornell.edu/tve/papers/sc97.ps
Refering-URL: 
Root-URL: 
Title: Evaluating the Performance Limitations of MPMD Communication  
Author: ChiChao Chang Grzegorz Czajkowski Thorsten von Eicken and Carl Kesselman 
Address: Ithaca, NY 14853  Marina Del Rey, CA 90292  
Affiliation: Department of Computer Science Cornell University  Information Sciences Institute University of Southern California  
Abstract: This paper investigates the fundamental limitations of MPMD communication using a case study of two parallel programming languages, Compositional C++ (CC++) and Split-C, that provide support for a global name space. To establish a common comparison basis, our implementation of CC++ was developed to use MRPC, a RPC system optimized for MPMD parallel computing and based on Active Messages. Basic RPC performance in CC++ is within a factor of two from those of Split-C and other messaging layers. CC++ applications perform within a factor of two to six from comparable Split-C versions, which represent an order of magnitude improvement over previous CC++ implementations. The results suggest that RPC-based communication can be used effectively in many high-performance MPMD parallel applications. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Birrel and G. Nelson. </author> <title> Implementing Remote Procedure Calls. </title> <journal> In ACM Transactions on Computer Systems (TOCS), </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year> <month> 10 </month>
Reference: 2. <author> A. Birrel, G. Nelson, S. Owicki, and E. Wobber. </author> <title> Network Objects. </title> <booktitle> In Proceedings of the 14 th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year>
Reference: 3. <author> R. Blumofe, C. Joerg, B. Kuszmaul, C. Leiserson, K. Randall, and Y. Zhou. Cilk: </author> <title> An Efficient Multithreaded Runtime System. </title> <booktitle> In Proceedings of 5 th ACM Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference: 4. <author> K. Chandy and C. Kesselman. </author> <title> Compositional C++: Compositional Parallel Programming. </title> <booktitle> In Proceedings of 6 th International Workshop in Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 124-144, </pages> <year> 1993. </year>
Reference-contexts: This approach is well suited for applications that exhibit irregular or unknown communication patterns, or that can benefit from a client-server type of setting. Software engineering is made easier due to the modularity of the code, which promotes code reuse and the ability to compose programs <ref> [4] </ref>. In its most general form, an RPC specifies the data that is to be transferred and the remote operation that is to be performed with the data. <p> However, these systems generally sacrifice the elegance (and IDL support) of an RPC system, and require that the receiver know when to expect incoming communication, limiting the range of MPMD applications that can be expressed conveniently. High-level MPMD languages (e.g. Mentat [13], CC++ <ref> [4] </ref>, and Fortran-M [9]) and runtime systems (e.g. Nexus [11]) support some combination of dynamic task creation, load balancing, global name space, concurrency, and heterogeneity.
Reference: 5. <author> CC. Chang, G. Czajkowski, and T. von Eicken. MRPC: </author> <title> A High Performance RPC System for MPMD Parallel Computing. Submitted to Software: </title> <journal> Practice and Experience, special issue on Parallel and Distributed Operating Systems, </journal> <month> June </month> <year> 1997. </year>
Reference-contexts: Because of the difficulty in distinguishing the fundamental MPMD communication overheads from those introduced by software engineering constraints, a new implementation of CC++ was developed to use MRPC <ref> [5] </ref>, an RPC system designed and optimized for MPMD parallel computing. MRPC combines the efficient control and data transfer provided by AM, a simplified SPMD RPC mechanism, with a minimal multithreaded runtime system that extends AM with the features required to support MPMD. <p> The communication module is layered on top of AM and uses a custom, non-preemptive threads package. We present a brief overview of the MRPC implementation in this section a thorough description can be found in <ref> [5] </ref>. CC++ has been ported to use MRPC. It provides MRPC with stub generation and a global name space. The system is composed of a front-end (CC++ to C++) translator and a back-end C++ compiler.
Reference: 6. <author> CC. Chang, G. Czajkowski, C. Hawblitzel, and T. von Eicken. </author> <title> Low-Latency Communication on the IBM RISC System/6000 SP. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> Pittsburgh, PA, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: This optimization has not been incorporated into the current implementation. Due to the high cost of software interrupts on message arrival on the IBM SP, message reception is based on polling that occurs on a node every time a message is sent <ref> [6] </ref>.
Reference: 7. <author> D. Culler, S. Goldstein, K. Schauser, and T. von Eicken. </author> <title> TAM A Compiler Controlled Threaded Abstract Machine. </title> <journal> Journal of Parallel and Distributed Computing (JPDC), </journal> <month> June </month> <year> 1993. </year>
Reference: 8. <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumeta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Due to the need for crossing program domains, for asynchronously detecting incoming communication, and for potentially spawning new threads, the communication overheads in these systems are often prohibitively high for a multi-computer. As a result, systems based on an SPMD model (e.g. Split-C <ref> [8] </ref> and CRL [14]) have a significant performance advantage over 2 MPMD-based ones and are preferred for parallel application development. This paper investigates the feasibility of high-performance parallel computing using an MPMD model by analyzing the fundamental performance limitations in communication.
Reference: 9. <author> I. Foster and K. Chandy. Fortran-M: </author> <title> A Language for Modular Parallel Programming. </title> <journal> Journal of Parallel and Distributed Computing (JPDC), </journal> <volume> 25(1), </volume> <year> 1994. </year>
Reference-contexts: However, these systems generally sacrifice the elegance (and IDL support) of an RPC system, and require that the receiver know when to expect incoming communication, limiting the range of MPMD applications that can be expressed conveniently. High-level MPMD languages (e.g. Mentat [13], CC++ [4], and Fortran-M <ref> [9] </ref>) and runtime systems (e.g. Nexus [11]) support some combination of dynamic task creation, load balancing, global name space, concurrency, and heterogeneity.
Reference: 10. <author> I. Foster, J. Geisler, S. Tuecke, and C. Kesselman. </author> <title> Multimethod Communication for High-Performance Metacomputing. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> Pittsburgh, PA, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: The result of the operation is sent back to the callers address space through stubs, which then resumes computation. From a software-engineering point of view, RPC is a widely accepted communication abstraction for an MPMD environment <ref> [10] </ref>. In multi-computers, lower-level messaging systems such as MPI [27] and PVM [23] can be used for MPMD programming and typically achieve good performance.
Reference: 11. <author> I. Foster, C. Kesselman, and S. Tuecke. </author> <title> The Nexus Approach for Integrating Multithreading and Communication. </title> <journal> Journal of Parallel and Distributed Computing (JPDC), </journal> <volume> 37, </volume> <year> 1996. </year>
Reference-contexts: High-level MPMD languages (e.g. Mentat [13], CC++ [4], and Fortran-M [9]) and runtime systems (e.g. Nexus <ref> [11] </ref>) support some combination of dynamic task creation, load balancing, global name space, concurrency, and heterogeneity. Due to the need for crossing program domains, for asynchronously detecting incoming communication, and for potentially spawning new threads, the communication overheads in these systems are often prohibitively high for a multi-computer. <p> But OAM assumes an SPMD model and does not specifically address the communication bottlenecks when that assumption is no longer valid. Nexus also provides a framework for integrating threads with communication <ref> [11] </ref>, but does not investigate the performance impact in applications. A large amount of literature [7,12,15,20,21] describes sophisticated compiler and runtime schemes that reduce the cost of thread management in languages that support fine-grain concurrency.
Reference: 12. <author> S. Goldstein, K. Schauser, and D. Culler. </author> <title> Lazy Threads, Stacklets, and Synchronizers: Enabling Primitives for Compiling Parallel Languages. </title> <booktitle> In 3 rd Workshop on Languages, Compilers, and Runtime Systems for Scalable Computers, </booktitle> <year> 1995. </year>
Reference-contexts: Most threaded SPMD systems [3,7] minimize the threading costs by making threads run to completion to eliminate context switches, by performing custom stack management (e.g. using stacklets <ref> [12] </ref> or spaghetti stacks), or by reducing synchronization costs through custom code generation [12]. Split-C takes an even more radical approach offering only a single computation thread and relies on split-phase remote accesses to tolerate latencies. <p> Most threaded SPMD systems [3,7] minimize the threading costs by making threads run to completion to eliminate context switches, by performing custom stack management (e.g. using stacklets <ref> [12] </ref> or spaghetti stacks), or by reducing synchronization costs through custom code generation [12]. Split-C takes an even more radical approach offering only a single computation thread and relies on split-phase remote accesses to tolerate latencies. Message Reception: A critical component of the communication latency is the queuing delay incurred by messages at the receiving end before they are serviced.
Reference: 13. <author> A. Grimshaw. </author> <title> An Introduction to Parallel ObjectOriented Programming with Mentat, </title> <type> Technical Report 91-07, </type> <institution> University of Virginia, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: However, these systems generally sacrifice the elegance (and IDL support) of an RPC system, and require that the receiver know when to expect incoming communication, limiting the range of MPMD applications that can be expressed conveniently. High-level MPMD languages (e.g. Mentat <ref> [13] </ref>, CC++ [4], and Fortran-M [9]) and runtime systems (e.g. Nexus [11]) support some combination of dynamic task creation, load balancing, global name space, concurrency, and heterogeneity.
Reference: 14. <author> K. Johnson, M. Kaashoek, and D. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 15 th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Cooper Mountain, CO, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Due to the need for crossing program domains, for asynchronously detecting incoming communication, and for potentially spawning new threads, the communication overheads in these systems are often prohibitively high for a multi-computer. As a result, systems based on an SPMD model (e.g. Split-C [8] and CRL <ref> [14] </ref>) have a significant performance advantage over 2 MPMD-based ones and are preferred for parallel application development. This paper investigates the feasibility of high-performance parallel computing using an MPMD model by analyzing the fundamental performance limitations in communication.
Reference: 15. <author> V. Karamcheti and A. Chien. </author> <title> Concert Efficient Runtime Support for Concurrent ObjectOriented Programming Languages on Stock Hardware. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference: 16. <author> C. Lee, C. Kesselman, and S. Schwab. </author> <title> Near-Real-Time Satellite Image Processing: Meta-Computing in CC++. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 16(4), </volume> <month> July </month> <year> 1996. </year>
Reference-contexts: This analysis is based on the implementations of two representative languages, CC++ and Split-C, running on the IBM RS/6000 SP (a.k.a. SP2). CC++ offers an MPMD programming model with RPC as the primary communication abstraction and has been used in meta-computing applications <ref> [16] </ref>. Split-C is built on Active Messages (AM) [26] and has been widely used in high-performance parallel computing research [6,17]. This work focuses on a homogeneous environment in order to isolate the inherent costs of MPMD over SPMD communication.
Reference: 17. <author> B-H. Lim, CC. Chang, G. Czajkowski, and T. von Eicken. </author> <title> Performance Implications of Communication Mechanisms in All-Software Global Address Space Systems. </title> <booktitle> In Proceedings of 6 th ACM Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Las Vegas, NV, </address> <note> June 1997 (to appear). </note>
Reference-contexts: of the null RPC (calling a null method of a remote object referenced by a global pointer and waiting for its completion): 0-Word Simple (no thread switches at the caller or callee), 0-Word, 1-Word, 2 3 The CC++ version of these applications is heavily based on the original Split-C implementations <ref> [17] </ref> to allow for a fair comparison. Word (each with a thread switch at the caller side only), 0-Word Threaded 4 (thread switches at both caller and callee sides), and 0-Word Atomic (0-Word Threaded with the method executed atomically).
Reference: 18. <author> N. Madsen. </author> <title> Divergence Preserving Discrete Surface Integral Methods for Maxwells Curl Equations Using Non-Orthogonal Unstructured Grids. </title> <type> Technical Report 92-04, </type> <institution> RIACS, </institution> <month> February </month> <year> 1992. </year>
Reference: 19. <author> S. OMalley, T. Proebsting, and A. Montz. </author> <title> USC: A Universal Stub Compiler. </title> <booktitle> In Proceedings of the Symposium on Communication Architectures and Protocols (SIGCOMM), </booktitle> <address> London, UK, </address> <month> August </month> <year> 1994. </year>
Reference: 20. <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs. </title> <journal> In IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: // GP 2-Word Read lx = *gpY; // GP 2-Word Write *gpY = lx; // Bulk Read lA = gpObj-&gt;get (gpA); // Bulk Write gpObj-&gt;put (lA, gpA); // Prefetch parfor (i = 0; i &lt; 20; i++) lx = *gpY; // Split-C definitions double lx; double *global gpY; double lA <ref> [20] </ref>; void *global gpA; // 0-Word N/A // 1-Word N/A // 2-Word N/A // 0-Word Atomic RPC atomic (foo, 0); // GP 2-Word Read lx = *gpY; // GP 2-Word Write *gpY = lx; // Bulk Read bulk_read (&lA,gpA,20*sizeof (double)); // Bulk Write bulk_write (gpA,&lA,20*sizeof (double)); // Prefetch for (i =
Reference: 21. <author> J. Plevyak, V. Karamcheti, X. Zhang, and A. Chien. </author> <title> A Hybrid Execution Model for Fine-Grained Languages on Distributed Memory Multicomputers. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference: 22. <author> J. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The fraction of edges that cross processor boundaries is varied from 10% to 100% in order to change the computation to communication ratio. Water: is an N-body molecular dynamics application taken from the SPLASH benchmark suite <ref> [22] </ref> that computes the forces and energies of a system of water molecules. The computation iterates over a number of steps, and each of which involves computing the intra- and inter-molecular forces for molecules contained in a cubical box, which runs in O (N 2 ) time. <p> Both versions are run with inputs of 64 and 512 molecules distributed over 4 processors. Blocked LU Decomposition: implements LU factorization of a dense matrix as described in the SPLASH benchmark suite <ref> [22] </ref>. The matrix is divided into blocks distributed among processors. Every step comprises three substeps: first, the pivot block (I,I) is factored by its owner; second, all processors which have blocks in the Ith row or Ith column obtain the updated pivot block; third, all internal blocks are updated.
Reference: 23. <author> V. Sunderam, G. Geist, J. Dongarra, and R.Manchek. </author> <title> The PVM Concurrent Computing System: Evolution, Experiences, and Trends. </title> <journal> Parallel Computing, </journal> <volume> 20(4), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: The result of the operation is sent back to the callers address space through stubs, which then resumes computation. From a software-engineering point of view, RPC is a widely accepted communication abstraction for an MPMD environment [10]. In multi-computers, lower-level messaging systems such as MPI [27] and PVM <ref> [23] </ref> can be used for MPMD programming and typically achieve good performance. However, these systems generally sacrifice the elegance (and IDL support) of an RPC system, and require that the receiver know when to expect incoming communication, limiting the range of MPMD applications that can be expressed conveniently.
Reference: 24. <author> K. Taura, S. Matsuoka, and A. Yonezawa. </author> <title> An Efficient Implementation Scheme of Concurrent ObjectOriented Languages on Stock Multi-computers. </title> <booktitle> In Proceedings of the 4 th ACM Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference: 25. <author> K. Taura and A. Yonezawa. </author> <title> Fine-Grain Multithreading with Minimal Compiler Support A Cost-Effective Approach to Implementing Efficient Multithreading Languages. </title> <booktitle> In Proceedings of the ACM Conference on Programming Language Design and Implementation (PLDI), </booktitle> <address> Las Vegas, NV, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Our results show that these optimizations could potentially reduce the performance gap between SPMD and MPMD parallel applications by about 10 to 15%. Taura and Yonezawa provide a comprehensive summary of these schemes in <ref> [25] </ref>, and propose a cost-effective, library-based implementation of threads that maps onto the single stack execution model of C. 8 Conclusion This paper investigates the feasibility of high-performance MPMD parallel computing by analyzing the performance limitations of the MPMD communication paradigm.
Reference: 26. <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19 th International Symposium in Computer Architecture (ISCA), </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: SP2). CC++ offers an MPMD programming model with RPC as the primary communication abstraction and has been used in meta-computing applications [16]. Split-C is built on Active Messages (AM) <ref> [26] </ref> and has been widely used in high-performance parallel computing research [6,17]. This work focuses on a homogeneous environment in order to isolate the inherent costs of MPMD over SPMD communication. <p> Moreover, such systems are usually evaluated in isolation. A key performance aspect in the MPMD model is the efficient integration of communication and threads. The simplest form of singlethreaded remote method invocation was introduced by Active Messages <ref> [26] </ref>. Optimistic Active Messages (OAM) [28] augments AM with threads, removing some of the restrictions in AM handlers. To implement a fast RPC, OAM optimistically executes the handler code on the stack the handler is aborted and restarted on a separate thread if it blocks.
Reference: 27. <author> D. Walker and J. Dongarra. </author> <title> MPI: A Standard Message Passing Interface. </title> <booktitle> Supercomputing, </booktitle> <volume> 12(1), </volume> <year> 1996. </year>
Reference-contexts: The result of the operation is sent back to the callers address space through stubs, which then resumes computation. From a software-engineering point of view, RPC is a widely accepted communication abstraction for an MPMD environment [10]. In multi-computers, lower-level messaging systems such as MPI <ref> [27] </ref> and PVM [23] can be used for MPMD programming and typically achieve good performance.
Reference: 28. <author> D. Wallach, W. Hsieh, K. Johnson, M. Kaashoek, and W. Weihl. </author> <title> Optimistic Active Messages: A Mechanism for Scheduling Communication with Computation. </title> <booktitle> In Proceedings of 5 th Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Moreover, such systems are usually evaluated in isolation. A key performance aspect in the MPMD model is the efficient integration of communication and threads. The simplest form of singlethreaded remote method invocation was introduced by Active Messages [26]. Optimistic Active Messages (OAM) <ref> [28] </ref> augments AM with threads, removing some of the restrictions in AM handlers. To implement a fast RPC, OAM optimistically executes the handler code on the stack the handler is aborted and restarted on a separate thread if it blocks.
References-found: 28

