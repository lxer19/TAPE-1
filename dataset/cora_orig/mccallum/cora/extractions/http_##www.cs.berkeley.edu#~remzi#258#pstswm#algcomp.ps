URL: http://www.cs.berkeley.edu/~remzi/258/pstswm/algcomp.ps
Refering-URL: http://www.cs.berkeley.edu/~remzi/258/pstswm/highlevel.html
Root-URL: http://www.cs.berkeley.edu
Title: PARALLEL ALGORITHMS FOR THE SPECTRAL TRANSFORM METHOD  
Author: Ian T. Foster Patrick H. Worley P. O. 
Date: April 1994  
Note: Date Published:  Research was supported by the Atmospheric and Climate Research Division and by the Applied Mathematical Sciences Research Program, both of the Office of Energy Research, U.S. Department of Energy Prepared by the  managed by Martin Marietta Energy Systems, Inc. for the U.S. DEPARTMENT OF ENERGY under Contract No. DE-AC05-84OR21400  
Address: Argonne, IL 60439-4801  Box 2008 Oak Ridge, TN 37831-6367  Oak Ridge, Tennessee 37831  
Affiliation: Engineering Physics and Mathematics Division Mathematical Sciences Section  Argonne National Laboratory Mathematics and Computer Science Division  Oak Ridge National Laboratory Mathematical Sciences Section  Oak Ridge National Laboratory  
Pubnum: ORNL/TM-12507  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. H. Bailey, </author> <title> FFTs in external or hierarchical memory, </title> <editor> J. </editor> <booktitle> Supercomputing, 4 (1990), </booktitle> <pages> pp. 23-45. - 35 </pages> - 
Reference-contexts: In each vertical layer of the physical domain, fields are approximated on an I fi J longitude-latitude grid, where the I longitude grid lines are evenly spaced and the J latitude grid lines are placed at the Gaussian quadrature points f j g in <ref> [1; 1] </ref>. Transforming from physical coordinates to spectral coordinates involves first performing a Fourier transform for each line of constant latitude, generating the values f~ m ( j )g on an M fi J wavenumber-latitude grid that we will refer to as the Fourier grid.
Reference: [2] <author> M. Barnett, R. van de Geijn, S. Gupta, D. G. Payne, L. Shuler, and J. Watts, </author> <title> Interprocessor collective communication library (InterCom), </title> <booktitle> in Proc. Scalable High Performance Computing Conf., </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994, </year> <note> (in press). </note>
Reference: [3] <author> S. Barros and K. </author> <title> T, On the parallelization of global spectral Eulerian shallow-water models, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science: Proceedings of the Fifth ECMWF Workshop on Use of Parallel Processors in Meteorology, </booktitle> <editor> G.-R. Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 36-43. </pages>
Reference-contexts: Because the transpose FFT/transpose LT algorithm also partitions the wavenumber dimension, it also suffers from load imbalance. Since all equipartitions incur the same communication costs in the transpose algorithms, we minimize load imbalance by using the partitioning strategy described by Barros and Kauranne <ref> [3] </ref>. This pairs "short" transforms with "long" transforms in the assignment, and there is no load imbalance when P Y divides (M + 1)=2 evenly.
Reference: [4] <author> W. Bourke, </author> <title> An efficient, one-level, primitive-equation spectral model, </title> <journal> Mon. Wea. Rev., </journal> <volume> 102 (1972), </volume> <pages> pp. 687-701. </pages>
Reference-contexts: in climate models comprises a Fourier transform phase, in which fast Fourier transforms (FFTs) are applied to each latitude of a latitude/longitude grid, and a Legendre transform phase, in which Gaussian quadrature is used to approximate the Legendre transform (LT) applied to each longitude (now wavenumber) of the same grid <ref> [4] </ref>. Efficient parallel FFT and LT algorithms have been the topic of intensive research (e.g., see [16, 25,28,29]).
Reference: [5] <author> G. L. Browning, J. J. Hack, and P. N. Swarztrauber, </author> <title> A comparison of three numerical methods for solving differential equations on the sphere, </title> <journal> Mon. Wea. Rev., </journal> <volume> 117 (1989), </volume> <pages> pp. 1058-1075. </pages>
Reference-contexts: These equations are frequently used to investigate and compare numerical methods because they present many of the difficulties found in simulating the horizontal dynamics in three-dimensional global atmospheric models <ref> [5] </ref>. The algorithms used to solve the shallow water equations via the spectral transform method are similar to those employed in the NCAR Community Climate Model to handle the horizontal dynamics component of the primitive equations [19].
Reference: [6] <author> D. Dent, </author> <title> The ECMWF model on the Cray Y-MP8, in The Dawn of Massively Parallel Processing in Meteorology, </title> <editor> G.-R. Hoffman and D. K. Maretis, eds., </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: Other researchers have examined a transpose approach, in which communication requirements are encapsulated in a matrix transpose operation. This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model <ref> [6] </ref> and in Loft and Sato's data parallel implementation of CCM2 [23]. It has also been explored by Kauranne and Barros [22], Pelz and Stern [26], and Gartel, Joppich, and Schuller [17].
Reference: [7] <author> J. B. Drake, R. E. Flanery, I. T. Foster, J. J. Hack, J. G. Michalakes, R. L. Stevens, D. W. Walker, D. L. Williamson, and P. H. Worley, </author> <title> The message-passing version of the parallel community climate model, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science: Proceedings of the Fifth ECMWF Workshop on Use of Parallel Processors in Meteorology, </booktitle> <editor> G.-R. Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 500-513. </pages>
Reference-contexts: This approach performs twice as many transposes as the transpose FFT/distributed LT algorithm, but can use 5 times more processors without load imbalance. It has been used successfully in the message-passing version of CCM2 <ref> [7] </ref>. 1b. We can avoid the double transpose at the cost of redundant work and some other additional communication by duplicating one field and decomposing over K levels and 3 sets of 3 fields.
Reference: [8] <author> J. B. Drake, I. T. Foster, J. J. Hack, J. G. Michalakes, B. D. Semeraro, B. Toonen, D. L. Williamson, and P. H. Worley, PCCM2: </author> <title> A GCM adapted for scalable parallel computer, </title> <booktitle> in Fifth Symposium on Global Change Studies, </booktitle> <publisher> American Meteorological Society, </publisher> <address> Boston, </address> <year> 1994, </year> <pages> pp. 91-98. </pages>
Reference: [9] <author> A. Dubey, M. Zubair, and C. E. Grosch, </author> <title> A general purpose subroutine for fast Fourier transform on a distributed memory parallel machine, Parallel Computing, (to appear). [10] , Performance of the Intel iPSC/860 and the Ncube 6400 hypercubes, </title> <booktitle> Parallel Computing, 17 (1991), </booktitle> <pages> pp. 1285-1302. </pages>
Reference-contexts: The distributed FFT can also be modified to use log 4 Q stages and can then exploit factors of 4 to reduce computation costs. And it is possible to apply a transpose-like algorithm within the FFT itself <ref> [9] </ref>. These hybrid algorithms can improve performance somewhat in regimes where message startup costs and data volume costs are comparable. However, they place additional requirements on problem size and processor counts. Mesh-based algorithms. We have restricted ourselves to algorithms designed for one-dimensional processor meshes.
Reference: [11] <author> T. H. Dunigan, </author> <title> Communication performance of the Intel Touchstone DELTA Mesh, </title> <type> Tech. Report ORNL/TM-11983, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> December </month> <year> 1991. </year>
Reference: [12] <author> A. Edelman, </author> <title> Optimal matrix transposition and bit reversal on hypercubes: all-to-all personalized communication, </title> <journal> J. Par. Dist. Comp., </journal> <volume> 11 (1991), </volume> <pages> pp. 328-331. </pages>
Reference: [13] <author> J. O. Eklundh, </author> <title> A fast computer method for matrix transposing, </title> <journal> IEEE Trans. Comput., </journal> <volume> C-21 (1972), </volume> <pages> pp. 801-803. - 36 </pages> - 
Reference: [14] <author> I. Foster, W. Gropp, and R. Stevens, </author> <title> The parallel scalability of the spectral transform method, </title> <journal> Mon. Wea. Rev., </journal> <volume> 120 (1992), </volume> <pages> pp. 835-850. </pages>
Reference: [15] <author> I. T. Foster and P. H. Worley, </author> <title> Parallelizing the spectral transform method: A comparison of alternative parallel algorithms, in Parallel Processing for Scientific Computing, </title> <editor> R. F. Sincovec, D. E. Keyes, M. R. Leuze, L. R. Petzold, and D. A. Reed, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, PA, </address> <year> 1993, </year> <pages> pp. 100-107. </pages>
Reference-contexts: In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both. A comprehensive comparison of these algorithms has not previously been attempted. (Both <ref> [15] </ref> and [22] provide a qualitative analysis of some algorithms, but not detailed quantitative results or performance models.) Hence, it is difficult to evaluate the performance tradeoffs that arise when choosing a parallel algorithm for a particular application.
Reference: [16] <author> G. C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker, </author> <title> Solving Problems on Concurrent Processors, </title> <journal> vol. </journal> <volume> 1, </volume> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Efficient parallel FFT and LT algorithms have been the topic of intensive research (e.g., see <ref> [16, 25,28,29] </ref>). <p> This process requires twice as many messages, as indicated in Table 1, but has been shown to be cost effective on some multiprocessors. Bandwidth Limitations. Both the one- and two-block algorithms can be mapped to a hypercube without competition for bandwidth <ref> [16] </ref>. <p> Butterfly Sum. The butterfly sum algorithm is a hybrid of two algorithms [31]. For long vectors, we use a recursive halving algorithm <ref> [16] </ref> that utilizes a butterfly communication pattern like the distributed FFT. Each processor communicates (and sums) D=2 data in the first stage, half as much (D=4) in the second, and so on, so that each processor communicates a total of D (Q 1)=Q data in (log 2 Q) steps.
Reference: [17] <author> U. G artel, W. Joppich, and A. Sch uller, </author> <title> Parallelizing the ECMWF's weather forecast program: The 2D case, </title> <booktitle> Parallel Computing, 19 (1993), </booktitle> <pages> pp. 1413-1426. </pages>
Reference-contexts: This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model [6] and in Loft and Sato's data parallel implementation of CCM2 [23]. It has also been explored by Kauranne and Barros [22], Pelz and Stern [26], and Gartel, Joppich, and Schuller <ref> [17] </ref>. In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both.
Reference: [18] <author> A. Gupta and V. Kumar, </author> <title> The scalability of FFT on parallel computers, </title> <journal> IEEE Trans. Par. Dist. Sys., </journal> <volume> 4 (1993), </volume> <pages> pp. 922-932. </pages>
Reference: [19] <author> J. J. Hack, B. A. Boville, B. P. Briegleb, J. T. Kiehl, P. J. Rasch, and D. L. Williamson, </author> <title> Description of the NCAR Community Climate Model (CCM2), </title> <type> NCAR Tech. </type> <institution> Note NCAR/TN-382+STR, National Center for Atmospheric Research, Boulder, Colo., </institution> <year> 1992. </year>
Reference-contexts: We and colleagues at Argonne and Oak Ridge national laboratories have developed a parallel transform approach based on parallel FFT and quadrature algorithms [14,32,35]; this work has been incorporated in a parallel implementation [7,8] of the National Center for Atmospheric Research (NCAR)'s Community Climate Model (CCM2) <ref> [19] </ref>. Other researchers have examined a transpose approach, in which communication requirements are encapsulated in a matrix transpose operation. This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model [6] and in Loft and Sato's data parallel implementation of CCM2 [23]. <p> The algorithms used to solve the shallow water equations via the spectral transform method are similar to those employed in the NCAR Community Climate Model to handle the horizontal dynamics component of the primitive equations <ref> [19] </ref>. Hence, a model that solves the shallow water equations on multiple (independent) levels during each timestep of the simulation provides a framework in which the performance of CCM2's horizontal dynamics can be studied in isolation from the other aspects of the full model.
Reference: [20] <author> J. J. Hack and R. Jakob, </author> <title> Description of a global shallow water model based on the spectral transform method, </title> <type> NCAR Tech Note NCAR/TN-343+STR, </type> <institution> National Center for Atmospheric Research, Boulder, CO, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: When the spectral transform method is applied in a three-dimensional atmospheric model, the principal data structures are as shown in Fig. 1. P denotes the physical grid, F the Fourier grid, and S the spectral grid. In the shallow water equation code <ref> [20] </ref>, each timestep begins by calculating the nonlinear terms U , V , U , V , and +(U 2 +V 2 )=(2 (1 2 )) on the physical grid. Next, the nonlinear terms and the state variables , ffi, and are Fourier transformed. <p> PSTSWM is a message-passing parallel implementation of the sequential Fortran code STSWM 2.0 <ref> [20] </ref>. STSWM uses the spectral transform method to solve the nonlinear shallow water equations on a rotating sphere; its data structures and implementation are based directly on equivalent structures and algorithms in CCM2.
Reference: [21] <author> S. L. Johnsson and C.-T. Ho, </author> <title> Algorithms for matrix transposition on Boolean N-cube configured ensemble architectures, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 9 (1988), </volume> <pages> pp. 419-454. </pages>
Reference-contexts: For example, the FFT described above can be organized to execute without competition for bandwidth on a hypercube <ref> [21] </ref>. In contrast, on a 1-D mesh of Q processors, each processor generates messages that must traverse 1, 2, ..., 2 q1 hops distant in the q steps of the algorithm [14,18]. The total number of hops traversed by these messages is Q i=0 2 i = Q (Q 1). <p> the two-dimensional logical processor mesh of size P X fi P Y = 2 q fi 2 r = 2 p is mapped into a hypercube of dimension p in such a way that each processor row and column is mapped to a subcube of dimension q and r, respectively <ref> [21] </ref>. Hence, performance analysis reduces to the problem of determining the cost of an FFT or LT in a hypercube. On a 2-D mesh computer, we assume that the P X fi P Y logical processor mesh is mapped to an equivalent physical mesh.
Reference: [22] <author> T. Kauranne and S. Barros, </author> <title> Scalability estimates of parallel spectral atmospheric models, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science: Proceedings of the Fifth ECMWF Workshop on Use of Parallel Processors in Meteorology, </booktitle> <editor> G.-R. Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 312-328. </pages>
Reference-contexts: This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model [6] and in Loft and Sato's data parallel implementation of CCM2 [23]. It has also been explored by Kauranne and Barros <ref> [22] </ref>, Pelz and Stern [26], and Gartel, Joppich, and Schuller [17]. In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both. A comprehensive comparison of these algorithms has not previously been attempted. (Both [15] and [22] provide a qualitative analysis of <p> explored by Kauranne and Barros <ref> [22] </ref>, Pelz and Stern [26], and Gartel, Joppich, and Schuller [17]. In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both. A comprehensive comparison of these algorithms has not previously been attempted. (Both [15] and [22] provide a qualitative analysis of some algorithms, but not detailed quantitative results or performance models.) Hence, it is difficult to evaluate the performance tradeoffs that arise when choosing a parallel algorithm for a particular application. <p> As K can be significantly smaller than I, this restriction is limiting for the transpose algorithms. One approach to mitigating this problem is to decompose also over the field "dimension" (8 for the forward FFT and 5 for the inverse) <ref> [22] </ref>. Many of these fields must be reunited for the LT phase, however, resulting in other performance problems. This generalization and the associated problems are discussed in x9. The fi (log Q) transpose algorithm requires that P X be a power of two. Load Balance. <p> Nonpower-of-two problem dimensions also cause load imbalances, and the amount of performance degradation is strongly algorithm dependent. Real weather and climate models often use a number of vertical levels significantly smaller than the other dimensions of the problem. For example, T213L31 is used in some operational weather-forecast models <ref> [22] </ref>), corresponding to a 640 fi 320 fi 31 physical grid. The transpose FFT algorithms suffer because they must use a larger number of processors for the LT than an algorithm that uses a distributed FFT. Decomposing "field" dimension.
Reference: [23] <author> R. D. Loft and R. K. Sato, </author> <title> Implementation of the NCAR CCM2 on the Connection Machine, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science: Proceedings of the Fifth ECMWF Workshop on Use of Parallel Processors in Meteorology, </booktitle> <editor> G.-R. Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 371-393. </pages>
Reference-contexts: Other researchers have examined a transpose approach, in which communication requirements are encapsulated in a matrix transpose operation. This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model [6] and in Loft and Sato's data parallel implementation of CCM2 <ref> [23] </ref>. It has also been explored by Kauranne and Barros [22], Pelz and Stern [26], and Gartel, Joppich, and Schuller [17]. In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both.
Reference: [24] <author> B. Machenhauer, </author> <title> The spectral method, in Numerical Methods Used in Atmospheric Models, vol. II of GARP Pub. </title> <journal> Ser. </journal> <volume> No. 17. </volume> <publisher> JOC, World Meteorological Organization, </publisher> <address> Geneva, Switzerland, </address> <year> 1979, </year> <journal> ch. </journal> <volume> 3, </volume> <pages> pp. 121-275. </pages>
Reference-contexts: The spherical harmonic functions are the eigensolutions of the Laplacian operator in spherical coordinates and constitute a complete and orthogonal expansion basis for square integrable functions on the sphere. Additional properties of these functions can be found in <ref> [24] </ref>. In the truncated expansion, M is the highest Fourier mode and N (m) is the highest degree of the associated Legendre function in the north-south representation. Since the physical quantities - 5 - are real, ~ m n is the complex conjugate of ~ m n .
Reference: [25] <author> M. Pease, </author> <title> An adaptation of the fast Fourier transform for parallel processing, </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 15 (1968), </volume> <pages> pp. 252-264. - 37 </pages> - 
Reference: [26] <author> R. B. Pelz and W. F. Stern, </author> <title> A balanced parallel algorithm for spectral global climate models, in Parallel Processing for Scientific Computing, </title> <editor> R. F. Sincovec, D. E. Keyes, M. R. Leuze, L. R. Petzold, and D. A. Reed, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, PA, </address> <year> 1993, </year> <pages> pp. 126-128. </pages>
Reference-contexts: This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model [6] and in Loft and Sato's data parallel implementation of CCM2 [23]. It has also been explored by Kauranne and Barros [22], Pelz and Stern <ref> [26] </ref>, and Gartel, Joppich, and Schuller [17]. In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both.
Reference: [27] <author> H. S. Stone, </author> <title> Parallel processing with the perfect shu*e, </title> <journal> IEEE Trans. Comput., </journal> <volume> C-21 (1971), </volume> <pages> pp. 153-161. </pages>
Reference: [28] <author> P. N. Swarztrauber, </author> <title> Multiprocessor FFTs, </title> <booktitle> Parallel Computing, 5 (1987), </booktitle> <pages> pp. 197-210. </pages>
Reference: [29] <author> P. N. Swarztrauber, W. L. Briggs, R. A. Sweet, V. E. Henson, and J. Otto, </author> <title> Bluestein's FFT for arbitrary n on the hypercube, </title> <booktitle> Parallel Computing, 17 (1991), </booktitle> <pages> pp. 607-618. </pages>
Reference: [30] <author> S. S. Takkella and S. R. Seidel, </author> <title> Complete exchange and broadcast algorithms for meshes, </title> <booktitle> in Proc. Scalable High Performance Computing Conf., </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994, </year> <note> (in press). </note>
Reference: [31] <author> R. A. van de Geijn, </author> <title> Efficient global combine operations, </title> <booktitle> in The Sixth Distributed Memory Computing Conference Proceedings, </booktitle> <editor> Q. F. Stout and M. Wolfe, eds., </editor> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991, </year> <pages> pp. 291-294. </pages>
Reference-contexts: Butterfly Sum. The butterfly sum algorithm is a hybrid of two algorithms <ref> [31] </ref>. For long vectors, we use a recursive halving algorithm [16] that utilizes a butterfly communication pattern like the distributed FFT.
Reference: [32] <author> D. W. Walker, P. H. Worley, and J. B. Drake, </author> <title> Parallelizing the spectral transform method. Part II, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4 (1992), </volume> <pages> pp. 509-531. </pages>
Reference-contexts: Computation/Communication Overlap. To exploit overlap, the single-block FFT can be divided into two, allowing one block's communication to be overlapped with the other's computation <ref> [32] </ref>. Only the first swap involving the first block is not overlapped with computation. This process requires twice as many messages, as indicated in Table 1, but has been shown to be cost effective on some multiprocessors. Bandwidth Limitations. <p> As the Fourier transform is unordered, the distributed FFT algorithm assigns blocks of permuted Fourier coefficients to the P X processor columns. This assignment approximately balances the assignment of "short" Legendre transforms (large wavenumbers) and "long" Legendre transforms (small wavenumbers) <ref> [32] </ref>, but the load balance is not perfect and some processor columns have more work than others.
Reference: [33] <author> W. Washington and C. Parkinson, </author> <title> An Introduction to Three-Dimensional Climate Modeling, </title> <publisher> University Science Books, </publisher> <address> Mill Valley, CA, </address> <year> 1986. </year>
Reference-contexts: Let i, j, and k denote unit vectors in spherical geometry, V denote the horizontal velocity, V = iu + jv, denote the geopotential, and f denote the Coriolis term. Then the horizontal momentum and mass continuity equations can be written as <ref> [33] </ref> DV = f k fi V r (1) Dt where the substantial derivative is given by D ( ) @t The spectral transform method does not solve these equations directly; rather, it uses a streamfunction-vorticity formulation in order to work with scalar fields. <p> For a triangular truncation, exact, unaliased transforms of quadratic terms are obtained if I 3M + 1 and if I = 2J <ref> [33] </ref>. In this work we also use a fast Fourier transform (FFT) algorithm that requires I to be a power of two. As is commonly done, for a given M we choose I to be the minimum power of two satisfying I 3M + 1, and set J = I=2.
Reference: [34] <author> D. L. Williamson, J. B. Drake, J. J. Hack, R. Jakob, and P. N. Swarztrauber, </author> <title> A standard test set for numerical approximations to the shallow water equations on the sphere, </title> <journal> J. Computational Physics, </journal> <volume> 102 (1992), </volume> <pages> pp. 211-224. </pages>
Reference-contexts: Section 10 presents our conclusions. 2. The Shallow Water Equations The nonlinear shallow water equations on a rotating sphere constitute a two-dimensional atmospheric-like fluid prediction model that exhibits many of the features of more complete models <ref> [34] </ref>. These equations are frequently used to investigate and compare numerical methods because they present many of the difficulties found in simulating the horizontal dynamics in three-dimensional global atmospheric models [5]. <p> All experiments used the performance benchmark described in <ref> [34] </ref>: global steady state nonlinear zonal geostrophic flow. Experiments were performed for problem sizes T21L8, T42L16, and T85L32. 8.4. Results: Algorithm Selection In presenting the results of the algorithm selection experiments, we do not discuss the communication parameters studied (see [36] for details) but focus on the algorithms.
Reference: [35] <author> P. H. Worley and J. B. Drake, </author> <title> Parallelizing the spectral transform method, </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> 4 (1992), </volume> <pages> pp. 269-291. </pages>
Reference-contexts: When the interleaving is organized so that the communication of one stage of the algorithm is overlapped with the computation of the next stage, the ring sum is able to perform fi (J 4 =P Y ) computation while communicating fi (P Y J 3 ) data <ref> [35] </ref>. This overlapping can be highly effective for small P Y and/or large J , decreasing the cost of communication significantly. Overlap is less effective for the butterfly sum.
Reference: [36] <author> P. H. Worley and I. T. Foster, </author> <title> Parallel Spectral Transform Shallow Water Model: A runtime-tunable parallel benchmark code, </title> <booktitle> in Proc. Scalable High Performance Computing Conf., </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994, </year> <note> (in press). </note>
Reference-contexts: In the analytic studies, we develop models that characterize the performance of the various spectral transform algorithms by relating communication requirements and load imbalances to problem size, processor count, and other parameters. The empirical studies utilize a parallel shallow water equation solver designed specifically for these experiments <ref> [36] </ref>. Considerable care has been taken to ensure that experiments are as fair as possible, that is, that one algorithm is not unduly favored through choice of data structures, greater optimization, etc. <p> In addition, the distributed FFT can use either the two-block algorithm that permits computation/communication overlap or the one-block algorithm, and the ring sum LT can use either the overlap or nonoverlap algorithms. Additional parameters select a range of variants of each of these major algorithms <ref> [36] </ref>. Note that all parallel algorithms were carefully implemented, eliminating unnecessary buffer copying and exploiting our knowledge of the context in which they are called. At the present time, this allows us to achieve better performance than can be achieved by calling available vendor-supplied routines. <p> All experiments used the performance benchmark described in [34]: global steady state nonlinear zonal geostrophic flow. Experiments were performed for problem sizes T21L8, T42L16, and T85L32. 8.4. Results: Algorithm Selection In presenting the results of the algorithm selection experiments, we do not discuss the communication parameters studied (see <ref> [36] </ref> for details) but focus on the algorithms. Table 8 summarizes both the algorithms considered and those selected for further consideration on different machines. For the most part, the table is self-explanatory. We always selected at least one distributed algorithm and one transpose algorithm for both the LT and FFT.
References-found: 35

