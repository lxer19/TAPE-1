URL: http://www.cs.ucsb.edu/~acha/publications/sigmod99-submitted.ps.gz
Refering-URL: http://www.cs.ucsb.edu/~acha/publications/sigmod99-submitted.html
Root-URL: http://www.cs.ucsb.edu
Title: Structure and Performance of Decision Support Algorithms on Active Disks  
Author: Anurag Acharya Mustafa Uysal Joel Saltz 
Date: October 1998  
Address: Santa Barbara College Park College Park  Santa Barbara  
Affiliation: Dept. of Computer Science Dept. of Computer Science Dept. of Computer Science University of California University of Maryland University of Maryland  Dept of Computer Science University of California,  
Pubnum: Technical Report TRCS98-28  
Abstract: Growth and usage trends for large decision support databases indicate that there is a need for architectures that scale the processing power as the dataset grows. These trends indicate that the processing demand for large decision support databases is growing faster than the improvement in performance of commodity processors. To meet this need, several researchers have recently proposed Active Disk/IDISK architectures which integrate substantial processing power and memory into disk units. In this paper, we examine the utility of Active Disks for decision support databases. We try to answer the following questions. First, is it possible to restructure algorithms for common decision support tasks to utilize Active Disks? Second, how does the performance of Active Disks compare with that of traditional servers for these tasks? Finally, how would Active Disks be integrated into the software architecture of decision support databases?
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya, M. Uysal, R. Bennett, A. Mendelson, M. Beynon, J. Hollingsworth, J. Saltz, and A. Suss-man. </author> <title> Tuning the performance of I/O-intensive parallel applications. </title> <booktitle> In Proceedings of the Fourth ACM Workshop on I/O in Parallel and Distributed Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Given the volume of data processed and the cost of fetching data from disk, optimizing I/O-intensive algorithms is often a matter of setting up efficient pipelines where each stage performs some processing on the data being read from disk and passes it on to the next stage <ref> [1, 7] </ref>. As a result, dataflow-based models have been proposed by several researchers. Barclay et al [11] proposed a dataflow-based technique for parallelizing the loading of a large database. Similar techniques are used by the Gamma [17] and Volcano [20] parallel databases.
Reference: [2] <author> A. Acharya, M. Uysal, and J. Saltz. </author> <title> Active Disks: Programming Model, Algorithms and Evaluation. </title> <booktitle> In Proceedings of ASPLOS VIII, </booktitle> <pages> pages 81-91, </pages> <month> Oct </month> <year> 1998. </year>
Reference-contexts: To meet this need, several researchers have recently proposed Active Disk/IDISK architectures which integrate substantial processing power and memory into disk units <ref> [2, 22, 27, 42] </ref>. These architectures allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk. To utilize Active Disks, an application is partitioned between a host-resident component and a disk-resident component. <p> Shared memory multiprocessors are widely used for relational databases (Strenstrom et al [48] estimate that in 2000, 40% of such machines will be sold for handling relational databases). Finally, we show how the stream-based programming model that has been proposed for Active Disks <ref> [2] </ref> meshes well with the software architecture of relational databases. 2 Background: Active Disks In this section, we provide a brief introduction to Active Disks. <p> The key idea is to o*oad bulk of the processing to the disk-resident processors and to use the host processor primarily for coordination, scheduling and combination of results from individual disks. Figure 1 presents a schematic for Active Disk architectures. Acharya et al <ref> [2] </ref> propose a stream-based programming model for disk-resident code (a disklet) and its interaction with its peers on other disks as well as on the front-end host. Disklets take streams as inputs and generate streams as outputs. <p> It uses a modified version of DiskSim that is driven by the disk operating system layer. Disklets are written in C and interact with Howsim using a stream-based API <ref> [2] </ref>. Howsim has additional parameters for the DiskOS. For this study, we assumed the system call and context switch costs on the DiskOS to be 1 s. In addition, another 1 s is charged to initiate a disk request from DiskOS and to service an interrupt from the disk mechanism. <p> Unlike the limited repertoire of operations performed by database machines, Active Disks take advantage of algorithmic research for shared-nothing architectures to provide efficient implementations of a wide variety of operations. Furthermore, Active Disks can be used to perform operations for non-relational data such as image processing <ref> [2, 42] </ref> and file-system and security-related processing [18, 51]. Riedel et al [42] have also evaluated Active Disk architectures for databases.
Reference: [3] <author> R. Agarwal. </author> <title> A super scalar sort algorithm for RISC processors. </title> <booktitle> In Proceedings of 1996 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 240-6, </pages> <year> 1996. </year>
Reference-contexts: Note that all the streams destined to a combine disklet are merged. External sort: we used the two-pass parallel NOW-sort [7] as the starting point for our sort algorithms. NOW-sort is based on a long history of external sorting research in the database community (e.g. <ref> [3] </ref> and [34]) and currently holds the record for the fastest external sort (the Indy MinuteSort record [23]). The Active Disk algorithm uses two disklets for the first phase, the partitioner and the sorter (Figure 4 (a)). <p> After the first two passes, it clusters the candidate itemsets into equivalence classes and uses these classes 2 Making two passes over the keys with a radix size of 11-bits <ref> [3] </ref> plus a cleanup. 3 Two buffers in the original algorithm. 6 CREATE VIEW foo AS SELECT store.manager, sale.sale_id, item.item_id, item.item_name FROM store, sale, item, line WHERE store.store_id = sale.sale_id and sale.sale_id = line.sale_id and line.item_id = item.item_id and store.state = ``CA'' and sale.year = 1998 to filter, transpose and repartition
Reference: [4] <author> S. Agarwal, R. Agrawal, P. Deshpande, A. Gupta, J. Naughton, R. Ramakrishnan, and S. Sarawagi. </author> <title> On the computation of multidimensional aggregates. </title> <booktitle> In Proceedings of the 22nd International Conference on Very Large Databases, </booktitle> <pages> pages 506-21, </pages> <year> 1996. </year>
Reference-contexts: First, we present Active Disk algorithms for a suite of eight common decision support tasks: select, aggregation, group-by, the datacube operation [24], external sort, project-join queries, datamining association rules from retail transaction data and materialized views. We have derived these algorithms from well-known efficient algorithms in the literature <ref> [4, 7, 14, 21, 40, 54] </ref>. Second, we compare the performance of Active Disks with that of shared memory multiprocessor servers for these tasks. <p> It computes multidimensional aggregates that are indexed by values of multiple aggregates [24]. In effect, a datacube computes group-bys for all possible combinations of a list of attributes. Several efficient methods for computing a datacube are presented in <ref> [4] </ref>. We use one of these algorithms, called PipeHash, as the starting point for our algorithms. PipeHash represents the datacube as a lattice of related group-bys. A directed edge connects group-by i to group-by j if j can be generated from i and has exactly one less attribute. <p> The number of distinct values for each of the group-by attributes were 5.36 million, 536,000, 53,600 and 5,360. We created this dataset by scaling one of the datasets used in the paper that described the PipeHash algorithm <ref> [4] </ref>. Sort: for sort, we used a dataset with 100-byte tuples and 10-byte uniformly distributed keys. The total number of tuples was about 170 million. We created this dataset based on the standard sort benchmark described in [23].
Reference: [5] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pages 207-16, </pages> <year> 1993. </year>
Reference-contexts: Note that the first pass of join is communication-intensive and requires all-to-all communication. Since both sort and join repartition their entire dataset, their communication requirements are similar. Datamining: we focus on determining frequent itemsets for mining association rules in retail transaction data <ref> [5] </ref>. We used the eclat algorithm [54] as the starting point for our algorithms. It is a multi-pass algorithm with the first two passes same as the count distribution algorithm proposed by Agrawal et al [6].
Reference: [6] <author> R. Agrawal and J. Shafer. </author> <title> Parallel mining of association rules. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(6) </volume> <pages> 962-9, </pages> <year> 1996. </year>
Reference-contexts: We used the eclat algorithm [54] as the starting point for our algorithms. It is a multi-pass algorithm with the first two passes same as the count distribution algorithm proposed by Agrawal et al <ref> [6] </ref>.
Reference: [7] <author> A. Arpaci-Dusseau, R. Arpaci-Dusseau, D. Culler, J. Hellerstein, and D. Patterson. </author> <title> High-performance sorting on networks of workstations. </title> <booktitle> In Proceedings of SIGMOD'97, </booktitle> <year> 1997. </year>
Reference-contexts: First, we present Active Disk algorithms for a suite of eight common decision support tasks: select, aggregation, group-by, the datacube operation [24], external sort, project-join queries, datamining association rules from retail transaction data and materialized views. We have derived these algorithms from well-known efficient algorithms in the literature <ref> [4, 7, 14, 21, 40, 54] </ref>. Second, we compare the performance of Active Disks with that of shared memory multiprocessor servers for these tasks. <p> Note that for sort and join, which shu*e their entire dataset and write it back to disk, we partitioned the disks into separate read and write groups (as in NOW-sort <ref> [7] </ref>). Since all processors can address all disks, we did not a priori partition the input datasets to processors. Instead, we maintained two shared queues (read/write) of fixed-size blocks in the order they appear on disk. <p> After all contributions are combined, group-bys are written to disk. Note that all the streams destined to a combine disklet are merged. External sort: we used the two-pass parallel NOW-sort <ref> [7] </ref> as the starting point for our sort algorithms. NOW-sort is based on a long history of external sorting research in the database community (e.g. [3] and [34]) and currently holds the record for the fastest external sort (the Indy MinuteSort record [23]). <p> The Active Disk algorithm is fully pipelined in that it overlaps reading data, sending data to peers and sorting and writing data. The SMP algorithm overlaps just two operations; reading and writing operations are performed synchronously (Arpaci-Dusseau et al <ref> [7] </ref> recommend that for less than four disks, all operations should be overlapped whereas for more than four disks, only the first two should be overlapped). The first pass of these algorithms repartitions their entire input on disks. <p> Note that join seems to achieve better than perfect scaling for both architectures. We suspect that this is due to cache capacity effects during the merge phase. A similar result was reported by Arpaci-Dusseau et al <ref> [7] </ref> for NOW-sort with more than 17 runs per node (our experiments had between 2 and 80 runs per node). doubling the I/O interconnect bandwidth has a large impact on the performance of SMP configurations for all tasks. <p> Given the volume of data processed and the cost of fetching data from disk, optimizing I/O-intensive algorithms is often a matter of setting up efficient pipelines where each stage performs some processing on the data being read from disk and passes it on to the next stage <ref> [1, 7] </ref>. As a result, dataflow-based models have been proposed by several researchers. Barclay et al [11] proposed a dataflow-based technique for parallelizing the loading of a large database. Similar techniques are used by the Gamma [17] and Volcano [20] parallel databases.
Reference: [8] <author> R. Arpaci-Dusseau, E. Anderson, N. Treuhaft, D. Culler, J. Hellerstein, D. Patterson, and K. Yelick. </author> <title> Cluster I/O with Rivers: Making the Fast Case Common. </title> <note> Submitted for publication, </note> <year> 1998. </year>
Reference-contexts: As a result, dataflow-based models have been proposed by several researchers. Barclay et al [11] proposed a dataflow-based technique for parallelizing the loading of a large database. Similar techniques are used by the Gamma [17] and Volcano [20] parallel databases. Recently, Arpaci-Dusseau et al <ref> [8] </ref> have proposed a dataflow-based programming model for scheduling I/O-intensive tasks on clusters. 8 Conclusions There are four main conclusions of our study.
Reference: [9] <institution> The Avalon FAQ 5 , 1998. </institution>
Reference-contexts: We estimated the price of the SMP configuration using the SGI Origin 2000. The Avalon project at Los Alamos Labs quotes the list price of a 64-processor SGI Origin 2000 with 250MHz processors and 8 GB memory to be about $1.8 million <ref> [9] </ref>. Estimating the cost of 4 GB of memory to be (a generous) $300,000, we estimate the cost of the configuration studied in this paper to be about $1.5 million.
Reference: [10] <author> J. Banerjee, R. Baum, and D. Hsiao. </author> <title> Concepts and capabilities of a database computer. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 3(4), </volume> <month> Dec </month> <year> 1978. </year>
Reference-contexts: One of the ISAM implementations on the IBM 360 used channel programs to traverse disk-resident linked lists. Database machines [46] in the late 1970s proposed various levels of processor integration into disk: processor per track [30, 35, 47, 49], processor per head <ref> [10, 29] </ref>, and "off-the-disk" designs that used a shared disk cache with multiple processors and multiple disks [16, 44]. There are several differences between the database machines of late 1970's and today's Active Disks.
Reference: [11] <author> T. Barclay, R. Barnes, J. Gray, and P. Sundaresan. </author> <title> Loading databases using dataflow parallelism. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 23(4) </volume> <pages> 72-83, </pages> <year> 1994. </year>
Reference-contexts: Operators implemented in this model are called iterators, streams, synchronous pipelines, row-sources, or similar names in the lingo of commercial systems". According to Barclay et. al. <ref> [11] </ref>, "relational databases are ideally suited to dataflow approach" and that "the database community has adopted a dataflow approach to describe and implement parallel algorithms". The stream-based programming model proposed for Active Disks closely resembles the operator/iterator based model used by relational databases. <p> As a result, dataflow-based models have been proposed by several researchers. Barclay et al <ref> [11] </ref> proposed a dataflow-based technique for parallelizing the loading of a large database. Similar techniques are used by the Gamma [17] and Volcano [20] parallel databases.
Reference: [12] <author> D. Batory, J. Barnett, J. Garza, K. Smith, K. Tsukuda, B. Twitchell, and T. Wise. </author> <title> GENESIS: An extensible database management system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(11), </volume> <year> 1988. </year>
Reference-contexts: According to Graefe [21], "this model of operator implementation and scheduling resembles very closely those used in relational systems, e.g., System R (and later SQL/DS and DB2), Ingres, Informix, and Oracle as well as in experimental systems, e.g., the E programming language used in EXODUS [41], Genesis <ref> [12] </ref>, and Starburst [25]. Operators implemented in this model are called iterators, streams, synchronous pipelines, row-sources, or similar names in the lingo of commercial systems".
Reference: [13] <author> E. Brewer, F. Chong, L. Liu, S. Sharma, and J. Kubiatowicz. </author> <title> Remote queues: Exposing message queues for optimization and atomicity. </title> <booktitle> In Proc. of the 7th SPAA, </booktitle> <pages> pages 42-53, </pages> <year> 1995. </year>
Reference-contexts: For Active Disks, we adapted the algorithms to use the stream-based programming model. Note that, overlapping computation, communication, and I/O is handled by the DiskOS (the disk-resident OS layer) by using multiple buffers per stream. For SMPs, we adapted the algorithms to use one-way block-transfers (shmemput/shmemget) and remote queues <ref> [13] </ref> for moving data between processors. Given the volume of data being transferred and the one-way nature of the data movement, block-transfers and remote queues 3 are suitable for these tasks. We striped each file over all disks using a 64 KB chunk size per disk. <p> We assumed that these machines ran a standard full-function operating system like IRIX and provided the lio listio asynchronous I/O interface and user-controllable disk striping for individual files. Further, we assumed that these machines provided a remote queue abstraction (as suggested by Brewer et al <ref> [13] </ref>). To study the impact of variation in the I/O interconnect, we studied alternative configurations that scaled the bandwidth of the serial I/O interconnect to 400 MB/s. 4.2 Simulator To conduct these experiments, we developed a simulator called Howsim which simulates all three architectures. <p> For communication, it models one-way block-transfers, shmemget/shmemput, as available on the SGI Origin. Block transfers are suitable for the algorithms under consideration as they move large volumes of data in relatively large chunks. For synchronization on SMPs, Howsim provides spin-locks, remote queues <ref> [13] </ref> and global barriers. We used the at-memory fetch-and-op primitive as provided by SGI Origin for spin-locks (which cost around 3s [26]). Howsim models a high-bandwidth I/O subsystem similar to the XIO subsystem available in the Origin 2000.
Reference: [14] <author> L. Colby, T. Griffin, L. Libkin, I. Mumick, and H. Trickey. </author> <title> Algorithms for deferred view maintenance. </title> <booktitle> In Proc. of SIGMOD'97, </booktitle> <pages> pages 469-80, </pages> <year> 1997. </year>
Reference-contexts: First, we present Active Disk algorithms for a suite of eight common decision support tasks: select, aggregation, group-by, the datacube operation [24], external sort, project-join queries, datamining association rules from retail transaction data and materialized views. We have derived these algorithms from well-known efficient algorithms in the literature <ref> [4, 7, 14, 21, 40, 54] </ref>. Second, we compare the performance of Active Disks with that of shared memory multiprocessor servers for these tasks. <p> Materialized views are pre-computed views that are used to speedup queries to decision support databases in effect, materialized views are cached versions of views. Incremental view maintenance keeps cached views consistent with their base relations as the base relations are modified. Mumick et al <ref> [14, 40] </ref> have proposed efficient algorithms for maintaining select-project-join views (see Figure 5 for an example of such views). We borrowed the ideas of deferred maintenance and self-maintainable views from their algorithms. We first describe our core algorithm for parallel maintenance of select-project-join views.
Reference: [15] <institution> IBM DB2 Java Enablement. </institution> <note> http://www.software.ibm.com/data/db2/java/index.html, 1998. </note>
Reference-contexts: IBM's DB2 UDB5 allows extension of the database server using Java user-defined functions and stored procedures <ref> [15] </ref>. User-defined functions may be used in an SQL expression to compute a complex function of several values in a given row. They can also be used in the FROM clause in a query for on-the-fly creation of tables.
Reference: [16] <author> D. DeWitt. </author> <title> DIRECT amultiprocessor organization for supporting relational database management systems. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 28(6) </volume> <pages> 395-406, </pages> <month> Jun </month> <year> 1979. </year>
Reference-contexts: Database machines [46] in the late 1970s proposed various levels of processor integration into disk: processor per track [30, 35, 47, 49], processor per head [10, 29], and "off-the-disk" designs that used a shared disk cache with multiple processors and multiple disks <ref> [16, 44] </ref>. There are several differences between the database machines of late 1970's and today's Active Disks. Unlike the database machine processors, the processors proposed for Active Disks are general-purpose commodity components and are likely to evolve as the disks evolve.
Reference: [17] <author> D. DeWitt, S. Ghandeharizadeh, and D. Schneider. </author> <title> A performance analysis of the Gamma database machine. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 17(3) </volume> <pages> 350-60, </pages> <year> 1988. </year>
Reference-contexts: As a result, dataflow-based models have been proposed by several researchers. Barclay et al [11] proposed a dataflow-based technique for parallelizing the loading of a large database. Similar techniques are used by the Gamma <ref> [17] </ref> and Volcano [20] parallel databases. Recently, Arpaci-Dusseau et al [8] have proposed a dataflow-based programming model for scheduling I/O-intensive tasks on clusters. 8 Conclusions There are four main conclusions of our study.
Reference: [18] <author> G. Gibson et al. </author> <title> File server scaling with network-attached secure disks. </title> <booktitle> In Proceedings of the ACM International Conference on Measurement and Modeling of Computer Systems (Sigmetrics '97), </booktitle> <year> 1997. </year> <note> 5 http://cnls.lanl.gov/avalon/FAQ.html 16 </note>
Reference-contexts: Furthermore, Active Disks can be used to perform operations for non-relational data such as image processing [2, 42] and file-system and security-related processing <ref> [18, 51] </ref>. Riedel et al [42] have also evaluated Active Disk architectures for databases.
Reference: [19] <author> G. Ganger, B. Worthington, and Y. Patt. </author> <title> The DiskSim Simulation Environment Version 1.0 Reference Manual 6 . Technical Report CSE-TR-358-98, </title> <institution> Dept of Electrical Engineering and Computer Science, </institution> <month> Feb </month> <year> 1998. </year>
Reference-contexts: Howsim contains detailed models for disks, networks and the associated libraries and device drivers and relatively coarse-grain models of processors and I/O interconnects. For modeling the behavior of disk drives, controllers and device drivers, Howsim uses the Disksim simulator developed by Ganger et al <ref> [19] </ref>. Disksim has a detailed disk model that supports zoned disks, spare regions, segmented caches, defect management, prefetch algorithms, bus delays and control overheads. <p> Disksim has been validated against several disk drives using the published disk specifications and SCSI logic analyzers; it achieves high accuracy theworst case demerit figure [43] for Disksim is only 2.0% of the corresponding average response time <ref> [19] </ref>. For modeling I/O interconnects, Howsim uses a simple queue-based model that has parameters for startup latency, transfer speed and the capacity of the interconnect. For modeling the behavior of user processes, Howsim uses a trace of processing times and I/O requests for individual tasks.
Reference: [20] <author> G. Graefe. </author> <title> Encapsulation of parallelism in the Volcano query processing system. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 19(2) </volume> <pages> 102-11, </pages> <year> 1990. </year>
Reference-contexts: As a result, dataflow-based models have been proposed by several researchers. Barclay et al [11] proposed a dataflow-based technique for parallelizing the loading of a large database. Similar techniques are used by the Gamma [17] and Volcano <ref> [20] </ref> parallel databases. Recently, Arpaci-Dusseau et al [8] have proposed a dataflow-based programming model for scheduling I/O-intensive tasks on clusters. 8 Conclusions There are four main conclusions of our study.
Reference: [21] <author> G. Graefe. </author> <title> Query evaluation techniques for large databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> Jun </month> <year> 1993. </year>
Reference-contexts: First, we present Active Disk algorithms for a suite of eight common decision support tasks: select, aggregation, group-by, the datacube operation [24], external sort, project-join queries, datamining association rules from retail transaction data and materialized views. We have derived these algorithms from well-known efficient algorithms in the literature <ref> [4, 7, 14, 21, 40, 54] </ref>. Second, we compare the performance of Active Disks with that of shared memory multiprocessor servers for these tasks. <p> SQL group-by: The group-by operation computes a one-dimensional vector of aggregates indexed by a list of attributes [32]. It partitions a relation into disjoint sets of tuples based on the value (s) of index attribute (s) and computes an aggregate value for each set of tuples. Graefe <ref> [21] </ref> shows that hashing-based techniques outperform sort-based and nested-loop-based techniques for implementing the group-by operation. Accordingly, we used the hashing-based algorithm from [21] as the starting point for our algorithms. The Active Disk algorithm performs the group-by in two steps. <p> Graefe <ref> [21] </ref> shows that hashing-based techniques outperform sort-based and nested-loop-based techniques for implementing the group-by operation. Accordingly, we used the hashing-based algorithm from [21] as the starting point for our algorithms. The Active Disk algorithm performs the group-by in two steps. In the first step, each disk performs local group-bys as long as the number of aggregates being computed fits in its memory. <p> The results of each pipeline are stored back on disk and are used as input for following pipelines (see Figure 2 for examples of pipelines). For individual group-bys, PipeHash uses a hashing-based technique <ref> [21] </ref>. The Active Disk algorithm partitions the available memory at each disk in proportion to the estimated size of the group-bys being performed in the pipeline (Figure 3). <p> First, we discuss the integration of Active Disks into the software architecture of decision support databases. Second, we give an indication of the cost of comparable configurations for Active Disks and shared memory multiprocessors. Relational databases have long used demand-driven dataflow based architectures. According to Graefe <ref> [21] </ref>, "this model of operator implementation and scheduling resembles very closely those used in relational systems, e.g., System R (and later SQL/DS and DB2), Ingres, Informix, and Oracle as well as in experimental systems, e.g., the E programming language used in EXODUS [41], Genesis [12], and Starburst [25]. <p> Disklets attached to a stream perform the operations corresponding to the iterator function. The scratch space for a disklet corresponds to the state record for an iterator. According to Graefe <ref> [21] </ref>, "the type of state records is different for each iterator as it contains iterator-specific arguments and local variables (state), while the iterator is suspended, e.g., currently not active between invocations of the operator's next procedure". This is exactly the purpose of the scratch space in disklets.
Reference: [22] <author> J. Gray. </author> <title> Put EVERYTHING in the Storage Device. Talk at NASD workshop on storage embedded computing 7 , June 1998. </title>
Reference-contexts: To meet this need, several researchers have recently proposed Active Disk/IDISK architectures which integrate substantial processing power and memory into disk units <ref> [2, 22, 27, 42] </ref>. These architectures allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk. To utilize Active Disks, an application is partitioned between a host-resident component and a disk-resident component.
Reference: [23] <author> J. Gray. </author> <title> The Sort Benchmark Home Page. </title> <note> Available at http://research.microsoft.com/research/barc/- SortBenchmark/, </note> <year> 1998. </year>
Reference-contexts: NOW-sort is based on a long history of external sorting research in the database community (e.g. [3] and [34]) and currently holds the record for the fastest external sort (the Indy MinuteSort record <ref> [23] </ref>). The Active Disk algorithm uses two disklets for the first phase, the partitioner and the sorter (Figure 4 (a)). The partitioner uses its scratch-space to form as many buckets as the number of disks. It examines each record and appends it to the bucket corresponding to its destination disk. <p> Sort: for sort, we used a dataset with 100-byte tuples and 10-byte uniformly distributed keys. The total number of tuples was about 170 million. We created this dataset based on the standard sort benchmark described in <ref> [23] </ref>. Project-Join: for join, we used a dataset with 64-byte tuples and 4-byte uniformly distributed keys. The projection operation extracted eight 4-byte fields from each tuple. Each relation was 16 GB and contained 268 million tuples. The output for join was about 108 MB.
Reference: [24] <author> J. Gray, A. Bosworth, A. Layman, and H. Pirahesh. </author> <title> Data cube: A relational aggregation operator generalizing group-by, </title> <booktitle> cross-tab, and sub-totals. In Proceedings of the 12th International Conference on Data Engineering, </booktitle> <pages> pages 152-9, </pages> <address> New Orleans, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: First, we present Active Disk algorithms for a suite of eight common decision support tasks: select, aggregation, group-by, the datacube operation <ref> [24] </ref>, external sort, project-join queries, datamining association rules from retail transaction data and materialized views. We have derived these algorithms from well-known efficient algorithms in the literature [4, 7, 14, 21, 40, 54]. <p> A disklet is ready to run whenever there is new data available on one or more of its input streams. 3 Algorithms In this section, we present Active Disk algorithms for eight common decision support tasks: select, aggregation, group-by, the datacube operation <ref> [24] </ref>, external sort, project-join queries, datamining association rules from retail transaction data and maintaining materialized views. In addition, we also describe shared memory multiprocessor (SMP) algorithms for these tasks. We use the SMP algorithms to help compare the performance of Active Disks with that of SMPs. <p> It also performs more computation/byte as it needs to maintain a hash-table of aggregates. Datacube: the datacube is the most general form of aggregation for relational databases. It computes multidimensional aggregates that are indexed by values of multiple aggregates <ref> [24] </ref>. In effect, a datacube computes group-bys for all possible combinations of a list of attributes. Several efficient methods for computing a datacube are presented in [4]. We use one of these algorithms, called PipeHash, as the starting point for our algorithms.
Reference: [25] <author> L. Haas, W. Chang, G. Lohman, J. McPherson, P. Wilms, G. Lapis, B. Lindsay, H. Pirahesh, M. Carey, and E. Shekita. </author> <title> Starburst midfight: As the dust clears. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1), </volume> <year> 1990. </year>
Reference-contexts: According to Graefe [21], "this model of operator implementation and scheduling resembles very closely those used in relational systems, e.g., System R (and later SQL/DS and DB2), Ingres, Informix, and Oracle as well as in experimental systems, e.g., the E programming language used in EXODUS [41], Genesis [12], and Starburst <ref> [25] </ref>. Operators implemented in this model are called iterators, streams, synchronous pipelines, row-sources, or similar names in the lingo of commercial systems".
Reference: [26] <author> D. Jiang and J. Singh. </author> <title> A methodology and an evaluation of the SGI Origin 2000. </title> <booktitle> In Proc. of the Intl. Conf. on Measurement and Modeling of Computer Systems (SIGMETRICS), </booktitle> <pages> pages 171-81, </pages> <address> Madison, WI, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: For synchronization on SMPs, Howsim provides spin-locks, remote queues [13] and global barriers. We used the at-memory fetch-and-op primitive as provided by SGI Origin for spin-locks (which cost around 3s <ref> [26] </ref>). Howsim models a high-bandwidth I/O subsystem similar to the XIO subsystem available in the Origin 2000.
Reference: [27] <author> K. Keeton, D. Patterson, and J. Hellerstein. </author> <title> The Case for Intelligent Disks (IDISKS). </title> <booktitle> SIGMOD Record, </booktitle> <volume> 27(3), </volume> <year> 1998. </year>
Reference-contexts: To meet this need, several researchers have recently proposed Active Disk/IDISK architectures which integrate substantial processing power and memory into disk units <ref> [2, 22, 27, 42] </ref>. These architectures allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk. To utilize Active Disks, an application is partitioned between a host-resident component and a disk-resident component. <p> This allowed these configurations to tolerate longer communication and I/O latencies. Shared memory multiprocessors (SMPs): For the SMP configurations, we followed the guidelines for configuring decision support servers (as quoted in <ref> [27] </ref>): (1) put as many processors in a box as possible to amortize the cost of enclosures and interconnects; (2) put as much memory as possible into the box to avoid going to disk as much as possible; and (3) attach as many disks as needed for capacity and stripe data <p> It remains a question, however, whether database vendors would choose to integrate Active Disks into their software. Keeton et. al. <ref> [27] </ref> indicate that initial reactions to active disks have been mixed: some industrial database experts suggesting that Active Disks can succeed only if they require minimal change to existing database software. We believe that this initial reluctance can be overcome for two reasons. <p> Their results indicate that these algorithms can achieve significant gains from the use of Active Disks. Based on an analysis of several technological trends, Keeton et al <ref> [27] </ref> propose an architecture (IDISK) in which a processor-in-memory chip (IRAM [37]) is integrated into the disk unit and the disk units are connected by a crossbar. They argue that IDISK architectures offer several potential price and performance advantages over traditional server architectures for decision support.
Reference: [28] <author> J. Laudon and D. Lenoski. </author> <title> The SGI Origin: a ccNUMA highly scalable server. </title> <booktitle> In In Proc. of Intl. Symposium on Computer Architecture, </booktitle> <pages> pages 241-51, </pages> <address> Denver, CO, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: We assumed an SMP configuration similar to the SGI Origin 2000: (1) two-processor boards (with 250 MHz processors) that directly share 128 MB memory; (2) a low-latency, high-bandwidth interconnect between these boards (1s latency and 780 MB/s bandwidth); (3) a high-performance block-transfer engine (521 MB/s sustained bandwidth <ref> [28] </ref>); (4) a high-bandwidth I/O subsystem (two I/O nodes with a total of 1.4 GB/s bandwidth), similar to XIO, that connects to the network interconnect; and (5) a dual-loop Fibre Channel I/O interconnect (200 MB/s) for all disks. Figure 7 illustrates the SMP configurations.
Reference: [29] <author> H. Leilich, G. Stiege, and H. Zeidler. </author> <title> A search processor for database management systems. </title> <booktitle> In Proc. of VLDB'78, </booktitle> <year> 1978. </year>
Reference-contexts: One of the ISAM implementations on the IBM 360 used channel programs to traverse disk-resident linked lists. Database machines [46] in the late 1970s proposed various levels of processor integration into disk: processor per track [30, 35, 47, 49], processor per head <ref> [10, 29] </ref>, and "off-the-disk" designs that used a shared disk cache with multiple processors and multiple disks [16, 44]. There are several differences between the database machines of late 1970's and today's Active Disks.
Reference: [30] <author> S. Lin, D. Smith, and J. Smith. </author> <title> The design of a rotating associative memory for relational database applications. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 1(1) </volume> <pages> 53-75, </pages> <month> Mar </month> <year> 1976. </year>
Reference-contexts: One of the ISAM implementations on the IBM 360 used channel programs to traverse disk-resident linked lists. Database machines [46] in the late 1970s proposed various levels of processor integration into disk: processor per track <ref> [30, 35, 47, 49] </ref>, processor per head [10, 29], and "off-the-disk" designs that used a shared disk cache with multiple processors and multiple disks [16, 44]. There are several differences between the database machines of late 1970's and today's Active Disks.
Reference: [31] <author> L. McVoy and C. Staelin. lmbench: </author> <title> portable tools for performance analysis. </title> <booktitle> In In Proc. of 1996 USENIX Technical Conference, </booktitle> <month> Jan </month> <year> 1996. </year>
Reference-contexts: We obtained the first two using lmbench <ref> [31] </ref> on a 300MHz Pentium II running Linux (10s for read/write calls, 103s for context-switch). We charged a fixed cost of 16s to queue an I/O request in the device-driver.
Reference: [32] <author> J. Melton and A. Simon. </author> <title> Understanding the New SQL: A Complete Guide. </title> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference-contexts: Both select and aggregate perform little computation/byte. SQL group-by: The group-by operation computes a one-dimensional vector of aggregates indexed by a list of attributes <ref> [32] </ref>. It partitions a relation into disjoint sets of tuples based on the value (s) of index attribute (s) and computes an aggregate value for each set of tuples. Graefe [21] shows that hashing-based techniques outperform sort-based and nested-loop-based techniques for implementing the group-by operation.
Reference: [33] <author> S. Meyer. </author> <title> Oracle's Aurora Java Virtual Machine. </title> <booktitle> In Proc. of OOPSLA-98, </booktitle> <pages> page 181, </pages> <year> 1998. </year> <title> In the panel on "The New Crop of Java Virtual Machines". </title>
Reference-contexts: Second, in other situations requiring modifications to existing software, database vendors have not been inflexible. Examples include the migration of most database servers to SMPs and the recent incorporation of Java virtual machines in some databases. To cite examples of the latter, Oracle's Aurora project <ref> [33] </ref> plans to integrate a Java virtual machine in the database engine such that Java code can call SQL and SQL can call Java code (Aurora is scheduled to ship with Oracle 8.1). IBM's DB2 UDB5 allows extension of the database server using Java user-defined functions and stored procedures [15].
Reference: [34] <author> C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: </author> <title> a RISC machine sort. </title> <booktitle> In Proceedings of 1994 ACM SIGMOD International Conference on Management of Data, </booktitle> <address> Minniapolis, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Note that all the streams destined to a combine disklet are merged. External sort: we used the two-pass parallel NOW-sort [7] as the starting point for our sort algorithms. NOW-sort is based on a long history of external sorting research in the database community (e.g. [3] and <ref> [34] </ref>) and currently holds the record for the fastest external sort (the Indy MinuteSort record [23]). The Active Disk algorithm uses two disklets for the first phase, the partitioner and the sorter (Figure 4 (a)). The partitioner uses its scratch-space to form as many buckets as the number of disks.
Reference: [35] <author> E. Ozkarahan, S. Schuster, and K. Sevcik. </author> <title> Performance evaluation of a relational associative processor. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 2(2), </volume> <month> Jun </month> <year> 1977. </year>
Reference-contexts: One of the ISAM implementations on the IBM 360 used channel programs to traverse disk-resident linked lists. Database machines [46] in the late 1970s proposed various levels of processor integration into disk: processor per track <ref> [30, 35, 47, 49] </ref>, processor per head [10, 29], and "off-the-disk" designs that used a shared disk cache with multiple processors and multiple disks [16, 44]. There are several differences between the database machines of late 1970's and today's Active Disks.
Reference: [36] <author> G. Papadopolous. </author> <title> The future of computing. Unpublished talk at NOW Workshop, </title> <month> July </month> <year> 1997. </year> <note> 6 Available at http://www.ece.cmu.edu/ ganger/disksim/disksim1.0.tar.gz 7 http://www.nsic.org/nasd/1998-jun/gray.pdf 17 </note>
Reference-contexts: Patterson et al [37] quote an observation by Greg Papadopolous while processors are doubling performance every 18 months, customers are doubling data storage every nine-to-twelve months and would like to "mine" this data overnight to shape their business practices <ref> [36] </ref>. To meet this need, several researchers have recently proposed Active Disk/IDISK architectures which integrate substantial processing power and memory into disk units [2, 22, 27, 42]. These architectures allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk.
Reference: [37] <author> D. Patterson et al. </author> <title> Intelligent RAM (IRAM): the Industrial Setting, Applications, and Architectures. </title> <booktitle> In Proceedings of the International Conference on Computer Design, </booktitle> <year> 1997. </year>
Reference-contexts: For example, the Sears Roebuck and Co decision support database grew from 1.3 TB in 1997 to 4.6 TB in 1998. The usage trends indicate that there is a change in user expectations regarding large databases from primarily archival storage to frequent reprocessing in their entirety. Patterson et al <ref> [37] </ref> quote an observation by Greg Papadopolous while processors are doubling performance every 18 months, customers are doubling data storage every nine-to-twelve months and would like to "mine" this data overnight to shape their business practices [36]. <p> Their results indicate that these algorithms can achieve significant gains from the use of Active Disks. Based on an analysis of several technological trends, Keeton et al [27] propose an architecture (IDISK) in which a processor-in-memory chip (IRAM <ref> [37] </ref>) is integrated into the disk unit and the disk units are connected by a crossbar. They argue that IDISK architectures offer several potential price and performance advantages over traditional server architectures for decision support.
Reference: [38] <author> D. Patterson and J. Hennessey. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufman, </publisher> <address> 2nd edition, </address> <year> 1996. </year>
Reference-contexts: The I/O processors in the IBM 360 allowed users to download channel programs that were able to make I/O requests on behalf of the host programs <ref> [38] </ref>. One of the ISAM implementations on the IBM 360 used channel programs to traverse disk-resident linked lists.
Reference: [39] <institution> IBM Quest Data Mining Project. </institution> <note> The Quest retail transaction data generator 8 , 1996. </note>
Reference-contexts: Datamining: for dmine, we used a dataset with 300 million transactions. The total number of items was 1 million and the average length of the transactions was 4 items. We generated this dataset using the Quest datamining dataset generator which we obtained from IBM Almaden <ref> [39] </ref>. For generating the frequent 10 itemsets, we used a minimum support parameter of 0.001 (0.1%). Materialized views: for mview, we used a dataset with three 4 GB derived relations (134 million tuples each).
Reference: [40] <author> D. Quass, A. Gupta, I. Mumick, and J. Widom. </author> <title> Making views self-maintainable for data warehousing. </title> <booktitle> In Proc. of PDIS'96, </booktitle> <year> 1996. </year>
Reference-contexts: First, we present Active Disk algorithms for a suite of eight common decision support tasks: select, aggregation, group-by, the datacube operation [24], external sort, project-join queries, datamining association rules from retail transaction data and materialized views. We have derived these algorithms from well-known efficient algorithms in the literature <ref> [4, 7, 14, 21, 40, 54] </ref>. Second, we compare the performance of Active Disks with that of shared memory multiprocessor servers for these tasks. <p> Materialized views are pre-computed views that are used to speedup queries to decision support databases in effect, materialized views are cached versions of views. Incremental view maintenance keeps cached views consistent with their base relations as the base relations are modified. Mumick et al <ref> [14, 40] </ref> have proposed efficient algorithms for maintaining select-project-join views (see Figure 5 for an example of such views). We borrowed the ideas of deferred maintenance and self-maintainable views from their algorithms. We first describe our core algorithm for parallel maintenance of select-project-join views. <p> For the view defined in Figure 5, the relations store, sale, line and item are partitioned on the attributes store id, sale id, item id and item id respectively. An auxiliary view V i is created for every base relation R i (as in <ref> [40] </ref>) by applying the appropriate select and project operations. The auxiliary view corresponding to a relation is partitioned the same way as the relation itself and is kept sorted. Updates to the base relations are allowed to proceed immediately. This allows base relations to be consistent at all times.
Reference: [41] <author> J. Richardson and M. Carey. </author> <title> Programming constructs for database system implementation in EXODUS. </title> <booktitle> In Proc. of SIGMOD'87, </booktitle> <year> 1987. </year>
Reference-contexts: According to Graefe [21], "this model of operator implementation and scheduling resembles very closely those used in relational systems, e.g., System R (and later SQL/DS and DB2), Ingres, Informix, and Oracle as well as in experimental systems, e.g., the E programming language used in EXODUS <ref> [41] </ref>, Genesis [12], and Starburst [25]. Operators implemented in this model are called iterators, streams, synchronous pipelines, row-sources, or similar names in the lingo of commercial systems".
Reference: [42] <author> E. Riedel, G. Gibson, and C. Faloutsos. </author> <title> Active storage for large scale data mining and multimedia applications. </title> <booktitle> In Proceedings of 24th Conference on Very Large Databases, </booktitle> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: To meet this need, several researchers have recently proposed Active Disk/IDISK architectures which integrate substantial processing power and memory into disk units <ref> [2, 22, 27, 42] </ref>. These architectures allow application-specific code to be downloaded and executed on the data that is being read from (written to) disk. To utilize Active Disks, an application is partitioned between a host-resident component and a disk-resident component. <p> Unlike the limited repertoire of operations performed by database machines, Active Disks take advantage of algorithmic research for shared-nothing architectures to provide efficient implementations of a wide variety of operations. Furthermore, Active Disks can be used to perform operations for non-relational data such as image processing <ref> [2, 42] </ref> and file-system and security-related processing [18, 51]. Riedel et al [42] have also evaluated Active Disk architectures for databases. <p> Furthermore, Active Disks can be used to perform operations for non-relational data such as image processing [2, 42] and file-system and security-related processing [18, 51]. Riedel et al <ref> [42] </ref> have also evaluated Active Disk architectures for databases. They show that Active 4 http://www.emulex.com 14 Disks are able to provide scalable performance for nearest neighbor search in multimedia databases, for frequent itemset determination for datamining association rules in retail transaction data and for edge-detection algorithms.
Reference: [43] <author> C. Ruemmler and J. Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-29, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Disksim has a detailed disk model that supports zoned disks, spare regions, segmented caches, defect management, prefetch algorithms, bus delays and control overheads. Disksim has been validated against several disk drives using the published disk specifications and SCSI logic analyzers; it achieves high accuracy theworst case demerit figure <ref> [43] </ref> for Disksim is only 2.0% of the corresponding average response time [19]. For modeling I/O interconnects, Howsim uses a simple queue-based model that has parameters for startup latency, transfer speed and the capacity of the interconnect.
Reference: [44] <author> S. Schuster, H. Nguyen, E. Ozkarahan, and K. Smith. </author> <title> RAP.2 an associative processor for databases and its applications. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 28(6), </volume> <year> 1979. </year>
Reference-contexts: Database machines [46] in the late 1970s proposed various levels of processor integration into disk: processor per track [30, 35, 47, 49], processor per head [10, 29], and "off-the-disk" designs that used a shared disk cache with multiple processors and multiple disks <ref> [16, 44] </ref>. There are several differences between the database machines of late 1970's and today's Active Disks. Unlike the database machine processors, the processors proposed for Active Disks are general-purpose commodity components and are likely to evolve as the disks evolve.
Reference: [45] <author> Seagate Technology Inc. </author> <title> The Cheetah 9LP Family: ST39102 Product Manual, </title> <month> July </month> <year> 1998. </year> <note> Publication number 83329240 Rev B. </note>
Reference-contexts: To understand the impact of scaling individual components and to identify the bottleneck resources for individual algorithms, we performed additional experiments by selectively scaling individual components. For all configurations, we assumed disks similar to the Seagate 39102FC from the Cheetah 9LP disk family <ref> [45] </ref>. These disks have a spindle speed of 10,025 rpm, a formatted media transfer rate of 14.5-21.3 MB/s, an average seek time of 5.4 ms/6.2 ms (read/write) and a maximum seek time of 12.2 ms/13.2 ms (read/write). They support Ultra2 SCSI and dual-ported Fibre Channel interfaces.
Reference: [46] <author> D. Slotnick. </author> <title> Logic per track devices. </title> <booktitle> Advances in Computers, </booktitle> <volume> 10 </volume> <pages> 291-6, </pages> <year> 1970. </year>
Reference-contexts: The I/O processors in the IBM 360 allowed users to download channel programs that were able to make I/O requests on behalf of the host programs [38]. One of the ISAM implementations on the IBM 360 used channel programs to traverse disk-resident linked lists. Database machines <ref> [46] </ref> in the late 1970s proposed various levels of processor integration into disk: processor per track [30, 35, 47, 49], processor per head [10, 29], and "off-the-disk" designs that used a shared disk cache with multiple processors and multiple disks [16, 44].
Reference: [47] <author> D. Slotnick. </author> <title> Logic per track devices. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 1(3), </volume> <month> Sep </month> <year> 1976. </year>
Reference-contexts: One of the ISAM implementations on the IBM 360 used channel programs to traverse disk-resident linked lists. Database machines [46] in the late 1970s proposed various levels of processor integration into disk: processor per track <ref> [30, 35, 47, 49] </ref>, processor per head [10, 29], and "off-the-disk" designs that used a shared disk cache with multiple processors and multiple disks [16, 44]. There are several differences between the database machines of late 1970's and today's Active Disks.
Reference: [48] <author> P. Strenstrom, E. Hagersten, D. Lilja, M. Martonosi, and M. Venugopal. </author> <booktitle> Trends in shared memory multiprocessing. IEEE Computer, </booktitle> <year> 1997. </year>
Reference-contexts: We have derived these algorithms from well-known efficient algorithms in the literature [4, 7, 14, 21, 40, 54]. Second, we compare the performance of Active Disks with that of shared memory multiprocessor servers for these tasks. Shared memory multiprocessors are widely used for relational databases (Strenstrom et al <ref> [48] </ref> estimate that in 2000, 40% of such machines will be sold for handling relational databases).
Reference: [49] <author> S. Su and G. Lipovski. CASSM: </author> <title> a cellular system for very large databases. </title> <booktitle> In Proc. of VLDB'75, </booktitle> <pages> pages 456-72, </pages> <year> 1975. </year>
Reference-contexts: One of the ISAM implementations on the IBM 360 used channel programs to traverse disk-resident linked lists. Database machines [46] in the late 1970s proposed various levels of processor integration into disk: processor per track <ref> [30, 35, 47, 49] </ref>, processor per head [10, 29], and "off-the-disk" designs that used a shared disk cache with multiple processors and multiple disks [16, 44]. There are several differences between the database machines of late 1970's and today's Active Disks.
Reference: [50] <author> R. Wahbe, S. Lucco, T. Anderson, and S. Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 203-16, </pages> <year> 1993. </year>
Reference-contexts: Furthermore, all memory accesses by a disklet must be within a sandbox defined by the buffers for its input stream (s) and the long-term scratch space (if any). The disklet binary is analyzed at download-time (as in software fault-isolation <ref> [50] </ref>); disklets that may violate memory-safety are rejected. The stream-based programming model simplifies the analysis as it defines a natural sandbox for disklets. Communication between a disklet and its environment is restricted to its input and output streams.
Reference: [51] <author> R. Wang. </author> <title> A file system for intelligent disks 9 . Talk at NASD/NSIC Meeting, </title> <month> June </month> <year> 1998. </year>
Reference-contexts: Furthermore, Active Disks can be used to perform operations for non-relational data such as image processing [2, 42] and file-system and security-related processing <ref> [18, 51] </ref>. Riedel et al [42] have also evaluated Active Disk architectures for databases.
Reference: [52] <author> R. Winter and K. </author> <title> Auerbach. Giants walk the earth: </title> <booktitle> the 1997 VLDB survey. Database Programming and Design, </booktitle> <volume> 10(9), </volume> <month> Sep </month> <year> 1997. </year>
Reference-contexts: These trends indicate that the processing demand for large decision support databases is growing faster than the improvement in performance of commodity processors. Results from the 1997 and 1998 Winter Very Large Database surveys document the growth of decision support databases <ref> [52, 53] </ref>. For example, the Sears Roebuck and Co decision support database grew from 1.3 TB in 1997 to 4.6 TB in 1998. The usage trends indicate that there is a change in user expectations regarding large databases from primarily archival storage to frequent reprocessing in their entirety.
Reference: [53] <author> R. Winter and K. </author> <title> Auerbach. The big time: </title> <booktitle> the 1998 VLDB survey. Database Programming and Design, </booktitle> <volume> 11(8), </volume> <month> Aug </month> <year> 1998. </year>
Reference-contexts: These trends indicate that the processing demand for large decision support databases is growing faster than the improvement in performance of commodity processors. Results from the 1997 and 1998 Winter Very Large Database surveys document the growth of decision support databases <ref> [52, 53] </ref>. For example, the Sears Roebuck and Co decision support database grew from 1.3 TB in 1997 to 4.6 TB in 1998. The usage trends indicate that there is a change in user expectations regarding large databases from primarily archival storage to frequent reprocessing in their entirety.
Reference: [54] <author> M. Zaki, S. Parthasarathy, and W. Li. </author> <title> A localized algorithm for parallel association mining. </title> <booktitle> In Proceedings of the 9th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1997. </year> <note> 8 Available at http://www.almaden.ibm.com/cs/quest/syndata.html. 9 http://www.nsic.org/nasd/1998-jun/wang.pdf 18 </note>
Reference-contexts: First, we present Active Disk algorithms for a suite of eight common decision support tasks: select, aggregation, group-by, the datacube operation [24], external sort, project-join queries, datamining association rules from retail transaction data and materialized views. We have derived these algorithms from well-known efficient algorithms in the literature <ref> [4, 7, 14, 21, 40, 54] </ref>. Second, we compare the performance of Active Disks with that of shared memory multiprocessor servers for these tasks. <p> Note that the first pass of join is communication-intensive and requires all-to-all communication. Since both sort and join repartition their entire dataset, their communication requirements are similar. Datamining: we focus on determining frequent itemsets for mining association rules in retail transaction data [5]. We used the eclat algorithm <ref> [54] </ref> as the starting point for our algorithms. It is a multi-pass algorithm with the first two passes same as the count distribution algorithm proposed by Agrawal et al [6].
References-found: 54

