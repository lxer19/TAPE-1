URL: http://www.research.att.com/~wcohen/postscript/sigir-96.ps
Refering-URL: http://www.csi.uottawa.ca/~debruijn/irbib.html
Root-URL: 
Email: fwcohen,singerg@research.att.com  
Phone: Phone: 908 582  Fax: 908 582 7550  
Title: Context-sensitive learning methods for text categorization  
Author: William W. Cohen and Yoram Singer 
Address: 600 Mountain Avenue Murray Hill, NJ 07974  2092  
Affiliation: AT&T Research  
Abstract: Two recently implemented machine learning algorithms, RIPPER and sleeping experts for phrases, are evaluated on a number of large text categorization problems. These algorithms both construct classifiers that allow the "context" of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification. However, RIPPER and sleeping experts differ radically in many other respects: differences include different notions as to what constitutes a context, different ways of combining contexts to construct a classifier, different methods to search for a combination of contexts, and different criteria as to what contexts should be included in such a combination. In spite of these differences, both RIPPER and sleeping experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods. We view this result as a confirmation of the usefulness of classifiers that represent contextual information. 
Abstract-found: 1
Intro-found: 1
Reference: [ Apte et al., 1994 ] <author> Chidanand Apte, Fred Damerau, and Sholom M. Weiss. </author> <title> Automated learning of decision rules for text categorization. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 12(3) </volume> <pages> 233-251, </pages> <year> 1994. </year>
Reference: [ Blum, 1990 ] <author> Avrim Blum. </author> <title> Learning boolean functions in a infinite attribute space. </title> <booktitle> In 22nd Annual Symposium on the Theory of Computing. </booktitle> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: It is based on two recent advances in multiplicative update algorithms. The first is a weight allocation algorithm called Hedge [ Freund and Schapire, 1995 ] , which is applicable to a broad class of learning problems and loss functions. The second is the infinite attribute model <ref> [ Blum, 1990 ] </ref> . In this setting, there may be any number of experts, but only a few actually post predictions on any given example; the remainder are said to be "sleeping" on that example. In the context of document classification, an expert can be any lexical unit.
Reference: [ Blum, 1995 ] <author> Avrim Blum. </author> <title> Empirical support for WIN NOW and weighted majority algorithms: results on a calendar scheduling domain. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <address> Lake Taho, California, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Moreover, empirical evidence indicates that multiplicative update algorithms often outperform traditional learning techniques for linear classifiers <ref> [ Blum, 1995; Lewis et al., 1996 ] </ref> . The sleeping experts algorithm is a procedure of the above type. It is based on two recent advances in multiplicative update algorithms.
Reference: [ Buckley et al., 1994 ] <author> C. Buckley, G. Salton, and J. Allan. </author> <title> The effect of adding relevance information in a relevance feedback environment. </title> <booktitle> In 17'th Int'l ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1994. </year>
Reference-contexts: The parameters fl and fi control the relative contribution of the positive and negative examples to the prototypes vector; we use the standard values fi = 16 and fl = 4 <ref> [ Buckley et al., 1994 ] </ref> . Documents to be classified are first converted into weight vectors, and then compared against the prototype ~v c by computing the dot product.
Reference: [ Cesa-Bianchi et al., 1993 ] <author> Nicolo Cesa-Bianchi, Yoav Fre und, David P. Helmbold, David Haussler, Robert E. Schapire, and Manfred K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <month> May </month> <year> 1993. </year> <note> Submitted to the Journal of the ACM. </note>
Reference-contexts: To construct this large linear combination, sleeping experts uses a multiplicative update algorithm that has strong theoretical justifications, being based on recent work in competitive analysis of learning algorithms <ref> [ Cesa-Bianchi et al., 1993 ] </ref> and weak-hypothesis boosting [ Freund and Schapire, 1995 ] . Finally, the conjunctions that are included in Ripper's hypotheses always represent "contexts" that are positively correlated with the class being learned. <p> However, since f fi is monotonically increasing such that f fi (0) = 0; f fi (1=2) = 1=2, and f fi (1) = 1 (see <ref> [ Vovk, 1990; Cesa-Bianchi et al., 1993 ] </ref> ), then for classification purposes we can simply use the weighted sum of the predictions itself. document, and then updates the weights of the mini-experts appropriately.
Reference: [ Church and Gale, 1995 ] <author> K. W. Church and W. A. Gale. </author> <title> Poisson mixtures. </title> <booktitle> Natural Language Engineering, </booktitle> <volume> 1(2) </volume> <pages> 163-190, </pages> <year> 1995. </year>
Reference-contexts: We would like to note that more `sophisticated' experts can be constructed. For instance, an expert may take into account the number of appearances of the phrase in the current (and previous documents), or use an inverse document frequency or a mixture of Poisson models <ref> [ Church and Gale, 1995 ] </ref> to predict the classification of the document. However, it is not clear whether an on-line (incremental) prediction scheme that is based on such information can be efficiently constructed. Two properties of this algorithm require further explanation.
Reference: [ Cohen and Singer, 1996 ] <author> William W. Cohen and Yoram Singer. </author> <title> Learning to query the Web. </title> <booktitle> In Proceedings of AAAI-96 Workshop on Internet-Based Information Systems, </booktitle> <year> 1996. </year>
Reference-contexts: If a ruleset is compact, it is relatively easy for people to understand; this may make it easier for users to accept a learned classifier as being reasonable. Rulesets can also be easily converted to queries for a boolean search engine <ref> [ Cohen and Singer, 1996 ] </ref> . Ripper builds a ruleset by repeatedly adding rules to an empty ruleset until all positive examples are covered.
Reference: [ Cohen, 1995a ] <author> William W. Cohen. </author> <title> Fast effective rule induc tion. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <address> Lake Taho, California, 1995. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: In this paper, we will investigate the performance of two recently implemented machine learning algorithms on a number of large text categorization problems. The two algorithms considered are set-valued Ripper, a recent rule learning algorithm <ref> [ Cohen, 1995a; Cohen, 1996 ] </ref> , and sleeping experts, a new on-line learning method. These algorithms share several features that make them attractive for large text categorization problems. First, both algorithms are efficient on large, noisy corpora, running in linear or nearly linear time.
Reference: [ Cohen, 1995b ] <author> William W. Cohen. </author> <title> Text categorization and relational learning. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <address> Lake Taho, Califor-nia, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Cohen, 1996 ] <author> William W. Cohen. </author> <title> Learning with set-valued features. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> Portland, Oregon, </address> <year> 1996. </year>
Reference-contexts: In this paper, we will investigate the performance of two recently implemented machine learning algorithms on a number of large text categorization problems. The two algorithms considered are set-valued Ripper, a recent rule learning algorithm <ref> [ Cohen, 1995a; Cohen, 1996 ] </ref> , and sleeping experts, a new on-line learning method. These algorithms share several features that make them attractive for large text categorization problems. First, both algorithms are efficient on large, noisy corpora, running in linear or nearly linear time. <p> All symbols w i that appear as elements of attribute a for some training example are considered by Ripper. This extension is described in more detail elsewhere <ref> [ Cohen, 1996 ] </ref> . In our experiments, a document is generally represented with a single set-valued feature, the value of which is the set of all words appearing in the document.
Reference: [ Freund and Schapire, 1995 ] <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Proceedings of the Second European Conference on Computational Learning Theory, </booktitle> <pages> pages 23-37. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> A long version will appear in JCSS. </note>
Reference-contexts: To construct this large linear combination, sleeping experts uses a multiplicative update algorithm that has strong theoretical justifications, being based on recent work in competitive analysis of learning algorithms [ Cesa-Bianchi et al., 1993 ] and weak-hypothesis boosting <ref> [ Freund and Schapire, 1995 ] </ref> . Finally, the conjunctions that are included in Ripper's hypotheses always represent "contexts" that are positively correlated with the class being learned. <p> The sleeping experts algorithm is a procedure of the above type. It is based on two recent advances in multiplicative update algorithms. The first is a weight allocation algorithm called Hedge <ref> [ Freund and Schapire, 1995 ] </ref> , which is applicable to a broad class of learning problems and loss functions. The second is the infinite attribute model [ Blum, 1990 ] .
Reference: [ Ittner et al., 1995 ] <author> David J. Ittner, David D. Lewis, and David D. Ahn. </author> <title> Text categorization of low quality images. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 301-315, </pages> <address> Las Vegas, NV, </address> <year> 1995. </year> <institution> ISRI; Univ. of Nevada, </institution> <address> Las Vegas. </address>
Reference: [ Kivinen and Warmuth, 1994 ] <author> J. Kivinen and M. K. </author> <title> War muth. Exponentiated gradient versus gradient descent for linear predictors. </title> <type> Technical Report UCSC-CRL-94-16, </type> <institution> University of California, Santa Cruz, Computer Research Laboratory, </institution> <month> June </month> <year> 1994. </year> <note> Revised December 7, 1995. An extended abstract to appeared in the STOC 95, pp. 209-218. </note>
Reference-contexts: Several weight updating methods have been examined and analysed. Formal results show that under some circumstances, very high dimensional weight allocation problems can be handled with moderate amounts of training data, if an appropriate multiplicative weight update algorithm is used <ref> [ Littlestone and Warmuth, 1994; Kivinen and Warmuth, 1994 ] </ref> . Moreover, empirical evidence indicates that multiplicative update algorithms often outperform traditional learning techniques for linear classifiers [ Blum, 1995; Lewis et al., 1996 ] . The sleeping experts algorithm is a procedure of the above type.
Reference: [ Lewis and Catlett, 1994 ] <author> David Lewis and Jason Catlett. </author> <title> Heterogeneous uncertainty sampling for supervised learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh Annual Conference, </booktitle> <address> New Brunswick, New Jersey, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Unless stated otherwise, Ripper will be used with negative word tests forbidden. A second extension to Ripper motivated by text categorization problems allows the user to specify a loss ratio <ref> [ Lewis and Catlett, 1994 ] </ref> . A loss ratio indicates the ratio of the cost of a false negative to the cost of a false positive; the goal of learning is to minimize misclassification cost on unseen data. <p> This dataset is described in more detail elsewhere <ref> [ Lewis and Gale, 1994; Lewis and Catlett, 1994 ] </ref> . The corpus contains 319,463 documents in the training set and 51,991 documents in the test set. The headlines are an average of nine words long, with a total vocabulary is 67,331 words.
Reference: [ Lewis and Gale, 1994 ] <author> David Lewis and William Gale. </author> <title> Training text classifiers by uncertainty sampling. </title> <booktitle> In Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1994. </year>
Reference-contexts: This dataset is described in more detail elsewhere <ref> [ Lewis and Gale, 1994; Lewis and Catlett, 1994 ] </ref> . The corpus contains 319,463 documents in the training set and 51,991 documents in the test set. The headlines are an average of nine words long, with a total vocabulary is 67,331 words.
Reference: [ Lewis and Ringuette, 1994 ] <author> David Lewis and Mark Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval, </booktitle> <address> Las Vegas, Nevada, </address> <year> 1994. </year>
Reference: [ Lewis et al., 1996 ] <author> David Lewis, Robert Schapire, James P. Callan, and Ron Papka. </author> <title> Training algorithms for linear classifiers. </title> <booktitle> In Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <address> Zurich, Switzerland, </address> <year> 1996. </year>
Reference-contexts: Moreover, empirical evidence indicates that multiplicative update algorithms often outperform traditional learning techniques for linear classifiers <ref> [ Blum, 1995; Lewis et al., 1996 ] </ref> . The sleeping experts algorithm is a procedure of the above type. It is based on two recent advances in multiplicative update algorithms.
Reference: [ Lewis, 1992 ] <author> David Lewis. </author> <title> Representation and learning in information retrieval. </title> <type> Technical Report 91-93, </type> <institution> Computer Science Dept., University of Massachusetts at Amherst, </institution> <year> 1992. </year> <type> PhD Thesis. </type>
Reference-contexts: and a version of Rocchio that uses a threshold chosen to optimize the F measure at fi = 1. (Re-call that our implementation of Rocchio chooses a threshold so as to minimize error rate.) 3.3 The Reuters-22173 corpus 3.3.1 Experiments Another set of experiments were conducted on the Reuters-22173 dataset <ref> [ Lewis, 1992 ] </ref> , a corpus of 22,173 news stories which have been tagged with 135 different topics. Here we largely followed the methodology of Lewis and Ringuette [ 1994 ] . <p> Table 5 summarizes these "micro-averaged break-even" points for sleeping experts, with three word phrases, two word phrases, and single word phrases; Ripper, with and 4 "Function words" include high frequency by contentless words like "of" and "the". We used the stop-list given by Lewis <ref> [ Lewis, 1992 ] </ref> . Micro-Dataset Learner Options Averaged Break-Even Lewis Experts 3-words 0.753 dataset Experts 2-words 0.737 Ripper (negative tests) 0.719 Ripper 0.716 decision tree 90 boolean feat. 0.670 Rocchio 0.660 Experts 1-word 0.656 prop. Bayes 10 boolean feat. 0.650 Apte Ripper (negative tests) 0.796 et al.
Reference: [ Littlestone and Warmuth, 1994 ] <author> Nick Littlestone and Manfred Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: Several weight updating methods have been examined and analysed. Formal results show that under some circumstances, very high dimensional weight allocation problems can be handled with moderate amounts of training data, if an appropriate multiplicative weight update algorithm is used <ref> [ Littlestone and Warmuth, 1994; Kivinen and Warmuth, 1994 ] </ref> . Moreover, empirical evidence indicates that multiplicative update algorithms often outperform traditional learning techniques for linear classifiers [ Blum, 1995; Lewis et al., 1996 ] . The sleeping experts algorithm is a procedure of the above type.
Reference: [ Rocchio, 1971 ] <author> J. Rocchio. </author> <title> Relevance feedback information retrieval. </title> <editor> In Gerard Salton, editor, </editor> <booktitle> The Smart retrieval system|experiments in automatic document processing, </booktitle> <pages> pages 313-323. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1971. </year>
Reference-contexts: distribution defined by the set of active mini-experts on each example,~p, the cumulative loss of the master algorithm over all t can be bounded relative to the loss suffered by the best possible fixed weight vector. 2.3 Rocchio As a basis for comparison, we implemented a version of Roc-chio's algorithm <ref> [ Rocchio, 1971 ] </ref> , as adapted to text categorization by Ittner et al. [ 1995 ] . We represent the data (both training and test documents) as vectors of numeric weights.
Reference: [ Salton, 1991 ] <author> Gerard Salton. </author> <title> Developments in automatic text retrieval. </title> <journal> Science, </journal> <volume> 253 </volume> <pages> 974-980, </pages> <year> 1991. </year>
Reference-contexts: The weight vector for the mth document is v m = (v m 2 ; : : : ; v m l ), where l is the number of indexing terms used. We use single words as terms. We follow the TF-IDF weighting <ref> [ Salton, 1991 ] </ref> and define the weight v m k to be: m f m P l j log (N D =n j ) Here, N D is the number of documents, n k is the number of documents in which the indexing term k appears, and f m k
Reference: [ Van Rijsbergen, 1979 ] <author> C. J. Van Rijsbergen. </author> <title> Information Retrieval. Butterworth, </title> <address> London, </address> <note> second edition, </note> <year> 1979. </year>
Reference-contexts: C4.5 using a sample of 10,000 examples (from Lewis and Catlett), and Ripper with negative word tests allowed (i.e., Ripper, allowing tests of the form e 62 S.) Again, sleeping experts with single word phrases is also included as an additional example of a 3 The F-measure is defined as <ref> [ Van Rijsbergen, 1979, pages 168-176 ] </ref> F fi (fi 2 + 1)precision recall fi 2 precision + recall where fi is a parameter that controls the importance given to precision relative to recall; a value of fi = 1 corresponds to equal weighting of precision and recall, with higher scores
Reference: [ Vovk, 1990 ] <author> V. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Pro ceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1990. </year>
Reference-contexts: However, since f fi is monotonically increasing such that f fi (0) = 0; f fi (1=2) = 1=2, and f fi (1) = 1 (see <ref> [ Vovk, 1990; Cesa-Bianchi et al., 1993 ] </ref> ), then for classification purposes we can simply use the weighted sum of the predictions itself. document, and then updates the weights of the mini-experts appropriately.
Reference: [ Wiener et al., 1995 ] <author> E. Wiener, J. O. Pederson, and A. S. Wiegend. </author> <title> A neural network approach to topic spotting. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 317-332, </pages> <address> Las Vegas, Nevada, </address> <year> 1995. </year>
References-found: 24

