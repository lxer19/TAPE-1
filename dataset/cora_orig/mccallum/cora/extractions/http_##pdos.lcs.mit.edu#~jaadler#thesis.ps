URL: http://pdos.lcs.mit.edu/~jaadler/thesis.ps
Refering-URL: http://pdos.lcs.mit.edu/~jaadler/
Root-URL: 
Title: Implementing Distributed Shared Memory on an Extensible Operating System  
Author: by Joseph Adler 
Degree: Submitted to the Department of Electrical Engineering and Computer Science  in partial fulfillment of the requirements for the degrees of Masters of Engineering and Bachelor of Science in Computer Science and Engineering Thesis Supervisor: M. Frans Kaashoek Title: Associate Professor of Computer Science and Engineering  
Date: on May 23 1997,  
Address: 545 Technology Square Cambridge MA 02139  
Affiliation: MIT Laboratory for Computer Science  
Abstract: The adoption of parallel computing systems for networks of workstations has been not only been limited by a lack of appropriate hardware and application software, but has also been limited by a lack of adequate operating system support. A version of the C Region Library (CRL) has been constructed for a system on a standard UNIX operating system and on an extensible operating system. These experiments have demonstrated that it is exceedingly difficult to get adequate communication performance using an off-the-shelf operating system. CRL is a distributed shared memory system designed to facilitate the programming of multiprocessing computers that share information through message passing. Performance tests comparing CRL on UNIX and XOK have indicated that CRL in conjunction with an extensible operating system allows user applications to achieve better communication performance on similar hardware. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Barak A. and La'adan O. </author> <title> Performance of the mosix parallel system for a cluster of pc's. </title> <booktitle> In Proceedings on HPCN Europe '97, </booktitle> <month> April 28-30, April </month> <year> 1997. </year>
Reference-contexts: While giving user processes the ability to directly communicate with network hardware yields high efficiency, it limits the security of the system, and makes it somewhat more difficult to run multiple threads on each node, or to use an interrupt-driven system. The MOSIX project [2], <ref> [1] </ref>, at the Hebrew University also constructed a NOW from readily available components. They used a set of Pentium PCs and a Myranet network. The MOSIX project did investigate operating system support, but focused mainly on scheduling questions and code mobility, and not on cheap communications.
Reference: [2] <author> Barak A., Laden O., and Yarom Y. </author> <title> The now mosix and its preemptive process migration scheme. </title> <journal> Bulletin of the IEEE Technical Committee on Operating Systems and Application Environments, </journal> <volume> 7(2) </volume> <pages> 5-11, </pages> <month> summer </month> <year> 1995. </year>
Reference-contexts: While giving user processes the ability to directly communicate with network hardware yields high efficiency, it limits the security of the system, and makes it somewhat more difficult to run multiple threads on each node, or to use an interrupt-driven system. The MOSIX project <ref> [2] </ref>, [1], at the Hebrew University also constructed a NOW from readily available components. They used a set of Pentium PCs and a Myranet network. The MOSIX project did investigate operating system support, but focused mainly on scheduling questions and code mobility, and not on cheap communications.
Reference: [3] <author> David Culler Alan Mainwaring. </author> <title> Active messages: Organization and applications programming interface. working document, </title> <month> November </month> <year> 1995. </year>
Reference-contexts: The NOW project made use of Globally Layered Unix (GLUNIX) [58] as an abstraction. The GLUNIX layer allowed easy job control over the network. Several NOW efforts ([15], [41]) have address the issue of communication latency. The project made liberal use of Active Messages [60], <ref> [3] </ref>, [59] to allow efficient communication. Of particular interest is the Fast Sockets implementation [48], which bypasses the common TCP/IP network path to give much better LAN performance in the common case. <p> The idea of remote procedure calls 2 What we have implemented here is mostly just the idea of active messages. Mainwaring and Culler have produced a set of specifications for an active message layer <ref> [3] </ref>.
Reference: [4] <author> Chistina Amza, Alan L. Cox, Sandhya Dwaarkadas, Pete Keleher, Houghui Lu, Ramakrishnan Rajamony, Waimin Yu, and Willy Zwaenepoel. Treadmarks: </author> <title> Shared memory computing on networks of workstations, </title> <year> 1994. </year>
Reference-contexts: The Parastation project [64] focused on building a system that allowed fast communications through PVM or MPI using specialized network hardware. NASA's Beowulf project linked together a set of Intel motherboards running UNIX with Fast Ethernet, and achieved far slower communication than in our system. 18 Treadmarks <ref> [4] </ref> was one of the earliest distributed shared memory systems to be assembled on a network of workstations. Treadmarks provides a simple set of programming abstractions and uses UDP packets on an Ethernet network, or AAL3/4 packets on an ATM network for communication. <p> Much of this research has focused on the pros and cons of various programming models or of page based versus object based models. We briefly describe some systems which can be run on a network of workstations environment. The Treadmarks <ref> [4] </ref> system described above used a novel consistency model, called Release consistency, in order to improve performance. The SHRIMP system [14] used a page based system as well, and included extra memory hardware (called Automatic Update) to improve performance.
Reference: [5] <author> Thomas E. Anderson, David E. Culler, and David A. Patterson. </author> <title> A case for now (networks of workstations. </title> <journal> IEEE Micro, </journal> <note> 1996. URL for this document: http://now.cs.berkeley.edu/Case/case.ps. </note>
Reference-contexts: Since this time, many systems have developed for using the combined computing resources of a network to solve computational problems. There are excellent arguments for constructing parallel computing systems using ordinary workstations, as spelled out more explicitly in <ref> [5] </ref>. First, real multiprocessors cost a lot of money for several reasons. The market for workstations or desktop computers dwarfs the market for parallel computing hardware. Moreover, parallel computers are often a good deal more complex and difficult to design than serial machines. <p> Despite the difficulty of programming these machines, they have become very popular in the scientific community because of their portability, and the availability of high performance libraries for many mathematical functions. Neither system requires operating system modification. The Berkeley NOW project <ref> [5] </ref>, [18], was one of the first large projects devoted to using a network of workstations a parallel machine. The NOW project first used Sun workstations, and experimented with ATM and Myranet for networking. The NOW project made use of Globally Layered Unix (GLUNIX) [58] as an abstraction.
Reference: [6] <author> Anindya Basu, Vineet Buch, Werner Vogels, and Thorsten von Eicken. U-net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Copper Mountain, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: In [45], they outline specific reasons why a traditional TCP/IP protocol stack often yields poor latencies, and propose several solutions (often at the instruction level) to optimize network performance. The U-net project, as described in [66], [65], <ref> [6] </ref>, was designed to give user level processes direct access to network interfaces to provide low latency communication. They implemented prototype systems using ATM and Fast Ethernet networks. The Spin project is another extensible operating system, which achieves safe extensibility through the use of type safe languages [7].
Reference: [7] <author> Brian Bershad, Stefan Savage, Przemyslaw Pardyak, Emin Gun Sirer, Marc Fiuczynski David Becker, Craig Chambers, and Susan Eggers. </author> <title> Extensibility, safety, and performance in the spin operating system. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Operating Systems Principles, </booktitle> <year> 1995. </year>
Reference-contexts: They implemented prototype systems using ATM and Fast Ethernet networks. The Spin project is another extensible operating system, which achieves safe extensibility through the use of type safe languages <ref> [7] </ref>. This project has investigated fast network interfaces [26]. The Scout project: [44] is another extensible operating system, and its designers have also investigated fast network interface [45]. Within our own research group, Deborah Wallach has explored mechanisms for fast communication in depth on the Aegis Exokernel [63], [62].
Reference: [8] <author> Brian N. Bershad, Matthew J Zekauskas, and Wayne A Sawdon. </author> <title> The midway distributed shared memory system. </title> <booktitle> In Proceedings of COMPCON 1993, </booktitle> <year> 1993. </year>
Reference-contexts: The Treadmarks [4] system described above used a novel consistency model, called Release consistency, in order to improve performance. The SHRIMP system [14] used a page based system as well, and included extra memory hardware (called Automatic Update) to improve performance. The Midway system <ref> [8] </ref> developed at Carnegie Melon was run on several machines, including an ATM based cluster. The Midway system achieved good performance using Entry consistency, but required some compiler support.
Reference: [9] <author> Dmitri Bertsekas and Robert Gallager. </author> <title> Data Networks. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, 2 edition, </address> <year> 1992. </year> <month> 86 </month>
Reference-contexts: Effect: If rn &gt; rn in Then rn in rn send a nack to sender 78 MsgNack (sender, sn, rn) Precondition: Effect: If rn &gt; rn in Then rn in rn Else resend packet rn A.2 Correctness Arguments For formal proofs of the correctness of the ARQ protocol, see either <ref> [9] </ref> or [40]. Here, we do not try to present complete proofs, but instead offer intuitive arguments as to why these protocols work. The arguments are separated into two portions; a liveness argument and a safety argument.
Reference: [10] <author> A. D. Birrell and B. J. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2 </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: Networks of Workstations The earliest systems for parallel programming on a Network of Workstations emerged at the same time as the earliest local area networks. During the 1970's, Birrell and Nelson invented the Remote Procedure Call <ref> [10] </ref> interface as a way of running computations on a remote machine. Since this time, many systems have developed for using the combined computing resources of a network to solve computational problems. <p> Some additional procedures which provide broadcasting and global reductions are also included in this layer. 2.4.2 Using Active Messages Active messages are usually used as a sort of asynchronous Remote Procedure Call <ref> [10] </ref>. RPCs are an abstraction developed in the 1970s to allow simple communication between computers on a network. Remote procedure calls were designed to look like normal procedure calls, so that a programmer could easily program network applications, without worrying about the specifics of the network interface.
Reference: [11] <author> Guy E. Blelloch. </author> <title> Scan primitives and parallel vector models. </title> <type> Technical Report 463, </type> <institution> Laborarory For Computer Science, Massachusetts Insitute of Technology, </institution> <year> 1989. </year>
Reference-contexts: A large number of computers actually fit this category, including vector supercomputers, and even machines like the Intel Pentium with MMX (which can execute an instruction on several 11 pieces of data simultaneously). This is a very powerful computation model, as described in <ref> [11] </ref>. * Multiple Instruction, Single Data Stream (MISD) machines This is a fairly obscure type of machine; it is much more limited than MIMD machines, and could be easily simulated by them should a programmer choose to do so. * Multiple Instruction, Multiple Data Stream (MIMD) machines These are the most
Reference: [12] <author> Robert D. Blumofe. </author> <title> Executing multithreaded programs efficiently. </title> <type> Technical Report 677, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: The CVM [57] system is another Distributed Shared Memory system written as a user level library using UDP for communication between nodes. CVM uses a variant of release consistency as its consistency model, and allows multiple threads on the same node. The CILK language and run-time system <ref> [12] </ref>, [32] developed at MIT includes an implementation for a Network of Workstations, and provides a novel consistency model called DAG-consistency [13], which is a derivative of its multithreaded, divide-and-conquer based model. CILK uses a provably efficient run time system to achieve good program performance for many applications. <p> Secondly, a better system for work division would be helpful. The task of dividing work among many nodes is extremely complex and difficult. It would be interesting to see what would happen if the distributed shared memory capabilities of CRL were merged with the scheduling capabilities of CILK <ref> [12] </ref>. The CILK language, developed by Leiserson, Blumofe, and many others, is a language designed for efficiency multithreaded programming.
Reference: [13] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Charles E. Leiserson, and Keith H. Randall. </author> <title> An analysis of dag-consistent distributed shared-memory algorithms. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 297-308, </pages> <address> Pauda, Italy, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: CVM uses a variant of release consistency as its consistency model, and allows multiple threads on the same node. The CILK language and run-time system [12], [32] developed at MIT includes an implementation for a Network of Workstations, and provides a novel consistency model called DAG-consistency <ref> [13] </ref>, which is a derivative of its multithreaded, divide-and-conquer based model. CILK uses a provably efficient run time system to achieve good program performance for many applications.
Reference: [14] <author> Matthias A. Blumrish, Kai Li, Richard Alpert, Cezry Dubnicki, and Edward W Felten. </author> <title> Virtual memory mapped network interface for the shrimp multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The project made liberal use of Active Messages [60], [3], [59] to allow efficient communication. Of particular interest is the Fast Sockets implementation [48], which bypasses the common TCP/IP network path to give much better LAN performance in the common case. The SHRIMP project <ref> [14] </ref>, [21] at Princeton built a parallel machine using commodity Pentium PCs and an Intel Paragon backplane to provide communications. The SHRIMP project used some extra caching hardware to improve the system's performance. SHRIMP allowed low cost communication using user-level instructions. <p> We briefly describe some systems which can be run on a network of workstations environment. The Treadmarks [4] system described above used a novel consistency model, called Release consistency, in order to improve performance. The SHRIMP system <ref> [14] </ref> used a page based system as well, and included extra memory hardware (called Automatic Update) to improve performance. The Midway system [8] developed at Carnegie Melon was run on several machines, including an ATM based cluster.
Reference: [15] <author> Brent Chun, Alan Mainwaring, Saul Schleimer, and Daniel Wilkerson. </author> <title> System area network mapping. </title> <booktitle> In Proceedings of SPAA'97, </booktitle> <address> Newport, Rhode Island, </address> <month> June </month> <year> 1997. </year>
Reference: [16] <author> David D. Clark. </author> <title> Structuring of systems using upcalls. </title> <booktitle> In Proceedings of the 10th ACM SOSP, </booktitle> <pages> pages 171-180, </pages> <month> December </month> <year> 1985. </year>
Reference: [17] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, 1 edition, </address> <year> 1992. </year>
Reference-contexts: An appendix is included which gives detailed descriptions of the communication protocols used in this project. 1.2 Introduction to Parallel Computing Parallel computing is an old idea; J. Von Neumann invented the first model of parallel computing almost fifty years ago. <ref> [17] </ref>. The basic notion of parallel computing is that sharing work among more processors lets you finish a job more quickly. By dividing a workload between multiple nodes, it is possible to accomplish more work in less time; parallel computing is only useful as a way to make computers faster. <p> Here, we mean that the number of broadcast messages increases proportionally to the number of nodes. For more details on this notation, see <ref> [17] </ref>. 27 needless additional complexity. With the TCP/IP version of CRL, we were unable to adequately exploit special knowledge held by the application about how messages were going to be used.
Reference: [18] <author> David E. Culler, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau, Brent Chun, Steven Lumetta, Alan Mainwaring, Richard Martin, Chad Yoshikawa, and Frederick Wong. </author> <title> Parallel computing on the berkeley now. </title> <booktitle> In Proceedings of JSPP'97 (9th Joint Symposium on Parallel Processing), </booktitle> <address> Kobe, Japan, </address> <year> 1997. </year>
Reference-contexts: Despite the difficulty of programming these machines, they have become very popular in the scientific community because of their portability, and the availability of high performance libraries for many mathematical functions. Neither system requires operating system modification. The Berkeley NOW project [5], <ref> [18] </ref>, was one of the first large projects devoted to using a network of workstations a parallel machine. The NOW project first used Sun workstations, and experimented with ATM and Myranet for networking. The NOW project made use of Globally Layered Unix (GLUNIX) [58] as an abstraction.
Reference: [19] <author> David E. Culler, Richard M. Karp, David Patterson, Abhijit Sahay, Eunice E. Santos, Klaus Erik Schauser, Ramesh Subramonian, and Thorsten von Eicken. </author> <title> Logp: a practical model of parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 39(11) </volume> <pages> 78-85, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Some simplistic models of parallel computation (like the Parallel Random Access Machine, or PRAM 10 model) ignore these costs and focus on the fundamental algorithm. More recent work, such as <ref> [19] </ref>, has given us parallel models which take these communication costs into account. The degree to which communication is required to solve a problem is described by the problem's granularity.
Reference: [20] <author> Jank J. Dongarra, Steve W. Otto, Marc Snir, and David Walker. </author> <title> Mpi for message passing in massively parallel machines and workstations. </title> <journal> Communication of the ACM, </journal> <volume> 39 </volume> <pages> 84-90, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: The two most prevalent systems for parallel computing, the Parallel Virtual Machine (PVM) system [28], and the Message Passing Interface (MPI) <ref> [20] </ref>, are based on a large set of obscure and complex commands. To efficiently program these systems, a programmer may need to learn hundreds of different communication modes! Moreover, because these machines are based on message passing, they can be extremely difficult to program. <p> designing and evaluating such a system, and our experiences implementing a DSM system with an extensible operating system. 17 1.7 Related Work 1.7.1 Networks of Workstations The most popular and widely available systems for programming a network of workstations as a single large parallel machine are PVM [28] and MPI <ref> [20] </ref>, two message passing systems which run on a large number of different machines. Both of these systems can run in a heterogeneous environment, so that a program can be simultaneously run on, for example, Sun workstations, and PCs running LINUX.
Reference: [21] <author> Cezary Dubnicki, Liviu Iftode, Edward W Felten, and Kai Li. </author> <title> Software support for virtual memory-mapped communication. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year> <month> 87 </month>
Reference-contexts: The project made liberal use of Active Messages [60], [3], [59] to allow efficient communication. Of particular interest is the Fast Sockets implementation [48], which bypasses the common TCP/IP network path to give much better LAN performance in the common case. The SHRIMP project [14], <ref> [21] </ref> at Princeton built a parallel machine using commodity Pentium PCs and an Intel Paragon backplane to provide communications. The SHRIMP project used some extra caching hardware to improve the system's performance. SHRIMP allowed low cost communication using user-level instructions.
Reference: [22] <author> Dawson M. Engler and M. Frans Kaashoek. Dpf: </author> <title> Fast, flexible message demultiplexing using dy-namic code generation. </title> <booktitle> In Proceedings of the ACM SIGCOMM'96 Conference on Applications,, Technologies, Architectures, and Protocols for Computer Communication, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: The user process can copy information directly into these buffers, and use a lightweight system call to tell the operating system when to send the message. Dynamic Packet Filters The DPF interface [23], <ref> [22] </ref> provides a very fast way to associate packets with user processes, by using dynamic compilation techniques to build filters. <p> For our high performance XOK implementation, we used a specialized packet format for our messages, encapsulated by raw Ethernet packets and not UDP packets. (See figure 3-1). We also chose to use several innovative features of the Exokernel. Fist, we took advantage of the fast Dynamic Packet Filter interface <ref> [22] </ref> to quickly demultiplex packets for CRL processes. The DPF interface provides a very fast way to associate packets with user processes by using dynamic compilation techniques to build filters. We used the user-level DMA buffer access to send assembled packets with almost no copying.
Reference: [23] <author> Dawson R. Engler. </author> <title> The design and implementation of a prototype exokernel operating system. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Below, we briefly describe the mechanisms and structures which give a user process low cost access to the network hardware: User-level DMA Buffers Fast sending is accomplished by giving user level applications direct access to DMA buffers <ref> [23] </ref>. The user process can copy information directly into these buffers, and use a lightweight system call to tell the operating system when to send the message. Dynamic Packet Filters The DPF interface [23], [22] provides a very fast way to associate packets with user processes, by using dynamic compilation techniques <p> User-level DMA Buffers Fast sending is accomplished by giving user level applications direct access to DMA buffers <ref> [23] </ref>. The user process can copy information directly into these buffers, and use a lightweight system call to tell the operating system when to send the message. Dynamic Packet Filters The DPF interface [23], [22] provides a very fast way to associate packets with user processes, by using dynamic compilation techniques to build filters.
Reference: [24] <author> Dawson R. Engler and M. Frans Kaashoek. </author> <title> Exterminate all operating system abstractions. </title> <booktitle> In Proceedings of the Fifth Workshop on Hot Topics in Operating Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference: [25] <author> Dawson R. Engler, M. Frans Kaashoek, and James O'Toole Jr. Exokernel: </author> <title> An operating system architecture for application-level resource management. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Unfortunately, standard operating systems often do not let user applications take full advantage of the performance of this hardware. We have examined the operating system problem by exploring the benefits or using an extensible operating system, the XOK Exokernel <ref> [25] </ref>, to implement a fast communication layer for the C Region Library system [34]. <p> The Exokernel is a revolutionary new operating system design, described by Dawson Engler, Frans Kaashoek in <ref> [25] </ref>. Traditional operating systems are monolithic programs which present the programmer with a specific set of abstractions for process control, communication, and storage; often, they hide memory management, scheduling, and other tasks from user level applications.
Reference: [26] <author> Marc Fiuczynski and Brian Bershad. </author> <title> An extensible protocol architecture for application-specific networking. </title> <booktitle> In Proceedings of the 1996 Winter USENIX Conference, </booktitle> <pages> pages 55-64, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: They implemented prototype systems using ATM and Fast Ethernet networks. The Spin project is another extensible operating system, which achieves safe extensibility through the use of type safe languages [7]. This project has investigated fast network interfaces <ref> [26] </ref>. The Scout project: [44] is another extensible operating system, and its designers have also investigated fast network interface [45]. Within our own research group, Deborah Wallach has explored mechanisms for fast communication in depth on the Aegis Exokernel [63], [62].
Reference: [27] <author> Gregory R. Ganger and M. Frans Kaashoek. </author> <title> Embedded inodes and explicit grouping: Exploiting disk bandwidth for small files. </title> <booktitle> In Proceedings of the USENIX Technical Conference, </booktitle> <month> January </month> <year> 1997, </year> <month> January </month> <year> 1997. </year>
Reference: [28] <author> Al Geist, Adam Beguilin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sun-deram. </author> <title> PVM: Parallel Virtual Machine, A Users' Guide and Tutorial for Networked Parallel Computing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: The two most prevalent systems for parallel computing, the Parallel Virtual Machine (PVM) system <ref> [28] </ref>, and the Message Passing Interface (MPI) [20], are based on a large set of obscure and complex commands. <p> the process of designing and evaluating such a system, and our experiences implementing a DSM system with an extensible operating system. 17 1.7 Related Work 1.7.1 Networks of Workstations The most popular and widely available systems for programming a network of workstations as a single large parallel machine are PVM <ref> [28] </ref> and MPI [20], two message passing systems which run on a large number of different machines. Both of these systems can run in a heterogeneous environment, so that a program can be simultaneously run on, for example, Sun workstations, and PCs running LINUX.
Reference: [29] <author> Sandeep K. Gupta. </author> <title> Protocol optimizations for the crl distributed shared memory system. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: For a complete description of the CRL protocol messages, see <ref> [29] </ref> and [33]. 2.2 History of NOW CRL 2.2.1 Early Implementations CRL was designed to work on the Thinking Machines CM-5. The CM-5 is a massively parallel supercomputer, with a Sparc processor and a vector unit at each node. <p> When we moved this application to a different network (a cluster of a dozen PCs running Linux), the poor performance of the underlying network became a bigger problem. 2.2.2 Protocol Optimizations Sandeep Gupta added several optimizations to CRL in the version designed for the IBM SP-2 <ref> [29] </ref>. Based on the observation that most of the cost of the CRL protocol was due to the cost of transporting messages, he made several changes to the CRL protocol to cut down on unnecessary communication. The first optimization was the "Three-way Invalidate" protocol change.
Reference: [30] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The HPAM [41] implementation of Active Messages on a network of HP workstations (using a FDDI network) got very low latency reliable communication by using some simple user level structures. However, the implementation did not provide security against malicious processes. Several projects at the University of Arizona [44], <ref> [30] </ref> have focused on building an operating systems for efficient communication. In [45], they outline specific reasons why a traditional TCP/IP protocol stack often yields poor latencies, and propose several solutions (often at the instruction level) to optimize network performance.
Reference: [31] <author> Norman C. Hutchinson and Larry ? Peterson. </author> <title> The X-kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference: [32] <author> Chistopher F. Joerg. </author> <title> The cilk system for parallel multithreaded computing. </title> <type> Technical Report 701, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: The CVM [57] system is another Distributed Shared Memory system written as a user level library using UDP for communication between nodes. CVM uses a variant of release consistency as its consistency model, and allows multiple threads on the same node. The CILK language and run-time system [12], <ref> [32] </ref> developed at MIT includes an implementation for a Network of Workstations, and provides a novel consistency model called DAG-consistency [13], which is a derivative of its multithreaded, divide-and-conquer based model. CILK uses a provably efficient run time system to achieve good program performance for many applications.
Reference: [33] <author> Kirk L. Johnson. </author> <title> High Performance All Software Distributed Shared Memory. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: These systems use more complicated networks and protocols to keep memory consistent. At some level all shared memory systems send messages between nodes (or between memory hardware). There are many different ways to implement shared memory systems. Kirk Johnson divided these up as follows in <ref> [33] </ref>: * All Hardware An all-hardware shared memory system provides all mechanisms for sharing in hardware. <p> For a complete description of the CRL protocol messages, see [29] and <ref> [33] </ref>. 2.2 History of NOW CRL 2.2.1 Early Implementations CRL was designed to work on the Thinking Machines CM-5. The CM-5 is a massively parallel supercomputer, with a Sparc processor and a vector unit at each node. <p> The nodes are connected by a high speed network which allows small messages to be sent with low latency [38]. The performance on the CM-5 was good, achieving near linear speedups on a number of different types of problems. Kirk Johnson indicated in <ref> [33] </ref> and [34] that the performance of CRL on this machines would be similar to what could be achieved on a network of workstations with a modern network. With this in mind, Sandeep Gupta and I adopted CRL to a network of Sun workstations. <p> The Active Message protocol acknowledges each message in its current implementation. This reduces the number of network messages needed for many operations. 1 This is part of CRL's CM-5 legacy. On the CM-5, interrupts could be very expensive, so polling could result in big performance gains <ref> [33] </ref>. On the XOK implementation, this saves us a context switch, and thus several hundred cycles.
Reference: [34] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> Crl: High-performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Operating Systems Principles. ACM, </booktitle> <year> 1995. </year> <month> 88 </month>
Reference-contexts: We have examined the operating system problem by exploring the benefits or using an extensible operating system, the XOK Exokernel [25], to implement a fast communication layer for the C Region Library system <ref> [34] </ref>. <p> The nodes are connected by a high speed network which allows small messages to be sent with low latency [38]. The performance on the CM-5 was good, achieving near linear speedups on a number of different types of problems. Kirk Johnson indicated in [33] and <ref> [34] </ref> that the performance of CRL on this machines would be similar to what could be achieved on a network of workstations with a modern network. With this in mind, Sandeep Gupta and I adopted CRL to a network of Sun workstations.
Reference: [35] <author> M. Frans Kaashoek, Dawson R. Engler, Gregory R. Ganger, and Deborah A. Wallach. </author> <title> Server operating systems, </title> <note> 1996. submitted for something, I think. </note>
Reference-contexts: It seemed likely that we would be able to achieve a level of performance which approached the predicted hardware performance. Other experiments with the Exokernel, such as Greg Ganger's fast web server Cheetah <ref> [35] </ref>, and Deborah Wallach's work with CRL on the Aegis Exokernel [63], [62], had 29 shown that an order of magnitude performance difference could be achieved by managing resources at the application level. 2.3.1 The Exokernel Network Interface An Exokernel should use the minimum amount of abstraction necessary to securely multiplex
Reference: [36] <author> Butler Lampson. </author> <title> Hints for computer system design. </title> <journal> IEEE Software, </journal> <volume> 1 </volume> <pages> 11-29, </pages> <month> October </month> <year> 1984. </year>
Reference: [37] <author> Samuel J Le*e, William N. Joy, Robert S Fabry, and Michael J Karels. </author> <booktitle> Netowrking implementation notes 4.4bsd edition. included in OpenBSD documentation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: This is usually done by creating a linked-list structure (called "membufs") of packet components in the kernel, which are assembled into a single 16 packet when the packet is copied to the Ethernet card's DMA buffers <ref> [37] </ref>. It has been shown that this complicated hierarchical structure often leads to poor network performance [45]. Some systems have tried to circumvent these problems by giving user processes unprotected and direct access to network facilities through memory mapping ([41], [66]).
Reference: [38] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, , and Robert Zak. </author> <title> The network architecture of the connection machine cm-5. </title> <booktitle> In Proceedings of the 1992 ACM Symposium on Parallel Algorithms, </booktitle> <pages> pages 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The CM-5 is a massively parallel supercomputer, with a Sparc processor and a vector unit at each node. The nodes are connected by a high speed network which allows small messages to be sent with low latency <ref> [38] </ref>. The performance on the CM-5 was good, achieving near linear speedups on a number of different types of problems.
Reference: [39] <author> Will E. Leland, Murrad S. Taqqu, Walter Willinger, and Daniel V. Wilso. </author> <title> On the self-similar nature of ethernet traffic. </title> <booktitle> In Proceedings of ACM SIGComm '93, </booktitle> <address> San Fransisco, CA, </address> <month> September </month> <year> 1993. </year>
Reference: [40] <author> Nancy A. Lynch. </author> <title> Distributed Algorithms. </title> <publisher> Morgan Kauffman Publishers, Inc., </publisher> <address> San Francisco, California, 1 edition, </address> <year> 1996. </year>
Reference-contexts: The programmer would be released from writing complex protocols to manage the operation of her program, and could instead concentrate on algorithmic issues. 74 Appendix A Protocol Details In this section, we will describe the active message protocol more carefully using a precondition-effect style, as in <ref> [40] </ref>. We also argue very briefly as to the correctness of these algorithms (though not their implementations). This formal description of the algorithms is designed to supplement the informal description in the design section of this document. <p> rn &gt; rn in Then rn in rn send a nack to sender 78 MsgNack (sender, sn, rn) Precondition: Effect: If rn &gt; rn in Then rn in rn Else resend packet rn A.2 Correctness Arguments For formal proofs of the correctness of the ARQ protocol, see either [9] or <ref> [40] </ref>. Here, we do not try to present complete proofs, but instead offer intuitive arguments as to why these protocols work. The arguments are separated into two portions; a liveness argument and a safety argument.
Reference: [41] <author> Richard P. Martin. Hpam: </author> <title> An active message layer for a network of hp workstations. </title> <booktitle> In Proceedings of Hot Interconnects , 1994, </booktitle> <year> 1997. </year>
Reference-contexts: The NOW project first used Sun workstations, and experimented with ATM and Myranet for networking. The NOW project made use of Globally Layered Unix (GLUNIX) [58] as an abstraction. The GLUNIX layer allowed easy job control over the network. Several NOW efforts ([15], <ref> [41] </ref>) have address the issue of communication latency. The project made liberal use of Active Messages [60], [3], [59] to allow efficient communication. Of particular interest is the Fast Sockets implementation [48], which bypasses the common TCP/IP network path to give much better LAN performance in the common case. <p> These include the Orca object based system [54], the emerald system [53] and several other. 1.7.3 Operating Systems and Fast Networking Several projects have been devoted to finding ways to provide fast, though not necessarily secure, communication for user level processes. The HPAM <ref> [41] </ref> implementation of Active Messages on a network of HP workstations (using a FDDI network) got very low latency reliable communication by using some simple user level structures. However, the implementation did not provide security against malicious processes.
Reference: [42] <author> Richard P. Martin, Amin M. Vahdat, David E. Culler, and Thomas E. Anderson. </author> <title> Effects of communication latency, overhead, and bandwidth in a cluster architecture. </title> <booktitle> In Proceedings of ISCA 24, </booktitle> <address> Denver, Co, </address> <month> June </month> <year> 1997. </year>
Reference: [43] <author> R. M. Metcalfe and D. R. Boggs. </author> <title> Ethernet: Distributed packet switching for local computer networks. </title> <journal> Communications of the ACM, </journal> <volume> 19(7) </volume> <pages> 395-404, </pages> <month> July </month> <year> 1976. </year>
Reference: [44] <author> A. B. Montz, D. Mosberger, S. W. O'Malley, L. L. Peterson, T. A. Proebsting, and J. H. Hartman. </author> <title> Scout: A communications-oriented operating system. </title> <type> Technical Report 94-20, </type> <institution> Department of Computer Science, University of Arizona, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: The HPAM [41] implementation of Active Messages on a network of HP workstations (using a FDDI network) got very low latency reliable communication by using some simple user level structures. However, the implementation did not provide security against malicious processes. Several projects at the University of Arizona <ref> [44] </ref>, [30] have focused on building an operating systems for efficient communication. In [45], they outline specific reasons why a traditional TCP/IP protocol stack often yields poor latencies, and propose several solutions (often at the instruction level) to optimize network performance. <p> They implemented prototype systems using ATM and Fast Ethernet networks. The Spin project is another extensible operating system, which achieves safe extensibility through the use of type safe languages [7]. This project has investigated fast network interfaces [26]. The Scout project: <ref> [44] </ref> is another extensible operating system, and its designers have also investigated fast network interface [45]. Within our own research group, Deborah Wallach has explored mechanisms for fast communication in depth on the Aegis Exokernel [63], [62].
Reference: [45] <author> D. Mosberger, L. Peterson, P. Bridges, and S. O'Malley. </author> <title> Analysis of techniques to improve protocol latency. </title> <booktitle> In Proceedings of SIGCOMM '96, </booktitle> <pages> pages 73-84, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: It has been shown that this complicated hierarchical structure often leads to poor network performance <ref> [45] </ref>. Some systems have tried to circumvent these problems by giving user processes unprotected and direct access to network facilities through memory mapping ([41], [66]). However, these systems do not allow a network event to cheaply interrupt an executing process, but instead require parallel programs to explicitly poll for messages. <p> However, the implementation did not provide security against malicious processes. Several projects at the University of Arizona [44], [30] have focused on building an operating systems for efficient communication. In <ref> [45] </ref>, they outline specific reasons why a traditional TCP/IP protocol stack often yields poor latencies, and propose several solutions (often at the instruction level) to optimize network performance. <p> The Spin project is another extensible operating system, which achieves safe extensibility through the use of type safe languages [7]. This project has investigated fast network interfaces [26]. The Scout project: [44] is another extensible operating system, and its designers have also investigated fast network interface <ref> [45] </ref>. Within our own research group, Deborah Wallach has explored mechanisms for fast communication in depth on the Aegis Exokernel [63], [62].
Reference: [46] <author> David A. Patterson and John L. Hennessey. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kauffman Publishers, Inc., </publisher> <address> San Francisco, California, 2 edition, </address> <year> 1996. </year>
Reference-contexts: The calculations here are based on slightly different problem sizes than the problem sizes in the plots listed above. Table 3.1 illustrates the region sizes for different matrix and block size parameters. The next table gives the frequency with which blocks move between nodes. According to <ref> [46] </ref>, communication scales with n p 2 . Communication scaling values are given for different matrix sizes and node counts in table 3.2. The ratio of computation to communication decreases as the number of nodes increases; less work per computation is accomplished for each additional processor.
Reference: [47] <author> Abhiram G Ranade. </author> <title> How to emulate shared memory. </title> <note> preliminary version, 1997. 89 </note>
Reference: [48] <author> Steve Rodrigues, Tom Anderson, and David Culler. </author> <title> High-performance local-area communication using fast sockets. </title> <booktitle> In Proceedings of USENIX '97, </booktitle> <year> 1997. </year>
Reference-contexts: The GLUNIX layer allowed easy job control over the network. Several NOW efforts ([15], [41]) have address the issue of communication latency. The project made liberal use of Active Messages [60], [3], [59] to allow efficient communication. Of particular interest is the Fast Sockets implementation <ref> [48] </ref>, which bypasses the common TCP/IP network path to give much better LAN performance in the common case. The SHRIMP project [14], [21] at Princeton built a parallel machine using commodity Pentium PCs and an Intel Paragon backplane to provide communications.
Reference: [49] <author> J. H. Saltzer, D. P D. P. Reed, and D. D. Clark. </author> <title> End-to-end arguments in system design. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4), </volume> <month> November </month> <year> 1984. </year>
Reference: [50] <author> Daniel J. Scales, Kourosh Gharachorloo, and Chandramohan A. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Stanford's SAM [51] is another distributed memory system, which provides sharing at the level of objects. SAM is implemented a user level library, and run on a standard UNIX operating system using PVM. The Shasta <ref> [50] </ref> system developed at DEC WRL is a software based distributed shared memory system, implemented on a network of Alpha workstations connected by an ATM network or a Memory Channel network.
Reference: [51] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The degisn and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In Proceeding of the First Symposium on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: The CILK team has expressed some frustration with the non-extensible operating systems that they have used, and are interested in investigating the benefits of securely giving the CILK system control over memory mapping and communications. Stanford's SAM <ref> [51] </ref> is another distributed memory system, which provides sharing at the level of objects. SAM is implemented a user level library, and run on a standard UNIX operating system using PVM.
Reference: [52] <author> Jaswidner Pal Singh, Wold-Dietrich Weber, and Anoop Gupta. </author> <title> Splash: Stanford parallel application for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference: [53] <author> Bjarne Steensgaard and Eric Jul. </author> <title> Object and native code thread mobility among heteroge-nous computers. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Shasta works by converting executables compiled for a hardware shared 19 memory system to a form that runs in a distributed shared memory system. Additionally, several object based systems using specialized languages have been constructed. These include the Orca object based system [54], the emerald system <ref> [53] </ref> and several other. 1.7.3 Operating Systems and Fast Networking Several projects have been devoted to finding ways to provide fast, though not necessarily secure, communication for user level processes.
Reference: [54] <author> Andrew S. tAnenbaum, M. Frans Kaashoek, and Henri E. Bal. </author> <title> Parallel programming using shared object and broadcasting. </title> <booktitle> Computer, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Shasta works by converting executables compiled for a hardware shared 19 memory system to a form that runs in a distributed shared memory system. Additionally, several object based systems using specialized languages have been constructed. These include the Orca object based system <ref> [54] </ref>, the emerald system [53] and several other. 1.7.3 Operating Systems and Fast Networking Several projects have been devoted to finding ways to provide fast, though not necessarily secure, communication for user level processes.
Reference: [55] <author> Andrew S. Tannenbaum. </author> <title> Modern Operating Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, 1 edition, </address> <year> 1992. </year>
Reference: [56] <author> Andrew S. Tannenbaum. </author> <title> Distributed Operating Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, 1 edition, </address> <year> 1995. </year>
Reference: [57] <author> Kritchalach Thitikamol and Pete Keleher. </author> <title> Multi-threading and remote latency in software dsms. </title> <booktitle> In Proceedings of the 17th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: The Midway system achieved good performance using Entry consistency, but required some compiler support. Midway does not use a page based strategy, but instead asks the user to explicitly specify the relation between data and synchronization objects. The CVM <ref> [57] </ref> system is another Distributed Shared Memory system written as a user level library using UDP for communication between nodes. CVM uses a variant of release consistency as its consistency model, and allows multiple threads on the same node.
Reference: [58] <author> Amin Vahdat, Douglas Ghormley, and Thomas Anderson. </author> <title> Efficient, portable, and robust extension of operating system functionality, </title> <month> December </month> <year> 1994. </year>
Reference-contexts: The Berkeley NOW project [5], [18], was one of the first large projects devoted to using a network of workstations a parallel machine. The NOW project first used Sun workstations, and experimented with ATM and Myranet for networking. The NOW project made use of Globally Layered Unix (GLUNIX) <ref> [58] </ref> as an abstraction. The GLUNIX layer allowed easy job control over the network. Several NOW efforts ([15], [41]) have address the issue of communication latency. The project made liberal use of Active Messages [60], [3], [59] to allow efficient communication.
Reference: [59] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the Nineteenth Annual Internation Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The NOW project made use of Globally Layered Unix (GLUNIX) [58] as an abstraction. The GLUNIX layer allowed easy job control over the network. Several NOW efforts ([15], [41]) have address the issue of communication latency. The project made liberal use of Active Messages [60], [3], <ref> [59] </ref> to allow efficient communication. Of particular interest is the Fast Sockets implementation [48], which bypasses the common TCP/IP network path to give much better LAN performance in the common case.
Reference: [60] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Shauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the Nineteenth Annual International Symposium on Computer Architecture. ACM, </booktitle> <year> 1992. </year>
Reference-contexts: The NOW project made use of Globally Layered Unix (GLUNIX) [58] as an abstraction. The GLUNIX layer allowed easy job control over the network. Several NOW efforts ([15], [41]) have address the issue of communication latency. The project made liberal use of Active Messages <ref> [60] </ref>, [3], [59] to allow efficient communication. Of particular interest is the Fast Sockets implementation [48], which bypasses the common TCP/IP network path to give much better LAN performance in the common case. <p> CRL manages regions by sending Active Messages <ref> [60] </ref> between nodes. Active messages are an excellent tool for protocol design; they have been shown to provide an order of magnitude performance improvement over other message passing tools. 2 2.4.1 The Active Message Layer Interface An Active Message consists of two pieces of information.
Reference: [61] <author> Deborah Wallach, Wilson C. Hsieh, Kirk L. Johnson, M. Frans Kaashoek, and William E. Weihl. </author> <title> Optimistic active messages: A mechanism for scheduling communication with computation. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference: [62] <author> Deborah A Wallach, Dawson R. Engler, and M. Frans Kaashoek. Ashs: </author> <title> Application-specific handlers for high-performance messaging. </title> <booktitle> In Proceedings of ACM SIGCOMM '96, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: This project has investigated fast network interfaces [26]. The Scout project: [44] is another extensible operating system, and its designers have also investigated fast network interface [45]. Within our own research group, Deborah Wallach has explored mechanisms for fast communication in depth on the Aegis Exokernel [63], <ref> [62] </ref>. <p> It seemed likely that we would be able to achieve a level of performance which approached the predicted hardware performance. Other experiments with the Exokernel, such as Greg Ganger's fast web server Cheetah [35], and Deborah Wallach's work with CRL on the Aegis Exokernel [63], <ref> [62] </ref>, had 29 shown that an order of magnitude performance difference could be achieved by managing resources at the application level. 2.3.1 The Exokernel Network Interface An Exokernel should use the minimum amount of abstraction necessary to securely multiplex systems resources. <p> Packet Rings and ASHes When a packet is received, the XOK Exokernel provides two low-cost ways to give the packet to the application. When a packet needs to be dealt with immediately, whether or not the associated user process is running, Application Safe Handler, or ASHes <ref> [62] </ref>, are the appropriate choice. An ASH is a small piece of code which is allowed to execute for a limited time on packet arrival. ASHes are also guaranteed to be safe, because they execute in kernel mode. <p> On the Aegis Exokernel (running on Decstations), safety was assured by "sandboxing" ASH code <ref> [62] </ref>. Before installing and executing the code, the operating system would make sure that the code did not access or jump to arbitrary memory locations (or make up code to do the same thing).
Reference: [63] <author> Deborah Anne Wallach. </author> <title> High-Performance Application-Specific Networking. </title> <type> PhD thesis, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: This project has investigated fast network interfaces [26]. The Scout project: [44] is another extensible operating system, and its designers have also investigated fast network interface [45]. Within our own research group, Deborah Wallach has explored mechanisms for fast communication in depth on the Aegis Exokernel <ref> [63] </ref>, [62]. <p> It seemed likely that we would be able to achieve a level of performance which approached the predicted hardware performance. Other experiments with the Exokernel, such as Greg Ganger's fast web server Cheetah [35], and Deborah Wallach's work with CRL on the Aegis Exokernel <ref> [63] </ref>, [62], had 29 shown that an order of magnitude performance difference could be achieved by managing resources at the application level. 2.3.1 The Exokernel Network Interface An Exokernel should use the minimum amount of abstraction necessary to securely multiplex systems resources. <p> In many cases, there can be benefits to preemptive scheduling, but that is beyond the scope of this thesis. For more information, see <ref> [63] </ref>. To simplify the interface to many of these mechanisms, Greg Ganger has written an Extensible I/O abstraction (XIO) to simplify the process of reading packets from packet rings, and sending packets through DMA buffers. <p> instead, we were forced to design a different mechanism to achieve reasonable performance without placing arbitrary restrictions on application memory usage. 3.2 Fast Upcalls from ASHes Instead of trying to satisfy all request from within ASHes, we used a quick and simple upcall mechanism, similar to the one described in <ref> [63] </ref>. If the target application is currently running, fast 44 upcalls preempt the running process and, if it is not in a critical section, force it to execute a handler immediately. If a process is not running, the upcall occurs immediately when the process is next given CPU time.
Reference: [64] <author> Thomas M Warshcko, Joachim M. Blum, and Walter F Tichy. </author> <title> The parastation project: Using workstations as building blocks for parallel computing. </title> <booktitle> In Proceedings of the International Conference on Parallel and Distributed Processing, Techniques and Applications, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: They used a set of Pentium PCs and a Myranet network. The MOSIX project did investigate operating system support, but focused mainly on scheduling questions and code mobility, and not on cheap communications. The Parastation project <ref> [64] </ref> focused on building a system that allowed fast communications through PVM or MPI using specialized network hardware.
Reference: [65] <author> Matt Welsh, Anindya Basu, and Thorsten von Eicken. </author> <title> Low-latency communication over fast ethernet. </title> <booktitle> In Proceedings of Euro-Par '96, </booktitle> <address> Lyon, France, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: In [45], they outline specific reasons why a traditional TCP/IP protocol stack often yields poor latencies, and propose several solutions (often at the instruction level) to optimize network performance. The U-net project, as described in [66], <ref> [65] </ref>, [6], was designed to give user level processes direct access to network interfaces to provide low latency communication. They implemented prototype systems using ATM and Fast Ethernet networks. The Spin project is another extensible operating system, which achieves safe extensibility through the use of type safe languages [7].
Reference: [66] <author> Matt Welsh, Anindya Basu, and Thorsten von Eicken. </author> <title> Atm and fast ethernet network interfaces for user-level communication. </title> <booktitle> In Proceedings of the Third International Symposium on High Performance Computer Architecture (HPCA), </booktitle> <address> San Antonio, Texas, </address> <month> February </month> <year> 1997. </year>
Reference-contexts: It has been shown that this complicated hierarchical structure often leads to poor network performance [45]. Some systems have tried to circumvent these problems by giving user processes unprotected and direct access to network facilities through memory mapping ([41], <ref> [66] </ref>). However, these systems do not allow a network event to cheaply interrupt an executing process, but instead require parallel programs to explicitly poll for messages. Moreover, some user-level systems are not designed for a multi-user environment. <p> In [45], they outline specific reasons why a traditional TCP/IP protocol stack often yields poor latencies, and propose several solutions (often at the instruction level) to optimize network performance. The U-net project, as described in <ref> [66] </ref>, [65], [6], was designed to give user level processes direct access to network interfaces to provide low latency communication. They implemented prototype systems using ATM and Fast Ethernet networks. The Spin project is another extensible operating system, which achieves safe extensibility through the use of type safe languages [7].
Reference: [67] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The splash-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <address> Santa Margherita Ligure, Italy, </address> <year> 1995. </year>
Reference-contexts: We borrowed a set of applications from the Stanford Parallel Applications for Shared Memory (SPLASH-2) suite <ref> [67] </ref> for our testing. The SPLASH suite consists of a set of applications for shared memory multiprocessors which have been selected because they present challenging workloads which are representative of the types of applications people would like to run on multiprocessors.
Reference: [68] <author> Songnian Zhou, Michael Stumm, Kai Li, and David Wortman. </author> <title> Heterogeneous distributed shared memory. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 540-554, </pages> <month> September </month> <year> 1992. </year>
References-found: 68

