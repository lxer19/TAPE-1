URL: http://www.medg.lcs.mit.edu/people/wjl/papers/ml-paper.ps
Refering-URL: http://www.medg.lcs.mit.edu/publications/papers.html
Root-URL: 
Title: A Comparison of Logistic Regression to Decision-Tree Induction in a Medical Domain  
Author: William J. Long, John L. Griffith and Harry P. Selker, and Ralph B. D'Agostino 
Date: 74-97, 1993  
Note: Reprinted from Computers in Biomedical Research,26:  
Address: Cambridge, MA, USA  New England Medical Center, Boston, MA, USA  Boston, MA, USA  
Affiliation: MIT Laboratory for Computer Science,  Center for Cardiovascular Health Services Research,  Mathematics Department, Boston University,  
Abstract: This paper compares the performance of logistic regression to decision-tree induction in classifying patients as having acute cardiac ischemia. This comparison was performed using the database of 5,773 patients originally used to develop the logistic-regression tool and test it prospectively. Both the ability to classify cases and ability to estimate the probability of ischemia were compared on the default tree generated by the C4 version of ID3. They were also compared on a tree optimized on the learning set by increased pruning of overspecified branches, and on a tree incorporating clinical considerations. Both the LR tool and the improved trees performed at a level fairly close to that of the physicians, although the LR tool definitely performed better than the decision tree. There were a number of differences in the performance of the two methods, shedding light on their strengths and weaknesses.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Pozen, M. W., D'Agostino, R. B., et al, </author> <title> "The Usefulness of a Predictive Instrument to Reduce Inappropriate Admissions to the Coronary Care Unit," </title> <journal> Annals of Internal Medicine 92 </journal> <pages> 238-242, </pages> <year> 1980. </year>
Reference-contexts: Interestingly, decision-tree induction techniques have also been developed in the statistics community, but have been called "regression trees" there. These two techniques, logistic regression and decision-tree induction have often been used for very similar tasks. Pozen et al <ref> [1, 2] </ref>, and Selker et al [3], use LR to develop predictive instruments for determining the probability that an emergency-room patient with chest pain or other related symptoms actually has acute cardiac ischemia. <p> Logistic Regression has proven to be very robust in a number of domains and proves an effective way of estimating probabilities from dichotomous variables. A particularly attractive attribute of the original LR tool developed by Pozen, et al <ref> [1, 2] </ref>, and the slightly modified version generated later by the same group [3] is that they have been shown to be useful clinically and are now being used in clinical settings to assist physician decision making. 3 Data for the Comparison The data used for this comparison were collected for
Reference: [2] <author> Pozen, M. W., D'Agostino, R. B., et al, </author> <title> "A Predictive Instrument to Improve Coronary-Care-Unit Admission Practices in Acute Ischemic Heart Disease," </title> <journal> New England Journal of Medicine 310 </journal> <pages> 1273-1278, </pages> <year> 1984. </year>
Reference-contexts: Interestingly, decision-tree induction techniques have also been developed in the statistics community, but have been called "regression trees" there. These two techniques, logistic regression and decision-tree induction have often been used for very similar tasks. Pozen et al <ref> [1, 2] </ref>, and Selker et al [3], use LR to develop predictive instruments for determining the probability that an emergency-room patient with chest pain or other related symptoms actually has acute cardiac ischemia. <p> Since all of these tests were done on datasets of size in the hundreds, a possible reason for the general lack of significant differences between the methodologies is the small size. In the following study we will use the data set collected by Pozen and colleagues <ref> [2] </ref> containing 5,773 patients. <p> Logistic Regression has proven to be very robust in a number of domains and proves an effective way of estimating probabilities from dichotomous variables. A particularly attractive attribute of the original LR tool developed by Pozen, et al <ref> [1, 2] </ref>, and the slightly modified version generated later by the same group [3] is that they have been shown to be useful clinically and are now being used in clinical settings to assist physician decision making. 3 Data for the Comparison The data used for this comparison were collected for <p> they have been shown to be useful clinically and are now being used in clinical settings to assist physician decision making. 3 Data for the Comparison The data used for this comparison were collected for the multicenter development and clinical trial of the predictive instrument for coronary care unit admission <ref> [2] </ref>. The data were collected at six New England hospitals, ranging from urban teaching hospitals to rural non-teaching hospitals. <p> When the LR equation was developed, the clinical relevance and reproducibility of the variables was a primary consideration and many of the variables were eliminated on that basis alone <ref> [2] </ref>. Since no such considerations were made in producing the decision tree, it contains a number of closely related variables such as the report of chest pain in the emergency room and the indication of chest pain recorded in a later interview.
Reference: [3] <author> Selker, Harry P., Griffith, John L., and D'Agostino, Ralph B., </author> <title> "A Tool for Judging Coronary Care Unit Admission Appropriateness Valid for Both Real-Time and Retrospective Use: A Time-Insensitive Predictive Instrument (TIPI) for Acute Cardiac Ischemia," </title> <publisher> forthcoming. </publisher>
Reference-contexts: Interestingly, decision-tree induction techniques have also been developed in the statistics community, but have been called "regression trees" there. These two techniques, logistic regression and decision-tree induction have often been used for very similar tasks. Pozen et al [1, 2], and Selker et al <ref> [3] </ref>, use LR to develop predictive instruments for determining the probability that an emergency-room patient with chest pain or other related symptoms actually has acute cardiac ischemia. <p> A particularly attractive attribute of the original LR tool developed by Pozen, et al [1, 2], and the slightly modified version generated later by the same group <ref> [3] </ref> is that they have been shown to be useful clinically and are now being used in clinical settings to assist physician decision making. 3 Data for the Comparison The data used for this comparison were collected for the multicenter development and clinical trial of the predictive instrument for coronary care
Reference: [4] <author> Goldman, L., Weinberg, M., et al, </author> <title> "A Computer-Derived Protocol to Aid in the Diagnosis of Emergency Room Patients with Acute Chest Pain," </title> <journal> New England Journal of Medicine 307 </journal> <pages> 588-596, </pages> <year> 1982. </year>
Reference-contexts: Pozen et al [1, 2], and Selker et al [3], use LR to develop predictive instruments for determining the probability that an emergency-room patient with chest pain or other related symptoms actually has acute cardiac ischemia. Goldman et al <ref> [4, 5] </ref>, used the CART (Classification and Regression Trees) methodology [6] to develop decision trees for deciding whether patients fl This research was supported by National Institutes of Health Grant No. R01 HL33041 from the National Heart, Lung, and Blood Institute, No.
Reference: [5] <author> Goldman, L., Cook, E. F., et al, </author> <title> "A Computer Protocol to Predict Myocardial Infarction in Emergency Department Patients with Chest Pain," </title> <journal> New England Journal of Medicine 318 </journal> <pages> 797-803, </pages> <year> 1988. </year> <title> Logistic Regression Versus Decision-Tree Induction 21 </title>
Reference-contexts: Pozen et al [1, 2], and Selker et al [3], use LR to develop predictive instruments for determining the probability that an emergency-room patient with chest pain or other related symptoms actually has acute cardiac ischemia. Goldman et al <ref> [4, 5] </ref>, used the CART (Classification and Regression Trees) methodology [6] to develop decision trees for deciding whether patients fl This research was supported by National Institutes of Health Grant No. R01 HL33041 from the National Heart, Lung, and Blood Institute, No.
Reference: [6] <author> Breiman, L., Friedman, J. H. et al, </author> <title> Classification and Regression Trees, </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: Pozen et al [1, 2], and Selker et al [3], use LR to develop predictive instruments for determining the probability that an emergency-room patient with chest pain or other related symptoms actually has acute cardiac ischemia. Goldman et al [4, 5], used the CART (Classification and Regression Trees) methodology <ref> [6] </ref> to develop decision trees for deciding whether patients fl This research was supported by National Institutes of Health Grant No. R01 HL33041 from the National Heart, Lung, and Blood Institute, No. <p> This offers an opportunity to compare logistic regression and decision-tree induction on a large dataset where the existing logistic regression equation was carefully prepared and thoroughly tested. 2 Methodology 2.1 Decision-Tree Generation Techniques for generating decision trees, also called classification or regression trees <ref> [6] </ref>, have been developed over the past twenty years. In the machine-learning community, a number of researchers have been developing methods for inducing decision trees automatically from data sets. <p> The information-gain-ratio statistic is not the only statistic that has been used to select attributes. Mingers [14] compares the information-gain statistic with the chi-square contingency table, the G statistic, probability calculations, the GINI index of diversity (the statistic used in the CART program <ref> [6] </ref>), the information-gain ratio, and the information gain with the Marshall correction. Since that paper was written, Quinlan and Rivest [15] have added the minimum description Logistic Regression Versus Decision-Tree Induction 4 length principle to the armamentarium.
Reference: [7] <author> Mingers, J., </author> <title> "Rule Induction with Statistical Data | A Comparison with Multiple Regression," </title> <journal> J. Operational Research Society 38 </journal> <pages> 347-351, </pages> <year> 1987. </year>
Reference-contexts: Given that both approaches are being used for similar purposes, it is important to gain an understanding of the relationship between statistical regression techniques and decision-tree techniques, and their relative strengths and weaknesses. A few papers have started to look at this issue. Mingers <ref> [7] </ref> compared the ID3 rule induction algorithm (using the G-statistic rather than Quinlan's information measure) to multiple regression on a data base of football results using 164 games in the learning set and 182 games in the test set.
Reference: [8] <author> Segal, M. R. and Bloch, D. A., </author> <title> "A Comparison of Estimated Proportional Hazards Models and Regression Trees," </title> <booktitle> Statistics in Medicine 8 </booktitle> <pages> 539-550, </pages> <year> 1989. </year>
Reference-contexts: The results of this comparison favor ID3, but it is hard to draw any general conclusions because of the rather artificial nature of the five variables derived from past scores, only two of which were used by the multiple regression. Segal and Bloch <ref> [8] </ref> compared proportional-hazard models to regression trees in two studies of 604 patients and 435 patients respectively. The variables selected by each method were similar but beyond that, they were difficult to compare.
Reference: [9] <author> Harrell Jr., F. E., Lee, K. L. et al, </author> <title> "Regression Models for Prognostic Prediction: Advantages, Problems, and Suggested Solutions," Cancer Treatment Reports 69 </title> <type> 1071-1077, </type> <year> 1985. </year>
Reference-contexts: They suggest using the selection of nodes in the tree to suggest variables and interaction terms for the equation and also using the regression statistical test (in this case Cox partial likelihood) as the splitting criterion for generating trees. Harrell et al <ref> [9] </ref> compared CART trees to several other strategies. Stepwise regression did not perform as well as CART on a training sample of 110, but better than CART on a training sample of 224.
Reference: [10] <author> Kinney, E. L. and Murphy, D. D., </author> <title> "Comparison of the ID3 Algorithm versus Discriminant Analysis for Performing Feature Selection," </title> <booktitle> Computers and Biomedical Research 20 </booktitle> <pages> 467-476, </pages> <year> 1987. </year>
Reference-contexts: Harrell et al [9] compared CART trees to several other strategies. Stepwise regression did not perform as well as CART on a training sample of 110, but better than CART on a training sample of 224. Kinney and Murphy <ref> [10] </ref> compared ID3 to discriminant analysis in a learning set with 107 items and a test set of 67 items in a medical domain (detecting aortic regurgitation by auscultation). Their conclusion was that both methods performed equally poorly.
Reference: [11] <author> Gilpin, A. E., Ohlsen, R. A., </author> <title> "Predicting 1-Year Outcome following Acute Myocardial Infarc-tion: Physicians versus Computers," </title> <booktitle> Computers and Biomedical Research 23 </booktitle> <pages> 46-63, </pages> <year> 1990. </year>
Reference-contexts: Kinney and Murphy [10] compared ID3 to discriminant analysis in a learning set with 107 items and a test set of 67 items in a medical domain (detecting aortic regurgitation by auscultation). Their conclusion was that both methods performed equally poorly. A more recent study, Gilpin, et al <ref> [11] </ref>, compared regression trees, stepwise linear discriminant analysis, logistic regression, and three cardiologists predicting the probability of one-year survival of patients who had myocardial infarctions. This comparison used 781 patients in the learning set and 400 in the test set.
Reference: [12] <author> Michalski, R. S., </author> <title> "AQVAL/1 Computer Implementation of a Variable Valued Logic System VL1 and Examples of its Application to Pattern Recognition," </title> <booktitle> Proceedings of the First International Joint Conference on Pattern Recognition, </booktitle> <address> Washington, DC, </address> <pages> pp 3-17, </pages> <year> 1973. </year>
Reference-contexts: In the machine-learning community, a number of researchers have been developing methods for inducing decision trees automatically from data sets. The best known of these methods are AQ11 <ref> [12] </ref> and ID3 [13], each of which has spawned a family of programs to fit the demands of real world domains. In the statistics community the CART program is the Logistic Regression Versus Decision-Tree Induction 3 best known of the programs.
Reference: [13] <author> Quinlan, J. R., </author> <title> "Induction of Decision Trees," </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: In the machine-learning community, a number of researchers have been developing methods for inducing decision trees automatically from data sets. The best known of these methods are AQ11 [12] and ID3 <ref> [13] </ref>, each of which has spawned a family of programs to fit the demands of real world domains. In the statistics community the CART program is the Logistic Regression Versus Decision-Tree Induction 3 best known of the programs.
Reference: [14] <author> Mingers, J., </author> <title> "An Empirical Comparison of Selection Measures for Decision-Tree Induction," </title> <booktitle> Machine Learning 3 </booktitle> <pages> 319-342, </pages> <year> 1989. </year>
Reference-contexts: This is the approach taken by CART and is available in current versions of ID3, but is not used here since there is no need to lump the values together. The information-gain-ratio statistic is not the only statistic that has been used to select attributes. Mingers <ref> [14] </ref> compares the information-gain statistic with the chi-square contingency table, the G statistic, probability calculations, the GINI index of diversity (the statistic used in the CART program [6]), the information-gain ratio, and the information gain with the Marshall correction.
Reference: [15] <author> Quinlan, J. R. and Rivest, R. L., </author> <title> "Inferring Decision Trees Using the Minimum Description Length Principle," </title> <booktitle> Information and Computation 80 </booktitle> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: Mingers [14] compares the information-gain statistic with the chi-square contingency table, the G statistic, probability calculations, the GINI index of diversity (the statistic used in the CART program [6]), the information-gain ratio, and the information gain with the Marshall correction. Since that paper was written, Quinlan and Rivest <ref> [15] </ref> have added the minimum description Logistic Regression Versus Decision-Tree Induction 4 length principle to the armamentarium. Mingers' conclusion is that the predictive accuracy of the induced decision trees is not sensitive to the choice of statistic.
Reference: [16] <author> Quinlan, J. R., </author> <title> "Simplifying Decision Trees," </title> <journal> International Journal of Man Machine Studies 27 </journal> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: This paper will use the C4 version of ID3 1 using the information-gain-ratio statistic, although this comparison could certainly be extended to include other statistics. There are also multiple strategies for pruning the tree once it is generated. Quinlan <ref> [16] </ref> discusses five such strategies and finds the differences over a range of different kinds of data to be insignificant. The strategy used in C4 is called by Quinlan, pessimistic pruning.
Reference: [17] <author> Quinlan, J. R., </author> <title> "Improved Estimates for the Accuracy of Small Disjuncts," </title> <booktitle> Machine Learning 6 </booktitle> <pages> 93-98, </pages> <year> 1991. </year>
Reference-contexts: Since ID3 makes no independence assumption, it throws away this potential source of information. This issue has been addressed in the literature <ref> [17, 18] </ref> and we have adopted the solution suggested by Quinlan.
Reference: [18] <author> Holte, R. C., Acker, L. E., and Porter, B. W., </author> <title> "Concept Learning and the Problem of Small Disjuncts," </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <pages> pp 813-818, </pages> <year> 1989. </year>
Reference-contexts: Since ID3 makes no independence assumption, it throws away this potential source of information. This issue has been addressed in the literature <ref> [17, 18] </ref> and we have adopted the solution suggested by Quinlan.
Reference: [19] <author> Hanley, J. A., and McNeil, B. J., </author> <title> "A Method of Comparing the Areas under Receiver Operating Characteristic Curves Derived from the Same Cases," </title> <booktitle> Radiology 148 </booktitle> <pages> 839-843, </pages> <year> 1983. </year>
Reference-contexts: The area under the ROC curve for LR on the test set is 0.89 while that under the ID3 curve is 0.82. The difference between these two areas is significant well beyond the .0001 level using the Hanley-McNeil method with correlations computed using the Kendall tau <ref> [19] </ref>. Since the other point of comparison is the performance of the physicians in the emergency room, their performance is indicated by the markers on the graph. The squares are the performance on the learning set and the diamonds are the performance on the test set.
Reference: [20] <author> Wirth, J. and Catlett, J., </author> <title> "Experiments on the Costs and Benefits of Windowing in ID3," </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> Ann Arbor, MI, pp87-99, </address> <year> 1988. </year>
Reference-contexts: Each tree was started with windows containing 10% of the data. The error rates for the five trees are given in figure 7. These error rates are no better than the error rate with the tree generated without windowing. These conclusions concur with those of Wirth and Catlett <ref> [20] </ref>. Thus, we have chosen not to use windowing for generating trees. If one examines the default tree, it is clear that there are many places where branches divide the cases into a very small set versus the rest.
Reference: [21] <author> Baxt, W. G., </author> <title> "Use of an Artificial Neural Network for the Diagnosis of Myocardial Infarction," </title> <journal> Annals of Internal Medicine 115 </journal> <pages> 843-848, </pages> <year> 1991. </year> <title> Logistic Regression Versus Decision-Tree Induction 22 </title>
Reference-contexts: However, there are always new strategies to consider. Two that may hold promise are neural networks <ref> [21] </ref> and multivariate adaptive regression splines [22]. Both of these methods have the potential to represent more complex relationships, but with more flexibility in the model there is more potential for overspecification.
Reference: [22] <author> Friedman, J. H., </author> <title> "Multivariate Adaptive Regression Splines," </title> <journal> The Annals of Statistics 19 </journal> <pages> 1-141, </pages> <year> 1991. </year>
Reference-contexts: However, there are always new strategies to consider. Two that may hold promise are neural networks [21] and multivariate adaptive regression splines <ref> [22] </ref>. Both of these methods have the potential to represent more complex relationships, but with more flexibility in the model there is more potential for overspecification.
References-found: 22

