URL: ftp://info.mcs.anl.gov/pub/fortran-m/reports/mardi-gras.ps.Z
Refering-URL: http://www.mcs.anl.gov/fortran-m/papers/
Root-URL: http://www.mcs.anl.gov
Title: LIBRARIES FOR PARALLEL PARADIGM INTEGRATION  
Author: Ian Foster and Ming Xu 
Address: Argonne, Illinois 60439, U.S.A.  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: A programming paradigm is a method for structuring programs in order to reduce the complexity of the programming task. In parallel programming, task and data parallelism are the most common paradigms, although object, functional, logic, and database parallelism are also used. Most existing programming languages and tools are designed to support either task parallelism or data parallelism. Yet there are many applications that can benefit from a combination of both paradigms. For example, image processing problems can exploit the regularity of data-parallel computation within each stage of a pipeline. This process requires tools that support both task and data parallelism and that allow task- and data-parallel components to interact. In particular, it should be possible to reuse libraries developed in either paradigm in a variety of situations. To meet these requirements, we have designed a small set of extensions to Fortran, called Fortran M, that provides a framework on which portable, reusable libraries can be built to implement programming paradigms. In this paper, we illustrate the approach by showing how it is used to implement libraries for file I/O, message-passing, and data-parallel computation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Bischof, L. Green, K.Haigler, and T. Knauff, </author> <title> Parallel Calculation of Sensitivity Derivatives for Aircraft Design Using Automatic Differentiation, </title> <type> Preprint MCS-P419-0294, </type> <institution> Argonne National Laboratory, Argonne, Ill., </institution> <year> 1994. </year>
Reference-contexts: To explore the utility of the FM extensions for multiparadigm programming, we have been working with colleagues to develop libraries for the integration of message passing, for data parallel programming [5, 2], for redirection of file I/O, for scientific programming "templates," and for exploitation of parallelism in automatic differentiation <ref> [1] </ref>. In this paper, we describe the mechanisms used to support the first three of these applications. 2. FORTRAN M Fortran M (FM) is a language designed by researchers at Argonne and Caltech for expressing task-parallel computation [6].
Reference: [2] <author> Chandy, K. M., I. Foster, K. Kennedy, C. Koelbel, and C.-W. Tseng, </author> <title> Integrated Support for Task and Data Parallelism, </title> <journal> Intl. J. Supercomputer Applications, </journal> <note> 8(2), 1994 (in press). </note>
Reference-contexts: In a related effort, colleagues at Caltech are using an extended C++ for the same purpose [3]. To explore the utility of the FM extensions for multiparadigm programming, we have been working with colleagues to develop libraries for the integration of message passing, for data parallel programming <ref> [5, 2] </ref>, for redirection of file I/O, for scientific programming "templates," and for exploitation of parallelism in automatic differentiation [1]. In this paper, we describe the mechanisms used to support the first three of these applications. 2.
Reference: [3] <author> Chandy, K. M., and C. Kesselman, </author> <title> CC++: A Declarative Concurrent Object-Oriented Programming Notation, Research Directions in Concurrent Object-Oriented Programming, </title> <editor> Gul Agha, Peter Wegner, and Akinori Yonezawa (Eds.), </editor> <publisher> MIT Press, </publisher> <year> 1993 </year>
Reference-contexts: In this paper, we use an extended Fortran called Fortran M (FM) [6] to illustrate the approach; however, the basic ideas are language independent. In a related effort, colleagues at Caltech are using an extended C++ for the same purpose <ref> [3] </ref>. <p> We are also investigating compiler and runtime system optimization techniques and the integration of paradigm libraries written in different languages. In the latter area, a particular focus is integration with the CC++ language under development at Caltech <ref> [3] </ref>. This task is simplified by the fact that CC++ and FM use the same runtime system. More information on this work is available via Mosaic at the following address: http://www.mcs.anl.gov/fortran-m/FM.html. In addition, a FM compiler is available from anonymous ftp server info.mcs.anl.gov, in directory pub/fortran-m.
Reference: [4] <author> G. Cheng, G. Fox, and K. Mills, </author> <title> Integrating Multiple Programming Paradigms on Connection Machine CM5 in a Dataflow-based Software Environment, </title> <type> Technical Report, </type> <institution> NPAC, Syracuse University, Syracuse, </institution> <address> N.Y., </address> <year> 1993. </year>
Reference-contexts: More limited in scope are approaches based on the use of a task-parallel coordination language able to invoke data-parallel computations. For example, Cheng et al. propose the use of the AVS dataflow visualization system to implement multidis-ciplinary applications, in which some components may be data-parallel programs <ref> [4] </ref>. This provides an elegant graphical programming model but is less expressive than the approach described here. For example, cyclic communication structures are not easily expressed. Similarly, Quinn et al. describe work on iWARP in which a configuration language is used to connect Dataparallel C computations [8].
Reference: [5] <author> Foster, I., B. Avalani, A. Choudhary, and M. Xu, </author> <title> A Compilation System That Integrates High Performance Fortran and Fortran M, </title> <booktitle> Proc. 1994 Scalable High Performance Computing Conf., IEEE (to appear), </booktitle> <year> 1994. </year>
Reference-contexts: In a related effort, colleagues at Caltech are using an extended C++ for the same purpose [3]. To explore the utility of the FM extensions for multiparadigm programming, we have been working with colleagues to develop libraries for the integration of message passing, for data parallel programming <ref> [5, 2] </ref>, for redirection of file I/O, for scientific programming "templates," and for exploitation of parallelism in automatic differentiation [1]. In this paper, we describe the mechanisms used to support the first three of these applications. 2. <p> Hence, a simple integration strategy is first to compile the HPF program using an HPF compiler and then to link the resulting code with the FM message-passing library. We have pursued this approach successfully in collaboration with Bhaven Avalani and Alok Choudhary of Syracuse University <ref> [5] </ref>. This allows us to write code similar to the following. This code fragment implements an image-processing convolution pipeline, in which two input image streams flow through two forward fast Fourier transform (FFT) stages, and then through a multiplication and inverse FFT stage.
Reference: [6] <author> Foster, I., and K. M. Chandy, </author> <title> Fortran M: A Language for Modular Parallel Programming, </title> <type> Preprint MCS-P327-0992, </type> <institution> Argonne National Laboratory, Argonne, Ill., </institution> <year> 1992. </year>
Reference-contexts: The extended sequential language is then used to write paradigm libraries that support various paradigms. Thanks to compositionality, paradigm libraries can be integrated in flexible ways to implement multiparadigm programs. In this paper, we use an extended Fortran called Fortran M (FM) <ref> [6] </ref> to illustrate the approach; however, the basic ideas are language independent. In a related effort, colleagues at Caltech are using an extended C++ for the same purpose [3]. <p> In this paper, we describe the mechanisms used to support the first three of these applications. 2. FORTRAN M Fortran M (FM) is a language designed by researchers at Argonne and Caltech for expressing task-parallel computation <ref> [6] </ref>. It comprises a small set of extensions to Fortran and provides a message-passing parallel programming model, in which programs create processes that interact by sending and receiving messages on typed channels. Two key features of the extensions are their support for determinism and modularity.
Reference: [7] <author> Fox, G., S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu, </author> <title> Fortran D Language Specification, </title> <type> Technical Report TR90-141, </type> <institution> Computer Science, Rice Univ., Houston, Texas, </institution> <year> 1990. </year>
Reference: [8] <author> P. Hatcher and M. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1991. </year>
Reference-contexts: This provides an elegant graphical programming model but is less expressive than the approach described here. For example, cyclic communication structures are not easily expressed. Similarly, Quinn et al. describe work on iWARP in which a configuration language is used to connect Dataparallel C computations <ref> [8] </ref>. The Dataparallel C programs use specialized versions of C I/O libraries for communication. Process and communication structures are static, and all communication passes via a central communication server. Subhlok et al. describe a compile-time approach for exploiting task and data parallelism on the iWarp mesh-connected multicomputer [10].
Reference: [9] <author> High Performance Fortran Forum 1993. </author> <title> High Performance Fortran Language Specification, Version 1.0. </title> <type> Technical Report CRPC-TR92225. </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, Texas, </institution> <year> 1993. </year>
Reference: [10] <author> J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Exploiting Task and Data Parallelism on a Multicomputer. </title> <booktitle> In Proc. 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, Calif., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The Dataparallel C programs use specialized versions of C I/O libraries for communication. Process and communication structures are static, and all communication passes via a central communication server. Subhlok et al. describe a compile-time approach for exploiting task and data parallelism on the iWarp mesh-connected multicomputer <ref> [10] </ref>. The input program incorporates HPF-like data parallel constructs and directives indicating data dependencies and parallel sections. An appropriate mix of task and data parallelism is then generated automatically by the compiler.
References-found: 10

