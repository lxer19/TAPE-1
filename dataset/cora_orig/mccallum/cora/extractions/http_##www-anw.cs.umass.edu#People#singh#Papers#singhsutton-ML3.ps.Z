URL: http://www-anw.cs.umass.edu/People/singh/Papers/singhsutton-ML3.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/People/singh/Papers/
Root-URL: 
Title: Reinforcement Learning with Replacing Eligibility Traces  
Author: SATINDER P. SINGH RICHARD S. SUTTON Editor: Leslie P. Kaelbling 
Keyword: reinforcement learning, temporal difference learning, eligibility trace, Monte Carlo method, Markov chain, CMAC  
Address: Cambridge, Mass. 02139  Amherst, Mass. 01003  
Affiliation: Dept. of Brain and Cognitive Sciences Massachusetts Institute of Technology,  Dept. of Computer Science University of Massachusetts,  
Note: Machine Learning, 1?? (1996) c 1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: singh@psyche.mit.edu  rich@cs.umass.edu  
Date: Received November 7, 1994; Revised May 8, 1995  
Abstract: The eligibility trace is one of the basic mechanisms used in reinforcement learning to handle delayed reward. In this paper we introduce a new kind of eligibility trace, the replacing trace, analyze it theoretically, and show that it results in faster, more reliable learning than the conventional trace. Both kinds of trace assign credit to prior events according to how recently they occurred, but only the conventional trace gives greater credit to repeated events. Our analysis is for conventional and replace-trace versions of the o*ine TD(1) algorithm applied to undiscounted absorbing Markov chains. First, we show that these methods converge under repeated presentations of the training set to the same predictions as two well known Monte Carlo methods. We then analyze the relative efficiency of the two Monte Carlo methods. We show that the method corresponding to conventional TD is biased, whereas the method corresponding to replace-trace TD is unbiased. In addition, we show that the method corresponding to replacing traces is closely related to the maximum likelihood solution for these tasks, and that its mean squared error is always lower in the long run. Computational results confirm these analyses and show that they are applicable more generally. In particular, we show that replacing traces significantly improve performance and reduce parameter sensitivity on the "Mountain-Car" task, a full reinforcement-learning problem with a continuous state space, when using a feature-based function approximator. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Albus, J. S. </author> <year> (1981). </year> <title> Brain, Behavior, </title> <journal> and Robotics, </journal> <volume> chapter 6, </volume> <pages> pages 139-179. </pages> <publisher> Byte Books. </publisher>
Reference: <author> Baase, S. </author> <year> (1988). </year> <title> Computer Algorithms: Introduction to design and analysis. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: For this reason, this estimate is sometimes also referred to as the certainty equivalent estimate (e.g., Kumar and Varaiya, 1986). 5. In theory it is possible to get this down to O (n 2:376 ) operations <ref> (Baase, 1988) </ref>, but, even if practical, this is still far too complex for many applications. 6.
Reference: <author> Barnard, E. </author> <year> (1993). </year> <title> Temporal-difference methods and Markov models. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23 (2), </volume> <pages> 357-365. </pages>
Reference: <author> Barto, A. G. & Duff, M. </author> <year> (1994). </year> <title> Monte Carlo matrix inversion and reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 687-694, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> 13, </volume> <pages> 835-846. </pages>
Reference: <author> Bellman, R. E. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press. </publisher>
Reference-contexts: The mean squared error (MSE) is Bias 2 + V ar. 3.4. Bias Results First consider the true value of state s in Figure 3. From Bellman's equation <ref> (Bellman, 1957) </ref>: V (s) = P s (R s + V (s)) + P T (R T + V T ) (1 P s )V (s) = P s R s + P T R T ; and therefore V (s) = P T Theorem 6: First-visit MC is unbiased, i.e.,
Reference: <author> Curtiss, J. H. </author> <year> (1954). </year> <title> A theoretical comparison of the efficiencies of two classical methods and a Monte Carlo method for computing one component of the solution of a set of linear algebraic equations. </title> <editor> In Meyer, H. A. (Ed.), </editor> <booktitle> Symposium on Monte Carlo Methods, </booktitle> <pages> pages 191-233, </pages> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Dayan, P. </author> <year> (1992). </year> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8(3/4), </booktitle> <pages> 341-362. </pages> <note> 36 S.P. </note> <author> SINGH AND R.S. SUTTON Dayan, P. </author> <year> (1993). </year> <title> Improving generalization for temporal difference learning: </title> <booktitle> The successor representation. Neural Computation, </booktitle> <volume> 5 (4), </volume> <pages> 613-624. </pages>
Reference-contexts: However, this is at the cost of losing some of the theoretical advantages of conventional TD (1). In particular, conventional TD (1) converges in many cases to a minimal mean-squared-error solution when function approximators are used <ref> (Dayan, 1992) </ref> and has been shown to be useful in non-Markov problems (Jaakkola, Singh & Jordan, 1995). The replace version of TD (1) does not share these theoretical guarantees. Like &lt; 1 methods, it appears to achieve greater efficiency in part by relying on the Markov property.
Reference: <author> Dayan, P. & Sejnowski, T. </author> <year> (1994). </year> <title> TD() converges with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> 295-301. </pages>
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems, </title> <booktitle> Volume 2 of Machine Learning: An Artificial Intelligence Approach, chapter 20. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Jaakkola, T., Jordan, M. I., & Singh, S. P. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6 (6), </volume> <pages> 1185-1201. </pages>
Reference: <author> Jaakkola, T., Singh, S. P., & Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, this is at the cost of losing some of the theoretical advantages of conventional TD (1). In particular, conventional TD (1) converges in many cases to a minimal mean-squared-error solution when function approximators are used (Dayan, 1992) and has been shown to be useful in non-Markov problems <ref> (Jaakkola, Singh & Jordan, 1995) </ref>. The replace version of TD (1) does not share these theoretical guarantees. Like &lt; 1 methods, it appears to achieve greater efficiency in part by relying on the Markov property.
Reference: <author> Klopf, A. H. </author> <year> (1972). </year> <title> Brain function and adaptive systems|A heterostatic theory. </title> <type> Technical Report AFCRL-72-0164, </type> <institution> Air Force Cambridge Research Laboratories, Bedford, </institution> <address> MA. </address>
Reference: <author> Kumar, P. R. & Varaiya, P. P. </author> <year> (1986). </year> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: For example, if one trial reaches a terminal state at time t , then the next trial begins at time t + 1. 4. For this reason, this estimate is sometimes also referred to as the certainty equivalent estimate <ref> (e.g., Kumar and Varaiya, 1986) </ref>. 5. In theory it is possible to get this down to O (n 2:376 ) operations (Baase, 1988), but, even if practical, this is still far too complex for many applications. 6.
Reference: <author> Lin, L. J. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 293-321. </pages>
Reference: <author> Miller, W. T., Glanz, F. H., & Kraft, L. G. </author> <year> (1990). </year> <title> CMAC: An associative neural network alternative to backpropagation. </title> <journal> Proc. of the IEEE, </journal> <volume> 78, </volume> <pages> 1561-1567. </pages>
Reference: <author> Moore, A. W. </author> <year> (1991). </year> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state-spaces. </title> <booktitle> In Machine Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 333-337, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Peng, J. </author> <year> (1993). </year> <title> Dynamic Programming-based Learning for Control. </title> <type> PhD thesis, </type> <institution> Northeastern University. </institution>
Reference: <author> Peng, J. & Williams, R. J. </author> <year> (1994). </year> <title> Incremental multi-step Q-learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 226-232. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rubinstein, R. </author> <year> (1981). </year> <title> Simulation and the Monte Carlo method. </title> <address> New York: </address> <publisher> John Wiley Sons. </publisher>
Reference: <author> Rummery, G. A. & Niranjan, M. </author> <year> (1994). </year> <title> On-line Q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFENG/TR 166, </type> <institution> Cambridge University Engineering Dept. </institution>
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: 1. Eligibility Traces Two fundamental mechanisms have been used in reinforcement learning to handle delayed reward. One is temporal-difference (TD) learning, as in the TD () algo rithm <ref> (Sutton, 1988) </ref> and in Q-learning (Watkins, 1989). TD learning in effect constructs an internal reward signal that is less delayed than the original, exter nal one. However, TD methods can eliminate the delay completely only on fully Markov problems, which are rare in practice.
Reference: <author> Sutton, R. S. </author> <year> (1995). </year> <title> TD models: Modeling the world at a mixture of time scales. </title> <booktitle> In Machine Learning: Proceedings of the Twelth International Conference, </booktitle> <pages> pages 531-539. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S. & Barto, A. G. </author> <year> (1987). </year> <title> A temporal-difference model of classical conditioning. </title> <booktitle> In Proceedings of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 355-378, </pages> <address> Hillsdale, NJ. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: The trial terminates with the first position value that exceeds p t+1 &gt; 0:5. Notes 1. Arguably, yet a third mechanism for managing delayed reward is to change representations or world models (e.g., Dayan, 1993 ; Sutton, 1995). 2. In some previous work <ref> (e.g., Sutton & Barto, 1987, 1990) </ref> the traces were normalized by a factor of 1 fl, which is equivalent to replacing the "1" in these equations by 1 fl.
Reference: <author> Sutton, R. S. & Barto, A. G. </author> <year> (1990). </year> <title> Time-derivative models of Pavlovian conditioning. </title> <editor> In Gabriel, M. & Moore, J. W. (Eds.), </editor> <booktitle> Learning and Computational Neuroscience, </booktitle> <pages> pages 497-537. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Sutton, R. S. & Singh, S. P. </author> <year> (1994). </year> <title> On step-size and bias in temporal-difference learning. </title> <booktitle> In Eighth Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 91-96, </pages> <address> New Haven, CT. </address>
Reference: <author> Sutton, R. S. & Whitehead, S. D. </author> <year> (1993). </year> <title> Online learning with random representations. </title> <booktitle> In Machine Learning: Proceedings of the Tenth Int. Conference, </booktitle> <pages> pages 314-321. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tesauro, G. J. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8(3/4), </volume> <pages> 257-277. </pages>
Reference: <author> Tsitsiklis, J. </author> <year> (1994). </year> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16 (3), </volume> <pages> 185-202. </pages>
Reference: <author> Wasow, W. R. </author> <year> (1952). </year> <title> A note on the inversion of matrices by random walks. </title> <journal> Math. Tables Other Aids Comput., </journal> <volume> 6, </volume> <pages> 78-81. </pages>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <address> Cambridge, England. </address>
Reference-contexts: 1. Eligibility Traces Two fundamental mechanisms have been used in reinforcement learning to handle delayed reward. One is temporal-difference (TD) learning, as in the TD () algo rithm (Sutton, 1988) and in Q-learning <ref> (Watkins, 1989) </ref>. TD learning in effect constructs an internal reward signal that is less delayed than the original, exter nal one. However, TD methods can eliminate the delay completely only on fully Markov problems, which are rare in practice. <p> Programming optimizations can reduce the expense per iteration to a small multiple (dependent on ) of the number of features, m, present on a typical time step. Here m is 5. REINFORCEMENT LEARNING WITH REPLACING ELIGIBILITY TRACES 21 <ref> (Watkins, 1989) </ref> and to various simplified forms of the bucket brigade (Holland, 1986; Wilson, to appear). It is also identical to the TD () algorithm applied to state-action pairs rather than to states. 6 The mountain-car task has a continuous two-dimensional state space with an infinite number of states.
Reference: <author> Wilson, S. W. </author> <title> (to appear). Classifier fitness based on accuracy. Evolutionary Computation. </title>
References-found: 33

