URL: http://www.robotics.stanford.edu/~kuffner/papers/ca99.ps.gz
Refering-URL: http://www.robotics.stanford.edu/~kuffner/publications.html
Root-URL: http://www.robotics.stanford.edu
Email: fkuffner, latombeg@cs.stanford.edu  
Title: Fast Synthetic Vision, Memory, and Learning Models for Virtual Humans  
Author: James J. Kuffner, Jr Jean-Claude Latombe 
Address: Stanford, CA 94305-9010, USA  
Affiliation: Computer Science Robotics Lab Stanford University  
Abstract: This paper presents a simple and efficient method of modeling synthetic vision, memory, and learning for autonomous animated characters in real-time virtual environments. The model is efficient in terms of both storage requirements and update times, and can be flexibly combined with a variety of higher-level reasoning modules or complex memory rules. The design is inspired by research in motion planning, control, and sensing for autonomous mobile robots. We apply this framework to the problem of quickly synthesizing from navigation goals the collision-free motions for animated human figures in changing virtual environments. We combine a low-level path planner, a path-following controller, and cyclic motion capture data to generate the underlying animation. Graphics rendering hardware is used to simulate the visual perception of a character, providing a feedback loop to the overall navigation strategy. The synthetic vision and memory update rules can handle dynamic environments where objects appear, disappear, or move around unpredictably. The resulting model is suitable for a variety of real-time applications involving autonomous animated characters. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. Arkin. </author> <title> Cooperation without communication: Multi-agent schema based robot navigation. Journal of Robotic are those contained in M, while those rendered wireframe are unknown. The top-left image shows the inital frame with the character given the task of navigating from the bottom-right corner to the top-left corner of the maze. A portion of the character's viewing frustum, along with the current path is also shown. maze environment of Figure 12. </title> <booktitle> Systems, </booktitle> <pages> pages 351364, </pages> <year> 1992. </year>
Reference-contexts: However, as processor speeds continue to increase, algorithms originally intended for off-line animations will gradually become feasible in real-time virtual environments. Much research effort in robotics has been focused on designing control architectures for autonomous agents that operate in the real world <ref> [15, 7, 1] </ref>. More recently, many of these same techniques have gradually been adapted for the purposes of creating autonomous animated characters in virtual worlds. The ultimate goal being the creation of fully-autonomous, interactive, artificial agents.
Reference: [2] <author> N. Badler. </author> <title> Real-time virtual humans. </title> <journal> Pacific Graphics, </journal> <year> 1997. </year>
Reference-contexts: These ideas were later expanded to include virtual tactility and audition [18, 27]. Researchers at the University of Pennsylvania have been exploring algorithms for controlling their Jack human character model [10, 8], incorporating body dynamics [12], and high-level scripting <ref> [2] </ref>. Koga et al. combined robot motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks [11].
Reference: [3] <author> J. Bates, A. B. Loyall, and W. S. Reilly. </author> <title> An architecture for action, emotion, and social behavior. </title> <booktitle> In Artificial Social Systems : Proc of 4th European Wkshp on Modeling Autonomous Agents in a Multi-Agent World. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Other systems include Perlin and Gold-berg's Improv software for interactive agents [20, 19], the ALIVE project at MIT [5, 16], Johnson's WavesWorld, the Oz project at CMU <ref> [3] </ref>, and the work of Strassman [24, 25], and Ridsdale, et al.[23]. Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics [6]. They have also designed a controller for human running in 3D [9].
Reference: [4] <author> B. M. Blumberg. </author> <title> Old Tricks, New Dogs : Ethology and Interactive Creatures. </title> <type> PhD thesis, </type> <institution> MIT Media Laboratory, </institution> <address> Boston, MA, </address> <year> 1996. </year>
Reference-contexts: Tu and Terzopoulos implemented a synthetic vision for their artificial fishes based on ray-casting [29, 28]. Blumberg experimented with image-based motion energy techniques for obstacle avoidance for his autonomous virtual dog <ref> [4] </ref>. Terzopoulos and Rabie proposed using a database of pre-rendered models of objects along with an iterative pattern-matching scheme based on color histograms for object recognition [26].
Reference: [5] <author> B. M. Blumberg and T. A. Galyean. </author> <title> Multi-level direction of autonomous creatures for real-time virtual environments. </title> <editor> In R. Cook, editor, </editor> <booktitle> Proc. SIGGRAPH '95, Annual Conference Series, </booktitle> <pages> pages 4754. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <address> Aug. 1995. held in Los Angeles, California, </address> <month> 06-11 August </month> <year> 1995. </year>
Reference-contexts: Koga et al. combined robot motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks [11]. Other systems include Perlin and Gold-berg's Improv software for interactive agents [20, 19], the ALIVE project at MIT <ref> [5, 16] </ref>, Johnson's WavesWorld, the Oz project at CMU [3], and the work of Strassman [24, 25], and Ridsdale, et al.[23]. Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics [6]. They have also designed a controller for human running in 3D [9].
Reference: [6] <author> D. C. Brogan and J. K. Hodgins. </author> <title> Group behaviors with significant dynamics. </title> <booktitle> In Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <year> 1995. </year>
Reference-contexts: Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics <ref> [6] </ref>. They have also designed a controller for human running in 3D [9]. Despite these achievements, building autonomous agents that respond intelligently to task-level commands remains an elusive goal, particularly in real-time applications.
Reference: [7] <author> R. A. Brooks. </author> <title> A layered intelligent control system for a mobile robot. </title> <booktitle> In Robotics Research The Third International Symposium, </booktitle> <pages> pages 365372. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: However, as processor speeds continue to increase, algorithms originally intended for off-line animations will gradually become feasible in real-time virtual environments. Much research effort in robotics has been focused on designing control architectures for autonomous agents that operate in the real world <ref> [15, 7, 1] </ref>. More recently, many of these same techniques have gradually been adapted for the purposes of creating autonomous animated characters in virtual worlds. The ultimate goal being the creation of fully-autonomous, interactive, artificial agents.
Reference: [8] <author> J. P. Granieri, W. Becket, B. D. Reich, J. Crabtree, and N. L. Badler. </author> <title> Behavioral control for real-time simulated human agents. </title> <editor> In P. Hanrahan and J. Winget, editors, </editor> <booktitle> 1995 Symposium on Interactive 3D Graphics, </booktitle> <pages> pages 173180. </pages> <publisher> ACM SIGGRAPH, </publisher> <month> Apr. </month> <year> 1995. </year> <note> ISBN 0-89791-736-7. </note>
Reference-contexts: Their implementation also included memory and learning models based on dynamic octrees [17]. These ideas were later expanded to include virtual tactility and audition [18, 27]. Researchers at the University of Pennsylvania have been exploring algorithms for controlling their Jack human character model <ref> [10, 8] </ref>, incorporating body dynamics [12], and high-level scripting [2]. Koga et al. combined robot motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks [11].
Reference: [9] <author> J. K. Hodgins. </author> <title> Three-dimensional human running. </title> <booktitle> In Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <year> 1996. </year>
Reference-contexts: Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics [6]. They have also designed a controller for human running in 3D <ref> [9] </ref>. Despite these achievements, building autonomous agents that respond intelligently to task-level commands remains an elusive goal, particularly in real-time applications. Previous researchers have argued the case for employing some kind of virtual perception for animated characters [21, 28].
Reference: [10] <author> M. R. Jung, N. Badler, and T. Noma. </author> <title> Animated human agents with motion planning capability for 3D-space postural goals. </title> <journal> The Journal of Visualization and Computer Animation, </journal> <volume> 5(4):225246, </volume> <month> October </month> <year> 1994. </year>
Reference-contexts: Their implementation also included memory and learning models based on dynamic octrees [17]. These ideas were later expanded to include virtual tactility and audition [18, 27]. Researchers at the University of Pennsylvania have been exploring algorithms for controlling their Jack human character model <ref> [10, 8] </ref>, incorporating body dynamics [12], and high-level scripting [2]. Koga et al. combined robot motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks [11].
Reference: [11] <author> Y. Koga, K. Kondo, J. Kuffner, and J.-C. Latombe. </author> <title> Planning motions with intentions. </title> <booktitle> In Proc. SIGGRAPH '94, </booktitle> <pages> pages 395408, </pages> <year> 1994. </year>
Reference-contexts: Researchers at the University of Pennsylvania have been exploring algorithms for controlling their Jack human character model [10, 8], incorporating body dynamics [12], and high-level scripting [2]. Koga et al. combined robot motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks <ref> [11] </ref>. Other systems include Perlin and Gold-berg's Improv software for interactive agents [20, 19], the ALIVE project at MIT [5, 16], Johnson's WavesWorld, the Oz project at CMU [3], and the work of Strassman [24, 25], and Ridsdale, et al.[23].
Reference: [12] <author> E. Kokkevis, D. Metaxas, and N. I. Badler. </author> <title> Autonomous animation and control of four-legged animals. </title> <editor> In W. A. Davis and P. Prusinkiewicz, editors, </editor> <booktitle> Graphics Interface '95, pages 1017. Canadian Information Processing Society, Canadian Human-Computer Communications Society, </booktitle> <month> May </month> <year> 1995. </year> <note> ISBN 0-9695338-4-5. </note>
Reference-contexts: Their implementation also included memory and learning models based on dynamic octrees [17]. These ideas were later expanded to include virtual tactility and audition [18, 27]. Researchers at the University of Pennsylvania have been exploring algorithms for controlling their Jack human character model [10, 8], incorporating body dynamics <ref> [12] </ref>, and high-level scripting [2]. Koga et al. combined robot motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks [11].
Reference: [13] <author> J. J. Kuffner, Jr. </author> <title> Goal-directed navigation for animated characters using real-time path planning and control. </title> <booktitle> In Proc. of CAPTECH '98 : Workshop on Modelling and Motion Cap ture Techniques for Virtual Environments. </booktitle> <publisher> Springer-Verlag, </publisher> <month> November </month> <year> 1998. </year>
Reference-contexts: More specifically, the problem of controlling an autonomous animated character in a virtual environment is viewed as that of controlling an autonomous virtual robot complete with virtual sensors. This paper combines the fast 2D path planner and controller originally presented in <ref> [13] </ref>, with a simple synthetic vision and memory model to compute natural-looking mo multiple characters navigating outdoors. tions for navigation tasks. The controller is used to synthesize cyclic motion capture data for an animated character as it follows a computed path towards a goal location. <p> Finally, this set of observations is added to the character's internal memory model of the environment, and a navigation plan is computed. The navigation planning algorithm we use combines a fast path planner, motion controller, and cyclic motion capture data, and is presented in detail in <ref> [13] </ref>. However, any appropriate real-time navigation planning strategy may be used. For our system to work, we assume the environment is broken up into a collection of small to medium-sized objects, each assigned a unique ID. For example, a single object may be a chair or a door. <p> This can be useful for pursuit games, or having a group of characters follow a tour guide. For the purposes of path following, the simple PD control algorithm described in <ref> [13] </ref> was implemented for a human-like character with 17 joints. Two sets of motion capture data were used in the experiments: a walk cycle and a jog cycle. Sample output involving an outdoor environment is illustrated in Figure 1.
Reference: [14] <author> J. J. Kuffner Jr. </author> <title> Designing Autonomous Agents for Real-Time Animation (working title). </title> <type> PhD thesis, </type> <institution> Stanford University (in preparation). </institution>
Reference-contexts: Research on techniques to control groups of multiple actors using the existing planning and perception modules would be useful, and is currently underway <ref> [14] </ref>. This includes some simple prediction based on observed velocities to take into account the estimated motion of other characters and obstacles dur ing planning. Although the synthetic vision module runs fast enough to support a small number of characters simultaneously, it is currently the bottleneck in the computation.
Reference: [15] <author> J. C. Latombe. </author> <title> Robot Motion Planning. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1991. </year>
Reference-contexts: However, as processor speeds continue to increase, algorithms originally intended for off-line animations will gradually become feasible in real-time virtual environments. Much research effort in robotics has been focused on designing control architectures for autonomous agents that operate in the real world <ref> [15, 7, 1] </ref>. More recently, many of these same techniques have gradually been adapted for the purposes of creating autonomous animated characters in virtual worlds. The ultimate goal being the creation of fully-autonomous, interactive, artificial agents.
Reference: [16] <author> P. Maes, D. Trevor, B. Blumberg, and A. Pentland. </author> <title> The ALIVE system full-body interaction with autonomous agents. </title> <booktitle> In Computer Animation '95, </booktitle> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Koga et al. combined robot motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks [11]. Other systems include Perlin and Gold-berg's Improv software for interactive agents [20, 19], the ALIVE project at MIT <ref> [5, 16] </ref>, Johnson's WavesWorld, the Oz project at CMU [3], and the work of Strassman [24, 25], and Ridsdale, et al.[23]. Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics [6]. They have also designed a controller for human running in 3D [9].
Reference: [17] <author> H. Noser, O. Renault, D. Thalmann, and N. Magnenat Thal-mann. </author> <title> Navigation for digital actors based on synthetic vision, memory and learning. </title> <journal> Comput. Graphics, </journal> <volume> 19:719, </volume> <year> 1995. </year>
Reference-contexts: Noser, et al. proposed a navigation system for animated characters using ideas for synthetic vision originally developed by Renault, et al.[21]. Their implementation also included memory and learning models based on dynamic octrees <ref> [17] </ref>. These ideas were later expanded to include virtual tactility and audition [18, 27]. Researchers at the University of Pennsylvania have been exploring algorithms for controlling their Jack human character model [10, 8], incorporating body dynamics [12], and high-level scripting [2]. <p> Noser, et al. presented a clever synthetic vision model that uses object false-coloring and dynamic octrees to represent the visual memory of the character <ref> [17] </ref>. We are primarily interested in synthetic vision techniques that are practical for real-time systems. Specifically, our goal is to allow an autonomous character endowed with synthetic vision to explore an unknown interactive environment, while maintaining a visual memory or cognitive map of what it has perceived. <p> Noser, et al. used an occupancy grid model (e.g. an octree) to represent the visual memory of each character <ref> [17] </ref>.
Reference: [18] <author> H. Noser and D. Thalmann. </author> <title> Synthetic vision and audition for digital actors. </title> <booktitle> In Proc. Eurographics '95, </booktitle> <year> 1995. </year>
Reference-contexts: Noser, et al. proposed a navigation system for animated characters using ideas for synthetic vision originally developed by Renault, et al.[21]. Their implementation also included memory and learning models based on dynamic octrees [17]. These ideas were later expanded to include virtual tactility and audition <ref> [18, 27] </ref>. Researchers at the University of Pennsylvania have been exploring algorithms for controlling their Jack human character model [10, 8], incorporating body dynamics [12], and high-level scripting [2].
Reference: [19] <author> K. Perlin. </author> <title> Real time responsive animation with personality. </title> <journal> IEEE Transactions on Visualization and Computer Graphics, </journal> <volume> 1(1):515, </volume> <month> March </month> <year> 1995. </year> <pages> ISSN 1077-2626. </pages>
Reference-contexts: Koga et al. combined robot motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks [11]. Other systems include Perlin and Gold-berg's Improv software for interactive agents <ref> [20, 19] </ref>, the ALIVE project at MIT [5, 16], Johnson's WavesWorld, the Oz project at CMU [3], and the work of Strassman [24, 25], and Ridsdale, et al.[23]. Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics [6].
Reference: [20] <author> K. Perlin and A. Goldberg. IMPROV: </author> <title> A system for scripting interactive actors in virtual worlds. </title> <editor> In H. Rushmeier, editor, </editor> <booktitle> Proc. SIGGRAPH '96, Annual Conference Series, pages 205216. ACM SIGGRAPH, </booktitle> <publisher> Addison Wesley, </publisher> <year> 1996. </year>
Reference-contexts: Koga et al. combined robot motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks [11]. Other systems include Perlin and Gold-berg's Improv software for interactive agents <ref> [20, 19] </ref>, the ALIVE project at MIT [5, 16], Johnson's WavesWorld, the Oz project at CMU [3], and the work of Strassman [24, 25], and Ridsdale, et al.[23]. Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics [6].
Reference: [21] <author> O. renault, N. M. Thalmann, and D. Thalmann. </author> <title> A vision-based approach to behavioral animation. Visualization and Computer Animation, </title> <address> 1:1821, </address> <year> 1990. </year>
Reference-contexts: They have also designed a controller for human running in 3D [9]. Despite these achievements, building autonomous agents that respond intelligently to task-level commands remains an elusive goal, particularly in real-time applications. Previous researchers have argued the case for employing some kind of virtual perception for animated characters <ref> [21, 28] </ref>. The key idea is to somehow realistically model the information flow from the environment to the character. Giving each character complete access to all objects in the environment is both conceptually unrealistic, and can be impractical to implement for large environments with many objects.
Reference: [22] <author> C. W. Reynolds. </author> <title> Flocks, herds, and schools: A distributed behavioral model. </title> <journal> Computer Graphics, </journal> <volume> 21(4):2534, </volume> <year> 1987. </year>
Reference-contexts: More recently, many of these same techniques have gradually been adapted for the purposes of creating autonomous animated characters in virtual worlds. The ultimate goal being the creation of fully-autonomous, interactive, artificial agents. Reactive behaviors applied to simple simulated creatures appeared in the graphics literature with Reynolds' BOIDS model <ref> [22] </ref>. Tu and Terzopoulos implemented a realistic simulation of autonomous artificial fishes, complete with integrated simple behaviors, physically-based motion generation, and simulated perception [29]. Noser, et al. proposed a navigation system for animated characters using ideas for synthetic vision originally developed by Renault, et al.[21]. <p> One way to limit the information a character has access to, is to consider only objects within a sphere centered around the character <ref> [22] </ref>. However, most characters of interest (including human characters) do not have such omnidirectional perception. Rather, sensory information from the environment flows from a primary direction, such as the cone of vision for a human character.
Reference: [23] <author> G. Ridsdale, S. Hewitt, and T. W. Calvert. </author> <title> The interactive specification of human animation. </title> <editor> In M. Green, editor, </editor> <booktitle> Proc. of Graphics Interface '86, </booktitle> <pages> pages 121130, </pages> <month> May </month> <year> 1986. </year>
Reference: [24] <author> S. Strassmann. </author> <title> Desktop Theater: Automating the Generation of Expressive Animation. </title> <type> PhD thesis, </type> <institution> M.I.T. Media Arts and Sciences Program, Toronto, Canada, </institution> <year> 1991. </year>
Reference-contexts: Other systems include Perlin and Gold-berg's Improv software for interactive agents [20, 19], the ALIVE project at MIT [5, 16], Johnson's WavesWorld, the Oz project at CMU [3], and the work of Strassman <ref> [24, 25] </ref>, and Ridsdale, et al.[23]. Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics [6]. They have also designed a controller for human running in 3D [9].
Reference: [25] <author> S. Strassmann. </author> <title> Semi-autonomous animated actors. </title> <booktitle> In Proc. AAAI '94, </booktitle> <pages> pages 128134, </pages> <year> 1994. </year>
Reference-contexts: Other systems include Perlin and Gold-berg's Improv software for interactive agents [20, 19], the ALIVE project at MIT [5, 16], Johnson's WavesWorld, the Oz project at CMU [3], and the work of Strassman <ref> [24, 25] </ref>, and Ridsdale, et al.[23]. Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics [6]. They have also designed a controller for human running in 3D [9].
Reference: [26] <author> D. Terzopoulos and T. Rabie. </author> <title> Animat vision: </title> <booktitle> Active vision in artificial animals. In Proc. Fifth Int. Conf. on Computer Vision (ICCV'95), </booktitle> <pages> pages 801808, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Blumberg experimented with image-based motion energy techniques for obstacle avoidance for his autonomous virtual dog [4]. Terzopoulos and Rabie proposed using a database of pre-rendered models of objects along with an iterative pattern-matching scheme based on color histograms for object recognition <ref> [26] </ref>. Noser, et al. presented a clever synthetic vision model that uses object false-coloring and dynamic octrees to represent the visual memory of the character [17]. We are primarily interested in synthetic vision techniques that are practical for real-time systems.
Reference: [27] <author> D. Thalmann, H. Noser, and Z. Huang. </author> <title> Interactive Animation, </title> <booktitle> chapter 11, </booktitle> <pages> pages 263291. </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Noser, et al. proposed a navigation system for animated characters using ideas for synthetic vision originally developed by Renault, et al.[21]. Their implementation also included memory and learning models based on dynamic octrees [17]. These ideas were later expanded to include virtual tactility and audition <ref> [18, 27] </ref>. Researchers at the University of Pennsylvania have been exploring algorithms for controlling their Jack human character model [10, 8], incorporating body dynamics [12], and high-level scripting [2]. <p> The pixel color information is extracted to obtain a list of the currently visible objects. As pointed out by Thalmann, et al. in <ref> [27] </ref>, synthetic vision differs from vision computations for real robots, since we can skip all of the problems of distance detection, pattern recognition, and noisy images. This allows us to implement a reasonable model of visual information flow that operates fast enough for real-time systems.
Reference: [28] <author> X. Tu. </author> <title> Artificial Animals for Computer Animation: Biome-chanics, Locomotion, Perception, and Behavior. </title> <type> PhD thesis, </type> <institution> University of Toronto, Toronto, Canada, </institution> <year> 1996. </year>
Reference-contexts: They have also designed a controller for human running in 3D [9]. Despite these achievements, building autonomous agents that respond intelligently to task-level commands remains an elusive goal, particularly in real-time applications. Previous researchers have argued the case for employing some kind of virtual perception for animated characters <ref> [21, 28] </ref>. The key idea is to somehow realistically model the information flow from the environment to the character. Giving each character complete access to all objects in the environment is both conceptually unrealistic, and can be impractical to implement for large environments with many objects. <p> Tu and Terzopoulos implemented a synthetic vision for their artificial fishes based on ray-casting <ref> [29, 28] </ref>. Blumberg experimented with image-based motion energy techniques for obstacle avoidance for his autonomous virtual dog [4]. Terzopoulos and Rabie proposed using a database of pre-rendered models of objects along with an iterative pattern-matching scheme based on color histograms for object recognition [26].
Reference: [29] <author> X. Tu and D. Terzopoulos. </author> <title> Artificial fishes: Physics, locomotion, perception, behavior. </title> <editor> In A. Glassner, editor, </editor> <booktitle> Proc. SIGGRAPH '94, Computer Graphics Proceedings, Annual Conference Series, </booktitle> <pages> pages 4350. </pages> <publisher> ACM SIGGRAPH, ACM Press, </publisher> <month> July </month> <year> 1994. </year> <note> ISBN 0-89791-667-0. </note>
Reference-contexts: Reactive behaviors applied to simple simulated creatures appeared in the graphics literature with Reynolds' BOIDS model [22]. Tu and Terzopoulos implemented a realistic simulation of autonomous artificial fishes, complete with integrated simple behaviors, physically-based motion generation, and simulated perception <ref> [29] </ref>. Noser, et al. proposed a navigation system for animated characters using ideas for synthetic vision originally developed by Renault, et al.[21]. Their implementation also included memory and learning models based on dynamic octrees [17]. These ideas were later expanded to include virtual tactility and audition [18, 27]. <p> Tu and Terzopoulos implemented a synthetic vision for their artificial fishes based on ray-casting <ref> [29, 28] </ref>. Blumberg experimented with image-based motion energy techniques for obstacle avoidance for his autonomous virtual dog [4]. Terzopoulos and Rabie proposed using a database of pre-rendered models of objects along with an iterative pattern-matching scheme based on color histograms for object recognition [26].
References-found: 29

