URL: http://www.cs.cmu.edu/~mccallum/papers/emactive-icml98.ps.gz
Refering-URL: http://www.cs.cmu.edu/~mccallum/
Root-URL: 
Email: mccallum@justresearch.com  knigam@cs.cmu.edu  
Title: Employing EM and Pool-Based Active Learning for Text Classification  
Author: Andrew Kachites McCallum zy Kamal Nigam 
Address: 4616 Henry Street Pittsburgh, PA 15213  Pittsburgh, PA 15213  
Affiliation: Just Research  School of Computer Science Carnegie Mellon University  
Abstract: This paper shows how a text classifier's need for labeled training documents can be reduced by taking advantage of a large pool of unlabeled documents. We modify the Query-by-Committee (QBC) method of active learning to use the unlabeled pool for explicitly estimating document density when selecting examples for labeling. Then active learning is combined with Expectation-Maximization in order to "fill in" the class labels of those documents that remain unlabeled. Experimental results show that the improvements to active learning require less than two-thirds as many labeled training examples as previous QBC approaches, and that the combination of EM and active learning requires only slightly more than half as many labeled training examples to achieve the same accuracy as either the improved ac tive learning or EM alone.
Abstract-found: 1
Intro-found: 1
Reference: <author> Cohn, D.; Ghahramani, Z.; and Jordan, M. </author> <year> 1996. </year> <title> Active learning with statistical models. </title> <journal> Journal of Artificial Intelligence Research 4 </journal> <pages> 129-145. </pages>
Reference: <author> Cohn, D. </author> <year> 1994. </year> <title> Neural network exploration using optimal experiment design. </title> <booktitle> In NIPS 6. </booktitle>
Reference: <author> Craven, M.; DiPasquo, D.; Freitag, D.; McCallum, A.; Mitchell, T.; Nigam, K.; and Slattery, S. </author> <year> 1998. </year> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <booktitle> In AAAI-98. </booktitle>
Reference: <author> Dagan, I., and Engelson, S. </author> <year> 1995. </year> <title> Committee-based sampling for training probabilistic classifiers. </title> <booktitle> In ICML-95. </booktitle>
Reference: <author> Dempster, A. P.; Laird, N. M.; and Rubin, D. B. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM. algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> Series B 39 </volume> <pages> 1-38. </pages>
Reference: <author> Domingos, P., and Pazzani, M. </author> <year> 1997. </year> <title> On the optimality of the simple Bayesian classifier under zero-one loss. </title> <booktitle> Machine Learning 29 </booktitle> <pages> 103-130. </pages>
Reference: <author> Freund, Y.; Seung, H.; Shamir, E.; and Tishby, N. </author> <year> 1997. </year> <title> Selective sampling using the query by committee algorithm. </title> <booktitle> Machine Learning 28 </booktitle> <pages> 133-168. </pages>
Reference: <author> Friedman, J. H. </author> <year> 1997. </year> <title> On bias, variance, 0/1 loss, </title> <booktitle> and the curse-of-dimensionality. Data Mining and Knowledge Discovery 1 </booktitle> <pages> 55-77. </pages>
Reference: <author> Ghahramani, Z., and Jordan, M. </author> <year> 1994. </year> <title> Supervised learning from incomplete data via an EM approach. </title> <booktitle> In NIPS 6. </booktitle>
Reference: <author> Joachims, T. </author> <year> 1997. </year> <title> A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In ICML-97. </booktitle>
Reference: <author> Joachims, T. </author> <year> 1998. </year> <title> Text categorization with Support Vector Machines: Learning with many relevant features. </title> <booktitle> In ECML-98. </booktitle>
Reference: <author> Lewis, D., and Gale, W. </author> <year> 1994. </year> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of ACM SIGIR. </booktitle>
Reference: <author> Lewis, D., and Ringuette, M. </author> <year> 1994. </year> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> 81-93. </pages>
Reference: <author> Lewis, D. D. </author> <year> 1995. </year> <title> A sequential algorithm for training text classifiers: Corrigendum and additional data. </title> <booktitle> SIGIR Forum 29(2) </booktitle> <pages> 13-19. </pages>
Reference: <author> Liere, R., and Tadepalli, P. </author> <year> 1997. </year> <title> Active learning with committees for text categorization. </title> <booktitle> In AAAI-97. </booktitle>
Reference: <author> McCallum, A., and Nigam, K. </author> <year> 1998. </year> <title> A comparison of event models for naive Bayes text classification. </title> <booktitle> In AAAI-98 Workshop on Learning for Text Categorization. </booktitle> <address> http://www.cs.cmu.edu/~mccallum. </address>
Reference: <author> Miller, D. J., and Uyar, H. S. </author> <year> 1997. </year> <title> A mixture of experts classifier with learning based on both labelled and unlabelled data. </title> <booktitle> In NIPS 9. </booktitle>
Reference: <author> Nigam, K.; McCallum, A.; Thrun, S.; and Mitchell, T. </author> <year> 1998. </year> <title> Learning to classify text from labeled and unlabeled documents. </title> <booktitle> In AAAI-98. </booktitle>
Reference: <author> Pereira, F.; Tishby, N.; and Lee, L. </author> <year> 1993. </year> <title> Distributional clustering of English words. </title> <booktitle> In Proceedings of the 31st ACL. </booktitle>
Reference: <author> Shahshahani, B., and Landgrebe, D. </author> <year> 1994. </year> <title> The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon. </title> <journal> IEEE Trans. on Geoscience and Remote Sensing 32(5) </journal> <pages> 1087-1095. </pages>
References-found: 20

