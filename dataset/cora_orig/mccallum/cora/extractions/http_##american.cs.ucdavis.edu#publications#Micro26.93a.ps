URL: http://american.cs.ucdavis.edu/publications/Micro26.93a.ps
Refering-URL: http://american.cs.ucdavis.edu/ArchLabPersonnel/Farrens/PubList.html
Root-URL: http://www.cs.ucdavis.edu
Title: d d A Comparison of Superscalar and Decoupled Access/Execute Architectures  
Author: Matthew K. Farrens, Pius Ng, and Phil Nico 
Address: Davis, CA 95616  
Affiliation: Department of Computer Science University of California Davis  
Abstract: This paper presents a comparison of superscalar and decoupled access/execute architectures. Both architectures attempt to exploit instruction-level parallelism by issuing multiple instructions per cycle, employing dynamic scheduling to maximize performance. Simulation results are presented for four different configurations, demonstrating that the architectural queues of the decou-pled machines provide similar functionality to register renaming, dynamic loop unrolling, and out-of-order execution of the superscalar machines with significantly less complexity. 
Abstract-found: 1
Intro-found: 1
Reference: [GHLP85] <author> J. R. Goodman, J. T. Hsieh, K. Liou, A. R. Pleszkun, P. B. Schechter and H. C. Young, </author> <title> ``PIPE: a VLSI Decoupled Architecture'', </title> <booktitle> Proceedings of the Twelveth Annual International Symposium on Computer Architecture(June 1985), </booktitle> <pages> pp. 20-27. </pages>
Reference-contexts: In different instructions the same logical register identifier may refer to several different physical registers, depending on the locations of register references relative to register assignment [John91]. By removing these constraints, available instruction-level parallelism is increased. Architectural queues provide an alternate register renaming mechanism. In PIPE <ref> [GHLP85] </ref> for example, the heads of the queues are accessed as registers, while the empty slots in these queues serve as the extra registers. When an instruction references a queue, the queue is advanced, and the next element will be available for the next reference.
Reference: [HePa90] <author> J. Hennessy and D. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, California, </address> <year> (1990). </year>
Reference-contexts: Sophisticated compiler techniques such as trace scheduling, procedural inlining, loop unrolling, and software pipelining must also be employed in order to enlarge the size of basic blocks to provide sufficient instruction-level parallelism to ease the code scheduling process <ref> [HePa90] </ref> 4. Register Renaming and Out-Of-Order Execution The reuse of registers can cause interlocks between instructions even though the conflicting instructions are otherwise independent. These interlocks are not true data dependencies, but rather an artifact of the finite number of registers available. <p> Dynamic Loop Unrolling Due to the limitation of instruction-level parallelism within basic blocks, loop structures are often unfolded and replicated multiple times <ref> [HePa90] </ref>. This technique, referred to as loop unrolling [John91], allows a code scheduler to arrange code across multiple iterations of a loop in order to exploit instruction-level parallelism across multiple basic blocks.
Reference: [John91] <author> M. Johnson, </author> <title> Superscalar Microprocessor Design, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> (1991). </year>
Reference-contexts: Research over the last 30 years has demonstrated the difficulty of writing parallel programs; however, there are a number of techniques for exploiting instruction-level parallelism that have been proposed. The most commercially successful approach has been superscalar architectures <ref> [John91] </ref>, which execute two or more independent instructions in parallel. Superscalar implementations provide dynamically scheduled multiple instruction issue and often out-of-order execution using techniques like register renaming [Kell75]. Decoupled Access/Execute Architectures (DAE) [Smit82, SmWP86] use an alternate approach to exploiting instruction level parallelism. <p> An Overview of Superscalars Superscalar processors are defined as uniprocessors of any architecture capable of executing and completing multiple instructions per cycle from a single instruction d d stream <ref> [John91] </ref>. Since a number of studies have implied that the amount of instruction-level parallelism available is between two and three instructions, only a superscalar machine of degree 3 is considered in this study. <p> Thus, an instruction will fetch the value from the newly allocated physical register rather than the outdated physical register. In different instructions the same logical register identifier may refer to several different physical registers, depending on the locations of register references relative to register assignment <ref> [John91] </ref>. By removing these constraints, available instruction-level parallelism is increased. Architectural queues provide an alternate register renaming mechanism. In PIPE [GHLP85] for example, the heads of the queues are accessed as registers, while the empty slots in these queues serve as the extra registers. <p> Dynamic Loop Unrolling Due to the limitation of instruction-level parallelism within basic blocks, loop structures are often unfolded and replicated multiple times [HePa90]. This technique, referred to as loop unrolling <ref> [John91] </ref>, allows a code scheduler to arrange code across multiple iterations of a loop in order to exploit instruction-level parallelism across multiple basic blocks.
Reference: [Kell75] <author> R. M. Keller, </author> <title> ``Look-ahead Processors'', </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 7, no. </volume> <month> 4 (December </month> <year> 1975), </year> <pages> pp. 177-195. </pages>
Reference-contexts: The most commercially successful approach has been superscalar architectures [John91], which execute two or more independent instructions in parallel. Superscalar implementations provide dynamically scheduled multiple instruction issue and often out-of-order execution using techniques like register renaming <ref> [Kell75] </ref>. Decoupled Access/Execute Architectures (DAE) [Smit82, SmWP86] use an alternate approach to exploiting instruction level parallelism. In DAE machines the memory access and execution portions of an instruction stream are decoupled. <p> If additional registers are provided, they can be allocated by the hardware at run time by coupling the registers with the values needed by the program using a technique called register renaming <ref> [Kell75] </ref>. Register renaming works as follows: If the hardware detects a dependency between registers, it assigns a different logical identifier to the physical register, and replaces the original logical register identifier in the instruction with a reference to the new physical register containing correct value.
Reference: [Smit82] <author> J. E. Smith, </author> <title> ``Decoupled Access/Execute Computer Architectures'', </title> <booktitle> Proceedings of the Ninth Annual International Symposium on Computer Architecture, </booktitle> <address> Austin, Texas (April 26-29, </address> <year> 1982), </year> <pages> pp. 112-119. </pages>
Reference-contexts: The most commercially successful approach has been superscalar architectures [John91], which execute two or more independent instructions in parallel. Superscalar implementations provide dynamically scheduled multiple instruction issue and often out-of-order execution using techniques like register renaming [Kell75]. Decoupled Access/Execute Architectures (DAE) <ref> [Smit82, SmWP86] </ref> use an alternate approach to exploiting instruction level parallelism. In DAE machines the memory access and execution portions of an instruction stream are decoupled. <p> The development of a DAE compiler for the PIPE processor <ref> [Smit82] </ref> is an on-going research project at the University of California, Davis. 3. An Overview of Superscalars Superscalar processors are defined as uniprocessors of any architecture capable of executing and completing multiple instructions per cycle from a single instruction d d stream [John91].
Reference: [SmWP86] <author> J. E. Smith, S. Weiss and N. Y. Pang, </author> <title> ``A Simulation Study of Decoupled Architecture Computers'', </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-35, no. </volume> <month> 8 (August </month> <year> 1986), </year> <pages> pp. 692-702. </pages>
Reference-contexts: The most commercially successful approach has been superscalar architectures [John91], which execute two or more independent instructions in parallel. Superscalar implementations provide dynamically scheduled multiple instruction issue and often out-of-order execution using techniques like register renaming [Kell75]. Decoupled Access/Execute Architectures (DAE) <ref> [Smit82, SmWP86] </ref> use an alternate approach to exploiting instruction level parallelism. In DAE machines the memory access and execution portions of an instruction stream are decoupled. <p> This dynamic loop unrolling makes DAE architectures relatively insensitive to memory latency. Studies have also d d shown that DAE can sustain higher throughput than other architectures like the Cray-1 in memory systems with high latency <ref> [SmWP86] </ref>. One advantage of dynamic loop unrolling over static loop unrolling is that, since the access processor leads the execute processor and determines the termination conditions for loops, it may be able to unroll a loop completely.
Reference: [SDVK87] <author> J. E. Smith, G. E. Dermer, B. D. Vanderwarn, S. D. Klinger, C. M. Rozewski, D. L. Fowler, K. R. Scidmore and J. P. Laudon, </author> <title> ``The ZS-1 Central Processor'', </title> <booktitle> Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Palo Alto, California (October 1987), </address> <pages> pp. 199-204. </pages>
Reference: [SmPl88] <author> J. E. Smith and A. R. Pleszkun, </author> <title> ``Implementation of Precise Interrupts in Pipelined Processors'', </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, no. </volume> <month> 5 (May </month> <year> 1988), </year> <pages> pp. 562-573. </pages>
Reference-contexts: increase the amount of available instruction-level parallelism available; however, this requires the use of hardware techniques such as Tomasulo's algorithm [Toma67] in addition to register renaming to eliminate dependencies among instructions [SoVa87] The adoption of out-of-order execution also raises the question of how to deal with interrupts, branches, and exceptions <ref> [SmPl88] </ref>. Architectural queues also support out-of-order execution between the two processors with respect to the original program sequence. The register renaming effect of removing storage conflicts allows data fetch instructions to complete before their corresponding positions in the original sequence of the program.
Reference: [SoVa87] <author> G. S. Sohi and S. Vajapeyam, </author> <title> ``Instruction Issue Logic for High-Performance, Interruptable Pipelined Processors'', </title> <booktitle> Proceedings of the Fourteenth Annual International Symposium on Computer Architecture, </booktitle> <address> Pittsburgh, Pennsylvania (June 2-5, </address> <year> 1987). </year>
Reference-contexts: Supporting out-of-order execution is another way to increase the amount of available instruction-level parallelism available; however, this requires the use of hardware techniques such as Tomasulo's algorithm [Toma67] in addition to register renaming to eliminate dependencies among instructions <ref> [SoVa87] </ref> The adoption of out-of-order execution also raises the question of how to deal with interrupts, branches, and exceptions [SmPl88]. Architectural queues also support out-of-order execution between the two processors with respect to the original program sequence.
Reference: [Toma67] <author> R. M. Tomasulo, </author> <title> ``An Efficient Algorithm for Exploiting Multiple Arithmetic Units'', </title> <journal> IBM Journal, </journal> <volume> vol. </volume> <month> 11 (January </month> <year> 1967), </year> <pages> pp. 25-33. </pages>
Reference-contexts: The same logical identifier in several different instructions will access different slots in queues, providing correct operation without false data dependencies. Supporting out-of-order execution is another way to increase the amount of available instruction-level parallelism available; however, this requires the use of hardware techniques such as Tomasulo's algorithm <ref> [Toma67] </ref> in addition to register renaming to eliminate dependencies among instructions [SoVa87] The adoption of out-of-order execution also raises the question of how to deal with interrupts, branches, and exceptions [SmPl88]. Architectural queues also support out-of-order execution between the two processors with respect to the original program sequence.
Reference: [Wall91] <author> D. Wall, </author> <title> ``Limits of Instruction-Level Parallelism'', </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA (April 8-11, </address> <year> 1991), </year> <pages> pp. 176-189. </pages> <address> d d </address>
Reference-contexts: The Ideal model assumes perfect register renaming and an infinite number of functional units each with unit latency. Essentially, this model provides the upper bound of available instruction-level parallelism. The result is similar to <ref> [Wall91] </ref>, and is essentially the longest path through the DAG. The DAE model assumes both access and execute processors have pipelined integer and instruction fetch units, while only the execute processor has a pipelined floating point unit. All data accesses are handled by the access processor.
References-found: 11

