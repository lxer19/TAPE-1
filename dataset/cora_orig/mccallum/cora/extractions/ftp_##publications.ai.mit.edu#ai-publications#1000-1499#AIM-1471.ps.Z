URL: ftp://publications.ai.mit.edu/ai-publications/1000-1499/AIM-1471.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/cbcl/course9.520-96/class-5.html
Root-URL: 
Title: A Nonparametric Approach to Pricing and Hedging Derivative Securities Via Learning Networks  
Author: James M. Hutchinson, Andrew Lo and Tomaso Poggio 
Note: Copyright c Massachusetts Institute of Technology, 1994  
Date: 1471 April 1994  92  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING  
Pubnum: A.I. Memo No.  C.B.C.L. Paper No.  
Abstract: We propose a nonparametric method for estimating the pricing formula of a derivative asset using learning networks. Although not a substitute for the more traditional arbitrage-based pricing formulas, network pricing formulas may be more accurate and computationally more efficient alternatives when the underlying asset's price dynamics are unknown, or when the pricing equation associated with no-arbitrage condition cannot be solved analytically. To assess the potential value of network pricing formulas, we simulate Black-Scholes option prices and show that learning networks can recover the Black-Scholes formula from a two-year training set of daily options prices, and that the resulting network formula can be used successfully to both price and delta-hedge options out-of-sample. For comparison, we estimate models using four popular methods: ordinary least squares, radial basis function networks, multilayer perceptron networks, and projection pursuit. To illustrate the practical relevance of our network pricing approach, we apply it to the pricing and delta-hedging of S&P 500 futures options from 1987 to 1991. This report describes research done within the Artificial Intelligence Laboratory and the Sloan School of Management's Research Program in Computational Finance at the Massachusetts Institute of Technology. This research is sponsored by grants from the Office of Naval Research under contracts N00014-92-J-1879 (AASERT) and N00014-93-1-0385. Support for the A.I. Laboratory's artificial intelligence research is provided by ARPA contract N00014-91-J-4038. Additional support was provided by the National Science Foundation under contract ASC-9217041, the Research Program in Computational Finance, Siemens AG, ATR Audio and Visual Perception Research Laboratories. J. Hutchinson is with PHZ Partners (One Cambridge Center, Cambridge, MA 02142). A portion of this research was conducted during A. Lo's tenure as an Alfred P. Sloan Research Fellow. T. Poggio is supported by the Uncas and Helen Whitaker Chair at MIT's Whitaker College. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.R. Barron. </author> <title> Universal approximation bounds for superpositions of a sigmoidal function. </title> <type> Technical Report 58, </type> <institution> Department of Statistics, University of Illinois at Urbana-Champaign, Champaign, IL, </institution> <month> March </month> <year> 1991. </year>
Reference: [2] <author> A.R. Barron and R.L. Barron. </author> <title> Statistical learning networks: a unifying view. </title> <booktitle> In 20th Symposium 11 on the Interface: Computing Science and Statistics, </booktitle> <pages> pages 192-203, </pages> <year> 1988. </year>
Reference: [3] <author> D.S. Broomhead and D. Lowe. </author> <title> Multivariable functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 321-355, </pages> <year> 1988. </year>
Reference: [4] <author> H. Chen. </author> <title> Estimation of a projection-pursuit type regression model. </title> <journal> Ann. Statistics, </journal> <volume> 19 </volume> <pages> 142-157, </pages> <year> 1991. </year>
Reference: [5] <author> G. Cybenko. </author> <title> Approximation by superpositions of a sigmoidal function. </title> <type> Technical Report 856, </type> <institution> University of Illinois, Dept. of Electrical and Computer Engineering, </institution> <year> 1988. </year>
Reference: [6] <author> P. Diaconis and M. Shahshahani. </author> <title> On nonlinear functions of linear combinations. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 5(1) </volume> <pages> 175-191, </pages> <year> 1984. </year>
Reference: [7] <author> D.L. Donoho and I. Johnstone. </author> <title> Projection-based approximation and a duality with kernel methods. </title> <journal> Ann. Stat., </journal> <volume> 17 </volume> <pages> 58-106, </pages> <year> 1989. </year>
Reference: [8] <author> J.H. Friedman and W. Stuetzle. </author> <title> Projection pursuit regression. </title> <journal> Journal of the American Statistical Association, Theory and Methods Section, </journal> <volume> 76(376), </volume> <month> December </month> <year> 1981. </year>
Reference: [9] <author> A. Gallant and H. White. </author> <title> On learning the derivatives of an unknown mapping with multilayer feed-forward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 128-138, </pages> <year> 1992. </year>
Reference: [10] <author> F. Girosi and G. Anzellotti. </author> <title> Rates of convergence of approximation by translates. A.I. </title> <type> Memo 1288, </type> <institution> Massachusetts Institute of Technology Artificial Intelligence Laboratory, </institution> <year> 1992. </year>
Reference: [11] <author> F. Girosi, M. Jones, and T. Poggio. </author> <title> Priors, stabilizers and basis functions: from regularization to radial, tensor and additive splines. </title> <booktitle> Artificial Intelligence Memo 1430, </booktitle> <institution> Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference: [12] <author> F. Girosi and T. Poggio. </author> <title> Networks and the best approximation property. </title> <journal> Biological Cybernetics, </journal> <volume> 63 </volume> <pages> 169-176, </pages> <year> 1990. </year>
Reference: [13] <author> K. Hornik. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2(5) </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference: [14] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Universal approximation of an unknown mapping and its derivatives. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 551-560, </pages> <year> 1990. </year>
Reference: [15] <author> P.J. Huber. </author> <title> Projection pursuit. </title> <journal> Ann. Stat., </journal> <volume> 13(2) </volume> <pages> 435-525, </pages> <year> 1985. </year>
Reference: [16] <author> J. C. Hull. </author> <title> Options, Futures, and Other Derivative Securities. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, 2nd edition, </address> <year> 1993. </year>
Reference: [17] <author> L.K. Jones. </author> <title> On a conjecture of Huber concerning the convergence of projection pursuit regression. </title> <journal> Ann. Stat., </journal> <volume> 15(2) </volume> <pages> 880-882, </pages> <year> 1987. </year>
Reference: [18] <author> L. Ljung and T. Soderstrom. </author> <title> Theory and Practice of Recursive Identification. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference: [19] <author> A. Lo and J. Wang. </author> <title> Implementing option pricing models when asset returns are predictable. </title> <note> Research Program in Computational Finance Working Paper RPCF-1001-93, </note> <institution> MIT Sloan School of Management, </institution> <year> 1993. </year>
Reference: [20] <author> M. Maruyama, F. Girosi, and T. Poggio. </author> <title> A connection between GRBF and MLP. </title> <booktitle> Artificial Intelligence Memo 1291, </booktitle> <institution> Massachusetts Institute of Technology, </institution> <year> 1991. </year>
Reference: [21] <author> Charles A. Micchelli. </author> <title> Interpolation of scattered data: Distance matrices and conditionally positive definite functions. Constructive Approximation, </title> <booktitle> 2 </booktitle> <pages> 11-22, </pages> <year> 1986. </year>
Reference: [22] <author> J. Moody and C. Darken. </author> <title> Fast learning in networks of locally tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference: [23] <author> K. Ng and R. Lippman. </author> <title> A comparative study of the practical characteristics of neural network and conventional pattern classifiers. </title> <editor> In R. Lippman, J. Moody, and D. Touretsky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <address> Morgan-Kaufman, </address> <year> 1991. </year>
Reference: [24] <author> P. Niyogi and F. Girosi. </author> <title> On the relationship between generalization error, hypothesis complexity, and sample complexity for radial basis functions. A.I. </title> <type> Memo 1467, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1994. </year>
Reference: [25] <author> D.B. Parker. </author> <title> Learning logic. </title> <type> Technical Report 47, </type> <institution> Center for Computational Research in Economics and Management Science, MIT, </institution> <month> April </month> <year> 1985. </year>
Reference: [26] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of IEEE, </booktitle> <volume> 78(9) </volume> <pages> 1481-1497, </pages> <year> 1990. </year>
Reference: [27] <author> M. J. D. Powell. </author> <title> Radial basis functions for multi-variable interpolation: A review. </title> <editor> In J. C. Mason and M. G. Cox, editors, </editor> <booktitle> Algorithms for Approximation, </booktitle> <pages> pages 143-167. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1987. </year> <month> 12 </month>
Reference: [28] <author> C. H. Reinsch. </author> <title> Smoothing by spline functions. </title> <journal> Nu--mer Math, </journal> <volume> 10 </volume> <pages> 177-183, </pages> <year> 1967. </year>
Reference: [29] <author> H. Robbins and S. Monro. </author> <title> A stochastic approximation model. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22 </volume> <pages> 400-407, </pages> <year> 1951. </year>
Reference: [30] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representation by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClel-land, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, chapter 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1986. </year>
Reference: [31] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1982. </year>
Reference: [32] <author> G. Wahba. </author> <title> Spline Models for Observational Data, </title> <booktitle> volume 59 of Regional Conference Series in Applied Mathematics. </booktitle> <publisher> SIAM Press, </publisher> <address> Philadelphia, </address> <year> 1990. </year>

References-found: 32

