URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/dxbsp-spaa95.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/dxbsp-spaa95.html
Root-URL: 
Email: fblelloch,marcozg@cs.cmu.edu  fgibbons,matiasg@research.att.com  
Title: Accounting for Memory Bank Contention and Delay in High-Bandwidth Multiprocessors  
Author: Guy E. Blelloch Phillip B. Gibbons Yossi Matias Marco Zagha 
Address: Pittsburgh, PA 15213-3891  600 Mountain Avenue Murray Hill, NJ 07974  
Affiliation: School of Computer Science Carnegie Mellon University  AT&T Bell Laboratories  
Abstract: This paper considers issues of memory performance in shared memory multiprocessors that provide a high-bandwidth network and in which the memory banks are slower than the processors. We are concerned with the effects of memory bank contention, memory bank delay, and the bank expansion factor (the ratio of number of banks to number of processors) on performance, particularly for irregular memory access patterns. This work was motivated by observed discrepancies between predicted and actual performance in a number of irregular algorithms implemented for the cray C90 when the memory contention at a particular location is high. We develop a formal framework for studying memory bank contention and delay, and show several results, both experimental and theoretical. We first show experimentally that our framework is a good predictor of performance on the cray C90 and J90, providing a good accounting of bank contention and delay. Second, we show that it often improves performance to have additional memory banks, even beyond the natural choice of d banks per processor to compensate for a bank delay of d. Third, we explore scenarios under which high-level models, the erew pram and qrqw pram, can be efficiently mapped onto high-bandwidth machines. We provide a work-preserving qrqw pram emulation, whose slowdown is a nonlinear function of the bank delay and the number of banks per processor. Finally, we evaluate the impact of contention on performance for several algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [ACC + 90] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In Proceedings 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Second, we study to what extent we can ignore the effects of multiple memory locations residing in a single bank when using pseudo-random mappings of memory locations to memory banks. Many researchers have studied the effect of randomly mapping memory to banks (e.g. <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown [MV84, KU86, Ran91, Val90a] that with high probability the memory references will be reasonably balanced across the banks. <p> is unlikely. 4 Using random memory mappings Randomly mapping memory locations into banks is a standard technique to reduce module map contention (contention due to multiple memory locations being mapped to the same bank) in simulations of shared memory on machines with a fixed set of memory modules (see, e.g., <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). The primary advantage is that it ensures that concurrently requested memory locations will likely be distributed evenly across the banks.
Reference: [Bai87] <author> D. H. Bailey. </author> <title> Vector computer memory bank contention. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:293-298, </volume> <month> March </month> <year> 1987. </year>
Reference-contexts: We are also interested in arbitrary access patterns. In particular we are interested in patterns that appear in irregular (unstructured) applications such as sparse matrix or pointer-based algorithms. Bailey <ref> [Bai87] </ref> studied the number of additional memory banks needed to compensate for increases in the bank delay. His analysis is in the context of a lightly-loaded system where a processor may have at most one request outstanding and at most one request is ever waiting at a bank.
Reference: [BBDS94] <author> D. H. Bailey, E. Barszcz, L. Dagum, and H. D. Simon. </author> <title> NAS parallel benchmark results 10-94. </title> <type> Technical Report NAS-94-001, </type> <institution> NASA Ames Research Center, </institution> <month> Oc-tober </month> <year> 1994. </year>
Reference-contexts: The qrqw algorithm performs better over a wider range of problem sizes, and even a simple C implementation outperforms the erew version, which is based on a highly-optimized radix sort [ZB91]. (The radix sort is currently the fastest implementation of the NAS sorting benchmark <ref> [BBDS94] </ref>.) Each element i then writes its self-index into a destination array at the location specified by the ith random-index. Elements for which there are no collisions are considered done and drop out. Elements for which there are collisions repeat another round.
Reference: [BHZ93] <author> G. E. Blelloch, M. A. Heroux, and M. Zagha. </author> <title> Segmented operations for sparse matrix computation on vector multiprocessors. </title> <type> Technical Report CMU-CS-93-173, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: We denote this model as the (d; x)-bsp (the "deluxe" bsp). The research presented in this paper was motivated by the need to explain observed discrepancies between predicted performance based on previous models and actual performance in a number of programs written for the cray C90 <ref> [BHZ93, Zag95] </ref>. <p> For example, Figure 1 and later Figure 12 compare predicted and measured times for an implementation of a graph-connectivity algorithm [Gre94] and a sparse-matrix vector multiplication <ref> [BHZ93] </ref> algorithm. These times are compared with the predictions based on models that do not account for the bank delay (e.g. the bsp or logp). <p> Our implementation uses a compressed row format containing the number of non-zero elements in each row, and the values of each non-zero matrix element along with its column index. The computation is vectorized using "segmented scan" operations <ref> [BHZ93] </ref>, a technique that allows the latency to be hidden regardless of the structure of the matrix. For the purposes of analyzing contention, the most important characteristic of our implementation is that elements from the input vector are gathered based on the column indices of the nonzero matrix element.
Reference: [Cal88] <author> D. A. Calahan. </author> <title> Characterization of memory conflict loading on the CRAY-2. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 299-302, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Related Work: There have been several studies of memory bank contention on vector multiprocessors <ref> [OL85, CS86, Cal88, Cal89] </ref>. These studies include more detailed models of the memory system and concentrate on constant stride access patterns. We are interested in having as simple a model as possible that captures the important effects but is independent of most machine-specific details.
Reference: [Cal89] <author> D. A. Calahan. </author> <title> Some results in memory conflict analysis. </title> <booktitle> In Proceedings Supercomputing '89, </booktitle> <pages> pages 775-778, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Related Work: There have been several studies of memory bank contention on vector multiprocessors <ref> [OL85, CS86, Cal88, Cal89] </ref>. These studies include more detailed models of the memory system and concentrate on constant stride access patterns. We are interested in having as simple a model as possible that captures the important effects but is independent of most machine-specific details.
Reference: [CKP + 93] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Proc. 4th ACM SIGPLAN Symp. on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: This is the point where the total bandwidth available at the processors and network matches the total bandwidth available at the memories. Although we have chosen the bsp model to extend it should be straightforward to extend other related models, such as the logp <ref> [CKP + 93] </ref> or dmm [MV84] models, with the d and x parameters. To extend the logp it is assumed that the banks are separate modules from the processors.
Reference: [CS86] <author> T. Cheung and J. E. Smith. </author> <title> A simulation study of the CRAY X-MP memory system. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(7):613-622, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: Related Work: There have been several studies of memory bank contention on vector multiprocessors <ref> [OL85, CS86, Cal88, Cal89] </ref>. These studies include more detailed models of the memory system and concentrate on constant stride access patterns. We are interested in having as simple a model as possible that captures the important effects but is independent of most machine-specific details. <p> All the experiments are based on using the scatter operation although experiments with the gather operation give almost identical results. The patterns we are interested in cannot be created with strided access|timings for various strided access patterns can be found elsewhere <ref> [CS86, Soh93] </ref>. All experiments were run on a dedicated 8 processor system. All graphs are for the cray J90 except where noted|cray C90 results are qualitatively similar. For all experiments S = 64K and L is negligible.
Reference: [CW79] <author> L.J. Carter and M.N. Wegman. </author> <title> Universal classes of hash functions. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18 </volume> <pages> 143-154, </pages> <year> 1979. </year>
Reference-contexts: The function h 1 a , which is called the multiplicative hashing scheme in [Knu73, p. 509], was recently shown by Dietzfelbinger et al. [DHKP93] to be 2-universal in the sense of Carter and Wegman <ref> [CW79] </ref>: for any two distinct numbers x; y 2 [0::2 u 1], Prob a (x) = h 1 the collision probability is approximately the same as for a random mapping.
Reference: [DGMP92] <author> M. Dietzfelbinger, J. Gil, Y. Matias, and N. Pip-penger. </author> <title> Polynomial hash functions are reliable. </title> <booktitle> In Proc. 19th Int. Colloquium on Automata Languages and Programming, </booktitle> <publisher> Springer LNCS 623, </publisher> <pages> pages 235-246, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Thus, the choice of a hash function may be influenced by several parameters, including its degree of universality, its evaluation cost, and its congestion behavior, both theoretically (see <ref> [DGMP92] </ref>) and experimentally (see [EK93]). Hash Function T =n Linear h 1 h 1 Quadratic h 2 h 2 Cubic h 3 h 3 Table 3: The evaluation cost of various hash functions in terms of clock cycles per element (for each cray C90 processor).
Reference: [DHKP93] <author> M. Dietzfelbinger, T. Hagerup, J. Katajainen, and M. Penttonen. </author> <title> A reliable randomized algorithm for the closest-pair problem. </title> <type> Technical Report Research Report 513, </type> <institution> Universitat Dortmund, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: The function h 1 a , which is called the multiplicative hashing scheme in [Knu73, p. 509], was recently shown by Dietzfelbinger et al. <ref> [DHKP93] </ref> to be 2-universal in the sense of Carter and Wegman [CW79]: for any two distinct numbers x; y 2 [0::2 u 1], Prob a (x) = h 1 the collision probability is approximately the same as for a random mapping.
Reference: [EK93] <author> C. Engelmann and J. Keller. </author> <title> Simulation-based comparison of hash functions for emulated shared memory. </title> <booktitle> In Proc. Parallel architectures and languages Europe, </booktitle> <publisher> Springer LNCS 694, </publisher> <pages> pages 1-11, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Thus, the choice of a hash function may be influenced by several parameters, including its degree of universality, its evaluation cost, and its congestion behavior, both theoretically (see [DGMP92]) and experimentally (see <ref> [EK93] </ref>). Hash Function T =n Linear h 1 h 1 Quadratic h 2 h 2 Cubic h 3 h 3 Table 3: The evaluation cost of various hash functions in terms of clock cycles per element (for each cray C90 processor).
Reference: [GMR94a] <author> P. B. Gibbons, Y. Matias, and V. Ramachan-dran. </author> <title> Efficient low-contention parallel algorithms. </title> <booktitle> In Proc. 6th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 236-247, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: case x d we observe that (d=x) is an inevitable work overhead, and provide an emulation of the qrqw pram on the (d; x)-bsp in which the overhead matches this factor; the emulation, as well as its analysis, follow and generalize the qrqw pram emulation onto the bsp given in <ref> [GMR94a] </ref>. For the case x d, we present a work-preserving emulation of the qrqw pram on the (d; x)-bsp, assuming high bandwidth, where the effect of d on the slowdown is partially compensated for by the expansion factor x; the analysis for this emulation requires a somewhat different approach. <p> In the next section, we discuss some experimental results regarding the implementation of two qrqw pram algorithms from <ref> [GMR94a] </ref> on the cray. <p> Binary search: The first qrqw algorithm is a simple parallel binary search to look up n keys in a balanced binary search tree of size m <ref> [GMR94a] </ref>. Such binary searching is an important substep in several algorithms for sorting and merging (e.g. [RV87]). The algorithm replicates nodes of the search tree to avoid contention, and at each level selects one of the replicated nodes at random. <p> The rounds continue until there are no elements left. At this point the values written into the destination are packed into contiguous locations, producing the index for the random permutation. The algorithm runs in O (n=p + lg n) time on a qrqw pram <ref> [GMR94a] </ref>. In our experiments we compare the running time of the algorithm to an algorithm designed for the erew model (a sorting based algorithm, which is the most practical erew algorithm in the literature, to the best of our knowledge). <p> This experiment illustrates that by allowing some well-accounted memory contention, we can get a more practical algorithm than in a scenario where we do not allow any contention. (A similar experiment, on the MasPar MP-1, was reported in <ref> [GMR94a] </ref>; for the erew algorithm, the system sort was used.) Results of the experiments are shown in Figure 11. (The timings do not include the time for random number generation; however, the two algorithms require approximately the same number of random bits.) Sparse matrix multiplication: In our third algorithm experiment, we
Reference: [GMR94b] <author> P. B. Gibbons, Y. Matias, and V. Ramachan-dran. </author> <title> The QRQW PRAM: Accounting for contention in parallel algorithms. </title> <booktitle> In Proc. 5th ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 638-648, </pages> <month> Jan-uary </month> <year> 1994. </year>
Reference-contexts: The effect of expansion is shown in Third, we explore scenarios under which high-level models for algorithm design, the erew pram (e.g. [JaJ92]) and the stronger qrqw pram <ref> [GMR94b] </ref> can be effectively mapped onto high-bandwidth machines (small g) when memory bank delay is accounted for. <p> x from the domain [0::2 u ] into the range [0::2 m ], and a, b, c, and d are odd numbers selected at random from [1::2 u 1]. 5 High-level programming model In this section, we explore scenarios under which a high-level model for algorithm design, the qrqw pram <ref> [GMR94b] </ref>, can be effectively mapped onto high-bandwidth machines. In the context of this section, we assume that high-bandwidth machines have a small g. When x d, we provide a work-preserving emulation of the qrqw pram on the (d; x)-bsp, whose slowdown is a nonlinear function of d and x. <p> The qrqw pram allows for concurrent reading and writing to shared memory locations, but at cost proportional to the number of readers/writers to any one memory location. In particular the charge is based on the maximum contention at any location. It was argued in <ref> [GMR94b] </ref> that neither the exclusive nor the concurrent rules for memory access accurately reflect the contention capabilities of most commercial and research machines, and that the queue rule, used in the qrqw cost metric, is more appropriate. <p> Therefore, the time on the (d; x)-bsp is at least Rd max n xp ; kd : The following two theorems are generalizations of the qrqw pram emulation on a bsp given in <ref> [GMR94b] </ref>. Theorem 5.1 handles the case where x d; the proof follows the one given for the bsp simulation, and is given in the appendix of this extended abstract. Theorem 5.2 handles the case of larger x; its proof requires a somewhat different approach.
Reference: [GMR94c] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> The queue-read queue-write PRAM model: Accounting for contention in parallel algorithms. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ, </address> <month> September </month> <year> 1994. </year> <note> Submitted for publication. </note>
Reference: [Gre94] <author> J. Greiner. </author> <title> A comparison of data-parallel algorithms for connected components. </title> <booktitle> In Proceedings Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 16-25, </pages> <address> Cape May, NJ, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: both for the cray C90, which uses static ram (sram) and has a bank delay of 6 clock cycles, and for the more modestly priced cray J90, which uses dy set of memory access patterns extracted from a trace of Greiner's algorithm for finding the connected components of a graph <ref> [Gre94] </ref>. Measured times on an 8 processor cray J90 for several patterns are shown with squares. Predicted times are given both for the bsp and the (d; x)-bsp as a function of contention. namic ram (dram) and has a bank delay of 14 clock cycles. <p> For example, Figure 1 and later Figure 12 compare predicted and measured times for an implementation of a graph-connectivity algorithm <ref> [Gre94] </ref> and a sparse-matrix vector multiplication [BHZ93] algorithm. These times are compared with the predictions based on models that do not account for the bank delay (e.g. the bsp or logp). <p> Except in the dense column, column indices are selected at random. Figure 12 shows measured and predicted time as a function of the length of the dense column. Connected components: Our final algorithm experiment measures the contention in Greiner's algorithm for finding the connected components of a graph <ref> [Gre94] </ref>. The algorithm consists of several phases: hooking nodes together to form a forest, performing repeated shortcutting operations to contract each tree to a single node, contracting the graph to form a new graph that is processed recursively, and expanding the graph to propagate the new labels.
Reference: [Hoe63] <author> W. Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58 </volume> <pages> 13-30, </pages> <year> 1963. </year>
Reference: [HS93] <author> W.-C. Hsu and J. E. Smith. </author> <title> Performance of cached DRAM organizations in vector supercomputers. </title> <booktitle> In Proc. 20th International Symp. on Computer Architecture, </booktitle> <pages> pages 327-336, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: There are several issues that the (d; x)-bsp does not capture, which would be needed for a more refined model of the memory system. These include the effects of the network, the effects of caching at the memory banks (available on the tera and discussed by Hsu and Smith <ref> [HS93] </ref>), and the effects of the order of injecting messages into the network. We are currently looking into analyzing the contention properties of other practical parallel algorithms, e.g. for merging, multiprefix [She93], and list ranking [RM94]. Acknowledgements.
Reference: [IC93] <author> D. T. Harper III and Y. Costa. </author> <title> Analytical estimation of vector access performance in parallel memory architectures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(5) </volume> <pages> 616-624, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Second, we study to what extent we can ignore the effects of multiple memory locations residing in a single bank when using pseudo-random mappings of memory locations to memory banks. Many researchers have studied the effect of randomly mapping memory to banks (e.g. <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown [MV84, KU86, Ran91, Val90a] that with high probability the memory references will be reasonably balanced across the banks. <p> is unlikely. 4 Using random memory mappings Randomly mapping memory locations into banks is a standard technique to reduce module map contention (contention due to multiple memory locations being mapped to the same bank) in simulations of shared memory on machines with a fixed set of memory modules (see, e.g., <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). The primary advantage is that it ensures that concurrently requested memory locations will likely be distributed evenly across the banks.
Reference: [JaJ92] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year>
Reference-contexts: The effect of expansion is shown in Third, we explore scenarios under which high-level models for algorithm design, the erew pram (e.g. <ref> [JaJ92] </ref>) and the stronger qrqw pram [GMR94b] can be effectively mapped onto high-bandwidth machines (small g) when memory bank delay is accounted for.
Reference: [Knu73] <author> D.E. Knuth. </author> <title> Sorting and Searching, </title> <booktitle> volume 3 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <address> Reading, Massachusetts, </address> <year> 1973. </year>
Reference-contexts: When hash functions are used for pseudo-random mapping of memory locations to memory banks, it is important that they exhibit favorable properties for any given input (i.e. that they are "universal"). The function h 1 a , which is called the multiplicative hashing scheme in <ref> [Knu73, p. 509] </ref>, was recently shown by Dietzfelbinger et al. [DHKP93] to be 2-universal in the sense of Carter and Wegman [CW79]: for any two distinct numbers x; y 2 [0::2 u 1], Prob a (x) = h 1 the collision probability is approximately the same as for a random mapping.
Reference: [KU86] <author> A.R. Karlin and E. Upfal. </author> <title> Parallel hashing|an efficient implementation of shared memory. </title> <booktitle> In Proc. 18th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 160-168, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Second, we study to what extent we can ignore the effects of multiple memory locations residing in a single bank when using pseudo-random mappings of memory locations to memory banks. Many researchers have studied the effect of randomly mapping memory to banks (e.g. <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown [MV84, KU86, Ran91, Val90a] that with high probability the memory references will be reasonably balanced across the banks. <p> Many researchers have studied the effect of randomly mapping memory to banks (e.g. [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93]). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown <ref> [MV84, KU86, Ran91, Val90a] </ref> that with high probability the memory references will be reasonably balanced across the banks. <p> is unlikely. 4 Using random memory mappings Randomly mapping memory locations into banks is a standard technique to reduce module map contention (contention due to multiple memory locations being mapped to the same bank) in simulations of shared memory on machines with a fixed set of memory modules (see, e.g., <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). The primary advantage is that it ensures that concurrently requested memory locations will likely be distributed evenly across the banks.
Reference: [MV84] <author> K. Mehlhorn and U. Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memories. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 339-374, </pages> <year> 1984. </year>
Reference-contexts: Second, we study to what extent we can ignore the effects of multiple memory locations residing in a single bank when using pseudo-random mappings of memory locations to memory banks. Many researchers have studied the effect of randomly mapping memory to banks (e.g. <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown [MV84, KU86, Ran91, Val90a] that with high probability the memory references will be reasonably balanced across the banks. <p> Many researchers have studied the effect of randomly mapping memory to banks (e.g. [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93]). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown <ref> [MV84, KU86, Ran91, Val90a] </ref> that with high probability the memory references will be reasonably balanced across the banks. <p> This is the point where the total bandwidth available at the processors and network matches the total bandwidth available at the memories. Although we have chosen the bsp model to extend it should be straightforward to extend other related models, such as the logp [CKP + 93] or dmm <ref> [MV84] </ref> models, with the d and x parameters. To extend the logp it is assumed that the banks are separate modules from the processors. <p> is unlikely. 4 Using random memory mappings Randomly mapping memory locations into banks is a standard technique to reduce module map contention (contention due to multiple memory locations being mapped to the same bank) in simulations of shared memory on machines with a fixed set of memory modules (see, e.g., <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). The primary advantage is that it ensures that concurrently requested memory locations will likely be distributed evenly across the banks.
Reference: [OL85] <author> W. Oed and O. Lange. </author> <title> On the effective bandwidth of interleaved memories in vector processor systems. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 949-957, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Related Work: There have been several studies of memory bank contention on vector multiprocessors <ref> [OL85, CS86, Cal88, Cal89] </ref>. These studies include more detailed models of the memory system and concentrate on constant stride access patterns. We are interested in having as simple a model as possible that captures the important effects but is independent of most machine-specific details.
Reference: [Rag88] <author> P. Raghavan. </author> <title> Probabilistic construction of deterministic algorithms: approximating packing integer programs. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 37 </volume> <pages> 130-143, </pages> <year> 1988. </year>
Reference-contexts: Let fi = j=1 a j x j . Then, E ( fi ) = j=1 B 1 m X k j = xp Z = x p k t : By a theorem of Raghavan and Spencer <ref> [Rag88] </ref>, which provides a tail inequality for the weighted sum of Bernoulli trials, for any - &gt; 0, Prob ( fi &gt; (1 + -)E ( fi )) &lt; e - E ( fi ) We set - = ff x d 1, implying (1 + -)E ( fi ) =
Reference: [Ran91] <author> A.G. Ranade. </author> <title> How to emulate shared memory. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 42 </volume> <pages> 307-326, </pages> <year> 1991. </year>
Reference-contexts: Second, we study to what extent we can ignore the effects of multiple memory locations residing in a single bank when using pseudo-random mappings of memory locations to memory banks. Many researchers have studied the effect of randomly mapping memory to banks (e.g. <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown [MV84, KU86, Ran91, Val90a] that with high probability the memory references will be reasonably balanced across the banks. <p> Many researchers have studied the effect of randomly mapping memory to banks (e.g. [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93]). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown <ref> [MV84, KU86, Ran91, Val90a] </ref> that with high probability the memory references will be reasonably balanced across the banks. <p> We show that having a high expansion factor can alleviate this problem to some extent. For the cray C90, which has a high expansion 1 In the case of Ranade's work <ref> [Ran91] </ref> it is assumed that references to a single location are combined in the network. multiple memory locations being mapped to the same bank to the time that excludes the effect, when using random mapping. This is given as a function of expansion and is for a worst-case reference pattern. <p> is unlikely. 4 Using random memory mappings Randomly mapping memory locations into banks is a standard technique to reduce module map contention (contention due to multiple memory locations being mapped to the same bank) in simulations of shared memory on machines with a fixed set of memory modules (see, e.g., <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). The primary advantage is that it ensures that concurrently requested memory locations will likely be distributed evenly across the banks.
Reference: [Rau91] <author> B. Rau. </author> <title> Pseudo-randomly interleaved memory. </title> <booktitle> In Proceedings Int. Symp. Computer Architecture, </booktitle> <pages> pages 74-83, </pages> <year> 1991. </year>
Reference-contexts: Second, we study to what extent we can ignore the effects of multiple memory locations residing in a single bank when using pseudo-random mappings of memory locations to memory banks. Many researchers have studied the effect of randomly mapping memory to banks (e.g. <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown [MV84, KU86, Ran91, Val90a] that with high probability the memory references will be reasonably balanced across the banks. <p> is unlikely. 4 Using random memory mappings Randomly mapping memory locations into banks is a standard technique to reduce module map contention (contention due to multiple memory locations being mapped to the same bank) in simulations of shared memory on machines with a fixed set of memory modules (see, e.g., <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). The primary advantage is that it ensures that concurrently requested memory locations will likely be distributed evenly across the banks.
Reference: [RH90] <author> R. Raghavan and J. P. Hayes. </author> <title> On randomly interleaved memories. </title> <booktitle> In Proceedings Supercomputing '90, </booktitle> <pages> pages 49-58, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Second, we study to what extent we can ignore the effects of multiple memory locations residing in a single bank when using pseudo-random mappings of memory locations to memory banks. Many researchers have studied the effect of randomly mapping memory to banks (e.g. <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown [MV84, KU86, Ran91, Val90a] that with high probability the memory references will be reasonably balanced across the banks. <p> is unlikely. 4 Using random memory mappings Randomly mapping memory locations into banks is a standard technique to reduce module map contention (contention due to multiple memory locations being mapped to the same bank) in simulations of shared memory on machines with a fixed set of memory modules (see, e.g., <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). The primary advantage is that it ensures that concurrently requested memory locations will likely be distributed evenly across the banks.
Reference: [RM94] <author> M. Reid-Miller. </author> <title> List ranking and list scan on the Cray C-90. </title> <booktitle> In Proceedings Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 104-113, </pages> <address> Cape May, NJ, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: We are currently looking into analyzing the contention properties of other practical parallel algorithms, e.g. for merging, multiprefix [She93], and list ranking <ref> [RM94] </ref>. Acknowledgements. The experiments were run on the cray C90 at the Pittsburgh Supercomputing Center (PSC) and on a cray J90 at Cray Research.
Reference: [RV87] <author> J. H. Reif and L. G. Valiant. </author> <title> A logarithmic time sort for linear size networks. </title> <journal> Journal of the ACM, </journal> <volume> 34(1) </volume> <pages> 60-76, </pages> <year> 1987. </year>
Reference-contexts: Binary search: The first qrqw algorithm is a simple parallel binary search to look up n keys in a balanced binary search tree of size m [GMR94a]. Such binary searching is an important substep in several algorithms for sorting and merging (e.g. <ref> [RV87] </ref>). The algorithm replicates nodes of the search tree to avoid contention, and at each level selects one of the replicated nodes at random.
Reference: [She93] <author> T. J. She*er. </author> <title> Implementing the multiprefix operation on parallel and vector computers. </title> <booktitle> In Proceedings Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 377-386, </pages> <address> Velen, Germany, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: We are currently looking into analyzing the contention properties of other practical parallel algorithms, e.g. for merging, multiprefix <ref> [She93] </ref>, and list ranking [RM94]. Acknowledgements. The experiments were run on the cray C90 at the Pittsburgh Supercomputing Center (PSC) and on a cray J90 at Cray Research.
Reference: [Soh93] <author> G. S. Sohi. </author> <title> High-bandwidth interleaved memories for vector processors | a simulation study. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(1) </volume> <pages> 34-44, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: All the experiments are based on using the scatter operation although experiments with the gather operation give almost identical results. The patterns we are interested in cannot be created with strided access|timings for various strided access patterns can be found elsewhere <ref> [CS86, Soh93] </ref>. All experiments were run on a dedicated 8 processor system. All graphs are for the cray J90 except where noted|cray C90 results are qualitatively similar. For all experiments S = 64K and L is negligible.
Reference: [ST91] <author> J. E. Smith and W. R. Taylor. </author> <title> Accurate modeling of interconnection networks in vector supercomputers. </title> <booktitle> In Proc. International Conference on Supercomputing, </booktitle> <pages> pages 264-273, </pages> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Versions (a) and (b) are quite close to the predicted performance. Version (c), however, is up to a factor of 2.5 off from the prediction because of congestion at one of the subsections of the network. A more refined model would be needed to take account of this <ref> [ST91] </ref>, but the experiment shows that even in what we expect to be the worst case the predictions are not catastrophic.
Reference: [TS92] <author> K. Thearling and S. Smith. </author> <title> An improved supercomputer sorting benchmark. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <pages> pages 14-19, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Experiment 3: To verify that the running time can be accurately predicted for less regular distributions of memory accesses, we constructed an experiment using the entropy distributions suggested by Thearling and Smith <ref> [TS92] </ref>. The distributions are generated by starting with a set of random keys and then bitwise anding together each key with another key selected at random. Iterating this process generates a family of distributions each with a higher contention than the previous and eventually all keys become 0.
Reference: [TTT + 94] <author> S. Tanoi, Y. Tanaka, T. Tanabe, A. Kita, T. Inada, R. Hamazaki, Y. Ohtsuki, and M. Uesugi. </author> <title> A 32-bank 256-Mb DRAM with cache and tag. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 29(11) </volume> <pages> 1330-1335, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Over the years processor speeds have been increasing much more rapidly than memory speeds. This divergence has motivated several computer manufacturers to design memory systems with many more memory banks than processors (see Table 1) and also memory manufacturers to design memory modules with multiple internal banks <ref> [TTT + 94] </ref>. When used in conjunction with latency hiding techniques (e.g. vectorization, multithreading, prefetching) and a high-bandwidth network, supplying more banks than processors provides an effective way to support shared memory access with relatively slow inexpensive (commodity) memory parts.
Reference: [Val90a] <author> L.G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Commun. ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: Our goal is to use a simple model for such machines that accounts for the slow access rate of memory banks on the one hand, and for the expanded number of memory banks on the other hand. For this purpose we extend Valiant's bulk-synchronous parallel (bsp) model <ref> [Val90a] </ref> with two additional parameters: (i) the bank delay, d, which is the number of cycles between accesses to a memory bank, and (ii) the expansion factor , x, which is the ratio of the number of memory banks to the number of processors. <p> Second, we study to what extent we can ignore the effects of multiple memory locations residing in a single bank when using pseudo-random mappings of memory locations to memory banks. Many researchers have studied the effect of randomly mapping memory to banks (e.g. <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown [MV84, KU86, Ran91, Val90a] that with high probability the memory references will be reasonably balanced across the banks. <p> Many researchers have studied the effect of randomly mapping memory to banks (e.g. [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93]). If there is sufficient parallel "slackness" (extra parallelism) so that each bank is receiving multiple requests, it has been shown <ref> [MV84, KU86, Ran91, Val90a] </ref> that with high probability the memory references will be reasonably balanced across the banks. <p> is unlikely. 4 Using random memory mappings Randomly mapping memory locations into banks is a standard technique to reduce module map contention (contention due to multiple memory locations being mapped to the same bank) in simulations of shared memory on machines with a fixed set of memory modules (see, e.g., <ref> [MV84, KU86, Ran91, Val90a, ACC + 90, RH90, Rau91, IC93] </ref>). The primary advantage is that it ensures that concurrently requested memory locations will likely be distributed evenly across the banks.
Reference: [Val90b] <author> L.G. Valiant. </author> <title> General purpose parallel architectures. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume A, chapter 18, </booktitle> <pages> pages 944-971. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1990. </year>
Reference: [Zag95] <author> M. Zagha. </author> <title> Effiicient irregular computation on pipelined-memory multiprocessors. </title> <type> Ph.D. </type> <note> Thesis (In Preparation), </note> <year> 1995. </year>
Reference-contexts: We denote this model as the (d; x)-bsp (the "deluxe" bsp). The research presented in this paper was motivated by the need to explain observed discrepancies between predicted performance based on previous models and actual performance in a number of programs written for the cray C90 <ref> [BHZ93, Zag95] </ref>.
Reference: [ZB91] <author> M. Zagha and G. E. Blelloch. </author> <title> Radix sort for vector multiprocessors. </title> <booktitle> In Proceedings Supercomputing '91, </booktitle> <pages> pages 712-721, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: The qrqw algorithm performs better over a wider range of problem sizes, and even a simple C implementation outperforms the erew version, which is based on a highly-optimized radix sort <ref> [ZB91] </ref>. (The radix sort is currently the fastest implementation of the NAS sorting benchmark [BBDS94].) Each element i then writes its self-index into a destination array at the location specified by the ith random-index. Elements for which there are no collisions are considered done and drop out.
References-found: 39

