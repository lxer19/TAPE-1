URL: ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1567.ps.Z
Refering-URL: http://www.ai.mit.edu/people/mmp/mmp.html
Root-URL: 
Title: Learning Fine Motion by Markov Mixtures of Experts  
Author: Marina Meila Michael I. Jordan 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology, 1996  
Date: 1567 November, 1995  133  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY  
Pubnum: A.I. Memo No.  C.B.C.L. Memo No.  
Abstract: Compliant control is a standard method for performing fine manipulation tasks, like grasping and assembly, but it requires estimation of the state of contact between the robot arm and the objects involved. Here we present a method to learn a model of the movement from measured data. The method requires little or no prior knowledge and the resulting model explicitly estimates the state of contact . The current state of contact is viewed as the hidden state variable of a discrete HMM. The control dependent transition probabilities between states are modeled as parametrized functions of the measurement. We show that their parameters can be estimated from measurements concurrently with the estimation of the parameters of the movement in each state of contact . The learning algorithm is a variant of the EM procedure. The E step is computed exactly; solving the M step exactly would require solving a set of coupled nonlinear algebraic equations in the parameters. Instead, gradient ascent is used to produce an increase in likelihood. This report describes research done at the Dept. of Electrical Engineering and Computer Science, the Dept. of Brain and Cognitive Sciences, the Center for Biological and Computational Learning and the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Dept. of Defense and by the Office of Naval Research. Michael I. Jordan is a NSF Presidential Young Investigator. The authors can be reached at M.I.T., Dept. of Brain and Cognitive Sciences, 79 Amherst St., Cambridge MA 02139, USA. E-mail: mmp@psyche.mit.edu, jordan@psyche.mit.edu 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Bengio and P. Frasconi. </author> <title> An input output HMM architecture. </title> <editor> In J. D. Covan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Neural Information Processing Systems - 7, </booktitle> <year> 1995. </year>
Reference-contexts: It becomes the model of <ref> [1] </ref> when A and f are neural networks.
Reference: [2] <author> R. Bolles and R. Paul. </author> <title> The use of sensory feedback in a programmable assembly system. </title> <type> Technical report, </type> <institution> Stanford U., </institution> <year> 1973. </year>
Reference-contexts: But each of them have to face and solve the problem of estimating the state of contact (i.e. checking if the contact with the correct surface is achieved), a direct consequence of dealing with noisy measurements. The earliest approaches such as guarded moves <ref> [2] </ref> and compliance control [9] are problem dependent and deal only indirectly with uncertainty . Preimage fine motion planning [8] directly incorporates uncertainty and provides a formal problem statement and approach, but becomes computationally intractable in the case of noisy measurements.
Reference: [3] <author> T. W. Cacciatore and S. J. Nowlan. </author> <title> Mixtures of controllers for jump linear and non-linear plants. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Neural Information Processing Systems - 6, </booktitle> <year> 1994. </year>
Reference-contexts: It becomes the model of [1] when A and f are neural networks. A model where the Markov chain transitions are implemented by a recurrent network was proposed in <ref> [3] </ref>. 3 An EM Algorithm for MME To estimate the values of the unknown parameters 2 ; W k ; k ; k = 1; : : : ; m given the sequence of observations 2 u 0;T ; y 0;T ; T &gt; 0 and a prior probability of the
Reference: [4] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: ; W k ; k ; k = 1; : : : ; m given the sequence of observations 2 u 0;T ; y 0;T ; T &gt; 0 and a prior probability of the initial state j (0) = P r [q (0) = j]: the Expectation Maximization (EM) <ref> [4] </ref> algorithm will be used. The EM algorithm is an iterative procedure which converges asymptotically to a local maximum of the likelihood function. It requires the introduction of unobserved variables, which, in our case will be the hidden state variables fq (t)g T t=0 .
Reference: [5] <author> B. S. Eberman. </author> <title> A sequential decision approach to sensing manipulation contact features. </title> <type> PhD thesis, </type> <institution> M.I.T., </institution> <year> 1995. </year>
Reference-contexts: Preimage fine motion planning [8] directly incorporates uncertainty and provides a formal problem statement and approach, but becomes computationally intractable in the case of noisy measurements. Attempting to overcome this last difficulty are the proposals of [10] local control around a nominal path (LCNP) and <ref> [5] </ref> feature based programming. The latter treats the uncertanty in position and velocity in a probabilistic framework. It associates to each state of contact the coresponding movement model; that is: a relationship between positions and nominal 1 and actual velocities that holds over a domain of the position-nominal velocity space. <p> The functions in A depend both on the movement and on the noise models. Because the noise is propagated through non-linearities to the output, a closed form as in (1) may be difficult to obtain. Moreover, a correct noise model for each of the possible uncertainties is rarely available <ref> [5] </ref>. A common practical approach is to trade accuracy for computability and to parametrize A in a form which is easy to update but devoid of physical meaning. In all the cases where maximization cannot be performed exactly, one can resort to Generalized EM by merely increasing J .
Reference: [6] <author> Stuart Geman, Elie Bienenstock, and Rene Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: In the case where we can talk about true parameters, the prediction error is reflected by the accuracy of the parameters. For a more rigorous and detailed discussion of data complexity consult <ref> [6] </ref> and its references. For the purposes of this paper it is sufficient to state that for a given class of models, the data complexity increases with the number of parameters.
Reference: [7] <author> M. I. Jordan and R. A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: depends on u and of a set of parameters W j : a ij (u (t); W j ) = P r [q (t+1) = ijq (t) = j; u (t)] t = 0; 1; : : : with X i that this model generalizes the mixture of experts architecture <ref> [7] </ref>, to which it reduces in the case where a ij are independent of j (the columns of A are all equal). It becomes the model of [1] when A and f are neural networks. <p> In the special case when the MME reduces to a mixture of experts, the probability of a state depends only on the input u, and it is easy to see (also cf. <ref> [7] </ref>) that the gating network partitions the input space, assigning to each expert a subset of it. As the domains of the experts are generally contiguous, the mixture of experts can be viewed as a combination of local models, each being accurate in a confined region of the input space.
Reference: [8] <author> T. Lozano-Perez. </author> <title> Spatial planning: a configuration space approach. </title> <journal> IEEE Transactions on Computers, </journal> <year> 1983. </year>
Reference-contexts: The earliest approaches such as guarded moves [2] and compliance control [9] are problem dependent and deal only indirectly with uncertainty . Preimage fine motion planning <ref> [8] </ref> directly incorporates uncertainty and provides a formal problem statement and approach, but becomes computationally intractable in the case of noisy measurements. Attempting to overcome this last difficulty are the proposals of [10] local control around a nominal path (LCNP) and [5] feature based programming.
Reference: [9] <author> M. T. Mason. </author> <title> Compliance and force control for computer controlled manipulation. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <year> 1981. </year>
Reference-contexts: Uncertainty is due to inaccurate knowledge of the geometric shapes and positions of the objects and of their physical properties (surface friction coefficients), or to positioning errors in the manipulator. The standard solution to this problem is controlled compliance <ref> [9] </ref>. Under compliant motion, the task is performed in stages. In each stage the robot arm maintains contact with a selected surface or feature of the environment. The stage ends when contact with the feature corresponding to the next stage is made. <p> But each of them have to face and solve the problem of estimating the state of contact (i.e. checking if the contact with the correct surface is achieved), a direct consequence of dealing with noisy measurements. The earliest approaches such as guarded moves [2] and compliance control <ref> [9] </ref> are problem dependent and deal only indirectly with uncertainty . Preimage fine motion planning [8] directly incorporates uncertainty and provides a formal problem statement and approach, but becomes computationally intractable in the case of noisy measurements.
Reference: [10] <author> S. Narasimhan. </author> <title> Task-level strategies for robots. </title> <type> PhD thesis, </type> <institution> M.I.T., </institution> <year> 1994. </year>
Reference-contexts: Preimage fine motion planning [8] directly incorporates uncertainty and provides a formal problem statement and approach, but becomes computationally intractable in the case of noisy measurements. Attempting to overcome this last difficulty are the proposals of <ref> [10] </ref> local control around a nominal path (LCNP) and [5] feature based programming. The latter treats the uncertanty in position and velocity in a probabilistic framework. <p> The inputs were 4-dimensional vectors of positions (x; y) and nominal velocities (v x ; v y ) and the output was the predicted position (x out ; y out ). The coordinate range was <ref> [0, 10] </ref> and the admissible velocities were confined to the upper right quadrant (v x ; v y V min &gt; 0) and had magnitudes (after multiplication by T ) between 0 and 4. The restriction in direction guaranteed that the trajectories remained in the coordinate domain.
Reference: [11] <author> R. L. Rabiner and B. H. Juang. </author> <title> An introduction to hidden Markov models. </title> <journal> ASSP Magazine, </journal> <volume> 3(1) </volume> <pages> 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: r [q (t) = i j u 0;T ; y 0;T ; W; ; 2 ; (0)](6) y 0;T ; W; ; 2 ; (0)] The last two quantities are well-known in the HMM literature and can be computed efficiently by means of a procedure which parallels the forward-backward algorithm <ref> [11] </ref>.
Reference: [12] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: This can be attributed to the fact that, for any fixed model structure, maximizing likelihood is equivalent to minimizing the description length of the data <ref> [12] </ref>. On the other hand, the number of parameters in A increases proportionally to m 2 , increasing both the computational burden and the data complexity. The clustering becomes also harder for a large number of alternatives.
References-found: 12

