URL: http://theory.lcs.mit.edu/~luca/pubs/eatcs.ps.Z
Refering-URL: http://theory.lcs.mit.edu/~luca/papers.html
Root-URL: 
Email: allender@cs.rutgers.edu  
Title: The Computational Complexity Column  Towards Proving P BPP  
Author: Eric Allender Andrea E. F. Clementi Jose D. P. Rolim Luca Trevisan 
Note: Recent Advances  
Address: Piscataway, NJ 08855 USA  
Affiliation: Rutgers University, Department of Computer Science  
Abstract: Are there too many complexity classes? Merely trying to understand one aspect of computation, such as the power of randomness, leads to a whole range of complexity classes, such as ZPP, RP, and BPP, to name but a few. Do we really need all of these classes? One of the most exciting developments in complexity theory in the past few years is the growing body of evidence that all of the aforementioned classes are merely pseudonyms for P. Our guest column this issue gives an overview of this area. Abstract Two independent techniques have been developed recently that yield sufficient conditions for P = BPP in terms of worst-case circuit complexity of functions computable in exponential time. Andreev, Clementi and Rolim proved that P = BPP provided that a sparse "efficiently enumerable" language exists of sufficiently high circuit complexity. This result has been subsequently improved by Impagliazzo and Wigderson by showing that either P = BPP or all the decision problems solvable in time 2 O(n) are solvable by circuits of size 2 o(n) . In this column we discuss these results and their relation with previously known sufficient conditions for P = BPP.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andreev A., Clementi A., and Rolim J. </author> <year> (1996), </year> <title> "A New General De-Randomization Method", </title> <editor> J. </editor> <booktitle> of the ACM. to appear. Extended Abstract in 23-th Annual International Colloquium on Algorithms, Logic and Programming (ICALP'96), LNCS, </booktitle> <volume> 1099, </volume> <pages> pp. 357-368. </pages>
Reference-contexts: On the other hand, a bottleneck for the parallelization of the techniques of Andreev et al. [3] was the use of Theorem 3.1, whose proof in <ref> [1] </ref> was inherently sequential. A new proof of Theorem 3.1 appeared in [5] extends to parallel and space-bounded classes, and so do, to a certain extent, the techniques of [3].
Reference: [2] <author> Andreev A., Clementi A., and Rolim J. </author> <title> (1996) "Hitting Properties of Hard Boolean Operators and Their Consequences on BPP, </title> <note> ECCC Report TR96-055. </note>
Reference-contexts: Intuitively speaking, the worst-case hardness of a Boolean function seems to be much closer to its hitting properties than to its discrepancy properties. In <ref> [2] </ref> and successively in [3], Andreev et al indeed gave the first worst-case hardness condition which is sufficient to construct quick HSG's that satisfy Theorem 3.1 thus obtaining P = BPP. A more formal description of this result follows.
Reference: [3] <author> Andreev A., Clementi A., and Rolim J. </author> <year> (1997), </year> <title> "Worst-case Hardness Suffices for Derandom-ization: a New method for Hardness-Randomness Trade-Offs", </title> <booktitle> in 24-th Annual International Colloquium on Algorithms, Logic and Programming (ICALP'97), LNCS, </booktitle> <volume> 1256, </volume> <pages> pp. 177-187. </pages>
Reference-contexts: Intuitively speaking, the worst-case hardness of a Boolean function seems to be much closer to its hitting properties than to its discrepancy properties. In [2] and successively in <ref> [3] </ref>, Andreev et al indeed gave the first worst-case hardness condition which is sufficient to construct quick HSG's that satisfy Theorem 3.1 thus obtaining P = BPP. A more formal description of this result follows. The circuit complexity of a Boolean operator H will be denoted as L op (H). <p> The issue of parallelization is not addressed by Impagliazzo and Wigderson [8], and some steps in their construction appear to be hard to parallelize. On the other hand, a bottleneck for the parallelization of the techniques of Andreev et al. <ref> [3] </ref> was the use of Theorem 3.1, whose proof in [1] was inherently sequential. A new proof of Theorem 3.1 appeared in [5] extends to parallel and space-bounded classes, and so do, to a certain extent, the techniques of [3]. <p> bottleneck for the parallelization of the techniques of Andreev et al. <ref> [3] </ref> was the use of Theorem 3.1, whose proof in [1] was inherently sequential. A new proof of Theorem 3.1 appeared in [5] extends to parallel and space-bounded classes, and so do, to a certain extent, the techniques of [3]. The current state of the art on this topic is that a reasonable worst-case sufficient condition exists implying BPNC = NC but no worst-case circuit complexity condition is known to imply BPL = L (or even RL = L).
Reference: [4] <author> Andreev A., Baskakov J., Clementi A., and Rolim J. </author> <year> (1997), </year> <title> "Efficient Construction of *-Biased Sample Spaces for Systems of Linear Tests and Applications", </title> <note> Technical Report in ECCC TR-97-053. </note>
Reference-contexts: This is for instance the case for extractors and OR-dispersers ([13]). Another case in which one-sided randomness seems to be easier to achieve is in the case of "small" linear subspaces of f0; 1g <ref> [4] </ref>. It is indeed possible to construct small hitting sets for this class of subsets (and, so, for the corresponding characteristic functions) that imply some explicit, exponential lower bounds for the branching program model [4], but no construction of non-trivial discrepancy sets for this class is known. <p> to be easier to achieve is in the case of "small" linear subspaces of f0; 1g <ref> [4] </ref>. It is indeed possible to construct small hitting sets for this class of subsets (and, so, for the corresponding characteristic functions) that imply some explicit, exponential lower bounds for the branching program model [4], but no construction of non-trivial discrepancy sets for this class is known.
Reference: [5] <author> Andreev A., Clementi A., Rolim J. and Trevisan L. </author> <year> (1997), </year> <title> "Weak Random Sources, Hitting Sets, and BPP Simulation", </title> <journal> SIAM J. of Comput., </journal> <note> to appear. Extended abstract in 38-th Annual IEEE Symposium on Foundations on Computer Science, pp. 264-273. </note>
Reference-contexts: An alternative approach deals with the use of weak sources of randomness (see [13]). Even in this case there is a difference between one-sided pseudorandom structures and two-sided pseudorandom structures and Theorem 3.1 is a useful tool <ref> [5] </ref>. A natural question to ask is whether the results described in this column (or at least part of them) extend to parallel and space-bounded classes. The issue of parallelization is not addressed by Impagliazzo and Wigderson [8], and some steps in their construction appear to be hard to parallelize. <p> On the other hand, a bottleneck for the parallelization of the techniques of Andreev et al. [3] was the use of Theorem 3.1, whose proof in [1] was inherently sequential. A new proof of Theorem 3.1 appeared in <ref> [5] </ref> extends to parallel and space-bounded classes, and so do, to a certain extent, the techniques of [3].
Reference: [6] <author> Babai L., Fortnow L., Nisan N. and Wigderson A. </author> <title> (1993) "BPP has Subexponential Time Simulations unless EXPTIME has Publishable Proofs" Computational Complexity, </title> <booktitle> 3, </booktitle> <pages> pp. 307-318. </pages>
Reference-contexts: This possible shape of the "complexity world" would be quite different from what most complexity theorists expect. Nisan and Wigderson's work thus had a tremendous impact in the Complexity Community: people started to think that the gap between BPP and P might be very small. In <ref> [6] </ref>, another Hardness-vs Randomness trade off has been obtained that states that if there is a function in EXP having hardness 2 n (1) then BPP DTIME (n poly log n ). <p> Babai et al. <ref> [6] </ref> later proved that if a function f in EXP exists having circuit complexity 2 (n) then there exists another function g in EXP such that, for some fixed * &gt; 0, any circuit of size 2 *n can only achieve success probability 1 1=n 2 while trying to predict f <p> From this assumption it is only possible to infer from the techniques of <ref> [14, 6] </ref> that BPP can be deterministically simulated in time n poly log n . Impagliazzo and Wigderson [8] recently made further progress by showing how to de-randomize the Xor Lemma.
Reference: [7] <author> Blum M., and Micali S. </author> <year> (1984), </year> <title> "How to generate cryptographically strong sequences of pseudorandom bits", </title> <journal> SIAM J. of Computing, </journal> <volume> 13(4), </volume> <pages> pp. 850-864. </pages>
Reference-contexts: The first foundational results for this line of research can be found in the seminal works of Blum and Micali <ref> [7] </ref> and Yao [19], motivated by cryptographic applications.
Reference: [8] <author> Impagliazzo R., and Wigderson A. </author> <year> (1997), </year> <title> "P= BPP if E requires exponential circuits: </title> <booktitle> Deran-domizing the XOR lemma" In 29-th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 220-229. </pages>
Reference-contexts: From this assumption it is only possible to infer from the techniques of [14, 6] that BPP can be deterministically simulated in time n poly log n . Impagliazzo and Wigderson <ref> [8] </ref> recently made further progress by showing how to de-randomize the Xor Lemma. Their main result is a procedure that, given a function g with n inputs and a certain unpredictability, constructs another function h with O (n + k) inputs whose unpredictability decreases exponentially in k. <p> In turn, the latter statement implies the existence of PRG of logarithmic price and thus P = BPP. Theorem 3.3 <ref> [8] </ref> If a function in f exists having circuit complexity 2 (n) then P = BPP. 4 Related Results and Conclusion De-randomization is not the only research direction that has been explored about the use of randomness in computation. <p> A natural question to ask is whether the results described in this column (or at least part of them) extend to parallel and space-bounded classes. The issue of parallelization is not addressed by Impagliazzo and Wigderson <ref> [8] </ref>, and some steps in their construction appear to be hard to parallelize. On the other hand, a bottleneck for the parallelization of the techniques of Andreev et al. [3] was the use of Theorem 3.1, whose proof in [1] was inherently sequential.
Reference: [9] <author> Motwani R., and Raghavan P. </author> <year> (1995), </year> <title> Randomized Algorithms, </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: 1 Introduction Randomness is very useful in the design of efficient algorithms for several important problems. Probabilistic algorithms are often the simpler ones to solve a given problem, or the most efficient, or the only efficiently parallelizable ones (see <ref> [9] </ref>). For some problems, including primality testing and approximation of # P-complete counting problems, only randomized solutions are known. In computational geometry, problems such as the approximate computation of the volume of a convex body only admit randomized solutions.
Reference: [10] <author> Goldreich O. </author> <year> (1997), </year> <title> "A Sample of Samplers: A Computational Perspective on Sampling", </title> <publisher> ECCC, TR97-20. </publisher>
Reference-contexts: Furthermore, in several cases the construction of combinatorial objects having one-sided random (i.e. hitting) properties has turned out to be more efficient than that of combinatorial objects having two-sided random (i.e. discrepancy) properties (for a survey of these cases see Appendix C of <ref> [10] </ref>). This is for instance the case for extractors and OR-dispersers ([13]). Another case in which one-sided randomness seems to be easier to achieve is in the case of "small" linear subspaces of f0; 1g [4].
Reference: [11] <author> Impagliazzo, R. </author> <year> (1995), </year> <title> "Hard-core distributions for somewhat hard problems", </title> <booktitle> in Proceedings of the 36th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 538-545. </pages>
Reference-contexts: Thus, assuming that a function f in EXP exists with circuit complexity 2 (n) , one can deduce that a function g exists in EXP such that circuits of size 2 (n) only have success probability 1 1=n 2 on g. Then, using a result of Impagliazzo <ref> [11] </ref> it follows that a function g 0 in EXP exists such that circuits of size 2 (n) have success probability at most 2=3, and, eventually, that a function h exists in EXP such that circuits of size 2 *n only have success probability 2 *n .
Reference: [12] <author> Nisan N. </author> <year> (1990), </year> <title> Using Hard Problems to Create Pseudorandom Generators, </title> <publisher> ACM Distinguished Dissertation, MIT Press. </publisher>
Reference-contexts: Notice that this result is comparable to the one in <ref> [12, 14] </ref> stating that the existence of a quick PRG G : k (n) ! n implies BPTIME (t) DTIME (2 O (k (t 2 )) ).
Reference: [13] <author> Nisan N. </author> <year> (1996), </year> <title> "Extracting randomness: How and why", </title> <booktitle> In Proceedings of the 11th Annual IEEE Conference on Computational Complexity, </booktitle> <pages> pp. 44-58. </pages>
Reference-contexts: An alternative approach deals with the use of weak sources of randomness (see <ref> [13] </ref>). Even in this case there is a difference between one-sided pseudorandom structures and two-sided pseudorandom structures and Theorem 3.1 is a useful tool [5]. A natural question to ask is whether the results described in this column (or at least part of them) extend to parallel and space-bounded classes.
Reference: [14] <author> Nisan N., and Wigderson A. </author> <year> (1994), </year> <title> "Hardness vs Randomness", </title> <journal> J. Comput. System Sci. </journal> <volume> 49, </volume> <pages> pp. 149-167. </pages>
Reference-contexts: A Boolean operator is said to be quick if it is computable in time polynomial in the length of its output <ref> [14] </ref>. In particular, a PRG G is quick if G n : f0; 1g k (n) n is computable in time polynomial in n. <p> In particular, Nisan and Wigderson <ref> [14] </ref> presented a method to construct quick PRG's based on the existence of Boolean functions in EXP that have exponential hardness [14]. The hardness condition used by Nisan and Wigderson requires the existence of a function in EXP that has hard average-case circuit complexity. <p> In particular, Nisan and Wigderson <ref> [14] </ref> presented a method to construct quick PRG's based on the existence of Boolean functions in EXP that have exponential hardness [14]. The hardness condition used by Nisan and Wigderson requires the existence of a function in EXP that has hard average-case circuit complexity. <p> Notice that this result is comparable to the one in <ref> [12, 14] </ref> stating that the existence of a quick PRG G : k (n) ! n implies BPTIME (t) DTIME (2 O (k (t 2 )) ). <p> From this assumption it is only possible to infer from the techniques of <ref> [14, 6] </ref> that BPP can be deterministically simulated in time n poly log n . Impagliazzo and Wigderson [8] recently made further progress by showing how to de-randomize the Xor Lemma.
Reference: [15] <author> Papadimitriou C.H. </author> <year> (1993), </year> <title> Computational Complexity, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: For some problems, including primality testing and approximation of # P-complete counting problems, only randomized solutions are known. In computational geometry, problems such as the approximate computation of the volume of a convex body only admit randomized solutions. There are several classes of efficient probabilistic algorithms (see e.g. <ref> [15] </ref>) that differ on the adopted acceptance criteria. In particular, we will consider two classes of algorithms for decision problems: two-sided bounded error polynomial time algorithms (BPP algorithms) and one-sided bounded error polynomial time algorithms (RP algorithms).
Reference: [16] <author> Saks M., Srinivasan A., and Zhou S. </author> <year> (1995). </year> <title> "Explicit dispersers with polylog degree", </title> <booktitle> In Proceedings of the 27th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 479-488, </pages> <year> 1995. </year>
Reference-contexts: So, according to the definition of HSG's, any function that has a hard worst-case circuit complexity turns out to have also a good hitting property. This property is used to construct the preliminary version of the HSG which is then combined with a convenient use of OR Dispersers <ref> [16] </ref>, a family of particular expander graphs. 3.3 A Stronger Result Via the De-randomization of the XOR Lemma Theorem 2.1 requires the existence of a Boolean function f in EXP such that, for some * &gt; 0, any circuit C of size 2 *n can only achieve success probability 1=2 +
Reference: [17] <author> Wegener, I. </author> <year> (1987), </year> <title> The complexity of finite Boolean functions, </title> <booktitle> Wiley-Teubner Series in Computer Science. </booktitle>
Reference-contexts: A more formal description of this result follows. The circuit complexity of a Boolean operator H will be denoted as L op (H). Observe that if L op (k; n) denotes the worst-case circuit complexity of Boolean operators H : k (n) ! n, then it is known <ref> [17] </ref> that, for any log n k n, Furthermore, for almost every Boolean operator H : k ! n, we have L op (H) = fi ((2 k n)=(k + log n)) : The following theorem gives a sufficient condition for P = BPP in terms of the worst-case circuit complexity
Reference: [18] <author> Ta-Shma A. </author> <year> (1996), </year> <title> "On extracting randomness from weak random sources", </title> <booktitle> In 28th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 276-285. </pages>
Reference: [19] <author> Yao A. </author> <year> (1982), </year> <title> "Theory and applications of trapdoor functions", </title> <booktitle> in 23th Annual IEEE Symposium on Foundations on Computer Science, </booktitle> <pages> pp. 80-91. </pages>
Reference-contexts: The first foundational results for this line of research can be found in the seminal works of Blum and Micali [7] and Yao <ref> [19] </ref>, motivated by cryptographic applications. <p> The difficulty of predicting g on a random input can be increased by defining a function h (x 1 ; : : : ; x k ) = g (x 1 ) : : : g (x k ): from Yao's Xor Lemma <ref> [19] </ref> it follows that the success probability of a circuit for h of size 2 *n goes down to 1/2 exponentially fast in k.
References-found: 19

