URL: ftp://ftp.nj.nec.com/pub/horne/spectral.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00073.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: horne@research.nj.nec.com  
Title: Lower bounds for the spectral radius of a matrix  
Author: Bill G. Horne 
Date: December 18, 1995  95-14  
Address: 4 Independence Way Princeton, NJ 08540  
Affiliation: NEC Research Institute  
Pubnum: NECI Technical Report  
Abstract: In this paper we develop lower bounds for the spectral radius of symmetric, skew-symmetric, and arbitrary real matrices. Our approach utilizes the well-known Leverrier-Faddeev algorithm for calculating the coefficients of the characteristic polynomial of a matrix in conjunction with a theorem by Lucas which states that the critical points of a polynomial lie within the convex hull of its roots. Our results generalize and simplify a proof recently published by Tarazaga for a lower bound on the spectral radius of a symmetric positive definite matrix. In addition, we provide new lower bounds for the spectral radius of skew-symmetric matrices. We apply these results to a problem involving the stability of fixed points in recurrent neural networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F.R. Gantmacher. </author> <title> The Theory of Matrices. </title> <publisher> Chelsea, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: symmetric with zeros along the diagonal, or skew symmetric. 3 2 Preliminaries Consider an n fi n real matrix A having characteristic polynomial A () = n a 1 n1 a 2 n2 a n : (6) The coefficients of A () can be computed using the well-known Leverrier-Faddeev algorithm <ref> [1] </ref>, a j = j Y j = AY j1 a j A; j = 1; : : : ; n with Y 0 = A.
Reference: [2] <author> M.W. Hirsch. </author> <title> Saturation at high gain in discrete time recurrent networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(3) </volume> <pages> 449-453, </pages> <year> 1994. </year>
Reference-contexts: Of interest in these types of networks is the nature of fixed points of (13). In particular, we are concerned with the number of stable fixed points, and their location in the state space. Using the results in this paper we demonstrate a result similar to one in <ref> [2] </ref>, which shows that as the weight matrix becomes large, the stable fixed points of (13) cannot be located near the center of the state space. Our results apply to weight matrices that are either symmetric with zeros along the diagonal, or skew symmetric.
Reference: [3] <author> J.J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proceedings of the National Academy of Science USA, </booktitle> <volume> 79 </volume> <pages> 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: Then using the fact that tr = kAk F completes the proof. 6 Application to neural networks Discrete-time recurrent neural networks have been a topic of much interest in recent years (see [5] and references therein). The basic model, sometimes called a Hopfield network <ref> [3] </ref>, is defined to be x (t + 1) = (W x (t) + b) ; (13) where W is the weight matrix, b is a vector of biases and is a nonlinear vector activation function, where each component is typically a hyperbolic tangent.
Reference: [4] <author> R.A. Horn and C.R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1985. </year>
Reference-contexts: Moreover, it is doubtful that in practice one would have the values of a or b in (2). If one had an expression for the characteristic polynomial of A, then a bound on the spectral radius could be found using Cauchy's bound, Montel's bound, or Charmichael and Mason's bound <ref> [4] </ref>. However, such bounds are not likely to be very tight since they bound the magnitude of all eigenvalues. In fact, none of these bounds can ever be greater than one, and so they are not useful for determining the instability of (1).
Reference: [5] <author> B.G. Horne and C.L. Giles. </author> <title> An experimental comparison of recurrent neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 697-704. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Then using the fact that tr = kAk F completes the proof. 6 Application to neural networks Discrete-time recurrent neural networks have been a topic of much interest in recent years (see <ref> [5] </ref> and references therein).
Reference: [6] <author> M. Marcus and H. </author> <title> Minc. A Survey of Matrix Theory and Matrix Inequalities. </title> <publisher> Dover, </publisher> <address> New York, </address> <year> 1964. </year>
Reference-contexts: Most of the work involving bounds on the spectral radius are concerned with upper bounds, typically to determine necessary conditions for the stability of a system such as (1). However, there some lower bounds have been developed. According to Bendixson's Theorem <ref> [6] </ref>, (A) min i i (H) where i (H) is an eigenvalue of H = (A + A fl )=2, the Hermitian part of A. Another result can be found using Browne's Theorem [6], (A) min (A) where min (A) is the minimum singular value of A. <p> However, there some lower bounds have been developed. According to Bendixson's Theorem <ref> [6] </ref>, (A) min i i (H) where i (H) is an eigenvalue of H = (A + A fl )=2, the Hermitian part of A. Another result can be found using Browne's Theorem [6], (A) min (A) where min (A) is the minimum singular value of A. <p> For example, clearly it is trivial to determine the spectral radius of diagonal or triangular matrices. In addition, 2 much work as been done on obtaining lower bounds for the spectral radius of nonnegative matrices <ref> [6, 8] </ref>.
Reference: [7] <author> M. Marden. </author> <title> Geometry of Polynomials. </title> <publisher> American Mathematical Society, </publisher> <address> Providence, RI, </address> <year> 1949. </year>
Reference-contexts: We will then use the following well-known result of Lucas (see <ref> [7] </ref>), Lemma 1 The critical points 1 of a non-constant polynomial p () lie in the convex hull of the roots of p (). The lemma implies Corollary 1 Let p () be an nth order polynomial, and 1 m n.
Reference: [8] <author> J.K. Merikoski. </author> <title> On l p 1 ;p 2 antinorms of nonnegative matrices. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 140 </volume> <pages> 31-44, </pages> <year> 1990. </year>
Reference-contexts: For example, clearly it is trivial to determine the spectral radius of diagonal or triangular matrices. In addition, 2 much work as been done on obtaining lower bounds for the spectral radius of nonnegative matrices <ref> [6, 8] </ref>.
Reference: [9] <author> A. Rachid. </author> <title> Some new bounds for the spectral radius. </title> <journal> IEEE Transaction on Automatic Control, </journal> <volume> 39(1) </volume> <pages> 196-198, </pages> <year> 1994. </year>
Reference-contexts: Another result can be found using Browne's Theorem [6], (A) min (A) where min (A) is the minimum singular value of A. In <ref> [9] </ref>, Rachid developed the lower bounds ((A)) 2 2 max (S) + 2b 2 min (S) 2 where H and S are the Hermitian and skew-Hermitian parts of A, max (X) represents the maximum singular value of X, and ff = a+jb is an eigenvalue of A such that (A) =
Reference: [10] <author> P. Tarazaga. </author> <title> More estimates for eigenvalues and singular values. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 149 </volume> <pages> 97-110, </pages> <year> 1991. </year> <month> 12 </month>
Reference-contexts: For example, clearly it is trivial to determine the spectral radius of diagonal or triangular matrices. In addition, 2 much work as been done on obtaining lower bounds for the spectral radius of nonnegative matrices [6, 8]. One bound relevant to our results is due to Tarazaga <ref> [10] </ref>, who showed that the spectral radius of a symmetric, positive semidefinite matrix of rank m is (A) m s m (m 1) kAk F m where kAk F , q P i;j is the Frobenius norm of A.
References-found: 10

