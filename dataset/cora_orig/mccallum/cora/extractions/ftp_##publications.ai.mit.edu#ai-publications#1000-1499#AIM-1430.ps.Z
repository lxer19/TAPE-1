URL: ftp://publications.ai.mit.edu/ai-publications/1000-1499/AIM-1430.ps.Z
Refering-URL: http://www.ai.mit.edu/people/girosi/home-page/memos.html
Root-URL: 
Title: Priors, Stabilizers and Basis Functions: from regularization to radial, tensor and additive splines  
Author: Federico Girosi, Michael Jones and Tomaso Poggio 
Date: June 1993  75  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY  
Pubnum: A.I. Memo No.1430  C.B.C.L. Paper No.  
Abstract: We had previously shown that regularization principles lead to approximation schemes which are equivalent to networks with one layer of hidden units, called Regularization Networks. In particular we had discussed how standard smoothness functionals lead to a subclass of regularization networks, the well-known Radial Basis Functions approximation schemes. In this paper we show that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular we introduce new classes of smoothness functionals that lead to different classes of basis functions. Additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore, the same extension that leads from Radial Basis Functions (RBF) to Hyper Basis Functions (HBF) also leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions and some forms of Projection Pursuit Regression. We propose to use the term Generalized Regularization Networks for this broad class of approximation schemes that follow from an extension of regularization. In the probabilistic interpretation of regularization, the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces, and therefore to different types of smoothness assumptions. In the final part of the paper, we show the relation between activation functions of the Gaussian and sigmoidal type by considering the simple case of the kernel G(x) = jxj. In summary, different multilayer networks with one hidden layer, which we collectively call Generalized Regularization Networks, correspond to different classes of priors and associated smoothness functionals in a classical regularization principle. Three broad classes are a) Radial Basis Functions that generalize into Hyper Basis Functions, b) some tensor product splines, and c) additive splines that generalize into schemes of the type of ridge approximation, hinge functions and one-hidden-layer perceptrons. This paper describes research done within the Center for Biological and Computational Learning in the Department of Brain and Cognitive Sciences and at the Artificial Intelligence Laboratory. This research is sponsored by grants from the Office of Naval Research under contracts N00014-91-J-1270 and N00014-92-J-1879; by a grant from the National Science Foundation under contract ASC-9217041 (which includes funds from DARPA provided under the HPCC program); and by a grant from the National Institutes of Health under contract NIH 2-S07-RR07047. Additional support is provided by the North Atlantic Treaty Organization, ATR Audio and Visual Perception Research Laboratories, Mitsubishi Electric Corporation, Sumitomo Metal Industries, and Siemens AG. Support for the A.I. Laboratory's artificial intelligence research is provided by ONR contract N00014-91-J-4038. Tomaso Poggio is supported by the Uncas and Helen Whitaker Chair at the Whitaker College, Massachusetts Institute of Technology. c fl Massachusetts Institute of Technology, 1993
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. R. Barron and Barron R. L. </author> <title> Statistical learning networks: a unifying view. </title> <booktitle> In Symposium on the Interface: Statistics and Computing Science, </booktitle> <address> Reston, Virginia, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: To do this we used the model f (x) = P n ff c ff G (x t ff ) to approximate the function h (x) = sin (2x) on <ref> [0; 1] </ref>, where G (x) is one of the basis functions of figure 5. The function sin (2x) is plotted in figure 6. Fifty training points and 10,000 test points were chosen uniformly on [0; 1]. <p> ff G (x t ff ) to approximate the function h (x) = sin (2x) on <ref> [0; 1] </ref>, where G (x) is one of the basis functions of figure 5. The function sin (2x) is plotted in figure 6. Fifty training points and 10,000 test points were chosen uniformly on [0; 1]. The parameters were learned using the iterative backfitting algorithm that will be described in section 7. We looked at the function learned after fitting 1, 2, 4, 8 and 16 basis functions. <p> The training data for the additive function consisted of 20 points picked from a uniform distribution on <ref> [0; 1] </ref> fi [0; 1]. Another 10,000 points were randomly chosen to serve as test data. The training data for the Gabor function consisted of 20 points picked from a uniform distribution on [1; 1] fi [1; 1] with an additional 10,000 points used as test data. <p> The training data for the additive function consisted of 20 points picked from a uniform distribution on <ref> [0; 1] </ref> fi [0; 1]. Another 10,000 points were randomly chosen to serve as test data. The training data for the Gabor function consisted of 20 points picked from a uniform distribution on [1; 1] fi [1; 1] with an additional 10,000 points used as test data. <p> Another 10,000 points were randomly chosen to serve as test data. The training data for the Gabor function consisted of 20 points picked from a uniform distribution on <ref> [1; 1] </ref> fi [1; 1] with an additional 10,000 points used as test data. <p> Another 10,000 points were randomly chosen to serve as test data. The training data for the Gabor function consisted of 20 points picked from a uniform distribution on <ref> [1; 1] </ref> fi [1; 1] with an additional 10,000 points used as test data.
Reference: [2] <author> M. Bertero, T. Poggio, and V. Torre. </author> <title> Ill-posed problems in early vision. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 76 </volume> <pages> 869-889, </pages> <year> 1988. </year>
Reference: [3] <author> L. Breiman. </author> <title> Hinging hyperplanes for regression, classification, and function approximation. </title> <note> 1992. (submitted for publication). </note>
Reference: [4] <author> D.S. Broomhead and D. Lowe. </author> <title> Multivariable functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 321-355, </pages> <year> 1988. </year>
Reference: [5] <author> A. Buja, T. Hastie, and R. Tibshirani. </author> <title> Linear smoothers and additive models. </title> <journal> The Annals of Statistics, </journal> <volume> 17 </volume> <pages> 453-555, </pages> <year> 1989. </year>
Reference: [6] <author> B. Caprile and F. Girosi. </author> <title> A nondeterministic minimization algorithm. A.I. </title> <type> Memo 1254, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1990. </year>
Reference: [7] <author> D.L. Donoho and I.M. Johnstone. </author> <title> Projection-based approximation and a duality with kernel methods. </title> <journal> The Annals of Statistics, </journal> <volume> 17(1) </volume> <pages> 58-106, </pages> <year> 1989. </year>
Reference: [8] <author> J. Duchon. </author> <title> Spline minimizing rotation-invariant semi-norms in Sobolev spaces. </title> <editor> In W. Schempp and K. Zeller, editors, </editor> <title> Constructive theory of functions os several variables, </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> 571. </volume> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1977. </year>
Reference: [9] <author> N. Dyn. </author> <title> Interpolation of scattered data by radial functions. </title> <editor> In C.K. Chui, L.L. Schumaker, and F.I. Utreras, editors, </editor> <title> Topics in multivariate approximation. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference: [10] <author> N. Dyn. </author> <title> Interpolation and approximation by radial and related functions. </title> <editor> In C.K. Chui, L.L. Schu-maker, and D.J. Ward, editors, </editor> <booktitle> Approximation Theory, VI, </booktitle> <pages> pages 211-234. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [11] <author> N. Dyn, D. Levin, and S. Rippa. </author> <title> Numerical procedures for surface fitting of scattered data by radial functions. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7(2) </volume> <pages> 639-659, </pages> <month> April </month> <year> 1986. </year>
Reference: [12] <author> J.H. Friedman and W. Stuetzle. </author> <title> Projection pursuit regression. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 76(376) </volume> <pages> 817-823, </pages> <year> 1981. </year>
Reference: [13] <author> R.L. </author> <title> Harder and R.M. Desmarais. Interpolation using surface splines. </title> <journal> J. Aircraft, </journal> <volume> 9 </volume> <pages> 189-191, </pages> <year> 1972. </year>
Reference: [14] <author> T. Hastie and R. Tibshirani. </author> <title> Generalized additive models. </title> <journal> Statistical Science, </journal> <volume> 1 </volume> <pages> 297-318, </pages> <year> 1986. </year>
Reference: [15] <author> T. Hastie and R. Tibshirani. </author> <title> Generalized additive models: some applications. </title> <journal> J. Amer. Statistical As-soc., </journal> <volume> 82 </volume> <pages> 371-386, </pages> <year> 1987. </year>
Reference: [16] <author> T. Hastie and R. Tibshirani. </author> <title> Generalized Additive Models, </title> <booktitle> volume 43 of Monographs on Statistics and Applied Probability. </booktitle> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1990. </year>
Reference: [17] <author> P.J. Huber. </author> <title> Projection pursuit. </title> <journal> The Annals of Statistics, </journal> <volume> 13(2) </volume> <pages> 435-475, </pages> <year> 1985. </year>
Reference: [18] <author> W.R. Madych and S.A. Nelson. </author> <title> Multivari-ate interpolation and conditionally positive definite functions. II. </title> <journal> Mathematics of Computation, </journal> <volume> 54(189) </volume> <pages> 211-230, </pages> <month> January </month> <year> 1990. </year>
Reference: [19] <author> J. L. Marroquin, S. Mitter, and T. Poggio. </author> <title> Probabilistic solution of ill-posed problems in computational vision. </title> <journal> J. Amer. Stat. Assoc., </journal> <volume> 82 </volume> <pages> 76-89, </pages> <year> 1987. </year>
Reference: [20] <author> M. Maruyama, F. Girosi, and T. Poggio. </author> <title> A connection between HBF and MLP. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1291, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference: [21] <author> C. A. Micchelli. </author> <title> Interpolation of scattered data: distance matrices and conditionally positive definite functions. Constructive Approximation, </title> <booktitle> 2 </booktitle> <pages> 11-22, </pages> <year> 1986. </year>
Reference: [22] <author> J. Moody and C. Darken. </author> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference: [23] <author> T. Poggio and F. Girosi. </author> <title> A theory of networks for approximation and learning. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1140, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1989. </year>
Reference: [24] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(9), </volume> <month> September </month> <year> 1990. </year>
Reference: [25] <author> T. Poggio and F. Girosi. </author> <title> Extension of a theory of networks for approximation and learning: dimensionality reduction and clustering. </title> <booktitle> In Proceedings Image Understanding Workshop, </booktitle> <pages> pages 597-603, </pages> <address> Pittsburgh, Pennsylvania, September 11-13 1990a. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [26] <author> T. Poggio and F. Girosi. </author> <title> Regularization algorithms for learning that are equivalent to multilayer networks. </title> <journal> Science, </journal> <volume> 247 </volume> <pages> 978-982, </pages> <year> 1990b. </year>
Reference: [27] <author> M. J. D. Powell. </author> <title> Radial basis functions for multi-variable interpolation: a review. </title> <editor> In J. C. Mason and M. G. Cox, editors, </editor> <title> Algorithms for Approximation. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1987. </year>
Reference: [28] <author> M.J.D. Powell. </author> <title> The theory of radial basis functions approximation in 1990. </title> <type> Technical Report NA11, </type> <institution> Department of Applied Mathematics and Theoretical Physics, </institution> <address> Cambridge, England, </address> <month> December </month> <year> 1990. </year>
Reference: [29] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference: [30] <author> N. Sivakumar and J.D. Ward. </author> <title> On the best least square fit by radial functions to multidimensional scattered data. </title> <type> Technical Report 251, </type> <institution> Center for Approximation Theory, Texas A & M University, </institution> <month> June </month> <year> 1991. </year>
Reference: [31] <author> R.J. Solomonoff. </author> <title> Complexity-based induction systems: comparison and convergence theorems. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 24, </volume> <year> 1978. </year>
Reference: [32] <author> A. N. </author> <title> Tikhonov. Solution of incorrectly formulated problems and the regularization method. </title> <journal> Soviet Math. Dokl., </journal> <volume> 4 </volume> <pages> 1035-1038, </pages> <year> 1963. </year>
Reference: [33] <author> A. N. Tikhonov and V. Y. Arsenin. </author> <title> Solutions of Ill-posed Problems. </title> <editor> W. H. Winston, </editor> <address> Washington, D.C., </address> <year> 1977. </year>

References-found: 33

