URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/cai-efficient-focus.ps.gz
Refering-URL: http://ai.iit.nrc.ca/bibliographies/feature-selection.html
Root-URL: 
Email: almualh@cs.orst.edu tgd@cs.orst.edu  
Title: Efficient Algorithms for Identifying Relevant Features  
Author: Hussein Almuallim and Thomas G. Dietterich Dearborn Hall U. S. A. 
Keyword: FEATURES bias.  
Address: Corvallis, OR 97331-3202  
Affiliation: Department of Computer Science Oregon State University  
Abstract: This paper describes efficient methods for exact and approximate implementation of the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This bias is useful for learning domains where many irrelevant features are present in the training data. We first introduce FOCUS-2, a new algorithm that exactly implements the MIN-FEATURES bias. This algorithm is empirically shown to be substantially faster than the FOCUS algorithm previously given in [ Al-muallim and Dietterich, 1991 ] . We then introduce the Mutual-Information-Greedy, Simple-Greedy and Weighted-Greedy algorithms, which apply efficient heuristics for approximating the MIN-FEATURES bias. These algorithms employ greedy heuristics that trade optimality for computational efficiency. Experimental studies show that the learning performance of ID3 is greatly improved when these algorithms are used to preprocess the training data by eliminating the irrelevant features from ID3's consideration. In particular, the Weighted-Greedy algorithm provides an excellent and efficient approximation of the MIN 
Abstract-found: 1
Intro-found: 1
Reference: [ Almuallim, 1992 ] <author> Almuallim, H. </author> <title> Concept Coverage and Its Application to Two Learning Tasks. </title> <type> Ph.D. Thesis. </type> <institution> Department of Computer Science, Oregon State University, Corvallis, Oregon. Forthcoming. </institution>
Reference-contexts: The only difference between the three algorithms is the criterion used in selecting the best feature in each iteration. In Figure 3, we give a sketch of each of our algorithms. More detailed and computationally efficient implementations can be found in <ref> [ Almuallim, 1992 ] </ref> . In the following, we describe the selection criteria implemented by each algorithm. <p> FRINGE is terminated after at most 10 iterations. The algorithms are evaluated through a set of experiments similar to those reported in [ Almuallim and Diet-terich, 1991 ] . Details of the experiments can be found in <ref> [ Almuallim, 1992 ] </ref> . We report here two kinds of experiments. In the first experiment, we are interested in the worst-case performance over a class of concepts, where each concept is definable over at most p out of n features (and hence, the MIN-FEATURES bias is appropriate).
Reference: [ Almuallim and Dietterich, 1991 ] <author> Almuallim, H. and Di-etterich, T. G. </author> <title> Learning With Many Irrelevant Features. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> 547-552, </pages> <year> 1991. </year>
Reference-contexts: In these domains, an appropriate bias is the MIN-FEATURES bias, which prefers any consistent hypothesis definable over as few features as possible. Previous work <ref> [ Almuallim and Dietterich, 1991 ] </ref> showed that this simple bias is strong enough to yield polynomial sample complexity. The same study showed that|contrary to expectations| the performance of conventional inductive learning algo fl The authors gratefully acknowledge the support of the NSF under grant number IRI-86-57316. <p> Section 3 introduced the FOCUS-2 algorithm, which provides an implementation of this bias that is substantially faster than the FOCUS algorithm previously given in <ref> [ Almuallim and Dietterich, 1991 ] </ref> . Section 4 introduced three efficient heuristics for approximating the MIN-FEATURES bias. Experimental studies were reported in which each of these algorithms was used to preprocess the training data to remove the irrelevant features.
Reference: [ Almuallim, 1991 ] <author> Almuallim, H. </author> <title> Exploiting Symmetry Properties in the Evaluation of Inductive Learning Algorithms: An Empirical Domain-Independent Comparative Study. </title> <type> Technical Report, </type> <institution> 91-30-09, Dept. of Computer Science, Oregon State University, Corvallis, </institution> <address> OR 97331-3202, </address> <year> 1991. </year>
Reference-contexts: To reduce the computational costs involved in this experiment, we exploited the fact that the algorithms are symmetric with respect to the permutation and negation of any subset of the features of the target concept <ref> [ Almuallim, 1991 ] </ref> . The results of this experiment for n = 8; 10 and 12 are shown in Figure 4. EXPERIMENT 2: Learning Curve. The purpose of this experiment is to perform a kind of "average-case" comparison between the algorithms. The experiment is conducted as follows.
Reference: [ Blumer et al., 1987 ] <author> Blumer, A.; Ehrenfeucht, A.; Haussler, D.; and Warmuth, M. </author> <title> Learnability and the Vapnik-Chervonenkis Dimension, </title> <type> Technical Report UCSC-CRL-87-20, </type> <institution> Department of Computer and Information Sciences, University of California, Santa Cruz, </institution> <month> Nov. </month> <year> 1987. </year> <note> Also in Journal of ACM, </note> <month> 36(4) </month> <pages> 929-965, </pages> <year> 1990. </year>
Reference-contexts: As our measure of performance, we employ the sample complexity|the minimum number of training examples needed to ensure that every concept in the class can be learned in the PAC sense <ref> [ Blumer et al., 1987 ] </ref> . We estimate the sample complexity with respect to fixed learning parameters p; n; *; and ffi and with training samples drawn according to the uniform distribution. In the second experiment, we are interested in the average-case performance of the algorithms. <p> We conclude that c is learned by L if and only if for at least 90% of these samples L returns a hypothesis that is at least 90% correct. Thus, the quantity measured here can be viewed as an empirical estimate of the sample complexity of each algorithm <ref> [ Blumer et al., 1987 ] </ref> for * = ffi = 0:1.
Reference: [ Ichino and Sklansky, 1984a ] <author> Ichino, M. and Sklansky, J. </author> <title> Optimum Feature Selection by Zero-One Integer Programming. </title> <journal> In IEEE Trans. Sys. Man & Cyb., </journal> <volume> Vol. SMC-14, No. 5, </volume> <pages> 737-746, </pages> <month> Sep </month> <year> 1984. </year>
Reference-contexts: For example, [ Kittler, 1980 ] show methods for selecting a small subset of features that optimizes the expected error of the nearest neighbor classifier. Similar work has addressed feature selection for the Box classifier <ref> [ Ichino and Sklansky, 1984a ] </ref> , the linear classifier [ Ichino and Sklansky, 1984b ] and the Bayes classifier [ Queiros and Gelsma, 1984 ] .

Reference: [ Garey and Johnson, 1979 ] <author> Garey, M. R. and Johnson D. S. </author> <title> Computers and Intractability. W.H. </title> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: If A x i is the set of conflicts explained by a feature x i , then the score of x i is computed as score x i = X 1 # of features explaining a 1 Cover problem, which is known to be NP-hard <ref> [ Garey and Johnson, 1979 ] </ref> . However, note that we assume here the existence of a small set of features that forms a solution. This corresponds to restricting the Minimum-Set-Cover problem to instances that have small covers. Algorithm: Mutual-Information-Greedy (Sample) 1. Q = . 2.
Reference: [ Kittler, 1980 ] <author> Kittler, J. </author> <title> Computational Problems of Feature Selection Pertaining to Large Data Sets. In Pattern Recognition in Practice. Gelsma, E.S. </title> <editor> and Kanal, L.N. (eds.) </editor> <publisher> North-Holland Publishing Company, </publisher> <pages> 405-414, </pages> <year> 1980. </year>
Reference-contexts: The goal of feature selection is to eliminate as many features as possible without significantly degrading performance. Most feature selection criteria in pattern recognition are defined with respect to a specific classifier or group of classifiers. For example, <ref> [ Kittler, 1980 ] </ref> show methods for selecting a small subset of features that optimizes the expected error of the nearest neighbor classifier.
Reference: [ Morgera, 1986 ] <author> Morgera, </author> <title> S.D. Computational Complexity and VLSI Implementation of an Optimal Feature Selection Strategy In Pattern Recognition In Practice II, Gelsma, E.S. </title> <editor> and Kanal, L.N. (eds.), </editor> <booktitle> 1986. </booktitle> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <pages> 389-400, </pages> <year> 1986. </year>
Reference-contexts: Other work (aimed at removing feature redundancy when features are highly correlated) is based on performing a principal components analysis to find a reduced set of new uncorrelated features defined by combining the original features using the eigenvectors <ref> [ Morgera, 1986; Mucciardi and Gose, 1971 ] </ref> .
Reference: [ Mucciardi and Gose, 1971 ] <author> Mucciardi, </author> <title> A.N. and Gose, E.E. A Comparison of Seven Techniques for Choosing Subsets of Pattern Recognition Properties, </title> <journal> IEEE Trans. Computers, </journal> <volume> Vol. C-20, No. 9, </volume> <pages> 1023-1031, </pages> <month> Sep </month> <year> 1971. </year>
Reference-contexts: Other work (aimed at removing feature redundancy when features are highly correlated) is based on performing a principal components analysis to find a reduced set of new uncorrelated features defined by combining the original features using the eigenvectors <ref> [ Morgera, 1986; Mucciardi and Gose, 1971 ] </ref> .
Reference: [ Narendra and Fukunaga, 1977 ] <author> Narendra, P.M. and Fukunaga, K. </author> <title> A Branch and Bound Algorithm for Feature Subset Selection, </title> <journal> IEEE Trans. Computers, </journal> <volume> Vol. C-26, No. 9, </volume> <pages> 917-922, </pages> <year> 1977. </year>
Reference-contexts: However, for ease of hardware implementation and speed of processing, it is necessary to reduce the number of features considered by the classifier. Generally, the classifiers studied in pattern recognition have the so-called monotonicity property ( <ref> [ Narendra and Fukunaga, 1977 ] </ref> ) that as the number of features is reduced, the accuracy decreases. The goal of feature selection is to eliminate as many features as possible without significantly degrading performance.
Reference: [ Pagallo and Haussler, 1990 ] <author> Pagallo, G.; and Haussler, D. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-100, </pages> <year> 1990. </year>
Reference-contexts: We will refer to the three algorithms as MIG+ID3, SG+ID3 and WG+ID3. Likewise, using FOCUS instead of these heuristics to find a sufficient subset of features will be denoted FOCUS+ID3. For comparison, our experiments also include ID3 with no preprocessing in addition to FRINGE <ref> [ Pagallo and Haussler, 1990 ] </ref> . Our version of ID3 performs no win-dowing or forward pruning and employs the information gain (mutual information) criterion to select features. FRINGE is terminated after at most 10 iterations.
Reference: [ Queiros and Gelsma, 1984 ] <author> Queiros, C.E. and Gelsma, </author> <title> E.S. On Feature Selection, </title> <booktitle> In The Seventh International Conference on Pattern Recognition, </booktitle> <pages> 128-130, </pages> <year> 1984. </year>
Reference-contexts: Similar work has addressed feature selection for the Box classifier [ Ichino and Sklansky, 1984a ] , the linear classifier [ Ichino and Sklansky, 1984b ] and the Bayes classifier <ref> [ Queiros and Gelsma, 1984 ] </ref> .
Reference: [ Quinlan, 1986 ] <author> Quinlan, J. R. </author> <title> Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: The same study showed that|contrary to expectations| the performance of conventional inductive learning algo fl The authors gratefully acknowledge the support of the NSF under grant number IRI-86-57316. Hussein Almual-lim was supported by a scholarship from the University of Petroleum and Minerals, Dhahran, Saudi Arabia. rithms such as ID3 <ref> [ Quinlan, 1986 ] </ref> and FRINGE [ Pa-gallo and Haussler, 1990 ] is seriously reduced by the presence of irrelevant features. These results suggested that one should not rely on these algorithms to filter out irrelevant features. <p> We then introduce the Mutual-Information-Greedy, Simple-Greedy and Weighted-Greedy algorithms, which apply efficient heuristics for approximating the MIN-FEATURES bias. Unlike FOCUS-2, these algorithms employ greedy heuristics that trade optimality for computational efficiency. Experimental studies show that the learning performance of ID3 <ref> [ Quinlan, 1986 ] </ref> is greatly improved when these algorithms are used to preprocess the training data by eliminating the irrelevant features from ID3's consideration. In particular, the Weighted-Greedy algorithm provides an excellent and efficient approximation of the MIN-FEATURES bias. <p> Given a sufficient subset of features, it is easy to construct a consistent hypothesis. For example, the algorithm ID3 <ref> [ Quinlan, 1986 ] </ref> can be applied to the training sample but restricted to consider only the features in the given subset.
References-found: 13

