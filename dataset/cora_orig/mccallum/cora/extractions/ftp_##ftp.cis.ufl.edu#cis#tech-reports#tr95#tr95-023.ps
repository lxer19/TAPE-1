URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr95/tr95-023.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr95-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: A PARALLEL IMPLEMENTATION OF THE BLOCK-PARTITIONED INVERSE MULTIFRONTAL ZSPARSE ALGORITHM  
Author: YOGIN E. CAMPBELL AND TIMOTHY A. DAVIS 
Abstract: Technical Report TR-95-023, Computer and Information Sciences Department, University of Florida, Gainesville, FL, 32611 USA. October, 1995. Key words. Symmetric multifrontal method, supernode, Takahashi equations, symmetric inverse multifrontal method, inverse frontal matrix, inverse contribution matrix, inverse assembly tree, Zsparse. AMS (MOS) subject classifications. 05C50, 65F50, 65F05. Abbreviated title. A parallel inverse multifrontal Zsparse algorithm Abstract. The sparse inverse subset problem is the computation of the entries of the inverse of a sparse matrix for which the corresponding entry is nonzero in the factors of the matrix. We present a parallel, block-partitioned formulation of the inverse multifrontal algorithm to compute the sparse inverse subset. Numerical results for an implementation of this algorithm on an 8-processor, shared-memory Cray-C98 architecture are discussed. We show that for large problems we obtain efficiency ratings of over 80% and performance in excess of 1 Gflop. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. R. Amestoy and I. S. Duff. </author> <title> Memory management issues in sparse multifrontal methods on multiprocessors. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 7(1) </volume> <pages> 64-82, </pages> <month> Spring </month> <year> 1993. </year>
Reference-contexts: Memory partitioning and management. The available memory is partitioned into two main regions, storage and active. This is essentially the same partitioning scheme used by Amestoy and Duff in <ref> [1] </ref>. Figure 3.6 shows the general layout of the two areas. The main difficulty is in deciding on the proportion of memory to allocate to the two areas. In a preprocessing step we compute the amount of memory required for the sequential algorithm and use this amount plus 5-10%. <p> For the state of a tree node to change from waiting to ready enough memory needs to be available to accommodate the staircase upper triangular data structure of the node. As in <ref> [1] </ref>, the active memory is subdivided into a buddy area and a fixed-block area. The motivation behind this subdivision is to more efficiently use the available memory and, at the same time, lessen memory bottlenecks that would hamper concurrency.
Reference: [2] <author> Y. E. Campbell. </author> <title> Multifrontal algorithms for sparse inverse subsets and incomplete LU factorization. </title> <type> PhD thesis, </type> <institution> Computer and Information Science and Engineering Department, Univ. of Florida, </institution> <address> Gainesville, FL, </address> <month> November </month> <year> 1995. </year> <note> Also CISE Technical Report TR-95-025. </note>
Reference-contexts: This paper discusses a parallel implementation of the block-partitioned form of the Zsparse algorithm. The target implementation platform is the Cray-C98, a parallel-vector shared-memory machine. Extensions of the method (for arbitrary subsets) are discussed in [4]. See also <ref> [2] </ref>. We explicitly exploit two levels of parallelism in this implementation: the tree level, and the node level. The tree level parallelism is the coarse grain type of parallelism inherent among the nodes in the inverse assembly tree.
Reference: [3] <author> Y. E. Campbell and T. A. Davis. </author> <title> Computing the sparse inverse subset: an inverse multifrontal approach. </title> <type> Technical Report TR-95-021, </type> <institution> University of Florida, </institution> <address> Gainesville, FL, </address> <year> 1995. </year>
Reference-contexts: circuit currents in power systems or in estimating the variances of the fitted parameters in the least-squared data-fitting problem. (The sparse inverse subset (Zsparse), is defined as the set of inverse entries in locations corresponding to the positions of nonzero entries in the LDU factorized form of the matrix.) In <ref> [3] </ref> Campbell and Davis introduced a new algorithm to compute Zsparse for symmetric matrices based on one of the equations presented by Takahashi, Fagan, and Chin in [11], and an inverted form of the symmetric multifrontal method. <p> Technical reports and matrices are available via the World Wide Web at http://www.cis.ufl.edu/~davis, or by anonymous ftp at ftp.cis.ufl.edu:cis/tech-reports. 1 2 Fig. 2.1. Entries computed when a supernode is inverted discussed in [10]. Section 2 presents a brief summary of the sequential inverse multifrontal algorithm presented in <ref> [3] </ref>. Various aspects of the block-partitioned algorithm including task partitioning and task dependencies, scheduling, and memory management are discussed in Section 3. Section 4 contains the results and discussion of the numerical experiments. Finally, some concluding remarks are given in Section 5. 2. The inverse multifrontal algorithm. In [3] the sequential <p> presented in <ref> [3] </ref>. Various aspects of the block-partitioned algorithm including task partitioning and task dependencies, scheduling, and memory management are discussed in Section 3. Section 4 contains the results and discussion of the numerical experiments. Finally, some concluding remarks are given in Section 5. 2. The inverse multifrontal algorithm. In [3] the sequential computation of Zsparse for numerically symmetric matrices was formulated using an inverse multifrontal approach. We briefly review the basic features of this approach in preparation for the development of the parallel Zsparse algorithm. <p> We use the theoretical value in reporting megaflop rates in Section 4. The total number of required operations is identical to the number of operations required to factorize the matrix, as discussed in <ref> [3] </ref>. The theoretical operation count includes the unnecessary operations done in partially computing entries in the lower triangular part of the diagonal blocks.
Reference: [4] <author> Y. E. Campbell and T. A. Davis. </author> <title> On computing an arbitrary subset of entries of the inverse of a matrix. </title> <type> Technical Report TR-95-022, </type> <institution> Computer and Information Science and Engineering Department, Univ. of Florida, </institution> <year> 1995. </year>
Reference-contexts: This paper discusses a parallel implementation of the block-partitioned form of the Zsparse algorithm. The target implementation platform is the Cray-C98, a parallel-vector shared-memory machine. Extensions of the method (for arbitrary subsets) are discussed in <ref> [4] </ref>. See also [2]. We explicitly exploit two levels of parallelism in this implementation: the tree level, and the node level. The tree level parallelism is the coarse grain type of parallelism inherent among the nodes in the inverse assembly tree.
Reference: [5] <author> T. A. Davis and I. S. Duff. </author> <title> A combined unifrontal/multifrontal method for unsymmetric sparse matrices. </title> <type> Technical Report TR-95-020, </type> <institution> Computer and Information Science and Engineering Department, Univ. of Florida, </institution> <year> 1995. </year> <month> 18 </month>
Reference-contexts: We present the results for runs done on of three matrices shown in Table 4 [7]: Plat1919, Bcsstk25 and Finan512. The factors and assembly tree information were obtained using a modified version of a modified form of UMFPACK with strict diagonal pivoting <ref> [6, 5] </ref>, although in principal any supernodal or multifrontal factorization algorithm for symmetric matrices could generate the supernodal factors. Tables 4.2 through 4.10 give the results for these runs.
Reference: [6] <author> T. A. Davis and I. S. Duff. </author> <title> An unsymmetric-pattern multifrontal method for sparse LU factorization. SIAM J. Matrix Analysis and Application, </title> <note> (to appear). Also CISE Technical Report TR-94-038. </note>
Reference-contexts: We present the results for runs done on of three matrices shown in Table 4 [7]: Plat1919, Bcsstk25 and Finan512. The factors and assembly tree information were obtained using a modified version of a modified form of UMFPACK with strict diagonal pivoting <ref> [6, 5] </ref>, although in principal any supernodal or multifrontal factorization algorithm for symmetric matrices could generate the supernodal factors. Tables 4.2 through 4.10 give the results for these runs.
Reference: [7] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Users' guide for the Harwell-Boeing sparse matrix collection (release 1). </title> <type> Technical Report RAL-92-086, </type> <institution> Rutherford Appleton Laboratory, </institution> <address> Didcot, Oxon, England, </address> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Fortunately, the EVTEST library call is relatively cheap, about 200 clock cycles. We present the results for runs done on of three matrices shown in Table 4 <ref> [7] </ref>: Plat1919, Bcsstk25 and Finan512. The factors and assembly tree information were obtained using a modified version of a modified form of UMFPACK with strict diagonal pivoting [6, 5], although in principal any supernodal or multifrontal factorization algorithm for symmetric matrices could generate the supernodal factors.
Reference: [8] <author> G. H. Golub and C. F. van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD and London, UK, </address> <note> second edition, </note> <year> 1990. </year>
Reference-contexts: m 1 + m 2 , m 1 and m 2 are defined by Equation (3.2), 1 i m 1 , 1 j m, and T ij = U ij . m 1 = djU j=be 00 (3.2) 5 In addition, we use the notation of Golub and Van Loan <ref> [8] </ref>, where X [s:t;u:v] refers to a matrix X with row indices ranging from s to t, and column indices from u to v.
Reference: [9] <author> T. Johnson and T. A. Davis. </author> <title> Parallel buddy memory management. </title> <journal> Parallel Processing Letters, </journal> <volume> 2(4) </volume> <pages> 391-398, </pages> <year> 1992. </year>
Reference-contexts: Note that it is necessary for the size of the active area to be at least as large as the amount of memory required by the largest inverse frontal matrix. The buddy area is managed using a buddy memory manager <ref> [9] </ref>, while a very simple link-list manager is used for the fixed-block area. Memory can be allocated and/or deallocated concurrently from the buddy and fixed-block areas. 3.6. The parallel algorithm. An outline of the parallel block-partitioned Zsparse algorithm is given in Figure 3.7.
Reference: [10] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Technical reports and matrices are available via the World Wide Web at http://www.cis.ufl.edu/~davis, or by anonymous ftp at ftp.cis.ufl.edu:cis/tech-reports. 1 2 Fig. 2.1. Entries computed when a supernode is inverted discussed in <ref> [10] </ref>. Section 2 presents a brief summary of the sequential inverse multifrontal algorithm presented in [3]. Various aspects of the block-partitioned algorithm including task partitioning and task dependencies, scheduling, and memory management are discussed in Section 3. Section 4 contains the results and discussion of the numerical experiments. <p> For Z ij 2 S 1 and Z pq 2 S 2 , Z ij depends on Z pq if j=p. Figures 3.2 and 3.3 illustrate some of these dependencies. 7 3.3. Task scheduling. We use the guided self-scheduling technique (GSS) <ref> [10] </ref> to assign node tasks to processors. The basic idea behind the GSS scheduling scheme is that a variable-sized group of tasks is assigned to a processor each time it becomes available. The number of tasks in the variable-sized group depends on the number of tasks left to be scheduled. <p> As shown in <ref> [10] </ref>, by "guiding" (or adjusting) the amount of work given to a processor on a given scheduling pass, guided self-scheduling can result in both good load balancing and low scheduling overhead. A purely self-scheduling scheme would ordinarily assign a fixed number of tasks to every processor on each assignment.
Reference: [11] <author> K. Takahashi, J. Fagan, and M. Chin. </author> <title> Formation of a sparse bus impedance matrix and its application to short circuit study. </title> <booktitle> 8th PICA Conference Proc., Minneapolis, Minn, </booktitle> <pages> pages 177-179, </pages> <month> June, 4-6 </month> <year> 1973. </year> <note> Note: all University of Florida technical reports in this list of references are available in postscript form via anonymous ftp to ftp.cis.ufl.edu in the directory cis/tech-reports, or via the World Wide Web at http://www.cis.ufl.edu/~davis. </note>
Reference-contexts: the set of inverse entries in locations corresponding to the positions of nonzero entries in the LDU factorized form of the matrix.) In [3] Campbell and Davis introduced a new algorithm to compute Zsparse for symmetric matrices based on one of the equations presented by Takahashi, Fagan, and Chin in <ref> [11] </ref>, and an inverted form of the symmetric multifrontal method. This paper discusses a parallel implementation of the block-partitioned form of the Zsparse algorithm. The target implementation platform is the Cray-C98, a parallel-vector shared-memory machine. Extensions of the method (for arbitrary subsets) are discussed in [4]. See also [2].
References-found: 11

