URL: http://www.cs.ucsb.edu/~tyang/papers/JPDC96_spdp.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/RAPID.html
Root-URL: http://www.cs.ucsb.edu
Email: ftyang,ibarrag@cs.ucsb.edu  
Title: Performance Prediction in Symbolic Scheduling of Partitioned Programs with Weight Variation  
Author: Tao Yang and Oscar H. Ibarra 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: In this paper we consider the symbolic scheduling of partitioned loop programs which are modeled as iterative task graphs (ITGs). Each task in such a graph is coarse grained and contains a large chunk of computations. The weights of computation and communication vary from one iteration to another depending on the index value of the loop. The goal of scheduling such graphs is to incorporate the symbolic variables in weight functions and loop bounds and provide an asymptotically optimal schedule and predict its performance accurately. We provide a lower bound for optimal scheduling when weights of iterative task graphs change monotonically in the course of iterations and there is a sufficient number of processors. We present a technique that devises a valid symbolic schedule without searching all task instances and examine the asymptotic performance of this schedule compared to an optimal solution. Finally, we present case studies and experimental results on nCUBE-2 to verify our solutions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aiken and A. Nicolau, </author> <title> Optimal Loop Parallelization, </title> <booktitle> SIGPLAN 88 Conf. on Programming Language Design and Implementation. </booktitle> <address> pp.308-317. </address>
Reference-contexts: The difficulty in utilizing this macro-data flow model for scheduling loop programs is that task graph representation in [17, 25] does not precisely capture the symbolic information arising in loop programs. The problem of loop parallelization has been addressed extensively in the literature <ref> [1, 6, 14, 19, 21] </ref>. One issue is that many program transformation techniques for exploiting loop parallelism are based on heuristics that achieve local optimum. Graph scheduling techniques that examine the relationship between different program segments are attractive if they can guarantee global performance competitive to the optimal solution. <p> Graph scheduling techniques that examine the relationship between different program segments are attractive if they can guarantee global performance competitive to the optimal solution. If an innermost loop body contains a sequence of statements, pipelining techniques <ref> [1, 2, 12, 20, 16, 24] </ref> have been proposed to uncover parallelism across iterations for VLIW and superscalar architectures. They use computational weight information for load balancing, determine a schedule and predict the performance of parallelization. <p> These v tasks will be executed N times with iteration numbers 0; 1; 2; ; N 1. If each task is a statement, the instruction level scheduling has been studied in the context of software pipelining proposed for VLIW and superscalar architectures <ref> [1, 12, 20] </ref>. For message-passing architectures such as the Intel Paragon, CM-5, nCUBE-2 and Meiko CS-2, fine grain partitioning is not appropriate due to high communication startup time. <p> We will provide an analysis of its asymptotic performance. Then we add more processors if necessary to improve the scheduling performance. Periodic scheduling proposed in software pipelining <ref> [1] </ref> assumes that task weights are fixed and if task T k i is executed at time t then task T k+1 i is executed at time t + c where c is a constant. This model does not apply to our case because task weights change during iterations. <p> Next we examine how to use more processors to improve the performance. 3.4 Adding more processors We need to compute how many processors are needed if a max &gt; (1 + 1=g)Q (G). We use graph unfolding techniques proposed in software pipelining <ref> [1] </ref> to increase the number of tasks within the loop body and use more processors to execute these tasks. For example, we unfold Fig 4 (a) by 2 (i.e. unroll the loop of Fig 3 by 2). The unrolled program is shown in Fig. 5.
Reference: [2] <author> L. F. Chao and E. H. Sha, </author> <title> Unified static scheduling on various models. </title> <booktitle> Proc. ICPP 1993, </booktitle> <volume> Vol II, </volume> <pages> pp 231-235. </pages>
Reference-contexts: Graph scheduling techniques that examine the relationship between different program segments are attractive if they can guarantee global performance competitive to the optimal solution. If an innermost loop body contains a sequence of statements, pipelining techniques <ref> [1, 2, 12, 20, 16, 24] </ref> have been proposed to uncover parallelism across iterations for VLIW and superscalar architectures. They use computational weight information for load balancing, determine a schedule and predict the performance of parallelization.
Reference: [3] <author> Y. C. Chung and S. Ranka, </author> <title> Application and performance analysis of a compile-time optimization approach for list scheduling algorithms on distributed-memory multiprocessors, </title> <booktitle> Supercomputing'92, </booktitle> <pages> 512-521. </pages>
Reference-contexts: The task weights and communication cost play an important role in schedule optimization, as demonstrated in task scheduling based on directed acyclic graph (DAG) <ref> [3, 13, 17, 25] </ref>. The difficulty in scheduling such coarse grain tasks produced from a loop program is that dependence structures and weights may contain symbolic information [5].
Reference: [4] <author> S. Chittor, S. and R. Enbody, </author> <title> Performance degradation in large wormhole-routed inter-processor communication networks, </title> <booktitle> Proc. of ICPP '90, </booktitle> <pages> pp. </pages> <note> Vol. I 424-428. </note>
Reference-contexts: The weight information of this task graph is discussed in Section 4. Noted that the dependence is actually overestimated since only 3 If network load is high, communication delay is still sensitive to distances in a wormhole-routing scheme due to network contention <ref> [4] </ref>.
Reference: [5] <author> M. Cosnard and M. Loi, </author> <title> Automatic Task Graph Generation Techniques, </title> <booktitle> Proc. of the Hawaii International Conference on System Sciences, IEEE, </booktitle> <volume> Vol II. </volume> <year> 1995, </year> <pages> pp. 113-122. </pages> <note> A revised version will appear in Parallel Processing Letters, a special issue on partitioning and scheduling. 19 </note>
Reference-contexts: The difficulty in scheduling such coarse grain tasks produced from a loop program is that dependence structures and weights may contain symbolic information <ref> [5] </ref>. In this paper we consider the scheduling of partitioned loop programs which are modeled as iterative task graphs (ITGs) and many scientific computations can be represented by ITGs. An ITG 1 represents a sequence of parallel task computation. <p> For message-passing architectures, communication could be asynchronous and coarse grain partitioning is commonly used to produce tasks. Techniques for program partitioning and weighted coarse grain graph generation are described in <ref> [5, 19] </ref>. Scheduling such graphs is hard since graph representation involves symbolic information on loop bounds and task weights, and task weights may vary during iterations. In [26, 27], we have considered the ITG scheduling algorithms and applications with constant task and communication weights. <p> Each task T i can be a statement or a chunk of computation obtained by partitioning techniques [14, 19, 21]. We assume that the information on task dependence and weights are available using techniques described in <ref> [5, 18] </ref>. These v tasks will be executed N times with iteration numbers 0; 1; 2; ; N 1. If each task is a statement, the instruction level scheduling has been studied in the context of software pipelining proposed for VLIW and superscalar architectures [1, 12, 20]. <p> Kumar et.al. [10] discussed the formula for communication delay in a store-forward or wormhole routing network. In this paper, we assume that the worst-case cost is used in estimating task communication delay. Cosnard and Loi <ref> [5] </ref>, Sarkar [18] discussed methods for estimating communication and computation cost. We will discuss the assumptions on weight functions later. Assigning communication weights zero indicates that tasks communicate through local memory. This is related to the concept of "exploiting data locality" used in the literature. <p> In practice, weight functions 5 usually possess certain regularity depending on iteration numbers <ref> [5] </ref>. In this paper we first consider weights monotonically increase or remain the same from one iteration to another.
Reference: [6] <author> A. Darte, A. and Y. Robert, </author> <title> Affine-by-statement scheduling of uniform and affine loop nests over parametric domains. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> 15 Aug. </month> <year> 1995, </year> <note> vol.29, (no.1):43-59. </note>
Reference-contexts: The difficulty in utilizing this macro-data flow model for scheduling loop programs is that task graph representation in [17, 25] does not precisely capture the symbolic information arising in loop programs. The problem of loop parallelization has been addressed extensively in the literature <ref> [1, 6, 14, 19, 21] </ref>. One issue is that many program transformation techniques for exploiting loop parallelism are based on heuristics that achieve local optimum. Graph scheduling techniques that examine the relationship between different program segments are attractive if they can guarantee global performance competitive to the optimal solution.
Reference: [7] <author> V. Donaldson and J. Ferrante, </author> <title> Determing Asynchronous Acyclic Pipeline Execution Times, </title> <note> submitted for publication, </note> <year> 1995. </year>
Reference-contexts: Our scheme uses a nonlinear function to model the starting time of task instances, but function coefficients for all tasks are uniform. The previous approaches usually use linear functions with uniform coefficient for constant task weights. Donaldson and Ferrante <ref> [7] </ref> show that using nonuniform coefficients can further improve the mapping performance. It will be interesting to see how such a scheme can be used for ITGs with varying task weights.
Reference: [8] <author> T. H. Dunigan, </author> <title> Performance of the INTEL iPSC/860 and nCUBE 6400 Hypercube, </title> <institution> ORNL/TM-11790, Oak Ridge National Lab., TN, </institution> <year> 1991. </year>
Reference-contexts: The sequential time is 21890 milliseconds. Since the number of operations for the sequential algorithm is n 2 w, we estimate w = 0:8756s. For nCUBE-2, s = 160s and t = 0:6s per byte <ref> [8] </ref>. In Fig. 6, we plot the predicted performance by using these parameters in the above formula and compare it to the actual performance in the left part of Fig.6. The predicted performance is close to the actual parallel time in nCUBE-2.
Reference: [9] <author> A. Gerasoulis and T. Yang, </author> <title> On the Granularity and Clustering of Directed Acyclic Task Graphs, </title> <journal> IEEE Trans. on Parallel and Distributed Systems., </journal> <volume> Vol. 4, no. 6, </volume> <month> June </month> <year> 1993, </year> <pages> pp 686-701. </pages>
Reference-contexts: 1 Introduction Parallel programming on distributed memory machines requires coarse grain partitioning since there is a large startup overhead in message transmission <ref> [9, 19, 21] </ref>. A partitioned program usually contains a set of tasks with large chunk of computation. Finding a mapping of weighted coarse-grain dependence task graphs onto target architectures with the shortest parallel time is important for studying the impact of partitioning and generating efficient code. <p> This paper is focused on this clustering stage. 2) Map task clusters (threads) to p physical processors for multi-threaded computation. The idea of such a multi-stage approach has been used in SISAL compilation and DAG scheduling [17, 22, 25]. We use the ideas of the macro dataflow task model <ref> [9, 17] </ref> and pipelining techniques, and examine how these ideas can be combined and extended for the symbolic scheduling of iterative task computation without searching the entire iteration space. <p> Notice that a program partitioning results in a task graph which possesses certain characteristics: a avg ; a max , dependence cycles, and task granularity. We will show how these characteristics affect the scheduling performance and examine what kinds of partitioning are preferred. The previous work <ref> [9] </ref> has shown that the performance of DAG scheduling depends on the granularity of DAGs. Our result also indicates this correlation but works for task graphs with cycles. A definition and formal analysis on granularity of a DAG is given in [9] and we use a slightly different definition to quantify <p> The previous work <ref> [9] </ref> has shown that the performance of DAG scheduling depends on the granularity of DAGs. Our result also indicates this correlation but works for task graphs with cycles. A definition and formal analysis on granularity of a DAG is given in [9] and we use a slightly different definition to quantify the granularity of an ITG G as: g = min f min T y 2succ (T x ) a x g: Theorem 3 If a max and b max are bounded constants, let Q (G) = max all cycles C A1
Reference: [10] <author> V. Kumar, A. Grama, A. Gupta, G. Karypis, </author> <title> Introduction to Parallel Computing, </title> <publisher> Ben-jamin/Cummings Pub, </publisher> <year> 1994. </year>
Reference-contexts: Usually the communication delay is modeled as communication startup time+ transmission speed fi size of message. The physical distance between processors may affect communication delay; however, in a wormhole routing network the cost is not so sensitive to processor distances 3 . Kumar et.al. <ref> [10] </ref> discussed the formula for communication delay in a store-forward or wormhole routing network. In this paper, we assume that the worst-case cost is used in estimating task communication delay. Cosnard and Loi [5], Sarkar [18] discussed methods for estimating communication and computation cost. <p> For the partitioning we use, the scheduling results in a good speedup with small memory consumption. The memory-efficient algorithms for matrix multiplication have been studied in the previous work (see <ref> [10] </ref>) and task communication is needed to transfer matrix elements in these algorithms. 17 the solution in Section 3, we can determine the parallel time as P T = (P 1)(s + t + 2wh) + w=2 + (2wn + (P 1)t + w)(n 1) 2 + (hw + w=4)(n 1)
Reference: [11] <author> C. E. Leiserson and J. B. Saxes, </author> <title> Optimizing synchronous systems, </title> <booktitle> IEEE 22nd Annual Symp. Foundations of Computer Science, </booktitle> <month> Oct </month> <year> 1981. </year> <pages> pp. 23-35. </pages>
Reference: [12] <author> M. Lam, </author> <title> Software pipelining: an effective scheduling technique for VLIW machines, </title> <booktitle> Proc. of ACM Conf. on Programming Language Design and Implementation, </booktitle> <year> 1988, </year> <pages> 318-328. </pages>
Reference-contexts: Graph scheduling techniques that examine the relationship between different program segments are attractive if they can guarantee global performance competitive to the optimal solution. If an innermost loop body contains a sequence of statements, pipelining techniques <ref> [1, 2, 12, 20, 16, 24] </ref> have been proposed to uncover parallelism across iterations for VLIW and superscalar architectures. They use computational weight information for load balancing, determine a schedule and predict the performance of parallelization. <p> These v tasks will be executed N times with iteration numbers 0; 1; 2; ; N 1. If each task is a statement, the instruction level scheduling has been studied in the context of software pipelining proposed for VLIW and superscalar architectures <ref> [1, 12, 20] </ref>. For message-passing architectures such as the Intel Paragon, CM-5, nCUBE-2 and Meiko CS-2, fine grain partitioning is not appropriate due to high communication startup time. <p> When a avg = 0, the bound is reduced to L crit M C fl B1 C = b N1 D C cB1 C which is a lower bound for any cyclic scheduling <ref> [12] </ref>. 3.2 Scheduling on v processors We first present a schedule when the number of processors is v (equal to the number of tasks in an ITG). We will provide an analysis of its asymptotic performance. Then we add more processors if necessary to improve the scheduling performance.
Reference: [13] <author> S. S. Pande, D. P. Agrawal and J. Mauney, </author> <title> Compiling functional parallelism on distributed-memory systems. </title> <journal> IEEE Parallel & Distributed Technology: Systems & Applications, </journal> <note> Spring 1994, vol.2, (no.1):64-76. </note>
Reference-contexts: The task weights and communication cost play an important role in schedule optimization, as demonstrated in task scheduling based on directed acyclic graph (DAG) <ref> [3, 13, 17, 25] </ref>. The difficulty in scheduling such coarse grain tasks produced from a loop program is that dependence structures and weights may contain symbolic information [5].
Reference: [14] <author> C. D. Polychronopoulos, </author> <title> Parallel Programming and Compilers, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: The difficulty in utilizing this macro-data flow model for scheduling loop programs is that task graph representation in [17, 25] does not precisely capture the symbolic information arising in loop programs. The problem of loop parallelization has been addressed extensively in the literature <ref> [1, 6, 14, 19, 21] </ref>. One issue is that many program transformation techniques for exploiting loop parallelism are based on heuristics that achieve local optimum. Graph scheduling techniques that examine the relationship between different program segments are attractive if they can guarantee global performance competitive to the optimal solution. <p> Each task T i can be a statement or a chunk of computation obtained by partitioning techniques <ref> [14, 19, 21] </ref>. We assume that the information on task dependence and weights are available using techniques described in [5, 18]. These v tasks will be executed N times with iteration numbers 0; 1; 2; ; N 1.
Reference: [15] <author> M. A. Palis, J.-C. Liou; Wei, D.S.L. </author> <title> A greedy task clustering heuristic that is provably good. </title> <booktitle> Proceedings of the 1994 International Symposium on Parallel Architectures, Algorithms and Networks (ISPAN). IEEE 1994. p. </booktitle> <pages> 398-405. </pages>
Reference-contexts: The previous work on scheduling task computation has used the macro-data flow model to exploit task parallelism [17]. Many algorithms for mapping weighted DAGs in the presence of communication have been proposed and used in compiler tools, e.g. <ref> [15, 17, 25] </ref>. The difficulty in utilizing this macro-data flow model for scheduling loop programs is that task graph representation in [17, 25] does not precisely capture the symbolic information arising in loop programs.
Reference: [16] <author> J. Ramanujam, </author> <title> Optimal software pipelining of nested loops, </title> <booktitle> Proc. of 1994 IPPS, </booktitle> <pages> 335-342. </pages>
Reference-contexts: Graph scheduling techniques that examine the relationship between different program segments are attractive if they can guarantee global performance competitive to the optimal solution. If an innermost loop body contains a sequence of statements, pipelining techniques <ref> [1, 2, 12, 20, 16, 24] </ref> have been proposed to uncover parallelism across iterations for VLIW and superscalar architectures. They use computational weight information for load balancing, determine a schedule and predict the performance of parallelization.
Reference: [17] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: The task weights and communication cost play an important role in schedule optimization, as demonstrated in task scheduling based on directed acyclic graph (DAG) <ref> [3, 13, 17, 25] </ref>. The difficulty in scheduling such coarse grain tasks produced from a loop program is that dependence structures and weights may contain symbolic information [5]. <p> The symbolic information considered in this paper is the loop bound and the loop index in weight functions. The previous work on scheduling task computation has used the macro-data flow model to exploit task parallelism <ref> [17] </ref>. Many algorithms for mapping weighted DAGs in the presence of communication have been proposed and used in compiler tools, e.g. [15, 17, 25]. <p> The previous work on scheduling task computation has used the macro-data flow model to exploit task parallelism [17]. Many algorithms for mapping weighted DAGs in the presence of communication have been proposed and used in compiler tools, e.g. <ref> [15, 17, 25] </ref>. The difficulty in utilizing this macro-data flow model for scheduling loop programs is that task graph representation in [17, 25] does not precisely capture the symbolic information arising in loop programs. <p> Many algorithms for mapping weighted DAGs in the presence of communication have been proposed and used in compiler tools, e.g. [15, 17, 25]. The difficulty in utilizing this macro-data flow model for scheduling loop programs is that task graph representation in <ref> [17, 25] </ref> does not precisely capture the symbolic information arising in loop programs. The problem of loop parallelization has been addressed extensively in the literature [1, 6, 14, 19, 21]. One issue is that many program transformation techniques for exploiting loop parallelism are based on heuristics that achieve local optimum. <p> This paper is focused on this clustering stage. 2) Map task clusters (threads) to p physical processors for multi-threaded computation. The idea of such a multi-stage approach has been used in SISAL compilation and DAG scheduling <ref> [17, 22, 25] </ref>. We use the ideas of the macro dataflow task model [9, 17] and pipelining techniques, and examine how these ideas can be combined and extended for the symbolic scheduling of iterative task computation without searching the entire iteration space. <p> This paper is focused on this clustering stage. 2) Map task clusters (threads) to p physical processors for multi-threaded computation. The idea of such a multi-stage approach has been used in SISAL compilation and DAG scheduling [17, 22, 25]. We use the ideas of the macro dataflow task model <ref> [9, 17] </ref> and pipelining techniques, and examine how these ideas can be combined and extended for the symbolic scheduling of iterative task computation without searching the entire iteration space. <p> Some definitions on dependence and weights are listed below. * We use the macro-dataflow task model for computation execution <ref> [17] </ref>. A task receives all input before starting execution, executes to completion without interruption, and immediately sends the output to all successor tasks. * Let the instance of task T i at iteration k be T k i (0 k N 1). <p> We will discuss the assumptions on weight functions later. Assigning communication weights zero indicates that tasks communicate through local memory. This is related to the concept of "exploiting data locality" used in the literature. In task computation model <ref> [17] </ref>, exploiting data locality means to localize data communication between tasks by assigning them in the same processor and then tasks could exchange data through a local memory to avoid high-cost inter-processor communication. <p> j + v j;1 + i=1 A2 = max all cycles C A2 C ; B2 = max all cycles C B2 C : 3 Scheduling with nonzero communication and weight vari- ation The problem of task graph scheduling with nonzero communication and a sufficient number of processors is NP-complete <ref> [17] </ref>. We first present a bound on the performance of any optimal schedule derived by searching the entire iteration space. This result is useful when we compare it with our heuristic schedule to determine the performance difference. <p> For these two numerical computing problems, we use the partitioned loop programs and evaluate the performance of scheduling. In the implementation of parallel programs, we follow the task computation model <ref> [17, 25] </ref>. As soon as data needed by a task are available in the local memory, this task starts the computation and then it sends the produced results to its successors assigned in different processors.
Reference: [18] <author> V. Sarkar, </author> <title> Determining average program execution times and their variance, </title> <booktitle> Proc. of 1989 SIGPLAN, ACM, </booktitle> <pages> pp. 298-312. </pages>
Reference-contexts: Each task T i can be a statement or a chunk of computation obtained by partitioning techniques [14, 19, 21]. We assume that the information on task dependence and weights are available using techniques described in <ref> [5, 18] </ref>. These v tasks will be executed N times with iteration numbers 0; 1; 2; ; N 1. If each task is a statement, the instruction level scheduling has been studied in the context of software pipelining proposed for VLIW and superscalar architectures [1, 12, 20]. <p> Kumar et.al. [10] discussed the formula for communication delay in a store-forward or wormhole routing network. In this paper, we assume that the worst-case cost is used in estimating task communication delay. Cosnard and Loi [5], Sarkar <ref> [18] </ref> discussed methods for estimating communication and computation cost. We will discuss the assumptions on weight functions later. Assigning communication weights zero indicates that tasks communicate through local memory. This is related to the concept of "exploiting data locality" used in the literature.
Reference: [19] <author> V. Sarkar and R. Thekkath, </author> <title> A general framework for iteration-reordering loop transformations, </title> <booktitle> ACM SIGPLAN 92 PLDI. </booktitle> <pages> pp 175-187. </pages>
Reference-contexts: 1 Introduction Parallel programming on distributed memory machines requires coarse grain partitioning since there is a large startup overhead in message transmission <ref> [9, 19, 21] </ref>. A partitioned program usually contains a set of tasks with large chunk of computation. Finding a mapping of weighted coarse-grain dependence task graphs onto target architectures with the shortest parallel time is important for studying the impact of partitioning and generating efficient code. <p> The difficulty in utilizing this macro-data flow model for scheduling loop programs is that task graph representation in [17, 25] does not precisely capture the symbolic information arising in loop programs. The problem of loop parallelization has been addressed extensively in the literature <ref> [1, 6, 14, 19, 21] </ref>. One issue is that many program transformation techniques for exploiting loop parallelism are based on heuristics that achieve local optimum. Graph scheduling techniques that examine the relationship between different program segments are attractive if they can guarantee global performance competitive to the optimal solution. <p> For message-passing architectures, communication could be asynchronous and coarse grain partitioning is commonly used to produce tasks. Techniques for program partitioning and weighted coarse grain graph generation are described in <ref> [5, 19] </ref>. Scheduling such graphs is hard since graph representation involves symbolic information on loop bounds and task weights, and task weights may vary during iterations. In [26, 27], we have considered the ITG scheduling algorithms and applications with constant task and communication weights. <p> Each task T i can be a statement or a chunk of computation obtained by partitioning techniques <ref> [14, 19, 21] </ref>. We assume that the information on task dependence and weights are available using techniques described in [5, 18]. These v tasks will be executed N times with iteration numbers 0; 1; 2; ; N 1.
Reference: [20] <author> V. H. Van Dongen, G. R. Gao and Q. </author> <title> Ning A polynomial time method for optimal software pipelining. </title> <booktitle> Proc. of CONPAR 92, </booktitle> <pages> pp. 613-624. 20 </pages>
Reference-contexts: Graph scheduling techniques that examine the relationship between different program segments are attractive if they can guarantee global performance competitive to the optimal solution. If an innermost loop body contains a sequence of statements, pipelining techniques <ref> [1, 2, 12, 20, 16, 24] </ref> have been proposed to uncover parallelism across iterations for VLIW and superscalar architectures. They use computational weight information for load balancing, determine a schedule and predict the performance of parallelization. <p> These v tasks will be executed N times with iteration numbers 0; 1; 2; ; N 1. If each task is a statement, the instruction level scheduling has been studied in the context of software pipelining proposed for VLIW and superscalar architectures <ref> [1, 12, 20] </ref>. For message-passing architectures such as the Intel Paragon, CM-5, nCUBE-2 and Meiko CS-2, fine grain partitioning is not appropriate due to high communication startup time.
Reference: [21] <author> M. Wolfe, </author> <title> Optimizing Supercompilers for Supercomputers, </title> <publisher> MIT Press, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Parallel programming on distributed memory machines requires coarse grain partitioning since there is a large startup overhead in message transmission <ref> [9, 19, 21] </ref>. A partitioned program usually contains a set of tasks with large chunk of computation. Finding a mapping of weighted coarse-grain dependence task graphs onto target architectures with the shortest parallel time is important for studying the impact of partitioning and generating efficient code. <p> The difficulty in utilizing this macro-data flow model for scheduling loop programs is that task graph representation in [17, 25] does not precisely capture the symbolic information arising in loop programs. The problem of loop parallelization has been addressed extensively in the literature <ref> [1, 6, 14, 19, 21] </ref>. One issue is that many program transformation techniques for exploiting loop parallelism are based on heuristics that achieve local optimum. Graph scheduling techniques that examine the relationship between different program segments are attractive if they can guarantee global performance competitive to the optimal solution. <p> Each task T i can be a statement or a chunk of computation obtained by partitioning techniques <ref> [14, 19, 21] </ref>. We assume that the information on task dependence and weights are available using techniques described in [5, 18]. These v tasks will be executed N times with iteration numbers 0; 1; 2; ; N 1. <p> Output and anti-dependence can be removed by renaming tech niques. Control dependence is not considered in this paper. 2 Some dependence graphs may contain imprecise dependence direction information such as '+' instead of precise distance values <ref> [21] </ref>. Our result can be extended for this case easily. For example, reduce '+' as 1. 3 * The computation weight function of task T k i is t i (k), which is the time that T k i takes to execute. <p> For this case, it is not easy to summarize inter-iteration dependence accurately. In fact, current dependence analysis algorithms <ref> [21] </ref> may overestimate dependence since accurate estimation may be intractable; however, overestimation does not affect the correctness of parallel execution. Fig. 3 is another example of a partitioned loop program whose loop body contains three tasks.
Reference: [22] <author> R. Wolski and J. Feo, </author> <title> Program Partitioning for NUMA Multiprocessor Computer Systems, </title> <type> Tech. Report, </type> <institution> Lawrence Livermore Nat. Lab., </institution> <year> 1992. </year> <editor> J. </editor> <booktitle> of Parallel and Distributed Computing, </booktitle> <year> 1993. </year>
Reference-contexts: This paper is focused on this clustering stage. 2) Map task clusters (threads) to p physical processors for multi-threaded computation. The idea of such a multi-stage approach has been used in SISAL compilation and DAG scheduling <ref> [17, 22, 25] </ref>. We use the ideas of the macro dataflow task model [9, 17] and pipelining techniques, and examine how these ideas can be combined and extended for the symbolic scheduling of iterative task computation without searching the entire iteration space.
Reference: [23] <author> M. Y. Wu and D. Gajski, Hypertool: </author> <title> A programming aid for message-passing systems, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 1, no. 3, pp.330-343, </volume> <year> 1990. </year>
Reference: [24] <author> A. Zaky and P. Sadayappan, </author> <title> Optimal static scheduling of sequential loops on multiprocessors. </title> <booktitle> Proc. of ICPP 1989, </booktitle> <volume> Vol 3, </volume> <pages> 130-137 </pages>
Reference-contexts: Graph scheduling techniques that examine the relationship between different program segments are attractive if they can guarantee global performance competitive to the optimal solution. If an innermost loop body contains a sequence of statements, pipelining techniques <ref> [1, 2, 12, 20, 16, 24] </ref> have been proposed to uncover parallelism across iterations for VLIW and superscalar architectures. They use computational weight information for load balancing, determine a schedule and predict the performance of parallelization.
Reference: [25] <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors, </title> <booktitle> Proc. of 6th ACM Inter. Confer. on Supercomputing, </booktitle> <address> Washington D.C., </address> <year> 1992, </year> <pages> pp. 428-437. </pages>
Reference-contexts: The task weights and communication cost play an important role in schedule optimization, as demonstrated in task scheduling based on directed acyclic graph (DAG) <ref> [3, 13, 17, 25] </ref>. The difficulty in scheduling such coarse grain tasks produced from a loop program is that dependence structures and weights may contain symbolic information [5]. <p> The previous work on scheduling task computation has used the macro-data flow model to exploit task parallelism [17]. Many algorithms for mapping weighted DAGs in the presence of communication have been proposed and used in compiler tools, e.g. <ref> [15, 17, 25] </ref>. The difficulty in utilizing this macro-data flow model for scheduling loop programs is that task graph representation in [17, 25] does not precisely capture the symbolic information arising in loop programs. <p> Many algorithms for mapping weighted DAGs in the presence of communication have been proposed and used in compiler tools, e.g. [15, 17, 25]. The difficulty in utilizing this macro-data flow model for scheduling loop programs is that task graph representation in <ref> [17, 25] </ref> does not precisely capture the symbolic information arising in loop programs. The problem of loop parallelization has been addressed extensively in the literature [1, 6, 14, 19, 21]. One issue is that many program transformation techniques for exploiting loop parallelism are based on heuristics that achieve local optimum. <p> This paper is focused on this clustering stage. 2) Map task clusters (threads) to p physical processors for multi-threaded computation. The idea of such a multi-stage approach has been used in SISAL compilation and DAG scheduling <ref> [17, 22, 25] </ref>. We use the ideas of the macro dataflow task model [9, 17] and pipelining techniques, and examine how these ideas can be combined and extended for the symbolic scheduling of iterative task computation without searching the entire iteration space. <p> For these two numerical computing problems, we use the partitioned loop programs and evaluate the performance of scheduling. In the implementation of parallel programs, we follow the task computation model <ref> [17, 25] </ref>. As soon as data needed by a task are available in the local memory, this task starts the computation and then it sends the produced results to its successors assigned in different processors.
Reference: [26] <author> T. Yang, C. Fu, A. Gerasoulis and V. Sarkar, </author> <title> Mapping iterative task graphs on distributed-memory machines. </title> <booktitle> Proc. of 24th Inter. Conference on Parallel Processing, </booktitle> <address> Wisconsin, </address> <month> Aug. </month> <journal> 1995. </journal> <volume> Vol II. </volume> <pages> 151-158. </pages>
Reference-contexts: Techniques for program partitioning and weighted coarse grain graph generation are described in [5, 19]. Scheduling such graphs is hard since graph representation involves symbolic information on loop bounds and task weights, and task weights may vary during iterations. In <ref> [26, 27] </ref>, we have considered the ITG scheduling algorithms and applications with constant task and communication weights. In this paper, we consider the ITG scheduling with weight variation and communication delay. We take a two-stage scheduling approach: 1) Cluster tasks into a set of threads. <p> The results studied in this paper are used for the two-stage scheduling approach and one of our future work is to investigate the techniques to execute multiple clusters on a limited number of processors. Directly scheduling ITGs on a fixed number of processors is studied in <ref> [26, 27] </ref> where weights are constant but communication is nonzero. Our scheme uses a nonlinear function to model the starting time of task instances, but function coefficients for all tasks are uniform. The previous approaches usually use linear functions with uniform coefficient for constant task weights.
Reference: [27] <author> T. Yang, C. Fu, </author> <title> Heuristic Algorithms for Scheduling Iterative Task Computations on Distributed Memory Machines, </title> <type> Tech Report TRCS95-16, </type> <institution> UCSB, </institution> <year> 1995. </year> <note> HTTP://www.cs.ucsb.edu/Research/rapid sweb/RAPID.html. </note>
Reference-contexts: Techniques for program partitioning and weighted coarse grain graph generation are described in [5, 19]. Scheduling such graphs is hard since graph representation involves symbolic information on loop bounds and task weights, and task weights may vary during iterations. In <ref> [26, 27] </ref>, we have considered the ITG scheduling algorithms and applications with constant task and communication weights. In this paper, we consider the ITG scheduling with weight variation and communication delay. We take a two-stage scheduling approach: 1) Cluster tasks into a set of threads. <p> The results studied in this paper are used for the two-stage scheduling approach and one of our future work is to investigate the techniques to execute multiple clusters on a limited number of processors. Directly scheduling ITGs on a fixed number of processors is studied in <ref> [26, 27] </ref> where weights are constant but communication is nonzero. Our scheme uses a nonlinear function to model the starting time of task instances, but function coefficients for all tasks are uniform. The previous approaches usually use linear functions with uniform coefficient for constant task weights.
References-found: 27

