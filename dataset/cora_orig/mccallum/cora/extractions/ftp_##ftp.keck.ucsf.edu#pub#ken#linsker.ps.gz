URL: ftp://ftp.keck.ucsf.edu/pub/ken/linsker.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Email: mackay@aurel.cns.caltech.edu  ken@phyb.ucsf.edu  e-mail: mackay@mrao.cam.ac.uk;  e-mail: ken@phy.ucsf.edu;  
Title: Analysis of Linsker's application of Hebbian rules to Linear Networks  
Author: David J. C. MacKay Kenneth D. Miller David MacKay Ken Miller 
Web: www: ftp://131.111.48.24/pub/mackay/homepage.html  www: http://keck.ucsf.edu/  
Note: Published as: Network 1:257-298 (1990). 6/29/95: Present addresses of authors:  ken  
Address: Pasadena CA 91125  San Francisco San Francisco CA 94143 0444  Madingley Road, Cambridge, CB3 0HE. U.K.  Physiology, UCSF, SF, CA 94143-0444  
Affiliation: Department of Computation and Neural Systems California Institute of Technology 164-30 CNS  Department of Physiology University of California at  Radio Astronomy, Cavendish Laboratory,  Dept. of  
Abstract: Linsker has reported the development of structured receptive fields in simulations using a Hebb-type synaptic plasticity rule in a feed-forward linear network. The synapses develop under dynamics determined by a matrix that is closely related to the covariance matrix of input cell activities. We analyse the dynamics of the learning rule in terms of the eigenvectors of this matrix. These eigenvectors represent independently evolving weight structures. Some general theorems are presented regarding the properties of these eigenvectors and their eigenvalues. For a general covariance matrix four principal parameter regimes are predicted. We concentrate on the gaussian covariances at layer B ! C of Linsker's network. Analytic and numerical solutions for the eigenvectors at this layer are presented. Three eigenvectors dominate the dynamics: a DC eigenvector, in which all synapses have the same sign; a bi-lobed, oriented eigenvector; and a circularly symmetric, centre-surround eigenvector. Analysis of the circumstances in which each of these vectors dominates yields an explanation of the emergence of centre-surround structures and symmetry-breaking bi-lobed structures. Criteria are developed estimating the boundary of the parameter regime in which centre-surround structures emerge. The application of our analysis to Linsker's higher layers, at which the covariance functions were oscillatory, is briefly discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F.V. </author> <title> Atkinson (1964). Discrete and continuous boundary problems, </title> <publisher> Academic Press. </publisher>
Reference-contexts: Lemma 6 If A (t) is a differentiable Hermitian matrix function with positive semi-definite derivative dA dt then the eigenvalues a (t) of A are non-decreasing functions of t. For proof see Theorem V.2.3 in <ref> [1, p. 459ff.] </ref>. Now d=dk 2 (Q + k 2 J) = J, and J is positive semi-definite, so: Lemma 7 The DC-mixed eigenvalues of (Q + k 2 J) are monotonically increasing continuous functions of k 2 .
Reference: [2] <author> R.N. </author> <title> Bracewell (1965). The Fourier transform and its applications, </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: cos (l)F l (k), where F l (k) = 0 is the l-th order Hankel transform of f (r), and J l (x) is a Bessel function J l (x) = 0 cos (l xsin)d The inverse transform is symmetric: f (r) = 0 We use the following Hankel transforms <ref> [13, 2] </ref>: 19 f (r) F 0 (k) r 2 e r 2 =2B B 2 (2 Bk 2 )e Bk 2 =2 f (r) F l (k) f (ar) a 2 F l (k=a) D.1.2 Eigenfunctions We guess radial functions with the form of a gaussian e r 2 =2R
Reference: [3] <author> R. </author> <title> Linsker (1986). From basic network principles to neural architecture (series), </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci. </institution> <address> USA 83, </address> <pages> 7508-7512, 8390-8394, 8779-8783. </pages>
Reference-contexts: 1 Introduction Linsker has studied by simulation the evolution of synaptic weight vectors in a feed-forward linear network <ref> [3, 4] </ref>. The network is shown in figure 1. Synaptic weight modification occured under a Hebbian rule that was linear up to saturating nonlinearities limiting the sizes of synaptic weights. Linsker found that in certain parameter regimes, "centre-surround" synaptic structures emerged at the third layer of the network (figure 2). <p> Linsker used various values for the ratio C=A. Our analytic results leave this ratio as a free parameter; in the figures we have used C=A = 2=3, the value frequently used for layer B ! C in <ref> [3] </ref>. We now investigate the eigenfunctions of the integral operator above for Linsker's layer B ! C. We will continue to refer to this integral operator as the matrix Q+k 2 J; we will use the terms eigenfunction and eigenvector interchangeably. <p> In the limit of large negative k 2 the constraint enforced by the eigenvector ^n determines the final average synaptic strength: w = w FP n=N . Linsker showed <ref> [3] </ref> that all or all but one of the synapses in a stable final configuration have 9 A: Eigenvectors with non-zero DC component (solid lines) vary with k 2 , and have eigenvalues that increase monotonically with k 2 . <p> This is the case if the largest eigenvalue of k 2 J, k 2 N , is much larger than any eigenvalue of Q (Appendix C). Using the eigenvalue of 1s in Table 1, this yields the requirement jk 2 j C 2A 2 (1 1 + 4A=C). In <ref> [3] </ref>, Linsker used values of C A for layer B ! C ranging between 2 5 and 2 3 , for which the above requirement ranges from k 2 t 0:22 to k 2 t 0:44. <p> On the constraint surface, we can divide the weight vector into w (t) = w AC (t) + w DC , where w DC = k 1 k 2 N n is the 7 The additional term largely resolves the discrepancy between Linsker's g and k 1 =k 2 in <ref> [3] </ref>. In the continuum limit, q = 1 1+2A=C , using the notation of table 1. In the example on p. 7511 of [3], A=C = 1:5, k 1 = 0:45, k 2 = 3. <p> DC = k 1 k 2 N n is the 7 The additional term largely resolves the discrepancy between Linsker's g and k 1 =k 2 in <ref> [3] </ref>. In the continuum limit, q = 1 1+2A=C , using the notation of table 1. In the example on p. 7511 of [3], A=C = 1:5, k 1 = 0:45, k 2 = 3. Hence k 1 =jk 2 j = 0:15 and k 1 =jk 2 + qj = 0:164, while the observed value of g was 0:166 0:002. <p> However, the qualitative picture of the division of parameter regimes that they present is informative. 6.1 Energy Criterion Linsker <ref> [3, 4] </ref> suggested analysis of equation (3) in terms of the energy function on which the dynamics perform constrained gradient descent: E = 2 Neglecting the initial conditions, we can examine the result of minimising this energy subject to the hard limit constraints on w i . <p> Justification for this assumption at layer B ! C is provided by Linsker's account of the simulated development of centre-surround cells, in which centre-surround structures emerge before any saturation occurs <ref> [3, page 7512] </ref>. In certain parameter regimes (regions B and D of figure 9), the initial conditions may give preference to energetically unfavored weight configurations. In these regimes, assumption 19 1 may be violated, because the nonlinear dynamics do not always preserve saturated structures that are energetically unfavourable. <p> Linsker suggested that the emergence of centre-surround structures may depend on the peaked synaptic density function that he used <ref> [3, page 7512] </ref>. However, with a flat `pill-box' density function, we have calculated that the eigenfunctions are qualitatively unchanged. Therefore we conjecture that centre-surround structures should emerge by the same `head start' mechanism with a pill-box density function, though the locations of the parameter regime boundaries will be different. <p> In biology, all the synapses from a single neuron are either exclusively positive or exclusively negative. Synaptic strengths that may take either positive or negative values can be used to describe the summed strength of two separate populations, one of exclusively positive synapses and one of exclusively negative synapses <ref> [3] </ref>. <p> In the absence of constraints, receptive fields may vary their spatial phase. This leads to very different predictions for the organization of orientation selectivity across the cortex from those made by Linsker in <ref> [3] </ref> ([9]).
Reference: [4] <author> R. </author> <title> Linsker (1988). Self-organization in a perceptual network, </title> <booktitle> Computer 21(3), </booktitle> <pages> 105-117. </pages>
Reference-contexts: 1 Introduction Linsker has studied by simulation the evolution of synaptic weight vectors in a feed-forward linear network <ref> [3, 4] </ref>. The network is shown in figure 1. Synaptic weight modification occured under a Hebbian rule that was linear up to saturating nonlinearities limiting the sizes of synaptic weights. Linsker found that in certain parameter regimes, "centre-surround" synaptic structures emerged at the third layer of the network (figure 2). <p> The dynamical and biological bases for Linsker's results have not been clearly established. It has been pointed out that a Hebbian mechanism can perform certain types of principal component analysis <ref> [4, 14] </ref>, but Linsker's results do not represent principal components of the input statistics, as we shall demonstrate. In this paper, we present an analysis of the dynamical mechanisms responsible for Linsker's results. <p> However, the qualitative picture of the division of parameter regimes that they present is informative. 6.1 Energy Criterion Linsker <ref> [3, 4] </ref> suggested analysis of equation (3) in terms of the energy function on which the dynamics perform constrained gradient descent: E = 2 Neglecting the initial conditions, we can examine the result of minimising this energy subject to the hard limit constraints on w i .
Reference: [5] <author> R. </author> <title> Linsker (1988). Development of feature-analyzing cells and their columnar organization in a layered self-adaptive network. In Computer Simulation in Brain Science, </title> <editor> R.M.J. Cotterill, Ed., </editor> <publisher> Cambridge University Press, </publisher> <pages> 416-431. </pages>
Reference-contexts: Since the multi-lobed outcome occurs only for a narrow range of g, this suggests that the multi-lobed outcome is determined in the nonlinear regime. Indeed, within the parameter-space "bubble" in which Linsker obtained oriented cells, the only published example of time development <ref> [5] </ref> shows that a 3s structure develops initially. Only in the nonlinear regime, when many synapses were saturated, does this 3s structure convert to a tri-lobed, oriented structure.
Reference: [6] <author> D.J.C. MacKay and K.D. </author> <title> Miller (1990). Analysis of Linsker's simulations of Hebbian rules, </title> <booktitle> Neural Computation 2, </booktitle> <pages> 169-182. </pages>
Reference-contexts: We also briefly discuss the biological plausibility of the dynamical mechanisms found to underly Linsker's results. In appendix G the same methods are applied to a model one-dimensional network analogous to Linsker's two-dimensional network. Some of these results have been presented in briefer form elsewhere <ref> [6, 7] </ref>. 2 Analysis in Terms of Eigenvectors We write equation (3) as a first order differential equation for the weight vector w: 3 The synaptic density function and covariance function can be treated explicitly, as discussed in section 3.2 and appendix A. 3 (a) Relative to the fixed point, the <p> D: There is only one negative eigenvalue. The leading eigenvector at k 2 = 1 and the negative eigenvector at k 2 = 1 are both equal to the DC vector, ^n. The annotations in parentheses identify the eigenvectors of Linsker's system at layer B ! C. From <ref> [6] </ref>. synaptic strength w max , so this constraint on the average synaptic strength effectively fixes the final percentages of positive synapses and of negative synapses. 4.3 What constitutes large k 2 ? We say that k 2 is large and negative when the eigenfunctions of Q + k 2 J <p> The estimates of g E and N fl (g) are both obtained from the analytic results using perturbation theory (Appendices C, D). From <ref> [6] </ref>. w max and w max , we obtain w 1 (0) rms = (g)w max d (9) where (g) is a dimensionless standard deviation derived in appendix F and d is the degeneracy of e (1) . <p> See text for explanation of the regimes. When k 2 is large and negative, the DC constraint is approximately constant along the radial lines of constant k 1 =(k 2 + q), so each of the parameter regimes with large negative k 2 is wedge shaped. From <ref> [6] </ref>. negative Q in which 2s is the principal eigenvector at k 2 = 1: then in both regimes 3 and 4, 2s would dominate.
Reference: [7] <author> D.J.C. MacKay and K.D. </author> <title> Miller (1990). Analysis of Linsker's simulations of Hebbian rules, </title> <booktitle> in Advances in Neural Information Processing Systems II, </booktitle> <editor> D. Touretzky, Ed. </editor> <publisher> (Morgan Kaufman, </publisher> <address> San Mateo CA), </address> <pages> 694-701. </pages>
Reference-contexts: We also briefly discuss the biological plausibility of the dynamical mechanisms found to underly Linsker's results. In appendix G the same methods are applied to a model one-dimensional network analogous to Linsker's two-dimensional network. Some of these results have been presented in briefer form elsewhere <ref> [6, 7] </ref>. 2 Analysis in Terms of Eigenvectors We write equation (3) as a first order differential equation for the weight vector w: 3 The synaptic density function and covariance function can be treated explicitly, as discussed in section 3.2 and appendix A. 3 (a) Relative to the fixed point, the
Reference: [8] <author> E. </author> <month> Merzbacher </month> <year> (1970). </year> <title> Quantum mechanics, 2nd ed., </title> <publisher> Wiley. </publisher>
Reference-contexts: The lines show the only way of joining the dots to satisfy the lemmas. C Perturbation theory We derive perturbation theory <ref> [8] </ref> approximations for some of the eigenvalues k 2 a and DC components n k 2 a of the eigenvectors of Q + k 2 J for large k 2 .
Reference: [9] <author> K.D. </author> <title> Miller (1989). Orientation-selective cells can emerge from a Hebbian mechanism through interactions between ON- and OFF-center inputs, </title> <journal> Soc. Neurosc. Abst. </journal> <volume> 15, </volume> <pages> 794. </pages>
Reference-contexts: One can derive a linear Hebb rule like Linsker's without the use of negative synapses by studying the difference between the innervation strengths of two equivalent excitatory projections [11]. Biological examples include ON-centre and OFF-centre inputs <ref> [9] </ref> and left-eye and right-eye inputs [12] in the mammalian visual system. In this case, however, the constants k 1 and k 2 disappear from the equation for the development of the difference of synaptic strengths because these constants take on equal values for each of the two equivalent populations. <p> Such a model can nonetheless develop orientation-selective receptive field structures if oscillations exist in the covariance functions of the input layer and if lateral interactions are introduced in the output layer <ref> [9] </ref>. In this case, orientation-selective receptive fields develop in the early, linear regime of development. Linsker's constraints, applied to a model with ON-and OFF-centre inputs, would fix the final percentages of ON and of OFF inputs in the receptive field.
Reference: [10] <author> K.D. </author> <title> Miller (1990). Correlation-based mechanisms of neural development, in Neuroscience and Connectionist Theory, M.A. </title> <editor> Gluck and D.E. Rumelhart, Eds., </editor> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsboro NJ, </address> <pages> 267-353. </pages>
Reference-contexts: In the absence of either of these features, only all-excitatory or all-inhibitory synaptic structures would develop. We discuss these features briefly here. A thorough discussion of correlation-driven learning rules as models of biological systems can be found in <ref> [10] </ref>. In biology, all the synapses from a single neuron are either exclusively positive or exclusively negative.
Reference: [11] <author> K.D. </author> <title> Miller (1990). Derivation of Linear Hebbian Equations from a Nonlinear Hebbian Model of Synaptic Plasticity. </title> <note> To appear in Neural Computation 2(3). </note>
Reference-contexts: However, for such a sum to be described by a simple equation like equation (3), the following conditions must hold <ref> [11] </ref>: the rules of cortical activation and of Hebbian plasticity must be linear; the two populations must be statistically indistinguishable in their connectivities and patterns of activity, so that a single covariance function describes both the covariance within each population and the covariance between the populations; and the positive and negative <p> However, many feedforward projections, such as the retinogeniculate and geniculocortical projections in the mammalian visual system, are exclusively excitatory. Furthermore, where excitatory and inhibitory projections do coexist, they are not likely to be equivalent (discussed in <ref> [11] </ref>): excitatory and inhibitory populations often have distinct patterns of connectivity and of activation; and there is currently no evidence that inhibitory synapses can be modified by Hebbian rules. <p> One can derive a linear Hebb rule like Linsker's without the use of negative synapses by studying the difference between the innervation strengths of two equivalent excitatory projections <ref> [11] </ref>. Biological examples include ON-centre and OFF-centre inputs [9] and left-eye and right-eye inputs [12] in the mammalian visual system.
Reference: [12] <author> K.D. Miller, J.B. Keller and M.P. </author> <month> Stryker </month> <year> (1989). </year> <title> Ocular dominance column development: analysis and simulation, </title> <booktitle> Science 245, </booktitle> <pages> 605-615. </pages>
Reference-contexts: One can derive a linear Hebb rule like Linsker's without the use of negative synapses by studying the difference between the innervation strengths of two equivalent excitatory projections [11]. Biological examples include ON-centre and OFF-centre inputs [9] and left-eye and right-eye inputs <ref> [12] </ref> in the mammalian visual system. In this case, however, the constants k 1 and k 2 disappear from the equation for the development of the difference of synaptic strengths because these constants take on equal values for each of the two equivalent populations.
Reference: [13] <author> F. </author> <title> Oberhettinger (1972). Tables of Bessel transforms, </title> <publisher> Springer. </publisher>
Reference-contexts: cos (l)F l (k), where F l (k) = 0 is the l-th order Hankel transform of f (r), and J l (x) is a Bessel function J l (x) = 0 cos (l xsin)d The inverse transform is symmetric: f (r) = 0 We use the following Hankel transforms <ref> [13, 2] </ref>: 19 f (r) F 0 (k) r 2 e r 2 =2B B 2 (2 Bk 2 )e Bk 2 =2 f (r) F l (k) f (ar) a 2 F l (k=a) D.1.2 Eigenfunctions We guess radial functions with the form of a gaussian e r 2 =2R
Reference: [14] <author> E. Oja. </author> <title> A Simplified Neuron Model as a Principal Component Analyzer, </title> <journal> J. Math Biology, </journal> <volume> 15, </volume> <year> 1982, </year> <pages> pp. 267-273. </pages>
Reference-contexts: The dynamical and biological bases for Linsker's results have not been clearly established. It has been pointed out that a Hebbian mechanism can perform certain types of principal component analysis <ref> [4, 14] </ref>, but Linsker's results do not represent principal components of the input statistics, as we shall demonstrate. In this paper, we present an analysis of the dynamical mechanisms responsible for Linsker's results.
Reference: [15] <author> E. </author> <month> Seneta </month> <year> (1973). </year> <title> Non-negative matrices, </title> <publisher> Wiley. </publisher>
Reference-contexts: The subsequent eigenvectors of Q become important as k 1 and k 2 are varied. 3.1 Eigenvectors of non-negative covariance matrices In general, where there are only positive correlations in the inputs, Q is a non-negative matrix, and the Perron-Frobenius Theorem <ref> [15, page 1] </ref> holds: Theorem 1 For a matrix whose entries are all non-negative, the components of the principal eigenvector all have the same sign.
Reference: [16] <author> D.S. </author> <title> Tang (1989). Information-theoretic solutions to early visual information processing: An alytic results. </title> <journal> Phys. Rev. A, </journal> <volume> 40(11), </volume> <pages> 6626-6635. 38 </pages>
Reference-contexts: Tang <ref> [16] </ref> has independently derived these analytic results, and also developed approximations for the eigenfunctions for nonzero k 2 . 3.5 Summary for k 1 = 0, k 2 = 0 For k 1 = 0, k 2 = 0, the dynamics are dominated by the principal eigenfunction, 1s, in which all <p> So the regime k 2 ! 1, k 1 = 0 will be dominated by oriented bi-lobed weight structures, and the circular symmetry is broken. 6 Tang <ref> [16] </ref> showed that there is an intermediate regime of small negative k 2 in which the principal eigenfunction has centre-surround structure.
References-found: 16

