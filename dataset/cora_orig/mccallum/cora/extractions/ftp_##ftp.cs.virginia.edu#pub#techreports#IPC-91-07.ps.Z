URL: ftp://ftp.cs.virginia.edu/pub/techreports/IPC-91-07.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Performance of the iPSC/860 Node Architecture  
Author: Steven A. Moyer 
Pubnum: IPC-TR-91-007  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Baumann, R. </author> <title> Sparseness in Power Systems Equations, Large Sparse Sets of Linear Equations: </title> <booktitle> Proc. Oxford Conf. </booktitle> <institution> Inst. Math. Appl. </institution> - <address> April 1970, </address> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1971, </year> <pages> pp 105-126. </pages>
Reference-contexts: element load with every reuse. 5.2 Completely Sparse Matrix-Vector Multiply A completely sparse matrix is a matrix with no discernible structure and a relatively large percentage of zero elements in each row and column; such matrices arise frequently in engineering problems, for example in the analysis of power distribution systems <ref> [1] </ref>.
Reference: [2] <author> Bokhari, S. </author> <title> Communication Overhead on the Intel iPSC/860 Hypercube, </title> <institution> ICASE, NASA Langley Research Center, Hampton, VA. </institution>
Reference: [3] <author> George, A., and Liu, J. </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981, </year> <pages> pp 138-152. </pages>
Reference: [4] <author> Intel Corporation, </author> <title> i860 64-Bit Microprocessor Hardware Reference Manual, </title> <address> ISBN 1-55512-106-3, </address> <year> 1989. </year>
Reference: [5] <author> Intel Corporation, </author> <title> i860 64-Bit Microprocessor Programmers Reference Manual, </title> <address> ISBN 1-55512-080-6, </address> <year> 1989. </year>
Reference: [6] <author> Nugent, S. </author> <title> The iPSC/2 Direct Connect Communications Technology, </title> <booktitle> Proc. 3rd Conf. on Hypercube Concurrent Comp. and Appl., ACM, </booktitle> <address> New York, NY, </address> <year> 1988, </year> <pages> pp 51-60. </pages>
Reference: [7] <author> Lawson, C., Hanson, R., Kincaid, D., and Krogh, F. </author> <title> Basic Linear Algebra Subprograms for Fortran Usage, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 5, 3, </volume> <month> Sept. </month> <year> 1979, </year> <pages> pp 308-323. </pages>
Reference-contexts: For vector operations which do not cache operands, asymptotic performance is equivalent to static performance; only static performance measurements are presented. Both static and asymptotic performance measurements are given in terms of millions of oating-point operations per second (Mops). 20 4.3 DAXPY Operation The BLAS routine daxpy <ref> [7] </ref> implements a double-precision vector operation of the form: where x and y are vectors and a is a scalar. The graph of Figure 6 depicts the asymptotic and static performance of the vector operation daxpy_y* and the static performance of the vector operations daxpy_1 and daxpy_2.
Reference: [8] <author> Ortega, J. </author> <title> Introduction to Parallel and Vector Solution of Linear Systems, </title> <publisher> Plenum Press, </publisher> <address> New York, NY, </address> <year> 1988, </year> <pages> pp 50-53. </pages>
Reference-contexts: A matrix-vector multiply algorithm which operates on the diagonals of a matrix is described in <ref> [8] </ref>; given an matrix A and a vector x of length n, the product Ax can be represented as: In (EQ 3), A i x i+1 and A -i x n-i are elementwise vector multiplications where the vectors A i are the diagonals of A; A 0 denotes the main diagonal,
Reference: [9] <author> Ortega, J., and Poole, W. </author> <title> An Introduction to Numerical Methods for Differential Equations, </title> <publisher> Pitman Publishing, </publisher> <address> Marshfield, MA, </address> <year> 1981, </year> <pages> pp 268-275. </pages>
Reference-contexts: of matrix, storage schemes and implementation issues are discussed for optimizing performance on the RX-1. 5.1 Diagonally Sparse Matrix-Vector Multiply A diagonally sparse matrix is a matrix composed of a relatively few non-zero diagonals; such matrices arise frequently in practice, for example in the discretization of elliptic partial differential equations <ref> [9] </ref>. Due to the relatively large percentage of zero elements in each row and column of a diagonally sparse matrix, it is common practice to store only the non-zero diagonals; in performing operations involving such matrices, the diagonals become the natural vectors.
Reference: [10] <author> Scott, D., and Withers, G. </author> <title> Performance and Assembly Language Programming of the iPSC/860 System (Preliminary), Intel Scientific Computers, </title> <address> Beaver-ton, OR, </address> <year> 1990. </year>
Reference-contexts: For example, the RX-1 DRAM controller supports the pipelining of read requests to partially mask row select and write-read mode switch latencies. A state transition diagram describing the operation of the RX-1 DRAM controller can be found in <ref> [10] </ref>; characteristics of the controller which affect memory system performance are discussed in subsequent subsections.
Reference: [11] <author> Sedra, A., and Smith, K. </author> <title> Microelectronic Circuits, </title> <publisher> CBS College Publishing, </publisher> <address> New York, NY, </address> <year> 1982, </year> <pages> pp 748-753. </pages>
Reference: [12] <author> Serang, O. </author> <title> Various Features of Fujitsu DRAMs, MOS Memory Products, </title> <institution> Fujitsu Microelectronics, </institution> <year> 1989, </year> <pages> pp 1 3-24. </pages>
Reference: [13] <author> Tewarson, R. </author> <title> Sparse Matrices, </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1973, </year> <pages> pp 1-11. </pages>
Reference: [14] <author> Zilka, A., </author> <title> Intel Scientific Computers, </title> <type> personal communication, </type> <month> March 22, </month> <year> 1991. </year>
Reference-contexts: The explanation for this discrepancy lies in the implementation of the i860 cache line fill procedure; between the fourth memory access of the previous cache line fill and the first memory access of the subsequent cache line fill there is a delay of 7 cycles <ref> [14] </ref>. As discussed in 3.2.3, this memory access delay between consecutive cache line fills is sufficient to allow the DRAM controller to initiate an idle-state transition; the net result is that the first of the 4 memory accesses in each line load is a far read.
References-found: 14

