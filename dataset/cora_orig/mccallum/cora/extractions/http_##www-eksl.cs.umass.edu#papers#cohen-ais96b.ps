URL: http://www-eksl.cs.umass.edu/papers/cohen-ais96b.ps
Refering-URL: http://www.cs.umass.edu/~jensen/papers/ais97b.html
Root-URL: 
Email: cohen,jensen@cs.umass.edu  
Title: Overfitting Explained  
Author: Paul R. Cohen, David Jensen 
Date: October 31, 1996  
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science, University of Massachussetts  
Abstract: Overfitting arises when model components are evaluated against the wrong reference distribution. Most modeling algorithms iteratively find the best of several components and then test whether this component is good enough to add to the model. We show that for independently distributed random variables, the reference distribution for any one variable underestimates the reference distribution for the the highest-valued variable; thus variate values will appear significant when they are not, and model components will be added when they should not be added. We relate this problem to the well-known statistical theory of multiple comparisons or simultaneous inference. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Kotz, S. </author> <title> and N.L. </title> <editor> Johnson (Eds.) </editor> <booktitle> (1982-1989). Encyclopedia of Statistical Sciences. </booktitle> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: Alternatively, one might adjust the critical value in the F i reference distribution to ensure that the probability of falsely rejecting the null hypothesis on the basis of x max is, say, .1 as desired. This approach is reminiscent of the Bonferroni adjustment, and it works quite well <ref> [1, 2, 4] </ref>, although the adjustment tends to be conservative, especially when the variables are not i. i. d. <p> The upshot of this result is that we may apply techniques developed for problems of multiple comparisons (such as the Bonferroni adjustment) to the overfitting problem <ref> [2, 1] </ref>. All these techniques adjust T V to account for the fact that we are testing not one, but the best of several, model components.
Reference: [2] <author> Miller, Rupert G. </author> <year> (1981). </year> <title> Simultaneous Statistical Inference. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Alternatively, one might adjust the critical value in the F i reference distribution to ensure that the probability of falsely rejecting the null hypothesis on the basis of x max is, say, .1 as desired. This approach is reminiscent of the Bonferroni adjustment, and it works quite well <ref> [1, 2, 4] </ref>, although the adjustment tends to be conservative, especially when the variables are not i. i. d. <p> The upshot of this result is that we may apply techniques developed for problems of multiple comparisons (such as the Bonferroni adjustment) to the overfitting problem <ref> [2, 1] </ref>. All these techniques adjust T V to account for the fact that we are testing not one, but the best of several, model components.
Reference: [3] <author> Jensen, D. </author> <year> (1992). </year> <title> Induction with Randomization Testing: Decision-Oriented Analysis of Large Data Sets, </title> <type> Doctoral Dissertation, </type> <institution> Sever Institute of Technology, Wash-ington University, St. Louis, Missouri. </institution>
Reference-contexts: To avoid overfitting, simply replace F i with F max in the procedure, above. To do this, one must estimate F max , which is easy to do by randomization, bootstrapping or some other Monte Carlo procedure <ref> [3, 8] </ref>. Figure 1 shows that a randomized estimate of F max controls overfitting perfectly.
Reference: [4] <author> Jensen, D. </author> <year> (1997). </year> <title> Adjusting for multiple testing in decision tree pruning. </title> <booktitle> Accepted to the Sixth International Workshop on Artificial Intelligence and Statistics (poster). </booktitle>
Reference-contexts: It will then be obvious how the problem of overfitting is a version of the classical statistical problem of multiple comparisons. This equivalence suggests numerous overfitting-avoidance techniques, which have been tested empirically (see <ref> [4] </ref>). 2 The Distribution of the Maximum Score Recall that a score is an evaluation of a component c i that IMA is considering adding to a model m (): x i = V (c i ; m (); S). <p> We have not extended the results to non-independent variables. However, empirically we have shown that the errors introduced by non-independence are small relative to the errors incurred by not using the reference distribution for the maximum (see Figure 1 and <ref> [4] </ref>). For simplicity and concreteness, assume x 1 and x 2 are random variables drawn from a uniform distribution of integers (0 : : : 6). The distribution of max (x 1 ; x 2 ) is shown in table 1. <p> One can easily build models in which most of the components shouldn't be there. Decision tree induction algorithms, for instance, are exquisitely prone to overfitting <ref> [4] </ref>. incorrectly. In each trial, ten binary attributes with equal class probability and 50 instances were compared to a randomly-generated binary classification variable. The scores for these attributes, x 1 ; : : :; x 10 , measure strength of association between the attribute and the binary classification variable. <p> Alternatively, one might adjust the critical value in the F i reference distribution to ensure that the probability of falsely rejecting the null hypothesis on the basis of x max is, say, .1 as desired. This approach is reminiscent of the Bonferroni adjustment, and it works quite well <ref> [1, 2, 4] </ref>, although the adjustment tends to be conservative, especially when the variables are not i. i. d. <p> While it prevents overfitting, it also prevents us adding any model components. See <ref> [4] </ref> for details. 4 Underestimation and Multiple Comparisons The Bonferroni adjustment is popular for problems involving multiple comparisons, or simultaneous inference. There is a direct mapping from the problem of estimating the distribution of the maximum to the problem of multiple or simultaneous comparisons.
Reference: [5] <author> Kass, G.V. </author> <year> (1980). </year> <title> An exploratory technique for investigating large quantities of categorical data. </title> <journal> Applied Statistics 29(2) </journal> <pages> 199-127. </pages>
Reference: [6] <author> J. </author> <title> Galambos (1978). The Asymptotic Theory of Extreme Order Statistics. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: In the general case, where x 1 ; x 2 ; : : : ; x n are dependent, the probability P r (max (x 1 i ; x 2 j ; : : : ; x n m ) k) is not so easy to estimate (but see <ref> [6] </ref>). It is not simply a product of probabilities, as in expressions 2 and 4, because P r (a; b) 6= P r (a)P r (b) when a and b are dependent.
Reference: [7] <author> Oates, T. and D. </author> <title> Jensen (1997). The effects of training set size on decision tree complexity. </title> <booktitle> Accepted to the Sixth International Workshop on Artificial Intelligence and Statistics. </booktitle>
Reference: [8] <author> Cohen, P.R. </author> <year> (1995). </year> <title> Empirical Methods for Artificial Intelligence. </title> <address> Cambridge, MA: MIT Press. </address> <note> 5 Acknowledgements This research is supported by DARPA/Rome Laboratory contract F30602-93-C-0076 and by a Subcontract from Sterling Software Inc. </note> <author> 7335-UOM-001 (DARPA/Rome Laboratory F30602-95-C-0257). </author> <title> The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation hereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements either expressed or implied, </title> <institution> of the Defense Advanced Research Projects Agency, Rome Laboratory or the U.S. Government. </institution>
Reference-contexts: To avoid overfitting, simply replace F i with F max in the procedure, above. To do this, one must estimate F max , which is easy to do by randomization, bootstrapping or some other Monte Carlo procedure <ref> [3, 8] </ref>. Figure 1 shows that a randomized estimate of F max controls overfitting perfectly.
References-found: 8

