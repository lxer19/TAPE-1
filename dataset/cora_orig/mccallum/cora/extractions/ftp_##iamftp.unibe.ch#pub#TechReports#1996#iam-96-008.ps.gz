URL: ftp://iamftp.unibe.ch/pub/TechReports/1996/iam-96-008.ps.gz
Refering-URL: 
Root-URL: 
Title: An Experimental Study of the Optimal Class-Selective Rejection Rule  
Author: Thien M. HA 
Keyword: CR Categories and Subject Descriptors: I.5.0 [Pattern Recognition]: General; I.5.1 [Pattern Recognition]: Models; I.5.2 [Pattern Recognition]: Design Methodol ogy; I.5.m [Pattern Recognition]: Decision. Key Words: classification, decision rule, Bayes rule, Chow's rule, neural networks.  
Address: Neubruckstr. 10, CH-3012 Berne, Switzerland  
Affiliation: University of Berne Institut fur Informatik und Angewandte Mathematik  
Email: E-Mail: haminh@iam.unibe.ch  
Phone: Phone: +41 31 631 86 99 Fax.: +41 31 631 39 65  
Date: March 11, 1996  
Abstract: This report reviews various class-selective rejection rules for pattern recognition. A rejection rule is called class-selective if it does not reject an ambiguous pattern from all classes but only from those classes that are most unlikely to issue the pattern. Both optimal and suboptimal rules, e.g. top-n ranking, are considered. Experimental comparisons performed on the recognition of isolated numerals from the NIST databases show that the optimal class selective rejection rule is actually better than two other heuristic rules. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C.K. Chow, </author> <title> "An Optimum Character Recognition System Using Decision Functions," </title> <journal> Institute of Radio Engineers (IRE) Transactions on Electronic Computers, </journal> <volume> Vol. EC-6, No. 4, </volume> <pages> pp. 247-254, </pages> <month> December </month> <year> 1957. </year>
Reference-contexts: It is known that this rule is optimal in the sense that no other rules can yield a lower error probability, or error rate. The Bayes rule has also been modified by Chow to cope with a reject option <ref> [1, 2] </ref>. The idea is that when a pattern lies on or near a separation plane between two classes, the assignment to one or the other class is merely a guess.
Reference: [2] <author> C.K. Chow, </author> <title> "On Optimum Recognition Error and Reject Tradeoff," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. IT-16, No. 1, </volume> <pages> pp. 41-46, </pages> <month> January </month> <year> 1970. </year>
Reference-contexts: It is known that this rule is optimal in the sense that no other rules can yield a lower error probability, or error rate. The Bayes rule has also been modified by Chow to cope with a reject option <ref> [1, 2] </ref>. The idea is that when a pattern lies on or near a separation plane between two classes, the assignment to one or the other class is merely a guess.
Reference: [3] <author> P.A. Devijver, </author> <title> "Error and Reject Tradeoff for Nearest Neighbor Decision Rules," </title> <editor> in G. Tacconi (Ed.) </editor> <booktitle> Aspects of Signal Processing,, Part 2, </booktitle> <address> D. </address> <publisher> Reidel Publishing Company, Dordrecht-Holland, </publisher> <pages> pp. 525-538, </pages> <year> 1977. </year>
Reference-contexts: The well-known Bayes rule is a typical example, where it is implicitly assumed that the posterior probabilities are exactly known. Note that some authors take into account the classification method in the design of the decision rule <ref> [7, 3] </ref>. However, the former approach - optimise the decision rule assuming that the posterior probabilities are known still provides the theoretical limit that the latter attempts to reach.
Reference: [4] <author> R.O. Duda and P.E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: (i=x) through the Bayes formula: P i (x) P (i=x) = p (x) where p (x=i) is the i th class conditional probability density function (p.d.f.), i is the a priori probability of observing the i th class, P N p (x) = j=1 is the absolute probability density function <ref> [4, 5] </ref>. It follows immediately that the posterior probabilities sum up to 1, i.e., N X P i (x) = 1 (3) The connection between classification and decision is illustrated in Fig. 1, for a three-class problem. <p> In most practical applications, fP i (x); i = 1; ::; N g are unknown but can be estimated from a set of labelled patterns, called training or learning set. Many estimation methods exist, e.g. Parzen estimate, nearest neighbour, potential functions, and neural networks <ref> [4, 5, 8] </ref>. When the estimated functions are used instead of the true (unknown) functions, the optimality of the decision rule is no longer guaranteed.
Reference: [5] <author> K. Fukunaga, </author> <title> Introduction to Statistical Pattern Recognition, second edition, </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: (i=x) through the Bayes formula: P i (x) P (i=x) = p (x) where p (x=i) is the i th class conditional probability density function (p.d.f.), i is the a priori probability of observing the i th class, P N p (x) = j=1 is the absolute probability density function <ref> [4, 5] </ref>. It follows immediately that the posterior probabilities sum up to 1, i.e., N X P i (x) = 1 (3) The connection between classification and decision is illustrated in Fig. 1, for a three-class problem. <p> In most practical applications, fP i (x); i = 1; ::; N g are unknown but can be estimated from a set of labelled patterns, called training or learning set. Many estimation methods exist, e.g. Parzen estimate, nearest neighbour, potential functions, and neural networks <ref> [4, 5, 8] </ref>. When the estimated functions are used instead of the true (unknown) functions, the optimality of the decision rule is no longer guaranteed.
Reference: [6] <author> Thien M. Ha, </author> <title> "An Optimum Decision Rule for Pattern Recognition," </title> <type> Technical Report IAM-95-009, </type> <institution> Institute of Computer Science and Applied Mathematics, University of Berne, Switzerland, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Therefore it is important to test a decision rule not only under ideal conditions but also under realistic ones, in which estimated posterior probabilities obtained from training patterns are used. In this report, we test a newly introduced decision rule, namely the optimum class-selective rejection rule <ref> [6] </ref>. <p> In order to define the optimality of the class-selective rejection rule while avoiding the trivial partition, an additional constraint the average number of classes n - was introduced <ref> [6] </ref>. n = X where n (x) is the number of classes assigned to pattern x. <p> If there exist no such classes, the rule simply selects the (a) single best class <ref> [6] </ref>. The domain of the pre-specified threshold is 0 t 2 When t = 1 2 , the rule is equivalent to Bayes rule, i.e., select the (a) single best class. When t = 0, we obtain the trivial partition of the pattern space.
Reference: [7] <author> M.E. Hellman, </author> <title> "The Nearest Neighbor Classification Rule with a Reject Option," </title> <journal> IEEE Transactions on Systems, Science, and Cybernetics, </journal> <volume> Vol. SSC-6, No. 3, </volume> <pages> pp. 179-185, </pages> <month> July </month> <year> 1970. </year>
Reference-contexts: The well-known Bayes rule is a typical example, where it is implicitly assumed that the posterior probabilities are exactly known. Note that some authors take into account the classification method in the design of the decision rule <ref> [7, 3] </ref>. However, the former approach - optimise the decision rule assuming that the posterior probabilities are known still provides the theoretical limit that the latter attempts to reach.
Reference: [8] <editor> C.G.Y. Lau (Editor), </editor> <booktitle> Neural Networks: Theoretical Foundations and Analysis, </booktitle> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: In most practical applications, fP i (x); i = 1; ::; N g are unknown but can be estimated from a set of labelled patterns, called training or learning set. Many estimation methods exist, e.g. Parzen estimate, nearest neighbour, potential functions, and neural networks <ref> [4, 5, 8] </ref>. When the estimated functions are used instead of the true (unknown) functions, the optimality of the decision rule is no longer guaranteed. <p> See Appendix for details about the two feature extraction methods. Both neural networks are trained with the back-propagation algorithm <ref> [8] </ref>. Both neural networks were trained on the first 40000 numerals from SD3 and the next 10000 numerals were used to control the stopping of the training process.
Reference: [9] <author> R. Sedgewick, </author> <title> Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: Unlike the optimum rule, the constant risk rule needs sorting, so the time complexity is O (N log (N )) on the average and O (N 2 ) in the worst case <ref> [9] </ref>. 2.3 Top-n Ranking Rule The top-n ranking rule works as follows. For n = 1, select the (a) single best class, i.e., using the Bayes rule. For n = 2, select the best and the second best classes, and so on.
Reference: [10] <author> R.A. Wilkinson, J. Geist, S. Janet, P.J. Grother, C.J.C. Burges, R. Creecy, B. Hammond, J.J. Hull, N.W. Larsen, T.P. Vogl, and C.L. Wilson, </author> <title> The First Census Optical Character Recognition Systems Conference, The U.S. </title> <institution> Bureau of Census and the National Institute of Standards and Technology, </institution> <type> Technical Report #NISTIR 4912, </type> <address> Gaithersburg, MD, </address> <month> Aug. </month> <year> 1992. </year> <month> 13 </month>
Reference-contexts: The comparison is based on the error-(average number of classes) tradeoff curves. 3.1 Database Two databases, namely, SD3 and SD7, were provided by the American National Institute of Standards and Technology (NIST) in 1992 as parts of a conference to assess the state-of-the-art in isolated handwritten character recognition <ref> [10] </ref>. Twenty-nine groups from Europe and North America participated to compare the performance of their OCR systems. In total, 47 systems, both commercial and research, were presented. The databases contain isolated numerals (digits) as well as upper- and 7 8 lower-case letters.
References-found: 10

