URL: http://www.media.mit.edu/~nitin/projects/NomadicRadio/ISWC98/ISWC98.ps
Refering-URL: http://www.media.mit.edu/~nitin/projects/NomadicRadio/
Root-URL: http://www.media.mit.edu
Title: Speaking and Listening on the Run: Design for Wearable Audio Computing partial solution to information
Author: Nitin Sawhney and Chris Schmandt 
Keyword: Portable computing devices  
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: Speech Interface Group, MIT Media Laboratory  
Note: Proceedings of ISWC98, International Symposium on Wearable Computing, 19-20 October 1998 in Pittsburgh, Pennsylvania. Copyright 1998 IEEE. 1  1: Introduction A  should be made available to users  1  can download messages  
Email: nitin, geek-@media.mit.edu  ed.htm  
Web: http://www.pitneybowes.com/pbi/whatsnew/releases/workers_overwhelm  
Abstract: The use of speech and auditory interaction on wearable computers can provide an awareness of events and personal messages, without requiring ones full attention or disrupting the foreground activity. A passive hands-and-eyes-free approach is appropriate when users need convenient and timely access to remote information and communication services. Nomadic Radio is a distributed computing platform for wearable access to unified messaging via an auditory interface. We demonstrate the use of auditory cues, spatialized audio, and speech I/O in the wearable interface for passive awareness, scaleable notification and navigation/control. The architecture is designed for wired audio wearables and has been extended for distributed wireless operation. In an information rich environment, people access a multitude of content such as news, weather, stock reports, and data from a variety of information sources. People increasingly communicate via services such as email, fax, and telephony. Such a growth in information and communication options is fundamentally changing the workplace and beginning to have a seismic effect on peoples professional and personal lives (see the recent Pitney Bowes Study, April 8, 1997 information such as email, voice mail, news and scheduling, using digitized audio and synthesized speech. However telephones primarily operate on a synchronous model of communication, requiring availability of both parties, and high tolls for accessing services since a connection must be maintained while listening to the news or stock report. All processing must be done on the telephony servers, rather than the phone itself. The requirement of synchronous connection prevents the device from continuously sensing its user and environment. Several wearable computing projects have considered the use of speech and audio in the interface. Ubiquitous Talker [14] is a camera-enabled system that provides information related to recognized physical objects using a display and synthesized voice, and accepts queries via speech input. A prototype augmented audio tour guide [3] presented digital audio recordings indexed by the spatial location of visitors in a museum. SpeechWear [17] enabled users to perform data entry and retrieval using speech recognition and synthesis. A speech-enabled web browser allowed users to access local and remote documents through a wireless link. Audio Aura [13] explored the use of background auditory cues to provide serendipitous information coupled with people's physical location in the workplace. In a recent paper [22], researchers suggest the use of sensors and user modeling to allow wearables to infer when users should be interrupted by incoming messages. They suggest an approach based on waiting for a break in the conversation to post a summary of an urgent message onto the user's heads-up display. 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> Barry Arons. </author> <title> A Review of the Cocktail Party Effect. </title> <journal> Journal of American Voice I/O Society, </journal> <volume> Vol. 12, </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: This well known cognitive phenomenon, called the "Cocktail Party Effect" <ref> [1] </ref>, provides the justification that humans can in fact monitor several audio streams simultaneously, selectively focusing on any one and 3 placing the rest in the background. A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources [26]. <p> Yet the cognitive load of listening to simultaneous channels increases with the number of channels. Experiments show that increasing the number of channels beyond three causes a degradation in comprehension [25]. Bregman [4] claims that stream segregation is better when frequency separation is greater between sound streams. Arons <ref> [1] </ref> suggests that the effect of spatialization can be improved by allowing listeners to easily switch between channels and pull an audio stream into focus, as well as by allowing sufficient time to fully fuse the audio streams. <p> As the message is pulled in, its spatial direction is maintained allowing the listener to retain a sense of message arrival time. This spatial continuity is important for discriminating and holding the auditory streams together <ref> [1] </ref>. Spatial Scanning Sometimes listeners want to get a preview of all their messages quickly without manually selecting and playing each one. This is similar to the scan feature on modern radio tuners.
Reference: [2] <author> Barry Arons. SpeechSkimmer: </author> <title> Interactively Skimming Recorded Speech. </title> <booktitle> Proceedings of UIST 93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Speech is fast for the author but slow and tedious for the listener. Speech is sequential and exists only temporally; the ear cannot browse around a set of recordings the way the eye can scan a screen of text and images. Hence techniques such as interactive skimming <ref> [2] </ref>, nonlinear access and indexing [15, 24] and audio spatialization [10, 21] must be considered for browsing audio. Many of these audio techniques can be used to augment existing visual wearable interfaces.
Reference: [3] <author> Bederson, Benjamin B. </author> <title> Audio Augmented Reality: A Prototype Automated Tour Guide. </title> <booktitle> Proceedings of CHI 95, </booktitle> <month> May </month> <year> 1996, </year> <pages> pp. 210-211. </pages>
Reference-contexts: Several wearable computing projects have considered the use of speech and audio in the interface. Ubiquitous Talker [14] is a camera-enabled system that provides information related to recognized physical objects using a display and synthesized voice, and accepts queries via speech input. A prototype augmented audio tour guide <ref> [3] </ref> presented digital audio recordings indexed by the spatial location of visitors in a museum. SpeechWear [17] enabled users to perform data entry and retrieval using speech recognition and synthesis. A speech-enabled web browser allowed users to access local and remote documents through a wireless link.
Reference: [4] <author> Bregman, Albert S. </author> <title> Auditory Scene Analysis: The Perceptual Organization of Sound. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Yet the cognitive load of listening to simultaneous channels increases with the number of channels. Experiments show that increasing the number of channels beyond three causes a degradation in comprehension [25]. Bregman <ref> [4] </ref> claims that stream segregation is better when frequency separation is greater between sound streams.
Reference: [5] <author> Chalfonte, B.L., Fish, </author> <title> R.S. and Kraut, R.E. "Expressive richness: A comparison of speech and text as media for revision". </title> <booktitle> Proceedings of CHI'92, </booktitle> <pages> pp. 21-26. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: speech recognition in the interface and consider design solutions for two different wearable configurations. 2.3: Expressive and Efficient Interaction Voice is considered more expressive and efficient than text, as it places less cognitive demands on the speaker and permits more attention to be devoted to the content of the message <ref> [5] </ref>. The intonation in human voice also provides many implicit hints about the message content. VoiceCues (short audio signatures) played as notifications in Nomadic Radio, imply the sender of email messages in a quick and unobtrusive manner. Speech provides a natural and direct means for capturing users input.
Reference: [6] <author> Clarkson, Brian and Alex Pentland. </author> <title> "Extracting Context from Environmental Audio". </title> <booktitle> Proceedings of the International Symposium on Wearable Computing, IEEE, </booktitle> <month> October </month> <year> 1998. </year>
Reference-contexts: We are currently developing a system to infer environmental context by audio classification <ref> [6] </ref>. Our initial efforts have focused on discriminating speech from ambient noise in the environment. In the future, a rich classification of background sounds will help establish user context [19] in 8 offices, classrooms, and the outdoors.
Reference: [7] <author> Cohen, J. </author> <title> Monitoring background activities. In Auditory Display: Sonification, Audification, and Auditory Interfaces. </title> <address> Reading MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Speech and music in the background and peripheral auditory cues can provide an awareness of messages or signify events, without requiring ones full attention or disrupting their foreground activity. Audio easily fades into the background, but users are alerted when it changes <ref> [7] </ref>. In Nomadic Radio, ambient auditory cues are used to convey events and changes in background activity. 2.5: Simultaneous Listening It is possible for listeners to attend to multiple background processes via the auditory channel as long as the sounds representing each process are distinguishable.
Reference: [8] <author> Gardner, W. G., and Martin, K. D. </author> <title> HRTF measurements of a KEMAR. </title> <journal> Journal of the Acoustical. Society of America, </journal> <volume> 97 (6), </volume> <year> 1995, </year> <pages> pp. 3907-3908. </pages>
Reference-contexts: The perceptual audio models used in RSX 3D, are based on a set of head-related transfer function (HRTF) measurements of a KEMAR (electronic mannequin) done by Bill Gardner at the Media Lab <ref> [8] </ref>. The measurements consist of the left and right ear impulse responses from a loudspeaker mounted 1.4 meters from the KEMAR.
Reference: [9] <author> Gaver, W. </author> <title> The Sonic Finder: An interface that uses auditory icons. </title> <booktitle> Human Computer Interaction, </booktitle> <volume> 4 </volume> <pages> 67-94, </pages> <year> 1989. </year>
Reference-contexts: Synthetic speech can be distracting to listeners while they are performing other tasks or in conversation. On the other hand, nonspeech audio in the form of auditory icons <ref> [9] </ref> or cues can provide such feedback via short everyday sounds. Auditory cues are a crucial means for conveying awareness, notification and providing necessary assurances in a nonvisual interface.
Reference: [10] <author> Kobayashi, Minoru and Chris Schmandt. </author> <title> Dynamic Soundscape: Mapping Time to Space for Audio Browsing. </title> <booktitle> Proceedings of CHI 97, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: Speech is sequential and exists only temporally; the ear cannot browse around a set of recordings the way the eye can scan a screen of text and images. Hence techniques such as interactive skimming [2], nonlinear access and indexing [15, 24] and audio spatialization <ref> [10, 21] </ref> must be considered for browsing audio. Many of these audio techniques can be used to augment existing visual wearable interfaces. <p> The effective use of spatial layout can be used to aid auditory memory. The AudioStreamer [21] detects the gesture of head movement towards spatialized audio-based news sources to increase the relative gain of the source, allowing simultaneous browsing and listening of several news articles. Kobayashi <ref> [10] </ref> introduced a technique for browsing audio by allowing listeners to switch their attention between moving sound sources that play multiple portions of a single audio recording. On a wearable device, spatial audio requires the use of headphones or shoulder mounted directional speakers.
Reference: [11] <author> Martin, G.L. </author> <title> The utility of speech input in user interfaces. </title> <journal> International Journal of Man Machine Studies, </journal> <volume> 30 </volume> <pages> 355-375, </pages> <year> 1989. </year>
Reference-contexts: There are situations in which the users eyes are busy although she is otherwise able to attend to information from her wearable computer, such as when walking or driving. An eyes-free approach, using audio-based augmentation allows the user to simultaneously perform other tasks while listening or speaking <ref> [11] </ref>. Such an interface can be unobtrusive and designed to be discreet.
Reference: [12] <author> Marx, Matthew and Chris Schmandt. </author> <title> CLUES: Dynamic Personalized Message Filtering. </title> <booktitle> Proceedings of CSCW 96, </booktitle> <pages> pp. 113-121, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: hourly updates of ABC News, personal calendar, weather, and traffic reports. +RXUO"fl1HZV $%&fl5DGLR (YHQWfl5HPLQGHUV IURPfl'HVNWRS &DOHQGDU 5DGLRfl6HUYHU 1RPDGLF &OLHQW 3RVLWLRQfl6HUYHU &DOHQGDU 9PDLO 1HZV 6SHHFK 6HUYHU 56;fl' 6SDWLDOfl$XGLR ,56HQVRUV :HEfl6HUYHU 1RPDGLFfl5DGLR ,56HQVRU (PDLO &/8 (6 )LOWHULQJ 3ULRULW" communication between remote server processes and the Nomadic Client Content-based email filtering using CLUES <ref> [12] </ref>, a filtering and prioritization system, has been integrated in Nomadic Radio. CLUES determines the timely nature of messages by finding correlation between a user's calendar, rolodex, to-do list, as well as a record of outgoing messages and phone calls.
Reference: [13] <author> Mynatt, E.D., Back, M., Want, R. and Frederick, R. </author> <title> Audio Aura: LightWeight Audio Augmented Reality. </title> <booktitle> Proceedings of UIST '97, </booktitle> <address> Banff, Canada, Oct 15-17, </address> <year> 1997. </year>
Reference-contexts: SpeechWear [17] enabled users to perform data entry and retrieval using speech recognition and synthesis. A speech-enabled web browser allowed users to access local and remote documents through a wireless link. Audio Aura <ref> [13] </ref> explored the use of background auditory cues to provide serendipitous information coupled with people's physical location in the workplace. In a recent paper [22], researchers suggest the use of sensors and user modeling to allow wearables to infer when users should be interrupted by incoming messages.
Reference: [14] <author> Rekimoto, Jun and Katashi Nagao. </author> <title> The World through the Computer: Computer Augmented Interaction with Real World Environments. </title> <booktitle> Proceedings of UIST ''95, </booktitle> <month> November 14-17, </month> <year> 1995, </year> <pages> pp. 29-38. </pages>
Reference-contexts: Wearable auditory displays can be used to enhance an environment with timely information and provide a sense of peripheral awareness of people and background events. Several wearable computing projects have considered the use of speech and audio in the interface. Ubiquitous Talker <ref> [14] </ref> is a camera-enabled system that provides information related to recognized physical objects using a display and synthesized voice, and accepts queries via speech input. A prototype augmented audio tour guide [3] presented digital audio recordings indexed by the spatial location of visitors in a museum.
Reference: [15] <author> Roy, Deb K. and Chris Schmandt. NewsComm: </author> <title> A HandHeld Interface for Interactive Access to Structured Audio. </title> <booktitle> Proceedings of CHI 96, </booktitle> <month> April </month> <year> 1996, </year> <pages> pp. 173-180. </pages>
Reference-contexts: Personal Digital Assistants (PDAs) offer the benefit of personal applications in a smaller size; however they generally utilize pen-based graphical user interfaces, which are not ideal when the users hands and eyes are busy. Handheld audio devices <ref> [24, 15] </ref> with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) [16]. <p> Speech is sequential and exists only temporally; the ear cannot browse around a set of recordings the way the eye can scan a screen of text and images. Hence techniques such as interactive skimming [2], nonlinear access and indexing <ref> [15, 24] </ref> and audio spatialization [10, 21] must be considered for browsing audio. Many of these audio techniques can be used to augment existing visual wearable interfaces.
Reference: [16] <author> Roy, Deb K., Nitin Sawhney, Chris Schmandt, Alex Pentland. </author> <title> Wearable Audio Computing: A Survey of Interaction Techniques MIT Media Lab Technical Report #434, </title> <month> April </month> <year> 1997. </year> <note> http://www.media.mit.edu/~nitin/NomadicRadio/AudioWearables.html </note>
Reference-contexts: Handheld audio devices [24, 15] with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) <ref> [16] </ref>. Wearable auditory displays can be used to enhance an environment with timely information and provide a sense of peripheral awareness of people and background events. Several wearable computing projects have considered the use of speech and audio in the interface.
Reference: [17] <author> Rudnicky, Alexander, Reed, S. and Thayer, E. SpeechWear: </author> <title> A mobile speech system. </title> <booktitle> Proceedings of ICSLP '96, </booktitle> <year> 1996. </year>
Reference-contexts: Ubiquitous Talker [14] is a camera-enabled system that provides information related to recognized physical objects using a display and synthesized voice, and accepts queries via speech input. A prototype augmented audio tour guide [3] presented digital audio recordings indexed by the spatial location of visitors in a museum. SpeechWear <ref> [17] </ref> enabled users to perform data entry and retrieval using speech recognition and synthesis. A speech-enabled web browser allowed users to access local and remote documents through a wireless link.
Reference: [18] <author> Sawhney, Nitin. </author> <title> Contextual Awareness, Messaging and Communication in Nomadic Audio Environments, M.S. </title> <type> Thesis, </type> <institution> Media Arts and Sciences, MIT Media Lab, </institution> <month> May </month> <year> 1998. </year>
Reference-contexts: The physical design and social affordances of the audio interface play an important role in determining how well the wearable will be adopted in certain situations. 3: Nomadic Radio: Wearable Audio Messaging Nomadic Radio <ref> [18] </ref> provides a unified audio interface to a number of remote information services. Messages such as email, voice mail, hourly news broadcasts, and personal calendar events are automatically downloaded to the device throughout the day.
Reference: [19] <author> Sawhney, Nitin. </author> <title> Situational Awareness from Environmental Sounds, </title> <type> MIT Media Lab Technical Report, </type> <month> June </month> <year> 1997. </year> <note> http://www.media.mit.edu/~nitin/papers/Env_Snds/EnvSnds.html </note>
Reference-contexts: We are currently developing a system to infer environmental context by audio classification [6]. Our initial efforts have focused on discriminating speech from ambient noise in the environment. In the future, a rich classification of background sounds will help establish user context <ref> [19] </ref> in 8 offices, classrooms, and the outdoors. We are integrating IR-based receivers on the wearable to provide positioning data to a Position Server (that tracks users and messages) via the Locust Swarm [23], a distributed IR location system at the Media Lab.
Reference: [20] <author> Schmandt, Chris. </author> <title> Multimedia Nomadic Services on Todays Hardware. </title> <journal> IEEE Network, </journal> <note> September/October 1994, pp12-21. </note>
Reference-contexts: Simple devices such as pagers provide a convenient form of alerting users to remote information. Such devices offer an extremely low-bandwidth for communication and the interface does not afford rich delivery of information content. Telephones are ubiquitous and cellular services offer mobility. Computer-based telephony services, such as Phoneshell <ref> [20] </ref>, offer subscribers integrated access to 1 http://www.pitneybowes.com/pbi/whatsnew/releases/workers_overwhelm ed.htm information such as email, voice mail, news and scheduling, using digitized audio and synthesized speech. <p> In the Speech Interface group, we have developed an environment <ref> [20] </ref> that allows subscribers at the MIT Media Lab to access information such as email, voice messages, weather, hourly news and calendar events using a variety of interfaces such as desktops, telephones, pagers, fax, and more recently on wearable platforms.
Reference: [21] <author> Schmandt, Chris and Atty Mullins. AudioStreamer: </author> <title> Exploiting Simultaneity for Listening. </title> <booktitle> Proceedings of CHI 95, </booktitle> <pages> pp. 218-219, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Speech is sequential and exists only temporally; the ear cannot browse around a set of recordings the way the eye can scan a screen of text and images. Hence techniques such as interactive skimming [2], nonlinear access and indexing [15, 24] and audio spatialization <ref> [10, 21] </ref> must be considered for browsing audio. Many of these audio techniques can be used to augment existing visual wearable interfaces. <p> The effective use of spatial layout can be used to aid auditory memory. The AudioStreamer <ref> [21] </ref> detects the gesture of head movement towards spatialized audio-based news sources to increase the relative gain of the source, allowing simultaneous browsing and listening of several news articles.
Reference: [22] <author> Starner, Thad, Steve Mann, Bradley Rhodes, Jeffery Levine, Jennifer Healey, Dana Kirsch, Rosalind Picard, and Alex Pentland, </author> <title> Augmented Reality through Wearable Computing. </title> <journal> Presence, </journal> <volume> Vol. 6, No. 4, </volume> <month> August </month> <year> 1997, </year> <pages> pp. 386-398. </pages>
Reference-contexts: A speech-enabled web browser allowed users to access local and remote documents through a wireless link. Audio Aura [13] explored the use of background auditory cues to provide serendipitous information coupled with people's physical location in the workplace. In a recent paper <ref> [22] </ref>, researchers suggest the use of sensors and user modeling to allow wearables to infer when users should be interrupted by incoming messages.
Reference: [23] <author> Starner, Thad and Dana Kirsch. </author> <title> The locust swarm: An environmentally-powered, networkless location and messaging system. </title> <booktitle> Proceedings of the International Symposium on Wearable Computing, IEEE, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: In the future, a rich classification of background sounds will help establish user context [19] in 8 offices, classrooms, and the outdoors. We are integrating IR-based receivers on the wearable to provide positioning data to a Position Server (that tracks users and messages) via the Locust Swarm <ref> [23] </ref>, a distributed IR location system at the Media Lab. Integration with Nomadic Radio will allow contextual notification based on location and auditory characteristics of the environment. As messages arrive, an appropriate notification level will be computed based on these factors, and weighted appropriately for high or low interruption.
Reference: [24] <author> Stifelman, Lisa J., Barry Arons, Chris Schmandt, Eric A. Hulteen. VoiceNotes: </author> <title> A Speech Interface for a HandHeld Voice Notetaker. </title> <booktitle> Proceedings of INTERCHI 93, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Personal Digital Assistants (PDAs) offer the benefit of personal applications in a smaller size; however they generally utilize pen-based graphical user interfaces, which are not ideal when the users hands and eyes are busy. Handheld audio devices <ref> [24, 15] </ref> with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) [16]. <p> Hence new I/O modalities must be explored to provide natural and direct interaction with wearable computing. Speech and audio allow the physical interface to be scaled down in size, requiring only the use of strategically placed speakers and microphones <ref> [24] </ref> and perhaps tactile input for privacy and noisy environments. Yet scalability is also an issue with an audio modality where listeners may wish to hear varying levels of information content in different situations. <p> Speech is sequential and exists only temporally; the ear cannot browse around a set of recordings the way the eye can scan a screen of text and images. Hence techniques such as interactive skimming [2], nonlinear access and indexing <ref> [15, 24] </ref> and audio spatialization [10, 21] must be considered for browsing audio. Many of these audio techniques can be used to augment existing visual wearable interfaces.
Reference: [25] <author> Stifelman, Lisa J. </author> <title> The Cocktail Party Effect in Auditory Interfaces: A Study of Simultaneous Presentation. </title> <type> MIT Media Lab Technical Report, </type> <month> September </month> <year> 1994. </year>
Reference-contexts: A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources [26]. Yet the cognitive load of listening to simultaneous channels increases with the number of channels. Experiments show that increasing the number of channels beyond three causes a degradation in comprehension <ref> [25] </ref>. Bregman [4] claims that stream segregation is better when frequency separation is greater between sound streams.
Reference: [26] <author> Wenzel, </author> <title> E.M. Localization in virtual acoustic displays, </title> <journal> Presence, </journal> <volume> 1, 80, </volume> <year> 1992. </year>
Reference-contexts: A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources <ref> [26] </ref>. Yet the cognitive load of listening to simultaneous channels increases with the number of channels. Experiments show that increasing the number of channels beyond three causes a degradation in comprehension [25]. Bregman [4] claims that stream segregation is better when frequency separation is greater between sound streams.
References-found: 26

