URL: http://www.cs.iastate.edu/~honavar/Papers/ijcnn.bias.honavar.92.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/publist.html
Root-URL: 
Title: Some Biases for Efficient Learning of Spatial, Temporal, and Spatio-Temporal Patterns  
Author: Vasant Honavar 
Address: 226 Atanasoff Hall  Ames, IA 50011-1040  
Affiliation: Department of Computer Science  Iowa State University  
Abstract: This paper introduces and explores some representational biases for efficient learning of spatial, temporal, or spatio-temporal patterns in connectionist networks (CN) massively parallel networks of simple computing elements. It examines learning mechanisms that constructively build up network structures that encode information from environmental stimuli at successively higher resolutions as needed for the tasks (e.g., perceptual recognition) that the network has to perform. Some simple examples are presented to illustrate the the basic structures and processes used in such networks to ensure the parsimony of learned representations by guiding the system to focus its efforts at the minimal adequate resolution. Several extensions of the basic algorithm for efficient learning using multi-resolution representations of spatial, temporal, or spatio-temporal patterns are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Burt, P. J., & Adelson, E. H. </author> <year> (1983). </year> <title> The laplacian image as a compact image code. </title> <journal> IEEE Transactions on Communications 31 532-540. </journal>
Reference-contexts: This paper was published in the Proceedings of the International Joint Conference on Neural Networks, Beijing, China (1992). 1 2 be used to create a gaussian pyramid <ref> (Burt & Adelson, 1983) </ref> in which the successive levels have resolutions of 256x256, 128x128, 64x64, and so on (see below for details).
Reference: <author> Carpenter, G. A., & Grossberg, S. </author> <year> (1987). </year> <title> A massively parallel architecture for a self-organizing neural pattern recognition machine. Computer Vision, Graphics, </title> <booktitle> and Image Processing 37 54-115. </booktitle>
Reference-contexts: In particular, multi-resolution pyramids of competitive learning networks <ref> (e.g., adaptive resonance models proposed in Carpenter & Grossberg, 1987) </ref> are currently under study. Developmental neurobiology of the visual system suggests further refinements of the representational and inductive biases introduced in this paper (Honavar, 1989).
Reference: <author> Dyer, C. R. </author> <year> (1987). </year> <title> Multiscale image understanding. </title> <editor> In L. Uhr (Ed.) </editor> <booktitle> Parallel Computer Vision. </booktitle> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference: <author> Fahlman, S. E., & Lebiere, C. </author> <year> (1990). </year> <booktitle> The cascade-correlation learning architecture. In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 2). </volume> <editor> D. S. Touretzky (Ed.) </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gallant, S. I. </author> <year> (1990). </year> <title> Perceptron-based learning algorithms. </title> <journal> IEEE Transactions on Neural Networks 1 179-. </journal>
Reference: <author> Hanson, A. R., & Riseman, E. M. </author> <year> (1980). </year> <title> Processing cones: A computational structure for image analysis. </title> <booktitle> In S. </booktitle>
Reference: <author> Tanimoto, & A. Klinger (Eds.) </author> <title> Structured Computer Vision Machine Perception Through Hierarchical Computational Structures. </title> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1988). </year> <title> A network of neuron-like units that learns to perceive by generation as well as reweighting of its links. </title> <editor> In G. E. Hinton, T. J. Sejnowski, & D. S. Touretzky (Eds.) </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Honavar, V. </author> <year> (1989). </year> <title> Perceptual Development and Learning: From Behavioral, Neurophysiological, and Morphological Evidence to Computational Models. </title> <type> Technical report 818. </type> <institution> Madison, WI: Computer Sciences Department. </institution>
Reference-contexts: The nodes generated in this fashion are embedded into node clusters at the appropriate layer in the pyramid network. This process can proceed without supervision or external feedback (This has close parallels in the development of the visual system in mammals <ref> (Honavar, 1989) </ref>). Once the machinery for computing the multi-resolution maps is established in this manner, feedback-guided generation can take S k -2 (X ) S k -2 (Y ) S k (X ) S k (Y ) sub-sampling as in gaussian pyramids. <p> In particular, multi-resolution pyramids of competitive learning networks (e.g., adaptive resonance models proposed in Carpenter & Grossberg, 1987) are currently under study. Developmental neurobiology of the visual system suggests further refinements of the representational and inductive biases introduced in this paper <ref> (Honavar, 1989) </ref>. Development of computational models incorporating such biases and their empirical study is a topic of future research. Although the examples used to illustrate the process of successive refinement of internal representations are extremely simple, the techniques used can potentially handle more complex, multi-dimensional, even multi-modal stimuli.
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1989). </year> <title> Brain-Structured connectionist networks that perceive and learn. </title> <journal> Connection Science Journal of Neural Computing, Artificial Intelligence and Cognitive Research 1 139-160. </journal>
Reference-contexts: The nodes generated in this fashion are embedded into node clusters at the appropriate layer in the pyramid network. This process can proceed without supervision or external feedback (This has close parallels in the development of the visual system in mammals <ref> (Honavar, 1989) </ref>). Once the machinery for computing the multi-resolution maps is established in this manner, feedback-guided generation can take S k -2 (X ) S k -2 (Y ) S k (X ) S k (Y ) sub-sampling as in gaussian pyramids. <p> In particular, multi-resolution pyramids of competitive learning networks (e.g., adaptive resonance models proposed in Carpenter & Grossberg, 1987) are currently under study. Developmental neurobiology of the visual system suggests further refinements of the representational and inductive biases introduced in this paper <ref> (Honavar, 1989) </ref>. Development of computational models incorporating such biases and their empirical study is a topic of future research. Although the examples used to illustrate the process of successive refinement of internal representations are extremely simple, the techniques used can potentially handle more complex, multi-dimensional, even multi-modal stimuli.
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1992a). </year> <title> Generative learning structures and processes for generalized connectionist networks. </title> <note> Information Science Special Issue on Neural Networks and Artificial Intelligence (To appear). </note>
Reference-contexts: This section outlines some designs for such systems. With each of these designs, the entire range of generative learning algorithms <ref> (Honavar & Uhr, 1992a) </ref> can be implemented and empirically evaluated. However, the focus of this paper is on generation through extraction of information-rich sub patterns from the input (or successive abstractions of the input). <p> There are several schemes for increasing the information value and hence the utility of a generated node can be improved. Many such techniques <ref> (see Honavar & Uhr, 1992a for details) </ref> can be applied without modification to learning from multi-resolution representations: E.g., Instead of encoding a single novel pattern encountered, a small pool of nodes may be created with each node encoding a different novel pattern, and after a short exploratory period of evaluation, a
Reference: <author> Honavar, V. </author> <year> (1992b). </year> <title> Learning Parsimonious Representations of Three-Dimensional Shapes. </title> <booktitle> In Proceedings of NATO Advanced Research Workshop on Mathematical Representations of Shape. </booktitle> <address> Netherlands (To appear). </address>
Reference-contexts: The techniques developed in this paper extend such a framework to multi-resolution representations of spatial, temporal, or spatio-temporal patterns. Formulation of similar inductive biases for learning efficient representations of three-dimensional objects is cutrently in progress <ref> (Honavar, 1992b) </ref>. Although we have used feedback-guided minimal generation by novelty-driven extraction of potentially useful sub-patterns as the primary learning mechanism for learning from multi-resolution representations of environmental stimuli, the approach can easily be extended to use a variety of other learning techniques including weight modification.
Reference: <editor> Rosenfeld, A. (Ed.) </editor> <year> (1984). </year> <title> Multiresolution Image Processing and Analysis. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <author> Tanimoto, S., & Klinger, A. (Eds.) </author> <title> Structured Computer Vision Machine Perception Through Hierarchical Computational Structures. </title> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference: <author> Uhr, L. </author> <year> (1972). </year> <title> Layered recognition cone networks that preprocess, classify, and describe. </title> <journal> IEEE Transactions on Computers 21, </journal> <pages> 758-768. </pages>
References-found: 15

