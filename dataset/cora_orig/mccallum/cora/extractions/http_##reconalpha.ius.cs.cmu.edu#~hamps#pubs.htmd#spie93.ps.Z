URL: http://reconalpha.ius.cs.cmu.edu/~hamps/pubs.htmd/spie93.ps.Z
Refering-URL: http://reconalpha.ius.cs.cmu.edu/~hamps/pubs.htmd/index.html
Root-URL: 
Email: hamps@faraday.ece.cmu.edu kumar@previa.ece.cmu.edu  
Title: Differential theory of learning for efficient neural network pattern recognition  
Author: J. B. Hampshire II B.V.K. Vijaya Kumar 
Date: 1966, April, 1993.  
Note: Reprinted (with minor changes) from the Proceedings of the 1993 SPIE International Symposium on Optical Engineering and Photonics in Aerospace and Remote Sensing, vol. 1966: Science of Artificial Neural Networks, D. Ruck, ed., pp. 76-95, April 1993. Preprinted from Proceedings of the 1993 SPIE International Symposium on Optical Engineering and Photonics in Aerospace and Remote Sensing, vol.  
Address: Pittsburgh, PA 15213-3890  
Affiliation: Department of Electrical Computer Engineering Carnegie Mellon University,  
Abstract: We describe a new theory of differential learning by which a broad family of pattern classifiers (including many well-known neural network paradigms) can learn stochastic concepts efficiently. We describe the relationship between a classifier's ability to generalize well to unseen test examples and the efficiency of the strategy by which it learns. We list a series of proofs that differential learning is efficient in its information and computational resource requirements, whereas traditional probabilistic learning strategies are not. The proofs are illustrated by a simple example that lends itself to closed-form analysis. We conclude with an optical character recognition task for which three different types of differentially generated classifiers generalize significantly better than their probabilistically generated counterparts. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Barnard and D. Casasent. </author> <title> A Comparison between Criterion Functions for Linear Classifiers, with an Application to Neural Nets. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(5):1030--1041, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: 1 P Wjx (! i j x) (8) The parameterization that minimizes the classifier's MSE can be found by substituting the expressions of (5) into (7), deriving the expression for the gradient r fi fl , setting this gradient equal to the zero vector, and solving the resulting normal equations <ref> [1] </ref> [7, ch. 4]. <p> Analogous minimum-MSE expressions for the parameters of quadratic discriminant functions in (5) are given in [9] [7, ch. 4]. 5 Expressions for higher-order polynomial discriminant functions are cumbersome. However, the piece-wise constant 4 Equation (10) represents a multi-class generalization of a result for the two-class case in <ref> [1] </ref>. 5 Reference [9] incorrectly states that the minimum-complexity polynomial classifier has two linear discriminant functions and one quadratic discriminant function.
Reference: [2] <author> B. Boser, I. Guyon, and V. Vapnik. </author> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (COLT-92), pages 144--152, </booktitle> <address> New York, NY, 1992. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Its probabilistically generated counterpart exhibits more than double this error rate, as does the best independently-developed linear classifier (2570 parameters) operating on the original binary images (pre-processed by filtering and removal of ``non-supporting'' training examples see <ref> [2] </ref>). These empirical error rates are obtained from a benchmark partitioning of the 1200-example database into disjoint 600-example training and test samples. The database contains ten examples of each digit from 12 subjects.
Reference: [3] <author> D. S. Broomhead and D. Lowe. </author> <title> Multivariable Function Interpolation and Adaptive Networks. </title> <journal> Complex Systems, </journal> <volume> 2:321--355, </volume> <year> 1988. </year>
Reference-contexts: We refer to the classifier with such a discriminator as a modified Gaussian RBF classifier, in recognition of the fact that it constitutes a Gaussian radial basis function (RBF) classifier (e.g., <ref> [3, 15, 17, 14] </ref>) with diagonal covariance matrices when the discriminator is formed by cascaded layers of these functions. Figures 10 12 are comparative summaries of differential versus probabilistic learning for 650-parameter classifiers generated from the linear, logistic linear, and modified RBF hypothesis classes.
Reference: [4] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. Wiley Series in Telecommunications. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: The resulting partitioning of feature space (bottom bar-graph) is poor, and the classifier exhibits a 7.8% error rate. This is a classic case of Occam's razor <ref> [20, 4] </ref>, in which the classifier has so much functional complexity it fails to generalize well for small training sample sizes. 6 Note that the legend ``CFM 1-0-1'', for example, denotes the differentially generated classifier with polynomial discriminant functions of order 1 (linear), 0 (constant), and 1 (linear), associated with classes
Reference: [5] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1973. </year>
Reference-contexts: The Bayes-optimal classifier is one that always associates X with its most likely class, thereby assuring the minimum probability of making a classification error (e.g., <ref> [5, ch 2] </ref>). Any classifier that classifies X in this manner is said to yield Bayesian discrimination; equivalently, it is said to exhibit the Bayes error rate (i.e., the minimum probability of misclassification). <p> The figure is based on figure 2.3 of Duda & Hart <ref> [5] </ref>. the a posteriori probability P Wjx (! i j X) , one way the classifier will yield Bayesian discrimination is if its discriminant functions equal their corresponding a posteriori class probabilities. <p> The augmented feature vector X 0 is the (N + 1)-dimensional vector formed by prepending a single element of unit value to X : X 0 4 " X , (18) (e.g., <ref> [5, pp. 136-137] </ref>). The parameter vector for the ith discriminant function is part of the over-all parameter vector for the discriminator: i 2 fi ; i 2 &lt; N+1 .
Reference: [6] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural Networks and the Bias/Variance Dilemma. </title> <booktitle> Neural Computation, </booktitle> <address> 4(1):1--58, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: The inefficiency of probabilistic learning is clear in figure 8. The minimum-complexity classifier has no discriminant variance, but its empirical discriminant bias is 20% 2% = 18%. 88 compressed from the 256 binary pixel originals (e.g., see <ref> [6] </ref>). The low-complexity classifier has substantially lower empirical discriminant bias ( 8:5% 2% = 6:5% for n = 1000), but its discriminant variance is high (as indicated by the spread of the box plots).
Reference: [7] <author> J. B. </author> <title> Hampshire II. A Differential Theory of Learning for Efficient Statistical Pattern Recognition. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Department of Electrical & Computer Engineering, Hammerschlag Hall, </institution> <address> Pittsburgh, PA 15213-3890, </address> <month> expected July </month> <year> 1993. </year>
Reference-contexts: The classifier's discriminant variance is the variance in its error rate across trials. The classifier's mean-squared discriminant 79 error (MSDE) is its squared discriminant bias plus its discriminant variance. Formal definitions of these quantities are given in <ref> [7, ch. 3] </ref>. The classifier on the left in figure 3 is a poor estimator of the Bayes-optimal classifier because it exhibits both high discriminant bias and high discriminant variance. <p> An asymptotically efficient learning strategy requires large training sample sizes to guarantee the relatively efficient classifier. Rigorous proofs regarding the asymptotic efficiency of differential learning and the inefficiency of probabilistic learning are given in <ref> [7, ch's. 2-3] </ref>. The following is a summary: * Classifiers that learn by minimizing error measure objective functions (e.g., mean-squared error, the Kullback-Liebler information distance, etc.) learn probabilistically. <p> As a result, probabilistic learning is almost always inefficient (special circumstances may exist in which probabilistic learning generates the minimum-MSDE classifier; these circumstances, which are both rare and easily recognized, are described in <ref> [7, ch's. 3-4] </ref>). * Classifiers that learn by maximizing a classification figure of merit objective function (CFM appendix A) learn differentially. <p> As a result, differential learning is almost always efficient (special circumstances may exist in which this is not the case; these circumstances are described in <ref> [7, ch's. 3-4] </ref>). * Learning differentially by maximizing the synthetic CFM objective function described in appendix A requires discriminant functions with the least functional complexity (e.g., the fewest parameters) necessary for Bayesian discrimination. 80 1.3 Summary of differential learning applications We have applied differential learning to several real-world machine learning/pattern recognition <p> In each task the differentially generated classifier generalizes better than its probabilistically generated counterpart. The discrimination improvements range from moderate to significant, depending on the statistical nature of the learning task and its relationship to the functional basis of the classifier used. Reference <ref> [7] </ref> gives detailed summaries for each application domain. <p> P Wjx (! i j x) (8) The parameterization that minimizes the classifier's MSE can be found by substituting the expressions of (5) into (7), deriving the expression for the gradient r fi fl , setting this gradient equal to the zero vector, and solving the resulting normal equations [1] <ref> [7, ch. 4] </ref>. <p> Analogous minimum-MSE expressions for the parameters of quadratic discriminant functions in (5) are given in [9] <ref> [7, ch. 4] </ref>. 5 Expressions for higher-order polynomial discriminant functions are cumbersome. <p> herein, the third discriminant function need only be a constant for the classifier to yield Bayesian discrimination. 83 nature of the pdf and a posteriori probabilities of x (see figure 4) allow a straightforward expansion of (7), by which compact minimum-MSE expressions for higher-order polynomial discriminant functions can be obtained <ref> [7, ch. 4] </ref>. 2.2 Differential learning via CFM for the asymptotically large training sample Differential learning is implemented by maximizing the classification figure-of-merit (CFM) described in appendix A and [11, 7]. <p> figure 4) allow a straightforward expansion of (7), by which compact minimum-MSE expressions for higher-order polynomial discriminant functions can be obtained [7, ch. 4]. 2.2 Differential learning via CFM for the asymptotically large training sample Differential learning is implemented by maximizing the classification figure-of-merit (CFM) described in appendix A and <ref> [11, 7] </ref>. The procedure is virtually the same as that for probabilistic learning via MSE, except for the change in objective function. <p> Using the notational conventions of (6), the sample CFM is given by CFM S n j G (x j ) = i=1 p=1 n ffi i (x p j ) , n p ; p=1 Details of the CFM confidence parameter are given in appendix A and <ref> [7] </ref>. As the training sample size grows asymptotically large, the classifier's CFM can be expressed by the following expectation [7, ch. 2]: E x CFM x j G (x j ) = i=1 1 fi fl The discriminant differential ffi i (x j ) is given in (3). <p> As the training sample size grows asymptotically large, the classifier's CFM can be expressed by the following expectation <ref> [7, ch. 2] </ref>: E x CFM x j G (x j ) = i=1 1 fi fl The discriminant differential ffi i (x j ) is given in (3). <p> It can be shown that when the minimum-complexity polynomial classifier is differentially generated via the CFM objective function (lim !0 + see <ref> [7, ch's. 2,4] </ref>), the optimal values for the parameters are interdependent but non-unique. <p> That is, the box plots give us empirical estimates of the discriminant bias, discriminant variance, and mean-squared discriminant error (MSDE) of each classifier/learning strategy. The minimum-complexity differentially generated classifier is the most efficient, exhibiting consistently low error rates for small training sample sizes. Based on <ref> [8, 7] </ref>, we predict that 1121 samples of x are necessary to guarantee (with 95% confidence) an error rate of no more than 4.0% using differential learning. Note that the empirical upper bound on the differentially generated minimum-complexity classifier's error rate is 3.3% when the sample size is 1000. <p> The high-complexity classifier has moderate discriminant bias ( 5:6% 2% = 3:6% for n = 1000), and moderate discriminant variance substantially better than the probabilistically generated low-complexity classifier, but substantially worse than the differentially generated minimum-complexity classifier. Figures 6 8 illustrate the proofs of <ref> [7] </ref> outlined in section 1.2 differential learning produces the best generalizing (i.e., minimum MSDE) classifier and requires the least functional complexity necessary for Bayesian discrimination. As described in section 1.3, differential learning has generated improved classifiers across a broad range of pattern recognition tasks. <p> The differentiable function of ffi is nearly linear for a confidence measure of unity. In the limit that is zero, the function becomes a Heaviside step. The synthetic function and its first derivative are easily computed (see <ref> [7] </ref>). learn differentially when generated with the CFM objective function; that is, they learn a maximum-CFM approximation to the differential form of the Bayesian discriminant function. The CFM objective function was originally described in [11]; a newer synthetic form of the function is detailed in [7]. <p> derivative are easily computed (see <ref> [7] </ref>). learn differentially when generated with the CFM objective function; that is, they learn a maximum-CFM approximation to the differential form of the Bayesian discriminant function. The CFM objective function was originally described in [11]; a newer synthetic form of the function is detailed in [7]. Kullback-Leibler information distance [13] known as ``cross entropy'' (CE) in the neural network literature (e.g., [12]) compare the classifier's discriminator output state with a target vector t . <p> The maximum steepness of the sigmoid is regulated by the confidence parameter . Figure 14 shows a synthetic form of CFM detailed in <ref> [7] </ref> for several values of . Note that the function is nearly linear for = 1 and it converges to the heaviside step as ! 0 + .
Reference: [8] <author> J. B. Hampshire II and B. V. K. Vijaya Kumar. </author> <title> Shooting Craps in Search of an Optimal Strategy for Training Connectionist Pattern Classifiers. </title> <editor> In J. Moody, S. Hanson, and R. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> vol. 4, </volume> <pages> pages 1125--1132, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: That is, the box plots give us empirical estimates of the discriminant bias, discriminant variance, and mean-squared discriminant error (MSDE) of each classifier/learning strategy. The minimum-complexity differentially generated classifier is the most efficient, exhibiting consistently low error rates for small training sample sizes. Based on <ref> [8, 7] </ref>, we predict that 1121 samples of x are necessary to guarantee (with 95% confidence) an error rate of no more than 4.0% using differential learning. Note that the empirical upper bound on the differentially generated minimum-complexity classifier's error rate is 3.3% when the sample size is 1000.
Reference: [9] <author> J. B. Hampshire II and B. V. K. Vijaya Kumar. </author> <title> Why Error Measures are Sub-Optimal for Training Neural Network Pattern Classifiers. </title> <booktitle> In IEEE Proceedings of the 1992 International Joint Conference on Neural Networks, </booktitle> <volume> Vol. 4, </volume> <pages> pages 220--227, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Analogous minimum-MSE expressions for the parameters of quadratic discriminant functions in (5) are given in <ref> [9] </ref> [7, ch. 4]. 5 Expressions for higher-order polynomial discriminant functions are cumbersome. However, the piece-wise constant 4 Equation (10) represents a multi-class generalization of a result for the two-class case in [1]. 5 Reference [9] incorrectly states that the minimum-complexity polynomial classifier has two linear discriminant functions and one quadratic <p> Analogous minimum-MSE expressions for the parameters of quadratic discriminant functions in (5) are given in <ref> [9] </ref> [7, ch. 4]. 5 Expressions for higher-order polynomial discriminant functions are cumbersome. However, the piece-wise constant 4 Equation (10) represents a multi-class generalization of a result for the two-class case in [1]. 5 Reference [9] incorrectly states that the minimum-complexity polynomial classifier has two linear discriminant functions and one quadratic discriminant function.
Reference: [10] <author> J. B. Hampshire II and B. V. K. Vijaya Kumar. </author> <title> Differential Learning Leads to Efficient Neural Network Classifiers. </title> <booktitle> In IEEE Proceedings of the 1993 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages I:613--616, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Right: this classifier has low discriminant bias and low discriminant variance. As a result it yields a consistently good approximation to the Bayes error rate. It's MSDE is therefore low. discriminant functions (see <ref> [10] </ref>). 1.1 Efficient learning and generalization A classifier is said to generalize well if it discriminates test examples of the feature vector with the same empirical probability of error it exhibits on the training sample (i.e., the set of examples with which it is trained).
Reference: [11] <author> J. B. Hampshire II and A. H. Waibel. </author> <title> A Novel Objective Function for Improved Phoneme Recognition Using Time-Delay Neural Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2):216--228, </volume> <month> June </month> <year> 1990. </year> <note> A revised and extended version of work published earlier in 1) Carnegie Mellon University, </note> <institution> School of Computer Science Technical Report CMU-CS-89-118, </institution> <month> March 31, </month> <year> 1989, </year> <booktitle> and 2) the IEEE Proceedings of the 1989 International Joint Conference on Neural Networks, </booktitle> <volume> vol. I, </volume> <pages> pp. 235-241, </pages> <month> June, </month> <year> 1989. </year>
Reference-contexts: figure 4) allow a straightforward expansion of (7), by which compact minimum-MSE expressions for higher-order polynomial discriminant functions can be obtained [7, ch. 4]. 2.2 Differential learning via CFM for the asymptotically large training sample Differential learning is implemented by maximizing the classification figure-of-merit (CFM) described in appendix A and <ref> [11, 7] </ref>. The procedure is virtually the same as that for probabilistic learning via MSE, except for the change in objective function. <p> CFM then seeks to maximize a function of the difference (or discriminant differential) ffi t between this output and the largest other output (in this case, y 1 ). The function fi fl is shown in figure 14. classification figure-of-merit (CFM) <ref> [11] </ref> shown for discriminant differential values on the interval 1 ffi 1 . The shape of the sigmoid is controlled by a confidence measure on (0,1]: ffi , is shown for eight different values of . The differentiable function of ffi is nearly linear for a confidence measure of unity. <p> The synthetic function and its first derivative are easily computed (see [7]). learn differentially when generated with the CFM objective function; that is, they learn a maximum-CFM approximation to the differential form of the Bayesian discriminant function. The CFM objective function was originally described in <ref> [11] </ref>; a newer synthetic form of the function is detailed in [7]. Kullback-Leibler information distance [13] known as ``cross entropy'' (CE) in the neural network literature (e.g., [12]) compare the classifier's discriminator output state with a target vector t . <p> need not be binary; we describe it as such for the sake of simplicity. 9 It is important to note that the identity of the largest other output in ffi t is stochastic; it not only varies across examples, it may also change for a given example as learning progresses <ref> [11] </ref>. 94 continuum between a linear function of ffi and a step function of ffi . The maximum steepness of the sigmoid is regulated by the confidence parameter . Figure 14 shows a synthetic form of CFM detailed in [7] for several values of .
Reference: [12] <author> G. E. Hinton. </author> <title> Connectionist Learning Procedures. </title> <editor> In J. G. Carbonell, editor, </editor> <title> Machine Learning: Paradigms and Methods, </title> <publisher> pages 185--234. MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year> <note> Based on the 1987 Carnegie Mellon University technical report (CMU-CS-87-115) of the same title. </note>
Reference-contexts: The CFM objective function was originally described in [11]; a newer synthetic form of the function is detailed in [7]. Kullback-Leibler information distance [13] known as ``cross entropy'' (CE) in the neural network literature (e.g., <ref> [12] </ref>) compare the classifier's discriminator output state with a target vector t .
Reference: [13] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1959. </year>
Reference-contexts: The CFM objective function was originally described in [11]; a newer synthetic form of the function is detailed in [7]. Kullback-Leibler information distance <ref> [13] </ref> known as ``cross entropy'' (CE) in the neural network literature (e.g., [12]) compare the classifier's discriminator output state with a target vector t .
Reference: [14] <author> P. Medgassy. </author> <title> Decomposition of Superposition of Distribution Functions. </title> <publisher> Publishing House of the Hungarian Academy of Sciences, </publisher> <address> Budapest, </address> <year> 1961. </year>
Reference-contexts: We refer to the classifier with such a discriminator as a modified Gaussian RBF classifier, in recognition of the fact that it constitutes a Gaussian radial basis function (RBF) classifier (e.g., <ref> [3, 15, 17, 14] </ref>) with diagonal covariance matrices when the discriminator is formed by cascaded layers of these functions. Figures 10 12 are comparative summaries of differential versus probabilistic learning for 650-parameter classifiers generated from the linear, logistic linear, and modified RBF hypothesis classes.
Reference: [15] <author> J. Moody and C. Darken. </author> <title> Learning with Localised Receptive Fields. </title> <editor> In Touretzky, Hinton, and Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We refer to the classifier with such a discriminator as a modified Gaussian RBF classifier, in recognition of the fact that it constitutes a Gaussian radial basis function (RBF) classifier (e.g., <ref> [3, 15, 17, 14] </ref>) with diagonal covariance matrices when the discriminator is formed by cascaded layers of these functions. Figures 10 12 are comparative summaries of differential versus probabilistic learning for 650-parameter classifiers generated from the linear, logistic linear, and modified RBF hypothesis classes.
Reference: [16] <author> B. K. Natarajan. </author> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Given a particular training sample size n , a particular choice of discriminant functions (or hypothesis class <ref> [16] </ref>), and a particular initial parameterization, the transformation depends entirely on the learning strategy employed.
Reference: [17] <author> T. Poggio and F. Girosi. </author> <title> A Theory of Networks for Approximation and Learning. AI Memo 1140, </title> <publisher> MIT, </publisher> <year> 1989. </year>
Reference-contexts: We refer to the classifier with such a discriminator as a modified Gaussian RBF classifier, in recognition of the fact that it constitutes a Gaussian radial basis function (RBF) classifier (e.g., <ref> [3, 15, 17, 14] </ref>) with diagonal covariance matrices when the discriminator is formed by cascaded layers of these functions. Figures 10 12 are comparative summaries of differential versus probabilistic learning for 650-parameter classifiers generated from the linear, logistic linear, and modified RBF hypothesis classes.
Reference: [18] <author> M. D. Richard and R. P. Lippmann. </author> <title> Neural Network Classifiers Estimate Bayesian a posteriori Probabilities. </title> <booktitle> Neural Computation, </booktitle> <address> 3(4):461--483, </address> <year> 1991. </year>
Reference-contexts: (x j ) = i=1 p=1 n 2 n p,i + g i (x p j ) n p ; p=1 (6) It is straightforward to prove that as the training sample size grows asymptotically large, the classifier's MSE can be expressed 82 by the following expectation (see, for example, <ref> [18] </ref>): E x MSE x j G (x j ) = i=1 1 g i (x j ) 1 P Wjx (! i j x) + g i (x j ) P Wjx (:! i j x) x (x) dx (7) where P Wjx (:! i j x) = 1 P
Reference: [19] <editor> D. E. Rumelhart, J. L. McClelland, et al. </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: As described in section 1, we interpret the discriminator output with the largest value as the classifier's vote for the class of its scalar input x. This polynomial classifier is depicted in figure 5, and it is trained with a modified form of the backpropagation algorithm (e.g., <ref> [19] </ref>). <p> Backpropagation is a well-known probabilistic learning paradigm for which the error measure is the mean-squared error (MSE) objective function, the iterative search procedure is gradient descent, and the gradient of the classifier's MSE with respect to the parameter vector is computed by the chain-rule <ref> [19] </ref>. We denote the training sample of size n by S n , and we denote a particular unique value (or pattern) of x by x p . <p> Learning takes the form of a steepest descent (MSE) or steepest ascent (CFM) search over parameter space, using a modified form of the backpropagation algorithm (e.g., <ref> [19] </ref>). Learning begins from a tabula rasa state in which all parameters are initialized randomly according to a uniform distribution on the closed interval [:3 , :3]. All trials are completely automated, so learning is done without any human intervention. <p> We refer to the classifier with such a discriminator as a logistic linear classifier because it forms piece-wise linear class boundaries on . Note that when the discriminator is formed by cascaded layers of these functions, it constitutes a multi-layer perceptron (e.g., <ref> [19] </ref>).
Reference: [20] <author> S. C. Tornay. </author> <title> Ockham: Studies and Selections. </title> <publisher> Open Court Publishers, </publisher> <address> La Salle, IL, </address> <year> 1938. </year>
Reference-contexts: The resulting partitioning of feature space (bottom bar-graph) is poor, and the classifier exhibits a 7.8% error rate. This is a classic case of Occam's razor <ref> [20, 4] </ref>, in which the classifier has so much functional complexity it fails to generalize well for small training sample sizes. 6 Note that the legend ``CFM 1-0-1'', for example, denotes the differentially generated classifier with polynomial discriminant functions of order 1 (linear), 0 (constant), and 1 (linear), associated with classes
Reference: [21] <author> J. W. Tukey. </author> <title> Exploratory Data Analysis. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1977. </year> <month> 95 </month>
Reference-contexts: Results for differential learning via CFM are shown in white; results for probabilistic learning via MSE are shown in gray. The results are shown in box-plot <ref> [21, ch. 2] </ref> statistical summaries.
References-found: 21

