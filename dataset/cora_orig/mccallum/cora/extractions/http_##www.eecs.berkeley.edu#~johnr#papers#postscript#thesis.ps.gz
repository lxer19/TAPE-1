URL: http://www.eecs.berkeley.edu/~johnr/papers/postscript/thesis.ps.gz
Refering-URL: http://www.eecs.berkeley.edu/~johnr/papers/thesis.html
Root-URL: 
Title: Realtime Signal Processing Dataflow, Visual, and Functional Programming  
Author: Hideki John Reekie 
Degree: Submitted for the Degree of Doctor of Philosophy at the  in the School of Electrical Engineering  
Date: September 1995  
Address: Sydney  
Affiliation: University of Technology at  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> William B. Ackerman. </author> <title> Data flow languages. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 15-24, </pages> <month> February </month> <year> 1982. </year>
Reference-contexts: Still, side-effect-free languages are likely to contain more implicit parallelism than imperative languages because they lack artificial time-dependencies. Two examples of languages and architectures that support implicit parallelism are dataflow languages and architectures <ref> [1, 7] </ref> and multi-processor graph reduction of pure functional programs [105]. In both cases, the languages are side-effect free. However, Gajski et al point out that parallelising compilers can in fact perform better than single-assignment languages [49]. Implicit parallelism is also a key ingredient of modern single-processor compilers. <p> The fby operator produces a stream containing the first element of its left argument, followed by its right argument. So, if x = <ref> [1; 2; 3; 4] </ref>, then runner produces zero followed by itself summed with x|that is, [0; 1; 3; 6; 10]. The dataflow process network model [87] is lower-level, since communication is explicit. A process is formed by repeatedly firing an "actor"; a complete program consists of a network of actors. <p> The fby operator produces a stream containing the first element of its left argument, followed by its right argument. So, if x = [1; 2; 3; 4], then runner produces zero followed by itself summed with x|that is, <ref> [0; 1; 3; 6; 10] </ref>. The dataflow process network model [87] is lower-level, since communication is explicit. A process is formed by repeatedly firing an "actor"; a complete program consists of a network of actors. <p> BACKGROUND MATERIAL 44 these programs. Finally, because most of the work in following chapters is influenced by the goal of real-time execution on embedded DSP device, I provided an overview of real-time programming concepts and DSP devices. Chapter 3 Dataflow Process Networks Dataflow <ref> [1, 7] </ref> is a model of computation in which data items, called tokens, flow between computing agents, or actors . A program is represented by a dataflow graph (DFG), in which vertices correspond to actors and edges to a "flow" of data from one actor to another. <p> Actors are fine-grained machine operations such as addition, while edges of the graph represent transfer of tokens through the machine. The machine executes an actor when all required tokens are present on its inputs. The languages used to program such machines <ref> [1] </ref> are often called dataflow languages. Dataflow analyses (DFAs) are used for code optimisation in compilers [2]. <p> One of motivations for using a functional language in this CHAPTER 4. VISUAL HASKELL 93 thesis is the apparent correspondence between functional languages and dataflow. This is highlighted by the fact that languages for programming dataflow machines are often functional <ref> [1] </ref>. On closer examination, however, there are some important differences. Ambler et al , in their survey and comparison of programming paradigms [4], highlight some of the differences between the functional and dataflow paradigms. <p> As an example, consider this expression: meshlV (a x : (x; a)) 0 h1; 2; 3i ! (3; h0; 1; 2i) 1 Readers familiar with functional programming will notice that this definition differs from the standard Haskell scanning functions on lists. In comparison, scanl (+) 0 <ref> [1; 2; 3] </ref> ! [0; 1; 3; 6] The vector versions are defined as they are so that the output vector is the same length as the input vector: this affords some efficiency improvement if the input vector can be over-written with the result. CHAPTER 5. <p> In comparison, scanl (+) 0 [1; 2; 3] ! <ref> [0; 1; 3; 6] </ref> The vector versions are defined as they are so that the output vector is the same length as the input vector: this affords some efficiency improvement if the input vector can be over-written with the result. CHAPTER 5. <p> ! Stream fi To define a function that produces a stream containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver <ref> [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] </ref> ! [1; 2; 3; 4; 5; 5; 5; : : :] If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one <p> containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] ! <ref> [1; 2; 3; 4; 5; 5; 5; : : :] </ref> If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one stream until it terminates, and then outputs elements of a second
Reference: [2] <author> A.V. Aho, R. Sethi, and J.D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Messages are exchanged by calling a message-passing library. For example, a sending task would contain code like this: sometype message; chan_out_message ( sizeof (sometype), &message, outs [0] ); A receiving task would contain code like this: chan_in_message ( sizeof (sometype), &message, ins <ref> [2] </ref> ); Control parallelism is low-level|that is, the programmer interface is essentially that of the operations that can be performed directly by a target machine: transmission of messages between processors (distributed-memory machines), or synchronisation of memory accesses (shared-memory machines). <p> The fby operator produces a stream containing the first element of its left argument, followed by its right argument. So, if x = <ref> [1; 2; 3; 4] </ref>, then runner produces zero followed by itself summed with x|that is, [0; 1; 3; 6; 10]. The dataflow process network model [87] is lower-level, since communication is explicit. A process is formed by repeatedly firing an "actor"; a complete program consists of a network of actors. <p> The result of evaluating the right-hand side is bound to the corresponding identifiers in the pattern. For example, the binding (x:xs) = [1,2,3,4] will cause the integer 1 to be bound to x, and the list <ref> [2; 3; 4] </ref> to be bound to xs. Patterns can be nested arbitrarily, as in ((x,y) : z : zs) = ... Arguments to function bindings are often patterns; in this case, they also serve to select one of possibly multiple clauses of a function definition. <p> The machine executes an actor when all required tokens are present on its inputs. The languages used to program such machines [1] are often called dataflow languages. Dataflow analyses (DFAs) are used for code optimisation in compilers <ref> [2] </ref>. DFAs provide the compiler with information such as variable lifetimes and usage, which enables it to eliminate code that will never be executed, to re-use registers when their contents are no longer needed, and to re-order instructions in order to generate improved machine code CHAPTER 3. <p> As an example, consider this expression: meshlV (a x : (x; a)) 0 h1; 2; 3i ! (3; h0; 1; 2i) 1 Readers familiar with functional programming will notice that this definition differs from the standard Haskell scanning functions on lists. In comparison, scanl (+) 0 <ref> [1; 2; 3] </ref> ! [0; 1; 3; 6] The vector versions are defined as they are so that the output vector is the same length as the input vector: this affords some efficiency improvement if the input vector can be over-written with the result. CHAPTER 5. <p> ! Stream fi To define a function that produces a stream containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver <ref> [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] </ref> ! [1; 2; 3; 4; 5; 5; 5; : : :] If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one <p> containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] ! <ref> [1; 2; 3; 4; 5; 5; 5; : : :] </ref> If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one stream until it terminates, and then outputs elements of a second
Reference: [3] <author> Sudhir Ahuja, Nicholas Carriero, and David Gelernter. Linda and friends. </author> <booktitle> IEEE Computer, </booktitle> <pages> pages 26-34, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Proponents of the implicit or data-parallel language styles sometimes forget that there are other approaches to functional parallelism that provide better support for managing parallelism. 2.1.5 The Linda model Linda <ref> [3] </ref> is a simple, elegant, architecture-independent model for MIMD computation. <p> The fby operator produces a stream containing the first element of its left argument, followed by its right argument. So, if x = <ref> [1; 2; 3; 4] </ref>, then runner produces zero followed by itself summed with x|that is, [0; 1; 3; 6; 10]. The dataflow process network model [87] is lower-level, since communication is explicit. A process is formed by repeatedly firing an "actor"; a complete program consists of a network of actors. <p> The fby operator produces a stream containing the first element of its left argument, followed by its right argument. So, if x = [1; 2; 3; 4], then runner produces zero followed by itself summed with x|that is, <ref> [0; 1; 3; 6; 10] </ref>. The dataflow process network model [87] is lower-level, since communication is explicit. A process is formed by repeatedly firing an "actor"; a complete program consists of a network of actors. <p> The result of evaluating the right-hand side is bound to the corresponding identifiers in the pattern. For example, the binding (x:xs) = [1,2,3,4] will cause the integer 1 to be bound to x, and the list <ref> [2; 3; 4] </ref> to be bound to xs. Patterns can be nested arbitrarily, as in ((x,y) : z : zs) = ... Arguments to function bindings are often patterns; in this case, they also serve to select one of possibly multiple clauses of a function definition. <p> As an example, consider this expression: meshlV (a x : (x; a)) 0 h1; 2; 3i ! (3; h0; 1; 2i) 1 Readers familiar with functional programming will notice that this definition differs from the standard Haskell scanning functions on lists. In comparison, scanl (+) 0 <ref> [1; 2; 3] </ref> ! [0; 1; 3; 6] The vector versions are defined as they are so that the output vector is the same length as the input vector: this affords some efficiency improvement if the input vector can be over-written with the result. CHAPTER 5. <p> In comparison, scanl (+) 0 [1; 2; 3] ! <ref> [0; 1; 3; 6] </ref> The vector versions are defined as they are so that the output vector is the same length as the input vector: this affords some efficiency improvement if the input vector can be over-written with the result. CHAPTER 5. <p> ! Stream fi To define a function that produces a stream containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver <ref> [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] </ref> ! [1; 2; 3; 4; 5; 5; 5; : : :] If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one <p> containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] ! <ref> [1; 2; 3; 4; 5; 5; 5; : : :] </ref> If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one stream until it terminates, and then outputs elements of a second
Reference: [4] <author> Alan L. Ambler, Margaret M. Burnett, and Betsy A. Zimmerman. </author> <title> Operational versus definitional: A perspective on programming paradigms. </title> <journal> IEEE Computer, </journal> <volume> 25(9) </volume> <pages> 28-43, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: The fby operator produces a stream containing the first element of its left argument, followed by its right argument. So, if x = <ref> [1; 2; 3; 4] </ref>, then runner produces zero followed by itself summed with x|that is, [0; 1; 3; 6; 10]. The dataflow process network model [87] is lower-level, since communication is explicit. A process is formed by repeatedly firing an "actor"; a complete program consists of a network of actors. <p> The result of evaluating the right-hand side is bound to the corresponding identifiers in the pattern. For example, the binding (x:xs) = [1,2,3,4] will cause the integer 1 to be bound to x, and the list <ref> [2; 3; 4] </ref> to be bound to xs. Patterns can be nested arbitrarily, as in ((x,y) : z : zs) = ... Arguments to function bindings are often patterns; in this case, they also serve to select one of possibly multiple clauses of a function definition. <p> This is highlighted by the fact that languages for programming dataflow machines are often functional [1]. On closer examination, however, there are some important differences. Ambler et al , in their survey and comparison of programming paradigms <ref> [4] </ref>, highlight some of the differences between the functional and dataflow paradigms. Because pipeline dataflow operates on streams of data, its evaluation mechanism is neither strict nor non-strict (see section 5.3). <p> ! Stream fi To define a function that produces a stream containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver <ref> [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] </ref> ! [1; 2; 3; 4; 5; 5; 5; : : :] If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one <p> containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] ! <ref> [1; 2; 3; 4; 5; 5; 5; : : :] </ref> If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one stream until it terminates, and then outputs elements of a second
Reference: [5] <institution> Analog Devices. </institution> <note> ADSP-21020 User's Manual, </note> <year> 1991. </year>
Reference-contexts: In this section, I give an overview of the characteristics of these devices. I focus only on the modern, floating-point devices; for more detailed information on specific devices, see <ref> [115, 119, 34, 5, 6, 98, 138, 137] </ref>. The device contains six functional units|an ALU, a multiplier, two address units, and two load-store units (not shown here)|all of which can operate simultaneously, a bank of floating-point data registers, and a bank of address registers. <p> The multiply-add instruction is the core of the FIR (finite-impulse response) filter, a key DSP benchmark. Some devices also have a parallel multiply-add-subtract instruction, which substantially speeds up execution of the FFT (Fast Fourier Transform), another key benchmark. In contrast to the TMS320C30 and TMS320C40, the ADSP-21020 and DSP96002 <ref> [5, 98] </ref> are load-store machines: operands are explicitly loaded into data registers prior to operating on them. The programmer writes separate fields of the instruction to control the ALU and multiplier, and the load/store and address units. <p> ! Stream fi To define a function that produces a stream containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver <ref> [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] </ref> ! [1; 2; 3; 4; 5; 5; 5; : : :] If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one <p> containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] ! <ref> [1; 2; 3; 4; 5; 5; 5; : : :] </ref> If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one stream until it terminates, and then outputs elements of a second
Reference: [6] <institution> Analog Devices. ADSP-21060 SHARC Super Harvard Architecture Computer, </institution> <note> Oc-tober 1993. Preliminary datasheet. </note>
Reference-contexts: The fby operator produces a stream containing the first element of its left argument, followed by its right argument. So, if x = [1; 2; 3; 4], then runner produces zero followed by itself summed with x|that is, <ref> [0; 1; 3; 6; 10] </ref>. The dataflow process network model [87] is lower-level, since communication is explicit. A process is formed by repeatedly firing an "actor"; a complete program consists of a network of actors. <p> In this section, I give an overview of the characteristics of these devices. I focus only on the modern, floating-point devices; for more detailed information on specific devices, see <ref> [115, 119, 34, 5, 6, 98, 138, 137] </ref>. The device contains six functional units|an ALU, a multiplier, two address units, and two load-store units (not shown here)|all of which can operate simultaneously, a bank of floating-point data registers, and a bank of address registers. <p> In comparison, scanl (+) 0 [1; 2; 3] ! <ref> [0; 1; 3; 6] </ref> The vector versions are defined as they are so that the output vector is the same length as the input vector: this affords some efficiency improvement if the input vector can be over-written with the result. CHAPTER 5. <p> ! Stream fi To define a function that produces a stream containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver <ref> [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] </ref> ! [1; 2; 3; 4; 5; 5; 5; : : :] If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one
Reference: [7] <author> Arvind and David E. Culler. </author> <title> Dataflow architectures. </title> <booktitle> Annual Reviews in Computer Science, </booktitle> <volume> 1 </volume> <pages> 225-253, </pages> <year> 1986. </year>
Reference-contexts: Still, side-effect-free languages are likely to contain more implicit parallelism than imperative languages because they lack artificial time-dependencies. Two examples of languages and architectures that support implicit parallelism are dataflow languages and architectures <ref> [1, 7] </ref> and multi-processor graph reduction of pure functional programs [105]. In both cases, the languages are side-effect free. However, Gajski et al point out that parallelising compilers can in fact perform better than single-assignment languages [49]. Implicit parallelism is also a key ingredient of modern single-processor compilers. <p> BACKGROUND MATERIAL 44 these programs. Finally, because most of the work in following chapters is influenced by the goal of real-time execution on embedded DSP device, I provided an overview of real-time programming concepts and DSP devices. Chapter 3 Dataflow Process Networks Dataflow <ref> [1, 7] </ref> is a model of computation in which data items, called tokens, flow between computing agents, or actors . A program is represented by a dataflow graph (DFG), in which vertices correspond to actors and edges to a "flow" of data from one actor to another. <p> It is often associated with dataflow machines, in which the dataflow model is the basis of the execution mechanism of a highly-parallel machine <ref> [7] </ref>. Actors are fine-grained machine operations such as addition, while edges of the graph represent transfer of tokens through the machine. The machine executes an actor when all required tokens are present on its inputs. The languages used to program such machines [1] are often called dataflow languages. <p> ! Stream fi To define a function that produces a stream containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver <ref> [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] </ref> ! [1; 2; 3; 4; 5; 5; 5; : : :] If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one
Reference: [8] <author> Arvind and Kattamuri Ekanadham. </author> <title> Future scientific programming on parallel machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 460-493, </pages> <year> 1988. </year>
Reference-contexts: Dataflow process networks are examined in chapter 3. 2.2 Functional programming in five minutes Functional programming languages are "higher-level" than more conventional imperative languages. There have been many persuasive arguments advanced for functional programming languages in general <ref> [8] </ref>, and lazy functional languages in particular [64, 139, 62]. A recent study indicates that at least some of the claimed advantages of functional languages|brevity, rapidity of development, and ease of understanding|can be confirmed [63]. The study compares several languages, including C++ and Ada, in a substantial rapid prototyping exercise. <p> ! Stream fi To define a function that produces a stream containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver <ref> [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] </ref> ! [1; 2; 3; 4; 5; 5; 5; : : :] If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one
Reference: [9] <author> E. A. Ashcroft. </author> <title> Eazyflow architecture. </title> <type> SRI Technical Report CSL-147, </type> <institution> SRI International, </institution> <address> 333 Ravenswood Ave, Menlo Park, CA 94025, </address> <month> April </month> <year> 1985. </year>
Reference-contexts: Some form of throttling is needed: bounding channel buffers is one solution; pre-emptive scheduling is another. Neither solution is elegant, suggesting that perhaps a hybrid demand- and data-driven solution is really needed. Ashcroft <ref> [9] </ref> proposes a machine architecture for Lucid based on mixing both methods within the same network. Edges of the network graph are coloured according to whether they are data CHAPTER 3. DATAFLOW PROCESS NETWORKS 55 driven or demand-driven. <p> ! Stream fi To define a function that produces a stream containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver <ref> [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] </ref> ! [1; 2; 3; 4; 5; 5; 5; : : :] If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one
Reference: [10] <author> Tom Axford and Mike Joy. </author> <title> List processing primitives for parallel computation. </title> <booktitle> Computer Language, </booktitle> <volume> 19(1) </volume> <pages> 1-17, </pages> <year> 1993. </year>
Reference-contexts: The fby operator produces a stream containing the first element of its left argument, followed by its right argument. So, if x = [1; 2; 3; 4], then runner produces zero followed by itself summed with x|that is, <ref> [0; 1; 3; 6; 10] </ref>. The dataflow process network model [87] is lower-level, since communication is explicit. A process is formed by repeatedly firing an "actor"; a complete program consists of a network of actors. <p> In addition, the programmer is more likely to be better able to understand the behaviour of a parallel implementation, and thus achieve better performance. Axford and Joy <ref> [10] </ref>, for example, propose a set of list primitives that allow parallel evaluation of list functions, including the following: - [x] is the list containing one element. - s ++ t is the concatenation of lists s and t. splits is a pair of lists, with the split point chosen non-deterministically. <p> ! Stream fi To define a function that produces a stream containing the first five elements of a list followed by an infinite number of copies of the fifth element, we write: fiver xs = let ys = take 5 xs in ys ++ repeat (last ys) For example, fiver <ref> [1; 2; 3; 4; 5; 6; 7; 8; 9; 10] </ref> ! [1; 2; 3; 4; 5; 5; 5; : : :] If we interpret the functions used in fiver as processes, there are four: take, ++, repeat, and last. ++ and last do almost no work: ++ outputs elements of one
Reference: [11] <author> John Backus. </author> <title> Can programming be liberated from the Von Neumann style? Communications of the ACM, </title> <booktitle> 21(8) </booktitle> <pages> 613-641, </pages> <year> 1978. </year>
Reference-contexts: ESTL also includes a visual notation for a polymorphic type system; this may be a good starting point for Visual Haskell's missing type notation. Poswig et al describe VisaVis [110], a visual functional language based on Backus' functional programming language FP <ref> [11] </ref>. The language is essentially a visual dataflow language, with explicit support for higher-order functions: function arguments are placed within icons of higher-order functions. The VisaVis editor, implemented in Smalltalk, supports some interesting concepts to make the act of programming easier. <p> Because all functions on streams are defined in terms of the six primitive functions, the programmer does not define recursive functions on streams. This style of programming is advocated by Backus <ref> [11] </ref> and by Bird [20], because: i) non-recursive programs tend to be more concise than recursive ones; and ii) there is less chance of making errors, such as forgetting a termination condition.
Reference: [12] <author> Brian Barrera and Edward A. Lee. </author> <title> Multirate signal processing in Comdisco's SPW. </title> <booktitle> In ICASSP 91, </booktitle> <pages> pages 1113-1116, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The computational model on which these systems are based is called pipeline dataflow . The combination of a visual interface with pipeline dataflow is well-established in several fields, including signal processing <ref> [88, 12, 85] </ref>, image processing and visualisation [112, 83], instrumentation [82], and general-purpose visual programming languages [99, 59]. Signal processing systems are based on a special class of pipeline dataflow, dataflow process net 1 CHAPTER 1. INTRODUCTION 2 works (see chapter 3).
Reference: [13] <author> Albert Benveniste, Paul Caspi, Paul Le Guernic, and Nicolas Halbwachs. </author> <title> Data-flow synchronous languages. In A Decade of Concurrency: </title> <booktitle> Reflections and Perspective, </booktitle> <pages> pages 1-45. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1993. </year> <note> 195 BIBLIOGRAPHY 196 </note>
Reference-contexts: Because it explicitly models time, it is more general than dynamic dataflow systems employing switch and select actors [26]. It contrasts with the approach taken in the family of "synchronous" (in a different sense to SDF) languages <ref> [13] </ref>, which compile dataflow-like programs into finite-state machines. <p> From the earliest input streams, the earliest time at which any token is produced is calculated; from the latest output streams, the latest time at which any token can be produced is calculated. The synchronous languages <ref> [13] </ref> also explicitly model time. They are based on the synchrony hypothesis: each reaction to an event is supposed to be instantaneous [15]. Inter-process communication in Esterel, for example, is done by broadcasting events to all processes; the broadcast and all responses to it are instantaneous. <p> Further work is needed to make this set of functions CHAPTER 7. SUMMARY 194 complete, and to relate these functions to phased-form dataflow actors. Also, the relation of this work to synchronous languages <ref> [13] </ref> needs to be clarified. The exploration of dynamic process networks in sections 6.4 and 6.5 is intriguing, but further examples need to be developed.
Reference: [14] <author> Albert Benveniste, Paul Le Guernic, and Christian Jacquenot. </author> <title> Synchronous programming with events and relations: the SIGNAL language and its semantics. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 16(2) </volume> <pages> 103-149, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: This simplifying assumption allows a precise specification of time semantics and compilation of programs into finite-state automata. The family of synchronous languages includes Esterel [15], Lustre [33], Signal <ref> [14] </ref>, and RLucid, a version of Lucid extended with time-stamps [109]. CHAPTER 6. DYNAMIC PROCESS NETWORKS 169 6.2 Timed signals and streams In section 2.4.1, a non-uniformly clocked signal e x was defined at times given by its clock , e t x .
Reference: [15] <author> Gerard Berry and Georges Gonthier. </author> <title> The ESTEREL synchronous programming language. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 19 </volume> <pages> 87-152, </pages> <year> 1992. </year>
Reference-contexts: The synchronous languages [13] also explicitly model time. They are based on the synchrony hypothesis: each reaction to an event is supposed to be instantaneous <ref> [15] </ref>. Inter-process communication in Esterel, for example, is done by broadcasting events to all processes; the broadcast and all responses to it are instantaneous. This simplifying assumption allows a precise specification of time semantics and compilation of programs into finite-state automata. The family of synchronous languages includes Esterel [15], Lustre [33], <p> be instantaneous <ref> [15] </ref>. Inter-process communication in Esterel, for example, is done by broadcasting events to all processes; the broadcast and all responses to it are instantaneous. This simplifying assumption allows a precise specification of time semantics and compilation of programs into finite-state automata. The family of synchronous languages includes Esterel [15], Lustre [33], Signal [14], and RLucid, a version of Lucid extended with time-stamps [109]. CHAPTER 6. DYNAMIC PROCESS NETWORKS 169 6.2 Timed signals and streams In section 2.4.1, a non-uniformly clocked signal e x was defined at times given by its clock , e t x .
Reference: [16] <author> Shuvra S. Bhattacharyya and Edward A. Lee. </author> <title> Scheduling synchronous dataflow graphs for efficient looping. </title> <journal> Journal of VLSI Signal Processing, </journal> <volume> 6, </volume> <year> 1993. </year>
Reference-contexts: Because embedded DSP processors often have severely limited memory space, more sophisticated scheduling techniques have been developed that produce loops in the generated output code, thus minimising the code memory required <ref> [16] </ref>. 3.1.3 Kahn's process networks Kahn described the semantics of a language for parallel programming based on process networks [75]. In Kahn's language, a program consists of a network of "computing stations" (that is, processes) connected by FIFO-buffered channels.
Reference: [17] <author> J. C. Bier, E. E. Goei, W. H. Ho, P. D. Lapsley, M. P. O'Reilly, G. C. Sih, and E. A. Lee. Gabriel: </author> <title> A design environment for DSP. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 28-45, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Two of the most successful interface paradigms in visual programming languages are dataflow and forms [80]. In signal processing, dataflow visual languages have been used for some years <ref> [17, 88] </ref>, because of the common use of block diagrams in describing signal processing systems. They have been used successfully in other fields as well, such as image processing and instrumentation [112]; Hils [59] surveys a number of dataflow visual languages. <p> Chapter 6 Dynamic Process Networks In the previous chapter, I considered static, SDF networks expressed in Haskell. The use of SDF networks in block-diagram systems like Ptolemy [88] is well-established, and SDF networks are adequate to describe the bulk of signal processing systems|Ptolemy's predecessor, Gabriel <ref> [17] </ref>, supported only SDF. As soon as we consider systems which interact with asynchronous events, however, synchronous dataflow is inadequate. For example, consider the "digital gain control" shown in figure 6.1. The gain control signal has a value only when a person changes the position of a gain control knob.
Reference: [18] <author> Richard Bird and Philip Wadler. </author> <title> Introduction to Functional Programming. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: CHAPTER 1. INTRODUCTION 6 An interesting possibility opened up by incorporating a functional language into the programming framework is that of program transformation. Program transformation is advocated in functional programming texts as a means of obtaining "efficient" realisation of programs from "inefficient" (but still executable) specifications <ref> [18] </ref>. In the context of dataflow networks, it offers a means of performing provably-correct alterations to the structure of a network (section 5.5).
Reference: [19] <author> R.S. Bird. </author> <title> Using circular programs to eliminate multiple traversals of data. </title> <journal> Acta Informatica, </journal> <volume> 21(3) </volume> <pages> 239-250, </pages> <year> 1984. </year>
Reference-contexts: Laziness is a nice property for a number of reasons. Firstly, it is necessary to guarantee referential transparency: an expression can always be substituted with an equivalent expression without changing program meaning. Secondly, it allows the use of some programming techniques that cannot be used in non-lazy languages|see <ref> [19, 66, 147] </ref> for examples of programs that can only be constructed in a lazy language. For our purposes, laziness makes it easy to build infinite data structures and therefore to simulate pipeline dataflow systems.
Reference: [20] <author> R.S. Bird. </author> <title> Lectures on constructive functional programming. </title> <type> Technical Report Technical Monograph PRG-69, </type> <institution> Oxford University Computing Laboratory, Programming Research Group, Oxford, </institution> <year> 1988. </year>
Reference-contexts: Because all functions on streams are defined in terms of the six primitive functions, the programmer does not define recursive functions on streams. This style of programming is advocated by Backus [11] and by Bird <ref> [20] </ref>, because: i) non-recursive programs tend to be more concise than recursive ones; and ii) there is less chance of making errors, such as forgetting a termination condition. The final task of this chapter is to adapt the algebraic style of program transformation to streams and processes. <p> In this section, I will demonstrate the algebraic style of program transformation, exemplified by the Bird-Meertens formalism (BMF) <ref> [20] </ref>. The key characteristic of this style is that it makes extensive use of a "catalogue" of known algebraic laws of functions, rather than relying on discovery through lower-level methods. BMF, also known as "Squiggol," is being developed as a "programming by calculation" method of developing programs from specifications. <p> BMF, also known as "Squiggol," is being developed as a "programming by calculation" method of developing programs from specifications. I will use Haskell as the notation for writing transformations, in a similar manner to [21], instead of the more concise notation of <ref> [20] </ref>. The key contribution of this section is to adapt the Bird-Meertens theory of lists to work with streams; the result is a powerful tool for restructuring process networks. Particularly powerful transformations become available for networks constructed with higher-order functions.
Reference: [21] <author> R.S. Bird. </author> <title> Algebraic identities for program calculation. </title> <journal> The Computer Journal, </journal> <volume> 32(2) </volume> <pages> 122-126, </pages> <year> 1989. </year>
Reference-contexts: BMF, also known as "Squiggol," is being developed as a "programming by calculation" method of developing programs from specifications. I will use Haskell as the notation for writing transformations, in a similar manner to <ref> [21] </ref>, instead of the more concise notation of [20]. The key contribution of this section is to adapt the Bird-Meertens theory of lists to work with streams; the result is a powerful tool for restructuring process networks. Particularly powerful transformations become available for networks constructed with higher-order functions. <p> The right-hand side CHAPTER 5. STATIC PROCESS NETWORKS 163 (figure 5.24b) is a pipeline of processes, each parameterised by its own i from vector v. 5.5.5 Promotion The fourth class of identity is derived from Bird's "promotion" identities <ref> [21] </ref>. <p> the fold functions; on foldl, the law is 3 foldl f a concat foldl (foldl f ) a Laws of this kind are useful for controlling the grain size of processes in a pipeline - see the derivation of the pipeline FIR filter in [123] for an example. 3 Bird <ref> [21, page 123] </ref> gives this law as a variant of the fold promotion law for non-associative operators. CHAPTER 5. STATIC PROCESS NETWORKS 165 5.6 Summary This chapter presented an approach to programming dataflow networks in a functional programming language.
Reference: [22] <author> Tore A. Bratvold. </author> <title> Skeleton-Based Parallelisation of Functiona Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computing and Electrical Engineering, Heriot-Watt University, Edinburgh, UK, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: The programmer can therefore: i) choose a skeleton that most easily represents the problem to be solved, and then ii) transform the program, based on abstract performance measures, into a skeleton that is more efficient on a given machine architecture. Bratvold <ref> [22] </ref> proposes a different set of skeletons: map, filter, fold, and composition. Functionally these functions are the same as Haskell's map, filter, foldl, and (:). The first three are implemented as processor "farms," and composition as pipeline connection of farms. <p> Prototyping image processing programs in a functional language is advocated by Michaelson et al , who use four higher-order functions to code a range of image processing algorithms [97]. In related work, Bratvold compiles these higher-order functions into parallel code for a Meiko Transputer system <ref> [22] </ref>. 5.2 Vectors Vectors play an important role in signal processing. Recall from section 2.4.4 that the instruction sets of modern DSPs support very efficient vector operations. This section describes a Vector datatype implemented in Haskell, and a set of functions in Haskell that can be implemented efficiently on DSPs.
Reference: [23] <author> K. P. Brooks. </author> <title> Lilac: A two-view document editor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 7-19, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: This "two-view" idea is not new: a multiple-view program development environment has been proposed for procedural languages [124]; and two-view document formatting systems seem a natural synthesis of the compiled and WYSIWYG styles of document production <ref> [23] </ref>. The choice of a lazy language is not without its drawbacks: there is an inevitable overhead associated with building a run-time representation of unevaluated expressions. This CHAPTER 1.
Reference: [24] <author> Manfred Broy. </author> <title> Applicative real-time programming. </title> <editor> In R. E. A. Mason, editor, </editor> <booktitle> Information Processing 83, </booktitle> <year> 1983. </year>
Reference-contexts: There are two key approaches to modelling time: Insert hiatons into the stream. A hiaton is a special token representing the passage of time [144]. 166 CHAPTER 6. DYNAMIC PROCESS NETWORKS 167 Attach a time-stamp to each token, denoting the real time at which the token occurs <ref> [24] </ref>. In this chapter, I develop a hybrid approach suitable for real-time implementation: a hiaton can represent any number of ticks of the "base clock" to which all timing of the stream is quantised.
Reference: [25] <author> Joseph Buck, Soonhoi Ha, Edward A. Lee, and David G. Messerschmitt. Ptolemy: </author> <title> A framework for simulating and prototyping heterogenous systems. </title> <journal> International Journal of Computer Simulation, 1992. Special issue on "Simulation Software Development". </journal>
Reference-contexts: A general approach to supporting multiple models of computation has been taken in Ptolemy: a domain is a graph that supports a given model of computation; this graph has an interface that allows it to be embedded within graphs of a different domain <ref> [25] </ref>. The "dynamic dataflow" (DDF) domain [42] implements dynamic scheduling of dataflow graphs. In the DDF domain, input channels to the graph are initialised with a fixed number of tokens, and the actors fired until no further firings can be made. This allows CHAPTER 6.
Reference: [26] <author> Joseph T. Buck. </author> <title> Scheduling Dynamic Dataflow Graphs with Bounded Memory Using the Token Flow Model. </title> <type> PhD thesis, </type> <institution> Electrical Engineering and Computer Sciences, University of California Berkeley, </institution> <year> 1993. </year>
Reference-contexts: The efficiency improvement over dynamic scheduling is so great that considerable effort has been expended in dataflow models that are less restricted than SDF, but which are sufficiently constrained that SDF code can be applied to parts of the graph <ref> [26, 43] </ref>. SDF code generators operate by "firing" code generation actors at compile-time; each time an actor is fired, it emits C or assembler code. For assembler code generation, macros and symbolic names make the programmer's task easier and allow some optimisations. <p> Buck extends scheduling techniques developed for boolean dataflow (switch and select are the canonical boolean dataflow actors) to determine if a graph containing integer dataflow actors has bounded schedules, and to "cluster" a graph in order to extract control structures such as for-loops and do-while loops <ref> [26] </ref>. 3.3.5 Execution mechanisms Phased-form actors are non-strict. In this section, I examine the implications for dynamic scheduling of phased-form actors; in the next I will look at the implications for networks. So far, I have not specified any restrictions on the granularity of phases. <p> The approach described here contrasts with other approaches to modelling time, in that it maintains a dataflow implementation focus through the use of time-passing tokens. Because it explicitly models time, it is more general than dynamic dataflow systems employing switch and select actors <ref> [26] </ref>. It contrasts with the approach taken in the family of "synchronous" (in a different sense to SDF) languages [13], which compile dataflow-like programs into finite-state machines.
Reference: [27] <author> Joseph T. Buck. </author> <title> Static scheduling and code generation from dynamic dataflow graphs with integer-valued control streams. </title> <booktitle> In 28th Asilomar Conference on Circuits, Signals and Systems, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Phased form has the potential to provide a more consistent semantics of hierarchical actors, although I have not yet been able to develop an algorithm that computes the phased form of an actor network. (See section 3.3.6.) Phased-form actors are a generalisation of Buck's multi-phase integer dataflow actors <ref> [27] </ref> and the cyclo-static dataflow of Engels et al [43]; a comparison is given in section 3.3.4. 3.3.1 Syntax Phased-form actors are an extension of standard-form actors. There are two key differences: At each firing, only a subset of the rule set is considered for firing. <p> Of the other graphs in figure 3.13, select is a Kahn process because it has no choice vertices; delay and merge are not, because they fail the equal consumption and production vector condition. 3.3.4 Cyclo-static and multi-phase integer dataflow Buck <ref> [27] </ref> proposed multi-phase integer dataflow actors. An integer dataflow actor is such that the number of tokens consumed or produced on each arc is a function of an integer-valued control token. <p> It is also possible to analyse a graph to extract the constructs encoded therein <ref> [27] </ref>. Nonetheless, Kodosky et al [82] argue that the dataflow paradigm should be extended to incorporate structured constructs. They add loops, a case construct, and a sequencing construct to the Labview language, calling the result "structured dataflow." General-purpose visual dataflow languages generally provide such constructs|see Hils' survey [59].
Reference: [28] <author> W. H. Burge. </author> <title> Stream processing functions. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 19(1) </volume> <pages> 12-25, </pages> <month> January </month> <year> 1975. </year> <note> BIBLIOGRAPHY 197 </note>
Reference-contexts: Stream elements are thus "evaluated when they are come to"|calculation of each successive loop control value and execution of the loop body proceed in an interleaved manner. Burge further developed the idea of streams, as a way of structuring programs as a set of independent sub-programs <ref> [28] </ref>.
Reference: [29] <author> Margaret Burnett and Benjamin Summers. </author> <title> Some real-world uses of visual programming systems. </title> <type> Technical Report TR 94-60-7, </type> <institution> Oregon State University, </institution> <year> 1994. </year>
Reference-contexts: Although still the subject of disagreement on their worth, there are nonetheless many "real-world" projects that use visual programming <ref> [29] </ref>. The success of these systems indicates very strongly that visual programming|if applied to appropriate problem domains and supported by appropriate tools|offers a kind of understanding that is absent in purely-textual languages. Two of the most successful interface paradigms in visual programming languages are dataflow and forms [80].
Reference: [30] <author> David Busvine. </author> <title> Implementing recursive functions as proceeor farms. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1141-1153, </pages> <year> 1993. </year>
Reference-contexts: His compiler uses profiling and performance prediction to decide when to execute a skeleton in parallel; in addition, this approach allows the compiler can detect parallelism in recursive functions <ref> [30] </ref>.
Reference: [31] <author> Luca Cardelli. </author> <title> Two-dimensional syntax for functional languages. </title> <booktitle> In Proc. Integrated Interactive Computing Systems, </booktitle> <pages> pages 107-119, </pages> <year> 1983. </year>
Reference-contexts: Reade [113], for example, explains function application using box-and-arrow diagrams; Kelly [78] illustrates networks of streams and processes written using an annotated pure functional language; Waugh et al [152] illustrate the effect of program transformation on parallelism. There have also been proposals for formal visual functional languages. Cardelli <ref> [31] </ref> proposed a visual functional language in which function application is denoted by juxtaposition of a function name with its arguments; bracketing is denoted by containing boxes; pattern-matching can select one of several expressions; bindings are denoted by placing a CHAPTER 4. <p> To generate Haskell code from a completed Visual Haskell function, the system traverses the graph, generating (linear) Haskell code on the way|there is never any need to parse a two-dimensional picture. Cardelli made much the same observation, although his system translated abstract syntax trees into visual representations <ref> [31] </ref>. Despite the fact that Visual Haskell does not include all of Haskell, Dawson's system was able to generate executable Haskell functions. The type declaration for each function was inserted textually into a form associated with the function; other information in the form included the numbers of inputs and outputs.
Reference: [32] <author> Nicholas Carriero and David Gelernter. </author> <title> How to Write Parallel Programs: A First Course. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: This section reviews key models of parallelism; they are shown as a hierarchy in figure 2.1. 11 CHAPTER 2. BACKGROUND MATERIAL 12 2.1.1 A "meta-model" of parallelism Carriero and Gelernter suggest three "paradigms" of parallelism <ref> [32] </ref>: Result parallelism Result parallelism focuses on the structure of the solution. Paral lelism arises by simultaneous production of components of the result. Specialist parallelism Specialist parallelism focuses on the kind of knowledge needed to produce a solution. <p> I identify four distinct classes of transformation suitable for use with process networks. The transformations given here are common cases, and are by no means an exhaustive catalogue. Transforming programs to improve performance is also advocated by Carriero and Gelernter <ref> [32] </ref>, who transform between programs expressed in the most "natural" of their three categories of parallelism, into a more efficiently-implementable category. However, their transformations are ad-hoc, lacking the equational basis of the Bird-Meertens style. CHAPTER 5.
Reference: [33] <author> P. Caspi, D. Pilaud, N. Halbwachs, and J. A. Plaice. LUSTRE: </author> <title> A declaration language for programming synchronous systems. </title> <booktitle> In 14th ACM Symp. on Principles of Programming Languages, </booktitle> <address> Munich, West Germany, </address> <pages> pages 178-188, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Inter-process communication in Esterel, for example, is done by broadcasting events to all processes; the broadcast and all responses to it are instantaneous. This simplifying assumption allows a precise specification of time semantics and compilation of programs into finite-state automata. The family of synchronous languages includes Esterel [15], Lustre <ref> [33] </ref>, Signal [14], and RLucid, a version of Lucid extended with time-stamps [109]. CHAPTER 6. DYNAMIC PROCESS NETWORKS 169 6.2 Timed signals and streams In section 2.4.1, a non-uniformly clocked signal e x was defined at times given by its clock , e t x .
Reference: [34] <author> Rulph Chassaing. </author> <title> Digital Signal Processing with C and the TMS320C30. Topics in Digital Signal Processing. </title> <publisher> John Wiley and Sons, </publisher> <year> 1992. </year>
Reference-contexts: In this section, I give an overview of the characteristics of these devices. I focus only on the modern, floating-point devices; for more detailed information on specific devices, see <ref> [115, 119, 34, 5, 6, 98, 138, 137] </ref>. The device contains six functional units|an ALU, a multiplier, two address units, and two load-store units (not shown here)|all of which can operate simultaneously, a bank of floating-point data registers, and a bank of address registers.
Reference: [35] <author> Murray I. Cole. </author> <title> Algorithmic Skeletons: Structured Management of Parallel Computation. </title> <publisher> Pitman Publishing, </publisher> <year> 1989. </year>
Reference-contexts: The current implementation supports only head-hyper-strict streams|in other words, only normal-form elements can be sent over channels. 2.3.7 Skeletons Section 2.3.4 showed that parallelism can be captured in terms of data structures. In contrast, Cole's algorithmic skeletons <ref> [35] </ref> capture parallel computation in terms of a relatively abstract algorithmic description: a "skeleton" is a particular pattern of parallel computation. Although Cole used an imperative framework, he points out that higher-order functions are an ideal mechanism for expressing skeletons.
Reference: [36] <author> Michael Colefax. </author> <title> A realtime polyphonic music synthesiser. </title> <type> Technical report, </type> <institution> School of Electrical Engineering, University of Technology, </institution> <address> Sydney, </address> <month> November </month> <year> 1993. </year> <type> Undergraduate thesis report. </type>
Reference-contexts: That it is possible has been shown by the implementation of a polyphonic synthesiser in C on a single TMS320C30, by Michael Colefax <ref> [36] </ref>. Colefax' program implements a synthesiser with a similar (although simpler) structure to that of the following Haskell program. It can generate up to 17 notes when generating sine waves, but only two with formant-wave-function (FWF) synthesis (section 6.5.4).
Reference: [37] <author> Gennaro Costagliola, Genoveffa Tortora, Sergio Orefice, and Andrea de Lucia. </author> <title> Automatic generation of visual programming environments. </title> <journal> IEEE Computer, </journal> <volume> 28(3) </volume> <pages> 56-66, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Visual languages cannot be represented by conventional grammars and other means of describing programs, and are often described by example. There is, however, an increasing amount of work on grammars for visual syntax; this is particularly relevant to work on automatic generation of visual language parsers, as in <ref> [37] </ref>. Golin and Reiss [53], for example, use a "picture layout grammar." The underlying grammar model is based on multisets to eliminate the ordering implicit in textual grammars.
Reference: [38] <author> Stuart Cox, Shell-Ying Huang, Paul Kelly, Junxian Liu, and Frank Taylor. </author> <title> An implementation of static functional process networks. </title> <booktitle> In PARLE'92|Parallel Architectures and Languages Europe, </booktitle> <pages> pages 497-512. </pages> <publisher> Springer Verlag, </publisher> <year> 1992. </year> <note> LNCS 605. </note>
Reference-contexts: BACKGROUND MATERIAL 35 Using pipeline, the above network can be written pipeline [map abs; scanl (+) 0] xs (2.1) The current implementation of Caliban requires that the process network be static. At compile-time, all moreover annotations are expanded to extract a single top-level annotation which describes a static network <ref> [38] </ref>. Kelly suggests that strictness analysis or programmer annotations be used to min-imise or eliminate the overheads associated with sending non-normal form expressions over channels.
Reference: [39] <author> Alan D. Culloch. </author> <title> Porting the 3L Parallel C environment to the Texas Instruments TMS320C40. </title> <editor> In A. Veronis and Y. Paker, editors, </editor> <booktitle> Transputer Research and Applications 5. </booktitle> <publisher> IOS Press, </publisher> <year> 1992. </year>
Reference-contexts: In effect, the programmer writes many separate programs; embedded within each program are commands for communication and synchronisation with other programs. An example of a control-parallel language implementation for distributed-memory DSP machines is Parallel C <ref> [39] </ref>, based very loosely on Hoare's CSP (Communicating Sequential CHAPTER 2. BACKGROUND MATERIAL 16 parallelism; c) Linda; d) pipeline parallelism Processes) formalism [60]. The programmer writes a number of tasks|that is, independent C programs. <p> Another approach uses light-weight operating system kernels and support libraries <ref> [104, 39, 143] </ref>. This is particular evident in parallel systems, where the added complexities of multi-processor communication and synchronisation seem to encourage adoption of multi-tasking kernels. The approach in which I am interested here is to use a programming language, but at a much higher level than C.
Reference: [40] <author> J. Darlington, A. J. Field, P. G. Harrison, P. H. J. Kelly, D. W. N. Sharp, Q. Wu, and R. L. </author> <title> While. Parallel programming using skeleton functions. </title> <booktitle> In PARLE'93| Parallel Architectures and Languages Europe, </booktitle> <pages> pages 146-160. </pages> <publisher> Springer Verlag, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: Although Cole used an imperative framework, he points out that higher-order functions are an ideal mechanism for expressing skeletons. The idea of selecting a parallel program structure from a catalogue of such structures is appealing. Darlington et al <ref> [40] </ref> adapt Cole's idea to a functional language framework. They point out that a skeleton has an implementation-independent meaning, as given by its functional definitions, and a behaviour tailored to the target parallel machine| parallelism arises from the behavioural aspect.
Reference: [41] <author> Ken Dawson. </author> <title> Visual Haskell editor and parser. </title> <type> Technical report, </type> <institution> School of Electrical Engineering, University of Technology, </institution> <address> Sydney, </address> <month> November </month> <year> 1993. </year> <type> Undergraduate thesis report. </type>
Reference-contexts: The original paper also contains motivating examples of possible uses of Visual Haskell, and a screen dump of the prototype Visual Haskell editor written by Ken Dawson <ref> [41] </ref>. Chapter 2 Background Material Because this thesis covers quite a broad range of topics, I have collected into this chapter some useful background material. Firstly, I give a broad overview of models of parallel computation in terms of a simple "meta-model" of computation. <p> Visual Haskell evolved out of a need to explain some of the work in this thesis to non-functional-programmers. It began as an ad-hoc notation for "drawing" functional programs (see [114]); later, Ken Dawson of UTS implemented a prototype editor for this language <ref> [41] </ref>, stimulating its development into a more precise and usable language. The style of Visual Haskell is based on dataflow: programs are described as data and function boxes, connected by arcs representing flow of data. <p> Visual Haskell, however, does not care about the direction, and some of the diagrams in this thesis use left-to-right data flow instead. A prototype visual editor was constructed by Ken Dawson as an undergraduate project <ref> [41] </ref>; the editor indicates that a two-view development system based on Visual Haskell is feasible. One interesting point noted during the construction of this prototype is that a visual parser is not needed to interpret visual programs. <p> Although there is no implementation of the complete framework, I believe I have laid much of the groundwork for further implementation. Apart from the working Haskell prototype code (appendix A and in the text), portions of the framework have been implemented: Dawson's prototype Visual Haskell editor <ref> [41] </ref>; the two implementations of SPOOK|Signal Processing Object-Oriented Kernel|described in [118] and [96] (see also page 9); and some portions of a compiler for modern digital signal processors [119]. Key to the framework is an efficient implementation model. <p> When used to draw functions on streams, it closely resembles block diagrams (see, for example, figure 5.11). Dawson's prototype implementation of the language <ref> [41] </ref> demonstrates the feasibility of constructing an editor for it. Throughout the thesis, I have maintained that a functional programming language is an excellent vehicle for expressing dataflow and "block-diagram" style systems.
Reference: [42] <institution> Electronics Research Laboratory, University of California Berkeley. </institution> <note> The Almagest: Ptolemy User's Manual Version 0.5, 1994. BIBLIOGRAPHY 198 </note>
Reference-contexts: A general approach to supporting multiple models of computation has been taken in Ptolemy: a domain is a graph that supports a given model of computation; this graph has an interface that allows it to be embedded within graphs of a different domain [25]. The "dynamic dataflow" (DDF) domain <ref> [42] </ref> implements dynamic scheduling of dataflow graphs. In the DDF domain, input channels to the graph are initialised with a fixed number of tokens, and the actors fired until no further firings can be made. This allows CHAPTER 6. <p> This allows CHAPTER 6. DYNAMIC PROCESS NETWORKS 168 a DDF domain to be embedded within an SDF domain. An entirely different approach to time is taken in the discrete-event (DE) domain <ref> [42] </ref>. Here, actors operate on timed events; a global clock is used to order actor firings correctly in time, according to the times of occurrence of the actor's input events. None of these approaches is, however, really suitable for figure 6.1.
Reference: [43] <author> Marc Engels, Greet Bilson, Rudy Lauwereins, and Jean Peperstrate. </author> <title> Cyclo-static dataflow: Model and implementation. </title> <booktitle> In 28th Asilomar Conference on Circuits, Signals and Systems, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: The efficiency improvement over dynamic scheduling is so great that considerable effort has been expended in dataflow models that are less restricted than SDF, but which are sufficiently constrained that SDF code can be applied to parts of the graph <ref> [26, 43] </ref>. SDF code generators operate by "firing" code generation actors at compile-time; each time an actor is fired, it emits C or assembler code. For assembler code generation, macros and symbolic names make the programmer's task easier and allow some optimisations. <p> a more consistent semantics of hierarchical actors, although I have not yet been able to develop an algorithm that computes the phased form of an actor network. (See section 3.3.6.) Phased-form actors are a generalisation of Buck's multi-phase integer dataflow actors [27] and the cyclo-static dataflow of Engels et al <ref> [43] </ref>; a comparison is given in section 3.3.4. 3.3.1 Syntax Phased-form actors are an extension of standard-form actors. There are two key differences: At each firing, only a subset of the rule set is considered for firing. This subset is called the eligible rule set. <p> Except for its consumption and production vectors, this graph is exactly the same as that of another special class of actor, the cyclo-static synchronous dataflow (CSSDF) actors proposed by Engels et al <ref> [43] </ref>.
Reference: [44] <editor> Paul Hudak et al. </editor> <title> Report on the functional programming language Haskell, a non-strict purely-functional language, version 1.2. </title> <journal> SIGPLAN Notices, </journal> <month> May </month> <year> 1992. </year>
Reference-contexts: In this thesis, I will use Haskell as the high-level language; Haskell is a modern functional language that provides a common ground for research into functional languages and functional programming <ref> [44] </ref>. The visual language is Visual Haskell, a visual equivalent for Haskell of my own design. In the next two sections I summarise the arguments in favour of adding this third dimension. <p> For example, the class of ordered types Ord inherits from the class of equable types Eq: class Eq a =&gt; Ord a where (&lt;), (&lt;=), (&gt;), (&gt;=) :: a -&gt; a -&gt; Bool ... The standard prelude implements a full hierarchy of primitive and numeric types|see <ref> [44] </ref> for details. CHAPTER 2. BACKGROUND MATERIAL 27 2.3 Evaluation mechanisms Implementations of lazy functional languages are often based on graph reduction. This section briefly reviews graph reduction and other topics related to evaluation of functional programs. <p> Finally, a key difference between functional and dataflow languages is the treatment of I/O. In a functional language, I/O is performed in two key ways <ref> [44] </ref>: i) by calling system primitives with lazy lists representing the whole history of input or output; or ii) by calling system primitives with a continuation function. <p> To illustrate, figure 4.17a shows the expression let t = w * x1 in (x0+t, x0-t) CHAPTER 4. VISUAL HASKELL 110 A series of conditional (if-then-else) expressions is grouped into a single visual construct (figure 4.16b). (Translation into case-expressions, as in the Haskell report <ref> [44] </ref>, would be too cumbersome visually.) Each guard g i and its consequent e i are placed into one horizontal section of the containing box, but separated by the heavy arrow glyph.
Reference: [45] <author> Antony A. Faustini and Edgar B. Lewis. </author> <title> Toward a real-time dataflow language. </title> <journal> IEEE Software, </journal> <pages> pages 29-35, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Other languages that explicitly model time include real-time extensions to Lucid, and the synchronous languages. Lucid [144] is a dataflow-like language, in which all data|even constants|occurs in streams. Faustini and Lewis <ref> [45] </ref> propose that each stream also have an associated stream of time windows, where a time window is a pair (a; b) that denotes the earliest time at which the corresponding daton can be produced, and the latest time it needs to be available.
Reference: [46] <author> John T. Feo, David C. Cann, and Rodney R. Oldehoeft. </author> <title> A report on the SISAL language project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10 </volume> <pages> 349-366, </pages> <year> 1990. </year>
Reference-contexts: A process suspends whenever it attempts to read from a channel with no data; it is resumed by the operating system some time after data is written to the channel. The implementation of stream-processing functions in SISAL <ref> [46] </ref> uses this technique [50]. 3.1.4 Dataflow processes Lee defines a dataflow process to be the sequence of firings of a dataflow actor [87].
Reference: [47] <author> Antony J. Field and Peter G. Harrison. </author> <title> Functional Programming. </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: Dataflow architectures have also been proposed for implementation of functional programming languages [141]. Field and Har-rison point out that demand-driven evaluation of functional programs gives normal-order semantics, while data-driven evaluation gives applicative-order semantics <ref> [47, chapter 14] </ref>. 3.1.5 Firing rules Lee has recently formalised the notion of "firing" an actor [87].
Reference: [48] <author> Markus Freericks and Alaois Knoll. </author> <title> Formally correct translation of DSP algorithms specified in an asynchronous applicative language. </title> <booktitle> In ICASSP 93, </booktitle> <address> Minneapolis, USA, pages I-417-I-420, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Arithmetic operations extend point-wise to streams. Silage also supports fixed-point data types and arithmetic, an important aspect of programming real DSP devices. Freericks and Knoll use recursive functions to define signal-processing functions [81]. Higher-order functions can also be used to capture particular patterns of recursion <ref> [48] </ref>. An unusual aspect of their language is an explicit suspension construct that supports data-driven programming; this is used to code interfaces to real-time I/O channels. They propose to use partial evaluation in their compiler to eliminate the potential run-time overhead of recursion.
Reference: [49] <author> Daniel D. Gajski, David A. Padua, David J. Kuck, and Robert H. Kuhn. </author> <title> A second opinion on data flow machines and languages. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 58-69, </pages> <month> February </month> <year> 1982. </year>
Reference-contexts: In both cases, the languages are side-effect free. However, Gajski et al point out that parallelising compilers can in fact perform better than single-assignment languages <ref> [49] </ref>. Implicit parallelism is also a key ingredient of modern single-processor compilers. Modern processors (including DSP micro-processors) exhibit significant instruction-level parallelism, which compilers must attempt to exploit. 2.1.3 Data parallelism Data parallelism is parallelism at the level of elements of a data set.
Reference: [50] <author> H. Garsden and A. L. Wendelborn. </author> <title> Experiments with pipelining parallelism in SISAL. </title> <booktitle> In 25th Intl. Hawaii Conf. on System Sciences, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: A process suspends whenever it attempts to read from a channel with no data; it is resumed by the operating system some time after data is written to the channel. The implementation of stream-processing functions in SISAL [46] uses this technique <ref> [50] </ref>. 3.1.4 Dataflow processes Lee defines a dataflow process to be the sequence of firings of a dataflow actor [87].
Reference: [51] <author> Andrew Gill, John Launchbury, and Simon L. Peyton Jones. </author> <title> A short cut to deforestation. </title> <booktitle> In Functional Languages and Computer Architecture (FPCA) 93, </booktitle> <year> 1993. </year>
Reference-contexts: This led to the development of (successively) the techniques of listlessness [146] and deforestation [148], aimed at automatic fusion of list functions for program efficiency. More recently, Gill et al discovered an improved method <ref> [51] </ref>, which is now part of some Haskell compilers. It is not easy to see, however, how these techniques can be applied to our purposes, for two reasons.
Reference: [52] <author> David M. Globirsch. </author> <title> An introduction to Haskell with applications to digital signal processing. </title> <type> Technical report, </type> <institution> The MITRE Corporation, </institution> <address> 7525 Colshire Drive, McLean, Virginia 22102-3481, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Landin and Burge, for example, use a function to represent the remainder of the stream; lazy functional languages implement streams as recursively-defined, lazily-evaluated lists. CHAPTER 5. STATIC PROCESS NETWORKS 126 The ease with which lazily-evaluated streams lend themselves to signal processing has also been noted by Globirsch <ref> [52] </ref>. He uses lazy lists to simulate signals, and gives a number of simple filters in Haskell, suggesting that the conciseness of the programs makes Haskell an excellent tool for prototyping.
Reference: [53] <author> Eric J. Golin and Steven P. Reiss. </author> <title> The specification of visual language syntax. </title> <booktitle> In Proc. 1989 IEEE Workshop on Visual Languages, </booktitle> <pages> pages 105-110, </pages> <address> Rome, Italy, </address> <year> 1989. </year>
Reference-contexts: There is, however, an increasing amount of work on grammars for visual syntax; this is particularly relevant to work on automatic generation of visual language parsers, as in [37]. Golin and Reiss <ref> [53] </ref>, for example, use a "picture layout grammar." The underlying grammar model is based on multisets to eliminate the ordering implicit in textual grammars.
Reference: [54] <author> Kevin Hammond. </author> <title> Parallel functional programming: An introduction. </title> <booktitle> In Proc. 1st Intl. Symp. on Parallel Symbolic Computation (PASCO '94), </booktitle> <pages> pages 181-193, </pages> <address> Hagenburg, Austria, 1994. </address> <publisher> World Scientific. </publisher>
Reference-contexts: This section briefly reviews graph reduction and other topics related to evaluation of functional programs. The aim here is to place into context the suggested implementation (in chapter 5) of stream functions as dataflow actors. A recent survey of issues in parallel functional programming is given by Hammond <ref> [54] </ref>. 2.3.1 Graph reduction Evaluation of a functional program proceeds by successively reducing expressions. The evaluator chooses a redex (for red ucible ex pression), reduces it, and then repeats. An expression that contains no redexes is in normal form.
Reference: [55] <author> Philip J. Hatcher and Michael J. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: In a data-parallel language, an operation over all elements of a data set is invoked by a single function call or language construct. In the DataParallel C language <ref> [55] </ref>, for example, one would calculate the inner product of two vectors by: domain vpair float x; float y; float t; - v [N]; CHAPTER 2. <p> Although this is often the case, it is important not to associate a language model with the physical hardware on which a program might run, since the connection between a language-level model of parallelism and its supposed "obvious" implementation platform is rather tenuous. For example, Hatcher and Quinn <ref> [55] </ref> describe a data-parallel language compiler for MIMD machines, while Sabot [127] describes how an SIMD computer could be used to simulate an MIMD computer. 2.1.4 Control parallelism Control parallelism is a form of functional parallelism characterised mainly by explicit communication ("message-passing") and synchronisation between processes. <p> It is criticised as being low-level and error-prone because the programmer must explicitly manage communication and synchronisation, and keep track of the internal states of many processors <ref> [55] </ref>. Although control-parallel programs are often machine-specific, there are some projects, such as PVM [136], which use a virtual machine abstraction to achieve architecture-independence. CHAPTER 2. BACKGROUND MATERIAL 17 There is no single agent-script-data model for control-parallel programs.
Reference: [56] <author> P. Henderson. </author> <title> Purely functional operating systems. In Functional Programming and its Applications. </title> <publisher> Cambridge University Press, </publisher> <year> 1982. </year>
Reference-contexts: CHAPTER 2. BACKGROUND MATERIAL 33 Apparently, an operating system that needs to deal with asynchronous events cannot be written in such a language! One approach to resolving this dilemma is to add a non-deterministic merge operator to the language <ref> [56] </ref>, which merges two lists in order of the "arrival time" of elements. Because, however, merge is referentially opaque, reasoning about programs that use it becomes difficult.
Reference: [57] <author> P. Hilfinger, J. Rabaey, D. Genin, C. Scheers, and H. De Man. </author> <title> DSP specification using the Silage language. </title> <booktitle> In ICASSP 90, </booktitle> <address> Alburqurque, New Mexico, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: This is particular evident in parallel systems, where the added complexities of multi-processor communication and synchronisation seem to encourage adoption of multi-tasking kernels. The approach in which I am interested here is to use a programming language, but at a much higher level than C. Silage <ref> [57] </ref> is perhaps the best example of a language designed specifically for DSP programming; recent work explores higher-level signal pro CHAPTER 5. STATIC PROCESS NETWORKS 125 cessing programming concepts [142]. It has also evolved into a commercial product, DFL [154].
Reference: [58] <author> Daniel D. Hils. DataVis: </author> <title> A visual programming language for scientific visualisation. </title> <booktitle> In Proc. 1991 ACM Computer Science Conference, </booktitle> <pages> pages 439-448, </pages> <address> San Antonio, Texas, </address> <month> March </month> <year> 1991. </year> <note> BIBLIOGRAPHY 199 </note>
Reference-contexts: VISUAL HASKELL 118 occurrences. In the second rule, any pict and a pattern connected by a binding arc can be attached. Argument slots Enhanced Show-and-Tell (ESTL) [99] and DataVis <ref> [58] </ref> have function slots . A function slot is a position in the icon of a higher-order function in which a function argument can be placed.
Reference: [59] <author> Daniel D. Hils. </author> <title> Visual languages and computing survey: Data flow visual programming languages. </title> <journal> Journal of Visual Languages and Computing, </journal> <volume> 3 </volume> <pages> 69-101, </pages> <year> 1992. </year>
Reference-contexts: The computational model on which these systems are based is called pipeline dataflow . The combination of a visual interface with pipeline dataflow is well-established in several fields, including signal processing [88, 12, 85], image processing and visualisation [112, 83], instrumentation [82], and general-purpose visual programming languages <ref> [99, 59] </ref>. Signal processing systems are based on a special class of pipeline dataflow, dataflow process net 1 CHAPTER 1. INTRODUCTION 2 works (see chapter 3). Block-diagram systems, although well-established, are a practical compromise of usability and implementation technology, and lack some of the features of modern programming languages. <p> In signal processing, dataflow visual languages have been used for some years [17, 88], because of the common use of block diagrams in describing signal processing systems. They have been used successfully in other fields as well, such as image processing and instrumentation [112]; Hils <ref> [59] </ref> surveys a number of dataflow visual languages. A form is a layout of cells, each containing a formula that computes its value, usually in terms of the values of other cells. <p> Nonetheless, Kodosky et al [82] argue that the dataflow paradigm should be extended to incorporate structured constructs. They add loops, a case construct, and a sequencing construct to the Labview language, calling the result "structured dataflow." General-purpose visual dataflow languages generally provide such constructs|see Hils' survey <ref> [59] </ref>. Special-purpose languages such as that of Ptolemy [88] do not, relying instead on the lower-level host language. Haskell does not have explicit loops, since it uses recursion instead of iteration. Visual Haskell does, however, have several structures that resemble structured dataflow: let-expressions, conditionals (if-then-else), -abstractions, and case-expressions. <p> If necessary, a type annotation is enclosed by a box, which can in turn be annotated. 4.5.2 Iteration boxes Most general-purpose visual programming languages support iteration in some form or another. All of the visual dataflow languages in Hils' survey <ref> [59] </ref>, for example, support CHAPTER 4. VISUAL HASKELL 119 iteration. One technique is the use of special iteration constructs; Labview, for example, incorporates for-loops and while-loops into its structured dataflow model in this way [82].
Reference: [60] <author> Charles Antony Richard Hoare. </author> <title> Communicating Sequential Processes. </title> <publisher> Prentice-Hall, </publisher> <year> 1985. </year>
Reference-contexts: An example of a control-parallel language implementation for distributed-memory DSP machines is Parallel C [39], based very loosely on Hoare's CSP (Communicating Sequential CHAPTER 2. BACKGROUND MATERIAL 16 parallelism; c) Linda; d) pipeline parallelism Processes) formalism <ref> [60] </ref>. The programmer writes a number of tasks|that is, independent C programs. A configuration file specifies the processor topology and the communications links between them, and assigns tasks to processors. Messages are exchanged by calling a message-passing library. <p> In other words, a process cannot test for the presence of data on an input channel; nor can it wait for data on one or another of its input channels (unlike other concurrent systems such as CSP <ref> [60] </ref>). The semantics of a Kahn process network are expressed in terms of the histories of data transmitted on each channel: a Kahn process is a function or set of functions from histories to histories.
Reference: [61] <editor> Paul Hudak. </editor> <booktitle> Para-functional programming. IEEE Computer, </booktitle> <pages> pages 60-70, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: One of e 2 or e 3 will be discarded as soon as the value of e 1 is known. The third-choice is programmer-controlled parallelism. This can be in the form of annotations <ref> [61] </ref>, or in the form of primitive combinators. Roe [126], for example, points out difficulties with conservative parallel graph reduction, and suggests two new combinators, par and seq: par x y = y CHAPTER 2.
Reference: [62] <author> Paul Hudak. </author> <title> Conception, evolution, and application of functional programming languages. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(3) </volume> <pages> 359-411, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Dataflow process networks are examined in chapter 3. 2.2 Functional programming in five minutes Functional programming languages are "higher-level" than more conventional imperative languages. There have been many persuasive arguments advanced for functional programming languages in general [8], and lazy functional languages in particular <ref> [64, 139, 62] </ref>. A recent study indicates that at least some of the claimed advantages of functional languages|brevity, rapidity of development, and ease of understanding|can be confirmed [63]. The study compares several languages, including C++ and Ada, in a substantial rapid prototyping exercise.
Reference: [63] <author> Paul Hudak and Mark Jones. </author> <title> Haskell vs Ada vs C++ vs Awk vs ...: An experiment in software prototyping productivity. </title> <type> Research report, </type> <institution> Dept. of Computer Science, Yale University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: There have been many persuasive arguments advanced for functional programming languages in general [8], and lazy functional languages in particular [64, 139, 62]. A recent study indicates that at least some of the claimed advantages of functional languages|brevity, rapidity of development, and ease of understanding|can be confirmed <ref> [63] </ref>. The study compares several languages, including C++ and Ada, in a substantial rapid prototyping exercise. Several metrics were used to compare the solutions given; the Haskell solution was one of the highest-rated. This section introduces functional programming using Haskell.
Reference: [64] <author> John Hughes. </author> <title> Why functional programming matters. </title> <journal> The Computer Journal, </journal> <volume> 32(2) </volume> <pages> 98-107, </pages> <year> 1989. </year>
Reference-contexts: Dataflow process networks are examined in chapter 3. 2.2 Functional programming in five minutes Functional programming languages are "higher-level" than more conventional imperative languages. There have been many persuasive arguments advanced for functional programming languages in general [8], and lazy functional languages in particular <ref> [64, 139, 62] </ref>. A recent study indicates that at least some of the claimed advantages of functional languages|brevity, rapidity of development, and ease of understanding|can be confirmed [63]. The study compares several languages, including C++ and Ada, in a substantial rapid prototyping exercise. <p> In effect, each of them captures a particular pattern of iteration, allowing the programmer to re-use these patterns without risk of error. This is one of the most persuasive arguments in favour of inclusion of higher-order functions in a programming language <ref> [64] </ref>. CHAPTER 5. STATIC PROCESS NETWORKS 128 illustrate the pattern of computation represented by each iterator in a very structural way; this viewpoint of higher-order functions is particularly important when using them to build networks of processes (section 5.4).
Reference: [65] <author> R. Jagannathan. </author> <title> Dataflow models. </title> <editor> In E. Y. Zomaya, editor, </editor> <booktitle> Parallel and Distributed Computing Handbook. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1995. </year>
Reference-contexts: When the up-stream actors to which the questons are sent produce datons, the datons are propagated in a data-driven fashion through the bypassed (by the questons) actor. Jagannathan surveys the current state of dataflow computing models, including a comparison of demand- and data-driven execution <ref> [65] </ref>. Dataflow architectures have also been proposed for implementation of functional programming languages [141].
Reference: [66] <author> Thomas Johnsson. </author> <title> Attribute grammars as a functional programming paradigm. </title> <booktitle> In Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 154-173, </pages> <address> Portland, Oregon, 1987. </address> <publisher> Springer-Verlag. LNCS 274. </publisher>
Reference-contexts: Laziness is a nice property for a number of reasons. Firstly, it is necessary to guarantee referential transparency: an expression can always be substituted with an equivalent expression without changing program meaning. Secondly, it allows the use of some programming techniques that cannot be used in non-lazy languages|see <ref> [19, 66, 147] </ref> for examples of programs that can only be constructed in a lazy language. For our purposes, laziness makes it easy to build infinite data structures and therefore to simulate pipeline dataflow systems.
Reference: [67] <author> G. Jones. </author> <title> Deriving the fast Fourier algorithm by calculation. </title> <editor> In K. Davis and J. Hughes, editors, </editor> <booktitle> Functional Programming: Proceedings of the 1989 Glasgow Workshop, </booktitle> <month> 21-23 August </month> <year> 1989, </year> <pages> pages 80-102. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Hand derivation can be quite difficult, as seen, for example, by my derivation of a parallel FIR in [123], and Jones' derivation of the FFT algorithm <ref> [67] </ref>. I believe the most appropriate path for further development of the transformations presented here would be in the context of a transformational dataflow programming tool: the programmer would choose from a catalog of transformations to be applied by the tool.
Reference: [68] <author> Geraint Jones and Mary Sheeran. </author> <title> Circuit design in ruby. </title> <editor> In J. Staunstrup, editor, </editor> <booktitle> Formal Methods for VLSI Design, </booktitle> <pages> pages 13-70. </pages> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: This language has evolved into the hardware design and verification language Ruby, in which circuits are represented as relations between signals (instead of as functions from signals to signals) <ref> [68] </ref>. Haskell has also been used to specify and simulate systolic arrays: McKeown and Revitt give higher-order functions for expressing systolic arrays and illustrate with a number of algorithms [93]. 5.4.5 Network construction in dataflow systems Recently, two block diagram systems have included network construction.
Reference: [69] <author> Mark Jones. </author> <title> A system of constructor classes: Overloading and implicit higher-order polymorphism. </title> <booktitle> In Proc. ACM Conf. on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 52-61, </pages> <address> Copenhagen, Denmark, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Similarly to Jones and Sinclair, wrapper functions are hyper-strict. Apart from eliminating the need for global garbage collection, this allows the operating system to be executed on a loosely-coupled network of processors. Wallace and Runciman use constructor classes <ref> [69] </ref> to express type-safe communication between processes [151]. In their scheme, the message type itself serves as the message address. Wallace has implemented an operating system for embedded systems in Gofer; unlike the other systems reviewed here, the stream constructor is not hyper-strict in its head [150].
Reference: [70] <author> S. B. Jones and A. F. Sinclair. </author> <title> Functional programming and operating systems. </title> <journal> The Computer Journal, </journal> <volume> 32(2) </volume> <pages> 162-174, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Because, however, merge is referentially opaque, reasoning about programs that use it becomes difficult. Jones and Sinclair <ref> [70] </ref> reduce the difficulties associated with merge by restricting its use to "systems programmers." They describe an operating system design and give examples of access to operating system services such as file editing and disk control.
Reference: [71] <author> Simon B. Jones. </author> <title> A range of operating systems written in a purely functional style. </title> <type> Technical Monograph PRG-42, </type> <institution> Oxford University Computing Laboratory, </institution> <month> Septem-ber </month> <year> 1984. </year>
Reference-contexts: in contrast, describes the use of bags (multi-sets) for expressing parallel computation, and shows how the Bird-Meertens formalism can be applied to parallel computation using bags [126]. 2.3.5 Functional operating systems The expression of operating system functionality in a functional language revolves around the deferred evaluation of streams of messages <ref> [71] </ref>.
Reference: [72] <editor> Simon L. Peyton Jones. </editor> <booktitle> The Implementation of Functional Programming Languages. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: The only difference to Lee's firing rules is that identifiers are used instead of the symbol "*". Some patterns are irrefutable|that is, they cannot fail to match <ref> [72, pp. 72-74] </ref>. <p> The tmatch function performs pattern-matching on tokens, as in functional languages <ref> [72] </ref>. Implicit in the definition of match is the concept of failure: if the input patterns do not match the input sequences, then match returns ? (pronounced "bottom"), denoting an undefined value.
Reference: [73] <editor> Simon Peyton Jones and Philip Wadler. </editor> <title> Imperative functional programming. </title> <booktitle> In ACM Principles of Programming Languages 93, </booktitle> <pages> pages 71-84, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: In a functional language, I/O is performed in two key ways [44]: i) by calling system primitives with lazy lists representing the whole history of input or output; or ii) by calling system primitives with a continuation function. Recently, the idea of monadic I/O <ref> [73] </ref> has been adopted: this technique is superficially similar to continuations; however, the external "world" is single-threaded through the I/O code, allowing I/O to be performed immediately as a side-effecting operation.
Reference: [74] <author> John L. Kelly Jr., Carol Lochbaum, and V. A. Vyssotsky. </author> <title> A block diagram compiler. </title> <journal> The Bell System Technical Journal, </journal> <pages> pages 669-678, </pages> <month> May </month> <year> 1961. </year> <note> BIBLIOGRAPHY 200 </note>
Reference-contexts: The concept dates back at least to the work by Kelly et al in 1961 <ref> [74] </ref>. In modern systems, visual interfaces add to the appeal of the metaphor: complete systems can be constructed by making connections on a computer screen. The computational model on which these systems are based is called pipeline dataflow . <p> This model is identical to Kahn process networks, for which Kahn gave a denotational semantics [75]. A pipeline dataflow network corresponds directly to a signal flow block diagram <ref> [74] </ref>: for example, a box labelled "+" on a block diagram sums two signals in a point-wise manner. In dataflow terms, the "+" actor is fired repeatedly; each time, it reads a token from both input channels and writes their sum to its output channel.
Reference: [75] <author> Gilles Kahn. </author> <title> The semantics of a simple language for parallel processing. </title> <booktitle> In Information Processing 74, </booktitle> <pages> pages 471-475. </pages> <publisher> North-Holland, </publisher> <year> 1974. </year>
Reference-contexts: These sections include much of the basic background material needed for the remaining chapters, although further reviews are given in each chapter. Chapter 3, Dataflow Process Networks , presents a formalisation of dataflow actors and processes. This work builds on seminal work by Kahn on process networks <ref> [75] </ref>, and on more recent work by Lee [87], to whom the term "dataflow process network" is due. The emphasis is on modelling and describing dataflow actors, rather than on the practical issues associated with implementation of dataflow programming environments. <p> The term "dataflow" is used in a number of different contexts; here we are interested in pipeline dataflow : actors are long-lived, producing and consuming many items during their lifetimes. This model is identical to Kahn process networks, for which Kahn gave a denotational semantics <ref> [75] </ref>. A pipeline dataflow network corresponds directly to a signal flow block diagram [74]: for example, a box labelled "+" on a block diagram sums two signals in a point-wise manner. <p> Because embedded DSP processors often have severely limited memory space, more sophisticated scheduling techniques have been developed that produce loops in the generated output code, thus minimising the code memory required [16]. 3.1.3 Kahn's process networks Kahn described the semantics of a language for parallel programming based on process networks <ref> [75] </ref>. In Kahn's language, a program consists of a network of "computing stations" (that is, processes) connected by FIFO-buffered channels. Each process repeatedly reads data from one or more input channels and writes data on one or more output channels. Kahn processes can only communicate through channels .
Reference: [76] <author> Gilles Kahn and David MacQueen. </author> <title> Coroutines and networks of parallel processes. </title> <editor> In B. Gilchrist, editor, </editor> <booktitle> Information Processing 77. </booktitle> <publisher> North-Holland, </publisher> <year> 1977. </year>
Reference-contexts: This means that: i) a network of Kahn processes also exhibits the desirable property of producing output before receiving all its input; and ii) networks can be connected in the same manner as single processes, thus allowing hierarchical construction of networks. Kahn and MacQueen <ref> [76] </ref> suggest that a Kahn process network be implemented in a demand-driven style, using co-routines. When a process attempts to read a token from an empty input channel, it suspends, and the process that writes to that channel is activated.
Reference: [77] <author> Richard M. Karp and Raymond E. Miller. </author> <title> Properties of a model for parallel computations, determinacy, termination, and queueing. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 14(6) </volume> <pages> 1390-1411, </pages> <month> November </month> <year> 1966. </year>
Reference-contexts: The edges of the DFG correspond to communications channels between the actors. This is the model in which I am interested in this thesis. 3.1.1 Computation graphs In 1966, Karp and Miller described a graph-theoretic model of parallel computation that is essentially pipeline dataflow <ref> [77] </ref>. The computation is represented by a finite directed graph: each node n j represents a computational operation O j ; each arc d p represents a queue of data from one node to another.
Reference: [78] <author> Paul Kelly. </author> <title> Functional Programming for Loosely-coupled Multiprocessors. </title> <booktitle> Research Monographs in Parallel and Distributed Computing. </booktitle> <publisher> Pitman, </publisher> <year> 1989. </year>
Reference-contexts: CHAPTER 2. BACKGROUND MATERIAL 34 2.3.6 Functional process networks Kelly proposed that list-manipulating functions be treated as processes to be mapped onto processors of a distributed-memory MIMD machine <ref> [78] </ref>. In his Caliban language, programs are written in a pure functional language using standard list functions such as map and filter. An additional language construct, moreover, contains declarations that specify which lists represent communications channels between processes. <p> Reade [113], for example, explains function application using box-and-arrow diagrams; Kelly <ref> [78] </ref> illustrates networks of streams and processes written using an annotated pure functional language; Waugh et al [152] illustrate the effect of program transformation on parallelism. There have also been proposals for formal visual functional languages. <p> The use of these language facilities for constructing process networks was explored by Kelly <ref> [78] </ref>; here, I extend this work 123 CHAPTER 5. STATIC PROCESS NETWORKS 124 to real-time streams and signal processing. This style of programming could perhaps be called "data-parallel process programming," and provides a counter to the position that functional parallelism does not provide adequate abstraction from individual threads of computation. <p> A technique for automatically deriving identities of this class would be helpful indeed. 5.5.4 Pipelining The third kind of identity constructs or combines pipelines of processes, where each process is parameterised by a different value. Pipelining was explored extensively by Kelly <ref> [78] </ref>. The following identity relates a single process to a pipeline of parameterised processes, each constructed with mapS: mapS (series fs) series (mapV mapS fs) (5.13) :: Stream n ff ! Stream n ff Suppose now that fs is produced by mapping a function to a vector of parameters, v.
Reference: [79] <author> Joel Kelso. </author> <title> A visual representation for functional programs. </title> <type> Technical Report CS-95/01, </type> <institution> Murdoch University, Australia, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: VisaVis does enforce an artificial (in my view) distinction between first-order and higher-order functions, mainly because it is based on FP. Kelso also proposes a visual functional language in which programmer interaction CHAPTER 4. VISUAL HASKELL 92 influences the design of the language <ref> [79] </ref>. Expression graphs are similar to other dataflow languages; however, inputs to the expression are connected to terminator nodes containing the argument type; outputs are connected to terminator nodes labelled with the type of the entire expression (that is, a function type).
Reference: [80] <author> Takayuki Dan Kimura, Ajay Apte, Samudra Sengupta, and Julie W. Chan. Form/formula: </author> <title> A visual programming paradigm for user-definable user interfaces. </title> <journal> IEEE Computer, </journal> <volume> 28(3) </volume> <pages> 27-35, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: The success of these systems indicates very strongly that visual programming|if applied to appropriate problem domains and supported by appropriate tools|offers a kind of understanding that is absent in purely-textual languages. Two of the most successful interface paradigms in visual programming languages are dataflow and forms <ref> [80] </ref>. In signal processing, dataflow visual languages have been used for some years [17, 88], because of the common use of block diagrams in describing signal processing systems.
Reference: [81] <author> Alois Knoll and Markus Freericks. </author> <title> An applicative real-time language for DSP programming supporting asynchronous data-flow concepts. </title> <journal> Microprocessing and Microprogramming, </journal> <volume> 32 </volume> <pages> 541-548, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The expression x@1 is the signal x delayed by one sample. Arithmetic operations extend point-wise to streams. Silage also supports fixed-point data types and arithmetic, an important aspect of programming real DSP devices. Freericks and Knoll use recursive functions to define signal-processing functions <ref> [81] </ref>. Higher-order functions can also be used to capture particular patterns of recursion [48]. An unusual aspect of their language is an explicit suspension construct that supports data-driven programming; this is used to code interfaces to real-time I/O channels.
Reference: [82] <author> Jeffrey Kodosky, Jack MacCrisken, and Gary Rymar. </author> <title> Visual programming using structured data flow. </title> <booktitle> In Proc. 1991 IEEE Workshop on Visual Languages, </booktitle> <pages> pages 34-39, </pages> <address> Kobe, Japan, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: The computational model on which these systems are based is called pipeline dataflow . The combination of a visual interface with pipeline dataflow is well-established in several fields, including signal processing [88, 12, 85], image processing and visualisation [112, 83], instrumentation <ref> [82] </ref>, and general-purpose visual programming languages [99, 59]. Signal processing systems are based on a special class of pipeline dataflow, dataflow process net 1 CHAPTER 1. INTRODUCTION 2 works (see chapter 3). <p> The style of Visual Haskell is based on dataflow: programs are described as data and function boxes, connected by arcs representing flow of data. It is in some ways a superset of dataflow, supporting not only "structured dataflow" <ref> [82] </ref> constructs such as conditionals and case-statements, but also pattern-matching, higher-order functions, and scoping. And it has Haskell's polymorphic type system, although no visual notation for it. <p> Many dataflow languages use forms as well as dataflow: Ptolemy [88], for example, uses a form to enter actor parameters, and dataflow to connect streams; Labview <ref> [82] </ref> uses forms as "front panels" for "virtual instruments." Informal diagrammatic notations have been used to describe or explain functional programs for some time. <p> Functions are not first-class objects in dataflow, whereas they are in Visual Haskell|figure 4.17a, for example, shows a function value (a partially-applied -abstraction) flowing along an arc. Ambler et al do not comment on structuring mechanisms for dataflow, such as described by Kodosky at al <ref> [82] </ref>. As I point out in section 4.4.2, Visual Haskell includes some mechanisms similar to those used in structured dataflow languages, but more general. On the down-side, these constructs may interfere with dataflow schedulers and so on, since the "flow" of data into these constructs is not always explicit. <p> CHAPTER 4. VISUAL HASKELL 108 three steps in the translation 4.4.2 Structured expressions With the aid of some simple conditional actors such as select (page 56), it is possible to encode loops and other constructs such as conditional execution in a pure dataflow framework <ref> [82] </ref>. It is also possible to analyse a graph to extract the constructs encoded therein [27]. Nonetheless, Kodosky et al [82] argue that the dataflow paradigm should be extended to incorporate structured constructs. <p> the aid of some simple conditional actors such as select (page 56), it is possible to encode loops and other constructs such as conditional execution in a pure dataflow framework <ref> [82] </ref>. It is also possible to analyse a graph to extract the constructs encoded therein [27]. Nonetheless, Kodosky et al [82] argue that the dataflow paradigm should be extended to incorporate structured constructs. They add loops, a case construct, and a sequencing construct to the Labview language, calling the result "structured dataflow." General-purpose visual dataflow languages generally provide such constructs|see Hils' survey [59]. <p> The conditional in figure 4.17b, for example, uses b even though b is not supplied to the construct by a dataflow arc. Compare this with the Labview case, which selects one of several functions , and then applies that to an argument <ref> [82] </ref>. The Haskell case can be used this way, as in figure 4.17d, but need not be. In this sense, then, the Visual Haskell case is more flexible. <p> All of the visual dataflow languages in Hils' survey [59], for example, support CHAPTER 4. VISUAL HASKELL 119 iteration. One technique is the use of special iteration constructs; Labview, for example, incorporates for-loops and while-loops into its structured dataflow model in this way <ref> [82] </ref>. Each is a containing box with an internal dataflow graph, which executes each time through the loop.
Reference: [83] <author> Konstantinos Konstantinides and John R. Rasure. </author> <title> The Khoros software development environment for image and signal processing. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 3(3) </volume> <pages> 243-252, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The computational model on which these systems are based is called pipeline dataflow . The combination of a visual interface with pipeline dataflow is well-established in several fields, including signal processing [88, 12, 85], image processing and visualisation <ref> [112, 83] </ref>, instrumentation [82], and general-purpose visual programming languages [99, 59]. Signal processing systems are based on a special class of pipeline dataflow, dataflow process net 1 CHAPTER 1. INTRODUCTION 2 works (see chapter 3).
Reference: [84] <author> P.J. Landin. </author> <title> A correspondence between ALGOL60 and Church's lambda-notation: Part I. </title> <journal> Communications of the ACM, </journal> <volume> 8 </volume> <pages> 89-101, </pages> <year> 1965. </year>
Reference-contexts: A recurring theme in DSP programming is the use of streams to represent discrete-time signals. Streams were first proposed by Landin as a means of separating the control structure of Algol-60 loops from the loop body <ref> [84] </ref>. Landin represents a stream as a pair of the head element, and a nullary function representing the rest of the stream. Stream elements are thus "evaluated when they are come to"|calculation of each successive loop control value and execution of the loop body proceed in an interleaved manner.
Reference: [85] <author> Rudy Lauwereins, Piet Wauters, Merleen Ade, and J. A. Peperstraete. </author> <title> Geometric parallelism and cyclo-static data flow in GRAPE-II. </title> <booktitle> In 5th Intl Workshop on Rapid System Prototyping, </booktitle> <address> Grenoble, France, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The computational model on which these systems are based is called pipeline dataflow . The combination of a visual interface with pipeline dataflow is well-established in several fields, including signal processing <ref> [88, 12, 85] </ref>, image processing and visualisation [112, 83], instrumentation [82], and general-purpose visual programming languages [99, 59]. Signal processing systems are based on a special class of pipeline dataflow, dataflow process net 1 CHAPTER 1. INTRODUCTION 2 works (see chapter 3). <p> DATAFLOW PROCESS NETWORKS 83 recognise it as such. Once instantiated and n is known, the actor can be expanded into n + 1 phases; in this case, the phase graph contains only a single cycle and is recognisable as SDF. Lauwereins et al <ref> [85] </ref> show that a network of CSSDF actors can be scheduled statically, and that the schedule can have (with certain assumptions about execution timing) lower execution time than an SDF static schedule. <p> Both of these styles show programs using higher-order functions in a very "structural" way; sections 5.4 and 5.5 use unfolded representations extensively. I am unaware of any proposal to incorporate this kind of representation into a visual language. Although Lauwereins et al <ref> [85] </ref> use unfolded diagrams, they do not appear to be part of their visual interface yet. In Visual Haskell, these representations are still informal. <p> In GRAPE-II, geometric parallelism specifies multiple invocations of a block <ref> [85] </ref>. In the visual language, these invocations all appear as one block. The number of invocations is specified in a textual language; 2 these values must be known at compile-time. The connection topology is also specified in the textual language.
Reference: [86] <author> Edward A. Lee. </author> <title> private communication, </title> <year> 1993. </year>
Reference-contexts: I explore this idea in section 5.4. A simple form of higher-order function mechanism has recently been added to Ptolemy [87], inspired by one of my earlier papers <ref> [114, 86] </ref>. CHAPTER 1. INTRODUCTION 6 An interesting possibility opened up by incorporating a functional language into the programming framework is that of program transformation. Program transformation is advocated in functional programming texts as a means of obtaining "efficient" realisation of programs from "inefficient" (but still executable) specifications [18]. <p> Although well-developed in the functional programming community, these concepts were foreign to the signal processing community. The use of higher-order functions in signal processing has been implemented in the Ptolemy system <ref> [86] </ref>, based on my work on this topic. Finally, I have proposed and explored a novel representation of timed streams (chapter 6). A realistic and complete example illustrates how this representation can be used CHAPTER 7. SUMMARY 193 to express dynamically-changing process networks.
Reference: [87] <author> Edward A. Lee. </author> <title> Dataflow process networks. </title> <institution> Memorandum UCB/ERL M94/53, Electronics Reserach Laboratory, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: I explore this idea in section 5.4. A simple form of higher-order function mechanism has recently been added to Ptolemy <ref> [87] </ref>, inspired by one of my earlier papers [114, 86]. CHAPTER 1. INTRODUCTION 6 An interesting possibility opened up by incorporating a functional language into the programming framework is that of program transformation. <p> Chapter 3, Dataflow Process Networks , presents a formalisation of dataflow actors and processes. This work builds on seminal work by Kahn on process networks [75], and on more recent work by Lee <ref> [87] </ref>, to whom the term "dataflow process network" is due. The emphasis is on modelling and describing dataflow actors, rather than on the practical issues associated with implementation of dataflow programming environments. The for-malisation leads to some new insights into the nature of dataflow actors. <p> The fby operator produces a stream containing the first element of its left argument, followed by its right argument. So, if x = [1; 2; 3; 4], then runner produces zero followed by itself summed with x|that is, [0; 1; 3; 6; 10]. The dataflow process network model <ref> [87] </ref> is lower-level, since communication is explicit. A process is formed by repeatedly firing an "actor"; a complete program consists of a network of actors. Dataflow process networks are examined in chapter 3. 2.2 Functional programming in five minutes Functional programming languages are "higher-level" than more conventional imperative languages. <p> The scale actor multiplies each element of its input stream by a constant|in this case, the value a. 45 CHAPTER 3. DATAFLOW PROCESS NETWORKS 46 Lee coined the term dataflow process to describe Kahn processes implemented as a sequence of firings of a dataflow actor <ref> [87] </ref>. Each firing is governed by a rule that states conditions on input values needed to fire the actor. Provided the sequence of actor firings is deterministic, then the actor forms a deterministic Kahn process. <p> The implementation of stream-processing functions in SISAL [46] uses this technique [50]. 3.1.4 Dataflow processes Lee defines a dataflow process to be the sequence of firings of a dataflow actor <ref> [87] </ref>. A dataflow process is thus a special kind of Kahn process|one in which execution is broken into a series of "firings." Because the process is a sequence of actor firings, a complete network can be executed by firing actors in an appropriate order. <p> Dataflow architectures have also been proposed for implementation of functional programming languages [141]. Field and Har-rison point out that demand-driven evaluation of functional programs gives normal-order semantics, while data-driven evaluation gives applicative-order semantics [47, chapter 14]. 3.1.5 Firing rules Lee has recently formalised the notion of "firing" an actor <ref> [87] </ref>. <p> Non-deterministic merge can be made deterministic by making time part of the data in the stream|see chapter 6. Lee gives an algorithm that can determine whether a finite set of firing rules can be tested by a process that performs only non-blocking reads <ref> [87] </ref>. A set of firing rules that satisfies this condition Lee calls sequential . Briefly, his algorithm works as follows: Choose an input j such that all P i;j contain at least one element. <p> In <ref> [87] </ref>, Lee stresses the difference between parameter arguments and stream arguments in Ptolemy: parameters are evaluated during an initialisation phase; streams are evaluated during the main execution phase. As a result, code generation can take place with the parameters known, but with the stream data unknown. <p> An actor of this kind mimics higher-order functions in functional languages, and could therefore be called a higher-order actor . I will now argue informally that any SDF actor can be implemented by delays and these five schemata. As pointed out by Lee <ref> [87] </ref>, a stateful actor can be represented as a stateless actor together with a unit-delay feedback loop carrying the "state" value. The remaining five schemata can implement any stateless SDF actor, as follows. <p> This would be yet another possible source of non-determinism, and is I believe best avoided. 3.3.6 Hierarchy and strictness One of the issues raised by Lee in his examination of dataflow actors <ref> [87] </ref> is the dataflow equivalent of procedural abstraction. The natural choice for this role is the dataflow network, and block-diagram systems such as Ptolemy hierarchically compose actors of networks of actors. <p> If the output of one invocation feeds into the input of the next, an additional feedback arc is also shown. This technique allows complex structures to be expressed without needing to provide higher-order functions. Ptolemy has adopted a form of higher-order function notation <ref> [87] </ref>. Special blocks represent multiple invocations of a "replacement actor." The Map actor, for example, is a generalised form of mapV. At compile time, Map is replaced by the specified number of invocations of its replacement actor; as for GRAPE-II, this number must be known at compile-time.
Reference: [88] <author> Edward A. Lee and David G. Messerschmitt et al. </author> <title> An overview of the Ptolemy project. </title> <note> Anonymous ftp from ptolemy.eecs.berkeley.edu, </note> <month> March </month> <year> 1994. </year>
Reference-contexts: The computational model on which these systems are based is called pipeline dataflow . The combination of a visual interface with pipeline dataflow is well-established in several fields, including signal processing <ref> [88, 12, 85] </ref>, image processing and visualisation [112, 83], instrumentation [82], and general-purpose visual programming languages [99, 59]. Signal processing systems are based on a special class of pipeline dataflow, dataflow process net 1 CHAPTER 1. INTRODUCTION 2 works (see chapter 3). <p> Two of the most successful interface paradigms in visual programming languages are dataflow and forms [80]. In signal processing, dataflow visual languages have been used for some years <ref> [17, 88] </ref>, because of the common use of block diagrams in describing signal processing systems. They have been used successfully in other fields as well, such as image processing and instrumentation [112]; Hils [59] surveys a number of dataflow visual languages. <p> A form is a layout of cells, each containing a formula that computes its value, usually in terms of the values of other cells. Many dataflow languages use forms as well as dataflow: Ptolemy <ref> [88] </ref>, for example, uses a form to enter actor parameters, and dataflow to connect streams; Labview [82] uses forms as "front panels" for "virtual instruments." Informal diagrammatic notations have been used to describe or explain functional programs for some time. <p> They add loops, a case construct, and a sequencing construct to the Labview language, calling the result "structured dataflow." General-purpose visual dataflow languages generally provide such constructs|see Hils' survey [59]. Special-purpose languages such as that of Ptolemy <ref> [88] </ref> do not, relying instead on the lower-level host language. Haskell does not have explicit loops, since it uses recursion instead of iteration. Visual Haskell does, however, have several structures that resemble structured dataflow: let-expressions, conditionals (if-then-else), -abstractions, and case-expressions. <p> Chapter 6 Dynamic Process Networks In the previous chapter, I considered static, SDF networks expressed in Haskell. The use of SDF networks in block-diagram systems like Ptolemy <ref> [88] </ref> is well-established, and SDF networks are adequate to describe the bulk of signal processing systems|Ptolemy's predecessor, Gabriel [17], supported only SDF. As soon as we consider systems which interact with asynchronous events, however, synchronous dataflow is inadequate. For example, consider the "digital gain control" shown in figure 6.1.
Reference: [89] <author> Edward A. Lee and David G. Messerschmitt. </author> <title> Static scheduling of synchronous data flow programs for digital signal processing. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 36(1) </volume> <pages> 24-35, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: It is, however, designed CHAPTER 3. DATAFLOW PROCESS NETWORKS 49 specifically for processing infinite streams. SDF scheduling was developed by Lee and Messerschmitt <ref> [89, 90] </ref>, and has since formed the backbone of efforts to implement dataflow networks efficiently for both real-time and non-real-time execution. outputs are equivalent to W p and U p ; a D annotation indicates that an arc contains one zero initial value; the small circle is a "fork" node.
Reference: [90] <author> Edward A. Lee and David G. Messerschmitt. </author> <title> Synchronous data flow. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 75(9) </volume> <pages> 1235-1245, </pages> <month> September </month> <year> 1987. </year> <note> BIBLIOGRAPHY 201 </note>
Reference-contexts: It is, however, designed CHAPTER 3. DATAFLOW PROCESS NETWORKS 49 specifically for processing infinite streams. SDF scheduling was developed by Lee and Messerschmitt <ref> [89, 90] </ref>, and has since formed the backbone of efforts to implement dataflow networks efficiently for both real-time and non-real-time execution. outputs are equivalent to W p and U p ; a D annotation indicates that an arc contains one zero initial value; the small circle is a "fork" node.
Reference: [91] <author> Allen Leung and Prateek Mishra. </author> <title> Reasoning about simple and exhaustive demand in higher-order lazy languages. </title> <booktitle> In 5th ACM Conf. on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 328-351, </pages> <address> Cambridge, MA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Let denote a term without a normal form, just as ? denotes a term without WHNF. A function f is hyper-strict iff f = ? That is, the result of applying f is necessarily undefined if its argument has no normal form. This is also called "exhaustive demand" <ref> [91] </ref>, as opposed to the "simple demand" of evaluation only to WHNF.
Reference: [92] <author> Andreas Maasen. </author> <title> Parallel programming with data structures and higher-order functions. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 18 </volume> <pages> 1-38, </pages> <year> 1992. </year>
Reference-contexts: All but three of the 17 Haskell standard prelude functions that use foldl or foldr meet this criterion. Maassen proposes a set of three finite data structures|sequences, tables, and sets|and a range of first- and higher-order functions defined on them <ref> [92] </ref>. He gives the complexities for implementation using lists and AVL trees; the latter has logarithmic complexity for many of the functions. A range of examples illustrate the utility of his chosen functions for programming.
Reference: [93] <author> G. P. McKeown and A. P. Revitt. </author> <title> Specification and simulation of systolic systems functional programming. </title> <booktitle> In Proc. 6th Intl. Workshop on Implementation of Functional Languages, </booktitle> <institution> University of East Anglia, Norwich, UK, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Haskell has also been used to specify and simulate systolic arrays: McKeown and Revitt give higher-order functions for expressing systolic arrays and illustrate with a number of algorithms <ref> [93] </ref>. 5.4.5 Network construction in dataflow systems Recently, two block diagram systems have included network construction. In GRAPE-II, geometric parallelism specifies multiple invocations of a block [85]. In the visual language, these invocations all appear as one block.
Reference: [94] <author> David G. Messerschmitt. </author> <title> A tool for structured functional simulation. </title> <journal> IEEE Trans. on Special Topics in Communications, </journal> <month> January </month> <year> 1984. </year>
Reference-contexts: An external scheduler tests if enough tokens are available, and fires the actor if they are. A naive implementation of data-driven scheduling simply cycles through all actors, firing any which have sufficient input tokens. More sophisticated schedulers attempt to minimise unnecessary testing by tracing the graph in topological order <ref> [94] </ref>, or by "pushing" tokens through the graph [96]. Because of the "eager" nature of data-driven execution, processes may produce more data than will ever be needed. Some form of throttling is needed: bounding channel buffers is one solution; pre-emptive scheduling is another. <p> Messerschmitt's BLOSIM simulator <ref> [94] </ref>, for example, fired actors in round-robin fashion; an actor that had insufficient input data was required to return to the simulator immediately.
Reference: [95] <author> Bertrand Meyer. </author> <title> Applying `design by contract'. </title> <journal> IEEE Computer, </journal> <volume> 25(10) </volume> <pages> 40-51, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: This is a key part of the "programming-by-contract" approach to software development <ref> [95] </ref>. In this case, the two input vectors must be the same length. Since a compiler is not required to generate correct code if pre-conditions are violated, an implementation can focus on generating the fastest code possible for two equal-length vectors (for example, by omitting a test for equal length).
Reference: [96] <author> Matthias Meyer. </author> <title> A pilot implementation of the host-engine software architecture for parallel digital signal processing. </title> <type> Technical report, </type> <institution> School of Electrical Engineering, University of Technology Sydney, and Technical University Hamburg-Harburg, </institution> <month> November </month> <year> 1994. </year> <note> FTP from ftp.ee.uts.edu.au as /pub/DSP/papers/spook.ps.gz. </note>
Reference-contexts: The TMS320C40 implementation was in progress when the paper was written; for a detailed description of the final implementation see Meyer's report <ref> [96] </ref>. Again, I have decided CHAPTER 1. INTRODUCTION 10 against including this material in the thesis, as I feel it is tangential to its main theme. Modelling Asynchronous Streams in Haskell [116] develops Haskell code for modelling timed streams. <p> The difficulties with demand-driven execution of dataflow networks led myself and Matthias Meyer to abandon attempts to incorporate it into the initial design of the SPOOK (Signal Processing Object-Oriented Kernel) parallel DSP kernel <ref> [118, 96] </ref>. Nonetheless, we still believe that it is important and that an efficient hybrid scheduling mechanism is possible. With data-driven scheduling, an actor is fired when it has sufficient tokens on its input channels. <p> A naive implementation of data-driven scheduling simply cycles through all actors, firing any which have sufficient input tokens. More sophisticated schedulers attempt to minimise unnecessary testing by tracing the graph in topological order [94], or by "pushing" tokens through the graph <ref> [96] </ref>. Because of the "eager" nature of data-driven execution, processes may produce more data than will ever be needed. Some form of throttling is needed: bounding channel buffers is one solution; pre-emptive scheduling is another. <p> Apart from the working Haskell prototype code (appendix A and in the text), portions of the framework have been implemented: Dawson's prototype Visual Haskell editor [41]; the two implementations of SPOOK|Signal Processing Object-Oriented Kernel|described in [118] and <ref> [96] </ref> (see also page 9); and some portions of a compiler for modern digital signal processors [119]. Key to the framework is an efficient implementation model. The model, dataflow process networks, although widely used in signal processing development environments, lacks a formal description of its semantics.
Reference: [97] <author> G. J. Michaelson, N. R. Scaife, and A. M. Wallace. </author> <title> Prototyping parallel algorithms using Standard ML. </title> <booktitle> In Proc. British Machine Vision Conference, </booktitle> <year> 1995. </year>
Reference-contexts: Prototyping image processing programs in a functional language is advocated by Michaelson et al , who use four higher-order functions to code a range of image processing algorithms <ref> [97] </ref>. In related work, Bratvold compiles these higher-order functions into parallel code for a Meiko Transputer system [22]. 5.2 Vectors Vectors play an important role in signal processing. Recall from section 2.4.4 that the instruction sets of modern DSPs support very efficient vector operations.
Reference: [98] <author> Motorola Inc. </author> <title> DSP96002 IEEE Floating-Point Dual-Port Processor User's Manual, </title> <year> 1989. </year>
Reference-contexts: In this section, I give an overview of the characteristics of these devices. I focus only on the modern, floating-point devices; for more detailed information on specific devices, see <ref> [115, 119, 34, 5, 6, 98, 138, 137] </ref>. The device contains six functional units|an ALU, a multiplier, two address units, and two load-store units (not shown here)|all of which can operate simultaneously, a bank of floating-point data registers, and a bank of address registers. <p> The multiply-add instruction is the core of the FIR (finite-impulse response) filter, a key DSP benchmark. Some devices also have a parallel multiply-add-subtract instruction, which substantially speeds up execution of the FFT (Fast Fourier Transform), another key benchmark. In contrast to the TMS320C30 and TMS320C40, the ADSP-21020 and DSP96002 <ref> [5, 98] </ref> are load-store machines: operands are explicitly loaded into data registers prior to operating on them. The programmer writes separate fields of the instruction to control the ALU and multiplier, and the load/store and address units.
Reference: [99] <author> Marc A. Najork and Eric Golin. </author> <title> Enhancing Show-and-Tell with a polymorphic type system and higher-order functions. </title> <booktitle> In Proc. 1990 IEEE Workshop on Visual Languages, </booktitle> <address> Skokie, </address> <publisher> Illinois, </publisher> <pages> pages 215-220, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The computational model on which these systems are based is called pipeline dataflow . The combination of a visual interface with pipeline dataflow is well-established in several fields, including signal processing [88, 12, 85], image processing and visualisation [112, 83], instrumentation [82], and general-purpose visual programming languages <ref> [99, 59] </ref>. Signal processing systems are based on a special class of pipeline dataflow, dataflow process net 1 CHAPTER 1. INTRODUCTION 2 works (see chapter 3). Block-diagram systems, although well-established, are a practical compromise of usability and implementation technology, and lack some of the features of modern programming languages. <p> Of particular interest in Visual Haskell is the way it handles higher-order functions: because Visual Haskell is curried, higher-order functions are as easy to construct as first-order functions. There is no need for special "function slots" <ref> [99] </ref>, and no enforced distinc 88 CHAPTER 4. VISUAL HASKELL 89 tion between first-order and higher-order functions as in VisaVis [110]. Any construct|a case-expression, say|can be a function, and can be applied in the same way as a named function. <p> He gives examples of manipulations on boxes containing characters and other symbols, and of functions which place a frame around a box, and which decompose a compound box into a list of its components. Although not a functional language, Najork and Golin's visual dataflow language ESTL (Enhanced Show-and-Tell) <ref> [99] </ref> has many similar features, including higher-order functions and a polymorphic type system. A key concept in ESTL is inconsistency : an arc that passes through an inconsistent box cannot carry data. For example, figure 4.2 illustrates the factorial function in ESTL. <p> On the down-side, these constructs may interfere with dataflow schedulers and so on, since the "flow" of data into these constructs is not always explicit. Another powerful feature of functional languages that is rare in dataflow languages is pattern-matching, although ESTL <ref> [99] </ref> and CUBE [100] both support a form of pattern-matching. Finally, a key difference between functional and dataflow languages is the treatment of I/O. <p> Patterns are not common in visual languages, although Enhanced Show-and-Tell (ESTL) <ref> [99] </ref> supports a very similar facility based on inconsistency . CHAPTER 4. VISUAL HASKELL 112 as grey boxes, with a single live semi-circular input port (figure 4.18a, b, and d). Non-function variables also have a single output port. <p> The figure uses a blank diamond to denote any visual syntax; a numeral in the bottom-right corner distinguishes unique CHAPTER 4. VISUAL HASKELL 118 occurrences. In the second rule, any pict and a pattern connected by a binding arc can be attached. Argument slots Enhanced Show-and-Tell (ESTL) <ref> [99] </ref> and DataVis [58] have function slots . A function slot is a position in the icon of a higher-order function in which a function argument can be placed.
Reference: [100] <author> Marc A. Najork and Simon M. Kaplan. </author> <title> The CUBE language. </title> <booktitle> In Proc. 1991 IEEE Workshop on Visual Languages, </booktitle> <pages> pages 218-224, </pages> <address> Kobe, Japan, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: They also show that a CSRS can specify a three-dimensional visual language (presumably motivated by their work on Cube, a three-dimensional logic programming language <ref> [100] </ref>). Following sections of this chapter will touch on differences between Visual Haskell and visual dataflow languages. One of motivations for using a functional language in this CHAPTER 4. VISUAL HASKELL 93 thesis is the apparent correspondence between functional languages and dataflow. <p> On the down-side, these constructs may interfere with dataflow schedulers and so on, since the "flow" of data into these constructs is not always explicit. Another powerful feature of functional languages that is rare in dataflow languages is pattern-matching, although ESTL [99] and CUBE <ref> [100] </ref> both support a form of pattern-matching. Finally, a key difference between functional and dataflow languages is the treatment of I/O.
Reference: [101] <author> Marc A. Najork and Simon M. Kaplan. </author> <title> Specifying visual languages with conditional set rewrite systems. </title> <booktitle> In Proc. 1993 IEEE Symposium on Visual Languages, </booktitle> <pages> pages 12-18, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: More complex productions can be defined in terms of these operators, together with additional constraints on picture attributes. A more general formalism, conditional set rewrite systems (CSRS), is proposed by Najork and Kaplan <ref> [101] </ref>. A CSRS consists of a set of rewrite rules on sets of terms, each governed by a predicate on terms.
Reference: [102] <author> Jeffrey V. Nickerson. </author> <title> Visual programming: Limits of graphical representation. </title> <booktitle> In Proc. 1994 IEEE Symposium on Visual Languages, </booktitle> <pages> pages 178-179, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: And a pict contains another if the second is wholly enclosed by the first (figure 4.9c). Nickerson <ref> [102] </ref> claims that these three constructions|which he calls adjoinment , linkage, and containment |account for the majority of diagrams used in computer science. CHAPTER 4.
Reference: [103] <author> John K. Ousterhout. </author> <title> Tcl and the Tk Toolkit. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: The remainder of this section describes the means by which the visual syntax is specified. 4.3.1 Visual elements A Visual Haskell program is composed of many visual elements, arranged on paper or on a computer screen. The surface on which elements are arranged is called the canvas (from <ref> [103] </ref>). Any valid program fragment is a pict (short for "picture"). Picts are recursively composed of simpler picts, and ultimately of primitive picts|boxes, icons, lines, strings, and so on. There are two special kinds of primitive pict: ports and arcs. Ports serve as connection points between picts. <p> It would be an interesting effort to construct a complete editor; an interesting approach may be build a complete Visual Haskell domain into the Ptolemy system for use in signal processing exploration and simulation, using say Tcl/Tk <ref> [103] </ref> as the graphical environment. Further work is also needed on iconic representations and type annotations, to find useful representations of the dynamic process networks of chapter 6.
Reference: [104] <author> Perihelion Software Ltd. </author> <title> The Helios Parallel Operating System. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Another approach uses light-weight operating system kernels and support libraries <ref> [104, 39, 143] </ref>. This is particular evident in parallel systems, where the added complexities of multi-processor communication and synchronisation seem to encourage adoption of multi-tasking kernels. The approach in which I am interested here is to use a programming language, but at a much higher level than C.
Reference: [105] <author> Simon L. Peyton-Jones. </author> <title> Parallel implementations of functional programming languages. </title> <journal> The Computer Journal, </journal> <volume> 32(2) </volume> <pages> 175-186, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Still, side-effect-free languages are likely to contain more implicit parallelism than imperative languages because they lack artificial time-dependencies. Two examples of languages and architectures that support implicit parallelism are dataflow languages and architectures [1, 7] and multi-processor graph reduction of pure functional programs <ref> [105] </ref>. In both cases, the languages are side-effect free. However, Gajski et al point out that parallelising compilers can in fact perform better than single-assignment languages [49]. Implicit parallelism is also a key ingredient of modern single-processor compilers. <p> Applicative-order reduction, in contrast, evaluates all argument expressions, regardless of whether their values are needed. The two Church-Rosser theorems are fundamental to considerations of reduction order (see <ref> [105, pp. 24-25] </ref>). Informally, the first says that any two reduction sequences will end at the same normal form provided that they both terminate. This is of great interest, as it allows the freedom to not only change the reduction order, but to perform reductions in parallel. <p> CHAPTER 2. BACKGROUND MATERIAL 30 2.3.3 Parallel graph reduction A program graph typically contains, at any given moment, many redexes. Parallel evaluation of a graph can therefore be achieved by having many processors evaluate these redexes simultaneously. Peyton Jones <ref> [105] </ref> describes the issues in parallel graph reduction on a shared-memory machine.
Reference: [106] <author> Simon L. Peyton-Jones and David Lester. </author> <title> A modular fully-lazy lambda-lifter in Haskell. </title> <journal> Software Practice and Experience, </journal> <volume> 21(5) </volume> <pages> 479-506, </pages> <month> May </month> <year> 1991. </year> <note> BIBLIOGRAPHY 202 </note>
Reference-contexts: The Haskell case can be used this way, as in figure 4.17d, but need not be. In this sense, then, the Visual Haskell case is more flexible. For -abstractions, a transformation known as lambda-lifting can be used to eliminate free variables <ref> [106] </ref>; it may be possible to find a similar transformation for case-expressions. 4.4.3 Patterns Patterns "de-construct" arguments, and bind names to expressions (or parts of expressions).
Reference: [107] <author> Keshav Pingali and Arvind. </author> <title> Efficient demand-driven evaluation, part 1. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 7(2) </volume> <pages> 311-333, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: Edges of the network graph are coloured according to whether they are data CHAPTER 3. DATAFLOW PROCESS NETWORKS 55 driven or demand-driven. Request tokens are propagated backwards down the demand-driven arcs|these tokens are called questons, while normal data tokens are called datons . Pingali and Arvind <ref> [107] </ref> take another approach: they give a transformation by which a dataflow graph with demand-driven semantics is transformed into an equivalent graph with data-driven semantics. In essence, demands are made explicit by adding arcs and actors to carry questons.
Reference: [108] <author> Jose Luis Pino, Thomas M. Parks, and Edward A. Lee. </author> <title> Mapping multiple independent synchronous dataflow graphs onto heterogeneous multiprocessors. </title> <booktitle> In 28th Asilomar Conference on Circuits, Signals and Systems, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Here, actors operate on timed events; a global clock is used to order actor firings correctly in time, according to the times of occurrence of the actor's input events. None of these approaches is, however, really suitable for figure 6.1. Pino et al <ref> [108] </ref> have recently developed the notion of "peek" and "poke" actors. These actors act as the interface between two independently-scheduled graphs, running on two different processors.
Reference: [109] <author> John A. Plaice. RLucid, </author> <title> a general real-time dataflow language. </title> <booktitle> In Proc. Formal Techniques in Real-time and Fault-tolerant Systems, </booktitle> <address> Nijmegan, the Netherlands, </address> <month> January </month> <year> 1992. </year> <note> Springer-Verlag. LNCS 571. </note>
Reference-contexts: This simplifying assumption allows a precise specification of time semantics and compilation of programs into finite-state automata. The family of synchronous languages includes Esterel [15], Lustre [33], Signal [14], and RLucid, a version of Lucid extended with time-stamps <ref> [109] </ref>. CHAPTER 6. DYNAMIC PROCESS NETWORKS 169 6.2 Timed signals and streams In section 2.4.1, a non-uniformly clocked signal e x was defined at times given by its clock , e t x .
Reference: [110] <author> Jorg Poswig, Guido Vrankar, and Claudio Moraga. </author> <title> VisaVis|a higher-order functional visual programming language. </title> <journal> Journal of Visual Languages and Computing, </journal> <volume> 5 </volume> <pages> 83-111, </pages> <year> 1994. </year>
Reference-contexts: There is no need for special "function slots" [99], and no enforced distinc 88 CHAPTER 4. VISUAL HASKELL 89 tion between first-order and higher-order functions as in VisaVis <ref> [110] </ref>. Any construct|a case-expression, say|can be a function, and can be applied in the same way as a named function. The next two sections briefly survey work on visual languages, and then introduce Visual Haskell by example. <p> Visual Haskell's argument slots (section 4.5.1) are a generalisation of ESTL's function slots. ESTL also includes a visual notation for a polymorphic type system; this may be a good starting point for Visual Haskell's missing type notation. Poswig et al describe VisaVis <ref> [110] </ref>, a visual functional language based on Backus' functional programming language FP [11]. The language is essentially a visual dataflow language, with explicit support for higher-order functions: function arguments are placed within icons of higher-order functions. <p> In a higher-order language such as Haskell, patterns of iteration are captured by higher-order functions; applying the higher-order function is equivalent to coding an iterative loop in other languages (see sections 5.2.2 and 5.3.4). Visual Haskell, along with the visual functional language VisaVis <ref> [110] </ref>, thus uses a function icon to represent iteration. There is, however, a limit to how much information one icon can convey, and so in previous work [114] I have used a representation similar to structured dataflow iteration constructs. Two examples are given in figure 4.23.
Reference: [111] <author> Douglas B. Powell, Edward A. Lee, and William C. Newman. </author> <title> Direct synthesis of optimized DSP assembly code from signal flow block diagrams. </title> <booktitle> In ICASSP 92, </booktitle> <pages> pages V-553-V-556, </pages> <year> 1992. </year>
Reference-contexts: SDF code generators operate by "firing" code generation actors at compile-time; each time an actor is fired, it emits C or assembler code. For assembler code generation, macros and symbolic names make the programmer's task easier and allow some optimisations. Powell et al <ref> [111] </ref>, for example, describe assembler code generation in SPW, and give some examples of the assembler source code. A typical instruction looks like this: CHAPTER 3. DATAFLOW PROCESS NETWORKS 51 ADDL tA,out tA,tX Y:(table.reg)+,tY Here, tA, out, tX, and tY stand for registers, and table for a region of memory.
Reference: [112] <author> John Rasure and Mark Young. </author> <title> Dataflow visual languages. </title> <journal> IEEE Potentials, </journal> <volume> 11(2) </volume> <pages> 30-33, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The computational model on which these systems are based is called pipeline dataflow . The combination of a visual interface with pipeline dataflow is well-established in several fields, including signal processing [88, 12, 85], image processing and visualisation <ref> [112, 83] </ref>, instrumentation [82], and general-purpose visual programming languages [99, 59]. Signal processing systems are based on a special class of pipeline dataflow, dataflow process net 1 CHAPTER 1. INTRODUCTION 2 works (see chapter 3). <p> In signal processing, dataflow visual languages have been used for some years [17, 88], because of the common use of block diagrams in describing signal processing systems. They have been used successfully in other fields as well, such as image processing and instrumentation <ref> [112] </ref>; Hils [59] surveys a number of dataflow visual languages. A form is a layout of cells, each containing a formula that computes its value, usually in terms of the values of other cells.
Reference: [113] <author> Chris Reade. </author> <title> Elements of Functional Programming. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Reade <ref> [113] </ref>, for example, explains function application using box-and-arrow diagrams; Kelly [78] illustrates networks of streams and processes written using an annotated pure functional language; Waugh et al [152] illustrate the effect of program transformation on parallelism. There have also been proposals for formal visual functional languages.
Reference: [114] <author> H. John Reekie. </author> <title> Towards effective programming for parallel digital signal processing. </title> <type> Technical Report 92.1, </type> <institution> Key Centre for Advanced Computing Sciences, University of Technology, </institution> <address> Sydney, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: I explore this idea in section 5.4. A simple form of higher-order function mechanism has recently been added to Ptolemy [87], inspired by one of my earlier papers <ref> [114, 86] </ref>. CHAPTER 1. INTRODUCTION 6 An interesting possibility opened up by incorporating a functional language into the programming framework is that of program transformation. Program transformation is advocated in functional programming texts as a means of obtaining "efficient" realisation of programs from "inefficient" (but still executable) specifications [18]. <p> I have not included all of this material in this thesis; the reader interested in particular portions of elided work is directed to the electronic copies referenced in the bibliography, or to the list of papers at the address http://www.ee.uts.edu.au/~johnr/papers Towards Effective Programming for Parallel Digital Signal Processing <ref> [114] </ref> contains many of the key ingredients of this thesis. This report proposed that a functional programming language would be a good choice for programming block-diagram systems, and illustrated programs written using vectors and streams with an early version of the visual language of this thesis. <p> Even so, it is sufficiently well-developed to be useful. Visual Haskell evolved out of a need to explain some of the work in this thesis to non-functional-programmers. It began as an ad-hoc notation for "drawing" functional programs (see <ref> [114] </ref>); later, Ken Dawson of UTS implemented a prototype editor for this language [41], stimulating its development into a more precise and usable language. The style of Visual Haskell is based on dataflow: programs are described as data and function boxes, connected by arcs representing flow of data. <p> Visual Haskell, along with the visual functional language VisaVis [110], thus uses a function icon to represent iteration. There is, however, a limit to how much information one icon can convey, and so in previous work <ref> [114] </ref> I have used a representation similar to structured dataflow iteration constructs. Two examples are given in figure 4.23. In this representation, the computation performed on each iteration is shown as a dataflow graph inside a generic icon representing the type of iteration. <p> I have used the structure on the left-hand side of equation 5.14 (figure 5.24a) in the FFT example of <ref> [114] </ref>; it is also similar to the FIR filter example of a second paper devoted to process network transformation [123]. The right-hand side CHAPTER 5.
Reference: [115] <author> H. John Reekie. </author> <title> Real-time DSP in C and assembler. </title> <note> FTP from ftp.ee.uts.edu.au as /pub/prose/c30course.ps.gz, </note> <year> 1993. </year>
Reference-contexts: Process Network Transformation [123] is a condensed version of the above paper, but using a visual notation much closer to the current Visual Haskell. It identifies three classes of process network transformation|this has since become four (section 5.5). Real-time DSP in C and Assembler <ref> [115] </ref> is a set of course notes on the TMS320C30 DSP, which may be of interest to the reader seeking an accessible introduction to the features of modern DSP devices. <p> In this section, I give an overview of the characteristics of these devices. I focus only on the modern, floating-point devices; for more detailed information on specific devices, see <ref> [115, 119, 34, 5, 6, 98, 138, 137] </ref>. The device contains six functional units|an ALU, a multiplier, two address units, and two load-store units (not shown here)|all of which can operate simultaneously, a bank of floating-point data registers, and a bank of address registers.
Reference: [116] <author> H. John Reekie. </author> <title> Modelling asynchronous streams in Haskell. </title> <type> Technical Report 94.3, </type> <institution> Key Centre for Advanced Computing Sciences, University of Technology, </institution> <address> Sydney, </address> <month> June </month> <year> 1994. </year> <note> FTP from ftp.ee.uts.edu.au as /pub/prose/async-streams.ps.gz. </note>
Reference-contexts: Again, I have decided CHAPTER 1. INTRODUCTION 10 against including this material in the thesis, as I feel it is tangential to its main theme. Modelling Asynchronous Streams in Haskell <ref> [116] </ref> develops Haskell code for modelling timed streams. Two approaches are used: hiatons, which mark "empty" slots, and time-stamps, which mark the times of occurrence of tokens.
Reference: [117] <author> H. John Reekie. </author> <title> Visual Haskell: A first attempt. </title> <type> Technical Report 94.5, </type> <institution> Key Centre for Advanced Computing Sciences, University of Technology, </institution> <address> Sydney, </address> <month> August </month> <year> 1994. </year> <note> FTP from ftp.ee.uts.edu.au as /pub/prose/visual-haskell.ps.gz. </note>
Reference-contexts: Because the visual language is general|that is, it is a visual notation for Haskell and it not particularly oriented towards signal processing|it promises to be a powerful tool for functional programming in general. The presentation in chapter 4 and in an earlier paper <ref> [117] </ref> are written assuming that Visual Haskell will be used as a visual functional language, not as a visual dataflow language. The connection between functional languages and pipeline dataflow (more specifically, dataflow process networks|see chapter 3) provides a new implementation model for a particular class of functional programs. <p> Chapter 6 of this thesis is a complete revision of this paper; in particular, a new form of timed stream is developed, and the music synthesiser example is extended. Visual Haskell: A First Attempt <ref> [117] </ref> is the only paper I have written devoted to explaining Visual Haskell, the final form of the visual language developed and refined over the last few years. Chapter 4 is a revised version of the core of this paper. <p> Note that the reason for port matching is to make it easier to support curried functions, not merely to change the colour of connected ports! Although this seems complex at first, it has removed the need to translate Haskell into an intermediate language, as in an earlier paper <ref> [117] </ref>. Having translated a program into a picture, it is unlikely that this picture will be as appealing as it could be. Icons, for example, are a popular ingredient in visual interfaces. <p> A novel feature arises because Haskell is higher-order: if a structured expression is function-valued, it has one or more live input ports, and can thus be connected in the same way as a function variable. This is an improvement over the language of my earlier paper <ref> [117] </ref>, which required explicit apply nodes for all structures CHAPTER 4. VISUAL HASKELL 109 except -abstractions. <p> NullT is the stream terminator (equivalent to f g on synchronous streams); :-: is the timed stream constructor: data Timed ff = NullT | Token ff :-: (Timed ff) In an earlier paper <ref> [117] </ref>, I used two representations of passing time: time-stamps, and unit hiatons. The problem with unit hiatons is that they imply that processes will do a great deal of work just marking time|that is, processing hiatons.
Reference: [118] <author> H. John Reekie and Matthias Meyer. </author> <title> The host-engine software architecture for parallel digital signal processing. </title> <booktitle> In Proc. PART'94, Workshop on Parallel and Real-time Systems, </booktitle> <address> Melbourne, Australia, </address> <month> July </month> <year> 1994. </year> <note> FTP from ftp.ee.uts.edu.au as /pub/prose/host-engine.ps.gz. </note>
Reference-contexts: None of this work is included in this thesis. The Host-Engine Software Architecture for Parallel Digital Signal Processing <ref> [118] </ref> is, again, totally unrelated to earlier papers: it describes a software architecture called SPOOK (Signal Processing Object-Oriented Kernel) that could best be described as an API-level (Application Programmer Interface) implementation of a dataflow engine for parallel DSP machines. <p> This is a very important CHAPTER 3. DATAFLOW PROCESS NETWORKS 54 point for real-time signal processing: context-switching presents a very high overhead to a real-time digital signal processor, as the time needed to save and restore all the registers of a modern DSP is quite high|see <ref> [118] </ref> for further discussion of this issue. Demand-driven execution on a single processor can be implemented simply and elegantly. As each actor executes, an attempt to read from an empty channel causes the source actor to fire immediately. <p> The difficulties with demand-driven execution of dataflow networks led myself and Matthias Meyer to abandon attempts to incorporate it into the initial design of the SPOOK (Signal Processing Object-Oriented Kernel) parallel DSP kernel <ref> [118, 96] </ref>. Nonetheless, we still believe that it is important and that an efficient hybrid scheduling mechanism is possible. With data-driven scheduling, an actor is fired when it has sufficient tokens on its input channels. <p> Apart from the working Haskell prototype code (appendix A and in the text), portions of the framework have been implemented: Dawson's prototype Visual Haskell editor [41]; the two implementations of SPOOK|Signal Processing Object-Oriented Kernel|described in <ref> [118] </ref> and [96] (see also page 9); and some portions of a compiler for modern digital signal processors [119]. Key to the framework is an efficient implementation model. The model, dataflow process networks, although widely used in signal processing development environments, lacks a formal description of its semantics.
Reference: [119] <author> H. John Reekie and John M. Potter. </author> <title> Generating efficient loop code for programmable dsps. </title> <booktitle> In ICASSP 94, pages II-469-II-472. IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: Real-time DSP in C and Assembler [115] is a set of course notes on the TMS320C30 DSP, which may be of interest to the reader seeking an accessible introduction to the features of modern DSP devices. Generating Efficient Loop Code for Programmable DSPs <ref> [119] </ref> is somewhat of a leap from the earlier papers, as it describes work aimed at compiling vector functions into efficient code for DSPs. <p> In this section, I give an overview of the characteristics of these devices. I focus only on the modern, floating-point devices; for more detailed information on specific devices, see <ref> [115, 119, 34, 5, 6, 98, 138, 137] </ref>. The device contains six functional units|an ALU, a multiplier, two address units, and two load-store units (not shown here)|all of which can operate simultaneously, a bank of floating-point data registers, and a bank of address registers. <p> working Haskell prototype code (appendix A and in the text), portions of the framework have been implemented: Dawson's prototype Visual Haskell editor [41]; the two implementations of SPOOK|Signal Processing Object-Oriented Kernel|described in [118] and [96] (see also page 9); and some portions of a compiler for modern digital signal processors <ref> [119] </ref>. Key to the framework is an efficient implementation model. The model, dataflow process networks, although widely used in signal processing development environments, lacks a formal description of its semantics. In chapter 3, I presented a formal syntax and semantics of dataflow actors and processes.
Reference: [120] <author> Hideki John Reekie. </author> <title> A real-time performance-oriented music synthesiser. </title> <type> Technical report, </type> <institution> School of Electrical Engineering, University of Technology, </institution> <address> Sydney, </address> <month> November </month> <year> 1987. </year> <type> Undergraduate thesis report. BIBLIOGRAPHY 203 </type>
Reference-contexts: Formant-wave function (FWF) synthesis is a time-domain method for simulating the excitation-pulse + resonant-filter model of sound synthesis [125]. For music synthesis, the FWF algorithm is relatively simple to implement, as demonstrated by the implementation of a monophonic synthesiser in assembler on a TMS320C25 fixed-point DSP chip <ref> [120] </ref>. Parameters of the algorithm have a direct relation to the shape of the frequency spectrum, which is helpful for programming musical sounds, and it can produce rich and interesting sounds. <p> The exponential e fft never terminates, so, to minimise the computational burden, the envelope is changed to a linear decay when the rate of decay drops below some slope ffi <ref> [120] </ref>. The modified envelope is then: e (t) = &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 0; t 0 2 (1 cos (fit))e fft ; 0 &lt; t e fft ; e fft (t t )ffi; t &gt; t CHAPTER 6.
Reference: [121] <author> John Reekie. </author> <title> Integrating block-diagram and textual programming for parallel DSP. </title> <booktitle> In Proc. 3rd Intl. Symp. on Signal Processing and its Applications (ISSPA 92), </booktitle> <pages> pages 622-625, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: It identified the usefulness of program transformation for parallel programming, and pipelined the FFT (Fast Fourier Transform) to illustrate. CHAPTER 1. INTRODUCTION 9 Integrating Block-Diagram and Textual Programming for Parallel DSP <ref> [121] </ref> is essentially a brief summary of the above report, although it focuses more on the notion of the "two-view" development system. Transforming Process Networks [122] continues the program transformation theme by pipelining an FIR (finite-impulse-response) filter. <p> It is likely to be easier to precisely define the semantics of Visual Haskell in terms of Haskell's semantics, than attempting to define a new semantics from scratch. It makes the notion of a "two-view" development system <ref> [121] </ref> feasible. Visual Haskell programs can be executed by standard Haskell compilers and inter preters; all that is required is a translator from visual to textual syntax.
Reference: [122] <author> John Reekie and John Potter. </author> <title> Transforming process networks. </title> <booktitle> In Proc. MFPW'92, the Massey Functional Programming Workshop, </booktitle> <address> Palmerston North, New Zealand, </address> <month> August </month> <year> 1992. </year> <institution> Massey University. </institution>
Reference-contexts: CHAPTER 1. INTRODUCTION 9 Integrating Block-Diagram and Textual Programming for Parallel DSP [121] is essentially a brief summary of the above report, although it focuses more on the notion of the "two-view" development system. Transforming Process Networks <ref> [122] </ref> continues the program transformation theme by pipelining an FIR (finite-impulse-response) filter. The transformation proceeds by fusing vector iterators (section 5.2.2) to form a single process, and then by using pipelining and promotion (sections 5.5.4 and 5.5.5) to produce a pipeline with controllable grain size.
Reference: [123] <author> John Reekie and John Potter. </author> <title> Process network transformation. </title> <editor> In David Arnold, editor, </editor> <booktitle> Parallel Computing and Transputers (PCAT-93), </booktitle> <pages> pages 376-383. </pages> <publisher> IOS Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: I suggested in the paper that less difficult means of finding program transformations will be required if program transformation is to become useful to programmers. I have not yet found this means, so have decided against reproducing the paper's key example here. Process Network Transformation <ref> [123] </ref> is a condensed version of the above paper, but using a visual notation much closer to the current Visual Haskell. It identifies three classes of process network transformation|this has since become four (section 5.5). <p> I have used the structure on the left-hand side of equation 5.14 (figure 5.24a) in the FFT example of [114]; it is also similar to the FIR filter example of a second paper devoted to process network transformation <ref> [123] </ref>. The right-hand side CHAPTER 5. STATIC PROCESS NETWORKS 163 (figure 5.24b) is a pipeline of processes, each parameterised by its own i from vector v. 5.5.5 Promotion The fourth class of identity is derived from Bird's "promotion" identities [21]. <p> A similar law holds for the fold functions; on foldl, the law is 3 foldl f a concat foldl (foldl f ) a Laws of this kind are useful for controlling the grain size of processes in a pipeline - see the derivation of the pipeline FIR filter in <ref> [123] </ref> for an example. 3 Bird [21, page 123] gives this law as a variant of the fold promotion law for non-associative operators. CHAPTER 5. STATIC PROCESS NETWORKS 165 5.6 Summary This chapter presented an approach to programming dataflow networks in a functional programming language. <p> Note that, although I have not addressed it in this thesis, many similar transformations can be applied to vector functions|for concrete examples, see <ref> [123] </ref>. In functional languages, program transformation techniques are applied in two ways: i) for hand derivation and optimisation of programs, and ii) as an automatic optimisation tool in functional language compilers. Hand derivation can be quite difficult, as seen, for example, by my derivation of a parallel FIR in [123], and <p> see <ref> [123] </ref>. In functional languages, program transformation techniques are applied in two ways: i) for hand derivation and optimisation of programs, and ii) as an automatic optimisation tool in functional language compilers. Hand derivation can be quite difficult, as seen, for example, by my derivation of a parallel FIR in [123], and Jones' derivation of the FFT algorithm [67]. I believe the most appropriate path for further development of the transformations presented here would be in the context of a transformational dataflow programming tool: the programmer would choose from a catalog of transformations to be applied by the tool.
Reference: [124] <author> Steven P. Reiss. </author> <title> PECAN: Program development systems that support multiple views. </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> 11(3) </volume> <pages> 324-333, </pages> <month> March </month> <year> 1985. </year>
Reference-contexts: This "two-view" idea is not new: a multiple-view program development environment has been proposed for procedural languages <ref> [124] </ref>; and two-view document formatting systems seem a natural synthesis of the compiled and WYSIWYG styles of document production [23]. The choice of a lazy language is not without its drawbacks: there is an inevitable overhead associated with building a run-time representation of unevaluated expressions. This CHAPTER 1.
Reference: [125] <author> Xavier Rodet. </author> <title> Time-domain formant-wave-function synthesis. </title> <journal> Computer Music Journal, </journal> <volume> 8(3) </volume> <pages> 9-14, </pages> <year> 1984. </year>
Reference-contexts: Formant-wave function (FWF) synthesis is a time-domain method for simulating the excitation-pulse + resonant-filter model of sound synthesis <ref> [125] </ref>. For music synthesis, the FWF algorithm is relatively simple to implement, as demonstrated by the implementation of a monophonic synthesiser in assembler on a TMS320C25 fixed-point DSP chip [120]. <p> In this example, I will produce only a single, non-time-varying, formant. Each formant-wave function (FWF) is the product of a sine wave at the resonant frequency, and an envelope, which determines the shape of the spectral peak <ref> [125] </ref>. The envelope is given by: e (t) = &gt; &gt; &gt; &gt; &lt; 0; t 0 2 (1 cos (fit))e fft ; 0 &lt; t e fft ; t &gt; (6.1) The parameters ff and fi control the shape of the spectral peak.
Reference: [126] <author> Paul Roe. </author> <title> Parallel Programming using Functional Languages. </title> <type> PhD thesis, </type> <institution> Dept. of Computing Science, University of Glasgow, </institution> <year> 1991. </year>
Reference-contexts: One of e 2 or e 3 will be discarded as soon as the value of e 1 is known. The third-choice is programmer-controlled parallelism. This can be in the form of annotations [61], or in the form of primitive combinators. Roe <ref> [126] </ref>, for example, points out difficulties with conservative parallel graph reduction, and suggests two new combinators, par and seq: par x y = y CHAPTER 2. <p> A range of examples illustrate the utility of his chosen functions for programming. Roe, in contrast, describes the use of bags (multi-sets) for expressing parallel computation, and shows how the Bird-Meertens formalism can be applied to parallel computation using bags <ref> [126] </ref>. 2.3.5 Functional operating systems The expression of operating system functionality in a functional language revolves around the deferred evaluation of streams of messages [71].
Reference: [127] <author> Gary Sabot. </author> <title> The Paralation Model: Architecture-Independent Parallel Programming. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: For example, Hatcher and Quinn [55] describe a data-parallel language compiler for MIMD machines, while Sabot <ref> [127] </ref> describes how an SIMD computer could be used to simulate an MIMD computer. 2.1.4 Control parallelism Control parallelism is a form of functional parallelism characterised mainly by explicit communication ("message-passing") and synchronisation between processes.
Reference: [128] <author> Mary Sheeran. </author> <title> Designing regular array architectures using higher order functions. </title> <editor> In J.-P. Jouannaud, editor, </editor> <booktitle> Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 220-237, </pages> <address> Nancy, France, </address> <month> September </month> <year> 1985. </year> <note> Springer-Verlag. LNCS 201. </note>
Reference-contexts: Patterns such as these algorithms developed for hardware implementation, and there has been some interesting work on using functional languages for hardware design. Sheeran's FP language, for example, is a functional language based on Backus' FP, and includes structuring operations similar to those presented here <ref> [128] </ref>. This language has evolved into the hardware design and verification language Ruby, in which circuits are represented as relations between signals (instead of as functions from signals to signals) [68].
Reference: [129] <author> Jay M. Sipelstein and Guy E. Blelloch. </author> <booktitle> Collection-oriented languages. Proceedings of the IEEE, </booktitle> <volume> 79(4) </volume> <pages> 504-523, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Most of the work on parallel computing for "scientific" applications uses data parallelism. Sipelstein and Blelloch call data-parallel languages "collection-oriented." In <ref> [129] </ref>, they survey this class of languages and the ways in which they support data-parallel computation. In a data-parallel language, an operation over all elements of a data set is invoked by a single function call or language construct. <p> This section describes a Vector datatype implemented in Haskell, and a set of functions in Haskell that can be implemented efficiently on DSPs. These functions are typical of those present in what Sipelstein and Blelloch call "collection-oriented" languages <ref> [129] </ref>. Appendix A lists the code for the Vector module. As for the standard prelude, I will treat this code as a semantic definition only|in other words, the compiler "understands" the vector functions and generates efficient code for them.
Reference: [130] <author> David Skillicorn. </author> <title> Stream languages and dataflow. </title> <editor> In J.-L Gaudiot and L. Bic, editors, </editor> <booktitle> Advanced Topics in Dataflow Computing, </booktitle> <pages> pages 439-454. </pages> <publisher> Prentice-Hall, </publisher> <year> 1991. </year>
Reference-contexts: In essence, demands are made explicit by adding arcs and actors to carry questons. The propagation of questons, and the resulting propagation of datons back up the graph, is performed using data-driven execution. Skillicorn <ref> [130] </ref> proposes that strictness analysis be applied to Lucid programs to allow questons to be sent up the graph in fewer hops.
Reference: [131] <author> David Skillicorn and Janice Glasgow. </author> <title> Real-time specification using Lucid. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 15(2) </volume> <pages> 221-229, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Skillicorn and Glasgow <ref> [131] </ref> use a similar approach: they associate earliest and latest time streams with each data stream, and construct isomorphic nets operating on these streams.
Reference: [132] <author> D.B. Skillicorn. </author> <title> Parallelism and the Bird-Meertens formalism. </title> <note> FTP from qucis.queensu.ca in /pub/skill, </note> <month> April </month> <year> 1992. </year>
Reference-contexts: Then all instances of t are summed and the result placed into ip. agents read from the same script, and read each others' data when they need it. The greatest advantage of data-parallelism is its descriptive simplicity <ref> [132] </ref>: the programmer can easily control many thousands of processes because there is only one "thread of control" to manipulate. Data parallelism is often associated with SIMD machines, while functional parallelism is often associated with MIMD machines.
Reference: [133] <author> John A. Stankovic and Krithi Ramamritham. </author> <title> Introduction. </title> <editor> In John A. Stankovic and Krithi Ramamritham, editors, </editor> <title> Hard Real-Time Systems, chapter 1. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1988. </year>
Reference-contexts: Measurements on compiled programs show execution to be within 20% of the estimates. 2.4 Real-time signal processing The term "real-time" refers to systems in which "the correctness of the system depends not only on the logical result of computation, but also on the time at which the results are produced" <ref> [133] </ref>. Examples of real-time systems include command and control systems, process control systems, flight control systems, and so on. This thesis focuses on a particular class of real-time system: digital signal processing systems, which analyse, produce, and transform discrete-time signals. Whereas conven CHAPTER 2.
Reference: [134] <author> W. D. Stanley, G. R. Dougherty, and R. Dougherty. </author> <title> Digital Signal Processing. </title> <publisher> Reston Publishing, </publisher> <year> 1984. </year>
Reference-contexts: As an aid to understanding the iterative algorithm, a so-called "butterfly diagram" can be drawn; figure 5.7 shows the diagram for the FFT coded here, for a 16-point input vector. This particular form of the FFT is a complex, radix-2, decimation-in-time algorithm with bit-reversed output <ref> [134] </ref>. The top level of this function has a guard that checks that the supplied log 2 vector length is correct. To explain how the rest of this code works, I will start at the inner functions and work outwards.
Reference: [135] <author> William Stoye. </author> <title> Message-based functional operating systems. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 6(3) </volume> <pages> 291-311, </pages> <month> May </month> <year> 1986. </year> <note> BIBLIOGRAPHY 204 </note>
Reference-contexts: To ensure that computation proceeds in an operationally useful way, streams are constructed with head-strict cons |that is, elements are fully evaluated before transmission. A second approach to non-determinism is Stoye's "sorting office" <ref> [135] </ref>. In this scheme, every output message contains an address of a destination process; the sorting office receives all of these messages and routes each one to the addressed process.
Reference: [136] <author> V. S. Sunderan, G. A. Geist, J. Dongarra, and R. Manchek. </author> <title> The PVM concurrent computing system: Evolution, experience, </title> <booktitle> and trends. Parallel Computing, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: It is criticised as being low-level and error-prone because the programmer must explicitly manage communication and synchronisation, and keep track of the internal states of many processors [55]. Although control-parallel programs are often machine-specific, there are some projects, such as PVM <ref> [136] </ref>, which use a virtual machine abstraction to achieve architecture-independence. CHAPTER 2. BACKGROUND MATERIAL 17 There is no single agent-script-data model for control-parallel programs. On machines that support message-passing, the message-passing model of figure 2.2b is appropriate.
Reference: [137] <institution> Texas Instruments Inc. </institution> <note> TMS320C4x User's Guide, 1991. Literature number SPRU063. </note>
Reference-contexts: In this section, I give an overview of the characteristics of these devices. I focus only on the modern, floating-point devices; for more detailed information on specific devices, see <ref> [115, 119, 34, 5, 6, 98, 138, 137] </ref>. The device contains six functional units|an ALU, a multiplier, two address units, and two load-store units (not shown here)|all of which can operate simultaneously, a bank of floating-point data registers, and a bank of address registers. <p> More complex instructions also place limitations on source and destination registers. The TMS320C30 and TMS320C40 <ref> [138, 137] </ref> are essentially register-memory machines: operands can be located in memory or in registers. A typical instruction for these devices is the parallel multiply-and-add instruction: mpyf r0,r1,r0 || addf *ar0++,*ar1++(ir0)%,r2 In addition to the multiply-and-add operations, two operands are loaded from memory CHAPTER 2.
Reference: [138] <institution> Texas Instruments Inc. </institution> <note> TMS320C3x User's Guide, 1992. Literature number SPRU031C. </note>
Reference-contexts: In this section, I give an overview of the characteristics of these devices. I focus only on the modern, floating-point devices; for more detailed information on specific devices, see <ref> [115, 119, 34, 5, 6, 98, 138, 137] </ref>. The device contains six functional units|an ALU, a multiplier, two address units, and two load-store units (not shown here)|all of which can operate simultaneously, a bank of floating-point data registers, and a bank of address registers. <p> More complex instructions also place limitations on source and destination registers. The TMS320C30 and TMS320C40 <ref> [138, 137] </ref> are essentially register-memory machines: operands can be located in memory or in registers. A typical instruction for these devices is the parallel multiply-and-add instruction: mpyf r0,r1,r0 || addf *ar0++,*ar1++(ir0)%,r2 In addition to the multiply-and-add operations, two operands are loaded from memory CHAPTER 2.
Reference: [139] <author> D.A. Turner. </author> <title> The semantic elegance of applicative languages. </title> <booktitle> Proc. ACM Conf. on Functional Programming and Computer Architecture, </booktitle> <pages> pages 85-92, </pages> <year> 1981. </year>
Reference-contexts: Higher-order functions provide a very powerful mechanism for constructing and parameterising process networks. (A higher-order function takes a function argument, or delivers a function result.) This allows the expression of visual programs with "large thoughts" <ref> [139] </ref>. I explore this idea in section 5.4. A simple form of higher-order function mechanism has recently been added to Ptolemy [87], inspired by one of my earlier papers [114, 86]. CHAPTER 1. <p> Dataflow process networks are examined in chapter 3. 2.2 Functional programming in five minutes Functional programming languages are "higher-level" than more conventional imperative languages. There have been many persuasive arguments advanced for functional programming languages in general [8], and lazy functional languages in particular <ref> [64, 139, 62] </ref>. A recent study indicates that at least some of the claimed advantages of functional languages|brevity, rapidity of development, and ease of understanding|can be confirmed [63]. The study compares several languages, including C++ and Ada, in a substantial rapid prototyping exercise.
Reference: [140] <author> David A. Turner. </author> <title> An approach to functional operating systems. In D.A. </title> <editor> Turner, editor, </editor> <booktitle> Research Topics in Functional Programming, </booktitle> <pages> pages 199-218. </pages> <address> Addison-Welsey, Reading, MA, </address> <year> 1990. </year>
Reference-contexts: In the context of operating systems, a process is a function from a list of input messages of type ff to a list of output messages of type fi; Turner <ref> [140] </ref>, for example, gives an example of a process: process :: [ff] ! [fi] process = p s0 where p s (a : x) = out a s ++ p (trans a s) x where s0 is the initial state of the process, trans :: ff ! ! is the state <p> Non-determinism is thus contained in one place only|the sorting office|simplifying reasoning about the system and eliminating any need for awkward extensions to the language. New processes can be created by sending a message to the distinguished process-creation process. Turner <ref> [140] </ref> develops Stoye's model, adding "wrappers" to messages to allow type checking, and using synchronous (unbuffered) communication instead of asynchronous communication. Similarly to Jones and Sinclair, wrapper functions are hyper-strict.
Reference: [141] <author> Steven R. Vegdahl. </author> <title> A survey of proposed architectures for the execution of functional languages. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-23(12):1050-1071, </volume> <month> December </month> <year> 1984. </year>
Reference-contexts: Jagannathan surveys the current state of dataflow computing models, including a comparison of demand- and data-driven execution [65]. Dataflow architectures have also been proposed for implementation of functional programming languages <ref> [141] </ref>. Field and Har-rison point out that demand-driven evaluation of functional programs gives normal-order semantics, while data-driven evaluation gives applicative-order semantics [47, chapter 14]. 3.1.5 Firing rules Lee has recently formalised the notion of "firing" an actor [87].
Reference: [142] <author> Ingrid M. Verbauwhede, Chris J. Scheers, and Jan M. Rabaey. </author> <title> Specification and support for multi-dimensional DSP in the Silage language. </title> <booktitle> In ICASSP 94, pages II-473-II-476, </booktitle> <address> Adelaide, Australia, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Silage [57] is perhaps the best example of a language designed specifically for DSP programming; recent work explores higher-level signal pro CHAPTER 5. STATIC PROCESS NETWORKS 125 cessing programming concepts <ref> [142] </ref>. It has also evolved into a commercial product, DFL [154]. The most noticeable feature of Silage is its support for streams of data and delays. The expression x@1 is the signal x delayed by one sample. Arithmetic operations extend point-wise to streams.
Reference: [143] <author> Eric Verhulst. </author> <title> Meeting the parallel DSP challenge with the real-time Virtuoso programming system. </title> <booktitle> DSP Applications, </booktitle> <pages> pages 41-56, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Another approach uses light-weight operating system kernels and support libraries <ref> [104, 39, 143] </ref>. This is particular evident in parallel systems, where the added complexities of multi-processor communication and synchronisation seem to encourage adoption of multi-tasking kernels. The approach in which I am interested here is to use a programming language, but at a much higher level than C.
Reference: [144] <author> W. W. Wadge and A. Ashcroft. </author> <title> Lucid|the Dataflow Programming Language. </title> <publisher> Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: This kind of parallelism is therefore the functionally-parallel counterpart to data parallelism|it focuses on the essential aspects of parallelism without excessive concern for low-level detail. CHAPTER 2. BACKGROUND MATERIAL 19 Pipeline parallelism also includes certain "dataflow" programming languages. Lucid <ref> [144] </ref> is a (first-order) pipeline-parallel language. <p> Slightly more sophisticated are examples like mapS (fl4), which multiplies each element of its stream argument by four. There is no need for a "constant stream" of fours, as would be required by languages such as Lucid <ref> [144] </ref>. A further point to note is a difference in the way that processes are instantiated. In section 3.2.3, an actor is instantiated by binding it to a unique vertex name; stream CHAPTER 5. STATIC PROCESS NETWORKS 142 arguments are supplied via the graph topology. <p> There are two key approaches to modelling time: Insert hiatons into the stream. A hiaton is a special token representing the passage of time <ref> [144] </ref>. 166 CHAPTER 6. DYNAMIC PROCESS NETWORKS 167 Attach a time-stamp to each token, denoting the real time at which the token occurs [24]. <p> The approach I develop in this chapter explicitly represents time. It thus has the advantage of precision over Pino et al 's method, although it will be more complex to implement. Other languages that explicitly model time include real-time extensions to Lucid, and the synchronous languages. Lucid <ref> [144] </ref> is a dataflow-like language, in which all data|even constants|occurs in streams.
Reference: [145] <author> P. Wadler. </author> <title> Applicative style programming, program transformation and list operators. </title> <booktitle> In Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 25-32. </pages> <publisher> ACM, </publisher> <year> 1981. </year>
Reference-contexts: A difficulty with the process fusion identities is that there seems to be no end to the CHAPTER 5. STATIC PROCESS NETWORKS 160 number of identities that we may require. This problem was noted some time ago by Wadler in the context of lazy lists <ref> [145] </ref>. This led to the development of (successively) the techniques of listlessness [146] and deforestation [148], aimed at automatic fusion of list functions for program efficiency. More recently, Gill et al discovered an improved method [51], which is now part of some Haskell compilers.
Reference: [146] <author> Philip Wadler. </author> <title> Listlessness is better than laziness: Lazy evaluation and garbage collection at compile time. </title> <booktitle> In Proc. ACM Symp. on Lisp and Functional Programming, </booktitle> <year> 1984. </year>
Reference-contexts: STATIC PROCESS NETWORKS 160 number of identities that we may require. This problem was noted some time ago by Wadler in the context of lazy lists [145]. This led to the development of (successively) the techniques of listlessness <ref> [146] </ref> and deforestation [148], aimed at automatic fusion of list functions for program efficiency. More recently, Gill et al discovered an improved method [51], which is now part of some Haskell compilers.
Reference: [147] <author> Philip Wadler. </author> <title> How to replace failure by a list of successes|a method for exception-handling, backtracking, and pattern-matching in lazy functional languages. </title> <editor> In J.-P. Jouannaud, editor, </editor> <booktitle> Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 113-128. </pages> <publisher> Springer-Verlag, </publisher> <year> 1985. </year> <note> LNCS 201. </note>
Reference-contexts: Laziness is a nice property for a number of reasons. Firstly, it is necessary to guarantee referential transparency: an expression can always be substituted with an equivalent expression without changing program meaning. Secondly, it allows the use of some programming techniques that cannot be used in non-lazy languages|see <ref> [19, 66, 147] </ref> for examples of programs that can only be constructed in a lazy language. For our purposes, laziness makes it easy to build infinite data structures and therefore to simulate pipeline dataflow systems.
Reference: [148] <author> Philip Wadler. </author> <title> Deforestation: Transforming programs to eliminate trees. </title> <journal> Theoretical Computer Science, </journal> <volume> 73 </volume> <pages> 231-248, </pages> <year> 1990. </year>
Reference-contexts: STATIC PROCESS NETWORKS 160 number of identities that we may require. This problem was noted some time ago by Wadler in the context of lazy lists [145]. This led to the development of (successively) the techniques of listlessness [146] and deforestation <ref> [148] </ref>, aimed at automatic fusion of list functions for program efficiency. More recently, Gill et al discovered an improved method [51], which is now part of some Haskell compilers. It is not easy to see, however, how these techniques can be applied to our purposes, for two reasons.
Reference: [149] <author> Philip Wadler and Stephen Blott. </author> <title> How to make ad-hoc polymorphism less ad-hoc. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 60-76, </pages> <address> Austin, Texas, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: For example, map can be defined as the following -abstraction: map = "f xs -&gt; case xs of [] -&gt; [] 2.2.6 Polymorphism and type classes One of the key innovations of Haskell is its type classes, which add ad-hoc polymorphism to the Hindley-Milner type system <ref> [149] </ref>. A type class is a way of grouping types together with functions that operate on those types. Given in the class declaration are the types CHAPTER 2. BACKGROUND MATERIAL 26 of the functions of that class, and (optionally) default definitions of some functions.
Reference: [150] <author> Malcolm Wallace. </author> <title> Functional Programming and Embedded Systems. </title> <type> PhD thesis, </type> <institution> Dept. Of Computer Science, University of York, UK, </institution> <month> January </month> <year> 1995. </year> <note> BIBLIOGRAPHY 205 </note>
Reference-contexts: In their scheme, the message type itself serves as the message address. Wallace has implemented an operating system for embedded systems in Gofer; unlike the other systems reviewed here, the stream constructor is not hyper-strict in its head <ref> [150] </ref>. Nonetheless, Wallace suggests that head-hyper-strictness would be an important ingredient in making evaluation of functional programs predictable enough to meet real-time scheduling constraints. CHAPTER 2.
Reference: [151] <author> Malcolm Wallace and Colin Runciman. </author> <title> Type-checked message-passing between functional processes. </title> <booktitle> In Proc. Glasgow Functional Programming Workshop. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1994. </year> <booktitle> Workshops in Computer Science Series. </booktitle>
Reference-contexts: Similarly to Jones and Sinclair, wrapper functions are hyper-strict. Apart from eliminating the need for global garbage collection, this allows the operating system to be executed on a loosely-coupled network of processors. Wallace and Runciman use constructor classes [69] to express type-safe communication between processes <ref> [151] </ref>. In their scheme, the message type itself serves as the message address. Wallace has implemented an operating system for embedded systems in Gofer; unlike the other systems reviewed here, the stream constructor is not hyper-strict in its head [150].
Reference: [152] <author> Kevin Waugh, Patrick McAndrew, and Greg Michaelson. </author> <title> Parallel implementations from functional prototypes|a case study. </title> <type> Technical Report 90/4, </type> <institution> Heriot-Watt University, Edinburgh, UK, </institution> <year> 1990. </year>
Reference-contexts: Reade [113], for example, explains function application using box-and-arrow diagrams; Kelly [78] illustrates networks of streams and processes written using an annotated pure functional language; Waugh et al <ref> [152] </ref> illustrate the effect of program transformation on parallelism. There have also been proposals for formal visual functional languages.
Reference: [153] <author> A. L. Wendelborn and H. Garsden. </author> <title> Exploring the stream data type in SISAL and other languages. </title> <editor> In M. Cosnard, K. Ebcioglu, and J.-L. Gaudiot, editors, </editor> <booktitle> Architectures for Fine and Medium Grain Parallelism, </booktitle> <pages> pages 283-294. </pages> <publisher> IFIP, Elsevier Science Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Wendelborn and Garsden compare a number of stream implementations <ref> [153] </ref>, and point out the difference between channels and streams, noting that the terms overlap somewhat in usage. A channel is destructive, since elements are appended to the channel. Kahn process networks and the dataflow process model use channels, since new tokens are appended to them.
Reference: [154] <author> Patrick Willekens, Dirk Devisch, Marc Van Canneyt, Paul Conflitti, and Dominique Genin. </author> <title> Algorithm specification in DSP station using Data Flow Language. </title> <booktitle> DSP Applications, </booktitle> <pages> pages 8-16, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Willekens et al argue that block-diagrams alone are inadequate for specifying DSP systems, since they do not permit expression of control flow; detailed computation is best specified in a textual language, leaving the block diagram to higher levels of description <ref> [154] </ref>. Another approach uses light-weight operating system kernels and support libraries [104, 39, 143]. This is particular evident in parallel systems, where the added complexities of multi-processor communication and synchronisation seem to encourage adoption of multi-tasking kernels. <p> Silage [57] is perhaps the best example of a language designed specifically for DSP programming; recent work explores higher-level signal pro CHAPTER 5. STATIC PROCESS NETWORKS 125 cessing programming concepts [142]. It has also evolved into a commercial product, DFL <ref> [154] </ref>. The most noticeable feature of Silage is its support for streams of data and delays. The expression x@1 is the signal x delayed by one sample. Arithmetic operations extend point-wise to streams. Silage also supports fixed-point data types and arithmetic, an important aspect of programming real DSP devices.
Reference: [155] <author> Carla S. Williams and John R. Rasure. </author> <title> A visual language for image processing. </title> <booktitle> In Proc. 1990 IEEE Workshop on Visual Languages, </booktitle> <pages> pages 86-91, </pages> <year> 1990. </year>
Reference-contexts: Other languages use cycles and special-purpose dataflow nodes; in Khoros, for example, the programmer places a LOOP glyph at the start of a section of flow-graph to be iterated, and a feedback connection to this glyph from the end of the flow-graph section <ref> [155] </ref>. Pure functional languages rely on recursion instead of iteration. In a higher-order language such as Haskell, patterns of iteration are captured by higher-order functions; applying the higher-order function is equivalent to coding an iterative loop in other languages (see sections 5.2.2 and 5.3.4).
References-found: 155

