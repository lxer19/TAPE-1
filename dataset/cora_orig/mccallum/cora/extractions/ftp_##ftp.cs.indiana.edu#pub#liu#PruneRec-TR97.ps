URL: ftp://ftp.cs.indiana.edu/pub/liu/PruneRec-TR97.ps
Refering-URL: http://www.cs.indiana.edu/hyplan/liu.html
Root-URL: http://www.cs.indiana.edu
Title: Eliminating Dead Computations on Recursive Data  
Author: Yanhong A. Liu 
Date: July 1997  
Abstract: This paper describes a general and effective method for backward dependence analysis in the presence of recursive data constructions. The goal is to identify partially dead recursive data and eliminate dead computations on them. The method uses projections based on general regular tree grammars extended with the notion of live and dead, and defines the analysis as mutually recursive grammar transformers. To guarantee that the analysis terminates, we describe how to use finite grammar abstract domains or carefully designed approximation operations. For methods based on finite domains, we describe a new application in caching intermediate results for program improvement. For methods based on approximation operations, we describe algorithms for three such operations that together produce more precise analysis results than previous methods. Finally, the analysis results are used to identify and eliminate dead computations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> S. Abramsky and C. Hankin, editors. </editor> <title> Abstract Interpretation of Declarative Languages. Ellis Horwood Series in Computers and Their Applications. </title> <editor> E. </editor> <publisher> Horwood, Chichester; Halsted Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: In Section 6, we describe three approximation operations that together allow our analysis to give more precise results than previous methods. 5 Using finite grammar abstract domains As with most analyses based on abstract interpretation <ref> [1] </ref>, we can use a finite abstract domain D. In particular, we can do this on a per-program basis. Since the set I of indices is finite for a given program, we just need to use a finite domain G of grammars.
Reference: [2] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1974. </year>
Reference-contexts: This is related to incrementalization, as has been discussed in Section 4. However, the cache-and-prune method [27] allows us to achieve drastic program optimizations that are not possible otherwise. We have used this method in deriving a collection of dynamic programming programs found in standard texts <ref> [2, 34, 10] </ref>. Powerful and automatic pruning techniques are essential in such optimizations. The methods in this paper are being implemented in the Synthesizer Generator [35].
Reference: [3] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Dead computations produce values that never get used <ref> [3] </ref>. While programmers are not likely to write code that performs dead computations, such code appears often as the result of program optimization, modification, and reuse. <p> It is essentially dead code analysis used for other software engineering purposes. It can also be regarded as a kind of program specialization with respect to program output [36]. Dead code elimination. This is a most traditional compiler optimization <ref> [3] </ref> and is a most straightforward application of our analysis and transformation. Since programmers do not intentionally write much dead code, this optimization is usually most effective when combined with other code optimizations or reorganizations, such as deforestation, incrementalization, and code reuse. Deforestation and fusion.
Reference: [4] <author> A. Aiken and B. R. Murphy. </author> <title> Implementing regular tree expressions. </title> <booktitle> In Proceedings of the 5th ACM Conference on FPCA, volume 523 of Lecture Notes in Computer Science, </booktitle> <pages> pages 427-447, </pages> <address> Cambridge, Massachusetts, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag, Berlin. </note>
Reference-contexts: We reason about grammar orderings modulo this equivalence. This equivalence is decidable for two reasons. First, given G 1 and G 2 , the question L G 1 L G 2 is decidable <ref> [14, 4] </ref>. Second, given G 1 and G 2 , we can construct G 0 1 and G 0 2 such that G 1 G 2 iff L G 0 1 L G 0 2 . <p> This approximates the possibly non-existing fixed-point, forcing the iteration to terminate while guaranteeing that the sufficiency conditions are satisfied. Widening operations have been defined implicitly <ref> [4, 38] </ref> or explicitly [12] for regular tree grammars. The idea is to enforce the use of deterministic regular tree grammars, i.e., grammars that do not produce N s s 1 ; :::; N 0 n ). <p> Powerful and automatic pruning techniques are essential in such optimizations. The methods in this paper are being implemented in the Synthesizer Generator [35]. The implementation involves efficient algorithms on regular tree grammars and their uses in program analysis <ref> [4, 29] </ref>. 9 Related work and conclusion Our backward dependence analysis uses domain projections to specify sufficient information. Wadler and Hughes use projections for strictness analysis [41]. Their analysis is also backward but seeks necessary rather than sufficient information, and it uses a fixed finite abstract domain for all programs.
Reference: [5] <author> A. Aiken and B. R. Murphy. </author> <title> Static type inference in a dynamically typed language. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on POPL, </booktitle> <month> January </month> <year> 1991. </year>
Reference-contexts: Formal language theory is a powerful framework for representing unbounded structures using bounded representations, which are often even precise [12]. In particular, regular tree grammars have been used to describe partial data structures and other data flow information <ref> [20, 30, 31, 5, 38, 12, 36] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [19], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [30, 31, 5, 38, 36] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [36] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [6] <author> R. Bodik and R. Gupta. </author> <title> Partial dead code elimination using slicing transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '97 Conference on PLDI, </booktitle> <pages> pages 159-170, </pages> <address> Las Vegas, Nevada, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so that more dead code can be identified in more complicated computations <ref> [18, 13, 23, 27, 36, 6] </ref>. A particularly important problem is to identify dead computations on recursive data. Recursive data constructions are used increasingly widely in high-level languages, and dead code analysis for them is important for various program manipulations [21, 18, 28, 27, 36].
Reference: [7] <author> W.-N. Chin. </author> <title> Safe fusion of functional expressions. </title> <booktitle> In Proceedings of the 1992 ACM Conference on LFP, </booktitle> <pages> pages 11-20, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Deforestation and fusion. These optimizations combine function applications to avoid building large intermediate results, i.e., results of function calls to be passed as arguments to other function calls <ref> [40, 7] </ref>. To guarantee that the optimizations can be done effectively, the functions and subexpressions involved must satisfy certain conditions, e.g., be in blazed treeless forms [40]. Our analysis can help identify dead functions and subexpressions not satisfying these conditions and prune them out, thus making deforestation more widely applicable.
Reference: [8] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> 20(11) 850-856, November 1977. 
Reference-contexts: Incrementalization, finite differencing, and strength reduction. These optimizations focus on finding and replacing subcomputations whose values can be retrieved from the result of a previous computation, and they can often achieve asymptotic speedup <ref> [8, 32, 28, 26] </ref>. Dead code elimination is the last step in such optimizations; it removes computations whose values were used in computing the replaced subcomputations and is crucial for the overall speedup. Caching intermediate results for program improvement.
Reference: [9] <author> J. Cocke and J. T. Schwartz. </author> <title> Programming Languages and Their Compilers; Preliminary Notes. </title> <type> Technical report, </type> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <year> 1970. </year>
Reference: [10] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press/McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: This is related to incrementalization, as has been discussed in Section 4. However, the cache-and-prune method [27] allows us to achieve drastic program optimizations that are not possible otherwise. We have used this method in deriving a collection of dynamic programming programs found in standard texts <ref> [2, 34, 10] </ref>. Powerful and automatic pruning techniques are essential in such optimizations. The methods in this paper are being implemented in the Synthesizer Generator [35].
Reference: [11] <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Conference Record of the 14th Annual ACM Symposium on POPL, </booktitle> <pages> pages 238-252, </pages> <address> Los Angeles, California, </address> <month> Jan. </month> <year> 1977. </year>
Reference-contexts: To guarantee that all f i (G)'s involved stabilize after a finite number of iterations, we use a widening operation. The idea of widening was first proposed by Cousot and Cousot <ref> [11] </ref>. A widening operation G 1 O G 2 of two grammars G 1 and G 2 has two properties: 1. G 1 G 1 O G 2 and G 2 G 1 O G 2 . 2.
Reference: [12] <author> P. Cousot and R. Cousot. </author> <title> Formal language, grammar and set-constraint-based program analysis by abstract interpretation. </title> <booktitle> In Proceedings of the 17th International Conference on FPCA, </booktitle> <pages> pages 170-181, </pages> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Formal language theory is a powerful framework for representing unbounded structures using bounded representations, which are often even precise <ref> [12] </ref>. In particular, regular tree grammars have been used to describe partial data structures and other data flow information [20, 30, 31, 5, 38, 12, 36]. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> Formal language theory is a powerful framework for representing unbounded structures using bounded representations, which are often even precise [12]. In particular, regular tree grammars have been used to describe partial data structures and other data flow information <ref> [20, 30, 31, 5, 38, 12, 36] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> Finite transformers can be obtained by restricting them to be written in a specific meta-language <ref> [12] </ref>. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors [20, 12] and can be rewritten as a set of constraints [16, 17, 12]. <p> Finite transformers can be obtained by restricting them to be written in a specific meta-language [12]. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors <ref> [20, 12] </ref> and can be rewritten as a set of constraints [16, 17, 12]. As discussed by Cousot and Cousot, this is essentially a masking of the explicit use of an approximation, called widening, when defining transformers [12]. <p> Finite transformers can be obtained by restricting them to be written in a specific meta-language [12]. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors [20, 12] and can be rewritten as a set of constraints <ref> [16, 17, 12] </ref>. As discussed by Cousot and Cousot, this is essentially a masking of the explicit use of an approximation, called widening, when defining transformers [12]. <p> As discussed by Cousot and Cousot, this is essentially a masking of the explicit use of an approximation, called widening, when defining transformers <ref> [12] </ref>. Appropriate finite abstract domains can often be obtained for various applications of the analysis, and they can provide sufficiently precise analysis results on a per-program basis. We discuss this method and one of its new applications in Section 5. <p> This approximates the possibly non-existing fixed-point, forcing the iteration to terminate while guaranteeing that the sufficiency conditions are satisfied. Widening operations have been defined implicitly [4, 38] or explicitly <ref> [12] </ref> for regular tree grammars. The idea is to enforce the use of deterministic regular tree grammars, i.e., grammars that do not produce N s s 1 ; :::; N 0 n ). <p> Finding an appropriate widening operation is diffi cult. For example, we found that the so-called widening operation proposed by Cousot and Cousot <ref> [12] </ref> does not satisfy the second condition in the definition. We describe below an appropriate widening operation for regular-tree-grammar-based projections. To facilitate widening, for every f i (G) used in the iteration, we associate a unique tag with its initial value AB. <p> To summarize, our analysis applies to general recursive data constructions and produces more precise analysis results than previous methods. We believe that our treatment is also more rigorous, since we adopt the view that regular-tree-grammar-based program analysis is also abstract interpretation <ref> [12] </ref>. We extend the grammars and handle ID and AB specially in grammar ordering, equivalence, and approximation operations. Our transformers for dead code analysis are general. Our methods for computing them terminate by either using appropriate finite grammar domains or combining carefully designed approximation operations. <p> Such operations are difficult to design, e.g., we found that the so-called widening operation given in a previous work <ref> [12] </ref> may still lead to an infinitely increasing chain. While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis [16, 17, 12], we do not know any work that treats precise dead code analysis as we do. <p> Such operations are difficult to design, e.g., we found that the so-called widening operation given in a previous work [12] may still lead to an infinitely increasing chain. While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [16, 17, 12] </ref>, we do not know any work that treats precise dead code analysis as we do. To conclude, the overall goal is to analyze dead data and eliminate computations on them across recursions and loops, possibly interleaved with wrappers like classes in object oriented programming styles.
Reference: [13] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. M. Wegman, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so that more dead code can be identified in more complicated computations <ref> [18, 13, 23, 27, 36, 6] </ref>. A particularly important problem is to identify dead computations on recursive data. Recursive data constructions are used increasingly widely in high-level languages, and dead code analysis for them is important for various program manipulations [21, 18, 28, 27, 36].
Reference: [14] <author> F. Gecseg and M. Steinb. </author> <title> Tree Automata. </title> <publisher> Akademiai Kiado, </publisher> <address> Budapest, </address> <year> 1984. </year>
Reference-contexts: We reason about grammar orderings modulo this equivalence. This equivalence is decidable for two reasons. First, given G 1 and G 2 , the question L G 1 L G 2 is decidable <ref> [14, 4] </ref>. Second, given G 1 and G 2 , we can construct G 0 1 and G 0 2 such that G 1 G 2 iff L G 0 1 L G 0 2 . <p> As a safe case, if size (G 2 ) &lt; size (G 1 ) for a given well-founded measure size, then we continue the normal on-demand evaluation. Since each regular tree grammar corresponds to a finite automata, which can be minimized <ref> [14] </ref>, we can use the number of states as the measure.
Reference: [15] <author> C. A. Gunter. </author> <title> Semantics of Programming Languages. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: Since constructed data can be recursive, and subparts of them can be dead, to present analysis results, we first need a way of describing partially dead recursive data. Note that completely live or dead data are special cases of partially dead data. Domain projection <ref> [37, 15] </ref> has been shown to be a clean tool for describing partial constructed data by simply projecting out the parts that are of interest [41, 24, 31, 36]; we use this notion to project out the parts of data that are live.
Reference: [16] <author> N. Heintze. </author> <title> Set-Based Program Analysis. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Finite transformers can be obtained by restricting them to be written in a specific meta-language [12]. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors [20, 12] and can be rewritten as a set of constraints <ref> [16, 17, 12] </ref>. As discussed by Cousot and Cousot, this is essentially a masking of the explicit use of an approximation, called widening, when defining transformers [12]. <p> Such operations are difficult to design, e.g., we found that the so-called widening operation given in a previous work [12] may still lead to an infinitely increasing chain. While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [16, 17, 12] </ref>, we do not know any work that treats precise dead code analysis as we do. To conclude, the overall goal is to analyze dead data and eliminate computations on them across recursions and loops, possibly interleaved with wrappers like classes in object oriented programming styles.
Reference: [17] <author> N. Heintze. </author> <title> Set-based analysis of ML programs. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LFP, </booktitle> <pages> pages 306-317, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Finite transformers can be obtained by restricting them to be written in a specific meta-language [12]. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors [20, 12] and can be rewritten as a set of constraints <ref> [16, 17, 12] </ref>. As discussed by Cousot and Cousot, this is essentially a masking of the explicit use of an approximation, called widening, when defining transformers [12]. <p> Such operations are difficult to design, e.g., we found that the so-called widening operation given in a previous work [12] may still lead to an infinitely increasing chain. While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [16, 17, 12] </ref>, we do not know any work that treats precise dead code analysis as we do. To conclude, the overall goal is to analyze dead data and eliminate computations on them across recursions and loops, possibly interleaved with wrappers like classes in object oriented programming styles.
Reference: [18] <author> J. Hughes. </author> <title> Compile-time analysis of functional programs. </title> <editor> In D. Turner, editor, </editor> <booktitle> Research Topics in Functional Programming, chapter 5, </booktitle> <pages> pages 117-153. </pages> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1999. </year>
Reference-contexts: We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so that more dead code can be identified in more complicated computations <ref> [18, 13, 23, 27, 36, 6] </ref>. A particularly important problem is to identify dead computations on recursive data. Recursive data constructions are used increasingly widely in high-level languages, and dead code analysis for them is important for various program manipulations [21, 18, 28, 27, 36]. <p> A particularly important problem is to identify dead computations on recursive data. Recursive data constructions are used increasingly widely in high-level languages, and dead code analysis for them is important for various program manipulations <ref> [21, 18, 28, 27, 36] </ref>. <p> There, partial dead code refers to code that is dead on some but not all computation paths. 1 head context and every tail context is the same <ref> [18] </ref>, relying on crude methods to force termination [27], or using a limited class of regular tree grammars [36]. This paper describes a general and effective method for eliminating dead computations on recursive data. <p> Several analyses are in the same spirit as ours, even though some do not use the name projection. The necessity interpretation by Jones and Le Metayer [21] uses necessity patterns that correspond to projections. Necessity patterns specify only heads and tails of list values. The absence analysis by Hughes <ref> [18] </ref> uses the name context in place of projection. Even if it is extended for recursive data types, it handles only a finite domain of list contexts where every head context and every tail context is the same.
Reference: [19] <author> N. D. Jones and S. S. Muchnick. </author> <title> Flow analysis and optimization of LISP-like structures. </title> <booktitle> In Conference Record of the 6th Annual ACM Symposium on POPL, </booktitle> <pages> pages 244-256, </pages> <address> San Antonio, Texas, </address> <month> January </month> <year> 1979. </year>
Reference-contexts: The analysis in this paper represents projections using regular tree grammars extended with live and dead, which more precisely describe partially dead data structures and more naturally guarantee bounded growth. The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick <ref> [19] </ref>, where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times [30, 31, 5, 38, 36].
Reference: [20] <author> N. D. Jones and S. S. Muchnick. </author> <title> Flow analysis and optimization of LISP-like structures. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <title> Program Flow Analysis, </title> <booktitle> chapter 4, </booktitle> <pages> pages 102-131. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year> <month> 20 </month>
Reference-contexts: Formal language theory is a powerful framework for representing unbounded structures using bounded representations, which are often even precise [12]. In particular, regular tree grammars have been used to describe partial data structures and other data flow information <ref> [20, 30, 31, 5, 38, 12, 36] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> such that L G = L G 0 : P 0 = P [ fN ) R : N ) Rg fN ) Z : Z is of the form of a right hand side of (7)g: (10) This construction is similar to when only the selector form is used <ref> [20] </ref>. The proof that L G = L G 0 is also similar, and we omit it here. We simply call an extended regular tree grammar a regular tree grammar. Grammar abstract domains. <p> Finite transformers can be obtained by restricting them to be written in a specific meta-language [12]. This meta-language corresponds to a restricted class of regular tree grammars extended with selectors <ref> [20, 12] </ref> and can be rewritten as a set of constraints [16, 17, 12]. As discussed by Cousot and Cousot, this is essentially a masking of the explicit use of an approximation, called widening, when defining transformers [12].
Reference: [21] <author> S. B. Jones and D. Le Metayer. </author> <title> Compile-time garbage collection by sharing analysis. </title> <booktitle> In Proceedings of the 4th International Conference on FPCA, </booktitle> <pages> pages 54-74, </pages> <address> London, U.K., </address> <month> September </month> <year> 1989. </year>
Reference-contexts: A particularly important problem is to identify dead computations on recursive data. Recursive data constructions are used increasingly widely in high-level languages, and dead code analysis for them is important for various program manipulations <ref> [21, 18, 28, 27, 36] </ref>. <p> Several methods have been studied, but all have limitations: specifying only heads and tails of list values <ref> [21] </ref>, handling only list context where every fl This work is supported by a junior-faculty start-up grant from Indiana University. Author's address: Computer Science Department, Indiana University, 215 Lindley Hall, Bloomington, IN 47405. Phone: (812)855-4373. <p> Several analyses are in the same spirit as ours, even though some do not use the name projection. The necessity interpretation by Jones and Le Metayer <ref> [21] </ref> uses necessity patterns that correspond to projections. Necessity patterns specify only heads and tails of list values. The absence analysis by Hughes [18] uses the name context in place of projection.
Reference: [22] <author> K. Kennedy. </author> <title> Use-definition chains with applications. </title> <journal> Journal of Computer Languages, </journal> <volume> 3(3) </volume> <pages> 163-179, </pages> <year> 1978. </year>
Reference: [23] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Partial dead code elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on PLDI, </booktitle> <pages> pages 147-158, </pages> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so that more dead code can be identified in more complicated computations <ref> [18, 13, 23, 27, 36, 6] </ref>. A particularly important problem is to identify dead computations on recursive data. Recursive data constructions are used increasingly widely in high-level languages, and dead code analysis for them is important for various program manipulations [21, 18, 28, 27, 36].
Reference: [24] <author> J. Launchbury. </author> <title> Projection Factorisations in Partial Evaluation. </title> <type> PhD thesis, </type> <institution> Department of Computing, University of Glasgow, </institution> <year> 1989. </year>
Reference-contexts: Note that completely live or dead data are special cases of partially dead data. Domain projection [37, 15] has been shown to be a clean tool for describing partial constructed data by simply projecting out the parts that are of interest <ref> [41, 24, 31, 36] </ref>; we use this notion to project out the parts of data that are live. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> Wadler and Hughes use projections for strictness analysis [41]. Their analysis is also backward but seeks necessary rather than sufficient information, and it uses a fixed finite abstract domain for all programs. Launchbury uses projections for binding-time analysis of partially static data structures in 18 partial evaluation <ref> [24] </ref>. It is a forward analysis and is proved equivalent to strictness analysis [25]. Mogensen also uses projections, based on grammars in particular, to do binding-time analysis, but he uses only a restricted class of regular tree grammars [31].
Reference: [25] <author> J. Launchbury. </author> <title> Strictness and binding-time analysis: Two for the price of one. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on PLDI, </booktitle> <pages> pages 80-91, </pages> <address> Toronto, Ontario, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Launchbury uses projections for binding-time analysis of partially static data structures in 18 partial evaluation [24]. It is a forward analysis and is proved equivalent to strictness analysis <ref> [25] </ref>. Mogensen also uses projections, based on grammars in particular, to do binding-time analysis, but he uses only a restricted class of regular tree grammars [31]. Several analyses are in the same spirit as ours, even though some do not use the name projection.
Reference: [26] <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Discovering auxiliary information for incremental computation. </title> <booktitle> In Conference Record of the 23rd Annual ACM Symposium on POPL, </booktitle> <pages> pages 157-170, </pages> <address> St. Petersburg Beach, Florida, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: Incrementalization, finite differencing, and strength reduction. These optimizations focus on finding and replacing subcomputations whose values can be retrieved from the result of a previous computation, and they can often achieve asymptotic speedup <ref> [8, 32, 28, 26] </ref>. Dead code elimination is the last step in such optimizations; it removes computations whose values were used in computing the replaced subcomputations and is crucial for the overall speedup. Caching intermediate results for program improvement.
Reference: [27] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Caching intermediate results for program improvement. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on PEPM, </booktitle> <pages> pages 190-201, </pages> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so that more dead code can be identified in more complicated computations <ref> [18, 13, 23, 27, 36, 6] </ref>. A particularly important problem is to identify dead computations on recursive data. Recursive data constructions are used increasingly widely in high-level languages, and dead code analysis for them is important for various program manipulations [21, 18, 28, 27, 36]. <p> A particularly important problem is to identify dead computations on recursive data. Recursive data constructions are used increasingly widely in high-level languages, and dead code analysis for them is important for various program manipulations <ref> [21, 18, 28, 27, 36] </ref>. <p> There, partial dead code refers to code that is dead on some but not all computation paths. 1 head context and every tail context is the same [18], relying on crude methods to force termination <ref> [27] </ref>, or using a limited class of regular tree grammars [36]. This paper describes a general and effective method for eliminating dead computations on recursive data. The method represents partially dead recursive data using projections based on general regular tree grammars extended with the notion of live and dead. <p> Even though the above simple method always works, we may develop appropriate finite grammar abstract domains for particular applications. As a special example, an incremental program f 0 (x; ffix; r) may use values embedded in r, a tree of all intermediate results of a previous computation f (x) <ref> [27] </ref>. In particular, r is obtained from the definition of f and has a finite structure. This finite structure is used to define a finite grammar abstract domain. <p> Using this information, we can prune f so that it returns only intermediate results that are useful in the incremental computation. This problem is important in the cache-and-prune method for program improvement <ref> [27] </ref>. We illustrate this method with an example. <p> Using the cache-and-prune method <ref> [27] </ref>, we (i) cache all intermediate results of b and obtain b, so b = 1st ( b (i; j)), and (ii) incrementalize b and obtain b 0 , so b 0 (i; j; r) = b (i + 1; j) for r = b (i; j): b (i; j) = <p> T triple (ID; S; AB)g; fS ) T triple (ID; AB; AB)g: Then, for incremental computation, we need only parts of r, in particular, the closure of the dependencies computed from G 0 = fS ) T triple (ID; AB; AB)g and G k+1 = b 03 (G k ) <ref> [27] </ref>. <p> Caching intermediate results for program improvement. This is related to incrementalization, as has been discussed in Section 4. However, the cache-and-prune method <ref> [27] </ref> allows us to achieve drastic program optimizations that are not possible otherwise. We have used this method in deriving a collection of dynamic programming programs found in standard texts [2, 34, 10]. Powerful and automatic pruning techniques are essential in such optimizations. <p> Even if it is extended for recursive data types, it handles only a finite domain of list contexts where every head context and every tail context is the same. The analysis for pruning by Liu and Teitelbaum <ref> [27] </ref> uses projections to specify specific components of tuple values and thus provide more accurate information. However, methods used there for handling unbounded growth of such projections are crude. <p> Our transformers for dead code analysis are general. Our methods for computing them terminate by either using appropriate finite grammar domains or combining carefully designed approximation operations. In particular, we describe how to use finite grammar domains for pruning analysis in a new application <ref> [27] </ref>, and we develop a previously unknown approximation operation that allows us to compute very precise analysis result. Such operations are difficult to design, e.g., we found that the so-called widening operation given in a previous work [12] may still lead to an infinitely increasing chain.
Reference: [28] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 24(1) </volume> <pages> 1-39, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: A particularly important problem is to identify dead computations on recursive data. Recursive data constructions are used increasingly widely in high-level languages, and dead code analysis for them is important for various program manipulations <ref> [21, 18, 28, 27, 36] </ref>. <p> Incrementalization, finite differencing, and strength reduction. These optimizations focus on finding and replacing subcomputations whose values can be retrieved from the result of a previous computation, and they can often achieve asymptotic speedup <ref> [8, 32, 28, 26] </ref>. Dead code elimination is the last step in such optimizations; it removes computations whose values were used in computing the replaced subcomputations and is crucial for the overall speedup. Caching intermediate results for program improvement.
Reference: [29] <author> D. Melski and T. Reps. </author> <title> Interconvertibility of set constraints and conext-free language reachability. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on PEPM, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Powerful and automatic pruning techniques are essential in such optimizations. The methods in this paper are being implemented in the Synthesizer Generator [35]. The implementation involves efficient algorithms on regular tree grammars and their uses in program analysis <ref> [4, 29] </ref>. 9 Related work and conclusion Our backward dependence analysis uses domain projections to specify sufficient information. Wadler and Hughes use projections for strictness analysis [41]. Their analysis is also backward but seeks necessary rather than sufficient information, and it uses a fixed finite abstract domain for all programs.
Reference: [30] <author> P. Mishra and U. Reddy. </author> <title> Declaration-free type checking. </title> <booktitle> In Conference Record of the 12th Annual ACM Symposium on POPL, </booktitle> <pages> pages 7-21, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Formal language theory is a powerful framework for representing unbounded structures using bounded representations, which are often even precise [12]. In particular, regular tree grammars have been used to describe partial data structures and other data flow information <ref> [20, 30, 31, 5, 38, 12, 36] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [19], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [30, 31, 5, 38, 36] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [36] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [31] <author> T. Mogensen. </author> <title> Separating binding times in language specifications. </title> <booktitle> In Proceedings of the 4th International Conference on FPCA, </booktitle> <pages> pages 12-25, </pages> <address> London, U.K., </address> <month> September </month> <year> 1989. </year>
Reference-contexts: Note that completely live or dead data are special cases of partially dead data. Domain projection [37, 15] has been shown to be a clean tool for describing partial constructed data by simply projecting out the parts that are of interest <ref> [41, 24, 31, 36] </ref>; we use this notion to project out the parts of data that are live. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> Formal language theory is a powerful framework for representing unbounded structures using bounded representations, which are often even precise [12]. In particular, regular tree grammars have been used to describe partial data structures and other data flow information <ref> [20, 30, 31, 5, 38, 12, 36] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> It is a forward analysis and is proved equivalent to strictness analysis [25]. Mogensen also uses projections, based on grammars in particular, to do binding-time analysis, but he uses only a restricted class of regular tree grammars <ref> [31] </ref>. Several analyses are in the same spirit as ours, even though some do not use the name projection. The necessity interpretation by Jones and Le Metayer [21] uses necessity patterns that correspond to projections. Necessity patterns specify only heads and tails of list values. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [19], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [30, 31, 5, 38, 36] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [36] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [32] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Incrementalization, finite differencing, and strength reduction. These optimizations focus on finding and replacing subcomputations whose values can be retrieved from the result of a previous computation, and they can often achieve asymptotic speedup <ref> [8, 32, 28, 26] </ref>. Dead code elimination is the last step in such optimizations; it removes computations whose values were used in computing the replaced subcomputations and is crucial for the overall speedup. Caching intermediate results for program improvement.
Reference: [33] <author> W. Pugh and E. Rosser. </author> <title> Iteration space slicing and its application to communication optimization. </title> <type> Technical Report CS-TR-3737, </type> <institution> Department of Computer Science, University of Maryland, College Park, Maryland, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: This paper discusses techniques for recursion. The basic ideas should extend to loops. A recent work has just started this direction; it extends slicing to symbolically capture particular iterations in a loop <ref> [33] </ref>. Object-oriented programming style is used widely, but cross-class optimization heavily depends on inlining, which often causes code blow-up.
Reference: [34] <author> P. W. Purdom, Jr. and C. A. Brown. </author> <title> The Analysis of Algorithms. </title> <publisher> Holt, Rinehart and Winston, </publisher> <year> 1985. </year>
Reference-contexts: This is related to incrementalization, as has been discussed in Section 4. However, the cache-and-prune method [27] allows us to achieve drastic program optimizations that are not possible otherwise. We have used this method in deriving a collection of dynamic programming programs found in standard texts <ref> [2, 34, 10] </ref>. Powerful and automatic pruning techniques are essential in such optimizations. The methods in this paper are being implemented in the Synthesizer Generator [35].
Reference: [35] <author> T. Reps and T. Teitelbaum. </author> <title> The Synthesizer Generator: A System for Constructing Language-Based Editors. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: We have used this method in deriving a collection of dynamic programming programs found in standard texts [2, 34, 10]. Powerful and automatic pruning techniques are essential in such optimizations. The methods in this paper are being implemented in the Synthesizer Generator <ref> [35] </ref>. The implementation involves efficient algorithms on regular tree grammars and their uses in program analysis [4, 29]. 9 Related work and conclusion Our backward dependence analysis uses domain projections to specify sufficient information. Wadler and Hughes use projections for strictness analysis [41].
Reference: [36] <author> T. Reps and T. Turnidge. </author> <title> Program specialization via program slicing. </title> <editor> In O. Danvy, R. Gluck, and P. Thiemann, editors, </editor> <booktitle> Proceedings of the Dagstuhl Seminar on Partial Evaluation, volume 1110 of Lecture Notes in Computer Science, </booktitle> <pages> pages 409-429. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: While programmers are not likely to write code that performs dead computations, such code appears often as the result of program optimization, modification, and reuse. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions, e.g., program slices <ref> [42, 36] </ref>. Analysis for identifying live or dead code, or code of similar relevance, has been studied and used widely [9, 8, 22, 32, 3, 21, 18, 13, 23, 28, 27, 39, 36]. <p> We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so that more dead code can be identified in more complicated computations <ref> [18, 13, 23, 27, 36, 6] </ref>. A particularly important problem is to identify dead computations on recursive data. Recursive data constructions are used increasingly widely in high-level languages, and dead code analysis for them is important for various program manipulations [21, 18, 28, 27, 36]. <p> A particularly important problem is to identify dead computations on recursive data. Recursive data constructions are used increasingly widely in high-level languages, and dead code analysis for them is important for various program manipulations <ref> [21, 18, 28, 27, 36] </ref>. <p> There, partial dead code refers to code that is dead on some but not all computation paths. 1 head context and every tail context is the same [18], relying on crude methods to force termination [27], or using a limited class of regular tree grammars <ref> [36] </ref>. This paper describes a general and effective method for eliminating dead computations on recursive data. The method represents partially dead recursive data using projections based on general regular tree grammars extended with the notion of live and dead. <p> Note that completely live or dead data are special cases of partially dead data. Domain projection [37, 15] has been shown to be a clean tool for describing partial constructed data by simply projecting out the parts that are of interest <ref> [41, 24, 31, 36] </ref>; we use this notion to project out the parts of data that are live. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> Formal language theory is a powerful framework for representing unbounded structures using bounded representations, which are often even precise [12]. In particular, regular tree grammars have been used to describe partial data structures and other data flow information <ref> [20, 30, 31, 5, 38, 12, 36] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> We can make our analysis more careful so that the new program behaves the same as the original one on more inputs that cause error. Reps and Turnidge do this partially by doing a separate shape analysis <ref> [36] </ref>, but they do not report any error after all. In any case, errors can be handled only partially due to undecidabilities, thus we can not make the optimized program behave exactly the same as the original program. This difference is unavoidable for our language that has call-by-value semantics. <p> This is called backward slicing [42], and it helps debug and reuse program pieces. It is essentially dead code analysis used for other software engineering purposes. It can also be regarded as a kind of program specialization with respect to program output <ref> [36] </ref>. Dead code elimination. This is a most traditional compiler optimization [3] and is a most straightforward application of our analysis and transformation. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [19], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [30, 31, 5, 38, 36] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [36] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis. <p> It is later used to describe other data flow information such as types and binding times [30, 31, 5, 38, 36]. In particular, the analysis for backward slicing by Reps and Turnidge <ref> [36] </ref> explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [37] <author> D. S. Scott. </author> <title> Lectures on a mathematical theory of computation. </title> <editor> In M. Broy and G. Schmidt, editors, </editor> <booktitle> Theoretical Foundations of Programming Methodology, </booktitle> <pages> pages 145-292. </pages> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1982. </year> <booktitle> Lecture notes of 1981 Marktoberdorf Summer School on Theoretical Foundations of Programming Methodology, directed by F.L. Bauer, E.W. Dijkstra, and C.A.R. Hoare. </booktitle>
Reference-contexts: Since constructed data can be recursive, and subparts of them can be dead, to present analysis results, we first need a way of describing partially dead recursive data. Note that completely live or dead data are special cases of partially dead data. Domain projection <ref> [37, 15] </ref> has been shown to be a clean tool for describing partial constructed data by simply projecting out the parts that are of interest [41, 24, 31, 36]; we use this notion to project out the parts of data that are live.
Reference: [38] <author> M. H. Sorensen. </author> <title> A grammar-based data-flow analysis to stop deforestation. </title> <editor> In S. Tison, editor, CAAP'94: </editor> <booktitle> Proceedings of the 19th International Colloquium on Trees in Algebra and Programming, volume 787 of Lecture Notes in Computer Science, </booktitle> <pages> pages 335-351, </pages> <address> Edinburgh, U.K., April 1994. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Formal language theory is a powerful framework for representing unbounded structures using bounded representations, which are often even precise [12]. In particular, regular tree grammars have been used to describe partial data structures and other data flow information <ref> [20, 30, 31, 5, 38, 12, 36] </ref>. We describe partially dead recursive data using projections that are represented using regular tree grammars. <p> This approximates the possibly non-existing fixed-point, forcing the iteration to terminate while guaranteeing that the sufficiency conditions are satisfied. Widening operations have been defined implicitly <ref> [4, 38] </ref> or explicitly [12] for regular tree grammars. The idea is to enforce the use of deterministic regular tree grammars, i.e., grammars that do not produce N s s 1 ; :::; N 0 n ). <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [19], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [30, 31, 5, 38, 36] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [36] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [39] <author> F. </author> <title> Tip. A survey of program slicing techniques. </title> <journal> Journal of Programming Languages, </journal> <volume> 3(3) </volume> <pages> 121-189, </pages> <month> September </month> <year> 1995. </year>
Reference: [40] <author> P. Wadler. </author> <title> Deforestation: Transforming programs to eliminate trees. </title> <journal> Theoretical Computer Science, </journal> <volume> 73 </volume> <pages> 231-248, </pages> <year> 1990. </year> <note> Special issue of selected papers from the 2nd ESOP. </note>
Reference-contexts: Deforestation and fusion. These optimizations combine function applications to avoid building large intermediate results, i.e., results of function calls to be passed as arguments to other function calls <ref> [40, 7] </ref>. To guarantee that the optimizations can be done effectively, the functions and subexpressions involved must satisfy certain conditions, e.g., be in blazed treeless forms [40]. Our analysis can help identify dead functions and subexpressions not satisfying these conditions and prune them out, thus making deforestation more widely applicable. <p> To guarantee that the optimizations can be done effectively, the functions and subexpressions involved must satisfy certain conditions, e.g., be in blazed treeless forms <ref> [40] </ref>. Our analysis can help identify dead functions and subexpressions not satisfying these conditions and prune them out, thus making deforestation more widely applicable. Incrementalization, finite differencing, and strength reduction.
Reference: [41] <author> P. Wadler and R. J. M. Hughes. </author> <title> Projections for strictness analysis. </title> <booktitle> In Proceedings of the 3rd International Conference on FPCA, volume 274 of Lecture Notes in Computer Science, </booktitle> <pages> pages 385-407, </pages> <address> Portland, Oregon, </address> <month> September </month> <year> 1987. </year>
Reference-contexts: Note that completely live or dead data are special cases of partially dead data. Domain projection [37, 15] has been shown to be a clean tool for describing partial constructed data by simply projecting out the parts that are of interest <ref> [41, 24, 31, 36] </ref>; we use this notion to project out the parts of data that are live. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> The implementation involves efficient algorithms on regular tree grammars and their uses in program analysis [4, 29]. 9 Related work and conclusion Our backward dependence analysis uses domain projections to specify sufficient information. Wadler and Hughes use projections for strictness analysis <ref> [41] </ref>. Their analysis is also backward but seeks necessary rather than sufficient information, and it uses a fixed finite abstract domain for all programs. Launchbury uses projections for binding-time analysis of partially static data structures in 18 partial evaluation [24].
Reference: [42] <author> M. Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(4):352-357, </volume> <month> July </month> <year> 1984. </year> <month> 21 </month>
Reference-contexts: While programmers are not likely to write code that performs dead computations, such code appears often as the result of program optimization, modification, and reuse. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions, e.g., program slices <ref> [42, 36] </ref>. Analysis for identifying live or dead code, or code of similar relevance, has been studied and used widely [9, 8, 22, 32, 3, 21, 18, 13, 23, 28, 27, 39, 36]. <p> Slicing. Starting at a particular index in the program, not necessarily the final result of the entire program, the analysis helps slice out data and computations that are possibly needed for that index. This is called backward slicing <ref> [42] </ref>, and it helps debug and reuse program pieces. It is essentially dead code analysis used for other software engineering purposes. It can also be regarded as a kind of program specialization with respect to program output [36]. Dead code elimination.
References-found: 42

