URL: http://www.cs.brandeis.edu/~waterman/papers/proposal.ps
Refering-URL: http://www.cs.brandeis.edu/~waterman/home.html
Root-URL: http://www.cs.brandeis.edu
Title: An Associative Model of Word Use  
Author: Scott A. Waterman 
Degree: a dissertation proposal  
Note: Applications of the model to practical NLP and language understanding tasks will be shown.  
Date: October 18, 1994  
Abstract: I present an automatic method for distinguishing the various modes of use of words by categorizing context cues, and describe a model of word usage that characterizes the various syntactic contexts in which words appear without resort to a traditional grammar or part of speech labels. The model is built inductively by identifying patterns of word configurations through observation of large text corpora. The model and the acquisition techniques are based a class of models of word use I call associative models. The premise for these models is that the use of a word in a particular context is licensed by associative patterns among the other words in that context and their configuration. The model is derived by classifying the contexts of words with respect to the items they contain and their configuration as they appear in corpus. The model provides a quantitative measure of word similarity along several syntactic and semantic dimensions, as well as a corresponding hierarchical categorization of words into syntactic and semantic classes. 
Abstract-found: 1
Intro-found: 1
Reference: <editor> (1988). </editor> <booktitle> Proc. of the 12 th International Conference on Computational Linguistics, </booktitle> <address> Budapest. </address> <year> (1988). </year> <title> The wall street journal. </title> <booktitle> (1991). Proceedings of the 29 th Annual Meeting of the ACL. Association for Computational Lingis-tics. (1992). Proceedings of the 14 th International Conference on Computational Linguistics, </booktitle> <address> Nantes, France. </address> <year> (1992). </year> <booktitle> Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <address> San Mateo. </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Anderberg, M. R. </author> <year> (1973). </year> <title> Cluster Analysis for Applications. </title> <publisher> Academic Press, </publisher> <address> New York. </address> <note> originally Anerbergs thesis. 17 Ayuso, </note> <author> D., Boisen, S., Fox, H., Ingria, R., & Weischedel, R. </author> <year> (1992). </year> <title> Bbn: Description of the plum system as used for muc-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4). </booktitle>
Reference: <author> Baker, J. K. </author> <year> (1975). </year> <title> Stochastic modeling for automatic speech understanding. </title> <editor> In Reddy, D. R. (Ed.), </editor> <title> Speech Recognition. </title> <publisher> Academic Press. </publisher>
Reference-contexts: Like non-probabilistic grammars induction schemes, PCFGs must also either be constrained to a normal form or to a backbone of assumed rules. In trying to induce the structure and organization of language, there is no guarantee that these constraints will correspond with the actual structure. * n-gram models - <ref> (Baker, 1975) </ref>, (Meteer et al., 1991) n-gram models are Markov models in which a subsequence of input is used to predict the probability or interpretation of following tokens.
Reference: <author> Basili, R., Pazienza, M. T., & Velardi, P. </author> <year> (1993). </year> <title> Hierarchical clustering of verbs. In Acquisition of Lexical Knowledge from Text, </title> <booktitle> Proceedings of the SIGLEX Workshop. </booktitle>
Reference: <author> Black, E., Jelinek, F., Lafferty, J., Magerman, D., Mercer, R., & Roukos, S. </author> <year> (1992). </year> <title> Towards history-bases grammars: Using richer models of probabilistic parsing. </title> <booktitle> In Proceedings of the Fifth DARPA Workshop on Speech & Natural Language. </booktitle>
Reference: <author> Boguraev, B. & Briscoe, T. (Eds.) </author> <year> (1989). </year> <title> Computational Lexicography for Natural Language Processing. Longman, London. gidlines to using Longman's LDOCE for lexical acquisition. </title>
Reference: <author> Brent, M. R. </author> <year> (1991). </year> <title> Automatic acquisition of subcategorization frames from untagged text. </title> <booktitle> In Proceedings of the 29 th Annual Meeting of the ACL. </booktitle>
Reference: <author> Brown, P., Della Pietra, V. J., DeSouza, P. V., Lai, J. C., & Mercer, R. L. </author> <year> (1992). </year> <title> Class-bassed n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4). </volume>
Reference: <author> Chan, S. & Wang, A. </author> <year> (1991). </year> <title> Synthesis and recognition of sequences. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(12) </volume> <pages> 1245-1255. </pages>
Reference-contexts: studies used various similarity measures, such as manhattan metric ((Hughs, 1994), vector cosine measures ((Hughs, 1994), (Futrelle & Gauch, 1993)), cosine measures over a reduced space ((Schutze, 1993)), or entropy measures (<ref> (Chan & Wang, 1991) </ref>. Various clustering methods were used to form categories based on these measures. Chan and Wang (Chan & Wang, 1991) have used 0 th -order simply conditioned models to characterize genetic sequences, with some success. They refer to these models as syntheses. 10 3.4.1 Higher order conditioning The information loss of this simple vector model can be reduced through conditioning the probabilities.
Reference: <author> Cowie, J., Guthrie, L., Pustejovsky, J., Wakao, T., Wang, J., & Waterman, S. </author> <year> (1993). </year> <title> The diderot information extraction system. </title> <booktitle> In Proceedings of the First Conference of the Pacific Association for Computational Linguistics, </booktitle> <address> Vancouver. </address>
Reference: <author> Dagan, I., Marcus, S., & Markovitch, S. </author> <year> (1993). </year> <title> Contextual word similarity and estimation from sparse data. </title> <booktitle> In Proceedings of the 31 st Annual Meeting of the ACL. </booktitle>
Reference: <author> Dagan, I., Periera, F., & Lee, L. </author> <year> (1994). </year> <title> Similarity-based estimation of word coocurrence probablities. </title> <booktitle> In Proceedings of the 32 nd Annual Meeting of the ACL. </booktitle>
Reference: <author> DeMarcken, C. </author> <year> (1990). </year> <title> Parsing the lob corpus. </title> <booktitle> In Proceedings of the 28 th Annual Meeting of the ACL. </booktitle>
Reference: <author> Finch, S. & Chater, N. </author> <year> (1992). </year> <title> Bootstrapping syntactic caegories using statsitical methods. </title> <editor> In Daelemans, W. & Powers, D. (Eds.), </editor> <booktitle> Background and Experiments in Machine Learning of Natural Langague. </booktitle>
Reference: <author> Fu, K.-S. & Booth, T. L. </author> <year> (1975). </year> <title> Grammatical inference: Introduction and survey. </title> <journal> IEEE Transactions on Sytems, Man, and Cybernetics, 5(1,4). </journal>
Reference-contexts: If one wants to use this data to inform the understanding of language, however, one must have a clear model on which to base one's study. Traditional rule-based grammars can be induced from corpus data (e.g. <ref> (Fu & Booth, 1975) </ref>), but if the researcher does not supply a `backbone' of high level rules, current techniques must either posit rules in some highly constrained form (CNF, for example), or else suffer through searching an enormous combinatorial space of rules.
Reference: <author> Fukunaga, K. </author> <year> (1990). </year> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <note> second edition. </note>
Reference: <author> Futrelle, R. & Gauch, S. </author> <year> (1993). </year> <title> Experiments in syntactic and semantic classifications and disambiguations using bootstrapping. In Acquisition of Lexical Knowledge from Text, </title> <booktitle> Proceedings of the SIGLEX Workshop. </booktitle>
Reference-contexts: sets of (0; 2)-contexts s 1 = f [kAB]; [kBA]g (25) will have identical representations p (B) = :5 k p (A) = :5 Even with this deleterious loss of correlation, this sort of representation has worked fairly well for a few word-based classifications ((Schutze, 1993),(Hughs, 1994),(Finch & Chater, 1992), <ref> (Futrelle & Gauch, 1993) </ref>). All of these include studies of (2; 2)-contexts represented as 0 th -order conditioned models. In each study, one set of contexts was used for all the occurrences of a given word. <p> The conditioned models for every set were compared, and classes of contexts were formed which corresponded to categories of words, all having similar contexts. These studies used various similarity measures, such as manhattan metric ((Hughs, 1994), vector cosine measures ((Hughs, 1994), <ref> (Futrelle & Gauch, 1993) </ref>), cosine measures over a reduced space ((Schutze, 1993)), or entropy measures ((Chan & Wang, 1991). Various clustering methods were used to form categories based on these measures.
Reference: <author> Gale, W. A. & Church, K. </author> <year> (1990). </year> <title> Poor estimates of context are worse than none. </title> <booktitle> In Proceedings of the 28 th Annual Meeting of the ACL. </booktitle> <volume> 18 Goodman, </volume> <editor> M., Waterman, S., & Alterman, R. </editor> <year> (1991). </year> <title> Interactive reasoning about spatial compo-nents. </title> <booktitle> In Proceedings 13 th Conference of the Cognitive Science Society, </booktitle> <address> Hillsdale, NJ. </address> <publisher> Earlbaum. </publisher>
Reference: <author> Grishman, R., Macleod, C., & Sterling, J. </author> <year> (1992). </year> <title> New york university: Description of the proteus system as used for muc-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4). </booktitle>
Reference: <author> Grishman, R. & Sterling, J. </author> <year> (1992). </year> <title> Acquisition of selectional patterns. </title> <booktitle> In Proceedings of the 14 th International Conference on Computational Linguistics. </booktitle>
Reference: <author> Hindle, D. & Rooth, M. </author> <year> (1991). </year> <title> Structual ambigity and lexical relations. </title> <booktitle> In Proceedings of the 29 th Annual Meeting of the ACL. </booktitle>
Reference: <author> Hobbs, J. & Appelt, D. </author> <year> (1992). </year> <title> Sri international: Description of the fastus system used for muc-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4). </booktitle>
Reference: <author> Hobbs, J., Appelt, D., & Pal, M. </author> <year> (1990). </year> <title> Interpretation as abduction. </title> <type> Technical Report Technical Note 449, </type> <institution> SRI International AI Center, Palo Alto. </institution>
Reference: <author> Hogeweg, P. & Hesper, B. </author> <year> (1984). </year> <title> The alignment of sets of sequences and the construction of phyletic trees: An integrated method. </title> <journal> J. Molecular Evolution, </journal> <volume> 20 </volume> <pages> 175-186. </pages>
Reference: <author> Hughs, J. </author> <year> (1994). </year> <title> Automatically acquiring a classification of words. </title> <type> PhD thesis, </type> <institution> University of Leeds. </institution>
Reference-contexts: This was problem seems to be the cause of most mis-classifications in <ref> (Hughs, 1994) </ref>. There is no reason to expect that two words which are used in multiple ways will/should exhibit the same variety of usage. Take, for example, the words walk and chair. <p> Another approach is to only compute probabilities for some select set of items that the researcher feels has indicative power. For instance, (Schutze, 1993) uses only the 5000 most frequent words of English in his study, while <ref> (Hughs, 1994) </ref> uses only the 25 most frequent items. Other possibilities include high frequency function words, such as prepositions, case markers, and determiners. A more serious problem is the loss of useful information in this representation.
Reference: <author> Jelinek, F., Lafferty, J. D., & Mercer, R. L. </author> <year> (1992). </year> <title> Basic methods of probabilistic context free grammars. Technical report, Continuous speech recognition group, </title> <institution> IBM T.J. Wason Reserch Center. </institution>
Reference: <author> Johansson, S. & Stenstrom, A.-B. (Eds.) </author> <year> (1991). </year> <title> English Computer Corpora: Selected Papers and Research Guide. </title> <publisher> Mouton de Gruyter, </publisher> <address> Berlin. </address>
Reference: <author> Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., Riloff, E., & Soderland, S. </author> <year> (1992). </year> <title> University of massachusetts: Muc-4 test results and analysis. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4). </booktitle>
Reference: <author> Lehnert, W., Cardie, C., Fisher, D., Riloff, E., & Williams, R. </author> <year> (1991). </year> <booktitle> In Third Message Understanding Conference (MUC-3), </booktitle> <pages> pp. 223-233. </pages>
Reference: <author> Levenshtein, V. </author> <title> Binary codes capable of correcting deletions, </title> <journal> insertions, and reversals. Cybernetics and control Theory, </journal> <volume> 10(8) </volume> <pages> 707-710. </pages> <note> originally published as (Levenshtein, </note> <year> 1965). </year>
Reference: <author> Levenshtein, V. </author> <year> (1965). </year> <title> Binary codes capable of correcting deletions, </title> <journal> insertions, and reversals. Doklady Akademii Nauk SSR, </journal> <volume> 163(4) </volume> <pages> 845-848. </pages>
Reference: <author> Lu, S. & Fu, K. S. </author> <year> (1977). </year> <title> A clustering procedure for syntactic patterns. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics. </journal>
Reference: <author> Meteer, M., Schartz, R., & Weischedel, R. </author> <year> (1991). </year> <title> Empirical studies in part of speech labelling. </title> <booktitle> In Proc. of the 4th DARPA Workshop on Speech and Natural Language, </booktitle> <pages> pp. 331-336. </pages>
Reference-contexts: In trying to induce the structure and organization of language, there is no guarantee that these constraints will correspond with the actual structure. * n-gram models - (Baker, 1975), <ref> (Meteer et al., 1991) </ref> n-gram models are Markov models in which a subsequence of input is used to predict the probability or interpretation of following tokens.
Reference: <author> Periera, F., Tishby, N., & Lee, L. </author> <year> (1993). </year> <title> Distribution clustering of english words. </title> <booktitle> In Proceedings of the 31 st Annual Meeting of the ACL. </booktitle>
Reference: <author> Pustejovsky, J. </author> <year> (1991). </year> <title> The generative lexicon. </title> <journal> Computational Linguistics, </journal> <volume> 17(4). 19 Pustejovsky, </volume> <editor> J. </editor> <year> (1992). </year> <title> The acquisition of lexical semantic knowledge from large corpora. </title> <booktitle> In Pro--ceedings of the Fifth DARPA Workshop on Speech & Natural Language. </booktitle>
Reference: <author> Pustejovsky, J. </author> <title> (forthcoming). The generative Lexicon: A Theory of Computational Lexical Semantics. </title> <publisher> MIT Press. </publisher>
Reference: <author> Pustejovsky, J. & Anick, P. </author> <year> (1988). </year> <title> The semantic interpretation of nominals. </title> <booktitle> In Proc. of the 12 th International Conference on Computational Linguistics. </booktitle>
Reference: <author> Pustejovsky, J. & Boguraev, B. </author> <year> (1993). </year> <title> Lexical knowledge representation and natural language processing. </title> <journal> Artificial Intelligence. </journal>
Reference: <author> Sankoff, D. & Kruskal, J. (Eds.) </author> <year> (1983). </year> <title> Time warps, string edits, and macromolecules. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: two strings ((Sellers, 1974; Wagner & Fisher, 1974)) have been used on a variety of problems in biology, genetics, speech and handwriting analysis (<ref> (Sankoff & Kruskal, 1983) </ref>), as well as in syntactic analysis of formal languages ((Lu & Fu, 1977)). (For a good introduction with applications to many domains, see (Sankoff & Kruskal, 1983).) To demonstrate the generalized edit distance, consider the two strings: the path that is the path the way that is not the way The first string can be transformed into the second by a number of insertion, deletion, and substitution operations.
Reference: <author> Schutze, H. </author> <year> (1993). </year> <title> Part of speech induction from scratch. </title> <booktitle> In Proceedings of the 31 st Annual Meeting of the ACL. </booktitle>
Reference-contexts: This can be remedied by including only probabilities for those items that have been seen, leaving any excluded values implicitly zero. Another approach is to only compute probabilities for some select set of items that the researcher feels has indicative power. For instance, <ref> (Schutze, 1993) </ref> uses only the 5000 most frequent words of English in his study, while (Hughs, 1994) uses only the 25 most frequent items. Other possibilities include high frequency function words, such as prepositions, case markers, and determiners.
Reference: <author> Sellers, P. </author> <year> (1974). </year> <title> An algorithm for the distance between two finite sequences. </title> <journal> J. Comb. Thy, A16:253-258. </journal>
Reference: <author> Skakakibara, Y., MichaelBrown, Underwood, R. C., Mian, I. S., & Haussler, D. </author> <year> (1993). </year> <title> Stochastic context-free grammars for modeling rna. </title> <type> Technical report, </type> <institution> U.C. Santa Cruz. </institution>
Reference: <author> Smadja, F. </author> <year> (1989). </year> <title> Macrocoding the lexicon with co-occurrence knowledge. </title> <booktitle> In First Int'l Language Acquisition Workshop, IJCAI 89. </booktitle>
Reference-contexts: not extend well to explain linguistic phenomena, even those as basic as subject-verb agreement. * Collocation studies - (Smadja, 1989; Grishman & Sterling, 1992) These techniques find distributions of words in conjunction; either studying pairs or triples of adjoining words in sequence, of in particular syntactic relationships (subject-verb, modifier-noun, etc.) <ref> (Smadja, 1989) </ref> has introduced a surprise statistic called mutual information which gives a measure of the selectivity of a given collocation.
Reference: <author> Wagner, R. & Fisher, M. </author> <year> (1974). </year> <journal> J. ACM, </journal> <volume> 21 </volume> <pages> 168-173. </pages>
Reference: <author> Waterman, S. A. </author> <year> (1993). </year> <title> Structural methods for lexical/semantic patterns. In Acquisition of Lexical Knowledge from Text, </title> <booktitle> Proceedings of the SIGLEX Workshop. </booktitle>
Reference: <author> Waterman, S. A. </author> <year> (1994). </year> <title> Distinguished usage. </title> <editor> In Boguraev, B. & Pustejovsky, J. (Eds.), </editor> <title> Corpus Processing for Lexical Acquisition. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Some initial studies <ref> (Waterman, 1994) </ref> have already been done. June-September employed at Apple, Cupertino to develop semantic categorization for reference material. Further development of code, especially evaluation tools. Some of the small initial models will be built here. October-December completion of initial models and evaluation.
Reference: <author> Wilks, Y., Fass, D., Gou, C., McDonald, J., Plate, T., & Slator, B. M. </author> <year> (1990). </year> <title> Providing machine tractable dictionary tools. </title> <journal> Machine Translation, </journal> <volume> 5. </volume>
Reference: <author> Zernik, U. </author> <year> (1990). </year> <title> Lexical acquisition: Where is the semantics? Machine Translation, </title> <booktitle> 5 </booktitle> <pages> 155-174. 20 </pages>
References-found: 48

