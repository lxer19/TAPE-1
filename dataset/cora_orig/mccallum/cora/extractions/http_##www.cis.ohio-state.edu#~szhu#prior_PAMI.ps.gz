URL: http://www.cis.ohio-state.edu/~szhu/prior_PAMI.ps.gz
Refering-URL: http://www.cis.ohio-state.edu/~szhu/publication.html
Root-URL: http://www.cis.ohio-state.edu
Title: Prior Learning and Gibbs Reaction-Diffusion  
Author: Song Chun Zhu and David Mumford 
Keyword: P K  
Date: Nov. 1997)  
Note: IEEE Trans. on PAMI (vol.19, no.11,  
Abstract: This article addresses two important themes in early visual computation: first it presents a novel theory for learning the universal statistics of natural images a prior model for typical cluttered scenes of the world from a set of natural images, second it proposes a general framework of designing reaction-diffusion equations for image processing. We start by studying the statistics of natural images including the scale invariant properties, then generic prior models were learned to duplicate the observed statistics, based on the minimax entropy theory studied in two previous papers. The resulting Gibbs distributions have potentials of the form U (I; fl; S) = P with S = fF (1) ; F (2) ; :::; F (K) g being a set of filters and fl = f (1) (); (2) (); :::; (K) ()g the potential functions. The learned Gibbs distributions confirm and improve the form of existing prior models such as line-process, but in contrast to all previous models, inverted potentials (i.e. (x) decreasing as a function of jxj) were found to be necessary. We find that the partial differential equations given by gradient descent on U (I; fl; S) are essentially reaction-diffusion equations, where the usual energy terms produce anisotropic diffusion while the inverted energy terms produce reaction associated with pattern formation, enhancing preferred image features. We illustrate how these models can be used for texture pattern rendering, denoising, image enhancement and clutter removal by careful Song Chun Zhu is now with the Computer Science Department, Stanford University, Stanford, CA 94305, and David Mumford is with the Division of Applied Mathematics, Brown University, Providence, RI 02912. This work started when the authors were at Harvard University. choice of both prior and data models of this type, incorporating the appropriate features.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Berger, V. Della Pietra, and S. Della Pietra, </author> <title> "A maximum entropy approach to natural language processing", </title> <journal> Computational Linguistics, </journal> <volume> 22(1), </volume> <year> 1996. </year>
Reference-contexts: Recently we found that similar ideas of model inference using maximum entropy have also been used in natural language modeling <ref> [1] </ref>. In this paper, we want to study to what extent probability distributions of this type can be used to model generic natural images, and we try to answer the three questions raised above.
Reference: [2] <author> Black, M. Sapiro, G. Marimont, D and Heeger, D. </author> <title> "Robust anisotropic diffusion", </title> <journal> IEEE Trans. Image Processing, </journal> <note> (to appear). </note>
Reference-contexts: As we can see that as t ! 1, I (t) becomes a flat image. A robust anisotropic diffusion equation is recently reported in <ref> [2] </ref>. 6 Conclusion In this paper, we studied the statistics of natural images, based on which a novel theory is proposed for learning the generic prior model the universal statistics of real world scenes. We argue that the same strategy developed in this paper can be used in other applications.
Reference: [3] <author> M. J. Black and A. Rangarajan, </author> <title> "On the unification of line processes, outlier rejection, and robust statistics with applications in early vision", </title> <journal> Int'l J. Computer Vision, </journal> <volume> 19(1), </volume> <year> 1996. </year>
Reference-contexts: These prior models have been motivated by regularization theory [26, 18], 1 phys 1 Where the smoothness term is explained as a stabilizer for solving "ill-posed" problems [32]. 1 ical modeling [31, 4], 2 Bayesian theory [9, 20] and robust statistics <ref> [19, 13, 3] </ref>. Some connections between these interpretations are also observed in [12, 13] based on effective energy in statistics mechanics. Prior models of this kind are either generalized from traditional physical models [37] or chosen for mathematical convenience. <p> Similar forms of the energy functions are widely used as prior distributions [9, 4, 20, 11], and they can also be equivalently interpreted in the sense of robust statistics <ref> [13, 3] </ref> -30 -20 -10 0 10 20 30 -1 1 3 5 x -30 -20 -10 0 10 20 30 -1 -0.6 -0.2 0.2 0.6 1 a () b () 0 1+(=b) 2 In the following, we address three important properties of the Gibbs reaction- diffusion equations.
Reference: [4] <author> A. Blake and A. Zisserman. </author> <title> Visual Reconstruction. </title> <publisher> MIT press, </publisher> <year> 1987. </year>
Reference-contexts: For example, in image restoration general smoothness models are expressed as probability distributions <ref> [9, 4, 20, 11] </ref>: p (I) = Z (x;y) (r x I (x;y))+ (r y I (x;y)) where I is the image, Z is a normalization factor, and r x I (x; y) = I (x+1; y)I (x; y), r y I (x; y) = I (x; y + 1) I <p> These prior models have been motivated by regularization theory [26, 18], 1 phys 1 Where the smoothness term is explained as a stabilizer for solving "ill-posed" problems [32]. 1 ical modeling <ref> [31, 4] </ref>, 2 Bayesian theory [9, 20] and robust statistics [19, 13, 3]. Some connections between these interpretations are also observed in [12, 13] based on effective energy in statistics mechanics. Prior models of this kind are either generalized from traditional physical models [37] or chosen for mathematical convenience. <p> Similar forms of the energy functions are widely used as prior distributions <ref> [9, 4, 20, 11] </ref>, and they can also be equivalently interpreted in the sense of robust statistics [13, 3] -30 -20 -10 0 10 20 30 -1 1 3 5 x -30 -20 -10 0 10 20 30 -1 -0.6 -0.2 0.2 0.6 1 a () b () 0 1+(=b) 2
Reference: [5] <author> J. Daugman, </author> <title> "Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters. </title> <journal> Journal of Optical Society of America. </journal> <volume> vol. 2, No. 7, pp1160-1169. </volume> <year> 1985. </year>
Reference: [6] <author> D. J. </author> <title> Field, "Relations between the statistics of natural images and the response properties of cortical cells", </title> <journal> J. of Optical soc. America, A, vol.4, </journal> <volume> No. 12, </volume> <year> 1987. </year>
Reference-contexts: (ff) have in common. 3 Experiments on natural images This section presents experiments on learning prior models, and we start from exploring the statistical properties of natural images. 9 3.1 Statistic of natural images It is well known that natural images have statistical properties distinguishing them from random noise images <ref> [28, 6, 24] </ref>. In our experiments, we collected a set of 44 natural images, six of which are shown in figure 2. These images are from various sources, some digitized from books and postcards, and some from a Corel image database. <p> The histogram of natural images has higher kurtosis and heavier tails. Similar results are reported in <ref> [6] </ref>. To see the difference of the tails, figure 5b plots the logarithm of the two curves. Third, the statistics of natural images are essentially scale invariant with respect to some features. As an example, we look at filters r x and r y .
Reference: [7] <author> D. Gabor, </author> <title> "Theory of communication." </title> <booktitle> IEE Proc.vol 93, </booktitle> <address> no.26. </address> <year> 1946. </year>
Reference-contexts: In fact, the filters used above lie in the two extremes of the spectrum of all linear filters. As discussed by Gabor <ref> [7] </ref>, the ffi filter is localized in space but is extended uniformly in frequency. In contrast, some other filters, like the sine waves, are well 3 In RBF, the basis functions are presumed to be smooth, such as a Gaussian function.
Reference: [8] <author> S. B. Gelfand, and S.K. Mitter, </author> <title> "On sampling methods and annealing algorithms", in Markov Random Fields Theory and Applications Academic Press, </title> <publisher> Inc, </publisher> <year> 1993. </year>
Reference-contexts: The analyses of convergence of the equations can be found in <ref> [14, 10, 8] </ref>. The computational load for the annealing process is notorious, but for applications like denoising, a fast decrease of temperature may not affect the final result very much. Experiment I In the first experiment, we take U C to be quadratic, i.e. C to be an i.i.d.
Reference: [9] <author> S. Geman and D. Geman. </author> <title> "Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images". </title> <journal> IEEE Trans. on PAMI 6(7), </journal> <pages> pp 721-741, </pages> <year> 1984. </year>
Reference-contexts: For example, in image restoration general smoothness models are expressed as probability distributions <ref> [9, 4, 20, 11] </ref>: p (I) = Z (x;y) (r x I (x;y))+ (r y I (x;y)) where I is the image, Z is a normalization factor, and r x I (x; y) = I (x+1; y)I (x; y), r y I (x; y) = I (x; y + 1) I <p> These prior models have been motivated by regularization theory [26, 18], 1 phys 1 Where the smoothness term is explained as a stabilizer for solving "ill-posed" problems [32]. 1 ical modeling [31, 4], 2 Bayesian theory <ref> [9, 20] </ref> and robust statistics [19, 13, 3]. Some connections between these interpretations are also observed in [12, 13] based on effective energy in statistics mechanics. Prior models of this kind are either generalized from traditional physical models [37] or chosen for mathematical convenience. <p> Unfortunately, there is no simple way to express the (ff) 's in terms of the (ff) as in the two extreme examples. To compute (ff) 's, we adopted the Gibbs sampler 7 <ref> [9] </ref>, which simulates an inhomogeneous Markov chain in image space L jN 2 j . <p> Similar forms of the energy functions are widely used as prior distributions <ref> [9, 4, 20, 11] </ref>, and they can also be equivalently interpreted in the sense of robust statistics [13, 3] -30 -20 -10 0 10 20 30 -1 1 3 5 x -30 -20 -10 0 10 20 30 -1 -0.6 -0.2 0.2 0.6 1 a () b () 0 1+(=b) 2
Reference: [10] <author> S. Geman and C. Hwang, </author> <title> "Diffusion for global optimization", </title> <journal> SIAM J. Control and optimization, </journal> <volume> Vol. 24, No.5, </volume> <year> 1986. </year>
Reference-contexts: The analyses of convergence of the equations can be found in <ref> [14, 10, 8] </ref>. The computational load for the annealing process is notorious, but for applications like denoising, a fast decrease of temperature may not affect the final result very much. Experiment I In the first experiment, we take U C to be quadratic, i.e. C to be an i.i.d.
Reference: [11] <author> D. Geman and G. Reynoids, </author> <title> "Constrained restoration and the recover of discontinu-ities", </title> <journal> IEEE Trans. PAMI, vol.14, </journal> <volume> pp.367-383, </volume> <year> 1992. </year> <month> 34 </month>
Reference-contexts: For example, in image restoration general smoothness models are expressed as probability distributions <ref> [9, 4, 20, 11] </ref>: p (I) = Z (x;y) (r x I (x;y))+ (r y I (x;y)) where I is the image, Z is a normalization factor, and r x I (x; y) = I (x+1; y)I (x; y), r y I (x; y) = I (x; y + 1) I <p> Similar forms of the energy functions are widely used as prior distributions <ref> [9, 4, 20, 11] </ref>, and they can also be equivalently interpreted in the sense of robust statistics [13, 3] -30 -20 -10 0 10 20 30 -1 1 3 5 x -30 -20 -10 0 10 20 30 -1 -0.6 -0.2 0.2 0.6 1 a () b () 0 1+(=b) 2
Reference: [12] <author> D. Geiger and F. Girosi, </author> <title> "Parallel and deterministic algorithms for MRFs: surface reconstruction", </title> <journal> IEEE Trans PAMI, </journal> <volume> 13(5) </volume> <pages> 401-412, </pages> <year> 1991. </year>
Reference-contexts: Some connections between these interpretations are also observed in <ref> [12, 13] </ref> based on effective energy in statistics mechanics. Prior models of this kind are either generalized from traditional physical models [37] or chosen for mathematical convenience.
Reference: [13] <author> D. Geiger and A. L. Yuille. </author> <title> "A common framework for image segmentation". </title> <booktitle> Inter.J.of Computer Vision 6(3), </booktitle> <pages> pp 227-243 1991. </pages>
Reference-contexts: These prior models have been motivated by regularization theory [26, 18], 1 phys 1 Where the smoothness term is explained as a stabilizer for solving "ill-posed" problems [32]. 1 ical modeling [31, 4], 2 Bayesian theory [9, 20] and robust statistics <ref> [19, 13, 3] </ref>. Some connections between these interpretations are also observed in [12, 13] based on effective energy in statistics mechanics. Prior models of this kind are either generalized from traditional physical models [37] or chosen for mathematical convenience. <p> Some connections between these interpretations are also observed in <ref> [12, 13] </ref> based on effective energy in statistics mechanics. Prior models of this kind are either generalized from traditional physical models [37] or chosen for mathematical convenience. <p> Similar forms of the energy functions are widely used as prior distributions [9, 4, 20, 11], and they can also be equivalently interpreted in the sense of robust statistics <ref> [13, 3] </ref> -30 -20 -10 0 10 20 30 -1 1 3 5 x -30 -20 -10 0 10 20 30 -1 -0.6 -0.2 0.2 0.6 1 a () b () 0 1+(=b) 2 In the following, we address three important properties of the Gibbs reaction- diffusion equations.
Reference: [14] <author> B. Gidas, </author> <title> "A renormalization group approach to image processing problems". </title> <journal> IEEE Trans. on PAMI, </journal> <volume> vol. 11, </volume> <pages> No.2, </pages> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: The analyses of convergence of the equations can be found in <ref> [14, 10, 8] </ref>. The computational load for the annealing process is notorious, but for applications like denoising, a fast decrease of temperature may not affect the final result very much. Experiment I In the first experiment, we take U C to be quadratic, i.e. C to be an i.i.d.
Reference: [15] <author> P. Grindrod, </author> <title> The theory and applications of reaction-diffusion equations, </title> <publisher> Oxford Univ. Press, </publisher> <year> 1996. </year>
Reference-contexts: 0.45 -30 -20 -10 0 10 20 30 0 0.04 0.08 0.12 0.16 0.2 an individual natural image, c, a uniform noise image. -15 -10 -5 0 5 10 15 0 0.1 0.2 0.3 0.4 -15 -10 -5 0 5 10 15 -20 -16 -12 -8 -4 0 in domain <ref> [15; 15] </ref>. b, The logarithm of the two curves in a. 12 To see this, figure 5a plots it against a Gaussian curve (dashed) of the same mean and same variance. The histogram of natural images has higher kurtosis and heavier tails. Similar results are reported in [6]. <p> Two canonical patterns that the Turing equations can synthesize are leopard blobs and zebra stripes [34, 38]. These equations are also applied to image processing such as image halfton-ing [29] and a theoretical analysis can be found in <ref> [15] </ref>. (II) The Swindale equation which simulates the development of the ocular dominance stripes in the visual cortex of cats and monkey [30]. The simulated patterns are very similar to the zebra stripes.
Reference: [16] <author> B. Kimia, A. Tannebaum, and S. Zucker, </author> <title> "Shapes, shocks, and deformations I: the components of two-dimensional shape and the reaction-diffusion space", </title> <journal> Int'l J. Computer Vision, </journal> <volume> vol. 15, pp189-224, </volume> <year> 1995. </year>
Reference-contexts: A more sophisticated prior model should incorporate concepts like object geometry, and we call such prior models second level priors. Diffusion equations derived from this second level priors are studied in image segmentation [39], and in scale space of shapes <ref> [16] </ref>. A discussion of some typical diffusion equations is given in [22]. It is our hope that this article will stimulate further investigations on building more realistic prior models as well as sophisticated PDEs for visual 33 computation.
Reference: [17] <author> S. Kullback and R. A. Leibler, </author> <title> "On information and sufficiency", </title> <journal> Annual Math. Stat. vol.22, </journal> <volume> pp79-86, </volume> <year> 1951. </year>
Reference-contexts: Given a set of filters S, and an ME distribution p (I; fl; S), the goodness of p (I; fl; S) is often measured by the Kullback-Leibler information distance between p (I; fl; S) and the ideal distribution f (I) <ref> [17] </ref>, Z Z f (I) dI = E f [log f (I)] E f [log p (I; fl; S)]: Then for a fixed model complexity K, the best feature set S fl is selected by the following criterion, S fl = arg min KL (f (I); p (I; fl; S)); where
Reference: [18] <author> J. Marroguin, S. Mitter, and T. Poggio, </author> <title> "Probabilistic solution of ill-posed problems in computational vision", </title> <journal> J. Amer.Stat. Assoc. </journal> <volume> 82(397), </volume> <year> 1987. </year>
Reference-contexts: These prior models have been motivated by regularization theory <ref> [26, 18] </ref>, 1 phys 1 Where the smoothness term is explained as a stabilizer for solving "ill-posed" problems [32]. 1 ical modeling [31, 4], 2 Bayesian theory [9, 20] and robust statistics [19, 13, 3].
Reference: [19] <author> P. Meer, D. Mintz, D.Y. Kim, and A. Rosenfeld, </author> <title> "Robust regression methods for computer vision: a review", </title> <journal> Int'l J. of Computer Vision, </journal> <volume> 6(1), </volume> <year> 1991. </year>
Reference-contexts: These prior models have been motivated by regularization theory [26, 18], 1 phys 1 Where the smoothness term is explained as a stabilizer for solving "ill-posed" problems [32]. 1 ical modeling [31, 4], 2 Bayesian theory [9, 20] and robust statistics <ref> [19, 13, 3] </ref>. Some connections between these interpretations are also observed in [12, 13] based on effective energy in statistics mechanics. Prior models of this kind are either generalized from traditional physical models [37] or chosen for mathematical convenience.
Reference: [20] <author> D. Mumford and J. Shah. </author> <title> "Optimal approximations by piecewise smooth functions and associated variational problems." </title> <journal> Comm. Pure Appl. Math., </journal> <volume> 42, </volume> <pages> pp 577-684, </pages> <year> 1989. </year>
Reference-contexts: For example, in image restoration general smoothness models are expressed as probability distributions <ref> [9, 4, 20, 11] </ref>: p (I) = Z (x;y) (r x I (x;y))+ (r y I (x;y)) where I is the image, Z is a normalization factor, and r x I (x; y) = I (x+1; y)I (x; y), r y I (x; y) = I (x; y + 1) I <p> These prior models have been motivated by regularization theory [26, 18], 1 phys 1 Where the smoothness term is explained as a stabilizer for solving "ill-posed" problems [32]. 1 ical modeling [31, 4], 2 Bayesian theory <ref> [9, 20] </ref> and robust statistics [19, 13, 3]. Some connections between these interpretations are also observed in [12, 13] based on effective energy in statistics mechanics. Prior models of this kind are either generalized from traditional physical models [37] or chosen for mathematical convenience. <p> Similar forms of the energy functions are widely used as prior distributions <ref> [9, 4, 20, 11] </ref>, and they can also be equivalently interpreted in the sense of robust statistics [13, 3] -30 -20 -10 0 10 20 30 -1 1 3 5 x -30 -20 -10 0 10 20 30 -1 -0.6 -0.2 0.2 0.6 1 a () b () 0 1+(=b) 2
Reference: [21] <author> J. D. Murray, </author> <title> "A pre-pattern formation mechanism for mammalian coat markings", </title> <journal> Journal of Theoretical Biology, </journal> <volume> vol. 88, </volume> <pages> pp 161-199, </pages> <year> 1981. </year>
Reference-contexts: We will 24 demonstrate this property in the experiments below. 4.3 Gibbs reaction-diffusion for pattern formation In the literature, there are many nonlinear PDEs for pattern formation, of which the following two examples are interesting. (I) The Turing reaction-diffusion equation which models the chemical mechanism of animal coats <ref> [33, 21] </ref>. Two canonical patterns that the Turing equations can synthesize are leopard blobs and zebra stripes [34, 38].
Reference: [22] <author> W. </author> <title> Niessen etc. "A general framework for geometry-driven evolution equations", </title> <journal> Int'l J. computer Vision, </journal> <volume> 21(3), </volume> <year> 187-205,1997. </year>
Reference-contexts: Since (1) obs (z) = 0 if j z j 22 for ff = 2; 3, we only plot (1) (z) for z 2 [9:5; 9:5] and (2) (z); (3) (z) for z 2 <ref> [22; 22] </ref>. These three curves are fitted with the functions 1 (z) = 2:1 (1 1=(1 + (j z j =4:8) 1:32 ), 2 (z) = 1:25 (1 1=(1 + (j z j =2:8) 1:5 ), 3 (z) = 1:95 (1 1=(1 + (j z j =2:8) 1:5 ) respectively. <p> Diffusion equations derived from this second level priors are studied in image segmentation [39], and in scale space of shapes [16]. A discussion of some typical diffusion equations is given in <ref> [22] </ref>. It is our hope that this article will stimulate further investigations on building more realistic prior models as well as sophisticated PDEs for visual 33 computation.
Reference: [23] <author> M. Nitzberg and T. Shiota, </author> <title> "Nonlinear image filtering with edge and corner enhancement" IEEE. </title> <journal> Trans. PAMI, </journal> <volume> vol.14, </volume> <month> Aug. </month> <year> 1992. </year> <month> 35 </month>
Reference-contexts: In <ref> [25, 23] </ref> anisotropic diffusion equations for generating image scale spaces are introduced in the following form, I t = div (c (x; y; t)rI); I (x; y; 0) = I in ; (15) where div is the divergence operator, i.e., div ( ~ V ) = r x P +r y
Reference: [24] <author> B. A. Olshausen and D. J. </author> <title> Field, "Natural image statistics and efficient coding", </title> <booktitle> Proc. of workshop on Information Theory and the Brain, </booktitle> <address> Setpember, </address> <year> 1995. </year>
Reference-contexts: (ff) have in common. 3 Experiments on natural images This section presents experiments on learning prior models, and we start from exploring the statistical properties of natural images. 9 3.1 Statistic of natural images It is well known that natural images have statistical properties distinguishing them from random noise images <ref> [28, 6, 24] </ref>. In our experiments, we collected a set of 44 natural images, six of which are shown in figure 2. These images are from various sources, some digitized from books and postcards, and some from a Corel image database.
Reference: [25] <author> P. Perona and J. Malik, </author> <title> "Scale-space and edge detection using anisotropic diffusion" IEEE Trans. </title> <journal> on PAMI, vol.12, </journal> <volume> No.7, </volume> <month> July </month> <year> 1990. </year>
Reference-contexts: We find that the partial differential equations given by gradient descent on U (I; fl; S) are essentially reaction-diffusion equations, which we call the Gibbs Reaction And Diffusion Equations (GRADE). In GRADE, the diffusion components produce denoising effects which is similar to the anisotropic diffusion <ref> [25] </ref>, while reaction components form patterns and enhance preferred image features. The learned prior models are applied to the following applications. <p> In <ref> [25, 23] </ref> anisotropic diffusion equations for generating image scale spaces are introduced in the following form, I t = div (c (x; y; t)rI); I (x; y; 0) = I in ; (15) where div is the divergence operator, i.e., div ( ~ V ) = r x P +r y <p> As a comparison, we run the anisotropic diffusion process <ref> [25] </ref> on figure 19a, and images at iterations t = 50; 100; 300 are displayed in figure 20. As we can see that as t ! 1, I (t) becomes a flat image.
Reference: [26] <author> T. Poggio, V. Torre and C. Koch, </author> <title> "Computational vision and regularization theory", </title> <journal> Nature, </journal> <volume> vol. 317, </volume> <pages> pp 314-319, </pages> <year> 1985. </year>
Reference-contexts: These prior models have been motivated by regularization theory <ref> [26, 18] </ref>, 1 phys 1 Where the smoothness term is explained as a stabilizer for solving "ill-posed" problems [32]. 1 ical modeling [31, 4], 2 Bayesian theory [9, 20] and robust statistics [19, 13, 3].
Reference: [27] <author> T. Poggio and F. Girosi, </author> <title> "Networks for approximation and learning", </title> <booktitle> Proc. of IEEE, vol.78, </booktitle> <pages> 1481-1497, </pages> <year> 1990. </year>
Reference-contexts: These two solutions stand for two typical mechanisms for constructing probability models in the literature. The first is often used for image coding [35], and the second is a special case of the learning scheme using radial basis functions (RBF) <ref> [27] </ref>. 3 Although the philosophies for learning these two prior models are very differ ent, we observe that they share two common properties. 1. The potentials 1 (), 2 () are built on the responses of linear filters.
Reference: [28] <author> D. L. Ruderman and Bialek, </author> <title> "Statistics of natural images: scaling in the woods", </title> <journal> Phys. Rev. Letter, </journal> <volume> 73 </volume> <pages> 814-817, </pages> <year> 1994. </year>
Reference-contexts: (ff) have in common. 3 Experiments on natural images This section presents experiments on learning prior models, and we start from exploring the statistical properties of natural images. 9 3.1 Statistic of natural images It is well known that natural images have statistical properties distinguishing them from random noise images <ref> [28, 6, 24] </ref>. In our experiments, we collected a set of 44 natural images, six of which are shown in figure 2. These images are from various sources, some digitized from books and postcards, and some from a Corel image database.
Reference: [29] <author> A. Sherstinsky and R. Picard, "M-lattice: </author> <title> from morphogenesis to image processing" IEEE Trans. </title> <booktitle> on Image Processing, </booktitle> <address> vol.5, no.7, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: Two canonical patterns that the Turing equations can synthesize are leopard blobs and zebra stripes [34, 38]. These equations are also applied to image processing such as image halfton-ing <ref> [29] </ref> and a theoretical analysis can be found in [15]. (II) The Swindale equation which simulates the development of the ocular dominance stripes in the visual cortex of cats and monkey [30]. The simulated patterns are very similar to the zebra stripes.
Reference: [30] <author> N. V. Swindale, </author> <title> "A model for the formation of ocular dominance stripes", </title> <journal> Proc. R. Soc. Lond. </journal> <volume> B 208, pp243-264, </volume> <year> 1980. </year>
Reference-contexts: These equations are also applied to image processing such as image halfton-ing [29] and a theoretical analysis can be found in [15]. (II) The Swindale equation which simulates the development of the ocular dominance stripes in the visual cortex of cats and monkey <ref> [30] </ref>. The simulated patterns are very similar to the zebra stripes. In this section, we show that these patterns can be easily generated with only 2 or 3 filters using the GRADE.
Reference: [31] <author> D. Terzopoulos, </author> <title> "Multilevel computational processes for visual surface reconstruction". </title> <journal> Computer Visiaon, Graphics, and Image Processing, </journal> <volume> 24, </volume> <pages> 52-96, </pages> <year> 1983. </year>
Reference-contexts: These prior models have been motivated by regularization theory [26, 18], 1 phys 1 Where the smoothness term is explained as a stabilizer for solving "ill-posed" problems [32]. 1 ical modeling <ref> [31, 4] </ref>, 2 Bayesian theory [9, 20] and robust statistics [19, 13, 3]. Some connections between these interpretations are also observed in [12, 13] based on effective energy in statistics mechanics. Prior models of this kind are either generalized from traditional physical models [37] or chosen for mathematical convenience. <p> Remark 1. In figure 9, we notice that x;s () are inverted, i.e. decreasing functions of j z j for s = 1; 2; 3, distinguishing this model from other prior models in computer vision. First of all, as the image intensity has finite range <ref> [0; 31] </ref>, r x I [s] is defined in [31; 31]. Therefore we may define x;s (z) = 0 for j z j&gt; 31, so p s (I) is still well-defined. Second, such inverted potentials have significant meaning in visual computation. <p> First of all, as the image intensity has finite range [0; 31], r x I [s] is defined in <ref> [31; 31] </ref>. Therefore we may define x;s (z) = 0 for j z j&gt; 31, so p s (I) is still well-defined. Second, such inverted potentials have significant meaning in visual computation. <p> For the ffi () filter, AI F is very big, and AIG is only slightly bigger than AIF . Since all the prior models that we learned have no preference about the image intensity domain, the image intensity has uniform distribution, but we limit it inside <ref> [0; 31] </ref>, thus the first row of table 1 has the same value for IC and AIG. For filter I (obsi) , AIF = M 1 M i.e. the biggest among all filters, and AIG ! 1. <p> In this paper, we only consider clutter as two dimensional pattern despite its geometry and 3D structure. We collected a set of images of buildings and a set of images of trees all against clean background the sky. For the tree images, we translate the image intensities to <ref> [31; 0] </ref>, i.e., 0 for sky. In this case, since the trees are always darker than the build, thus the negative intensity will approximately take care of the occlusion effects.
Reference: [32] <author> A. N. Tikhonov and V. Y. Arsenin, </author> <title> Solutions of Ill-posed Problems, 1906, (Translated version), </title> <publisher> V.H.Winston & Sons, </publisher> <year> 1977. </year>
Reference-contexts: These prior models have been motivated by regularization theory [26, 18], 1 phys 1 Where the smoothness term is explained as a stabilizer for solving "ill-posed" problems <ref> [32] </ref>. 1 ical modeling [31, 4], 2 Bayesian theory [9, 20] and robust statistics [19, 13, 3]. Some connections between these interpretations are also observed in [12, 13] based on effective energy in statistics mechanics.
Reference: [33] <author> A. M. </author> <title> Turing, "The chemical basis of morphogenesis", </title> <journal> Philosophy Trans. Royal Soc. London, </journal> <volume> vol. 237, No. B, </volume> <pages> pp, 37-72, </pages> <year> 1952. </year>
Reference-contexts: We will 24 demonstrate this property in the experiments below. 4.3 Gibbs reaction-diffusion for pattern formation In the literature, there are many nonlinear PDEs for pattern formation, of which the following two examples are interesting. (I) The Turing reaction-diffusion equation which models the chemical mechanism of animal coats <ref> [33, 21] </ref>. Two canonical patterns that the Turing equations can synthesize are leopard blobs and zebra stripes [34, 38]. <p> Furthermore our prior learning method provides a novel framework for designing reaction-diffusion equations based on the observed images in a given application, without modeling the physical or chemical processes as people did before <ref> [33] </ref>. Although the synthesized images bear important features of natural images, they are still far from realistic ones. In other words, these generic prior models can do very little beyond image restoration.
Reference: [34] <author> G. Turk, </author> <title> "Generating textures on arbitrary surfaces using reaction-diffusion", </title> <journal> Computer Graphics. </journal> <volume> Vol. 25, No.4, </volume> <year> 1991. </year>
Reference-contexts: The learned prior models are applied to the following applications. First, we run the GRADE starting with white noise images, and demonstrate how GRADE can easily generate canonical texture patterns such as leopard blobs and zebra stripe, as the Turing reaction-diffusion equations do <ref> [34, 38] </ref>. Thus our theory provides a new method for designing PDEs for pattern synthesis. <p> Two canonical patterns that the Turing equations can synthesize are leopard blobs and zebra stripes <ref> [34, 38] </ref>. These equations are also applied to image processing such as image halfton-ing [29] and a theoretical analysis can be found in [15]. (II) The Swindale equation which simulates the development of the ocular dominance stripes in the visual cortex of cats and monkey [30].
Reference: [35] <author> A. B. Watson, </author> <title> "Efficiency of model human image code", </title> <journal> Journal of Optical Society of America A. Vol.4, </journal> <volume> No.12, </volume> <year> 1987. </year>
Reference-contexts: These two solutions stand for two typical mechanisms for constructing probability models in the literature. The first is often used for image coding <ref> [35] </ref>, and the second is a special case of the learning scheme using radial basis functions (RBF) [27]. 3 Although the philosophies for learning these two prior models are very differ ent, we observe that they share two common properties. 1.
Reference: [36] <author> K. Wilson, </author> <title> "The renormalization group: critical phenonmena and the Knodo problem," </title> <journal> Rev. Mod. Phys., Vol.47, </journal> <volume> pp.773-840, </volume> <year> 1975. </year> <month> 36 </month>
Reference-contexts: However none of the existing prior models has the scale-invariance property on the 2D image lattice, i.e., is renormalizable in terms of renormalization group theory <ref> [36] </ref>.
Reference: [37] <author> G, Winkler. </author> <title> Image Analysis, Random Fields and Dynamic Monte Carlo Methods, </title> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Some connections between these interpretations are also observed in [12, 13] based on effective energy in statistics mechanics. Prior models of this kind are either generalized from traditional physical models <ref> [37] </ref> or chosen for mathematical convenience. There is, however, little rigorous theoretical or empirical justification for applying these prior models to generic images, and there is little theory to guide the construction and selection of prior models. One may ask the following questions. 1. <p> k 2 = c n for n = 1; 2; :::; M , equation (6) becomes p (I) = M P M where &lt;; &gt; is inner product, 2 (0) = 0, and 2 (x) = 1 if x 6= 0, i.e., 2 () is similar to the Potts model <ref> [37] </ref>. These two solutions stand for two typical mechanisms for constructing probability models in the literature.
Reference: [38] <author> A. Witkin and M. Kass, </author> <title> "Reaction-diffusion textures", </title> <journal> Computer Graphics. </journal> <volume> Vol. 25, No.4, </volume> <year> 1991. </year>
Reference-contexts: The learned prior models are applied to the following applications. First, we run the GRADE starting with white noise images, and demonstrate how GRADE can easily generate canonical texture patterns such as leopard blobs and zebra stripe, as the Turing reaction-diffusion equations do <ref> [34, 38] </ref>. Thus our theory provides a new method for designing PDEs for pattern synthesis. <p> Two canonical patterns that the Turing equations can synthesize are leopard blobs and zebra stripes <ref> [34, 38] </ref>. These equations are also applied to image processing such as image halfton-ing [29] and a theoretical analysis can be found in [15]. (II) The Swindale equation which simulates the development of the ocular dominance stripes in the visual cortex of cats and monkey [30].
Reference: [39] <author> S. C. Zhu and A. L. </author> <title> Yuille "Region Competition: unifying snakes, region growing, Bayes/MDL for multi-band image segmentation". </title> <journal> IEEE Trans.on PAMI. Vol.18, </journal> <volume> No.9, </volume> <year> 1996. </year>
Reference-contexts: We call the generic prior models studied in this paper the first level prior. A more sophisticated prior model should incorporate concepts like object geometry, and we call such prior models second level priors. Diffusion equations derived from this second level priors are studied in image segmentation <ref> [39] </ref>, and in scale space of shapes [16]. A discussion of some typical diffusion equations is given in [22]. It is our hope that this article will stimulate further investigations on building more realistic prior models as well as sophisticated PDEs for visual 33 computation.
Reference: [40] <author> S. C. Zhu, Y. N. Wu and D. B. Mumford. </author> <title> "Filters, Random Fields, and Minimax Entropy (FRAME): Towards a unified theory for texture modeling". </title> <booktitle> Proc. </booktitle> <address> CVPR., </address> <year> 1996a. </year> <note> (to appear in IJCV). </note>
Reference-contexts: However none of the existing prior models has the scale-invariance property on the 2D image lattice, i.e., is renormalizable in terms of renormalization group theory [36]. In previous work on modeling textures, we proposed a new class of Gibbs dis tributions of the following form <ref> [40, 41] </ref>, p (I; fl; S) = Z U (I; fl; S) = ff=1 (x;y) In the above equation, S = fF (1) ; F (2) ; :::; F (K) g is a set of linear filters, and fl = f (1) (); (2) (); :::; (K) ()g is a set <p> This goal is achieved by a minimax entropy theory studied for modeling textures in our previous papers <ref> [40, 41] </ref>. <p> For a detailed account of the computation of (ff) 's, the readers are referred to <ref> [40, 41] </ref>. In our previous papers, the following two propositions are observed. Proposition 1 Given a filter set S, and observed statistics f (ff) there is an unique solution for f (ff) ; ff = 1; 2; :::; Kg. <p> Similar phenomenon was observed in our learned texture models <ref> [40] </ref>. <p> As shown in <ref> [40] </ref>, the Gibbs distribution are capable of modeling a large variety of texture patterns, but filters and different forms for () have to be learned for a given texture pattern. 26 5 Image enhancement and clutter removal So far we have studied the use of a single energy function U (I)
Reference: [41] <author> S. C. Zhu, Y. N. Wu and D. B. Mumford. </author> <title> "Minimax entropy principle and its application to texture modeling", Neural Computation, </title> <note> (to appear) 1996b. </note>
Reference-contexts: However none of the existing prior models has the scale-invariance property on the 2D image lattice, i.e., is renormalizable in terms of renormalization group theory [36]. In previous work on modeling textures, we proposed a new class of Gibbs dis tributions of the following form <ref> [40, 41] </ref>, p (I; fl; S) = Z U (I; fl; S) = ff=1 (x;y) In the above equation, S = fF (1) ; F (2) ; :::; F (K) g is a set of linear filters, and fl = f (1) (); (2) (); :::; (K) ()g is a set <p> the maximum entropy and the best set of features fF (1) ; F (2) ; :::; F (K) g is 2 If () is quadratic, then variational solutions minimizing the potential are splines, such as flexible membrane or thin plate models. 2 selected by minimizing the entropy of p (I) <ref> [41] </ref>. The conclusion of our earlier papers is that for an appropriate choice of a small set of filters S, random samples from these models can duplicate very general classes of textures as far as normal human perception is concerned. <p> This goal is achieved by a minimax entropy theory studied for modeling textures in our previous papers <ref> [40, 41] </ref>. <p> For a detailed account of the computation of (ff) 's, the readers are referred to <ref> [40, 41] </ref>. In our previous papers, the following two propositions are observed. Proposition 1 Given a filter set S, and observed statistics f (ff) there is an unique solution for f (ff) ; ff = 1; 2; :::; Kg. <p> Enumerating all possible sets of features S in the filter bank and comparing their entropies is computational too expensive. Instead, in <ref> [41] </ref> we propose a step wise greedy procedure for minimizing the KL-distance. We start from S = ; and p (I; fl; S) a uniform distribution, and introduce one filter at a time.
Reference: [42] <author> S. C. Zhu and D. B. Mumford, </author> <title> "Learning generic prior models for visual computation" Harvard Robotics Lab TR-96-05, </title> <note> a short version appeared in CVPR97. 37 </note>
Reference-contexts: In the experiments of this paper, we have used a simpler measure of the "information gain" achieved by adding one new filter to our feature set S. This is roughly an L 1 -distance (vs. the L 2 -measure implicit in the Kullback-Leibler distance), the readers are referred to <ref> [42] </ref> for a detailed account).
References-found: 42

