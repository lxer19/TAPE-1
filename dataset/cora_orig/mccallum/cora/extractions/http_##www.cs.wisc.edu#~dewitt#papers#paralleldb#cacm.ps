URL: http://www.cs.wisc.edu/~dewitt/papers/paralleldb/cacm.ps
Refering-URL: http://www.cs.wisc.edu/~dewitt/paralleldb.html
Root-URL: 
Title: Parallel Database Systems: The Future of High Performance Database Processing 1  
Author: David J. DeWitt Jim Gray 
Date: January 1992  
Web: dewitt cs.wisc.edu Gray SFbay.enet.dec.com  
Address: 1210 W. Dayton St. 455 Market St. 7'th floor Madison, WI. 53706 San Francisco, CA. 94105-2403  
Affiliation: Computer Sciences Department San Francisco Systems Center University of Wisconsin Digital Equipment Corporation  
Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 
Abstract-found: 1
Intro-found: 1
Reference: [ALEX88] <author> Alexander, W., et. al., </author> <title> "Process and Dataflow Control in Distributed Data-Intensive Systems," </title> <booktitle> Proc. ACM SIGMOD Conf., </booktitle> <address> Chicago, IL, </address> <month> June </month> <year> 1988. </year> <month> October, </month> <year> 1983. </year>
Reference: [BITT88] <author> Bitton, D. and J. Gray, </author> <title> "Disk Shadowing," </title> <booktitle> Proceedings of the Fourteenth International Conference on Very Large Data Bases, </booktitle> <address> Los Angeles, CA, </address> <month> August, </month> <year> 1988. </year>
Reference-contexts: There is no front-end back-end distinction between programs and machines. The systems are configured at a disk per MIPS, so each ten-MIPS processor has about ten disks. Disks are typically duplexed 18 <ref> [BITT88] </ref>. Each disk is served by a set of processes managing a large shared RAM cache, a set of locks, and log records for the data on that disk pair.
Reference: [BORA83] <author> Boral, H. and D. DeWitt, </author> <title> "Database Machines: An Idea Whose Time has Passed? A Critique of the Future of Database Machines," </title> <booktitle> Proceedings of the 1983 Workshop on Database Machines, edited by H.-O. </booktitle> <editor> Leilich and M. Missikoff, </editor> <publisher> Springer-Verlag, </publisher> <year> 1983. </year>
Reference: [BORA90] <author> Boral, H. et. al., </author> <title> "Prototyping Bubba: A Highly Parallel Database System," </title> <journal> IEEE Knowledge and Data Engineering, </journal> <volume> Vol. 2, No. 1, </volume> <month> March, </month> <year> 1990. </year>
Reference-contexts: In these cases no algorithm is known to speedup or scaleup. The hash-join example shows that new parallel algorithms can improve the performance of relational operators. This is a fruitful research area <ref> [BORA90, DEWI86, KITS83, KITS90, SCHN89, SCHN90, WOLF90, ZELL90] </ref>. Even though parallelism can be obtained from conventional sequential relational algorithms by using split and merge operators, we expect that many new algorithms will be discovered in the future. 17 3. The State of the Art 3.1. <p> Time will tell whether these special-purpose components offer better price performance or peak performance than shared-nothing designs built of conventional hardware. 3.5. Bubba The Bubba prototype was implemented using a 40 node FLEX/32 multiprocessor with 40 disks <ref> [BORA90] </ref>. Although this is a shared-memory multiprocessor, Bubba was designed as a shared-nothing system and the shared-memory is only used for message passing.
Reference: [CODD70] <author> Codd, E. F. </author> <title> ,A Relational Model of Data for Large Shared Databanks. </title> <journal> CACM. </journal> <volume> Vol. 13, No 6., </volume> <month> June </month> <year> 1970. </year>
Reference: [COPE88] <author> Copeland, G., Alexander, W., Boughter, E., and T. Keller, </author> <title> "Data Placement in Bubba," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Chicago, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Bubba uses this concept by considering the access frequency (heat) of each tuple when creating partitions a relation; the goal being to balance the frequency with which each partition is accessed (its temperature) rather than the actual number of tuples on each disk (its volume) <ref> [COPE88] </ref>. While partitioning is a simple concept that is easy to implement, it raises several new physical database design issues. Each relation must now have a partitioning strategy and a set of disk fragments. <p> There is a point beyond which further partitioning actually increases the response time of a query. This point occurs when the cost of starting a query on a node becomes a significant fraction of the actual execution time <ref> [COPE88, GHAN90a] </ref>. Parallelism Within Relational Operators Data partitioning is the first step in partitioned execution of relational dataflow graphs. The basic idea is to use parallel data streams instead of writing new parallel operators (programs).
Reference: [DEWI84] <author> DeWitt, D. J., Katz, R., Olken, F., Shapiro, D., Stonebraker, M. and D. Wood, </author> <title> "Implementation Techniques for Main Memory Database Systems", </title> <booktitle> Proceedings of the 1984 SIGMOD Conference, </booktitle> <address> Boston, MA, </address> <month> June, </month> <year> 1984. </year>
Reference-contexts: The indices are implemented as B-trees or hash-tables. Gamma uses split and merge operators to execute relational algebra operators using both parallelism and pipelining [DEWI90]. Sort-merge and three different hash join methods are supported <ref> [DEWI84] </ref>. Near-linear speedup and scaleup for relational queries has been measured on this architecture [SCHN89, DEWI90, SCHN90]. 3.4. The Super Database Computer The Super Database Computer (SDC) project at the University of Tokyo presents an interesting contrast to other database systems [KITS90, HIRA90].
Reference: [DEWI86] <author> DeWitt, D., et. al., </author> <title> "GAMMA - A High Performance Dataflow Database Machine," </title> <booktitle> Proceedings of the 1986 VLDB Conference, </booktitle> <address> Japan, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: In these cases no algorithm is known to speedup or scaleup. The hash-join example shows that new parallel algorithms can improve the performance of relational operators. This is a fruitful research area <ref> [BORA90, DEWI86, KITS83, KITS90, SCHN89, SCHN90, WOLF90, ZELL90] </ref>. Even though parallelism can be obtained from conventional sequential relational algorithms by using split and merge operators, we expect that many new algorithms will be discovered in the future. 17 3. The State of the Art 3.1.
Reference: [DEWI90] <author> DeWitt, D., et. al., </author> <title> "The Gamma Database Machine Project," </title> <journal> IEEE Knowledge and Data Engineering, </journal> <volume> Vol. 2, No. 1, </volume> <month> March, </month> <year> 1990. </year>
Reference-contexts: Once a relation has been partitioned, Gamma provides both clustered and non-clustered indices on either the partitioning or non-partitioning attributes. The indices are implemented as B-trees or hash-tables. Gamma uses split and merge operators to execute relational algebra operators using both parallelism and pipelining <ref> [DEWI90] </ref>. Sort-merge and three different hash join methods are supported [DEWI84]. Near-linear speedup and scaleup for relational queries has been measured on this architecture [SCHN89, DEWI90, SCHN90]. 3.4. <p> The indices are implemented as B-trees or hash-tables. Gamma uses split and merge operators to execute relational algebra operators using both parallelism and pipelining [DEWI90]. Sort-merge and three different hash join methods are supported [DEWI84]. Near-linear speedup and scaleup for relational queries has been measured on this architecture <ref> [SCHN89, DEWI90, SCHN90] </ref>. 3.4. The Super Database Computer The Super Database Computer (SDC) project at the University of Tokyo presents an interesting contrast to other database systems [KITS90, HIRA90]. SDC takes a combined hardware and software approach to the performance problem.
Reference: [ENGL89] <author> Englert, S, J. Gray, T. Kocher, and P. Shah, </author> <title> "A Benchmark of NonStop SQL Release 2 Demonstrating Near-Linear Speedup and Scaleup on Large Databases," Tandem Computers, </title> <type> Technical Report 89.4, Tandem Part No. 27469, </type> <month> May </month> <year> 1989. </year>
Reference-contexts: By doing the index maintenance in parallel, the maintenance time for multiple indices can be held almost constant if the indices are spread among many processors and disks. Overall, the Tandem systems demonstrate near-linear scaleup on transaction processing workloads, and near-linear speedup and scaleup on large relational queries <ref> [TAND87, ENGL89] </ref>. 3.3. Gamma The current version of Gamma runs on a 32 node Intel iPSC/2 Hypercube with a disk attached to each node. In addition to round-robin, range and hash partitioning, Gamma also provides hybrid-range partitioning that combines the best features of the hash and range partitioning strategies [GHAN90b].
Reference: [GIBB91] <author> Gibbs, J, </author> " <title> Massively Parallel Systems, Rethinking Computing for Business and Science," </title> <journal> Oracle, </journal> <volume> Vol. 6, </volume> <month> No.1 December, </month> <year> 1991. </year>
Reference-contexts: The resulting system is the first to demonstrate more than 1000 transactions per second on the industry-standard TPC-B benchmark. This is far in excess of Oracle's performance on conventional mainframe systems - both in peak performance and in price/performance <ref> [GIBB91] </ref>. NCR has announced the 3600 and 3700 product lines that employ shared-nothing architectures running System V R4 of Unix on Intel 486 and 586 processors. <p> Its price/performance on these benchmarks is three times cheaper than the comparable mainframe numbers. Oracle on an nCUBE has the highest reported TPC-B numbers, and has very competitive price performance <ref> [GRAY91, GIBB91] </ref>. These benchmarks demonstrate linear scaleup on transaction processing benchmarks. Gamma, Tandem, and Teradata have demonstrated linear speedup and scaleup on complex relational database benchmarks. They scale well beyond the size of the largest mainframes.
Reference: [GHAN90a] <author> Ghandeharizadeh, S., and D.J. DeWitt, </author> <title> "Performance Analysis of Alternative Declustering Strategies", </title> <booktitle> Proceedings of the 6th International Conference on Data Engineering, </booktitle> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: There is a point beyond which further partitioning actually increases the response time of a query. This point occurs when the cost of starting a query on a node becomes a significant fraction of the actual execution time <ref> [COPE88, GHAN90a] </ref>. Parallelism Within Relational Operators Data partitioning is the first step in partitioned execution of relational dataflow graphs. The basic idea is to use parallel data streams instead of writing new parallel operators (programs).
Reference: [GHAN90b] <author> Ghandeharizadeh, S. and D. J. DeWitt, </author> <title> "Hybrid-Range Partitioning Strategy: A New Declustering Strategy for Multiprocessor Database Machines" Proceedings of the Sixteenth International Conference on Very Large Data Bases", </title> <address> Melbourne, Australia, </address> <month> August, </month> <year> 1990. </year>
Reference-contexts: Gamma The current version of Gamma runs on a 32 node Intel iPSC/2 Hypercube with a disk attached to each node. In addition to round-robin, range and hash partitioning, Gamma also provides hybrid-range partitioning that combines the best features of the hash and range partitioning strategies <ref> [GHAN90b] </ref>. Once a relation has been partitioned, Gamma provides both clustered and non-clustered indices on either the partitioning or non-partitioning attributes. The indices are implemented as B-trees or hash-tables. Gamma uses split and merge operators to execute relational algebra operators using both parallelism and pipelining [DEWI90].
Reference: [GRAE89] <author> Graefe, G., and K. Ward, </author> <title> "Dynamic Query Evaluation Plans", </title> <booktitle> Proceedings of the 1989 SIGMOD Conference, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Some dynamically select from among several plans at run time depending on, for example, the amount of physical memory actually available and the cardinalities of the intermediate results <ref> [GRAE89] </ref>. To date, no query optimizers consider all the parallel algorithms for each operator and all the query tree organizations. More work is needed in this area. Another optimization problem relates to highly skewed value distributions.
Reference: [GRAE90] <author> Graefe, G., </author> <title> "Encapsulation of Parallelism in the Volcano Query Processing System," </title> <booktitle> Proceedings of the 1990 ACM-SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: The split operator in Figure 9 is just an example. Other split operators might duplicate the input stream, or partition it round-robin, or partition it by hash. The partitioning function can be an arbitrary program. Gamma, Volcano, and Tandem use this approach <ref> [GRAE90] </ref>. It has several advantages including the automatic parallelism of any new operator added to the system, plus support for a many kinds of parallelism. The split and merge operators have flow control and buffering built into them. This prevents one operator from getting too far ahead in the computation. <p> This approach simplified the implementation of the upper levels of the Bubba software. 3.6. Other Systems Other parallel database system prototypes include XPRS [STON88], Volcano <ref> [GRAE90] </ref>, Arbre [LORI89], and the PERSIST project under development at IBM Research Labs in Hawthorne and Almaden. While both Volcano and XPRS are implemented on shared-memory multiprocessors, XPRS is unique in its exploitation of the availability of massive shared-memory in its design.
Reference: [GRAY91] <institution> The Performance Handbook for Database and Transaction Processing Systems, </institution> <note> J. </note> <editor> Gray editor. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address> <year> 1991. </year>
Reference-contexts: Its price/performance on these benchmarks is three times cheaper than the comparable mainframe numbers. Oracle on an nCUBE has the highest reported TPC-B numbers, and has very competitive price performance <ref> [GRAY91, GIBB91] </ref>. These benchmarks demonstrate linear scaleup on transaction processing benchmarks. Gamma, Tandem, and Teradata have demonstrated linear speedup and scaleup on complex relational database benchmarks. They scale well beyond the size of the largest mainframes.
Reference: [HIRA90] <author> Hirano, M, S. et al, </author> <title> Architecture of SDC, the Super Database Computer, </title> <booktitle> Proceedings of JSPP 90. </booktitle> <year> 1990. </year>
Reference-contexts: Near-linear speedup and scaleup for relational queries has been measured on this architecture [SCHN89, DEWI90, SCHN90]. 3.4. The Super Database Computer The Super Database Computer (SDC) project at the University of Tokyo presents an interesting contrast to other database systems <ref> [KITS90, HIRA90] </ref>. SDC takes a combined hardware and software approach to the performance problem. The basic unit, called a processing 19 module (PM), consists of one or more processors on a shared memory.
Reference: [HUA91] <author> Hua, K.A. and C. Lee, </author> <title> "Handling Data Skew in Multiprocessor Database Computers Using Partition Tuning," </title> <booktitle> Proceedings of the Seventeenth International Conference on Very Large Data Bases", Barcelon a, </booktitle> <address> Spain, </address> <month> September, </month> <year> 1991. </year>
Reference: [KITS83] <author> Kitsuregawa, M., Tanaka, H., and T. Moto-oka, </author> <title> "Application of Hash to Data Base Machine and Its Architecture", </title> <journal> New Generation Computing, </journal> <volume> Vol. 1, No. 1, </volume> <year> 1983. </year>
Reference-contexts: In these cases no algorithm is known to speedup or scaleup. The hash-join example shows that new parallel algorithms can improve the performance of relational operators. This is a fruitful research area <ref> [BORA90, DEWI86, KITS83, KITS90, SCHN89, SCHN90, WOLF90, ZELL90] </ref>. Even though parallelism can be obtained from conventional sequential relational algorithms by using split and merge operators, we expect that many new algorithms will be discovered in the future. 17 3. The State of the Art 3.1.
Reference: [KITS89] <author> Kitsuregawa, M., W. Yang, S. Fushimi, </author> <title> "Evaluation of 18-Stage Pipeline Hardware Sorter", </title> <booktitle> Proceedings of the 3rd International Conference on Data Engineering, </booktitle> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: The basic unit, called a processing 19 module (PM), consists of one or more processors on a shared memory. These processors are augmented by a special purpose sorting engine that sorts at high speed (3MB/s at present), and by a disk subsystem <ref> [KITS89] </ref>. Clusters of processing modules are connected via an omega network that provides both non-blocking NxN interconnect and some dynamic routing minimize skewed data distribution during hash joins. The SDC is designed to scale to thousands of PMs, and so considerable attention is paid to the problem of data skew.
Reference: [KITS90] <author> Kitsuregawa, M., and Y. Ogawa, </author> <title> "A New Parallel Hash Join Method with Robustness for Data Skew in Super Database Computer (SDC)", </title> <booktitle> Proceedings of the Sixteenth International Conference on Very Large Data Bases", </booktitle> <address> Melbourne, Australia, </address> <month> August, </month> <year> 1990. </year>
Reference-contexts: In these cases no algorithm is known to speedup or scaleup. The hash-join example shows that new parallel algorithms can improve the performance of relational operators. This is a fruitful research area <ref> [BORA90, DEWI86, KITS83, KITS90, SCHN89, SCHN90, WOLF90, ZELL90] </ref>. Even though parallelism can be obtained from conventional sequential relational algorithms by using split and merge operators, we expect that many new algorithms will be discovered in the future. 17 3. The State of the Art 3.1. <p> Near-linear speedup and scaleup for relational queries has been measured on this architecture [SCHN89, DEWI90, SCHN90]. 3.4. The Super Database Computer The Super Database Computer (SDC) project at the University of Tokyo presents an interesting contrast to other database systems <ref> [KITS90, HIRA90] </ref>. SDC takes a combined hardware and software approach to the performance problem. The basic unit, called a processing 19 module (PM), consists of one or more processors on a shared memory. <p> Another optimization problem relates to highly skewed value distributions. Data skew can lead to high variance in the size of intermediate relations, leading to both poor query plan cost estimates and sub-linear speedup. Solutions to this problem are an area of active research <ref> [KITS90, WOLF90, HUA91,WALT91] </ref>. 23 4.3. Application Program Parallelism The parallel database systems offer parallelism within the database system. Missing are tools to structure application programs to take advantage of parallelism inherent in these parallel systems.
Reference: [LORI89] <author> Lorie, R., J. Daudenarde, G. Hallmark, J. Stamos, and H. Young, </author> <title> "Adding Intra-Transaction Parallelism to an Existing DBMS: Early Experience", </title> <journal> IEEE Data Engineering Newsletter, </journal> <volume> Vol. 12, No. 1, </volume> <month> March </month> <year> 1989. </year>
Reference-contexts: This approach simplified the implementation of the upper levels of the Bubba software. 3.6. Other Systems Other parallel database system prototypes include XPRS [STON88], Volcano [GRAE90], Arbre <ref> [LORI89] </ref>, and the PERSIST project under development at IBM Research Labs in Hawthorne and Almaden. While both Volcano and XPRS are implemented on shared-memory multiprocessors, XPRS is unique in its exploitation of the availability of massive shared-memory in its design.
Reference: [PATT88] <author> Patterson, D. A., G. Gibson, and R. H. Katz, </author> <title> "A Case for Redundant Arrays of Inexpensive Disks (RAID)," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Chicago, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Data partitioning allows parallel database systems to exploit the I/O bandwidth of multiple disks by reading and writing them in parallel. This approach provides I/O bandwidth superior to RAIDstyle systems without needing any specialized hardware <ref> [SALE84, PATT88] </ref>. The simplest partitioning strategy distributes tuples among the fragments in a round-robin fashion. This is the partitioned version of the classic entry-sequence file. Round robin partitioning is excellent if all applications want to access the relation by sequentially scanning all of it on each query.
Reference: [RIES78] <author> Ries, D. and R. Epstein, </author> <title> "Evaluation of Distribution Criteria for Distributed Database Systems," </title> <type> UCB/ERL Technical Report M78/22, </type> <institution> UC Berkeley, </institution> <month> May, </month> <year> 1978. </year> <month> 26 </month>
Reference-contexts: Distributed databases use data partitioning when they place relation fragments at different network sites <ref> [RIES78] </ref>. Data partitioning allows parallel database systems to exploit the I/O bandwidth of multiple disks by reading and writing them in parallel. This approach provides I/O bandwidth superior to RAIDstyle systems without needing any specialized hardware [SALE84, PATT88].
Reference: [SALE84] <author> Salem, K. and H. Garcia-Molina, </author> <title> Disk Striping, </title> <institution> Department of Computer Science Princeton University Technical Report EEDS-TR-332-84, Princeton N.J., </institution> <month> Dec. </month> <year> 1984 </year>
Reference-contexts: Data partitioning allows parallel database systems to exploit the I/O bandwidth of multiple disks by reading and writing them in parallel. This approach provides I/O bandwidth superior to RAIDstyle systems without needing any specialized hardware <ref> [SALE84, PATT88] </ref>. The simplest partitioning strategy distributes tuples among the fragments in a round-robin fashion. This is the partitioned version of the classic entry-sequence file. Round robin partitioning is excellent if all applications want to access the relation by sequentially scanning all of it on each query.
Reference: [SCHN89] <author> Schneider, D. and D. DeWitt, </author> <title> "A Performance Evaluation of Four Parallel Join Algorithms in a Shared Nothing Multiprocessor Environment", </title> <booktitle> Proceedings of the 1989 SIGMOD Conference, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: In these cases no algorithm is known to speedup or scaleup. The hash-join example shows that new parallel algorithms can improve the performance of relational operators. This is a fruitful research area <ref> [BORA90, DEWI86, KITS83, KITS90, SCHN89, SCHN90, WOLF90, ZELL90] </ref>. Even though parallelism can be obtained from conventional sequential relational algorithms by using split and merge operators, we expect that many new algorithms will be discovered in the future. 17 3. The State of the Art 3.1. <p> The indices are implemented as B-trees or hash-tables. Gamma uses split and merge operators to execute relational algebra operators using both parallelism and pipelining [DEWI90]. Sort-merge and three different hash join methods are supported [DEWI84]. Near-linear speedup and scaleup for relational queries has been measured on this architecture <ref> [SCHN89, DEWI90, SCHN90] </ref>. 3.4. The Super Database Computer The Super Database Computer (SDC) project at the University of Tokyo presents an interesting contrast to other database systems [KITS90, HIRA90]. SDC takes a combined hardware and software approach to the performance problem.
Reference: [SCHN90] <author> Schneider, D. and D. DeWitt, </author> <title> "Tradeoffs in Processing Complex Join Queries via Hashing in Multiprocessor Database Machines," </title> <booktitle> Proceedings of the Sixteenth International Conference on Very Large Data Bases", </booktitle> <address> Melbourne, Australia, </address> <month> August, </month> <year> 1990. </year>
Reference-contexts: In these cases no algorithm is known to speedup or scaleup. The hash-join example shows that new parallel algorithms can improve the performance of relational operators. This is a fruitful research area <ref> [BORA90, DEWI86, KITS83, KITS90, SCHN89, SCHN90, WOLF90, ZELL90] </ref>. Even though parallelism can be obtained from conventional sequential relational algorithms by using split and merge operators, we expect that many new algorithms will be discovered in the future. 17 3. The State of the Art 3.1. <p> The indices are implemented as B-trees or hash-tables. Gamma uses split and merge operators to execute relational algebra operators using both parallelism and pipelining [DEWI90]. Sort-merge and three different hash join methods are supported [DEWI84]. Near-linear speedup and scaleup for relational queries has been measured on this architecture <ref> [SCHN89, DEWI90, SCHN90] </ref>. 3.4. The Super Database Computer The Super Database Computer (SDC) project at the University of Tokyo presents an interesting contrast to other database systems [KITS90, HIRA90]. SDC takes a combined hardware and software approach to the performance problem.
Reference: [SELI79] <author> Selinger,P. G., et. al., </author> <title> "Access Path Selection in a Relational Database Management System," </title> <booktitle> Proceedings of the 1979 SIGMOD Conference, </booktitle> <address> Boston, MA., </address> <month> May </month> <year> 1979. </year>
Reference-contexts: There have been several ad-hoc attempts at solving this problem, but considerably more work is needed. 4.2. Parallel Query Optimization Current database query optimizers do not consider all possible plans when optimizing a relational query. While cost models for relational queries running on a single processor are now well-understood <ref> [SELI79] </ref>, they still depend on cost estimators that are a guess at best. Some dynamically select from among several plans at run time depending on, for example, the amount of physical memory actually available and the cardinalities of the intermediate results [GRAE89].
Reference: [STON79] <author> Stonebraker, M., "Muffin: </author> <title> A Distributed Database Machine," </title> <type> ERL Technical Report UCB/ERL M79/28, </type> <institution> University of California at Berkeley, </institution> <month> May </month> <year> 1979. </year>
Reference: [STON86] <author> Stonebraker, M., </author> <title> "The Case for Shared Nothing," </title> <journal> Database Engineering, </journal> <volume> Vol. 9, No. 1, </volume> <year> 1986. </year>
Reference: [STON88] <author> Stonebraker, M., R. Katz, D. Patterson, and J. Ousterhout, </author> <booktitle> "The Design of XPRS", Proceedings of the Fourteenth International Conference on Very Large Data Bases, </booktitle> <address> Los Angeles, CA, </address> <month> August, </month> <year> 1988. </year>
Reference-contexts: This approach simplified the implementation of the upper levels of the Bubba software. 3.6. Other Systems Other parallel database system prototypes include XPRS <ref> [STON88] </ref>, Volcano [GRAE90], Arbre [LORI89], and the PERSIST project under development at IBM Research Labs in Hawthorne and Almaden. While both Volcano and XPRS are implemented on shared-memory multiprocessors, XPRS is unique in its exploitation of the availability of massive shared-memory in its design.
Reference: [TAND87] <author> Tandem Database Group, </author> <title> "NonStop SQL, A Distributed, High-Performance, High-Reliability Implementation of SQL," </title> <booktitle> Workshop on High Performance Transaction Systems, Asilomar, </booktitle> <address> CA, </address> <month> September </month> <year> 1987. </year>
Reference-contexts: Parallelization of operators in a query plan is achieved by inserting split and merge operators between operator nodes in the query tree. Scans, aggregates, joins, updates, and deletes are executed in parallel. In addition several utilities use parallelism (e.g., load, reorganize, ...) <ref> [TAND87, ZELL90] </ref>. Tandem systems are primary designed for online transaction processing (OLTP) running many simple transactions against a large shared database. Beyond the parallelism inherent in running many independent transactions in parallel, the main parallelism feature for OLTP is parallel index update. <p> By doing the index maintenance in parallel, the maintenance time for multiple indices can be held almost constant if the indices are spread among many processors and disks. Overall, the Tandem systems demonstrate near-linear scaleup on transaction processing workloads, and near-linear speedup and scaleup on large relational queries <ref> [TAND87, ENGL89] </ref>. 3.3. Gamma The current version of Gamma runs on a 32 node Intel iPSC/2 Hypercube with a disk attached to each node. In addition to round-robin, range and hash partitioning, Gamma also provides hybrid-range partitioning that combines the best features of the hash and range partitioning strategies [GHAN90b].
Reference: [TAND88] <author> Tandem Performance Group, </author> <title> "A Benchmark of NonStop SQL on the Debit Credit Transaction," </title> <booktitle> Proceedings of the 1988 SIGMOD Conference, </booktitle> <address> Chicago, IL, </address> <month> June </month> <year> 1988. </year>
Reference: [TERA83] <author> Teradata: </author> <title> DBC/1012 Data Base Computer Concepts & Facilities, Teradata Corp. Document No. </title> <address> C02 0001-00, </address> <year> 1983. </year>
Reference-contexts: The AMPs are responsible for executing queries. Each AMP typically has several disks and a large memory cache. IFPs and AMPs are interconnected by a dual redundant, treeshaped interconnect called the Y-net <ref> [TERA83] </ref>. Each relation is hash partitioned over a subset of the AMPs. When a tuple is inserted into a relation, a hash function is applied to the primary key of the tuple to select an AMP for storage.
Reference: [TEVA87] <author> Tevanian, A., et. al, </author> <title> "A Unix Interface for Shared Memory and Memory Mapped Files Under Mach," </title> <institution> Dept. of Computer Science Technical Report, Carnegie Mellon University, </institution> <month> July, </month> <year> 1987. </year>
Reference-contexts: This is in contrast to the traditional approach of files and pages. Similar mechanisms are used in IBM's AS400 mapping of SQL databases into virtual memory, HP's mapping of the Image Database into the operating system 20 virtual address space, and Mach's mapped file <ref> [TEVA87] </ref> mechanism. This approach simplified the implementation of the upper levels of the Bubba software. 3.6. Other Systems Other parallel database system prototypes include XPRS [STON88], Volcano [GRAE90], Arbre [LORI89], and the PERSIST project under development at IBM Research Labs in Hawthorne and Almaden.
Reference: [THAK90] <author> Thakkar, S.S. and M. Sweiger, </author> <title> "Performance of an OLTP Application on Symmetry Multiprocessor System," </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <address> Seattle, WA., </address> <month> May, </month> <year> 1990. </year>
Reference: [WALT91] <author> Walton, C.B., Dale, </author> <title> A.G., and R.M. Jenevein, "A Taxonomy and Performance Model of Data Skew Effects in Parallel Joins," </title> <booktitle> Proceedings of the Seventeenth International Conference on Very Large Data Bases", Barcelon a, </booktitle> <address> Spain, </address> <month> September, </month> <year> 1991. </year>
Reference: [WOLF90] <author> Wolf, J.L., Dias, </author> <title> D.M., and P.S. Yu, "An Effective Algorithm for Parallelizing Sort-Merge Joins in the Presence of Data Skew," </title> <booktitle> 2nd International Symposium on Databases in Parallel and Distributed Systems, </booktitle> <address> Dublin, Ireland, </address> <month> July, </month> <year> 1990. </year>
Reference-contexts: In these cases no algorithm is known to speedup or scaleup. The hash-join example shows that new parallel algorithms can improve the performance of relational operators. This is a fruitful research area <ref> [BORA90, DEWI86, KITS83, KITS90, SCHN89, SCHN90, WOLF90, ZELL90] </ref>. Even though parallelism can be obtained from conventional sequential relational algorithms by using split and merge operators, we expect that many new algorithms will be discovered in the future. 17 3. The State of the Art 3.1. <p> Another optimization problem relates to highly skewed value distributions. Data skew can lead to high variance in the size of intermediate relations, leading to both poor query plan cost estimates and sub-linear speedup. Solutions to this problem are an area of active research <ref> [KITS90, WOLF90, HUA91,WALT91] </ref>. 23 4.3. Application Program Parallelism The parallel database systems offer parallelism within the database system. Missing are tools to structure application programs to take advantage of parallelism inherent in these parallel systems.
Reference: [ZELL90] <author> Zeller, H.J. and J. Gray, </author> <title> Adaptive Hash Joins for a Multiprogramming Environment, </title> <booktitle> Proceedings of the 1990 VLDB Conference, </booktitle> <address> Australia, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: In these cases no algorithm is known to speedup or scaleup. The hash-join example shows that new parallel algorithms can improve the performance of relational operators. This is a fruitful research area <ref> [BORA90, DEWI86, KITS83, KITS90, SCHN89, SCHN90, WOLF90, ZELL90] </ref>. Even though parallelism can be obtained from conventional sequential relational algorithms by using split and merge operators, we expect that many new algorithms will be discovered in the future. 17 3. The State of the Art 3.1. <p> Parallelization of operators in a query plan is achieved by inserting split and merge operators between operator nodes in the query tree. Scans, aggregates, joins, updates, and deletes are executed in parallel. In addition several utilities use parallelism (e.g., load, reorganize, ...) <ref> [TAND87, ZELL90] </ref>. Tandem systems are primary designed for online transaction processing (OLTP) running many simple transactions against a large shared database. Beyond the parallelism inherent in running many independent transactions in parallel, the main parallelism feature for OLTP is parallel index update.
References-found: 39

