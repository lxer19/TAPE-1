URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/towell.aaai90.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/towell.aaai90.ps.abstract.html
Root-URL: 
Title: Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks  
Author: Geoffrey G. Towell Jude W. Shavlik Michiel O. Noordewier 
Address: 1210 West Dayton Street Madison, Wisconsin 53706  
Affiliation: University of Wisconsin Madison  
Note: Appears in the Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI-90, pp. 861-866)  
Abstract: Standard algorithms for explanation-based learning require complete and correct knowledge bases. The KBANN system relaxes this constraint through the use of empirical learning methods to refine approximately correct knowledge. This knowledge is used to determine the structure of an artificial neural network and the weights on its links, thereby making the knowledge accessible for modification by neural learning. KBANN is evaluated by empirical tests in the domain of molecular biology. Networks created by KBANN are shown to be superior, in terms of their ability to correctly classify unseen examples, to randomly initialized neural networks, decision trees, "nearest neighbor" matching, and standard techniques reported in the biological literature. In addition, KBANN's networks improve the initial knowledge in biologically interesting ways. 
Abstract-found: 1
Intro-found: 1
Reference: <author> DeJong, G. and Mooney, R. </author> <year> 1986. </year> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 145-176. </pages>
Reference: <author> Flann, N. and Dietterich, T. </author> <year> 1989. </year> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 187-226. </pages>
Reference-contexts: ANNs have been essentially unused as a tool for improving approximately correct domain theories. However, much work has been done on the use of other empirical learning techniques to modify and correct domain theories. For instance, the IOE system <ref> (Flann & Dietterich 1989) </ref> uses conventional inductive learning to empirically analyze a collection of explanations, thereby refining the domain theory. Current Research Issues An extension to KBANN being pursued is automatic interpretation of networks after training.
Reference: <author> Hall, R. </author> <year> 1988. </year> <title> Learning by failing to explain: Using partial explanations to learn in incomplete or intractable domains. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 45-77. </pages>
Reference: <author> Harley, C. and Reynolds, R. </author> <year> 1987. </year> <title> Analysis of E. coli promoter sequences. </title> <journal> Nucleic Acids Research, </journal> <volume> 15 </volume> <pages> 2343-2361. </pages>
Reference: <author> Hawley, D. and McClure, W. </author> <year> 1983. </year> <title> Compilation and analysis of Escherichia coli promoter DNA sequences. </title> <journal> Nucleic Acids Research, </journal> <volume> 11 </volume> <pages> 2237-2255. </pages>
Reference-contexts: Unfortunately, such a consensus fails to recognize any true promoters, due to excessive stringency if exact matches are required at each position. Furthermore, KBANN's neural network assigned particular importance to bases in certain positions. These highlighted positions correspond exactly to the most conserved bases in <ref> (Hawley & McClure 1983) </ref>. Finally, the network learned that certain values for some base pairs indicate that a promoter is probably not present. For instance, a C in base pair -8 and an A in base pair -36 both strongly suggest that a promoter is not present.
Reference: <author> Honavar, V. and Uhr, L. </author> <year> 1988. </year> <title> A network of neuron-like units that learns to perceive by generation as well as reweighting of links. </title> <booktitle> In Proc. Connectionist Models Summer School, </booktitle> <pages> pages 472-484. </pages>
Reference-contexts: The first approach, similar in spirit to KBANN, makes most or all topological decisions prior to training (Rueckl et al. 1988; Katz 1989). The second approach modifies network structure as a part of the learning process. This approach includes recruitment learning <ref> (e.g., Honavar & Uhr 1988) </ref> in which hidden units are added to the network as during learning and methods for removing excess hidden units (e.g., Kruschke 1988). A second problem specific to neural networks is the integration of existing information into the network. <p> These added units would allow the network to learn relations not anticipated in the pre-existing knowledge. Currently we are considering adding hidden units as a fixed percentage of the existing hidden units at each layer in the ANN. Other methods for adding hidden units such as recruitment learning <ref> (e.g., Honavar & Uhr 1988) </ref> are also being investigated. Conclusions The KBANN approach has been shown to make it possible to use ANNs to refine pre-existing knowledge.
Reference: <author> Katz, B. </author> <year> 1989. </year> <title> EBL and SBL: A neural network synthesis. </title> <booktitle> In Proc. Eleventh Conference of the Cognitive Science Society Conference, </booktitle> <pages> pages 683-689. </pages>
Reference: <author> Kruschke, J. </author> <year> 1988. </year> <title> Creating local and distributed bottlenecks in hidden layers of back-propagation networks. </title> <booktitle> In Proc. 1988 Connectionist Models Summer School, </booktitle> <pages> pages 357-370. </pages>
Reference-contexts: This decision is important, because an ANN with too few units will be unable to learn a concept, and an ANN with too many hidden units may generalize poorly <ref> (Kruschke 1988) </ref>. More recently, full connectivity has been shown to hinder learning on some tasks (Rueckl et al. 1988). Moreover, different random settings of link weights can result in radically different learning rates and generalization (Shavlik et al. in press). <p> The second approach modifies network structure as a part of the learning process. This approach includes recruitment learning (e.g., Honavar & Uhr 1988) in which hidden units are added to the network as during learning and methods for removing excess hidden units <ref> (e.g., Kruschke 1988) </ref>. A second problem specific to neural networks is the integration of existing information into the network. Complex, hand-designed networks (e.g., Rueckl et al. 1988) can be viewed as an attempt to give networks some implicit knowledge of a problem domain.
Reference: <author> Lapedes, A.; Barnes, C.; Burkes, C.; Farber, R.; and Sirotkin, K. </author> <year> 1989. </year> <title> Application of neural networks and other machine learning algorithms to DNA sequence analysis. In Computers and DNA, </title> <booktitle> SFI Studies in the Science of Complexity VII. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: An initial concern of ours was the construction of negative training examples (i.e., sequences that contained no promoters). Most studies randomly permute sequences in an effort to derive examples that do not meet consensus criteria described below, but nonetheless retain the correct nucleotide frequencies <ref> (Lapedes et al. 1989 ) </ref>. DNA, however, is known to be highly non-random. Negative training examples were thus derived by selecting contiguous substrings from a 1.5 kilobase sequence provided by Prof. T. Record of the Univ. of Wisconsin's Chemistry Dept.
Reference: <author> Minton, S. </author> <year> 1988. </year> <title> Quantitative results concerning the utility of explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 363-391. </pages>
Reference-contexts: In so doing, KBANN makes it possible to apply neural learning techniques to the empirical, incremental improvement of knowledge bases. At present, KBANN is restricted to non-recursive, propositional (i.e., variable-free) domain theories. Under these restrictions, the ability of EBL to speedup a problem solver <ref> (Minton 1988) </ref> is not utilized. While this speedup is the primary strength of EBL, the secondary strengths of this form of learning are directly applicable. Specifically, the domain theory indicates the features which are believed to be important to an example's classification.
Reference: <author> Mitchell, T.; Keller, R.; and Kedar-Cabelli, S. </author> <year> 1986. </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 47-80. </pages>
Reference-contexts: Introduction Explanation-based learning (EBL) (Mitchell et al. 1986; DeJong & Mooney 1986) provides a way of incorporating pre-existing knowledge into a learning system. However, the basic algorithms suffer from the fact that the pre-existing knowledge cannot contain imperfections <ref> (Mitchell et al. 1986) </ref>. Conversely, empirical learning is a method for learning solely from training examples (e.g., Quinlan 1986). Empirical learning systems have problems such as misclassification due to spurious correlations in the training data.
Reference: <author> O'Neill, M. </author> <year> 1989a. </year> <title> Escherichia coli promoters: I. Consensus as it relates to spacing class, specificity, repeat substructure, and three dimensional organization. </title> <journal> Journal of Biological Chemistry, </journal> <volume> 264 </volume> <pages> 5522-5530. </pages>
Reference: <author> O'Neill, M. </author> <year> 1989b. </year> <title> Escherichia coli promoters: II. A spacing class-dependent promoter search protocol. </title> <journal> Journal of Biological Chemistry, </journal> <volume> 264 </volume> <pages> 5531-5534. </pages>
Reference: <author> Quinlan, J. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: However, the basic algorithms suffer from the fact that the pre-existing knowledge cannot contain imperfections (Mitchell et al. 1986). Conversely, empirical learning is a method for learning solely from training examples <ref> (e.g., Quinlan 1986) </ref>. Empirical learning systems have problems such as misclassification due to spurious correlations in the training data. <p> In the subsequent section, KBANN is applied to a real-world problem in the domain of molecular biology. KBANN is shown to produce results better than those reported in the biological literature. Additionally, KBANN's results are shown to be superior to randomly started ANNs, ID3 <ref> (Quinlan 1986) </ref> a symbolic empirical learning system, and "nearest neighbor" classification. Moreover, ANNs created by KBANN are shown to have improved upon the original domain theory in biologically interesting ways. The paper concludes with a discussion of research related to KBANN and the areas which our research is currently pursuing.
Reference: <author> Rueckl, J.; Cave, K.; and Kosslyn, S. </author> <year> 1988. </year> <title> Why are "what" and "where" processed by separate cortical visual systems? Journal of Cognitive Neuroscience, </title> <type> 1(2). </type>
Reference-contexts: This decision is important, because an ANN with too few units will be unable to learn a concept, and an ANN with too many hidden units may generalize poorly (Kruschke 1988). More recently, full connectivity has been shown to hinder learning on some tasks <ref> (Rueckl et al. 1988) </ref>. Moreover, different random settings of link weights can result in radically different learning rates and generalization (Shavlik et al. in press). Thus, determining the topology of an ANN requires deciding about: the pattern of connectivity, the number and distribution of hidden units, and the link weights. <p> A second problem specific to neural networks is the integration of existing information into the network. Complex, hand-designed networks <ref> (e.g., Rueckl et al. 1988) </ref> can be viewed as an attempt to give networks some implicit knowledge of a problem domain. However, little work other than KBANN, has been done on how to explicitly give ANNs background information.
Reference: <author> Rumelhart, D.; Hinton, G.; and Williams, J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumel-hart, D. and McClelland, J., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Third, the network must be perturbed by adding random numbers within * of zero to all link weights and biases to avoid symmetry breaking problems <ref> (Rumelhart et al. 1986) </ref>. 3 The KBANN algorithm is summarized in Table 2. Once the network is produced, it is refined by providing training examples which are processed using backpropagation (Rumelhart et al. 1986). Table 2: Overview of the KBANN Algorithm 1. Translate rules to set initial network structure. 2. <p> perturbed by adding random numbers within * of zero to all link weights and biases to avoid symmetry breaking problems <ref> (Rumelhart et al. 1986) </ref>. 3 The KBANN algorithm is summarized in Table 2. Once the network is produced, it is refined by providing training examples which are processed using backpropagation (Rumelhart et al. 1986). Table 2: Overview of the KBANN Algorithm 1. Translate rules to set initial network structure. 2. Add units not specified by translation. 3. Add links not specified by translation. 4. Perturb the network by adding near zero ran dom numbers to all link weights and biases. <p> One problem, specific to neural networks, addressed by KBANN is topology determination. In relatively early work on ANNs, topological decisions were restricted to the size of a single layer of hidden units in fully-connected networks <ref> (e.g., Rumelhart et al. 1986) </ref>. This decision is important, because an ANN with too few units will be unable to learn a concept, and an ANN with too many hidden units may generalize poorly (Kruschke 1988).
Reference: <author> Shavlik, J. and Towell, G. </author> <year> 1989. </year> <title> An approach to combining explanation-based and neural learning algorithms. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 233-255. </pages>
Reference-contexts: This experiment demonstrates, using an important real world problem, the promise of the KBANN approach. It produced a more accurate recognizer of promoters, demonstrating the value of incorporating preexisting knowledge about the task being learned. Related Work This paper extends and realistically tests the ideas first presented in <ref> (Shavlik & Towell 1989) </ref>. One problem, specific to neural networks, addressed by KBANN is topology determination. In relatively early work on ANNs, topological decisions were restricted to the size of a single layer of hidden units in fully-connected networks (e.g., Rumelhart et al. 1986).
Reference: <author> Shavlik, J.; Mooney, R.; and Towell, G. </author> <title> in press. Symbolic and neural net learning algorithms: An empirical comparison. Machine Learning. </title> <publisher> Forthcoming. </publisher>
Reference: <author> Youderian, P.; Bouvier, S.; and Susskind, M. </author> <year> 1982. </year> <title> Sequence determinants of promoter activity. </title> <journal> Cell, </journal> <volume> 10 </volume> <pages> 843-853. </pages>
Reference-contexts: For instance, a C in base pair -8 and an A in base pair -36 both strongly suggest that a promoter is not present. This ability may be useful to address the problem that promoters lose their biological activity when specific single nucleotides are mutated <ref> (Youderian et al. 1982) </ref>. O'Neill notes that this is an unresolved problem for consensus methods, since the alteration of a single base does not degrade the quality of the match very much.
References-found: 19

