URL: ftp://gaia.cs.umass.edu/pub/Yate95:Networking.ps.Z
Refering-URL: http://www.cs.umass.edu/~nahum/home.html
Root-URL: 
Email: -yates,nahum,kurose,towsley-@cs.umass.edu  
Title: Networking Support for Large Scale Multiprocessor Servers  
Author: David J. Yates, Erich M. Nahum, James F. Kurose, and Don Towsley 
Address: Amherst, MA 01003-4610, USA  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Over the next several years the performance demands on globally available information servers are expected to increase dramatically. These servers must be capable of sending and receiving data over hundreds or even thousands of simultaneous connections. In this paper, we show that connection-level parallel protocols (where different connections are processed in parallel) running on a shared-memory multiprocessor can deliver high network bandwidth across a large number of connections. We experimentally evaluate connection-level parallel implementations of both TCP/IP and UDP/IP protocol stacks. We focus on three questions in our performance evaluation: how throughput scales with the number of processors, how throughput changes as the number of connections increases, and how fairly the aggregate bandwidth is distributed across connections. We show how several factors impact performance: the number of processors used, the number of threads in the system, the number of connections assigned to each thread, and the type of protocols in the stack (i.e., TCP versus UDP). Our results show that with careful implementation connection-level parallel protocol stacks scale well with the number of processors, and deliver high throughput which is, for the most part, sustained as the number of connections increases. Maximizing the number of threads in the system yields the best overall throughput. However, the best fairness behavior is achieved by matching the number of threads to the number of processors and scheduling connections assigned to threads in a round-robin manner. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. L. Bailey, B. Gopal, M. A. Pagels, L. L. Peterson, and P. Sarkar. PathFinder: </author> <title> a pattern-based packet classifier. </title> <booktitle> In First USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 115-123, </pages> <address> Monterey, CA, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Protocols are conceptually replicated, and multiplexing occurs at the lowest layer of the protocol stack. On the receive path, a packet is immediately demultiplexed to the appropriate connection. This demultiplexing is performed by a packet filter or classifier <ref> [1, 19, 20, 32] </ref>. Given a set of connections, threads, and processors, the assignment or mapping between them can be done in a number of different ways. Choosing a mapping defines the granularity of a connection-level parallel implementation. <p> For example, TCP connections must be uniquely instantiated. Thus, our CLP implementation requires a packet filter mechanism to demultiplex received packets to the appropriate virtual processor for a TCP (or UDP) connection. Packet filters are becoming more popular (and efficient <ref> [1, 32] </ref>) in contemporary operating systems since early demultiplexing yields other performance gains. For example, depositing received packets directly into application buffers avoids copying data [5, 18, 30].
Reference: [2] <author> J. M. Barton and N. Bitar. </author> <title> A scalable multi-discipline, multiple-processor scheduling framework for IRIX. </title> <booktitle> In IPPS '95 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pages 24-40, </pages> <address> Santa Barbara, CA, </address> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: We found that in all experiments wiring threads to processors decreased the aggregate throughput. In some configurations this decrease was as much as 50%. Furthermore, the version of IRIX we used schedules threads for cache affinity <ref> [2] </ref>, which has been shown to benefit connection-level parallelism [26, 27]. data on 12 to 3072 TCP connections, where the checksum is computed. Figure 7 shows the corresponding results for our UDP protocol stack, without checksumming.
Reference: [3] <author> M. Bj orkman and P. Gunningberg. </author> <title> Locking effects in multiprocessor implementations of protocols. </title> <booktitle> In SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 74-83, </pages> <address> San Francisco, CA, </address> <month> Sept. </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: Our implementation runs in user space on a shared-memory Silicon Graphics (SGI) Challenge multiprocessor [7]. Several other approaches to parallelizing network protocols have also been proposed and are briefly described here; more detailed surveys can be found in <ref> [3, 10] </ref>. Functional parallelism decomposes functions within a protocol stack and assigns them to processing elements. Examples include [15, 16, 24]. In layered parallelism, protocols are assigned to specific processors, and messages are passed between layers through interprocess communication. <p> Parallelism gains can be achieved mainly through pipelining effects, as shown in [9]. Packet-level parallelism associates processing with each individual packet, achieving speedup both with multiple connections and within a single connection. Examples include <ref> [3, 10, 13, 23] </ref>. The remainder of the paper is structured as follows: Section 2 provides background on connection-level parallelism. Section 3 discusses our implementation of connection-level parallelism, and describes our experiments. In section 4 we present our results. <p> One innovative feature of our implementation is that there are no locks on the fast path through the protocol stack, on either the send or receive side (once packets have been demultiplexed to the appropriate virtual processor). This is in contrast to earlier implementations <ref> [3, 23, 25, 29] </ref> in which data structures are locked on the fast path. We accomplish this by replicating data structures on a per virtual processor basis where possible. <p> The drivers emulate an FDDI interface capable of operating at memory speeds (i.e., much faster than 100 Mbps), and support the FDDI maximum transmission unit (MTU) of slightly over 4KB. This approach is similar to the those used by other researchers <ref> [3, 10, 17, 23, 29] </ref>. In our experiments, we assume that a connection always has data to send (i.e., each connection is an infinite data source). Therefore, the drivers act as peer senders or receivers, producing or consuming packets as fast as possible.
Reference: [4] <author> K. C. Claffy, H.-W. Braun, and G. C. Polyzos. </author> <title> Internet traffic flow profiling. </title> <type> Technical Report UCSD Report CS93-328, </type> <institution> SDSC Report GA-A21526, University of Californa at San Diego, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: We also implemented a connection-level parallel version of UDP. Even though UDP is a connectionless protocol, we consider long term associations between a sender and receiver to be a connection, or flow <ref> [4] </ref>. A UDP association is used to map connections to virtual processors, and provides a handle for demultiplexing, in the same fashion as TCP.
Reference: [5] <author> P. Druschel, L. Peterson, and B. Davie. </author> <title> Experiences with a high-speed network adaptor: A software perspective. </title> <booktitle> In SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 2-13, </pages> <address> London, England, </address> <month> Aug. </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: Packet filters are becoming more popular (and efficient [1, 32]) in contemporary operating systems since early demultiplexing yields other performance gains. For example, depositing received packets directly into application buffers avoids copying data <ref> [5, 18, 30] </ref>. Our use of a packet filter, to demultiplex to the appropriate virtual processor, simply leverages this existing mechanism for an additional purpose. 3.2 Connection-Level Parallel Protocols Our Internet protocols are all based on the uniprocessor implementations distributed with the December 1993 version of the x-kernel.
Reference: [6] <author> P. Druschel and L. L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 189-202, </pages> <address> Asheville, NC, </address> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: For send-side experiments, we always cache communication buffers in LIFO lists. These lists are maintained per virtual processor, and therefore require no locks. This technique has been shown to improve throughput in protocol stacks running on a uniprocessor <ref> [6] </ref>. This means that our send-side experiments measure either a cache-to-cache or a memory-to-cache copy depending on the fate of the application data buffer in the caches.
Reference: [7] <author> M. Galles and E. Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <type> Technical report, </type> <institution> Silicon Graphics Inc., Mt. View, </institution> <address> CA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Our implementation runs in user space on a shared-memory Silicon Graphics (SGI) Challenge multiprocessor <ref> [7] </ref>. Several other approaches to parallelizing network protocols have also been proposed and are briefly described here; more detailed surveys can be found in [3, 10]. Functional parallelism decomposes functions within a protocol stack and assigns them to processing elements. Examples include [15, 16, 24].
Reference: [8] <author> A. Garg. </author> <title> Parallel STREAMS: a multi-processor implementation. </title> <booktitle> In Proceedings of the Winter 1990 USENIX Conference, </booktitle> <address> Washington, D.C., </address> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: Connection-level parallelism associates the protocol processing required by connections with individual processes or threads. On a shared-memory multiprocessor, performance gains can be realized over multiple connections by executing these threads concurrently on different processors. Previous work on connection-level parallelism can be found in <ref> [8, 25, 28, 29] </ref>. In particular, Schmidt and Suda [29] have shown good scalability of the receive-side data path in connection-level parallelism, using a thread for each connection, on a 20-processor Sun SPARCCenter 2000. In this paper, we experimentally evaluate connection-level parallelism in a number of previously unexamined dimensions. <p> Given a set of connections, threads, and processors, the assignment or mapping between them can be done in a number of different ways. Choosing a mapping defines the granularity of a connection-level parallel implementation. Previous work on connection-level parallelism <ref> [8, 25, 28, 29] </ref> has focused on relatively static assignments of connections to processes. One novel aspect of our imple mentation is that it allows us to vary the mapping between processors, connections, and threads. <p> These threads are free to execute on either of the two processors, and perform protocol processing on behalf of any of the connections. In practice, however, each connection is typically associated with an individual thread <ref> [8, 28] </ref>. While TPC allows easy load balancing, it may not scale well with large numbers of connections, since each thread must be allocated resources (such as a thread control block and a stack). Our implementation facilitates varying the mapping of connections to threads and processors by using virtual processors.
Reference: [9] <author> D. Giarrizzo, M. Kaiserswerth, T. Wicki, and R. C. Williamson. </author> <title> High-speed parallel protocol implementation. </title> <booktitle> First IFIP WG6.1/WG6.4 International Workshop on Protocols for High-Speed Networks, </booktitle> <pages> pages 165-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Functional parallelism decomposes functions within a protocol stack and assigns them to processing elements. Examples include [15, 16, 24]. In layered parallelism, protocols are assigned to specific processors, and messages are passed between layers through interprocess communication. Parallelism gains can be achieved mainly through pipelining effects, as shown in <ref> [9] </ref>. Packet-level parallelism associates processing with each individual packet, achieving speedup both with multiple connections and within a single connection. Examples include [3, 10, 13, 23]. The remainder of the paper is structured as follows: Section 2 provides background on connection-level parallelism.
Reference: [10] <author> M. W. Goldberg, G. W. Neufeld, and M. R. Ito. </author> <title> A parallel approach to OSI connection-oriented protocols. </title> <booktitle> Third IFIP WG6.1/WG6.4 International Workshop on Protocols for High-Speed Networks, </booktitle> <pages> pages 219-232, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Our implementation runs in user space on a shared-memory Silicon Graphics (SGI) Challenge multiprocessor [7]. Several other approaches to parallelizing network protocols have also been proposed and are briefly described here; more detailed surveys can be found in <ref> [3, 10] </ref>. Functional parallelism decomposes functions within a protocol stack and assigns them to processing elements. Examples include [15, 16, 24]. In layered parallelism, protocols are assigned to specific processors, and messages are passed between layers through interprocess communication. <p> Parallelism gains can be achieved mainly through pipelining effects, as shown in [9]. Packet-level parallelism associates processing with each individual packet, achieving speedup both with multiple connections and within a single connection. Examples include <ref> [3, 10, 13, 23] </ref>. The remainder of the paper is structured as follows: Section 2 provides background on connection-level parallelism. Section 3 discusses our implementation of connection-level parallelism, and describes our experiments. In section 4 we present our results. <p> The drivers emulate an FDDI interface capable of operating at memory speeds (i.e., much faster than 100 Mbps), and support the FDDI maximum transmission unit (MTU) of slightly over 4KB. This approach is similar to the those used by other researchers <ref> [3, 10, 17, 23, 29] </ref>. In our experiments, we assume that a connection always has data to send (i.e., each connection is an infinite data source). Therefore, the drivers act as peer senders or receivers, producing or consuming packets as fast as possible.
Reference: [11] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: allocated to threads by the scheduler, and by the per-packet latency seen by each thread. * Finally, we demonstrate that processor per connection pro vides better fairness than thread per connection. 1 Our evaluation of connection-level parallelism is performed for both TCP/IP and UDP/IP protocol stacks, implemented using the x-kernel <ref> [11] </ref>. Our implementation runs in user space on a shared-memory Silicon Graphics (SGI) Challenge multiprocessor [7]. Several other approaches to parallelizing network protocols have also been proposed and are briefly described here; more detailed surveys can be found in [3, 10]. <p> To execute protocol code on a virtual processor, a thread must acquire the appropriate semaphore. Once running on a virtual processor, a thread runs to completion, as in the original x-kernel <ref> [11] </ref>. One innovative feature of our implementation is that there are no locks on the fast path through the protocol stack, on either the send or receive side (once packets have been demultiplexed to the appropriate virtual processor).
Reference: [12] <author> V. Jacobson, R. Braden, and D. </author> <title> Borman. TCP extensions for high performance. </title> <booktitle> In Network Information Center RFC 1323, </booktitle> <pages> pages 1-37, </pages> <address> Menlo Park, CA, </address> <month> May </month> <year> 1992. </year> <note> SRI International. </note>
Reference-contexts: In addition to modifying them for connection-level parallelism, we made two other important changes to these protocols. First, we updated the TCP code to be current with the Berkeley 4.4Lite implementation, excluding the RFC 1323 extensions <ref> [12] </ref>. We also replaced the Internet checksum code with the fastest available portable algorithm that we were aware of, which was from UCSD [14]. We also implemented a connection-level parallel version of UDP.
Reference: [13] <author> N. Jain, M. Schwartz, and T. R. Bashkow. </author> <title> Transport protocol processing at Gbps rates. </title> <booktitle> In SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 188-199, </pages> <address> Philadelphia, PA, </address> <month> Sept. </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: Parallelism gains can be achieved mainly through pipelining effects, as shown in [9]. Packet-level parallelism associates processing with each individual packet, achieving speedup both with multiple connections and within a single connection. Examples include <ref> [3, 10, 13, 23] </ref>. The remainder of the paper is structured as follows: Section 2 provides background on connection-level parallelism. Section 3 discusses our implementation of connection-level parallelism, and describes our experiments. In section 4 we present our results.
Reference: [14] <author> J. Kay and J. Pasquale. </author> <title> Measurement, analysis, and improvement of UDP/IP throughput for the DECStation 5000. </title> <booktitle> In Proceedings of the Winter 1993 USENIX Conference, </booktitle> <pages> pages 249-258, </pages> <address> San Diego, CA, </address> <year> 1993. </year>
Reference-contexts: First, we updated the TCP code to be current with the Berkeley 4.4Lite implementation, excluding the RFC 1323 extensions [12]. We also replaced the Internet checksum code with the fastest available portable algorithm that we were aware of, which was from UCSD <ref> [14] </ref>. We also implemented a connection-level parallel version of UDP. Even though UDP is a connectionless protocol, we consider long term associations between a sender and receiver to be a connection, or flow [4].
Reference: [15] <author> O. G. Koufopavlou and M. Zitterbart. </author> <title> Parallel TCP for high performance communication subsystems. </title> <booktitle> In Proceedings of the Global Telecommunications Conference (GLOBECOM), </booktitle> <pages> pages 1395-1399, </pages> <year> 1992. </year>
Reference-contexts: Several other approaches to parallelizing network protocols have also been proposed and are briefly described here; more detailed surveys can be found in [3, 10]. Functional parallelism decomposes functions within a protocol stack and assigns them to processing elements. Examples include <ref> [15, 16, 24] </ref>. In layered parallelism, protocols are assigned to specific processors, and messages are passed between layers through interprocess communication. Parallelism gains can be achieved mainly through pipelining effects, as shown in [9].
Reference: [16] <author> T. F. La Porta and M. Schwartz. </author> <title> A high-speed protocol parallel implementation: Design and analysis. </title> <booktitle> Fourth IFIP TC6.1/WG6.4 International Conference on High Performance Networking, </booktitle> <pages> pages 135-150, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Several other approaches to parallelizing network protocols have also been proposed and are briefly described here; more detailed surveys can be found in [3, 10]. Functional parallelism decomposes functions within a protocol stack and assigns them to processing elements. Examples include <ref> [15, 16, 24] </ref>. In layered parallelism, protocols are assigned to specific processors, and messages are passed between layers through interprocess communication. Parallelism gains can be achieved mainly through pipelining effects, as shown in [9].
Reference: [17] <author> B. Lindgren, B. Krupczak, M. Ammar, and K. Schwan. </author> <title> An architecture and toolkit for parallel and configurable protocols. </title> <booktitle> In Proceedings of the International Conference on Network Protocols, </booktitle> <pages> pages 234-242, </pages> <address> San Francisco, CA, </address> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: The drivers emulate an FDDI interface capable of operating at memory speeds (i.e., much faster than 100 Mbps), and support the FDDI maximum transmission unit (MTU) of slightly over 4KB. This approach is similar to the those used by other researchers <ref> [3, 10, 17, 23, 29] </ref>. In our experiments, we assume that a connection always has data to send (i.e., each connection is an infinite data source). Therefore, the drivers act as peer senders or receivers, producing or consuming packets as fast as possible.
Reference: [18] <author> C. Maeda and B. N. Bershad. </author> <title> Protocol service decomposition for high-performance networking. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 244-255, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Packet filters are becoming more popular (and efficient [1, 32]) in contemporary operating systems since early demultiplexing yields other performance gains. For example, depositing received packets directly into application buffers avoids copying data <ref> [5, 18, 30] </ref>. Our use of a packet filter, to demultiplex to the appropriate virtual processor, simply leverages this existing mechanism for an additional purpose. 3.2 Connection-Level Parallel Protocols Our Internet protocols are all based on the uniprocessor implementations distributed with the December 1993 version of the x-kernel.
Reference: [19] <author> S. McCanne and V. Jacobson. </author> <title> The BSD packet filter: A new architecture for user-level packet capture. </title> <booktitle> In Proceedings of the Winter 1993 USENIX Conference, </booktitle> <pages> pages 259-269, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Protocols are conceptually replicated, and multiplexing occurs at the lowest layer of the protocol stack. On the receive path, a packet is immediately demultiplexed to the appropriate connection. This demultiplexing is performed by a packet filter or classifier <ref> [1, 19, 20, 32] </ref>. Given a set of connections, threads, and processors, the assignment or mapping between them can be done in a number of different ways. Choosing a mapping defines the granularity of a connection-level parallel implementation.
Reference: [20] <author> J. Mogul, R. Rashid, and M. Accetta. </author> <title> The packet filter: An efficient mechanism for user-level network code. </title> <booktitle> In Proceedings 11th Symposium on Operating System Principles, </booktitle> <pages> pages 39-51, </pages> <address> Austin, TX, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: Protocols are conceptually replicated, and multiplexing occurs at the lowest layer of the protocol stack. On the receive path, a packet is immediately demultiplexed to the appropriate connection. This demultiplexing is performed by a packet filter or classifier <ref> [1, 19, 20, 32] </ref>. Given a set of connections, threads, and processors, the assignment or mapping between them can be done in a number of different ways. Choosing a mapping defines the granularity of a connection-level parallel implementation.
Reference: [21] <author> A. B. Montz, D. Mosberger, S. W. O'Malley, L. L. Peterson, T. A. Proebsting, and J. H. Hartman. </author> <title> Scout: A communications-oriented operating system. </title> <type> Technical Report TR 94-20, </type> <institution> University of Arizona, Tuscon, AZ, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: We suspect that this is because the code path for sending packets over UDP connections is shorter than the code path for TCP connections, making the UDP results more sensitive to conflicts in the caches on the multiprocessor. This phenomenon has been observed by others on a uniprocessor <ref> [21] </ref>. So far we have presented results only for experiments where the multiprocessor is the sender. However, for applications such as high-performance file service (which typically run on top of UDP), it is also interesting to examine the case where the multiprocessor is the receiver.
Reference: [22] <author> E. Nahum, D. J. Yates, S. O'Malley, H. Orman, and R. Schroeppel. </author> <title> Parallelized network security protocols. </title> <booktitle> In Proceedings of the Internet Society Symposium on Network and Distributed System Security, </booktitle> <address> San Diego, CA, </address> <month> Feb. </month> <year> 1996. </year>
Reference: [23] <author> E. M. Nahum, D. J. Yates, J. F. Kurose, and D. Towsley. </author> <title> Performance issues in parallelized network protocols. </title> <booktitle> In First USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 125-137, </pages> <address> Monterey, CA, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Parallelism gains can be achieved mainly through pipelining effects, as shown in [9]. Packet-level parallelism associates processing with each individual packet, achieving speedup both with multiple connections and within a single connection. Examples include <ref> [3, 10, 13, 23] </ref>. The remainder of the paper is structured as follows: Section 2 provides background on connection-level parallelism. Section 3 discusses our implementation of connection-level parallelism, and describes our experiments. In section 4 we present our results. <p> One innovative feature of our implementation is that there are no locks on the fast path through the protocol stack, on either the send or receive side (once packets have been demultiplexed to the appropriate virtual processor). This is in contrast to earlier implementations <ref> [3, 23, 25, 29] </ref> in which data structures are locked on the fast path. We accomplish this by replicating data structures on a per virtual processor basis where possible. <p> The drivers emulate an FDDI interface capable of operating at memory speeds (i.e., much faster than 100 Mbps), and support the FDDI maximum transmission unit (MTU) of slightly over 4KB. This approach is similar to the those used by other researchers <ref> [3, 10, 17, 23, 29] </ref>. In our experiments, we assume that a connection always has data to send (i.e., each connection is an infinite data source). Therefore, the drivers act as peer senders or receivers, producing or consuming packets as fast as possible.
Reference: [24] <author> A. N. Netravali, W. D. Roome, and K. Sabnani. </author> <title> Design and implementation of a high-speed transport protocol. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(11) </volume> <pages> 2010-2024, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Several other approaches to parallelizing network protocols have also been proposed and are briefly described here; more detailed surveys can be found in [3, 10]. Functional parallelism decomposes functions within a protocol stack and assigns them to processing elements. Examples include <ref> [15, 16, 24] </ref>. In layered parallelism, protocols are assigned to specific processors, and messages are passed between layers through interprocess communication. Parallelism gains can be achieved mainly through pipelining effects, as shown in [9].
Reference: [25] <author> D. Presotto. </author> <title> Multiprocessor streams for Plan 9. </title> <booktitle> In Proceedings of the United Kingdom UNIX Users Group, </booktitle> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Connection-level parallelism associates the protocol processing required by connections with individual processes or threads. On a shared-memory multiprocessor, performance gains can be realized over multiple connections by executing these threads concurrently on different processors. Previous work on connection-level parallelism can be found in <ref> [8, 25, 28, 29] </ref>. In particular, Schmidt and Suda [29] have shown good scalability of the receive-side data path in connection-level parallelism, using a thread for each connection, on a 20-processor Sun SPARCCenter 2000. In this paper, we experimentally evaluate connection-level parallelism in a number of previously unexamined dimensions. <p> Given a set of connections, threads, and processors, the assignment or mapping between them can be done in a number of different ways. Choosing a mapping defines the granularity of a connection-level parallel implementation. Previous work on connection-level parallelism <ref> [8, 25, 28, 29] </ref> has focused on relatively static assignments of connections to processes. One novel aspect of our imple mentation is that it allows us to vary the mapping between processors, connections, and threads. <p> One innovative feature of our implementation is that there are no locks on the fast path through the protocol stack, on either the send or receive side (once packets have been demultiplexed to the appropriate virtual processor). This is in contrast to earlier implementations <ref> [3, 23, 25, 29] </ref> in which data structures are locked on the fast path. We accomplish this by replicating data structures on a per virtual processor basis where possible.
Reference: [26] <author> J. D. Salehi, J. F. Kurose, and D. Towsley. </author> <title> The performance impact of scheduling for cache affinity in parallel network processing. </title> <booktitle> In International Symposium on High Performance Distributed Computing (HPDC-4), </booktitle> <address> Pentagon City, VA, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: We found that in all experiments wiring threads to processors decreased the aggregate throughput. In some configurations this decrease was as much as 50%. Furthermore, the version of IRIX we used schedules threads for cache affinity [2], which has been shown to benefit connection-level parallelism <ref> [26, 27] </ref>. data on 12 to 3072 TCP connections, where the checksum is computed. Figure 7 shows the corresponding results for our UDP protocol stack, without checksumming. Where VPPC data are shown, the number of virtual processors is indicated by V in the legend.
Reference: [27] <author> J. D. Salehi, J. F. Kurose, and D. Towsley. </author> <title> The effectiveness of affinity-based scheduling in multiprocessor networking. </title> <booktitle> In Proceedings of the Conference on Computer Communications (IEEE Infocom), </booktitle> <address> page 2C.2, San Francisco, CA, </address> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: We found that in all experiments wiring threads to processors decreased the aggregate throughput. In some configurations this decrease was as much as 50%. Furthermore, the version of IRIX we used schedules threads for cache affinity [2], which has been shown to benefit connection-level parallelism <ref> [26, 27] </ref>. data on 12 to 3072 TCP connections, where the checksum is computed. Figure 7 shows the corresponding results for our UDP protocol stack, without checksumming. Where VPPC data are shown, the number of virtual processors is indicated by V in the legend. <p> This is encouraging since under conditions where connection sources do not have an infinite amount of data to send (i.e., are occasionally idle), the scheduling flexibility gained by increasing the number of virtual processors widens the performance improvement over that achieved by using fewer virtual processors <ref> [27, 31] </ref>. Figures 6 and 7 also show a surprising result: the throughput of TCP is less sensitive to an increase in the number of connections than that of UDP.
Reference: [28] <author> S. Saxena, J. K. Peacock, F. Yang, V. Verma, and M. Krishnan. </author> <title> Pitfalls in multithreading SVR4 STREAMS and other weightless processes. </title> <booktitle> In Proceedings of the Winter 1993 USENIX Conference, </booktitle> <pages> pages 85-96, </pages> <address> San Diego, CA, </address> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Connection-level parallelism associates the protocol processing required by connections with individual processes or threads. On a shared-memory multiprocessor, performance gains can be realized over multiple connections by executing these threads concurrently on different processors. Previous work on connection-level parallelism can be found in <ref> [8, 25, 28, 29] </ref>. In particular, Schmidt and Suda [29] have shown good scalability of the receive-side data path in connection-level parallelism, using a thread for each connection, on a 20-processor Sun SPARCCenter 2000. In this paper, we experimentally evaluate connection-level parallelism in a number of previously unexamined dimensions. <p> Given a set of connections, threads, and processors, the assignment or mapping between them can be done in a number of different ways. Choosing a mapping defines the granularity of a connection-level parallel implementation. Previous work on connection-level parallelism <ref> [8, 25, 28, 29] </ref> has focused on relatively static assignments of connections to processes. One novel aspect of our imple mentation is that it allows us to vary the mapping between processors, connections, and threads. <p> These threads are free to execute on either of the two processors, and perform protocol processing on behalf of any of the connections. In practice, however, each connection is typically associated with an individual thread <ref> [8, 28] </ref>. While TPC allows easy load balancing, it may not scale well with large numbers of connections, since each thread must be allocated resources (such as a thread control block and a stack). Our implementation facilitates varying the mapping of connections to threads and processors by using virtual processors.
Reference: [29] <author> D. C. Schmidt and T. Suda. </author> <title> Measuring the performance of parallel message-based process architectures. </title> <booktitle> In Proceedings of the Conference on Computer Communications (IEEE Infocom), </booktitle> <pages> pages 624-633, </pages> <address> Boston, MA, </address> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Connection-level parallelism associates the protocol processing required by connections with individual processes or threads. On a shared-memory multiprocessor, performance gains can be realized over multiple connections by executing these threads concurrently on different processors. Previous work on connection-level parallelism can be found in <ref> [8, 25, 28, 29] </ref>. In particular, Schmidt and Suda [29] have shown good scalability of the receive-side data path in connection-level parallelism, using a thread for each connection, on a 20-processor Sun SPARCCenter 2000. In this paper, we experimentally evaluate connection-level parallelism in a number of previously unexamined dimensions. <p> On a shared-memory multiprocessor, performance gains can be realized over multiple connections by executing these threads concurrently on different processors. Previous work on connection-level parallelism can be found in [8, 25, 28, 29]. In particular, Schmidt and Suda <ref> [29] </ref> have shown good scalability of the receive-side data path in connection-level parallelism, using a thread for each connection, on a 20-processor Sun SPARCCenter 2000. In this paper, we experimentally evaluate connection-level parallelism in a number of previously unexamined dimensions. <p> Given a set of connections, threads, and processors, the assignment or mapping between them can be done in a number of different ways. Choosing a mapping defines the granularity of a connection-level parallel implementation. Previous work on connection-level parallelism <ref> [8, 25, 28, 29] </ref> has focused on relatively static assignments of connections to processes. One novel aspect of our imple mentation is that it allows us to vary the mapping between processors, connections, and threads. <p> One innovative feature of our implementation is that there are no locks on the fast path through the protocol stack, on either the send or receive side (once packets have been demultiplexed to the appropriate virtual processor). This is in contrast to earlier implementations <ref> [3, 23, 25, 29] </ref> in which data structures are locked on the fast path. We accomplish this by replicating data structures on a per virtual processor basis where possible. <p> The drivers emulate an FDDI interface capable of operating at memory speeds (i.e., much faster than 100 Mbps), and support the FDDI maximum transmission unit (MTU) of slightly over 4KB. This approach is similar to the those used by other researchers <ref> [3, 10, 17, 23, 29] </ref>. In our experiments, we assume that a connection always has data to send (i.e., each connection is an infinite data source). Therefore, the drivers act as peer senders or receivers, producing or consuming packets as fast as possible. <p> Connection. 4 Figures 4 and 5 give compelling evidence that connection-level parallel protocol stacks (even with TCP) can scale with the number of processors. These results are consistent with those reported in <ref> [29] </ref>, which focus on the receive side for thread per connection parallelism. We extend their results in two significant ways. First, our UDP and send-side TCP results are new.
Reference: [30] <author> C. A. Thekkath, T. D. Nguyen, E. Moy, and E. D. Lazowska. </author> <title> Implementing network protocols at user level. </title> <booktitle> In SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 64-73, </pages> <address> San Francisco, CA, </address> <month> Sept. </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: Packet filters are becoming more popular (and efficient [1, 32]) in contemporary operating systems since early demultiplexing yields other performance gains. For example, depositing received packets directly into application buffers avoids copying data <ref> [5, 18, 30] </ref>. Our use of a packet filter, to demultiplex to the appropriate virtual processor, simply leverages this existing mechanism for an additional purpose. 3.2 Connection-Level Parallel Protocols Our Internet protocols are all based on the uniprocessor implementations distributed with the December 1993 version of the x-kernel.
Reference: [31] <author> D. J. Yates, E. M. Nahum, J. F. Kurose, and D. Towsley. </author> <title> Networking support for large scale multiprocessor servers. </title> <note> Technical Report CMPSCI 95-83 (in preparation), URL = ftp://gaia.cs.umass.edu/pub/Yate95:Networking.ps.Z, </note> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: This is encouraging since under conditions where connection sources do not have an infinite amount of data to send (i.e., are occasionally idle), the scheduling flexibility gained by increasing the number of virtual processors widens the performance improvement over that achieved by using fewer virtual processors <ref> [27, 31] </ref>. Figures 6 and 7 also show a surprising result: the throughput of TCP is less sensitive to an increase in the number of connections than that of UDP.
Reference: [32] <author> M. Yuhara, B. N. Bershad, C. Maeda, and J. E. Moss. </author> <title> Efficient packet demultiplexing for multiple endpoints and large messages. </title> <booktitle> In Proceedings of the Winter 1994 USENIX Conference, </booktitle> <month> Jan. </month> <year> 1994. </year> <month> 10 </month>
Reference-contexts: Protocols are conceptually replicated, and multiplexing occurs at the lowest layer of the protocol stack. On the receive path, a packet is immediately demultiplexed to the appropriate connection. This demultiplexing is performed by a packet filter or classifier <ref> [1, 19, 20, 32] </ref>. Given a set of connections, threads, and processors, the assignment or mapping between them can be done in a number of different ways. Choosing a mapping defines the granularity of a connection-level parallel implementation. <p> For example, TCP connections must be uniquely instantiated. Thus, our CLP implementation requires a packet filter mechanism to demultiplex received packets to the appropriate virtual processor for a TCP (or UDP) connection. Packet filters are becoming more popular (and efficient <ref> [1, 32] </ref>) in contemporary operating systems since early demultiplexing yields other performance gains. For example, depositing received packets directly into application buffers avoids copying data [5, 18, 30].
References-found: 32

