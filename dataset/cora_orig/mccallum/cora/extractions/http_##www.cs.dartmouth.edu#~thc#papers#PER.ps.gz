URL: http://www.cs.dartmouth.edu/~thc/papers/PER.ps.gz
Refering-URL: http://www.cs.dartmouth.edu/~thc/papers.html
Root-URL: http://www.cs.dartmouth.edu
Email: fthc,nicolg@cs.dartmouth.edu  
Title: Out-of-Core FFTs with Parallel Disks  
Author: Thomas H. Cormen David M. Nicol 
Address: Hanover, NH 03755  
Affiliation: Dartmouth College Department of Computer Science  
Abstract: We examine approaches to computing the Fast Fourier Transform (FFT) when the data size exceeds the size of main memory. Analytical and experimental evidence shows that relying on native virtual memory with demand paging can yield extremely poor performance. We then present approaches based on minimizing I/O costs with the Parallel Disk Model (PDM). Each of these approaches explicitly plans and performs disk accesses so as to minimize their number. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alok Aggarwal and Jeffrey Scott Vitter. </author> <title> The input/output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Asymptotically, the number of parallel I/O operations is fi N lg N lg min (M;N=M) , which can be shown via simple manipulations to equal the lower bound of N lg (N=B) proven by Aggarwal and Vitter <ref> [1] </ref>. Implementation notes We briefly mention some implementation details; see [6] for more information. * If lg F does not divide lg N, then we compensate in the last superlevel, computing mini-butterflies of depth (lg N ) mod (lg F ). * The BMMC permutations affect the twiddle-factor computations.
Reference: [2] <author> Jon F. Claerbout. </author> <title> Imaging the Earth's Interior. </title> <publisher> Blackwell Scientific Publications, </publisher> <year> 1985. </year>
Reference-contexts: 1 Introduction Although in most cases, Fast Fourier Transforms (FFTs) can be computed entirely in the main memory of a computer, in a few exceptional cases, the input vector is too large to fit. One application that uses very large FFTs is seismic analysis <ref> [2] </ref>; in one industrial application, an out-of-core one-dimensional FFT is necessary (as part of a higher dimensional FFT) even when the computer memory has 16 gigabytes of available RAM. Another application is in the area of radio astronomy.
Reference: [3] <author> Thomas H. Cormen. </author> <title> Determining an out-of-core FFT decomposition strategy for parallel disks by dynamic programming. </title> <type> Technical Report PCS-TR97-322, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1997. </year> <note> To appear in [9]. </note>
Reference-contexts: In contrast, starting from the same point, it took several weeks to develop and debug the stripe-major/communication method. Finally, we note that there are other frameworks for out-of-core FFTs with parallel disks. Rather than the Cooley-Tukey-based methods examined herein, <ref> [3] </ref> shows how to adapt Swarztrauber's method for an out-of-core setting on the PDM.
Reference: [4] <author> Thomas H. Cormen and Melissa Hirschl. </author> <title> Early experiences in evaluating the Parallel Disk Model with the ViC* implementation. </title> <booktitle> Parallel Computing, </booktitle> <address> 23(4-5):571-600, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Network speeds vary greatly, but for the multiprocessors that we consider, interprocessor communication times are far less than I/O latencies. The M -record memory is distributed among the P processors so that each processor holds M=P records. The implementation of the PDM we use is the ViC* API <ref> [4] </ref>, in which D P and each processor P i communicates only with the D=P disks D iD=P ; D iD=P +1 ; : : : ; D (i+1)D=P 1 . (If D &lt; P in a given physical configuration, the ViC* implementation provides the illusion that D = P by <p> We can still compute twiddle fac tors efficiently. * The BMMC permutation subroutine is taken from the implementation in <ref> [4] </ref>. It calls the ViC* API to perform striped reads and independent writes. It is carefully optimized for both in-core computation and I/O. * We implemented the FFT algorithm with both synchronous (i.e., blocking) and asynchronous (non-blocking) I/O calls; the ViC* interface sup ports both. <p> FDDI network. Each node runs AIX 4.1. Interpro-cessor communication is performed via the MPI calls MPI_Sendrecv () and MPI_Sendrecv_replace () using mpich version 1.1. Parallel I/O calls are through the ViC* API <ref> [4] </ref>, which in turn makes calls to the Galley File System [11]. Galley uses separate I/O processes (IOPs) to manage parallel I/O calls. The ViC* API treats each IOP like a disk. On FLEET, it is fastest to run the IOPs on separate nodes from the computational processes.
Reference: [5] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: For further background on the FFT, see any of the texts <ref> [5, 12, 15] </ref>. The FFT is a particular method of computing the Discrete Fourier Transform (DFT) of an N -element vector. <p> Then lg N = 3 stages of butterfly operations are performed, and the results (y 0 ; y 1 ; : : : ; y N1 ) emerge from the right. This figure is taken from <ref> [5, p. 796] </ref>. where ! N = e 2i=N and i = p 1. For any real number u, we can directly compute e iu = cos (u) + i sin (u). the Cooley-Tukey method of computing an FFT, drawn for N = 8.
Reference: [6] <author> Thomas H. Cormen and David M. Nicol. </author> <title> Performing out-of-core FFTs on parallel disk systems. </title> <type> Technical Report PCS-TR96-294, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> August </month> <year> 1996. </year> <note> To appear in Parallel Computing. </note>
Reference-contexts: Section 4 describes the Parallel Disk Model (PDM) of Vitter and Shriver [16], which we use to measure I/O costs for our explicit out-of-core algorithms. This paper surveys approaches from two of our earlier papers: 1. In <ref> [6] </ref>, we modify the Cooley-Tukey method to perform FFTs on a uniprocessor with parallel disks. Section 5 will review the method of that paper and show that it is a significant improvement over using demand paging. 2. In [8], we extend the uniprocessor method to multiple processors. <p> Analysis of bit reversal The following pseudocode expresses an in-place bit-reversal permutation of N -element array A: for j 0 to N 1 do let j 0 be the lg N-bit reversal of j if j &lt; j 0 then exchange A [j] $ A [j 0 ] In <ref> [6] </ref>, we prove the following theorem, which shows that the number of page faults for the Cooley-Tukey bit-reversal computation is at least N=4. Theorem 1 Suppose that the in-place bit-reversal permutation code above is performed under demand paging with least-recently-used page replacement. <p> Asymptotically, the number of parallel I/O operations is fi N lg N lg min (M;N=M) , which can be shown via simple manipulations to equal the lower bound of N lg (N=B) proven by Aggarwal and Vitter [1]. Implementation notes We briefly mention some implementation details; see <ref> [6] </ref> for more information. * If lg F does not divide lg N, then we compensate in the last superlevel, computing mini-butterflies of depth (lg N ) mod (lg F ). * The BMMC permutations affect the twiddle-factor computations.
Reference: [7] <author> Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <type> Technical Report PCS-TR94-223, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1994. </year> <note> Preliminary version appeared in Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures. Revised version to appear in SIAM Journal on Computing. </note>
Reference-contexts: As long as the characteristic matrix H is non-singular, the mapping of source indices to target indices is one-to-one. An efficient algorithm for BMMC permutations on the PDM appears in <ref> [7] </ref>.
Reference: [8] <author> Thomas H. Cormen, Jake Wegmann, and David M. Nicol. </author> <title> Multiprocessor out-of-core FFTs with distributed memory and parallel disks. </title> <booktitle> In Proceedings of the Fifth Workshop on I/O in Parallel and Distributed Systems (IOPADS '97), </booktitle> <pages> pages 68-78, </pages> <month> November </month> <year> 1997. </year>
Reference-contexts: In [6], we modify the Cooley-Tukey method to perform FFTs on a uniprocessor with parallel disks. Section 5 will review the method of that paper and show that it is a significant improvement over using demand paging. 2. In <ref> [8] </ref>, we extend the uniprocessor method to multiple processors. Section 6 will discuss the four variations of a multiprocessor method that we implemented. In two of these variations, no communication occurs during butterfly computations, even though we are running on a multiprocessor.
Reference: [9] <institution> Proceedings of the Workshop on Algorithms for Parallel Machines, 1996-97 Special Year on Mathematics of High Performance Computing, Institute for Mathematics and Its Applications, University of Minnesota, Minneapolis, </institution> <month> September </month> <year> 1996. </year>
Reference: [10] <author> Todd C. Mowry, Angela K. Demke, and Or-ran Kreiger. </author> <title> Automatic compiler-inserted I/O prefetching for out-of-core applications. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 3-17, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The methods described in Sections 5 and 6 substantially restructure the FFT computation by introducing permutation steps to maintain data locality. Consequently, they are far beyond anything we can expect a compiler or custom paging policies to do, as in the approaches of <ref> [10, 13] </ref>. 2 FFTs This section reviews Fourier transforms and outlines some well-known FFT methods for in-core computation. For further background on the FFT, see any of the texts [5, 12, 15]. The FFT is a particular method of computing the Discrete Fourier Transform (DFT) of an N -element vector.
Reference: [11] <author> Nils Nieuwejaar and David Kotz. </author> <title> The Galley parallel file system. </title> <journal> Parallel Computing, </journal> <volume> 23(4) </volume> <pages> 447-476, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: FDDI network. Each node runs AIX 4.1. Interpro-cessor communication is performed via the MPI calls MPI_Sendrecv () and MPI_Sendrecv_replace () using mpich version 1.1. Parallel I/O calls are through the ViC* API [4], which in turn makes calls to the Galley File System <ref> [11] </ref>. Galley uses separate I/O processes (IOPs) to manage parallel I/O calls. The ViC* API treats each IOP like a disk. On FLEET, it is fastest to run the IOPs on separate nodes from the computational processes. Consequently, we report results for P = 4 and D = 4.
Reference: [12] <author> Henri J. Nussbaumer. </author> <title> Fast Fourier Transform and Convolution Algorithms. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1982. </year>
Reference-contexts: For further background on the FFT, see any of the texts <ref> [5, 12, 15] </ref>. The FFT is a particular method of computing the Discrete Fourier Transform (DFT) of an N -element vector.
Reference: [13] <author> Yoonho Park, Ridgway Scott, and Stuart Sechrest. </author> <title> Virtual memory versus file interfaces for large, </title> <booktitle> memory-intensive scientific applications. In Proceedings of Supercomputing '96, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: The methods described in Sections 5 and 6 substantially restructure the FFT computation by introducing permutation steps to maintain data locality. Consequently, they are far beyond anything we can expect a compiler or custom paging policies to do, as in the approaches of <ref> [10, 13] </ref>. 2 FFTs This section reviews Fourier transforms and outlines some well-known FFT methods for in-core computation. For further background on the FFT, see any of the texts [5, 12, 15]. The FFT is a particular method of computing the Discrete Fourier Transform (DFT) of an N -element vector.
Reference: [14] <author> M. Snir. </author> <title> I/O limitations on multi-chip VLSI systems. </title> <booktitle> In Proceedings of the 19th Allerton Conference on Communication, Control and Computation, </booktitle> <pages> pages 224-233, </pages> <year> 1981. </year>
Reference-contexts: The characteristic matrix is formed by taking the identity matrix and rotating its columns k positions to the right, and rank min (k; lg N k; lg M; lg (N=M )). Redrawing the butterfly redrawing of the butterfly was devised by Snir <ref> [14] </ref> and is implicitly used in the FFT algorithm of Vitter and Shriver [16]. We use an effective memory size F . Assume for now that F = M and that lg F divides lg N. As in the Cooley-Tukey method, we start with a bit-reversal permutation.
Reference: [15] <author> Charles Van Loan. </author> <title> Computational Frameworks for the Fast Fourier Transform. </title> <publisher> SIAM Press, </publisher> <address> Philadel-phia, </address> <year> 1992. </year>
Reference-contexts: For further background on the FFT, see any of the texts <ref> [5, 12, 15] </ref>. The FFT is a particular method of computing the Discrete Fourier Transform (DFT) of an N -element vector. <p> Several other methods have been developed to improve performance on vector machines and in memory hierarchies, by avoiding the bit-reversal permutation to improve locality of reference. Stockham's method <ref> [15, pp. 49-58] </ref> eliminates bit-reversal by incorporating permutations into each of the lg N stages of the butterfly network. Its memory requirement, however, is twice that of the Cooley-Tukey method.
Reference: [16] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <journal> Algorithmica, </journal> 12(2/3):110-147, August and September 1994. 
Reference-contexts: Section 3 shows how poorly they perform when they run with demand paging. Section 4 describes the Parallel Disk Model (PDM) of Vitter and Shriver <ref> [16] </ref>, which we use to measure I/O costs for our explicit out-of-core algorithms. This paper surveys approaches from two of our earlier papers: 1. In [6], we modify the Cooley-Tukey method to perform FFTs on a uniprocessor with parallel disks. <p> Nevertheless, we shall see in Section 5 (and Figure 2 shows) that our explicit out-of-core algorithm runs faster than even Swarztrauber's method on the same system for a problem size of N = 2 24 . 4 The Parallel Disk Model This section describes the Parallel Disk Model <ref> [16] </ref>, which underlies all the out-of-core FFT algorithms herein. In the Parallel Disk Model, or PDM, N records are stored on D disks D 0 ; D 1 ; : : : ; D D1 , with N=D records stored on each disk. <p> Redrawing the butterfly redrawing of the butterfly was devised by Snir [14] and is implicitly used in the FFT algorithm of Vitter and Shriver <ref> [16] </ref>. We use an effective memory size F . Assume for now that F = M and that lg F divides lg N. As in the Cooley-Tukey method, we start with a bit-reversal permutation.
References-found: 16

