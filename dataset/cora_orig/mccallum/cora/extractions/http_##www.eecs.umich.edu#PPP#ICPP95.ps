URL: http://www.eecs.umich.edu/PPP/ICPP95.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Email: alexe@eecs.umich.edu abraham@hpl.hp.com  
Title: Impact of Load Imbalance on the Design of Software Barriers  
Author: Alexandre E. Eichenberger Santosh G. Abraham 
Keyword: Synchronization barrier, fuzzy barriers, combining tree, shared-memory multiprocessors, parallel processing  
Address: 1501 Page Mill Road Ann Arbor, MI 48109-2122 Palo Alto, CA 94303  
Affiliation: Advanced Computer Architecture Laboratory Hewlett Packard Laboratories EECS Department, University of Michigan  
Abstract: Software barriers have been designed and evaluated for barrier synchronization in large-scale shared-memory multiprocessors, under the assumption that all processors reach the synchronization point simultaneously. When relaxing this assumption, we demonstrate that the optimum degree of combining trees is not four as previously thought but increases from four to as much as 128 in a 4K system as the load imbalance increases. The optimum degree calculated using our analytic model yields a performance that is within 7% of the optimum obtained by exhaustive simulation with a range of degrees. We also investigate a dynamic placement barrier where slow processors migrate toward the root of the software combining tree. We show that through dynamic placement the synchronization delay can be reduced by a factor close to the depth of the tree, when sufficient slack is available. By choosing a suitable tree degree and using dynamic placement, software barriers that are scalable to large numbers of processors can be constructed. We demonstrate the applicability of our results by performing measurements on a small SOR relaxation program running on a 56-processor KSR1. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Gottlieb et al., </author> <title> The NYU ultracomputer-designing an MIMD shared memory parallel computer, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 32, no. 2, </volume> <pages> pp. 175-189, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: The overhead of such barriers increases linearly with the number of processors and can dominate overall execution time. In response to the potential overhead of software barrier synchronization schemes, several hardware schemes have been proposed. The NYU Ultracomputer <ref> [1] </ref> and the IBM RP3 [2] em fl Appeared in the Proceedings of the 1995 International Conference on Parallel Processing, pp 63-72, August 14-18, 1995. ploy combining networks which combine accesses to the same memory location, thus alleviating contention on the counters used to implement synchronization barriers. <p> In response to performance concerns, hardware support has been designed and implemented in several parallel machines. Combining networks <ref> [1] </ref> [2], that combine concurrent accesses to the same memory location, have been advocated as a technique that significantly reduces the impact of busy waiting. Similarly, special purpose cache protocols [9] [10] have been designed to include synchronization primitives that reduce communication due to synchronization.
Reference: [2] <author> G. Pfister et al., </author> <title> The IBM research parallel processor prototype (RP3): Introduction and architecture, </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. 764-771, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: The overhead of such barriers increases linearly with the number of processors and can dominate overall execution time. In response to the potential overhead of software barrier synchronization schemes, several hardware schemes have been proposed. The NYU Ultracomputer [1] and the IBM RP3 <ref> [2] </ref> em fl Appeared in the Proceedings of the 1995 International Conference on Parallel Processing, pp 63-72, August 14-18, 1995. ploy combining networks which combine accesses to the same memory location, thus alleviating contention on the counters used to implement synchronization barriers. <p> In response to performance concerns, hardware support has been designed and implemented in several parallel machines. Combining networks [1] <ref> [2] </ref>, that combine concurrent accesses to the same memory location, have been advocated as a technique that significantly reduces the impact of busy waiting. Similarly, special purpose cache protocols [9] [10] have been designed to include synchronization primitives that reduce communication due to synchronization.
Reference: [3] <institution> Cray T3D System Architecture Overview, Cray Research, Inc, </institution> <address> revision 1.c edition, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Other machines such as the Sequent, SGI, and Alliant have provided special synchronization buses. Vector supercomputers such as the Cray and the Convex provide a set of communication registers which are used for fast barrier synchronization. The Cray T3D multiprocessor has a fast synchronization network for barrier synchronization <ref> [3] </ref>. The hardware support for efficient synchronization is indicative of the potential impact of synchronization on overall performance. Software barrier synchronization schemes that can approach the performance of hardware techniques are extremely attractive because hardware schemes have several disadvantages.
Reference: [4] <author> G. Pfister and V. A. Norton, </author> <title> Hot spot contention and combining in multistage interconnection networks, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-34, no. 4, </volume> <pages> pp. 943-948, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Software barrier synchronization schemes that can approach the performance of hardware techniques are extremely attractive because hardware schemes have several disadvantages. Hardware synchronization schemes are expensive; for instance, the combining network is at least six times as expensive as a non-combining network <ref> [4] </ref>. Also, hardware schemes employ special-purpose logic that has a large design cost, especially significant for low-volume parallel systems. Thirdly, software barriers are more flexible and can be adapted to suit the application or to exploit advances in synchronization techniques such as fuzzy barriers. <p> Section 7 presents our measurements on a parallel machine (KSR1). Finally, we conclude in Section 8. 2 Related Work Performance degradations due to busy wait synchronization are widely regarded as a serious performance problem. Pfister and Norton <ref> [4] </ref> showed that the presence of hot spots can severely degrade performance for all traffic in multistage interconnection networks.
Reference: [5] <author> M. M. Michael and M. L. Scott, </author> <title> Fast mutual exclusion, even with contention, </title> <type> Technical Report TR-460, </type> <institution> University of Rochester, </institution> <year> 1993. </year>
Reference-contexts: Finally, software barrier schemes are more easily portable to different platforms. The disadvantages of hardware synchronization schemes have motivated the study of software barriers using software combining trees. In some cases, these studies have demonstrated that the synchronization performance of software schemes approach that of hardware synchronization <ref> [5] </ref> [6]. In a software combining tree, a tree of counters is used for synchronization. Processors are divided into groups and a group is assigned to each leaf of the combining tree. <p> Furthermore, they showed that the optimal degree of a combining tree (fan in) is around four. Mellor-Crummey and Scott [7] refined this technique further, presenting an algorithm that generates the theoretical minimum number of communications on machines without broadcast. Michael and Scott <ref> [5] </ref> showed that a software implemented exclusion mechanism could outperform naive hardware locks, even under heavy contention. Alternatives to the usual synchronization barriers have also been investigated. Gupta [11] developed and investigated Fuzzy Barriers. He measured significant performance improvements with software implemented Fuzzy Barriers on a four processor Encore Multimax.
Reference: [6] <author> P.-C. Yew, N.-F. Tzeng, and D. H. Lawrie, </author> <title> Distributing hot-spots addressing in large-scale multiprocessors, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 36, no. 4, </volume> <pages> pp. 388-395, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Finally, software barrier schemes are more easily portable to different platforms. The disadvantages of hardware synchronization schemes have motivated the study of software barriers using software combining trees. In some cases, these studies have demonstrated that the synchronization performance of software schemes approach that of hardware synchronization [5] <ref> [6] </ref>. In a software combining tree, a tree of counters is used for synchronization. Processors are divided into groups and a group is assigned to each leaf of the combining tree. <p> Similarly, special purpose cache protocols [9] [10] have been designed to include synchronization primitives that reduce communication due to synchronization. New software synchronization mechanisms have been developed to approach the synchronization performance of dedicated hardware at lower cost. Yew, Tzeng and Lawrie <ref> [6] </ref> investigated the use of software combining trees to distribute hot spots in large scale multiprocessors. Their analysis indicates that combining trees effectively decrease memory contention. Furthermore, they showed that the optimal degree of a combining tree (fan in) is around four. <p> The performance of a combining tree of optimal degree is compared to a combining tree of degree four because degree four was previously considered as optimal <ref> [6] </ref> [7]. Processors Optimal Degree (Sync. Speedup) = 0t c = 25t c = 500t c 256 4 (1.00) 32 (1.97) 256 (4.00) To obtain these optimal degrees we used a conventional event driven simulator.
Reference: [7] <author> J. M. Mellor-Crummey and M. L. Scott, </author> <title> Algorithms for scalable synchronization on shared memory multiprocessors, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 9, no. 1, </volume> <pages> pp. 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: In presence of systemic load imbalance, some processors tend to be consistently late and are consistently delayed further by the synchronization barrier. We explore the possibility of giving less synchronization work to these processors by modifying a tree structure first proposed in Mellor-Crummey and Scott <ref> [7] </ref>. In their tree structure, one processor is statically attached to each non-leaf counter in the software combining tree. The rest of the processors are split into groups and assigned to the leaf counters. <p> Yew, Tzeng and Lawrie [6] investigated the use of software combining trees to distribute hot spots in large scale multiprocessors. Their analysis indicates that combining trees effectively decrease memory contention. Furthermore, they showed that the optimal degree of a combining tree (fan in) is around four. Mellor-Crummey and Scott <ref> [7] </ref> refined this technique further, presenting an algorithm that generates the theoretical minimum number of communications on machines without broadcast. Michael and Scott [5] showed that a software implemented exclusion mechanism could outperform naive hardware locks, even under heavy contention. Alternatives to the usual synchronization barriers have also been investigated. <p> The performance of a combining tree of optimal degree is compared to a combining tree of degree four because degree four was previously considered as optimal [6] <ref> [7] </ref>. Processors Optimal Degree (Sync. Speedup) = 0t c = 25t c = 500t c 256 4 (1.00) 32 (1.97) 256 (4.00) To obtain these optimal degrees we used a conventional event driven simulator. <p> Indeed the optimal degree combining trees are only 7% faster on average than the estimated degrees. We also investigated the combining trees proposed by Mellor-Crummey and Scott <ref> [7] </ref>, as described in Section 1. We simulated these trees and obtained results similar to the one of Figure 4. Comparing these results with the ones of Figure 4, we noticed performance improvements of 5%, on average, for all combining trees with an optimal degree of four. <p> The next section will investigate the feasibility of this technique. 5 Dynamic Placement in Combining Trees In this section, we use the combining tree presented by Mellor-Crummey and Scott <ref> [7] </ref>. As mentioned in Section 1, this technique uses combining trees of degree d where each counter in the tree is connected to at least one processor, and where leaf counters are connected statically to at most d + 1 processors. <p> This algorithm is based on the combining tree presented in <ref> [7] </ref>. However, we expect to reduce the critical path from O (log p) to O (1) when the prediction is successful. The dynamic placement barrier proceeds as follows. When a processor propagates toward the top of the tree, it positions itself at the highest level counter where it arrived last.
Reference: [8] <author> A. Agrawal and M. Cherian, </author> <title> Adaptive backoff synchronization techniques, </title> <booktitle> Proceedings of the Sixteenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 396-406, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Finally, we conclude in Section 8. 2 Related Work Performance degradations due to busy wait synchronization are widely regarded as a serious performance problem. Pfister and Norton [4] showed that the presence of hot spots can severely degrade performance for all traffic in multistage interconnection networks. Agarwal and Cherian <ref> [8] </ref> investigated the impact of synchronization on overall program performance and showed that cache line invalidations due to synchronization references can account for more than half of all invalidations. In response to performance concerns, hardware support has been designed and implemented in several parallel machines.
Reference: [9] <author> J. R. Goodman, M. K. Vernon, and P. J. Woest, </author> <title> Efficient synchronization primitives for large-scale cache-coherent multiprocessors, </title> <booktitle> 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 64-75, </pages> <month> apr </month> <year> 1989. </year>
Reference-contexts: In response to performance concerns, hardware support has been designed and implemented in several parallel machines. Combining networks [1] [2], that combine concurrent accesses to the same memory location, have been advocated as a technique that significantly reduces the impact of busy waiting. Similarly, special purpose cache protocols <ref> [9] </ref> [10] have been designed to include synchronization primitives that reduce communication due to synchronization. New software synchronization mechanisms have been developed to approach the synchronization performance of dedicated hardware at lower cost.
Reference: [10] <author> J. Lee and U. Ramachandran, </author> <title> Synchronization with multiprocessor cache, </title> <booktitle> Proceedings of the Seventeenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 27-37, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In response to performance concerns, hardware support has been designed and implemented in several parallel machines. Combining networks [1] [2], that combine concurrent accesses to the same memory location, have been advocated as a technique that significantly reduces the impact of busy waiting. Similarly, special purpose cache protocols [9] <ref> [10] </ref> have been designed to include synchronization primitives that reduce communication due to synchronization. New software synchronization mechanisms have been developed to approach the synchronization performance of dedicated hardware at lower cost.
Reference: [11] <author> R. Gupta, </author> <title> The fuzzy barrier: A mechanism for high speed synchronization of processors, </title> <booktitle> 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 54-63, </pages> <year> 1989. </year>
Reference-contexts: Michael and Scott [5] showed that a software implemented exclusion mechanism could outperform naive hardware locks, even under heavy contention. Alternatives to the usual synchronization barriers have also been investigated. Gupta <ref> [11] </ref> developed and investigated Fuzzy Barriers. He measured significant performance improvements with software implemented Fuzzy Barriers on a four processor Encore Multimax. He presents techniques [11] [12] that detect and increase the number of independent operations, and hence the slack time. <p> Alternatives to the usual synchronization barriers have also been investigated. Gupta <ref> [11] </ref> developed and investigated Fuzzy Barriers. He measured significant performance improvements with software implemented Fuzzy Barriers on a four processor Encore Multimax. He presents techniques [11] [12] that detect and increase the number of independent operations, and hence the slack time. Eichenberger and Abraham [13] characterized the performance improvements due to fuzzy barriers and showed that the expected idle time at a fuzzy barrier is inversely proportional to the slack time. <p> A similar situation arises with evolving workload imbalance, where the workload slowly fluctuates from iteration to iteration. In both cases, recent history is a good indication of future processor arrival order. With fuzzy barriers <ref> [11] </ref>, independent operations are inserted between the release and the enforce phase, thus significantly reducing the expected idle time due to non-deterministic work ( = 0:25 ms, p = 64) load imbalance. The independent operation execution time corresponds to the slack of a fuzzy barrier.
Reference: [12] <author> R. Gupta, </author> <title> Loop displacement: An approach for transforming and scheduling loops for parallel execution, </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <pages> pp. 388-397, </pages> <year> 1990. </year>
Reference-contexts: Alternatives to the usual synchronization barriers have also been investigated. Gupta [11] developed and investigated Fuzzy Barriers. He measured significant performance improvements with software implemented Fuzzy Barriers on a four processor Encore Multimax. He presents techniques [11] <ref> [12] </ref> that detect and increase the number of independent operations, and hence the slack time. Eichenberger and Abraham [13] characterized the performance improvements due to fuzzy barriers and showed that the expected idle time at a fuzzy barrier is inversely proportional to the slack time.
Reference: [13] <author> A. E. Eichenberger and S. G. Abraham, </author> <title> Modeling load imbalance and fuzzy barriers for scalable shared-memory multiprocessors, </title> <booktitle> Proceeding of the 28th Hawaii International Conferenceon System Sciences, </booktitle> <volume> vol. I, </volume> <pages> pp. 262-271, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Gupta [11] developed and investigated Fuzzy Barriers. He measured significant performance improvements with software implemented Fuzzy Barriers on a four processor Encore Multimax. He presents techniques [11] [12] that detect and increase the number of independent operations, and hence the slack time. Eichenberger and Abraham <ref> [13] </ref> characterized the performance improvements due to fuzzy barriers and showed that the expected idle time at a fuzzy barrier is inversely proportional to the slack time. Finally, Nguyen [14] investigated compiler techniques that transform synchronization barriers into point II-64 to point synchronizations, showing encouraging performance im-provements. <p> Sarkar [17] provided a framework to estimate the execution time and its variance based on the program's internal structure and control dependence graph. Finally, Eichenberger and Abraham <ref> [13] </ref> analyzed the fluctuation of processor execution time due to random replacement caches and communication contention. We derived an analytical formula describing the expected variance for programs with simple memory and communication access patterns. <p> We assumed the processors to be normally distributed, an assumption supported by measurements in <ref> [13] </ref> and [15]. First, we will determine the optimal degree experimentally. <p> However, when fuzzy barriers are employed, load imbalance does not necessarily translate into idle times <ref> [13] </ref> and an application could have substantial load imbalance and still be efficient, provided synchronization delays are not excessive. We now investigate the use of the approximated synchronization delay of Equation (8) to estimate the optimal degree of a combining tree. <p> With fuzzy barriers [11], independent operations are inserted between the release and the enforce phase, thus significantly reducing the expected idle time due to non-deterministic work ( = 0:25 ms, p = 64) load imbalance. The independent operation execution time corresponds to the slack of a fuzzy barrier. In <ref> [13] </ref>, we showed that the expected idle time is inversely proportional to slack. Here, we will consider another interesting property of fuzzy barriers. With increasing slack, some processors can be significantly slower than others without requiring faster processors to wait for them. <p> The two-dimensional data of size (d x , d y ) is partitioned along the x-dimension, resulting in 4dd y =16e communication events per processors 3 . By varying the y-dimension, we change the total number of communications and therefore the variation of execution time <ref> [13] </ref>. All measurements consist of 200 relaxations on 56 processors 4 with d x sets to 60 data points per processor. In the first set of measurements, we investigated the optimal combining tree degree for various numbers of data along the y-dimension.
Reference: [14] <author> J. Nguyen, </author> <title> Compiler Analysis to Implement Point-to-Point Synchronization in Parallel Programs, </title> <type> PhD thesis, </type> <institution> MIT, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Eichenberger and Abraham [13] characterized the performance improvements due to fuzzy barriers and showed that the expected idle time at a fuzzy barrier is inversely proportional to the slack time. Finally, Nguyen <ref> [14] </ref> investigated compiler techniques that transform synchronization barriers into point II-64 to point synchronizations, showing encouraging performance im-provements. The source and extent of variation of thread (processor) execution times have been investigated in a few studies.
Reference: [15] <author> V. S. Adve and M. K. Vernon, </author> <title> The influence of random delays on parallel execution times, </title> <booktitle> ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 61-73, </pages> <year> 1993. </year>
Reference-contexts: Finally, Nguyen [14] investigated compiler techniques that transform synchronization barriers into point II-64 to point synchronizations, showing encouraging performance im-provements. The source and extent of variation of thread (processor) execution times have been investigated in a few studies. Adve and Vernon <ref> [15] </ref> have measured the fluctuations of parallel execution times for a large number of applications and observed that the empirical execution time distribution very closely tracks the normal distribution. <p> We assumed the processors to be normally distributed, an assumption supported by measurements in [13] and <ref> [15] </ref>. First, we will determine the optimal degree experimentally. Then, we will use the approximation presented in Section 3 to estimate this degree and compare the performance improvement between the experimental and estimated optimal degree. the degree that resulted in the smallest synchronization delay, as defined in Section 1.
Reference: [16] <author> M. Dubois and F. A. Briggs, </author> <title> Performance of synchronized iterative processes in multiprocessor systems, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. SE-8, no. 4, </volume> <pages> pp. 419-431, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Adve and Vernon [15] have measured the fluctuations of parallel execution times for a large number of applications and observed that the empirical execution time distribution very closely tracks the normal distribution. Dubois and Briggs <ref> [16] </ref> obtained an analytical formula describing the expected number of cycles and its variance for memory references in tightly coupled systems. Sarkar [17] provided a framework to estimate the execution time and its variance based on the program's internal structure and control dependence graph.
Reference: [17] <author> V. Sarkar, </author> <title> Determining average program execution times and their variance, </title> <booktitle> Proceedings of the ACM SIGPLAN'89 Conference on Programming Language Design and Implementation, </booktitle> <volume> vol. 24, no. 7, </volume> <pages> pp. 298-312, </pages> <year> 1989. </year>
Reference-contexts: Dubois and Briggs [16] obtained an analytical formula describing the expected number of cycles and its variance for memory references in tightly coupled systems. Sarkar <ref> [17] </ref> provided a framework to estimate the execution time and its variance based on the program's internal structure and control dependence graph. Finally, Eichenberger and Abraham [13] analyzed the fluctuation of processor execution time due to random replacement caches and communication contention.
Reference: [18] <author> C. P. Kruskal and A. Weiss, </author> <title> Allocating independent subtasks on parallel processors, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. SE-11, no. 10, </volume> <pages> pp. 1001-1016, </pages> <month> Oc-tober </month> <year> 1985. </year>
Reference-contexts: The effects of load imbalance on idle times, assuming a perfect barrier with zero synchronization delay, have been investigated in several articles. Kruskal and Weiss <ref> [18] </ref> have investigated the total execution time required to complete k tasks for various distributions. The performance of parallel algorithms that have regular control structures and non-deterministic task execution times is quantified by Madala and Sinclair [19].
Reference: [19] <author> S. Madala and J. B. Sinclair, </author> <title> Performance of synchronous parallel algorithms with regular structure, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 105-116, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Kruskal and Weiss [18] have investigated the total execution time required to complete k tasks for various distributions. The performance of parallel algorithms that have regular control structures and non-deterministic task execution times is quantified by Madala and Sinclair <ref> [19] </ref>. Durand et al provide experimental measurements on the impact of memory contention in NUMA parallel machines [20]. Axelrod [21] has considered both the effects of load imbalance and synchronization costs and derived an analytical result that takes both load imbalance and synchronization costs into consideration.
Reference: [20] <author> M. D. Durand, T. Montaut, L. Kervella, and W. Jalby, </author> <title> Impact of memory contention on dynamic scheduling on numa multiprocessors, </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 258-267, </pages> <year> 1993. </year>
Reference-contexts: The performance of parallel algorithms that have regular control structures and non-deterministic task execution times is quantified by Madala and Sinclair [19]. Durand et al provide experimental measurements on the impact of memory contention in NUMA parallel machines <ref> [20] </ref>. Axelrod [21] has considered both the effects of load imbalance and synchronization costs and derived an analytical result that takes both load imbalance and synchronization costs into consideration.
Reference: [21] <author> T. S. Axelrod, </author> <title> Effects of synchronization barriers on multiprocessor performance, </title> <journal> Parallel Computing, </journal> <volume> vol. 3, </volume> <pages> pp. 129-140, </pages> <year> 1986. </year>
Reference-contexts: The performance of parallel algorithms that have regular control structures and non-deterministic task execution times is quantified by Madala and Sinclair [19]. Durand et al provide experimental measurements on the impact of memory contention in NUMA parallel machines [20]. Axelrod <ref> [21] </ref> has considered both the effects of load imbalance and synchronization costs and derived an analytical result that takes both load imbalance and synchronization costs into consideration. However, while considering the synchronization costs, he assumed that processors arrive simultaneously at the synchronization point thus overestimating the effects of contention.
Reference: [22] <author> C. J. Beckmann and C. D. Polychronopoulos, </author> <title> The effect of barrier synchronization and scheduling overhead on parallel loops, </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 200-204, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: However, while considering the synchronization costs, he assumed that processors arrive simultaneously at the synchronization point thus overestimating the effects of contention. We determine synchronization delays as a function of both the particular synchronization structure used and the load imbalance. Beckmann and Polychronopoulos <ref> [22] </ref> have investigated the effects of barrier synchronization and dynamic loop dispatch overhead. They classify loops as synchronization bound or arrival-time bound, depending on the spread of processor arrival time at the barrier.
Reference: [23] <author> A. H.-S. Ang and W. H. Tang, </author> <title> Probability Concepts In Engineering Planning And Design, volume 2, </title> <publisher> New York : Wiley, </publisher> <year> 1984. </year>
Reference-contexts: Two problems arise when extending the previous model to processors that do not arrive simultaneously. The first problem is that only the slowest processors propagate upward in the tree, requiring the use of order statistics <ref> [23] </ref> at each level of the tree. The second problem is that contention at one level changes the distribution of the processors that propagate to the next level in the combining tree. <p> Since we consider here only the arrival time relative to the mean, we omitted the term in the preceding equation. Finally, we can asymptotically estimate the arrival time of the last processor with the help of order statistics <ref> [23] </ref>: T arr (last) = 2 log p 2 2 log p (5) 1 Since P before (S L1 ) = 0 and 1 (0) = 1 we approximate P before (S L1 ) as P before (S L2 )=2. Furthermore, we compute the release times as follows.
Reference: [24] <institution> KSR1 Principles of Operation, Kendall Square Research Corporation, </institution> <year> 1991. </year> <month> II-72 </month>
Reference-contexts: This effect is due to the fact that flatter trees have proportionately fewer processors near the top of the tree, making the choice of the slower processors more critical. 7 Measurements In order to test the performance improvements due to optimal tree degree and dynamic placements on the KSR1 <ref> [24] </ref>, we used a program that has a well defined computation and communication pattern. We used a relaxation algorithm (SOR) where each element is averaged with its four neighbors. The relaxation is performed in two alternating arrays, thus avoiding additional communication due to race conditions among processes.
References-found: 24

