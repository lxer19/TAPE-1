URL: http://www.cs.nyu.edu/phd_students/lindaoi/linkedemtr742.ps.gz
Refering-URL: http://www.cs.nyu.edu/phd_students/lindaoi/index.html
Root-URL: http://www.cs.nyu.edu
Title: Pincer-Search: A New Algorithm for Discovering the Maximum Frequent Set  
Author: Dao-I Lin Zvi M. Kedem 
Date: September 11, 1997  
Affiliation: New York University  New York University  
Abstract: Discovering frequent itemsets is a key problem in important data mining applications, such as the discovery of association rules, strong rules, episodes, and minimal keys. Typical algorithms for solving this problem operate in a bottom-up breadth-first search direction. The computation starts from frequent 1-itemsets (minimal length frequent itemsets) and continues until all maximal (length) frequent itemsets are found. During the execution, every frequent itemset is explicitly considered. Such algorithms perform reasonably well when all maximal frequent item-sets are short. However, performance drastically decreases when some of the maximal frequent itemsets are relatively long. We present a new algorithm which combines both the bottom-up and top-down directions. The main search direction is still bottom-up but a restricted search is conducted in the top-down direction. This search is used only for maintaining and updating a new data structure we designed, the maximum frequent candidate set. It is used to prune candidates in the bottom-up search. As a very important characteristic of the algorithm, it is not necessary to explicitly examine every frequent itemset. Therefore it performs well even when some maximal frequent itemsets are long. As its output, the algorithm produces the maximum frequent set, i.e., the set containing all maximal frequent itemsets, which therefore specifies immediately all frequent itemsets. We evaluate the performance of the algorithm using a well-known benchmark database. The improvements can be up to several orders of magnitude, compared to the best current algorithms.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and R. Srikant. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> SIGMOD, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Section 4 presents the results of our experiments. Section 5 briefly discusses the related research. Finally, Section 6 concludes this paper. 2 Association Rule Mining This section briefly introduces the association rule mining problem. To the extent feasible, we follow the terminology of <ref> [1] </ref>. 2 2.1 The Setting of the Problem Let I = fi 1 ; i 2 ; : : : ; i m g be a set of m distinct items. A transaction T is defined as any subset of items in I.
Reference: [2] <author> R. Agrawal, T. Imielinski, and R. Srikant. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> Vol. 5, No. 6, </volume> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Some papers concentrate on parallel algorithms ([4] [9] [16]). Other papers focus on mining generalized association rules ([6] [15]). Another direction, as described in <ref> [2] </ref> and [11], is to provide a unified model for the process of classification, association, and sequence rules discovery. The discovery of frequent set is an important process in solving these problems. Our approach can be applied to solve these problems.
Reference: [3] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> In Proc. 20th VLDB, </booktitle> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as association rules (e.g., <ref> [3] </ref> [12]), strong rules (e.g., [14]), episodes (e.g., [10]) and minimal keys (e.g., [11]). Typical algorithms for finding the frequent set, i.e., the set of all frequent itemsets (e.g., [3] [12]), operate in a bottom-up breadth-first fashion. <p> frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as association rules (e.g., <ref> [3] </ref> [12]), strong rules (e.g., [14]), episodes (e.g., [10]) and minimal keys (e.g., [11]). Typical algorithms for finding the frequent set, i.e., the set of all frequent itemsets (e.g., [3] [12]), operate in a bottom-up breadth-first fashion. In other words, the computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. <p> In this paper, we present a novel Pincer-Search algorithm which searches for MFS from both bottom-up and top-down directions. This algorithm performs well even when the maximal frequent itemsets are long. In our algorithm, the bottom-up search is similar to the Apriori <ref> [3] </ref> and the OCD [12] algorithms. However, the top-down search is unique. The top-down search is implemented efficiently by introducing a set that we call the maximum-frequent-candidate-set or MFCS. <p> In this paper, we apply the MFCS idea to association rules mining and to the discovery of frequent itemsets in market basket data. Popular benchmark databases designed by Agrawal and Srikant <ref> [3] </ref> have been used in [9], [16], and [18]: we use these benchmarks to evaluate the performance of our algorithm. In most cases, our algorithm not only reduces the number of passes of reading the database but also can reduce the number of candidates (for whom support is counted). <p> The problem of association rule mining is to discover all rules that have support and confidence greater than some user-defined minimum support and minimum confidence thresholds, respectively. Association rules that satisfy these requirements are interesting. The normally followed scheme for mining association rules consists of two stages <ref> [3] </ref>: 1. the discovery of frequent itemsets, followed by 2. the generation of association rules. <p> Initially the frequent and the infrequent sets are empty. The process terminates when every itemset is either in the frequent set or in the infrequent set. We now briefly sketch a realization of this process as in, e.g., <ref> [3] </ref>. This is a bottom-up approach. It consists of repeatedly applying a pass, itself consisting of two steps. At the end of pass k all 3 frequent itemsets of size k or less have been discovered. <p> Thus obviously at any point of the algorithm MFCS is a superset of MFS. When the algorithm terminates, MFCS and MFS are equal. The computation of our algorithm follows the bottom-up (breadth-first) search approach. We base our presentation on the Apriori algorithm <ref> [3] </ref>, and for greatest ease of exposition we present our algorithm as a modification to that algorithm. <p> Therefore, this algorithm generates MFCS correctly. 3.3 Some Salient Features of the Apriori-gen Algorithm We first recall that itemsets are maintained as sequences in sorted lexicographical order, and the algorithm relies on this fact. The candidate generation algorithm is schematically as follows: Algorithm (from <ref> [3] </ref>): Candidate generation algorithm Input: L k , the set containing frequent itemsets found in pass k Output: new candidate set C k+1 1. call the join procedure (see below) 2. call the prune procedure (see below) The join procedure of the Apriori-gen algorithm is to combine two frequent k-itemsets, which <p> The join procedure works as follows: 7 Algorithm (from <ref> [3] </ref>): The join procedure of the Apriori-gen algorithm Input: L k , the set containing frequent itemsets found in pass k Output: preliminary candidate set C k+1 /* The itemsets in L k are sorted */ 1. for i from 1 to jL k 1j begin 2. for j from i <p> In other words, the supersets of an infrequent itemset are pruned. The procedure is specified by Algorithm (from <ref> [3] </ref>): The prune procedure of the Apriori-gen algorithm Input: preliminary candidate set C k+1 generated from the join procedure above Output: final candidate set C k+1 which does not contain any infrequent subset 1. for all itemsets c in C k+1 2. for all k-subsets s of c 3. if s <p> In this case it may not be worthwhile to maintain the MFCS, since there will not be many frequent itemsets to discover. In that case, we may simply count candidates of different sizes in one pass, as in <ref> [3] </ref> and [12]. The algorithm we have implemented is in fact an adaptive version of the algorithm described above. This adaptive version does not maintain the MFCS, when doing so would be counterproductive. This is also the algorithm whose performance is being evaluated in Section 4. <p> There can be no categorical answer, as this really depends on the distribution of the frequent and infrequent itemsets. However, according to both <ref> [3] </ref> and our experiments, a large fraction the 2-itemsets will usually be infrequent. These infrequent itemsets will cause MFCS to go down the levels very fast, allowing it to reach some maximal frequent itemsets after only a few passes. <p> For instance, in the experiment on database T20.I15.D100K (Figure 4), all maximal frequent itemsets containing up to 17 items are found in 3 passes only! 10 The performance evaluation presented compares our adaptive Pincer-Search algorithm to the Apriori algorithm <ref> [3] </ref>. <p> We simply used a link-list data structure to store the frequent set and the candidate set. Some of the execution time (for both algorithms), shown in Figure 3, is greater than the execution time shown in <ref> [3] </ref>. However, since both Apriori and Pincer-Search algorithms are using the same data structure, the comparison is fair. The databases used in performance evaluation, are the synthetic databases used in [3]. <p> Some of the execution time (for both algorithms), shown in Figure 3, is greater than the execution time shown in <ref> [3] </ref>. However, since both Apriori and Pincer-Search algorithms are using the same data structure, the comparison is fair. The databases used in performance evaluation, are the synthetic databases used in [3]. Also, as suggested by Ozden et al.'s [13], we used a one-dimensional array and a two-dimensional array to speed up the process of the first and the second pass correspondingly. The support counting phase runs very fast by using an array, since no searching is needed. <p> If the frequent itemsets do not have many common elements, the distribution is scattered. We will present experiments to examine the impact of the distribution type on the performance of the two algorithms. The number of the maximal frequent itemsets jLj is set to 2000, as in <ref> [3] </ref>, in the first set of experiments. The frequent itemsets found in this set of experiments are rather scattered. To produce databases having a concentrated distribution of the frequent itemsets, we adjust the parameter jLj to a smaller value. <p> Therefore, our algorithm does have the potential to benefit from this nonmonotonicity. 11 4.2 Experiments The test databases are generated synthetically by an algorithm designed by the IBM Quest project. The synthetic data generation procedure is described in detail in <ref> [3] </ref>, whose parameter settings we follow. The number of items N is set to 1000. jDj is the number of transactions. jT j is the average size of transactions. jIj is the average size of maximal frequent itemsets. Scattered Distributions. <p> One can expect even greater improvements when the average size of the maximal frequent itemsets is further increased. 12 13 14 5 Related Work There has been extensive research on designing association rule mining algorithms ([1] <ref> [3] </ref> [7] [9] [12] [16] [18]). Some papers concentrate on parallel algorithms ([4] [9] [16]). Other papers focus on mining generalized association rules ([6] [15]). Another direction, as described in [2] and [11], is to provide a unified model for the process of classification, association, and sequence rules discovery.
Reference: [4] <author> R. Agrawal and J. C. Shafer. </author> <title> Parallel Mining of association rules: Design, Implementation and Experience. </title> <institution> IBM Research Report RJ10004, </institution> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: If all maximal frequent itemsets are expected to be long (close to n in size) it seems efficient to search for them top-down. In a "pure" bottom-up approach, only Observation 1 above is used to prune candidates. This is the technique that existing algorithms ([3] <ref> [4] </ref> [6] [9] [11] [12] [15] [16] [17]) use to decrease the number of candidates. In a "pure" top-down approach, only Observation 2 is used to prune candidates.
Reference: [5] <author> D. Gunopulos, H. Mannila, and S. Saluja. </author> <title> Discovering all most specific sentences by randomized algorithm. </title> <booktitle> In Proc. International Conference of Database Theory, </booktitle> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: The discovery of frequent set is an important process in solving these problems. Our approach can be applied to solve these problems. A randomized algorithm for discovering the maximum frequent set was presented by Gunopulos et al. <ref> [5] </ref>. We present a deterministic algorithm for solving this problem. Reading the database repeatedly can be very time consuming. Attempts have been made to reduce the number of passes in which the database is read.
Reference: [6] <author> J. Han, Y. Fu. </author> <title> Discovery of multiple-level association rules from large databases. </title> <booktitle> In 21st VLDB, </booktitle> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: If all maximal frequent itemsets are expected to be long (close to n in size) it seems efficient to search for them top-down. In a "pure" bottom-up approach, only Observation 1 above is used to prune candidates. This is the technique that existing algorithms ([3] [4] <ref> [6] </ref> [9] [11] [12] [15] [16] [17]) use to decrease the number of candidates. In a "pure" top-down approach, only Observation 2 is used to prune candidates.
Reference: [7] <author> M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A. I. Berkamo. </author> <title> Finding interesting rules from large sets of discovered association rules. </title> <booktitle> In Proc. Third International Conference on Information and Knowledge Management, </booktitle> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: One can expect even greater improvements when the average size of the maximal frequent itemsets is further increased. 12 13 14 5 Related Work There has been extensive research on designing association rule mining algorithms ([1] [3] <ref> [7] </ref> [9] [12] [16] [18]). Some papers concentrate on parallel algorithms ([4] [9] [16]). Other papers focus on mining generalized association rules ([6] [15]). Another direction, as described in [2] and [11], is to provide a unified model for the process of classification, association, and sequence rules discovery.
Reference: [8] <author> T. M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 18, </volume> <year> 1982. </year>
Reference-contexts: Mannila and Toivonen [11] analyze the complexity of the bottom-up breadth-first search style algorithms. As our algorithm does not fit in this model, their complexity low bound does not apply to it. Our work was inspired by the notion of version space in Mitchell's machine learning paper <ref> [8] </ref>.
Reference: [9] <author> A. Mueller. </author> <title> Fast sequential and parallel algorithms for association rule mining: A comparison. </title> <type> Technical Report No. </type> <institution> CS-TR-3515 of CS Department, University of Maryland-College Park. </institution>
Reference-contexts: In this paper, we apply the MFCS idea to association rules mining and to the discovery of frequent itemsets in market basket data. Popular benchmark databases designed by Agrawal and Srikant [3] have been used in <ref> [9] </ref>, [16], and [18]: we use these benchmarks to evaluate the performance of our algorithm. In most cases, our algorithm not only reduces the number of passes of reading the database but also can reduce the number of candidates (for whom support is counted). <p> If all maximal frequent itemsets are expected to be long (close to n in size) it seems efficient to search for them top-down. In a "pure" bottom-up approach, only Observation 1 above is used to prune candidates. This is the technique that existing algorithms ([3] [4] [6] <ref> [9] </ref> [11] [12] [15] [16] [17]) use to decrease the number of candidates. In a "pure" top-down approach, only Observation 2 is used to prune candidates. <p> One can expect even greater improvements when the average size of the maximal frequent itemsets is further increased. 12 13 14 5 Related Work There has been extensive research on designing association rule mining algorithms ([1] [3] [7] <ref> [9] </ref> [12] [16] [18]). Some papers concentrate on parallel algorithms ([4] [9] [16]). Other papers focus on mining generalized association rules ([6] [15]). Another direction, as described in [2] and [11], is to provide a unified model for the process of classification, association, and sequence rules discovery. <p> One can expect even greater improvements when the average size of the maximal frequent itemsets is further increased. 12 13 14 5 Related Work There has been extensive research on designing association rule mining algorithms ([1] [3] [7] <ref> [9] </ref> [12] [16] [18]). Some papers concentrate on parallel algorithms ([4] [9] [16]). Other papers focus on mining generalized association rules ([6] [15]). Another direction, as described in [2] and [11], is to provide a unified model for the process of classification, association, and sequence rules discovery. The discovery of frequent set is an important process in solving these problems. <p> We present a deterministic algorithm for solving this problem. Reading the database repeatedly can be very time consuming. Attempts have been made to reduce the number of passes in which the database is read. Some ([3] [12] <ref> [9] </ref>) proposed combining candidates from different levels to reduce the number of database readings. However, this technique is only useful in the later passes of the frequent itemsets discovery process. Others, like Partition [16] and Sampling [18]), proposed effective ways to reduce the I/O time.
Reference: [10] <author> H. Mannila and H. Toivonen. </author> <booktitle> In Proc. Second International Conference on Knowledge Discovery and Data Mining (KDD'96) AAAI Press. </booktitle> <year> 1996. </year>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as association rules (e.g., [3] [12]), strong rules (e.g., [14]), episodes (e.g., <ref> [10] </ref>) and minimal keys (e.g., [11]). Typical algorithms for finding the frequent set, i.e., the set of all frequent itemsets (e.g., [3] [12]), operate in a bottom-up breadth-first fashion.
Reference: [11] <author> H. Mannila and H. Toivonen. </author> <title> Levelwise search and borders of theories in knowledge discovery. </title> <institution> Technical Report C-1997-8 of the Department of Computer Science, University of Helsinki, Finland, </institution> <year> 1997. </year>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as association rules (e.g., [3] [12]), strong rules (e.g., [14]), episodes (e.g., [10]) and minimal keys (e.g., <ref> [11] </ref>). Typical algorithms for finding the frequent set, i.e., the set of all frequent itemsets (e.g., [3] [12]), operate in a bottom-up breadth-first fashion. <p> If all maximal frequent itemsets are expected to be long (close to n in size) it seems efficient to search for them top-down. In a "pure" bottom-up approach, only Observation 1 above is used to prune candidates. This is the technique that existing algorithms ([3] [4] [6] [9] <ref> [11] </ref> [12] [15] [16] [17]) use to decrease the number of candidates. In a "pure" top-down approach, only Observation 2 is used to prune candidates. <p> Some papers concentrate on parallel algorithms ([4] [9] [16]). Other papers focus on mining generalized association rules ([6] [15]). Another direction, as described in [2] and <ref> [11] </ref>, is to provide a unified model for the process of classification, association, and sequence rules discovery. The discovery of frequent set is an important process in solving these problems. Our approach can be applied to solve these problems. <p> The Pincer-Search algorithm presents a new approach that can reduce both I/O and CPU time. This algorithm is useful in the early passes and is good for cases when the maximal frequent itemsets are long. Mannila and Toivonen <ref> [11] </ref> analyze the complexity of the bottom-up breadth-first search style algorithms. As our algorithm does not fit in this model, their complexity low bound does not apply to it. Our work was inspired by the notion of version space in Mitchell's machine learning paper [8].
Reference: [12] <author> H. Mannila, H. Toivonen, and A. I. Verkamo. </author> <title> Improved methods for finding association rules. </title> <booktitle> In Proc. AAAI Workshop on Knowledge Discovery, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as association rules (e.g., [3] <ref> [12] </ref>), strong rules (e.g., [14]), episodes (e.g., [10]) and minimal keys (e.g., [11]). Typical algorithms for finding the frequent set, i.e., the set of all frequent itemsets (e.g., [3] [12]), operate in a bottom-up breadth-first fashion. <p> itemsets, and the term "occurs," we get the key components of different data mining problems such as association rules (e.g., [3] <ref> [12] </ref>), strong rules (e.g., [14]), episodes (e.g., [10]) and minimal keys (e.g., [11]). Typical algorithms for finding the frequent set, i.e., the set of all frequent itemsets (e.g., [3] [12]), operate in a bottom-up breadth-first fashion. In other words, the computation starts from frequent 1-itemsets (minimal length frequent itemsets at the bottom) and then extends one level up in every pass until all maximal (length) frequent itemsets are discovered. <p> In this paper, we present a novel Pincer-Search algorithm which searches for MFS from both bottom-up and top-down directions. This algorithm performs well even when the maximal frequent itemsets are long. In our algorithm, the bottom-up search is similar to the Apriori [3] and the OCD <ref> [12] </ref> algorithms. However, the top-down search is unique. The top-down search is implemented efficiently by introducing a set that we call the maximum-frequent-candidate-set or MFCS. <p> If all maximal frequent itemsets are expected to be long (close to n in size) it seems efficient to search for them top-down. In a "pure" bottom-up approach, only Observation 1 above is used to prune candidates. This is the technique that existing algorithms ([3] [4] [6] [9] [11] <ref> [12] </ref> [15] [16] [17]) use to decrease the number of candidates. In a "pure" top-down approach, only Observation 2 is used to prune candidates. <p> In this case it may not be worthwhile to maintain the MFCS, since there will not be many frequent itemsets to discover. In that case, we may simply count candidates of different sizes in one pass, as in [3] and <ref> [12] </ref>. The algorithm we have implemented is in fact an adaptive version of the algorithm described above. This adaptive version does not maintain the MFCS, when doing so would be counterproductive. This is also the algorithm whose performance is being evaluated in Section 4. <p> One can expect even greater improvements when the average size of the maximal frequent itemsets is further increased. 12 13 14 5 Related Work There has been extensive research on designing association rule mining algorithms ([1] [3] [7] [9] <ref> [12] </ref> [16] [18]). Some papers concentrate on parallel algorithms ([4] [9] [16]). Other papers focus on mining generalized association rules ([6] [15]). Another direction, as described in [2] and [11], is to provide a unified model for the process of classification, association, and sequence rules discovery. <p> We present a deterministic algorithm for solving this problem. Reading the database repeatedly can be very time consuming. Attempts have been made to reduce the number of passes in which the database is read. Some ([3] <ref> [12] </ref> [9]) proposed combining candidates from different levels to reduce the number of database readings. However, this technique is only useful in the later passes of the frequent itemsets discovery process. Others, like Partition [16] and Sampling [18]), proposed effective ways to reduce the I/O time.
Reference: [13] <author> B. Ozden, S. Ramaswamy. and A. Silberschatz. </author> <title> Cyclic Association Rules. </title> <type> Personal communication. </type>
Reference-contexts: However, since both Apriori and Pincer-Search algorithms are using the same data structure, the comparison is fair. The databases used in performance evaluation, are the synthetic databases used in [3]. Also, as suggested by Ozden et al.'s <ref> [13] </ref>, we used a one-dimensional array and a two-dimensional array to speed up the process of the first and the second pass correspondingly. The support counting phase runs very fast by using an array, since no searching is needed.
Reference: [14] <author> G. Piatetsky-Shapiro. </author> <title> Discovery, analysis, and presentation of strong rules. In Knowledge Discovery in Databases, </title> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: Depending on the semantics attached to the input database, the frequent itemsets, and the term "occurs," we get the key components of different data mining problems such as association rules (e.g., [3] [12]), strong rules (e.g., <ref> [14] </ref>), episodes (e.g., [10]) and minimal keys (e.g., [11]). Typical algorithms for finding the frequent set, i.e., the set of all frequent itemsets (e.g., [3] [12]), operate in a bottom-up breadth-first fashion.
Reference: [15] <author> R. Srikant and R. Agrawal. </author> <title> Mining generalized association rules. </title> <booktitle> In 21st VLDB, </booktitle> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: In a "pure" bottom-up approach, only Observation 1 above is used to prune candidates. This is the technique that existing algorithms ([3] [4] [6] [9] [11] [12] <ref> [15] </ref> [16] [17]) use to decrease the number of candidates. In a "pure" top-down approach, only Observation 2 is used to prune candidates. <p> Some papers concentrate on parallel algorithms ([4] [9] [16]). Other papers focus on mining generalized association rules ([6] <ref> [15] </ref>). Another direction, as described in [2] and [11], is to provide a unified model for the process of classification, association, and sequence rules discovery. The discovery of frequent set is an important process in solving these problems. Our approach can be applied to solve these problems.
Reference: [16] <author> A. Sarasere, E. Omiecinsky, and S. Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In Proc. 21st VLDB, </booktitle> <month> Sept. </month> <year> 1995. </year> <month> 16 </month>
Reference-contexts: In this paper, we apply the MFCS idea to association rules mining and to the discovery of frequent itemsets in market basket data. Popular benchmark databases designed by Agrawal and Srikant [3] have been used in [9], <ref> [16] </ref>, and [18]: we use these benchmarks to evaluate the performance of our algorithm. In most cases, our algorithm not only reduces the number of passes of reading the database but also can reduce the number of candidates (for whom support is counted). <p> In a "pure" bottom-up approach, only Observation 1 above is used to prune candidates. This is the technique that existing algorithms ([3] [4] [6] [9] [11] [12] [15] <ref> [16] </ref> [17]) use to decrease the number of candidates. In a "pure" top-down approach, only Observation 2 is used to prune candidates. <p> One can expect even greater improvements when the average size of the maximal frequent itemsets is further increased. 12 13 14 5 Related Work There has been extensive research on designing association rule mining algorithms ([1] [3] [7] [9] [12] <ref> [16] </ref> [18]). Some papers concentrate on parallel algorithms ([4] [9] [16]). Other papers focus on mining generalized association rules ([6] [15]). Another direction, as described in [2] and [11], is to provide a unified model for the process of classification, association, and sequence rules discovery. <p> One can expect even greater improvements when the average size of the maximal frequent itemsets is further increased. 12 13 14 5 Related Work There has been extensive research on designing association rule mining algorithms ([1] [3] [7] [9] [12] <ref> [16] </ref> [18]). Some papers concentrate on parallel algorithms ([4] [9] [16]). Other papers focus on mining generalized association rules ([6] [15]). Another direction, as described in [2] and [11], is to provide a unified model for the process of classification, association, and sequence rules discovery. The discovery of frequent set is an important process in solving these problems. <p> Some ([3] [12] [9]) proposed combining candidates from different levels to reduce the number of database readings. However, this technique is only useful in the later passes of the frequent itemsets discovery process. Others, like Partition <ref> [16] </ref> and Sampling [18]), proposed effective ways to reduce the I/O time. However, they are still inefficient when the maximal frequent itemsets are long. The Pincer-Search algorithm presents a new approach that can reduce both I/O and CPU time.
Reference: [17] <author> H. Toivonen. </author> <title> Discovery of frequent patterns in large data collections. </title> <institution> Technical Report A-1996-5 of the Department of Computer Science, University of Helsinki, Finland, </institution> <year> 1996. </year>
Reference-contexts: In a "pure" bottom-up approach, only Observation 1 above is used to prune candidates. This is the technique that existing algorithms ([3] [4] [6] [9] [11] [12] [15] [16] <ref> [17] </ref>) use to decrease the number of candidates. In a "pure" top-down approach, only Observation 2 is used to prune candidates.
Reference: [18] <author> H. Toivonen. </author> <title> Sampling large databases for association rules. </title> <booktitle> In Proc. 22nd VLDB 1996. </booktitle> <pages> 17 </pages>
Reference-contexts: In this paper, we apply the MFCS idea to association rules mining and to the discovery of frequent itemsets in market basket data. Popular benchmark databases designed by Agrawal and Srikant [3] have been used in [9], [16], and <ref> [18] </ref>: we use these benchmarks to evaluate the performance of our algorithm. In most cases, our algorithm not only reduces the number of passes of reading the database but also can reduce the number of candidates (for whom support is counted). <p> One can expect even greater improvements when the average size of the maximal frequent itemsets is further increased. 12 13 14 5 Related Work There has been extensive research on designing association rule mining algorithms ([1] [3] [7] [9] [12] [16] <ref> [18] </ref>). Some papers concentrate on parallel algorithms ([4] [9] [16]). Other papers focus on mining generalized association rules ([6] [15]). Another direction, as described in [2] and [11], is to provide a unified model for the process of classification, association, and sequence rules discovery. <p> Some ([3] [12] [9]) proposed combining candidates from different levels to reduce the number of database readings. However, this technique is only useful in the later passes of the frequent itemsets discovery process. Others, like Partition [16] and Sampling <ref> [18] </ref>), proposed effective ways to reduce the I/O time. However, they are still inefficient when the maximal frequent itemsets are long. The Pincer-Search algorithm presents a new approach that can reduce both I/O and CPU time.
References-found: 18

