URL: http://www-lsi.upc.es/~talavera/papers/eurovav97.ps.gz
Refering-URL: http://www-lsi.upc.es/~talavera/papers.html
Root-URL: 
Email: ftalavera,iag@lsi.upc.es  
Title: Proceedings of the Fourth European Symposium on the Validation and Verification of Knowledge Based Systems,
Author: Luis Talavera and Ulises Cortes 
Keyword: Bias selection, hypothesis validation, knowledge discovery.  
Address: Campus Nord, M odul C5, Jordi Girona 3 08034 Barcelona, Catalonia, Spain  
Affiliation: Universitat Polit ecnica de Catalunya Departament de Llenguatges i Sistemes Inform atics  
Abstract: This paper approaches the importance of bias selection in the context of validating Knowledge Bases (KB) obtained by inductive learning systems. We propose a framework for automatic validation of induced KBs based on the capability of shifting the bias in the inductive learning system. We claim that this framework is useful not only when the system has to validate its own results, but also when human experts are available to perform the validation process. Experiments are made using accuracy in attribute prediction as performance goal. The unsupervised inductive learning system ISAAC [TC96] is coupled with the wrapper method [JKP94] to search for the best bias. The results support the proposed ideas and suggest some future work that seems interesting from both KB validation and Machine Learning points of view. 
Abstract-found: 1
Intro-found: 1
Reference: [CKS + 88] <author> P. Cheeseman, J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman. </author> <title> AutoClass: A bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Workshop on Machine Learning, </booktitle> <pages> pages 54-64. </pages> <publisher> Morgan Kauffmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: In unsupervised learning, there are no labels available and algorithms must decide the domain structure. The very first approaches to this problem assumed that the main goal of unsupervised learning was to discover the underlying concepts on the dataset to give a conceptual interpretation of data [Mic83]. More recently <ref> [Fis87, CKS + 88] </ref> an alternative performance task has been proposed based on that explained for supervised systems. Unsupervised systems support prediction not only of a single class attribute, but of every attribute present in data. <p> As ISAAC only works with discrete attributes, this probabilities are computed from the relative frequencies of values. Other well-known unsupervised systems such as Cobweb [Fis87], Witt [HB89] or Autoclass <ref> [CKS + 88] </ref> use this sort of representation too. ISAAC allows the user to specify some preferences about the hierarchies it produces.
Reference: [Fis87] <author> D. H. Fisher. </author> <title> Knowledge acquisition via conceptual clustering. </title> <type> PhD thesis, </type> <institution> University of California, Irvine, </institution> <year> 1987. </year>
Reference-contexts: In unsupervised learning, there are no labels available and algorithms must decide the domain structure. The very first approaches to this problem assumed that the main goal of unsupervised learning was to discover the underlying concepts on the dataset to give a conceptual interpretation of data [Mic83]. More recently <ref> [Fis87, CKS + 88] </ref> an alternative performance task has been proposed based on that explained for supervised systems. Unsupervised systems support prediction not only of a single class attribute, but of every attribute present in data. <p> As ISAAC only works with discrete attributes, this probabilities are computed from the relative frequencies of values. Other well-known unsupervised systems such as Cobweb <ref> [Fis87] </ref>, Witt [HB89] or Autoclass [CKS + 88] use this sort of representation too. ISAAC allows the user to specify some preferences about the hierarchies it produces. <p> In fact, several proposals combining this two measures have been made [Jon85, GC85] and some have been adapted by the ML community, being the Cobweb system the most relevant example <ref> [Fis87] </ref>. Instead of trying to maximize both measures, ISAAC favors one of the measures over the other. Since each of the measures is biasing the selection of partitions in a different direction as regards generality of partition, this favors a different level of generality.
Reference: [Fis95] <author> D. Fisher. </author> <title> Optimization and simplification of hierarchical clusterings. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 118-123, </pages> <address> Montreal, Quebec, Canada, 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Averages and standard deviations over 10 runs of 10-fold cross-validation. being predicted. The results show that APIsaac achieves similar and even higher accuracies that those found in the literature for the same datasets <ref> [Fis95] </ref>. Low standard deviations in table 1 show that results are very homogeneous suggesting that the bias search procedure is fairly robust. <p> Accuracy does not have to be necessarily the only dimension to evaluate the quality of an induced KB. Size of the induced KB may be an important factor too, especially if reducing the KB size does Inductive hypothesis validation and bias selection in unsupervised learning not decrease accuracy <ref> [Fis95] </ref>. Moreover, efficiency of an induced KB in prediction may be a critical dimension for some applications. The attentional mechanism of ISAAC lends itself to achieving more efficient KBs, given that efficiency is a function of the number of attributes used in classification.
Reference: [GC85] <author> M. A. Gluck and J. E. Corter. </author> <title> Information, uncertainty and the utility of categories. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 283-287. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1985. </year>
Reference-contexts: Since both measures work in an inverse manner to each other, they can be used to search for good partitions by seeking partitions which score highly using both. In fact, several proposals combining this two measures have been made <ref> [Jon85, GC85] </ref> and some have been adapted by the ML community, being the Cobweb system the most relevant example [Fis87]. Instead of trying to maximize both measures, ISAAC favors one of the measures over the other.
Reference: [GD95] <editor> D. F. Gordon and M. Desjardins. </editor> <title> Evaluation and selection of biases in machine learning. </title> <booktitle> Machine Learning, </booktitle> <address> 20(1-2):5-22, </address> <year> 1995. </year>
Reference-contexts: this capability, the outlined comparison procedure can not be considered a real validation of the KB, but just an evaluation. 3 Inductive bias selection and KB validation From the ML point of view, a bias can be defined as any factor that influences the definition or selection of inductive hypothesis <ref> [GD95] </ref>. We can divide biases along several dimensions, but for the rest of the discussion, suffice to say that there are two types of bias: representational and procedural.
Reference: [HB89] <author> S. J. Hanson and M. Bauer. </author> <title> Conceptual clustering, categorization and polymorphy. </title> <journal> Machine Learning, </journal> (3):343-372, 1989. 
Reference-contexts: As ISAAC only works with discrete attributes, this probabilities are computed from the relative frequencies of values. Other well-known unsupervised systems such as Cobweb [Fis87], Witt <ref> [HB89] </ref> or Autoclass [CKS + 88] use this sort of representation too. ISAAC allows the user to specify some preferences about the hierarchies it produces.
Reference: [HM91] <author> T. Hoppe and P. Meseguer. </author> <title> On the terminology of VVT. </title> <booktitle> In Proceedings of the European Workshop on the Verification and Validation of KBS, EUROVAV91, </booktitle> <pages> pages 3-13, </pages> <address> Cambridge, England, </address> <year> 1991. </year>
Reference-contexts: On the other hand, validation implies to asses if the level of quality of this knowledge is enough to achieve a performance goal. Here, we use the term validation in the same sense some authors refer to as functional validation <ref> [HM91] </ref>. While validating an induced KB, the performance goal may be evaluated with either an absolute or a relative performance measure. If an absolute quality measure is used, some performance threshold must be known.
Reference: [JKP94] <author> G. H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 121-129. </pages> <publisher> Morgan Kauffmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: Then, the relationship between inductive biases and performance goals is established, thus introducing the utility of bias selection in inductive hypothesis validation. A general framework which incorporate these ideas is next proposed. Experiments are made using the ISAAC [TC96] unsupervised system and the wrapper model <ref> [JKP94] </ref> as a bias search method to verify this claims. Finally, some conclusions and future work are presented. 2 Validation of induced KBs In order to asses the quality of induced knowledge, inductive learning systems need some performance goal to be specified. <p> In this section, an automatic parameter setting version of ISAAC which we will call APIsaac will be presented. APIsaac is based upon previous work by McEwen (1996) who used the wrapper method <ref> [JKP94] </ref> to give the system the capability of decide the parameter setting automatically. Formerly used to find an optimal subset of features, the wrapper method can also serve as a means to automatically search for the best bias of an inductive learning system for a given dataset [KJ95]. <p> This leads to some well-known problems. A technique which partially addresses the problems with the hill-climbing strategy is the best-first search. Several implementations of the wrapper method have employed the best-first search strategy <ref> [JKP94, KJ95, Koh95] </ref> to assure a reasonable approximation to the optimal solution. This technique is similar to hill-climbing in that it works by continually expanding the highest performing state. There are however, two differences between this technique and the hill-climbing technique.
Reference: [Jon85] <author> G. V. Jones. </author> <title> Identifying basic categories. </title> <journal> Psychological Bulletin, </journal> (94):423-428, 1985. Inductive hypothesis validation and bias selection in unsupervised learning 
Reference-contexts: Since both measures work in an inverse manner to each other, they can be used to search for good partitions by seeking partitions which score highly using both. In fact, several proposals combining this two measures have been made <ref> [Jon85, GC85] </ref> and some have been adapted by the ML community, being the Cobweb system the most relevant example [Fis87]. Instead of trying to maximize both measures, ISAAC favors one of the measures over the other.
Reference: [KJ95] <author> R. Kohavi and G. H. John. </author> <title> Automatic parameter selection by minimizing estimated error. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <address> San Francisco, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Formerly used to find an optimal subset of features, the wrapper method can also serve as a means to automatically search for the best bias of an inductive learning system for a given dataset <ref> [KJ95] </ref>. The method takes into account the properties of the training data set and their interaction with the system biases. It is possible to include these factors because this method uses the inductive learning system itself as part of the evaluation function. <p> This leads to some well-known problems. A technique which partially addresses the problems with the hill-climbing strategy is the best-first search. Several implementations of the wrapper method have employed the best-first search strategy <ref> [JKP94, KJ95, Koh95] </ref> to assure a reasonable approximation to the optimal solution. This technique is similar to hill-climbing in that it works by continually expanding the highest performing state. There are however, two differences between this technique and the hill-climbing technique.
Reference: [Kod88] <author> Y. Kodratoff. </author> <title> Introduction to machine learning. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: This feature along with the assumption that initial knowledge is correct, allows the semantic correctness of the knowledge form to be assumed. On the other hand, inductive learning systems transform their inputs by means of generalizations which may be not valid <ref> [Kod88] </ref>. Therefore, one can check an induced KB for syntactic anomalies (such as consistency or completeness) but if an erroneous generalization is done, this KB may be completely inadequate. This is not an specific problem of induced knowledge, but a general problem in KB validation.
Reference: [Koh95] <author> R. Kohavi. </author> <title> The power of decision tables. </title> <booktitle> In Machine Learning: ECML-95, Lecture notes in artificial intelligence, </booktitle> <pages> pages 174-189. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: This leads to some well-known problems. A technique which partially addresses the problems with the hill-climbing strategy is the best-first search. Several implementations of the wrapper method have employed the best-first search strategy <ref> [JKP94, KJ95, Koh95] </ref> to assure a reasonable approximation to the optimal solution. This technique is similar to hill-climbing in that it works by continually expanding the highest performing state. There are however, two differences between this technique and the hill-climbing technique.
Reference: [LdM91] <author> R. L opez de Mantaras. </author> <title> A distance based attribute selection measure for decision tree induction. </title> <journal> Machine Learning, </journal> (6):81-92, 1991. 
Reference-contexts: New abstract concept descriptions are generated so that they contain only the subset of attributes considered useful. This is done in the Reflection stage, which uses a relevance measure formerly used in attribute selection in decision trees, called the distance measure <ref> [LdM91] </ref>, and the current set of hypothesis, to compute a relevance for each individual attribute. The attributes which do not score high enough are declared irrelevant and are discarded for the next generalization step. The useful attributes are used in the Refinement stage, weighting the CN and CS computation.
Reference: [McE96] <author> M. McEwen. </author> <title> ML experiments with an accuracy-efficiency trade off in unsupervised attribute prediction. </title> <type> Master's thesis, </type> <institution> University of Aberdeen, Computer Science Department, </institution> <year> 1996. </year>
Reference-contexts: The attentional mechanism of ISAAC lends itself to achieving more efficient KBs, given that efficiency is a function of the number of attributes used in classification. We think that exploring the relationship between accuracy and efficiency in unsupervised attribute prediction <ref> [McE96] </ref> still deserves further investigation. Finally, we have to point out that, even though the approach proposed in this paper is general enough to cope with a variety of algorithms and performance tasks, it is likely to be difficult to use in some situations.
Reference: [Mic83] <author> R. S. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial intelligence approach, </booktitle> <pages> pages 83-134. </pages> <publisher> Morgan Kauffmann, </publisher> <address> Los Altos, CA, </address> <year> 1983. </year>
Reference-contexts: In unsupervised learning, there are no labels available and algorithms must decide the domain structure. The very first approaches to this problem assumed that the main goal of unsupervised learning was to discover the underlying concepts on the dataset to give a conceptual interpretation of data <ref> [Mic83] </ref>. More recently [Fis87, CKS + 88] an alternative performance task has been proposed based on that explained for supervised systems. Unsupervised systems support prediction not only of a single class attribute, but of every attribute present in data.
Reference: [MP96] <author> P. Meseguer and A. D. Preece. </author> <title> Assessing the role of formal specifications in verification and validation of knowledge based systems. </title> <booktitle> In Proceedings of the 3rd. IFIP International Conference on Achieving Quality in Software, AQuIS96, </booktitle> <pages> pages 317-328. </pages> <publisher> Chapman & Hall, </publisher> <year> 1996. </year>
Reference-contexts: This is not an specific problem of induced knowledge, but a general problem in KB validation. Some authors call this checking procedure static verification of a KB <ref> [MP96] </ref>. Since static verification is not aimed at detecting semantic errors, usually an inspection technique, which involves the active participation of human experts is used.
Reference: [Mur82] <author> G. L. Murphy. </author> <title> Cue validity and levels of categorization. </title> <journal> Psychological Bulletin, </journal> (91):174-177, 1982. 
Reference-contexts: The CN and CS measures retain the properties of cue validity and category validity in that CS increases going up a hierarchy towards more general concepts, and CN increases going down a hierarchy towards more specific concepts <ref> [Mur82] </ref>. Since both measures work in an inverse manner to each other, they can be used to search for good partitions by seeking partitions which score highly using both.
Reference: [Qui83] <author> J. R. Quinlan. </author> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial intelligence approach, </booktitle> <pages> pages 463-482. </pages> <publisher> Morgan Kauffmann, </publisher> <address> Los Altos, CA, </address> <year> 1983. </year>
Reference-contexts: This subsets are usually called the training and test sets respectively and are widely used, for example, in decision tree evaluation <ref> [Qui83] </ref>. In unsupervised learning, there are no labels available and algorithms must decide the domain structure. The very first approaches to this problem assumed that the main goal of unsupervised learning was to discover the underlying concepts on the dataset to give a conceptual interpretation of data [Mic83].
Reference: [RMG + 76] <author> E. Rosch, C. B. Mervis, W. D. Gray, D. M. Johnson, and P. Boyes-Braem. </author> <title> Basic objects in natural categories. </title> <journal> Cognitive Psychology, </journal> (8):573-605, 1976. 
Reference-contexts: The CN and CS measures can be also interpreted as a function of cue validity and category validity measures, which have their origin in Cognitive Psychology <ref> [RMG + 76] </ref>. These measures were supposed to be capable of predicting the so called basic level that is, the level in which knowledge is maximized.
Reference: [SM81] <author> E. E. Smith and D. L. Medin. </author> <title> Categories and concepts. </title> <publisher> Harvard University Press, </publisher> <address> Cam-bridge,MA, </address> <year> 1981. </year>
Reference-contexts: If this condition does not hold, a poor performing bias may be selected. Inductive hypothesis validation and bias selection in unsupervised learning that induces knowledge in the form of probabilistic concepts <ref> [SM81] </ref>, which in turn can be arranged in a concept hierarchy. In a probabilistic concept representation, a conditional probability of the form P (A i = V ij j C) is associated with each attribute value V ij of a concept definition.
Reference: [Tal96] <author> L. Talavera. </author> <title> Reflexi on y refinamiento del conocimiento en la formaci on de conceptos. </title> <type> Master's thesis, </type> <institution> Facultat d'Informatica de Barcelona, UPC, </institution> <year> 1996. </year>
Reference-contexts: ISAAC <ref> [TC96, Tal96] </ref> is an unsupervised learning system 1 Of course, we are assuming that the different biases of the system cover a wide spectrum of results. If this condition does not hold, a poor performing bias may be selected.
Reference: [TC96] <author> L. Talavera and U. Cortes. </author> <title> Generalizaci on y atencion selectiva para la formacion de conceptos. </title> <booktitle> In V Congreso Iberoamericano de Inteligencia Artificial, IBERAMIA96, </booktitle> <pages> pages 320-330, </pages> <address> Cholula, Puebla, Mexico, 1996. </address> <publisher> Limusa, Mexico. </publisher>
Reference-contexts: Then, the relationship between inductive biases and performance goals is established, thus introducing the utility of bias selection in inductive hypothesis validation. A general framework which incorporate these ideas is next proposed. Experiments are made using the ISAAC <ref> [TC96] </ref> unsupervised system and the wrapper model [JKP94] as a bias search method to verify this claims. Finally, some conclusions and future work are presented. 2 Validation of induced KBs In order to asses the quality of induced knowledge, inductive learning systems need some performance goal to be specified. <p> ISAAC <ref> [TC96, Tal96] </ref> is an unsupervised learning system 1 Of course, we are assuming that the different biases of the system cover a wide spectrum of results. If this condition does not hold, a poor performing bias may be selected.
References-found: 22

