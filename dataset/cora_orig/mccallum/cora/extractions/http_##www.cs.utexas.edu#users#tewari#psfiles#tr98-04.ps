URL: http://www.cs.utexas.edu/users/tewari/psfiles/tr98-04.ps
Refering-URL: http://www.cs.utexas.edu/users/tewari/papers.html
Root-URL: 
Email: E-mail: ftewari, dahlin, vin, jkayg@cs.utexas.edu  
Title: Beyond Hierarchies: Design Considerations for Distributed Caching on the Internet  
Author: Renu Tewari, Michael Dahlin, Harrick M. Vin and Jonathan S. Kay 
Address: Austin, TX 78712-1188  
Affiliation: Department of Computer Sciences The University of Texas at Austin  
Pubnum: UTCS Technical Report: TR98-04  
Abstract: In this paper, we examine several distributed caching strategies to improve the response time for accessing data over the Internet. By studying several Internet caches and workloads, we derive four basic design principles for large scale distributed caches: (1) minimize the number of hops to locate and access data, (2) do not slow down misses, (3) share data among many caches, and (4) cache data close to clients. Although these principles are simple, they are often violated in traditional cache hierarchies. Based on these principles, we have designed, simulated, and implemented two strategies for organizing distributed caches. First, we separate data paths from metadata paths and use location hints to allow caches to directly access remote copies of data. Second, we use two simple push caching algorithms to move data near clients. Together, these strategies provide response time speedups of 1.27 to 2.43 compared to a traditional three-level data cache hierarchy for a range of trace workloads and simulated environments.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Anderson, M. Dahlin, J. Neefe, D. Patterson, D. Roselli, and R. Wang. </author> <title> Serverless Network File Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(1):4179, </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: Several other cache systems use metadata directories or multicast to locate data and then allow direct cache-to-cache data transfers. The primary differences among these schemes is how they structure their metadata directories. In local area networks, cooperative caching <ref> [8, 1] </ref> and global shared memory [11] hash a global directory across a set of nodes, and clients access the distributed global directory with network messages; in a LAN, the cost of this extra network hop is acceptable.
Reference: [2] <author> M. Arlitt and C. Williamson. </author> <title> Web Server Workload Characterization: The Search for Invariants. </title> <booktitle> In Proceedings of the SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1996. </year> <note> http://www.cs.usask.ca/projects/discus/discus reports.html. </note>
Reference-contexts: The caches then send the data down the hierarchy to the client, and each cache along the path caches the data. Unfortunately, these hierarchies of data caches often achieve modest hit rates <ref> [2, 10, 15, 18] </ref>, can yield poor response times on a cache hit [26, 32], and can slow down cache misses. <p> As we describe in detail in Section 3.2.1, our system compresses hints to 16-byte, fixed-sized records. At this size, each hint is almost three orders of magnitude smaller than an average 10 KB data object stored in a cache <ref> [2] </ref>. <p> This section considers other areas of work relevant to understanding our results. Several studies have examined Internet workloads in depth with the goal of understanding how to improve performance. Arlitt and Williamson <ref> [2] </ref> examined basic workload characteristics for six servers. Duska et. al [10] examined workloads for several 3-week traces, Gribble and Bewer [18] examined a large trace of Berkeley HomeIP users.
Reference: [3] <author> G. Banga and P. Druschel. </author> <title> Measuring the Capacity of a Web Server. </title> <booktitle> In Proceedings of the USENIX Symposium on Internet Technologies and Systems, </booktitle> <month> December </month> <year> 1997. </year>
Reference-contexts: Note that, in our experiments, the caches were idle other than our requests, which were made one at a time. If the caches were heavily loaded, queuing delays and implementation inefficiencies of the caches <ref> [26, 3] </ref> might significantly increase the per-hop costs we observe. Busy nodes would probably increase the importance of reducing the number of hops in a cache system. standard three level data hierarchy.
Reference: [4] <author> M. </author> <title> Blaze. Caching in Large-Scale Distributed File Systems. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: These studies suggest that long network round trip latencies may be a significant factor. Hierarchical caching has been examined in the context of file systems <ref> [4, 33, 16] </ref>. Muntz and Honeyman [28] concluded that the additional hops in such a system often more than offset improvements in hit rate and characterized the extra level of cache as a delay server.
Reference: [5] <author> A. Chankhunthod, P. Danzig, C. Neerdaels, M. Schwartz, and K. Worrell. </author> <title> A Hierarchical Internet Object Cache. </title> <booktitle> In Proceedings of the 1996 USENIX Technical Conference, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: caches (Section 2.2) Separate data paths and metadata paths, location-hints (Section 3) Cache data close to clients (Section 2.1) Push caching (Section 4) Table 1: Design principles and strategies for distributed caching in the Internet 2.1 Access costs in a traditional cache hierarchy Traditional hierarchical cache architectures such as Harvest <ref> [5] </ref> or Squid [39] define parent-child relationships among caches. Each cache in the hierarchy is shared by a group of clients or a group of children caches. <p> The proxies then send the data down the hierarchy and each cache along the path caches the data. Although hierarchies such as Harvest and Squid were designed under the assumption that caches could be layered without adding much delay <ref> [5] </ref>, we hypothesize that two aspects of this architecture as applied to Internet caches can significantly limit performance. First, the cost of accessing a series of caches in the hierarchy adds significant store-and-forward delays to higher-level cache hits and to cache misses [28]. <p> system [10, 18], allowing servers to send new data to cache systems before any nodes in the system request the data, providing better cache consistency [21, 25, 41] to reduce the number of requests marked as uncachable, caching dynamically generated results [36], dynamically replicating servers [37], and negative result caching <ref> [27, 5] </ref>. <p> Rousskov [32] and Maltzahn et. al [26] measure the performance of cache servers deployed in the field and examine why the performance of these servers has been worse than laboratory tests predicted <ref> [5] </ref>. These studies suggest that long network round trip latencies may be a significant factor. Hierarchical caching has been examined in the context of file systems [4, 33, 16]. <p> Hierarchical caching has been more widely accepted on the Internet, and it is common in areas with low-speed connections to the Internet [29, 35]. In addition, many sites add proxy caching to their corporate firewalls. A widely deployed and studied system has been the publicly-available Harvest cache <ref> [5] </ref> and its successor, the Squid cache [39]. A key motivation of our design is that we disagree with the conclusion that cache hierarchies do not significantly reduce cache performance on cache misses [5]. <p> A widely deployed and studied system has been the publicly-available Harvest cache <ref> [5] </ref> and its successor, the Squid cache [39]. A key motivation of our design is that we disagree with the conclusion that cache hierarchies do not significantly reduce cache performance on cache misses [5]. The evidence suggests to us that for large scale systems with long network delays or busy servers or both, reducing the number of hops is vital to performance. Several researchers have proposed improving the scalability of a data hierarchy by splitting responsibilities according to a hash function [38].
Reference: [6] <author> Inktomi Corporation. </author> <title> The Economics of Large-Scale Caching. </title> <note> http://www.inktomi.com/Tech/EconOfLargeScaleCache.html, 1997. http://www.inktomi.com/Tech/EconOfLargeScaleCache.html. </note>
Reference-contexts: We believe our model will be interesting to system designers building large-scale, distributed cache infrastructures in a range of environments including network service providers, independent service providers, cache service providers <ref> [6, 29, 35, 39] </ref>, collections of caches linked by formal service agreements [34], and large intranets. Also, once the trade-offs in these cooperative environments are understood, we plan to examine ways to apply some of these techniques in more competitive environments. The rest of the paper is organized as follows.
Reference: [7] <author> M. Dahlin, C. Mather, R. Wang, T. Anderson, and D. Patterson. </author> <title> A Quantitative Analysis of Cache Policies for Scalable Network File Systems. </title> <booktitle> In Proceedings of the SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 150160, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: In wide area networks, the WAN-xFS proposal <ref> [7] </ref> uses a hierarchy of metadata nodes to improve scalability, and the CRISP cache [14] uses a centralized global directory.
Reference: [8] <author> M. Dahlin, R. Wang, T. Anderson, and D. Patterson. </author> <title> Cooperative Caching: Using Remote Client Memory to Improve File System Performance. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 267280, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Several other cache systems use metadata directories or multicast to locate data and then allow direct cache-to-cache data transfers. The primary differences among these schemes is how they structure their metadata directories. In local area networks, cooperative caching <ref> [8, 1] </ref> and global shared memory [11] hash a global directory across a set of nodes, and clients access the distributed global directory with network messages; in a LAN, the cost of this extra network hop is acceptable.
Reference: [9] <institution> Digital Equipment Corporation. </institution> <note> Digital's Web Proxy Traces. ftp://ftp.digital.com/pub/DEC/traces/proxy/webtraces.html, </note> <month> September </month> <year> 1996. </year>
Reference-contexts: One algorithm pushes data that have been recently updated, and the other pushes data that are widely shared. We evaluate our strategies using simulation studies of three large workloads that contain accesses by users of Digital Corporation's proxy server <ref> [9] </ref>, UC Berkeley's Home-IP service [18], and Prodigy ISP's dial-up web browsing service. Our algorithms focus on improving hit and miss response times rather than trying to improve global hit and miss rates. <p> First, we expect capacity misses (misses to objects that have been replaced due to limited cache capacity) to be a secondary consideration for large-scale cache architectures because 5 Trace # of Clients # of Accesses # of Distinct URLs Dates # of Days Client ID DEC <ref> [9] </ref> 16,660 22.1 million 4.15 million Sep. 1 Sep. 21, 1996 21 preserved Berkeley [17] 8,372 8.8 million 1.8 million Nov. 1 Nov. 19, 1996 19 preserved Prodigy 35,354 4.2 million 1.2 million Jan. 13 Jan 15, 1998 3 dynamic IP Table 4: Characteristics of trace workloads. it is economical to
Reference: [10] <author> B. Duska, D. Marwood, and M. Feeley. </author> <title> The Measured Access Characteristics of World-Wide-Web Client Proxy Caches. </title> <booktitle> In Proceedings of the USENIX Symposium on Internet Technologies and Systems, </booktitle> <month> December </month> <year> 1997. </year>
Reference-contexts: Large-scale distributed caches appear to provide an opportunity to combat this latency because they allow users to benefit from data fetched by other users <ref> [10, 18] </ref>, and their distributed architectures allow clients to access nearby copies of data in the common case. <p> The caches then send the data down the hierarchy to the client, and each cache along the path caches the data. Unfortunately, these hierarchies of data caches often achieve modest hit rates <ref> [2, 10, 15, 18] </ref>, can yield poor response times on a cache hit [26, 32], and can slow down cache misses. <p> Duska et. al conclude that as the rate of requests a cache system handles increases, its achievable hit rate will also increase <ref> [10] </ref>. The second limitation of these traces is that they are gathered at proxies rather than at clients. Thus all of these traces will display less locality and lower total hit rates than would be seen by clients using such a system. <p> However, we do not address that problem here. Possible avenues include increasing the number of clients sharing a cache system <ref> [10, 18] </ref>, allowing servers to send new data to cache systems before any nodes in the system request the data, providing better cache consistency [21, 25, 41] to reduce the number of requests marked as uncachable, caching dynamically generated results [36], dynamically replicating servers [37], and negative result caching [27, 5]. <p> For example, in the DEC traces the hit rates improve from 50% for L1 to 62% for L2 and 78% for L3. Gribble and Brewer [18] and Duska et. al <ref> [10] </ref> reach similar 7 2048, and infinite L3 caches shared by all clients in the trace. As sharing increases, so does the achievable hit rate. conclusions. <p> This section considers other areas of work relevant to understanding our results. Several studies have examined Internet workloads in depth with the goal of understanding how to improve performance. Arlitt and Williamson [2] examined basic workload characteristics for six servers. Duska et. al <ref> [10] </ref> examined workloads for several 3-week traces, Gribble and Bewer [18] examined a large trace of Berkeley HomeIP users. These studies support the conclusion that cache architectures that scale are important because increasing the number of users sharing a cache system increases the hit rates achievable by that system.
Reference: [11] <author> M. Feeley, W. Morgan, F. Pighin, A. Karlin, H. Levy, and C. Thekkath. </author> <title> Implementing Global Memory Management in a Workstation Cluster. </title> <booktitle> In Proceedings of the Fifteenth ACMSymposium on Operating Systems Principles, </booktitle> <pages> pages 201212, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Several other cache systems use metadata directories or multicast to locate data and then allow direct cache-to-cache data transfers. The primary differences among these schemes is how they structure their metadata directories. In local area networks, cooperative caching [8, 1] and global shared memory <ref> [11] </ref> hash a global directory across a set of nodes, and clients access the distributed global directory with network messages; in a LAN, the cost of this extra network hop is acceptable.
Reference: [12] <author> S. Floyd and V. Jacobson. </author> <title> The Synchronization of Periodic Routing Messages. </title> <journal> IEEE Transactions on Networking, </journal> <volume> 2(2):122136, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: object identifier (part of the MD5 signature of the object's URL), and a 8-byte machine identifier (an IP address and port number.) Nodes randomly choose the period between updates using a uniform distribution between 0 and 60 seconds to avoid the routing protocol capture effects observed by Floyd and Jacobson <ref> [12] </ref>. It would be desirable if these updates were multicast efficiently to exactly the interested parties. However, the amount of data involved seems overly small to warrant the complexities of reliable multicast, and the interval until the next transmission concerning the object is too large to use raw UDP multicast.
Reference: [13] <author> S. Gadde, J. Chase, and M. Rabinovich. </author> <title> Directory Structures for Scalable Internet Caches. </title> <type> Technical Report CS-1997-18, </type> <institution> Duke University Department of Computer Science, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: Recently, the designers of CRISP have hypothesized that hashing the global directory for scalability or caching portions of this global directory at clients might be a useful addition to their design <ref> [13] </ref>. Several systems, including the Internet Cache Protocol (ICP) [40] and Zhang et. al's adaptive caching proposal [42], replace directories with multicast queries to nearby caches. In contrast with these approaches, our system uses a scalable hierarchy of location hints combined with caching of these hints near clients.
Reference: [14] <author> S. Gadde, M. Rabinovich, and J. Chase. </author> <title> Reduce, Reuse, Recycle: An Approach to Building Large Internet Caches. </title> <booktitle> In Proceedings of the Sixth Workshop on Hot Topics in Operating Systems, </booktitle> <pages> pages 9398, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: In wide area networks, the WAN-xFS proposal [7] uses a hierarchy of metadata nodes to improve scalability, and the CRISP cache <ref> [14] </ref> uses a centralized global directory. Recently, the designers of CRISP have hypothesized that hashing the global directory for scalability or caching portions of this global directory at clients might be a useful addition to their design [13].
Reference: [15] <author> Steve Glassman. </author> <title> A Caching Relay for the World Wide Web. </title> <booktitle> In Proceedings of the Third International World Wide Web Conference, </booktitle> <pages> pages 6976, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The caches then send the data down the hierarchy to the client, and each cache along the path caches the data. Unfortunately, these hierarchies of data caches often achieve modest hit rates <ref> [2, 10, 15, 18] </ref>, can yield poor response times on a cache hit [26, 32], and can slow down cache misses.
Reference: [16] <author> J. Goldick, K. Benninger, W. Brown, C. Kirby, C. Maher, D. Nydick, and B. Zumach. </author> <title> An AFS-Based Supercomputing Environment. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Mass Storage Systems, </booktitle> <pages> pages 127132, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: These studies suggest that long network round trip latencies may be a significant factor. Hierarchical caching has been examined in the context of file systems <ref> [4, 33, 16] </ref>. Muntz and Honeyman [28] concluded that the additional hops in such a system often more than offset improvements in hit rate and characterized the extra level of cache as a delay server.
Reference: [17] <author> S. Gribble. </author> <title> UC Berkeley Home IP HTTP Traces, </title> <month> July </month> <year> 1997. </year> <note> Available at http://www.acm.org/sigcomm/ITA/. </note>
Reference-contexts: been replaced due to limited cache capacity) to be a secondary consideration for large-scale cache architectures because 5 Trace # of Clients # of Accesses # of Distinct URLs Dates # of Days Client ID DEC [9] 16,660 22.1 million 4.15 million Sep. 1 Sep. 21, 1996 21 preserved Berkeley <ref> [17] </ref> 8,372 8.8 million 1.8 million Nov. 1 Nov. 19, 1996 19 preserved Prodigy 35,354 4.2 million 1.2 million Jan. 13 Jan 15, 1998 3 dynamic IP Table 4: Characteristics of trace workloads. it is economical to build shared caches with small numbers of capacity misses.
Reference: [18] <author> S. Gribble and E. Brewer. </author> <title> System Design Issues for Internet Middleware Services: Deductions from a Large Client Trace. </title> <booktitle> In Proceedings of the USENIX Symposium on Internet Technologies and Systems, </booktitle> <month> December </month> <year> 1997. </year>
Reference-contexts: Large-scale distributed caches appear to provide an opportunity to combat this latency because they allow users to benefit from data fetched by other users <ref> [10, 18] </ref>, and their distributed architectures allow clients to access nearby copies of data in the common case. <p> The caches then send the data down the hierarchy to the client, and each cache along the path caches the data. Unfortunately, these hierarchies of data caches often achieve modest hit rates <ref> [2, 10, 15, 18] </ref>, can yield poor response times on a cache hit [26, 32], and can slow down cache misses. <p> One algorithm pushes data that have been recently updated, and the other pushes data that are widely shared. We evaluate our strategies using simulation studies of three large workloads that contain accesses by users of Digital Corporation's proxy server [9], UC Berkeley's Home-IP service <ref> [18] </ref>, and Prodigy ISP's dial-up web browsing service. Our algorithms focus on improving hit and miss response times rather than trying to improve global hit and miss rates. Thus, we parameterize our results using estimates of Internet access times based on our own measurements and those in the literature [32]. <p> Gribble and Brewer's analysis of the Berkeley trace indicates that achievable hit rates will improve as more clients are included in a cache system, but they caution that it is difficult to know at what point beyond the population sizes studied that such a trend will stop <ref> [18] </ref>. Duska et. al conclude that as the rate of requests a cache system handles increases, its achievable hit rate will also increase [10]. The second limitation of these traces is that they are gathered at proxies rather than at clients. <p> However, we do not address that problem here. Possible avenues include increasing the number of clients sharing a cache system <ref> [10, 18] </ref>, allowing servers to send new data to cache systems before any nodes in the system request the data, providing better cache consistency [21, 25, 41] to reduce the number of requests marked as uncachable, caching dynamically generated results [36], dynamically replicating servers [37], and negative result caching [27, 5]. <p> For example, in the DEC traces the hit rates improve from 50% for L1 to 62% for L2 and 78% for L3. Gribble and Brewer <ref> [18] </ref> and Duska et. al [10] reach similar 7 2048, and infinite L3 caches shared by all clients in the trace. As sharing increases, so does the achievable hit rate. conclusions. <p> Several studies have examined Internet workloads in depth with the goal of understanding how to improve performance. Arlitt and Williamson [2] examined basic workload characteristics for six servers. Duska et. al [10] examined workloads for several 3-week traces, Gribble and Bewer <ref> [18] </ref> examined a large trace of Berkeley HomeIP users. These studies support the conclusion that cache architectures that scale are important because increasing the number of users sharing a cache system increases the hit rates achievable by that system.
Reference: [19] <author> J. Griffioen and R. Appleton. </author> <title> Automatic Prefetching in a WAN. </title> <booktitle> In IEEE Workshop on Advances in Parallel and Distributed Systems, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: Several studies have examined push caching and prefetching in the context of web workloads <ref> [19, 20, 30] </ref>. These systems all used more elaborate history information to predict future references than the algorithm we examine.
Reference: [20] <author> J. Gwertzman. </author> <title> Autonomous Replication in Wide-Area Networks. </title> <type> Masters thesis, </type> <institution> Harvard University, </institution> <month> April </month> <year> 1995. </year> <note> Available as Tech Report TR-17-95. </note>
Reference-contexts: Our second strategy addresses the fourth design principle by redistributing data across the caches in the system. It does this by push caching <ref> [20] </ref> data near clients that have not referenced the data but that are likely to reference it in the future. Thus, it reduces or eliminates the compulsory miss penalty normally experienced the first time a particular client accesses an object. <p> Several studies have examined push caching and prefetching in the context of web workloads <ref> [19, 20, 30] </ref>. These systems all used more elaborate history information to predict future references than the algorithm we examine.
Reference: [21] <author> J. Gwertzman and M. Seltzer. </author> <title> World-Wide Web Cache Consistency. </title> <booktitle> In Proceedings of the 1996 USENIX Technical Conference, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: In our simulations, we assume that the system approximates strong cache consistency by invalidating all cached copies whenever data changes. We do this for two reasons. First, techniques for approximating or providing strong cache consistency in this environment are improving <ref> [21, 25, 41] </ref>, so we expect this assumption to be a good reflection of achievable future cache technology. Second, weak cache consistency distorts cache performance either by increasing apparent hit rates by counting hits to stale data or by reducing apparent hit rates by discarding perfectly good data from caches. <p> However, we do not address that problem here. Possible avenues include increasing the number of clients sharing a cache system [10, 18], allowing servers to send new data to cache systems before any nodes in the system request the data, providing better cache consistency <ref> [21, 25, 41] </ref> to reduce the number of requests marked as uncachable, caching dynamically generated results [36], dynamically replicating servers [37], and negative result caching [27, 5].
Reference: [22] <author> J. Kistler and M. Satyanarayanan. </author> <title> Disconnected Operation in the Coda File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1):3 25, </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: We assume that the algorithms are not supplied with knowledge of future access patterns. They must therefore predict future access patterns based on the past. In particular, we do not assume any external directives about future accesses such as hoard lists <ref> [22, 24] </ref> or server hits [30]. 16 2. We limit pushing or prefetching to increasing the number of copies of data that are already stored at least once in the cache system.
Reference: [23] <author> T. Kroeger, D. Long, and J. Mogul. </author> <title> Exploring the Bounds of Web Latency Reduction from Caching and Prefetching. </title> <booktitle> In Proceedings of the USENIX Symposium on Internet Technologies and Systems, </booktitle> <month> December </month> <year> 1997. </year>
Reference-contexts: Because large, shared caches do a good job at satisfying references to popular objects, we explore prefetching strategies that will work well for the remaining large number of objects about whose access patterns little is known. Kroeger et. al <ref> [23] </ref> examined the limits of performance for caching and prefetching.
Reference: [24] <author> G. Kuenning. Seer: </author> <title> Predictive File Hoarding for Disconnected Mobile Operation. </title> <type> PhD thesis, </type> <institution> The University of California at Los Angeles, </institution> <year> 1997. </year> <month> 21 </month>
Reference-contexts: We assume that the algorithms are not supplied with knowledge of future access patterns. They must therefore predict future access patterns based on the past. In particular, we do not assume any external directives about future accesses such as hoard lists <ref> [22, 24] </ref> or server hits [30]. 16 2. We limit pushing or prefetching to increasing the number of copies of data that are already stored at least once in the cache system.
Reference: [25] <author> C. Liu and P. Cao. </author> <title> Maintaining Strong Cache Consistency in the World-Wide Web. </title> <booktitle> In Proceedings of the Seventeenth International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: In our simulations, we assume that the system approximates strong cache consistency by invalidating all cached copies whenever data changes. We do this for two reasons. First, techniques for approximating or providing strong cache consistency in this environment are improving <ref> [21, 25, 41] </ref>, so we expect this assumption to be a good reflection of achievable future cache technology. Second, weak cache consistency distorts cache performance either by increasing apparent hit rates by counting hits to stale data or by reducing apparent hit rates by discarding perfectly good data from caches. <p> However, we do not address that problem here. Possible avenues include increasing the number of clients sharing a cache system [10, 18], allowing servers to send new data to cache systems before any nodes in the system request the data, providing better cache consistency <ref> [21, 25, 41] </ref> to reduce the number of requests marked as uncachable, caching dynamically generated results [36], dynamically replicating servers [37], and negative result caching [27, 5].
Reference: [26] <author> C. Maltzahn, K. Richardson, and D. Grunwald. </author> <title> Performance Issues of Enterprise Level Web Proxies. </title> <booktitle> In Proceedings of the SIGMET-RICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: The caches then send the data down the hierarchy to the client, and each cache along the path caches the data. Unfortunately, these hierarchies of data caches often achieve modest hit rates [2, 10, 15, 18], can yield poor response times on a cache hit <ref> [26, 32] </ref>, and can slow down cache misses. In this paper, we first attempt to understand the factors that limit the performance of current web caching hierarchies through measurements of several caches on the Internet and analysis of several traces of web traffic. <p> Note that, in our experiments, the caches were idle other than our requests, which were made one at a time. If the caches were heavily loaded, queuing delays and implementation inefficiencies of the caches <ref> [26, 3] </ref> might significantly increase the per-hop costs we observe. Busy nodes would probably increase the importance of reducing the number of hops in a cache system. standard three level data hierarchy. <p> These studies support the conclusion that cache architectures that scale are important because increasing the number of users sharing a cache system increases the hit rates achievable by that system. Rousskov [32] and Maltzahn et. al <ref> [26] </ref> measure the performance of cache servers deployed in the field and examine why the performance of these servers has been worse than laboratory tests predicted [5]. These studies suggest that long network round trip latencies may be a significant factor.
Reference: [27] <author> P. Mockapetris and K. Dunlap. </author> <title> Development of the Domain Name System. </title> <journal> Computer Communications Review, </journal> <volume> 18(4):123133, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: system [10, 18], allowing servers to send new data to cache systems before any nodes in the system request the data, providing better cache consistency [21, 25, 41] to reduce the number of requests marked as uncachable, caching dynamically generated results [36], dynamically replicating servers [37], and negative result caching <ref> [27, 5] </ref>.
Reference: [28] <author> D. Muntz and P. Honeyman. </author> <title> Multi-level Caching in Distributed File Systems or Your cache ain't nuthin' but trash. </title> <booktitle> In Proceedings of the Winter 1992 USENIX Conference, </booktitle> <pages> pages 305313, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: First, the cost of accessing a series of caches in the hierarchy adds significant store-and-forward delays to higher-level cache hits and to cache misses <ref> [28] </ref>. Second, when high-level caches service a large number of clients distributed over a large geographic region, the network delays between a client and a high-level cache may be large, which reduces the benefit of hits to far-away caches. To understand these effects better, we use two sources of information. <p> These studies suggest that long network round trip latencies may be a significant factor. Hierarchical caching has been examined in the context of file systems [4, 33, 16]. Muntz and Honeyman <ref> [28] </ref> concluded that the additional hops in such a system often more than offset improvements in hit rate and characterized the extra level of cache as a delay server.
Reference: [29] <author> D. Neal. </author> <title> The Harvest Object Cache in New Zealand. </title> <booktitle> In Proceedings of the Fifth International World Wide Web Conference, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: We believe our model will be interesting to system designers building large-scale, distributed cache infrastructures in a range of environments including network service providers, independent service providers, cache service providers <ref> [6, 29, 35, 39] </ref>, collections of caches linked by formal service agreements [34], and large intranets. Also, once the trade-offs in these cooperative environments are understood, we plan to examine ways to apply some of these techniques in more competitive environments. The rest of the paper is organized as follows. <p> We reach similar conclusions in the context of Internet caching, leading to our design principle of minimizing the number of hops on a hit or miss. Hierarchical caching has been more widely accepted on the Internet, and it is common in areas with low-speed connections to the Internet <ref> [29, 35] </ref>. In addition, many sites add proxy caching to their corporate firewalls. A widely deployed and studied system has been the publicly-available Harvest cache [5] and its successor, the Squid cache [39].
Reference: [30] <author> V. Padmanabhan and J. Mogul. </author> <title> Using Predictive Prefetching to Improve World Wide Web Latency. </title> <booktitle> In Proceedings of the ACM SIGCOMM '96 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication, </booktitle> <pages> pages 2236, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: We assume that the algorithms are not supplied with knowledge of future access patterns. They must therefore predict future access patterns based on the past. In particular, we do not assume any external directives about future accesses such as hoard lists [22, 24] or server hits <ref> [30] </ref>. 16 2. We limit pushing or prefetching to increasing the number of copies of data that are already stored at least once in the cache system. Thus, our algorithms can only affect the number of L1, L2, and L3 hits, not the number of system-wide misses. <p> Several studies have examined push caching and prefetching in the context of web workloads <ref> [19, 20, 30] </ref>. These systems all used more elaborate history information to predict future references than the algorithm we examine.
Reference: [31] <author> C. Plaxton, R. Rajaram, and A. Richa. </author> <title> Accessing nearby copies of replicated objects in a distributed environment. </title> <booktitle> In Proceedings of the Ninth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 311320, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: In particular, our system constructs a hierarchy using an algorithm developed by Plaxton et. al <ref> [31] </ref> to embed virtual hint distribution trees onto the nodes in the system. This algorithm has the following properties: * Automatic configuration.
Reference: [32] <author> A. Rousskov. </author> <title> On Performance of Caching Proxies. </title> <address> http://www.cs.ndsu.nodak.edu/ rousskov/research/cache/squid/profiling/papers, </address> <year> 1996. </year>
Reference-contexts: The caches then send the data down the hierarchy to the client, and each cache along the path caches the data. Unfortunately, these hierarchies of data caches often achieve modest hit rates [2, 10, 15, 18], can yield poor response times on a cache hit <ref> [26, 32] </ref>, and can slow down cache misses. In this paper, we first attempt to understand the factors that limit the performance of current web caching hierarchies through measurements of several caches on the Internet and analysis of several traces of web traffic. <p> Our algorithms focus on improving hit and miss response times rather than trying to improve global hit and miss rates. Thus, we parameterize our results using estimates of Internet access times based on our own measurements and those in the literature <ref> [32] </ref>. For these traces and network configurations, we find that our hint architecture provides speedups of 1.27 to 2.30 compared to a standard, three-level cache hierarchy and that push caching improves performance by up to an additional factor of 1.25. <p> Second, to understand how such systems perform in real hierarchies and under real workloads, we examine Rousskov's measurements of several Squid caches deployed at different levels of a hierarchy <ref> [32] </ref>. Although Squid supports the Internet Cache Protocol (ICP) to allow a cache to query its neighbors before sending a miss to a parent [40], we are interested in the best costs for traversing a hierarchy, so neither configuration we examine uses ICP. Both analysis yield similar results. <p> This experiment suggests that in addition to reducing the number of hops needed to access distant data, cache hierarchies should take action to access nearby data as often as possible. In Section 4, we propose algorithms that address this design principle. 2.1.2 Analysis of Squid hierarchies Rousskov <ref> [32] </ref> has published detailed measurements and performance analysis of several Squid caches that are deployed on the Internet in the United States and Europe. <p> These studies support the conclusion that cache architectures that scale are important because increasing the number of users sharing a cache system increases the hit rates achievable by that system. Rousskov <ref> [32] </ref> and Maltzahn et. al [26] measure the performance of cache servers deployed in the field and examine why the performance of these servers has been worse than laboratory tests predicted [5]. These studies suggest that long network round trip latencies may be a significant factor.
Reference: [33] <author> H. Sandhu and S. Zhou. </author> <title> Cluster-Based File Replication in Large-Scale Distributed Systems. </title> <booktitle> In Proceedings of the SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 91102, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: These studies suggest that long network round trip latencies may be a significant factor. Hierarchical caching has been examined in the context of file systems <ref> [4, 33, 16] </ref>. Muntz and Honeyman [28] concluded that the additional hops in such a system often more than offset improvements in hit rate and characterized the extra level of cache as a delay server.
Reference: [34] <author> M. Schwartz. </author> <title> Formal Service Agreements for Scaling Internet Caching. In NLANR Web Cache Workshop, </title> <month> June </month> <year> 1997. </year> <note> http://ircache.nlanr.net/Cache/Workshop97/Papers/Schwartz/schwartz.html. </note>
Reference-contexts: We believe our model will be interesting to system designers building large-scale, distributed cache infrastructures in a range of environments including network service providers, independent service providers, cache service providers [6, 29, 35, 39], collections of caches linked by formal service agreements <ref> [34] </ref>, and large intranets. Also, once the trade-offs in these cooperative environments are understood, we plan to examine ways to apply some of these techniques in more competitive environments. The rest of the paper is organized as follows.
Reference: [35] <author> N. Smith. </author> <title> The UK National Web Cache: A State of the Art Report. </title> <booktitle> In Proceedings of the Fifth International World Wide Web Conference, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: We believe our model will be interesting to system designers building large-scale, distributed cache infrastructures in a range of environments including network service providers, independent service providers, cache service providers <ref> [6, 29, 35, 39] </ref>, collections of caches linked by formal service agreements [34], and large intranets. Also, once the trade-offs in these cooperative environments are understood, we plan to examine ways to apply some of these techniques in more competitive environments. The rest of the paper is organized as follows. <p> We reach similar conclusions in the context of Internet caching, leading to our design principle of minimizing the number of hops on a hit or miss. Hierarchical caching has been more widely accepted on the Internet, and it is common in areas with low-speed connections to the Internet <ref> [29, 35] </ref>. In addition, many sites add proxy caching to their corporate firewalls. A widely deployed and studied system has been the publicly-available Harvest cache [5] and its successor, the Squid cache [39].
Reference: [36] <author> A. Vahdat and T. Anderson. </author> <title> Transparent Result Caching. </title> <booktitle> In Proceedings of the 1998 USENIX Technical Conference, </booktitle> <month> June </month> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: include increasing the number of clients sharing a cache system [10, 18], allowing servers to send new data to cache systems before any nodes in the system request the data, providing better cache consistency [21, 25, 41] to reduce the number of requests marked as uncachable, caching dynamically generated results <ref> [36] </ref>, dynamically replicating servers [37], and negative result caching [27, 5].
Reference: [37] <author> A. Vahdat, P. Eastham, C. Yoshikawa, E. Belani, T. Anderson, D. Culler, and M. Dahlin. WebOS: </author> <title> Operating System Services for Wide Area Applications. </title> <type> Tech Report CSD-97-938, </type> <institution> U.C. Berkeley Computer Science Division, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: of clients sharing a cache system [10, 18], allowing servers to send new data to cache systems before any nodes in the system request the data, providing better cache consistency [21, 25, 41] to reduce the number of requests marked as uncachable, caching dynamically generated results [36], dynamically replicating servers <ref> [37] </ref>, and negative result caching [27, 5].
Reference: [38] <author> V. Valloppillil and K. Ross. </author> <title> Cache Array Routing Protocol v1.0. </title> <type> IETF Internet Draft, </type> <month> October </month> <year> 1997. </year> <note> http://www.etext.org/Internet/Internet-Drafts/draft-vinod-carp-v1-02.txt. </note>
Reference-contexts: The evidence suggests to us that for large scale systems with long network delays or busy servers or both, reducing the number of hops is vital to performance. Several researchers have proposed improving the scalability of a data hierarchy by splitting responsibilities according to a hash function <ref> [38] </ref>. This approach may work well for distributing load across a set of caches that are near one another and near their clients, but in larger systems where clients are closer to some caches than others, the hash function will prevent the system from exploiting locality.
Reference: [39] <author> D. Wessels. </author> <title> Squid Internet Object Cache. </title> <note> http://squid.nlanr.net/Squid/, January 1998. </note>
Reference-contexts: Overall, our techniques provide speedups of 1.27 to 2.43 compared to a traditional hierarchy. We have implemented a prototype by augmenting the widely-deployed Squid proxy cache <ref> [39] </ref>. It implements a hint distribution hierarchy, local hint caches, and cache-to-cache data transfers. A key insight gained in this implementation was an understanding of the desirability of storing hints as small, fixed-sized records. <p> We believe our model will be interesting to system designers building large-scale, distributed cache infrastructures in a range of environments including network service providers, independent service providers, cache service providers <ref> [6, 29, 35, 39] </ref>, collections of caches linked by formal service agreements [34], and large intranets. Also, once the trade-offs in these cooperative environments are understood, we plan to examine ways to apply some of these techniques in more competitive environments. The rest of the paper is organized as follows. <p> Separate data paths and metadata paths, location-hints (Section 3) Cache data close to clients (Section 2.1) Push caching (Section 4) Table 1: Design principles and strategies for distributed caching in the Internet 2.1 Access costs in a traditional cache hierarchy Traditional hierarchical cache architectures such as Harvest [5] or Squid <ref> [39] </ref> define parent-child relationships among caches. Each cache in the hierarchy is shared by a group of clients or a group of children caches. <p> In addition, many sites add proxy caching to their corporate firewalls. A widely deployed and studied system has been the publicly-available Harvest cache [5] and its successor, the Squid cache <ref> [39] </ref>. A key motivation of our design is that we disagree with the conclusion that cache hierarchies do not significantly reduce cache performance on cache misses [5].
Reference: [40] <author> D. Wessels and K. Claffy. </author> <title> Application of Internet Cache Protocol (ICP), </title> <type> version 2. </type> <institution> Request for Comments RFC-2186, Network Working Group, </institution> <year> 1997. </year> <note> http://ds.internic.net/rfc/rfc2187.txt. </note>
Reference-contexts: Although Squid supports the Internet Cache Protocol (ICP) to allow a cache to query its neighbors before sending a miss to a parent <ref> [40] </ref>, we are interested in the best costs for traversing a hierarchy, so neither configuration we examine uses ICP. Both analysis yield similar results. Both suggest that there is a significant per-hop cost for traversing multiple levels of cache. <p> Recently, the designers of CRISP have hypothesized that hashing the global directory for scalability or caching portions of this global directory at clients might be a useful addition to their design [13]. Several systems, including the Internet Cache Protocol (ICP) <ref> [40] </ref> and Zhang et. al's adaptive caching proposal [42], replace directories with multicast queries to nearby caches. In contrast with these approaches, our system uses a scalable hierarchy of location hints combined with caching of these hints near clients.
Reference: [41] <author> J. Yin, L. Alvisi, M. Dahlin, and C. Lin. </author> <title> Using Leases to Support Server-Driven Consistency in Large-Scale Systems. </title> <booktitle> In Proceedings of the Eightteenth International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1998. </year>
Reference-contexts: In our simulations, we assume that the system approximates strong cache consistency by invalidating all cached copies whenever data changes. We do this for two reasons. First, techniques for approximating or providing strong cache consistency in this environment are improving <ref> [21, 25, 41] </ref>, so we expect this assumption to be a good reflection of achievable future cache technology. Second, weak cache consistency distorts cache performance either by increasing apparent hit rates by counting hits to stale data or by reducing apparent hit rates by discarding perfectly good data from caches. <p> However, we do not address that problem here. Possible avenues include increasing the number of clients sharing a cache system [10, 18], allowing servers to send new data to cache systems before any nodes in the system request the data, providing better cache consistency <ref> [21, 25, 41] </ref> to reduce the number of requests marked as uncachable, caching dynamically generated results [36], dynamically replicating servers [37], and negative result caching [27, 5].
Reference: [42] <author> L. Zhang, S. Floyd, and V. Jacobson. </author> <title> Adaptive Web Caching. </title> <booktitle> In Proceedings of the NLANR Web Cache Workshop, </booktitle> <month> June </month> <year> 1997. </year> <note> http://ircache.nlanr.net/Cache/Workshop97/. 22 </note>
Reference-contexts: Recently, the designers of CRISP have hypothesized that hashing the global directory for scalability or caching portions of this global directory at clients might be a useful addition to their design [13]. Several systems, including the Internet Cache Protocol (ICP) [40] and Zhang et. al's adaptive caching proposal <ref> [42] </ref>, replace directories with multicast queries to nearby caches. In contrast with these approaches, our system uses a scalable hierarchy of location hints combined with caching of these hints near clients. Four aspects of this design help it meet the design principles developed in the previous section.
References-found: 42

