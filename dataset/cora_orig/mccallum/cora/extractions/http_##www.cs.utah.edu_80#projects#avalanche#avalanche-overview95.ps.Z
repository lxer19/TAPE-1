URL: http://www.cs.utah.edu:80/projects/avalanche/avalanche-overview95.ps.Z
Refering-URL: http://www.cs.umd.edu/users/keleher/syllabus.818.html
Root-URL: 
Title: Avalanche: A Communication and Memory Architecture for Scalable Parallel Computing  
Author: John B. Carter, Al Davis, Ravindra Kuramkote, Chen-Chi Kuo, Leigh B. Stoller, Mark Swanson 
Affiliation: Computer Systems Laboratory University of Utah  
Abstract: As the gap between processor and memory speeds widens, system designers will inevitably incorporate increasingly deep memory hierarchies to maintain the balance between processor and memory system performance. At the same time, most communication subsystems are permitted access only to main memory and not a processor's top level cache. As memory latencies increase, this lack of integration between the memory and communication systems will seriously impede interprocessor communication performance and limit effective scalability. In the Avalanche project we are redesigning the memory architecture of a commercial RISC multiprocessor, the HP PA-RISC 7100, to include a new multi-level context sensitive cache that is tightly coupled to the communication fabric. The primary goal of Avalanche's integrated cache and communication controller is attacking end to end communication latency in all of its forms. This includes cache misses induced by excessive invalidations and reloading of shared data by write-invalidate coherence protocols and cache misses induced by depositing incoming message data in main memory and faulting it into the cache. An execution-driven simulation study of Avalanche's architecture indicates that it can reduce cache stalls by 5-60% and overall execution times by 10-28%. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> References </institution>
Reference: [1] <author> A. Agarwal and D. Chaiken et al. </author> <title> The MIT Alewife Machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report Technical Memp 454, </type> <institution> MIT/LCS, </institution> <year> 1991. </year>
Reference-contexts: We call this problem the injection problem, and discuss several techniques that we are exploring to address the problem throughout the remainder of this paper. 3 One significant difference between Avalanche and related research projects <ref> [21, 20, 1, 24] </ref> is that we are not treating the CPU as an unmodifiable black box. The HP PA-RISC 7100 contains no on-chip cache but does contain the cache controller logic. The interface between the cache controller and the rest of the CPU is relatively straightforward. <p> We also plan to support more explicit software control of data placement in the form of directives in the incoming connection descriptors specifying message placement within the hierarchy. 2.3 Support for Shared Memory Spurred by scalable shared memory architectures developed in academia <ref> [1, 21] </ref>, the next generation of massively parallel systems will support shared memory in hardware (e.g., machines by Convex, Cray, and IBM). <p> This machine, called FLASH [20], is currently being designed to support both DASH-like shared memory and efficient message passing. However, their plans for exploiting the flexibility of their controller's operation have not been revealed. The MIT Alewife machine <ref> [1, 10] </ref> also uses a directory-based cache design that supports both low latency message passing and shared memory based on an invalidation-based consistency protocol.
Reference: [2] <author> J. Archibald and J.-L. Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: For example, data that is being accessed primarily by a single processor would likely be handled by a conventional write-invalidate protocol <ref> [2] </ref>, while data being heavily shared by multiple processes, such as global counters or edge elements in finite differencing codes, would likely be handled using a delayed write-update protocol [5].
Reference: [3] <author> David Beazley, </author> <year> 1994. </year> <title> Member of 1993 Gordon Bell Prize winning team, </title> <type> personal communication. </type>
Reference-contexts: For example, even highly-tuned applications often achieve well under 50% of peak performance on multiprocessors such as the CM-5 [27] and Cray T3D [12] despite their powerful communication fabrics <ref> [3] </ref>. Even when the interconnection fabric is capable of very high speed communication, the effects of the memory hierarchy and the parasitic influence of other overheads become the dominant latency components. For an architecture to scale effectively into the tera- and peta-op range, high latency cache misses must be avoided.
Reference: [4] <author> M. J. Beckerle. </author> <title> An Overview of the START (*T) Computer System. </title> <type> MCRC Technical Report MCRC-TR-28, </type> <institution> Motorola Cambridge Research Center, </institution> <year> 1992. </year>
Reference-contexts: However, it does not provide the tight integration of communication fabric and protocol into a realistic memory hierarchy, nor does it exploit context sensitivity to tune its behavior. 16 The Motorola and MIT *T machine <ref> [4] </ref> has many interesting components that offer excellent support to exploit dataflow style parallelism. The *T architecture provides tight coupling between the processor registers and the interconnect fabric, but isolates the memory hierarchy by placing the CPU between the interconnect fabric and the memory.
Reference: [5] <editor> Authors deleted to help preserve anonymity. </editor> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Using traces of shared memory parallel programs, researchers have found there are a small number of characteristic ways in which shared memory is accessed <ref> [5, 16, 28] </ref>. These characteristic "patterns" are sufficiently different from one another that any protocol designed to optimize one will not perform particularly well for the others. <p> For example, data that is being accessed primarily by a single processor would likely be handled by a conventional write-invalidate protocol [2], while data being heavily shared by multiple processes, such as global counters or edge elements in finite differencing codes, would likely be handled using a delayed write-update protocol <ref> [5] </ref>. Similarly, locks could be handled using conventional distributed locking protocols, while more complex synchronization operations like barriers and reduction operators for vector sums could be handled using specialized protocols. <p> Furthermore, the use of a write update protocol can significantly reduce the number of read misses that a write-invalidate protocol induces as a side effect of maintaining coherence when the degree of sharing is high <ref> [5] </ref>. For example, if processors a and b are both reading and writing data from a particular cache line, a write invalidate protocol will result in a large number of invalidations and subsequent read misses when the invalidated processor reloads the data that it needs.
Reference: [6] <author> B. Bershad, D. Lee, T. Romer, and J.B. Chen. </author> <title> Avoiding conflict misses dynamically in large direct-mapped caches. </title> <booktitle> In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 158-170, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Similarly, state will be added to the CCU to determine the "heat" of the cache, similar to the way in which the CAML buffer detects the "heat" of a particular set of pages in a direct-mapped cache <ref> [6] </ref>.
Reference: [7] <author> M.A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E.W. Felten, and J. Sandberg. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: As such, it currently requires extensive program modification or user effort to achieve scalable performance, although the designers are working on a number of compilation and performance debugging tools to help automate this process. The tradeoffs between the software and hardware approaches are being studied. The SHRIMP Multicomputer <ref> [7] </ref> employs a custom designed network interface to provide both shared memory and low-latency message passing. A virtual memory-mapped interface provides a constrained form of shared memory in which a process can map in pages that are physically located on another node.
Reference: [8] <author> N.J. Boden, D. Cohen, R.E. Felderman, A.E. Kulawik, C.L. Seitz, J.N. Seizovic, and W.-K. Su. </author> <title> Myrinet A gigabit-per-second local-area network. </title> <journal> IEEE MICRO, </journal> <note> 1995. To appear. </note>
Reference-contexts: By handling data with a flexible protocol that can be customized for its expected use, we expect the number of cache misses and messages required to maintain consistency to drop dramatically. By reducing the amount of communication required to maintain coherence, multiprocessor designers can either use a commodity interconnect <ref> [8] </ref> and achieve performance equal to that of a static controller and a fast special purpose interconnect like that found in the CM-5, or use the faster interconnect to support more processors. However, this greater power and flexibility increases hardware complexity, size, and cost. <p> Depending on the number of processors and the complexity of the cache controllers being simulated, our simulation runs took between twenty minutes and five hours to complete. 3.2 Network Model To accurately model network delays and contention, we have developed a very detailed, flit-by-flit model of the Myrinet fabric <ref> [8] </ref>. We use Myrinet as the basis for our network model because even though it has a relatively high latency when compared to proprietary interconnects such as that found in the CM-5, the Myrinet interconnect is the fastest commercially available interconnect suitable for our needs.
Reference: [9] <editor> Author deleted to help preserve anonymity. </editor> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis. </type>
Reference-contexts: These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols <ref> [9, 30] </ref>, support multiple communication models [10, 17], or accept guidance from software [20, 24]. Programmable controllers would seem at first glance to be an ideal combination of software's greater flexibility and hardware's greater speed.
Reference: [10] <author> D. Chaiken and A. Agarwal. </author> <title> Software-extended coherent shared memory: Performance and cost. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314-324, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols [9, 30], support multiple communication models <ref> [10, 17] </ref>, or accept guidance from software [20, 24]. Programmable controllers would seem at first glance to be an ideal combination of software's greater flexibility and hardware's greater speed. <p> This machine, called FLASH [20], is currently being designed to support both DASH-like shared memory and efficient message passing. However, their plans for exploiting the flexibility of their controller's operation have not been revealed. The MIT Alewife machine <ref> [1, 10] </ref> also uses a directory-based cache design that supports both low latency message passing and shared memory based on an invalidation-based consistency protocol.
Reference: [11] <author> D. Clark, V. Jacobson, J. Romkey, and H. Salwen. </author> <title> An analysis of TCP processing overhead. </title> <journal> IEEE Communications Magazine, </journal> <pages> pages 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: This mechanism avoids intermediate copies 4 of the data, which are a major source of inefficiency in many existing protocols <ref> [13, 11] </ref>, and is a prerequisite for successfully attacking the injection problem. On a 100 MHz HP7100 processor with an external I/O controller, our current protocol implementation takes 111 CPU cycles to write a DMA descriptor block using programmed I/O.
Reference: [12] <author> Cray Research, Inc. </author> <title> CRAY T3D System Architecture Overview, </title> <address> hr-04033 edition, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Evidence of this situation can be seen in the significant differences between the peak performance of today's fast multiprocessing systems and the achieved performance. For example, even highly-tuned applications often achieve well under 50% of peak performance on multiprocessors such as the CM-5 [27] and Cray T3D <ref> [12] </ref> despite their powerful communication fabrics [3]. Even when the interconnection fabric is capable of very high speed communication, the effects of the memory hierarchy and the parasitic influence of other overheads become the dominant latency components.
Reference: [13] <author> C. Dalton, G. Watson, D. Banks, C. Calamvokis, A. Edwards, and J. Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <pages> pages 36-43, </pages> <month> July </month> <year> 1993. </year> <month> 18 </month>
Reference-contexts: This mechanism avoids intermediate copies 4 of the data, which are a major source of inefficiency in many existing protocols <ref> [13, 11] </ref>, and is a prerequisite for successfully attacking the injection problem. On a 100 MHz HP7100 processor with an external I/O controller, our current protocol implementation takes 111 CPU cycles to write a DMA descriptor block using programmed I/O.
Reference: [14] <editor> Author deleted to help preserve anonymity. Mayfly: </editor> <title> A General-Purpose, Scalable, </title> <booktitle> Parallel Processing Architecture. Lisp and Symbolic Computation, </booktitle> 5(1/2):7-47, May 1992. 
Reference-contexts: The MIT M-Machine work [22] contains a context cache similar to previous designs such as the HP Mayfly system <ref> [14] </ref>. This context cache provides dynamic binding of variable names to register contents to permit rapid task switching and promote the interesting processor coupling mechanism of the M-machine.
Reference: [15] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: second memory request immediately after the first write, it will be delayed an extra cycle. 10 3.4 Protocols Investigated We evaluated the performance of four basic coherence protocols: (i) a sequentially consistent multiple reader, singler writer, write invalidate protocol (sc-wi), (ii) a no-replicate migratory protocol (mig), (iii) a release consistent <ref> [15] </ref> implementation of a conventional multiple reader, single writer, write invalidate protocol (rc-wi), and (iv) a release consistent multiple reader, multiple writer, write update protocol (rc-wu). We selected these four protocols because they covered a wide spectrum of options available to system designers. <p> This optimization assumes that the program is written using sufficient synchronization to avoid data races, which is most often the case. The details of why this results in correct behavior is beyond the scope of this paper a detailed explanation can be found elsewhere <ref> [15] </ref>. The rc-wu protocol uses the write state buffer in a different way. When a node writes to a word of shared data, it allocates an entry in the write buffer for the associated cache line and marks that word as dirty.
Reference: [16] <author> A. Gupta and W.-D. Weber. </author> <title> Cache invalidation patterns in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 794-810, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Using traces of shared memory parallel programs, researchers have found there are a small number of characteristic ways in which shared memory is accessed <ref> [5, 16, 28] </ref>. These characteristic "patterns" are sufficiently different from one another that any protocol designed to optimize one will not perform particularly well for the others.
Reference: [17] <author> M. Heinrich and J. Kuskin et al. </author> <title> The performance impact of flexibility in the Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274-285, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols [9, 30], support multiple communication models <ref> [10, 17] </ref>, or accept guidance from software [20, 24]. Programmable controllers would seem at first glance to be an ideal combination of software's greater flexibility and hardware's greater speed.
Reference: [18] <institution> Intel Supercomputer Systems Division. Paragon XP/S Product Overview, </institution> <year> 1991. </year>
Reference-contexts: Even this best case of 971 cycles dominates the propagation delay of the high-speed interconnects in current multiprocessors <ref> [18, 27] </ref>, which is a strong indication that as much effort needs to be placed in improving the performance of the memory system and the network controller's access to the memory as is being spent developing higher bandwidth and lower latency interconnects. <p> This imbalance becomes an even more dominant factor as message size grows. Specifically, with few exceptions [27], conventional memory architectures require that messages always be transferred to or from main memory and that the cache be flushed as appropriate to maintain consistency <ref> [18] </ref>. Unfortunately this restriction guarantees that the receiver will incur cache misses for the entire message body. Substantial improvement can be made by more tightly coupling the communication fabric and protocol with the memory system and the context of the processor. <p> A study by Pakin et al. showed that the choice of where to inject incoming data can have a tremendous impact on overall performance of an application [23]. Always injecting data to main memory, as in the Intel Paragon <ref> [18] </ref>, results in a message latency too high for very fine grained applications, and the effect is getting worse as processor speeds increase. <p> Thus, the on-chip cache miss penalties discussed earlier have proven problematic in terms of achieving a reasonable percentage of the impressive peak performance of the CM-5 on real applications. Another commercial scalable supercomputer of interest is the Intel Paragon <ref> [18] </ref>. The interconnect is a high performance mesh routing device. The fabric does not support direct DMA into the Paragon's memory hierarchy but utilizes a second i860XP CPU for this purpose on each processing element.
Reference: [19] <author> N.P. Jouppi. </author> <title> Cahe write policies and performance. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191-201, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Each entry is associated with a local dirty cache line and is used to keep track of which words are dirty in that line. Write state buffer entries are allocated on demand when the local cache writes to a shared cache line. Unlike a conventional write buffer <ref> [19] </ref>, which contains the modified data as well as its address, the write state buffer contains only an indication of what words have been modified. The modified data itself is stored 5 Although we cannot do so now without eliminating any pretext of anonymity 11 in the cache.
Reference: [20] <author> J. Kuskin and D. Ofelt et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: We call this problem the injection problem, and discuss several techniques that we are exploring to address the problem throughout the remainder of this paper. 3 One significant difference between Avalanche and related research projects <ref> [21, 20, 1, 24] </ref> is that we are not treating the CPU as an unmodifiable black box. The HP PA-RISC 7100 contains no on-chip cache but does contain the cache controller logic. The interface between the cache controller and the rest of the CPU is relatively straightforward. <p> The CCU will incorporate a protocol processing element (PPE) to support the DMA requirements and some of the protocol duties. Unlike the protocol processor in the FLASH multiprocessor <ref> [20] </ref>, Avalanche's protocol processor will only perform 1 The number of levels of cache in Avalanche has not been determined. Two levels are shown for purposes of illustration. 5 a very limited number of built-in operations it is not a general purpose processor. <p> These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols [9, 30], support multiple communication models [10, 17], or accept guidance from software <ref> [20, 24] </ref>. Programmable controllers would seem at first glance to be an ideal combination of software's greater flexibility and hardware's greater speed. <p> A second generation DASH multiprocessor is being developed that introduces a limited amount of processing power and state at the distributed directories to add flexibility to the consistency implementation. This machine, called FLASH <ref> [20] </ref>, is currently being designed to support both DASH-like shared memory and efficient message passing. However, their plans for exploiting the flexibility of their controller's operation have not been revealed.
Reference: [21] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We call this problem the injection problem, and discuss several techniques that we are exploring to address the problem throughout the remainder of this paper. 3 One significant difference between Avalanche and related research projects <ref> [21, 20, 1, 24] </ref> is that we are not treating the CPU as an unmodifiable black box. The HP PA-RISC 7100 contains no on-chip cache but does contain the cache controller logic. The interface between the cache controller and the rest of the CPU is relatively straightforward. <p> We also plan to support more explicit software control of data placement in the form of directives in the incoming connection descriptors specifying message placement within the hierarchy. 2.3 Support for Shared Memory Spurred by scalable shared memory architectures developed in academia <ref> [1, 21] </ref>, the next generation of massively parallel systems will support shared memory in hardware (e.g., machines by Convex, Cray, and IBM). <p> Our approach differs from the approaches taken in these systems in a number of important aspects, as described below. The Stanford DASH multiprocessor <ref> [21] </ref> uses a novel directory-based cache design to interconnect a collection of 4-processor SGI boards based on the MIPS 3000 RISC processor. The Convex Exemplar employs a similar design based around the HP7100 PA-RISC. Avalanche will employ a similar directory-based cache design.
Reference: [22] <author> P. Nuth and W. J. Dally. </author> <title> A Mechanism for Efficient Context Switching. </title> <booktitle> In Proceedings of the IEEE International Conference on Computer Design, </booktitle> <pages> pages 301-304, </pages> <year> 1991. </year>
Reference-contexts: Alewife incorporates a limited amount of flexibility by allowing the controller to invoke specialized low-level software trap handlers to handle uncommon consistency operations, but currently the Alewife designers are only planning to use this capability to support an arbitrary number of "replica" pointers. The MIT M-Machine work <ref> [22] </ref> contains a context cache similar to previous designs such as the HP Mayfly system [14]. This context cache provides dynamic binding of variable names to register contents to permit rapid task switching and promote the interesting processor coupling mechanism of the M-machine.
Reference: [23] <author> S. Pakin and A. Chien. </author> <title> The impact of message traffic on multicomputer memory hierarchy performance. Concurrent Systems Architecture Group Memo, </title> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: A study by Pakin et al. showed that the choice of where to inject incoming data can have a tremendous impact on overall performance of an application <ref> [23] </ref>. Always injecting data to main memory, as in the Intel Paragon [18], results in a message latency too high for very fine grained applications, and the effect is getting worse as processor speeds increase.
Reference: [24] <author> S.K. Reinhardt, J.R. Larus, and D.A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: We call this problem the injection problem, and discuss several techniques that we are exploring to address the problem throughout the remainder of this paper. 3 One significant difference between Avalanche and related research projects <ref> [21, 20, 1, 24] </ref> is that we are not treating the CPU as an unmodifiable black box. The HP PA-RISC 7100 contains no on-chip cache but does contain the cache controller logic. The interface between the cache controller and the rest of the CPU is relatively straightforward. <p> These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols [9, 30], support multiple communication models [10, 17], or accept guidance from software <ref> [20, 24] </ref>. Programmable controllers would seem at first glance to be an ideal combination of software's greater flexibility and hardware's greater speed. <p> The level of primary processor cycle stealing that this implies will seriously impede scalability on conventional style applications based on DSM or message passing that do not exploit the *T's powerful support for data flow languages. Like Avalanche, the user level shared memory in the Tempest and Typhoon systems <ref> [24] </ref> will support cooperation between software and hardware to implement both scalable shared memory and message passing abstractions. Like the Alewife system, will support low level interaction between software and hardware to provide flexibility.
Reference: [25] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: While it is probably not reasonable to assume that this performance is achievable in general, it provides us with some insight into the value of allowing software to specify the coherence protocol at a small grain. 3.5 Benchmark Programs We used five programs from the SPLASH benchmark suite <ref> [25] </ref> in our study, mp3d, water, barnes, LocusRoute, and cholesky. Table 4 contains the inputs for each test program. mp3d is a three-dimensional particle simulator used to simulated rarified hypersonic airflow.
Reference: [26] <editor> Authors deleted to help preserve anonymity. </editor> <title> PPE-level protocols for carpet clusters. </title> <type> Technical Report, </type> <month> April </month> <year> 1994. </year>
Reference-contexts: The following two subsections describe these features. 2.2 Support for Message Passing Our previous work on high speed networking <ref> [26] </ref> made it clear that for high bandwidth interconnects the software protocol overhead and the time spent handling the cache misses required to load the data from the recipient's main memory to its highest level cache were major sources of communication latency. <p> Two important characteristics of sender-based protocols are that they are connection oriented and that both the sender and the receiver reserve portions of their address space as buffers for a given connection <ref> [26] </ref>. Based on its knowledge of the state of that buffer space, a sender can transmit a message to the receiver with the certainty that the message will be received into a known location in the receiver's memory.
Reference: [27] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 technical summary, </title> <year> 1991. </year>
Reference-contexts: Evidence of this situation can be seen in the significant differences between the peak performance of today's fast multiprocessing systems and the achieved performance. For example, even highly-tuned applications often achieve well under 50% of peak performance on multiprocessors such as the CM-5 <ref> [27] </ref> and Cray T3D [12] despite their powerful communication fabrics [3]. Even when the interconnection fabric is capable of very high speed communication, the effects of the memory hierarchy and the parasitic influence of other overheads become the dominant latency components. <p> Even this best case of 971 cycles dominates the propagation delay of the high-speed interconnects in current multiprocessors <ref> [18, 27] </ref>, which is a strong indication that as much effort needs to be placed in improving the performance of the memory system and the network controller's access to the memory as is being spent developing higher bandwidth and lower latency interconnects. <p> This imbalance becomes an even more dominant factor as message size grows. Specifically, with few exceptions <ref> [27] </ref>, conventional memory architectures require that messages always be transferred to or from main memory and that the cache be flushed as appropriate to maintain consistency [18]. Unfortunately this restriction guarantees that the receiver will incur cache misses for the entire message body. <p> Always injecting data to main memory, as in the Intel Paragon [18], results in a message latency too high for very fine grained applications, and the effect is getting worse as processor speeds increase. On the other hand, always injecting data to the cache, as in the CM-5 <ref> [27] </ref>, displaced so much active data that the overall cache miss rate of the applications increased 85-290%. Hence the appropriate level in the memory hierarchy for message placement will critically depend on the current context of the processing element. We are employing a number of techniques to make this decision. <p> In addition, incoming messages are placed into main memory via a DMA engine, using invalidation to maintain consistency, which results in cache misses that would not occur if the network controller was more tightly coupled with the memory system. The Thinking Machines CM-5 <ref> [27] </ref> did not directly support DSM or a multilevel external memory hierarchy, and as such the excellent communication fabric of the CM-5 is not well integrated into the memory architecture.
Reference: [28] <author> J.E. Veenstra and R.J. Fowler. </author> <title> A performance evaluation of optimal hybrid cache coherency protocols. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 149-160, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Using traces of shared memory parallel programs, researchers have found there are a small number of characteristic ways in which shared memory is accessed <ref> [5, 16, 28] </ref>. These characteristic "patterns" are sufficiently different from one another that any protocol designed to optimize one will not perform particularly well for the others.
Reference: [29] <author> J.E. Veenstra and R.J. Fowler. Mint: </author> <title> A front end for efficient simulation of shared-memory multiprocessors. </title> <booktitle> In MASCOTS 1994, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: Specifically, we report the results of a detailed architecture simulation study in which we explored the impact of using a number of coherence protocols, both individually and in an "optimal" combination. 3.1 MINT Multiprocessor Simulator We used the Mint memory hierarchy simulator <ref> [29] </ref> running on Silicon Graphics and Hewlett-Packard workstations to perform our simulations. Mint simulates a collection of processors and provides support for spinlocks, semaphores, barriers, shared memory, and most Unix system calls. We augmented it to support message passing and multiple processes per node.
Reference: [30] <author> A. Wilson and R. LaRowe. </author> <title> Hiding shared memory reference latency on the GalacticaNet distributed shared memory architecture. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 351-367, </pages> <year> 1992. </year> <title> 19 20 performance (Myrinet) performance (10*Myrinet) 21 </title>
Reference-contexts: These observations have led a number of researchers to propose building programmable multiprocessor cache controllers that can execute a variety of caching protocols <ref> [9, 30] </ref>, support multiple communication models [10, 17], or accept guidance from software [20, 24]. Programmable controllers would seem at first glance to be an ideal combination of software's greater flexibility and hardware's greater speed.
References-found: 31

