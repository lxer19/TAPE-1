URL: http://polaris.cs.uiuc.edu/reports/1218.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks T M Programs  
Author: William Blume and Rudolf Eigenmann 
Keyword: Automatic parallelization, restructuring techniques, effectiveness analysis, compiler evaluation, Perfect Benchmarks  
Address: Urbana, Illinois 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: We have studied the effectiveness of parallelizing compilers and the underlying transformation techniques. This paper reports the speedups of the Perfect Benchmarks TM codes that result from automatic parallelization. We have further measured the performance gains caused by individual restructuring techniques. Specific reasons for the successes and failures of the transformations are discussed, and potential improvements that result in measurably better program performance are analyzed. Our most important findings are that available restructurers often cause insignificant performance gains in real programs and that only few restructuring techniques contribute to this gain. However, we can also show that there is potential for advancing compiler technology so that many of the most important loops in these programs can be parallelized. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aiken and A. Nicolau. </author> <title> Optimal loop parallelization. </title> <booktitle> In Proceedings of the 1988 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: We will discuss some potential improvements through interprocedural analysis techniques in Section 5. 6 There is an additional range of newly proposed restructuring techniques that are not yet available in compilers. Notable techniques are those for high-level spreading [15, 19], loop pipelining <ref> [1] </ref>, optimizing loop synchronization [20, 22, 25], tiling [3, 18, 30, 31], recurrence parallelization [2, 5], and runtime data-dependence tests [32, 33].
Reference: [2] <author> Zahira Ammarguellat and Luddy Harrison. </author> <title> Automatic Recognition of Induction & Recurrence Relations by Abstract Interpretation. </title> <booktitle> Proceedings of Sigplan 1990, </booktitle> <address> Yorktown Heights, </address> <month> 25(6) </month> <pages> 283-295, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Notable techniques are those for high-level spreading [15, 19], loop pipelining [1], optimizing loop synchronization [20, 22, 25], tiling [3, 18, 30, 31], recurrence parallelization <ref> [2, 5] </ref>, and runtime data-dependence tests [32, 33]. The quantitative evaluation of this technology will take considerable effort in future projects. 3 Speedups from vectorization and concurrent execution Our measurements of the speedups from vectorization and concurrent execution are displayed in Figure 2.
Reference: [3] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with do loops. </title> <booktitle> In Proceedings of the Thrid ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 39-50, </pages> <month> April, </month> <year> 1991. </year>
Reference-contexts: Notable techniques are those for high-level spreading [15, 19], loop pipelining [1], optimizing loop synchronization [20, 22, 25], tiling <ref> [3, 18, 30, 31] </ref>, recurrence parallelization [2, 5], and runtime data-dependence tests [32, 33]. The quantitative evaluation of this technology will take considerable effort in future projects. 3 Speedups from vectorization and concurrent execution Our measurements of the speedups from vectorization and concurrent execution are displayed in Figure 2.
Reference: [4] <author> Clifford N. Arnold. </author> <title> Performance Evaluation of three Automatic Vectorizer Packages. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 235-242, </pages> <year> 1982. </year>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [4, 6, 7, 10, 23, 26] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [8, 13]. However, very few papers have reported evaluation measures of individual restructuring techniques [9, 13].
Reference: [5] <author> E. Ayguade, J. Labarta, J. Torres, and P. Borensztejn. GTS: </author> <title> Parrallelization and vectorization of tight recurrences. </title> <booktitle> In Proceedings Supercomputing '89, </booktitle> <address> Reno, Nevada, </address> <month> November 13-17, </month> <pages> pages 531-539. </pages> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: Notable techniques are those for high-level spreading [15, 19], loop pipelining [1], optimizing loop synchronization [20, 22, 25], tiling [3, 18, 30, 31], recurrence parallelization <ref> [2, 5] </ref>, and runtime data-dependence tests [32, 33]. The quantitative evaluation of this technology will take considerable effort in future projects. 3 Speedups from vectorization and concurrent execution Our measurements of the speedups from vectorization and concurrent execution are displayed in Figure 2.
Reference: [6] <author> Robert N. Braswell and Malcolm S. Keech. </author> <title> An Evaluation of Vector Fortran 200 Generated by Cyber 205 and ETA-10 Pre-Compilation Tools. </title> <booktitle> In Proc. Supercomputing `88, </booktitle> <pages> pages 106-113, </pages> <year> 1988. </year>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [4, 6, 7, 10, 23, 26] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [8, 13]. However, very few papers have reported evaluation measures of individual restructuring techniques [9, 13].
Reference: [7] <author> David Callahan, Jack Dongarra, and David Levine. </author> <title> Vectorizing Compilers: A Test Suite and Results. </title> <booktitle> In Proc. Supercomputing `88, </booktitle> <pages> pages 98-105, </pages> <year> 1988. </year> <month> 25 </month>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [4, 6, 7, 10, 23, 26] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [8, 13]. However, very few papers have reported evaluation measures of individual restructuring techniques [9, 13].
Reference: [8] <author> Doreen Y. Cheng and Douglas M. Pase. </author> <title> An Evaluation of Automatic and Interactive Parallel Programming Tools. </title> <booktitle> In Proc. Supercomputing '91, </booktitle> <pages> pages 412-422, </pages> <year> 1991. </year>
Reference-contexts: The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs <ref> [8, 13] </ref>. However, very few papers have reported evaluation measures of individual restructuring techniques [9, 13]. This paper extends the measurments presented in [13]. We measure both overall performance improvements from automatic parallelization and the contribution of individual restructuring techniques.
Reference: [9] <author> Ron Cytron, David J. Kuck, and Alex V. Veidenbaum. </author> <title> The effect of restructuring compilers on program performance for high-speed computers. </title> <booktitle> Special Issue of Computer Physics Communications devoted to the Proceedings of the Conference on Vector and Parallel Processors in Computational Science II, </booktitle> <volume> 37 </volume> <pages> 39-48, </pages> <year> 1985. </year> <type> Invited paper. </type>
Reference-contexts: The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [8, 13]. However, very few papers have reported evaluation measures of individual restructuring techniques <ref> [9, 13] </ref>. This paper extends the measurments presented in [13]. We measure both overall performance improvements from automatic parallelization and the contribution of individual restructuring techniques. The measurements are taken on an Alliant FX/80 machine using the Perfect Benchmarks programs fl This work was supported by the U.S.
Reference: [10] <author> Ulrich Detert. </author> <title> Programmiertechniken fur die Vektorisierung. </title> <booktitle> In Proc. Supercomputer `87, Mannheim, </booktitle> <address> Germany, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [4, 6, 7, 10, 23, 26] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [8, 13]. However, very few papers have reported evaluation measures of individual restructuring techniques [9, 13].
Reference: [11] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, Zhiyuan Li, and D. Padua. </author> <title> Restructuring Fortran Programs for Cedar. </title> <note> to appear in Concurrency: Practice and Experience, </note> <year> 1992. </year>
Reference-contexts: Solutions to these problems seem feasible, possibly at the cost of increased compilation time. Further evidence for potential improvement was delivered by the compiler group at CSRD which has been working on manually parallelizing the Perfect Benchmarks programs, applying mostly automatable techniques <ref> [11, 14] </ref>. The attained speedups ranged from 4 to 17, using the same measures as in Figure 2. 5.1 A case study: Parallelization of the code adm We use the adm program to illustrate these findings. Adm simulates air pollution concentration and deposition patterns for lake shore environments [29]. <p> While these two techniques illustrate two important areas that deserve more future attention, there is a range of techniques whose automation can cause significant program performance improvements, as described in <ref> [11, 14] </ref>. Given the number of newly proposed techniques, an interesting question is, which of these techniques would cause notable performance gains. We are not in the position to give a final answer to this, as we pointed out in Section 2.7. <p> We have derived this from manually restructuring the Perfect code Adm using techniques that we believe to be automatable. The findings for Adm are similar to those for other Perfect codes, reported elsewhere <ref> [11, 14] </ref>. Among the areas we identified are: certain implementation issues, advances of existing techniques, new analysis and transformation techniques, improved drivers for techniques and compiler passes, interprocedural optimization, and run-time driven optimization. The major performance gain is attributable to analysis and restructuring techniques that enable the recognition of parallelism.
Reference: [12] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> Cedar fortran and its restructuring compiler. </title> <editor> In A. Nicolau D. Gelernter, T. Gross and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing II. </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: This compiler was modified as part of the Cedar project conducted at CSRD <ref> [12] </ref>. As a byproduct, an Alliant/FX80 version of the restructurer has been developed. It contains a number of improvements over the original Kap version. For example, we have added strip mining capabilities to allow single loops to be vectorized and concurrentized. We also added switches to disable individual restructuring techniques.
Reference: [13] <author> Rudolf Eigenmann and William Blume. </author> <title> An Effectiveness Study of Parallelizing Compiler Techniques. </title> <booktitle> Proceedings of ICPP'91, </booktitle> <address> St. Charles, IL, II:17-25, </address> <month> August 12-16, </month> <year> 1991. </year>
Reference-contexts: The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs <ref> [8, 13] </ref>. However, very few papers have reported evaluation measures of individual restructuring techniques [9, 13]. This paper extends the measurments presented in [13]. We measure both overall performance improvements from automatic parallelization and the contribution of individual restructuring techniques. <p> The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [8, 13]. However, very few papers have reported evaluation measures of individual restructuring techniques <ref> [9, 13] </ref>. This paper extends the measurments presented in [13]. We measure both overall performance improvements from automatic parallelization and the contribution of individual restructuring techniques. The measurements are taken on an Alliant FX/80 machine using the Perfect Benchmarks programs fl This work was supported by the U.S. <p> However, very few papers have reported evaluation measures of individual restructuring techniques [9, 13]. This paper extends the measurments presented in <ref> [13] </ref>. We measure both overall performance improvements from automatic parallelization and the contribution of individual restructuring techniques. The measurements are taken on an Alliant FX/80 machine using the Perfect Benchmarks programs fl This work was supported by the U.S. Department of Energy under grant no.
Reference: [14] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the Automatic Paral-lelization of Four Perfect-Benchmark Programs. </title> <booktitle> Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: For adm, manually parallelizing these loop nests achieved a speedup of 6.6 from serial execution time. Impressive speedups of the 9 other three codes were also achieved either by parallelizing loops with call statements or using inline expansion <ref> [14, 17] </ref>. Other techniques that were used to achieve these improvements are described in section 5. The effect of I/O statements on program performance is less pronounced. In bdna, the important loop nest containing an I/O statement accounted for only 7.8% of the vector-concurrent execution time. <p> While this shows that this technique is not applied frequently in available compilers, there is an indication that it may become more important in the future: induction variable recognition was critical in the manual parallelization of trfd and mdg <ref> [14] </ref>. We pick loop interchanging to illustrate the applicability of a technique. Table 5 displays the percentage of loop nests that are parallelizable and at least doubly nested. This shows how many loops can potentially be interchanged; 47% of the loop nests can use the technique. <p> Solutions to these problems seem feasible, possibly at the cost of increased compilation time. Further evidence for potential improvement was delivered by the compiler group at CSRD which has been working on manually parallelizing the Perfect Benchmarks programs, applying mostly automatable techniques <ref> [11, 14] </ref>. The attained speedups ranged from 4 to 17, using the same measures as in Figure 2. 5.1 A case study: Parallelization of the code adm We use the adm program to illustrate these findings. Adm simulates air pollution concentration and deposition patterns for lake shore environments [29]. <p> Our search for new compilation technology has led to a much more aggressive parallelization of Adm, with a resulting speedup of 6.6. All of the top-level, important loop nests in this code were transformed into concurrent loops. We have found these transformations to be representative for those described in <ref> [14] </ref>. The following paragraphs illustrate the most important analysis and restructuring techniques applied to Adm. 5.2 Advancing restructuring techniques The performance gain in our experiments with adm were the result of two techniques: array privatization and parallel reductions. Array privatization can be considered an extension of the scalar expansion technique. <p> While these two techniques illustrate two important areas that deserve more future attention, there is a range of techniques whose automation can cause significant program performance improvements, as described in <ref> [11, 14] </ref>. Given the number of newly proposed techniques, an interesting question is, which of these techniques would cause notable performance gains. We are not in the position to give a final answer to this, as we pointed out in Section 2.7. <p> We have derived this from manually restructuring the Perfect code Adm using techniques that we believe to be automatable. The findings for Adm are similar to those for other Perfect codes, reported elsewhere <ref> [11, 14] </ref>. Among the areas we identified are: certain implementation issues, advances of existing techniques, new analysis and transformation techniques, improved drivers for techniques and compiler passes, interprocedural optimization, and run-time driven optimization. The major performance gain is attributable to analysis and restructuring techniques that enable the recognition of parallelism.
Reference: [15] <author> M. Girkar and C. Polychronopoulos. </author> <title> Optimization of data/control conditions in task graphs. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: We will discuss some potential improvements through interprocedural analysis techniques in Section 5. 6 There is an additional range of newly proposed restructuring techniques that are not yet available in compilers. Notable techniques are those for high-level spreading <ref> [15, 19] </ref>, loop pipelining [1], optimizing loop synchronization [20, 22, 25], tiling [3, 18, 30, 31], recurrence parallelization [2, 5], and runtime data-dependence tests [32, 33].
Reference: [16] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the ACM SIGPLAN `91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June 26-28, </month> <year> 1991, 1991. </year> <journal> Available as SIGPLAN Notices, </journal> <volume> vol. 26, no. 6, </volume> <pages> pp. 15-29, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Examples are statement reordering and loop distribution. In addition, there are complementing techniques not commonly called transformations, but which are important parts of restructuring compilers. Examples are last-value assignments and loop-normalizations. We did not evaluate data dependence tests; such measurements are being done in complementary projects <ref> [16, 24, 28] </ref>. Another important basis of restructuring compilers that we have not covered is the set of analysis techniques, such as the life-time analysis.
Reference: [17] <author> Jay Hoeflinger. </author> <title> QCD Optimization Report. </title> <type> Technical Report 1115, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomp. R&D, </institution> <year> 1991. </year>
Reference-contexts: For adm, manually parallelizing these loop nests achieved a speedup of 6.6 from serial execution time. Impressive speedups of the 9 other three codes were also achieved either by parallelizing loops with call statements or using inline expansion <ref> [14, 17] </ref>. Other techniques that were used to achieve these improvements are described in section 5. The effect of I/O statements on program performance is less pronounced. In bdna, the important loop nest containing an I/O statement accounted for only 7.8% of the vector-concurrent execution time.
Reference: [18] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 319-329, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: Notable techniques are those for high-level spreading [15, 19], loop pipelining [1], optimizing loop synchronization [20, 22, 25], tiling <ref> [3, 18, 30, 31] </ref>, recurrence parallelization [2, 5], and runtime data-dependence tests [32, 33]. The quantitative evaluation of this technology will take considerable effort in future projects. 3 Speedups from vectorization and concurrent execution Our measurements of the speedups from vectorization and concurrent execution are displayed in Figure 2.
Reference: [19] <author> H. Kasahara, H. Honda, M. Iwata, and M. Hirota. </author> <title> A compilation scheme for macro-dataflow computation on hierarchical multiprocessor systems. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <year> 1990. </year>
Reference-contexts: We will discuss some potential improvements through interprocedural analysis techniques in Section 5. 6 There is an additional range of newly proposed restructuring techniques that are not yet available in compilers. Notable techniques are those for high-level spreading <ref> [15, 19] </ref>, loop pipelining [1], optimizing loop synchronization [20, 22, 25], tiling [3, 18, 30, 31], recurrence parallelization [2, 5], and runtime data-dependence tests [32, 33].
Reference: [20] <author> V.P. Krothapalli and P. Sadayappan. </author> <title> Removal of redundant dependences in DOACROSS loops with constant dependences. </title> <booktitle> In SIGPLAN NOTICES: Third ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <address> Williamsburg, Virginia, </address> <month> April 21-24, </month> <pages> pages 51-60. </pages> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: We will discuss some potential improvements through interprocedural analysis techniques in Section 5. 6 There is an additional range of newly proposed restructuring techniques that are not yet available in compilers. Notable techniques are those for high-level spreading [15, 19], loop pipelining [1], optimizing loop synchronization <ref> [20, 22, 25] </ref>, tiling [3, 18, 30, 31], recurrence parallelization [2, 5], and runtime data-dependence tests [32, 33].
Reference: [21] <institution> Kuck & Associates, Inc., Champaign, Illinois. </institution> <note> KAP User's Guide, </note> <year> 1988. </year>
Reference-contexts: Wright Street, Urbana IL 61801 2 The Conjugate Gradient Algorithm 3 2.3 Compilers used The parallelizing compiler we used for our measurements is a modified version of Kap, the source-to-source restructurer developed at Kuck & Associates <ref> [21] </ref>. This compiler was modified as part of the Cedar project conducted at CSRD [12]. As a byproduct, an Alliant/FX80 version of the restructurer has been developed. It contains a number of improvements over the original Kap version.
Reference: [22] <author> Zhiyuan Li and Walid Abu-Sufah. </author> <title> On Reducing Data Synchronization in Multiprocessed Loops. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(1):105-109, </volume> <month> Jan., </month> <year> 1987. </year>
Reference-contexts: We will discuss some potential improvements through interprocedural analysis techniques in Section 5. 6 There is an additional range of newly proposed restructuring techniques that are not yet available in compilers. Notable techniques are those for high-level spreading [15, 19], loop pipelining [1], optimizing loop synchronization <ref> [20, 22, 25] </ref>, tiling [3, 18, 30, 31], recurrence parallelization [2, 5], and runtime data-dependence tests [32, 33].
Reference: [23] <author> G.R. Luecke, J. Coyle, W. Haque, J. Hoekstra, H. Jespersen, and R. Schmidt. </author> <title> A comparative study of KAP and VAST: two automatic preprocessors with Fortran 8x Output. Supercomputer 28, </title> <address> V(6):15-25, </address> <year> 1988. </year>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [4, 6, 7, 10, 23, 26] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [8, 13]. However, very few papers have reported evaluation measures of individual restructuring techniques [9, 13].
Reference: [24] <author> D. Maydan, J. Hennessy, and M. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In SIGPLAN NOTICES: Proceedings of the ACM SIGPLAN 91 Conferen ce on Programming Language Design and Implementation, </booktitle> <address> Toronto, Ontario, Canada, </address> <month> June 26-28, </month> <pages> pages 1-14. </pages> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: Examples are statement reordering and loop distribution. In addition, there are complementing techniques not commonly called transformations, but which are important parts of restructuring compilers. Examples are last-value assignments and loop-normalizations. We did not evaluate data dependence tests; such measurements are being done in complementary projects <ref> [16, 24, 28] </ref>. Another important basis of restructuring compilers that we have not covered is the set of analysis techniques, such as the life-time analysis.
Reference: [25] <author> Samuel Midkiff and David Padua. </author> <title> Compiler Algorithms for Synchronization. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(12):1485-1495, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: We will discuss some potential improvements through interprocedural analysis techniques in Section 5. 6 There is an additional range of newly proposed restructuring techniques that are not yet available in compilers. Notable techniques are those for high-level spreading [15, 19], loop pipelining [1], optimizing loop synchronization <ref> [20, 22, 25] </ref>, tiling [3, 18, 30, 31], recurrence parallelization [2, 5], and runtime data-dependence tests [32, 33].
Reference: [26] <author> H. Nobayashi and C. Eoyang. </author> <title> A Comparison Study of Automatically Vectorizing Fortran Compilers. </title> <booktitle> Proc. Supercomputing '89, </booktitle> <year> 1989. </year>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [4, 6, 7, 10, 23, 26] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [8, 13]. However, very few papers have reported evaluation measures of individual restructuring techniques [9, 13].
Reference: [27] <author> David A. Padua and Michael J. Wolfe. </author> <title> Advanced Compiler Optimizations for Supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: 1 Introduction 1.1 Motivation and Goals Over the years a large number of techniques have been developed to transform a sequential program so that it runs efficiently on a parallel architecture <ref> [27] </ref>. Many of these techniques have been incorporated into the compilers of these machines. These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. <p> We expect to learn what transformations cause or contribute to significant performance gains. Again, we will then investigate the reasons for successes or failures by examining whether and how the transformations were applied in important program sections. 4 We refer to <ref> [27] </ref> for an introduction to restructuring techniques. For the purposes of this paper we will define the terms by means of brief examples. All named techniques analyze and transform loops so that they can be executed in parallel.
Reference: [28] <author> Paul Petersen and David Padua. </author> <title> Machine-independent evaluation of parallelizing compilers. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1992. </year> <note> CSRD Report No. 1173. 26 </note>
Reference-contexts: Examples are statement reordering and loop distribution. In addition, there are complementing techniques not commonly called transformations, but which are important parts of restructuring compilers. Examples are last-value assignments and loop-normalizations. We did not evaluate data dependence tests; such measurements are being done in complementary projects <ref> [16, 24, 28] </ref>. Another important basis of restructuring compilers that we have not covered is the set of analysis techniques, such as the life-time analysis.
Reference: [29] <author> M. Berry; D. Chen; P. Koss; D. Kuck; L. Pointer, S. Lo; Y. Pang; R. Roloff; A. Sameh; E. Clementi, S. Chin; D. Schneider; G. Fox; P. Messina; D. Walker, C. Hsiung; J. Schwarzmeier; K. Lue; S. Orszag; F. Seidl, O. Johnson; G. Swanson; R. Goodrum, and J. Martin. </author> <title> The Perfect Club Benchmarks: Effective Performance Evalution of Supercomputers. </title> <booktitle> Int'l. Jour. of Supercomputer Applications, Fall 1989, </booktitle> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Finally, potential improvements are discussed. 1.2 Caveats As in all benchmarking reports, our measurements will be biased toward the machine and the compilers used, and inaccuracies from run-to-run variations. The program suite that we use as a representation of the "real world" is the Perfect Benchmarks T M suite <ref> [29] </ref>, and we are subject to all its caveats. These are: missing I/O information, compromised data sets, and missing throughput measures. Furthermore, one can always question the representativeness of these codes. There is no ideal way to avoid these problems. <p> The attained speedups ranged from 4 to 17, using the same measures as in Figure 2. 5.1 A case study: Parallelization of the code adm We use the adm program to illustrate these findings. Adm simulates air pollution concentration and deposition patterns for lake shore environments <ref> [29] </ref>. It is 6104 lines long and consists of 97 subroutines. The execution time is spread evenly throughout the program; 90% of the execution time is spent in 23 subroutines. Almost all of these subroutines contain 1-3 loop nests, all of which are important.
Reference: [30] <author> M.E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In SIGPLAN NOTICES: Proceedings of the ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Ontario, Canada, </address> <month> June 26-28, </month> <pages> pages 30-44. </pages> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: Notable techniques are those for high-level spreading [15, 19], loop pipelining [1], optimizing loop synchronization [20, 22, 25], tiling <ref> [3, 18, 30, 31] </ref>, recurrence parallelization [2, 5], and runtime data-dependence tests [32, 33]. The quantitative evaluation of this technology will take considerable effort in future projects. 3 Speedups from vectorization and concurrent execution Our measurements of the speedups from vectorization and concurrent execution are displayed in Figure 2.
Reference: [31] <author> M.J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proceedings Supercomputing '89, </booktitle> <address> Reno, Nevada, </address> <month> November 13-17, </month> <pages> pages 655-664. </pages> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: Notable techniques are those for high-level spreading [15, 19], loop pipelining [1], optimizing loop synchronization [20, 22, 25], tiling <ref> [3, 18, 30, 31] </ref>, recurrence parallelization [2, 5], and runtime data-dependence tests [32, 33]. The quantitative evaluation of this technology will take considerable effort in future projects. 3 Speedups from vectorization and concurrent execution Our measurements of the speedups from vectorization and concurrent execution are displayed in Figure 2.
Reference: [32] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proceedings of the 1991 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, Illinois, </address> <month> August 12-16, </month> <pages> pages 26-30. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1990. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: Notable techniques are those for high-level spreading [15, 19], loop pipelining [1], optimizing loop synchronization [20, 22, 25], tiling [3, 18, 30, 31], recurrence parallelization [2, 5], and runtime data-dependence tests <ref> [32, 33] </ref>. The quantitative evaluation of this technology will take considerable effort in future projects. 3 Speedups from vectorization and concurrent execution Our measurements of the speedups from vectorization and concurrent execution are displayed in Figure 2.
Reference: [33] <author> Chuan-Qi Zhu and Pen-Chung Yew. </author> <title> A Scheme to Enforce Data Dependence on Large Multiprocessor Systems. </title> <journal> IEEE Trans. on Software Eng., </journal> <volume> SE-13(6):726-739, </volume> <month> June </month> <year> 1987. </year> <month> 27 </month>
Reference-contexts: Notable techniques are those for high-level spreading [15, 19], loop pipelining [1], optimizing loop synchronization [20, 22, 25], tiling [3, 18, 30, 31], recurrence parallelization [2, 5], and runtime data-dependence tests <ref> [32, 33] </ref>. The quantitative evaluation of this technology will take considerable effort in future projects. 3 Speedups from vectorization and concurrent execution Our measurements of the speedups from vectorization and concurrent execution are displayed in Figure 2.
References-found: 33

