URL: http://www.cs.umn.edu/Users/dept/users/kumar/sparse-triangular-solve.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: agupta@cs.umn.edu kumar@cs.umn.edu  
Phone: Phone: 612-624-8023; Fax: 612-625-0572.  
Title: Parallel Algorithms for Forward Elimination and Backward Substitution in Direct Solution of Sparse Linear Systems  
Author: Anshul Gupta and Vipin Kumar 
Note: Presenting Author: Anshul Gupta Corresponding Author: Vipin Kumar;  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science University of Minnesota  
Abstract: A few parallel algorithms for solving triangular systems resulting from parallel factorization of sparse linear systems have been proposed and implemented recently. We present a detailed analysis of parallel complexity and scalability of the best of these algorithms and the results of its implementation on up to 256 processors of the Cray T3D parallel computer. It has been a common belief that parallel sparse triangular solvers are quite unscalable due to a high communication to computation ratio. Our analysis and experiments show that, although not as scalable as the best parallel sparse Cholesky factorization algorithms, parallel sparse triangular solvers can yield reasonable speedups in runtime on hundreds of processors. We also show that for a wide class of problems, the sparse triangular solvers described in this paper are optimal and are asymptotically as scalable as a dense triangular solver. fl This work is sponsored in part by the Army High Performance Computing Research Center under the auspices of the Department of the Army and Army Research Laboratory cooperative agreement number DAAH04-95-2-0003/contract number DAAH04-95-C-0008. The contents of of this paper do not necessarily reflect the position or the policy of the government, and no official endorsement should be inferred. Access to computing facilities was provided by Cray Research Inc., Minnesota Supercomputer Institute and by the Pittsburgh Supercomputing Center. Related papers are available via at URL: http://www.cs.umn.edu/users/kumar/papers.html via WWW. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: If a nested-dissection based ordering scheme is used to number the nodes of the graph corresponding to the coefficient matrix, then the number of nodes t in a supernode at level l is ff q N=2 l , where ff is a small constant <ref> [11, 10, 1, 4] </ref>. <p> is proportional to log p1 log p1 q which is O (p) + O ( p The overall computation is proportional to the number of non-zeros in L, which is O (N log N ) for an N fi N sparse coefficient matrix resulting from a two-dimensional finite element problem <ref> [1] </ref> with a nested-dissection based ordering. Assuming that the computation is divided uniformly among the processors, each processor spends O ((N log N )=p) time in computation. <p> p If the underlying graph corresponding to the coefficient matrix is a three-dimensional neighborhood graph [14] (as is the case in three-dimensional finite element and finite difference problems), then the value of t at level l is roughly ff (N=2 l ) 2=3 , where ff is a small constant <ref> [1, 4] </ref>. The value of q at level l is p=2 l . <p> Assuming that the overall computation of O (N 4=3 ) <ref> [1] </ref> is uniformly distributed among the processors, the parallel runtime is given by the following equation: T P = O ( p If more than one (say m) right-hand side vectors are present in the system, then each term in Equations 1 and 2 is multiplied with m. 3.2 Scalability analysis
Reference: [2] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Communication reduction in parallel sparse Cholesky factorization on a hypercube. </title> <editor> In M. T. Heath, editor, </editor> <booktitle> Hypercube Multiprocessors 1987, </booktitle> <pages> pages 576-586. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1987. </year>
Reference-contexts: However, this distribution is not suitable for the triangular solvers, which are scalable only with a one-dimensional partitioning of the supernodal blocks of L. We show that if the supernodes are distributed in a subtree-to-subcube manner <ref> [2] </ref> then the cost of converting the two-dimensional distribution to a one-dimensional distribution is only a constant times the cost of solving the triangular systems. <p> to multiple right-hand sides by replacing all vector operations by the corresponding matrix operations. 2.1 Forward elimination The basic approach to forward elimination is very similar to that of multifrontal numerical factorization [12] guided by an elimination tree [13, 8] with the distribution of computation determined by a subtree-to-subcube mapping <ref> [2] </ref>. A symmetric sparse matrix, its lower triangular Cholesky factor, and the corresponding elimination tree with subtree-to-subcube mapping onto 8 processors is shown in Figure 1. The computation in forward elimination starts with the leaf supernodes of the elimination tree and progresses upwards to terminate at the root supernode.
Reference: [3] <author> Ananth Grama, Anshul Gupta, and Vipin Kumar. Isoefficiency: </author> <title> Measuring the scalability of parallel algorithms and architectures. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(3) </volume> <pages> 12-21, </pages> <month> August, </month> <year> 1993. </year> <note> Also available as Technical Report TR 93-24, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: In this section we use the isoefficiency metric <ref> [8, 9, 3] </ref> to characterize the scalability of the algorithm described in Section 2. The isoefficiency function relates the problem size to the number of processors necessary to maintain a fixed efficiency or to deliver speedups increasing proportionally with increasing number of processors.
Reference: [4] <author> Anshul Gupta, George Karypis, and Vipin Kumar. </author> <title> Highly scalable parallel algorithms for sparse matrix factorization. </title> <type> Technical Report 94-63, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> Submitted for publication in IEEE Transactions on Parallel and Distributed Computing. Postscript file available in users/kumar at anonymous FTP site ftp.cs.umn.edu. </note>
Reference-contexts: Since numerical factorization is computationally the most expensive phase, a significant research effort has been directed towards developing efficient and scalable parallel sparse factorization algorithms. We have recently proposed <ref> [4] </ref> a parallel sparse Cholesky factorization algorithm that is optimally scalable for a wide class of problems. Experiments have shown that this algorithm can easily speedup Cholesky factorization by a factor of at least a few hundred on up to 1024 processors. <p> In addition to the performance and scalability analysis of parallel sparse triangular solvers, we discuss the redistribution of the triangular factor matrix among the processors between numerical factorization and triangular solution, and its impact on performance. In <ref> [4] </ref>, we describe an optimal data-distribution scheme for Cholesky factorization of sparse 2 matrices. This distribution leaves groups of consecutive columns of L with identical pat-tern of non-zeros (henceforth called supernodes) with a two-dimensional partitioning among groups of processors. <p> Although there are bound to be overheads due to unequal distribution of work, it is not possible to model such overheads analytically because the extent of such overheads is data-dependent. From our experience with actual implementations of parallel triangular solvers as well as parallel factorization codes <ref> [4] </ref>, we have observed that such overheads are usually not excessive. Moreover, the overhead due to load imbalance in most practical cases tends to saturate at 32 to 64 processors for most problems and does not continue to increase as the number of processors are increased. <p> If a nested-dissection based ordering scheme is used to number the nodes of the graph corresponding to the coefficient matrix, then the number of nodes t in a supernode at level l is ff q N=2 l , where ff is a small constant <ref> [11, 10, 1, 4] </ref>. <p> p If the underlying graph corresponding to the coefficient matrix is a three-dimensional neighborhood graph [14] (as is the case in three-dimensional finite element and finite difference problems), then the value of t at level l is roughly ff (N=2 l ) 2=3 , where ff is a small constant <ref> [1, 4] </ref>. The value of q at level l is p=2 l . <p> In <ref> [4] </ref>, we described parallel algorithms for sparse Cholesky factorization of the same class of matrices with an isoefficiency function of O (p 1:5 ), which is better than the O (p 2 ) isoefficiency function of the corresponding triangular solver. <p> However, as we have shown in <ref> [4] </ref>, the dense supernodes must be partitioned along both dimensions for the numerical factorization phase to be efficient. The table in Figure 5 shows the communication overheads and the isoefficiency functions for parallel dense and sparse factorization and triangular solution using one- and two-dimensional partitioning schemes. <p> If more than one right-hand side vectors are used, then the cost of redistribution can be amortized further because the redistribution needs to be done only once. 5 Experimental Results We implemented the algorithms described in Section 2 and integrated them with our sparse Cholesky factorization algorithms described in <ref> [4] </ref>. Figures 7 and 8 show the performance of the parallel triangular solvers on a Cray T3D. <p> The process of obtaining a direct solution to a sparse system of linear equations usually consists of four phases: reordering, symbolic factorization, numerical factorization, and forward elimination and backward substitution. A scalable parallel solver for sparse linear systems must implement all these phases effectively in parallel. In <ref> [4] </ref>, we introduced a highly scalable parallel algorithm for sparse Cholesky factorization, which is the most time consuming phase of solving a sparse 16 linear system with s symmetric positive definite (SPD) matrix of coefficients.
Reference: [5] <author> M. T. Heath and Padma Raghavan. </author> <title> Distributed solution of sparse linear systems. </title> <type> Technical Report 93-1793, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1993. </year>
Reference-contexts: In this paper, we address the problem of performing the final phase of forward and backward substitution in parallel on a distributed memory multiprocessor. We present a detailed analysis of the parallel complexity and scalability of parallel algorithm described briefly in <ref> [5] </ref> to obtain a solution to the system of sparse linear equations of the forms LY = B and U X = Y , where L is a lower triangular matrix and U is an upper triangular matrix. <p> Of course, if more than one systems need to be solved with the same coefficient matrix, then the one-time redistribution cost is amortized. 2 Algorithm Description In this section, we describe parallel algorithms for sparse forward elimination and backward substitution, which have been discussed briefly in <ref> [5] </ref>.
Reference: [6] <author> M. T. Heath and C. H. Romine. </author> <title> Parallel solution of triangular systems on distributed-memory multiprocessors. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(3) </volume> <pages> 558-588, </pages> <year> 1988. </year>
Reference-contexts: In <ref> [6] </ref>, Heath and Romine describe efficient pipelined or wavefront algorithms for solving dense triangular systems with block-cyclic row-wise and column-wise partitioning of the triangular matrices.
Reference: [7] <author> George Karypis and Vipin Kumar. </author> <title> Parallel graph partitioning and matrix reordering. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Minnesota, Minneapo-lis, MN, </institution> <year> 1995. </year> <note> Submitted to Supercomputing '95. </note>
Reference-contexts: In [4], we introduced a highly scalable parallel algorithm for sparse Cholesky factorization, which is the most time consuming phase of solving a sparse 16 linear system with s symmetric positive definite (SPD) matrix of coefficients. In <ref> [7] </ref>, Karypis and Kumar present an efficient parallel algorithm for a nested-dissection based fill-reducing ordering for such sparse matrices. The results of this paper bring us another step closer to a complete scalable direct solver for sparse SPD systems.
Reference: [8] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: a single right-hand side vector; however, the algorithm can easily be generalized to multiple right-hand sides by replacing all vector operations by the corresponding matrix operations. 2.1 Forward elimination The basic approach to forward elimination is very similar to that of multifrontal numerical factorization [12] guided by an elimination tree <ref> [13, 8] </ref> with the distribution of computation determined by a subtree-to-subcube mapping [2]. A symmetric sparse matrix, its lower triangular Cholesky factor, and the corresponding elimination tree with subtree-to-subcube mapping onto 8 processors is shown in Figure 1. <p> If the two child supernodes are each distributed among q processors, then this communication is equivalent to an all-to-all personalized communication <ref> [8] </ref> among 2q processors with a data size of roughly t=q on each processor. This communication can be accomplished in time proportional to t=q, which is asymptotically smaller than the O (q) + O (t) time spent during the pipelined computation phase at the child supernodes. <p> In this section we use the isoefficiency metric <ref> [8, 9, 3] </ref> to characterize the scalability of the algorithm described in Section 2. The isoefficiency function relates the problem size to the number of processors necessary to maintain a fixed efficiency or to deliver speedups increasing proportionally with increasing number of processors. <p> Thus, the overall scalability cannot be better than that of solving the topmost N 2=3 fi N 2=3 dense triangular system in parallel, which is O (p 2 ). 4 Data Distribution for Efficient Triangular Solution In Section 2 and in <ref> [8] </ref>, we discuss that in order to implement the steps of dense triangular solution efficiently, the matrix must be partitioned among the processors along the rows or along the columns. <p> As shown in Figure 6, the redistribution is equivalent to a transposition of each (n= p q) fi t rectangular block of the supernode among the p q processor on which it is horizontally partitioned. This is an all-to-all personalized communication operation <ref> [8] </ref> among p q processors with each processor holding nt=q words of data. Although 12 and triangular solution with different partitioning schemes. cost of redistribution are the same with block-cyclic partitioning as well. The communication time for this all-to-all personalized operation is O (nt=q) [8]. <p> is an all-to-all personalized communication operation <ref> [8] </ref> among p q processors with each processor holding nt=q words of data. Although 12 and triangular solution with different partitioning schemes. cost of redistribution are the same with block-cyclic partitioning as well. The communication time for this all-to-all personalized operation is O (nt=q) [8]. Note that for solving a triangular system with a single right-hand side, each processor performs O (nt=q) computation while processing an n fi t supernode on q processors.
Reference: [9] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 379-391, </pages> <year> 1994. </year> <note> Also available as Technical Report TR 91-18, </note> <institution> Department of Computer Science Department, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: In this section we use the isoefficiency metric <ref> [8, 9, 3] </ref> to characterize the scalability of the algorithm described in Section 2. The isoefficiency function relates the problem size to the number of processors necessary to maintain a fixed efficiency or to deliver speedups increasing proportionally with increasing number of processors.
Reference: [10] <author> R. J. Lipton, D. J. Rose, and R. E. Tarjan. </author> <title> Generalized nested dissection. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 </volume> <pages> 346-358, </pages> <year> 1979. </year>
Reference-contexts: If a nested-dissection based ordering scheme is used to number the nodes of the graph corresponding to the coefficient matrix, then the number of nodes t in a supernode at level l is ff q N=2 l , where ff is a small constant <ref> [11, 10, 1, 4] </ref>.
Reference: [11] <author> R. J. Lipton and R. E. Tarjan. </author> <title> A separator theorem for planar graphs. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 36 </volume> <pages> 177-189, </pages> <year> 1979. </year>
Reference-contexts: If a nested-dissection based ordering scheme is used to number the nodes of the graph corresponding to the coefficient matrix, then the number of nodes t in a supernode at level l is ff q N=2 l , where ff is a small constant <ref> [11, 10, 1, 4] </ref>.
Reference: [12] <author> J. W.-H. Liu. </author> <title> The multifrontal method for sparse matrix solution: Theory and practice. </title> <type> Technical Report CS-90-04, </type> <institution> York University, </institution> <address> Ontario, Canada, </address> <year> 1990. </year> <note> Also appears in SIAM Review, </note> <month> 34 </month> <pages> 82-109, </pages> <year> 1992. </year>
Reference-contexts: The description in this section assumes a single right-hand side vector; however, the algorithm can easily be generalized to multiple right-hand sides by replacing all vector operations by the corresponding matrix operations. 2.1 Forward elimination The basic approach to forward elimination is very similar to that of multifrontal numerical factorization <ref> [12] </ref> guided by an elimination tree [13, 8] with the distribution of computation determined by a subtree-to-subcube mapping [2]. A symmetric sparse matrix, its lower triangular Cholesky factor, and the corresponding elimination tree with subtree-to-subcube mapping onto 8 processors is shown in Figure 1. <p> One of the blocks of L shown in Figure 2 is the dense trapezoidal supernode consisting of nodes 6, 7, and 8. For this supernode, n = 4 and t = 3. As in the case of multifrontal numerical factorization <ref> [12] </ref>, the computation in forward and backward triangular solvers can also be organized in terms of dense matrix operations. In forward elimination (see Figure 2), before the computation starts at a supernode, the 3 mapping onto 8 processors.
Reference: [13] <author> J. W.-H. Liu. </author> <title> The role of elimination trees in sparse factorization. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 11 </volume> <pages> 134-172, </pages> <year> 1990. </year>
Reference-contexts: a single right-hand side vector; however, the algorithm can easily be generalized to multiple right-hand sides by replacing all vector operations by the corresponding matrix operations. 2.1 Forward elimination The basic approach to forward elimination is very similar to that of multifrontal numerical factorization [12] guided by an elimination tree <ref> [13, 8] </ref> with the distribution of computation determined by a subtree-to-subcube mapping [2]. A symmetric sparse matrix, its lower triangular Cholesky factor, and the corresponding elimination tree with subtree-to-subcube mapping onto 8 processors is shown in Figure 1.
Reference: [14] <author> Gary L. Miller, Shang-Hua Teng, and Stephen A. Vavasis. </author> <title> A unified geometric approach to graph separators. </title> <booktitle> In Proceedings of 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 538-547, </pages> <year> 1991. </year>
Reference-contexts: Therefore, we will focus on the problems in which the original matrix is the adjacency matrix of a two- or three-dimensional neighborhood graph <ref> [14] </ref>. These classes of matrices include the coefficient matrices generated in all two- and three-dimensional finite element and finite difference problems. We also assume as that a nested-dissection based fill-reducing ordering is used, which results in an almost balanced elimination tree. <p> Hence, the parallel runtime for forward elimination algorithm described in Section 2 is as follows 1 : T P = O ( p p If the underlying graph corresponding to the coefficient matrix is a three-dimensional neighborhood graph <ref> [14] </ref> (as is the case in three-dimensional finite element and finite difference problems), then the value of t at level l is roughly ff (N=2 l ) 2=3 , where ff is a small constant [1, 4]. The value of q at level l is p=2 l .
References-found: 14

