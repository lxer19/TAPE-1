URL: http://www.cs.umn.edu/Users/dept/users/kumar/tag_parse1_ncube.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: nurkkala@cs.umn.edu kumar@cs.umn.edu  
Title: A Parallel Parsing Algorithm for Natural Language using Tree Adjoining Grammar  
Author: Tom Nurkkala Vipin Kumar 
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science, University of Minnesota,  
Abstract: Tree Adjoining Grammar (TAG) is a powerful grammatical formalism for large-scale natural language processing. However, the computational complexity of parsing algorithms for TAG is high. We introduce a new parallel TAG parsing algorithm for MIMD hypercube multicomputers, using large-granularity grammar partitioning, asynchronous communication, and distributed termination detection. We describe our implementation on the nCUBE/2 parallel computer, and provide experimental results on both random and English grammars. Our algorithm delivers the best performance of any TAG parsing algorithm to date, yielding an almost two order-of-magnitude speedup and good efficiency on up to 256 processors. TAG parsing is a highly unstructured problem. Based on our experience developing a parallel TAG parser, we draw some general conclusions for solving other unstructured problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Abeille. </author> <title> A lexicalized tree adjoining grammar for French: The general framework. </title> <type> Technical Report MS-CIS-88-64, </type> <institution> Department of Computer and Information Science, University of Pennsylvania, </institution> <year> 1988. </year>
Reference-contexts: It is this property of TAGs that give them their expressive power. Several articles provide examples of the use of TAGs in natural language processing <ref> [8, 6, 3, 1] </ref>. The expressive power of TAG comes at increased parsing cost. Although context-free grammars can be parsed in O (n 3 ) time (for input strings of length n), general TAGs require time O (n 6 ). This complexity is prohibitive for even moderate length input strings.
Reference: [2] <author> A. Aho and J. Ullman. </author> <title> The Theory of Parsing, Translation, and Compiling, Vol. 1: Parsing. </title> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: This complexity is prohibitive for even moderate length input strings. Thus, TAG parsing algorithms are good candidates for parallelization. Vijay-Shankar and Joshi [17] presented the first feasible sequential parsing algorithm for TAGs. Their parser is a bottom-up, CYK-style parser <ref> [2] </ref> with a time complexity of fi (n 6 ). A drawback of this parser is that it requires TAGs in normalized form, similar to Chomsky normal form for context-free grammar. A parallel parsing algorithm for TAGs was introduced by Palis, Shende, and Wei [12]. <p> Thus, future work can be predicted by noting when adjunction or substitution is performed in a tree, and at what node. 6.2 Earley-Style Algorithm Schabes [15] introduces an Earley-style parsing algorithm <ref> [2] </ref> for general TAGs. The algorithm exhibits an asymptotic behavior of O (n 6 ), but has better average-case complexity because it uses top-down prediction as well as bottom-up parsing. Thus, a parallel version of this algorithm should yield improved performance relative to the parser described here.
Reference: [3] <author> K. Bishop, S. Cote, and A. Abeille. </author> <title> A lexicalized tree adjoining grammar for English. </title> <type> Technical report, </type> <institution> Department of Computer and Information Science, University of Pennsylvania, </institution> <year> 1989. </year>
Reference-contexts: It is this property of TAGs that give them their expressive power. Several articles provide examples of the use of TAGs in natural language processing <ref> [8, 6, 3, 1] </ref>. The expressive power of TAG comes at increased parsing cost. Although context-free grammars can be parsed in O (n 3 ) time (for input strings of length n), general TAGs require time O (n 6 ). This complexity is prohibitive for even moderate length input strings.
Reference: [4] <author> Edsger W. Dijkstra, W. H. J. Feijen, and A. J. M. van Gasteren. </author> <title> Derivation of a termination detection algorithm for distributed computations. </title> <journal> Information Processing Letters, </journal> <volume> 16 </volume> <pages> 217-219, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: Values from 100 s to 100 ms yield similar performance. Global termination is implemented using Dijkstra's distributed termination detection algorithm <ref> [4] </ref>. This algorithm circulates a termination token around a ring mapped onto the hypercube. The token circulates concurrently with parsing and one-to-all broadcast activity.
Reference: [5] <author> Ananth Grama, Anshul Gupta, and Vipin Kumar. </author> <title> Isoefficiency function: A scalability metric for parallel algorithms and architectures. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <year> 1993. </year>
Reference-contexts: Similar data show that the crossover point shifts toward smaller inputs as the grammar size decreases. This shift reflects the decreased communication required for smaller grammars, which have fewer root index tuples to broadcast. 5.2.3 Scalability In many parallel algorithms, more work implies greater concurrency and higher performance <ref> [9, 5] </ref>. Such algorithms are scalable in that they deliver good performance on larger numbers of processors provided the work load is increased sufficiently. In this algorithm, increased work is due to two factors: grammar size and sentence length. Effect of Grammar Size.
Reference: [6] <author> A. K. Joshi. </author> <title> How much context-sensitifity is necessary for characterizing structural descriptions|tree adjoining grammars. </title> <editor> In D. Dowty, L. Karttunen, and A. Zwicky, editors, </editor> <booktitle> Natural Language Proessing| Theoretical, Computational and Psychological Perspectives. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: It is this property of TAGs that give them their expressive power. Several articles provide examples of the use of TAGs in natural language processing <ref> [8, 6, 3, 1] </ref>. The expressive power of TAG comes at increased parsing cost. Although context-free grammars can be parsed in O (n 3 ) time (for input strings of length n), general TAGs require time O (n 6 ). This complexity is prohibitive for even moderate length input strings.
Reference: [7] <author> A. K. Joshi, L. S. Levy, and M. Takahashi. </author> <title> Tree adjunct grammars. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 10 </volume> <pages> 136-63, </pages> <year> 1975. </year>
Reference-contexts: One such language model is Tree Adjoining Grammar (TAG). TAGs can express complex linguistic phenomena more naturally than can context-free grammars, the underlying language model employed in most existing natural language systems. Tree Adjoining Grammar was introduced by Joshi, Levy, and Takahashi <ref> [7] </ref>, who show that the languages generated by TAGs are in the class of mildly context-sensitive languages, strictly between context-free and context-sensitive languages. It is this property of TAGs that give them their expressive power.
Reference: [8] <author> A. S. Kroch and A. K. Joshi. </author> <title> The linguistic relevance of tree adjoining grammars. </title> <type> Technical Report MS-CIS-85-16, </type> <institution> University of Pennsylvania, Department of Computer and Information Science, </institution> <month> April </month> <year> 1985. </year>
Reference-contexts: It is this property of TAGs that give them their expressive power. Several articles provide examples of the use of TAGs in natural language processing <ref> [8, 6, 3, 1] </ref>. The expressive power of TAG comes at increased parsing cost. Although context-free grammars can be parsed in O (n 3 ) time (for input strings of length n), general TAGs require time O (n 6 ). This complexity is prohibitive for even moderate length input strings.
Reference: [9] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Ben-jamin/Cummings, </address> <year> 1994. </year>
Reference-contexts: Similar data show that the crossover point shifts toward smaller inputs as the grammar size decreases. This shift reflects the decreased communication required for smaller grammars, which have fewer root index tuples to broadcast. 5.2.3 Scalability In many parallel algorithms, more work implies greater concurrency and higher performance <ref> [9, 5] </ref>. Such algorithms are scalable in that they deliver good performance on larger numbers of processors provided the work load is increased sufficiently. In this algorithm, increased work is due to two factors: grammar size and sentence length. Effect of Grammar Size.
Reference: [10] <author> Tom Nurkkala. </author> <title> Parallel Algorithms for a Highly Unstructured Problem: Natural Language Parsing using Tree Adjoining Grammar. </title> <type> PhD thesis, </type> <institution> University of Minnesota, </institution> <note> 1994 (to appear). </note>
Reference-contexts: This should result in better raw performance (i.e., smaller T p ), since fewer index sets need to be processed in each iteration of the parsing loop. Our current work focuses on parallel formulations of LTAG <ref> [10] </ref>. Early results on the KSR1 parallel computer at the University of Florida show promising results|80% efficiency on 32 processors for a moderate size grammar and input string [11]. 7 Conclusions We have introduced a new parallel algorithm for natural language parsing using Tree Adjoining Grammar.
Reference: [11] <author> Tom Nurkkala and Vipin Kumar. </author> <title> The performance of a highly unstructured parallel algorithm on the KSR1. In Proceedings of the Scalable High Performance Computing Conference. </title> <note> 1994 (to appear). </note>
Reference-contexts: Our current work focuses on parallel formulations of LTAG [10]. Early results on the KSR1 parallel computer at the University of Florida show promising results|80% efficiency on 32 processors for a moderate size grammar and input string <ref> [11] </ref>. 7 Conclusions We have introduced a new parallel algorithm for natural language parsing using Tree Adjoining Grammar. Our MIMD implementation delivers good speedup and efficiency on both random and English grammars. It delivers the best performance of any TAG parser to date.
Reference: [12] <author> Michael A. Palis, Sunil Shende, and David S. L. Wei. </author> <title> An optimal linear-time parallel parser for tree adjoining languages. </title> <journal> SIAM Journal on Computing, </journal> <volume> 19(1) </volume> <pages> 1-31, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: A drawback of this parser is that it requires TAGs in normalized form, similar to Chomsky normal form for context-free grammar. A parallel parsing algorithm for TAGs was introduced by Palis, Shende, and Wei <ref> [12] </ref>. Their algorithm runs in linear time on the length of the input string, but requires O (n 5 ) processors in a systolic array, and only works with normalized TAGs. More recently, Palis and Wei [14] have designed a new parsing algorithm for general TAGs.
Reference: [13] <author> Michael A. Palis and David S. L. Wei. </author> <title> Parallel parsing of tree adjoining grammars on the connection machine. </title> <type> Technical report, </type> <institution> University of Pennsylvania, </institution> <year> 1991. </year>
Reference-contexts: Typically, the algorithm runs in O (n 5 ) time, and thus is superior on average to Vijay-Shankar and Joshi's algorithm. Palis and Wei have introduced a SIMD parallel implementation of their parser on the Connection Machine CM-2 <ref> [13, 14] </ref>. Giving speedup results for various grammar sizes, they show that their algorithm has the expected asymptotic behavior relative to G. However, their data also show a speedup saturation, and hence an upper bound on efficiency. <p> If such a tree can be composed, the TAG accepts the input string; otherwise, it rejects it. 3 Sequential Algorithm Our parsing algorithm is based on the the sequential algorithm presented by Palis and Wei <ref> [13, 14] </ref>. It takes as input a TAG G and an input string w, where w is an n-word string of terminals a 1 a n , and returns a boolean value indicating whether G accepts w. <p> the speedup, and E = S=p is the efficiency. 5.1 Performance 5.1.1 Random Grammars Table 1 shows the performance of the algorithm with a grammar of 1,024 random elementary trees (eight nodes per tree average), parsing a 20-word input string. (These data are comparable to those of Palis and Wei <ref> [13, 14] </ref>.) Note that since the memory size of a single node of the nCUBE/2 is insufficient to store the number of index tuples generated by this large grammar, the value for p = 1 is estimated, based on the time spent performing useful parsing activity for p = 4. 1
Reference: [14] <author> Michael A. Palis and David S. L. Wei. </author> <title> Massively parallel parsing algorithms for natural language. </title> <editor> In L. Kanal, Vipin Kumar, Christian Suttner, and H. Ki-tano, editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence. </booktitle> <publisher> North-Holland, </publisher> <year> 1993. </year>
Reference-contexts: A parallel parsing algorithm for TAGs was introduced by Palis, Shende, and Wei [12]. Their algorithm runs in linear time on the length of the input string, but requires O (n 5 ) processors in a systolic array, and only works with normalized TAGs. More recently, Palis and Wei <ref> [14] </ref> have designed a new parsing algorithm for general TAGs. In previous work, grammar size (G) was considered a constant, and parsing complexity was expressed in terms of input string length (n). This makes sense for program compilation, where typically n AE G. <p> Typically, the algorithm runs in O (n 5 ) time, and thus is superior on average to Vijay-Shankar and Joshi's algorithm. Palis and Wei have introduced a SIMD parallel implementation of their parser on the Connection Machine CM-2 <ref> [13, 14] </ref>. Giving speedup results for various grammar sizes, they show that their algorithm has the expected asymptotic behavior relative to G. However, their data also show a speedup saturation, and hence an upper bound on efficiency. <p> If such a tree can be composed, the TAG accepts the input string; otherwise, it rejects it. 3 Sequential Algorithm Our parsing algorithm is based on the the sequential algorithm presented by Palis and Wei <ref> [13, 14] </ref>. It takes as input a TAG G and an input string w, where w is an n-word string of terminals a 1 a n , and returns a boolean value indicating whether G accepts w. <p> the speedup, and E = S=p is the efficiency. 5.1 Performance 5.1.1 Random Grammars Table 1 shows the performance of the algorithm with a grammar of 1,024 random elementary trees (eight nodes per tree average), parsing a 20-word input string. (These data are comparable to those of Palis and Wei <ref> [13, 14] </ref>.) Note that since the memory size of a single node of the nCUBE/2 is insufficient to store the number of index tuples generated by this large grammar, the value for p = 1 is estimated, based on the time spent performing useful parsing activity for p = 4. 1
Reference: [15] <author> Yves Schabes. </author> <title> Mathematical and Computational Aspects of Lexicalized Grammars. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, Department of Computer and Information Science, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Thus, future work can be predicted by noting when adjunction or substitution is performed in a tree, and at what node. 6.2 Earley-Style Algorithm Schabes <ref> [15] </ref> introduces an Earley-style parsing algorithm [2] for general TAGs. The algorithm exhibits an asymptotic behavior of O (n 6 ), but has better average-case complexity because it uses top-down prediction as well as bottom-up parsing. <p> The algorithm exhibits an asymptotic behavior of O (n 6 ), but has better average-case complexity because it uses top-down prediction as well as bottom-up parsing. Thus, a parallel version of this algorithm should yield improved performance relative to the parser described here. Furthermore, Schabes and Joshi <ref> [16, 15] </ref> detail a variant of TAG called Lexicalized Tree Adjoining Grammar (LTAG). In LTAG, each elementary tree contains at least one leaf node labeled with a terminal symbol called its anchor .
Reference: [16] <author> Yves Schabes and A. K. Joshi. </author> <title> An earley-type parsing algorithm for tree adjoining grammars. </title> <type> Technical Report MS-CIS-88-36, </type> <institution> University of Pennsylvania, Department of Computer and Information Science, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: The algorithm exhibits an asymptotic behavior of O (n 6 ), but has better average-case complexity because it uses top-down prediction as well as bottom-up parsing. Thus, a parallel version of this algorithm should yield improved performance relative to the parser described here. Furthermore, Schabes and Joshi <ref> [16, 15] </ref> detail a variant of TAG called Lexicalized Tree Adjoining Grammar (LTAG). In LTAG, each elementary tree contains at least one leaf node labeled with a terminal symbol called its anchor .
Reference: [17] <author> K. Vijay-Shankar and A. K. Joshi. </author> <title> Some computational properties of tree adjoining grammars. </title> <booktitle> In Proceedings of the 11th Meeting of the Association of Computational Linguistics, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: Although context-free grammars can be parsed in O (n 3 ) time (for input strings of length n), general TAGs require time O (n 6 ). This complexity is prohibitive for even moderate length input strings. Thus, TAG parsing algorithms are good candidates for parallelization. Vijay-Shankar and Joshi <ref> [17] </ref> presented the first feasible sequential parsing algorithm for TAGs. Their parser is a bottom-up, CYK-style parser [2] with a time complexity of fi (n 6 ). A drawback of this parser is that it requires TAGs in normalized form, similar to Chomsky normal form for context-free grammar.
References-found: 17

