URL: http://www.cs.toronto.edu/~mackay/cpi_short.ps.gz
Refering-URL: http://www.cs.toronto.edu/~mackay/README.html
Root-URL: http://www.cs.toronto.edu
Email: mackay@mrao.cam.ac.uk  
Title: PROGRAMME FOR INDUSTRY Modelling Phase Transformations in Steels Bayesian Non-Linear Modelling with Neural Networks Neural
Author: David J.C. MacKay 
Note: I will discuss the  
Date: 16:00, Tuesday 21 March 1995  
Address: Cambridge, CB3 0HE.  
Affiliation: UNIVERSITY OF CAMBRIDGE  Cavendish Laboratory,  
Abstract: Bayesian probability theory provides a unifying framework for data modeling which offers several benefits. First, the overfitting problem can be solved by using Bayesian methods to control model complexity. Second, probabilistic modelling handles uncertainty in a natural manner. There is a unique prescription, marginalization, for incorporating uncertainty about parameters into predictions. Third, we can define more sophisticated probabilistic models which are able to extract more information from data. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Berger, J. </author> <year> (1985). </year> <title> Statistical Decision theory and Bayesian Analysis, </title> <publisher> Springer. </publisher>
Reference-contexts: Bayesian inference satisfies the likelihood principle <ref> (Berger 1985) </ref>: our inferences depend only on the probabilities assigned to the data that were received, not on properties of other data sets 1 One would expect better predictions to be obtained by combining the predictions of several good models. 20March 6, 1995 D.J.C.
Reference: <author> Bhadeshia, H. K. D. H., MacKay, D. J. C. and Svensson, L. E. </author> <year> (1995). </year> <title> Bayesian neural network modelling of weld toughness, </title> <booktitle> Materials Science and Technology. </booktitle>
Reference: <author> Bishop, C. M. </author> <year> (1992). </year> <title> Exact calculation of the Hessian matrix for the multilayer perceptron, </title> <booktitle> Neural Computation 4(4): </booktitle> <pages> 494-501. </pages>
Reference: <author> Box, G. E. P. and Tiao, G. C. </author> <year> (1973). </year> <title> Bayesian inference in statistical analysis, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Method I trained a large number of neural nets using the ARD model, for each of the prediction problems. The data seemed to include some substantial glitches. Because I had not yet developed an automatic Bayesian noise model that anticipates outliers (though this could be done <ref> (Box and Tiao 1973) </ref>), I omitted by hand those data points which gave large residuals relative to the first models that were trained.
Reference: <author> Breiman, L. </author> <year> (1992). </year> <title> Stacked regressions, </title> <type> Technical Report 367, </type> <institution> Dept. of Stat., Univ. of Cal. Berkeley. </institution>
Reference-contexts: The weighting coefficients are the posterior probabilities, which are obtained from the evidences P (DjH). If we cannot evaluate these accurately then alternative pragmatic prescriptions for the weighting coefficients exist <ref> (Breiman 1992) </ref> (see appendix B for further details). 5 Prediction competition The American Society of Heating, Refrigeration and Air Conditioning Engineers organised a prediction competition in 1993. <p> The size of the committee was chosen so as to minimize the validation error of the mean predictions. This method of selecting committee size has also been described under the name `stacked generalization' <ref> (Breiman 1992) </ref>. In all cases, a committee was found that performed significantly better on the validation set than any individual model. 14March 6, 1995 D.J.C.
Reference: <author> Cox, R. </author> <year> (1946). </year> <title> Probability, frequency, and reasonable expectation, </title> <journal> Am. J. Physics 14: </journal> <pages> 1-13. </pages>
Reference-contexts: Finally, Bayesian model comparison embodies Occam's razor, the principle that states a preference for simple models. This point is expanded on in MacKay (1992a) and other references. Joint, conditional and marginal probabilities The language of coherent inference is probability theory <ref> (Cox 1946) </ref>. All coherent beliefs and predictions can be mapped onto probabilities. I will use the following notation for conditional probabilities: P (AjB; H) is pronounced `the probability of A, given B and H'.
Reference: <author> Gull, S. F. </author> <year> (1989). </year> <title> Developments in maximum entropy data analysis, </title> <editor> in J. Skilling (ed.), </editor> <title> Maximum Entropy and Bayesian Methods, </title> <address> Cambridge 1988, </address> <publisher> Kluwer, Dordrecht, </publisher> <pages> pp. 53-71. </pages>
Reference-contexts: As shown in <ref> (Gull 1989, MacKay 1992a) </ref>, the maximum evidence ff = ff MP satisfies the following implicit equation: 9March 6, 1995 D.J.C.
Reference: <author> Jervis, T. T. and Fitzgerald, W. J. </author> <year> (1993). </year> <title> Optimization schemes for neural networks, </title> <type> Technical Report CUED/F-INFENG/TR 144, </type> <institution> Cambridge University Engineering Department, </institution> <address> Trumpington Street, Cambridge, England. </address>
Reference: <author> MacKay, D. J. C. </author> <year> (1992a). </year> <title> Bayesian interpolation, </title> <booktitle> Neural Computation 4(3): </booktitle> <pages> 415-447. </pages>
Reference: <author> MacKay, D. J. C. </author> <year> (1992b). </year> <title> The evidence framework applied to classification networks, </title> <booktitle> Neural Computation 4(5): </booktitle> <pages> 698-714. </pages>
Reference: <author> MacKay, D. J. C. </author> <year> (1992c). </year> <title> A practical Bayesian framework for backpropagation networks, </title> <booktitle> Neural Computation 4(3): </booktitle> <pages> 448-472. </pages>
Reference-contexts: Details of how to compute A are given elsewhere, e.g., <ref> (MacKay 1992c, Bishop 1992) </ref>.] The inverse A 1 of this matrix defines the covariance matrix of the Gaussian approximation. <p> One might argue that this matrix should be evaluated anyway, in order to compute the uncertainty in a model's predictions. This matrix may be evaluated explicitly <ref> (MacKay 1992c, Bishop 1992) </ref>, which does not take significant time when the number of parameters is small (a few hundred). For large problems these calculations can be performed more efficiently using algorithms which evaluate products Av without explicitly evaluating A (Skilling 1993, Pearlmutter 1994).
Reference: <author> MacKay, D. J. C. </author> <year> (1995a). </year> <title> Bayesian neural networks and density networks, Nuclear Instruments and Methods in Physics Research, </title> <booktitle> Section A 354(1): </booktitle> <pages> 73-80. </pages>
Reference-contexts: That there are no missing inputs. If there are in fact missing inputs then caution is needed. It is not valid to set missing inputs to zero. Probabilistic models that can handle missing inputs <ref> (MacKay 1995a) </ref> are hard work to implement if they are based on neural networks. An idea worth looking into might be to represent each input by two numbers defining a confidence interval; missing inputs would have the broades interval.
Reference: <author> MacKay, D. J. C. </author> <year> (1995b). </year> <title> Probable networks and plausible predictions a review of practical Bayesian methods for supervised neural networks, Network: Computation in Neural Systems. </title> <note> to appear. </note>
Reference-contexts: Just as in the previous section we optimized a single regularization constant ff by maximizing the evidence, we can derive equivalent prescriptions for optimizing multiple regularization constants. The formulae are similar to equations (14) and (15), and are given for example in <ref> (MacKay 1995b) </ref>. For simplicity, the following discussion will assume once more that there is only a single parameter ff.
Reference: <author> MacKay, D. J. C. and Oldfield, M. J. </author> <year> (1995). </year> <title> Generalization error and the number of hidden units in a multilayer perceptron, </title> <note> in preparation. 25March 6, 1995 D.J.C. </note> <author> MacKay: </author> <title> Bayesian Neural Networks Neal, </title> <editor> R. M. </editor> <year> (1993). </year> <title> Bayesian learning via stochastic dynamics, </title> <editor> in C. L. Giles, S. J. Hanson and J. D. Cowan (eds), </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 475-482. </pages>
Reference-contexts: Eyeball the data to check for glitches and outliers. 3. Choose the measure of validation and test performance carefully. It is popular to use the test error (sum squared error) as the default performance measure, but in fact this may be a misleading criterion <ref> (MacKay and Oldfield 1995) </ref>. In many applications there will be an opportunity not to simply make a scalar prediction, but rather to make a prediction with error bars, or maybe an even more complicated predictive distribution.
Reference: <author> Neal, R. M. </author> <year> (1994). </year> <title> Priors for infinite networks, </title> <note> Technical Report in preparation, </note> <institution> Univ. of Toronto. </institution>
Reference-contexts: Figure 4a shows one function for each of a sequence of values of oe bias , oe in and oe out . The sort of functions that we obtain depend on the values of oe bias , oe in and oe out <ref> (Neal 1994) </ref>. As the weights and biases are made bigger we obtain more complex functions with more features and a greater sensitivity to the input variable.
Reference: <author> Pearlmutter, B. A. </author> <year> (1994). </year> <title> Fast exact multiplication by the Hessian, </title> <booktitle> Neural Computation 6(1): </booktitle> <pages> 147-160. </pages>
Reference: <author> Press, W., Flannery, B., Teukolsky, S. A. and Vetterling, W. T. </author> <year> (1988). </year> <title> Numerical Recipes in C, </title> <publisher> Cambridge. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E. and Williams, R. J. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors, </title> <booktitle> Nature 323: </booktitle> <pages> 533-536. </pages>
Reference-contexts: This minimization is based on repeated evaluation of the gradient of E D using `backpropagation' <ref> (Rumelhart, Hinton and Williams 1986) </ref>.
Reference: <author> Skilling, J. </author> <year> (1993). </year> <title> Bayesian numerical analysis, </title> <editor> in W. T. Grandy, Jr. and P. Milonni (eds), </editor> <title> Physics and Probability, C.U.P., </title> <publisher> Cambridge. </publisher>
Reference-contexts: This matrix may be evaluated explicitly (MacKay 1992c, Bishop 1992), which does not take significant time when the number of parameters is small (a few hundred). For large problems these calculations can be performed more efficiently using algorithms which evaluate products Av without explicitly evaluating A <ref> (Skilling 1993, Pearlmutter 1994) </ref>. Multiple regularization constants and `automatic relevance determina tion' For simplicity, it has so far been assumed that there is only a single class of weights, which are modelled as coming from a single Gaussian prior with oe 2 W = 1=ff.
Reference: <author> Spiegelhalter, D. J. and Lauritzen, S. L. </author> <year> (1990). </year> <title> Sequential updating of conditional probabilities on directed graphical structures, </title> <booktitle> Networks 20: </booktitle> <pages> 579-605. </pages> <address> 26March 6, </address> <year> 1995 </year> <month> D.J.C. </month> <title> MacKay: Bayesian Neural Networks </title>
Reference-contexts: An idea worth looking into might be to represent each input by two numbers defining a confidence interval; missing inputs would have the broades interval. An alternative approach is to turn to more tractable graphical probabilistic models <ref> (Spiegelhalter and Lauritzen 1990) </ref>, which can handle missing inputs with no problems, but don't have the same non-linear capabilities as neural networks. The BUGS program developed by Spiegelhalter et al is an excellent environment for implementing graphical models.
References-found: 20

