URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3755/3755.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: aporter@cs.umd.edu johnson@hawaii.edu  
Title: Analysis of Two Experimental Studies  
Author: Adam A. Porter Philip M. Johnson 
Date: February 22, 1997  
Abstract: Assessing Software Review Meetings: Results of a Comparative Abstract Software review is a fundamental tool for software quality assurance. Nevertheless, there are significant controversies as to the most efficient and effective review method. One of the most important questions currently being debated is the utility of meetings. Although almost all industrial review methods are centered around the inspection meeting, recent findings call their value into question. To gain insight into these issues, the two authors of this paper separately and independently conducted controlled experimental studies. This paper discusses a joint effort to understand the broader implications of these two studies. To do this, we designed and carried out a process of "reconciliation" in which we established a common framework for the comparison of the two experimental studies, re-analyzed the experimental data with respect to this common framework, and compared the results. Through this process we found many striking similarities between the results of the two studies, strengthening their individual conclusions. It also revealed interesting differences between the two experiments, suggesting important avenues for future research.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> IEEE Guide to Software Requirements Specifications. Soft. Eng. Tech. Comm. of the IEEE Computer Society, </institution> <year> 1984. </year> <note> IEEE Std 830-1984. </note>
Reference-contexts: The references for these lectures were Fagan [6], Parnas [16], and the IEEE Guide to Software Requirements Specifications <ref> [1] </ref>. The participants were then divided into three-person teams - (see Section 4.3.4 for details.) Within each team, members were randomly assigned to act as the moderator, the recorder, or the reader during the collection meeting. Draft: February 22, 1997 13 panel shows each team's fault detection ratio.
Reference: [2] <author> Mark A. Ardis. </author> <title> Lessons from using basic lotos. </title> <booktitle> In Sixteenth International Conference on Software Engineering, </booktitle> <pages> pages 5-14, </pages> <address> Sorrento, Italy, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The specification documents may not be representative of real programming problems. The experimental specifications are atypical of industrial SRS in two ways. First, most of the experimental specification is written in a formal requirements notation (see Section 4.3.6). Although some industrial groups are experimenting with formal notations <ref> [2, 9] </ref>, it is not the industry's standard practice. Second, the specifications used are considerably shorter than industrial specifications. Finally, the review process in our experimental design may not be representative of software development practice.
Reference: [3] <author> G. E. P. Box, W. G. Hunter, and J. S. Hunter. </author> <title> Statistics for Experimenters. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: The second step was to evaluate the combined effect of the variables shown to be significant in the initial analysis. Both analyses used standard nonparametric analysis methods (see <ref> [3] </ref> or [12]).
Reference: [4] <author> M. Diehl and W. Stroebe. </author> <title> Productivity loss in brainstorming groups: Toward the solution of a riddle. </title> <journal> Journal of Personality and Social Psychology, </journal> (53):497-509, 1987. 
Reference-contexts: By comparing the performance of real groups with nominal groups, the experiment attempts to tease out the effects that the meeting alone has on overall review performance. Although there is prior research on real vs. nominal group performance, these studies have focussed on idea generation, not software review <ref> [4, 15] </ref>. 3.2 Hypotheses The main research question was, "Are there differences in detection effectiveness (the number of program defects detected) and detection cost (the amount of effort/time to find a defect) when subjects review source code using a synchronous, same-place same-time interaction (real groups) versus an asynchronous, same-place same-time interaction
Reference: [5] <author> Stephen G. Eick, Clive R. Loader, M. David Long, Scott A. Vander Wiel, and Lawrence G. Votta. </author> <title> Estimating software fault content before coding. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 59-65, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Furthermore, he was unable to demonstrate the presence of "synergy" within the inspection meetings. A related study by Votta and others found that 90% of the defects were found during the preparation phase, leaving only 10% discovered during the meeting <ref> [5] </ref>. These results appear to support Parnas and Weiss' claim that whole group meetings are unnecessary for defect collection. Parnas and Weiss' claim and Votta's results stand in direct contradiction to Fagan's. <p> Interestingly, this is consistent with capture-recapture techniques <ref> [5] </ref> for estimating the number of defects remaining in a previously inspected artifact. That research says that, in general, the amount of overlap in N independent reviews will increase as the observed defect density increases.
Reference: [6] <author> M. E. Fagan. </author> <title> Design and code inspections to reduce errors in program development. </title> <journal> IBM Systems Journal, </journal> <volume> 15(3) </volume> <pages> 182-211, </pages> <year> 1976. </year>
Reference-contexts: The references for these lectures were Fagan <ref> [6] </ref>, Parnas [16], and the IEEE Guide to Software Requirements Specifications [1]. The participants were then divided into three-person teams - (see Section 4.3.4 for details.) Within each team, members were randomly assigned to act as the moderator, the recorder, or the reader during the collection meeting.
Reference: [7] <author> Michael E. Fagan. </author> <title> Design and code inspections to reduce errors in program development. </title> <journal> IBM System Journal, </journal> <volume> 15(3) </volume> <pages> 182-211, </pages> <year> 1976. </year>
Reference-contexts: After that we present the comparative analysis, and, finally, we discuss our conclusions and recommendations for future research. 2 RELATED WORK Reviewing software is as old as programming itself. However, the first structured, measurement-based review process was Michael Fagan's five-step Inspection method <ref> [7] </ref>. * Overview: the author presents an overview of the scope and purpose of the work product. * Preparation: reviewers analyze the work product with the goal of understanding it thoroughly. * Inspection meeting: the inspection team assembles and the reader paraphrases the work product. <p> Only during the Inspection meeting does defect identification become an explicit goal. Fagan notes that "sometimes flagrant errors are found during [preparation], but in general, the number of errors found is not nearly as high as in the [inspection meeting]" <ref> [7] </ref>. Second, the Inspection meeting involves a specific technique, paraphrasing, which generates an in-depth analysis of the entire document in real-time by the review team during the meeting.
Reference: [8] <author> George A. Ferguson and Yoshio Takane. </author> <title> Statistical Analysis In Psychology And Education. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> 6 edition, </address> <year> 1989. </year>
Reference-contexts: To minimize the third threat, the experimental review methods were based on descriptions of industrial practice of software review. 3.3.5 Analysis Strategy Most of the research questions were tested using the Wilcoxon signed rank test <ref> [8] </ref>. This non-parametric test of significance does not make any assumption regarding the underlying distribution of the data. It is based on the rank of differences between each pair of observations in the dataset. The data analysis proceeds in the following way.
Reference: [9] <author> S. Gerhart, D. Craigen, and T. Ralston. </author> <title> Experience with formal methods in critical systems. </title> <journal> IEEE Software, </journal> <volume> 11(1) </volume> <pages> 21-28, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The specification documents may not be representative of real programming problems. The experimental specifications are atypical of industrial SRS in two ways. First, most of the experimental specification is written in a formal requirements notation (see Section 4.3.6). Although some industrial groups are experimenting with formal notations <ref> [2, 9] </ref>, it is not the industry's standard practice. Second, the specifications used are considerably shorter than industrial specifications. Finally, the review process in our experimental design may not be representative of software development practice.
Reference: [10] <author> Tom Gilb and Dorothy Graham. </author> <title> Software Inspection. </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: For example, Tom Gilb developed a comprehensive inspection method with precisely defined phases, metrics, and suggested process rates for optimum defect removal effectiveness <ref> [10] </ref>. With few exceptions, these variations never challenged a fundamental premise of Fagan's original method: that a face-to-face meeting of the entire review team is essential to the review's success. <p> In this case, reviewers have two preparation goals: comprehending the work product and detecting defects. A second modification is to change the meeting technique from paraphrasing to defect collection <ref> [10, 13] </ref>. This shifts the focus of the meeting away from the work product and onto the issues raised during preparation. The Active Design Review technique [16] invented by David Parnas and David Weiss makes even more radical modifications to the preparation goals and meeting technique.
Reference: [11] <author> G.V. Glass, B. McGaw, </author> <title> and M.L Smith. Meta-analysis in Social Research. </title> <publisher> SAGE, </publisher> <address> Beverly Hills, CA, </address> <year> 1981. </year>
Reference-contexts: The drawback of this approach is that it lacks precise methods for combining different results. A statistical approach for integrating multiple studies is called meta-analysis <ref> [11] </ref>. This approach has two steps. First, the experimenters attempt to reconcile the primary experiments - i.e define a common framework with which to compare different studies. This involves defining common terms, hypotheses, and metrics, and characterizing key differences.
Reference: [12] <author> R. M. </author> <title> Heiberger. Computation for the Analysis of Designed Experiments. </title> <publisher> Wiley & Sons, </publisher> <address> New York, New York, </address> <year> 1989. </year>
Reference-contexts: The second step was to evaluate the combined effect of the variables shown to be significant in the initial analysis. Both analyses used standard nonparametric analysis methods (see [3] or <ref> [12] </ref>).
Reference: [13] <author> Watts S. Humphrey. </author> <title> Managing the Software Process. </title> <publisher> Addison Wesley Publishing Company Inc., </publisher> <year> 1990. </year>
Reference-contexts: In this case, reviewers have two preparation goals: comprehending the work product and detecting defects. A second modification is to change the meeting technique from paraphrasing to defect collection <ref> [10, 13] </ref>. This shifts the focus of the meeting away from the work product and onto the issues raised during preparation. The Active Design Review technique [16] invented by David Parnas and David Weiss makes even more radical modifications to the preparation goals and meeting technique.
Reference: [14] <author> Philip M. Johnson. </author> <title> An instrumented approach to improving software quality through formal technical review. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering, </booktitle> <address> Sorrento, Italy, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Such research has ranged from the design of computer-supported cooperative work systems that implement an asynchronous, non-meeting-based review procedure <ref> [14] </ref>, to alternative manual methods that also shift the process away from reliance on meetings [17]. fl This work is supported in part by a National Science Foundation Faculty Early Career Development Award CCR-9501354. y This work is supported in part by a grant from the National Science Foundation (CCR-9403475). 1
Reference: [15] <author> B. Mullen, C. Johnson, and E. Salas. </author> <title> Productivity loss in brainstorming groups. </title> <journal> Basic and Applied Social Psychology, </journal> (12):3-24, 1991. 
Reference-contexts: By comparing the performance of real groups with nominal groups, the experiment attempts to tease out the effects that the meeting alone has on overall review performance. Although there is prior research on real vs. nominal group performance, these studies have focussed on idea generation, not software review <ref> [4, 15] </ref>. 3.2 Hypotheses The main research question was, "Are there differences in detection effectiveness (the number of program defects detected) and detection cost (the amount of effort/time to find a defect) when subjects review source code using a synchronous, same-place same-time interaction (real groups) versus an asynchronous, same-place same-time interaction
Reference: [16] <author> Dave L. Parnas and David M. Weiss. </author> <title> Active design reviews: </title> <booktitle> principles and practices. In Proceedings of the 8th International Conference on Software Engineering, </booktitle> <pages> pages 215-222, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: A second modification is to change the meeting technique from paraphrasing to defect collection [10, 13]. This shifts the focus of the meeting away from the work product and onto the issues raised during preparation. The Active Design Review technique <ref> [16] </ref> invented by David Parnas and David Weiss makes even more radical modifications to the preparation goals and meeting technique. <p> The references for these lectures were Fagan [6], Parnas <ref> [16] </ref>, and the IEEE Guide to Software Requirements Specifications [1]. The participants were then divided into three-person teams - (see Section 4.3.4 for details.) Within each team, members were randomly assigned to act as the moderator, the recorder, or the reader during the collection meeting.
Reference: [17] <author> Adam Porter, Lawrence G. Votta, and Victor Basili. </author> <title> Comparing detection methods for software requirement inspections: A replicated experim ent. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(6) </volume> <pages> 563-575, </pages> <month> June </month> <year> 1995. </year> <note> Draft: February 22, 1997 25 </note>
Reference-contexts: Such research has ranged from the design of computer-supported cooperative work systems that implement an asynchronous, non-meeting-based review procedure [14], to alternative manual methods that also shift the process away from reliance on meetings <ref> [17] </ref>. fl This work is supported in part by a National Science Foundation Faculty Early Career Development Award CCR-9501354. y This work is supported in part by a grant from the National Science Foundation (CCR-9403475). 1 Draft: February 22, 1997 2 In addition to this research, a few studies have tried
Reference: [18] <author> Adam Porter, Lawrence G. Votta, and Victor Basili. </author> <title> Comparing detection methods for software requirement inspections: A replicated experim ent. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(6) </volume> <pages> 563-575, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Review specialization can increase overall defect detection effectiveness by reducing the issue duplication rate. More research needs to be done concerning the most effective ways to specialize the tasks of reviewers for particular domains (See Porter et al. <ref> [18] </ref>). Draft: February 22, 1997 24 Finally, this analysis provides no definitive answer to the tantalizing question of whether or not there are classes of defects that are predictably better found using meetings (or, conversely, better found not using meetings).
Reference: [19] <author> Adam A. Porter and Lawrence G. Votta. </author> <title> An experiment to assess different defect detection methods for software requriements inspections. </title> <type> Technical Report CS-TR-3130, </type> <institution> University of Maryland, College Park, </institution> <address> MA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Therefore, the analysis strategy was to isolate the first and second phase performances to see how well they explain differences in total review performance. This analysis is restricted to WLMS reviews only, and includes data from 14 WLMS reviews taken from two earlier review experiments <ref> [19] </ref>. These reviews involved both graduate student and professional populations and all used the DC method. This was done to increase our sample of DC reviews.
Reference: [20] <author> Glen W. Russel. </author> <title> Experience with inspections in ultralarge-scale developments. </title> <journal> IEEE Software, </journal> <volume> 8(1) </volume> <pages> 25-31, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: They require the simultaneous attendance of all participants. Their effectiveness depends on satisfying many conditions, such as adequate preparation, readiness of the work product for review, high quality moderation, and cooperative interpersonal relationships. Meeting-based review appears to add 15-20% new overhead onto development costs <ref> [20] </ref>, and simple scheduling issues have been shown to lengthen the start-to-finish interval for review by almost one third [22].
Reference: [21] <author> Danu Tjahjono. </author> <title> Exploring the effectiveness of formal technical review factors with CSRS, a collaborative software review system. </title> <type> PhD thesis, </type> <institution> Department of Information and Computer Sciences, University of Hawaii, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: Space constraints preclude a complete discussion of all the research questions any hypotheses considered in this experiment; for complete details, see the dissertation 1 by Danu Tjahjono <ref> [21] </ref>. 3.3 Experimental Design 3.3.1 Subjects The subjects were 27 undergraduate students enrolled in ICS-313 (Programming Language Theory) and 45 undergraduate students enrolled in ICS-411 (System Programming) classes at the University of Hawaii in the Spring of 1995.
Reference: [22] <author> Lawrence G. Votta. </author> <booktitle> Does every inspection need a meeting? In Proceedings of ACM SIGSOFT '93 Symposium on Foundations of Software Engineering. Association for Computing Machinery, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Meeting-based review appears to add 15-20% new overhead onto development costs [20], and simple scheduling issues have been shown to lengthen the start-to-finish interval for review by almost one third <ref> [22] </ref>. The costs of meeting-based review have stimulated more recent research designed to investigate whether new review methods can be devised that minimize or eliminate the cost of meetings while preserving the remaining benefits of review. <p> Parnas and Weiss deployed this method for the design of a military flight navigation system with favorable results, although he did not report any quantitative measures of effectiveness. Larry Votta built upon Parnas' research in a study of Lucent (Formerly AT&T) developers <ref> [22] </ref>. He collected data on the perceived utility of meetings by developers as well as several statistics on their outcome. His data showed that within the development environment studied, scheduling conflicts appeared to lengthen the total time of an inspection by approximately 30%.
References-found: 22

