URL: http://www.cs.purdue.edu/homes/li/draft/event.ps.gz
Refering-URL: http://www.cs.purdue.edu/homes/li/publications.html
Root-URL: http://www.cs.purdue.edu
Email: li@csrd.uiuc.edu  
Title: Compiler Algorithms for Event Variable Synchronization  
Author: Zhiyuan Li 
Date: May 13, 1991  
Address: St., Urbana, IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign 305 Talbot Lab., 104 S. Wright  
Abstract: Event variable synchronization is a well-known mechanism for enforcing data dependences in a program that runs in parallel on a shared memory multiprocessor. This paper presents compiler algorithms to automatically generate event variable synchronization code. Previously published algorithms dealt with single parallel loops in which dependence distances are constant and known by the compiler. However, loops in real application programs are often arbitrarily nested. Moreover, compilers are often unable to determine dependence distances. In contrast, our algorithms generate synchronization code based directly on array subscripts and do not require constant distances in data dependences. The algorithms are designed for arbitrarily nested loops, including triangular or trapezoidal loops. 
Abstract-found: 1
Intro-found: 1
Reference: [AK87] <author> J.R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transaction on Programming Languages and Systems, </journal> <volume> 9(4), </volume> <month> October </month> <year> 1987. </year>
Reference-contexts: Second, our algorithms do not require constant dependence distances. Instead, we generate synchronization code based directly on array subscripts and loop bounds. Of course, we only generate synchronization for a dependence that is proved or assumed to exist between parallel loop iterations by data dependence analysis <ref> [WB87, AK87] </ref>. 2 Background and Assumptions Program constructs Given an arbitrary nest of DO loops, a compiler may decide to parallelize all loops or serialize some of the loops. Our task is to generate the necessary synchronization code to support the intended parallelism and to guarantee the correct computation results. <p> Our task is to generate the necessary synchronization code to support the intended parallelism and to guarantee the correct computation results. Data dependences between different loop iterations are called loop carried dependences <ref> [AK87] </ref>. We make two assumptions to ensure that explicit synchronization 1 is needed only for loop carried dependences: First, the processors executing a parallel loop may exit only after all iterations are completed, and second, statements in the same iteration of a parallel loop execute sequentially in their original order.
Reference: [ASU86] <author> A.V. Aho, R. Sethi, and J.D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1986. </year>
Reference-contexts: In the above definition, we use the term "dominates" in the same sense as in flow analysis <ref> [ASU86] </ref>. We 8 DO I1 = 1, N DOALL I1 = 1, N DO I2 = 1, N DOALL I2 = 1, N IF (C) THEN IF (C) THEN ... ... ELSE POST (EV1 (I1,I2)) ... ELSE ... ...
Reference: [CKS90] <author> D. Callahan, K. Kennedy, and J. Subhlok. </author> <title> Analysis of event synchronization in a parallel programming tool. </title> <booktitle> In Proceedings of the 2nd ACM SIGPAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 21-31, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Implementation of the event variable synchronization is straightforward on virtually any shared memory multiprocessor. Manually inserted synchronization code is error-prone. Recently, Callahan, Kennedy, and Subhlok proposed a static analysis to detect errors in user-inserted synchronization code <ref> [CKS90] </ref>. In this paper, we describe a method that generates event variable synchronization automatically. Midkiff and Padua presented compiler algorithms to generate event posts and waits in single parallel loops, assuming that all data dependences have known constant distances [MP87]. <p> Using this fact, we can avoid inserting complementary loops. A complete discussion is beyond the scope of this paper. Dependence coverage If one dependence covers others, then only one pair of post and wait is needed to preserve all those dependences. Some previous works <ref> [LAS87, MP87, CKS90] </ref> discussed dependence coverage in simple cases such as constant dependence distances. Using mask predicates, more complicated cases can be handled. A necessary condition for one dependence to cover another is that the mask predicate of the latter should imply that of the former.
Reference: [CSY90] <author> D.-K. Chen, H.-M. Su, and P.-C. Yew. </author> <title> The impact of synchronization and granularity on parallel systems. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Some recent simulation studies report tremendous potential parallelism in many application programs. However, the studies also report frequent data dependences across execution threads which must be preserved by means of synchronization <ref> [Kum88, CSY90] </ref>. Most commercial shared memory multiprocessors provide synchronization mechanisms to preserve data dependences between parallel loop iterations.
Reference: [EPY89] <author> P.A. Emrath, D.A. Padua, and P.-C. Yew. </author> <title> Cedar architecture and its software. </title> <booktitle> In Proceedings of 22nd Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1989. </year>
Reference-contexts: Correspondingly, instead of performing multiple waits at the dependence sink, the processor busy-waits until the shared counter accumulates to the number of contacts. Such a counter has been implemented on some research machines such as the Cedar multiprocessor <ref> [EPY89] </ref>, but not on commercial multiprocessors. In real programs, we found anti dependences that do not require explicit synchronization because they can be covered by flow or output dependences. We hope that our further experiments can establish this as a common case.
Reference: [For88] <author> The Parallel Computing Forum. </author> <title> PCF Fortran language definition, </title> <address> 1 edition, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Many of those mechanisms are either equivalent or close to the event variable synchronization described in the widely circulated PCF docu fl This work is supported by the Department of Energy Grant DOE DE-FG02-85ER25001. ment <ref> [For88] </ref>. Implementation of the event variable synchronization is straightforward on virtually any shared memory multiprocessor. Manually inserted synchronization code is error-prone. Recently, Callahan, Kennedy, and Subhlok proposed a static analysis to detect errors in user-inserted synchronization code [CKS90].
Reference: [FOW87] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: ELSE POST (EV1 (I1,I2)) ... ELSE ... ... ENDIF POST (EV1 (I1,I2)) . = A (I1-1,I2-1) ENDIF ENDDO IF ((I1.GE.2).AND.(I2.GE.2)) ENDDO WAIT (EV1 (I1-1,I2-1)) ENDDO ENDDO (a) (b) use the term "post-dominates" in the same sense as in <ref> [FOW87] </ref>: A node in a flow graph G post-dominates another if the former dominates the latter in the reverse graph of G. Suppose a dependence source R is nested in several loops and some of the loops are on one of the branches of an IF statement.
Reference: [Kuc78] <author> D.J. Kuck. </author> <title> The Structure of Computers and Computations, volume 1. </title> <publisher> John Wiley & Sons, </publisher> <year> 1978. </year>
Reference-contexts: A data dependence may be a flow dependence, which requires a read reference to follow a write reference; an anti dependence, which requires a write reference to follow a read reference; or an output dependence, which requires a write reference to follow another write reference <ref> [Kuc78] </ref>. If one reference depends on another, we call the former dependence sink and the latter the dependence source.
Reference: [Kum88] <author> M. Kumar. </author> <title> Measuring parallelism in computation-intensive scientific/engineering applications. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(9), </volume> <month> September </month> <year> 1988. </year>
Reference-contexts: Some recent simulation studies report tremendous potential parallelism in many application programs. However, the studies also report frequent data dependences across execution threads which must be preserved by means of synchronization <ref> [Kum88, CSY90] </ref>. Most commercial shared memory multiprocessors provide synchronization mechanisms to preserve data dependences between parallel loop iterations.
Reference: [LAS87] <author> Z. Li and W. Abu-Sufah. </author> <title> On reducing data synchronization in multiprocessed loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(1):105-109, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: Using this fact, we can avoid inserting complementary loops. A complete discussion is beyond the scope of this paper. Dependence coverage If one dependence covers others, then only one pair of post and wait is needed to preserve all those dependences. Some previous works <ref> [LAS87, MP87, CKS90] </ref> discussed dependence coverage in simple cases such as constant dependence distances. Using mask predicates, more complicated cases can be handled. A necessary condition for one dependence to cover another is that the mask predicate of the latter should imply that of the former.
Reference: [MP87] <author> S. P. Midkiff and D. A. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1485-1495, </volume> <month> De-cember </month> <year> 1987. </year>
Reference-contexts: In this paper, we describe a method that generates event variable synchronization automatically. Midkiff and Padua presented compiler algorithms to generate event posts and waits in single parallel loops, assuming that all data dependences have known constant distances <ref> [MP87] </ref>. However, more powerful algorithms are needed because loops in real application programs are often arbitrarily nested and compilers are often unable to determine dependence distances [SLY90]. <p> each dependence source R and its event array ev, call Insert a Post (ev, R, T op) in Figure 6 to insert branch-induced posts, where T op is the top node in SSG. 2 In Step 4 of the algorithm, we traverse SSG in a similar way to that in <ref> [MP87] </ref>. <p> Using this fact, we can avoid inserting complementary loops. A complete discussion is beyond the scope of this paper. Dependence coverage If one dependence covers others, then only one pair of post and wait is needed to preserve all those dependences. Some previous works <ref> [LAS87, MP87, CKS90] </ref> discussed dependence coverage in simple cases such as constant dependence distances. Using mask predicates, more complicated cases can be handled. A necessary condition for one dependence to cover another is that the mask predicate of the latter should imply that of the former. <p> We hope that our further experiments can establish this as a common case. Related work Previous works on data synchronization concentrate mainly on devising synchronization mechanisms [Smi85, Sar88, ZY87, SY89]. Only a few works discuss compiler algorithms. The work in <ref> [MP87] </ref> deals mainly with single parallel loops. Recently, Tang, Yew and Zhu present an algorithm based on special counters [TYZ90]. Their paper considers a subset of our loop bounds and subscripts, but does not address if statements.
Reference: [Sar88] <author> V. Sarkar. </author> <title> Synchronization using counting semaphores. </title> <booktitle> In Proceedings of the 1988 ACM International Conference on Supercomputing, </booktitle> <pages> pages 627-637, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: In real programs, we found anti dependences that do not require explicit synchronization because they can be covered by flow or output dependences. We hope that our further experiments can establish this as a common case. Related work Previous works on data synchronization concentrate mainly on devising synchronization mechanisms <ref> [Smi85, Sar88, ZY87, SY89] </ref>. Only a few works discuss compiler algorithms. The work in [MP87] deals mainly with single parallel loops. Recently, Tang, Yew and Zhu present an algorithm based on special counters [TYZ90].
Reference: [SLY90] <author> Z. Shen, Z. Li, and P.-C. Yew. </author> <title> An empirical study of Fortran programs for paralleliz-ing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: However, more powerful algorithms are needed because loops in real application programs are often arbitrarily nested and compilers are often unable to determine dependence distances <ref> [SLY90] </ref>. We make two contributions toward meeting this need: First, we present algorithms to deal with arbitrarily nested loops, including the triangular or trapezoidal loops, in which the lower and upper bounds of inner loops may vary with the index value of the outer loops.
Reference: [Smi85] <author> B. Smith. </author> <title> The architecture of HEP. In Parallel MIMD Computation: HEP Supercomputer and Its Applications, </title> <editor> J. S. Kowalik, </editor> <publisher> ed., </publisher> <pages> pages 41-45. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1985. </year>
Reference-contexts: In real programs, we found anti dependences that do not require explicit synchronization because they can be covered by flow or output dependences. We hope that our further experiments can establish this as a common case. Related work Previous works on data synchronization concentrate mainly on devising synchronization mechanisms <ref> [Smi85, Sar88, ZY87, SY89] </ref>. Only a few works discuss compiler algorithms. The work in [MP87] deals mainly with single parallel loops. Recently, Tang, Yew and Zhu present an algorithm based on special counters [TYZ90].
Reference: [SY89] <author> H.-M. Su and P.-C. Yew. </author> <title> On data synchronization for multiprocessors. </title> <booktitle> In Proceedings of 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 416-423, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In real programs, we found anti dependences that do not require explicit synchronization because they can be covered by flow or output dependences. We hope that our further experiments can establish this as a common case. Related work Previous works on data synchronization concentrate mainly on devising synchronization mechanisms <ref> [Smi85, Sar88, ZY87, SY89] </ref>. Only a few works discuss compiler algorithms. The work in [MP87] deals mainly with single parallel loops. Recently, Tang, Yew and Zhu present an algorithm based on special counters [TYZ90].
Reference: [TYZ90] <author> P. Tang, P.-C. Yew, and C.-Q. Zhu. </author> <title> Compiler techniques for data synchronization in nested parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: Related work Previous works on data synchronization concentrate mainly on devising synchronization mechanisms [Smi85, Sar88, ZY87, SY89]. Only a few works discuss compiler algorithms. The work in [MP87] deals mainly with single parallel loops. Recently, Tang, Yew and Zhu present an algorithm based on special counters <ref> [TYZ90] </ref>. Their paper considers a subset of our loop bounds and subscripts, but does not address if statements. They assume that each data word is associated with a counter, both of which may be accessed atomically. This may double the memory requirement of a program.
Reference: [WB87] <author> M.J. Wolfe and U. Banerjee. </author> <title> Data dependence and its application to parallel processing. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(2), </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Second, our algorithms do not require constant dependence distances. Instead, we generate synchronization code based directly on array subscripts and loop bounds. Of course, we only generate synchronization for a dependence that is proved or assumed to exist between parallel loop iterations by data dependence analysis <ref> [WB87, AK87] </ref>. 2 Background and Assumptions Program constructs Given an arbitrary nest of DO loops, a compiler may decide to parallelize all loops or serialize some of the loops. Our task is to generate the necessary synchronization code to support the intended parallelism and to guarantee the correct computation results.
Reference: [ZY87] <author> C.-Q. Zhu and P.-C. Yew. </author> <title> A scheme to enforce data dependences on large multiprocessor systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(6):726-739, </volume> <month> June </month> <year> 1987. </year> <month> 11 </month>
Reference-contexts: In real programs, we found anti dependences that do not require explicit synchronization because they can be covered by flow or output dependences. We hope that our further experiments can establish this as a common case. Related work Previous works on data synchronization concentrate mainly on devising synchronization mechanisms <ref> [Smi85, Sar88, ZY87, SY89] </ref>. Only a few works discuss compiler algorithms. The work in [MP87] deals mainly with single parallel loops. Recently, Tang, Yew and Zhu present an algorithm based on special counters [TYZ90].
References-found: 18

