URL: http://www.research.att.com/~lewis/papers/lewis94e.ps
Refering-URL: http://www.research.att.com/~lewis/chronobib.html
Root-URL: 
Email: lewis@research.att.com, catlett@research.att.com  
Title: Heterogeneous Uncertainty Sampling for Supervised Learning  
Author: David D. Lewis and Jason Catlett William W. Cohen and Haym Hirsh, eds., 
Date: 148-156.  
Note: Appeared (with same pagination) in  Machine Learning: Proceedings of the Eleventh International Conference, Morgan Kaufmann Publishers, San Francisco, CA, pp.  
Address: Murray Hill, NJ 07974  
Affiliation: AT&T Bell Laboratories  
Abstract: Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: Section 9 lists several opportunites for future work. 2 Background Theoretical analysis and practical experience have shown that a classifier can often be built from fewer instances if the learning algorithm is allowed to create artificial instances or membership queries that are given to an expert to label <ref> [1, 25] </ref>. Unfortunately such queries may create nonsensical examples: is a pregnant non-smoking male at high risk for heart disease? In applications where instances are images or natural language texts, arbitrary membership queries are also implausible.
Reference: [2] <author> I. Bratko, I. Mozetic, and N. Lavrac. KARDIO: </author> <title> a study in deep and qualitative knowledge for expert systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: In some applications unlabeled training instances are abundant but the cost of labeling an instance with its class is high. In the information retrieval application described here the class labels are assigned by a human, but they could also be assigned by a computer simulation <ref> [2] </ref> or a combination of both [30]. The terms oracle and teacher have been used for the source of labels; we will usually call it the expert.
Reference: [3] <author> Leo Breiman, Jerome H. Friedman, Richard A. Ol-shen, and Charles J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: A new classifier trained on the uncertainty sample will then be unduly biased toward predicting low frequency classes. Some mechanism to counterbalance this effect is needed. A feature of the CART <ref> [3] </ref> software for decision trees for specifying priors on classes could be used, but our application required decision rules. We used a version of C4.5 modified by Catlett to accept a parameter specifying the relative cost of two types of error: false positives and false negatives [9, Chapter 1].
Reference: [4] <author> J. Catlett. </author> <title> Megainduction: a test flight. </title> <booktitle> In Machine Learning: Proceedings of the Eigth International Workshop, </booktitle> <pages> pages 596-599, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction Machine learning algorithms have been used to build classification rules from data sets consisting of hundreds of thousands of instances <ref> [4] </ref>. In some applications unlabeled training instances are abundant but the cost of labeling an instance with its class is high. <p> Because decision rules [27, 34] can be converted into this form (unlike probabilistic models requiring arithmetic), they make a good choice for the final classifier. Another important advantage is that they can more comprehensible to humans than decision trees <ref> [4] </ref>. Our databases contain hundreds of thousands of unlabeled instances, so uncertainty sampling is a natural approach. However, as discussed in Section 5, our current decision rule induction software cannot practicably be used for uncertainty sampling from large text databases.
Reference: [5] <author> William G. Cochran. </author> <title> Sampling Techniques. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <note> 3rd edition, </note> <year> 1977. </year>
Reference-contexts: The terms oracle and teacher have been used for the source of labels; we will usually call it the expert. Where one of the constraints on the induction process is a limit on the number of instances presented to the oracle, the choice of instances becomes important. Random sampling <ref> [5] </ref> may be ineffective if one class is very rare: all of the training instances presented may have the majority class. <p> These approaches can be viewed as a combination of stratified and sequential approaches to sampling <ref> [5, 32] </ref>, so we refer to them as uncertainty sampling. A simple form of uncertainty sampling is possible for classifiers that operate by testing a numeric score against a threshold.
Reference: [6] <author> David Cohn, Les Atlas, and Richard Ladner. </author> <title> Improving generalization with self-directed learning, </title> <note> 1992. To appear in Machine Learning. </note>
Reference-contexts: Several algorithms have been proposed that base querying on filtering a stream of unlabeled instances rather than on creating artificial instances <ref> [6, 10, 20, 31] </ref>. The expert is asked to label only those instances whose class membership is sufficiently uncertain. Several definitions of uncertainty and sufficiency have been used, but all are based on esti 1. Obtain an initial classifier 2. <p> The cycle is described in Figure 1 for the finite case. Single classifier approaches to uncertainty sampling have been criticized <ref> [6, 20] </ref> on the grounds that one classifier is not representative of the set of all classifiers consistent with the labeled data: the version space [24]. The degree to which this is a problem in practice has not been established.
Reference: [7] <author> Stuart L. Crawford, Robert M. Fung, Lee A. Appel-baum, and Richard M. Tong. </author> <title> Classification trees for information retrieval. </title> <booktitle> In Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 245-249, </pages> <year> 1991. </year>
Reference-contexts: Although this step lacks a theoretical justification, performance appeared satisfactory. 4 Task and Data Set The applications motivating this research fall under the heading of text categorization: the classification of instances composed partly or fully of natural language text into pre-specified categories <ref> [7, 19] </ref>. We have found several business applications where categorizing text would aid its use, routing, or analysis. These texts often reside in large databases supporting boolean queries [29, pages 231-236], a restricted version of propositional logic.
Reference: [8] <author> Daniel T. Davis and Jenq-Neng Hwang. </author> <title> Attentional focus training by boundary region data selection. </title> <booktitle> In International Joint Conference on Neural Networks, pages I-676 to I-681, </booktitle> <address> Baltimore, MD, </address> <month> June 7-11 </month> <year> 1992. </year>
Reference-contexts: The degree to which this is a problem in practice has not been established. Single classifier approaches have successfully been used in generating arbitrary queries [16] and in sampling from labeled data <ref> [8, 25] </ref>. Uncertainty sampling with a single classifier can also be viewed as a variation on the heuristic of training on misclassified instances [15, 33, 35].
Reference: [9] <author> James P. Egan. </author> <title> Signal Detection Theory and ROC Analysis. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: We used a version of C4.5 modified by Catlett to accept a parameter specifying the relative cost of two types of error: false positives and false negatives <ref> [9, Chapter 1] </ref>. We call this number the loss ratio (LR). A loss ratio of 1 indicates that the two errors have equal costs (the original assumption of C4.5).
Reference: [10] <author> Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. </author> <title> Information, prediction, and query by committee. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Several algorithms have been proposed that base querying on filtering a stream of unlabeled instances rather than on creating artificial instances <ref> [6, 10, 20, 31] </ref>. The expert is asked to label only those instances whose class membership is sufficiently uncertain. Several definitions of uncertainty and sufficiency have been used, but all are based on esti 1. Obtain an initial classifier 2.
Reference: [11] <author> William A. Gale, Kenneth W. Church, and David Yarowsky. </author> <title> A method for disambiguating word senses in a large corpus. </title> <journal> Computers and the Humanities, </journal> <volume> 26 </volume> <pages> 415-439, </pages> <year> 1993. </year>
Reference-contexts: Those w i 's with large values of P (w i ) fi log P (w i jC)=P (w i j C) were selected as features, a strategy found useful in other text classification problems <ref> [11] </ref>.
Reference: [12] <author> B. K. Ghosh. </author> <title> A brief history of sequential analysis. </title> <editor> In B. K. Ghosh and P. K. Sen, editors, </editor> <title> Handbook of Sequential Analysis, </title> <booktitle> chapter 1, </booktitle> <pages> pages 1-19. </pages> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The null hypothesis was that differences in average error rate across the 10 runs for each category were normally distributed with mean zero and a category-specific variance. We believe uncertainty sampling and other sequential, active, or exploratory approaches to learning <ref> [12, 25] </ref> enable both learning research and learning applications on large, complex, real-world data sets where fixed training sets are impracticable. Natural language processing, where there is great interest in inducing knowledge to support tagging, parsing, semantic interpretation, and other forms of analysis, is a particularly fruitful application area.
Reference: [13] <author> Norm Goldstein, </author> <title> editor. The Associated Press Stylebook and Libel Manual. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year>
Reference-contexts: The data matrix was therefore extremely sparse, with each instance having an average of 8.9 nonzero attribute values. The AP data is labeled with several types of subject categories. We defined ten binary categories of AP titles based on the keyword slug line from the article <ref> [13, page 317] </ref>. Frequency information on these categories is given in Table 1.
Reference: [14] <author> Donna Harman. </author> <title> Ranking algorithms. </title> <editor> In William B. Frakes and Ricardo Baeza-Yates, editors, </editor> <booktitle> Information Retrieval: Data Structures and Algorithms, </booktitle> <pages> pages 363-392. </pages> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: Feature selection methods requiring class labels are not a solution because most labels are unknown. 5.1 Uncertainty Sampling with a Probabilistic Classifier Methods for efficient training of probabilistic classifiers from large, sparse data sets are widely used in information retrieval <ref> [14] </ref>. We used this type of classifier to select instances in uncertainty sampling.
Reference: [15] <author> Peter E. Hart. </author> <title> The condensed nearest neighbor rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:515-516, </volume> <month> May </month> <year> 1968. </year> <title> Reprinted in Agrawala, Machine Recognition of Patterns, </title> <publisher> IEEE Press, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: Single classifier approaches have successfully been used in generating arbitrary queries [16] and in sampling from labeled data [8, 25]. Uncertainty sampling with a single classifier can also be viewed as a variation on the heuristic of training on misclassified instances <ref> [15, 33, 35] </ref>. A familiar example of this is windowing, which appeared in Quinlan's first paper on ID3 [26], was questioned in [36] and re-examined in Chapter 6 of the C4.5 book [27].
Reference: [16] <author> Jenq-Neng Hwang, Jai J. Choi, Seho Oh, and Robert J. Marks II. </author> <title> Query-based learning applied to partially trained multilayer perceptrons. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1) </volume> <pages> 131-136, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The degree to which this is a problem in practice has not been established. Single classifier approaches have successfully been used in generating arbitrary queries <ref> [16] </ref> and in sampling from labeled data [8, 25]. Uncertainty sampling with a single classifier can also be viewed as a variation on the heuristic of training on misclassified instances [15, 33, 35]. <p> Using a subsample size of four rather than one was a compromise for efficiency. Selecting examples both above and below 0.5 was a simple way to halve the potential number of duplicate examples, and may also have benefits for training <ref> [16] </ref>. 5.2 Initial Classifier Without an initial classifier our sampling algorithm would commence with a long period of nearly random sampling before finding any examples of a low frequency class.
Reference: [17] <author> Igor Kononerko, Ivan Bratko, and Esidija Roskar. </author> <title> Experiments in automatic learning of medical diagnostic rules. </title> <type> Technical report, </type> <institution> Jozef Stefan Institute, Ljubl-jana, Slovenia, </institution> <year> 1984. </year>
Reference-contexts: Indeed, we have seen some evidence of such instances being selected in the later iterations of an uncertainty sampling run. These murky instances are not the best ones for training <ref> [17, 20] </ref>. This suggests a goal of producing a classifier that estimates P (Cjw) accurately rather than simply classifying accurately.
Reference: [18] <author> David D. Lewis and William A. Gale. </author> <title> Training text classifiers by uncertainty sampling. </title> <booktitle> In Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: The key difference is its assumption that the class labels of all training instances are known: it examines them in order to choose misclassified examples to add. A large scale test of uncertainty sampling with a single classifier approach <ref> [18] </ref> showed that uncertainty sampling could reduce by a factor of up to 500 the amount of data that had to be labeled to achieve a given level of accuracy. 3 Heterogeneous Uncertainty Sampling Uncertainty sampling requires the construction of large numbers (perhaps thousands) of classifiers which are applied to very <p> We used this type of classifier to select instances in uncertainty sampling. The model is described in detail elsewhere <ref> [18] </ref>, but in brief it gives the following estimate for the probability that an instance belongs to class C: exp (a + b i=1 log P (w i j C) ) P d P (w i jC) : (1) C indicates class membership, and w i is the ith of d
Reference: [19] <author> David D. Lewis and Philip J. Hayes. </author> <title> Editorial. </title> <journal> ACM Transactions on Information Systems. </journal> <note> Special Issue on Text Categorization, 1994. To appear. </note>
Reference-contexts: Although this step lacks a theoretical justification, performance appeared satisfactory. 4 Task and Data Set The applications motivating this research fall under the heading of text categorization: the classification of instances composed partly or fully of natural language text into pre-specified categories <ref> [7, 19] </ref>. We have found several business applications where categorizing text would aid its use, routing, or analysis. These texts often reside in large databases supporting boolean queries [29, pages 231-236], a restricted version of propositional logic.
Reference: [20] <author> David J. C. MacKay. </author> <title> The evidence framework applied to classification networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 720-736, </pages> <year> 1992. </year>
Reference-contexts: Several algorithms have been proposed that base querying on filtering a stream of unlabeled instances rather than on creating artificial instances <ref> [6, 10, 20, 31] </ref>. The expert is asked to label only those instances whose class membership is sufficiently uncertain. Several definitions of uncertainty and sufficiency have been used, but all are based on esti 1. Obtain an initial classifier 2. <p> The cycle is described in Figure 1 for the finite case. Single classifier approaches to uncertainty sampling have been criticized <ref> [6, 20] </ref> on the grounds that one classifier is not representative of the set of all classifiers consistent with the labeled data: the version space [24]. The degree to which this is a problem in practice has not been established. <p> Indeed, we have seen some evidence of such instances being selected in the later iterations of an uncertainty sampling run. These murky instances are not the best ones for training <ref> [17, 20] </ref>. This suggests a goal of producing a classifier that estimates P (Cjw) accurately rather than simply classifying accurately.
Reference: [21] <author> David J. C. MacKay. </author> <title> Information-based objective functions for active data selection. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 589-603, </pages> <year> 1992. </year>
Reference-contexts: This suggests a goal of producing a classifier that estimates P (Cjw) accurately rather than simply classifying accurately. The variance of this estimate becomes important, and it may be more appropriate to treat the problem as one of regression or interpolation <ref> [21, 25] </ref> rather than classification. 10 Summary Using partially formed classifiers to select training data incrementally can reduce the number of instances the expert must label to achieve a given error rate.
Reference: [22] <author> Michel Manago. </author> <title> Knowledge intensive induction. </title> <booktitle> In Machine Learning: Proceedings of the Sixth International Workshop, </booktitle> <pages> pages 151-155, </pages> <year> 1989. </year>
Reference-contexts: With 319,463 training instances and 67,331 attributes this would have required over 40 gigabytes. The extravagance of expanding such spase data was stressed in <ref> [22] </ref>. The C4.5 algorithm could be implemented in a manner suited to sparse data, but almost no machine learning software has this feature.
Reference: [23] <author> P. McCullagh and J. A. Nelder. </author> <title> Generalized Linear Models. </title> <publisher> Chapman & Hall, </publisher> <address> London, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: However, it must be scaled to provide an explicit estimate of P (Cjw). One approach to this scaling is logistic regression <ref> [23] </ref>. Training proceeds as follows. The values P (w i jC) and P (w i j C), as well as P (w i ) are estimated for every word w i .
Reference: [24] <author> Tom M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: The cycle is described in Figure 1 for the finite case. Single classifier approaches to uncertainty sampling have been criticized [6, 20] on the grounds that one classifier is not representative of the set of all classifiers consistent with the labeled data: the version space <ref> [24] </ref>. The degree to which this is a problem in practice has not been established. Single classifier approaches have successfully been used in generating arbitrary queries [16] and in sampling from labeled data [8, 25].
Reference: [25] <author> Mark Plutowski and Halbert White. </author> <title> Selecting concise training sets from clean data. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(2) </volume> <pages> 305-318, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Section 9 lists several opportunites for future work. 2 Background Theoretical analysis and practical experience have shown that a classifier can often be built from fewer instances if the learning algorithm is allowed to create artificial instances or membership queries that are given to an expert to label <ref> [1, 25] </ref>. Unfortunately such queries may create nonsensical examples: is a pregnant non-smoking male at high risk for heart disease? In applications where instances are images or natural language texts, arbitrary membership queries are also implausible. <p> The degree to which this is a problem in practice has not been established. Single classifier approaches have successfully been used in generating arbitrary queries [16] and in sampling from labeled data <ref> [8, 25] </ref>. Uncertainty sampling with a single classifier can also be viewed as a variation on the heuristic of training on misclassified instances [15, 33, 35]. <p> The null hypothesis was that differences in average error rate across the 10 runs for each category were normally distributed with mean zero and a category-specific variance. We believe uncertainty sampling and other sequential, active, or exploratory approaches to learning <ref> [12, 25] </ref> enable both learning research and learning applications on large, complex, real-world data sets where fixed training sets are impracticable. Natural language processing, where there is great interest in inducing knowledge to support tagging, parsing, semantic interpretation, and other forms of analysis, is a particularly fruitful application area. <p> This suggests a goal of producing a classifier that estimates P (Cjw) accurately rather than simply classifying accurately. The variance of this estimate becomes important, and it may be more appropriate to treat the problem as one of regression or interpolation <ref> [21, 25] </ref> rather than classification. 10 Summary Using partially formed classifiers to select training data incrementally can reduce the number of instances the expert must label to achieve a given error rate.
Reference: [26] <author> J. R. Quinlan. </author> <title> Discovering rules by induction from large collections of examples. </title> <booktitle> In Expert systems in the micro-electronic age, </booktitle> <address> Edinburgh, UK, 1979. </address> <publisher> Ed-inburgh University Press. </publisher>
Reference-contexts: Uncertainty sampling with a single classifier can also be viewed as a variation on the heuristic of training on misclassified instances [15, 33, 35]. A familiar example of this is windowing, which appeared in Quinlan's first paper on ID3 <ref> [26] </ref>, was questioned in [36] and re-examined in Chapter 6 of the C4.5 book [27]. As with uncertainty sampling, windowing builds a sequence of classifiers, selecting instances to add to the training set at each iteration.
Reference: [27] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: A familiar example of this is windowing, which appeared in Quinlan's first paper on ID3 [26], was questioned in [36] and re-examined in Chapter 6 of the C4.5 book <ref> [27] </ref>. As with uncertainty sampling, windowing builds a sequence of classifiers, selecting instances to add to the training set at each iteration. The key difference is its assumption that the class labels of all training instances are known: it examines them in order to choose misclassified examples to add. <p> We have found several business applications where categorizing text would aid its use, routing, or analysis. These texts often reside in large databases supporting boolean queries [29, pages 231-236], a restricted version of propositional logic. Because decision rules <ref> [27, 34] </ref> can be converted into this form (unlike probabilistic models requiring arithmetic), they make a good choice for the final classifier. Another important advantage is that they can more comprehensible to humans than decision trees [4]. <p> 0.25 133 0.26 budget 1176 0.37 197 0.38 hostages 1560 0.49 228 0.44 Table 1: The ten categories used in our experiments, with the number and percentage of positive occurrences on training and test sets. 5 Training C4.5 with Text Data Although we used a modification of Quinlan's C4.5 software <ref> [27] </ref> to produce decision rules from the training data, using it to select examples is impractical for large text databases because it requires that training and test instances be presented as tuples specifying the values of all attributes.
Reference: [28] <author> J.R. Quinlan. </author> <title> Decision trees as probabilistic classifiers. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pages 31-37, </pages> <address> Irvine, California, </address> <year> 1987. </year>
Reference-contexts: At each iteration a new classifier is built (fortunately from a small sample) and then applied (unfortunately to a large sample). Our uncertainty sampling method also requires an estimate of the certainty of classifications (a class-probability value) <ref> [28] </ref>; not all induction systems provide this. This paper examines a heterogeneous approach in which a classifier of one type selects instances for training a classifier of another type. It is motivated by applications requiring a type of classifier that would be too computationally expensive to use to select instances.
Reference: [29] <author> Gerard Salton. </author> <title> Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: We have found several business applications where categorizing text would aid its use, routing, or analysis. These texts often reside in large databases supporting boolean queries <ref> [29, pages 231-236] </ref>, a restricted version of propositional logic. Because decision rules [27, 34] can be converted into this form (unlike probabilistic models requiring arithmetic), they make a good choice for the final classifier. Another important advantage is that they can more comprehensible to humans than decision trees [4].
Reference: [30] <author> Claude Sammut, Scott Hurst, Dana Kedzier, and Donald Michie. </author> <title> Learning to fly. </title> <booktitle> In Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 385-393, </pages> <year> 1992. </year>
Reference-contexts: In the information retrieval application described here the class labels are assigned by a human, but they could also be assigned by a computer simulation [2] or a combination of both <ref> [30] </ref>. The terms oracle and teacher have been used for the source of labels; we will usually call it the expert. Where one of the constraints on the induction process is a limit on the number of instances presented to the oracle, the choice of instances becomes important.
Reference: [31] <author> H. S. Seung, M. Opper, and H. Sompolinsky. </author> <title> Query by committee. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 287-294, </pages> <year> 1992. </year>
Reference-contexts: Several algorithms have been proposed that base querying on filtering a stream of unlabeled instances rather than on creating artificial instances <ref> [6, 10, 20, 31] </ref>. The expert is asked to label only those instances whose class membership is sufficiently uncertain. Several definitions of uncertainty and sufficiency have been used, but all are based on esti 1. Obtain an initial classifier 2.
Reference: [32] <author> Bikas Kumar Sinha. </author> <title> Sequential methods for finite populations. </title> <editor> In B. K. Ghosh and P. K. Sen, editors, </editor> <title> Handbook of Sequential Analysis, </title> <booktitle> chapter 1, </booktitle> <pages> pages 1-19. </pages> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: These approaches can be viewed as a combination of stratified and sequential approaches to sampling <ref> [5, 32] </ref>, so we refer to them as uncertainty sampling. A simple form of uncertainty sampling is possible for classifiers that operate by testing a numeric score against a threshold. <p> A better understanding of how to minimize the problems caused by a heterogeneous approach would be desirable. Note that we treated our large but finite set of instances as if it were infinite. By adapting results from sequential sampling <ref> [32] </ref> it may be possible both to improve uncertainty sampling and to tell when additional iterations are no longer providing any benefitwhen all the juice has been squeezed out of a data set.
Reference: [33] <author> Paul E. Utgoff. </author> <title> Improved training via incremental learning. </title> <booktitle> In Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 362-365, </pages> <year> 1989. </year>
Reference-contexts: Single classifier approaches have successfully been used in generating arbitrary queries [16] and in sampling from labeled data [8, 25]. Uncertainty sampling with a single classifier can also be viewed as a variation on the heuristic of training on misclassified instances <ref> [15, 33, 35] </ref>. A familiar example of this is windowing, which appeared in Quinlan's first paper on ID3 [26], was questioned in [36] and re-examined in Chapter 6 of the C4.5 book [27].
Reference: [34] <author> Sholom M. Weiss, Robert S. Galen, and Prasad V. Tadepalli. </author> <title> Maximizing the predictive value of production rules. </title> <journal> Artificial Intelligence, </journal> <volume> 45(1-2):47-71, </volume> <month> September </month> <year> 1990. </year>
Reference-contexts: We have found several business applications where categorizing text would aid its use, routing, or analysis. These texts often reside in large databases supporting boolean queries [29, pages 231-236], a restricted version of propositional logic. Because decision rules <ref> [27, 34] </ref> can be converted into this form (unlike probabilistic models requiring arithmetic), they make a good choice for the final classifier. Another important advantage is that they can more comprehensible to humans than decision trees [4].
Reference: [35] <author> P. H. Winston. </author> <title> Learning structural descriptions from examples. </title> <editor> In P. H. Winston, editor, </editor> <booktitle> The Psychology of Computer Vision, </booktitle> <pages> pages 157-209. </pages> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: Single classifier approaches have successfully been used in generating arbitrary queries [16] and in sampling from labeled data [8, 25]. Uncertainty sampling with a single classifier can also be viewed as a variation on the heuristic of training on misclassified instances <ref> [15, 33, 35] </ref>. A familiar example of this is windowing, which appeared in Quinlan's first paper on ID3 [26], was questioned in [36] and re-examined in Chapter 6 of the C4.5 book [27].
Reference: [36] <author> J. Wirth and J. Catlett. </author> <title> Costs and benefits of window-ing in ID3. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> Ann Arbor, Michi-gan, 1988. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 156 </pages>
Reference-contexts: Uncertainty sampling with a single classifier can also be viewed as a variation on the heuristic of training on misclassified instances [15, 33, 35]. A familiar example of this is windowing, which appeared in Quinlan's first paper on ID3 [26], was questioned in <ref> [36] </ref> and re-examined in Chapter 6 of the C4.5 book [27]. As with uncertainty sampling, windowing builds a sequence of classifiers, selecting instances to add to the training set at each iteration.
References-found: 36

