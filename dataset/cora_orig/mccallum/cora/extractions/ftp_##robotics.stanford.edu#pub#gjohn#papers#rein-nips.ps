URL: ftp://robotics.stanford.edu/pub/gjohn/papers/rein-nips.ps
Refering-URL: http://www.robotics.stanford.edu/~gjohn/pubs.html
Root-URL: http://www.robotics.stanford.edu
Email: gjohn@cs.stanford.edu  
Title: When the Best Move Isn't Optimal: Q-learning with Exploration  
Author: George H. John 
Web: http://robotics.stanford.edu/~gjohn  
Address: Stanford, CA 94305-4110  
Affiliation: Computer Science Department Stanford University  
Abstract: In delayed reinforcement learning, an agent is concerned with the problem of discovering an optimal policy, a function mapping states to actions. The most popular delayed reinforcement learning technique, Q-learning, has been proven to produce an optimal policy under certain conditions. However, often the agent does not follow the optimal policy faithfully|the agent must also explore the world. The optimal policy produced by Q-learning is no longer optimal if its prescriptions are only followed occasionally. In many situations (e.g., dynamic environments), the agent never stops exploring. In such domains Q-learning converges to policies that are suboptimal when combined with the exploration policy, and we give several examples of this problem. We present Q-learning, a slight modification of Q-learning that provides a policy resulting in higher reward when combined with a particular exploration strategy.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bellman, R. E. </author> <year> (1962), </year> <title> Dynamic Programming, </title> <publisher> Princeton University Press. </publisher>
Reference: <author> Chrisman, L. </author> <year> (1993), </year> <title> Notational conventions for the reinforcement learning workshop at ML93, </title> <type> Personal Communication. </type>
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1992), </year> <title> "Automatic programming of behavior-based robots using reinforcement learning", </title> <booktitle> Artificial Intelligence 55(2-3), </booktitle> <pages> 311-365. </pages>
Reference-contexts: Q-learning is a very powerful algorithm, because theorems regarding its convergence to the optimal policy have set it on sound theoretical ground (Watkins & Dayan 1992) and it also seems to work well in practice <ref> (Mahadevan & Connell 1992) </ref>. 1.1 When is Q-learning the Right Thing to Do? The theoretical objective of the Q-learning algorithm seems to be often misunderstood, and we wish to address this concern here in detail.
Reference: <author> Schwartz, A. </author> <year> (1993), </year> <title> A reinforcement learning method for maximizing undiscounted rewards, </title> <editor> in P. Utgoff, ed., </editor> <booktitle> "Proceedings of the Tenth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1990), </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming, </title> <editor> in B. Porter & R. Mooney, eds, </editor> <booktitle> "Proceedings of the Seventh International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Thrun, S. </author> <year> (1992), </year> <title> Efficient exploration in reinforcement learning, </title> <type> Technical Report CMU-CS-92-102, </type> <institution> CMU School of Computer Science. </institution>
Reference: <author> Watkins, C. </author> <year> (1989), </year> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> Cambridge University. Psychology Department. </institution>
Reference-contexts: Fortunately, Chrisman (1993) has done the reinforcement community the favor of collecting various suggestions and coming up with standard notation for Q-learning, a slightly modified subset of which is described in Table 1. The most popular reinforcement technique, Q-learning <ref> (Watkins 1989) </ref>, approxi mates V (s) stochastically without requiring a model using these update rules: b Q (x; a) fi (r + fl b V (y)) + (1 fi) b Q (x; a) (1) Equation 2 tells us how to update b V (s) after updating b Q (x; a): we
Reference: <author> Watkins, C. & Dayan, P. </author> <year> (1992), </year> <title> "Q-learning", </title> <booktitle> Machine Learning 8(3/4), </booktitle> <pages> 279-292. </pages>
Reference-contexts: Thus, once Q-values have converged an agent may perform optimally by greedily choosing from among the available actions the one with highest Q-value. Q-learning is a very powerful algorithm, because theorems regarding its convergence to the optimal policy have set it on sound theoretical ground <ref> (Watkins & Dayan 1992) </ref> and it also seems to work well in practice (Mahadevan & Connell 1992). 1.1 When is Q-learning the Right Thing to Do? The theoretical objective of the Q-learning algorithm seems to be often misunderstood, and we wish to address this concern here in detail.
Reference: <author> Whitehead, S. </author> <year> (1991), </year> <title> A complexity analysis of cooperative mechanisms in reinforcement learning, </title> <booktitle> in "AAAI-91: Proceedings of the Ninth National Conference on Artificial Intelligence", </booktitle> <publisher> AAAI Press/MIT Press. </publisher>
References-found: 9

