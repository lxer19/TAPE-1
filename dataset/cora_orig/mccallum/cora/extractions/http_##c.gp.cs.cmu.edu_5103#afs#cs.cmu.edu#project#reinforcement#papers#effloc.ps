URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/reinforcement/papers/effloc.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/Web/People/awm/papers.html
Root-URL: http://www.cs.cmu.edu
Email: awm@cs.cmu.edu  schneide@cs.cmu.edu  kdeng@cs.cmu.edu  
Title: Efficient Locally Weighted Polynomial Regression Predictions  
Author: Andrew W. Moore Jeff Schneider Kan Deng 
Address: Pittsburgh, PA 15213  Pittsburgh, PA 15213  Pittsburgh, PA 15213  
Affiliation: Robotics Institute and School of Computer Science Carnegie Mellon University  Robotics Institute Carnegie Mellon University  Robotics Institute Carnegie Mellon University  
Abstract: Locally weighted polynomial regression (LWPR) is a popular instance-based algorithm for learning continuous non-linear mappings. For more than two or three inputs and for more than a few thousand dat-apoints the computational expense of predictions is daunting. We discuss drawbacks with previous approaches to dealing with this problem, and present a new algorithm based on a multiresolution search of a quickly-constructible augmented kd-tree. Without needing to rebuild the tree, we can make fast predictions with arbitrary local weighting functions, arbitrary kernel widths and arbitrary queries. The paper begins with a new, faster, algorithm for exact LWPR predictions. Next we introduce an approximation that achieves up to a two-orders-of-magnitude speedup with negligible accuracy losses. Increasing a certain approximation parameter achieves greater speedups still, but with a correspondingly larger accuracy degradation. This is nevertheless useful during operations such as the early stages of model selection and locating optima of a fitted surface. We also show how the approximations can permit real-time query-specific optimization of the kernel width. We conclude with a brief discussion of potential extensions for tractable instance-based learning on datasets that are too large to fit in a com puter's main memory. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkeson, C. G., Moore, A. W., and Schaal, S. A. </author> <year> (1997a). </year> <title> Locally Weighted Learning. </title> <journal> AI Review, </journal> <volume> 11 </volume> <pages> 11-73. </pages>
Reference: <author> Atkeson, C. G., Moore, A. W., and Schaal, S. A. </author> <year> (1997b). </year> <title> Locally Weighted Learning for Control. </title> <journal> Accepted for publication in AI Review, </journal> <volume> 11 </volume> <pages> 75-113. </pages>
Reference-contexts: The bottom row shows that the error of Approx and Fast relative to the Regular algorithm is confidently estimated as being small. Let us examine the algorithms applied to a collection of five UCI-repository datasets and one robot dataset (described in <ref> (Atkeson et al., 1997b) </ref>). Figure 8 shows results in which all datasets had the same local model: locally weighted linear regression with a kernel width of 0.03 on the unit-scaled input attributes. Figure 9 shows the results on a variety of different local polynomial models described in the caption.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth. </publisher>
Reference: <author> Cleveland, W. S. and Delvin, S. J. </author> <year> (1988). </year> <title> Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 83(403) </volume> <pages> 596-610. </pages>
Reference: <author> Deng, K. and Moore, A. W. </author> <year> (1995). </year> <title> Multiresolution Instance-based Learning. </title> <booktitle> In To appear in proced-dings of IJCAI-95. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Much worse, for many datasets, the best kernel width is very broad, meaning that a significant fraction of the data (sometimes all the data) has non-zero weight. In that case, avoiding the zero-weight datapoints is not much help. In this paper we use the main idea from <ref> (Deng and Moore, 1995) </ref> in which a multiresolution data structure increased the speed of kernel regression (also known as Locally Weighted Averaging). Here, we extend that method to arbitrary locally weighted polynomials, and give a number of empirical evaluations of the resulting algorithms. <p> We do not believe that the choice between the numerous kd-tree splitting criteria is critical for this purpose, and so we choose the same "split in the middle of the dimension with the widest spread" method as <ref> (Deng and Moore, 1995) </ref>. Let ND be a node in the kd-tree.
Reference: <author> Friedman, J. H., Bentley, J. L., and Finkel, R. A. </author> <year> (1977). </year> <title> An Algorithm for Finding Best Matches in Logarithmic Expected Time. </title> <journal> ACM Trans. on Mathematical Software, </journal> <volume> 3(3) </volume> <pages> 209-226. </pages>
Reference: <author> Grosse, E. </author> <year> (1989). </year> <title> LOESS: Multivariate Smoothing by Moving Least Squares. </title> <editor> In C. K. Chul, L. L. S. and Ward, J. D., editors, </editor> <title> Approximation Theory VI. </title> <publisher> Academic Press. </publisher>
Reference-contexts: This results in an algorithm with rather different properties, in which some information about fine detail is necessarily lost. Another solution is caching, in which a multivariate spline model is built when the data is first loaded. <ref> (Grosse, 1989) </ref> do this with a variable resolution kd-tree structure and multilinear interpolation within tree leaves. Unfortunately, continuous interpolation above two dimensions is very expensive in computation and memory. (Quinlan, 1993) also uses a caching method but ignores continuity by storing separate discontinuous linear maps in the leaves.
Reference: <author> Hastie, T. J. and Tibshirani, R. J. </author> <year> (1990). </year> <title> Generalized additive models. </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: Both classical and Bayesian linear regression analysis tools can be extended to work in the locally weighted framework <ref> (Hastie and Tibshirani, 1990) </ref>, providing confidence intervals on predictions, on gradient estimates and on noise estimates|all important when a learned mapping is to be used by a controller (Atkeson et al., 1997b; Schneider, 1997). Let us review LWPR. We begin with linear regression on one input and one output.
Reference: <author> Moore, A. W. </author> <year> (1992). </year> <title> Fast, Robust Adaptive Control by Learning only Forward Models. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Omohundro, S. M. </author> <year> (1991). </year> <title> Bumptrees for Efficient Function, Constraint, and Classification Learning. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: This strengthens the W SoFar bound. * Ball trees <ref> (Omohundro, 1991) </ref> play a similar role to a kd-tree used for range searching, but it is possible that a hierarchy of balls, each containing the sufficient statistics of datapoints they contain, could be used beneficially in place of the bounding boxes we used. * The algorithms have been modified to permit
Reference: <author> Preparata, F. P. and Shamos, M. </author> <year> (1985). </year> <title> Computational Geometry. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning Efficient Classification Procedures and their Application to Chess End Games. </title> <editor> In Michalski, R. S., Carbonell, J. G., and Mitchell, T. M., editors, </editor> <booktitle> Machine Learning|An Artificial Intelligence Approach (I). </booktitle> <publisher> Tioga Publishing Company, </publisher> <address> Palo Alto. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> Combining Instance-Based and Model-Based Learning. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference. </booktitle>
Reference-contexts: Another solution is caching, in which a multivariate spline model is built when the data is first loaded. (Grosse, 1989) do this with a variable resolution kd-tree structure and multilinear interpolation within tree leaves. Unfortunately, continuous interpolation above two dimensions is very expensive in computation and memory. <ref> (Quinlan, 1993) </ref> also uses a caching method but ignores continuity by storing separate discontinuous linear maps in the leaves. Another downside to caching solutions is that they only record the fitted surface.
Reference: <author> Schaal, S. and Atkeson, C. </author> <year> (1994). </year> <title> Robot Juggling: An Implementation of Memory-based Learning. </title> <journal> Control Systems Magazine, </journal> <volume> 14. </volume>
Reference-contexts: With N datapoints and M terms, this means O (N M 2 ) multiplications to build the matrices and then another O (M 3 ) operations to compute the fi vector. Sometimes the computational expense isn't an inconvenience. The devil stick robot of <ref> (Schaal and Atkeson, 1994) </ref> was able to make 5 predictions a second with 10 inputs, 5 outputs and a thousand 1 X itself is an N -row, M-column matrix whose kth row is w k fi (t 1 (x k ) : : : t M (x k )).
Reference: <author> Schneider, J. G. </author> <year> (1997). </year> <title> Exploiting Model Uncertainty Estimates for Safe Dynamic Control Learning. </title> <booktitle> In Neural Information Processing Systems 9, 1996. </booktitle> <publisher> Morgan Kaufmann. </publisher>
References-found: 15

