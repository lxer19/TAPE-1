URL: http://www.cs.mu.oz.au/tr_db/mu_95_22.ps.gz
Refering-URL: http://www.cs.mu.oz.au/tr_db/TR.html
Root-URL: 
Title: Feature Replenishment for Long-Term Visual Motion Tracking  
Author: Tak Keung CHENG Leslie KITCHEN Zhi-Qiang LIU 
Pubnum: Technical Report 95/22  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J.(Y.) Aloimonos, I. Weiss, and A. Bandyopadhyay. </author> <title> Active vision. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 333-356, </pages> <year> 1988. </year>
Reference-contexts: Instead, it should maintain a relevant model of its environment, and extract from its environment only that information necessary for performing its current tasks. Our approach has similarities with the "active vision" paradigm (see, for example, <ref> [1, 2, 3, 11] </ref>), but is much wider in its scope. 3 Processing Strategies Under our approach, we have adopted a number of processing strategies [9], namely: model-based prediction/verification, foveation (focus of attention), multi-resolution processing (spatial and temporal), and speculative computing.
Reference: [2] <author> R. </author> <title> Bajcsy. Active perception. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 76 </volume> <pages> 996-1005, </pages> <year> 1988. </year>
Reference-contexts: Instead, it should maintain a relevant model of its environment, and extract from its environment only that information necessary for performing its current tasks. Our approach has similarities with the "active vision" paradigm (see, for example, <ref> [1, 2, 3, 11] </ref>), but is much wider in its scope. 3 Processing Strategies Under our approach, we have adopted a number of processing strategies [9], namely: model-based prediction/verification, foveation (focus of attention), multi-resolution processing (spatial and temporal), and speculative computing.
Reference: [3] <author> D.H. Ballard. </author> <title> Animate vision. </title> <journal> Artificial Intelligence, </journal> <volume> 48 </volume> <pages> 57-86, </pages> <year> 1991. </year>
Reference-contexts: Instead, it should maintain a relevant model of its environment, and extract from its environment only that information necessary for performing its current tasks. Our approach has similarities with the "active vision" paradigm (see, for example, <ref> [1, 2, 3, 11] </ref>), but is much wider in its scope. 3 Processing Strategies Under our approach, we have adopted a number of processing strategies [9], namely: model-based prediction/verification, foveation (focus of attention), multi-resolution processing (spatial and temporal), and speculative computing.
Reference: [4] <author> Tak-Keung Cheng and Leslie Kitchen. </author> <title> Preliminary results on real-time 3D feature-based tracker. In Digital Image Computing: Techniques and Applications, </title> <address> DICTA-93, Sydney, Australia, </address> <month> December </month> <year> 1993. </year> <title> Australian 14 30, </title> <journal> 50 Pattern Recognition Society. </journal> <note> Longer version available as TR 93/30, </note> <institution> Com--puter Science Department, University of Melbourne. </institution>
Reference-contexts: An earlier version of it was described in <ref> [4] </ref>. It was sufficient to demonstrate the soundness of our approach, but suffered from two major defects: 2 1. It needed human assistance in setting up the initial stereo correspondence of image features. <p> It will not be described further. Except for the Replenishment Agent and the Stereo Feature Agent, which are the subjects of Section 6, the functions of the other agents will be briefly described below. More details on these can be found in <ref> [4] </ref>. Prior to the work described here, the function of the Stereo Feature Agent was slightly different, and there was no Replenishment Agent. <p> Note, this shows only feature-level tracking, to show off the replenishment and correspondence processing. Results of multi-object segmentation, real-time tracking, and feature recovery are reported elsewhere <ref> [4, 12] </ref>, but only for shorter sequences (up to 10 frames). At the time of writing, we have not yet run the complete system on long sequences. The images used are 256 by 256 pixels in size, with 256 grey levels.
Reference: [5] <author> James Cooper. </author> <title> Real-Time Task-Directed Robot Vision. </title> <type> Ph.D. thesis, </type> <institution> The University of Western Australia, </institution> <year> 1992. </year>
Reference-contexts: This means that visual processing has to be done in real time, and must be directed towards accomplishing the robot's tasks. This has led us, over the past few years, to develop an approach which we call "real-time task-directed robot vision" <ref> [6, 9, 5] </ref>, in reaction to the traditional "data-processing" approach. At this point, some remarks are worth making: Real-time vision is not just static vision done very fast. It has a quite different nature, requiring (and exploiting) an interaction with the environment. "Task-directed" does not mean "task-specific". <p> Their relationship is very close, so we discuss them together. 6.2 Stereo Feature Agent The overall design structure for the Stereo Feature Agent is similar to the Feature agent in <ref> [7, 5] </ref>. The same feature matching technique (the Early Jump-Out Method) [10] is used for matching the features between two frame times. It takes the input from two calibrated cameras and sends the 3D feature points' location and velocity information to the 3D Motion Cluster Agent for further processing.
Reference: [6] <author> James Cooper and Leslie Kitchen. </author> <title> A proposal for real-time, task-directed robot vision. </title> <type> Technical Report 89/10, </type> <institution> Computer Science Department, The University of Western Australia, Nedlands, </institution> <address> W.A. 6009, Australia, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: This means that visual processing has to be done in real time, and must be directed towards accomplishing the robot's tasks. This has led us, over the past few years, to develop an approach which we call "real-time task-directed robot vision" <ref> [6, 9, 5] </ref>, in reaction to the traditional "data-processing" approach. At this point, some remarks are worth making: Real-time vision is not just static vision done very fast. It has a quite different nature, requiring (and exploiting) an interaction with the environment. "Task-directed" does not mean "task-specific".
Reference: [7] <author> James Cooper and Leslie Kitchen. </author> <title> Multi-agent segmentation for real-time task-directed vision. </title> <booktitle> In Fourth Australian Joint Artificial Intelligence Conference, </booktitle> <pages> pages 729-739, </pages> <address> Perth, Western Australia, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: is not a very interesting issue from the point of view of our research, except that our architecture is consciously designed to permit such fast implementations. 5 3D Feature-Based Tracker (3D-FBT) 5.1 Overview The 3D-FBT builds on previous work in real-time edge detection, region tracking [8], and 2D feature tracking <ref> [7] </ref>. It is the largest agent-based system we have built to date. Its goal is to track and segment moving 3D objects in real time, using input from a stereo pair of video cameras. <p> Their relationship is very close, so we discuss them together. 6.2 Stereo Feature Agent The overall design structure for the Stereo Feature Agent is similar to the Feature agent in <ref> [7, 5] </ref>. The same feature matching technique (the Early Jump-Out Method) [10] is used for matching the features between two frame times. It takes the input from two calibrated cameras and sends the 3D feature points' location and velocity information to the 3D Motion Cluster Agent for further processing.
Reference: [8] <author> James Cooper and Leslie Kitchen. </author> <title> A region-based object tracker. </title> <booktitle> In Aus-tralian Robot Association Third National Conference on Robotics, </booktitle> <pages> pages 154-164, </pages> <address> Melbourne, Australia, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: But this is not a very interesting issue from the point of view of our research, except that our architecture is consciously designed to permit such fast implementations. 5 3D Feature-Based Tracker (3D-FBT) 5.1 Overview The 3D-FBT builds on previous work in real-time edge detection, region tracking <ref> [8] </ref>, and 2D feature tracking [7]. It is the largest agent-based system we have built to date. Its goal is to track and segment moving 3D objects in real time, using input from a stereo pair of video cameras.
Reference: [9] <author> James Cooper and Leslie Kitchen. </author> <title> Issues, architectures and techniques in real-time vision. </title> <booktitle> In Proc. Conf. on AI, Simulation and Planning in High Autonomy Systems, </booktitle> <pages> pages 218-224, </pages> <address> Perth, Western Australia, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: This means that visual processing has to be done in real time, and must be directed towards accomplishing the robot's tasks. This has led us, over the past few years, to develop an approach which we call "real-time task-directed robot vision" <ref> [6, 9, 5] </ref>, in reaction to the traditional "data-processing" approach. At this point, some remarks are worth making: Real-time vision is not just static vision done very fast. It has a quite different nature, requiring (and exploiting) an interaction with the environment. "Task-directed" does not mean "task-specific". <p> Our approach has similarities with the "active vision" paradigm (see, for example, [1, 2, 3, 11]), but is much wider in its scope. 3 Processing Strategies Under our approach, we have adopted a number of processing strategies <ref> [9] </ref>, namely: model-based prediction/verification, foveation (focus of attention), multi-resolution processing (spatial and temporal), and speculative computing. This last one will be discussed in Section 4; the others here.
Reference: [10] <author> James Cooper, Svetha Venkatesh, and Leslie Kitchen. </author> <title> Early jump-out corner detectors. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-15(8):823-828, </volume> <month> August </month> <year> 1993. </year>
Reference-contexts: Their relationship is very close, so we discuss them together. 6.2 Stereo Feature Agent The overall design structure for the Stereo Feature Agent is similar to the Feature agent in [7, 5]. The same feature matching technique (the Early Jump-Out Method) <ref> [10] </ref> is used for matching the features between two frame times. It takes the input from two calibrated cameras and sends the 3D feature points' location and velocity information to the 3D Motion Cluster Agent for further processing.
Reference: [11] <author> E.D. Dickmanns and V. Graefe. </author> <title> Dynamic monocular machine vision. </title> <journal> Machine Vision Applications, </journal> <volume> 1 </volume> <pages> 223-240, </pages> <year> 1988. </year>
Reference-contexts: Instead, it should maintain a relevant model of its environment, and extract from its environment only that information necessary for performing its current tasks. Our approach has similarities with the "active vision" paradigm (see, for example, <ref> [1, 2, 3, 11] </ref>), but is much wider in its scope. 3 Processing Strategies Under our approach, we have adopted a number of processing strategies [9], namely: model-based prediction/verification, foveation (focus of attention), multi-resolution processing (spatial and temporal), and speculative computing.
Reference: [12] <author> L. Kitchen, T.-K. Cheng, and J. Cooper. </author> <title> A distributed systems architecture for real-time computer vision. </title> <booktitle> In Parallel and Real-Time Systems Workshop (PART'94), </booktitle> <address> Melbourne, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Note, this shows only feature-level tracking, to show off the replenishment and correspondence processing. Results of multi-object segmentation, real-time tracking, and feature recovery are reported elsewhere <ref> [4, 12] </ref>, but only for shorter sequences (up to 10 frames). At the time of writing, we have not yet run the complete system on long sequences. The images used are 256 by 256 pixels in size, with 256 grey levels.
Reference: [13] <author> C.-Y. Tang, L. Kitchen, Y.-P. Hung, and Z. Chen. </author> <title> Visual tracking of 3D motion using stereo. </title> <booktitle> In Computer Vision, Graphics and Image Processing Workshop, </booktitle> <address> Taiwan, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: This correspondence matching method has similarities with that in <ref> [13] </ref>. In the start-up state, after the Camera Calibration agent finishes its processing, it sends the camera parameters to the Converting Agent. As a side 10 effect, Stereo Feature has enough information to work on the matching problem. Figure 5 shows the idea of the epipolar and depth constraints.
Reference: [14] <author> Roger Y. Tsai. </author> <title> A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lens. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-3(4):323-344, </volume> <month> August </month> <year> 1987. </year> <month> 16 </month>
Reference-contexts: Our model does not take lens distortion into account, but this is negligible for our purposes. From the known 3D positions of control points on a special calibration object, and their measured 2D projections, this agent uses either Tsai's method <ref> [14] </ref> or non-linear optimization to search for a set of camera parameters that gives minimum sum of squared errors in the 2D image projections. Identification of control-point projections in an image is currently human-assisted.
References-found: 14

