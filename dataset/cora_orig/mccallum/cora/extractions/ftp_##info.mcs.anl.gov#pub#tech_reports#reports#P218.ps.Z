URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P218.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/preprints.htm
Root-URL: http://www.mcs.anl.gov
Title: A Comparative Study of Automatic Vectorizing Compilers  
Author: David Levine David Callahan Jack Dongarra 
Abstract: We compare the capabilities of several commercially available, vectorizing Fortran compilers using a test suite of Fortran loops. We present the results of compiling and executing these loops on a variety of supercomputers, mini-supercomputers, and mainframes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah and A. Malony. </author> <title> Vector processing on the Alliant FX/8 multiprocessor. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 559-566, </pages> <year> 1986. </year>
Reference-contexts: Also, (1) and (2) may not reflect the behavior of cache 14 Table 11: Basic Operation Classes Class Operation 0 Load 0 Gather (Load indirect) 1 Store 1 Scatter (Store indirect) 2 Arithmetic (Add, Multiply) 2 Reductions based machines under increasing vector lengths (see, for example <ref> [1] </ref>). Nevertheless, for the purposes of our model we believe (1) and (2) to be sufficient. <p> The second issue concerns the data set size relative to the cache size. A small data set will always fit in the cache. A large data set may not fit in the cache and will cause many performance-degrading cache misses to occur. The paper by Abu-Sufah and Maloney <ref> [1] </ref> contains a discussion of this issue and its impact on performance. Their uniprocessor performance results on an Alliant FX/8 show that there is only a narrow range of vector lengths for which optimal performance was achieved.
Reference: [2] <author> J. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> TOPLAS, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <year> 1987. </year>
Reference-contexts: All of the loops in this suite are classified into one of these categories. We define all terms and transformation names but discuss dependence analysis and program transformation only briefly. Recent discussions of these topics can be found in Allen and Kennedy <ref> [2] </ref>, Padua and Wolfe [11], and Wolfe [14]. For a practical exposition of the application of these techniques, see Levesque and Williamson [6]. 2.1 Dependence Analysis Dependence analysis comprises two areas: global data-flow analysis and dependence testing. Global data-flow analysis refers to the process of collecting information about array subscripts.
Reference: [3] <author> D. Callahan, J. Dongarra, and D. Levine. </author> <title> Vectorizing compilers: A test suite and results. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <pages> pages 98-105, </pages> <year> 1988. </year>
Reference-contexts: These loops reflect constructs whose vectorization ranges from easy to challenging to extremely difficult. We have collected the results from compiling and executing these loops using commercially available, vectorizing Fortran compilers. The results reported here expand on our earlier work <ref> [3] </ref>. In that paper, we focused principally on analyzing each compiler's output listing. For the present study, we ran the loops in both scalar and vector modes. In addition, the set of loops has been expanded. The remainder of this paper is organized into eight sections. <p> This weighting was an artifact of the selected categories and was reflected in the original collection of samples. We felt that this weighting was reasonable and made no attempt to adjust it. 19 Table 13: Loops Sorted by Difficulty Table 14: Loops Sorted by Difficulty, from <ref> [3] </ref> 8.1.2 Stress By "stress" we refer to how effectively the test suite tests the limits of the compilers. We wish the test to be difficult but not impossible. <p> Viewed from a historical perspective, the test appears less stressful now than it did originally. We can see this qualitatively from Table 14, which is reprinted from <ref> [3] </ref>. Here there seems to be 20 a more balanced distribution of tests between "easy" and "difficult" when compared to Table 13. Statistics also support this view. In [3] the average number of loops vectorized was 55%, and vectorized or partially vectorized was 61%. <p> We can see this qualitatively from Table 14, which is reprinted from <ref> [3] </ref>. Here there seems to be 20 a more balanced distribution of tests between "easy" and "difficult" when compared to Table 13. Statistics also support this view. In [3] the average number of loops vectorized was 55%, and vectorized or partially vectorized was 61%. Even if we restrict ourselves to just the eight vendors also participating in this test, the previous results are still only 59% and 64%, respectively.
Reference: [4] <author> J. Dongarra and E. Grosse. </author> <title> Distribution of mathematical software via electronic mail. </title> <journal> Communications of the ACM, </journal> <volume> 30(5) </volume> <pages> 403-407, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Real loops present combinations of vectorization problems rather than individual challenges. It will then be interesting to compare results on the "simple" loops with those on the real loops. A copy of the source code used in the test is available from the NETLIB electronic mail facility <ref> [4] </ref> at Oak Ridge National Laboratory. To receive a copy of the code, send electronic mail to netlib@ornl.gov. In the mail message, type send vectors from benchmark or send vectord from benchmark to get either the REAL or DOUBLE PRECISION versions, respectively.
Reference: [5] <author> R. Hockney and C. Jesshope. </author> <title> Parallel Computers: Architecture, Programming and Algorithms. Adam Hilger, </title> <publisher> Ltd., </publisher> <address> Bristol, United Kingdom, </address> <year> 1981. </year> <month> 24 </month>
Reference-contexts: Equivalent to (1) is the well-known model of Hockney (see Hockney and Jesshope <ref> [5] </ref>), t = r 1 where r 1 is the asymptotic performance rate and n 1=2 is the vector length necessary to achieve one half the asymptotic rate. <p> Equations (1) and (2) can be shown to be equivalent if we use the definitions r 1 = t 1 e and n 1=2 = t o =t e <ref> [5] </ref>. As Lubeck [7] points out, neither equation models the stripmining process used by compilers on register-to-register vector computers.
Reference: [6] <author> J. Levesque and J. Williamson. </author> <title> A Guidebook to Fortran on Supercomputers. </title> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1988. </year>
Reference-contexts: Recent discussions of these topics can be found in Allen and Kennedy [2], Padua and Wolfe [11], and Wolfe [14]. For a practical exposition of the application of these techniques, see Levesque and Williamson <ref> [6] </ref>. 2.1 Dependence Analysis Dependence analysis comprises two areas: global data-flow analysis and dependence testing. Global data-flow analysis refers to the process of collecting information about array subscripts.
Reference: [7] <author> O. Lubeck. </author> <title> Supercomputer performance: The theory, practice, and results. </title> <type> Technical Report LA-11204-MS, </type> <institution> Los Alamos National Laboratory, </institution> <year> 1988. </year>
Reference-contexts: Equations (1) and (2) can be shown to be equivalent if we use the definitions r 1 = t 1 e and n 1=2 = t o =t e [5]. As Lubeck <ref> [7] </ref> points out, neither equation models the stripmining process used by compilers on register-to-register vector computers.
Reference: [8] <author> O. Lubeck, J. Moore, and R. Mendez. </author> <title> A benchmark comparison of three supercomputers: Fujitsu VP-200, Hitachi S810/20, and Cray X-MP/2. </title> <journal> IEEE Computer, </journal> <volume> 18(12) </volume> <pages> 10-23, </pages> <year> 1985. </year>
Reference-contexts: We then compare the optimal performance predicted by this model with the actual vector execution results to determine the percent of the optimal vector performance actually achieved. 7.1 A Model of Compiler Performance A simple model of vector performance as a function of vector length is given by the formula <ref> [8] </ref> t = t o + nt e ; (1) where t is the time to execute a vector loop of length n, t o is the vector startup time, and t e is the time to execute a particular vector element.
Reference: [9] <author> W. Mangione-Smith, S. Abraham, and E. Davidson. </author> <title> A performance comparison of the IBM RS/6000 and the Astronautics ZS-1. </title> <journal> IEEE Computer, </journal> <volume> 24(1) </volume> <pages> 39-46, </pages> <year> 1991. </year>
Reference-contexts: The algorithm assumes that operations in different classes execute concurrently while operations in the same class execute sequentially. This model is based on the notion of a resource limit, similar to the model used to calculate performance bounds in <ref> [9, 13] </ref>. We assume that for each loop there exists a particular class of operations that use the same function unit and that the time to execute these operations provides a lower bound on the time to execute the loop.
Reference: [10] <author> F. McMahon. </author> <title> The Livermore Fortran kernels: A computer test of the numercial range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1986. </year>
Reference-contexts: The choice of mean clearly affects the results. In the Vectorization section, the arithmetic mean at vector length 1000 is 21.32, while the harmonic mean is only 1.91. These results show that a relatively small number of large speedups can greatly affect the arithmetic mean. 13 McMahon <ref> [10] </ref> and Smith [12] discuss the different means. If we compare the last row of Table 9 with the first two rows in Table 10, we see better speedups at all vector lengths when we consider only the loops fully or partially vectorized.
Reference: [11] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <year> 1986. </year>
Reference-contexts: All of the loops in this suite are classified into one of these categories. We define all terms and transformation names but discuss dependence analysis and program transformation only briefly. Recent discussions of these topics can be found in Allen and Kennedy [2], Padua and Wolfe <ref> [11] </ref>, and Wolfe [14]. For a practical exposition of the application of these techniques, see Levesque and Williamson [6]. 2.1 Dependence Analysis Dependence analysis comprises two areas: global data-flow analysis and dependence testing. Global data-flow analysis refers to the process of collecting information about array subscripts.
Reference: [12] <author> J. Smith. </author> <title> Characterizing computer performance with a single number. </title> <journal> Communications of the ACM, </journal> <volume> 31(10) </volume> <pages> 1202-1206, </pages> <year> 1988. </year>
Reference-contexts: In the Vectorization section, the arithmetic mean at vector length 1000 is 21.32, while the harmonic mean is only 1.91. These results show that a relatively small number of large speedups can greatly affect the arithmetic mean. 13 McMahon [10] and Smith <ref> [12] </ref> discuss the different means. If we compare the last row of Table 9 with the first two rows in Table 10, we see better speedups at all vector lengths when we consider only the loops fully or partially vectorized.
Reference: [13] <author> J. Tang and E. Davidson. </author> <title> An evaluation of Cray-1 and Cray X-MP performance on vector-izable Livermore Fortran kernels. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 510-518, </pages> <address> St. Malo, France, </address> <year> 1988. </year>
Reference-contexts: The algorithm assumes that operations in different classes execute concurrently while operations in the same class execute sequentially. This model is based on the notion of a resource limit, similar to the model used to calculate performance bounds in <ref> [9, 13] </ref>. We assume that for each loop there exists a particular class of operations that use the same function unit and that the time to execute these operations provides a lower bound on the time to execute the loop.
Reference: [14] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year> <month> 25 </month>
Reference-contexts: All of the loops in this suite are classified into one of these categories. We define all terms and transformation names but discuss dependence analysis and program transformation only briefly. Recent discussions of these topics can be found in Allen and Kennedy [2], Padua and Wolfe [11], and Wolfe <ref> [14] </ref>. For a practical exposition of the application of these techniques, see Levesque and Williamson [6]. 2.1 Dependence Analysis Dependence analysis comprises two areas: global data-flow analysis and dependence testing. Global data-flow analysis refers to the process of collecting information about array subscripts.
References-found: 14

