URL: http://www.cs.princeton.edu/~skumar/papers/isca98.ps
Refering-URL: http://www.cs.princeton.edu/~skumar/
Root-URL: http://www.cs.princeton.edu
Email: skumar@cs.princeton.edu  cbwilker@ichips.intel.com  
Title: Exploiting Spatial Locality in Data Caches using Spatial Footprints  
Author: Sanjeev Kumar Christopher Wilkerson 
Address: Princeton University  
Affiliation: Department of Computer Science  Microcomputer Research Labs Intel, Oregon  
Note: Appears in the Proceedings of 25th Annual ACM/IEEE International Symposium on Computer Architecture (ISCA'98) 1  
Abstract: Modern cache designs exploit spatial locality by fetching large blocks of data called cache lines on a cache miss. Subsequent references to words within the same cache line result in cache hits. Although this approach benefits from spatial locality, less than half of the data brought into the cache gets used before eviction. The unused portion of the cache line negatively impacts performance by wasting bandwidth and polluting the cache by replacing potentially useful data that would otherwise remain in the cache. This paper describes an alternative approach to exploit spatial locality available in data caches. On a cache miss, our mechanism, called Spatial Footprint Predictor (SFP), predicts which portions of a cache block will get used before getting evicted. The high accuracy of the predictor allows us to exploit spatial locality exhibited in larger blocks of data yielding better miss ratios without significantly impacting the memory access latencies. Our evaluation of this mechanism shows that the miss rate of the cache is improved, on average, by 18% in addition to a significant reduction in the bandwidth requirement. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. G. Abraham, R. A. Sugumar, D. Windheiser, B. R. Rau, and R. Gupta. </author> <title> Predictability of load/store instruction latencies. </title> <booktitle> In Proceedings of the 26th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 139152, </pages> <address> Austin, Texas, </address> <month> December 13, </month> <year> 1993. </year>
Reference-contexts: However, it does it at a much coarser granularity than our mechanism. They also do not address the problem of increased L1 cache access latency associated with small cache lines. Abraham et al. <ref> [1] </ref> show that a large percentage of misses are caused by a small percentage of instructions. They use this information to prefetch data referenced by a few instructions identified through profiling. Tyson et al. [18] use this information to dynamically make cache bypassing decisions.
Reference: [2] <author> J.-L. Baer and T.-F. Chen. </author> <title> An effective on-chip preloading chip scheme to reduce data access penalty. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: The predictors minimize the impact on the access latency by keeping the bulk of their operations off the critical path. Several hardware and software based prefetching mechanisms that improve the effectiveness of the caches have been studied. However, most of the work <ref> [16, 9, 2, 5, 13] </ref> has focussed on numeric applications with well-structured loops and regular access patterns. Other efforts [10, 12] have focussed on using the compiler to prefetch pointer targets on integer applications.
Reference: [3] <author> K. Bolland and A. Dollas. </author> <title> Predicting and precluding problems with memory latency. </title> <booktitle> In IEEE Micro, </booktitle> <pages> pages 5966, </pages> <month> August, </month> <year> 1994. </year>
Reference-contexts: 1. Introduction This paper introduces an approach to alleviate the growing memory latency problem <ref> [3] </ref> by better exploiting the spatial locality exhibited by applications. Spatial locality is the tendency of neighboring memory locations to be referenced close together in time.
Reference: [4] <author> D. Burger, J. R. Goodman, and A. Kagi. </author> <title> Memory bandwidth limitations of future microprocessors. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 7889, </pages> <address> Philadelphia, PA, </address> <month> May 2224, </month> <year> 1996. </year>
Reference-contexts: In addition, our mechanism does not affect the miss penalty. So the reduction in the miss rates and bandwidth from the Section 6.4 should translate into better cache performance. Most latency tolerance mechanisms do so at the expense of increased bandwidth requirements <ref> [4] </ref>. However, the SFP mechanism achieves a lower miss rate and reduces the bandwidth requirement at the same time. The L1-L2 bus protocol will need only minor modifications to benefit from the lower bandwidth.
Reference: [5] <author> W. Y. Chen, A. A. Mahlke, P. P. Chang, and W. W. Hwu. </author> <title> Data access microarchitectures for superscalar processors with compiler-assisted data prefetching. </title> <booktitle> In Proceedings of Microcomputing 24, </booktitle> <year> 1991. </year>
Reference-contexts: The predictors minimize the impact on the access latency by keeping the bulk of their operations off the critical path. Several hardware and software based prefetching mechanisms that improve the effectiveness of the caches have been studied. However, most of the work <ref> [16, 9, 2, 5, 13] </ref> has focussed on numeric applications with well-structured loops and regular access patterns. Other efforts [10, 12] have focussed on using the compiler to prefetch pointer targets on integer applications.
Reference: [6] <author> A. Gonzalez, C. Aliagas, and M. Valero. </author> <title> A data cache with multiple caching strategies tuned to different types of locality. </title> <booktitle> In Proceedings of International Conference on Supercomputing, </booktitle> <pages> pages 338347, </pages> <address> Barcelona, Spain, </address> <month> July, </month> <year> 1995. </year>
Reference-contexts: One of the studies [14] also investigated the effect of varying the fetch size independent of the line size and concluded that the optimal statically determined fetch size was generally twice the line size. The dual data cache <ref> [6] </ref> has two independent parts for data that exhibit temporal and spatial locality. However, their locality detection mechanism was geared for numeric codes with constant stride vectors. The Spatial Locality Detection Table [7] attempts to dynamically pick the right fetch size based on the memory reference.
Reference: [7] <author> T. Johnson, M. Merten, and W. mei W. Hwu. </author> <title> Run-time spatial locality detection and optimization. </title> <booktitle> In Proceedings of the 30th Annual International Symposium on Microarchitec-ture, </booktitle> <pages> pages 5764, </pages> <address> Research Triangle Park, NC, </address> <year> 1997. </year>
Reference-contexts: Other efforts [10, 12] have focussed on using the compiler to prefetch pointer targets on integer applications. However, very little work has been done to exploit the spatial locality in integer applications based on the dynamic behavior of the application. The SLDT <ref> [7] </ref> mechanism attempts to detect and exploit the spatial locality at run time. In this paper, we present a new predictor, called the Spatial Footprint Predictor (SFP), which predicts the neighboring words that should be prefetched on a cache miss. <p> The dual data cache [6] has two independent parts for data that exhibit temporal and spatial locality. However, their locality detection mechanism was geared for numeric codes with constant stride vectors. The Spatial Locality Detection Table <ref> [7] </ref> attempts to dynamically pick the right fetch size based on the memory reference. However, it does it at a much coarser granularity than our mechanism. They also do not address the problem of increased L1 cache access latency associated with small cache lines. <p> Tyson et al. [18] use this information to dynamically make cache bypassing decisions. Another study [8] shows that, in some applications, few instructions reference large amounts of data with widely varying data access patterns. So they use the data address to dynamically make cache bypassing and prefetching decisions <ref> [8, 7] </ref>. We study the effectiveness of using both instruction and data addresses to predict spatial locality. 2.2. Approach The goal is to exploit the spatial locality available to long cache lines without excessive pollution and without significantly impacting the cache access latency.
Reference: [8] <author> T. L. Johnson and W. mei W. Hwu. </author> <title> Run-time adaptive cache hierarchy management via reference analysis. </title> <booktitle> In Proceedings of the 24rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 315326, </pages> <address> Denver, Colorado, </address> <year> 1997. </year>
Reference-contexts: Abraham et al. [1] show that a large percentage of misses are caused by a small percentage of instructions. They use this information to prefetch data referenced by a few instructions identified through profiling. Tyson et al. [18] use this information to dynamically make cache bypassing decisions. Another study <ref> [8] </ref> shows that, in some applications, few instructions reference large amounts of data with widely varying data access patterns. So they use the data address to dynamically make cache bypassing and prefetching decisions [8, 7]. <p> Tyson et al. [18] use this information to dynamically make cache bypassing decisions. Another study [8] shows that, in some applications, few instructions reference large amounts of data with widely varying data access patterns. So they use the data address to dynamically make cache bypassing and prefetching decisions <ref> [8, 7] </ref>. We study the effectiveness of using both instruction and data addresses to predict spatial locality. 2.2. Approach The goal is to exploit the spatial locality available to long cache lines without excessive pollution and without significantly impacting the cache access latency.
Reference: [9] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364 373, </pages> <address> Seattle, Washington, </address> <month> May 2831, </month> <year> 1990. </year>
Reference-contexts: The predictors minimize the impact on the access latency by keeping the bulk of their operations off the critical path. Several hardware and software based prefetching mechanisms that improve the effectiveness of the caches have been studied. However, most of the work <ref> [16, 9, 2, 5, 13] </ref> has focussed on numeric applications with well-structured loops and regular access patterns. Other efforts [10, 12] have focussed on using the compiler to prefetch pointer targets on integer applications.
Reference: [10] <author> M. H. Lipasti, W. J. Schmidt, S. R. Kunkel, and R. R. Roediger. SPAID: </author> <title> Software prefetching in pointer- and call-intensive environments. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 231 236, </pages> <address> Ann Arbor, Michigan, </address> <year> 1995. </year>
Reference-contexts: Several hardware and software based prefetching mechanisms that improve the effectiveness of the caches have been studied. However, most of the work [16, 9, 2, 5, 13] has focussed on numeric applications with well-structured loops and regular access patterns. Other efforts <ref> [10, 12] </ref> have focussed on using the compiler to prefetch pointer targets on integer applications. However, very little work has been done to exploit the spatial locality in integer applications based on the dynamic behavior of the application.
Reference: [11] <author> L. Liu. </author> <title> Cache designs with partial address matching. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 128136, </pages> <address> San Jose, California, </address> <month> November 30December 2, </month> <year> 1994. </year>
Reference-contexts: It also maintains the validity information on a per line basis. On a cache reference, the tag verification is slightly more complicated than in traditional caches and, therefore, takes longer. However, since the data array access is not affected, a way-predictor <ref> [11] </ref> can be employed to optimistically use the data without waiting for the tag matching to complete [15]. This technique is already used by some processors to achieve low cache hit time in set-associative caches. So, using a decoupled sectored cache should have little impact on the cache access latency.
Reference: [12] <author> C.-K. Luk and T. C. Mowry. </author> <title> Compiler-based prefetching for recursive data structures. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 222 233, </pages> <address> Cambridge, MA, </address> <month> October 15, </month> <year> 1996. </year>
Reference-contexts: Several hardware and software based prefetching mechanisms that improve the effectiveness of the caches have been studied. However, most of the work [16, 9, 2, 5, 13] has focussed on numeric applications with well-structured loops and regular access patterns. Other efforts <ref> [10, 12] </ref> have focussed on using the compiler to prefetch pointer targets on integer applications. However, very little work has been done to exploit the spatial locality in integer applications based on the dynamic behavior of the application.
Reference: [13] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 6273, </pages> <address> Boston, Massachusetts, </address> <month> October 1215, </month> <year> 1992. </year>
Reference-contexts: The predictors minimize the impact on the access latency by keeping the bulk of their operations off the critical path. Several hardware and software based prefetching mechanisms that improve the effectiveness of the caches have been studied. However, most of the work <ref> [16, 9, 2, 5, 13] </ref> has focussed on numeric applications with well-structured loops and regular access patterns. Other efforts [10, 12] have focussed on using the compiler to prefetch pointer targets on integer applications.
Reference: [14] <author> S. Przybylski. </author> <title> The performance impact of block sizes and fetch strategies. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 160 169, </pages> <address> Seattle, Washington, </address> <month> May 2831, </month> <year> 1990. </year>
Reference-contexts: However, for most integer applications, the spatial locality exhibited is fairly non-uniform. This variation is hard to predict statically and, therefore, exploiting the spatial locality effectively requires run-time prediction mechanisms. 2.1. Related Work Several studies <ref> [14, 17] </ref> have examined the effects of different cache line sizes on cache performance. One of the studies [14] also investigated the effect of varying the fetch size independent of the line size and concluded that the optimal statically determined fetch size was generally twice the line size. <p> This variation is hard to predict statically and, therefore, exploiting the spatial locality effectively requires run-time prediction mechanisms. 2.1. Related Work Several studies [14, 17] have examined the effects of different cache line sizes on cache performance. One of the studies <ref> [14] </ref> also investigated the effect of varying the fetch size independent of the line size and concluded that the optimal statically determined fetch size was generally twice the line size. The dual data cache [6] has two independent parts for data that exhibit temporal and spatial locality.
Reference: [15] <author> A. Seznec. </author> <title> Decoupled sectored caches: Conciliating low tag implementation cost and low miss ratio. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 384393, </pages> <address> Chicago, IL, </address> <month> April, </month> <year> 1994. </year>
Reference-contexts: Although this approach would benefit from reduced bandwidth requirements, the unused cache lines within the sectors would result in poor utilization of the cache, resulting in worse hit ratios and, ultimately, worse cache performance. We use a decoupled sectored cache <ref> [15] </ref> (described in Section 6.2) that allows us to use smaller arrays with little impact on the cache access latency. 3. Experimental Setup We study Spatial Footprint Prediction for L1 data caches. In the base configuration, the L1 data cache used is a 16 K-byte four-way set-associative cache. <p> In a sectored cache, a single address tag is associated with a sector consisting of several cache lines, while validity tags are associated with each of the cache lines. However, this results in bad performance due to poor utilization of the cache resources. Seznec <ref> [15] </ref> proposed decoupled sectored caches as a way to reconcile low tag implementation cost with low miss ratio. In a decoupled sectored cache, instead of the static association between the tag and data, the association is determined dynamically at allocation time. <p> On a cache reference, the tag verification is slightly more complicated than in traditional caches and, therefore, takes longer. However, since the data array access is not affected, a way-predictor [11] can be employed to optimistically use the data without waiting for the tag matching to complete <ref> [15] </ref>. This technique is already used by some processors to achieve low cache hit time in set-associative caches. So, using a decoupled sectored cache should have little impact on the cache access latency. Decoupled sectored caches were originally proposed to reduce the size of tag array for L2 cache controllers.
Reference: [16] <author> A. J. Smith. </author> <title> Cache memories. </title> <journal> In Computing Surveys, </journal> <volume> pages 473530, vol. 14, no. 3, </volume> <year> 1982. </year>
Reference-contexts: The predictors minimize the impact on the access latency by keeping the bulk of their operations off the critical path. Several hardware and software based prefetching mechanisms that improve the effectiveness of the caches have been studied. However, most of the work <ref> [16, 9, 2, 5, 13] </ref> has focussed on numeric applications with well-structured loops and regular access patterns. Other efforts [10, 12] have focussed on using the compiler to prefetch pointer targets on integer applications.
Reference: [17] <author> A. J. Smith. </author> <title> Line (block) size choice for cpu cache memories. </title> <journal> In IEEE Transactions on Computers, </journal> <volume> pages 10631075, vol. C-36, </volume> <year> 1987. </year>
Reference-contexts: However, for most integer applications, the spatial locality exhibited is fairly non-uniform. This variation is hard to predict statically and, therefore, exploiting the spatial locality effectively requires run-time prediction mechanisms. 2.1. Related Work Several studies <ref> [14, 17] </ref> have examined the effects of different cache line sizes on cache performance. One of the studies [14] also investigated the effect of varying the fetch size independent of the line size and concluded that the optimal statically determined fetch size was generally twice the line size.
Reference: [18] <author> G. Tyson, M. Farrens, J. Matthews, and A. R. Pleszkun. </author> <title> A modified approach to data cache management. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Mi-croarchitecture, </booktitle> <pages> pages 93103, </pages> <address> Ann Arbor, Michigan, </address> <year> 1995. </year>
Reference-contexts: Abraham et al. [1] show that a large percentage of misses are caused by a small percentage of instructions. They use this information to prefetch data referenced by a few instructions identified through profiling. Tyson et al. <ref> [18] </ref> use this information to dynamically make cache bypassing decisions. Another study [8] shows that, in some applications, few instructions reference large amounts of data with widely varying data access patterns. So they use the data address to dynamically make cache bypassing and prefetching decisions [8, 7].
References-found: 18

