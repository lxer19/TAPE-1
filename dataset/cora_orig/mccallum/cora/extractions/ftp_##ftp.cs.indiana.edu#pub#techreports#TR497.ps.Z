URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR497.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Title: Multi-Stage Specialization with Relative Binding Times  
Author: Mark Leone Peter Lee 
Date: November 1997  
Affiliation: Indiana University  Carnegie Mellon University  Computer Science Department, Indiana University  
Pubnum: Technical Report #497  
Abstract: Programming systems that generate code at run time offer unique opportunities for specialization. Dynamic specialization can exploit run-time values that are not available at compile time, yielding code that is superior to statically optimal code. Unfortunately, conventional formulations of binding-time analysis prove overly restrictive in such a setting. The values computed by specialized procedures are classified as dynamic, which prevents a useful form of multi-stage specialization. We propose a simple notion of relative binding times that allow multiple stages of specialization to be realized in a two-level lambda calculus. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anders Bondorf and Olivier Danvy. </author> <title> Automatic autoprojection of recursive equations with global variables and abstract data types. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 16(2) </volume> <pages> 151-195, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: By avoiding the construction of intermediate syntax trees, the cost of specialization and code generation is reduced to approximately 6 to 10 cycles per generated instruction. Besides the usual transformations performed by partial evaluators such as Similix <ref> [1] </ref>, optimizations such as instruction selection, simple reduction of strength, and some register optimizations are also performed by the generating extensions.
Reference: [2] <author> Charles Consel and Fran~cois Noel. </author> <title> A general approach for run-time specialization and its application to C. </title> <booktitle> In Conference Record of POPL '96: The 23 rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 145-156, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: 1 Dynamic Specialization and Binding-Time Analysis Most partial evaluators perform specialization at compile time. However, recent research has demonstrated that it is sometimes profitable to perform specialization at later stages, such as run time <ref> [8, 7, 2, 4, 6] </ref>. Some implementations of dynamic specialization rely on a binding-time analysis (BTA) to determine how to specialize a program when some of its variables take on fixed values. BTA requires an initial binding-time division that classifies program inputs as static or dynamic.
Reference: [3] <author> Rowan Davies and Frank Pfenning. </author> <title> A modal analysis of staged computation. </title> <booktitle> In Conference Record of POPL '96: The 23 rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 258-270, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Our benchmarks [9, 7] demonstrate substantial speedups in application domains ranging from symbolic programs (such as regular-expression matching, cellular automata, etc.) to numerical applications (matrix multiply, conjugate gradient) to operating systems (network packet filtering). 4 Related Work Multi-level languages <ref> [5, 3, 11, 10, 12] </ref> allow multi-stage specialization by permitting an arbitrary number of binding times. In one such framework [5], expressions are annotated with numbered binding times.
Reference: [4] <author> Scott Draves. </author> <title> Compiler generation for interactive graphics using intermediate code. </title> <booktitle> In Dagstuhl Workshop on Partial Evaluation (LNCS1110), </booktitle> <year> 1996. </year>
Reference-contexts: 1 Dynamic Specialization and Binding-Time Analysis Most partial evaluators perform specialization at compile time. However, recent research has demonstrated that it is sometimes profitable to perform specialization at later stages, such as run time <ref> [8, 7, 2, 4, 6] </ref>. Some implementations of dynamic specialization rely on a binding-time analysis (BTA) to determine how to specialize a program when some of its variables take on fixed values. BTA requires an initial binding-time division that classifies program inputs as static or dynamic.
Reference: [5] <author> Robert Gluck and Jesper Jtrgensen. </author> <title> Efficient multi-level generating extensions. </title> <booktitle> In Programming Languages, Implementations, Logics and Programs (PLILP'95), volume 1181 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Our benchmarks [9, 7] demonstrate substantial speedups in application domains ranging from symbolic programs (such as regular-expression matching, cellular automata, etc.) to numerical applications (matrix multiply, conjugate gradient) to operating systems (network packet filtering). 4 Related Work Multi-level languages <ref> [5, 3, 11, 10, 12] </ref> allow multi-stage specialization by permitting an arbitrary number of binding times. In one such framework [5], expressions are annotated with numbered binding times. <p> In one such framework <ref> [5] </ref>, expressions are annotated with numbered binding times. <p> of "2", yielding the following multi-level program: big-spender? threshold 0 db 1 key 2 = let 1 big-spenders 1 = query @ 0 `(&gt; spent ,threshold) 0 @ 1 db 1 in member @ 1 big-spenders 1 @ 2 key 2 end Multi-level languages lead naturally to multi-level generating extensions <ref> [5] </ref>. For example, the generating extension for big-spender? specializes the query procedure and then creates a specialized generating extension parameterized by db. However, multi-level generating extensions are impractical when specialization is performed at run time, because the cost of creating specialized generating extensions is unlikely to be repaid.
Reference: [6] <author> Brian Grant, Markus Mock, Matthai Philipose, Craig Chambers, and Susan J. Eggers. </author> <title> Annotation-directed run-time specialization in C. </title> <booktitle> In PEPM 97 Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: 1 Dynamic Specialization and Binding-Time Analysis Most partial evaluators perform specialization at compile time. However, recent research has demonstrated that it is sometimes profitable to perform specialization at later stages, such as run time <ref> [8, 7, 2, 4, 6] </ref>. Some implementations of dynamic specialization rely on a binding-time analysis (BTA) to determine how to specialize a program when some of its variables take on fixed values. BTA requires an initial binding-time division that classifies program inputs as static or dynamic.
Reference: [7] <author> Peter Lee and Mark Leone. </author> <title> Optimizing ML with run-time code generation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 137-148, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: 1 Dynamic Specialization and Binding-Time Analysis Most partial evaluators perform specialization at compile time. However, recent research has demonstrated that it is sometimes profitable to perform specialization at later stages, such as run time <ref> [8, 7, 2, 4, 6] </ref>. Some implementations of dynamic specialization rely on a binding-time analysis (BTA) to determine how to specialize a program when some of its variables take on fixed values. BTA requires an initial binding-time division that classifies program inputs as static or dynamic. <p> opportunities for performing specialization at run time: the query procedure can be specialized to the dynamically constructed search query, eliminating substantial interpretive overhead, and the member predicate can be specialized to the list of big spenders. (The efficacy of specializing a membership predicate at run time has been previously demonstrated <ref> [7] </ref>.) However, a conventional BTA does not allow both of these opportunities to be realized. Specializing the query procedure requires threshold to be static and db to be dynamic; this results in the classification of big-spenders as dynamic, which prevents member from being specialized. <p> We believe that a wide range of real-world programs exhibit similar opportunities for multi-stage specialization, where the "dynamic" results computed by dynamically specialized code can be employed in successive specializations. For example, common library routines like matrix multiplication benefit from run-time specialization <ref> [7] </ref>, and it is easy to imagine that user code could benefit from being specialized to values computed by library code. We propose a simple notion of relative binding times that allows multiple stages of specialization to be realized in a calculus with just two levels. <p> In essence, we have discarded the global consistency requirement of conventional BTA in favor of a much simpler, and also more flexible, local annotation scheme. 4 3 The Fabius System We have implemented our approach in a prototype compiler called Fabius 4 <ref> [8, 9, 7] </ref>. It compiles a pure, first-order subset of ML that includes reals, vectors, and user-defined datatypes into native MIPS code. <p> Besides the usual transformations performed by partial evaluators such as Similix [1], optimizations such as instruction selection, simple reduction of strength, and some register optimizations are also performed by the generating extensions. Our benchmarks <ref> [9, 7] </ref> demonstrate substantial speedups in application domains ranging from symbolic programs (such as regular-expression matching, cellular automata, etc.) to numerical applications (matrix multiply, conjugate gradient) to operating systems (network packet filtering). 4 Related Work Multi-level languages [5, 3, 11, 10, 12] allow multi-stage specialization by permitting an arbitrary number of <p> His primary strategy was to delay confrontation; repeated small attacks eventually led to victory without a single decisive conflict. 5 and shown experimentally that this approach is effective <ref> [7] </ref>. We are currently implementing a successor to Fabius that compiles higher-order procedures into abstract machine code for a novel architecture called the Two-Level Abstract Machine, which simplifies experimentation while realistically modeling relevant characteristics of real architectures.
Reference: [8] <author> Mark Leone and Peter Lee. </author> <title> Lightweight run-time code generation. In PEPM 94 Workshop on Partial Evaluation and Semantics-Based Program Manipulation, </title> <type> pages 97-106. Technical Report 94/9, </type> <institution> Department of Computer Science, University of Melbourne, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: 1 Dynamic Specialization and Binding-Time Analysis Most partial evaluators perform specialization at compile time. However, recent research has demonstrated that it is sometimes profitable to perform specialization at later stages, such as run time <ref> [8, 7, 2, 4, 6] </ref>. Some implementations of dynamic specialization rely on a binding-time analysis (BTA) to determine how to specialize a program when some of its variables take on fixed values. BTA requires an initial binding-time division that classifies program inputs as static or dynamic. <p> In essence, we have discarded the global consistency requirement of conventional BTA in favor of a much simpler, and also more flexible, local annotation scheme. 4 3 The Fabius System We have implemented our approach in a prototype compiler called Fabius 4 <ref> [8, 9, 7] </ref>. It compiles a pure, first-order subset of ML that includes reals, vectors, and user-defined datatypes into native MIPS code.
Reference: [9] <author> Mark Leone and Peter Lee. </author> <title> A declarative approach to run-time code generation. </title> <booktitle> In WCSSS'96 Workshop on Compiler Support for System Software, </booktitle> <pages> pages 8-17, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: In essence, we have discarded the global consistency requirement of conventional BTA in favor of a much simpler, and also more flexible, local annotation scheme. 4 3 The Fabius System We have implemented our approach in a prototype compiler called Fabius 4 <ref> [8, 9, 7] </ref>. It compiles a pure, first-order subset of ML that includes reals, vectors, and user-defined datatypes into native MIPS code. <p> Besides the usual transformations performed by partial evaluators such as Similix [1], optimizations such as instruction selection, simple reduction of strength, and some register optimizations are also performed by the generating extensions. Our benchmarks <ref> [9, 7] </ref> demonstrate substantial speedups in application domains ranging from symbolic programs (such as regular-expression matching, cellular automata, etc.) to numerical applications (matrix multiply, conjugate gradient) to operating systems (network packet filtering). 4 Related Work Multi-level languages [5, 3, 11, 10, 12] allow multi-stage specialization by permitting an arbitrary number of
Reference: [10] <author> Flemming Nielson and Hanne Riis Nielson. </author> <title> Prescriptive frameworks for multi-level lambda-calculi. </title> <booktitle> In PEPM 97 Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Our benchmarks [9, 7] demonstrate substantial speedups in application domains ranging from symbolic programs (such as regular-expression matching, cellular automata, etc.) to numerical applications (matrix multiply, conjugate gradient) to operating systems (network packet filtering). 4 Related Work Multi-level languages <ref> [5, 3, 11, 10, 12] </ref> allow multi-stage specialization by permitting an arbitrary number of binding times. In one such framework [5], expressions are annotated with numbered binding times.
Reference: [11] <author> Walid Taha and Tim Sheard. </author> <title> Multi-stage programming with explicit annotations. </title> <booktitle> In PEPM 97 Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Our benchmarks [9, 7] demonstrate substantial speedups in application domains ranging from symbolic programs (such as regular-expression matching, cellular automata, etc.) to numerical applications (matrix multiply, conjugate gradient) to operating systems (network packet filtering). 4 Related Work Multi-level languages <ref> [5, 3, 11, 10, 12] </ref> allow multi-stage specialization by permitting an arbitrary number of binding times. In one such framework [5], expressions are annotated with numbered binding times.
Reference: [12] <author> Philip Wickline, Peter Lee, Frank Pfenning, and Rowan Davies. </author> <title> Modal types as staging specifications for run-time code generation. </title> <note> Submitted to Computing Surveys Symposium on Partial Evaluation, 1997. 7 </note>
Reference-contexts: Our benchmarks [9, 7] demonstrate substantial speedups in application domains ranging from symbolic programs (such as regular-expression matching, cellular automata, etc.) to numerical applications (matrix multiply, conjugate gradient) to operating systems (network packet filtering). 4 Related Work Multi-level languages <ref> [5, 3, 11, 10, 12] </ref> allow multi-stage specialization by permitting an arbitrary number of binding times. In one such framework [5], expressions are annotated with numbered binding times.
References-found: 12

