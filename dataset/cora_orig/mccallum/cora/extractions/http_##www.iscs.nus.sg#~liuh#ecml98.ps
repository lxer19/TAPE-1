URL: http://www.iscs.nus.sg/~liuh/ecml98.ps
Refering-URL: 
Root-URL: 
Email: fliuh, manoranjg@iscs.nus.edu.sg motoda@sanken.osaka-u.ac.jp  
Phone: 2  3  
Title: A Monotonic Measure for Optimal Feature Selection  
Author: Huan Liu and Hiroshi Motoda and Manoranjan Dash 
Address: Singapore, Singapore 119260.  Ibaraki, Osaka 567, Japan.  Singapore, Singapore 119074.  
Affiliation: 1 Dept of Info Sys Comp Sci, National University of  Division of Intelligent Sys Sci, Osaka University,  BioInformatics Centre, National University of  
Abstract: Feature selection is a problem of choosing a subset of relevant features. In general, only exhaustive search can bring about the optimal subset. With a monotonic measure, exhaustive search can be avoided without sacrificing optimality. Unfortunately, most error- or distance-based measures are not monotonic. A new measure is employed in this work that is monotonic and fast to compute. The search for relevant features according to this measure is guaranteed to be complete but not exhaustive. Experiments are conducted for verification.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> H. Almuallim and T.G. Dietterich. </author> <title> Learning with many irrelevant features. </title> <booktitle> In Proceedings of AAAI, </booktitle> <year> 1991. </year>
Reference-contexts: When N is large, this exhaustive approach is out of the question. Therefore, various feature selection methods have been designed to avoid exhaustive search while still aiming at the optimal subset. Examples are Branch & Bound [7], Focus <ref> [1] </ref>, Relief [4], Wrapper methods [3], and LVF [5]. The feature selection problem can be viewed as a search problem [9]. The search process starts with either an empty set or a full set. <p> The feature selection problem can be viewed as a search problem [9]. The search process starts with either an empty set or a full set. For the former, it expands the search space by adding one feature at a time (Sequential Forward Selection) <ref> [1] </ref>; for the latter, it expands the search space by deleting one feature at a time (Sequential Backward Selection) [7]. As we shall see, a good alternative to exhaustive search is Branch & Bound like algorithms if there exists a monotonic function of evaluating features.
Reference: 2. <author> M. Ben-Bassat. </author> <title> Pattern recognition and reduction of dimensionality. </title> <editor> In P. R. Krishnaiah and L. N. Kanal, editors, </editor> <booktitle> Handbook of statistics-II, </booktitle> <pages> pages 773-791. </pages> <publisher> North Holland, </publisher> <year> 1982. </year>
Reference-contexts: The task of feature selection is to determine which features to select in order to achieve maximum performance with the minimum measurement effort <ref> [2] </ref>. Reducing features directly alleviates the measurement effort. Performance of a classifier can be its predictive accuracy, i.e., 1 error rate. As was mentioned in [2], if the goal is to minimize the error rate, and the measurement cost for all the features is equal, then the most appealing function to <p> The task of feature selection is to determine which features to select in order to achieve maximum performance with the minimum measurement effort <ref> [2] </ref>. Reducing features directly alleviates the measurement effort. Performance of a classifier can be its predictive accuracy, i.e., 1 error rate. As was mentioned in [2], if the goal is to minimize the error rate, and the measurement cost for all the features is equal, then the most appealing function to evaluate the potency of a feature to differentiate between the classes is the Bayes Classifier. <p> Many researchers pointed out that the only remaining alternative is to use the error rate of a classifier as the measure. Among many classifiers, however, only the Bayes Classifier satisfies this monotonicity condition 1 because other classifiers adopt some assumptions and employ certain heuristics <ref> [9, 2, 3] </ref>. Another disadvantage of using the error rate as a measure in the wrapper models of feature selection is it is slow to compute. For example, to construct a decision tree, it would take at least O (N log N ).
Reference: 3. <author> G.H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant feature and the subset selection problem. </title> <booktitle> In Proceedings of ICML, </booktitle> <pages> pages 121-129. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: When N is large, this exhaustive approach is out of the question. Therefore, various feature selection methods have been designed to avoid exhaustive search while still aiming at the optimal subset. Examples are Branch & Bound [7], Focus [1], Relief [4], Wrapper methods <ref> [3] </ref>, and LVF [5]. The feature selection problem can be viewed as a search problem [9]. The search process starts with either an empty set or a full set. <p> Many researchers pointed out that the only remaining alternative is to use the error rate of a classifier as the measure. Among many classifiers, however, only the Bayes Classifier satisfies this monotonicity condition 1 because other classifiers adopt some assumptions and employ certain heuristics <ref> [9, 2, 3] </ref>. Another disadvantage of using the error rate as a measure in the wrapper models of feature selection is it is slow to compute. For example, to construct a decision tree, it would take at least O (N log N ). <p> We select two groups of data sets: one with known relevant features and the other with unknown relevant features as shown in Table 1. All data sets are from [6] except for Corral <ref> [3] </ref>. For the first group of 5 data sets we compare the subsets selected by ABB with the known. For the second group we compare the outputs of ABB with that of Focus, a popular method in literature that guarantees optimal subsets.
Reference: 4. <author> K. Kira and L.A. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Proceedings of AAAI, </booktitle> <pages> pages 129-134. </pages> <year> 1992. </year>
Reference-contexts: When N is large, this exhaustive approach is out of the question. Therefore, various feature selection methods have been designed to avoid exhaustive search while still aiming at the optimal subset. Examples are Branch & Bound [7], Focus [1], Relief <ref> [4] </ref>, Wrapper methods [3], and LVF [5]. The feature selection problem can be viewed as a search problem [9]. The search process starts with either an empty set or a full set.
Reference: 5. <author> H. Liu and R. Setiono. </author> <title> A probabilistic approach to feature selection a filter solution. </title> <booktitle> In Proceedings of ICML, </booktitle> <pages> pages 319-327. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: When N is large, this exhaustive approach is out of the question. Therefore, various feature selection methods have been designed to avoid exhaustive search while still aiming at the optimal subset. Examples are Branch & Bound [7], Focus [1], Relief [4], Wrapper methods [3], and LVF <ref> [5] </ref>. The feature selection problem can be viewed as a search problem [9]. The search process starts with either an empty set or a full set.
Reference: 6. <author> C.J. Merz and P.M. Murphy. </author> <title> UCI repository of machine learning databases. </title> <address> http://www.ics.uci.edu/~mlearn/MLRepository.html . Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1996. </year>
Reference-contexts: ABB indeed finds optimal subsets for various data sets, and 2. features selected are good for various learning algorithms. We select two groups of data sets: one with known relevant features and the other with unknown relevant features as shown in Table 1. All data sets are from <ref> [6] </ref> except for Corral [3]. For the first group of 5 data sets we compare the subsets selected by ABB with the known. For the second group we compare the outputs of ABB with that of Focus, a popular method in literature that guarantees optimal subsets.
Reference: 7. <author> P.M. Narendra and K. Fukunaga. </author> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Trans. on Computer, </journal> <volume> C-26(9):917-922, </volume> <month> September </month> <year> 1977. </year>
Reference-contexts: When N is large, this exhaustive approach is out of the question. Therefore, various feature selection methods have been designed to avoid exhaustive search while still aiming at the optimal subset. Examples are Branch & Bound <ref> [7] </ref>, Focus [1], Relief [4], Wrapper methods [3], and LVF [5]. The feature selection problem can be viewed as a search problem [9]. The search process starts with either an empty set or a full set. <p> For the former, it expands the search space by adding one feature at a time (Sequential Forward Selection) [1]; for the latter, it expands the search space by deleting one feature at a time (Sequential Backward Selection) <ref> [7] </ref>. As we shall see, a good alternative to exhaustive search is Branch & Bound like algorithms if there exists a monotonic function of evaluating features.
Reference: 8. <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: For the second group we compare the outputs of ABB with that of Focus, a popular method in literature that guarantees optimal subsets. For the second objective we choose two different learning algorithms: a decision tree method (C4.5 <ref> [8] </ref>) and a standard backpropagation neural network (SNNS [10]). Two thirds of the data is used for selecting features by ABB and Focus. The other one third is the testing data for SNNS. We run 10-fold cross validation with C4.5 on the whole data.
Reference: 9. <author> W. Siedlecki and J Sklansky. </author> <title> On automatic feature selection. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 197-220, </pages> <year> 1988. </year>
Reference-contexts: Therefore, various feature selection methods have been designed to avoid exhaustive search while still aiming at the optimal subset. Examples are Branch & Bound [7], Focus [1], Relief [4], Wrapper methods [3], and LVF [5]. The feature selection problem can be viewed as a search problem <ref> [9] </ref>. The search process starts with either an empty set or a full set. <p> In other words, the optimal subset is guaranteed. Many distance and information based measures have been shown to be non-monotonic <ref> [9] </ref>. Many researchers pointed out that the only remaining alternative is to use the error rate of a classifier as the measure. Among many classifiers, however, only the Bayes Classifier satisfies this monotonicity condition 1 because other classifiers adopt some assumptions and employ certain heuristics [9, 2, 3]. <p> Many researchers pointed out that the only remaining alternative is to use the error rate of a classifier as the measure. Among many classifiers, however, only the Bayes Classifier satisfies this monotonicity condition 1 because other classifiers adopt some assumptions and employ certain heuristics <ref> [9, 2, 3] </ref>. Another disadvantage of using the error rate as a measure in the wrapper models of feature selection is it is slow to compute. For example, to construct a decision tree, it would take at least O (N log N ).
Reference: 10. <author> A. Zell and et al. </author> <title> Stuttgart neural network simulator (SNNS), user manual, </title> <note> version 4.1. ftp.informatik.uni-stuttgart.de/pub/SNNS, </note> <year> 1995. </year>
Reference-contexts: For the second group we compare the outputs of ABB with that of Focus, a popular method in literature that guarantees optimal subsets. For the second objective we choose two different learning algorithms: a decision tree method (C4.5 [8]) and a standard backpropagation neural network (SNNS <ref> [10] </ref>). Two thirds of the data is used for selecting features by ABB and Focus. The other one third is the testing data for SNNS. We run 10-fold cross validation with C4.5 on the whole data.
References-found: 10

