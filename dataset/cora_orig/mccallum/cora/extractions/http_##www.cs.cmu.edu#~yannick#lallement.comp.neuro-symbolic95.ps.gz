URL: http://www.cs.cmu.edu/~yannick/lallement.comp.neuro-symbolic95.ps.gz
Refering-URL: http://www.cs.cmu.edu/~yannick/publis.html
Root-URL: 
Email: lallemen@loria.fr, falex@loria.fr  hilario@cui.unige.ch  
Phone: Fax: (+33) 83 41 30 79  (2)  Fax: (+41) 22 320 29 27  
Title: Neurosymbolic Integration: Cognitive Grounds and Computational Strategies  
Author: Yannick Lallement (), Melanie Hilario (), Frederic Alexandre () 
Keyword: Connectionism, symbolic AI, neurosymbolic integration, hybrid models, connectionist symbol processing  
Address: BP 239 Campus scientifique F-54506 Vanduvre-les-Nancy Cedex  24 rue General-Dufour CH-1211 Geneva 4  
Affiliation: (1) CRIN-INRIA Lorraine  CUI University of Geneva  
Abstract: The ultimate|if implicit|goal of artificial intelligence (AI) research is to model the full range of human cognitive capabilities. Symbolic AI and connectionism, the major AI paradigms, have each tried|and failed|to attain this goal. In the meantime, the idea has gained ground that this goal might still be within reach if we could harness the respective strengths of these two paradigms in integrated neurosymbolic models. This paper attempts to lay a cognitive basis for neurosymbolic integration and describes the different strategies that have been adopted to date. Unified approaches strive to attain symbol-processing capabilities using neural network techniques alone, while hybrid approaches blend symbolic and neural models in novel architectures with the hope of gleaning the best of both paradigms. 
Abstract-found: 1
Intro-found: 1
Reference: [ABGH91] <author> F. Alexandre, Y. Burnod, F. Guyot, and J-P. Haton. </author> <title> The cortical column: a new processing unit for multilayered networks. Neural Networks, </title> <type> 4(1), </type> <year> 1991. </year>
Reference-contexts: It has also been implemented and tested from a computational point of view <ref> [ABGH91] </ref>. A similar approach is proposed by Edelman [Ede87]. At the microscopic level, the cortical column is repeated throughout the cortex and is organized in a micro-circuit of neurons performing specific operations (transfer function tuning, modulation gating, temporal integration) on specific data.
Reference: [AGH90] <author> F. Alexandre, F. Guyot, and J-P. Haton. </author> <title> A connectionist network with two complementary visual processing systems for x-ray image interpretation. </title> <booktitle> In International Neural Networks Conference, </booktitle> <year> 1990. </year>
Reference-contexts: For example, vision and motion are linked through parietal and temporal associative areas to allow execution of tasks such as visually guided grasping. Within this framework, a set of applications have been carried out to assess the abilities of the model, including visual <ref> [AGH90] </ref> and speech processing [GADH90], but also more symbolic or reasoning oriented tasks [GAH90, Bla94, GDBS93b]. 3.1.2 Perceptually grounded language learning Katamic memories [ND93] and the dete model [ND94] also correspond to more deeply biologically inspired neural models.
Reference: [Ajj94] <author> V. Ajjanagadde. </author> <title> Unclear distinctions lead to unnecessary shortcomings: Examining the rule vs fact, role vs filler, and type vs predicate distinctions from a connectionist representation and reasoning perspective. </title> <journal> In American Association Artificial Intelligence, </journal> <year> 1994. </year>
Reference-contexts: However, the model has significant difficulties in scaling up to complex, high-level reasoning; more importantly, it fails to give any convincing account of the learning process. A later version of the system <ref> [Ajj94] </ref> uses a single type of nodes in the inference network, thus facilitating the process of learning. 3.2.2 Distributed models Smolensky [Smo90] proposes a representation scheme based on tensor products. This system can represent data structures (such as strings, sentences or trees).
Reference: [AS91] <author> V. Ajjanagadde and L. Shastri. </author> <title> Rules and variables in neural nets. </title> <journal> Neural Computation, </journal> (3):121-134, 1991. 
Reference-contexts: as localist (each node in the system corresponds to a single concept, and vice-versa), distributed (each concept is encoded using several nodes, and each node participates in the representation of several concepts), or combined localist/distributed. 3.2.1 Localist models A connectionist architecture for rule-based reasoning and variable binding is described in <ref> [AS91] </ref>. Knowledge is represented in a localist fashion using several types of nodes: predicate nodes (for predicate names), role nodes (for predicate arguments or variables), and filler nodes (for possible role values). Rules are encoded by interconnection patterns between predicate and role nodes.
Reference: [Bla94] <author> P. Blanchet. </author> <title> An architecture for representing and learning behaviors by trial and error. </title> <booktitle> In Symposium on Adaptive Behaviour, </booktitle> <address> Brighton, </address> <year> 1994. </year>
Reference-contexts: Within this framework, a set of applications have been carried out to assess the abilities of the model, including visual [AGH90] and speech processing [GADH90], but also more symbolic or reasoning oriented tasks <ref> [GAH90, Bla94, GDBS93b] </ref>. 3.1.2 Perceptually grounded language learning Katamic memories [ND93] and the dete model [ND94] also correspond to more deeply biologically inspired neural models. These perceptually based models permit acquisition of natural language semantics from interactions with the real world.
Reference: [BLN91] <author> W. R. Becraft, P. L. Lee, and R. B. Newell. </author> <title> Integration of neural networks and expert systems for process fault diagnosis. </title> <booktitle> In Proc. of the 12 th IJCAI, </booktitle> <pages> pages 832-837, </pages> <year> 1991. </year>
Reference-contexts: In examples found so far, the symbolic main processor delegates subtasks to a neural subprocessor; it remains an open question whether the reverse setup|a connectionist main processor subcontracting tasks to a symbolic subprocessor|is at all possible. In innate/qualms <ref> [BLN91] </ref>, the symbolic module is the main processor whose task is fault diagnosis in a distillation plant. It is a classical expert system linked to a simulation model of the process to be monitored.
Reference: [DA94] <author> S. Durand and F. Alexandre. </author> <title> A neural network based on sequence learning: Application to spoken digits recognition. </title> <booktitle> In 7th international conference on Neural Networks and Their Applications, </booktitle> <pages> pages 290-298, </pages> <address> Mar-seille, </address> <year> 1994. </year>
Reference-contexts: Katamic memories are cerebellar-like networks for sequence learning. Here too, the basic unit reveals more complex behavior than the classical formal neuron. We also find in this approach this novel trend in biologically inspired neural networks that consists in building models able to deal with sequence representation and manipulation <ref> [DA94] </ref>. The dete system uses a large set of Katamic memories performing associative tasks from verbal to motor or from visual to motor pole. Those associations then enable to build verbal sequences describing a visual representation.
Reference: [DD88] <author> H. Dreyfus and S. Dreyfus. </author> <title> Making a mind versus modeling the brain: </title> <booktitle> Artificial intelligence at a branchpoint. Daedalus, Winter, </booktitle> <year> 1988. </year>
Reference-contexts: Thus the neuron-to-symbol (or neuron-based) approach is a biologically based, bottom-up approach: it is perhaps the one AI strategy (whether symbolic or connectionist, whether integrative or not) which can be rightly described as "modelling the brain" instead of "making a mind" <ref> [DD88] </ref>. This approach is still in its infancy, and though successes have been scored at the perceptual level, it will take some time before a frontal connectionist attack on symbol processing can be envisaged.
Reference: [Ede87] <author> G. Edelman. </author> <title> Neural Darwinism: the theory of neural group selection. </title> <publisher> Basic Books, </publisher> <year> 1987. </year>
Reference-contexts: It has also been implemented and tested from a computational point of view [ABGH91]. A similar approach is proposed by Edelman <ref> [Ede87] </ref>. At the microscopic level, the cortical column is repeated throughout the cortex and is organized in a micro-circuit of neurons performing specific operations (transfer function tuning, modulation gating, temporal integration) on specific data. These data originate from the connectivity of this unit.
Reference: [GADH90] <author> F. Guyot, F. Alexandre, C. Dingeon, and J-P. Haton. </author> <title> The cortical column as a model for speech recognition: Principles and first experiments. In Speech Recognition and Understanding Recent Advances, Trends and Applications. </title> <publisher> Springer Verlag, </publisher> <year> 1990. </year>
Reference-contexts: For example, vision and motion are linked through parietal and temporal associative areas to allow execution of tasks such as visually guided grasping. Within this framework, a set of applications have been carried out to assess the abilities of the model, including visual [AGH90] and speech processing <ref> [GADH90] </ref>, but also more symbolic or reasoning oriented tasks [GAH90, Bla94, GDBS93b]. 3.1.2 Perceptually grounded language learning Katamic memories [ND93] and the dete model [ND94] also correspond to more deeply biologically inspired neural models. These perceptually based models permit acquisition of natural language semantics from interactions with the real world.
Reference: [GAH90] <author> F. Guyot, F. Alexandre, and J-P. Haton. </author> <title> Principles and applications of the cortical column symbolic neural model. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <address> San Diego, </address> <year> 1990. </year>
Reference-contexts: Within this framework, a set of applications have been carried out to assess the abilities of the model, including visual [AGH90] and speech processing [GADH90], but also more symbolic or reasoning oriented tasks <ref> [GAH90, Bla94, GDBS93b] </ref>. 3.1.2 Perceptually grounded language learning Katamic memories [ND93] and the dete model [ND94] also correspond to more deeply biologically inspired neural models. These perceptually based models permit acquisition of natural language semantics from interactions with the real world.
Reference: [Gal88] <author> S. I. Gallant. </author> <title> Connectionist expert systems. </title> <journal> Communications of the ACM, </journal> <volume> 31(2), </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Since its inception, artificial intelligence (AI) has been torn between two major paradigms|symbolic AI and connectionist AI. Each AI researcher has had to take a stand, at least implicitly, in the symbolic/connectionist debate. It is widely recognized (see, for example, <ref> [Gal88] </ref> and [Gut92]) that these two paradigms have different and complementary strengths. Connectionist models often offer good performance in pattern recognition and generalization, and present such qualities as natural learning ability, noise tolerance and graceful degradation. <p> According to conventional wisdom in AI, the distinction between low-level and high-level tasks/operations maps quite easily|albeit superficially|onto the distinction between the symbolic and the connectionist paradigms. It is widely recognized (see, e.g., <ref> [Gal88] </ref> and [Gut92]) that these two paradigms have different and complementary strengths. For example, connectionist models often offer good performance in pattern recognition and generalization, and present such qualities as natural learning ability, noise tolerance and graceful degradation.
Reference: [GDBS93a] <author> E. Guigon, B. Dorizzi, Y. Burnod, and W. Schultz. </author> <title> Neural correlates of learning in the prefrontal cortex: a predictive model. </title> <type> Cerebral Cortex, </type> <year> 1993. </year>
Reference-contexts: They aim at building a representation from these low-level cues that could permit interpretation, reasoning or planning. 3.1.1 Cortical Columns The cortical column model has been proposed as a basic functional unit of cortex-like networks from a biological point of view <ref> [GDBS93a] </ref>. It has also been implemented and tested from a computational point of view [ABGH91]. A similar approach is proposed by Edelman [Ede87].
Reference: [GDBS93b] <author> E. Guigon, B. Dorizzi, Y. Burnod, and W. Schultz. </author> <title> Neural correlates of learning in the prefrontal cortex: a predictive model. </title> <type> Cerebral Cortex, </type> <year> 1993. </year>
Reference-contexts: Within this framework, a set of applications have been carried out to assess the abilities of the model, including visual [AGH90] and speech processing [GADH90], but also more symbolic or reasoning oriented tasks <ref> [GAH90, Bla94, GDBS93b] </ref>. 3.1.2 Perceptually grounded language learning Katamic memories [ND93] and the dete model [ND94] also correspond to more deeply biologically inspired neural models. These perceptually based models permit acquisition of natural language semantics from interactions with the real world.
Reference: [GP90] <author> Matthias Gutknecht and Rolf Pfeifer. </author> <title> An approach to integrating expert systems with connectionist networks. </title> <journal> AICOM, </journal> <volume> 3 </volume> <pages> 116-127, </pages> <year> 1990. </year>
Reference-contexts: With this function replacement, the hybrid system is able to recognize grammatical as well as non-grammatical sentences, whether encountered previously or not. Another example of neural metaprocessing is a system in which the baselevel symbolic processor solves high school physics problems, and the connectionist component enforces search control <ref> [GP90] </ref>. Object-level rules encode equations such as velocity = initial-velocity + acceleration x time. If several of the needed variables are unknown, the metalevel connectionist module is called on to make a choice.
Reference: [Gut92] <author> M. Gutknecht. </author> <title> The postmodern mind: hybrid models of cognition. </title> <journal> Connection Science, </journal> <volume> 4(3 & 4), </volume> <year> 1992. </year>
Reference-contexts: 1 Introduction Since its inception, artificial intelligence (AI) has been torn between two major paradigms|symbolic AI and connectionist AI. Each AI researcher has had to take a stand, at least implicitly, in the symbolic/connectionist debate. It is widely recognized (see, for example, [Gal88] and <ref> [Gut92] </ref>) that these two paradigms have different and complementary strengths. Connectionist models often offer good performance in pattern recognition and generalization, and present such qualities as natural learning ability, noise tolerance and graceful degradation. <p> According to conventional wisdom in AI, the distinction between low-level and high-level tasks/operations maps quite easily|albeit superficially|onto the distinction between the symbolic and the connectionist paradigms. It is widely recognized (see, e.g., [Gal88] and <ref> [Gut92] </ref>) that these two paradigms have different and complementary strengths. For example, connectionist models often offer good performance in pattern recognition and generalization, and present such qualities as natural learning ability, noise tolerance and graceful degradation.
Reference: [HCK92] <author> S. Hayes, V.B. Ciesielski, and W. Kelly. </author> <title> A comparison of an expert system and a neural network for respiratory system monitoring. </title> <type> Technical Report TR #92/1, </type> <institution> Royal Melbourne Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: We distinguish four integration modes: chainprocessing, subprocess-ing, metaprocessing and coprocessing. 4.1 Chainprocessing In chainprocessing mode, one of the modules|either symbolic or neural|is the main processor while the other takes charge of pre and/or postprocessing tasks. In <ref> [HCK92] </ref>, for instance, the main processor is a respiratory monitoring expert system which recommends actions to be taken to avoid breathing complications. Rules are of the form: "If qualitative-state then action", where qualitative-state is a symbolic representation of a change in lung pressure over time (e.g., "pressure is rising rapidly").
Reference: [Hen89] <author> J. Hendler. </author> <title> Marker-passing over microfeatures : Towards a hybrid symbolic/connectionist model. </title> <journal> Cognitive Science, </journal> (13):79-106, 1989. 
Reference-contexts: A third example of neural subprocessing is the hybrid composed of scraps and a neural network that learns by backpropagation <ref> [Hen89] </ref>. The main processor is a marker-passing semantic net which performs complex reasoning tasks like planning.
Reference: [HLA95] <author> M. Hilario, Y. Lallement, and F. Alexandre. </author> <title> Neurosymbolic integration: Unified versus hybrid approaches. </title> <booktitle> In The European Symposium On Artificial Neural Networks, </booktitle> <address> Brussels, Belgium, </address> <year> 1995. </year>
Reference-contexts: Their integration into neuro-symbolic models is certainly a key issue for the future of AI and cognitive modeling. The main research issue met by the hybrid approach is to find ways of integration and communication between models from different origins <ref> [HLA95] </ref>. The unified models have to get to a middle sub-symbolic level that seems to be especially significant in cognition [Hof85]. To achieve this, the symbol-to-neuron and the neuron-to-symbol approaches have to go further in their respectively top-down and bottom-up ways.
Reference: [HLG92] <author> D.A. Handelman, S.H. Lane, and J.J. Gelfand. </author> <title> Robotic skill acquisition based on biological principles. </title> <editor> In A. Kandel and G. Langholz, editors, </editor> <booktitle> Hybrid Architectures for Intelligent Systems, </booktitle> <pages> pages 301-327. </pages> <publisher> CRC Press, </publisher> <year> 1992. </year>
Reference-contexts: Symbolic metaprocessing is illustrated in the Robotic Skill Acquisition Architecture <ref> [HLG92] </ref>. Its goal is to develop robots which accomplish complex tasks using designer-supplied instructions and self-induced practice. The architecture's base level is itself hybrid: a rule-base provides a declarative representation of human expert knowledge whereas neural networks embody reflexive procedural knowledge that comes with practice.
Reference: [Hof85] <author> D. Hofstadter. Metamagical themas: </author> <title> questing for the essence of mind and pattern. </title> <publisher> Basic Books, </publisher> <year> 1985. </year>
Reference-contexts: The main research issue met by the hybrid approach is to find ways of integration and communication between models from different origins [HLA95]. The unified models have to get to a middle sub-symbolic level that seems to be especially significant in cognition <ref> [Hof85] </ref>. To achieve this, the symbol-to-neuron and the neuron-to-symbol approaches have to go further in their respectively top-down and bottom-up ways. Results obtained so far by these integrated strategies are inversely proportional to the difficulty of their respective tasks.
Reference: [HPA94] <author> M. Hilario, C. Pellegrini, and F. Alexandre. </author> <title> Modular Integration of Connectionist and Symbolic Processing in Knowledge-Based Systems. </title> <booktitle> In Proceedings International Symposium on Integrating Knowledge and Neural Heuristics, </booktitle> <address> Pensacola Beach (Florida, USA), </address> <month> May </month> <year> 1994. </year>
Reference-contexts: To achieve this, the symbol-to-neuron and the neuron-to-symbol approaches have to go further in their respectively top-down and bottom-up ways. Results obtained so far by these integrated strategies are inversely proportional to the difficulty of their respective tasks. The hybrid approach has led to more convincing results so far <ref> [HPA94] </ref>: some industrial applications are already operational while unified models are still far from achieving the representational power of symbolic systems. However, considering biological evidence, unified approaches|in particular the neuron-to-symbol approach|appear to be very promising.
Reference: [JPL + 92] <author> M. Jabri, S. Pickard, P. Leong, Z. Chi, B. Flower, and Y. Xie. </author> <title> ANN-based classification for heart difibrillators. </title> <booktitle> In Advances in Neural Information Processing 4, </booktitle> <pages> pages 637-644. </pages> <publisher> Morgan-Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Cooperative neurosymbolic coprocessing is illustrated in a system which combines decision trees and neural nets to improve arrhythmia diagnosis <ref> [JPL + 92] </ref>.
Reference: [KF92] <author> S.C. Kwasny and K.A. Faisal. </author> <title> Symbolic parsing via subsymbolic rules. </title> <editor> In J. Dinsmore, editor, </editor> <booktitle> The Symbolic and Connectionist Paradigms: Closing the Gap, </booktitle> <pages> pages 209-235. </pages> <publisher> Lawrence Erlbaum, </publisher> <year> 1992. </year>
Reference-contexts: The metaprocessor is a rule-based execution monitor which supervises the training of the neural network and controls the operation of the system during the learning process. An example of neural metaprocessing is Kwasny and Faisal's natural language parser <ref> [KF92] </ref>. Classical parsing systems map an input sentence into a parse tree by maintaining an lookahead buffer and an internal stack. Tree-building actions (e.g., creation of a new node) are taken by a set of rules after examining both buffer and stack.
Reference: [Med94] <author> Larry R. Medsker. </author> <title> Hybrid Neural Network and Expert Systems. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: Each output neuron is associated with a particular fault and its activation is interpreted as the rating of the corresponding fault. The expert system then either confirms the neural diagnosis or proposes another hypothesis. lam tm , a hybrid system for window glazing design <ref> [Med94] </ref>, is another example of neural subprocessing.
Reference: [ND93] <author> V. Nenov and M. Dyer. </author> <title> Perceptually grounded language learning: Part 1 - a neural network architecture for robust sequence association. </title> <journal> Connection Science, </journal> <volume> 5(2), </volume> <year> 1993. </year>
Reference-contexts: Within this framework, a set of applications have been carried out to assess the abilities of the model, including visual [AGH90] and speech processing [GADH90], but also more symbolic or reasoning oriented tasks [GAH90, Bla94, GDBS93b]. 3.1.2 Perceptually grounded language learning Katamic memories <ref> [ND93] </ref> and the dete model [ND94] also correspond to more deeply biologically inspired neural models. These perceptually based models permit acquisition of natural language semantics from interactions with the real world.
Reference: [ND94] <author> V. Nenov and M. Dyer. </author> <title> Perceptually grounded language learning: Part 2 - dete: a neural/procedural model. </title> <journal> Connection Science, </journal> <volume> 6(1), </volume> <year> 1994. </year>
Reference-contexts: Within this framework, a set of applications have been carried out to assess the abilities of the model, including visual [AGH90] and speech processing [GADH90], but also more symbolic or reasoning oriented tasks [GAH90, Bla94, GDBS93b]. 3.1.2 Perceptually grounded language learning Katamic memories [ND93] and the dete model <ref> [ND94] </ref> also correspond to more deeply biologically inspired neural models. These perceptually based models permit acquisition of natural language semantics from interactions with the real world.
Reference: [New90] <author> Alan Newell. </author> <title> Unified theories of cognition. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: In particular, it is useful to distinguish between low-level and high-level operations. Low-level operations are mostly concerned with perceptual tasks (such as vision, speech recognition, locomotion), whereas high-level operations are mostly concerned with reasoning tasks that involve volition and deliberation (such as problem solving and decision-making). Newell <ref> [New90] </ref> has classified these operations according to their duration. This distinction between low-level and high-level tasks/operations also maps directly onto Steel's distinction between behavior-based and knowledge-based tasks [Ste89].
Reference: [PS94] <author> S. Piramuthu and M.J. Shaw. </author> <title> On using decision tree as feature selector for feed-forward neural networks. </title> <booktitle> In International Symposium on Integrating Knowledge and Neural Heuristics, </booktitle> <pages> pages 67-74, </pages> <year> 1994. </year>
Reference-contexts: The reverse setup is that of a symbolic preprocessor assisting a connectionist main processor. An example is the use of decision trees as feature selectors to limit the number of input nodes in feedforward neural networks <ref> [PS94] </ref>.
Reference: [Smo90] <author> P. Smolensky. </author> <title> Tensor product variable binding and the representation of symbolic structures in connectionist systems. </title> <journal> Artificial Intelligence, </journal> (46):159-216, 1990. 
Reference-contexts: A later version of the system [Ajj94] uses a single type of nodes in the inference network, thus facilitating the process of learning. 3.2.2 Distributed models Smolensky <ref> [Smo90] </ref> proposes a representation scheme based on tensor products. This system can represent data structures (such as strings, sentences or trees).
Reference: [Ste89] <author> L. Steels. </author> <title> Connectionist problem solving|an ai perspective. </title> <editor> In R. Pfeifer, Z. Schreter, and F. Fogelman-Soulie, editors, </editor> <booktitle> Connectionism in Perspective, </booktitle> <pages> pages 215-228. </pages> <publisher> Elsevier, </publisher> <year> 1989. </year>
Reference-contexts: Newell [New90] has classified these operations according to their duration. This distinction between low-level and high-level tasks/operations also maps directly onto Steel's distinction between behavior-based and knowledge-based tasks <ref> [Ste89] </ref>. Since an operation is seldom fully low-level or fully high-level, we propose to draw a continuum, named cognitive spectrum between these two extremities.
Reference: [Sun91] <author> R. Sun. </author> <title> Integrating Rules and Connectionism for Robust Reasoning : A Connectionist Architecture with Dual Representation. </title> <type> PhD thesis, </type> <institution> Bran-deis University, </institution> <address> Waltham, MA 02254, </address> <year> 1991. </year>
Reference-contexts: Pointer traversal is implemented via associative retrieval, and a connectionist maintenance system adds, deletes and updates memory triples. Fault tolerance and robustness are side benefits of this distributed representation: like biological networks, boltzcons will not crash abruptly if some cells cease to function. 3.2.3 Localist/distributed models consyderr <ref> [Sun91] </ref> is a unified system aimed at modelling common-sense reasoning (e.g., Can a duck fly? A duck is like a sparrow, so I guess it can fly.). It uses a dual but purely connectionist representation scheme.
Reference: [Tou90] <author> D. S. Touretzky. </author> <title> Boltzcons : Dynamic symbol structures in a connectionist network. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 46(1-2), </pages> <year> 1990. </year>
Reference-contexts: Connectionist methods are used to set and retrieve the bindings. This framework is general enough to represent complex (possibly recursive) data structures and allow classical operations (e.g. a la Lisp). However, learning procedures associated with usual connectionist networks are lost in this case. boltzcons <ref> [Tou90] </ref> uses distributed representations of linked lists to implement symbolic structures like trees and stacks. One cell of a linked list is encoded as a triple of symbols of form (tag, car, cdr).
References-found: 33

