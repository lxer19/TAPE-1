URL: http://www.eecs.umich.edu/techreports/cse/1998/CSE-TR-362-98.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse98.html
Root-URL: http://www.cs.umich.edu
Title: REDUCING COMMUNICATION COST IN SCALABLE SHARED MEMORY SYSTEMS  
Author: by Gheith Ali Abandah Professor Kang G. Shin 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Science and Engineering) in The  Doctoral Committee: Professor Edward S. Davidson, Chair Assistant Professor Peter M. Chen Assistant Professor Emad S. Ebbini Assistant Professor Steven K. Reinhardt  
Date: 1998  
Affiliation: University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 144 BIBLIOGRAPHY </institution>
Reference: [AAD + 93] <author> T. Asprey, G. Averill, E. DeLano, R. Mason, B. Weiner, and J. Yetter. </author> <title> Performance Features of the PA7100 Microprocessor. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 2234, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Yet, due to improved packaging and device density, a 2-node SPP2000 tower is physically the same size as a 2-node SPP1000 tower. has eight of the right functional block. RI is the ring interface. The SPP1000 uses the Hewlett-Packard PA 7100 <ref> [AAD + 93] </ref>, a two-way superscalar processor, running at 100 MHz. The SPP2000 uses the PA 8000 [Hun95], a four-way superscalar processor, running at 180 MHz.
Reference: [AB97] <author> G. Astfalk and T. Brewer. </author> <title> An Overview of the HP/Convex Exemplar Hardware. </title> <type> Tech. paper, </type> <institution> Hewlett-Packard Co., </institution> <month> June </month> <year> 1997. </year> <note> http://www.hp.com/wsg/tech/technical.html. </note>
Reference-contexts: Detailed surveys of distributed shared-memory concepts and systems are found in [LW95, PTM96, CSG98]. Many CC-NUMA systems, existing commercial machines as well as research prototypes, use caching to reduce the number of remote capacity misses <ref> [AB97, LC96, NAB + 95, LLG + 92, FW97, MD98] </ref>. Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. <p> Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. Many approaches have used DRAMs to serve as large interconnect caches (Exemplar <ref> [AB97] </ref>, NUMA-Q [LC96], S3.mp [NAB + 95], and NUMA-RC [ZT97]), or SRAMs to serve as fast interconnect caches (DASH [LLG + 92]), or both (R-NUMA [FW97] and VC-NUMA [MD98]). In Chapter 7, we evaluate the worthiness of an SRAM interconnect cache in reducing the number of remote communication misses. <p> Each node that has a copy of a memory line, maintains pointers to the next forward and next backward nodes in that line's sharing list. Most of the glue logic that coherently interconnects the distributed processors and memory banks is in custom-designed gate arrays <ref> [AB97] </ref>. The SPP1000 uses 250-K gate gallium arsenide technology, which provided the speed to pump a 16-bit flit onto the ring each 3.33 nanoseconds. The SPP2000 uses 0.35- CMOS technology, which has evolved to provide competitive speeds in addition to its lower power consumption and higher integration. <p> This improved performance is achieved by supporting up to 32 outstanding requests in each ring interface, rather than only one <ref> [AB97] </ref>. Therefore, the SPP2000 ring controller can process a line request concurrently with collapsing the sharing list of a replaced line. Notice that the SPP2000's curve shape differs slightly from the curve of the 512-MB direct-mapped cache model shown [AD98b].
Reference: [Aba96] <author> G. Abandah. </author> <title> Tools for Characterizing Distributed Shared Memory Applications. </title> <type> Technical Report HPL96157, </type> <institution> HP Laboratories, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: Sections 2.4, 2.5, and 2.6 describe the CIAT, CDAT, and CCAT trace analysis tools, respectively. Section 2.7 describes TDAT. Finally, Section 2.8 outlines our assessment of the accuracy of these tools. Further detail on these tools is reported in <ref> [Aba96] </ref>. 2.2 Trace Collection (SMAIT) SMAIT is based on RYO [ZK95], a tool developed by Zucker and Karp for instrumenting PA-RISC [Hew94] instruction sequences. RYO is a set of awk scripts that enable replacing individual machine instructions with calls to user written subroutines. <p> In a parallel phase, multiple threads are active. The tools recognize transitions between serial and parallel phases from the trace records of the thread-spawn and thread-join calls that activate and deactivate threads between serial and parallel phases. The user-defined phases are recognized when the tools encounter special marker records <ref> [Aba96] </ref>. The marker records can be generated by instrumenting the high-level source code. The tools perform analysis per phase and report characterization statistics at the end of each phase. They also report the characterization statistics aggregated over all phases at the end. <p> I/O requests are generated on DMA activities. The detailed trace contains 17 records of all the signals that are needed to satisfy processor requests and returns. More detail on the format of the two trace types is reported in <ref> [Aba96] </ref>. The general trace is used in time distribution analysis (TDAT) and to drive other system-level simulators (developed and used in Hewlett-Packard Laboratories, Palo Alto). The detailed trace is used for debugging CDAT.
Reference: [Aba97] <author> G. Abandah. </author> <title> Characterizing Shared-Memory Applications: A Case Study of the NAS Parallel Benchmarks. </title> <type> Technical Report HPL9724, </type> <institution> HP Laboratories, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: NPB consists of 5 kernels and 3 pseudo-applications that mimic the computation and data movement characteristics of large-scale computational fluid dynamic applications (an earlier report characterizes 5 benchmarks of NPB using CIAT and CDAT <ref> [Aba97] </ref>). The TPC benchmarks are intended to compare commercial database platforms. The following is a short description of the eight benchmarks. These particular applications were selected because they represent a wide range of applications. Radix is an integer sort kernel that iterates on radix r digits of the keys.
Reference: [ABC + 95] <author> A. Agarwal, R. Bianchini, D. Chaiken, K. Johnson, D. Kranz, J. Kubiatowicz, B-J. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proc. 22th ISCA, </booktitle> <pages> pages 213, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: To conserve space, we do not repeat this discussion here. But it is worthwhile to mention that many of the techniques and approaches used in the DSM systems that we focus on were developed in the Stanford DASH [LLG + 92], MIT Alewife <ref> [ABC + 95] </ref>, and SCI standard [SCI93] projects. Detailed surveys of distributed shared-memory concepts and systems are found in [LW95, PTM96, CSG98].
Reference: [ABP94] <author> G. Astfalk, T. Brewer, and G. Palmer. </author> <title> Cache Coherence in the Convex MPP. </title> <type> Tech. paper, </type> <institution> Hewlett-Packard Co., </institution> <month> February </month> <year> 1994. </year> <note> http://www.hp.com/wsg/tech/technical.html. </note>
Reference-contexts: Thus, the measured far WAR (or far RAW) latency is the average of four cases according to the location of the home node. The way that the internode coherence protocol satisfies a cache miss does in fact depend on the home node of the missed line <ref> [ABP94, SCI93] </ref>; the three main cases are: Local home: the line's home node is the local node. Remote home: the line's home node is the remote node. <p> Each tag holds the local and global sharing state of the respective line. The local state part includes the local caching status of each of the two 32-byte halves of the line. The intra-node coherence protocol is a three-state MSI protocol <ref> [ABP94] </ref>. The global sharing state is arranged in a doubly linked list distributed directory rooted in the home node. The tag in the home node contains the global status and a pointer to the head of the sharing list.
Reference: [AD96] <author> G. Abandah and E. Davidson. </author> <title> Modeling the Communication Performance of the IBM SP2. </title> <booktitle> In Proc. 10th IPPS, </booktitle> <pages> pages 249257, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Although the static scheduling overhead is incurred once per task and is only significant for short tasks, it usually increases with more processors and gives an indication on the parallel environment performance in scheduling processors and distributing the task's code to the scheduled processors <ref> [AD96, AD98b] </ref>. The parallel-loop scheduling overhead (PLSO) also usually increases as the number of processors increases. Since the PLSO can be significant for tens of processors, it may not be profitable to parallelize a short loop. <p> We have also shown that a corresponding communication performance characterization of message-passing multicomputers can be systematically carried out <ref> [AD96] </ref>. 60 CHAPTER 5 ASSESSING THE EFFECTS OF TECHNOLOGICAL ADVANCES WITH MICROBENCHMARKS In this chapter, we use the microbenchmarks developed in Chapter 4 to calibrate and evaluate the performance of two generations of the HP/Convex Exemplar. We characterize the performance effects of the changes in microarchitecture and implementation technology.
Reference: [AD98a] <author> G. Abandah and E. Davidson. </author> <title> A Comparative Study of Cache-Coherent Nonuniform Memory Access Systems. In High Performance Computing Systems and Applications. </title> <publisher> Kluwer Academic Publishers, </publisher> <month> May </month> <year> 1998. </year> <booktitle> 12th Ann. Int'l Symp. High Performance Computing Systems and Applications (HPCS'98). </booktitle>
Reference-contexts: Additionally, the invalidation time is linear with the number of sharing nodes. Although some of these issues are addressed in research work such as [KG96], linked-list protocols fundamentally require more directory accesses than other protocols <ref> [AD98a, CFKA90, CSG98] </ref>. Since the SPP2000 directory is implemented in memory, the high directory access rate adversely affects remote memory latency and bandwidth. 79 CHAPTER 6 EVALUATION OF THREE MAJOR CC-NUMA APPROACHES USING CDAT In this chapter, we present the results of our comparative study of three scalable shared-memory systems. <p> Moreover, we have used it to characterize several important aspects of a variety of scientific and commercial shared-memory applications [AD98c], calibrate and evaluate the performance of two generations of the Convex Exemplar [AD98b, AD98d], and evaluate three CC-NUMA approaches <ref> [AD98a] </ref>. Our methodology relies on instrumenting shared-memory applications to trace the data and code streams. The trace is analyzed using a combination of configuration independent and configuration dependent techniques. SMAIT is one of few tools that enable collecting traces of multi-threaded shared-memory applications.
Reference: [AD98b] <author> G. Abandah and E. Davidson. </author> <title> Characterizing Distributed Shared Memory Performance: A Case Study of the Convex SPP1000. </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> 9(2):206216, </volume> <month> February </month> <year> 1998. </year>
Reference-contexts: Although the static scheduling overhead is incurred once per task and is only significant for short tasks, it usually increases with more processors and gives an indication on the parallel environment performance in scheduling processors and distributing the task's code to the scheduled processors <ref> [AD96, AD98b] </ref>. The parallel-loop scheduling overhead (PLSO) also usually increases as the number of processors increases. Since the PLSO can be significant for tens of processors, it may not be profitable to parallelize a short loop. <p> The hit region ends at w = 1024 KB and the transition region ends at w = 2048 KB, indicating that the data cache is a direct-mapped 1024-KB cache <ref> [AD98b] </ref>. In the hit region, every access hits in the cache, while in the miss region (w 2048 KB), every access to a new cache line is a miss. <p> Therefore, the SPP2000 ring controller can process a line request concurrently with collapsing the sharing list of a replaced line. Notice that the SPP2000's curve shape differs slightly from the curve of the 512-MB direct-mapped cache model shown <ref> [AD98b] </ref>. In fact, it changes from one experiment to another according to the OS mapping of the array's virtual pages to the physical memory pages. <p> Parameterizing the Origin 2000 configuration file is based on the information available from SGI home page [O2K96, R1097]. As there is little publicly-available information about the SPP1000 latencies from Convex, we used the results of our calibration experiments <ref> [AD98b] </ref>. Table 6.1 summarizes the latencies used. The CDAT configuration files used in this study are listed in Appendix A. We refer to the three systems evaluated in this section as DASH, SPP, and Origin. <p> Moreover, we have used it to characterize several important aspects of a variety of scientific and commercial shared-memory applications [AD98c], calibrate and evaluate the performance of two generations of the Convex Exemplar <ref> [AD98b, AD98d] </ref>, and evaluate three CC-NUMA approaches [AD98a]. Our methodology relies on instrumenting shared-memory applications to trace the data and code streams. The trace is analyzed using a combination of configuration independent and configuration dependent techniques. SMAIT is one of few tools that enable collecting traces of multi-threaded shared-memory applications.
Reference: [AD98c] <author> G. Abandah and E. Davidson. </author> <title> Configuration Independent Analysis for Characterizing Shared-Memory Applications. </title> <booktitle> In Proc. 12th IPPS, </booktitle> <month> March </month> <year> 1998. </year>
Reference-contexts: The three systems have widely different miss ratios, especially with CG. DASH has the worst miss ratio followed by SPP. The large miss ratios of DASH and SPP are mainly due to capacity misses as the data working set is larger than the processor cache <ref> [AD98c] </ref>. Although CG's working set does not fit in the processor cache of SPP or DASH, SPP's miss ratio is smaller due to its wider cache. <p> Several researchers in the Hewlett-Packard Laboratories use it to characterize applications, parameterize workload generators, and evaluate alternative design options for the next generation of DSM systems. Moreover, we have used it to characterize several important aspects of a variety of scientific and commercial shared-memory applications <ref> [AD98c] </ref>, calibrate and evaluate the performance of two generations of the Convex Exemplar [AD98b, AD98d], and evaluate three CC-NUMA approaches [AD98a]. Our methodology relies on instrumenting shared-memory applications to trace the data and code streams. The trace is analyzed using a combination of configuration independent and configuration dependent techniques.
Reference: [AD98d] <author> G. Abandah and E. Davidson. </author> <title> Effects of Architectural and Technological Advances on the HP/Convex Exemplar's Memory and Communication Performance. </title> <booktitle> In Proc. 25th ISCA, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: Moreover, we have used it to characterize several important aspects of a variety of scientific and commercial shared-memory applications [AD98c], calibrate and evaluate the performance of two generations of the Convex Exemplar <ref> [AD98b, AD98d] </ref>, and evaluate three CC-NUMA approaches [AD98a]. Our methodology relies on instrumenting shared-memory applications to trace the data and code streams. The trace is analyzed using a combination of configuration independent and configuration dependent techniques. SMAIT is one of few tools that enable collecting traces of multi-threaded shared-memory applications.
Reference: [Amd67] <author> G. </author> <title> Amdahl. Validity of Single-Processor Approach to Achieving Large-Scale Computing Capability. </title> <booktitle> In Proc. AFIPS, </booktitle> <pages> pages 483485, </pages> <year> 1967. </year> <month> 145 </month>
Reference-contexts: Amdahl's serial fraction <ref> [Amd67] </ref>, ignoring the three parallel overheads, is Serial Fraction = Idle (p)=(p 1) Busy (1) Perfect speedup is only possible when the serial fraction, parallel overhead busy work, imbalance, and contention are zero.
Reference: [ASH86] <author> A. Agarwal, R. Sites, and M. Horwitz. ATUM: </author> <title> A New Technique for Capturing Address Traces Using Microcode. </title> <booktitle> In Proc. 13th ISCA, </booktitle> <pages> pages 119127, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Hardware monitors, e.g., the VAX microcode monitor [EC84], Convex CXpa [CXp93], and the IBM POWER performance monitor [WCNSH94] provide low-level information using event counters, but require special hardware support and so tend to be system-specific. ATUM <ref> [ASH86] </ref> generates a compressed trace file for post analysis, but is also based on hardware support since it enables collecting address traces by modifying the microcode.
Reference: [B + 94] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-94-07, </type> <institution> NASA Ames Research Center, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: mapping private data to local memory reduces the private access time, and mapping shared data to the node where it is most referenced generally reduces the average shared access time. 3.3 Applications We have analyzed Radix, FFT, LU, and Cholesky from SPLASH-2 [WOT + 95], CG and SP from NPB <ref> [B + 94] </ref>, and TPC benchmarks C and D [TPC92, TPC95]. SPLASH-2 consists of 8 applications and 4 computational kernels drawn from scientific, engineering, and graphics computing. <p> takes 460 nsec to satisfy a processor miss from the local memory, 106 and 690 nsec from a remote memory through one router. 7.4.2 Applications We used six applications to evaluate the four systems: Radix, FFT, LU, and Cholesky from SPLASH-2 [WOT + 95], and CG and SP from NPB <ref> [B + 94] </ref>. The inherent characteristics of these applications are presented in Chapter 3. Table 7.4 shows the two problem sizes used in this evaluation. These applications are compiled on an SPP1600 and instrumented using SMAIT.
Reference: [BA97] <author> T. Brewer and G. Astfalk. </author> <title> The Evolution of the HP/Convex Exemplar. </title> <booktitle> In Digest of papers, COMPCON'97, </booktitle> <pages> pages 8186, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: Several vendors are adopting the CC-NUMA architecture for building their new high-end servers. CC-NUMA achieves a nice balance between NUMA's programming complexity and COMA's hardware complexity. Examples of new CC-NUMA systems are: HP/Convex SPP2000 <ref> [BA97] </ref>, Sequent 2 NUMA-Q [LC96], and SGI Origin 2000 [LL97]. 1.2 DSM Performance and Programmability Computer architects increasingly rely on application characteristics for insight in designing cost-effective systems. This is true in the early design stages as well as later stages. <p> The SPP1000 was introduced in 1994 as Convex's first implementation of its Exemplar DSM architecture [Bre95]. In 1997, HP/Convex started to ship multi-node configurations of its second Exemplar generation, the SPP2000 <ref> [BA97] </ref>. Although the new system is upwardly binary compatible with older systems, it has significant differences that achieve higher performance and scalability. The SPP2000 uses a modern, superscalar processor that features out-of-order execution and non-blocking caches. <p> As visible from the measured data and the curve fits, the SPP2000 has much improved performance. The SPP2000 uses the new coherent increment primitive to reduce ring traffic and to lower barrier times <ref> [BA97] </ref>. In the SPP1000, when a thread reaches the barrier, it increments a counter and waits on a flag. When the last thread reaches the barrier, it updates the flag to signal to the other threads to go on. <p> CC-NUMA multiprocessors provide the convenience of memory sharing with one global address space, some portion of which is found in each node. CC-NUMA has a good balance between programming complexity and hardware complexity. Several vendors are adopting it for building new high-end servers, e.g., HP/Convex SPP2000 <ref> [BA97] </ref>, Sequent NUMA-Q [LC96], and SGI Origin 2000 [LL97].
Reference: [BCF96] <author> W. Bryg, K. Chan, and N. Fiduccia. </author> <title> A High-Performance, Low-Cost Multiprocessor Bus for Workstations and Midrange Servers. </title> <journal> Hewlett-Packard J., </journal> <volume> 47(1):1824, </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: The store transfer rate for strides 16 and 32 bytes is limited by a bus bandwidth bottleneck. Recall that the store kernel transfers two lines for each miss, a fetched line and a dirty replaced line. The SPP2000 uses the Runway bus <ref> [BCF96] </ref>, which is an 8-byte wide bus clocked at 120 MHz. This bus multiplexes data and addresses with a 4:2 ratio.
Reference: [BD94] <author> E. Boyd and E. Davidson. </author> <title> Communication in the KSR1 MPP: Performance Evaluation Using Synthetic Workload Experiments. </title> <booktitle> In Int'l Conf. on Supercomputing, </booktitle> <pages> pages 166175, </pages> <year> 1994. </year>
Reference-contexts: Commercial implementations usually employ hardware techniques because of their higher performance and easier programmability. Three commercial multiprocessors using hardware techniques, ordered by increasing degree of hardware support for coherent data replication and migration, are Cray Research T3D [KS93], Convex SPP1000 [Bre95], and Kendall Square Research KSR1 <ref> [FBR93, BD94] </ref>. These three multiprocessors represent three distinct paradigms for implementing DSM systems. The T3D interconnects dual-processor processing nodes using a 3-dimensional torus. The T3D is a non-uniform memory access (NUMA) multiprocessor. A processor can access memory in remote nodes using load and store instructions that optionally update its cache.
Reference: [BDCW91] <author> E. Brewer, C. Dellarocas, A. Colbrook, and W. Weihl. PROTEUS: </author> <title> A High-Performance Parallel-Architecture Simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: This technique often requires source code availability, perturbs the execution, and cannot be used with applications that generate code dynamically, e.g. data base systems. MPTRACE [EKKL90] uses assembly code instrumentation to collect traces of shared-memory multiprocessor applications. Simulation is also used in performance data collection, for example, Proteus <ref> [BDCW91] </ref>, Tango Lite [GH92], and SimOS [RHWG95]. Similar to code instrumentation, simulation enables collecting performance data of various kinds, but is often slower. SimOS collects traces for activities within the operating system in addition to the user-space activities and does not require source code availability.
Reference: [Boy95] <author> E. Boyd. </author> <title> Performance Evaluation and Improvement of Parallel Application on High Performance Architectures. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1995. </year>
Reference-contexts: The synchronization overhead is the time spent performing synchronization operations, e.g., 50 barrier synchronization, or acquiring a lock for a critical section [Hwa93]. The synchronization overhead does not include the wait time due to load imbalance, which can easily be treated separately <ref> [Tom95, Boy95] </ref>. We use simple Fortran programs to gather timing data for each of these three aspects of DSM performance. Each program calls and times a kernel 100 times and, after subtracting timer overheads, reports the minimum, average, and standard deviation of its call times.
Reference: [Bre95] <author> T. Brewer. </author> <title> A Highly Scalable System Utilizing up to 128 PA-RISC Processors. </title> <booktitle> In Digest of papers, COMPCON'95, </booktitle> <pages> pages 133140, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Commercial implementations usually employ hardware techniques because of their higher performance and easier programmability. Three commercial multiprocessors using hardware techniques, ordered by increasing degree of hardware support for coherent data replication and migration, are Cray Research T3D [KS93], Convex SPP1000 <ref> [Bre95] </ref>, and Kendall Square Research KSR1 [FBR93, BD94]. These three multiprocessors represent three distinct paradigms for implementing DSM systems. The T3D interconnects dual-processor processing nodes using a 3-dimensional torus. The T3D is a non-uniform memory access (NUMA) multiprocessor. <p> The SPLASH-2 benchmarks were developed at Stanford University to facilitate shared-memory multiprocessor research and are written in C. The NPB are specified algorithmically so that computer vendors can implement them on a wide range of parallel machines. We analyzed the Convex Exemplar <ref> [Bre95] </ref> implementation of NPB, which is written in Fortran. The performance of an earlier version of this implementation is reported in [SB95]. <p> We present experimental methods for measuring the memory access latency under varying conditions of access distance, sharing, and traffic. The distance is determined by where in the memory hierarchy, relative to the processor, the memory access is satisfied. For example, in the SPP1000 <ref> [Bre95] </ref>, a memory access can be satisfied by a hit in the processor cache, local memory, another processor's cache in the same node, remote shared memory, or a remote processor's cache. <p> The distributed memory is shared through a global address space, thus providing a natural and convenient programming model [LW95, CSG98]. The SPP1000 was introduced in 1994 as Convex's first implementation of its Exemplar DSM architecture <ref> [Bre95] </ref>. In 1997, HP/Convex started to ship multi-node configurations of its second Exemplar generation, the SPP2000 [BA97]. Although the new system is upwardly binary compatible with older systems, it has significant differences that achieve higher performance and scalability. <p> Thus, for this example, only the time of signal (3b) is not critical. 6.3 Convex SPP1000 The SPP1000 was introduced in 1994 and consists of 1 to 16 nodes that communicate via four rings <ref> [Bre95] </ref>. As shown in Figure 6.3, each node contains four functional blocks interconnected by a crossbar (XBR). Each functional block interfaces with one ring and contains two 100 MHz HP PA 7100 processors and two memory banks. Each processor has 1-MB instruction and data caches.
Reference: [CFKA90] <author> D. Chaiken, C. Fields, K. Kurihara, and A. Agarwal. </author> <title> Directory-Based Cache Coherence in Larg-Scale Multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 1224, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Additionally, the invalidation time is linear with the number of sharing nodes. Although some of these issues are addressed in research work such as [KG96], linked-list protocols fundamentally require more directory accesses than other protocols <ref> [AD98a, CFKA90, CSG98] </ref>. Since the SPP2000 directory is implemented in memory, the high directory access rate adversely affects remote memory latency and bandwidth. 79 CHAPTER 6 EVALUATION OF THREE MAJOR CC-NUMA APPROACHES USING CDAT In this chapter, we present the results of our comparative study of three scalable shared-memory systems.
Reference: [CHK + 96] <author> K. Chan, C. Hay, J. Keller, G. Kurpanek, F. Schumacher, and J. Zheng. </author> <title> Design of the HP PA 7200 CPU. </title> <journal> Hewlett-Packard J., </journal> <volume> 47(1):2533, </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: We have also verified CDAT's results against a report generated by Convex CXpa [CXp93]. CXpa collects performance statistics using hardware monitors on the SPP1600, which uses the PA7200 processor <ref> [CHK + 96] </ref>. For a single node configuration running a 256 fi 256 blocked matrix multiplication program, CDAT reported 14% more data misses. We believe that CDAT reported more misses because it does not model the PA7200's assist cache that eliminates some of the conflict misses.
Reference: [CLR94] <author> S. Chandra, J. Larus, and A. Rogers. </author> <title> Where is Time Spent in Message-Passing and Shared-Memory Programs? In ASPLOS-VI, </title> <booktitle> pages 6173, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Although message-passing applications can be efficient and portable, they are hard to develop and, because of the typically large setup overhead per message, they have problems in exploiting fine-grain parallelism <ref> [For94, CLR94] </ref>. On the other hand, distributed shared-memory (DSM) multiprocessors provide programmers convenience of memory sharing with one global address space, some portion of which is found in each node [PTM96]. <p> Uniprocessor applications are often easily ported to DSM multiprocessors, and their performance can then be incrementally tuned to exploit the available parallelism. Moreover, tuned shared-memory applications can be as efficient as message-passing applications <ref> [CLR94] </ref>, provided that support is available for high bandwidth block transfers. 1 Due to the increasing gap between processor speed and memory speed, DSM systems use one or more levels of caches that are often kept consistent by using one of many cache coherence protocols [CSG98, LW95]. <p> CIAT's concurrency characterization includes the serial fraction, load imbalance, and resource contention in addition to speedup. Chandra et al. also used simulation to characterize the performance of a collection of applications <ref> [CLR94] </ref>. Their main objective was to analyze where time is spent in message-passing versus 9 shared-memory programs. Perl and Sites [PS96] have studied some Windows NT applications on Alpha PCs. Their study includes analyzing the application bandwidth requirements, characterizing the memory access patterns, and analyzing application sensitivity to cache size.
Reference: [CMM + 95] <author> M. Calzarossa, L. Massari, A. Merlo, M. Pantano, and D. Tessera. Medea: </author> <title> A Tool for Workload Characterization of Parallel Systems. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <address> 3(4):7280, </address> <month> Winter </month> <year> 1995. </year>
Reference-contexts: Unlike hardware monitors and SimOS, SMAIT does require source code availability and cannot trace activity within the operating system, unless the operating system is made available for instrumentation. 1.5.2 Performance Analysis Available parallel performance analysis tools have mainly been developed for analyzing message-passing applications, e.g., Pablo [RAN + 93], Medea <ref> [CMM + 95] </ref>, and Paradyn [MCC + 95b]. There is, however, some work that focuses on characterizing shared-memory applications. Singh et al. demonstrated that it is often difficult to model the communication of parallel algorithms analytically [SRG94].
Reference: [CSG98] <author> D. Culler, J. Singh, and A. Gupta. </author> <title> Parallel Computer Architecture: A Hardware/Software Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1998. </year>
Reference-contexts: Section 1.3 specifies the main dissertation research objectives, Section 1.4 outlines our approach to accomplishing these objectives, and Section 1.5 surveys some related work. 1.1 Distributed Shared Memory Multiprocessors Distributed-memory systems are parallel processors that use high-bandwidth, low-latency interconnection networks to connect powerful processing nodes which contain processors and memory <ref> [CSG98, Hwa93] </ref>. The interconnection networks provide the communication channels through which nodes exchange data and coordinate their work in solving parallel applications. Distributed-memory systems reduce some bottlenecks that limit performance in systems with central memories. Thus, they have the potential for scaling in size and performance. <p> as efficient as message-passing applications [CLR94], provided that support is available for high bandwidth block transfers. 1 Due to the increasing gap between processor speed and memory speed, DSM systems use one or more levels of caches that are often kept consistent by using one of many cache coherence protocols <ref> [CSG98, LW95] </ref>. Various software and hardware techniques have been proposed for implementing DSM multiprocessors [PTM96]. Commercial implementations usually employ hardware techniques because of their higher performance and easier programmability. <p> Detailed surveys of distributed shared-memory concepts and systems are found in <ref> [LW95, PTM96, CSG98] </ref>. Many CC-NUMA systems, existing commercial machines as well as research prototypes, use caching to reduce the number of remote capacity misses [AB97, LC96, NAB + 95, LLG + 92, FW97, MD98]. <p> DSM systems use high-bandwidth, low-latency interconnection networks to connect powerful processing nodes that contain processors and memory. The distributed memory is shared through a global address space, thus providing a natural and convenient programming model <ref> [LW95, CSG98] </ref>. The SPP1000 was introduced in 1994 as Convex's first implementation of its Exemplar DSM architecture [Bre95]. In 1997, HP/Convex started to ship multi-node configurations of its second Exemplar generation, the SPP2000 [BA97]. <p> Additionally, the invalidation time is linear with the number of sharing nodes. Although some of these issues are addressed in research work such as [KG96], linked-list protocols fundamentally require more directory accesses than other protocols <ref> [AD98a, CFKA90, CSG98] </ref>. Since the SPP2000 directory is implemented in memory, the high directory access rate adversely affects remote memory latency and bandwidth. 79 CHAPTER 6 EVALUATION OF THREE MAJOR CC-NUMA APPROACHES USING CDAT In this chapter, we present the results of our comparative study of three scalable shared-memory systems. <p> This review discusses these issues in the context of a coherence protocol similar to the one used in the Origin 2000. The level of detail of the protocol description in this chapter is sufficient for our purposes; a more detailed description of this protocol is presented in <ref> [CSG98] </ref>. In CC-NUMA systems, processors communicate through accessing shared data. The coherence protocol ensures that when a processor accesses a shared memory location, it will get the most up-to-date data value. In order to exploit spatial locality, the coherence protocol usually transfers one complete line whenever a cache miss occurs.
Reference: [CSV + 97] <author> S. Chodnekar, V. Srinivasan, A. Vaidya, A. Sivasubramaniam, and C. Das. </author> <title> Towards a Communication Characterization Methodology for Parallel Applications. </title> <booktitle> In Proc. HPCA-3, </booktitle> <pages> pages 310319, </pages> <year> 1997. </year>
Reference-contexts: Their study includes analyzing the application bandwidth requirements, characterizing the memory access patterns, and analyzing application sensitivity to cache size. To get insight in designing interconnection networks, Chodnekar et al. analyzed the time distribution and locality of communication events in some message-passing and shared-memory applications <ref> [CSV + 97] </ref>. CIAT and TDAT characterize the time distribution of communication events as a function of time in addition to reporting the cumulative distribution function of the event rates.
Reference: [CXp93] <institution> CONVEX Computer Corp. </institution> <note> CXpa Reference Manual, second edition, </note> <month> March </month> <year> 1993. </year> <title> Order No. </title> <publisher> DSW253. </publisher>
Reference-contexts: Performance collection often involves collecting detailed information about the application's memory references. There are several classes of techniques for performance collection, each with its own advantages and limitations. Hardware monitors, e.g., the VAX microcode monitor [EC84], Convex CXpa <ref> [CXp93] </ref>, and the IBM POWER performance monitor [WCNSH94] provide low-level information using event counters, but require special hardware support and so tend to be system-specific. <p> While the results of the two approaches generally agree, there are some justifiable differences due to using two different hardware platforms and the fundamental limitations and differences of the two approaches. We have also verified CDAT's results against a report generated by Convex CXpa <ref> [CXp93] </ref>. CXpa collects performance statistics using hardware monitors on the SPP1600, which uses the PA7200 processor [CHK + 96]. For a single node configuration running a 256 fi 256 blocked matrix multiplication program, CDAT reported 14% more data misses.
Reference: [Den68] <author> P. Denning. </author> <title> Working Set Model for Program Behavior. </title> <journal> Commun. ACM, </journal> <volume> 11(6):323 333, </volume> <year> 1968. </year>
Reference-contexts: number of distinct touched instructions, a parallel execution profile (serial and parallel phases), number of synchronization barriers and locks, I/O traffic, and percentage of memory instructions (by type). * The working set of an application in an execution interval is the number of distinct memory locations accessed in this interval <ref> [Den68] </ref>. The working set often changes over time and may be hierarchical, e.g., multiple working sets may be accessed iteratively and collectively constitute a larger working set. The working set size is a measure of the application's temporal locality, which affects its cache performance.
Reference: [EC84] <author> J. Emer and D. Clark. </author> <title> A Characterization of Processor Performance in the VAX--11/780. </title> <booktitle> In Proc. 11th ISCA, </booktitle> <pages> pages 301309, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Performance collection often involves collecting detailed information about the application's memory references. There are several classes of techniques for performance collection, each with its own advantages and limitations. Hardware monitors, e.g., the VAX microcode monitor <ref> [EC84] </ref>, Convex CXpa [CXp93], and the IBM POWER performance monitor [WCNSH94] provide low-level information using event counters, but require special hardware support and so tend to be system-specific.
Reference: [EKKL90] <author> S. Eggers, D. Keppel, E. Koldinger, and H. Levy. </author> <title> Techniques for Efficient Inline Tracing on a Shared-Memory Multiprocessor. </title> <booktitle> Proc. ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 3747, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This technique often requires source code availability, perturbs the execution, and cannot be used with applications that generate code dynamically, e.g. data base systems. MPTRACE <ref> [EKKL90] </ref> uses assembly code instrumentation to collect traces of shared-memory multiprocessor applications. Simulation is also used in performance data collection, for example, Proteus [BDCW91], Tango Lite [GH92], and SimOS [RHWG95]. Similar to code instrumentation, simulation enables collecting performance data of various kinds, but is often slower.
Reference: [FBR93] <author> S. Frank, H. Burkhardt, and J. Rothnie. </author> <title> KSR1: Bridging the Gap Between Shared Memory and MPPs. </title> <booktitle> In Digest of papers, COMPCON'93, </booktitle> <pages> pages 285294, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Commercial implementations usually employ hardware techniques because of their higher performance and easier programmability. Three commercial multiprocessors using hardware techniques, ordered by increasing degree of hardware support for coherent data replication and migration, are Cray Research T3D [KS93], Convex SPP1000 [Bre95], and Kendall Square Research KSR1 <ref> [FBR93, BD94] </ref>. These three multiprocessors represent three distinct paradigms for implementing DSM systems. The T3D interconnects dual-processor processing nodes using a 3-dimensional torus. The T3D is a non-uniform memory access (NUMA) multiprocessor. A processor can access memory in remote nodes using load and store instructions that optionally update its cache.
Reference: [Fis95] <author> P. Fishwick. </author> <title> Simulation Model Design and Execution: Building Digital Worlds. </title> <publisher> Pren-tice Hall, </publisher> <year> 1995. </year>
Reference-contexts: Although CCAT uses the same front-end engine to parse the input traces, it is an event driven simulator that consumes the trace on demand according to the status of the simulated processors. CCAT has an event scheduling core that is adapted from the SimPack toolkit <ref> [Fis95] </ref>. The tools also support traces that do not contain information about thread management and synchronization, e.g., the TPC traces described in Chapter 3. The tools support analyzing T trace files taken from T different processes by interleaving these traces on p processors. <p> A system signal utilizes a facility by occupying it for a specified number of cycles. If the facility is busy, the signal is queued on the facility queue and served later in a FIFO style <ref> [Fis95] </ref>. The processor executes a specified number of instructions per cycle (IPC) when it is not stalled. The processor can have up to some specified number of outstanding misses, and thus it is able to overlap the latencies of multiple cache misses and generate high system traffic.
Reference: [For94] <author> MPI Forum. </author> <title> MPI: A Message-Passing Interface. </title> <type> Technical Report CS/E 94-013, </type> <institution> Department of Computer Science, Oregon Graduate Institute, </institution> <month> March 94. </month>
Reference-contexts: Although message-passing applications can be efficient and portable, they are hard to develop and, because of the typically large setup overhead per message, they have problems in exploiting fine-grain parallelism <ref> [For94, CLR94] </ref>. On the other hand, distributed shared-memory (DSM) multiprocessors provide programmers convenience of memory sharing with one global address space, some portion of which is found in each node [PTM96].
Reference: [FW78] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in Random Access Machines. </title> <booktitle> In Proc. 10th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 114118, </pages> <year> 1978. </year>
Reference-contexts: A multiprocessor configuration specifies the way that processors are clustered in a hierarchy, the interconnection topology, the coherence protocols, the cache configurations, and the sizes and speeds of the multiprocessor components. CIAT uses a model similar to the PRAM model <ref> [FW78] </ref> which assumes that p processors can execute p instructions concurrently and each instruction takes a fixed time. Therefore, CIAT keeps track of time in instruction units.
Reference: [FW97] <author> B. Falsafi and D. Wood. </author> <title> Reactive NUMA: A Design for Unifying S-COMA and CC-NUMA. </title> <booktitle> In Proc. 24th ISCA, </booktitle> <pages> pages 229240, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Detailed surveys of distributed shared-memory concepts and systems are found in [LW95, PTM96, CSG98]. Many CC-NUMA systems, existing commercial machines as well as research prototypes, use caching to reduce the number of remote capacity misses <ref> [AB97, LC96, NAB + 95, LLG + 92, FW97, MD98] </ref>. Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. <p> Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [AB97], NUMA-Q [LC96], S3.mp [NAB + 95], and NUMA-RC [ZT97]), or SRAMs to serve as fast interconnect caches (DASH [LLG + 92]), or both (R-NUMA <ref> [FW97] </ref> and VC-NUMA [MD98]). In Chapter 7, we evaluate the worthiness of an SRAM interconnect cache in reducing the number of remote communication misses. Unlike DASH and VC-NUMA, our implementation is compatible with the MESI cache coherence protocols supported by modern processors.
Reference: [FX] <institution> Digital FX!32 Home Page. </institution> <note> http://www.service.digital.com/fx32/. </note>
Reference-contexts: SimOS collects traces for activities within the operating system in addition to the user-space activities and does not require source code availability. Dynamic code translation can be used in performance collection, e.g., DEC FX!32 <ref> [FX] </ref> which enables executing some x86 programs on an Alpha workstation. In this technique, traces can be collected by instrumenting, during execution time, the basic blocks of an application. Our trace collection tool was developed to enable collecting traces of some Convex SPP1000 and SPP1600 applications.
Reference: [Gal96] <author> M. Galles. </author> <title> Scalable Pipelined Interconnect for Distributed Endpoint Routing: The SGI SPIDER Chip. </title> <booktitle> In HOT Interconnects IV, </booktitle> <pages> pages 141146, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: in other sharing nodes contains the caching status of the line and pointers to the previous and next nodes in the list. 6.4 SGI Origin 2000 The SGI Origin 2000 [LL97] was introduced in 1996 and connects dual-processor nodes with an interconnect that is based on the SPIDER router chip <ref> [Gal96] </ref>. The CCC has multiple paths to interconnect the processor pair, four memory banks with attached directory, the I/O interface, and the interconnect network (Figure 6.4). Each processor runs at 195 MHz and has 32-KB on-chip instruction and data caches and a 4-MB, 2-way associative, combined secondary cache.
Reference: [GGH91] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors. </title> <booktitle> In ASPLOS-IV, </booktitle> <pages> pages 245257, </pages> <year> 1991. </year>
Reference-contexts: Notice that DASH with SP spends a relatively small time in satisfying its store/S misses. This is because DASH uses the relaxed memory consistency model <ref> [GGH91] </ref> that enables it to hide most of the remote time spent to satisfy this type of miss. 6.5.2 Processor Requests and Returns Before going further, let us look closer at the processor/bus interaction in the three systems. processor write-back (w/b) signals, and supply signals.
Reference: [GGJ + 90] <author> K. Gallivan, D. Gannon, W. Jalby, A. Malony, and H. Wijshoff. </author> <title> Experimentally Characterizing the Behavior of Multiprocessor Memory Systems: A Case Study. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(2):216223, </volume> <month> February </month> <year> 1990. </year>
Reference-contexts: analyzed the TPC-C disk accesses to model its disk access patterns and showed that TPC-C can achieve close to linear speedup in a distributed system when some read-only data is replicated. 1.5.3 System Calibration Microbenchmarking has been used in many studies to characterize low-level performance of uniprocessor and multiprocessor systems <ref> [SS95, McC95a, MS96, MSSAD93, GGJ + 90, HLK97] </ref>. Saavedra and Smith have used microbenchmarking to characterize the configuration and performance of the cache and TLB subsystems in uniprocessors [SS95]. <p> His microbenchmark suite, lmbench, measures many aspects of processor and system performance. Although there are some studies that have used microbenchmarks to characterize the memory performance of shared-memory multiprocessors <ref> [GGJ + 90, SGC93, HLK97] </ref>, the microbenchmarks presented here offer wide coverage of the memory and communication performance for DSM systems. <p> Performance models are also useful for conducting quantitative comparisons among different multiprocessors and selecting the best available application implementation techniques. In this chapter, we present an experimental methodology to characterize the memory, scheduling, and synchronization performance of DSM systems. We extend the microbenchmark techniques used in previous studies <ref> [GGJ + 90, MSSAD93, SS95, McC95a, MS96] </ref> by adapting them to deal with the important attributes of a DSM system. Like previous studies, our benchmarks evaluate the cache, memory, and interconnection network performance.
Reference: [GH92] <author> S. Goldschmidt and J. Hennessy. </author> <title> The Accuracy of Trace-Driven Simulations of Multiprocessors. </title> <type> Technical Report CSL-TR-92-546, </type> <institution> Stanford University, </institution> <month> Septem-ber </month> <year> 1992. </year>
Reference-contexts: MPTRACE [EKKL90] uses assembly code instrumentation to collect traces of shared-memory multiprocessor applications. Simulation is also used in performance data collection, for example, Proteus [BDCW91], Tango Lite <ref> [GH92] </ref>, and SimOS [RHWG95]. Similar to code instrumentation, simulation enables collecting performance data of various kinds, but is often slower. SimOS collects traces for activities within the operating system in addition to the user-space activities and does not require source code availability. <p> Woo et al. have characterized several aspects of the SPLASH-2 suite of parallel applications [WOT + 95]. Their characterization includes load balance, working sets, communication to computation ratio, system traffic, and sharing. They used execution-driven simulation with the Tango Lite <ref> [GH92] </ref> tracing tool. In order to capture some of the fundamental properties of SPLASH-2, they adjusted model parameters between low and high values. In contrast, our configuration independent analysis characterizes these properties more naturely and efficiently, without resorting to a series of specific configurations.
Reference: [Hew94] <author> Hewlett-Packard. </author> <title> PA-RISC 1.1 Architecture and Instruction Set, </title> <booktitle> third edition, </booktitle> <month> Febru-ary </month> <year> 1994. </year>
Reference-contexts: Section 2.7 describes TDAT. Finally, Section 2.8 outlines our assessment of the accuracy of these tools. Further detail on these tools is reported in [Aba96]. 2.2 Trace Collection (SMAIT) SMAIT is based on RYO [ZK95], a tool developed by Zucker and Karp for instrumenting PA-RISC <ref> [Hew94] </ref> instruction sequences. RYO is a set of awk scripts that enable replacing individual machine instructions with calls to user written subroutines. SMAIT is designed to enable collecting traces of multi-threaded shared-memory parallel applications.
Reference: [HLK97] <author> C. Hristea, D. Lenoski, and J. Keen. </author> <title> Measuring Memory Hierarchy Performance of Cache-Coherent Multiprocessors Using Micro Benchmarks. </title> <booktitle> In Supercomputing, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: analyzed the TPC-C disk accesses to model its disk access patterns and showed that TPC-C can achieve close to linear speedup in a distributed system when some read-only data is replicated. 1.5.3 System Calibration Microbenchmarking has been used in many studies to characterize low-level performance of uniprocessor and multiprocessor systems <ref> [SS95, McC95a, MS96, MSSAD93, GGJ + 90, HLK97] </ref>. Saavedra and Smith have used microbenchmarking to characterize the configuration and performance of the cache and TLB subsystems in uniprocessors [SS95]. <p> His microbenchmark suite, lmbench, measures many aspects of processor and system performance. Although there are some studies that have used microbenchmarks to characterize the memory performance of shared-memory multiprocessors <ref> [GGJ + 90, SGC93, HLK97] </ref>, the microbenchmarks presented here offer wide coverage of the memory and communication performance for DSM systems. <p> As presented in Chapter 7, CCAT simulates systems similar to the SGI Origin 2000 [LL97]. CCAT's local and remote miss latencies agree with the latencies measured using microbenchmarks and reported in <ref> [HLK97] </ref>. 20 CHAPTER 3 APPLICATION CHARACTERIZATION CASE STUDIES WITH CIAT In this chapter, we present the collection of shared-memory applications that are used in this research. We characterize these applications using our configuration independent analysis approach to get a general and thorough understanding of their inherent properties. <p> The processor requests exclusive ownership of the line, and as for the store miss, the final state is modified (M). 1 A recent study reports higher Origin 2000 latencies <ref> [HLK97] </ref>. 86 Signal DASH SPP Origin From PRO to BUS 375/313 70/120 100 From BUS to CCC 250 - 20 From BUS to MEM 0/188 - From BUS to XBR - 32/128 - From XBR to CCC - 32/128 - From CCC to MEM - 0/140 20 From CCC to NET <p> Appendix A lists the CCAT configuration file of system D2, which specifies all the used system parameters, occupan cies, and latencies. The used latency and occupancy values are based on the typical processor and bus values from the R10000 user's manual [R1097], and an Origin 2000 microbenchmark evalua tion <ref> [HLK97] </ref>. Aspect Latency Processor request a 90 Processor snoop response 80 Processor data response 190 CCC 50 Memory 100 Router 40 Network link 10 a latency between detecting the miss and requesting the bus. Table 7.3: Values of the main latencies in nanoseconds.
Reference: [HNO97] <author> L. Hammond, B. Nayfeh, and K. Olukotun. </author> <title> A Single-Chip Multiprocessor. </title> <booktitle> Computer, </booktitle> <address> 30(9):7985, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: Many researchers are investigating approaches to exploit the increasing transistor budgets of a single chip. Some researchers are advocating using this budget to build a single-chip multiprocessor <ref> [HNO97] </ref>. Current research evaluates single-chip multiprocessor designs in a system that has only one processor chip. We think that this approach will get more momentum in the near future and that we should seriously consider how to incorporate multiple single-chip multiprocessors in a system.
Reference: [Hun95] <author> D. Hunt. </author> <title> Advanced Performance Features of the 64-bit PA-8000. </title> <booktitle> In Digest of papers, COMPCON'95, </booktitle> <pages> pages 123128, </pages> <month> March </month> <year> 1995. </year> <month> 147 </month>
Reference-contexts: In modern superscalar processors, the overhead of the load and use is minimized since these processors usually perform the load concurrently with the store and bypass the load's result for quick calculation of the new address. On the PA8000 <ref> [Hun95] </ref>, the load-use kernel and the store-load-use kernel each takes three cycles per iteration when all accesses hit in the cache, which is the load followed by use latency. <p> RI is the ring interface. The SPP1000 uses the Hewlett-Packard PA 7100 [AAD + 93], a two-way superscalar processor, running at 100 MHz. The SPP2000 uses the PA 8000 <ref> [Hun95] </ref>, a four-way superscalar processor, running at 180 MHz. In addition to the PA 8000's higher frequency and larger number of functional units, it supports out-of-order instruction execution, and memory latency overlapping and hiding.
Reference: [Hwa93] <author> K. Hwang. </author> <title> Advanced Computer Architecture: Parallelism, Scalability, </title> <publisher> Programma--bility. McGraw-Hill, </publisher> <year> 1993. </year>
Reference-contexts: Section 1.3 specifies the main dissertation research objectives, Section 1.4 outlines our approach to accomplishing these objectives, and Section 1.5 surveys some related work. 1.1 Distributed Shared Memory Multiprocessors Distributed-memory systems are parallel processors that use high-bandwidth, low-latency interconnection networks to connect powerful processing nodes which contain processors and memory <ref> [CSG98, Hwa93] </ref>. The interconnection networks provide the communication channels through which nodes exchange data and coordinate their work in solving parallel applications. Distributed-memory systems reduce some bottlenecks that limit performance in systems with central memories. Thus, they have the potential for scaling in size and performance. <p> Scheduling overhead may also exist within the execution when dynamic allocation is supported. The synchronization overhead is the time spent performing synchronization operations, e.g., 50 barrier synchronization, or acquiring a lock for a critical section <ref> [Hwa93] </ref>. The synchronization overhead does not include the wait time due to load imbalance, which can easily be treated separately [Tom95, Boy95]. We use simple Fortran programs to gather timing data for each of these three aspects of DSM performance.
Reference: [KCZ + 94] <author> G. Kurpanek, K. Chan, J. Zheng, E. DeLano, and W. Bryg. PA7200: </author> <title> A PA-RISC Processor with Integrated High Performance MP Bus Interface. </title> <booktitle> In Digest of papers, COMPCON'94, </booktitle> <pages> pages 375382, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Some processors support hardware prefetching where the cache controller, based on the miss history, speculates which uncached lines will be referenced in the near future and automatically generates requests to prefetch these lines. For example, the PA7200 processor <ref> [KCZ + 94] </ref> prefetches the next sequential cache line on a cache miss. Since the above two latency kernels scan the array backwards, the next (forward) sequential line is always in the cache, thus, no sequential prefetching is done.
Reference: [KG96] <author> S. Kaxiras and J. Goodman. </author> <title> The GLOW Cache Coherence Protocol Extension for Widely Shared Data. </title> <booktitle> In Proc. Int'l Conf. on Supercomputing, </booktitle> <pages> pages 3543, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Additionally, the invalidation time is linear with the number of sharing nodes. Although some of these issues are addressed in research work such as <ref> [KG96] </ref>, linked-list protocols fundamentally require more directory accesses than other protocols [AD98a, CFKA90, CSG98].
Reference: [KHW91] <author> Y. Kim, M. Hill, and D. Wood. </author> <title> Implementing Stack Simulation for Highly-Associative Memories. </title> <booktitle> In Proc. ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 212213, </pages> <year> 1991. </year>
Reference-contexts: A knee at cache size C indicates that there is a working set of size C. This is a time consuming procedure since it involves multiple simulation experiments. Although there are efficient techniques for simulating multiple cache sizes in one experiment <ref> [KHW91] </ref>, these techniques are not used with shared-memory multiprocessor simulations because of the interaction among the processors due to coherence traffic which is some function of the cache size. <p> With these two optimizations, the time for characterizing the working sets is about 30% of CIAT's total analysis time. When it is sufficient to characterize the working set with respect to a limited number of cache sizes M , the stack simulation algorithm described in <ref> [KHW91] </ref>, which has O (N fi M ) time complexity, can be used instead of our algorithm.
Reference: [KR92] <author> B. Kernighan and D. Ritchie. </author> <title> The C Programming Language. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: An assertion statement acts as a diagnostic point; it verifies that the program state at that point is as expected <ref> [KR92] </ref>. This frequent use of assertion statements helped us to expose and fix many programming bugs and algorithmic errors throughout the development process. We have also used simple synthetic traces to validate various aspects of these tools.
Reference: [KS93] <author> R. Kessler and J. Schwarzmeier. </author> <title> Cray T3D: A New Dimension for Cray Research. </title> <booktitle> In Digest of papers, COMPCON'93, </booktitle> <pages> pages 176182, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Commercial implementations usually employ hardware techniques because of their higher performance and easier programmability. Three commercial multiprocessors using hardware techniques, ordered by increasing degree of hardware support for coherent data replication and migration, are Cray Research T3D <ref> [KS93] </ref>, Convex SPP1000 [Bre95], and Kendall Square Research KSR1 [FBR93, BD94]. These three multiprocessors represent three distinct paradigms for implementing DSM systems. The T3D interconnects dual-processor processing nodes using a 3-dimensional torus. The T3D is a non-uniform memory access (NUMA) multiprocessor.
Reference: [Lam79] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Mul-tiprocess Programs. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-29(9):241248, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: The processor can have up to some specified number of outstanding misses, and thus it is able to overlap the latencies of multiple cache misses and generate high system traffic. The processor supports sequential memory consistency <ref> [Lam79] </ref> and stalls on the following events: 1. It attempts to fetch an instruction and misses in the secondary cache. The processor remains stalled until this code miss is satisfied. 2. It has a data miss and its outstanding request buffer is full. <p> The figure shows eight routerseach using five of its portsinterconnecting 16 nodes in a cube configuration. The Origin 2000 supports the sequential consistency model <ref> [Lam79] </ref> which supports a wide range of applications, but with minimal opportunities for hiding the latency of cache misses. For 84 attached directory, and cache coherence controller. <p> The processors run at 200 MHz and support the MESI cache-coherence protocol [SS86]. Each processor has a 4-MB, 2-way associative, combined secondary cache. The data cache line size is 128 bytes. The base system is sequentially consistent <ref> [Lam79] </ref>, and maintains cache coherence by using a directory-based protocol. The directory is implemented in the memory and contains a sharing vector and a status field for each 128-byte line. The directory is accessed in parallel with each memory access. As shown in Figure 7.3, two nodes share one router.
Reference: [LC96] <author> T. Lovett and R. Clapp. STiNG: </author> <title> A CC-NUMA Computer for the Commercial Marketplace. </title> <booktitle> In Proc. 23rd ISCA, </booktitle> <pages> pages 308317, </pages> <year> 1996. </year>
Reference-contexts: Several vendors are adopting the CC-NUMA architecture for building their new high-end servers. CC-NUMA achieves a nice balance between NUMA's programming complexity and COMA's hardware complexity. Examples of new CC-NUMA systems are: HP/Convex SPP2000 [BA97], Sequent 2 NUMA-Q <ref> [LC96] </ref>, and SGI Origin 2000 [LL97]. 1.2 DSM Performance and Programmability Computer architects increasingly rely on application characteristics for insight in designing cost-effective systems. This is true in the early design stages as well as later stages. In the early design stages, architects face a large and diverse design space. <p> Detailed surveys of distributed shared-memory concepts and systems are found in [LW95, PTM96, CSG98]. Many CC-NUMA systems, existing commercial machines as well as research prototypes, use caching to reduce the number of remote capacity misses <ref> [AB97, LC96, NAB + 95, LLG + 92, FW97, MD98] </ref>. Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. <p> Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [AB97], NUMA-Q <ref> [LC96] </ref>, S3.mp [NAB + 95], and NUMA-RC [ZT97]), or SRAMs to serve as fast interconnect caches (DASH [LLG + 92]), or both (R-NUMA [FW97] and VC-NUMA [MD98]). In Chapter 7, we evaluate the worthiness of an SRAM interconnect cache in reducing the number of remote communication misses. <p> CC-NUMA multiprocessors provide the convenience of memory sharing with one global address space, some portion of which is found in each node. CC-NUMA has a good balance between programming complexity and hardware complexity. Several vendors are adopting it for building new high-end servers, e.g., HP/Convex SPP2000 [BA97], Sequent NUMA-Q <ref> [LC96] </ref>, and SGI Origin 2000 [LL97].
Reference: [LD93] <author> S. Leutenegger and D. Dias. </author> <title> A Modeling Study of the TPC-C Benchmark. </title> <booktitle> In Proc. of ACM SIGMOD, </booktitle> <pages> pages 2231, </pages> <year> 1993. </year>
Reference-contexts: CIAT characterizes communication locality by characterizing the communication between each processor pair, not just characterizing the communication from one particular processor to the other processors. Thus, CIAT and TDAT present more useful characterizations for understanding and tuning shared-memory applications. Leutenegger and Dias <ref> [LD93] </ref> analyzed the TPC-C disk accesses to model its disk access patterns and showed that TPC-C can achieve close to linear speedup in a distributed system when some read-only data is replicated. 1.5.3 System Calibration Microbenchmarking has been used in many studies to characterize low-level performance of uniprocessor and multiprocessor systems
Reference: [LL97] <author> J. Laudon and D. Lenoski. </author> <title> The SGI Origin: A ccNUMA Highly Scalable Server. </title> <booktitle> In Proc. 24th ISCA, </booktitle> <pages> pages 241251, </pages> <year> 1997. </year>
Reference-contexts: Several vendors are adopting the CC-NUMA architecture for building their new high-end servers. CC-NUMA achieves a nice balance between NUMA's programming complexity and COMA's hardware complexity. Examples of new CC-NUMA systems are: HP/Convex SPP2000 [BA97], Sequent 2 NUMA-Q [LC96], and SGI Origin 2000 <ref> [LL97] </ref>. 1.2 DSM Performance and Programmability Computer architects increasingly rely on application characteristics for insight in designing cost-effective systems. This is true in the early design stages as well as later stages. In the early design stages, architects face a large and diverse design space. <p> We believe that CDAT reported more misses because it does not model the PA7200's assist cache that eliminates some of the conflict misses. As presented in Chapter 7, CCAT simulates systems similar to the SGI Origin 2000 <ref> [LL97] </ref>. CCAT's local and remote miss latencies agree with the latencies measured using microbenchmarks and reported in [HLK97]. 20 CHAPTER 3 APPLICATION CHARACTERIZATION CASE STUDIES WITH CIAT In this chapter, we present the collection of shared-memory applications that are used in this research. <p> The addition of this glue logic, however, raises the local memory and communication latencies, which penalizes sequential and small-scale parallel programs. Many modern processors are incorporating interfaces for small-scale multiprocessor buses, which can be exploited to build economic small nodes with low local latency <ref> [LL97] </ref>. However, for a system based on small nodes to succeed with large-scale parallel programs, the remote latency should not increase prohibitively. <p> CC-NUMA has a good balance between programming complexity and hardware complexity. Several vendors are adopting it for building new high-end servers, e.g., HP/Convex SPP2000 [BA97], Sequent NUMA-Q [LC96], and SGI Origin 2000 <ref> [LL97] </ref>. <p> Each ICC tag in other sharing nodes contains the caching status of the line and pointers to the previous and next nodes in the list. 6.4 SGI Origin 2000 The SGI Origin 2000 <ref> [LL97] </ref> was introduced in 1996 and connects dual-processor nodes with an interconnect that is based on the SPIDER router chip [Gal96]. The CCC has multiple paths to interconnect the processor pair, four memory banks with attached directory, the I/O interface, and the interconnect network (Figure 6.4).
Reference: [LLG + 92] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> Computer, </journal> <volume> 25:63 79, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: This figure is based on data found by simulating traces of a 256 fi 256 blocked matrix multiplication on a particular CC-NUMA system which has 4 processors per node and supports coherence protocols similar to the Stanford DASH protocols <ref> [LLG + 92] </ref>. As the number of nodes involved in solving the problem increases, the execution time decreases. 3 For any given number of nodes, as the latency of accessing data in remote nodes increases, the execution time increases. <p> To conserve space, we do not repeat this discussion here. But it is worthwhile to mention that many of the techniques and approaches used in the DSM systems that we focus on were developed in the Stanford DASH <ref> [LLG + 92] </ref>, MIT Alewife [ABC + 95], and SCI standard [SCI93] projects. Detailed surveys of distributed shared-memory concepts and systems are found in [LW95, PTM96, CSG98]. <p> Detailed surveys of distributed shared-memory concepts and systems are found in [LW95, PTM96, CSG98]. Many CC-NUMA systems, existing commercial machines as well as research prototypes, use caching to reduce the number of remote capacity misses <ref> [AB97, LC96, NAB + 95, LLG + 92, FW97, MD98] </ref>. Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. <p> Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [AB97], NUMA-Q [LC96], S3.mp [NAB + 95], and NUMA-RC [ZT97]), or SRAMs to serve as fast interconnect caches (DASH <ref> [LLG + 92] </ref>), or both (R-NUMA [FW97] and VC-NUMA [MD98]). In Chapter 7, we evaluate the worthiness of an SRAM interconnect cache in reducing the number of remote communication misses. Unlike DASH and VC-NUMA, our implementation is compatible with the MESI cache coherence protocols supported by modern processors. <p> Some conclusions are drawn in Section 6.7. 6.2 Stanford DASH The Stanford DASH project started in the fall of 1988, and the first prototype was completed in the spring of 1991 <ref> [LLG + 92] </ref>. The DASH is based on the SGI POWER Station 4D/340 which has 4 processor boards connected by a cache-coherent bus. As shown in Figure 6.1, the memory and the I/O interface are also connected to the shared bus. <p> The best approach to address their cost is to decrease the directory access and remote latencies. However, weaker consistency models address WAR by allowing the requester to perform its store without waiting for the acknowledgment signals <ref> [LLG + 92] </ref>. The main remote communication patterns with remote lines are: RAW: Load miss for a dirty line. The local coherence controller generates a request signal to the home node. The directory in the home node is then checked, which points to the dirty node.
Reference: [LW95] <author> D. Lenoski and W-D. Weber. </author> <title> Scalable Shared-Memory Multiprocessing. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: as efficient as message-passing applications [CLR94], provided that support is available for high bandwidth block transfers. 1 Due to the increasing gap between processor speed and memory speed, DSM systems use one or more levels of caches that are often kept consistent by using one of many cache coherence protocols <ref> [CSG98, LW95] </ref>. Various software and hardware techniques have been proposed for implementing DSM multiprocessors [PTM96]. Commercial implementations usually employ hardware techniques because of their higher performance and easier programmability. <p> Detailed surveys of distributed shared-memory concepts and systems are found in <ref> [LW95, PTM96, CSG98] </ref>. Many CC-NUMA systems, existing commercial machines as well as research prototypes, use caching to reduce the number of remote capacity misses [AB97, LC96, NAB + 95, LLG + 92, FW97, MD98]. <p> DSM systems use high-bandwidth, low-latency interconnection networks to connect powerful processing nodes that contain processors and memory. The distributed memory is shared through a global address space, thus providing a natural and convenient programming model <ref> [LW95, CSG98] </ref>. The SPP1000 was introduced in 1994 as Convex's first implementation of its Exemplar DSM architecture [Bre95]. In 1997, HP/Convex started to ship multi-node configurations of its second Exemplar generation, the SPP2000 [BA97]. <p> CC-NUMA systems use high bandwidth, low latency interconnection networks to connect powerful processing nodes that contain processors and memory <ref> [LW95] </ref>. Each node has a cache-coherence controller that enables data replication coherently, e.g., when a cache line is updated, the CCC invalidates other copies, and ensures that a processor request always gets a copy of the most recent data. <p> Nevertheless, based on analyzing the available information and some system calibration experiments via microbenchmarking, we believe that the latencies used represent good approximations of the respective systems. Parameterizing the configuration file of the DASH was the easiest because there are ample detailed DASH publications, e.g. <ref> [LW95] </ref>. Parameterizing the Origin 2000 configuration file is based on the information available from SGI home page [O2K96, R1097]. As there is little publicly-available information about the SPP1000 latencies from Convex, we used the results of our calibration experiments [AD98b]. Table 6.1 summarizes the latencies used.
Reference: [McC95a] <author> J. McCalpin. </author> <title> Memory Bandwidth and Machine Balance in Current High Performance Computers. </title> <journal> IEEE Technical Committee on Computer Architecture Newsletter, </journal> <month> December </month> <year> 1995. </year>
Reference-contexts: analyzed the TPC-C disk accesses to model its disk access patterns and showed that TPC-C can achieve close to linear speedup in a distributed system when some read-only data is replicated. 1.5.3 System Calibration Microbenchmarking has been used in many studies to characterize low-level performance of uniprocessor and multiprocessor systems <ref> [SS95, McC95a, MS96, MSSAD93, GGJ + 90, HLK97] </ref>. Saavedra and Smith have used microbenchmarking to characterize the configuration and performance of the cache and TLB subsystems in uniprocessors [SS95]. <p> Using his STREAM microbenchmarks, McCalpin measured the memory bandwidth of many high-performance systems and noticed that the ratio of CPU speed to memory speed is growing rapidly <ref> [McC95a] </ref>. McVoy has observed that special care and attention must be given to designing microbenchmarks that accurately measure the memory latency and bandwidth of modern processors which allow multiple outstanding misses [MS96]. His microbenchmark suite, lmbench, measures many aspects of processor and system performance. <p> Performance models are also useful for conducting quantitative comparisons among different multiprocessors and selecting the best available application implementation techniques. In this chapter, we present an experimental methodology to characterize the memory, scheduling, and synchronization performance of DSM systems. We extend the microbenchmark techniques used in previous studies <ref> [GGJ + 90, MSSAD93, SS95, McC95a, MS96] </ref> by adapting them to deal with the important attributes of a DSM system. Like previous studies, our benchmarks evaluate the cache, memory, and interconnection network performance.
Reference: [MCC + 95b] <author> B. Miller, M. Callaghan, J. Cargille, J. Hollingsworth, R. Irvin, K. Karavanic, K. Kun-chithapadam, and T. Newhall. </author> <title> The Paradyn Parallel Performance Measurement Tool. </title> <booktitle> Computer, </booktitle> <address> 28(11):3746, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: SMAIT does require source code availability and cannot trace activity within the operating system, unless the operating system is made available for instrumentation. 1.5.2 Performance Analysis Available parallel performance analysis tools have mainly been developed for analyzing message-passing applications, e.g., Pablo [RAN + 93], Medea [CMM + 95], and Paradyn <ref> [MCC + 95b] </ref>. There is, however, some work that focuses on characterizing shared-memory applications. Singh et al. demonstrated that it is often difficult to model the communication of parallel algorithms analytically [SRG94]. They suggested developing general-purpose simulation tools to obtain empirical information for supporting the design of parallel algorithms.
Reference: [MD98] <author> A. Moga and M. Dubois. </author> <title> The Effectiveness of SRAM Network Caches in Clustered DSMs. </title> <booktitle> In Proc. </booktitle> <address> HPCA-4, </address> <month> February </month> <year> 1998. </year>
Reference-contexts: Detailed surveys of distributed shared-memory concepts and systems are found in [LW95, PTM96, CSG98]. Many CC-NUMA systems, existing commercial machines as well as research prototypes, use caching to reduce the number of remote capacity misses <ref> [AB97, LC96, NAB + 95, LLG + 92, FW97, MD98] </ref>. Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. <p> Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [AB97], NUMA-Q [LC96], S3.mp [NAB + 95], and NUMA-RC [ZT97]), or SRAMs to serve as fast interconnect caches (DASH [LLG + 92]), or both (R-NUMA [FW97] and VC-NUMA <ref> [MD98] </ref>). In Chapter 7, we evaluate the worthiness of an SRAM interconnect cache in reducing the number of remote communication misses. Unlike DASH and VC-NUMA, our implementation is compatible with the MESI cache coherence protocols supported by modern processors.
Reference: [MS96] <author> L. McVoy and C. Staelin. lmbench: </author> <title> Portable Tools for Performance Analysis. </title> <booktitle> In Proc. USENIX'96 Ann. Technical Conf., </booktitle> <pages> pages 279294, </pages> <month> January </month> <year> 1996. </year> <month> 148 </month>
Reference-contexts: analyzed the TPC-C disk accesses to model its disk access patterns and showed that TPC-C can achieve close to linear speedup in a distributed system when some read-only data is replicated. 1.5.3 System Calibration Microbenchmarking has been used in many studies to characterize low-level performance of uniprocessor and multiprocessor systems <ref> [SS95, McC95a, MS96, MSSAD93, GGJ + 90, HLK97] </ref>. Saavedra and Smith have used microbenchmarking to characterize the configuration and performance of the cache and TLB subsystems in uniprocessors [SS95]. <p> McVoy has observed that special care and attention must be given to designing microbenchmarks that accurately measure the memory latency and bandwidth of modern processors which allow multiple outstanding misses <ref> [MS96] </ref>. His microbenchmark suite, lmbench, measures many aspects of processor and system performance. <p> Performance models are also useful for conducting quantitative comparisons among different multiprocessors and selecting the best available application implementation techniques. In this chapter, we present an experimental methodology to characterize the memory, scheduling, and synchronization performance of DSM systems. We extend the microbenchmark techniques used in previous studies <ref> [GGJ + 90, MSSAD93, SS95, McC95a, MS96] </ref> by adapting them to deal with the important attributes of a DSM system. Like previous studies, our benchmarks evaluate the cache, memory, and interconnection network performance.
Reference: [MSSAD93] <author> W. Mangione-Smith, T-P. Shih, S. Abraham, and E. Davidson. </author> <title> Approaching a Machine-Application Bound in Delivered Performance on Scientific Code. </title> <booktitle> IEEE Proc., </booktitle> <address> 81(8):11661178, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: analyzed the TPC-C disk accesses to model its disk access patterns and showed that TPC-C can achieve close to linear speedup in a distributed system when some read-only data is replicated. 1.5.3 System Calibration Microbenchmarking has been used in many studies to characterize low-level performance of uniprocessor and multiprocessor systems <ref> [SS95, McC95a, MS96, MSSAD93, GGJ + 90, HLK97] </ref>. Saavedra and Smith have used microbenchmarking to characterize the configuration and performance of the cache and TLB subsystems in uniprocessors [SS95]. <p> Performance models are also useful for conducting quantitative comparisons among different multiprocessors and selecting the best available application implementation techniques. In this chapter, we present an experimental methodology to characterize the memory, scheduling, and synchronization performance of DSM systems. We extend the microbenchmark techniques used in previous studies <ref> [GGJ + 90, MSSAD93, SS95, McC95a, MS96] </ref> by adapting them to deal with the important attributes of a DSM system. Like previous studies, our benchmarks evaluate the cache, memory, and interconnection network performance.
Reference: [NAB + 95] <author> A. Nowatzyk, G. Aybay, M. Browne, E. Kelly, M. Parkin, B. Radke, and S. Vishin. </author> <title> The S3.mp Scalable Shared Memory Multiprocessor. </title> <booktitle> In Proc. ICPP, </booktitle> <pages> pages I.1I.10, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Detailed surveys of distributed shared-memory concepts and systems are found in [LW95, PTM96, CSG98]. Many CC-NUMA systems, existing commercial machines as well as research prototypes, use caching to reduce the number of remote capacity misses <ref> [AB97, LC96, NAB + 95, LLG + 92, FW97, MD98] </ref>. Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. <p> Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [AB97], NUMA-Q [LC96], S3.mp <ref> [NAB + 95] </ref>, and NUMA-RC [ZT97]), or SRAMs to serve as fast interconnect caches (DASH [LLG + 92]), or both (R-NUMA [FW97] and VC-NUMA [MD98]). In Chapter 7, we evaluate the worthiness of an SRAM interconnect cache in reducing the number of remote communication misses.
Reference: [O2K96] <author> SGI. </author> <title> Performance Tuning for the Origin2000 and Onyx2, </title> <note> 1996. http://techpubs.sgi.com/library/. </note>
Reference-contexts: Parameterizing the configuration file of the DASH was the easiest because there are ample detailed DASH publications, e.g. [LW95]. Parameterizing the Origin 2000 configuration file is based on the information available from SGI home page <ref> [O2K96, R1097] </ref>. As there is little publicly-available information about the SPP1000 latencies from Convex, we used the results of our calibration experiments [AD98b]. Table 6.1 summarizes the latencies used. The CDAT configuration files used in this study are listed in Appendix A.
Reference: [PP84] <author> M. Papamarcos and J. Patel. </author> <title> A Low Overhead Coherence Solution for Multiprocessors with Private Cache Memories. </title> <booktitle> In Proc. 11th ISCA, </booktitle> <pages> pages 348354, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: The cache line size is 16 bytes. The shared bus operates at 16 MHz and supports the Illinois MESI cache-coherence protocol <ref> [PP84] </ref>, where the highest priority processor with a valid copy supplies data on bus requests. This is different from the MESI protocols supported by modern processors where a processor only supplies data when it has a modified copy [SS86].
Reference: [PS96] <author> S. Perl and R. </author> <title> Sites. Studies of Windows NT Performance Using Dynamic Execution Traces. </title> <booktitle> In Proc. of the USENIX 2nd Symp. on Operating Systems Design and Implementation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Chandra et al. also used simulation to characterize the performance of a collection of applications [CLR94]. Their main objective was to analyze where time is spent in message-passing versus 9 shared-memory programs. Perl and Sites <ref> [PS96] </ref> have studied some Windows NT applications on Alpha PCs. Their study includes analyzing the application bandwidth requirements, characterizing the memory access patterns, and analyzing application sensitivity to cache size.
Reference: [PTM96] <author> J. Protic, M. Tomasevic, and V. Milutinovic. </author> <title> Distributed Shared Memory: </title> <booktitle> Concepts and Systems. IEEE Parallel and Distributed Technology, </booktitle> <pages> pages 6379, </pages> <month> Summer </month> <year> 1996. </year>
Reference-contexts: On the other hand, distributed shared-memory (DSM) multiprocessors provide programmers convenience of memory sharing with one global address space, some portion of which is found in each node <ref> [PTM96] </ref>. Uniprocessor applications are often easily ported to DSM multiprocessors, and their performance can then be incrementally tuned to exploit the available parallelism. <p> Various software and hardware techniques have been proposed for implementing DSM multiprocessors <ref> [PTM96] </ref>. Commercial implementations usually employ hardware techniques because of their higher performance and easier programmability. Three commercial multiprocessors using hardware techniques, ordered by increasing degree of hardware support for coherent data replication and migration, are Cray Research T3D [KS93], Convex SPP1000 [Bre95], and Kendall Square Research KSR1 [FBR93, BD94]. <p> Detailed surveys of distributed shared-memory concepts and systems are found in <ref> [LW95, PTM96, CSG98] </ref>. Many CC-NUMA systems, existing commercial machines as well as research prototypes, use caching to reduce the number of remote capacity misses [AB97, LC96, NAB + 95, LLG + 92, FW97, MD98].
Reference: [R1097] <institution> MIPS Technologies Inc. </institution> <note> MIPS R10000 Microprocessor User's Manual, January 1997. Version 2.0. </note>
Reference-contexts: Parameterizing the configuration file of the DASH was the easiest because there are ample detailed DASH publications, e.g. [LW95]. Parameterizing the Origin 2000 configuration file is based on the information available from SGI home page <ref> [O2K96, R1097] </ref>. As there is little publicly-available information about the SPP1000 latencies from Convex, we used the results of our calibration experiments [AD98b]. Table 6.1 summarizes the latencies used. The CDAT configuration files used in this study are listed in Appendix A. <p> The figure shows eight routerseach utilizing five of its portsinterconnecting 16 nodes in a cube configuration. The processor pair share one split-transaction bus to communicate with the rest of the system. This bus has two modes of operation, snoopy and point-to-point <ref> [R1097] </ref>. In the snoopy mode, each processor observes the bus requests of other processors and checks its cache to ensure cache coherence. <p> Appendix A lists the CCAT configuration file of system D2, which specifies all the used system parameters, occupan cies, and latencies. The used latency and occupancy values are based on the typical processor and bus values from the R10000 user's manual <ref> [R1097] </ref>, and an Origin 2000 microbenchmark evalua tion [HLK97]. Aspect Latency Processor request a 90 Processor snoop response 80 Processor data response 190 CCC 50 Memory 100 Router 40 Network link 10 a latency between detecting the miss and requesting the bus.
Reference: [RAN + 93] <author> D. Reed, R. Aydt, R. Noe, P. Roth, K. Shields, B. Schwartz, and L. Tavera. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proc. Scalable Parallel Libraries Conf., </booktitle> <pages> pages 104113. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year>
Reference-contexts: Unlike hardware monitors and SimOS, SMAIT does require source code availability and cannot trace activity within the operating system, unless the operating system is made available for instrumentation. 1.5.2 Performance Analysis Available parallel performance analysis tools have mainly been developed for analyzing message-passing applications, e.g., Pablo <ref> [RAN + 93] </ref>, Medea [CMM + 95], and Paradyn [MCC + 95b]. There is, however, some work that focuses on characterizing shared-memory applications. Singh et al. demonstrated that it is often difficult to model the communication of parallel algorithms analytically [SRG94].
Reference: [RHWG95] <author> M. Rosenblum, S. Herrod, E. Witchel, and A. Gupta. </author> <title> Complete Computer Simulation: The SimOS Approach. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <month> Fall </month> <year> 1995. </year>
Reference-contexts: MPTRACE [EKKL90] uses assembly code instrumentation to collect traces of shared-memory multiprocessor applications. Simulation is also used in performance data collection, for example, Proteus [BDCW91], Tango Lite [GH92], and SimOS <ref> [RHWG95] </ref>. Similar to code instrumentation, simulation enables collecting performance data of various kinds, but is often slower. SimOS collects traces for activities within the operating system in addition to the user-space activities and does not require source code availability.
Reference: [RSG93] <author> E. Rothberg, J. Singh, and A. Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors. </title> <booktitle> In Proc. 20th ISCA, </booktitle> <pages> pages 1425, </pages> <year> 1993. </year>
Reference-contexts: A judicious combination of our configuration independent and configuration dependent analyses constitutes a comprehensive and efficient methodology. A sharper contrast between our approach and other related work is given in Chapter 3. There are several studies that combine source-code analysis with configuration dependent analysis to characterize shared-memory applications <ref> [SWG92, RSG93, WOT + 95] </ref>. Woo et al. have characterized several aspects of the SPLASH-2 suite of parallel applications [WOT + 95]. Their characterization includes load balance, working sets, communication to computation ratio, system traffic, and sharing. They used execution-driven simulation with the Tango Lite [GH92] tracing tool. <p> number of times, e.g. saving the previous state at the procedure entry for a procedure that is called a fixed number of times. 3.4.2 Working Sets The size of a shared-memory application's working set is sometimes characterized by conducting multiple simulation experiments using a fully-associative cache with LRU replacement policy <ref> [RSG93, WOT + 95] </ref>. Each experiment uses one cache size and measures the cache miss ratio. 29 A graph of the cache miss ratio versus cache size is used to deduce the working set sizes from the graph knees.
Reference: [SA95] <author> K. Shaw and G. Astfalk. </author> <title> Four-State Cache-Coherence in the Convex Exemplar System. Internal memo, </title> <institution> Convex Computer Corp., </institution> <month> October </month> <year> 1995. </year> <note> http://www.hp.com/wsg/tech/technical.html. </note>
Reference-contexts: The HP 9000 Model C180-XP runs a PA 8000 at 180 MHz with 1-MB data and instruction caches. 62 rings. Four 16-processor SPP2000 nodes are connected in two dimensions using 32 rings. The SPP1000 uses a three-state protocol, and the SPP2000 uses a four-state protocol <ref> [SA95] </ref>. Each memory line has an associated coherence tag to keep track of which processors have cached copies of this line. In the SPP1000, each memory controller has a ring interface that connects it to one ring. Thus, a multi-node SPP1000 system has 4 rings.
Reference: [SB95] <author> S. Saini and D. Bailey. </author> <title> NAS Parallel Benchmark Results 1295. </title> <type> Technical Report NAS95021, </type> <institution> NASA Ames Research Center, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: The NPB are specified algorithmically so that computer vendors can implement them on a wide range of parallel machines. We analyzed the Convex Exemplar [Bre95] implementation of NPB, which is written in Fortran. The performance of an earlier version of this implementation is reported in <ref> [SB95] </ref>. However, to get a general characterization of these benchmarks, we undid some of the Exemplar-specific optimizations, e.g., we removed the extra code that localizes shared data into the node-private memory of the 8-processor nodes. The six scientific benchmarks were instrumented, compiled, and analyzed on a 4-node Exemplar SPP1600 multiprocessor.
Reference: [SCI93] <institution> IEEE Computer Society. IEEE Standard for Scalable Coherent Interface (SCI), </institution> <month> August </month> <year> 1993. </year> <note> IEEE Std 1596-1992. </note>
Reference-contexts: To conserve space, we do not repeat this discussion here. But it is worthwhile to mention that many of the techniques and approaches used in the DSM systems that we focus on were developed in the Stanford DASH [LLG + 92], MIT Alewife [ABC + 95], and SCI standard <ref> [SCI93] </ref> projects. Detailed surveys of distributed shared-memory concepts and systems are found in [LW95, PTM96, CSG98]. Many CC-NUMA systems, existing commercial machines as well as research prototypes, use caching to reduce the number of remote capacity misses [AB97, LC96, NAB + 95, LLG + 92, FW97, MD98]. <p> The largest SPP2000 configuration is 4 by 8, comprised of 32 nodes interconnected with 96 rings. The two-dimensional topology provides lower latency and higher bisection bandwidth. The two systems use variants of the IEEE Scalable Coherent Interface <ref> [SCI93] </ref> to achieve intern-ode cache coherence. The SCI protocol uses a distributed doubly-linked list to keep track of which nodes share each memory line. Each node that has a copy of a memory line, maintains pointers to the next forward and next backward nodes in that line's sharing list. <p> Thus, the measured far WAR (or far RAW) latency is the average of four cases according to the location of the home node. The way that the internode coherence protocol satisfies a cache miss does in fact depend on the home node of the missed line <ref> [ABP94, SCI93] </ref>; the three main cases are: Local home: the line's home node is the local node. Remote home: the line's home node is the remote node. <p> Each ring is a pair of unidirectional links with a peak bandwidth of 600 MB/s for each link. The rings support the Scalable Coherent Interface (SCI) standard <ref> [SCI93] </ref>; which coherently transfer one 64-byte line in response to a global shared memory access. The coherence data is maintained in tags associated with the 64-byte memory lines. Each tag holds the local and global sharing state of the respective line.
Reference: [SE94] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A System for Building Customized Program Analysis Tools. </title> <type> Technical Report 94/2, </type> <institution> DEC WRL, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: ATUM [ASH86] generates a compressed trace file for post analysis, but is also based on hardware support since it enables collecting address traces by modifying the microcode. Code instrumentation, e.g., ATOM <ref> [SE94] </ref>, RYO [ZK95], and Pixie [Smi91], enables collecting various performance data and traces by instrumenting either the assembly or the object file of a uniprocessor application. This technique often requires source code availability, perturbs the execution, and cannot be used with applications that generate code dynamically, e.g. data base systems.
Reference: [SGC93] <author> R. Saavedra, R. Gaines, and M. Carlton. </author> <title> Micro Benchmark Analysis of the KSR1. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 202213, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: His microbenchmark suite, lmbench, measures many aspects of processor and system performance. Although there are some studies that have used microbenchmarks to characterize the memory performance of shared-memory multiprocessors <ref> [GGJ + 90, SGC93, HLK97] </ref>, the microbenchmarks presented here offer wide coverage of the memory and communication performance for DSM systems.
Reference: [Smi91] <author> M. Smith. </author> <title> Tracing with Pixie. </title> <note> ftp document, </note> <institution> Center for Integrated Systems, Stanford University, </institution> <month> April </month> <year> 1991. </year> <month> 149 </month>
Reference-contexts: ATUM [ASH86] generates a compressed trace file for post analysis, but is also based on hardware support since it enables collecting address traces by modifying the microcode. Code instrumentation, e.g., ATOM [SE94], RYO [ZK95], and Pixie <ref> [Smi91] </ref>, enables collecting various performance data and traces by instrumenting either the assembly or the object file of a uniprocessor application. This technique often requires source code availability, perturbs the execution, and cannot be used with applications that generate code dynamically, e.g. data base systems.
Reference: [SPE] <editor> SPEC CPU95 Benchmarks Results. </editor> <title> See the Standard Performance Evaluation Corp., </title> <note> web page http://www.spec.org/. </note>
Reference-contexts: On HP workstations that use these processors, 1 the PA 8000 achieves 4.7x floating-point performance and 3.6x integer performance on SPEC95, relative to the PA 7100 <ref> [SPE] </ref>. Each processor has two off-chip data and instruction caches that are virtually addressed. The data caches within one node are kept coherent using directory-based cache coherence protocols. 1 The HP 9000 Model 735/99 runs a PA 7100 at 99 MHz with 256-KB data and instruction caches.
Reference: [SRG94] <author> J. Singh, E. Rothberg, and A. Gupta. </author> <title> Modeling Communication in Parallel Algorithms: A Fruitful Interaction between Theory and Systems? In Proc. </title> <booktitle> Symp. Parallel Algorithms and Architectures, </booktitle> <pages> pages 189199, </pages> <year> 1994. </year>
Reference-contexts: There is, however, some work that focuses on characterizing shared-memory applications. Singh et al. demonstrated that it is often difficult to model the communication of parallel algorithms analytically <ref> [SRG94] </ref>. They suggested developing general-purpose simulation tools to obtain empirical information for supporting the design of parallel algorithms. The tools developed in this research address the shortage of tools for analyzing shared-memory applications, and the limited scope of the tools that do exist. <p> In a machine with high synchronization overheads, the contention time can become more significant than what is reported by CIAT. 34 3.4.4 Communication Patterns In configuration dependent analysis, communication is characterized from the traffic that a processor generates to access data that is not allocated in its local memory <ref> [SRG94] </ref>. This traffic includes traffic due to inherent coherence communication, cold-start misses, finite cache capacity, limited cache associativity, and false sharing.
Reference: [SS86] <author> P. Sweazy and A. Smith. </author> <title> A Class of Compatible Cache Consistency Protocols and Their Support by the IEEE Futurebus. </title> <booktitle> In Proc. 13th ISCA, </booktitle> <pages> pages 414423, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: This is different from the MESI protocols supported by modern processors where a processor only supplies data when it has a modified copy <ref> [SS86] </ref>. The system interconnection is through two 2-D meshes: one for requests and another for replies. This dual arrangement simplifies protocol deadlock elimination. The network links are 16 bits wide with a maximum transfer rate of 60 MB/s per link. <p> As shown in Figure 7.2, each node has a cache coherence controller. The CCC has multiple internal paths to interconnect the processor pair, four memory banks (with attached directory), the I/O devices, and the interconnection network. The processors run at 200 MHz and support the MESI cache-coherence protocol <ref> [SS86] </ref>. Each processor has a 4-MB, 2-way associative, combined secondary cache. The data cache line size is 128 bytes. The base system is sequentially consistent [Lam79], and maintains cache coherence by using a directory-based protocol.
Reference: [SS95] <author> R. Saavedra and A. Smith. </author> <title> Measuring Cache and TLB Performance and Their Effect on Benchmark Runtimes. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 44(10):12231235, </volume> <month> October </month> <year> 1995. </year>
Reference-contexts: analyzed the TPC-C disk accesses to model its disk access patterns and showed that TPC-C can achieve close to linear speedup in a distributed system when some read-only data is replicated. 1.5.3 System Calibration Microbenchmarking has been used in many studies to characterize low-level performance of uniprocessor and multiprocessor systems <ref> [SS95, McC95a, MS96, MSSAD93, GGJ + 90, HLK97] </ref>. Saavedra and Smith have used microbenchmarking to characterize the configuration and performance of the cache and TLB subsystems in uniprocessors [SS95]. <p> Saavedra and Smith have used microbenchmarking to characterize the configuration and performance of the cache and TLB subsystems in uniprocessors <ref> [SS95] </ref>. They have shown that mi-crobenchmark characterization can be used to parameterize performance models that predict the performance of simple applications to within 10% of the actual run times. <p> Performance models are also useful for conducting quantitative comparisons among different multiprocessors and selecting the best available application implementation techniques. In this chapter, we present an experimental methodology to characterize the memory, scheduling, and synchronization performance of DSM systems. We extend the microbenchmark techniques used in previous studies <ref> [GGJ + 90, MSSAD93, SS95, McC95a, MS96] </ref> by adapting them to deal with the important attributes of a DSM system. Like previous studies, our benchmarks evaluate the cache, memory, and interconnection network performance.
Reference: [SWG92] <author> J. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. Computer Architecture News, </title> <address> 20(1):544, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: A judicious combination of our configuration independent and configuration dependent analyses constitutes a comprehensive and efficient methodology. A sharper contrast between our approach and other related work is given in Chapter 3. There are several studies that combine source-code analysis with configuration dependent analysis to characterize shared-memory applications <ref> [SWG92, RSG93, WOT + 95] </ref>. Woo et al. have characterized several aspects of the SPLASH-2 suite of parallel applications [WOT + 95]. Their characterization includes load balance, working sets, communication to computation ratio, system traffic, and sharing. They used execution-driven simulation with the Tango Lite [GH92] tracing tool.
Reference: [Tom95] <author> K. Tomko. </author> <title> Domain Decomposition, Irregular Application, and Parallel Computers. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1995. </year>
Reference-contexts: The synchronization overhead is the time spent performing synchronization operations, e.g., 50 barrier synchronization, or acquiring a lock for a critical section [Hwa93]. The synchronization overhead does not include the wait time due to load imbalance, which can easily be treated separately <ref> [Tom95, Boy95] </ref>. We use simple Fortran programs to gather timing data for each of these three aspects of DSM performance. Each program calls and times a kernel 100 times and, after subtracting timer overheads, reports the minimum, average, and standard deviation of its call times.
Reference: [TPC] <institution> Transaction Processing Performance Council Home Page. </institution> <note> http://www.tpc.org/. </note>
Reference-contexts: Compared with other queries, although Query 3 takes a moderate run time, it has a high disk I/O and communication rates <ref> [TPC] </ref>. However, other queries can be characterized similarly. Table 3.1 shows the problem sizes analyzed in this study. The scientific benchmarks were analyzed using two problem sizes on a range of Processors from 1 to 32.
Reference: [TPC92] <author> Transaction Processing Performance Council. </author> <title> TPC Benchmark C, Standard Specification, </title> <month> August </month> <year> 1992. </year>
Reference-contexts: access time, and mapping shared data to the node where it is most referenced generally reduces the average shared access time. 3.3 Applications We have analyzed Radix, FFT, LU, and Cholesky from SPLASH-2 [WOT + 95], CG and SP from NPB [B + 94], and TPC benchmarks C and D <ref> [TPC92, TPC95] </ref>. SPLASH-2 consists of 8 applications and 4 computational kernels drawn from scientific, engineering, and graphics computing.
Reference: [TPC95] <author> Transaction Processing Performance Council. </author> <title> TPC Benchmark D, Decision Support, Standard Specification, </title> <month> May </month> <year> 1995. </year>
Reference-contexts: access time, and mapping shared data to the node where it is most referenced generally reduces the average shared access time. 3.3 Applications We have analyzed Radix, FFT, LU, and Cholesky from SPLASH-2 [WOT + 95], CG and SP from NPB [B + 94], and TPC benchmarks C and D <ref> [TPC92, TPC95] </ref>. SPLASH-2 consists of 8 applications and 4 computational kernels drawn from scientific, engineering, and graphics computing.
Reference: [WCNSH94] <author> E. Welbon, C. Cha-Nui, D. Shippy, and D. Hicks. </author> <title> The POWER2 Performance Monitor. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(5):545554, </volume> <month> September </month> <year> 1994. </year>
Reference-contexts: Performance collection often involves collecting detailed information about the application's memory references. There are several classes of techniques for performance collection, each with its own advantages and limitations. Hardware monitors, e.g., the VAX microcode monitor [EC84], Convex CXpa [CXp93], and the IBM POWER performance monitor <ref> [WCNSH94] </ref> provide low-level information using event counters, but require special hardware support and so tend to be system-specific. ATUM [ASH86] generates a compressed trace file for post analysis, but is also based on hardware support since it enables collecting address traces by modifying the microcode.
Reference: [Wol96] <author> M. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: This characterization is also useful to programmers; for example, when the working set size is larger than the cache size, the programmer can improve the application performance by reducing its working set, e.g., by segmenting a matrix computation into blocks <ref> [Wol96] </ref>. * The amount of concurrency available in an application influences how well the application performance scales as more processors are used. An application with high concurrency has the potential to efficiently utilize a large number of processors. Section 3.4.3 discusses factors that affect the concurrency of shared-memory applications.
Reference: [WOT + 95] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodology Considerations. </title> <booktitle> In Proc. 22nd ISCA, </booktitle> <pages> pages 24 36, </pages> <year> 1995. </year>
Reference-contexts: A judicious combination of our configuration independent and configuration dependent analyses constitutes a comprehensive and efficient methodology. A sharper contrast between our approach and other related work is given in Chapter 3. There are several studies that combine source-code analysis with configuration dependent analysis to characterize shared-memory applications <ref> [SWG92, RSG93, WOT + 95] </ref>. Woo et al. have characterized several aspects of the SPLASH-2 suite of parallel applications [WOT + 95]. Their characterization includes load balance, working sets, communication to computation ratio, system traffic, and sharing. They used execution-driven simulation with the Tango Lite [GH92] tracing tool. <p> There are several studies that combine source-code analysis with configuration dependent analysis to characterize shared-memory applications [SWG92, RSG93, WOT + 95]. Woo et al. have characterized several aspects of the SPLASH-2 suite of parallel applications <ref> [WOT + 95] </ref>. Their characterization includes load balance, working sets, communication to computation ratio, system traffic, and sharing. They used execution-driven simulation with the Tango Lite [GH92] tracing tool. In order to capture some of the fundamental properties of SPLASH-2, they adjusted model parameters between low and high values. <p> In particular, the CCAT detailed trace was very useful in verifying that CCAT correctly simulates the target cache coherence protocols. CIAT analysis of the SPLASH2 suite of benchmarks (presented in Chapter 3) was verified against the configuration dependent analysis presented in <ref> [WOT + 95] </ref>. While the results of the two approaches generally agree, there are some justifiable differences due to using two different hardware platforms and the fundamental limitations and differences of the two approaches. We have also verified CDAT's results against a report generated by Convex CXpa [CXp93]. <p> For example, mapping private data to local memory reduces the private access time, and mapping shared data to the node where it is most referenced generally reduces the average shared access time. 3.3 Applications We have analyzed Radix, FFT, LU, and Cholesky from SPLASH-2 <ref> [WOT + 95] </ref>, CG and SP from NPB [B + 94], and TPC benchmarks C and D [TPC92, TPC95]. SPLASH-2 consists of 8 applications and 4 computational kernels drawn from scientific, engineering, and graphics computing. <p> number of times, e.g. saving the previous state at the procedure entry for a procedure that is called a fixed number of times. 3.4.2 Working Sets The size of a shared-memory application's working set is sometimes characterized by conducting multiple simulation experiments using a fully-associative cache with LRU replacement policy <ref> [RSG93, WOT + 95] </ref>. Each experiment uses one cache size and measures the cache miss ratio. 29 A graph of the cache miss ratio versus cache size is used to deduce the working set sizes from the graph knees. <p> When there is no contention, it takes 460 nsec to satisfy a processor miss from the local memory, 106 and 690 nsec from a remote memory through one router. 7.4.2 Applications We used six applications to evaluate the four systems: Radix, FFT, LU, and Cholesky from SPLASH-2 <ref> [WOT + 95] </ref>, and CG and SP from NPB [B + 94]. The inherent characteristics of these applications are presented in Chapter 3. Table 7.4 shows the two problem sizes used in this evaluation. These applications are compiled on an SPP1600 and instrumented using SMAIT.
Reference: [ZK95] <author> D. Zucker and A. Karp. </author> <title> RYO a Versatile Instruction Instrumentation Tool for PA RISC. </title> <type> Technical Report CSLTR95658, </type> <institution> Stanford University, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: ATUM [ASH86] generates a compressed trace file for post analysis, but is also based on hardware support since it enables collecting address traces by modifying the microcode. Code instrumentation, e.g., ATOM [SE94], RYO <ref> [ZK95] </ref>, and Pixie [Smi91], enables collecting various performance data and traces by instrumenting either the assembly or the object file of a uniprocessor application. This technique often requires source code availability, perturbs the execution, and cannot be used with applications that generate code dynamically, e.g. data base systems. <p> Sections 2.4, 2.5, and 2.6 describe the CIAT, CDAT, and CCAT trace analysis tools, respectively. Section 2.7 describes TDAT. Finally, Section 2.8 outlines our assessment of the accuracy of these tools. Further detail on these tools is reported in [Aba96]. 2.2 Trace Collection (SMAIT) SMAIT is based on RYO <ref> [ZK95] </ref>, a tool developed by Zucker and Karp for instrumenting PA-RISC [Hew94] instruction sequences. RYO is a set of awk scripts that enable replacing individual machine instructions with calls to user written subroutines. SMAIT is designed to enable collecting traces of multi-threaded shared-memory parallel applications.
Reference: [ZT97] <author> Z. Zhang and J. Torrellas. </author> <title> Reducing Remote Conflict Misses: NUMA with Remote Cache versus COMA. </title> <booktitle> In Proc. HPCA-3, </booktitle> <pages> pages 272281, </pages> <year> 1997. </year> <month> 150 </month>
Reference-contexts: Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [AB97], NUMA-Q [LC96], S3.mp [NAB + 95], and NUMA-RC <ref> [ZT97] </ref>), or SRAMs to serve as fast interconnect caches (DASH [LLG + 92]), or both (R-NUMA [FW97] and VC-NUMA [MD98]). In Chapter 7, we evaluate the worthiness of an SRAM interconnect cache in reducing the number of remote communication misses. <p> In the SPP2000, although giving an exclusive copy of a modified line hurts repetitive near producer-consumer communication, it is profitable for migratory lines (lines that are accessed largely by one processor at a time <ref> [ZT97] </ref>). When a processor gets exclusive ownership of a migratory line, it does not need to request ownership when it subsequently updates this line. When the consumer performs a RAW access, the valid copy of the accessed line is in the producer's cache.
References-found: 91

