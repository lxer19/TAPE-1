URL: http://www.research.att.com/~mkearns/papers/testing.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Email: mkearns@research.att.com  danar@theory.lcs.mit.edu  
Title: Testing Problems with Sub-Learning Sample Complexity  
Author: Michael Kearns Dana Ron 
Date: January 1998  
Affiliation: AT&T Labs  M.I.T.  
Abstract: We study the problem of determining, for a class of functions H, whether an unknown target function f is contained in H or is far from any function in H. Thus, in contrast to problems of learning, where we must construct a good approximation to f in H on the basis of sample data, in problems of testing we are only required to determine the existence of a good approximation. Our main results demonstrate that, over the domain [0; 1] d for constant d, the number of examples required for testing grows only as ~ O( p s) for both decision trees of size s and a special class of neural networks with s hidden units. This is in contrast to the (s) examples required for learning these same classes. Our tests are based on combinatorial constructions demonstrating that these classes can be approximated by small classes of coarse partitions of space, and rely on repeated application of the well-known birthday paradox. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> O. Goldreich, S. Goldwasser, and D. Ron. </author> <title> Property testing and its connection to learning and approximation. </title> <booktitle> In Proceedings of the Thirty-Seventh Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 339-348, </pages> <year> 1996. </year>
Reference-contexts: Specifically, our main results demonstrate tests for s-node decision trees, and for a special class of neural networks of s hidden units (both over <ref> [0; 1] </ref> d ), that require only ~ O ( p s) 1 random examples when the input dimension d is held constant and the underlying distribution is uniform. <p> Improvements or variations on these combinatorial constructions and the resulting tests are interesting open problems. There are several lines of prior work that inspired the current investigation. Problems of testing and their relationship to learning were recently studied by Goldreich et. al. <ref> [1] </ref>, whose framework we follow and generalize; their positive results are for graph-theoretic problems not typically examined in the learning literature, and their tests all require queries. <p> For * 2 <ref> [0; 1] </ref>, if dist P (f; H ) &gt; *, then we say that f is *-far from H (with respect to P ). Otherwise, it is *-close. We use dist (; ) as a shorthand for dist U (; ), where U is the uniform distribution over X . <p> Our interest is in cases where testing is considerably easier than learning. In our first definition of testing we generalize the definition given by Goldreich et al. <ref> [1] </ref>. There the task was to determine whether an unknown function f belongs to a particular class of functions H or is *-far from H . <p> Proof: We first describe the testing algorithm in greater detail. Let s 0 = s=*, and consider the partition of the domain <ref> [0; 1] </ref> imposed by a one-dimensional grid with s 0 equal-size cells (intervals) c 1 ; : : : ; c s 0 . <p> assumption on f , E [ t ] &lt; 1 *, and by applying an additive Chernoff bound, with probability greater than 2=3, the average over the t 's is less than 1 3*=4. 4 Decision Trees In this section we study the problem of testing for decision trees over <ref> [0; 1] </ref> d . Given an input ~x = (x 1 ; : : : ; x d ), the (binary) decision at each node of the tree is whether x i a for some i 2 f1; : : : ; dg and a 2 [0; 1]. <p> for decision trees over <ref> [0; 1] </ref> d . Given an input ~x = (x 1 ; : : : ; x d ), the (binary) decision at each node of the tree is whether x i a for some i 2 f1; : : : ; dg and a 2 [0; 1]. The labels of the leaves of the decision tree are in f0; 1g. We define the size of such a tree to be the number of leaves it has, and we let DT d s denote the class of decision trees of size at most s over [0; 1] <p> 2 <ref> [0; 1] </ref>. The labels of the leaves of the decision tree are in f0; 1g. We define the size of such a tree to be the number of leaves it has, and we let DT d s denote the class of decision trees of size at most s over [0; 1] d . Thus, every tree in DT s determines a partition of the domain [0; 1] d into at most s axis aligned rectangles, each of dimension d (the leaves of the tree), where all points belonging to the same rectangle have the same label. <p> We define the size of such a tree to be the number of leaves it has, and we let DT d s denote the class of decision trees of size at most s over <ref> [0; 1] </ref> d . Thus, every tree in DT s determines a partition of the domain [0; 1] d into at most s axis aligned rectangles, each of dimension d (the leaves of the tree), where all points belonging to the same rectangle have the same label. <p> The following combinatorial lemma is the main step in proving Theorem 2. We shall need the following notation: For a d-dimensional rectangle R = (R 1 ; : : : ; R d ), where R j 2 <ref> [0; 1] </ref>, we let V (R) denote the volume of R, so V (R) = j=1 R j . If Q and R are d-dimensional rectangles, we say that Q fits in R if Q i R i for all i. <p> Note that this notion is independent of the position of Q and R in space. Lemma 4 Let R 1 ; : : : ; R t be rectangles in <ref> [0; 1] </ref> d , each of volume v 2 [0; 1]. Then for any natural number k d, there exists a rectangle Q in [0; 1] d such that V (Q) v 1+(d1)=k and Q fits in at least a fraction k (d1) of R 1 ; : : : ; <p> Note that this notion is independent of the position of Q and R in space. Lemma 4 Let R 1 ; : : : ; R t be rectangles in <ref> [0; 1] </ref> d , each of volume v 2 [0; 1]. Then for any natural number k d, there exists a rectangle Q in [0; 1] d such that V (Q) v 1+(d1)=k and Q fits in at least a fraction k (d1) of R 1 ; : : : ; R t . <p> Lemma 4 Let R 1 ; : : : ; R t be rectangles in <ref> [0; 1] </ref> d , each of volume v 2 [0; 1]. Then for any natural number k d, there exists a rectangle Q in [0; 1] d such that V (Q) v 1+(d1)=k and Q fits in at least a fraction k (d1) of R 1 ; : : : ; R t . Proof: We shall prove the lemma by induction. <p> Lemma 5 Let f be a decision tree in DT d s , and let R 1 ; : : : ; R s be the d-dimensional rectangles defined by the leaves of f . Let fi 2 <ref> [0; 1] </ref>, and suppose there exists a rectangle Q = (Q 1 ; : : : ; Q d ) such that the total volume of the rectangles among R 1 ; : : : ; R t in which Q fits is at least fi. <p> Consider a rectilinear partition G over <ref> [0; 1] </ref> d where for every j 2 f1; : : : ; dg, each cell in G is of length Q j =2 in dimension j. Then the fraction of grid cells in G that fall entirely inside leaves of f is at least fi 2 d . <p> For each setting of (i 1 ; : : : ; i d ) such that the i j 's are integers ranging between 1 and n, and P i j = n, the algorithm considers the grid G (i 1 ; : : : ; i d ) over <ref> [0; 1] </ref> d whose grid-cells have length 2 (i j +1) in dimension j. <p> From this point on we proceed as in the proof of Theorem 1. 5 Aligned Voting Networks In this section we study a restricted class of neural networks over <ref> [0; 1] </ref> d called Aligned Voting Networks. These are essentially neural networks in which the hyperplane defining each hidden unit is constrained to be parallel to some coordinate axis, and the output unit takes a majority vote of the hidden units. Definition 5 An aligned hyperplane over [0; 1] d is <p> neural networks over <ref> [0; 1] </ref> d called Aligned Voting Networks. These are essentially neural networks in which the hyperplane defining each hidden unit is constrained to be parallel to some coordinate axis, and the output unit takes a majority vote of the hidden units. Definition 5 An aligned hyperplane over [0; 1] d is a function h : [0; 1] d ! f+1; 1g of the form h (~x) = sign (x i a) for some dimension i 2 f1; : : : ; dg and some a 2 [1; 1] (where sign (0) is defined to be +1). <p> These are essentially neural networks in which the hyperplane defining each hidden unit is constrained to be parallel to some coordinate axis, and the output unit takes a majority vote of the hidden units. Definition 5 An aligned hyperplane over <ref> [0; 1] </ref> d is a function h : [0; 1] d ! f+1; 1g of the form h (~x) = sign (x i a) for some dimension i 2 f1; : : : ; dg and some a 2 [1; 1] (where sign (0) is defined to be +1). An aligned voting network over [0; 1] d is a <p> Definition 5 An aligned hyperplane over [0; 1] d is a function h : [0; 1] d ! f+1; 1g of the form h (~x) = sign (x i a) for some dimension i 2 f1; : : : ; dg and some a 2 <ref> [1; 1] </ref> (where sign (0) is defined to be +1). <p> a function h : <ref> [0; 1] </ref> d ! f+1; 1g of the form h (~x) = sign (x i a) for some dimension i 2 f1; : : : ; dg and some a 2 [1; 1] (where sign (0) is defined to be +1). An aligned voting network over [0; 1] d is a function f : [0; 1] d ! f+1; 1g of the form f (~x) = sign P s where each h j (~x) is an aligned hyperplane over [0; 1] d . The size of f is the number of voting hyperplanes s. <p> An aligned voting network over <ref> [0; 1] </ref> d is a function f : [0; 1] d ! f+1; 1g of the form f (~x) = sign P s where each h j (~x) is an aligned hyperplane over [0; 1] d . The size of f is the number of voting hyperplanes s. <p> An aligned voting network over <ref> [0; 1] </ref> d is a function f : [0; 1] d ! f+1; 1g of the form f (~x) = sign P s where each h j (~x) is an aligned hyperplane over [0; 1] d . The size of f is the number of voting hyperplanes s. An alternative way of viewing an aligned voting network f is as a constrained labeling of the cells of a rectilinear partition of [0; 1] d . <p> P s where each h j (~x) is an aligned hyperplane over <ref> [0; 1] </ref> d . The size of f is the number of voting hyperplanes s. An alternative way of viewing an aligned voting network f is as a constrained labeling of the cells of a rectilinear partition of [0; 1] d . For each dimension i, we have positions a j i 2 [0; 1] and orientations u j f+1; 1g. <p> The size of f is the number of voting hyperplanes s. An alternative way of viewing an aligned voting network f is as a constrained labeling of the cells of a rectilinear partition of <ref> [0; 1] </ref> d . For each dimension i, we have positions a j i 2 [0; 1] and orientations u j f+1; 1g. <p> Theorem 6 Let AVN d s denote the class of aligned voting networks of size at most s over <ref> [0; 1] </ref> d . <p> The following combinatorial lemma, which is central to our proof, exploits this property. Lemma 7 Let f be an aligned voting network over <ref> [0; 1] </ref> d of size at most s. <p> We prove the claim by induction on d. For the base case d = 1, we have at most s intervals of <ref> [0; 1] </ref> each labeled either +1 or 1. If the overall probability that f is +1 is at least (1=2) fl 1 , then in fact the total probability mass of the intervals labeled +1 is (1=2) fl 1 . <p> Solving 2fl 1 = (1=2) fl 1 yields fl 1 = 1=6. Now suppose the claim holds for every d 0 &lt; d, and assume that the probability that f is +1 over <ref> [0; 1] </ref> d is at least (1=2) fl d . <p> Thus we obtain the constraint fl d fl 2 d 1 =6 d , which is satisfied by the choice fl d = 1=(6 2 d+1 ) given in the lemma. The corollary below follows directly from Lemma 7. Corollary 8 Let f be an aligned voting network over <ref> [0; 1] </ref> d of size s. <p> Lemma 9 For any integer s 0 , dimension i 2 f1; : : : ; dg, and values fb j g s 0 j=1 , consider a partition of <ref> [0; 1] </ref> d into slices defined by the hyperplanes x i = b j . Then for any f+1; 1g labeling of these slices, there exists an aligned voting network g of size at most s 0 that is consistent with this labeling. <p> of the theorem follows from Corollary 8 and Lemma 9 using the same arguments applied in the proof of Theorem 1. 10 6 Testing and Weak Learning The intuition that testing is easier (or at least not harder) than learning can be formalized as follows (generalizing a similar statement in <ref> [1] </ref>). Proposition 10 Let F be a class of functions that is learnable by hypothesis class H , under distribution P , with confidence 5=6 and accuracy * 2 (0; 1=2], in m random examples.
Reference: [2] <author> S. Goldwasser and S. Micali. </author> <title> Probabilistic encryption. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 28(2) </volume> <pages> 270-299, </pages> <year> 1984. </year>
Reference-contexts: The proof of Theorem 11, which is given in Appendix B, applies a technique (known as the hybrid or probability walk technique), which was first used in the context of Cryptography <ref> [2] </ref>. By using the same technique we get a more general result that translates testing a pair of function classes (see Definition 4) to weakly learning (almost all functions in) one of the classes. The Proof of Theorem 12 is also given in Appendix B.
Reference: [3] <author> M. Kearns, M. Li, and L. Valiant. </author> <title> Learning boolean formulae. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 41(6) </volume> <pages> 1298-1328, </pages> <year> 1995. </year>
Reference: [4] <author> Jack Carl Kiefer. </author> <title> Introduction to Statistical Inference. </title> <publisher> Springer Verlag, </publisher> <year> 1987. </year> <month> 11 </month>
Reference-contexts: Our work can be viewed as a study of the sample complexity of classical hypothesis testing <ref> [4] </ref> in statistics, where one wishes to accept or reject a null hypothesis, such as the data is labeled by a function approximable by a small decision tree. The outline of the paper is as follows: in Section 2, we introduce several related notions of testing.
References-found: 4

