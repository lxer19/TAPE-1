URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1996/tr-96-015.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1996.html
Root-URL: http://www.icsi.berkeley.edu
Abstract-found: 0
Intro-found: 1
Reference: [BV92] <author> L. Bottou and V. Vapnik. </author> <title> Local learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 888-900, </pages> <year> 1992. </year>
Reference-contexts: The upper bound of the risk is a local measure and the capacity of the learning machine has to be controlled locally. The concept dealing with such cases, called Local Risk Minimization (LRM) principle, is a generalization of the SRM principle [Vap95], <ref> [BV92] </ref>. With that paper a new concept called Cyclical Local Structural Risk Minimization (CLSRM) is introduced. By combination of LRM and CSRM it is possible to minimize a global risk by cyclical minimization of residual local risks.
Reference: [Fah88] <author> Scott E. Fahlman. </author> <title> An empirical study of learning speed in back-propagation networks. </title> <type> Technical Report CMU-CS-88-162, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: They are trained cyclically after each insertion of a new hidden layer to minimize the mean square error at the output of the network by gradient descent using the Quickprop <ref> [Fah88] </ref> algorithm. The output units get the network input vector x and the outputs of all hidden units as input.
Reference: [Fie94] <author> E. Fiesler. </author> <title> Comparative bibliography od ontogenetic neural networks. </title> <editor> In P. G. Morasso M. Mariano, editor, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks 1994 Sorrento, </booktitle> <pages> pages 735 - 738. </pages> <publisher> Springer, </publisher> <month> 5 </month> <year> 1994. </year>
Reference-contexts: It minimizes so fare the upper bound of the risk. There are some neural network based heuristics realizing the CSRM principle. The algorithms differ in the growth rule and in the optimization procedure used for minimization of the empirical risk. A good overview is given by Fiesler <ref> [Fie94] </ref>. A common general approach used in feed forward networks, useful as well for function approximation as for classification tasks, is the cyclical minimization of the residual risk. The idea is to ask what additional input to the output layer is necessary to minimize the residual risk.
Reference: [FL90] <author> Scott E. Fahlman and Christian Lebiere. </author> <title> The Cascade-Correlation learning architecture. </title> <type> Technical Report CMU-CS-90-100, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> February </month> <year> 1990. </year> <note> 19 TACOMA </note>
Reference-contexts: The goal with each cycle is to reduce the residual empirical risk as much as possible by a minimum growth 1 of capacity. The learning stops if the capacity exceeds a threshold or if the empirical risk is small enough. The Cascade Correlation Learning Architecture (CASCOR) <ref> [FL90] </ref> is an example for the realization of that principle. The realization of that idea can be used with advantage if it is more clear how to increase the capacity at a certain point as what is a good maximum capacity a priori. <p> The idea is to ask what additional input to the output layer is necessary to minimize the residual risk. One useful answer, proposed by Fahlman with the Cascade Correlation Architecture <ref> [FL90] </ref> is to let the new input be correlated with the residual loss and than adapt the weights of output units again. The cyclical extension of the output layers input leads to a minimization of the empirical risk.
Reference: [GBD92] <author> Stuart Geman, Elie Bienenstock, and Rene Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: One can only build learning machines for distinct classes of problems which are bounded by similar a priori knowledge. The problem of small sample sizes is described by the Bias Variance Dilemma <ref> [GBD92] </ref>. As smaller the set of possible mapping rules is as greater is the convergence speed but as greater is also the minimal possible risk. That is the case of high bias and low variance.
Reference: [HYLJ93] <author> Jenq-Neng Hwang, Shih-Shien You, Shyh-Rong Lay, and I-Chang Jou. </author> <title> What's wrong with a cascaded correlation learning network: A projection pursuit learning perspective. </title> <booktitle> In Int. Symposium on Artificial Neural Networks, </booktitle> <pages> pages E11-E20, </pages> <year> 1993. </year>
Reference-contexts: The first task was been used as benchmark by Hwang <ref> [HYLJ93] </ref> for the Projection Pursuit Learning algorithm and Back Propagation. The function to be approximated is z (x; y) = 1:9 (1:35 + e x sin (13 (x 6) 2 )e y sin (7y)): The training set contains the function values from 225 [0; 1]-uniform distributed x; y values.
Reference: [LVW94a] <author> J. M. Lange, H.-M. Voigt, and D. Wolf. </author> <title> Growing artificial neural networks based on correlation measures, task decomposition and local attention neurons. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks 1994 as Part of the IEEE World Congress on Computational Intelligence Orlando, </booktitle> <pages> pages 1355 - 1358. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: An example for the realization of the CLSRM principle is the TACOMA (TAsk Decomposition, COrrelation Measures and local Attention neurons) learning Architecture [LVW94b], <ref> [LVW94a] </ref>. The algorithm generates a feed-forward network bottom up by cyclically inserting cascaded hidden layers. The Output of a hidden unit is locally restricted with respect to the input space of the network using an activation function combining the local characteristic of a radial basis function with a sigmoid unit. <p> This heuristic leads to a very well adapted network complexity. 4 The TACOMA Learning Architecture as Realization of the CLSRM Principle The TACOMA (TAsk decomposition, COrrelation Measures and local Attention neurons) algorithm [LVW94b], <ref> [LVW94a] </ref> generates a feed forward network bottom up by cyclical insertion of cascaded hidden layers starting with only output units. The capacity of the initial network is low and becomes increased locally with each growth step.
Reference: [LVW94b] <author> J. M. Lange, H.-M. Voigt, and D. Wolf. </author> <title> Task decomposition and correlations in growing artificial neural networks. </title> <editor> In P. G. Morasso M. Mariano, editor, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks 1994 Sorrento, </booktitle> <pages> pages 735 - 738. </pages> <publisher> Springer, </publisher> <month> 5 </month> <year> 1994. </year>
Reference-contexts: The concept to build the learning machine bottom up by cyclically local extension of the mapping rule leads to a global controlled development of locally adapted experts. An example for the realization of the CLSRM principle is the TACOMA (TAsk Decomposition, COrrelation Measures and local Attention neurons) learning Architecture <ref> [LVW94b] </ref>, [LVW94a]. The algorithm generates a feed-forward network bottom up by cyclically inserting cascaded hidden layers. <p> This heuristic leads to a very well adapted network complexity. 4 The TACOMA Learning Architecture as Realization of the CLSRM Principle The TACOMA (TAsk decomposition, COrrelation Measures and local Attention neurons) algorithm <ref> [LVW94b] </ref>, [LVW94a] generates a feed forward network bottom up by cyclical insertion of cascaded hidden layers starting with only output units. The capacity of the initial network is low and becomes increased locally with each growth step. <p> The Two Spirals Classification task [LW88] is to learn to discriminate between two sets of training points which lie on two distinct spirals in the x-y plane. These spirals coil three times around the origin and around one another. The Two Twin Spirals Classification task <ref> [LVW94b] </ref> has been designed to show the performance of the CLSRM principle realized with TACOMA. It is similar to the Two Spirals Classification task, see figure 6. For the evaluation of the Cascade Correlation algorithm the original program of S. Fahlman and Christian Lebiere was used.
Reference: [LW88] <author> K. J. Lang and M. J. Witbrock. </author> <title> Learning to tell two spirals apart. </title> <booktitle> In Proceedings of the 1988 Connectionist Models Summer School. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: The Two Spirals Classification task <ref> [LW88] </ref> is to learn to discriminate between two sets of training points which lie on two distinct spirals in the x-y plane. These spirals coil three times around the origin and around one another.
Reference: [Smi95] <author> Frank Smieja. </author> <title> private correspondence. </title> <institution> GMD Sankt Augustin, Research Group Adaptive Systems, </institution> <year> 1995. </year>
Reference-contexts: The approximation achieved by TACOMA is very precise as shown at figure 10. The next two benchmarks have been used by Frank Smieja <ref> [Smi95] </ref> for the evaluation of the Pandemonium system. These task are hard to approximate because the function is mostly very soft and only at some places are very strong chances.
Reference: [Vap95] <author> Vladimir N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year> <month> 20 </month>
Reference-contexts: The principle to approximate the mapping rule minimizing the risk by a mapping rule minimizing the empirical risk is called Empirical Risk Minimization (ERM) <ref> [Vap95] </ref>. The set of possible mapping rules, quasi the search space for the ERM, is restricted by additional assumptions called a priori knowledge. There is a need for learning machines as well with high convergence speed needing only a small number of training samples as with a small risk. <p> The maximum capacity is controlled by the choice of the topology and the node functions of the network. Another method is to minimize the capacity and to hold the empirical risk. Realizations are the Support Vector Machine of Vapnik <ref> [Vap95] </ref> and different pruning algorithms for neural networks. A new concept realizing the SRM principle is the Cyclical Structural Risk Minimization (CSRM). The idea is to increase the set of possible mapping rules and to minimize the residual empirical risk alternately over a number of cycles. <p> The upper bound of the risk is a local measure and the capacity of the learning machine has to be controlled locally. The concept dealing with such cases, called Local Risk Minimization (LRM) principle, is a generalization of the SRM principle <ref> [Vap95] </ref>, [BV92]. With that paper a new concept called Cyclical Local Structural Risk Minimization (CLSRM) is introduced. By combination of LRM and CSRM it is possible to minimize a global risk by cyclical minimization of residual local risks. <p> G = fg : R n ! R m gff : R n ! R m g The principle to approximate the mapping rule g fl minimizing the risk R (G) by a mapping rule g minimizing the empirical risk R emp (G) is called Empirical Risk Minimization (ERM) <ref> [Vap95] </ref>. 2.2 Balancing the A Priori Knowledge To get the right g by the ERM using an optimization procedure one has to take the condition lim P fsup g2G R (g) R emp (g) &gt; "g = 0 8" &gt; 0 (1) into account for which the ERM method is nontrivial <p> as shown 3 footnote The term generalization is borrowed by the neural network community from the psychology and describes how well a learning machine can generalize from the training set to unseen samples 4 these are two contradict claims representing the fundamental question of statistical learning pointed out by Vapnik <ref> [Vap95] </ref>: What must one know a priori about an unknown functional dependency in order to estimate it on the basis of observations ? 2.3 The Upper Bound of the Risk Vapnik and Chervonenkis did an important contribution with their fundamental works on the theory of statistical learning. <p> Based on that they formulated the conditions for the consistence of the ERM principle and derived an equation describing an upper bound of the risk depending on the capacity of the learning machine, the number of training samples and the empirical risk <ref> [Vap95] </ref>: Let A L (y; g (x)) B, g 2 G be a set of totally bounded functions. <p> concept to take the second summand into account while one constructs a learning machine and to control the capacity by a priori knowledge on the problem to be solved so that the upper bound resulting from the application of the ERM principle becomes minimal is called Structural Risk Minimization (SRM) <ref> [Vap95] </ref>. The concept of the SRM is to structure a set of possible loss functions resulting from G in nested subsets S k and to choose the smallest S k which contains the lossfunction and so fare the mapping rule minimizing the upper bound of the risk. <p> To estimate this factor from data one can use the cross validation technique. Another SRM method is to minimize the capacity and to hold the empirical risk fixed. Realizations of that are the Support Vector Machine of Vapnik <ref> [Vap95] </ref> and pruning algorithms for neural networks. 2.5 The Local Risk Minimization Principle If it is impossible to find a mapping reducing the empirical risk clearly on the basis of a given set of possible mapping rules then there are two chances to overcome this problem. <p> Extension of the set of possible mapping rules acording to the SRM principle or local approximation of the desired function at any point of interest. The second one is called Local Risk Minimization (LRM) principle <ref> [Vap95] </ref>. <p> The upper bound of local risk which holds with 1 for all functions A L (y; g (x)) B and all functions 0 w (x; x 0 ; fi) 1 is given by Vapnik <ref> [Vap95] </ref> as R (G; fi; x 0 ) l i=1 L (y i ; g (x i ))w (x i ; x 0 ; fi) + (B A)"(l; h P ) l i=1 w (x i ; x 0 ; fi) "(l; h fi )) + with "(l; h) = lh
References-found: 11

