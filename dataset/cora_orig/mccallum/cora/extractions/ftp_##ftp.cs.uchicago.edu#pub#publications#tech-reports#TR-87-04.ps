URL: ftp://ftp.cs.uchicago.edu/pub/publications/tech-reports/TR-87-04.ps
Refering-URL: http://cs-www.uchicago.edu/publications/tech-reports/
Root-URL: 
Title: Survey of the Equational Logic Programming Project  
Author: Michael J. O'Donnell 
Date: March, 1987  
Address: Chicago  
Affiliation: The University of  
Abstract: In 1975 I started a small project to explore the consequences of implementing equational programs with no semantic compromises. Latest results include a compiler that executes exactly the logical consequences of an equational program, with run-time speed comparable to compiled Franz LISP. This paper surveys the development of the project, through theoretical foundations, algorithm development, design and implementation, application, and directions for the future. 
Abstract-found: 1
Intro-found: 1
Reference: [AC75] <author> A. V. Aho and M. J. Corasick. </author> <title> Efficient string matching: An aid to bibliographic search. </title> <journal> Communications of the ACM, </journal> <volume> 18(6) </volume> <pages> 333-340, </pages> <year> 1975. </year>
Reference-contexts: The right basis for tree pattern-matching seems to be the Knuth-Morris-Pratt string matching algorithm [KMP77], with the Aho-Corasick extension for multiple strings <ref> [AC75] </ref>. The basic idea is to preprocess a set of pattern strings, producing a finite automaton to recognize instances of them. At run time, one need only access a transition table for the automaton to perform pattern matching.
Reference: [BG80] <author> R. M. Burstall and J. A. Goguen. </author> <title> The semantics of Clear, a specification language. </title> <booktitle> In Proceedings of the 1979 Copenhagen Winter School on Abstract Software Specification, volume 86 of Lecture Notes in Computer Science, </booktitle> <pages> pages 292-332, </pages> <year> 1980. </year>
Reference-contexts: Goguen et al. have developed some useful modular constructs for the equational language OBJ, but they do not follow any discipline, such as regularity, for guaranteeing completeness <ref> [BG80] </ref> [FGJM85]. While I still hope that a thoroughly satisfactory modular treatment of Church-Rosser equational programs can be found, there seems to be more immediate promise in abandoning the Church-Rosser property.
Reference: [BL79] <author> G. Berry and J.-J. Levy. </author> <title> Letter to the editor, SIGACT News, </title> <editor> v. </editor> <volume> 11, no. 1, </volume> <year> 1979. </year> <pages> pp. 3-4. </pages>
Reference-contexts: I also claimed a proof that certain strategies for choosing reduction steps, including the leftmost-outermost ones in the case described above, use the fewest possible reduction steps under reasonable assumptions about sharing of equivalent subexpressions. Berry and Levy pointed out an error in the definitions and the proof <ref> [BL79] </ref> [O'D79]. Huet and Levy produced a much stronger analysis of sequentiality, capable of automatically generating sequential strategies whenever such strategies depend only on the left-hand sides of rules, and not on the right-hand sides (the general problem is, of course, undecidable).
Reference: [BS76] <author> N. D. Belnap and T. B. Steel. </author> <title> The Logic of Questions and Answers. </title> <publisher> Yale University Press, </publisher> <year> 1976. </year> <month> 27 </month>
Reference-contexts: This analysis is developed in more detail by Belnap and Steel <ref> [BS76] </ref>. The concept of a question as a syntactic specification of possible answers is not very widely studied, but only extremely simple instances of it are required for interesting discussions of Logic Programming. The concept of correctness as logical consequence is thoroughly conventional and very well studied.
Reference: [CF58] <author> H. B. Curry and R. </author> <title> Feys. </title> <booktitle> Combinatory Logic, </booktitle> <volume> volume 1. </volume> <publisher> North--Holland, </publisher> <address> Amsterdam, </address> <year> 1958. </year>
Reference-contexts: Huet and Levy produced similar results, using a more elegant notation [HL79]. My results were inspired by similar results of Church [Chu41] and Curry and Feys <ref> [CF58] </ref> for the lambda calculus, although the lambda calculus is not an instance of the general sort of term rewriting system treated here. The following four conditions guarantee the Church-Rosser property and maximal termination of outermost reduction sequences. 1.
Reference: [Cha87] <author> D. R. Chase. </author> <title> An improvement to bottom-up tree pattern matching. </title> <booktitle> In 14th Annual Symposium on Principles of Programming Languages. </booktitle> <institution> SIGACT and SIGPLAN and Gesellschaft Fur Infor-matik, </institution> <year> 1987. </year>
Reference-contexts: Strandh is also pursuing techniques to deal with the nondeter-minism introduced by alternatives in patterns more efficient than the naive power-set method. Other interesting and potentially applicable work on tree pattern matching includes [COS82] <ref> [Cha87] </ref>. 12 Although the vast majority of effort was expended on term-rewriting im-plementations of Equational Logic Programming, Paul Chew in 1980 developed a radically different method of producing normal forms, based on congruence closure.
Reference: [Che80] <author> L. P. Chew. </author> <title> An improved algorithm for computing with equations. </title> <booktitle> In 21st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 108-117. </pages> <publisher> IEEE, </publisher> <year> 1980. </year>
Reference-contexts: Instead of actually performing a reduction, Chew's directed 13 congruence closure procedure adds the next instance of a reduction rule that would be applied to an initially empty finite set of ground instances, and performs congruence closure on the result <ref> [Che80] </ref>. The directed congruence closure technique has never been tested in practice, although it appears to be susceptible to efficient implementation.
Reference: [Che81] <author> L. P. Chew. </author> <title> Unique normal forms in term rewriting systems with repeated variables. </title> <booktitle> In 13th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 7-18, </pages> <year> 1981. </year>
Reference-contexts: Chew showed that uniqueness of normal forms still holds in a more liberal set of systems without restriction 3 (i.e., repeated variables are allowed), but where restriction 4 must hold for the related system in which left-hand side variables are renamed to be distinct <ref> [Che81] </ref> (i.e., f (g (x; x); x) is defined to overlap with g (a; b), in spite of the fact that the a and b are distinct and cannot be substituted for the same variable x).
Reference: [Chu41] <author> A. Church. </author> <title> The Calculi of Lambda-Conversion. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1941. </year>
Reference-contexts: Huet and Levy produced similar results, using a more elegant notation [HL79]. My results were inspired by similar results of Church <ref> [Chu41] </ref> and Curry and Feys [CF58] for the lambda calculus, although the lambda calculus is not an instance of the general sort of term rewriting system treated here. The following four conditions guarantee the Church-Rosser property and maximal termination of outermost reduction sequences. 1.
Reference: [COS82] <author> F. Cheng, S. Omdahl, and G. Strawn. </author> <title> Idiom matching: An optimization technique for an apl compiler. </title> <type> Technical report, </type> <institution> Iowa State University, </institution> <year> 1982. </year>
Reference-contexts: Strandh is also pursuing techniques to deal with the nondeter-minism introduced by alternatives in patterns more efficient than the naive power-set method. Other interesting and potentially applicable work on tree pattern matching includes <ref> [COS82] </ref> [Cha87]. 12 Although the vast majority of effort was expended on term-rewriting im-plementations of Equational Logic Programming, Paul Chew in 1980 developed a radically different method of producing normal forms, based on congruence closure.
Reference: [DH86] <author> V. J. Digricoli and M. C. Harrison. </author> <title> Equality-based binary resolution. </title> <journal> Journal of the ACM, </journal> <volume> 33(2) </volume> <pages> 253-289, </pages> <year> 1986. </year>
Reference-contexts: There appears to be some convergence between these ideas based on term rewriting, and recent work on equational theorem proving based on resolution <ref> [DH86] </ref>. Beyond proving equalities, it would be very useful to be able to solve equalities between terms with variables. This is precisely the problem of unification modulo a set of equations, and has been studied extensively in the attempt to combine Prolog with Functional Programming, but without definitive solution.
Reference: [DST80] <author> P. J. Downey, R. Sethi, and R. E. Tarjan. </author> <title> Variations on the common subexpression problem. </title> <journal> Journal of the ACM, </journal> <volume> 27(4) </volume> <pages> 758-771, </pages> <year> 1980. </year>
Reference-contexts: Kozen noticed that there is a polynomial time algorithm for congruence closure [Koz77], and Downey, Sethi and Tarjan developed the theoretically most efficient algorithms <ref> [DST80] </ref>, but the basis for practical work, and the inspiration for Chew's study of congruence closure, comes from Nelson and Oppen, who used a theoretically slower, but for most applications better, algorithm in automatic theorem provers [NO80].
Reference: [FGJM85] <author> K. Futatsugi, J. Goguen, J.-P. Jouannaud, and J. Meseguer. </author> <booktitle> Principles of OBJ2. In 12th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 52-66. </pages> <institution> SIGPLAN and SIGACT, </institution> <year> 1985. </year>
Reference-contexts: Goguen et al. have developed some useful modular constructs for the equational language OBJ, but they do not follow any discipline, such as regularity, for guaranteeing completeness [BG80] <ref> [FGJM85] </ref>. While I still hope that a thoroughly satisfactory modular treatment of Church-Rosser equational programs can be found, there seems to be more immediate promise in abandoning the Church-Rosser property.
Reference: [FW76] <author> D. Friedman and D. Wise. </author> <title> Cons should not evaluate its arguments. </title> <booktitle> In 3rd International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 257-284. </pages> <publisher> Edinburgh Unversity Press, </publisher> <year> 1976. </year>
Reference-contexts: Although inspired by McCarthy's paper on LISP, my early work aimed at a metasystem capable of treating LISP along with a wide class of alternatives. At about the same time, Henderson and Morris [HM76] and, independently, Friedman and Wise <ref> [FW76] </ref> proposed the use of outermost evaluation of cons-car-cdr expressions as an ad hoc improvement to LISP. The idea came to be known as lazy evaluation, and powerful programming techniques were developed to exploit it.
Reference: [HK86] <author> J. Heering and P. Klint. </author> <title> The efficiency of the equation interpreter compared with the UNH Prolog interpreter. </title> <journal> SIGPLAN Notices, </journal> <volume> 21(2) </volume> <pages> 18-21, </pages> <year> 1986. </year>
Reference-contexts: Further theoretical research is needed to determine whether some variation of the Knuth-Bendix procedure can provide a useful 20 generalization of equational programming allowing some nonregular systems of equations. Heering and Klint, working independently of this project, have applied the Equational Logic Programming Language to prototyping ML interpreters <ref> [HK86] </ref>. They demonstrated the very bad performance of the early interpretive version compared to Prolog.
Reference: [HL79] <author> G. Huet and J.-J. Levy. </author> <title> Computations in non-ambiguous linear term rewriting systems. </title> <type> Technical Report 359, IRIA, </type> <year> 1979. </year>
Reference-contexts: Huet and Levy produced similar results, using a more elegant notation <ref> [HL79] </ref>. My results were inspired by similar results of Church [Chu41] and Curry and Feys [CF58] for the lambda calculus, although the lambda calculus is not an instance of the general sort of term rewriting system treated here. <p> My own study of sequentiality was in a very sketchy state when I received the Huet-Levy work, and was essentially subsumed by theirs, although the appendix to [HO79] contains an explicit set-theoretic definition of some implicit concepts in <ref> [HL79] </ref>, which might yet be useful. Much later, I developed a significantly smaller class of strongly left-sequential term rewriting systems, allowing for an apparently simpler algorithm for choosing the next reduction step [O'D85]. <p> Given these restrictions, the new algorithm does the shortest possible traversal to find the correct instance of a left-hand side to reduce next. Huet and Levy also provide a pattern-matcher and optimal sequencer for their larger class of strongly sequential systems <ref> [HL79] </ref>. I understand from conversation that this pattern-matcher has been implemented, but there seems to be no analysis of its complexity in print.
Reference: [HM76] <author> P. Henderson and J. H. Morris. </author> <title> A lazy evaluator. </title> <booktitle> In 3rd Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 95-103. </pages> <institution> SIGPLAN and SIGACT, </institution> <year> 1976. </year>
Reference-contexts: Although inspired by McCarthy's paper on LISP, my early work aimed at a metasystem capable of treating LISP along with a wide class of alternatives. At about the same time, Henderson and Morris <ref> [HM76] </ref> and, independently, Friedman and Wise [FW76] proposed the use of outermost evaluation of cons-car-cdr expressions as an ad hoc improvement to LISP. The idea came to be known as lazy evaluation, and powerful programming techniques were developed to exploit it.
Reference: [HO79] <author> C. M. Hoffmann and M. J. O'Donnell. </author> <title> Interpreter generation using tree pattern matching. </title> <booktitle> In 6th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 169-179. </pages> <institution> SIGPLAN and SIGACT, </institution> <year> 1979. </year>
Reference-contexts: My own study of sequentiality was in a very sketchy state when I received the Huet-Levy work, and was essentially subsumed by theirs, although the appendix to <ref> [HO79] </ref> contains an explicit set-theoretic definition of some implicit concepts in [HL79], which might yet be useful. Much later, I developed a significantly smaller class of strongly left-sequential term rewriting systems, allowing for an apparently simpler algorithm for choosing the next reduction step [O'D85]. <p> choosing the right reduction steps | outermost reduction does not suffice for detecting equalities between terms that have no normal form. 4 Design and Implementation of an Equa tional Logic Programming Language Christoph Hoffmann and I developed the detailed design of a programming language based on Equational Logic in 1978 <ref> [HO79] </ref> [HO82b]. We decided to provide only minimal facilities to support programming in the small. In particular, we omitted type checking and modular constructs. The result is more like the assembly language for an unusual abstract machine than a high-level language.
Reference: [HO80] <author> G. Huet and D. Oppen. </author> <title> Equations and rewrite rules: a survey. </title> <editor> In R. Book, editor, </editor> <title> Formal Languages: Perspectives and Open Problems. </title> <publisher> Academic Press, </publisher> <year> 1980. </year>
Reference-contexts: Term rewriting is already widely used for testing equality, using systems of rules that are guaranteed to always produce unique normal forms <ref> [HO80] </ref>. A significant extension of these techniques to systems without termination could be very valuable. Even when applied to terminating systems, techniques designed for nonterminating systems are often more efficient, since they must avoid unnecessary and potentially infinite subcomputations. Three substantial technical problems arise in such a generalization.
Reference: [HO82a] <author> C. M. Hoffmann and M. J. O'Donnell. </author> <title> Pattern matching in trees. </title> <journal> Journal of the ACM, </journal> <volume> 29(1) </volume> <pages> 169-179, </pages> <year> 1982. </year>
Reference-contexts: In 1978-79, Hoffmann and I developed a number of ways of extending the efficient string techniques <ref> [HO82a] </ref>, some of them using automata that traversed the subject trees top-down, and others bottom-up. Each method works well on some problems, and poorly on others, and our work was mostly devoted to analyzing the properties of patterns that lead to good or bad behavior of each one. <p> In 1980, Paul Golick ported the CDC version to UNIX on the VAX, and reworked the run-time system. During 1982 and Spring of 1983, I took over the implementation effort and produced the first distributed version. After trying several variations on pattern-matching from <ref> [HO82a] </ref>, I chose a new algorithm specially adapted to strongly left-sequential systems of equations [HOS85] [O'D85]. Finally, in 1985 at Johns Hopkins, Robert Strandh converted my very slow interpreter into a compiler.
Reference: [HO82b] <author> C. M. Hoffmann and M. J. O'Donnell. </author> <title> Programming with equations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(1) </volume> <pages> 83-112, </pages> <year> 1982. </year>
Reference-contexts: the right reduction steps | outermost reduction does not suffice for detecting equalities between terms that have no normal form. 4 Design and Implementation of an Equa tional Logic Programming Language Christoph Hoffmann and I developed the detailed design of a programming language based on Equational Logic in 1978 [HO79] <ref> [HO82b] </ref>. We decided to provide only minimal facilities to support programming in the small. In particular, we omitted type checking and modular constructs. The result is more like the assembly language for an unusual abstract machine than a high-level language.
Reference: [HOS85] <author> C. M. Hoffmann, M. J. O'Donnell, and R. I. Strandh. </author> <title> Implementation of an interpreter for abstract equations. </title> <journal> Software | Practice and Experience, </journal> <volume> 15(12) </volume> <pages> 1185-1203, </pages> <year> 1985. </year>
Reference-contexts: During 1982 and Spring of 1983, I took over the implementation effort and produced the first distributed version. After trying several variations on pattern-matching from [HO82a], I chose a new algorithm specially adapted to strongly left-sequential systems of equations <ref> [HOS85] </ref> [O'D85]. Finally, in 1985 at Johns Hopkins, Robert Strandh converted my very slow interpreter into a compiler. He translated each entry in the transition table for the pattern-matching automaton into VAX assembly code, with transitions to new states implemented as goto instructions.
Reference: [Jay85] <author> B. Jayaraman. </author> <title> Equational programming: A unifying approach to functional and logic programming. </title> <type> Technical Report 85-030, </type> <institution> The University of North Carolina, </institution> <year> 1985. </year>
Reference-contexts: Significant generalization of the Knuth-Bendix procedure in this direction is completely open. The second problem is not solved, but a promising direction is apparent. Jayaraman noticed that, when trying to prove ff = fi, as long as the outermost symbols disagree, only outermost reductions need be tried <ref> [Jay85] </ref>. Once the outermost symbols agree, it is necessary to explore in parallel further outermost reductions and a decomposed problem of proving equality of corresponding arguments. Within each argument the same reasoning applies.
Reference: [KB70] <author> D. E. Knuth and P. Bendix. </author> <title> Simple word problems in universal algebras. </title> <editor> In J. Leech, editor, </editor> <booktitle> Computational Problems in Abstract Algebra, </booktitle> <pages> pages 127-146. </pages> <publisher> Pergammon Press, Oxford, </publisher> <year> 1970. </year> <month> 29 </month>
Reference-contexts: Restriction 2 is often strengthened to disallow common instances of different right-hand sides entirely (e.g. [HL79][Klo80]). The combination of the stronger restriction 2 with restriction 4 amounts to ruling out critical pairs 8 in the terminology of Knuth and Bendix <ref> [KB70] </ref>. Knuth and Bendix showed how to decide whether a term rewriting system with critical pairs has the Church-Rosser property, under the assumption that all reduction sequences terminate. Their method does not apply when nonterminating sequences are allowed. <p> It appears, however, that a really desirable equational programming language should allow benign overlaps that do not destroy the Church-Rosser property. Of course, the Church-Rosser property is undecidable, so decidable sufficient conditions are required. The Knuth-Bendix procedure <ref> [KB70] </ref> solves this problem for systems in which every reduction sequence leads to normal form. Their procedure detects overlaps (called critical pairs), constructs examples where a term ff reduces by overlapping equations to fi and fl, then reduces fi and fl to see if they reach a common normal form.
Reference: [Klo80] <author> J. W. Klop. </author> <title> Combinatory Reduction Systems. </title> <type> PhD thesis, </type> <institution> Math--ematisch Centrum, </institution> <address> Amsterdam, </address> <year> 1980. </year>
Reference: [KMP77] <author> D. E. Knuth, J. Morris, and V. Pratt. </author> <title> Fast pattern matching in strings. </title> <journal> SIAM Journal on Computing, </journal> <volume> 6(2) </volume> <pages> 127-146, </pages> <year> 1977. </year>
Reference-contexts: The right basis for tree pattern-matching seems to be the Knuth-Morris-Pratt string matching algorithm <ref> [KMP77] </ref>, with the Aho-Corasick extension for multiple strings [AC75]. The basic idea is to preprocess a set of pattern strings, producing a finite automaton to recognize instances of them. At run time, one need only access a transition table for the automaton to perform pattern matching.
Reference: [Koz77] <author> D. Kozen. </author> <title> Complexity of finitely presented algebras. </title> <booktitle> In 9th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 164-177, </pages> <year> 1977. </year>
Reference-contexts: It is easy to see that all equalities between terms corresponding to nodes in the graph that are logical consequences of the postulates will be generated by this procedure. Kozen noticed that there is a polynomial time algorithm for congruence closure <ref> [Koz77] </ref>, and Downey, Sethi and Tarjan developed the theoretically most efficient algorithms [DST80], but the basis for practical work, and the inspiration for Chew's study of congruence closure, comes from Nelson and Oppen, who used a theoretically slower, but for most applications better, algorithm in automatic theorem provers [NO80].
Reference: [McC60] <author> J. McCarthy. </author> <title> Recursive functions of symbolic expressions and their computation by machine. </title> <journal> Communications of the ACM, </journal> <volume> 3(4) </volume> <pages> 184-195, </pages> <year> 1960. </year>
Reference-contexts: I launched the Equational Logic Programming project in 1975, as dissertation research at Cornell University. The immediate inspiration came from reading McCarthy's paper on LISP <ref> [McC60] </ref>, in which LISP was motivated as an attempt to accommodate mathematical recursive definitions of functions in a practical programming language. <p> It is reasonable to view LISP as a language that started with a form of Equational Logic Programming as its motivation, then evolved like Prolog to satisfy pragmatic desires. Such a view is consistent with, but not compelled by, McCarthy's original presentation of LISP <ref> [McC60] </ref>. Having taken such a view of LISP in 1975, I decided to seek a reasonable set of restrictions on Equational Logic Programming that would allow a sound, complete, and efficient implementation, with absolutely no semantic compromises other than those inherent in the use of a physically finite computing machine.
Reference: [NO80] <author> G. Nelson and D. C. Oppen. </author> <title> Fast decision algorithms based on congruence closure. </title> <journal> Journal of the ACM, </journal> <volume> 27(2) </volume> <pages> 356-364, </pages> <year> 1980. </year>
Reference-contexts: congruence closure [Koz77], and Downey, Sethi and Tarjan developed the theoretically most efficient algorithms [DST80], but the basis for practical work, and the inspiration for Chew's study of congruence closure, comes from Nelson and Oppen, who used a theoretically slower, but for most applications better, algorithm in automatic theorem provers <ref> [NO80] </ref>. For Equational Logic Programming, the congruence closure technique looks very attractive, because it never evaluates the same term twice. Unfortunately, the technique does not apply directly, because of the use of variables in equational programs.
Reference: [O'D77] <author> M. J. O'Donnell. </author> <title> Computing in Systems Described by Equations, </title> <booktitle> volume 58 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1977. </year>
Reference-contexts: In addition, such strategies should avoid, as much as possible, unnecessary reduction steps. Note that the call-by-value techniques used in conventional LISP interpretation guarantee that normal forms are avoided as much as possible in favor of infinite paths, except in the computation of conditionals. In <ref> [O'D77] </ref> I developed sufficient conditions to guarantee that a system of schematic rewrite rules has the Church-Rosser property (slightly generalizing a result of Rosen's [Ros73]), and that a strategy that eventually chooses each outermost reduction step must reach a normal form whenever it exists. <p> In many cases, it is easy to identify a unique reduction step to take, in such a way that all normal forms are reached with no wasted steps. Such strategies are called sequential strategies, and systems admitting sequential strategies are sequential systems. In <ref> [O'D77] </ref> I made the elementary observation that leftmost-outermost reductions are sufficient to find all normal forms, when the variables in every right-hand side are grouped to the right, as in the conditional equations.
Reference: [O'D79] <author> M. J. O'Donnell. </author> <title> Letter to the editor, SIGACT News, </title> <editor> v. </editor> <volume> 11, no. 2, </volume> <year> 1979. </year> <note> p. 2. </note>
Reference-contexts: I also claimed a proof that certain strategies for choosing reduction steps, including the leftmost-outermost ones in the case described above, use the fewest possible reduction steps under reasonable assumptions about sharing of equivalent subexpressions. Berry and Levy pointed out an error in the definitions and the proof [BL79] <ref> [O'D79] </ref>. Huet and Levy produced a much stronger analysis of sequentiality, capable of automatically generating sequential strategies whenever such strategies depend only on the left-hand sides of rules, and not on the right-hand sides (the general problem is, of course, undecidable).
Reference: [O'D85] <author> M. J. O'Donnell. </author> <title> Equational Logic as a Programming Language. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: Much later, I developed a significantly smaller class of strongly left-sequential term rewriting systems, allowing for an apparently simpler algorithm for choosing the next reduction step <ref> [O'D85] </ref>. Although inspired by McCarthy's paper on LISP, my early work aimed at a metasystem capable of treating LISP along with a wide class of alternatives. <p> In 1983, I discovered a highly specialized pattern-matcher that works very efficiently, but only for patterns satisfying restrictions 1-4 of Section 2, and an additional restriction to guarantee strong left-sequentiality <ref> [O'D85] </ref>. Given these restrictions, the new algorithm does the shortest possible traversal to find the correct instance of a left-hand side to reduce next. Huet and Levy also provide a pattern-matcher and optimal sequencer for their larger class of strongly sequential systems [HL79]. <p> During 1982 and Spring of 1983, I took over the implementation effort and produced the first distributed version. After trying several variations on pattern-matching from [HO82a], I chose a new algorithm specially adapted to strongly left-sequential systems of equations [HOS85] <ref> [O'D85] </ref>. Finally, in 1985 at Johns Hopkins, Robert Strandh converted my very slow interpreter into a compiler. He translated each entry in the transition table for the pattern-matching automaton into VAX assembly code, with transitions to new states implemented as goto instructions. <p> The most common approach to formal, non-context-free, syntactic processing uses attribute grammars. The potential advantages of equational programs include their complete separation from parsing concerns, and their ability to express all of the details of syntactic processing <ref> [O'D85] </ref>. Attribute grammar notation organizes the dependence of attributes on one another, but not the actual method for computing each attribute, while an equational program includes both sorts of information. <p> The current system takes an entire input term, reduces to normal form, and outputs the result. In order to adapt to, for example, the interactive input and output of Lucid, a much more flexible interface is required. <ref> [O'D85] </ref> contains some general speculations about the nature of such an interface, but the details have not yet been addressed. <p> These equations code nicely into an executable equational program <ref> [O'D85] </ref>, but they fail to remove trailing zeroes. The conceptually natural way to eliminate the zeroes is to add the two equations X fi 0 = 0 but this introduces overlaps. There is a solution, but it is rather ugly [O'D85], and involves sneakily encoding control information into semantically irrelevant aspects <p> These equations code nicely into an executable equational program <ref> [O'D85] </ref>, but they fail to remove trailing zeroes. The conceptually natural way to eliminate the zeroes is to add the two equations X fi 0 = 0 but this introduces overlaps. There is a solution, but it is rather ugly [O'D85], and involves sneakily encoding control information into semantically irrelevant aspects of the form of a polynomial. <p> There is one example | weak reduction in the Combinator Calculus, with the binary function AP P LY appearing both leftmost and nonleftmost in left-hand sides | where the perfect system of equations is regular but not constructor <ref> [O'D85] </ref>. It appears, however, that a really desirable equational programming language should allow benign overlaps that do not destroy the Church-Rosser property. Of course, the Church-Rosser property is undecidable, so decidable sufficient conditions are required. <p> This problem is very simple to solve, in principle. Incremental reading of the input as it is needed is also very easy to program, but that has not yet been done <ref> [O'D85] </ref>. A more serious problem conceptually is the arbitrariness of the left-to-right prefix order in which symbols of a term appear in conventional notation. The current implementation's treatment of incremental output, and the easy version of incremental input, require terms to be processed in that fixed order. <p> The most natural representative of the meaning of a program P seems to be a triple consisting of the term language defined by P, the set of models satisfying the equations in P, and the set of normal forms. In <ref> [O'D85] </ref> I tried to develop semantic 23 constructs for combining such triples. Aside from some glaring errors in the definitions, the basic idea seems to be a failure, because combinations of regular systems are not necessarily regular, nor are they Church-Rosser.
Reference: [Ros73] <author> B. K. Rosen. </author> <title> Tree manipulation systems and Church-Rosser theorems. </title> <journal> Journal of the ACM, </journal> <volume> 20(1) </volume> <pages> 160-187, </pages> <year> 1973. </year>
Reference-contexts: Programmers are willing to abide by strong restrictions on their use of logical language, if they receive fast performance in return. Learning of the general study of Term Rewriting Systems through Rosen's work <ref> [Ros73] </ref>, I chose first to explore implementations based on term rewriting. That is, I chose to implement each equation t = in a program by replacing instances of t whenever possible by corresponding instances of . <p> In [O'D77] I developed sufficient conditions to guarantee that a system of schematic rewrite rules has the Church-Rosser property (slightly generalizing a result of Rosen's <ref> [Ros73] </ref>), and that a strategy that eventually chooses each outermost reduction step must reach a normal form whenever it exists. Huet and Levy produced similar results, using a more elegant notation [HL79].
Reference: [Sta77] <author> J. </author> <title> Staples. A class of replacement systems with simple optimality theory. </title> <journal> Bulletin of the Australian Mathematical Society, </journal> <volume> 17(3) </volume> <pages> 335-350, </pages> <year> 1977. </year>
Reference-contexts: To my knowledge, no one has actually proved optimality for these systems in terms of counting reduction steps, although Staples has done so for some related systems <ref> [Sta77] </ref>. My own study of sequentiality was in a very sketchy state when I received the Huet-Levy work, and was essentially subsumed by theirs, although the appendix to [HO79] contains an explicit set-theoretic definition of some implicit concepts in [HL79], which might yet be useful.
Reference: [Str84] <author> R. I. Strandh. </author> <title> Incremental suffix trees with multiple subject strings. </title> <type> Technical Report JHU/EECS-84/18, </type> <institution> The Johns-Hopkins University, </institution> <year> 1984. </year> <month> 30 </month>
Reference-contexts: In 1984, Robert Strandh designed a method, based on suffix trees, for adding patterns to a pattern-matching automaton, with a cost proportional to the added pattern, and independent of the old automaton <ref> [Str84] </ref>. This method has not yet been exploited in practice, but it promises to be useful both for incremental compilation of a changing equational program, and for optimizations involving the inclusion of dynamically derived reduction rules encompassing many steps of the original equational program.
References-found: 35

