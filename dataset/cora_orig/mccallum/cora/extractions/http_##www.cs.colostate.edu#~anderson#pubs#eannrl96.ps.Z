URL: http://www.cs.colostate.edu/~anderson/pubs/eannrl96.ps.Z
Refering-URL: http://www.cs.colostate.edu/~anderson/pubs/pubs.html
Root-URL: 
Email: fanderson,kretchmag@cs.colostate.edu  fhittle,along@lamar.colostate.edu  
Phone: 2  
Title: Reinforcement Learning, Neural Networks and PI Control Applied to a Heating Coil  
Author: Charles W. Anderson Douglas C. Hittle Alon D. Katz and R. Matt Kretchmar 
Address: Fort Collins, CO 80523  Fort Collins, CO 80523  
Affiliation: 1 Department of Computer Science Colorado State University  Department of Mechanical Engineering Colorado State University  
Abstract: An accurate simulation of a heating coil is used to compare the performance of a PI controller, a neural network trained to predict the steady-state output of the PI controller, a neural network trained to minimize the n-step ahead error between the coil output and the set point, and a reinforcement learning agent trained to minimize the sum of the squared error over time. Although the PI controller works very well for this task, the neural networks do result in improved performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Charles W. Anderson. </author> <title> Q-learning with hidden-unit restarting. </title> <editor> In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 81-88. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Variations of each strategy are being studied further and ways of combining them are being developed. For example, the reinforcement learner can be added to any existing control system, including the combined steady-state predictor and PI controller. Additional investigations continue on the state representation <ref> [1, 2] </ref>, the set of possible reinforcement-learning agent actions, different neural network architectures, and on strategies for slowly decreasing the amount of exploration performed by the reinforcement-learning agent during learning. Plans include the evaluation of this technique on a physical heating coil being controlled with a PC and control hardware.
Reference: [2] <author> Charles W. Anderson and S. G. Crawford-Hines. </author> <title> Multigrid Q-learning. </title> <type> Technical Report CS-94-121, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <year> 1994. </year>
Reference-contexts: Variations of each strategy are being studied further and ways of combining them are being developed. For example, the reinforcement learner can be added to any existing control system, including the combined steady-state predictor and PI controller. Additional investigations continue on the state representation <ref> [1, 2] </ref>, the set of possible reinforcement-learning agent actions, different neural network architectures, and on strategies for slowly decreasing the amount of exploration performed by the reinforcement-learning agent during learning. Plans include the evaluation of this technique on a physical heating coil being controlled with a PC and control hardware.
Reference: [3] <author> Peter S. Curtiss. </author> <title> Experimental results from a network-assisted PID controller. </title> <journal> ASHRAE Transactions, </journal> <volume> 102(1), </volume> <year> 1996. </year> <title> the set point and the actual output air temperature over time. The second graph shows the output of the reinforcement-learning agent. The third graph shows the combined PI output and reinforcement-learning agent output. </title>
Reference-contexts: Some of these issues have been addressed by applying neural networks and other artificial intelligence techniques to the control of heating and air-conditioning systems <ref> [4, 3, 7] </ref>. In this article, three approaches to improving the performance of an ordinary proportional plus integral controller are explored. One is a feed forward neural network controller designed to bring the controlled variable to its set point in n sampling intervals (where n is variable).
Reference: [4] <author> Peter S. Curtiss, Gideon Shavit, and Jan F. Krieder. </author> <title> Neural networks applied to buildings a tutorial and case studies in prediction and adaptive control. </title> <journal> ASHRAE Transactions, </journal> <volume> 102(1), </volume> <year> 1996. </year>
Reference-contexts: Some of these issues have been addressed by applying neural networks and other artificial intelligence techniques to the control of heating and air-conditioning systems <ref> [4, 3, 7] </ref>. In this article, three approaches to improving the performance of an ordinary proportional plus integral controller are explored. One is a feed forward neural network controller designed to bring the controlled variable to its set point in n sampling intervals (where n is variable).
Reference: [5] <author> S. J. Hepworth and A. L. Dexter. </author> <title> Neural control of a non-linear HVAC plant. </title> <booktitle> Proceedings 3rd IEEE Conference on Control Applications, </booktitle> <year> 1994. </year>
Reference-contexts: The apparent improved training performance of a two-hidden-unit network is due to a fortuitous, random selection of initial weight values. This approach of using a neural network to duplicate the steady state output of a PI controller was proposed by Hepworth, Dexter, and Willis <ref> [6, 5] </ref>, who used two inputs (discharge air temperature and inlet air temperature) to train a network of radial basis functions. In contrast, we used sigmoid neurons and took advantage of additional measurable variables such as air flow rate, inlet water temperature and water flow rate.
Reference: [6] <author> S. J. Hepworth, A. L. Dexter, and S. T. P. Willis. </author> <title> Control of a non-linear heater battery using a neural network. </title> <type> Technical report, </type> <institution> University of Oxford, </institution> <year> 1993. </year>
Reference-contexts: The apparent improved training performance of a two-hidden-unit network is due to a fortuitous, random selection of initial weight values. This approach of using a neural network to duplicate the steady state output of a PI controller was proposed by Hepworth, Dexter, and Willis <ref> [6, 5] </ref>, who used two inputs (discharge air temperature and inlet air temperature) to train a network of radial basis functions. In contrast, we used sigmoid neurons and took advantage of additional measurable variables such as air flow rate, inlet water temperature and water flow rate.
Reference: [7] <author> Minoru Kawashima, Charles E. Dorgan, and John W. Mitchell. </author> <title> Optimizing system control with load prediction by neural networks for an ice-storage system. </title> <journal> ASHRAE Transactions, </journal> <volume> 102(1), </volume> <year> 1996. </year>
Reference-contexts: Some of these issues have been addressed by applying neural networks and other artificial intelligence techniques to the control of heating and air-conditioning systems <ref> [4, 3, 7] </ref>. In this article, three approaches to improving the performance of an ordinary proportional plus integral controller are explored. One is a feed forward neural network controller designed to bring the controlled variable to its set point in n sampling intervals (where n is variable).
Reference: [8] <author> R. S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: The term within brackets is often referred to as a temporal-difference error, defined by Sutton <ref> [8] </ref>. To improve the action-selection policy and achieve optimal control, the dynamic programming method called value iteration can be applied. This method combines steps of policy evaluation with policy improvement.
Reference: [9] <author> David M. Underwood and Roy R. Crawford. </author> <title> Dynamic nonlinear modeling of a hot-water-to-air heat exchanger for control applications. </title> <journal> ASHRAE Transactions, </journal> <volume> 97(1) </volume> <pages> 149-155, </pages> <year> 1991. </year>
Reference-contexts: In Section 5, an optimal control method based on reinforcement learning is presented. The reinforcement learning agent learns to augment the output of the PI controller only when it results in improved performance. 2 Heating Coil Model and PI Control Underwood and Crawford <ref> [9] </ref> developed a model of an existing heating coil by fitting a set of second-order, nonlinear equations to measurements of air and water temperatures and flow rates obtained from the actual coil. A diagram of the model is shown in Figure 1.
Reference: [10] <author> C. Watkins. </author> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University Psychology Department, </institution> <year> 1989. </year>
Reference-contexts: Watkins <ref> [10] </ref> proves that it does converge to the optimal value function, meaning that selecting the action, a, that maximizes Q (s t ; a) for any state s t will result in the optimal sum of reinforcement over time. 5.1 Experiment Reinforcement at time step t is determined by the squared
References-found: 10

