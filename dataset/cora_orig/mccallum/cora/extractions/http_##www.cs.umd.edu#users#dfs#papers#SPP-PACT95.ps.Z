URL: http://www.cs.umd.edu/users/dfs/papers/SPP-PACT95.ps.Z
Refering-URL: http://www.cs.umd.edu/users/dfs/publications.html
Root-URL: 
Email: tron@cesdis.gsfc.nasa.gov  dfs@cs.umd.edu  merk@cesdis.gsfc.nasa.gov  olson@jeans.gsfc.nasa.gov  
Title: An Empirical Evaluation of the Convex SPP-1000 Hierarchical Shared Memory System  
Author: Thomas Sterling Daniel Savarese Phillip Merkey Kevin Olson 
Address: Greenbelt, MD 20771  College Park, MD 20742  Greenbelt, MD 20771  
Note: Excellence in Space Data and Information Sciences Code 930.5 NASA Goddard Space  Excellence in Space Data and Information Sciences Code 930.5 NASA Goddard Space  
Affiliation: Center of  Flight Center  Department of Computer Science University of Maryland  Center of  Flight Center  Institute for Computational Science and Informatics George Mason University  
Abstract: Cache coherency in a scalable parallel computer architecture requires mechanisms beyond the conventional common bus based snooping approaches which are limited to about 16 processors. The new Convex SPP-1000 achieves cache coherency across 128 processors through a two-level shared memory NUMA structure employing directory based and SCI protocol mechanisms. While hardware support for managing a common global name space minimizes overhead costs and simplifies programming, latency considerations for remote accesses may still dominate and can under unfavorable conditions constrain scalability. This paper provides the first published evaluation of the SPP-1000 hierarchical cache coherency mechanisms from the perspective of measured latency and its impact on basic global flow control mechanisms, scaling of a parallel science code, and sensitivity of cache miss rates to system scale. It is shown that global remote access latency is only a factor of seven greater than that of local cache miss penalty and that scaling of a challenging scientific application is not severely degraded by the hierarchical structure for achieving consistency across the system processor caches. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, K. Johnson, et al. </author> <title> "The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor," </title> <editor> M. Dubois and S.S. Thakkar, editors, </editor> <title> Scalable Shared Memory Multiprocessors, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992, </year> <pages> pp. 239-261. </pages>
Reference: [2] <author> J.E. Barnes and P. Hut, </author> <title> "A Hierarchical O(n log n) Force Calculation Algorithm," </title> <journal> Nature, </journal> <volume> vol. 342, </volume> <year> 1986. </year>
Reference-contexts: This parameter also serves to define a resolution limit to the problem. This equation also shows that the problem scales as N 2 and modeling systems with particle numbers larger than several thousand is infeasible. Tree codes are a collection of algorithms which approximate the solution to equation 1 <ref> [2, 8, 13] </ref>. In these algorithms the particles are sorted into a spatial hierarchy which forms a tree data structure.
Reference: [3] <author> CONVEX Computer Corporation, </author> <title> "Camelot MACH Mi-crokernel Interface Specification: Architecture Interface Library," Richardson, </title> <address> TX, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Once threads on a second hypernode become involved, there is an additional penalty, as evidenced by both the high locality and uniform distribution cases. This behavior may be caused by the implementation of the barrier primitive, which has each thread decrement an uncached counting semaphore <ref> [3] </ref> and then enter a while loop, waiting for a shared variable to be set to a particular value. The last thread to enter the barrier sets the shared variable to the expected value, thus releasing the other threads from their spin waiting.
Reference: [4] <author> D. Chaiken, J. Kubiatowitz, and A. Agarwal, </author> <title> "LimitLESS Directories: A Scalable Cache Coherence Scheme," </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <year> 1991, </year> <pages> pp. 224-234. </pages>
Reference: [5] <institution> CONVEX Computer Corporation, "Exemplar Architecture Manual," Richardson, TX, </institution> <year> 1993. </year>
Reference-contexts: The SPP-1000 employs a two-tier cache coherence scheme that is unique in the industry. It combines a directory based cache coherence mechanism within the cluster with the SCI protocol using an "interconnect cache" across clusters <ref> [5] </ref>. A true virtual memory system is supported. A parallel process with many concurrent sub-processes will have a single virtual address space.
Reference: [6] <institution> CONVEX Computer Corporation, "Exemplar Programming Guide," Richardson, TX, </institution> <year> 1993. </year>
Reference: [7] <author> Cray Research, Inc., </author> <title> "CRAY T3D System Architecture Overview," </title> <type> Eagan, </type> <institution> Minnesota. </institution>
Reference-contexts: This distinguishes the memory system of the SPP-1000 from the distributed memory of the TMC CM-5 [14] or the Intel Paragon [10] with their fragmented address spaces. The CRI T3D <ref> [7] </ref> also has a global shared memory. The primary means of ameliorating potential performance degradation due to access latency is through the use of caches which hold copies of data elements in fast buffer memory and rely on temporal and spatial locality for high hit rates.
Reference: [8] <author> L. Hernquist, </author> <title> "Vectorization of Tree Traversals," </title> <journal> Journal of Computational Physics, </journal> <volume> vol. 87, </volume> <year> 1990. </year>
Reference-contexts: This parameter also serves to define a resolution limit to the problem. This equation also shows that the problem scales as N 2 and modeling systems with particle numbers larger than several thousand is infeasible. Tree codes are a collection of algorithms which approximate the solution to equation 1 <ref> [2, 8, 13] </ref>. In these algorithms the particles are sorted into a spatial hierarchy which forms a tree data structure. <p> From this initial data it is not possible to predict how speedup will change as additional hypernodes are added. Finally, the 16 processor 384 Mflop/s result compares favorably to a highly vectorized, public domain tree code <ref> [8] </ref> which achieves 120 Mflop/s on one head of a C90. 5 Cache Consistency Behavior The Convex SPP-1000 cache coherency mechanism operates at two levels: the directory based mechanism within a given hypernode and the SCI method that maintains consistency across hypernodes.
Reference: [9] <author> Hewlett Packard Company, </author> <title> "PA-RISC 1.1 Architecture and Instruction Set Reference Manual," </title> <publisher> Hewlett Packard Company, </publisher> <year> 1992. </year>
Reference-contexts: The approach taken incorporates the HP PA-RISC <ref> [9] </ref> microprocessor in a hierarchical structure with hardware support for full global shared memory operation. Most notably, the architecture provides for full cache coherence, time and space sharing of system resources, and virtual memory for up to 128 processors in the current design.
Reference: [10] <author> Intel Corporation, </author> <title> "Paragon User's Guide," </title> <address> Beaverton, Oregon 1993. </address>
Reference-contexts: This distinguishes the memory system of the SPP-1000 from the distributed memory of the TMC CM-5 [14] or the Intel Paragon <ref> [10] </ref> with their fragmented address spaces. The CRI T3D [7] also has a global shared memory.
Reference: [11] <institution> IEEE Standard for Scalable Coherent Interface, IEEE, </institution> <year> 1993. </year>
Reference-contexts: The Convex SPP-1000 is the most recent entry in the commercial sector of scalable cache coherent architectures. Its structure is a two-level hierarchy with up to 16 clusters (hypernodes) of eight processors each. It employs a directory based cache management scheme at the lower (intra-hypernode) level and an SCI <ref> [11] </ref> protocol based global communication and cache control mechanism at the higher level. This paper provides an initial examination of the performance characteristics of the SPP-1000 hierarchical cache management.
Reference: [12] <institution> Kendall Square Research Corporation, </institution> <type> "KSR Technical Summary," </type> <address> Waltham, MA, </address> <year> 1992. </year>
Reference-contexts: A SPP-1000 cluster is self sufficient and can stand alone as a full and operational computing system. 2.3 Global Organization Scalability for the SPP-1000 is achieved through a scheme that combines data consistency with global communications. In a manner reminiscent of the KSR-1 <ref> [12] </ref>, the clusters are interconnected using rings. However, unlike its predecessor, the SPP-1000 connects all 16 clusters using four parallel rings. All 16 clusters are connected to all four rings. This is achieved through the SCI interfaces to the Functional Units within each cluster.
Reference: [13] <author> K. Olson and J. Dorband, </author> <title> "An Implementation of a Tree Code on a SIMD Parallel Computer," </title> <journal> Astrophysical Journal Supplement Series, </journal> <month> September </month> <year> 1994. </year>
Reference-contexts: This parameter also serves to define a resolution limit to the problem. This equation also shows that the problem scales as N 2 and modeling systems with particle numbers larger than several thousand is infeasible. Tree codes are a collection of algorithms which approximate the solution to equation 1 <ref> [2, 8, 13] </ref>. In these algorithms the particles are sorted into a spatial hierarchy which forms a tree data structure. <p> application, but is also of computational interest due to its unstructured and dynamic nature. 4.1 Experimental Results We have ported a FORTRAN 90 version of a tree code to the Convex SPP which was initially developed for the Mas-par MP-2 and implemented using the algorithm described in Olson and Dorband <ref> [13] </ref>. The changes to the original code were straightforward and the compiler directives and the shared memory programming model facilitated a very simple minded approach to be taken.
Reference: [14] <author> Thinking Machines Corporation, </author> <title> "Connection Machine CM-5 Technical Summary," </title> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: This distinguishes the memory system of the SPP-1000 from the distributed memory of the TMC CM-5 <ref> [14] </ref> or the Intel Paragon [10] with their fragmented address spaces. The CRI T3D [7] also has a global shared memory.
Reference: [15] <author> T. Sterling, D. Savarese, P. Merkey, J. Gardner, </author> <title> "An Initial Evaluation of the Convex SPP-1000 for Earth and Space Science Applications," </title> <booktitle> Proceedings of the First International Symposium on High Performance Computing Architecture, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: This paper examines the Convex SPP-1000 to evaluate and quantify the costs incurred in performing critical actions across its two-level cache coherent structure. These results represent the first published findings of evaluation of the SPP-1000's hierarchical cache coherent architecture employing the SCI protocol and builds on earlier studies <ref> [15] </ref> of the performance properties of the first level of this new system. The findings presented in this paper are important to both parallel architecture and compiler designers in determining the feasibility of achieving effective parallel computation in a scalable framework for this class of architecture. <p> Programs requiring fine grain parallelism demand efficient implementations of parallel control mechanisms, while those with coarser requirements are not significantly impacted by more costly implementations. The scaling and temporal costs of basic control constructs on a single hypernode were presented in <ref> [15] </ref>. This section presents the results of some of the same kinds of measurements on a two hypernode system with global shared memory. The experiments expose the limits on the degree of granularity of parallelism achievable on the system, and provide some insight as to its scalability. <p> Last In Last Out: the minimum time measured from when the last thread enters the barrier to when the last thread continues. The results from our earlier study of only one hypernode of the SPP-1000 <ref> [15] </ref> are also shown. Both the previous and current study used the same experimental method. A time-stamp was taken before each thread entered the barrier and after each thread exited the barrier. From this data an approximation of the barrier costs could be derived. <p> Even though their behavior is not linear, if one were to approximate it as such, the average cost per additional thread for the high locality case starting with 2 threads is about 1.5 microseconds and 1.3 microseconds for the uniform case. The single hy-pernode study <ref> [15] </ref> showed an average additional cost of 1 microsecond per thread. 4 Application: The Gravitational N-body Problem The solution of the gravitational N-body problem in Astrophysics is of general interest for a large number of problems ranging from the breakup of comet Shoemaker/Levy 9 to galaxy dynamics to the large scale
Reference: [16] <author> M.S. Warren and J.K. Salmon, </author> <title> "A Parallel Hashed Oct-tree N-Body Algorithm," </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <address> Washington: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
References-found: 16

