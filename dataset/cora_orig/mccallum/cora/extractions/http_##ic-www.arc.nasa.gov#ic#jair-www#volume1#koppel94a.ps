URL: http://ic-www.arc.nasa.gov/ic/jair-www/volume1/koppel94a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/koppel94a.html
Root-URL: 
Email: KOPPEL@BIMACS.CS.BIU.AC.IL  FELDMAN@BIMACS.CS.BIU.AC.IL  SEGRE@CS.CORNELL.EDU  
Title: Bias-Driven Revision of Logical Domain Theories  
Author: Moshe Koppel Ronen Feldman Alberto Maria Segre 
Address: Ramat-Gan, Israel  Ithaca, NY 14853, USA  
Affiliation: Department of Mathematics and Computer Science, Bar-Ilan University,  Department of Computer Science, Cornell University,  
Note: Journal of Artificial Intelligence Research 1 (1994) 159-208 Submitted 8/93; published 2/94  
Abstract: The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the ``ow'' of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair awed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories.
Abstract-found: 1
Intro-found: 1
Reference: <author> Buchanan, B. & Shortliffe, E.H. </author> <year> (1984). </year> <title> Rule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project. </title> <address> Reading, MA: </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: In the case of KBANN, a numerical rule is one which fires if the sum of weights associated with satisfied antecedents exceeds a threshold. In the case of RAPTURE, the rules are probabilistic rules using certainty factors along the lines of MYCIN <ref> (Buchanan & Shortliffe, 1984) </ref>.
Reference: <author> Feldman, R. </author> <year> (1993). </year> <title> Probabilistic Revision of Logical Domain Theories. Ithaca, NY: </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, Cornell University. </institution>
Reference-contexts: We wish to show that PTR scales well to larger, deeper theories. Since deeper, propositional, real-world theories are scarce, we have generated them synthetically. As an added bonus, we now know the target theory so we can perform controlled experiments on bias and radicality. In <ref> (Feldman, 1993) </ref> the aggregate results of experiments performed on a collection of synthetic theories are reported. <p> In order to avoid the dubious practice of averaging results over different theories and in order to highlight significant features of a particular application of PTR, we consider here one synthetic theory typical of those studied in <ref> (Feldman, 1993) </ref>. r A, B L T , p 1 A E, F M Z , p 17 B p 0 N p 0 , p 1 B p 4 , p 11 N p 10 , p 12 C p 2 , K Z p 2 , p 3 , <p> In f act, PTR never requires the processing of more than 8 exemplars to do so. Thus, in this case, the introduction of bias both speeds up the revision process and results in the consistent choice of the optimal revision. Moreover, it h as also been shown in <ref> (Feldman, 1993) </ref> that PTR is robust with respect to random perturbations in the initial weights. <p> Thus, in order to apply PTR to first-order theory revision it is necessary to determine ``optimal'' variable assignments on the basis of which probabilities can be updated. One method for doing so is discussed in <ref> (Feldman, 1993) </ref>. Inductive bias. PTR uses bias to locate awed elements of a theory. Another type of bias can be used to determine which revision to make. <p> Likewise, it might be known that a particular literal is replaceable but not deletable, etc. It has been shown <ref> (Feldman et al., 1993) </ref> that by modifying the inductive component of PTR to account for such bias, both convergence speed and cross-validation accuracy are substantially improved. Noisy exemplars. <p> This contradicts the assumption that E is misclassified by K. Let us now turn to the proof of Theorem C1. We will use the following four lemmas, slight variants of which are proved in <ref> (Feldman, 1993) </ref>.
Reference: <author> Feldman, R., Koppel, M. & Segre, A.M. </author> <month> (August </month> <year> 1993). </year> <title> The Relevance of Bias in the Revision of Approximate Domain Theories. </title> <booktitle> Working Notes of the 1993 IJCAI Workshop on Machine Learning and Knowledge Acquisition: Common Issues, Contrasting Methods, and Integrated Approaches, </booktitle> <pages> 44-60. </pages>
Reference-contexts: We wish to show that PTR scales well to larger, deeper theories. Since deeper, propositional, real-world theories are scarce, we have generated them synthetically. As an added bonus, we now know the target theory so we can perform controlled experiments on bias and radicality. In <ref> (Feldman, 1993) </ref> the aggregate results of experiments performed on a collection of synthetic theories are reported. <p> In order to avoid the dubious practice of averaging results over different theories and in order to highlight significant features of a particular application of PTR, we consider here one synthetic theory typical of those studied in <ref> (Feldman, 1993) </ref>. r A, B L T , p 1 A E, F M Z , p 17 B p 0 N p 0 , p 1 B p 4 , p 11 N p 10 , p 12 C p 2 , K Z p 2 , p 3 , <p> In f act, PTR never requires the processing of more than 8 exemplars to do so. Thus, in this case, the introduction of bias both speeds up the revision process and results in the consistent choice of the optimal revision. Moreover, it h as also been shown in <ref> (Feldman, 1993) </ref> that PTR is robust with respect to random perturbations in the initial weights. <p> Thus, in order to apply PTR to first-order theory revision it is necessary to determine ``optimal'' variable assignments on the basis of which probabilities can be updated. One method for doing so is discussed in <ref> (Feldman, 1993) </ref>. Inductive bias. PTR uses bias to locate awed elements of a theory. Another type of bias can be used to determine which revision to make. <p> Likewise, it might be known that a particular literal is replaceable but not deletable, etc. It has been shown <ref> (Feldman et al., 1993) </ref> that by modifying the inductive component of PTR to account for such bias, both convergence speed and cross-validation accuracy are substantially improved. Noisy exemplars. <p> This contradicts the assumption that E is misclassified by K. Let us now turn to the proof of Theorem C1. We will use the following four lemmas, slight variants of which are proved in <ref> (Feldman, 1993) </ref>.
Reference: <author> Ginsberg, A. </author> <month> (July </month> <year> 1990). </year> <title> Theory Reduction, Theory Revision, </title> <booktitle> and Retranslation. Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 777-782. </pages>
Reference-contexts: Finally, revised domain theories obtained via translation from neural networks tend to be significantly larger than their corresponding original domain theories. 160 BIAS DRIVEN REVISION Other approaches to theory revision which are much less closely related to the approach we will espouse here are RTLS <ref> (Ginsberg, 1990) </ref>, KR-FOCL (Pazzani & Brunk, 1991), and ODYSSEUS (Wilkins, 1988). 1.2. Probabilistic Theory Revision Probabilistic Theory Revision (PTR) is a new approach to theory revision which combines the best features of the two approaches discussed above. <p> unlike EITHER and RAPTURE) it can handle negated literals and non-mutually exclusive multiple roots; it is also strict in terms of the theories it yields in that (like EITHER, but unlike KBANN and RAPTURE) it produces strictly symbolic theories. 12 There are other interesting theory revision algorithms, such as RTLS <ref> (Ginsberg, 1990) </ref>, for which no comparable data is available. 184 BIAS DRIVEN REVISION We hav e noted that both KBANN and RAPTURE output ``numerical'' rules. In the case of KBANN, a numerical rule is one which fires if the sum of weights associated with satisfied antecedents exceeds a threshold.
Reference: <author> Koppel, M., Feldman, R. & Segre, A.M. </author> <month> (December </month> <year> 1993). </year> <title> Theory Revision Using Noisy Exemplars. </title> <booktitle> Proceedings of the Tenth Israeli Symposium on Artificial Intelligence and Computer Vision, </booktitle> <pages> 96-107. </pages>
Reference-contexts: Thus, if u E (e r ) is high, then E is probably IN regardless of what we are told; analogously, if u E (e r ) is low. A modified version of PTR based on this observation has already been successfully implemented <ref> (Koppel et al., 1993) </ref>. In conclusion, we believe the PTR system marks an important contribution to the domain theory revision problem.
Reference: <author> Mahoney, J. & M ooney, R. </author> ( <year> 1993). </year> <title> Combining Connectionist and Symbolic Learning to Refine Certainty-Factor Rule-Bases. </title> <journal> Connection Science, </journal> <volume> 5, </volume> <pages> 339-364. </pages>
Reference-contexts: Some of the rules in the theory output by KBANN might be numerical, i.e., not strictly symbolic. (4) RAPTURE <ref> (Mahoney & Mooney, 1993) </ref> uses a variant of backpropagation to adjust certainty factors in a probabilistic domain theory. If necessary, it can also add a clause to a root. All the rules produced by RAPTURE are numerical. <p> Results for EITHER, RAPTURE, and KBANN are taken from <ref> (Mahoney & Mooney, 1993) </ref>, while results for ID3 and PTR were generated using similar experimental procedures. <p> These numbers would grow substantially if the theory were converted into strictly symbolic terms. RAPTURE, on the other hand, does not change the theory size, but, like KBANN, yields numerical rules <ref> (Mahoney & Mooney, 1993) </ref>. 6.2.3. Complexity EITHER is exponential in the size of the theory and the number of training examples.
Reference: <author> Murphy, P.M. & Aha, D.W. </author> ( <year> 1992). </year> <title> UCI Repository of Machine Learning Databases [Machine-readable data repository]. </title> <address> Irvine, CA: </address> <institution> Department of Information and Computer Science, University of California at Irvine. </institution>
Reference-contexts: Results on the PROMOTER Theory We first consider the PROMOTER theory from molecular biology <ref> (Murphy & A ha, 1992) </ref>, which is of interest solely because it has been extensively studied in the theory revision literature (Towell & Shavlik, 1993), thus enabling explicit performance comparison with other algorithms. The PROMOTER theory is a awed theory intended to recognize promoters in DNA nucleotides.
Reference: <author> Ourston, D. </author> <month> (August </month> <year> 1991). </year> <title> Using Explanation-Based and Empirical Methods in Theory Revision. </title> <institution> Austin, TX: </institution> <type> Ph.D. Thesis, </type> <institution> University of Texas at Austin. </institution>
Reference-contexts: Training Mean Mean Mean Mean Set Size Clauses in Literals in Revisions to Exemplars to Output Output Convergence Convergence Original Theory 14 83 20 11 39 10.7 88 60 11 35 18.2 186 100 12 36 22.0 236 over one hundred trials (ten trials for each of ten example partitions). <ref> (Ourston, 1991) </ref>. For example, for 20 training examples the output theory size (clauses plus literals) is 78, while for 80 training examples, the output theory size is 106. Unfortunately, making direct comparisons with KBANN or RAPTURE is difficult.
Reference: <author> Ourston, D. & Mooney, R. </author> ( <title> in press). Theory Refinement Combining Analytical and Empirical Methods. </title> <journal> Artificial Intelligence. </journal>
Reference: <author> Pazzani, M. & Brunk, C. </author> <month> (June </month> <year> 1991). </year> <title> Detecting and Correcting Errors in Rule-Based Expert Systems: An Integration of Empirical and Explanation-Based Learning. </title> <journal> Knowledge Acquisition, </journal> <volume> 3(2), </volume> <pages> 157-173. </pages>
Reference-contexts: Finally, revised domain theories obtained via translation from neural networks tend to be significantly larger than their corresponding original domain theories. 160 BIAS DRIVEN REVISION Other approaches to theory revision which are much less closely related to the approach we will espouse here are RTLS (Ginsberg, 1990), KR-FOCL <ref> (Pazzani & Brunk, 1991) </ref>, and ODYSSEUS (Wilkins, 1988). 1.2. Probabilistic Theory Revision Probabilistic Theory Revision (PTR) is a new approach to theory revision which combines the best features of the two approaches discussed above.
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This lemma states that N E (e) and E , Q (E ) are conditionally independent given N E ( f (e)) <ref> (Pearl, 1988) </ref>. That is, once N E ( f (e)) is known, E, Q (E) adds no information regarding N E (e).
Reference: <author> Quinlan, J.R. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <pages> 81-106. </pages>
Reference-contexts: If n e is a literal, then t is interpreted as the clause n e l. If n e is a clause, 10 Any standard algorithm for constructing decision trees from positive and negative examples can be used. Our implementation of PTR uses ID3 <ref> (Quinlan, 1986) </ref>. <p> Comparison with other Methods In order to put our results in perspective we c ompare them with results obtained by other methods. 12 (1) ID3 <ref> (Quinlan, 1986) </ref> is the inductive component we use in PTR. Thus using ID3 is equivalent to learning directly from the examples without using the initial awed domain theory.
Reference: <author> To well, G.G. & Shavlik, J.W. </author> <month> (October </month> <year> 1993). </year> <title> Extracting Refined Rules From Knowledge-Based Neural Networks. </title> <journal> Machine Learning, </journal> <volume> 13(1), </volume> <pages> 71-102. </pages>
Reference-contexts: These problems suggest that it would be worthwhile to circumvent proof enumeration by employing incremental numerical schemes for focusing blame on specific elements. A completely different approach to the revision problem is based on the use of neural networks <ref> (KBANN, Towell & Shavlik, 1993) </ref>. The idea is to transform the original domain theory into network form, assigning weights in the graph according to some pre-established scheme. The connection weights are then adjusted in accordance with the observed examples using standard neural-network backpropagation techniques. <p> Repairs are then made using an inductive component. EITHER is exponential in the size of the theory. It cannot handle theories with negated internal literals. It also cannot handle theories with multiple roots unless those roots are mutually exclusive. (3) KBANN <ref> (Towell & Shavlik, 1993) </ref> translates a symbolic domain theory into a neural net, uses backpropagation to adjust the weights of the net's edges, and then translates back from net form to partially symbolic form. <p> Results on the PROMOTER Theory We first consider the PROMOTER theory from molecular biology (Murphy & A ha, 1992), which is of interest solely because it has been extensively studied in the theory revision literature <ref> (Towell & Shavlik, 1993) </ref>, thus enabling explicit performance comparison with other algorithms. The PROMOTER theory is a awed theory intended to recognize promoters in DNA nucleotides. <p> Nevertheless, it is clear that, as expected, KBANN produces significantly larger theories than PTR. For example, using 90 training examples from the PROMOTER theory, KBANN produces numerical theories with, on av erage, 10 clauses and 102 literals <ref> (Towell & Shavlik, 1993) </ref>. These numbers would grow substantially if the theory were converted into strictly symbolic terms. RAPTURE, on the other hand, does not change the theory size, but, like KBANN, yields numerical rules (Mahoney & Mooney, 1993). 6.2.3.
Reference: <author> Wilkins, D.C. </author> <month> (July </month> <year> 1988). </year> <title> Knowledge Base Refinement Using Apprenticeship Learning Techniques. </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 646-653. </pages>
Reference-contexts: obtained via translation from neural networks tend to be significantly larger than their corresponding original domain theories. 160 BIAS DRIVEN REVISION Other approaches to theory revision which are much less closely related to the approach we will espouse here are RTLS (Ginsberg, 1990), KR-FOCL (Pazzani & Brunk, 1991), and ODYSSEUS <ref> (Wilkins, 1988) </ref>. 1.2. Probabilistic Theory Revision Probabilistic Theory Revision (PTR) is a new approach to theory revision which combines the best features of the two approaches discussed above.
Reference: <author> Wogulis, J. & Pazzani, M.J. </author> <month> (August </month> <year> 1993). </year> <title> A Methodology for Evaluating Theory Revision Systems: Results with Audrey II. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1128-1134. 208 </pages>
Reference-contexts: We wish to point out, however, that even i n this case our definition of ``syntactic change'' differs from some previous definitions <ref> (Wogulis & Pazzani, 1993) </ref>. Whereas those definitions count the number of deleted and added edges, we count the number of edges deleted or added to.
References-found: 15

