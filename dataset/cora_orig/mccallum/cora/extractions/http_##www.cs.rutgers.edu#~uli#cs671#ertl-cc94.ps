URL: http://www.cs.rutgers.edu/~uli/cs671/ertl-cc94.ps
Refering-URL: http://www.cs.rutgers.edu/~uli/cs671/index.html
Root-URL: http://www.cs.rutgers.edu
Title: Instructions  
Author: M. Anton Ertl Andreas Krall 
Keyword: Key Words: instruction-level parallelism, superscalar, speculative execution, exception, software pipelining  
Address: Argentinierstrae 8, A-1040 Wien  
Affiliation: Institut fur Computersprachen Technische Universitat Wien  
Note: Trapping  
Email: fanton,andig@mips.complang.tuwien.ac.at  
Phone: Tel.: (+43-1) 58801 f4459,4462g  
Abstract: Paper and BibTeX entry are available at http://www.complang.tuwien.ac.at/papers/. This paper was published in: Compiler Construction (CC '94), Springer LNCS 786, 1994, pages 158-171 Delayed Exceptions | Speculative Execution of Abstract. Superscalar processors, which execute basic blocks sequentially, cannot use much instruction level parallelism. Speculative execution has been proposed to execute basic blocks in parallel. A pure software approach suffers from low performance, because exception-generating instructions cannot be executed speculatively. We propose delayed exceptions, a combination of hardware and compiler extensions that can provide high performance and correct exception handling in compiler-based speculative execution. Delayed exceptions exploit the fact that exceptions are rare. The compiler assumes the typical case (no exceptions), schedules the code accordingly, and inserts run-time checks and fix-up code that ensure correct execution when exceptions do happen.
Abstract-found: 1
Intro-found: 1
Reference: [AL91] <author> Andrew W. Appel and Kai Li. </author> <title> Virtual memory primitives for user programs. </title> <booktitle> In ASPLOS-IV [ASP91], </booktitle> <pages> pages 96-107. </pages>
Reference-contexts: Fig. 9. Instruction-level parallelism with and without delayed exceptions access, a garbage value is returned. The justification for this behaviour is that correct programs do not trap. Unfortunately this justification is wrong. Exceptions are used in many applications. <ref> [AL91] </ref> lists several applications of memory access exceptions. Besides, in our opinion the assumption of completely correct programs is unrealistic. Delayed exceptions solve the problem instead of ignoring it. Speculative loads [RL92] note the exception in a bit associated with the result register.
Reference: [ASP91] <institution> Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </institution> <year> 1991. </year>
Reference: [ASP92] <institution> Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </institution> <year> 1992. </year>
Reference: [BMH + 93] <author> Roger A. Bringman, Scott A. Mahlke, Richard E. Hank, John C. Gyllenhaal, and Wen-mei W. Hwu. </author> <title> Speculative execution exception recovery using write-back suppression. </title> <booktitle> In 26th Annual International Symposium on Microarchitec-ture (MICRO-26), </booktitle> <pages> pages 214-223, </pages> <year> 1993. </year>
Reference-contexts: Delayed exceptions use more instruction bandwidth (for exception-checking branches) and they produce more code (fix-up code). Write-back suppression <ref> [BMH + 93] </ref> is specific to superblock 6 scheduling. It uses hardware to suppress the register file updates by the excepting instruction and all subsequent instructions that have the same or higher speculation distance.
Reference: [BYP + 91] <author> Michael Butler, Tse-Yu Yeh, Yale Patt, Mitch Alsup, Hunter Scales, and Michael Shebanow. </author> <title> Single instruction stream parallelism is greater than two. </title> <booktitle> In ISCA-18 [ISC91], </booktitle> <pages> pages 276-286. </pages>
Reference-contexts: having bx instructions that test several notes and branch to a combined piece of fix-up code. 4 These are the same compiler techniques that ensure correct processing of delayed excep tions (see Section 4.3). 6 Potential Speedup To evaluate the potential benefit of delayed exceptions, we performed a trace-driven simulation <ref> [BYP + 91, Wal91, LW92] </ref>. To get an upper bound, we assumed infinite resources (instruction bandwidth, functional units), perfect register renaming, perfect alias detection, and perfect branch prediction. To have a few machine models between the extremes, we restricted perfect branch prediction, to predict only the next n branches.
Reference: [CMC + 91] <author> Pohua P. Chang, Scott A. Mahlke, William Y. Chen, Nancy J. Warter, and Wen-mei W. Hwu. </author> <title> IMPACT: An architectural framework for multiple-instruction-issue processors. </title> <booktitle> In ISCA-18 [ISC91], </booktitle> <pages> pages 266-275. </pages>
Reference-contexts: 1 Introduction Computer designers and computer architects have been striving to improve uniprocessor performance since the invention of computers <ref> [JW89, CMC + 91] </ref>. The next step in this quest for higher performance is the exploitation of significant amounts of instruction-level parallelism. To this end superscalar, superpipelined, and VLIW processors 1 can execute several instructions in parallel. The obstacle to using these resources is the dependences between the instructions. <p> If a different path is executed, the note has no effect. This is an advantage over the approach proposed for general percolation <ref> [CMC + 91] </ref>, where speculative page faults are always serviced, even if they are not needed. 5.6 Instruction Issue Bandwidth In the programs we measured (see Section 6) 16%-24% of the dynamically executed instructions can generate exceptions. <p> They report an average speedup of 32% for numeric benchmarks. Delayed exceptions should give similar results. 7 Related Work Ignoring exceptions by using non-trapping instructions has been proposed for circumventing the problem <ref> [CMC + 91] </ref>. Instead of trapping on e.g. an illegal memory 5 A compiler could use control-dependence analysis to move instructions up across branches in a non-speculative way.
Reference: [EK92] <author> M. Anton Ertl and Andreas Krall. </author> <title> Removing antidependences by repairing. </title> <type> Bericht TR 1851-1992-9, </type> <institution> Institut fur Computersprachen, Technische Universitat Wien, </institution> <year> 1992. </year>
Reference-contexts: The code motions have to take the data dependences into account, so the source registers of the operations in the fix-up code are preserved, since they are live until the fix-up code. Transformations like register renaming or repairing <ref> [EK92] </ref> can be used to remove these dependences and enable further moves.
Reference: [Ell85] <author> John R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: A less expensive approach relies on compiler techniques for global instruction scheduling like trace scheduling, software pipelining and percolation scheduling <ref> [RG81, Fis81, Ell85, Nic85] </ref>. Exceptions pose serious problems to compiler-only approaches: The compiler must not move exception-generating instructions up across conditional branches (unless the instruction appears in all directions that the branch can take). E.g., a load that is moved up across its guardian NULL pointer test will trap wrongly. <p> Guidance rules decide when and where to apply the transformations. In this framework, the exception delaying transformation described below is an enabling transformation. Delayed exceptions can also be fitted into other global scheduling models. E.g., in the context of trace scheduling <ref> [Fis81, Ell85] </ref>, exception-generating instructions would be moved around freely; the fix-up code is inserted by the book-keeping process. 4.2 The Exception Delaying Transformation The basic transformation used in delayed exceptions is shown in Fig. 4.
Reference: [Fis81] <author> Joseph A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 30(7) </volume> <pages> 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: A less expensive approach relies on compiler techniques for global instruction scheduling like trace scheduling, software pipelining and percolation scheduling <ref> [RG81, Fis81, Ell85, Nic85] </ref>. Exceptions pose serious problems to compiler-only approaches: The compiler must not move exception-generating instructions up across conditional branches (unless the instruction appears in all directions that the branch can take). E.g., a load that is moved up across its guardian NULL pointer test will trap wrongly. <p> Guidance rules decide when and where to apply the transformations. In this framework, the exception delaying transformation described below is an enabling transformation. Delayed exceptions can also be fitted into other global scheduling models. E.g., in the context of trace scheduling <ref> [Fis81, Ell85] </ref>, exception-generating instructions would be moved around freely; the fix-up code is inserted by the book-keeping process. 4.2 The Exception Delaying Transformation The basic transformation used in delayed exceptions is shown in Fig. 4.
Reference: [HP87] <author> Wen-mei Hwu and Yale N. Patt. </author> <title> Checkpoint repair for high-performance out-of-order execution machines. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12) </volume> <pages> 1496-1514, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: This can be implemented in hardware through backup register files, history buffers or reservation 1 For simplicity, we will use the term "superscalar" in the rest of the paper, but delayed exceptions can be used with any of the techniques for exploiting instruction-level parallelism. stations <ref> [Tom67, HP87, SP88, Soh90] </ref>. A less expensive approach relies on compiler techniques for global instruction scheduling like trace scheduling, software pipelining and percolation scheduling [RG81, Fis81, Ell85, Nic85]. <p> Therefore, the processor cannot be simply restarted from the faulting instructions. These imprecise exceptions have several disadvantages, e.g. they make exception handlers implementation-dependent. In order to implement precise interrupts, many expensive hardware schemes for restoring the processor state have been proposed <ref> [SP88, HP87] </ref>. Delayed exceptions open the road to precise exceptions without any backup hardware: The exception delaying transformation must be applied to every instruction in the program that can cause imprecise exceptions.
Reference: [ISC91] <institution> The 18 th Annual International Symposium on Computer Architecture (ISCA), Toronto, </institution> <year> 1991. </year>
Reference: [JW89] <author> Norman P. Jouppi and David W. Wall. </author> <title> Available instruction-level parallelism for superscalar and superpipelined machines. </title> <booktitle> In Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), </booktitle> <pages> pages 272-282, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Computer designers and computer architects have been striving to improve uniprocessor performance since the invention of computers <ref> [JW89, CMC + 91] </ref>. The next step in this quest for higher performance is the exploitation of significant amounts of instruction-level parallelism. To this end superscalar, superpipelined, and VLIW processors 1 can execute several instructions in parallel. The obstacle to using these resources is the dependences between the instructions. <p> The obstacle to using these resources is the dependences between the instructions. Scheduling (code reordering) has been employed to reduce the impact of dependences. However, the average instruction-level parallelism available within basic blocks is less than two simultaneous instruction executions <ref> [JW89] </ref>. To circumvent this barrier several methods have been developed to execute basic blocks in parallel. They are based on speculative execution, i.e. the processor executes instructions from possible, but not certain future execution paths.
Reference: [LW92] <author> Monica S. Lam and Robert P. Wilson. </author> <title> Limits of control flow on parallelism. </title> <booktitle> In The 19 th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <pages> pages 46-57, </pages> <year> 1992. </year>
Reference-contexts: having bx instructions that test several notes and branch to a combined piece of fix-up code. 4 These are the same compiler techniques that ensure correct processing of delayed excep tions (see Section 4.3). 6 Potential Speedup To evaluate the potential benefit of delayed exceptions, we performed a trace-driven simulation <ref> [BYP + 91, Wal91, LW92] </ref>. To get an upper bound, we assumed infinite resources (instruction bandwidth, functional units), perfect register renaming, perfect alias detection, and perfect branch prediction. To have a few machine models between the extremes, we restricted perfect branch prediction, to predict only the next n branches. <p> If our simulation took this into account (i.e. did not count those branches), it would result in a somewhat higher instruction-level parallelism for all models but the perfect model. Due to the data given in <ref> [LW92] </ref> and Amdahl's Law we believe that the effect of this optimization would not be very large and would not change our conclusions. Fig. 9. Instruction-level parallelism with and without delayed exceptions access, a garbage value is returned. The justification for this behaviour is that correct programs do not trap.
Reference: [MCH + 92] <author> Scott A. Mahlke, William Y. Chen, Wen-mei W. Hwu, B. Ramakrishna Rau, and Michael S. Schlansker. </author> <title> Sentinel scheduling for VLIW and superscalar processors. </title> <booktitle> In ASPLOS-V [ASP92], </booktitle> <pages> pages 238-247. </pages>
Reference-contexts: The improvement on a realistic machine with a real scheduling algorithm is of course somewhat lower, but still impressive: Mahlke et al. report a speedup of 18%- 135% (average 57%) for sentinel scheduling (see Section 7) on non-numeric programs for a superscalar processor of degree 8 <ref> [MCH + 92] </ref>. They report an average speedup of 32% for numeric benchmarks. Delayed exceptions should give similar results. 7 Related Work Ignoring exceptions by using non-trapping instructions has been proposed for circumventing the problem [CMC + 91]. <p> In contrast to delayed exceptions, TORCH uses hardware to restore the state before the exception and then executes the recovery code. TORCHs recovery code contains all speculative instructions since the exception, while our fix-up code contains only instructions that dependent on the trap-noting instruction. Sentinel scheduling <ref> [MCH + 92] </ref> uses a bit in every instruction that says whether the instruction is executed speculatively. Speculative instructions set and propagate exception bits; the first non-speculative instruction triggers the trap (if the bit is set).
Reference: [NE89] <author> Toshio Nakatani and Kemal Ebcioglu. </author> <title> "Combining" as a compilation technique for VLIW architectures. </title> <booktitle> In 22 nd Annual International Workshop on Microprogramming and Microarchitecture (MICRO-22), </booktitle> <pages> pages 43-55, </pages> <year> 1989. </year>
Reference-contexts: Before applying other transformations, the only register to be recomputed is the result register of the instruction. These functions are performed by the trapping version of the instruction 3 . 2 We did not exploit more parallelism (e.g. by combining <ref> [NE89] </ref> the addus or by speculating in both directions) in order to keep the example simple. 3 Of course, the transformation should not be reapplied to the trapping instruction in the fix-up code. Fig. 4.
Reference: [Nic85] <author> Alexandru Nicolau. </author> <title> Uniform parallelism exploitation in ordinary programs. </title> <booktitle> In 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 614-618, </pages> <year> 1985. </year>
Reference-contexts: A less expensive approach relies on compiler techniques for global instruction scheduling like trace scheduling, software pipelining and percolation scheduling <ref> [RG81, Fis81, Ell85, Nic85] </ref>. Exceptions pose serious problems to compiler-only approaches: The compiler must not move exception-generating instructions up across conditional branches (unless the instruction appears in all directions that the branch can take). E.g., a load that is moved up across its guardian NULL pointer test will trap wrongly. <p> We could have renamed registers to save the old value of s for the off-loop execution paths and for the fix-up code. Instead, our code repairs s by subtracting 3. 4 The Compiler Technique 4.1 The Percolation Scheduling Framework Percolation scheduling is a general framework for global instruction scheduling <ref> [Nic85] </ref>. It contains a few core transformations for moving instructions. Enabling transformations (e.g. register renaming) give the core transformations greater freedom to move the code. Guidance rules decide when and where to apply the transformations. In this framework, the exception delaying transformation described below is an enabling transformation.
Reference: [Nic89] <author> Alexandru Nicolau. </author> <title> Run-time disambiguation: Coping with statically unpredictable dependencies. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(5) </volume> <pages> 663-678, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The two remedies used in the context of run-time disambiguation <ref> [Nic89] </ref> should work well for delayed exceptions, too: applying delayed exceptions only to frequently used program parts; and using one piece of fix-up code for several trapping instructions. Finally, the cache and paging behaviour can be improved by moving the fix-up code out-of-line into extra pages. <p> Stores alter memory, so they can hardly be undone. Therefore exceptions are not the only obstacle to speculative execution of stores. Fortunately there is no need to execute stores speculatively, since dependences on the store can be eliminated by run-time disambiguation <ref> [Nic89] </ref> and register renaming. Therefore a trap-noting store is neither necessary nor sensible. Arithmetic exceptions can be generated by many floating-point and integer instructions. Trap-noting versions of these instructions are needed. Instructions that also have a non-trapping version (e.g. add (trapping) and addu (non-trapping)) are a special case.
Reference: [RG81] <author> B. R. Rau and C. D. Glaeser. </author> <title> Some scheduling techgniques and an easily schedulable horizontal architecture for high performance scientific computing. </title> <booktitle> In 14th Annual Microprogramming Workshop (MICRO-14), </booktitle> <pages> pages 183-198, </pages> <year> 1981. </year>
Reference-contexts: A less expensive approach relies on compiler techniques for global instruction scheduling like trace scheduling, software pipelining and percolation scheduling <ref> [RG81, Fis81, Ell85, Nic85] </ref>. Exceptions pose serious problems to compiler-only approaches: The compiler must not move exception-generating instructions up across conditional branches (unless the instruction appears in all directions that the branch can take). E.g., a load that is moved up across its guardian NULL pointer test will trap wrongly.
Reference: [RL92] <author> Anne Rogers and Kai Li. </author> <title> Software support for speculative loads. </title> <booktitle> In ASPLOS-V [ASP92], </booktitle> <pages> pages 38-50. </pages>
Reference-contexts: Unfortunately this justification is wrong. Exceptions are used in many applications. [AL91] lists several applications of memory access exceptions. Besides, in our opinion the assumption of completely correct programs is unrealistic. Delayed exceptions solve the problem instead of ignoring it. Speculative loads <ref> [RL92] </ref> note the exception in a bit associated with the result register. The first instruction that uses the result triggers the trap. This means that the load can be executed speculatively, but not its use. In contrast, delayed exceptions permit arbitrary chains of speculative operations.
Reference: [SHL92] <author> Michael D. Smith, Mark Horowitz, and Monica S. Lam. </author> <title> Efficient superscalar performance through boosting. </title> <booktitle> In ASPLOS-V [ASP92], </booktitle> <pages> pages 248-259. </pages>
Reference-contexts: The first instruction that uses the result triggers the trap. This means that the load can be executed speculatively, but not its use. In contrast, delayed exceptions permit arbitrary chains of speculative operations. The TORCH architecture <ref> [SLH90, SHL92, Smi92] </ref> uses programmer-visible shadow register files for compiler-based speculative execution. TORCH as described in [SLH90] can handle exceptions using a reexecution scheme implemented in hardware. In the meantime they have switched to using compiler-generated recovery code.
Reference: [SLH90] <author> Michael D. Smith, Monica S. Lam, and Mark A. Horowitz. </author> <title> Boosting beyond static scheduling in a superscalar processor. </title> <booktitle> In The 17 th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <pages> pages 344-354, </pages> <year> 1990. </year>
Reference-contexts: The first instruction that uses the result triggers the trap. This means that the load can be executed speculatively, but not its use. In contrast, delayed exceptions permit arbitrary chains of speculative operations. The TORCH architecture <ref> [SLH90, SHL92, Smi92] </ref> uses programmer-visible shadow register files for compiler-based speculative execution. TORCH as described in [SLH90] can handle exceptions using a reexecution scheme implemented in hardware. In the meantime they have switched to using compiler-generated recovery code. <p> This means that the load can be executed speculatively, but not its use. In contrast, delayed exceptions permit arbitrary chains of speculative operations. The TORCH architecture [SLH90, SHL92, Smi92] uses programmer-visible shadow register files for compiler-based speculative execution. TORCH as described in <ref> [SLH90] </ref> can handle exceptions using a reexecution scheme implemented in hardware. In the meantime they have switched to using compiler-generated recovery code. In contrast to delayed exceptions, TORCH uses hardware to restore the state before the exception and then executes the recovery code.
Reference: [Smi92] <author> Michael David Smith. </author> <title> Support for Speculative Execution in High-Performance Processors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: The first instruction that uses the result triggers the trap. This means that the load can be executed speculatively, but not its use. In contrast, delayed exceptions permit arbitrary chains of speculative operations. The TORCH architecture <ref> [SLH90, SHL92, Smi92] </ref> uses programmer-visible shadow register files for compiler-based speculative execution. TORCH as described in [SLH90] can handle exceptions using a reexecution scheme implemented in hardware. In the meantime they have switched to using compiler-generated recovery code.
Reference: [Soh90] <author> Gurindar S. Sohi. </author> <title> Instruction issue logic for high-performance, interruptable, multiple functional unit, pipelined processors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(3) </volume> <pages> 349-359, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: This can be implemented in hardware through backup register files, history buffers or reservation 1 For simplicity, we will use the term "superscalar" in the rest of the paper, but delayed exceptions can be used with any of the techniques for exploiting instruction-level parallelism. stations <ref> [Tom67, HP87, SP88, Soh90] </ref>. A less expensive approach relies on compiler techniques for global instruction scheduling like trace scheduling, software pipelining and percolation scheduling [RG81, Fis81, Ell85, Nic85].
Reference: [SP88] <author> James E. Smith and Andrew R. Pleszkun. </author> <title> Implementing precise interrupts in pipelined processors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(5) </volume> <pages> 562-573, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: This can be implemented in hardware through backup register files, history buffers or reservation 1 For simplicity, we will use the term "superscalar" in the rest of the paper, but delayed exceptions can be used with any of the techniques for exploiting instruction-level parallelism. stations <ref> [Tom67, HP87, SP88, Soh90] </ref>. A less expensive approach relies on compiler techniques for global instruction scheduling like trace scheduling, software pipelining and percolation scheduling [RG81, Fis81, Ell85, Nic85]. <p> Therefore, the processor cannot be simply restarted from the faulting instructions. These imprecise exceptions have several disadvantages, e.g. they make exception handlers implementation-dependent. In order to implement precise interrupts, many expensive hardware schemes for restoring the processor state have been proposed <ref> [SP88, HP87] </ref>. Delayed exceptions open the road to precise exceptions without any backup hardware: The exception delaying transformation must be applied to every instruction in the program that can cause imprecise exceptions.
Reference: [Tom67] <author> R. M. Tomasulo. </author> <title> An efficient algorithm for exploiting multiple arithmetic units. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 11(1) </volume> <pages> 25-33, </pages> <year> 1967. </year>
Reference-contexts: This can be implemented in hardware through backup register files, history buffers or reservation 1 For simplicity, we will use the term "superscalar" in the rest of the paper, but delayed exceptions can be used with any of the techniques for exploiting instruction-level parallelism. stations <ref> [Tom67, HP87, SP88, Soh90] </ref>. A less expensive approach relies on compiler techniques for global instruction scheduling like trace scheduling, software pipelining and percolation scheduling [RG81, Fis81, Ell85, Nic85].
Reference: [Wal91] <author> David W. Wall. </author> <title> Limits of instruction-level parallelism. In ASPLOS-IV [ASP91], pages 176-188. This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: having bx instructions that test several notes and branch to a combined piece of fix-up code. 4 These are the same compiler techniques that ensure correct processing of delayed excep tions (see Section 4.3). 6 Potential Speedup To evaluate the potential benefit of delayed exceptions, we performed a trace-driven simulation <ref> [BYP + 91, Wal91, LW92] </ref>. To get an upper bound, we assumed infinite resources (instruction bandwidth, functional units), perfect register renaming, perfect alias detection, and perfect branch prediction. To have a few machine models between the extremes, we restricted perfect branch prediction, to predict only the next n branches.
References-found: 26

