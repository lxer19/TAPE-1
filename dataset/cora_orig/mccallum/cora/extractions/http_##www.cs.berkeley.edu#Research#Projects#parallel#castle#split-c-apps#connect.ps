URL: http://www.cs.berkeley.edu/Research/Projects/parallel/castle/split-c-apps/connect.ps
Refering-URL: http://www.cs.berkeley.edu/Research/Projects/parallel/castle/split-c-apps/connect.html
Root-URL: http://www.cs.berkeley.edu
Title: Towards Modeling the Performance of a Fast Connected Components Algorithm on Parallel Machines  
Author: Steven S. Lumetta, Arvind Krishnamurthy, and David E. Culler 
Date: April 4, 1996  
Affiliation: Computer Science Division University of California, Berkeley  
Abstract: We present and analyze a portable, high-performance algorithm for finding connected components on modern distributed memory multiprocessors. The algorithm is a hybrid of the classic DFS on the subgraph local to each processor and a variant of the Shiloach-Vishkin PRAM algorithm on the global collection of subgraphs. We implement the algorithm in Split-C and measure performance on the the Cray T3D, the Meiko CS-2, and the Thinking Machines CM-5 using a class of graphs derived from cluster dynamics methods in computational physics. On a 256 processor Cray T3D, the implementation outperforms all previous solutions by an order of magnitude. A characterization of graph parameters allows us to select graphs that highlight key performance features. We study the effects of these parameters and machine characteristics on the balance of time between the local and global phases of the algorithm and find that edge density, surface-to-volume ratio, and relative communication cost dominate performance. By understanding the effect of machine characteristics on performance, the study sheds light on the impact of improvements in computational and/or communication performance on this challenging problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, D. E. Culler, D. Patterson, </author> <title> "A Case for NOW," </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995, </year> <pages> pp. 54-64. </pages>
Reference-contexts: The model explains observed speedup on our three platforms and outlines the possibilities for less tightly integrated systems, where greater computational performance is obtained by sacrificing communication performance <ref> [1] </ref>. Finally, the modeling process serves as a case study to aid in the understanding of other algorithms.
Reference: [2] <author> R. H. Arpaci, D. E. Culler, A. Krishnamurthy, S. Steinberg, K. Yelick, </author> <title> "Empirical Evaluation of the Cray T3D: A Compiler Perspective," </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: Split-C also gives our implementation portability, with versions running on the Cray T3D, the IBM SP-1 and SP-2, the Intel Paragon, the Thinking Machines CM-5, the Meiko CS-2, and networks of workstations <ref> [2, 24, 27, 34] </ref>. 2.3 Parallel platforms We consider three large-scale parallel machines: the Cray T3D, the Meiko CS-2, and the Thinking Machines CM-5. These machines offer a range of computational and communication performance against which to evaluate the algorithm implementation. <p> A Split-C global read involves a short instruction sequence to gain addressability to the remote node and to load from remote memory, taking approximately 1 s <ref> [2] </ref>. The CS-2 uses a 90 MHz, dual-issue Sparc microprocessor with a large cache. A dedicated "ELAN" processor within the network interface supports communication, and is capable of accessing remote memory via word-by-word or DMA network transactions.
Reference: [3] <author> B. Awerbuch, Y. Shiloach, </author> <title> "New Connectivity and MSF Algorithms for Ultracomputer and PRAM," </title> <booktitle> International Conference on Parallel Processing, </booktitle> <year> 1983, </year> <pages> pp. 175-179. </pages>
Reference-contexts: Sequential solutions are well understood and commonly used as an application of depth-first and breadth-first search. Parallel solutions have received a great deal of attention from theorists, and have proven difficult. Algorithms such as Shiloach-Vishkin obtain good results with the CRCW PRAM model <ref> [3, 11, 12, 28] </ref>, which assumes uniform memory access time and fl Copyright 1995 by the Association for Computing Machinery, Inc. (ACM).
Reference: [4] <author> M. Aydin, M. C. </author> <title> Yalabik, </title> <journal> Journal of Physics A 17, </journal> <volume> 2531, </volume> <year> 1984. </year>
Reference-contexts: The quality of statistics returned from these simulations depends heavily on the size of the sample, L. Although a larger sample size leads to better statistics, the number of steps needed to propagate information generally grows as O (L 2 ) or worse using traditional methods <ref> [4, 19, 25, 32, 33] </ref>. More recent methods, such as the Swendsen-Wang algorithm 4 T3D/256 CS-2/32 CM-5/256 C90/1 Graph (Greiner) Type (Mn/s) (Mn/s) (Mn/s) (Mn/s) 2D30 - 4.3 2D60/200 83.6 3.31 12.1 2.5 3D40/30 38.6 0.79 2.38 2.1 Table 2: Raw performance of the parallel connected components algorithm.
Reference: [5] <author> D. A. Bader, J. JaJa, </author> <title> "Parallel Algorithms for Image Histogramming and Connected Components with and Experimental Study," </title> <institution> University of Maryland Institute for Advanced Computer Science Technical Report #UMIACS-TR-94-133. </institution>
Reference-contexts: Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) [7, 10, 13, 14, 21, 26] and vector machines [9, 26]. With the exception of <ref> [5] </ref>, which focuses on 2D graphs for robot vision, these solutions typically emphasize performance over portability, scalability, and generality, but still rarely obtain good performance. We present a fast, portable, general-purpose algorithm for finding connected components on a distributed memory machine.
Reference: [6] <author> K. W. Chong, T. W. Lam, </author> <title> "Finding Connected Components in O(log n log log n) Time on EREW PRAM," </title> <booktitle> 4th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1993, </year> <pages> pp. 11-20. </pages>
Reference-contexts: Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of any organization. 1 arbitrary bandwidth to any memory location, but the inherent contention in the algorithm makes even EREW solutions much more challenging <ref> [6, 15, 18, 20] </ref>. Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23].
Reference: [7] <author> A. Choudhary, R. Thakur, </author> <title> "Connected Component Labeling on Coarse Grain Parallel Computers: An Experimental Study," </title> <journal> Journal of Parallel and Distributed Computing 20, </journal> <year> 1994, </year> <pages> pp. 78-83. </pages>
Reference-contexts: Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23]. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) <ref> [7, 10, 13, 14, 21, 26] </ref> and vector machines [9, 26]. With the exception of [5], which focuses on 2D graphs for robot vision, these solutions typically emphasize performance over portability, scalability, and generality, but still rarely obtain good performance.
Reference: [8] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, K. Yelick, </author> <title> "Parallel Programming in Split-C," </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993, </year> <pages> pp. 262-273. </pages>
Reference-contexts: With the exception of [5], which focuses on 2D graphs for robot vision, these solutions typically emphasize performance over portability, scalability, and generality, but still rarely obtain good performance. We present a fast, portable, general-purpose algorithm for finding connected components on a distributed memory machine. Implemented in Split-C <ref> [8] </ref>, the algorithm is a hybrid of the classic depth-first search on the subgraph local to each processor and a variant of the Shiloach-Vishkin PRAM algorithm on the global collection of subgraphs. On a 256 processor T3D, our implementation performs an order of magnitude better than any previous solution. <p> Edge-List Concatenation Step. For all leaf nodes of components, move remaining edges to the root. 4. Local Cleanup. For each node not selected as a representative node in the global graph, update the value of the node from its representative. 2.2 Programming language Split-C <ref> [8] </ref> provides a cost model that mirrors the basic structure of distributed memory machines with a set of simple programming abstractions. The model is valid across many machines, allowing for portable high-performance. Split-C combines the set of processor address spaces into a single global address space. <p> The CM-5 is based on the Cypress Sparc microprocessor, clocked at 33 MHz, with a 64 kB unified instruction and data cache. A Split-C global read involves issuing a CMAML active message to access the remote location and to reply with the value, taking approximately 12 s <ref> [8, 24] </ref>. Traditional measures of the computational performance of the node, such as LINPACK, MFLOPS, and SPECmarks, offer little indication of the performance on this integer graph algorithm, which stresses the storage hierarchy, so instead we calibrate the local node performance empirically in Section 4.1.
Reference: [9] <author> H. G. Evertz, </author> <title> "Vectorized Cluster Search," Nuclear Physics B, </title> <booktitle> Proceedings Supplements, </booktitle> <year> 1992, </year> <pages> pp. 620-622. </pages>
Reference-contexts: Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23]. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) [7, 10, 13, 14, 21, 26] and vector machines <ref> [9, 26] </ref>. With the exception of [5], which focuses on 2D graphs for robot vision, these solutions typically emphasize performance over portability, scalability, and generality, but still rarely obtain good performance. We present a fast, portable, general-purpose algorithm for finding connected components on a distributed memory machine.
Reference: [10] <author> M. Flanigan, P. Tamayo, </author> <title> "A Parallel Cluster Labeling Method for Monte Carlo Dynamics," </title> <journal> International Journal of Modern Physics C, </journal> <volume> Vol. 3, No. 6, </volume> <year> 1992, </year> <pages> 1235-1249. </pages>
Reference-contexts: Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23]. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) <ref> [7, 10, 13, 14, 21, 26] </ref> and vector machines [9, 26]. With the exception of [5], which focuses on 2D graphs for robot vision, these solutions typically emphasize performance over portability, scalability, and generality, but still rarely obtain good performance. <p> The fastest previous result, by Flanigan and Tamayo, achieved 12.5 million nodes per second on much larger 2D graphs using a 256 processor CM-5 <ref> [10] </ref>. Hackl et. al. achieved nearly 7 Mn/s on a 32 processor Intel IPSC/860, again for very large 2D graphs [13].
Reference: [11] <author> H. Gazit, </author> <title> "An Optimal Randomized Parallel Algorithm for Finding Connected Components in a Graph," </title> <journal> SIAM Journal of Computing 20(6), </journal> <month> December </month> <year> 1991. </year>
Reference-contexts: Sequential solutions are well understood and commonly used as an application of depth-first and breadth-first search. Parallel solutions have received a great deal of attention from theorists, and have proven difficult. Algorithms such as Shiloach-Vishkin obtain good results with the CRCW PRAM model <ref> [3, 11, 12, 28] </ref>, which assumes uniform memory access time and fl Copyright 1995 by the Association for Computing Machinery, Inc. (ACM).
Reference: [12] <author> J. Greiner, </author> <title> "A Comparison of Parallel Algorithms for Connected Components," </title> <booktitle> 6th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1994, </year> <pages> pp. 16-23. </pages>
Reference-contexts: Sequential solutions are well understood and commonly used as an application of depth-first and breadth-first search. Parallel solutions have received a great deal of attention from theorists, and have proven difficult. Algorithms such as Shiloach-Vishkin obtain good results with the CRCW PRAM model <ref> [3, 11, 12, 28] </ref>, which assumes uniform memory access time and fl Copyright 1995 by the Association for Computing Machinery, Inc. (ACM). <p> Implementation of the theoretical work has been restricted to shared-memory machines <ref> [12] </ref> and SIMD machines with very slow processors [12, 17, 23]. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) [7, 10, 13, 14, 21, 26] and vector machines [9, 26]. <p> Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors <ref> [12, 17, 23] </ref>. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) [7, 10, 13, 14, 21, 26] and vector machines [9, 26]. <p> The values in the table are in millions of nodes processed per second using the hybrid algorithm. The last column shows performance on a single node of a Cray C90 using an algorithm developed by Greiner <ref> [12] </ref>; Greiner used 2D30 rather than 2D40 in his work. (S-W), reduce correlation time for the simulations. For example, the correlation time using S-W grows as O (L 0:35 ) [30, 31] for a two-dimensional Ising model, allowing much larger samples to be studied. <p> These numbers provide a baseline for our measurements of scaled speedup. For comparison, the last column shows Greiner's Cray C90 results <ref> [12] </ref>. Parenthesized values are normalized to the performance of a T3D processor. the number of processors increases.
Reference: [13] <author> R. Hackl, H.-G. Matuttis, J. M. Singer, T. H. Husslein, I. Morgenstern, </author> <title> "Parallelization of the 2D Swendsen-Wang Algorithm," </title> <journal> International Journal of Modern Physics C, </journal> <volume> Vol. 4, No. 6, </volume> <year> 1993, </year> <pages> pp. 59-72. </pages>
Reference-contexts: Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23]. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) <ref> [7, 10, 13, 14, 21, 26] </ref> and vector machines [9, 26]. With the exception of [5], which focuses on 2D graphs for robot vision, these solutions typically emphasize performance over portability, scalability, and generality, but still rarely obtain good performance. <p> The fastest previous result, by Flanigan and Tamayo, achieved 12.5 million nodes per second on much larger 2D graphs using a 256 processor CM-5 [10]. Hackl et. al. achieved nearly 7 Mn/s on a 32 processor Intel IPSC/860, again for very large 2D graphs <ref> [13] </ref>.
Reference: [14] <author> S. Hambrusch, L. TeWinkel, </author> <title> "A Study of Connected Component Labeling Algorithms on the MPP," </title> <booktitle> 3rd International Conference on Supercomputing, </booktitle> <volume> Vol. 1, </volume> <month> May </month> <year> 1988, </year> <pages> pp. 477-483. </pages>
Reference-contexts: Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23]. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) <ref> [7, 10, 13, 14, 21, 26] </ref> and vector machines [9, 26]. With the exception of [5], which focuses on 2D graphs for robot vision, these solutions typically emphasize performance over portability, scalability, and generality, but still rarely obtain good performance.
Reference: [15] <author> D. S. Hirschberg, A. . Chandra, D. V. Sarwate, </author> <title> "Computing Connected Components on Parallel Computers," </title> <journal> Communications of the ACM, </journal> <volume> 22(8), </volume> <year> 1979, </year> <pages> pp. 461-464. 15 </pages>
Reference-contexts: Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of any organization. 1 arbitrary bandwidth to any memory location, but the inherent contention in the algorithm makes even EREW solutions much more challenging <ref> [6, 15, 18, 20] </ref>. Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23].
Reference: [16] <author> B. K. P. Horn, </author> <title> "Robot Vision," </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction The problem of finding the connected components of a graph has broad importance in both computer and computational science. In computer vision, for example, edge detection and object recognition depend on connected components <ref> [16] </ref>. Connected components algorithms have also advanced the study of physical phenomena, including properties of magnetic materials near critical temperatures. However, the problem offers a unique challenge for parallel computing. Sequential solutions are well understood and commonly used as an application of depth-first and breadth-first search.
Reference: [17] <author> T.-S. Hsu, V. Ramachandran, N. Dean, </author> <title> "Parallel Implementation of Algorithms for Finding Connected Components," </title> <booktitle> to be published in the Proceedings of the 3rd Annual DIMACS Challenge, </booktitle> <year> 1995. </year>
Reference-contexts: Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors <ref> [12, 17, 23] </ref>. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) [7, 10, 13, 14, 21, 26] and vector machines [9, 26].
Reference: [18] <author> D. B. Johnson, P. Metaxas, </author> <title> "Connected Components in O(log 3 =2n) Parallel Time for the CREW PRAM," </title> <booktitle> 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1991, </year> <pages> pp. 688-697. </pages>
Reference-contexts: Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of any organization. 1 arbitrary bandwidth to any memory location, but the inherent contention in the algorithm makes even EREW solutions much more challenging <ref> [6, 15, 18, 20] </ref>. Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23].
Reference: [19] <author> C. </author> <title> Kalle, </title> <journal> Journal of Physics A 17, </journal> <volume> L801, </volume> <year> 1984. </year>
Reference-contexts: The quality of statistics returned from these simulations depends heavily on the size of the sample, L. Although a larger sample size leads to better statistics, the number of steps needed to propagate information generally grows as O (L 2 ) or worse using traditional methods <ref> [4, 19, 25, 32, 33] </ref>. More recent methods, such as the Swendsen-Wang algorithm 4 T3D/256 CS-2/32 CM-5/256 C90/1 Graph (Greiner) Type (Mn/s) (Mn/s) (Mn/s) (Mn/s) 2D30 - 4.3 2D60/200 83.6 3.31 12.1 2.5 3D40/30 38.6 0.79 2.38 2.1 Table 2: Raw performance of the parallel connected components algorithm.
Reference: [20] <author> D. R. Karger, N. Nisan, M. Parnas, </author> <title> "Fast Connected Components Algorithm for the EREW PRAM," </title> <booktitle> 4th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> pp. 373-381. </pages>
Reference-contexts: Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of any organization. 1 arbitrary bandwidth to any memory location, but the inherent contention in the algorithm makes even EREW solutions much more challenging <ref> [6, 15, 18, 20] </ref>. Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23].
Reference: [21] <author> J. Kertesz, D. Stauffer, </author> <title> "Swendsen-Wang Dynamics of Large 2D Critical Ising Models," </title> <journal> International Journal of Modern Physics C, </journal> <volume> Vol. 3, No. 6, </volume> <year> 1992, </year> <pages> pp. 1275-1279. </pages>
Reference-contexts: Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23]. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) <ref> [7, 10, 13, 14, 21, 26] </ref> and vector machines [9, 26]. With the exception of [5], which focuses on 2D graphs for robot vision, these solutions typically emphasize performance over portability, scalability, and generality, but still rarely obtain good performance.
Reference: [22] <author> A. Krishnamurthy, S. Lumetta, D. Culler, K. Yelick, </author> <title> "Connected Components on Distributed Memory Machines," </title> <booktitle> to be published in the Proceedings of the 3rd Annual DIMACS Challenge, </booktitle> <year> 1995. </year>
Reference-contexts: The DFS then collapses connected components in the subgraph local to a processor, resulting in a much smaller graph for the global phase. The optimized algorithm follows. For more detail on the data structures or on the process of optimization, see <ref> [22] </ref>. 1. Local Phase. Perform a local DFS on each processor's portion of the graph, collapsing each local connected component into a representative node. Mark each component of this global graph with a unique value for the global phase. 2. Global Initialization.
Reference: [23] <author> S. Kumar, S. M. Goddard, J. F. Prins, </author> <title> "Connected-Components Algorithms for Mesh-Connected Parallel Computers," </title> <booktitle> to be published in the Proceedings of the 3rd Annual DIMACS Challenge, </booktitle> <year> 1995. </year>
Reference-contexts: Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors <ref> [12, 17, 23] </ref>. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) [7, 10, 13, 14, 21, 26] and vector machines [9, 26].
Reference: [24] <author> S. Luna, </author> <title> "Implementing an Efficient Portable Global Memory Layer on Distributed Memory Multiprocessors," </title> <editor> U. </editor> <address> C. </address> <institution> Berkeley Technical Report #CSD-94-810, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Split-C also gives our implementation portability, with versions running on the Cray T3D, the IBM SP-1 and SP-2, the Intel Paragon, the Thinking Machines CM-5, the Meiko CS-2, and networks of workstations <ref> [2, 24, 27, 34] </ref>. 2.3 Parallel platforms We consider three large-scale parallel machines: the Cray T3D, the Meiko CS-2, and the Thinking Machines CM-5. These machines offer a range of computational and communication performance against which to evaluate the algorithm implementation. <p> The CM-5 is based on the Cypress Sparc microprocessor, clocked at 33 MHz, with a 64 kB unified instruction and data cache. A Split-C global read involves issuing a CMAML active message to access the remote location and to reply with the value, taking approximately 12 s <ref> [8, 24] </ref>. Traditional measures of the computational performance of the node, such as LINPACK, MFLOPS, and SPECmarks, offer little indication of the performance on this integer graph algorithm, which stresses the storage hierarchy, so instead we calibrate the local node performance empirically in Section 4.1.
Reference: [25] <author> G. F. Mazenko, M. J. Nolan, O. T. </author> <title> Valls, </title> <journal> Physical Review Letters 41, </journal> <volume> 500, </volume> <year> 1978. </year>
Reference-contexts: The quality of statistics returned from these simulations depends heavily on the size of the sample, L. Although a larger sample size leads to better statistics, the number of steps needed to propagate information generally grows as O (L 2 ) or worse using traditional methods <ref> [4, 19, 25, 32, 33] </ref>. More recent methods, such as the Swendsen-Wang algorithm 4 T3D/256 CS-2/32 CM-5/256 C90/1 Graph (Greiner) Type (Mn/s) (Mn/s) (Mn/s) (Mn/s) 2D30 - 4.3 2D60/200 83.6 3.31 12.1 2.5 3D40/30 38.6 0.79 2.38 2.1 Table 2: Raw performance of the parallel connected components algorithm.
Reference: [26] <author> H. Mino, </author> <title> "A Vectorized Algorithm for Cluster Formation in the Swendsen-Wang Dynamics," </title> <journal> Computer Physics Communications 66, </journal> <year> 1991, </year> <pages> pp. 25-30. </pages>
Reference-contexts: Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23]. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) <ref> [7, 10, 13, 14, 21, 26] </ref> and vector machines [9, 26]. With the exception of [5], which focuses on 2D graphs for robot vision, these solutions typically emphasize performance over portability, scalability, and generality, but still rarely obtain good performance. <p> Implementation of the theoretical work has been restricted to shared-memory machines [12] and SIMD machines with very slow processors [12, 17, 23]. Many practical solutions have been developed independently of theoretical work for modern MIMD massively parallel platforms (MPP's) [7, 10, 13, 14, 21, 26] and vector machines <ref> [9, 26] </ref>. With the exception of [5], which focuses on 2D graphs for robot vision, these solutions typically emphasize performance over portability, scalability, and generality, but still rarely obtain good performance. We present a fast, portable, general-purpose algorithm for finding connected components on a distributed memory machine.
Reference: [27] <author> K. E. Schauser, C. J. Scheiman, </author> <title> "Experience with Active Messages on the Meiko CS-2," </title> <booktitle> Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: Split-C also gives our implementation portability, with versions running on the Cray T3D, the IBM SP-1 and SP-2, the Intel Paragon, the Thinking Machines CM-5, the Meiko CS-2, and networks of workstations <ref> [2, 24, 27, 34] </ref>. 2.3 Parallel platforms We consider three large-scale parallel machines: the Cray T3D, the Meiko CS-2, and the Thinking Machines CM-5. These machines offer a range of computational and communication performance against which to evaluate the algorithm implementation. <p> The Split-C global read issues a command to the communications co-processor via an exchange instruction, which causes a waiting ELAN thread to either access remote memory or to begin a DMA transfer, depending on length. A remote read requires roughly 20 s <ref> [27, 34] </ref>. The CM-5 is based on the Cypress Sparc microprocessor, clocked at 33 MHz, with a 64 kB unified instruction and data cache.
Reference: [28] <author> Y. Shiloach, U. Vishkin, </author> <title> "An O(log n) Parallel Connectivity Algorithm," </title> <journal> Journal of Algorithms, </journal> <volume> No. 3, </volume> <year> 1982, </year> <pages> pp. 57-67. </pages>
Reference-contexts: Sequential solutions are well understood and commonly used as an application of depth-first and breadth-first search. Parallel solutions have received a great deal of attention from theorists, and have proven difficult. Algorithms such as Shiloach-Vishkin obtain good results with the CRCW PRAM model <ref> [3, 11, 12, 28] </ref>, which assumes uniform memory access time and fl Copyright 1995 by the Association for Computing Machinery, Inc. (ACM). <p> time spent in the two types of phases, and is usually good if the local phases dominate the total time. 2.1 Hybrid connected components algorithm We follow a hybridization strategy to create our algorithm, merging a fast, local depth-first search (DFS) with a powerful and efficient variant of Shiloach-Vishkin (S-V) <ref> [28] </ref>. The graph is distributed across the 2 processor memories; each node is local to some processor, and we call edges crossing processor boundaries remote edges. The DFS then collapses connected components in the subgraph local to a processor, resulting in a much smaller graph for the global phase.
Reference: [29] <author> J. P. Singh, J. Hennessy, A. Gupta, </author> <title> "Scaling Parallel Programs for Multiprocessors: Methodology and Examples," </title> <booktitle> IEEE Computer 26(7), </booktitle> <month> July </month> <year> 1993, </year> <pages> pp. 42-50. </pages>
Reference-contexts: We choose to prune the input space by using four graphs varying in mesh dimension and edge probability. For each graph, we scale the size of the graph with the number of processors, so that the nodes per processor is held constant, i.e., memory constrained scaling <ref> [29] </ref>. For each data point, we average execution time for twenty graph instances with the specified degree and edge probability. The result is presented as a normalized rate: millions of nodes processed per second (Mn/s). <p> However, if the inherent work in the algorithm increases too rapidly with problem size, problems on large machines run too long to be useful <ref> [29] </ref>. For connected components, however, the work is roughly proportional to the number of nodes in the graph. A connected components algorithm must mark each node, examine each edge, and label each component in the graph. <p> We vary surface-to-volume ratio by changing the dimension of the graph and examine both disconnected and highly connected graphs. Each graph uses a fixed amount of memory per processor (memory constrained scaling <ref> [29] </ref>). The last column shows the percentage of nodes in the largest component.
Reference: [30] <author> R. H. Swendsen, J.-S. Wang, </author> <title> "Nonuniversal Critical Dynamics in Monte Carlo Simulations," </title> <journal> Physical Review Letters, </journal> <volume> Vol. 58, No. 2, </volume> <month> January </month> <year> 1987, </year> <pages> pp. 86-88. </pages>
Reference-contexts: The last column shows performance on a single node of a Cray C90 using an algorithm developed by Greiner [12]; Greiner used 2D30 rather than 2D40 in his work. (S-W), reduce correlation time for the simulations. For example, the correlation time using S-W grows as O (L 0:35 ) <ref> [30, 31] </ref> for a two-dimensional Ising model, allowing much larger samples to be studied. At the heart of S-W is a connected components algorithm. The S-W algorithm repeatedly generates a random graph, finds the connected components of the graph, and applies a simple transformation to each component [31].
Reference: [31] <author> J.-S. Wang, R. H. Swendsen, </author> <title> "Cluster Monte Carlo Algorithms," </title> <journal> Physica A, </journal> <volume> No. 167, </volume> <year> 1990, </year> <pages> pp. 565-579. </pages>
Reference-contexts: The last column shows performance on a single node of a Cray C90 using an algorithm developed by Greiner [12]; Greiner used 2D30 rather than 2D40 in his work. (S-W), reduce correlation time for the simulations. For example, the correlation time using S-W grows as O (L 0:35 ) <ref> [30, 31] </ref> for a two-dimensional Ising model, allowing much larger samples to be studied. At the heart of S-W is a connected components algorithm. The S-W algorithm repeatedly generates a random graph, finds the connected components of the graph, and applies a simple transformation to each component [31]. <p> At the heart of S-W is a connected components algorithm. The S-W algorithm repeatedly generates a random graph, finds the connected components of the graph, and applies a simple transformation to each component <ref> [31] </ref>. In the years since the introduction of the S-W algorithm, cluster dynamics, and hence the use of connected components, has found widespread use in other areas, including computational field theory, experimental high energy and particle physics, and statistical mechanics. The random graph in S-W is a probabilistic mesh.
Reference: [32] <author> J. K. </author> <title> Williams, </title> <journal> Journal of Physics A 18, </journal> <volume> 49, </volume> <year> 1985. </year>
Reference-contexts: The quality of statistics returned from these simulations depends heavily on the size of the sample, L. Although a larger sample size leads to better statistics, the number of steps needed to propagate information generally grows as O (L 2 ) or worse using traditional methods <ref> [4, 19, 25, 32, 33] </ref>. More recent methods, such as the Swendsen-Wang algorithm 4 T3D/256 CS-2/32 CM-5/256 C90/1 Graph (Greiner) Type (Mn/s) (Mn/s) (Mn/s) (Mn/s) 2D30 - 4.3 2D60/200 83.6 3.31 12.1 2.5 3D40/30 38.6 0.79 2.38 2.1 Table 2: Raw performance of the parallel connected components algorithm.
Reference: [33] <author> H. Yahata, M. </author> <title> Suzuki, </title> <journal> Journal of the Physics Society of Japan 27, </journal> <volume> 1421, </volume> <year> 1969. </year>
Reference-contexts: The quality of statistics returned from these simulations depends heavily on the size of the sample, L. Although a larger sample size leads to better statistics, the number of steps needed to propagate information generally grows as O (L 2 ) or worse using traditional methods <ref> [4, 19, 25, 32, 33] </ref>. More recent methods, such as the Swendsen-Wang algorithm 4 T3D/256 CS-2/32 CM-5/256 C90/1 Graph (Greiner) Type (Mn/s) (Mn/s) (Mn/s) (Mn/s) 2D30 - 4.3 2D60/200 83.6 3.31 12.1 2.5 3D40/30 38.6 0.79 2.38 2.1 Table 2: Raw performance of the parallel connected components algorithm.

References-found: 33

