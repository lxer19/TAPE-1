URL: http://www.research.att.com/~wcohen/postscript/ml-92.ps
Refering-URL: http://www.research.att.com/~wcohen/
Root-URL: 
Email: wcohen@research.att.com  
Title: From Proceedings of the Ninth International Conference on Machine Learning Compiling Prior Knowledge Into an
Author: William W. Cohen 
Address: Murray Hill, NJ 07974 USA  
Affiliation: AT&T Bell Laboratories AI Principles Research Department  
Abstract: Current theory-guided learning systems are inflexible, in that they are committed to performing one particular class of theory corrections; this is problematic because in some cases special-purpose theory-guided learning systems can dramatically outperform general-purpose ones. To address this problem, we describe a new system in which theory-guided learning is separated into two phases. The first phase is "theory intrepre-tation", in which the initial theory is translated into an explicit description of the bias for an inductive learning system; we introduce antecedent description grammars as a language for explicitly representing this bias. The second phase is "grammatically biased learning", in which this bias is used to search for a hypothesis. We demonstrate empirically that this approach leads to a flexible learning system which can, by use of suitable translators, emulate several useful learning systems; we also argue that this architecture makes it easier for a user unfamiliar with the details of the learning system to predict the impact that an initial theory will have on learning. 
Abstract-found: 1
Intro-found: 1
References-found: 0

