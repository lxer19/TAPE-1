URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/auto-partitioning.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/auto-partitioning.html
Root-URL: 
Email: email: agarwal@mit.edu  
Phone: Phone: (617) 253-1448  
Title: Automatic Partitioning of Parallel Loops and Data Arrays for Distributed Shared Memory Multiprocessors  
Author: Anant Agarwal, David Kranz Venkat Natarajan 
Address: Cambridge, MA 02139  Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science, NE43-624 Massachusetts Institute of Technology  Motorola Cambridge Research Center  
Abstract: This paper presents a theoretical framework for automatically partitioning parallel loops to minimize cache coherency traffic on shared-memory multiprocessors. While several previous papers have looked at hyperplane partitioning of iteration spaces to reduce communication traffic, the problem of deriving the optimal tiling parameters for minimal communication in loops with general affine index expressions has remained open. Our paper solves this open problem by presenting a method for deriving an optimal hyperparallelepiped tiling of iteration spaces for minimal communication in multiprocessors with caches. We show that the same theoretical framework can also be used to determine optimal tiling parameters for both data and loop partitioning in distributed memory multicomputers. Our framework uses matrices to represent iteration and data space mappings and the notion of uniformly intersecting references to capture temporal locality in array references. We introduce the notion of data footprints to estimate the communication traffic between processors and use linear algebraic methods and lattice theory to compute precisely the size of data footprints. We have implemented this framework in a compiler for Alewife, a distributed shared memory multiprocessor.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Constantine D. Polychronopoulos and David J. Kuck. </author> <title> Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12), </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Loop partitioning can be done by the programmer, by the run time system, or by the compiler. Relegating the partitioning task to the programmer defeats the central purpose of building cache-coherent shared-memory systems. While partitioning can be done at run time (for example, see <ref> [1, 2] </ref>), it is hard for the run time system to optimize for cache locality because much of the information required to compute communication patterns is either unavailable at run time or expensive to obtain. Thus compile-time partitioning of parallel loops is important.
Reference: [2] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Loop partitioning can be done by the programmer, by the run time system, or by the compiler. Relegating the partitioning task to the programmer defeats the central purpose of building cache-coherent shared-memory systems. While partitioning can be done at run time (for example, see <ref> [1, 2] </ref>), it is hard for the run time system to optimize for cache locality because much of the information required to compute communication patterns is either unavailable at run time or expensive to obtain. Thus compile-time partitioning of parallel loops is important. <p> Example 5 The following sets of references are uniformly intersecting. 1. A [i; j]; A [i + 1; j 3]; A [i; j + 4]. The following pairs are not uniformly intersecting. 12 1. A [i; j]; A [2i; j]. 3. A <ref> [j; 2; i] </ref>; A [j; 3; i]. 5. A [i + 2; 2i + 4]; A [i + 5; 2i + 8]. Footprints in the data space for a set of uniformly intersecting references are translations of one another, as shown below.
Reference: [3] <author> M. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN 91 Conference Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <year> 1991. </year>
Reference-contexts: This notion helps specify precisely the set of references that have substantially overlapping data sets. Overlap produces temporal locality in cache accesses. A similar concept of uniformly generated references has been used in earlier work in the context of reuse and iteration space tiling <ref> [3, 4] </ref>. <p> Our work complements the work of Wolfe and Lam <ref> [3] </ref> and Schreiber and Dongarra [11]. Wolfe and Lam derive loop transformations (and tile the iteration space) to improve data locality in multiprocessors with caches. <p> Note that ~ i; ~g ( ~ i), and ~a are row vectors. We often refer to an array reference by the pair (G; ~a). (An example of this function is presented in Section 3). Similar notation has been used in several papers in the past, for example, see <ref> [3, 4] </ref>. All our vectors and matrices have integer entries unless stated otherwise. We assume that the loop bounds are such that the iteration space is rectangular. However, we note that our methods can still be used to derive reasonable partitions when this condition is not met. <p> For the purpose of locality optimization through loop partitioning, our definition of reuse of array references will combine the concept of uniformly generated arrays and the notion of intersecting array references. This notion is similar to the equivalence classes within uniformly generated references defined in <ref> [3] </ref>. Definition 6 Two array references are uniformly intersecting if they are both intersecting and uniformly generated. Example 5 The following sets of references are uniformly intersecting. 1. A [i; j]; A [i + 1; j 3]; A [i; j + 4]. <p> Example 5 The following sets of references are uniformly intersecting. 1. A [i; j]; A [i + 1; j 3]; A [i; j + 4]. The following pairs are not uniformly intersecting. 12 1. A [i; j]; A [2i; j]. 3. A [j; 2; i]; A <ref> [j; 3; i] </ref>. 5. A [i + 2; 2i + 4]; A [i + 5; 2i + 8]. Footprints in the data space for a set of uniformly intersecting references are translations of one another, as shown below.
Reference: [4] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: This notion helps specify precisely the set of references that have substantially overlapping data sets. Overlap produces temporal locality in cache accesses. A similar concept of uniformly generated references has been used in earlier work in the context of reuse and iteration space tiling <ref> [3, 4] </ref>. <p> Note that ~ i; ~g ( ~ i), and ~a are row vectors. We often refer to an array reference by the pair (G; ~a). (An example of this function is presented in Section 3). Similar notation has been used in several papers in the past, for example, see <ref> [3, 4] </ref>. All our vectors and matrices have integer entries unless stated otherwise. We assume that the loop bounds are such that the iteration space is rectangular. However, we note that our methods can still be used to derive reasonable partitions when this condition is not met. <p> As mentioned previously, we deal with index expressions that are affine functions of loop indices. In other words, the index function can be expressed as in Equation 1. Consider the following example to illustrate the above expression of index functions. Example 1 The reference A <ref> [i 3 + 2; 5; i 2 1; 4] </ref> in a triply nested loop can be expressed by (i 1 ; i 2 ; i 3 ) 6 0 0 0 0 1 0 0 0 7 In this example, the second and fourth column of G are zero indicating that
Reference: [5] <author> Harold S. Stone and Dominique Thiebaut. </author> <title> Footprints in the Cache. </title> <booktitle> In Proceedings of ACM SIG-METRICS 1986, </booktitle> <pages> pages 4-8, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: The notion of data footprints is introduced to capture the combined set of data accesses made by references within each uniformly intersecting class. (The term footprint was originally coined by Stone and Thiebaut <ref> [5] </ref>.) Then, an algorithm to compute precisely the total size of the data footprint for a given loop partition is presented. Precisely computing of the size of the set of data elements accessed by a loop tile was itself an important and open problem. <p> As mentioned previously, we deal with index expressions that are affine functions of loop indices. In other words, the index function can be expressed as in Equation 1. Consider the following example to illustrate the above expression of index functions. Example 1 The reference A <ref> [i 3 + 2; 5; i 2 1; 4] </ref> in a triply nested loop can be expressed by (i 1 ; i 2 ; i 3 ) 6 0 0 0 0 1 0 0 0 7 In this example, the second and fourth column of G are zero indicating that
Reference: [6] <author> F. Irigoin and R. Triolet. </author> <title> Supernode Partitioning. </title> <booktitle> In 15th Symposium on Principles of Programming Languages (POPL XV), </booktitle> <pages> pages 319-329, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Although there have been several papers on hyperplane partitioning of iteration spaces, the problem of deriving the optimal hyperparallelepiped tile parameters for general affine index expressions has remained open. For example, Irigoin and Triolet <ref> [6] </ref> introduce the notion of loop partitioning with multiple hyperplanes which results in hyperparallelepiped tiles. <p> Under these conditions of homogeneous tiling, the partitioning is completely defined by specifying the tile at the origin, as indicated in Figure 6. Under homogeneous tiling, the concept 9 of the tile at the origin is similar to the notion of the clustering basis in <ref> [6] </ref>. (See Appendix A for a more general representation of hyperparallelepiped loop tiles based on bounding hyperplanes.) Definition 1 An l dimensional square integer matrix L defines a semi open hyperparallelepiped tile at the origin of an l dimensional iteration space as follows.
Reference: [7] <author> S. G. Abraham and D. E. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to reduce cache coherency traffic. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 318-328, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Our paper describes an algorithm for automatically computing the partition based on the notion of cumulative footprints, derived from the mapping from iteration space to data space. Abraham and Hudak <ref> [7] </ref> considered loop partitioning in multiprocessors with caches. However, they dealt only with index expressions of the form index variable plus a constant. They assumed that the array dimension was equal to the loop nesting and focused on rectangular and hexagonal tiles. <p> However, we note that our methods can still be used to derive reasonable partitions when this condition is not met. Loop indices are assumed to take all integer values between their lower and upper bounds, i.e, the strides are one. Previous work <ref> [7, 8, 13] </ref> in this area restricted the arrays in the loop body to be of dimension exactly equal to the loop nesting. <p> Loop indices are assumed to take all integer values between their lower and upper bounds, i.e, the strides are one. Previous work [7, 8, 13] in this area restricted the arrays in the loop body to be of dimension exactly equal to the loop nesting. Abraham and Hudak <ref> [7] </ref> further restrict the loop body to contain only references to a single array; furthermore, all references are restricted to be of the form A [i 1 + a 1 ; i 2 + a 2 ; : : : ; i d + a d ] where a j is <p> The constraint of equal size partitions is imposed to achieve load balancing. We restrict our discussions to hyperparallelepiped tiles, of which rectangular tiles are a special case. Like <ref> [7, 8, 13] </ref>, we do not include the effects of synchronization in our framework. Synchronization is handled separately to ensure correct behavior. <p> Unless otherwise stated, we assume that cache lines are of unit length. The effect of larger cache lines can be included easily as suggested in <ref> [7] </ref>, and is discussed further in Section 6.2. 3 Loop Partitions and Data Partitions This section presents examples to introduce and illustrate some of our definitions and to motivate the benefits of optimizing the shapes of loop and data tiles. More precise definitions are presented in the next section. <p> The size of the cumulative footprint is minimized when L i , L j , and L k are chosen in the proportions 2, 3, and 4, or L i : L j : L k :: 2 : 3 : 4 Abraham and Hudak's algorithm <ref> [7] </ref> gives an identical partition for this example. We now use an example to show how to minimize the total number of cache misses when there are multiple uniformly intersecting sets of references. The basic idea here is that the references from each set contribute additively to traffic. <p> The structure of our compiler is shown in Figure 17. The input to the compiler is a program where parallelism is specified either by the programmer, or in a previous compilation phase. As in <ref> [7] </ref>, we separate the notion of parallelization from that of implementation. The languages accepted at present are Mul-T, a parallel Lisp language, and Semi-C, a parallel version of C. An initial series of transformations are performed including constant-folding and procedure integration producing a graphical intermediate form called WAIF.
Reference: [8] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-Time Techniques for Data Distribution in Distributed Memory Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: It is able to handle much more general index expressions, and produce parallelogram partitions if desired. We also show that when Abraham and Hudak's methods can be applied to a given loop nest, our theoretical framework reproduces their results. Ramanujam and Sadayappan <ref> [8] </ref> deal with data partitioning in multicomputers with local memory and use a matrix formulation; their results do not apply to multiprocessors with caches. Their theory produces communication-free hyperplane partitions for loops with affine index expressions when such partitions exist. <p> However, we note that our methods can still be used to derive reasonable partitions when this condition is not met. Loop indices are assumed to take all integer values between their lower and upper bounds, i.e, the strides are one. Previous work <ref> [7, 8, 13] </ref> in this area restricted the arrays in the loop body to be of dimension exactly equal to the loop nesting. <p> The constraint of equal size partitions is imposed to achieve load balancing. We restrict our discussions to hyperparallelepiped tiles, of which rectangular tiles are a special case. Like <ref> [7, 8, 13] </ref>, we do not include the effects of synchronization in our framework. Synchronization is handled separately to ensure correct behavior. <p> The additional step of aligning corresponding loop and data tiles on the same node maximizes the number of local memory references. In fact, the above horizontal partitioning of the loop space and diagonal striping of the data space results in zero communication traffic. Ramanujam and Sadayappan <ref> [8] </ref> presented algorithms to derive such communication-free partitions when possible. On the other hand, in addition to producing the same partitions when communication-traffic-free partitions exist (see Sections 5.1 and 6.3), our analysis will discover partitions that minimize traffic when such partitions are non-existent as well (see Example 8).
Reference: [9] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of SIGPLAN '93, Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: In addition, we show that our framework correctly produces partitions identical to those of Ramanujam and Sadayappan when communication free partitions do exist. In a recent paper, Anderson and Lam <ref> [9] </ref> derive communication-free partitions for multi-computers when such partitions exist, and block loops into squares otherwise. Our notion of cumulative footprints allows us to derive optimal partitions even when communication-free partitions do not exist. Gupta and Banerjee [10] address the problem of automatic data partitioning by analyzing the entire program.
Reference: [10] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of Automatic Data Partitioning Techniques for Par-allelizing Compilers on Multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In a recent paper, Anderson and Lam [9] derive communication-free partitions for multi-computers when such partitions exist, and block loops into squares otherwise. Our notion of cumulative footprints allows us to derive optimal partitions even when communication-free partitions do not exist. Gupta and Banerjee <ref> [10] </ref> address the problem of automatic data partitioning by analyzing the entire program.
Reference: [11] <author> Robert Schreiber and Jack Dongarra. </author> <title> Automatic Blocking of Nested Loops. </title> <type> Technical report, </type> <month> May </month> <year> 1990. </year> <institution> RIACS, NASA Ames Research Center, and Oak Ridge National Laboratory. </institution>
Reference-contexts: Our work complements the work of Wolfe and Lam [3] and Schreiber and Dongarra <ref> [11] </ref>. Wolfe and Lam derive loop transformations (and tile the iteration space) to improve data locality in multiprocessors with caches.
Reference: [12] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On Estimating and Enhancing Cache Effectiveness, </title> <address> pages 328-341. </address> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year> <booktitle> Lecture Notes in Computer Science: Languages and Compilers for Parallel Computing. </booktitle> <editor> Editors U. Banerjee and D. Gelernter and A. Nicolau and D. </editor> <address> Padua. </address>
Reference-contexts: Ferrante, Sarkar, and Thrash <ref> [12] </ref> address the problem of estimating the number of cache misses for a nest of loops.
Reference: [13] <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling multidimensional iteration spaces for nonshared memory machines. </title> <booktitle> In Proceedings of Supercomputing '91. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: However, we note that our methods can still be used to derive reasonable partitions when this condition is not met. Loop indices are assumed to take all integer values between their lower and upper bounds, i.e, the strides are one. Previous work <ref> [7, 8, 13] </ref> in this area restricted the arrays in the loop body to be of dimension exactly equal to the loop nesting. <p> The constraint of equal size partitions is imposed to achieve load balancing. We restrict our discussions to hyperparallelepiped tiles, of which rectangular tiles are a special case. Like <ref> [7, 8, 13] </ref>, we do not include the effects of synchronization in our framework. Synchronization is handled separately to ensure correct behavior.
Reference: [14] <author> G. N. Srinivasa Prasanna, Anant Agarwal, and Bruce R. Musicus. </author> <title> Hierarchical Compilation of Macro Dataflow Graphs for Multiprocessors with Local Memory. </title> <note> To appear in IEEE Transactions on Parallel and Distributed Systems. Also available as MIT Laboratory for Computer Science TM-466, </note> <month> June </month> <year> 1992. </year> <month> 39 </month>
Reference-contexts: Synchronization is handled separately to ensure correct behavior. For example, in the doall loop in note that in many cases fine-grain data-level synchronization can be used within a parallel do loop to enforce data dependencies and its cost approximately modeled as slightly more expensive communication than usual <ref> [14] </ref>. See Appendix B for some details. 2.2 System Model Our analysis applies to systems whose structure is similar to that shown in Figure 2. The system comprises a set of processors, each with a coherent cache.
Reference: [15] <author> A. Carnevali, V. Natarajan, and A. Agarwal. </author> <title> A Relationship between the Number of Lattice Points within Hyperparallelepipeds and their Volume. </title> <note> In preparation, </note> <month> August </month> <year> 1993, </year> <institution> Motorola Cambridge Research Center. </institution>
Reference-contexts: Theorem 1 The number of integer points (iteration points) in tile L is equal to the volume of the tile, which is given by j det Lj. 10 Proof: We provide a sketch of the proof; a more detailed proof is given in <ref> [15] </ref>. It is easy to show that the theorem is true for an n-dimensional semi-open rectangle. For a given n-dimensional semi-open hyperparallelepiped, let its volume be V and let P be the number of integer points in it.
Reference: [16] <author> Gilbert Strang. </author> <title> Linear algebra and its applications, volume 3rd edition. </title> <publisher> Harcourt Brace Jovanovich, </publisher> <address> San Diego, CA, </address> <year> 1988. </year>
Reference-contexts: The latter implies that the nullspace of G T is of dimension 0. From a fundamental theorem of Linear Algebra <ref> [16] </ref>, this means that the rows of G are linearly independent. It is to be noted that when the rows of G are not independent there exists a nontrivial integer solution to ~xG = ~ 0, given that the entries in G are integers.
Reference: [17] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> John Wiley & Sons, </publisher> <year> 1990. </year>
Reference-contexts: iteration space to the data space as defined by G is onto if and only if the columns of G are independent and the g.c.d. of the subdeterminants of order equal to the number of columns is 1. 15 Proof: Follows from the Hermite normal form theorem as shown in <ref> [17] </ref>. Lemma 3 If G is invertible then ~ d 2 LG if and only if ~ dG 1 2 L. Proof: Clearly G is invertible implies, ~ dG 1 2 L implies ~ d 2 LG.
Reference: [18] <author> George Arfken. </author> <title> Mathematical Methods for Physics. </title> <publisher> Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: This optimization problem can be solved using standard methods, for example, using the method of Lagrange multipliers <ref> [18] </ref>.
Reference: [19] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: The differences become even greater if bigger offsets are used. This example also shows that rectangular partitions do not always yield the best partition. 7.2 Implementation on Alewife We have also implemented some of the ideas from our framework in a compiler for the Alewife machine <ref> [19] </ref> to understand the extent to which good loop partitioning impacts end application performance, and the extent to which our theory predicts the optimal loop partition. The Alewife machine implements a shared global address space with distributed physical memory and coherent caches.
Reference: [20] <author> Paul S. Barth, Rishiyur S. Nikhil, and Arvind. M-Structures: </author> <title> Extending a Parallel, Non-strict, Functional Language with State. </title> <booktitle> In Proceedings of the 5th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <month> August </month> <year> 1991. </year>
Reference: [21] <author> B.J. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System. </title> <journal> Society of Photocoptical Instrumentation Engineers, </journal> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
References-found: 21

