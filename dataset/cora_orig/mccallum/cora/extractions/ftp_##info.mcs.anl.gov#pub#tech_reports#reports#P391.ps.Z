URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P391.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts93.htm
Root-URL: http://www.mcs.anl.gov
Title: Language Constructs for Modular Parallel Programs  
Author: Ian Foster 
Keyword: modularity; parallel programming; programming languages; program composition; code reuse; virtual computer  
Date: January 1994.  
Address: Argonne, Ill.,  Argonne, IL 60439 USA  
Affiliation: Mathematics and Computer Science Division, Argonne National Laboratory,  Mathematics and Computer Science Division Argonne National Laboratory  
Note: Preprint MCS-P391-1093,  
Abstract: We describe programming language constructs that facilitate the application of modular design techniques in parallel programming. These constructs allow us to isolate resource management and processor scheduling decisions from the specification of individual modules, which can themselves encapsulate design decisions concerned with concurrency, communication, process mapping, and data distribution. This approach permits development of libraries of reusable parallel program components and the reuse of these components in different contexts. In particular, alternative mapping strategies can be explored without modifying other aspects of program logic. We describe how these constructs are incorporated in two practical parallel programming languages, PCN and Fortran M. Compilers have been developed for both languages, allowing experimentation in substantial applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agha, </author> <title> Actors. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The execution schedule is then determined by the availability of data and by the scheduling algorithm used to select executable processes. This approach is, of course, well known in actors, dataflow, parallel functional programming, and concurrent logic programming <ref> [1, 6, 21, 15] </ref>. 5 Interfaces. Concurrently-executing program components must be able to share data. As noted above, a mechanism is required that supports encapsulation, that is independent of resource allocation and scheduling decisions, and that is efficient on parallel computers. We achieve this as follows. Logical Resources.
Reference: [2] <author> V. Bala and S. Kipnis, </author> <title> "Process Groups: A mechanism for the coordination of and communication among processes in the Venus collective communication library," </title> <type> Preprint, </type> <institution> IBM T. J. Watson Research Center, </institution> <year> 1992. </year>
Reference-contexts: The integration of this calculus into a programming notation is not discussed, and the notion of virtual computer is absent. Some recent work on parallel message-passing libraries has explored the use of "process groups" and "communication contexts" to support the encapsulation of communication operations in parallel libraries <ref> [28, 2] </ref>. Chien and Dally's Concurrent Aggregates (CA) language allows the definition of homogeneous collections of objects called aggregates; messages addressed to an aggregate are routed to one of its members [12]. As in this paper, concurrent structures can be defined and composed with other structures to build concurrent programs.
Reference: [3] <author> G. Booch, </author> <title> Object-oriented Design. </title> <address> Benjamin/Cummings, </address> <year> 1991. </year> <month> 13 </month>
Reference-contexts: 1 Introduction In sequential programming, modular and object-oriented design and programming techniques are well understood and widely used to reduce complexity, permit separate development of components, and encourage reuse <ref> [31, 27, 3] </ref>. In parallel programming, the situation is less advanced. Parallel programs are, for the most part, developed in an ad-hoc fashion, as monolithic entities that cannot easily be adapted to changing circumstances. <p> The second is more naturally applied bottom up and can reduce the cost of program modifications. Both techniques are central to object-oriented design <ref> [3] </ref>. It is instructive to examine how these techniques can be applied in sequential and parallel programming. For illustrative purposes, we consider a simple example: the convolution operation used, for example, in motion estimation algorithms in image processing [11].
Reference: [4] <author> S. Borkar et al., </author> <title> "iWarp: An integrated solution to high-speed parallel computing," </title> <booktitle> in Proc. Supercomputing Conf., </booktitle> <year> 1988, </year> <pages> pp. 330-339. </pages>
Reference-contexts: Not surprisingly, the computing students showed a strong preference for PCN, while the science students preferred Fortran M. 5 Related Work Several parallel languages and programming environments have been developed to support the modular construction of parallel programs. Borkhar et al. <ref> [4] </ref> propose that parallel programs be constructed by plugging together "cells," in a manner analogous to VLSI. They use this technique to generate efficient programs for the iWarp systolic processor. occam [24] has been used for similar purposes.
Reference: [5] <author> J. Browne, J. Werth, and T. Lee, </author> <title> "Intersection of parallel structuring and reuse of software components," </title> <booktitle> in Proc. Intl Conf. on Parallel Processing, </booktitle> <address> Penn. </address> <publisher> State Press, </publisher> <year> 1989. </year>
Reference-contexts: Griswold et al. propose process ensembles as a means of organizing data, computation, and communication [19]. However, they do not consider hierarchies of virtual computers. Browne et al. propose a compositional calculus for specifying interconnections between software chips <ref> [5] </ref>. The integration of this calculus into a programming notation is not discussed, and the notion of virtual computer is absent. Some recent work on parallel message-passing libraries has explored the use of "process groups" and "communication contexts" to support the encapsulation of communication operations in parallel libraries [28, 2].
Reference: [6] <author> Cann, D. C., J. T. Feo, and T. M. DeBoni, </author> <title> "Sisal 1,2: High performance applicative computing," </title> <booktitle> in Proc. Symp. Parallel and Distributed Processing, </booktitle> <publisher> IEEE Press, </publisher> <year> 1990, </year> <pages> pp. 612-616. </pages>
Reference-contexts: The execution schedule is then determined by the availability of data and by the scheduling algorithm used to select executable processes. This approach is, of course, well known in actors, dataflow, parallel functional programming, and concurrent logic programming <ref> [1, 6, 21, 15] </ref>. 5 Interfaces. Concurrently-executing program components must be able to share data. As noted above, a mechanism is required that supports encapsulation, that is independent of resource allocation and scheduling decisions, and that is efficient on parallel computers. We achieve this as follows. Logical Resources.
Reference: [7] <author> K. M. Chandy and I. Foster, </author> <title> "A deterministic notation for cooperating processes," </title> <type> Preprint MCS-P346-0193, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill. </institution> <month> 60439, </month> <year> 1993. </year>
Reference-contexts: Second, virtual computer constructs are incorporated into compositional programming languages, in which concurrency is specified with explicit parallel constructs and interactions between concurrent processes are restricted so that neither physical location nor 1 execution schedule affects the result of a computation <ref> [7] </ref>. Languages with this prop-erty include Strand and PCN, in which processes interact by reading and writing shared single-assignment variables, and Fortran M, in which interactions occur via single-reader, single-writer virtual channels. <p> Schedule Independence. In order that scheduling decisions can be changed without modifying other program components, we restrict operations on shared data to those for which execution order does not affect the result of a computation <ref> [7] </ref>. For example, two processes may share a channel on which one performs nonblocking sends while the other performs blocking receives. Or, several processes may share a single-assignment variable that one writes and several read.
Reference: [8] <author> K. M. Chandy and C. Kesselman, </author> <title> Compositional parallel programming in CC++, </title> <type> Technical Report, </type> <institution> Caltech, </institution> <year> 1992. </year>
Reference-contexts: Mapping, data distribution, and embedding operations within the process are performed relative to this implicit virtual computer. An alternative approach, which also has its merits, is to represent the virtual computer as a data structure that can be manipulated in the language. This approach is adopted in CC++ <ref> [8] </ref>. Mapping in PCN is specified by using the infix operator @ to apply a mapping function to a process or block. A mapping function returns a node number within the current virtual computer. <p> For example, it is not straightforward to invoke the same module on processor subsets that may be overlapping or of different sizes. Some of the ideas developed in this paper have also been applied to CC++, extensions to C++ designed to support compositional parallel programming <ref> [8] </ref>. In CC++, virtual computers are represented explicitly, as arrays of pointers to processor objects. Mapping 12 is achieved by invoking a function in a processor object. Data distribution and embedding are not supported directly but can be specified as CC++ functions.
Reference: [9] <author> K. M. Chandy and S. Taylor, </author> <title> An Introduction to Parallel Programming. </title> <editor> Jones and Bartlett, </editor> <year> 1991. </year>
Reference-contexts: Compositional languages permit design decisions concerned with mapping, data distribution, communication, and scheduling to be made separately and to be modified without changing other aspects of a design [15]. In this paper, we show how virtual computer constructs are integrated into two compositional parallel programming languages: PCN <ref> [9, 16] </ref> and Fortran M [14]. PCN is a C-like language that integrates ideas from concurrent logic programming and imperative programming; Fortran M is a small set of extensions to Fortran. <p> In each case, it has proved possible to introduce the concepts in a manner that is consistent with other language concepts. 3.1 Program Composition Notation Program Composition Notation (PCN) <ref> [9, 16] </ref> is a high-level concurrent programming language with a C-like syntax that combines features of concurrent logic languages and imperative languages. Programs are constructed by using three composition operators | parallel, sequential, and choice | to compose procedure calls, other compositions, or primitive operations.
Reference: [10] <author> I. Chern and I. Foster, </author> <title> "Design and parallel implementation of two methods for solving PDEs on the sphere," in Parallel Computational Fluid Dynamics '91, </title> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1991. </year>
Reference-contexts: Here, we summarize some of these investigations, which provide insights into the strengths and weaknesses of the approach. 4.1 Parallel Solvers We first describe a study of parallel algorithms for numerical methods used to solve partial differential equations in spherical geometry, in which PCN was used to prototype algorithm variants <ref> [10] </ref>. Three different numerical methods were considered: a spectral transform method on a regular latitude/longitude grid, a finite difference method on overlapping stereoscopic grids, and a control volume on an icosahedral-hexagonal grid. In each case, the resulting parallel algorithms are complex, and there was a need to explore algorithmic variants. <p> Both have potential performance advantages: the former results in simpler communication patterns, while the latter makes more efficient use of available wires. We were able to experiment with both approaches. 10 These benefits were also observed when implementing the finite difference and control volume algorithms <ref> [10] </ref>. The study also revealed deficiencies in the PCN approach and tools. The use of two languages (PCN for the parallel harness, and Fortran for sequential computation) proved offputting to the applied mathematicians who assisted in the project.
Reference: [11] <author> A. Choudhary and R. Ponnusamy, </author> <title> "Parallel implementation and evaluation of a motion estimation system algorithm using several data decomposition strategies," </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> no. 14, </volume> <pages> pp. 50-65, </pages> <year> 1992. </year>
Reference-contexts: Both techniques are central to object-oriented design [3]. It is instructive to examine how these techniques can be applied in sequential and parallel programming. For illustrative purposes, we consider a simple example: the convolution operation used, for example, in motion estimation algorithms in image processing <ref> [11] </ref>. Each pair of images is first processed by using a two-dimensional fast Fourier transform (FFT). The resulting matrices are then multiplied, and an inverse FFT is applied to the result.
Reference: [12] <author> A. Chien and W. Dally, </author> <title> "Concurrent Aggregates," </title> <booktitle> in Proc. ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <year> 1990, </year> <pages> pp. 187-196. </pages>
Reference-contexts: Chien and Dally's Concurrent Aggregates (CA) language allows the definition of homogeneous collections of objects called aggregates; messages addressed to an aggregate are routed to one of its members <ref> [12] </ref>. As in this paper, concurrent structures can be defined and composed with other structures to build concurrent programs. However, resource allocation and locality are not addressed. Virtual computers have been used to achieve portability by hiding information concerning the size and topology of a physical computer.
Reference: [13] <author> J. Dongarra, R. van de Geijn, and D. Walker, </author> <title> "A look at scalable dense linear algebra libraries," </title> <booktitle> in Proc. Scalable High Performance Computing Conf., </booktitle> <address> Williamsburg, Virginia, </address> <year> 1992. </year>
Reference-contexts: Code reuse is rare, outside the specialized context of single-program multiple-data (SPMD) programming, where the same program is run on every processor and libraries can be called to perform common global operations <ref> [13, 25] </ref>. This paper presents programming language constructs that permit the benefits of modularity to be realized in parallel programs. The central ideas are as follows.
Reference: [14] <author> I. Foster and K. M. Chandy, </author> <title> "Fortran M: A language for modular parallel programming," </title> <type> Preprint MCS-P237-0992, </type> <institution> Argonne National Laboratory, Argonne Ill. </institution> <month> 60439, </month> <year> 1992. </year>
Reference-contexts: In this paper, we show how virtual computer constructs are integrated into two compositional parallel programming languages: PCN [9, 16] and Fortran M <ref> [14] </ref>. PCN is a C-like language that integrates ideas from concurrent logic programming and imperative programming; Fortran M is a small set of extensions to Fortran. <p> however, this cost is distributed over the physical processors on which the virtual computer is to be created and has not proved excessive on 500-processor computers. 3.2 Fortran M Fortran M is a small set of extensions to Fortran designed to support the modular construction of scientific and engineering applications <ref> [14] </ref>. A primary goal in designing Fortran M was to provide a minimal set of extensions that were consistent with Fortran concepts. The temptation to "fix" Fortran was avoided. We describe Fortran M for two reasons.
Reference: [15] <author> I. Foster, C. Kesselman, and S. Taylor, </author> <title> "Concurrency: Simple concepts and powerful tools," </title> <journal> The Computer Journal vol. </journal> <volume> 33, no. 6, </volume> <pages> pp. 501-507, </pages> <year> 1990. </year>
Reference-contexts: Compositional languages permit design decisions concerned with mapping, data distribution, communication, and scheduling to be made separately and to be modified without changing other aspects of a design <ref> [15] </ref>. In this paper, we show how virtual computer constructs are integrated into two compositional parallel programming languages: PCN [9, 16] and Fortran M [14]. PCN is a C-like language that integrates ideas from concurrent logic programming and imperative programming; Fortran M is a small set of extensions to Fortran. <p> The execution schedule is then determined by the availability of data and by the scheduling algorithm used to select executable processes. This approach is, of course, well known in actors, dataflow, parallel functional programming, and concurrent logic programming <ref> [1, 6, 21, 15] </ref>. 5 Interfaces. Concurrently-executing program components must be able to share data. As noted above, a mechanism is required that supports encapsulation, that is independent of resource allocation and scheduling decisions, and that is efficient on parallel computers. We achieve this as follows. Logical Resources. <p> Global Address Space. In order that mapping decisions can be changed without modifying other program components, we make no distinction at the language level between local and remote references to objects. Hence, a process can operate on a resource regardless of its location <ref> [15] </ref>. This requires some form of global address space, as provided, for example, by virtual channel and single-assignment variables. Concurrency. Concurrent module interfaces are achieved by permitting shared data (for example, arrays of virtual channels or single-assignment variables) to be distributed over processors and accessed concurrently by program components.
Reference: [16] <author> I. Foster, R. Olson, and S. Tuecke, </author> <title> "Productive parallel programming: The PCN approach," </title> <journal> Scientific Programming, </journal> <volume> vol. 1, no. 1, </volume> <year> 1992. </year>
Reference-contexts: Compositional languages permit design decisions concerned with mapping, data distribution, communication, and scheduling to be made separately and to be modified without changing other aspects of a design [15]. In this paper, we show how virtual computer constructs are integrated into two compositional parallel programming languages: PCN <ref> [9, 16] </ref> and Fortran M [14]. PCN is a C-like language that integrates ideas from concurrent logic programming and imperative programming; Fortran M is a small set of extensions to Fortran. <p> In each case, it has proved possible to introduce the concepts in a manner that is consistent with other language concepts. 3.1 Program Composition Notation Program Composition Notation (PCN) <ref> [9, 16] </ref> is a high-level concurrent programming language with a C-like syntax that combines features of concurrent logic languages and imperative languages. Programs are constructed by using three composition operators | parallel, sequential, and choice | to compose procedure calls, other compositions, or primitive operations.
Reference: [17] <author> I. Foster and S. Taylor, </author> <title> A compiler approach to scalable concurrent program design, </title> <journal> ACM Trans. Prog. Lang. Syst. </journal> <note> (to appear). </note>
Reference-contexts: The PCN extensions that support virtual computers, data distribution, and embedding have been incorporated into a PCN compiler <ref> [17] </ref>. A programmable source-to-source transformation system is used to translate the extended language into PCN as well as calls to a runtime library that manages virtual computers and handles requests for distributed data. This compiler has supported extensive experimentation with the language extensions, as described in Section 4.
Reference: [18] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu, </author> <title> "Fortran D language specification," </title> <type> Technical Report TR90-141, </type> <institution> Computer Science, Rice Univ., Houston, Texas, </institution> <year> 1990. </year> <month> 14 </month>
Reference-contexts: (i,j) - ? topology () ?= -"mesh",M,N-, i &lt; M, j &lt; N -&gt; return (M*i+j), default -&gt; return (-1) - Data distribution is supported in the form of blocked distributions of arrays of single-assignment variables. (In a blocked distribution, each processor is allocated a contiguous block of array elements <ref> [18] </ref>.) Elements of a distributed array are accessed in the same manner as ordinary array elements. More complex distributions and data structures, such as those supported in data-parallel programming languages [30, 18], can be integrated in the same manner, but are not supported in the current PCN compiler. <p> More complex distributions and data structures, such as those supported in data-parallel programming languages <ref> [30, 18] </ref>, can be integrated in the same manner, but are not supported in the current PCN compiler. A keyword port is used to declare these distributed arrays. <p> Virtual computers have been used to achieve portability by hiding information concerning the size and topology of a physical computer. Martin [26], Hudak [23], and Taylor [29] have investigated notations for specifying process mapping on a (potentially infinite) processing surface. In data-parallel languages <ref> [30, 18] </ref>, data distribution is specified with respect to a virtual computer, as proposed here; however, hierarchies cannot be defined. While these systems succeed in decoupling mapping or data distribution from other aspects of a parallel algorithm, they do not permit resource allocation decisions to be isolated from module definitions.
Reference: [19] <author> W. Griswold, G. Harrison, D. Notkin, and L. Snyder, </author> <title> "Port ensembles: A communi-cation abstraction for nonshared memory parallel programming," </title> <booktitle> in Proc. Intl Conf. on Parallel Processing, </booktitle> <address> Penn. </address> <publisher> State Press, </publisher> <year> 1990. </year>
Reference-contexts: The target hardware limits the programs that can be specified in these systems: in the iWARP work the contiguous submesh is the only virtual computer supported and the number of channels is limited. Griswold et al. propose process ensembles as a means of organizing data, computation, and communication <ref> [19] </ref>. However, they do not consider hierarchies of virtual computers. Browne et al. propose a compositional calculus for specifying interconnections between software chips [5]. The integration of this calculus into a programming notation is not discussed, and the notion of virtual computer is absent.
Reference: [20] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification Version 0.4, </title> <type> Technical Report, </type> <institution> CITI/CRPC, Rice University, </institution> <month> November 6, </month> <year> 1992. </year>
Reference-contexts: Channel operations are modeled on Fortran file I/O constructs. Mapping is specified with respect to virtual computers. In keeping with Fortran concepts, virtual computers are N -dimensional arrays of virtual processors. As in High Performance Fortran (HPF) <ref> [20] </ref>, a PROCESSORS declaration is used to define the size and shape 8 of the processor array that is to apply in a particular process, and the inquiry functions Number Of Processors and Processor Shape permit a process to determine the size and shape of the computer in which it executes.
Reference: [21] <author> P. Henderson, </author> <title> Functional Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1980. </year>
Reference-contexts: The execution schedule is then determined by the availability of data and by the scheduling algorithm used to select executable processes. This approach is, of course, well known in actors, dataflow, parallel functional programming, and concurrent logic programming <ref> [1, 6, 21, 15] </ref>. 5 Interfaces. Concurrently-executing program components must be able to share data. As noted above, a mechanism is required that supports encapsulation, that is independent of resource allocation and scheduling decisions, and that is efficient on parallel computers. We achieve this as follows. Logical Resources.
Reference: [22] <author> C. Hoare, </author> <title> "Communicating sequential processes," </title> <journal> Commun. ACM, </journal> <volume> vol. 21, no. 8, </volume> <pages> pp. 666-677, </pages> <year> 1978. </year>
Reference: [23] <author> P. Hudak, </author> <title> "Para-functional programming," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 60-70, </pages> <year> 1986. </year>
Reference-contexts: As in this paper, concurrent structures can be defined and composed with other structures to build concurrent programs. However, resource allocation and locality are not addressed. Virtual computers have been used to achieve portability by hiding information concerning the size and topology of a physical computer. Martin [26], Hudak <ref> [23] </ref>, and Taylor [29] have investigated notations for specifying process mapping on a (potentially infinite) processing surface. In data-parallel languages [30, 18], data distribution is specified with respect to a virtual computer, as proposed here; however, hierarchies cannot be defined.
Reference: [24] <author> Inmos Limited, </author> <title> occam 2 Reference Manual. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Borkhar et al. [4] propose that parallel programs be constructed by plugging together "cells," in a manner analogous to VLSI. They use this technique to generate efficient programs for the iWarp systolic processor. occam <ref> [24] </ref> has been used for similar purposes. The target hardware limits the programs that can be specified in these systems: in the iWARP work the contiguous submesh is the only virtual computer supported and the number of channels is limited.
Reference: [25] <author> M. Lemke and D. Quinlan, </author> <title> "A parallel C++ array class library for architecture-independent development of structured grid applications," </title> <journal> ACM SIGPLAN Notices vol. </journal> <volume> 28, no. 1, </volume> <pages> pp. 21-23, </pages> <year> 1992. </year>
Reference-contexts: Code reuse is rare, outside the specialized context of single-program multiple-data (SPMD) programming, where the same program is run on every processor and libraries can be called to perform common global operations <ref> [13, 25] </ref>. This paper presents programming language constructs that permit the benefits of modularity to be realized in parallel programs. The central ideas are as follows.
Reference: [26] <author> A. Martin, </author> <title> "The torus: An exercise in constructing a processing surface," </title> <booktitle> in Proc. Conf. on VLSI, </booktitle> <address> Caltech, </address> <year> 1979, </year> <pages> pp. 52-57. </pages>
Reference-contexts: As in this paper, concurrent structures can be defined and composed with other structures to build concurrent programs. However, resource allocation and locality are not addressed. Virtual computers have been used to achieve portability by hiding information concerning the size and topology of a physical computer. Martin <ref> [26] </ref>, Hudak [23], and Taylor [29] have investigated notations for specifying process mapping on a (potentially infinite) processing surface. In data-parallel languages [30, 18], data distribution is specified with respect to a virtual computer, as proposed here; however, hierarchies cannot be defined.
Reference: [27] <author> D. Parnas, </author> <title> "On the criteria to be used in decomposing systems into modules," </title> <journal> Com-mun. ACM vol. </journal> <volume> 15, no. 2, </volume> <pages> pp. 1053-1058, </pages> <year> 1972. </year>
Reference-contexts: 1 Introduction In sequential programming, modular and object-oriented design and programming techniques are well understood and widely used to reduce complexity, permit separate development of components, and encourage reuse <ref> [31, 27, 3] </ref>. In parallel programming, the situation is less advanced. Parallel programs are, for the most part, developed in an ad-hoc fashion, as monolithic entities that cannot easily be adapted to changing circumstances. <p> Stepwise refinement [31] the decomposition of a complex problem into simpler subproblems, each solved by a separate module with a well-defined interface. 2. Modular decomposition <ref> [27] </ref> the isolation in separate modules of design decisions that are difficult, likely to change, or common to several program components.
Reference: [28] <author> A. Skjellum and A. Leung, </author> <title> "Zipcode: A portable multicomputer communication library atop the Reactive Kernel," </title> <booktitle> in Proc. 5th Distributed Memory Concurrent Computing Conf., </booktitle> <publisher> IEEE Press, </publisher> <year> 1990, </year> <pages> pp. 767-776. </pages>
Reference-contexts: The integration of this calculus into a programming notation is not discussed, and the notion of virtual computer is absent. Some recent work on parallel message-passing libraries has explored the use of "process groups" and "communication contexts" to support the encapsulation of communication operations in parallel libraries <ref> [28, 2] </ref>. Chien and Dally's Concurrent Aggregates (CA) language allows the definition of homogeneous collections of objects called aggregates; messages addressed to an aggregate are routed to one of its members [12]. As in this paper, concurrent structures can be defined and composed with other structures to build concurrent programs.
Reference: [29] <author> S. Taylor, </author> <title> Parallel Logic Programming Techniques. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: However, resource allocation and locality are not addressed. Virtual computers have been used to achieve portability by hiding information concerning the size and topology of a physical computer. Martin [26], Hudak [23], and Taylor <ref> [29] </ref> have investigated notations for specifying process mapping on a (potentially infinite) processing surface. In data-parallel languages [30, 18], data distribution is specified with respect to a virtual computer, as proposed here; however, hierarchies cannot be defined.
Reference: [30] <author> Thinking Machines Corporation, </author> <title> CM Fortran Reference Manual, Thinking Machines, </title> <address> Cambridge, Mass., </address> <year> 1989. </year>
Reference-contexts: More complex distributions and data structures, such as those supported in data-parallel programming languages <ref> [30, 18] </ref>, can be integrated in the same manner, but are not supported in the current PCN compiler. A keyword port is used to declare these distributed arrays. <p> Virtual computers have been used to achieve portability by hiding information concerning the size and topology of a physical computer. Martin [26], Hudak [23], and Taylor [29] have investigated notations for specifying process mapping on a (potentially infinite) processing surface. In data-parallel languages <ref> [30, 18] </ref>, data distribution is specified with respect to a virtual computer, as proposed here; however, hierarchies cannot be defined. While these systems succeed in decoupling mapping or data distribution from other aspects of a parallel algorithm, they do not permit resource allocation decisions to be isolated from module definitions.
Reference: [31] <author> N. Wirth, </author> <title> "Program development by stepwise refinement," </title> <journal> Commun. ACM vol. </journal> <volume> 14, </volume> <pages> pp. 221-227, </pages> <year> 1971. </year>
Reference-contexts: 1 Introduction In sequential programming, modular and object-oriented design and programming techniques are well understood and widely used to reduce complexity, permit separate development of components, and encourage reuse <ref> [31, 27, 3] </ref>. In parallel programming, the situation is less advanced. Parallel programs are, for the most part, developed in an ad-hoc fashion, as monolithic entities that cannot easily be adapted to changing circumstances. <p> The next three sections of this paper address each of these issues in turn, after which we review related work and present our conclusions. 2 Modularity and Parallel Programs We use the term "modularity" to refer to two related program-structuring techniques: 1. Stepwise refinement <ref> [31] </ref> the decomposition of a complex problem into simpler subproblems, each solved by a separate module with a well-defined interface. 2. Modular decomposition [27] the isolation in separate modules of design decisions that are difficult, likely to change, or common to several program components.
References-found: 31

