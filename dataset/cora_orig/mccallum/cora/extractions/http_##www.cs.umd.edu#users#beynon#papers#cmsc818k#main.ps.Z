URL: http://www.cs.umd.edu/users/beynon/papers/cmsc818k/main.ps.Z
Refering-URL: http://www.cs.umd.edu/users/beynon/
Root-URL: 
Email: beynon@cs.umd.edu  
Title: Experiments with Parallel I/O  
Author: Michael D. Beynon 
Date: 301-405-7889  
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Abstract: Commodity hardware and operating systems can be used in place of expensive specialty parallel machines. On such Peer-to-Peer systems, high bandwidth I/O is needed for demanding applications. This work attempts to provide insight into what is needed to get acceptable performance from Peer-to-Peer style Parallel I/O using our Alpha Farm cluster of workstations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anurag Acharya, Mustafa Uysal, Robert Bennett, Assaf Mendelson, Michael Beynon, Jeffrey K. Hollingsworth, Joel Saltz, and Alan Sussman. </author> <title> Tuning the performance of i/o intensive parallel applications. </title> <publisher> ACM Press, </publisher> <year> 1996. </year>
Reference-contexts: Nodes one through six were actually connected to the atm switch, and all nodes were connected to the Ethernet. 2.2 Jovian-2 I/O Library Jovian-2 <ref> [1] </ref> is a parallel I/O library developed at the University of Maryland. It presents a simple interface to the user, similar in function to the POSIX lio listio () routines, where multiple requests for disk I/O can be handled in a single call. <p> The library can be used many ways, but in keeping with my initial goal of following the spirit of commodity hardware and non-dedicated servers, the library will be configured to act in a peer-to-peer fashion as shown in Figure 1 (b) from <ref> [1] </ref>. This means simply, that each node's process will act as both a client as well as a server. This introduces the problem of arbitration between the server and client portion of each node. <p> Note that higher block sizes than shown in Table 2 do not achieve a remarkable increase in I/O rate. This is in contrast to the SP-2 performance where 1MB sized blocks achieved the best performance <ref> [1] </ref>. Note these values can vary substantially from run to run. A more thorough test should be conducted for all nodes, for varying outstanding numbers of asynchronous I/O calls and for repeated runs. <p> Small speedups in these synchronous parts could contribute to significant overall system speedups. * Porting of real applications, like the AVHRR earth science applications pathfinder and climate used in a previous client-server study <ref> [1] </ref>. The small benchmarks studied here were suitable to show promise, but real applications are needed to stress the system in less artificial ways. * Investigate different scheduling algorithms more tailored to high band width Parallel I/O.
Reference: [2] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <type> Technical Report CS-94-230, </type> <institution> Computer Science Dept., University of Tennessee, </institution> <month> April </month> <year> 1994. </year> <journal> Also appears in the International Journal of Supercomputer Applications, </journal> <volume> Volume 8, Number 3/4, </volume> <year> 1994. </year>
Reference-contexts: Part of this work on the Alphas was getting away from the proprietary IBM Message Passing Library (MPL) used on the SP-2 machine that Jovian-2 was initially developed on. For reasons of availability and portability, the standard Message Passing Interface (MPI) library was chosen <ref> [2] </ref>. Only simple non-blocking sends and receives are used for communication between nodes, so the entire complexity of MPI is not really needed. Since MPI is a specification of a standard and not a piece of software, several implementations are available.
Reference: [3] <author> Nick Nevin. </author> <title> The performance of lam 6.0 and mpich 1.0.12 on a workstation cluster. </title> <institution> Ohio Supercomputing Center Technical Report, OSC-TR-1996-4., </institution> <year> 1996. </year>
Reference-contexts: MPICH version 1.0.12 was also used during development. The LAM/MPI implementation has been shown to be stable and perform well for large messages [5], and is shown to outperform MPICH in many benchmarks 2 (a) (b) models <ref> [3] </ref>. The library can be used many ways, but in keeping with my initial goal of following the spirit of commodity hardware and non-dedicated servers, the library will be configured to act in a peer-to-peer fashion as shown in Figure 1 (b) from [1].
Reference: [4] <author> Bill Norcutt. </author> <title> IOZONE benchmark program. </title> <note> Available on many ftp archives, such as ftp://ftp.cs.umn.edu/packages/FreeBSD/2.0.5-RELEASE/ports/utils/-iozone, </note> <year> 1991. </year>
Reference-contexts: Table 1 shows how the LAM implementation of MPI performed for blocking sends of 128K blocks between two nodes. 3.2 Disk Performance To measure the disk performance of a single node, the diskperf benchmark was used. This is loosely modeled after the iozone benchmark <ref> [4] </ref>. iozone determines the possible I/O bandwidth from a user program for both reads and 3 Ethernet ATM send 128k msg trial 1 0.63 9.77 send 128k msg trial 2 0.55 9.71 send 128k msg trial 3 0.67 9.75 Table 1: MPI blocking send single node user-level I/O rates (MB/sec). writes
Reference: [5] <author> Natawut Nupairoj and Lionel M. Ni. </author> <title> Performance evaluation of some mpi imple mentations on workstation clusters. </title> <address> IEEEE 0-8186-6895-4/95, </address> <year> 1995. </year> <month> 10 </month>
Reference-contexts: MPICH version 1.0.12 was also used during development. The LAM/MPI implementation has been shown to be stable and perform well for large messages <ref> [5] </ref>, and is shown to outperform MPICH in many benchmarks 2 (a) (b) models [3].
References-found: 5

