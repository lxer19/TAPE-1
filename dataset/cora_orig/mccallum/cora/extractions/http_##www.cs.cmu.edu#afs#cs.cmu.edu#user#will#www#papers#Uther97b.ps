URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/will/www/papers/Uther97b.ps
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: futher,velosog@cs.cmu.edu  
Title: Generalizing Adversarial Reinforcement Learning  
Author: William T. B. Uther and Manuela M. Veloso 
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract: Reinforcement Learning has been used for a number of years in single agent environments. This article reports on our investigation of Reinforcement Learning techniques in a multi-agent and adversarial environment with continuous observable state information. Our framework for evaluating algorithms is two-player hexagonal grid soccer. We introduce an extension to Prioritized Sweeping that allows generalization of learnt knowledge over neighboring states in the domain and we introduce an extension to the U Tree generalizing algorithm that allows the handling of continuous state spaces. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baird, L. C. </author> <year> 1995. </year> <title> Residual algorithms: Reinforcement learning with function approximation. </title> <editor> In Prieditis, A., and Russell, S., eds., </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference (ICML95), </booktitle> <pages> 30-37. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: This has been shown to be unstable in some cases (Boyan & Moore 1995) never converging to a solution, although some good results have been obtained with this method (Tesauro 1995). Stable Methods exist for using either Neural Nets <ref> (Baird 1995) </ref> or Decision Trees (McCallum 1995). First we introduce a stable method which uses both a table and a generalizing function approximator. Then McCallum's (1995) method using Decision Trees is described and we extend it to handle continuous state values. Finally we compare the methods.
Reference: <author> Boyan, J. A., and Moore, A. W. </author> <year> 1995. </year> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In Tesauro, G.; Touretzky, D. S.; and Leen, T. K., eds., </editor> <booktitle> Advances in Neural Information Processing Systems, volume 7. </booktitle> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: One naive generalization method is to take a simple model-free algorithm like Q Learning and replace the table of values with a generalizing function approxima-tor (e.g. a Neural Net). This has been shown to be unstable in some cases <ref> (Boyan & Moore 1995) </ref> never converging to a solution, although some good results have been obtained with this method (Tesauro 1995). Stable Methods exist for using either Neural Nets (Baird 1995) or Decision Trees (McCallum 1995).
Reference: <author> Boyan, J. A., and Moore, A. W. </author> <year> 1996. </year> <title> Learning evaluation functions for large acyclic domains. </title> <editor> In Saitta, L., ed., </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference (ICML96). </booktitle> <address> San Ma-teo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification And Regression Trees. </title> <booktitle> The Wadsworth Statistics/Probability Series. </booktitle> <address> Monterey, California: </address> <publisher> Wadsworth and Brooks/Cole Advanced Books and Software. </publisher>
Reference: <author> Kaelbling, L. P.; Littman, M. L.; and Moore, A. W. </author> <year> 1996. </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research 4 </journal> <volume> 237-285. Fitted Prioritized Sweeping Continuous U Tree First 500 games 246 101 49% 20% 254 101 51% 20% Second 500 games 223 73 45% 15% 277 73 55% 15% Total 461 91 46% 9% 539 91 54% 9% Table 3: </volume> <editor> Fitted Prioritized Sweeping vs. </editor> <title> Continuous U Tree Littman, </title> <editor> M. L. </editor> <year> 1994. </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference (ICML94), </booktitle> <pages> 157-163. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Indeed, one of the first areas studied in Artificial Intelligence was game playing. For example, the pioneering checkers playing algorithm by Samuel (1959) used both search and machine learning strategies. Interestingly, his approach is similar to modern Reinforcement Learning techniques <ref> (summarized in Kaelbling, Littman, & Moore 1996) </ref> in that both learn an evaluation function based on experience and a Markov assumption about the world. In the Reinforcement Learning paradigm an agent is placed in a situation without knowledge of any goals or other information about the environment. <p> Initially, it does not know that the objective of the game is to take the ball to a goal position. Reinforcement learning seems the appropriate technique to acquire the necessary action selection information for each state. Reinforcement Learning Revisited In the machine learning formulation of Reinforcement Learning <ref> (Kaelbling, Littman, & Moore 1996) </ref> there are a discrete set of states, s, and actions, a. The agent can detect its current state, and in each state can choose to perform an action which will in turn move it to its next state.
Reference: <author> McCallum, A. K. </author> <year> 1995. </year> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> Phd. thesis, </type> <institution> Department of Computer Science, University of Rochester, Rochester, NY. </institution>
Reference-contexts: Traditional Reinforcement Learning algorithms rely on a prior discretization and do not generalize over multiple states in that generalization. In this paper we introduce an extension to Prioritized Sweeping (Moore & Atkeson 1993), Fitted Prioritized Sweeping, and a modification of the U Tree algorithm <ref> (McCallum 1995) </ref>, Continuous U Tree, as examples of algorithms that generalize over multiple states. Tesauro (1995) and Thrun (1995) have both used neural nets in a Reinforcement Learning paradigm. <p> This has been shown to be unstable in some cases (Boyan & Moore 1995) never converging to a solution, although some good results have been obtained with this method (Tesauro 1995). Stable Methods exist for using either Neural Nets (Baird 1995) or Decision Trees <ref> (McCallum 1995) </ref>. First we introduce a stable method which uses both a table and a generalizing function approximator. Then McCallum's (1995) method using Decision Trees is described and we extend it to handle continuous state values. Finally we compare the methods. <p> If the split with the best combined 2 statistic is better than no split then that split is recorded and the splitting proceeds recursively. Continuous U Tree The U Tree algorithm <ref> (McCallum 1995) </ref> is a method for using a decision or regression tree (Breiman et al. 1984; Quinlan 1986) instead of a table of values in the Prioritized Sweeping algorithm. In the original U Tree algorithm the state space is described in terms of discrete attributes.
Reference: <author> Moore, A., and Atkeson, C. G. </author> <year> 1993. </year> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <booktitle> Machine Learning 13. </booktitle>
Reference-contexts: Over time the agent is learning to act so that it wins the game. Traditional Reinforcement Learning algorithms rely on a prior discretization and do not generalize over multiple states in that generalization. In this paper we introduce an extension to Prioritized Sweeping <ref> (Moore & Atkeson 1993) </ref>, Fitted Prioritized Sweeping, and a modification of the U Tree algorithm (McCallum 1995), Continuous U Tree, as examples of algorithms that generalize over multiple states. Tesauro (1995) and Thrun (1995) have both used neural nets in a Reinforcement Learning paradigm. <p> The value function is a function from states to sums of discounted reward. It is the expected sum of discounted reward for behaving optimally in that state as well as from then on. Prioritized Sweeping <ref> (Moore & Atkeson 1993) </ref> builds a model of the world by recording state transition probabilities and the reward associated with each state and action. Using this model and the Bellman equations Prioritized Sweeping can calculate Q values directly.
Reference: <author> Press, W. H.; Teukolsky, S. A.; Vetterling, W. T.; and Flannery, B. P. </author> <year> 1992. </year> <title> Numerical Recipies in C: </title> <booktitle> the art of scientific computing. </booktitle> <address> Cambridge: </address> <publisher> Cambridge University Press, 2nd edition. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages>
Reference: <author> Samuel, A. L. </author> <year> 1959. </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Research Journal 3(3). </journal>
Reference: <author> Tesauro, G. </author> <year> 1992. </year> <title> Practical issues in temporal difference learning. </title> <booktitle> Machine Learning 8 </booktitle> <pages> 257-277. </pages>
Reference: <author> Tesauro, G. </author> <year> 1995. </year> <title> Temporal difference learning and td-gammon. </title> <journal> Communications of the ACM 38(3) </journal> <pages> 58-67. </pages>
Reference-contexts: This has been shown to be unstable in some cases (Boyan & Moore 1995) never converging to a solution, although some good results have been obtained with this method <ref> (Tesauro 1995) </ref>. Stable Methods exist for using either Neural Nets (Baird 1995) or Decision Trees (McCallum 1995). First we introduce a stable method which uses both a table and a generalizing function approximator.
Reference: <author> Thrun, S. </author> <year> 1995. </year> <title> Learning to play the game of chess. </title>
Reference: <editor> In Tesauro, G., and Touretzky, D. S., eds., </editor> <booktitle> Advances in Neural Information Processing Systems, volume 7. </booktitle> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference: <author> Uther, W. T. B., and Veloso, M. M. </author> <year> 1997. </year> <title> Adversarial reinforcement learning. </title> <journal> Journal of Artificial Intelligence Research. </journal> <note> To be submitted. </note>
Reference: <author> Watkins, C. J. C. H., and Dayan, P. </author> <year> 1992. </year> <title> Q-learning. </title> <booktitle> Machine Learning 8(3) </booktitle> <pages> 279-292. </pages>
Reference-contexts: The Q function is a function from state/action pairs to an expected sum of discounted reward <ref> (Watkins & Dayan 1992) </ref>. The result is the expected discounted reward for executing that action in that state then behaving optimally from then on. The value function is a function from states to sums of discounted reward.
References-found: 16

