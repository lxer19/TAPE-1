URL: http://www.cs.ucsd.edu/~rik/papers/sigir92/lsi2.ps
Refering-URL: http://www.cs.ucsd.edu/~rik/bibliography3_5.html
Root-URL: http://www.cs.ucsd.edu
Title: Latent Semantic Indexing is an Optimal Special Case of Multidimensional Scaling  
Author: Brian T. Bartell Garrison W. Cottrell Richard K. Belew 
Address: La Jolla, California 92093-0114  
Affiliation: Department of Computer Science Engineering-0114 University of California, San Diego  
Abstract: Latent Semantic Indexing (LSI) is a technique for representing documents, queries, and terms as vectors in a multidimensional real-valued space. The representations are approximations to the original term space encoding, and are found using the matrix technique of Singular Value Decomposition. In comparison, Multidimensional Scaling (MDS) is a class of data analysis techniques for representing data points as points in a multidimensional real-valued space. The objects are represented so that inter-point similarities in the space match inter-object similarity information provided by the researcher. We illustrate how the document representations given by LSI are equivalent to the optimal representations found when solving a particular MDS problem in which the given inter-object similarity information is provided by the inner product similarities between the documents themselves. We further analyze a more general MDS problem in which the inter-document similarity information, although still in inner product form, is arbitrary with respect to the vector space encoding of the documents. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Borg and J. Lingoes. </author> <title> Multidimensional Similarity Structure Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: The similarity can be measured using, for example, the cosine measure, as was done in the original work [2]. 3 Multidimensional Scaling Multidimensional Scaling (MDS) <ref> [1] </ref> is a general class of data analysis, data reduction, and modeling techniques. MDS is used to find representations in &lt; k of objects, such that the similarities between the objects in the k-space correspond to known inter-object similar-ity information. <p> Thus, SVD tells us how to decompose the symmetric similarity matrix S into the product of the transpose of a real matrix and itself. This is a well known procedure in the scaling of inner product similarities [6, p. 209] <ref> [1, pp. 270-291] </ref>. <p> The k largest singular vectors of G provided the optimal representation of the data in k-space to preserve S. Finding W such that WX = G k in this special case was particularly easy. 3 Borg & Lingoes <ref> [1, pp. 292-295] </ref> discuss the necessary conditions (i.e. positive semi-definiteness) for an arbitrary symmetric matrix to be decomposed as B T B, for real B. Unfortunately, this procedure does not provide an optimal solution to the current more general problem.
Reference: [2] <author> Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harsh-man. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: Latent Semantic Indexing (LSI) <ref> [2] </ref> is a particular approach aimed at addressing these limitations. This technique maps each document from a vector space representation based on keyword frequency [8], to a vector in a lower dimensional space. Terms are also mapped to vectors in the reduced space. <p> We will emphasize the re-representation of documents here, although corresponding arguments can be made for terms as well. According to Deerwester, et. al. <ref> [2] </ref>, this reduced document representation has the advantages that * the dimensions in the space are uncorrelated (i.e. they are orthogonal), * the representations are less noisy, and * the representations incorporate higher-order (latent) association structure among terms and doc uments. <p> Thus, the similarity of each document to a query q is found by measuring the similarity between rows of A k L k and the vector qU k . The similarity can be measured using, for example, the cosine measure, as was done in the original work <ref> [2] </ref>. 3 Multidimensional Scaling Multidimensional Scaling (MDS) [1] is a general class of data analysis, data reduction, and modeling techniques. MDS is used to find representations in &lt; k of objects, such that the similarities between the objects in the k-space correspond to known inter-object similar-ity information. <p> We now discuss some implications of these results. 6.1 Implications for Latent Semantic Indexing This analysis is intended to have a complementary relationship with previous work on Latent Semantic Indexing. Specifically, previous work <ref> [2] </ref> has illustrated beneficial properties of the technique (such as reduction of noise, orthogonalization of the vector space, and incorporation of associational relationships in the representation), and the equivalence with Multidimensional Scaling does not detract from these results. <p> In this case, our analysis suggests that inner product is a well motivated metric to use in the reduced space. The analysis further suggests that certain term weightings in the original term space (which is permitted in LSI <ref> [2] </ref>) should improve the performance of the technique. That is, weighting the elements of the vector representation, to incorporate the frequency of each term throughout the documents for example, may improve the retrieval performance using the inner product measure.
Reference: [3] <author> Susan T. Dumais. </author> <title> Enhancing performance in latent semantic indexing (LSI) retrieval. </title> <type> Technical Report Technical Memorandum, </type> <institution> Bellcore, </institution> <month> Septem-ber </month> <year> 1990. </year>
Reference-contexts: When S is a better measure of relatedness than S, we may anticipate that LSI will perform better when operating on X than on X. Dumais' empirical study of LSI using various term weightings seems to agree with this suggestion <ref> [3] </ref>: Term weightings which tend to improve inner product retrieval in the original term space also tend to improve retrieval performance in the reduced space. In past applications of Latent Semantic Indexing, similarity in the reduced space has been measured by the cosine measure rather than by inner product.
Reference: [4] <author> James E. Everett and Antony Pecotich. </author> <title> A combined loglinear/MDS model for mapping journals by citation analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 42(6) </volume> <pages> 405-413, </pages> <year> 1991. </year>
Reference-contexts: Given some measure of the similarity between pairs of objects in a domain, MDS represents the objects as points in a multidimensional real-valued space, such that the inter-point distances correspond well with the imposed similarity information. In the work of Everett and Pecotich <ref> [4] </ref>, for example, MDS is used to visualize the interrelationships among journals by using citation frequency as a measure of journal "similarity". The main goal of this paper is to illustrate the commonality between these two techniques.
Reference: [5] <author> George W. Furnas, Thomas K. Landauer, L. M. Gomez, and Susan T. Dumais. </author> <title> The vocabulary problem in human-system communications. </title> <journal> Communications of the ACM, </journal> (30):964-971, 1987. 
Reference-contexts: 1 Introduction There is currently a great deal of interest in automatic document indexing schemes which are not based simply on the matching of keywords in the documents. This is partly motivated by the observation <ref> [5] </ref> that individual keywords are not adequate discriminators of seman tic content. Rather, the indexing relationship between word and document content is many-to-many: A number of concepts can be indexed by a single term, and a number of terms can index a single concept.
Reference: [6] <author> Michael J. Greenacre and Leslie G. Underhill. </author> <title> Scaling a data matrix in a low-dimensional euclidean space. </title> <editor> In Douglas M. Hawkins, editor, </editor> <booktitle> Topics in Applied Multivariate Analysis, </booktitle> <pages> pages 183-268. </pages> <publisher> Cambridge University Press, </publisher> <year> 1982. </year>
Reference-contexts: Thus, SVD tells us how to decompose the symmetric similarity matrix S into the product of the transpose of a real matrix and itself. This is a well known procedure in the scaling of inner product similarities <ref> [6, p. 209] </ref> [1, pp. 270-291].
Reference: [7] <author> J. B. Kruskal. </author> <title> Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. </title> <journal> Psychometrika, </journal> <volume> 29(1) </volume> <pages> 1-27, </pages> <month> March </month> <year> 1964. </year>
Reference-contexts: Furthermore, the similarity information can be metric (indicating the exact target similarities for the configuration of points), or non-metric (indicating only the relative ordering of inter-object similarities <ref> [7] </ref>). Related to the type of similarity information is the choice of fitness measure used to evaluate how well a particular configuration of points corresponds to the similarity information. Also, the mapping from an object to a point in &lt; k may be arbitrary or functionally constrained.
Reference: [8] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, Inc., </publisher> <year> 1983. </year>
Reference-contexts: Latent Semantic Indexing (LSI) [2] is a particular approach aimed at addressing these limitations. This technique maps each document from a vector space representation based on keyword frequency <ref> [8] </ref>, to a vector in a lower dimensional space. Terms are also mapped to vectors in the reduced space. <p> Section 6 addresses the value of this analysis, and points to directions for future research. 2 Latent Semantic Indexing and the SVD Latent Semantic Indexing begins with a vector space representation of documents <ref> [8] </ref>, and attempts to improve retrieval performance by re-representing both documents and terms in a new vector space with smaller dimension. We will emphasize the re-representation of documents here, although corresponding arguments can be made for terms as well.
Reference: [9] <author> Gilbert W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <year> 1973. </year>
Reference-contexts: A useful property of SVD is that it provides the best lower rank approximation of a matrix X in terms of the Euclidean matrix norm (or Frobenius norm, calculated by taking the square root of the sum of all squared entries of a matrix) <ref> [9] </ref>. More precisely, let U k be the t fi k (k r) matrix found by removing r k columns from U. The k columns remaining in U k correspond to the largest singular values in L (similar versions of L k and A k can be defined).
Reference: [10] <author> W. S. Torgerson. </author> <title> Theory and Methods of Scaling. </title> <address> New York: </address> <publisher> John Wiley, </publisher> <year> 1958. </year>
Reference-contexts: We proceed by deriving the optimal solution to the MDS problem. This solution is well known in the Multidimensional Scaling literature (e.g., <ref> [10] </ref>). We then demonstrate its correspondence with Latent Semantic Indexing. 4.1 Scaling of Data-Derived Inner Product Similarities Let X (t fi d) be a matrix of d objects represented in &lt; t .
References-found: 10

