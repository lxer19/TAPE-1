URL: http://www.icsi.berkeley.edu/~bilmes/icslp98.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/~bilmes/
Root-URL: http://www.icsi.berkeley.edu
Email: &lt;bilmes@cs.berkeley.edu&gt;  
Title: DATA-DRIVEN EXTENSIONS TO HMM STATISTICAL DEPENDENCIES  
Author: Jeff A. Bilmes 
Address: 1947 Center Street, Suite 600 Berkeley, CA 94704, USA  Berkeley, CA 94720, USA  
Affiliation: International Computer Science Institute  CS Division, Department of EECS University of California at Berkeley  
Note: To appear at ICSLP'98, Sydney, Austrailia  
Abstract: In this paper, a new technique is introduced that relaxes the HMM conditional independence assumption in a principled way. Without increasing the number of states, the modeling power of an HMM is increased by including only those additional probabilistic dependencies (to the surrounding observation context) that are believed to be both relevant and discriminative. Conditional mutual information is used to determine both relevance and discrim-inability. Extended Gaussian-mixture HMMs and new EM update equations are introduced. In an isolated word speech database, results show an average 34% word error improvement over an HMM with the same number of states, and a 15% improvement over an HMM with a comparable number of parameters. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J.A. Bilmes. </author> <title> Maximum mutual information based reduction strategies for cross-correlation based joint distributional modeling. </title> <booktitle> In Proc. IEEE ICASSP, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1998. </year>
Reference-contexts: Does additional information typically exist in the surrounding context given Q? Figure 4 shows a conditional mutual informa tion density plot I (Df; `jQ) = avg ij=Df I (X ti ; X t`;j jQ) in bits per unit area computed (as in <ref> [1] </ref>) from a 2 hour random selec tion of the Switchboard continuous-speech database where X ti is the i th element of the random vector X t and ` is time-lag. <p> Full EM training is performed until convergence is achieved and then HMM word error is calculated. Using the HMMs, the Viterbi path is computed for each word determining the state of each frame. Conditional mutual information is computed (as described in <ref> [1] </ref>) using the resulting labels. The BMM dependency selection algorithm of Section 3 is performed. The BMMs are trained starting with the means and covariances given by the corresponding HMM and with initial dependency link values set to zero.
Reference: 2. <author> H. Bourlard and N. Morgan. </author> <title> Connectionist Speech Recognition: A Hybrid Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: To increase the discriminability between different 2 The notation X 1:N represents the set fX 1 ; : : : ; X N g. 3 This might sound like an egregious mistake but it is actually quite common and can be beneficial in practice, e.g., delta features, hybrid ANN/HMM systems <ref> [2] </ref>, etc. The theoretical problems could potentially be eliminated if each probability distribution is considered a potential function (as in a Markov Random Field) and if appropriate normalization terms are used for each HMM.
Reference: 3. <author> B.-H. Juang and L.R. Rabiner. </author> <title> Mixture autoregressive hidden markov models for speech signals. </title> <journal> IEEE Trans. ASSP, </journal> <volume> 33(6) </volume> <pages> 1404-1413, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: This approach is distinct from previous work <ref> [3, 8, 4, 9] </ref> in that the dependency structure may be sparse and may change for each value of Q t rather than depending on an additional, fixed, and arbitrarily chosen sets of observations. <p> With z containing observations only from x's past, these equations alone constitute a generalization of auto-regressive HMMs [6, 7] (d = 1, M = 1, V = 1), vector-valued auto-regressive HMMs [4, 9, 8] 6 (d &gt; 1, M = 1, V = 1 ), mixture auto-regressive HMMs <ref> [3] </ref> (d = 1,M &gt; 1,V = 1), and the usual Gaussian mixture models (d = 1,M &gt; 1,V = 1,s = 0). With V &gt; 1 and M &gt; 1, this model can be considered a mixture of mixtures.
Reference: 4. <author> P. Kenny, M. Lennig, and P. Mermelstein. </author> <title> A linear predictive HMM for vector-valued observations with applications to speech recognition. </title> <journal> IEEE Trans. ASSP, </journal> <volume> 38(2) </volume> <pages> 220-225, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: This approach is distinct from previous work <ref> [3, 8, 4, 9] </ref> in that the dependency structure may be sparse and may change for each value of Q t rather than depending on an additional, fixed, and arbitrarily chosen sets of observations. <p> The dfi (s+1)-sized B qmv matrices have a sparse structure determined by the BMM dependencies for state q. With z containing observations only from x's past, these equations alone constitute a generalization of auto-regressive HMMs [6, 7] (d = 1, M = 1, V = 1), vector-valued auto-regressive HMMs <ref> [4, 9, 8] </ref> 6 (d &gt; 1, M = 1, V = 1 ), mixture auto-regressive HMMs [3] (d = 1,M &gt; 1,V = 1), and the usual Gaussian mixture models (d = 1,M &gt; 1,V = 1,s = 0).
Reference: 5. <author> M. Ostendorf, V. Digalakis, and O. Kimball. </author> <title> From HMM's to segment models: A unified view of stochastic modeling for speech recognition. </title> <journal> IEEE Trans. Speech and Audio Proc., </journal> <volume> 4(5) </volume> <pages> 360-378, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: And rather than depending on a location in a segment trajectory <ref> [5] </ref>, the dependencies are data-derived; using conditional mutual information, the dependencies are chosen to provide new and discriminative information about X t not already provided by the current value of Q t . This potentially leads to a more accurate statistical model without a large free-parameter increase.
Reference: 6. <author> A.B. Poritz. </author> <title> Linear predictive hidden markov models and the speech signal. </title> <booktitle> In ICASSP, </booktitle> <pages> pages 1291-1294, </pages> <year> 1982. </year>
Reference-contexts: The dfi (s+1)-sized B qmv matrices have a sparse structure determined by the BMM dependencies for state q. With z containing observations only from x's past, these equations alone constitute a generalization of auto-regressive HMMs <ref> [6, 7] </ref> (d = 1, M = 1, V = 1), vector-valued auto-regressive HMMs [4, 9, 8] 6 (d &gt; 1, M = 1, V = 1 ), mixture auto-regressive HMMs [3] (d = 1,M &gt; 1,V = 1), and the usual Gaussian mixture models (d = 1,M &gt; 1,V =
Reference: 7. <author> A.B. Poritz. </author> <title> Hidden markov models: A guided tour. </title> <booktitle> In ICASSP, </booktitle> <pages> pages 7-13, </pages> <year> 1988. </year>
Reference-contexts: The dfi (s+1)-sized B qmv matrices have a sparse structure determined by the BMM dependencies for state q. With z containing observations only from x's past, these equations alone constitute a generalization of auto-regressive HMMs <ref> [6, 7] </ref> (d = 1, M = 1, V = 1), vector-valued auto-regressive HMMs [4, 9, 8] 6 (d &gt; 1, M = 1, V = 1 ), mixture auto-regressive HMMs [3] (d = 1,M &gt; 1,V = 1), and the usual Gaussian mixture models (d = 1,M &gt; 1,V =
Reference: 8. <author> C.J. Wellekens. </author> <title> Explicit time correlation in hidden markov models for speech recognition. </title> <booktitle> In ICASSP, </booktitle> <year> 1987. </year>
Reference-contexts: This approach is distinct from previous work <ref> [3, 8, 4, 9] </ref> in that the dependency structure may be sparse and may change for each value of Q t rather than depending on an additional, fixed, and arbitrarily chosen sets of observations. <p> The dfi (s+1)-sized B qmv matrices have a sparse structure determined by the BMM dependencies for state q. With z containing observations only from x's past, these equations alone constitute a generalization of auto-regressive HMMs [6, 7] (d = 1, M = 1, V = 1), vector-valued auto-regressive HMMs <ref> [4, 9, 8] </ref> 6 (d &gt; 1, M = 1, V = 1 ), mixture auto-regressive HMMs [3] (d = 1,M &gt; 1,V = 1), and the usual Gaussian mixture models (d = 1,M &gt; 1,V = 1,s = 0).
Reference: 9. <author> P.C. Woodland. </author> <title> Hidden markov models using vector linear prediction and discriminative output distributions. </title> <booktitle> In ICASSP, </booktitle> <pages> pages I-509-512, </pages> <year> 1992. </year> <title> 7 Defined as avg k (W ER hmm k W ER bmm k )=W ER hmm k </title>
Reference-contexts: This approach is distinct from previous work <ref> [3, 8, 4, 9] </ref> in that the dependency structure may be sparse and may change for each value of Q t rather than depending on an additional, fixed, and arbitrarily chosen sets of observations. <p> The dfi (s+1)-sized B qmv matrices have a sparse structure determined by the BMM dependencies for state q. With z containing observations only from x's past, these equations alone constitute a generalization of auto-regressive HMMs [6, 7] (d = 1, M = 1, V = 1), vector-valued auto-regressive HMMs <ref> [4, 9, 8] </ref> 6 (d &gt; 1, M = 1, V = 1 ), mixture auto-regressive HMMs [3] (d = 1,M &gt; 1,V = 1), and the usual Gaussian mixture models (d = 1,M &gt; 1,V = 1,s = 0). <p> With V &gt; 1 and M &gt; 1, this model can be considered a mixture of mixtures. An important difference from previous work is that here the dependency structure, as represented by B qmv , is sparse, 6 <ref> [9] </ref> uses discriminative output distributions similar to state-specific LDA and also considers dependencies from future observations. data-derived, and hidden-variable dependent as described in Sec--tion 2. Furthermore, z is allowed to contain observations from x's past, present, and future.
References-found: 9

