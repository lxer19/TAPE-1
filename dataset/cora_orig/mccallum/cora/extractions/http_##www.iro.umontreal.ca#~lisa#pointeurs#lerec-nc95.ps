URL: http://www.iro.umontreal.ca/~lisa/pointeurs/lerec-nc95.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/bib/journals/journals.html
Root-URL: http://www.iro.umontreal.ca
Email: bengioy@iro.umontreal.ca yann@research.att.com  nohl@research.att.com burges@research.att.com  
Title: LeRec: A NN/HMM Hybrid for On-Line Handwriting Recognition  
Author: Yoshua Bengio Yann LeCun Craig Nohl Chris Burges 
Note: To appear in Neural Computation, Volume 7, Number 5, 1995 now,  
Address: Rm 4G332, 101 Crawfords Corner Road Holmdel, NJ 07733  Montreal, C.P. 6128, Succ. Centre-Ville, Montreal, Qc, H3C-3J7, Canada  
Affiliation: AT&T Bell Laboratories  Dept. IRO, Universite de  
Abstract-found: 0
Intro-found: 1
Reference: <author> Bengio, Y., R. De Mori and G. Flammia and R. Kompe. </author> <year> 1992. </year> <title> Global Optimization of a Neural Network-Hidden Markov Model Hybrid. </title> <journal> IEEE Transactions on Neural Networks v.3, nb.2, pp.252-259. </journal>
Reference: <author> Bottou, L. and Gallinari, P. </author> <year> 1991. </year> <title> A Framework for the Cooperation of Learning Algorithms. </title> <editor> In Lippman, M. and Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 781-788, </pages> <address> Denver, </address> <publisher> CO. </publisher>
Reference: <author> Burges, C.J.C., Ben, J.I., LeCun, Y., Denker, J.S., Nohl, C.R. </author> <title> 1993.Off Line Recognition of Handwritten Postal Words Using Neural Networks. </title> <journal> IJPRAI, </journal> <volume> Volume 7, Number 4, </volume> <editor> p. </editor> <booktitle> 45, 1993; also in "Advances in Pattern Recognition Systems Using Neural Network Technologies", Series in Machine Perception and Artificial Intelligence, </booktitle> <volume> Volume 7, </volume> <editor> Edited by I. Guyon and P.S.P Wang, </editor> <publisher> World Scientific, </publisher> <year> 1993. </year> <note> 11 Burges, </note> <author> C., O. Matan, Y. Le Cun, J. Denker, L. Jackel, C. Stenard, C. Nohl and J. Ben. </author> <year> 1992. </year> <title> Shortest Path Segmentation: A Method for Training a Neural Network to Recognize character Strings. </title> <booktitle> Proc. IJCNN'92 (Baltimore), </booktitle> <pages> pp. 165-172, </pages> <month> v.3. </month>
Reference: <author> Drucker, H., Schapire, R., and Simard, R. </author> <year> 1993. </year> <title> Improving performance in neural networks using a boosting algorithm. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 42-49, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Experiments were performed with the network architecture described above. Additional training data was generated by applying local affine transformations to the original pen trajectories, thus improving the robustness of the recognizer (see <ref> (Drucker et al 93) </ref> for a similar algorithm applied to optical character recognition). The second and third set of experiments concerned the recognition of lower case words (writer independent). The tests were performed on a database of 881 words.
Reference: <author> Guyon, I., Albrecht, P., Le Cun, Y., Denker, J. S., and Weissman, H. </author> <title> 1991 design of a neural network character recognizer for a touch terminal. </title> <journal> Pattern Recognition, </journal> <volume> 24(2) </volume> <pages> 105-119. </pages>
Reference-contexts: Typically, trajectories are normalized, and local geometrical or dynamical features are sometimes extracted. The recognition is performed using curve matching (Tappert 90), or other classification techniques such as Time-Delay Neural Networks <ref> (Guyon et al 91) </ref>. While these representations have several advantages, their dependence on stroke ordering and individual writing styles makes them difficult to use in high accuracy, writer independent systems that integrate the segmentation with the recognition.
Reference: <author> Keeler, J. and Rumelhart, D.and Leow, W. </author> <year> 1991. </year> <title> integrated segmentation and recognition of hand-printed numerals. </title> <editor> In Lippman, R. P., Moody, J. M., and Touretzky, D. S., editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pp. 557-563. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Le Cun, Y. </author> <year> 1986. </year> <title> Learning Processes in an Asymmetric Threshold Network. </title> <editor> In Bienen-stock, E., Fogelman-Soulie, F., and Weisbuch, G., editors, </editor> <booktitle> Disordered systems and biological organization, </booktitle> <pages> pp. 233-240, </pages> <address> Les Houches, France. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: MLCNNs are feed-forward neural networks whose architectures are tailored for minimizing the sensitivity to translations, rotations, or distortions of the input image. They are trained to recognize and spot characters with a variation of the Back-Propagation algorithm <ref> (Rumelhart et al 86, Le Cun 86) </ref>. Each unit in an MLCNN is connected only to a local neighborhood in the previous layer. Each unit can be seen as a local feature detector whose function is determined by the learning procedure.
Reference: <author> Le Cun, Y. 1989. Le Cun Y. </author> <year> (1989). </year> <title> Generalization and network design strategies. </title> <type> Technical report CRG-TR-89-4, </type> <institution> Department of Computer Science, University of Toronto. </institution>
Reference-contexts: point-spread function (a Gaussian), the curvature feature at a given pixel depends on the curvature at different points of the trajectory in the vicinity of that pixel. 4 Convolutional Neural Networks Image-like representations such as AMAPs are particularly well suited for use in combination with Multi-Layer Convolutional Neural Networks (MLCNN) <ref> (Le Cun 89, Le Cun et al 90) </ref>. MLCNNs are feed-forward neural networks whose architectures are tailored for minimizing the sensitivity to translations, rotations, or distortions of the input image. <p> The most discriminant criterion is the number of classification errors on the training set but unfortunately it is computationally very difficult to directly optimize such a discrete criterion. During global training, the MMI criterion was optimized using enhanced <ref> (LeCun 89) </ref> stochastic gradient descent with respect to all the parameters in the system, most notably the network weights. Experiments described in the next section have shown important reductions in error rates when training with this word-level criterion instead of just training the network separately for each character.
Reference: <author> Le Cun, Y., Matan, O., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., Jackel, L. D., and Baird, H. S. </author> <year> 1990. </year> <title> Handwritten Zip Code Recognition with Multilayer Networks. </title> <editor> In IAPR, editor, </editor> <booktitle> Proc. of the International Conference on Pattern Recognition, </booktitle> <address> Atlantic City. </address> <publisher> IEEE. </publisher>
Reference: <author> Matan, O., Burges, C. J. C., LeCun, Y., and Denker, J. S. </author> <year> 1992. </year> <title> Multi-Digit Recognition Using a Space Displacement Neural Network. </title> <editor> In Moody, J. M., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pp. 488-495. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Instead of producing a single output vector, such an application of an MLCNN produces a series of output vectors. The outputs detect and recognize characters at different (and overlapping) locations on the input. These multiple-input, multiple-output MLCNNs are called Space Displacement Neural Networks (SDNN) <ref> (Matan et al 92, Keeler et al 91) </ref>. local translations and distortions, with subsampling, shared weights and local receptive fields.
Reference: <author> Nadas, A., Nahamoo, D. and Picheny, M.A. </author> <year> (1988). </year> <title> On a model-robust training method for speech recognition. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> vol. ASSP-36, no. 9., </volume> <pages> pp. 1432-1436. </pages>
Reference-contexts: Within a probabilistic framework, this criterion corresponds to maximizing the mutual information (MMI) between the observations and the correct interpretation <ref> (Nadas et al 88) </ref>.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. In Parallel distributed processing: </title> <journal> Explorations in the microstruc-ture of cognition, </journal> <volume> volume I, </volume> <pages> pp. 318-362. </pages> <publisher> Bradford Books, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: MLCNNs are feed-forward neural networks whose architectures are tailored for minimizing the sensitivity to translations, rotations, or distortions of the input image. They are trained to recognize and spot characters with a variation of the Back-Propagation algorithm <ref> (Rumelhart et al 86, Le Cun 86) </ref>. Each unit in an MLCNN is connected only to a local neighborhood in the previous layer. Each unit can be seen as a local feature detector whose function is determined by the learning procedure.
Reference: <author> Schenkel, M., Guyon, I., Weissman, H., and Nohl, C. </author> <year> 1993. </year> <title> TDNN Solutions for Recognizing On-Line Natural Handwriting. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <address> pp.723-730. </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Tappert, C., Suen, C., and Wakahara, T. </author> <year> 1990. </year> <title> The state of the art in on-line handwriting recognition. </title> <journal> IEEE Trans. PAMI, </journal> <month> 12(8). </month> <title> 12 13 isolated character recognition (uppers, lowers, digits, symbols). The last five bars represent the results obtained by five competing commercial recognizers. The floor (12.9%) represents the best result we could obtain by not counting natural confusions as errors. </title> <type> 14 </type>
Reference-contexts: Convergence of the EM algorithm was typically obtained within 2 to 4 iterations (of maximization of the auxiliary function). 3 AMAP The recognition of handwritten characters from a pen trajectory on a digitizing surface is often done in the time domain <ref> (Tappert 90, Guyon et al 91) </ref>. Typically, trajectories are normalized, and local geometrical or dynamical features are sometimes extracted. The recognition is performed using curve matching (Tappert 90), or other classification techniques such as Time-Delay Neural Networks (Guyon et al 91). <p> Typically, trajectories are normalized, and local geometrical or dynamical features are sometimes extracted. The recognition is performed using curve matching <ref> (Tappert 90) </ref>, or other classification techniques such as Time-Delay Neural Networks (Guyon et al 91). While these representations have several advantages, their dependence on stroke ordering and individual writing styles makes them difficult to use in high accuracy, writer independent systems that integrate the segmentation with the recognition.
References-found: 14

