URL: ftp://ftp.cs.washington.edu/tr/1995/08/UW-CSE-95-08-07.PS.Z
Refering-URL: http://www.cs.washington.edu/homes/pighin/resume.html
Root-URL: 
Title: Implementing Global Memory Management in a Workstation Cluster  
Author: Michael J. Feeley, William E. Morgan, Frederic H. Pighin, Anna R. Karlin, Henry M. Levy Chandramohan A. Thekkath 
Date: December 1995.  
Note: First published in the Proceedings of the 15th ACM Symposium on Operating Systems Principles,  Also available as Technical Report UW-CSE-95-08-07.  
Affiliation: Department of Computer Science and Engineering University of Washington and  DEC Systems Research Center  
Abstract: Advances in network and processor technology have greatly changedthe communicationand computational power of local-area workstation clusters. However, operating systems still treat workstation clusters as a collection of loosely-connected processors, where each workstation acts as an autonomous and independent agent. This operating system structure makes it difficult to exploit the characteristics of current clusters, such as low-latency communication, huge primary memories, and high-speed processors, in order to improve the performance of cluster applications. This paper describes the design and implementation of global memory management in a workstation cluster. Our objective is to use a single, unified, but distributed memory management algorithm at the lowest level of the operating system. By managing memory globally at this level, all system- and higher-level software, including VM, file systems, transaction systems, and user applications, can benefit from available cluster memory. We have implemented our algorithm in the OSF/1 operating system running on an ATM-connected cluster of DEC Alpha workstations. Our measurements show that on a suite of memory-intensive programs, our system improves performance by a factor of 1.5 to 3.5. We also show that our algorithm has a performance advantage over others that have been proposed in the past. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, S. S. Owicki, J. B. Saxe, and C. P. Thacker. </author> <title> High-speed switch scheduling for local-area networks. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 11(4) </volume> <pages> 319-352, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: We have defined a global memory management algorithm and implemented it in the OSF/1 operating system, running on a collection of DEC Alpha workstations connected by a DEC AN2 ATM network <ref> [1] </ref>. By inserting a global memory management algorithm at the lowest OS level, our system integrates, in a natural way, all cluster memory for use by all higher-level functions, including VM paging, mapped files, and file system buffering.
Reference: [2] <author> M. Bern, D. Greene, and A. Raghunathan. </author> <title> Online algorithms for cache sharing. </title> <booktitle> In Proceedings of the 25th ACM Symposium on Theory of Computing, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Section 6 discusses limitations of our algorithm and implementation, and possible solutions to those limitations. Finally, we summarize and conclude in Section 7. 2 Comparison With Previous Work Several previous studies have examined various ways of using remote memory. Strictly theoretical results related to this problem include <ref> [2, 3, 7, 24] </ref>. Leach et al. describe remote paging in the context of the Apollo DOMAIN System [15]. Each machine in the network has a paging server that accepts paging requests from remote nodes.
Reference: [3] <author> D. Black and D. D. Sleator. </author> <title> Competitive algorithms for replication and migration poblems. </title> <type> Technical Report CMU-CS-89-201, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <year> 1989. </year>
Reference-contexts: Section 6 discusses limitations of our algorithm and implementation, and possible solutions to those limitations. Finally, we summarize and conclude in Section 7. 2 Comparison With Previous Work Several previous studies have examined various ways of using remote memory. Strictly theoretical results related to this problem include <ref> [2, 3, 7, 24] </ref>. Leach et al. describe remote paging in the context of the Apollo DOMAIN System [15]. Each machine in the network has a paging server that accepts paging requests from remote nodes.
Reference: [4] <author> M. J. Carey, D. J. Dewitt, and J. F. Naughton. </author> <title> The OO7 benchmark. </title> <booktitle> In Proc. of the ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: By far, the most time is spent in file I/O for compiler and linker access to temporary, source, and object files. OO7 is an object-oriented database benchmark that builds a parts-assembly database in virtual memory and then performs several traversals of this database <ref> [4] </ref>. The benchmark is designed to synthesize the characteristics of MCAD design data and has been used to evaluate the performance of commercial and research object databases. Render is a graphics rendering program that displays a computer-generated scene from a pre-computed 178-Mbyte database [5].
Reference: [5] <author> B. Chamberlain, T. DeRose, D. Salesin, J. Snyder, and D. Lischinski. </author> <title> Fast rendering of complex environments using a spatial hierarchy. </title> <type> Technical Report 95-05-02, </type> <institution> Department of Computer Science and Engineering, University of Wash-ington, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The benchmark is designed to synthesize the characteristics of MCAD design data and has been used to evaluate the performance of commercial and research object databases. Render is a graphics rendering program that displays a computer-generated scene from a pre-computed 178-Mbyte database <ref> [5] </ref>. In our experiment, we measured the elapsed time for a sequence of operations that move the viewpoint progressively closer to the scene without changing the view point angle.
Reference: [6] <author> J. S. Chase, H. M. Levy, M. J. Feeley, and E. D. Lazowska. </author> <title> Sharing and protection in a single-address-space operating system. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 12(4) </volume> <pages> 271-307, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Most fundamental is the work of Li and Hudak, who describe a number of alternative strategies for managing pages in a distributed shared virtual memory system [16]. Similar management issues exist at the software level in single address space systems such as Opal <ref> [6] </ref>, and at the hardware level in NUMA and COMA architectures [9, 21]. Eager et al. [11] describe strategies for choosing target nodes on which to offload tasks in a distributed load sharing environment. 3 Algorithm This section describes the basic algorithm used by GMS.
Reference: [7] <author> M. Chrobak, L. Larmore, N. Reingold, and J. Westbrook. </author> <title> Page migration algorithms using work functions. </title> <type> Technical Report YALE/DCS/RR-910, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1992. </year>
Reference-contexts: Section 6 discusses limitations of our algorithm and implementation, and possible solutions to those limitations. Finally, we summarize and conclude in Section 7. 2 Comparison With Previous Work Several previous studies have examined various ways of using remote memory. Strictly theoretical results related to this problem include <ref> [2, 3, 7, 24] </ref>. Leach et al. describe remote paging in the context of the Apollo DOMAIN System [15]. Each machine in the network has a paging server that accepts paging requests from remote nodes.
Reference: [8] <author> D. Comer and J. Griffioen. </author> <title> A new design for distributed systems: The remote memory model. </title> <booktitle> In Proceedings of the Summer 1990 USENIX Conference, </booktitle> <pages> pages 127-135, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This system allowed local users to statically restrict the amount of physical memory available to the paging server. Comer and Griffioen described a remote memory model in which the cluster contains workstations, disk servers, and remote memory servers <ref> [8] </ref>. The remote memory servers were dedicated machines whose large primary memories could be allocated by workstations with heavy paging activity. No client-to-client resource sharing occurred, except through the servers. Felten and Zahorjan generalized this idea to use memory on idle client machines as paging backing store [12].
Reference: [9] <author> A. L. Cox and R. J. Fowler. </author> <title> The implementation of a coherent memory abstraction on a NUMA multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <month> Decem-ber </month> <year> 1989. </year>
Reference-contexts: Similar management issues exist at the software level in single address space systems such as Opal [6], and at the hardware level in NUMA and COMA architectures <ref> [9, 21] </ref>. Eager et al. [11] describe strategies for choosing target nodes on which to offload tasks in a distributed load sharing environment. 3 Algorithm This section describes the basic algorithm used by GMS. The description is divided into two parts.
Reference: [10] <author> M. D. Dahlin, R. Y. Wang, T. E. Anderson, and D. A. Pat-terson. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In Proceedings of the USENIX Conference on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Franklin et al. evaluate several variants of this algorithm using a synthetic database workload. Dahlin et al. evaluate the use of several algorithms for utilizing remote memory, the best of which is called N-chance forwarding <ref> [10] </ref>. Using N-chance forwarding, when a node is about to replace a page, it checks whether that page is the last copy in the cluster (a singlet); if so, the node forwards that page to a randomly-picked node, otherwise it discards the page. <p> forwarding, the best algorithm defined by Dahlin et al.<ref> [10] </ref>. It is interesting to compare with N-chance, because it differs from our algorithm in significant ways, as described in Section 2. To the best of our knowledge, there is no existing implementation of the N-chance algorithm described by Dahlin et al. [10]. Consequently, we implemented N-chance in OSF/1. We made a few minor modifications to the original algorithm. Our implementation is as follows. Singlet pages are forwarded by nodes with an initial recirculation count of N = 2.
Reference: [11] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> Adaptive load sharing in homogeneous distributed systems. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-12(5), </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: Similar management issues exist at the software level in single address space systems such as Opal [6], and at the hardware level in NUMA and COMA architectures [9, 21]. Eager et al. <ref> [11] </ref> describe strategies for choosing target nodes on which to offload tasks in a distributed load sharing environment. 3 Algorithm This section describes the basic algorithm used by GMS. The description is divided into two parts. First, we present a high-level description of the global replacement algorithm.
Reference: [12] <author> E. W. Felten and J. Zahorjan. </author> <title> Issues in the implementation of a remote memory paging system. </title> <type> Technical Report 91-03-09, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: The remote memory servers were dedicated machines whose large primary memories could be allocated by workstations with heavy paging activity. No client-to-client resource sharing occurred, except through the servers. Felten and Zahorjan generalized this idea to use memory on idle client machines as paging backing store <ref> [12] </ref>. When a machine becomes idle, its kernel activates an otherwise dormant memory server, which registers itself for remote use. Whenever a kernel replaces a VM page, it queries a central registry to locate active memory servers, picking one at random to receive the replacement victim.
Reference: [13] <author> M. J. Frankling, M. J. Carey, and M. Livny. </author> <title> Global memory management in client-server DBMS architectures. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Their goal is to permit small memory-starved portable computers to page to the memories of larger servers nearby; pages could migrate from server to server as the portables migrate. Franklin et al. examine the use of remote memory in a client-server DBMS system <ref> [13] </ref>. Their system assumes a centralized database server that contains the disks for stable store plus a large memory cache. Clients interact with each other via a central server.
Reference: [14] <author> N. P. Kronenberg, H. M. Levy, and W. D. Strecker. VAX-clusters: </author> <title> A closely-coupled distributed system. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 4(2) </volume> <pages> 130-146, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Figure 4 shows three nodes involved in the lookup. When a faulted page is not shared, nodes A and B are identical, thereby decreasing the number of network messages needed. This distribution of the page management information is somewhat similar to the handling of distributed locks in VAXclusters <ref> [14] </ref>. The page-ownership-directory provides an extra level of indirection that enables us to handle the addition or deletion of nodes from the cluster without changing the hash function. <p> It is straightforward to extend our implementation to deal with master node failure through an election process to select a new master, as is done in other systems <ref> [14, 18] </ref>. Node deletions are straightforward as well. The master node checks periodically for the liveness of the other nodes. When it detects a crashed node, it redistributes the page-ownership-directory.
Reference: [15] <author> P. J. Leach, P. H. Levine, B. P. Douros, J. A. Hamilton, D. L. Nelson, and B. L. Stumpf. </author> <title> The architecture of an integrated local network. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 1(5) </volume> <pages> 842-857, </pages> <month> Nov. </month> <year> 1983. </year>
Reference-contexts: Strictly theoretical results related to this problem include [2, 3, 7, 24]. Leach et al. describe remote paging in the context of the Apollo DOMAIN System <ref> [15] </ref>. Each machine in the network has a paging server that accepts paging requests from remote nodes. This system allowed local users to statically restrict the amount of physical memory available to the paging server.
Reference: [16] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: Several other efforts, while not dealing directly with remote paging, relate to our work. Most fundamental is the work of Li and Hudak, who describe a number of alternative strategies for managing pages in a distributed shared virtual memory system <ref> [16] </ref>. Similar management issues exist at the software level in single address space systems such as Opal [6], and at the hardware level in NUMA and COMA architectures [9, 21].
Reference: [17] <author> M. N. Nelson, B. B. Welch, and J. K. Ousterhout. </author> <title> Caching in the Sprite network file system. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: The UBC contains pages from both mapped files and files accessed through normal read/write calls and is dynamically-sized; this is similar in some ways to the Sprite file system <ref> [17] </ref>. At the same level as VM and UBC, we have added the GMS module, which holds global pages housed on the node. Page-replacement decisions are made by the pageout daemon and GMS.
Reference: [18] <author> B. N. Schilit and D. Duchamp. </author> <title> Adaptive remote paging. </title> <type> Technical Report CUCS-004091, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: Felten and Zahorjan used a simple queueing model to predict performance. In a different environment, Schilit and Duchamp have used remote paging to enhance the performance of mobile computers <ref> [18] </ref>. Their goal is to permit small memory-starved portable computers to page to the memories of larger servers nearby; pages could migrate from server to server as the portables migrate. Franklin et al. examine the use of remote memory in a client-server DBMS system [13]. <p> It is straightforward to extend our implementation to deal with master node failure through an election process to select a new master, as is done in other systems <ref> [14, 18] </ref>. Node deletions are straightforward as well. The master node checks periodically for the liveness of the other nodes. When it detects a crashed node, it redistributes the page-ownership-directory.
Reference: [19] <author> M. D. Schroeder, A. D. Birrell, M. Burrows, H. Murray, R. M. Needham, T. L. Rodeheffer, E. H. Satterthwaite, and C. P. Thacker. Autonet: </author> <title> A high-speed, self-configuring local area network using point-to-point links. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 9(8) </volume> <pages> 1318-1335, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Without mutual trust, the solution is to encrypt the data on its way to or from global memory. This could be done most easily at the network hardware level <ref> [19] </ref>. Our current algorithm is essentially a modified global LRU replacement scheme. It is well known that in some cases, such as sequential file access, LRU may not be the best choice [22].
Reference: [20] <author> R. L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <address> One Burlington Woods Drive, Burlington, MA 01803, </address> <year> 1992. </year>
Reference-contexts: In order to collect age information about anonymous and mapped pages, we modified the TLB handler, which is implemented in PALcode <ref> [20] </ref>. Once a minute, we flush the TLB of all entries. Subsequently when the TLB handler performs a virtual-to-physical translation on a TLB miss, it sets a bit for that physical frame.
Reference: [21] <author> P. Stenstrom, T. Joe, and A. Gupta. </author> <title> Comparative performance evaluation of cache-coherent NUMA and COMA architectures. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Similar management issues exist at the software level in single address space systems such as Opal [6], and at the hardware level in NUMA and COMA architectures <ref> [9, 21] </ref>. Eager et al. [11] describe strategies for choosing target nodes on which to offload tasks in a distributed load sharing environment. 3 Algorithm This section describes the basic algorithm used by GMS. The description is divided into two parts.
Reference: [22] <author> M. Stonebraker. </author> <title> Operating system support for database management. </title> <journal> Commun. ACM, </journal> <volume> 24(7) </volume> <pages> 412-418, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: This could be done most easily at the network hardware level [19]. Our current algorithm is essentially a modified global LRU replacement scheme. It is well known that in some cases, such as sequential file access, LRU may not be the best choice <ref> [22] </ref>. The sequential case could be dealt with by limiting its buffer space, as is done currently in the OSF/1 file buffer cache. Other problems could exist as well. The most obvious is that a single badly-behaving program on one node could cause enormous paging activity, effectively flushing global memory.
Reference: [23] <author> C. P. Thacker and M. D. Schroeder. </author> <title> AN2 switch overview. </title> <note> In preparation. </note>
Reference-contexts: In our current implementation, we assume that the network is reliable and we marshal and unmarshal to and from IP datagrams directly. This is justified primarily by the increased reliability of modern local area networks such as AN2 that have flow control to eliminate cell loss due to congestion <ref> [23] </ref>. To date, we have not noticed a dropped packet in any of our experiments.
Reference: [24] <author> J. Westbrook. </author> <title> Randomized algorithms for multiprocessor page migration. </title> <booktitle> In DIMACS Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <volume> volume 7, </volume> <year> 1992. </year> <month> 12 </month>
Reference-contexts: Section 6 discusses limitations of our algorithm and implementation, and possible solutions to those limitations. Finally, we summarize and conclude in Section 7. 2 Comparison With Previous Work Several previous studies have examined various ways of using remote memory. Strictly theoretical results related to this problem include <ref> [2, 3, 7, 24] </ref>. Leach et al. describe remote paging in the context of the Apollo DOMAIN System [15]. Each machine in the network has a paging server that accepts paging requests from remote nodes.
References-found: 24

