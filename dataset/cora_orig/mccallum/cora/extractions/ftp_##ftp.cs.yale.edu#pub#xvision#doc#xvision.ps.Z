URL: ftp://ftp.cs.yale.edu/pub/xvision/doc/xvision.ps.Z
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/AI/VisionRobotics/XVision/
Root-URL: http://www.cs.yale.edu
Email: E-mail: hager@cs.yale.edu, toyama@cs.yale.edu  
Phone: Phone: (203) 432-6432 Fax: (203) 432-0593  
Title: X Vision: A Portable Substrate for Real-Time Vision Applications  
Author: Gregory D. Hager and Kentaro Toyama 
Note: To appear in Computer Vision and Image Understanding  
Address: P.O. Box 208285 New Haven, CT, 06520  
Affiliation: Department of Computer Science Yale University,  
Abstract: In the past several years, the speed of standard processors has reached the point where interesting problems requiring visual tracking can be carried out on standard workstations. However, relatively little attention has been devoted to developing visual tracking technology in its own right. In this article, we describe X Vision, a modular, portable framework for visual tracking. X Vision is designed to be a programming environment for real-time vision which provides high performance on standard workstations outfitted with a simple digitizer. X Vision consists of a small set of image-level tracking primitives, and a framework for combining tracking primitives to form complex tracking systems. Efficiency and robustness are achieved by propagating geometric and temporal constraints to the feature detection level, where image warping and specialized image processing are combined to perform feature detection quickly and robustly. Over the past several years, we have used X Vision to construct several vision-based systems. We present some of these applications as an illustration of how useful, robust tracking systems can be constructed by simple combinations of a few basic primitives combined with the appropriate task-specific constraints. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Allen, A. Timcenko, B. Yoshimi, and P. Michelman. </author> <title> Automated tracking and grasping of a moving object with a robotic hand-eye system. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 9(2) </volume> <pages> 152-165, </pages> <year> 1993. </year>
Reference-contexts: Specific applications include calibration of cameras and robots [9, 28], visual-servoing and hand-eye coordination [10, 18, 25, 27, 56], mobile robot navigation and map-making [45, 55], pursuit of moving objects [10, 26], grasping <ref> [1] </ref>, and telerobotics [23]. Robotic applications most often require the tracking of objects more complex than line segments or point features, and they frequently require the ability to track multiple objects. <p> The implicit assumption is that speed will come, in time, with better technology (perhaps a reasonable assumption, but one which does not help those seeking real-time applications today). Other tracking systems require specialized hardware <ref> [1] </ref>, making it difficult for researchers without such resources to replicate results. Finally, most, if not all, existing tracking methodologies lack modularity and portability, forcing tracking modules to be re-invented for every application.
Reference: [2] <author> A. A. Amini, S. Tehrani, and T. E. Weymouth. </author> <title> Using dynamic programming for minimizing the energy of active contours in the presence of hard constraints, </title> <booktitle> in Proc., 2nd Int. Conf. on Computer Vision, </booktitle> <pages> pp. 95-99, </pages> <year> 1988. </year>
Reference-contexts: Automatic road-following has been accomplished by tracking the edges of the road [34]. Various snake-like trackers are used to track objects in 2D as they move across the camera image <ref> [2, 8, 11, 29, 46, 49, 54] </ref>. Three-dimensional models, while more complex, allow for precise pose estimation [17, 31].
Reference: [3] <author> P. Anandan. </author> <title> A computational framework and an algorithm for the measurement of structure from motion. </title> <journal> Int. J. Computer Vision, </journal> <volume> 2 </volume> <pages> 283-310, </pages> <year> 1989. </year>
Reference-contexts: The low-level features currently available in X Vision include solid or broken contrast edges detected using several variations on standard edge-detection, general grey-scale patterns tracked using SSD (sum-of-squared differences) methods <ref> [3, 47] </ref>, and a variety of color and motion-based primitives used for initial detection of objects and subsequent match disambiguation [51]. The remainder of this section describes how edge-tracking and correlation-based tracking have been incorporated into X Vision.
Reference: [4] <author> R.L. Anderson. </author> <title> Dynamic sensing in a ping-pong playing robot. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 5(6) </volume> <pages> 723-739, </pages> <year> 1989. </year>
Reference-contexts: the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments [12, 17, 30, 31, 43, 45, 49, 57], corners based on line segments [23, 41], small patches of texture [13], and easily detectable highlights <ref> [4, 39] </ref>. Although the basic tracking principles for such simple features have been known for some time, experience has shown that tracking them is most effective when strong geometric, physical, and temporal constraints from the surrounding task can be brought to bear on the tracking problem.
Reference: [5] <author> N. Ayache and O. D. Faugeras. </author> <title> Building, registrating, and fusing noisy visual maps. </title> <journal> Int. J. Robotics Research, </journal> <volume> 7(6) </volume> <pages> 45-65, </pages> <year> 1988. </year>
Reference-contexts: Traditional feature detection methods utilize one or more convolutions, thresholding, and feature aggregation algorithms to detect edge segments. This is followed by a matching phase which utilizes orientation, segment length, and other cues to choose the segment which corresponds to the target <ref> [5] </ref>. Because the orientation and linearity constraints appear late in the detection process, such methods spend a large amount of time performing general purpose edge detection which in 3 turn generates large amounts of data that must then be analyzed in the subsequent match phase.
Reference: [6] <author> M. J. Black and Y. Yacoob. </author> <title> Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motions, </title> <booktitle> in Proc., Int. Conf. on Computer Vision, </booktitle> <address> Cambridge, MA, </address> <pages> pp. 374-381, </pages> <year> 1995. </year>
Reference-contexts: We note that this approach to region tracking is not itself new, however previous implementations were based on computing and integrating interframe motion [48, 40], did not operate in real-time <ref> [6, 7] </ref>, or computed a subset of the affine parameters [37]. Our region tracking uses the initial reference region throughout the image sequence to provide a fixed "setpoint" for the algorithm, and it computes up to full affine image deformations at or near frame rate.
Reference: [7] <author> M.J. Black and A.D. Jepson. Eigentracking: </author> <title> Robust matching and tracking of articulated objects using a view-based representation, </title> <booktitle> in Proc., European Conf. on Computer Vision, </booktitle> <address> Cambridge, UK, </address> <year> 1996. </year>
Reference-contexts: We note that this approach to region tracking is not itself new, however previous implementations were based on computing and integrating interframe motion [48, 40], did not operate in real-time <ref> [6, 7] </ref>, or computed a subset of the affine parameters [37]. Our region tracking uses the initial reference region throughout the image sequence to provide a fixed "setpoint" for the algorithm, and it computes up to full affine image deformations at or near frame rate.
Reference: [8] <author> A. Blake, R. Curwen, and A. Zisserman. </author> <title> Affile-invariant contour tracking with automatic control of spatiotemporal scale, </title> <booktitle> in Proc., Int. Conf. on Computer Vision, </booktitle> <pages> pp. 421-430, </pages> <address> Berlin, Germany, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: A third category of tracking applications are those which track modeled objects. Models may be anything from weak assumptions about the form of the object as it projects to the camera image 1 (e:g: , contour trackers which assume simple, closed contours <ref> [8] </ref>) to full-fledged three-dimensional models with variable parameters (such as a model for an automobile which allows for turning wheels, opening doors, etc.). Automatic road-following has been accomplished by tracking the edges of the road [34]. <p> Automatic road-following has been accomplished by tracking the edges of the road [34]. Various snake-like trackers are used to track objects in 2D as they move across the camera image <ref> [2, 8, 11, 29, 46, 49, 54] </ref>. Three-dimensional models, while more complex, allow for precise pose estimation [17, 31].
Reference: [9] <author> Y. L. Chang and P. Liang. </author> <title> On recursive calibration of cameras for robot hand-eye systems, </title> <booktitle> in Proc., IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> 2, </volume> <pages> pp. 838-843, </pages> <year> 1989. </year>
Reference-contexts: Robotic hand-eye applications also make heavy use of visual tracking. Robots often operate in environments rich with edges, corners, and textures, making feature-based tracking a natural choice for providing visual input. Specific applications include calibration of cameras and robots <ref> [9, 28] </ref>, visual-servoing and hand-eye coordination [10, 18, 25, 27, 56], mobile robot navigation and map-making [45, 55], pursuit of moving objects [10, 26], grasping [1], and telerobotics [23].
Reference: [10] <author> F. Chaumette, P. Rives, and B. Espiau. </author> <title> Positioning of a robot with respect to an object, tracking it, and estimating its velocity by visual servoing, </title> <booktitle> in Proc., IEEE Int. Conf. on Robotics and Automation, </booktitle> <pages> pp. 2248-2253, </pages> <address> Sacramento, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Robotic hand-eye applications also make heavy use of visual tracking. Robots often operate in environments rich with edges, corners, and textures, making feature-based tracking a natural choice for providing visual input. Specific applications include calibration of cameras and robots [9, 28], visual-servoing and hand-eye coordination <ref> [10, 18, 25, 27, 56] </ref>, mobile robot navigation and map-making [45, 55], pursuit of moving objects [10, 26], grasping [1], and telerobotics [23]. Robotic applications most often require the tracking of objects more complex than line segments or point features, and they frequently require the ability to track multiple objects. <p> Specific applications include calibration of cameras and robots [9, 28], visual-servoing and hand-eye coordination [10, 18, 25, 27, 56], mobile robot navigation and map-making [45, 55], pursuit of moving objects <ref> [10, 26] </ref>, grasping [1], and telerobotics [23]. Robotic applications most often require the tracking of objects more complex than line segments or point features, and they frequently require the ability to track multiple objects.
Reference: [11] <author> L.D. Cohen. </author> <title> On active contour models and balloons. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 53(2) </volume> <pages> 211-218, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Automatic road-following has been accomplished by tracking the edges of the road [34]. Various snake-like trackers are used to track objects in 2D as they move across the camera image <ref> [2, 8, 11, 29, 46, 49, 54] </ref>. Three-dimensional models, while more complex, allow for precise pose estimation [17, 31].
Reference: [12] <author> J. L. Crowley, P. Stelmaszyk, T. Skordas, and P. Puget. </author> <title> Measurement and integration of 3-D structures by tracking edge lines. </title> <journal> Int. J. Computer Vision, </journal> <volume> 8(1) </volume> <pages> 29-52, </pages> <year> 1992. </year>
Reference-contexts: One of the most common applications is determining structure from motion. Structure from motion algorithms attempt to recover the three-dimensional structure of objects by observing their movement in multiple camera frames. Most often, this research involves observation of line segments <ref> [12, 30, 43, 45, 57] </ref>, point features [41, 44], or both [14, 42], as they move in the image. <p> While the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments <ref> [12, 17, 30, 31, 43, 45, 49, 57] </ref>, corners based on line segments [23, 41], small patches of texture [13], and easily detectable highlights [4, 39].
Reference: [13] <author> M. W. Eklund, G. Ravichandran, M. M. Trivedi, and S. B. Marapane. </author> <title> Adaptive visual tracking algorithm and real-time implemenation, </title> <booktitle> in Proc., IEEE Int. Conf. on Robotics and Automation, </booktitle> <pages> pp. 2657-2662, </pages> <address> Nagoya, Japan, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: While the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments [12, 17, 30, 31, 43, 45, 49, 57], corners based on line segments [23, 41], small patches of texture <ref> [13] </ref>, and easily detectable highlights [4, 39].
Reference: [14] <author> O. D. Faugeras, F. Lustaman, and G. Toscani. </author> <title> Motion and structure from point and line matches, </title> <booktitle> in Proc., Int. Conf. on Computer Vision, </booktitle> <pages> pp. 25-33, </pages> <month> June </month> <year> 1987. </year> <month> 23 </month>
Reference-contexts: Structure from motion algorithms attempt to recover the three-dimensional structure of objects by observing their movement in multiple camera frames. Most often, this research involves observation of line segments [12, 30, 43, 45, 57], point features [41, 44], or both <ref> [14, 42] </ref>, as they move in the image. As with stereo vision research, a basic necessity for recovering structure accurately is a solution to the correspondence problem: three-dimensional structure cannot be accurately determined without knowing which image features correspond to the same physical point in successive image frames.
Reference: [15] <author> J.D. Foley, A. van Dam, S.K. Feiner, and J.F. Hughes. </author> <title> Computer Graphics. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: So, for example, a polygon may be decomposed into its polyhedral faces which are further decomposed into constituent lines. Given an object-viewer relationship, these lines are projected into the screen coordinate system and displayed. A good graphics system makes defining these types of geometric relationships simple and intuitive <ref> [15] </ref>. X Vision provides this functionality and its converse. In addition to stating how a complex object in a particular pose or configuration is decomposed into a list of primitive features, X Vision describes how the pose or attitude is computed given the locations of those primitives. <p> First, a rotated rectangular area is acquired using an algorithm closely related to Bresenham algorithms for fast line rendering <ref> [15] </ref>. The resulting buffer can be subsequently scaled and sheared using an optimized bilinear interpolation algorithm. The former is relatively inexpensive, requiring about 2 additions per pixel to implement. The latter is more expensive, requiring 3 multiplies and 6 additions per pixel in our implementation. <p> This effectively approximates rotation by image shear, a well-known technique in graphics <ref> [15] </ref>. Quadratic interpolation of the maximum of the three curves is used to estimate the orientation of the underlying edge. In the ideal case, if the convolution template is symmetric and the response function after superposition is unimodal, the horizontal displacement of the edge should agree between all three filters.
Reference: [16] <author> W. T. Freeman and E. H. Adelson. </author> <title> Steerable filters for early vision, image analysis, and wavelet decomposition, </title> <booktitle> in Proc., Int. Conf. on Computer Vision, </booktitle> <address> Osaka, Japan, </address> <year> 1990. </year>
Reference-contexts: We note that this is significantly cheaper than using, for example, steerable filters for this purpose <ref> [16] </ref>. The detection scheme described above requires orientation information to function correctly. If this information cannot be supplied from "higher-level" geometric constraints, it is estimated as follows (refer to Figure 3). As the orientation of the acquisition window rotates relative to the edge, the response of the filter drops sharply.
Reference: [17] <author> D. B. Gennery. </author> <title> Visual tracking of known three-dimensional objects. </title> <journal> Int. J. Computer Vision, </journal> <volume> 7(3) </volume> <pages> 243-270, </pages> <year> 1992. </year>
Reference-contexts: Automatic road-following has been accomplished by tracking the edges of the road [34]. Various snake-like trackers are used to track objects in 2D as they move across the camera image [2, 8, 11, 29, 46, 49, 54]. Three-dimensional models, while more complex, allow for precise pose estimation <ref> [17, 31] </ref>. The key problem in model-based tracking is to integrate simple features into a consistent whole, both to predict the configuration of features in the future and to evaluate the accuracy of any single feature. <p> While the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments <ref> [12, 17, 30, 31, 43, 45, 49, 57] </ref>, corners based on line segments [23, 41], small patches of texture [13], and easily detectable highlights [4, 39].
Reference: [18] <author> G. D. Hager. </author> <title> Calibration-free visual control using projective invariance, </title> <booktitle> in Int. Conf. on Computer Vision, </booktitle> <address> Cambridge, MA, </address> <pages> pp. 1009-1015, </pages> <year> 1995. </year>
Reference-contexts: Robotic hand-eye applications also make heavy use of visual tracking. Robots often operate in environments rich with edges, corners, and textures, making feature-based tracking a natural choice for providing visual input. Specific applications include calibration of cameras and robots [9, 28], visual-servoing and hand-eye coordination <ref> [10, 18, 25, 27, 56] </ref>, mobile robot navigation and map-making [45, 55], pursuit of moving objects [10, 26], grasping [1], and telerobotics [23]. Robotic applications most often require the tracking of objects more complex than line segments or point features, and they frequently require the ability to track multiple objects.
Reference: [19] <author> G. D. Hager. </author> <title> Real-time feature tracking and projective invariance as a basis for hand-eye coordination, </title> <booktitle> in Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 533-539. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: An instance of the intersection-based point feature can be instantiated either from edges detected in images or line features that are themselves computed from point features. 3 Applications We have used X Vision for several purposes including hand-eye coordination <ref> [19, 20, 22] </ref>, a pose-based object tracking system [32], a robust face-tracking system [51], a gesture-based drawing program, a six degree-of-freedom mouse [52], and a variety of small video games.
Reference: [20] <author> G. D. Hager. </author> <title> A modular system for robust hand-eye coordination. </title> <institution> DCS RR-1074, Yale University, </institution> <address> New Haven, CT, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: An instance of the intersection-based point feature can be instantiated either from edges detected in images or line features that are themselves computed from point features. 3 Applications We have used X Vision for several purposes including hand-eye coordination <ref> [19, 20, 22] </ref>, a pose-based object tracking system [32], a robust face-tracking system [51], a gesture-based drawing program, a six degree-of-freedom mouse [52], and a variety of small video games. <p> This system 16 17 defined in the plane (right). requires less than 10 milliseconds per iteration without graphics. 3.2 An Embedded Application As an illustration of the use of X Vision embedded within a larger system, we describe some results of using X Vision within a hand-eye coordination system <ref> [20, 22, 53] </ref>. The system relies on image-level feedback from two cameras to control the relative pose between an object held in a robot end-effector and a static object in the environment. <p> It can be shown that the accuracy of primitive skills depends only on the accuracy of feature location in the image <ref> [20] </ref>. Hence, the physical accuracy of hand-eye experiments can be used to directly determine the accuracy of our feature localization algorithms. We have performed several hundred point-to-point positioning experiments with a camera baseline of approximately 30cm at distances of 80 to 100cm. Accuracy is typically within a millimeter of position.
Reference: [21] <author> G. D. Hager and P.N. Belhumeur. </author> <title> Real-time tracking of image regions with changes in geometry and illumination, </title> <booktitle> in Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 403-410. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: We note that with these modifications, solving (4) for rigid motions (translation and rotation) is equivalent to maximizing normalized correlation [24]. Extensions to the SSD-based region tracking paradigm for more complex lighting models can be found in <ref> [21] </ref>. Another problem is that the image gradients are only locally valid. In order to guarantee tracking of motions larger than a fraction of a pixel, these calculations must be carried out at varying levels of resolution. <p> Another related problem is that of partial and full occlusion, and 19 target reacquisition. In recent work, we have begun to developed methods more robust to occlusion and distraction <ref> [21, 49] </ref>, and techniques for automatic target localization and initialization [51]. X Vision is on ongoing software development project, versions of which are freely available. Information on the current version is can be found at http://www.cs.yale.edu/HTML/YALE/CS/AI/VisionRobotics/YaleAI.html .
Reference: [22] <author> G. D. Hager, W-C. Chang, and A. S. Morse. </author> <title> Robot hand-eye coordination based on stereo vision. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 15(1) </volume> <pages> 30-39, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: An instance of the intersection-based point feature can be instantiated either from edges detected in images or line features that are themselves computed from point features. 3 Applications We have used X Vision for several purposes including hand-eye coordination <ref> [19, 20, 22] </ref>, a pose-based object tracking system [32], a robust face-tracking system [51], a gesture-based drawing program, a six degree-of-freedom mouse [52], and a variety of small video games. <p> This system 16 17 defined in the plane (right). requires less than 10 milliseconds per iteration without graphics. 3.2 An Embedded Application As an illustration of the use of X Vision embedded within a larger system, we describe some results of using X Vision within a hand-eye coordination system <ref> [20, 22, 53] </ref>. The system relies on image-level feedback from two cameras to control the relative pose between an object held in a robot end-effector and a static object in the environment.
Reference: [23] <author> G. D. Hager, G. Grunwald, and K. Toyama. </author> <title> Feature-based visual servoing and its application to telerobotics, in Intelligent Robots and Systems (V. Graefe, </title> <editor> Ed.), </editor> <publisher> Elsevier, </publisher> <year> 1995. </year>
Reference-contexts: Specific applications include calibration of cameras and robots [9, 28], visual-servoing and hand-eye coordination [10, 18, 25, 27, 56], mobile robot navigation and map-making [45, 55], pursuit of moving objects [10, 26], grasping [1], and telerobotics <ref> [23] </ref>. Robotic applications most often require the tracking of objects more complex than line segments or point features, and they frequently require the ability to track multiple objects. <p> While the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments [12, 17, 30, 31, 43, 45, 49, 57], corners based on line segments <ref> [23, 41] </ref>, small patches of texture [13], and easily detectable highlights [4, 39].
Reference: [24] <author> R. M. Haralick and L. G. Shapiro. </author> <title> Computer and Robot Vision: Volume II. </title> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: The solution is to normalize images to have zero first moment and unit second moment. We note that with these modifications, solving (4) for rigid motions (translation and rotation) is equivalent to maximizing normalized correlation <ref> [24] </ref>. Extensions to the SSD-based region tracking paradigm for more complex lighting models can be found in [21]. Another problem is that the image gradients are only locally valid.
Reference: [25] <author> T. Heikkila, Matsushita, and Sato. </author> <title> Planning of visual feedback with robot-sensor co-operation, </title> <booktitle> in Proc., IEEE Int. Workshop on Intelligent Robots and Systems, </booktitle> <address> Tokyo, </address> <year> 1988. </year>
Reference-contexts: Robotic hand-eye applications also make heavy use of visual tracking. Robots often operate in environments rich with edges, corners, and textures, making feature-based tracking a natural choice for providing visual input. Specific applications include calibration of cameras and robots [9, 28], visual-servoing and hand-eye coordination <ref> [10, 18, 25, 27, 56] </ref>, mobile robot navigation and map-making [45, 55], pursuit of moving objects [10, 26], grasping [1], and telerobotics [23]. Robotic applications most often require the tracking of objects more complex than line segments or point features, and they frequently require the ability to track multiple objects.
Reference: [26] <author> E. Huber and D. Kortenkamp. </author> <title> Using stereo vision to pursue moving agents with a mobile robot, </title> <booktitle> in Proc., IEEE Int. Conf. on Robotics and Automation, </booktitle> <pages> pp. 2340-2346, </pages> <address> Nagoya, Japan, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Specific applications include calibration of cameras and robots [9, 28], visual-servoing and hand-eye coordination [10, 18, 25, 27, 56], mobile robot navigation and map-making [45, 55], pursuit of moving objects <ref> [10, 26] </ref>, grasping [1], and telerobotics [23]. Robotic applications most often require the tracking of objects more complex than line segments or point features, and they frequently require the ability to track multiple objects.
Reference: [27] <author> I. Inoue. </author> <title> Hand eye coordination in rope handling, </title> <booktitle> in 1st Int. Symp. on Robotics Research, </booktitle> <address> Bretten Woods, USA, </address> <year> 1983. </year>
Reference-contexts: Robotic hand-eye applications also make heavy use of visual tracking. Robots often operate in environments rich with edges, corners, and textures, making feature-based tracking a natural choice for providing visual input. Specific applications include calibration of cameras and robots [9, 28], visual-servoing and hand-eye coordination <ref> [10, 18, 25, 27, 56] </ref>, mobile robot navigation and map-making [45, 55], pursuit of moving objects [10, 26], grasping [1], and telerobotics [23]. Robotic applications most often require the tracking of objects more complex than line segments or point features, and they frequently require the ability to track multiple objects.
Reference: [28] <author> A. Izaguirre, P. Pu, and J. Summers. </author> <title> A new development in camera calibration: Calibrating a pair of mobile cameras. </title> <journal> Int. J. Robot. Res., </journal> <volume> 6(3) </volume> <pages> 104-116, </pages> <year> 1987. </year>
Reference-contexts: Robotic hand-eye applications also make heavy use of visual tracking. Robots often operate in environments rich with edges, corners, and textures, making feature-based tracking a natural choice for providing visual input. Specific applications include calibration of cameras and robots <ref> [9, 28] </ref>, visual-servoing and hand-eye coordination [10, 18, 25, 27, 56], mobile robot navigation and map-making [45, 55], pursuit of moving objects [10, 26], grasping [1], and telerobotics [23].
Reference: [29] <author> H. Kass, A. Witkin, and D. Terzopoulos. Snakes: </author> <title> Active contour models. </title> <journal> Int. J. Computer Vision, </journal> <volume> 1 </volume> <pages> 321-331, </pages> <year> 1987. </year>
Reference-contexts: Automatic road-following has been accomplished by tracking the edges of the road [34]. Various snake-like trackers are used to track objects in 2D as they move across the camera image <ref> [2, 8, 11, 29, 46, 49, 54] </ref>. Three-dimensional models, while more complex, allow for precise pose estimation [17, 31]. <p> The edge trackers then do not need to compute orientation from image information. The net effect of these two changes is to create a highly constrained snake-like contour tracker <ref> [29] </ref>. that there is little or no loss of precision in determining the location of the corners with reasonable sampling rates. At the same time, we see a 10% to 20% speedup by not computing line orientations at the image level and a nearly linear speedup with image subsampling level.
Reference: [30] <author> Y. Liu and T. S. Huang. </author> <title> A linear algorithm for motion estimation using straight line correspondences, </title> <booktitle> in Int. Conf. on Pattern Recognition, </booktitle> <pages> pp. 213-219, </pages> <year> 1988. </year>
Reference-contexts: One of the most common applications is determining structure from motion. Structure from motion algorithms attempt to recover the three-dimensional structure of objects by observing their movement in multiple camera frames. Most often, this research involves observation of line segments <ref> [12, 30, 43, 45, 57] </ref>, point features [41, 44], or both [14, 42], as they move in the image. <p> While the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments <ref> [12, 17, 30, 31, 43, 45, 49, 57] </ref>, corners based on line segments [23, 41], small patches of texture [13], and easily detectable highlights [4, 39].
Reference: [31] <author> D. G. Lowe. </author> <title> Robust model-based motion tracking through the integration of search and estimation. </title> <journal> Int. J. Computer Vision, </journal> <volume> 8(2) </volume> <pages> 113-122, </pages> <year> 1992. </year>
Reference-contexts: Automatic road-following has been accomplished by tracking the edges of the road [34]. Various snake-like trackers are used to track objects in 2D as they move across the camera image [2, 8, 11, 29, 46, 49, 54]. Three-dimensional models, while more complex, allow for precise pose estimation <ref> [17, 31] </ref>. The key problem in model-based tracking is to integrate simple features into a consistent whole, both to predict the configuration of features in the future and to evaluate the accuracy of any single feature. <p> While the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments <ref> [12, 17, 30, 31, 43, 45, 49, 57] </ref>, corners based on line segments [23, 41], small patches of texture [13], and easily detectable highlights [4, 39].
Reference: [32] <author> C.-P. Lu. </author> <title> Online Pose Estimation and Model Matching. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1995. </year>
Reference-contexts: Image-level feature detection is then performed, and finally information is propagated upward by computing (21). Composite features that have been implemented within this scheme range from simple edge intersections as described above, to snake-like contour tracking [49], to three-dimensional model-based tracking using pose estimation <ref> [32] </ref>, as well as a variety of more specialized object trackers, some of which are described in Section 3. 2.3 Feature Typing In order to make feature composition simpler and more generic, we have included polymorphic type support in the tracking system. Each feature, basic or composite, carries a type. <p> An instance of the intersection-based point feature can be instantiated either from edges detected in images or line features that are themselves computed from point features. 3 Applications We have used X Vision for several purposes including hand-eye coordination [19, 20, 22], a pose-based object tracking system <ref> [32] </ref>, a robust face-tracking system [51], a gesture-based drawing program, a six degree-of-freedom mouse [52], and a variety of small video games. <p> While a generic model-based tracker for three-dimensional objects for this system can be constructed <ref> [32] </ref>, X Vision makes it possible to gain additional speed and robustness by customizing the tracking loop using object-specific geometric information. On example of this customization process is the development of a tracker for rectangular floppy disks that we use as test objects in our hand-eye experiments (described below).
Reference: [33] <author> B. D. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <journal> pp. </journal> <pages> 674-679, </pages> <year> 1981. </year> <month> 24 </month>
Reference-contexts: Once this transformation is performed, computing the remaining geometric differences between reference and the prospective regions is posed as a sum-of-squared differences (least-squares) optimization problem similar to that of stereo matching <ref> [33] </ref>. We note that this approach to region tracking is not itself new, however previous implementations were based on computing and integrating interframe motion [48, 40], did not operate in real-time [6, 7], or computed a subset of the affine parameters [37].
Reference: [34] <author> A. D. Morgan, E. L. Dagless, D. J. Milford, and B. T. Thomas. </author> <title> Road edge tracking for robot road following: a real-time implementation. </title> <journal> Image and Vision Computing, </journal> <volume> 8(3) </volume> <pages> 233-240, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Automatic road-following has been accomplished by tracking the edges of the road <ref> [34] </ref>. Various snake-like trackers are used to track objects in 2D as they move across the camera image [2, 8, 11, 29, 46, 49, 54]. Three-dimensional models, while more complex, allow for precise pose estimation [17, 31].
Reference: [35] <author> J. Mundy and A. Zisserman. </author> <title> Geometric Invariance in Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1992. </year>
Reference-contexts: For example, positioning or orienting the robot in a plane is practically useful for systems which use a table or other level surface as a work-space. In order to relate task coordinates to image coordinates, a planar invariant employing cross-ratios can be used <ref> [35] </ref>. This construction is implemented as a composite feature composed of several corner trackers. However, it is typed as a point feature, so it can be coupled directly with a point positioning skill in order to perform planar positioning as shown in Figure 11 (right).
Reference: [36] <author> H. K. Nishihara. Teleos AVP, </author> <title> in CVPR Demo Program, </title> <address> San Francisco, </address> <year> 1996. </year>
Reference-contexts: Full-frame algorithms such as optical flow calculation or region segmentation tend to lead to data intensive, processing which is performed o*ine or which is accelerated using specialized hardware (for a notable exception, see <ref> [36] </ref>). On the other hand, feature-based algorithms usually concentrate on spatially localized areas of the image. Since image processing is local, high data bandwidth between the host and the digitizer is not needed.
Reference: [37] <author> N. Papanikolopoulos, P. Khosla, and T. Kanade. </author> <title> Visual tracking of a moving target by a camera mounted on a robot: A combination of control and vision, </title> <booktitle> in Proc., IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> 9(1), </volume> <year> 1993. </year>
Reference-contexts: We note that this approach to region tracking is not itself new, however previous implementations were based on computing and integrating interframe motion [48, 40], did not operate in real-time [6, 7], or computed a subset of the affine parameters <ref> [37] </ref>. Our region tracking uses the initial reference region throughout the image sequence to provide a fixed "setpoint" for the algorithm, and it computes up to full affine image deformations at or near frame rate.
Reference: [38] <author> D. Reynard, A. Wildenberg, A. Blake, and J. Marchant. </author> <title> Learning dynamics of complex motions from image sequences. </title> <journal> pp. </journal> <volume> I:357-368, </volume> <year> 1996. </year>
Reference-contexts: Even though the edge detector described above is quite selective, as the edge segment moves through clutter, we can expect multiple local maxima to appear in the convolution output. This is a well-known and unavoidable problem for which many solutions have been proposed <ref> [38] </ref>. By default, X Vision declares a match if and only if a unique local maximum exists within an interval about the response value stored in the state.
Reference: [39] <author> A. A. Rizzi and D. E. Koditschek. </author> <title> An active visual estimator for dexterous manipulation. </title> <booktitle> Paper presented at the 1994 Workshop on Visual Servoing, </booktitle> <year> 1994. </year>
Reference-contexts: the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments [12, 17, 30, 31, 43, 45, 49, 57], corners based on line segments [23, 41], small patches of texture [13], and easily detectable highlights <ref> [4, 39] </ref>. Although the basic tracking principles for such simple features have been known for some time, experience has shown that tracking them is most effective when strong geometric, physical, and temporal constraints from the surrounding task can be brought to bear on the tracking problem.
Reference: [40] <author> J. Shi and C. Tomasi. </author> <title> Good features to track, </title> <booktitle> in Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 593-600. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: This has led to a proliferation of tracking techniques which, although effective for particular experiments, are not practical solutions in general. Many tracking systems, for example, are only applied to pre-stored video sequences and do not operate in real time <ref> [40] </ref>. The implicit assumption is that speed will come, in time, with better technology (perhaps a reasonable assumption, but one which does not help those seeking real-time applications today). Other tracking systems require specialized hardware [1], making it difficult for researchers without such resources to replicate results. <p> We note that this approach to region tracking is not itself new, however previous implementations were based on computing and integrating interframe motion <ref> [48, 40] </ref>, did not operate in real-time [6, 7], or computed a subset of the affine parameters [37]. <p> x2W In this form, the problem can be solved by joint optimization over all six unknowns in A 0 and d 0 : However, one difficulty with computing affine structure lies in the fact that many target regions do not have enough texture to fully determine all six geometric parameters <ref> [40] </ref>. Consider, for example, a window placed on a right-angle corner. A pure translation of the corner can be accounted for as translation, scaling or a linear combination of both.
Reference: [41] <author> D. Sinclair, A. Blake, S. Smith, and C. Rothwell. </author> <title> Planar region detection and motion recovery. </title> <journal> Image and Vision Computing, </journal> <volume> 11(4) </volume> <pages> 229-234, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: One of the most common applications is determining structure from motion. Structure from motion algorithms attempt to recover the three-dimensional structure of objects by observing their movement in multiple camera frames. Most often, this research involves observation of line segments [12, 30, 43, 45, 57], point features <ref> [41, 44] </ref>, or both [14, 42], as they move in the image. <p> While the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments [12, 17, 30, 31, 43, 45, 49, 57], corners based on line segments <ref> [23, 41] </ref>, small patches of texture [13], and easily detectable highlights [4, 39].
Reference: [42] <author> M. E. Spetsakis. </author> <title> A linear algorithm for point and line-based structure from motion. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 56(2) </volume> <pages> 230-241, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Structure from motion algorithms attempt to recover the three-dimensional structure of objects by observing their movement in multiple camera frames. Most often, this research involves observation of line segments [12, 30, 43, 45, 57], point features [41, 44], or both <ref> [14, 42] </ref>, as they move in the image. As with stereo vision research, a basic necessity for recovering structure accurately is a solution to the correspondence problem: three-dimensional structure cannot be accurately determined without knowing which image features correspond to the same physical point in successive image frames.
Reference: [43] <author> M. E. Spetsakis and J. Aloimonos. </author> <title> Structure from motion using line correspondences. </title> <journal> Int. J. Computer Vision, </journal> <volume> 4 </volume> <pages> 171-183, </pages> <year> 1990. </year>
Reference-contexts: One of the most common applications is determining structure from motion. Structure from motion algorithms attempt to recover the three-dimensional structure of objects by observing their movement in multiple camera frames. Most often, this research involves observation of line segments <ref> [12, 30, 43, 45, 57] </ref>, point features [41, 44], or both [14, 42], as they move in the image. <p> While the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments <ref> [12, 17, 30, 31, 43, 45, 49, 57] </ref>, corners based on line segments [23, 41], small patches of texture [13], and easily detectable highlights [4, 39].
Reference: [44] <author> T. N. Tan, K. D. Baker, and G. D. Sullivan. </author> <title> 3D structure and motion estimation from 2D image sequences. </title> <journal> Image and Vision Computing, </journal> <volume> 11(4) </volume> <pages> 203-210, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: One of the most common applications is determining structure from motion. Structure from motion algorithms attempt to recover the three-dimensional structure of objects by observing their movement in multiple camera frames. Most often, this research involves observation of line segments [12, 30, 43, 45, 57], point features <ref> [41, 44] </ref>, or both [14, 42], as they move in the image.
Reference: [45] <author> Camillo J. Taylor and D. J. Kriegman. </author> <title> Structure and motion from line segments in multiple images, </title> <booktitle> in Proc., IEEE Int. Conf. on Robotics and Automation, </booktitle> <pages> pp. 1615-1620, </pages> <year> 1992. </year>
Reference-contexts: One of the most common applications is determining structure from motion. Structure from motion algorithms attempt to recover the three-dimensional structure of objects by observing their movement in multiple camera frames. Most often, this research involves observation of line segments <ref> [12, 30, 43, 45, 57] </ref>, point features [41, 44], or both [14, 42], as they move in the image. <p> Robots often operate in environments rich with edges, corners, and textures, making feature-based tracking a natural choice for providing visual input. Specific applications include calibration of cameras and robots [9, 28], visual-servoing and hand-eye coordination [10, 18, 25, 27, 56], mobile robot navigation and map-making <ref> [45, 55] </ref>, pursuit of moving objects [10, 26], grasping [1], and telerobotics [23]. Robotic applications most often require the tracking of objects more complex than line segments or point features, and they frequently require the ability to track multiple objects. <p> While the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments <ref> [12, 17, 30, 31, 43, 45, 49, 57] </ref>, corners based on line segments [23, 41], small patches of texture [13], and easily detectable highlights [4, 39].
Reference: [46] <author> D. Terzopoulos and R. Szeliski. </author> <title> Tracking with Kalman snakes, in Active Vision (A. </title> <editor> Blake and A. Yuille, Ed.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Automatic road-following has been accomplished by tracking the edges of the road [34]. Various snake-like trackers are used to track objects in 2D as they move across the camera image <ref> [2, 8, 11, 29, 46, 49, 54] </ref>. Three-dimensional models, while more complex, allow for precise pose estimation [17, 31].
Reference: [47] <author> C. Tomasi and T. Kanade. </author> <title> Shape and motion from image streams: a factorization method, full report on the orthographic case. </title> <address> CMU-CS 92-104, CMU, </address> <year> 1992. </year>
Reference-contexts: The low-level features currently available in X Vision include solid or broken contrast edges detected using several variations on standard edge-detection, general grey-scale patterns tracked using SSD (sum-of-squared differences) methods <ref> [3, 47] </ref>, and a variety of color and motion-based primitives used for initial detection of objects and subsequent match disambiguation [51]. The remainder of this section describes how edge-tracking and correlation-based tracking have been incorporated into X Vision.
Reference: [48] <author> C. Tomasi and T. Kanade. </author> <title> Shape and motion without depth, </title> <booktitle> in Proc., Image Understanding Workshop, </booktitle> <pages> pp. 258-271, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: We note that this approach to region tracking is not itself new, however previous implementations were based on computing and integrating interframe motion <ref> [48, 40] </ref>, did not operate in real-time [6, 7], or computed a subset of the affine parameters [37].
Reference: [49] <author> K. Toyama and G. D. Hager. </author> <title> Keeping your eye on the ball: Tracking occluding contours of unfamiliar objects without distraction, </title> <booktitle> in Proc., Int. Conf. on Intel. Robotics and Sys., </booktitle> <pages> pp. 354-359, </pages> <address> Pittsburgh, PA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Automatic road-following has been accomplished by tracking the edges of the road [34]. Various snake-like trackers are used to track objects in 2D as they move across the camera image <ref> [2, 8, 11, 29, 46, 49, 54] </ref>. Three-dimensional models, while more complex, allow for precise pose estimation [17, 31]. <p> While the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments <ref> [12, 17, 30, 31, 43, 45, 49, 57] </ref>, corners based on line segments [23, 41], small patches of texture [13], and easily detectable highlights [4, 39]. <p> Other possibilities include matching on the brightness of the "foreground" object or matching based on nearness to an expected location passed from a higher-level object. Experimental results on line tracking using various match functions can be found in <ref> [49] </ref>. <p> Image-level feature detection is then performed, and finally information is propagated upward by computing (21). Composite features that have been implemented within this scheme range from simple edge intersections as described above, to snake-like contour tracking <ref> [49] </ref>, to three-dimensional model-based tracking using pose estimation [32], as well as a variety of more specialized object trackers, some of which are described in Section 3. 2.3 Feature Typing In order to make feature composition simpler and more generic, we have included polymorphic type support in the tracking system. <p> Another related problem is that of partial and full occlusion, and 19 target reacquisition. In recent work, we have begun to developed methods more robust to occlusion and distraction <ref> [21, 49] </ref>, and techniques for automatic target localization and initialization [51]. X Vision is on ongoing software development project, versions of which are freely available. Information on the current version is can be found at http://www.cs.yale.edu/HTML/YALE/CS/AI/VisionRobotics/YaleAI.html .
Reference: [50] <author> K. Toyama and G. D. Hager. </author> <title> Tracker fusion for robustness in visual feature tracking, </title> <booktitle> in SPIE Int. Sym. Intel. Sys. and Adv. Manufacturing, 2589, </booktitle> <address> Philadelphia, PA, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: the shearing in the top and bottom figures have been exaggerated for illustrative purposes.) 7 Line Length Sampling Length, Width Full 1/2 1/4 20, 20 0.39 0.29 0.20 60, 20 1.13 0.59 0.35 40, 30 0.93 0.55 0.35 20, 40 0.65 0.43 0.32 60, 40 2.09 0.97 0.57 as possible <ref> [50] </ref>. Long segments are less likely to become completely occluded, and changes in the background tend to affect a smaller proportion of the segment with a commensurately lower impact on the filter response.
Reference: [51] <author> K. Toyama and G. D. Hager. </author> <title> Incremental focus of attention for robust visual tracking, </title> <booktitle> in Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 189-195. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: The low-level features currently available in X Vision include solid or broken contrast edges detected using several variations on standard edge-detection, general grey-scale patterns tracked using SSD (sum-of-squared differences) methods [3, 47], and a variety of color and motion-based primitives used for initial detection of objects and subsequent match disambiguation <ref> [51] </ref>. The remainder of this section describes how edge-tracking and correlation-based tracking have been incorporated into X Vision. In the sequel, all reported timing figures were taken on an SGI Indy workstation equipped with a 175Mhz R4400 SC processor and an SGI VINO digitizing system. <p> of the intersection-based point feature can be instantiated either from edges detected in images or line features that are themselves computed from point features. 3 Applications We have used X Vision for several purposes including hand-eye coordination [19, 20, 22], a pose-based object tracking system [32], a robust face-tracking system <ref> [51] </ref>, a gesture-based drawing program, a six degree-of-freedom mouse [52], and a variety of small video games. <p> Another related problem is that of partial and full occlusion, and 19 target reacquisition. In recent work, we have begun to developed methods more robust to occlusion and distraction [21, 49], and techniques for automatic target localization and initialization <ref> [51] </ref>. X Vision is on ongoing software development project, versions of which are freely available. Information on the current version is can be found at http://www.cs.yale.edu/HTML/YALE/CS/AI/VisionRobotics/YaleAI.html .
Reference: [52] <author> K. Toyama and G. D. Hager. </author> <title> Vision-based 3D "Surfball" input device, 1996. Provisional patent application filed through Yale Office of Cooperative Research, </title> <booktitle> OCR 770. </booktitle> <pages> 25 </pages>
Reference-contexts: from edges detected in images or line features that are themselves computed from point features. 3 Applications We have used X Vision for several purposes including hand-eye coordination [19, 20, 22], a pose-based object tracking system [32], a robust face-tracking system [51], a gesture-based drawing program, a six degree-of-freedom mouse <ref> [52] </ref>, and a variety of small video games.
Reference: [53] <author> K. Toyama, J. Wang, and G. D. Hager. SERVOMATIC: </author> <title> a modular system for robust position-ing using stereo visual servoing, </title> <booktitle> in Proc., Int. Conf. Robotics and Automation, </booktitle> <pages> pp. 2636-2643, </pages> <year> 1996. </year>
Reference-contexts: This system 16 17 defined in the plane (right). requires less than 10 milliseconds per iteration without graphics. 3.2 An Embedded Application As an illustration of the use of X Vision embedded within a larger system, we describe some results of using X Vision within a hand-eye coordination system <ref> [20, 22, 53] </ref>. The system relies on image-level feedback from two cameras to control the relative pose between an object held in a robot end-effector and a static object in the environment.
Reference: [54] <author> D. J. Williams and M. Shah. </author> <title> A fast algorithm for active contours and curvature estimation. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 55(1) </volume> <pages> 14-26, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Automatic road-following has been accomplished by tracking the edges of the road [34]. Various snake-like trackers are used to track objects in 2D as they move across the camera image <ref> [2, 8, 11, 29, 46, 49, 54] </ref>. Three-dimensional models, while more complex, allow for precise pose estimation [17, 31].
Reference: [55] <author> Y. Yagi, K. Sato, and M. Yachida. </author> <title> Evaluating effectivity of map generation by tracking vertical edges in omnidirectional image sequence, </title> <booktitle> in Proc., IEEE Int. Conf. on Robotics and Automation, </booktitle> <pages> pp. 2334-2339, </pages> <address> Nagoya, Japan, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Robots often operate in environments rich with edges, corners, and textures, making feature-based tracking a natural choice for providing visual input. Specific applications include calibration of cameras and robots [9, 28], visual-servoing and hand-eye coordination [10, 18, 25, 27, 56], mobile robot navigation and map-making <ref> [45, 55] </ref>, pursuit of moving objects [10, 26], grasping [1], and telerobotics [23]. Robotic applications most often require the tracking of objects more complex than line segments or point features, and they frequently require the ability to track multiple objects.
Reference: [56] <author> B. H. Yoshimi and P. K. Allen. </author> <title> Active, uncalibrated visual servoing, </title> <booktitle> in Proc., IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> 4, </volume> <pages> pp. 156-161, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Robotic hand-eye applications also make heavy use of visual tracking. Robots often operate in environments rich with edges, corners, and textures, making feature-based tracking a natural choice for providing visual input. Specific applications include calibration of cameras and robots [9, 28], visual-servoing and hand-eye coordination <ref> [10, 18, 25, 27, 56] </ref>, mobile robot navigation and map-making [45, 55], pursuit of moving objects [10, 26], grasping [1], and telerobotics [23]. Robotic applications most often require the tracking of objects more complex than line segments or point features, and they frequently require the ability to track multiple objects.
Reference: [57] <author> Z. Zhang and O. Faugeras. </author> <title> Determining motion from 3D line segment matches: a comparative study. </title> <journal> Image and Vision Computing, </journal> <volume> 9(1) </volume> <pages> 10-19, </pages> <month> February </month> <year> 1991. </year> <month> 26 </month>
Reference-contexts: One of the most common applications is determining structure from motion. Structure from motion algorithms attempt to recover the three-dimensional structure of objects by observing their movement in multiple camera frames. Most often, this research involves observation of line segments <ref> [12, 30, 43, 45, 57] </ref>, point features [41, 44], or both [14, 42], as they move in the image. <p> While the list of tracking applications is long, the features used in these applications are variations on a very small set of primitives: "edgels" or line segments <ref> [12, 17, 30, 31, 43, 45, 49, 57] </ref>, corners based on line segments [23, 41], small patches of texture [13], and easily detectable highlights [4, 39].
References-found: 57

