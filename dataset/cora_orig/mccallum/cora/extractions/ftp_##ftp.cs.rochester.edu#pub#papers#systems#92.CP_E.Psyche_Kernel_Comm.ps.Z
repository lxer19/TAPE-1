URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/92.CP&E.Psyche_Kernel_Comm.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/scott/pubs.html
Root-URL: 
Title: Kernel-Kernel Communication in a Shared-Memory Multiprocessor  
Author: Eliseu M. Chaves, Jr., Prakash Ch. Das, Thomas J. LeBlanc, Brian D. Marsh, and Michael L. Scott 
Address: Rochester, New York 14627-0226  
Affiliation: Computer Science Department University of Rochester  
Abstract: In the standard kernel organization on a bus-based multiprocessor, all processors share the code and data of the operating system; explicit synchronization is used to control access to kernel data structures. Distributed-memory multicomputers use an alternative approach, in which each instance of the kernel performs local operations directly and uses remote invocation to perform remote operations. Either approach to inter-kernel communication can be used in a large-scale shared-memory multiprocessor. In this paper we discuss the issues and architectural features that must be considered when choosing between remote memory access and remote invocation. We focus in particular on experience with the Psyche multiprocessor operating system on the BBN Butterfly Plus. We find that the Butterfly architecture is biased towards the use of remote invocation for kernel operations that perform a significant number of memory references, and that current architectural trends are likely to increase this bias in future machines. This conclusion suggests that straightforward parallelization of existing kernels (e.g. by using semaphores to protect shared data) is unlikely in the future to yield acceptable performance. We note, however, that remote memory access is useful for small, frequently-executed operations, and is likely to remain so. This research was supported by NSF grant no. CCR-9005633, NSF Institutional Infrastructure grant no. CDA-8822724, a DARPA/NASA Graduate Research Assistantship in Parallel Processing, the Federal University of Rio de Janeiro, and the Brazilian National Research Council. Eliseu Chaves is with the Universidade Federal do Rio de Janeiro, Brazil. He spent six months on leave at the University of Rochester in 1990. Prakash Das is now with Transarc Corp. in Pittsburgh, PA. Brian Marsh is now with the Matsushita Information Technology Lab in Princeton, NJ. Current electronic addresses are: COS99286@UFRJ.BITNET (Chaves), prakash@transarc.com, marsh@mitl.com, and -leblanc,scott-@cs.rochester.edu. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, B. Lim, D. Kranz and J. Kubiatowicz, </author> <month> ``APRIL: </month> <title> A Processor Architecture for Multiprocessing,'' </title> <booktitle> Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <month> 28-31 May </month> <year> 1990, </year> <pages> pp. 104-114. </pages> <note> In CAN 18:2. </note>
Reference-contexts: for allocating data to processors (as in so-called NUMA non-uniform memory access machines such as the BBN Butterfly [17], IBM RP3 [30], Illinois Cedar [22], and Toronto Hector [34]), or the hardware may provide coherent caches (as in the Scalable Coherent Interface [21] and the Stanford DASH [26], MIT April/Alewife <ref> [1] </ref>, and Kendall Square [18] machines). In either case, the performance and conceptual tradeoffs between the use of remote invocation and remote memory access in the kernel are not well understood, and depend both on architectural parameters and on the overall design of the operating system. <p> It is not yet clear whether the large-scale multiprocessors of the future will provide systemwide hardware cache coherence. Several projects are moving in that direction <ref> [1, 18, 21, 26] </ref>, but others are pursuing software-managed coherence beyond the bounds of a single bus [6, 12, 34]. NUMA machines are likely to be cheaper to build than their cache-coherent cousins, and recent studies suggest [8] that they can provide comparable performance for reasonable applications.
Reference: [2] <author> T. E. Anderson, H. M. Levy, B. N. Bershad and E. D. Lazowska, </author> <title> ``The Interaction of Architecture and Operating System Design,'' </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> 8-11 April </month> <year> 1991, </year> <pages> pp. </pages> <month> 108-120. </month> <journal> In ACM SIGARCH Computer Architecture News 19:2, ACM SIGOPS Operating Systems Review 25 (special issue), and ACM SIGPLAN Notices 26:4. </journal>
Reference-contexts: The importance of these tradeoffs can be seen in current trends: operating system overhead has grown to 15-20% of execution time on modern microprocessors <ref> [2] </ref>, and the growing complexity of parallel systems software demands that kernels be made as clean and maintainable as possible.
Reference: [3] <institution> BBN Advanced Computers Incorporated, ``Inside the Butterfly Plus,'' </institution> <address> Cambridge, MA, </address> <month> 16 October </month> <year> 1987. </year>
Reference-contexts: On a machine with multiprocessor nodes, however, such locking may already be necessary. 5. Case Study: Psyche on the BBN Butterfly Our experimentation with alternative communication mechanisms took place in the kernel of the Psyche operating system [32] running on a BBN Butterfly Plus multiprocessor <ref> [3] </ref>. The Psyche implementation is written in C++, and uses shared memory as the primary kernel communication mechanism. The Psyche kernel was modified to provide performance figures for remote invocation as well, with and without fine-grain locking. Our results are based on experiments using these modified versions of the kernel.
Reference: [4] <author> M. J. Bach and S. J. Buroff, </author> <title> ``Multiprocessor Unix Systems,'' </title> <journal> AT&T Bell Laboratories Technical Journal 63:8 (October 1984), </journal> <pages> pp. 1733-1750. </pages>
Reference-contexts: Unfortunately, as the scale of the machine increases, the master processor inevitably becomes a bottleneck. By using locks to protect shared data structures, several manufacturers have parallelized the Unix kernel for concurrent execution on bus-based shared-memory multiprocessors of up to 30 processors <ref> [4] </ref>. Even on this scale, however, the modifications required to avoid performance-degrading contention are non-trivial [9, 11, 29]. Operating systems for machines with large numbers of processors (hundreds or even thousands) will require extensive re-writes of existing code, or will need to be written from scratch.
Reference: [5] <author> F. Baskett, J. H. Howard and J. T. Montague, </author> <title> ``Task Communication in Demos,'' </title> <booktitle> Proceedings of the Sixth ACM Symposium on Operating Systems Principles, </booktitle> <month> November </month> <year> 1977, </year> <pages> pp. 23-31. </pages>
Reference-contexts: The message-based organization closely mimics the hardware organization of a distributed-memory multicomputer. Because it minimizes context switching, the procedure-based organization is likely to perform better on a machine with uniform memory [13]. The message-based organization may be easier to debug [19]. Most Unix kernels are procedure-based. Demos <ref> [5] </ref> and Minix [33] are message-based. Remote invocation seems to be more in keeping with the message-based approach to kernel design. Remote memory access seems appropriate to the procedure-based approach.
Reference: [6] <author> R. Bisiani and M. Ravishankar, </author> <title> ``PLUS: A Distributed Shared-Memory System,'' </title> <booktitle> Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <month> 28-31 May </month> <year> 1990, </year> <pages> pp. 115-124. </pages> <note> In CAN 18:2. </note>
Reference-contexts: It is not yet clear whether the large-scale multiprocessors of the future will provide systemwide hardware cache coherence. Several projects are moving in that direction [1, 18, 21, 26], but others are pursuing software-managed coherence beyond the bounds of a single bus <ref> [6, 12, 34] </ref>. NUMA machines are likely to be cheaper to build than their cache-coherent cousins, and recent studies suggest [8] that they can provide comparable performance for reasonable applications.
Reference: [7] <author> D. L. Black, R. F. Rashid, D. B. Golub, C. R. Hill and R. V. Baron, </author> <title> ``Translation Looka-side Buffer Consistency: A Software Approach,'' </title> <booktitle> Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> 3-6 April </month> <year> 1989, </year> <pages> pp. 113-122. </pages>
Reference-contexts: significant amount of time to get around to executing a request on the target processor, and we assume that process-level RI will be used for longer operations anyway. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 3 Note that the ability to interrupt a remote processor at high priority is required for operations such as TLB shootdown <ref> [7, 15, 31] </ref>. 4 Nested interrupt-level RIs must be performed with care.
Reference: [8] <author> W. J. Bolosky and M. L. Scott, </author> <title> ``A Trace-Based Comparison of Shared Memory Multiprocessor Architectures,'' </title> <type> TR 432, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> July </month> <year> 1992. </year> <month> 24 </month>
Reference-contexts: Several projects are moving in that direction [1, 18, 21, 26], but others are pursuing software-managed coherence beyond the bounds of a single bus [6, 12, 34]. NUMA machines are likely to be cheaper to build than their cache-coherent cousins, and recent studies suggest <ref> [8] </ref> that they can provide comparable performance for reasonable applications. Our results apply most directly to NUMA machines, but suggest that remote invocation should be attractive for kernel-kernel communication on cache-coherent machines as well.
Reference: [9] <author> J. Boykin and A. Langerman, </author> <title> ``The Parallelization of Mach/4.3BSD: Design Philosophy and Performance Analysis,'' </title> <booktitle> Proceedings of the First USENIX Workshop on Experiences Building Distributed and Multiprocessor Systems, </booktitle> <month> 5-6 October, </month> <year> 1989, </year> <pages> pp. 105-126. </pages>
Reference-contexts: By using locks to protect shared data structures, several manufacturers have parallelized the Unix kernel for concurrent execution on bus-based shared-memory multiprocessors of up to 30 processors [4]. Even on this scale, however, the modifications required to avoid performance-degrading contention are non-trivial <ref> [9, 11, 29] </ref>. Operating systems for machines with large numbers of processors (hundreds or even thousands) will require extensive re-writes of existing code, or will need to be written from scratch. The latter approach has been employed successfully by the vendors of distributed-memory multicomputers.
Reference: [10] <author> R. Bryant, H. Chang and B. Rosenburg, </author> <title> ``Experience Developing the RP3 Operating System,'' </title> <booktitle> Proceedings of the Second USENIX Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <month> 21-22 March </month> <year> 1991, </year> <pages> pp. 1-18. </pages>
Reference-contexts: The bulk of the machine is dedicated to running parallel applications with little kernel support other than high-speed parallel I/O. Similarly, the version of Mach developed for the IBM RP3 performs best when most of the processors do not make system calls <ref> [10] </ref>. We focus in the remainder of this paper on design alternatives for general-purpose parallel operating systems, in which the full range of kernel services are available with reasonable performance on every processor.
Reference: [11] <author> M. D. Campbell, R. Holt and J. </author> <title> Slice, ``Lock Granularity Tuning Mechanisms in SVR4/MP,'' </title> <booktitle> Proceedings of the Second USENIX Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <month> 21-22 March </month> <year> 1991, </year> <pages> pp. 221-228. </pages>
Reference-contexts: By using locks to protect shared data structures, several manufacturers have parallelized the Unix kernel for concurrent execution on bus-based shared-memory multiprocessors of up to 30 processors [4]. Even on this scale, however, the modifications required to avoid performance-degrading contention are non-trivial <ref> [9, 11, 29] </ref>. Operating systems for machines with large numbers of processors (hundreds or even thousands) will require extensive re-writes of existing code, or will need to be written from scratch. The latter approach has been employed successfully by the vendors of distributed-memory multicomputers.
Reference: [12] <author> D. R. Cheriton, H. A. Goosen and P. D. Boyle, </author> <title> ``Paradigm: A Highly Scalable Shared-Memory Multicomputer Architecture,'' </title> <booktitle> Computer, </booktitle> <month> February </month> <year> 1991, </year> <pages> pp. 33-46. </pages>
Reference-contexts: It is not yet clear whether the large-scale multiprocessors of the future will provide systemwide hardware cache coherence. Several projects are moving in that direction [1, 18, 21, 26], but others are pursuing software-managed coherence beyond the bounds of a single bus <ref> [6, 12, 34] </ref>. NUMA machines are likely to be cheaper to build than their cache-coherent cousins, and recent studies suggest [8] that they can provide comparable performance for reasonable applications.
Reference: [13] <author> D. Clark, </author> <title> ``The Structuring of Systems Using Upcalls,'' </title> <booktitle> Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <month> 1-4 December </month> <year> 1985, </year> <pages> pp. 171-180. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 19:5. </booktitle>
Reference-contexts: The message-based organization closely mimics the hardware organization of a distributed-memory multicomputer. Because it minimizes context switching, the procedure-based organization is likely to perform better on a machine with uniform memory <ref> [13] </ref>. The message-based organization may be easier to debug [19]. Most Unix kernels are procedure-based. Demos [5] and Minix [33] are message-based. Remote invocation seems to be more in keeping with the message-based approach to kernel design. Remote memory access seems appropriate to the procedure-based approach.
Reference: [14] <author> D. W. Clark and J. S. Emer, </author> <title> ``Performance of the VAX-11/780 Translation Buffer: Simulation and Measurement,'' </title> <journal> ACM Transactions on Computer Systems 3:1 (February 1985), </journal> <pages> pp. 31-62. </pages>
Reference-contexts: At the same time, experience with uniprocessor operating systems suggests that it is very hard to build a kernel with a high degree of address locality. Consecutive memory references tend not to lie in any small set of dense address ranges <ref> [14] </ref>, due to heavy use of pointer-based dynamic data structures, operations on multiple process contexts, interrupt-driven activity, and a lack of nested loops.
Reference: [15] <author> A. L. Cox and R. J. Fowler, </author> <title> ``The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM,'' </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> 3-6 December </month> <year> 1989, </year> <pages> pp. 32-44. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 23:5. </booktitle>
Reference-contexts: significant amount of time to get around to executing a request on the target processor, and we assume that process-level RI will be used for longer operations anyway. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 3 Note that the ability to interrupt a remote processor at high priority is required for operations such as TLB shootdown <ref> [7, 15, 31] </ref>. 4 Nested interrupt-level RIs must be performed with care.
Reference: [16] <author> A. L. Cox, R. J. Fowler and J. E. Veenstra, </author> <title> ``Interprocessor Invocation on a NUMA Multiprocessor,'' </title> <type> TR 356, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: Fundamental Costs The Butterfly Plus is a NUMA machine with one processor per node, no caches, and a remote-to-local memory access time ratio of approximately 12:1. The average measured execution time <ref> [16] </ref> of an instruction to read a 32-bit remote memory location using register indirect addressing is 6.88 ms; the corresponding instruction to read local memory takes 0.518 ms.
Reference: [17] <author> W. Crowther, J. Goodhue, E. Starr, R. Thomas, W. Milliken and T. Blackadar, </author> <title> ``Performance Measurements on a 128-Node Butterfly Parallel Processor,'' </title> <booktitle> Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <month> 20-23 August </month> <year> 1985, </year> <pages> pp. 531-540. 25 </pages>
Reference-contexts: All of memory can be accessed directly, but at very different costs. The programmer may be responsible for allocating data to processors (as in so-called NUMA non-uniform memory access machines such as the BBN Butterfly <ref> [17] </ref>, IBM RP3 [30], Illinois Cedar [22], and Toronto Hector [34]), or the hardware may provide coherent caches (as in the Scalable Coherent Interface [21] and the Stanford DASH [26], MIT April/Alewife [1], and Kendall Square [18] machines).
Reference: [18] <author> T. H. Dunigan, </author> <title> ``Kendall Square Multiprocessor: Early Experiences and Performance,'' </title> <institution> ORNL/TM-12065, Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: processors (as in so-called NUMA non-uniform memory access machines such as the BBN Butterfly [17], IBM RP3 [30], Illinois Cedar [22], and Toronto Hector [34]), or the hardware may provide coherent caches (as in the Scalable Coherent Interface [21] and the Stanford DASH [26], MIT April/Alewife [1], and Kendall Square <ref> [18] </ref> machines). In either case, the performance and conceptual tradeoffs between the use of remote invocation and remote memory access in the kernel are not well understood, and depend both on architectural parameters and on the overall design of the operating system. <p> Moreover, in a kernel with a high degree of node locality, most data items will have a node at which they usually reside (even on cache-only machines <ref> [18] </ref> with no ``main'' memory), and will migrate back to that node if temporarily accessed elsewhere. Most of the tradeoffs between remote memory access and remote invocation discussed in the following sections apply to both classes of architecture, though we will couch our discussion in terms of NUMA machines. <p> It is not yet clear whether the large-scale multiprocessors of the future will provide systemwide hardware cache coherence. Several projects are moving in that direction <ref> [1, 18, 21, 26] </ref>, but others are pursuing software-managed coherence beyond the bounds of a single bus [6, 12, 34]. NUMA machines are likely to be cheaper to build than their cache-coherent cousins, and recent studies suggest [8] that they can provide comparable performance for reasonable applications.
Reference: [19] <author> R. A. Finkel, M. L. Scott, Y. Artsy and H. Chang, </author> <title> ``Experience with Charlotte: Simplicity and Function in a Distributed Operating System,'' </title> <journal> IEEE Transactions on Software Engineering 15:6 (June 1989), </journal> <pages> pp. 676-685. </pages>
Reference-contexts: The message-based organization closely mimics the hardware organization of a distributed-memory multicomputer. Because it minimizes context switching, the procedure-based organization is likely to perform better on a machine with uniform memory [13]. The message-based organization may be easier to debug <ref> [19] </ref>. Most Unix kernels are procedure-based. Demos [5] and Minix [33] are message-based. Remote invocation seems to be more in keeping with the message-based approach to kernel design. Remote memory access seems appropriate to the procedure-based approach.
Reference: [20] <author> M. Herlihy, </author> <title> ``Wait-Free Synchronization,'' </title> <journal> ACM Transactions on Programming Languages and Systems 13:1 (January 1991), </journal> <pages> pp. 124-149. </pages>
Reference-contexts: On the other hand, clever use of atomic fetch-and-F operations to create concurrent no-wait data structures <ref> [20] </ref> may allow explicit synchronization to be omitted even for data struc tures whose operations are implemented via remote memory access. 7 hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 7 A parallel data structure is said to be wait-free if each of its access functions is guaranteed to complete within a bounded amount of time. <p> The additional time required to acquire and release a lock through constructors is about 1 to 3 ms. Synchronization using remote locks is expensive because the Butterfly's microcoded atomic operations are significantly more costly than native processor instructions. Extensive use of no-wait data structures <ref> [20] </ref> might reduce the need for fine-grain locks, but would probably not be faster, given the cost of atomic operations.
Reference: [21] <author> D. V. James, A. T. Laundrie, S. Gjessing and G. S. Sohi, </author> <title> ``Scalable Coherent Interface,'' </title> <booktitle> Computer 23:6 (June 1990), </booktitle> <pages> pp. 74-77. </pages>
Reference-contexts: The programmer may be responsible for allocating data to processors (as in so-called NUMA non-uniform memory access machines such as the BBN Butterfly [17], IBM RP3 [30], Illinois Cedar [22], and Toronto Hector [34]), or the hardware may provide coherent caches (as in the Scalable Coherent Interface <ref> [21] </ref> and the Stanford DASH [26], MIT April/Alewife [1], and Kendall Square [18] machines). <p> It is not yet clear whether the large-scale multiprocessors of the future will provide systemwide hardware cache coherence. Several projects are moving in that direction <ref> [1, 18, 21, 26] </ref>, but others are pursuing software-managed coherence beyond the bounds of a single bus [6, 12, 34]. NUMA machines are likely to be cheaper to build than their cache-coherent cousins, and recent studies suggest [8] that they can provide comparable performance for reasonable applications.
Reference: [22] <author> D. J. Kuck, E. S. Davidson, D. H. Lawrie and A. H. Sameh, </author> <title> ``Parallel Supercomputing Today and the Cedar Approach,'' </title> <booktitle> Science 231 (28 February 1986), </booktitle> <pages> pp. 967-974. </pages>
Reference-contexts: All of memory can be accessed directly, but at very different costs. The programmer may be responsible for allocating data to processors (as in so-called NUMA non-uniform memory access machines such as the BBN Butterfly [17], IBM RP3 [30], Illinois Cedar <ref> [22] </ref>, and Toronto Hector [34]), or the hardware may provide coherent caches (as in the Scalable Coherent Interface [21] and the Stanford DASH [26], MIT April/Alewife [1], and Kendall Square [18] machines).
Reference: [23] <author> H. C. Lauer and R. M. Needham, </author> <title> ``On the Duality of Operating System Structures,'' </title> <booktitle> ACM SIGOPS Operating Systems Review 13:2 (April 1979), </booktitle> <pages> pp. 3-19. </pages> <booktitle> Originally presented at the Second International Symposium on Operating Systems, </booktitle> <month> October </month> <year> 1978. </year>
Reference-contexts: Compatibility With the Conceptual Model of Kernel Organization There are two broad classes of kernel organization, identified by Lauer and Needham <ref> [23] </ref> as the message-based and procedure-based approaches (see figure 2). In a procedure-based kernel there is no fundamental distinction between a process in user space and a process in the kernel.
Reference: [24] <author> T. J. LeBlanc, J. M. Mellor-Crummey, N. M. Gafter, L. A. Crowl and P. C. Dibble, </author> <title> ``The Elmwood Multiprocessor Operating System,'' </title> <journal> Software Practice and Experience 19:11 (November 1989), </journal> <pages> pp. 1029-1056. </pages>
Reference-contexts: Remote memory access seems appropriate to the procedure-based approach. When porting an operating system from some other environment, the pre-existence of a procedure-based or message-based bias in the implementation may suggest the use of the corresponding mechanism for kernel-kernel communication, though mixed approaches are possible <ref> [24] </ref>. If a procedure 14 based kernel is used on a uniprocessor, the lack of context switching in the kernel may obviate the need for explicit synchronization in many cases. Extending the procedure-based approach to include remote memory access may then incur substantial new costs for locks.
Reference: [25] <author> S. J. Leffler, M. K. McKusick, M. J. Karels and J. S. Quarterman, </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System, </title> <publisher> The Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Implementation of Remote Invocation The simplest way to perform a remote invocation is to execute the requested operation in the interrupt handler of the interprocessor communication mechanism. In Unix terminology <ref> [25] </ref>, this places the code for the operation in the ``bottom half'' of the target processor's kernel. Alternatively, one can arrange to execute remote invocations in a normal process context in the ``top half'' of the kernel.
Reference: [26] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta and J. Hennessy, </author> <title> ``The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor,'' </title> <booktitle> Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <month> 28-31 May </month> <year> 1990, </year> <pages> pp. 148-159. </pages> <note> In CAN 18:2. </note>
Reference-contexts: may be responsible for allocating data to processors (as in so-called NUMA non-uniform memory access machines such as the BBN Butterfly [17], IBM RP3 [30], Illinois Cedar [22], and Toronto Hector [34]), or the hardware may provide coherent caches (as in the Scalable Coherent Interface [21] and the Stanford DASH <ref> [26] </ref>, MIT April/Alewife [1], and Kendall Square [18] machines). In either case, the performance and conceptual tradeoffs between the use of remote invocation and remote memory access in the kernel are not well understood, and depend both on architectural parameters and on the overall design of the operating system. <p> It is not yet clear whether the large-scale multiprocessors of the future will provide systemwide hardware cache coherence. Several projects are moving in that direction <ref> [1, 18, 21, 26] </ref>, but others are pursuing software-managed coherence beyond the bounds of a single bus [6, 12, 34]. NUMA machines are likely to be cheaper to build than their cache-coherent cousins, and recent studies suggest [8] that they can provide comparable performance for reasonable applications.
Reference: [27] <author> E. P. Markatos and T. J. LeBlanc, </author> <title> ``Shared-Memory Multiprocessor Trends and the Implications for Parallel Program Performance,'' </title> <type> TR 420, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> May </month> <year> 1992. </year> <month> 26 </month>
Reference-contexts: Deliberate exploitation of node locality in the kernel for a cache-coherent machine would associate each data structure with a ``home node'' on which that structure is accessed most often. As inter-processor communication becomes slower and slower relative to processor performance <ref> [27] </ref>, operations that touch several cache lines that ``belong'' to another processor might profitably be dispatched to that processor for execution, rather than running locally. Further performance studies will be needed to quantify kernel-kernel communication tradeoffs more precisely on large cache-coherent machines.
Reference: [28] <author> J. M. Mellor-Crummey and M. L. Scott, </author> <title> ``Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors,'' </title> <journal> ACM Transactions on Computer Systems 9:1 (Febru-ary 1991), </journal> <pages> pp. 21-65. </pages>
Reference-contexts: Mapping a memory segment into the current address space, for example, can require up to 9 lock acquisitions. Creating a segment can require 38 lock acquisitions. A cheap implementation of locks is critical. We use a test-and-test&set lock <ref> [28] </ref> to minimize latency in the absence of contention. If the lock is in local memory, we use the native MC68020 TAS instruction.
Reference: [29] <author> N. Paciorek, S. LoVerso and A. Langerman, </author> <title> ``Debugging Multiprocessor Operating System Kernels,'' </title> <booktitle> Proceedings of the Second USENIX Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <month> 21-22 March </month> <year> 1991, </year> <pages> pp. 185-202. </pages>
Reference-contexts: By using locks to protect shared data structures, several manufacturers have parallelized the Unix kernel for concurrent execution on bus-based shared-memory multiprocessors of up to 30 processors [4]. Even on this scale, however, the modifications required to avoid performance-degrading contention are non-trivial <ref> [9, 11, 29] </ref>. Operating systems for machines with large numbers of processors (hundreds or even thousands) will require extensive re-writes of existing code, or will need to be written from scratch. The latter approach has been employed successfully by the vendors of distributed-memory multicomputers.
Reference: [30] <author> G. R. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P. McAu-liffe, E. A. Melton, V. A. Norton and J. Weiss, </author> <title> ``The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture,'' </title> <booktitle> Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <month> 20-23 August </month> <year> 1985, </year> <pages> pp. 764-771. </pages>
Reference-contexts: All of memory can be accessed directly, but at very different costs. The programmer may be responsible for allocating data to processors (as in so-called NUMA non-uniform memory access machines such as the BBN Butterfly [17], IBM RP3 <ref> [30] </ref>, Illinois Cedar [22], and Toronto Hector [34]), or the hardware may provide coherent caches (as in the Scalable Coherent Interface [21] and the Stanford DASH [26], MIT April/Alewife [1], and Kendall Square [18] machines).
Reference: [31] <author> B. Rosenburg, </author> <title> ``Low-Synchronization Translation Lookaside Buffer Consistency in Large-Scale Shared-Memory Multiprocessors,'' </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> 3-6 December </month> <year> 1989, </year> <pages> pp. 137-146. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 23:5. </booktitle>
Reference-contexts: significant amount of time to get around to executing a request on the target processor, and we assume that process-level RI will be used for longer operations anyway. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 3 Note that the ability to interrupt a remote processor at high priority is required for operations such as TLB shootdown <ref> [7, 15, 31] </ref>. 4 Nested interrupt-level RIs must be performed with care.
Reference: [32] <author> M. L. Scott, T. J. LeBlanc, B. D. Marsh, T. G. Becker, C. Dubnicki, E. P. Markatos and N. G. Smithline, </author> <title> ``Implementation Issues for the Psyche Multiprocessor Operating System,'' </title> <booktitle> Computing Systems 3:1 (Winter 1990), </booktitle> <pages> pp. </pages> <month> 101-137. </month> <title> Earlier version presented at the First USENIX Workshop on Experiences Building Distributed and Multiprocessor Systems, </title> <address> Ft. Lauderdale, FL, </address> <month> 5-6 October, </month> <year> 1989. </year>
Reference-contexts: Mechanisms to cache information about kernel data structures may be limited in their effectiveness by the lack of address locality. Systems that map remote kernel data into a separate kernel-kernel address space <ref> [32] </ref> may waste large amounts of time switching back and forth between the kernel-kernel space and the various user-kernel spaces. 4.3. Competition for Processor and Memory Cycles Operations that access a central resource must serialize at some level. <p> On a machine with multiprocessor nodes, however, such locking may already be necessary. 5. Case Study: Psyche on the BBN Butterfly Our experimentation with alternative communication mechanisms took place in the kernel of the Psyche operating system <ref> [32] </ref> running on a BBN Butterfly Plus multiprocessor [3]. The Psyche implementation is written in C++, and uses shared memory as the primary kernel communication mechanism. The Psyche kernel was modified to provide performance figures for remote invocation as well, with and without fine-grain locking. <p> The most plausible scenario arises with low-latency operations that cannot be performed via remote access. TLB shootdown is such an operation on most machines; instructions that manipulate the TLB cannot be invoked remotely. We also use interrupt-level RI for console I/O, and to implement our remote kernel debugging facility <ref> [32] </ref>.
Reference: [33] <author> A. S. Tanenbaum, </author> <title> Operating Systems: Design and Implementation, </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: Because it minimizes context switching, the procedure-based organization is likely to perform better on a machine with uniform memory [13]. The message-based organization may be easier to debug [19]. Most Unix kernels are procedure-based. Demos [5] and Minix <ref> [33] </ref> are message-based. Remote invocation seems to be more in keeping with the message-based approach to kernel design. Remote memory access seems appropriate to the procedure-based approach.

References-found: 33

