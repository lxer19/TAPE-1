URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/approx-rl-loss.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Email: singh@cs.umass.edu, yee@cs.umass.edu  e-mail: singh@psyche.mit.edu.  
Title: An Upper Bound on the Loss from Approximate Optimal-Value Functions  
Author: Satinder P. Singh Richard C. Yee 
Note: Running Head: Loss from Approximate Optimal-Value Functions Singh's address from September 1993 to August 1995 will be:  
Address: Amherst, MA 01003  Cambridge, MA 02139,  
Affiliation: Department of Computer Science University of Massachusetts  Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology,  
Abstract-found: 0
Intro-found: 1
Reference: <author> Anderson, C. W. </author> <year> (1986). </year> <title> Learning and Problem Solving with Multilayer Connectionist Systems. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA 01003. </address>
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1991). </year> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report TR-91-57, </type> <institution> Department of Computer Science, University of Mas-sachusetts. </institution>
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 (5), </volume> <pages> 834-846. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M. & Moore, J. (Eds.), </editor> <booktitle> Learning and Computational Neuroscience: Foundations of Adaptive Networks, chapter 13. </booktitle> <address> Cambridge, MA: </address> <publisher> Bradford Books/MIT Press. </publisher>
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic programming: Deterministic and stochastic models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: do not address the issue of converging to good approximations, they show that if good approximations of values are achieved, then reasonable performance can be guaranteed. 1 2 Problem Statement and Theorem We consider stationary Markovian decision processes (MDPs, henceforth also called tasks) that have finite state and action sets <ref> (e.g., see Bertsekas, 1987, Barto et al., 1990) </ref>. Let X be the state set, A (x) be the action set for state x 2 X, and P xy (a) be the probability of a transition from state x to state y, given the execution of action a 2 A (x). <p> Thus, the theoretical and empirical investigation of function approximation and DP-based learning remains an active area of research. Acknowledgements We thank Andrew Barto for identifying the connection to <ref> (Bertsekas, 1987) </ref>, and we thank the anonymous reviewers for many helpful comments. This work was supported by grants to Prof. A. G. Barto from the Air Force Office of Scientific Research, Bolling AFB, under Grant AFOSR-F49620-93-1-0269 and by the National Science Foundation under Grants ECS-8912623 and ECS-9214866.
Reference: <author> Bradtke, S. J. </author> <year> (1993). </year> <title> Reinforcement learning applied to linar quadratic regulation. </title> <editor> In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA. </address> <publisher> IEEE, Morgan Kaufmann. </publisher>
Reference: <author> Porteus, E. </author> <year> (1971). </year> <title> Some bounds for discounted sequential decision processes. </title> <journal> Management Science, </journal> <volume> 19, </volume> <pages> 7-11. </pages>
Reference-contexts: Nevertheless, upper bounds on approximation losses can be computed from the following upper bound on * <ref> (Porteus, 1971) </ref>. Let C (x) = V 0 (x) ~ V (x) a2A (x) 4 R (x; a) + y2X 3 and let ffi = max x2X C (x); then * ffi 1fl .
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <editor> In Porter, B. W. & Mooney, R. H. (Eds.), </editor> <booktitle> Machine Learning: Proceedings of the Seventh International Conference (ML90), </booktitle> <pages> pages 216-224, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. 8 Tesauro, G. </publisher> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 257-277. </pages>
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 279-292. </pages>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, University of Cambridge, </institution> <address> Cambridge, England. </address>
Reference-contexts: Using a natural definition of the loss in performance due to approximation, we derive an upper bound on the loss which is slightly tighter than the one indicated in Bertsekas (1987). We also show the corresponding extension to Q-learning <ref> (Watkins, 1989) </ref>. <p> Q.E.D. Q-learning If neither the payoffs nor the state-transition probabilities are known, then the analogous bound for Q-learning <ref> (Watkins, 1989) </ref> is as follows. Evaluations are defined by Q (x t ; a) = R (x t ; a) + flE fV (x t+1 )g ; where V (x) = max a Q (x; a).
Reference: <author> Werbos, P. J. </author> <year> (1987). </year> <title> Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 17 (1), </volume> <pages> 7-20. </pages>
Reference: <author> Williams, R. J. & Baird, L. C. </author> <year> (1993). </year> <title> Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems. </title> <type> Technical Report NU-CCS-93-11, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA 02115. </address> <month> 9 </month>
References-found: 13

