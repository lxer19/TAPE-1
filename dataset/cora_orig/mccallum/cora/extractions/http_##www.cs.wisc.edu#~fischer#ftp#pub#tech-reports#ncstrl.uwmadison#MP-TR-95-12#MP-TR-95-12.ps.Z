URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-95-12/MP-TR-95-12.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-95-12/
Root-URL: http://www.cs.wisc.edu
Title: SMOOTHING METHODS IN MATHEMATICAL PROGRAMMING  
Author: By Chunhui Chen 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1995  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Dimitri P. Bertsekas. </author> <title> Projected Newton methods for optimization problems with simple constraints. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 20 </volume> <pages> 221-246, </pages> <year> 1982. </year>
Reference-contexts: 0 otherwise (2.21) 18 ff log (1 + e ffx ) with ff = 5 s (x; ff) 1+e ffx with ff = 5 t (x; ff) (1+e ffx ) 2 with ff = 5 19 Here D 1 = 0; D 2 = 1 2 ; suppfd (x)g = <ref> [0; 1] </ref> and ^p (x; fi) = &gt; &gt; &gt; &gt; &gt; &gt; &lt; 0 if x &lt; 0 2fi if 0 x fi 2 if x &gt; fi This function can also be obtained by applying the Moreau-Yosida regular ization [16, p.13] to the plus function. <p> By Proposition 4.1.4 we get that x satisfies (4.2). If y is another solution of (4.2), then 0 = (x y)(R (x) R (y)) = (x y) t=0 for some t 2 <ref> [0; 1] </ref>. Since rR is positive definite by Proposition 4.1.4, it follows that x = y. Therefore equation (4.2) has a unique solution. Let x (fi) be a solution of (4.2). Then x (fi) = ^p (x (fi) F (x (fi)); fi). <p> Figures 4.6 to 4.8 indicate that our smooth algorithm is faster than PATH solver for the larger problems, whereas PATH solver is faster on smaller problems. The newest version of PATH (PATH 2.7) that uses a Newton method on the active set <ref> [1] </ref> as a preprocessor, improves solution times on the larger problems. Our smooth method can be similarly improved by adding the projected Newton preprocessor. We have compared PATH and SMOOTH with a Newton preprocessor on a Sun SPARCstation 20. The results are given in Figures 4.9 to 4.12.
Reference: [2] <author> Bintong Chen and P. T. Harker. </author> <title> A non-interior-point continuation method for linear complementarity problems. </title> <journal> SIAM Journal on Matrix Analysis and Applicatons, </journal> <volume> 14 </volume> <pages> 1168-1190, </pages> <year> 1993. </year>
Reference-contexts: With this approximation, many efficient algorithms, such as Newton and quasi-Newton methods, can be easily employed. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [24], multi-commodity flow problems [42], non-smooth programming [54, 22], linear and convex inequalities [4], and linear complementarity problems <ref> [2, 4, 19] </ref>. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> Following are several other smooth plus functions based on probability den sity functions proposed by other authors. Example 2.4.2 Chen-Harker-Kanzow-Smale Smooth Plus Function [51], [19] and <ref> [2] </ref> d (x) = (x 2 + 4) 2 Here D 1 = 1; D 2 = 0; suppfd (x)g = R and ^p (x; fi) = p 2 Example 2.4.3 Pinar-Zenios Smooth Plus Function [42] Let 8 &gt; &gt; : 0 otherwise (2.21) 18 ff log (1 + e ffx <p> Hence an exact solution of (4.2) is interior to the feasible region. However the iterates of the smooth method, which are only approximate solutions of (4.2), are not necessarily feasible. For the function ^p defined in Example 2.4.2 <ref> [51, 19, 2] </ref> , the exact solution x of the equation (4.2) satisfies x &gt; 0; F (x) &gt; 0; x i F i (x) = fi 2 ; i = 1; ; n which is precisely the central path of the interior point method for solving NCP. <p> Methods that trace this path but allow iterates to be exterior to the feasible region have been proposed in [51], <ref> [2] </ref> and [19]. In [20], the relation between Smale's method [51] and the central path was pointed out.
Reference: [3] <author> Chunhui Chen and O. L. Mangasarian. </author> <title> Hybrid misclassification minimization. </title> <type> Technical Report 95-05, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> February </month> <year> 1995. </year> <note> Advances in Computational Mathematics, submitted. Available from ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-05.ps.Z. </note>
Reference-contexts: We have shown that smoothing methods constitute a powerful computational tool for solving broad classes of optimization and related problems. Further study and application of these methods to problems in related areas such as machine learning <ref> [27, 3] </ref> appears to be promising. 88
Reference: [4] <author> Chunhui Chen and O.L. Mangasarian. </author> <title> Smoothing methods for convex inequalities and linear complementarity problems. </title> <type> Technical Report 1191, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, </institution> <address> Wis-consin 53706, </address> <month> November </month> <year> 1993. </year> <note> Mathematical Programming, to appear. 89 </note>
Reference-contexts: With this approximation, many efficient algorithms, such as Newton and quasi-Newton methods, can be easily employed. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [24], multi-commodity flow problems [42], non-smooth programming [54, 22], linear and convex inequalities <ref> [4] </ref>, and linear complementarity problems [2, 4, 19]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> With this approximation, many efficient algorithms, such as Newton and quasi-Newton methods, can be easily employed. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [24], multi-commodity flow problems [42], non-smooth programming [54, 22], linear and convex inequalities [4], and linear complementarity problems <ref> [2, 4, 19] </ref>. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> The first example, which will be used throughout this paper, is based on the following classical sigmoid function of neural networks <ref> [15, 27, 4] </ref>: s (x; ff) = 1 + e ffx ; ff &gt; 0 (2.12) This function approximates the step function (x) as ff tends to infinity. <p> Since the derivative with respect to x of this function tends to the Dirac delta function as ff tends to infinity, it follows that ff plays the role of fi 1 and we shall therefore take ff = fi Example 2.4.1 Neural Networks Smooth Plus Function <ref> [4, 5] </ref> Let e x Here D 1 = log 2; D 2 = 0 and suppfd (x)g = R, where D 1 and D 2 are defined by (2.10) and (2.11).
Reference: [5] <author> Chunhui Chen and O.L. Mangasarian. </author> <title> A class of smoothing functions for nonlinear and mixed complementarity problems. </title> <type> Technical Report 94-11, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, </institution> <address> Wis-consin 53706, </address> <month> August </month> <year> 1994. </year> <note> Computational Optimization and Applications, to appear. </note>
Reference-contexts: Since the derivative with respect to x of this function tends to the Dirac delta function as ff tends to infinity, it follows that ff plays the role of fi 1 and we shall therefore take ff = fi Example 2.4.1 Neural Networks Smooth Plus Function <ref> [4, 5] </ref> Let e x Here D 1 = log 2; D 2 = 0 and suppfd (x)g = R, where D 1 and D 2 are defined by (2.10) and (2.11).
Reference: [6] <author> R.W. Cottle, F. Giannessi, and J.-L. Lions. </author> <title> Variational Inequalities and Complementariy Problems. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: For example, most optimality conditions of mathematical programming [34] as well as variational inequalities <ref> [6] </ref> and extended complementarity problems [29, 12, 53] can be so formulated. In this sense, the plus function plays a key role in mathematical programming. But one big disadvantage of the plus function is that it is not smooth because it is not differentiable.
Reference: [7] <author> R.W. Cottle, J.-S. Pang, and R.E. Stone. </author> <title> The Linear Complementarity Problem. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: We shall denote this problem by LCP (M; q). 54 We will show that under the assumption that M is a P 0 matrix, that is a matrix with nonnegative principal minors <ref> [7] </ref>, all stationary points of (4.8) are solutions of (4.2). First we will state a simple lemma for P 0 matrices. Lemma 4.2.1 Suppose M 2 R nfin is a P 0 matrix. <p> Therefore x i (M x) i = x i (d i x i ) = d i x 2 i , which is negative whenever x i 6= 0; i = 1; ; n. This contradicts Theorem 3.4.2 of <ref> [7] </ref>. Theorem 4.2.2 Suppose that d (x) satisfies (A1) - (A3) and ^p (x; fi) is defined by Definitions 2.1.1 or 2.2.1. Consider LCP (M; q) with M 2 P 0 . <p> This theorem can be proved easily by Lemma 4.2.1 and the formulation of rf (x) that is given in the proof of Proposition 4.1.4. Note that the class of P 0 matrices contains the classes of P matrices, positive semi-definite matrices and row-sufficient matrices <ref> [7] </ref>. For this class of matrices, if f (x) defined by (4.8) has a stationary point, that point is also a solution of (4.2). Now we establish the existence of a solution to (4.2) for P 0 " R 0 matrices. <p> Now we establish the existence of a solution to (4.2) for P 0 " R 0 matrices. A matrix M is called an R 0 matrix if the only solution to LCP (M; 0) is the zero vector <ref> [7] </ref>. 55 Theorem 4.2.3 Suppose that d (x) satisfies (A1) - (A3) and ^p (x; fi) is defined by Definitions 2.1.1 or 2.2.1. Consider LCP (M; q) with M 2 P 0 " R 0 . The system of nonlinear equations (4.2) always has a solution. <p> We used a Newton method with a safeguarded linear search to solve the smooth nonlinear equation (4.2). We compared the smooth method with Lemke's method <ref> [7] </ref>. The smooth algorithms were implemented in C. All problems were generated randomly. The matrix M was determined by: M = AA T + C, where A is an n fi r random matrix, r is a random number between 1 to n and C is a random skew-symmetric matrix. <p> We note that the SOR method of De Leone and Tork Roth [23] does not apply to this class of nonsymmetric LCP nor do other splitting methods described in <ref> [7] </ref>. In fact, the SOR method of [23] failed on all test problems. 75 Figures 4.3 and 4.4 show the CPU times for the smooth algorithm and Lemke's method. The smooth algorithm is always better than Lemke's method for both dense as well as the sparse problems.
Reference: [8] <author> J.E. Dennis and R.B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: For the cases m = 2n and m = 4n, the smooth algorithm is faster than the other two algorithms. For the case m = n, the smooth algorithm is faster than MINOS and comparable with the relaxation method. For convex inequalities, we use the BFGS algorithm <ref> [8] </ref> to solve the unconstrained minimization problem for variables up to 150, and the limited memory BFGS algorithm [36] for larger problems. Starting with ff = 5, we increased ff by a factor of 1.05 to 1.2 at each minor iteration. <p> We will state the following convergence theorem <ref> [8] </ref> and omit its proof. Theorem 4.1.12 Consider a solvable monotone nonlinear complementarity problem (4.1) with F (x) 2 LC 1 K (R n ). <p> According to the standard result of local quadratic convergence for the Newton Method, Theorem 5.2.1 in <ref> [8] </ref>, the conclusion follows. (4) Let ff i ; i = 0; 1; , be the sequence of different parameters ff used in Algorithm 4.4.5.
Reference: [9] <author> S.P. Dirkse and M.C. Ferris. </author> <title> The path solver: A non-monotone stabilization scheme for mixed complementarity problems. </title> <institution> Computer Sciences Department Technical Report 1179, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1993. </year>
Reference-contexts: Since then, several approaches based on B-differentiable equations were investigated in [14, 38, 39]. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [40], as well as a Newton method with a path following technique <ref> [43, 9] </ref>, and a trust region Newton method for 4 solving a nonlinear least squares reformulation of the NCP [31]. With the exception of [31], a feature common to all these methods is that the subproblem at each Newton iteration is still a combinatorial problem. <p> In Section 4.5, encouraging numerical testing results are given for positive semidef-inite linear complementarity problems up to 10,000 variables and 52 problems from the MCPLIB [10] which includes all the problems attempted in <ref> [14, 40, 9] </ref>. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model [40, 52] which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> For this example, the error along our smooth path is smaller than that along the central path for the same value of the parameter fi. 4.4 The Mixed Complementarity Problem The mixed complementarity problem (MCP) is defined as follows <ref> [9] </ref>: Given a differentiable F : R n ! R n ; l; u 2 R n ; l &lt; u, where R = R [ f+1; 1g, find x; w; v 2 R n , such that F (x) w + v = 0 0 v ? u x 0 <p> For dense problems, we compared the smooth algorithm with Lemke's method which was implemented in FORTRAN. For sparse problems with density between 0.012 and 0.15 percent, we compared the smooth algorithm with a sparse version of Lemke's method <ref> [9] </ref>, which employs sparse basis updating techniques. We note that the SOR method of De Leone and Tork Roth [23] does not apply to this class of nonsymmetric LCP nor do other splitting methods described in [7]. <p> The details of implementing the smooth algorithm are given in Appendix C. For comparison, we also give the results for the PATH solver <ref> [9] </ref>. Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [14], [40] and [9], 51 problems are from the MCPLIB [10], and one is the generalized von Thunen model from [40] and [52]. <p> The details of implementing the smooth algorithm are given in Appendix C. For comparison, we also give the results for the PATH solver <ref> [9] </ref>. Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [14], [40] and [9], 51 problems are from the MCPLIB [10], and one is the generalized von Thunen model from [40] and [52]. Our smooth algorithm was run using one set of default parameters and so was the PATH solver. <p> Figures 4.6, 4.7 and 4.8 depict the CPU times for all remaining problems except the von Thunen model. We 76 method fails on all these problems. Comparison of solution times for sparse Lemke and smooth methods. SOR method fails on all these problems. 77 note that the PATH solver <ref> [9] </ref> is faster than Josephy's Newton method [18] and Rutherford's GAMS [11] mixed inequality and linear equation solver (MILES) [50] which is also Newton-based. Figures 4.6 to 4.8 indicate that our smooth algorithm is faster than PATH solver for the larger problems, whereas PATH solver is faster on smaller problems.
Reference: [10] <author> S.P. Dirkse and M.C. Ferris. MCPLIB: </author> <title> A collection of nonlinear mixed complementarity problems. </title> <institution> Computer Sciences Department Technical Report 1215, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1994. </year> <month> 90 </month>
Reference-contexts: An efficient smooth algorithm based on the Newton-Armijo approach with an adjusted smoothing parameter is also given and convergence is established. In Section 4.5, encouraging numerical testing results are given for positive semidef-inite linear complementarity problems up to 10,000 variables and 52 problems from the MCPLIB <ref> [10] </ref> which includes all the problems attempted in [14, 40, 9]. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model [40, 52] which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> For comparison, we also give the results for the PATH solver [9]. Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [14], [40] and [9], 51 problems are from the MCPLIB <ref> [10] </ref>, and one is the generalized von Thunen model from [40] and [52]. Our smooth algorithm was run using one set of default parameters and so was the PATH solver. The smooth algorithm is written in the C language and implemented by using the GAMS/CPLIB [11]. <p> A MINOS routine [33] was used to perform a sparse LU decomposition for solving sparse linear equations. Both algorithms use the same convergence tolerance of * = 1:0e 6. Table 4.5.1 gives a simple description of the test problems <ref> [10] </ref>. The average CPU times taken by PATH solver and smooth algorithm for all small problems are depicted in Figure 4.5. Figures 4.6, 4.7 and 4.8 depict the CPU times for all remaining problems except the von Thunen model. We 76 method fails on all these problems. <p> An efficient smooth algorithm based on the Newton-Armijo approach with an adjusted smoothing parameter, is also given and its convergence is established. Very encouraging numerical testing results are given for 52 problems from the MCPLIB <ref> [10] </ref> which includes all the problems attempted in [14, 40]. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model [40, 52] which is solved here to an accuracy of 1.0e-7.
Reference: [11] <author> S.P. Dirkse, M.C. Ferris, P.V. Preckel, and T. Rutherford. </author> <title> The GAMS callable program library for variational and complementarity solvers. </title> <type> Manuscript, </type> <institution> University of Wisconsin, </institution> <year> 1993. </year>
Reference-contexts: Our smooth algorithm was run using one set of default parameters and so was the PATH solver. The smooth algorithm is written in the C language and implemented by using the GAMS/CPLIB <ref> [11] </ref>. A MINOS routine [33] was used to perform a sparse LU decomposition for solving sparse linear equations. Both algorithms use the same convergence tolerance of * = 1:0e 6. Table 4.5.1 gives a simple description of the test problems [10]. <p> We 76 method fails on all these problems. Comparison of solution times for sparse Lemke and smooth methods. SOR method fails on all these problems. 77 note that the PATH solver [9] is faster than Josephy's Newton method [18] and Rutherford's GAMS <ref> [11] </ref> mixed inequality and linear equation solver (MILES) [50] which is also Newton-based. Figures 4.6 to 4.8 indicate that our smooth algorithm is faster than PATH solver for the larger problems, whereas PATH solver is faster on smaller problems.
Reference: [12] <author> M.S. Gowda. </author> <title> On the extended linear complementarity problem. </title> <type> Technical report, </type> <institution> Department of Mathematics & Statistics, University of Maryland Baltimore County, Baltimore, Maryland, </institution> <year> 1994. </year>
Reference-contexts: For example, most optimality conditions of mathematical programming [34] as well as variational inequalities [6] and extended complementarity problems <ref> [29, 12, 53] </ref> can be so formulated. In this sense, the plus function plays a key role in mathematical programming. But one big disadvantage of the plus function is that it is not smooth because it is not differentiable.
Reference: [13] <author> P. T. Harker and J.-S. Pang. </author> <title> Finite-dimensional variational inequality and nonlinear complementarity problems: A survey of theory, algorithms and applications. </title> <journal> Mathematical Programming, </journal> <volume> 48 </volume> <pages> 161-220, </pages> <year> 1990. </year>
Reference-contexts: Even for the case when the original linear inequalities are unsolvable, our method still gives an approximate solution in a least error sense. In Chapter 4 we consider mixed complementarity problems. There are many Newton-based algorithms for solving nonlinear complementarity problems, variational inequalities and mixed complementarity problems. In <ref> [13] </ref> a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in [46, 47, 48]. Since then, several approaches based on B-differentiable equations were investigated in [14, 38, 39]. <p> We first consider the strongly monotone NCP, that is, there exists a k &gt; 0 such that for any x; y 2 R n (F (x) F (y)) T (x y) kkx yk 2 (4.5) Since the NCP is strongly monotone, it has a unique solution <ref> [13] </ref>. The following error bound for the strongly monotone NCP is given as Theorem 3.2.1 in [44]. 44 Lemma 4.1.2 Let the NCP be strongly monotone and let F (x) be Lipschitz continuous.
Reference: [14] <author> P.T. Harker and B. Xiao. </author> <title> Newton's method for the nonlinear complementarity problem: A B-differentiable equation approach. </title> <journal> Mathematical Programming, </journal> <volume> 48 </volume> <pages> 339-357, </pages> <year> 1990. </year>
Reference-contexts: In [13] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in [46, 47, 48]. Since then, several approaches based on B-differentiable equations were investigated in <ref> [14, 38, 39] </ref>. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [40], as well as a Newton method with a path following technique [43, 9], and a trust region Newton method for 4 solving a nonlinear least squares reformulation of the NCP [31]. <p> In Section 4.5, encouraging numerical testing results are given for positive semidef-inite linear complementarity problems up to 10,000 variables and 52 problems from the MCPLIB [10] which includes all the problems attempted in <ref> [14, 40, 9] </ref>. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model [40, 52] which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> The details of implementing the smooth algorithm are given in Appendix C. For comparison, we also give the results for the PATH solver [9]. Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in <ref> [14] </ref>, [40] and [9], 51 problems are from the MCPLIB [10], and one is the generalized von Thunen model from [40] and [52]. Our smooth algorithm was run using one set of default parameters and so was the PATH solver. <p> An efficient smooth algorithm based on the Newton-Armijo approach with an adjusted smoothing parameter, is also given and its convergence is established. Very encouraging numerical testing results are given for 52 problems from the MCPLIB [10] which includes all the problems attempted in <ref> [14, 40] </ref>. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model [40, 52] which is solved here to an accuracy of 1.0e-7.
Reference: [15] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: The first example, which will be used throughout this paper, is based on the following classical sigmoid function of neural networks <ref> [15, 27, 4] </ref>: s (x; ff) = 1 + e ffx ; ff &gt; 0 (2.12) This function approximates the step function (x) as ff tends to infinity.
Reference: [16] <author> J.-B. Hiriart-Urruty and C. </author> <title> Lemarechal. Convex Analysis and Minimization Algorithms I. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: is parameter ized by ^ t (x; fi) = 1 fi d ( x fi ), where fi is a positive parameter, define the function ^p (x; fi) as follows ^p (x; fi) = 1 (x t) + ^ t (t; fi)dt R x (2.7) This formulation was given in <ref> [16, p.12] </ref> for a kernel function and in [22] for a density function with finite support. We will give our results in terms of a density function with arbitrary support. <p> D 2 = 1 2 ; suppfd (x)g = [0; 1] and ^p (x; fi) = &gt; &gt; &gt; &gt; &gt; &gt; &lt; 0 if x &lt; 0 2fi if 0 x fi 2 if x &gt; fi This function can also be obtained by applying the Moreau-Yosida regular ization <ref> [16, p.13] </ref> to the plus function.
Reference: [17] <author> A. J. Hoffman. </author> <title> On approximate solutions of systems of linear inequalities. </title> <journal> Journal of Research of the National Bureau of Standards, </journal> <volume> 49 </volume> <pages> 263-265, </pages> <year> 1952. </year> <month> 91 </month>
Reference-contexts: The justification for this appellation is that such a solution gives an approximate solution of (3.1) and is obtained by minimizing a smooth function. First we will state an error bound lemma for the linear inequalities (3.1). Lemma 3.2.1 Error bound <ref> [17] </ref> [25] Suppose that the linear inequalities Ax b have a nonempty solution set X.
Reference: [18] <author> N.H. Josephy. </author> <title> Newton's method for generalized equations. </title> <type> Technical Summary Report 1965, </type> <institution> Mathematics Research Center, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1979. </year>
Reference-contexts: We 76 method fails on all these problems. Comparison of solution times for sparse Lemke and smooth methods. SOR method fails on all these problems. 77 note that the PATH solver [9] is faster than Josephy's Newton method <ref> [18] </ref> and Rutherford's GAMS [11] mixed inequality and linear equation solver (MILES) [50] which is also Newton-based. Figures 4.6 to 4.8 indicate that our smooth algorithm is faster than PATH solver for the larger problems, whereas PATH solver is faster on smaller problems.
Reference: [19] <author> C. Kanzow. </author> <title> Some tools allowing interior-point methods to become non-interior. </title> <type> Technical report, </type> <institution> Institue of Applied Mathematics, Unversity of Hamburg, Germany, </institution> <year> 1994. </year>
Reference-contexts: With this approximation, many efficient algorithms, such as Newton and quasi-Newton methods, can be easily employed. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [24], multi-commodity flow problems [42], non-smooth programming [54, 22], linear and convex inequalities [4], and linear complementarity problems <ref> [2, 4, 19] </ref>. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> Following are several other smooth plus functions based on probability den sity functions proposed by other authors. Example 2.4.2 Chen-Harker-Kanzow-Smale Smooth Plus Function [51], <ref> [19] </ref> and [2] d (x) = (x 2 + 4) 2 Here D 1 = 1; D 2 = 0; suppfd (x)g = R and ^p (x; fi) = p 2 Example 2.4.3 Pinar-Zenios Smooth Plus Function [42] Let 8 &gt; &gt; : 0 otherwise (2.21) 18 ff log (1 + <p> Hence an exact solution of (4.2) is interior to the feasible region. However the iterates of the smooth method, which are only approximate solutions of (4.2), are not necessarily feasible. For the function ^p defined in Example 2.4.2 <ref> [51, 19, 2] </ref> , the exact solution x of the equation (4.2) satisfies x &gt; 0; F (x) &gt; 0; x i F i (x) = fi 2 ; i = 1; ; n which is precisely the central path of the interior point method for solving NCP. <p> Methods that trace this path but allow iterates to be exterior to the feasible region have been proposed in [51], [2] and <ref> [19] </ref>. In [20], the relation between Smale's method [51] and the central path was pointed out.
Reference: [20] <author> M. Kojima and N. Megiddo. </author> <title> The relation between the path of centers and smale's regularization of the linear programming problem. </title> <journal> Linear Algrbra and Its Applications, </journal> <volume> 152 </volume> <pages> 135-139, </pages> <year> 1991. </year>
Reference-contexts: Methods that trace this path but allow iterates to be exterior to the feasible region have been proposed in [51], [2] and [19]. In <ref> [20] </ref>, the relation between Smale's method [51] and the central path was pointed out.
Reference: [21] <author> M. Kojima, S. Mizuno, and A. Yoshise. </author> <title> A polynomial-time algorithm for a class of linear complementarity problems. </title> <journal> Mathematical Programming, </journal> <volume> 44 </volume> <pages> 1-27, </pages> <year> 1989. </year>
Reference-contexts: In Section 4.3 we show that exact solutions of our smooth nonlinear equation, for various values of the smoothing parameter fi generate an interior path to the feasible region, different from the central path of the interior point method <ref> [21] </ref>. We compare the two paths on a simple example and show that our path gives a smaller error for the same value 5 of the smoothing parameter fi. In Section 4.4, we treat the MCP, the mixed complementarity problem (4.14). <p> In the following theorem, we assume that all the elements of matrix M and vector q are integers and n 2. Let L be the size of LCP (M; q) defined by <ref> [21] </ref> L = b i=1 j=1 i=n X log (jq i j) + log (n 2 )c + 1: Theorem 4.2.5 Suppose that LCP (M; q) is solvable. Let x (ff) be a solution of (4.2) with ff ff = p n2 L . <p> Hence x (ff)(M x (ff) + q) = ff 2 &lt; ff 2 2 2L p n2 L . By the purification procedure described in Appendix B <ref> [21] </ref>, x (ff) can be purified to a solution of LCP (M; q). 4.3 Relation to the Interior Point Method In this section, we consider the NCP (4.1).
Reference: [22] <author> J. Kreimer and R.Y. Rubinstein. </author> <title> Nondifferentiable optimization via smooth approximation: General analytical approach. </title> <journal> Annals of Operations Research, </journal> <volume> 39 </volume> <pages> 97-119, </pages> <year> 1992. </year>
Reference-contexts: With this approximation, many efficient algorithms, such as Newton and quasi-Newton methods, can be easily employed. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [24], multi-commodity flow problems [42], non-smooth programming <ref> [54, 22] </ref>, linear and convex inequalities [4], and linear complementarity problems [2, 4, 19]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> = 1 fi d ( x fi ), where fi is a positive parameter, define the function ^p (x; fi) as follows ^p (x; fi) = 1 (x t) + ^ t (t; fi)dt R x (2.7) This formulation was given in [16, p.12] for a kernel function and in <ref> [22] </ref> for a density function with finite support. We will give our results in terms of a density function with arbitrary support.
Reference: [23] <author> R. De Leone and M.A. Tork Roth. </author> <title> Massively parallel solution of quadratic programs via successive overrelaxation. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5 </volume> <pages> 623-634, </pages> <year> 1993. </year>
Reference-contexts: For sparse problems with density between 0.012 and 0.15 percent, we compared the smooth algorithm with a sparse version of Lemke's method [9], which employs sparse basis updating techniques. We note that the SOR method of De Leone and Tork Roth <ref> [23] </ref> does not apply to this class of nonsymmetric LCP nor do other splitting methods described in [7]. In fact, the SOR method of [23] failed on all test problems. 75 Figures 4.3 and 4.4 show the CPU times for the smooth algorithm and Lemke's method. <p> We note that the SOR method of De Leone and Tork Roth <ref> [23] </ref> does not apply to this class of nonsymmetric LCP nor do other splitting methods described in [7]. In fact, the SOR method of [23] failed on all test problems. 75 Figures 4.3 and 4.4 show the CPU times for the smooth algorithm and Lemke's method. The smooth algorithm is always better than Lemke's method for both dense as well as the sparse problems.
Reference: [24] <author> K. Madsen and H.B. Nielsen. </author> <title> A finite smoothing algorithm for linear l 1 estimation. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 3(2) </volume> <pages> 223-235, </pages> <month> May </month> <year> 1993. </year> <month> 92 </month>
Reference-contexts: The basic idea of this thesis is to use a smooth function approximation to the plus function. With this approximation, many efficient algorithms, such as Newton and quasi-Newton methods, can be easily employed. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems <ref> [24] </ref>, multi-commodity flow problems [42], non-smooth programming [54, 22], linear and convex inequalities [4], and linear complementarity problems [2, 4, 19]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following.
Reference: [25] <author> O.L. Mangasarian. </author> <title> A condition number for linear inequalities and linear programs. </title> <editor> In G. Bamberg and O. Opitz, editors, </editor> <booktitle> Proceedings of 6. Symposium uber Operations Research, </booktitle> <address> Augsburg, </address> <month> 7-9 September </month> <year> 1981, </year> <pages> pages 3-15, </pages> <address> Konigstein, 1981. Verlagsgruppe Athenaum/Hain/Scriptor/Hanstein. </address>
Reference-contexts: The justification for this appellation is that such a solution gives an approximate solution of (3.1) and is obtained by minimizing a smooth function. First we will state an error bound lemma for the linear inequalities (3.1). Lemma 3.2.1 Error bound [17] <ref> [25] </ref> Suppose that the linear inequalities Ax b have a nonempty solution set X.
Reference: [26] <author> O.L. Mangasarian. </author> <title> A condition number for differentiable convex inequalities. </title> <journal> Mathematics of Operations Research, </journal> <volume> 10(2) </volume> <pages> 175-179, </pages> <year> 1985. </year>
Reference-contexts: and x 2 (fi) be solutions of (3.31) with f = f 1 and f = f 2 respectively. (i) Let X be bounded and let g satisfy the Slater constraint qualification: g (^x) &lt; 0 or let g (x) be differentiable and satisfy the Slater and asymptotic constraint qualification <ref> [26] </ref>. <p> both 36 in X, such that kx 1 (fi) x 1 (fi)k 1 mC 1 (D 1 + D 2 )fi (3.32) kx 2 (fi) x 2 (fi)k 2 mC 2 (D 1 + D 2 )fi; (3.33) where C 1 and C 2 are constants dependent on g (x) <ref> [45, 26] </ref>. (ii) If the Slater constraint qualification is satisfied by g (x) 0 and the density function d (x) satisfies (A3), then there exists an fi &gt; 0 such that for any fi fi, x 1 (fi) and x 2 (fi) solve the convex inequalities (3.28) exactly.
Reference: [27] <author> O.L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: The first example, which will be used throughout this paper, is based on the following classical sigmoid function of neural networks <ref> [15, 27, 4] </ref>: s (x; ff) = 1 + e ffx ; ff &gt; 0 (2.12) This function approximates the step function (x) as ff tends to infinity. <p> We have shown that smoothing methods constitute a powerful computational tool for solving broad classes of optimization and related problems. Further study and application of these methods to problems in related areas such as machine learning <ref> [27, 3] </ref> appears to be promising. 88
Reference: [28] <author> O.L. Mangasarian. </author> <title> Error bounds for inconsistent linear inequalities and programs. </title> <journal> Operations Research Letters, </journal> <volume> 15 </volume> <pages> 187-192, </pages> <year> 1994. </year>
Reference-contexts: In fact a multiple of the value of f (x) bounds the distance of x to the set of minimizers of k (Ax b) + k 1 for the case when f = f 1 , see <ref> [28] </ref>.
Reference: [29] <author> O.L. Mangasarian and J.-S. Pang. </author> <title> The extended linear complementarity problem. </title> <type> Technical Report 1188, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <month> November </month> <year> 1993. </year> <note> SIAM Journal on Matrix Analysis and Applications, to appear. </note>
Reference-contexts: For example, most optimality conditions of mathematical programming [34] as well as variational inequalities [6] and extended complementarity problems <ref> [29, 12, 53] </ref> can be so formulated. In this sense, the plus function plays a key role in mathematical programming. But one big disadvantage of the plus function is that it is not smooth because it is not differentiable.
Reference: [30] <author> O.L. Mangasarian and J. Ren. </author> <title> New error bounds for the linear complementarity problem. </title> <type> Technical Report 1156, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <month> June </month> <year> 1993. </year> <note> Mathematical Programming, to appear. 93 </note>
Reference-contexts: The vector q was then defined by: q = w M x. We chose the parameter ff inversely proportional to the 2-norm of the natural residual: k minfx; M x+qgk 2 <ref> [30] </ref>. The algorithm terminates when the infinity-norm of the natural residual is less than 1.0e-6. For dense problems, we compared the smooth algorithm with Lemke's method which was implemented in FORTRAN.
Reference: [31] <author> Jorge J. </author> <title> More. Global methods for nonlinear complementarity problems. </title> <type> Technical Report Preprint MCS-P429-0494, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1994. </year>
Reference-contexts: In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [40], as well as a Newton method with a path following technique [43, 9], and a trust region Newton method for 4 solving a nonlinear least squares reformulation of the NCP <ref> [31] </ref>. With the exception of [31], a feature common to all these methods is that the subproblem at each Newton iteration is still a combinatorial problem. <p> In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [40], as well as a Newton method with a path following technique [43, 9], and a trust region Newton method for 4 solving a nonlinear least squares reformulation of the NCP <ref> [31] </ref>. With the exception of [31], a feature common to all these methods is that the subproblem at each Newton iteration is still a combinatorial problem. In contrast, by using the smooth technique proposed here, we avoid this combinatorial difficulty by approximately reformulating the nonlinear or mixed complementarity problem as a smooth nonlinear equation.
Reference: [32] <author> T. S. Motzkin and I. J. </author> <title> Schoenberg. The relaxation method for linear inequalities. </title> <journal> Canadian Journal of Mathematics, </journal> <volume> 6 </volume> <pages> 393-404, </pages> <year> 1954. </year>
Reference-contexts: The smooth algorithms employ sparsity by evaluating the function and gradients using sparse matrix computation. For linear inequalities, we compared the smooth algorithm with MINOS as well as with the relaxation method of Motzkin and Schoenberg <ref> [32] </ref>. The relaxation method was implemented in C. All the algorithms for linear inequalities were run on a Sun SPARCstation 10. We used the truncated Newton algorithm [35] to solve the smooth unconstrained minimization problem.
Reference: [33] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.0 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: The smooth algorithms were implemented in C. The CPU times for all the algorithms do not include the time to input data. The time of MINOS 5.4 <ref> [33] </ref> is the execution time for subroutine M5SOLV and also does not include the input time. MINOS is a pivot-based solver, which employs sparsity by updating the basis using sparse linear algebra. The smooth algorithms employ sparsity by evaluating the function and gradients using sparse matrix computation. <p> Our smooth algorithm was run using one set of default parameters and so was the PATH solver. The smooth algorithm is written in the C language and implemented by using the GAMS/CPLIB [11]. A MINOS routine <ref> [33] </ref> was used to perform a sparse LU decomposition for solving sparse linear equations. Both algorithms use the same convergence tolerance of * = 1:0e 6. Table 4.5.1 gives a simple description of the test problems [10].
Reference: [34] <author> K.G. Murty. </author> <title> Linear Complementarity, Linear and Nonlinear Programming. </title> <address> Helderman-Verlag, Berlin, </address> <year> 1988. </year>
Reference-contexts: For example, most optimality conditions of mathematical programming <ref> [34] </ref> as well as variational inequalities [6] and extended complementarity problems [29, 12, 53] can be so formulated. In this sense, the plus function plays a key role in mathematical programming. But one big disadvantage of the plus function is that it is not smooth because it is not differentiable.
Reference: [35] <author> S.G. Nash. </author> <title> Newton-type minimization via the Lanczos method. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 21 </volume> <pages> 770-778, </pages> <year> 1984. </year>
Reference-contexts: For linear inequalities, we compared the smooth algorithm with MINOS as well as with the relaxation method of Motzkin and Schoenberg [32]. The relaxation method was implemented in C. All the algorithms for linear inequalities were run on a Sun SPARCstation 10. We used the truncated Newton algorithm <ref> [35] </ref> to solve the smooth unconstrained minimization problem. We started with ff = 1000:0 and increased it by a factor of 2 at each major iteration. The algorithms terminate when the infeasibilities are less than 1.0e-7. All problems 38 were generated randomly.
Reference: [36] <author> J. Nocedal. </author> <title> Theory of algorithms for unconstrained optimization. </title> <journal> Acta Numerica, </journal> <pages> pages 199-242, </pages> <year> 1992. </year>
Reference-contexts: For the case m = n, the smooth algorithm is faster than MINOS and comparable with the relaxation method. For convex inequalities, we use the BFGS algorithm [8] to solve the unconstrained minimization problem for variables up to 150, and the limited memory BFGS algorithm <ref> [36] </ref> for larger problems. Starting with ff = 5, we increased ff by a factor of 1.05 to 1.2 at each minor iteration. The algorithm terminates when the infeasibilities are less than 1.0e-7. We compared the smooth algorithm with MINOS. Both algorithms were run on a DECstation 3100.
Reference: [37] <author> J.M. Ortega. </author> <title> Numerical Analysis, a Second Course. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: fi) + ^p (x F (x); fi) (x F (x)) + k p +k^p (x F (x); fi) (x F (x)) + k p kx ^p (x F (x); fi)k p + fl p maxfD 1 ; D 2 gfi: The above result is also true for any monotone norm <ref> [37] </ref>. We first consider the strongly monotone NCP, that is, there exists a k &gt; 0 such that for any x; y 2 R n (F (x) F (y)) T (x y) kkx yk 2 (4.5) Since the NCP is strongly monotone, it has a unique solution [13]. <p> Since F 2 LC 1 K (R n ), for a compact set S whose interior contains fy k i g and y, we have that R (y) 2 LC 1 K 1 (S) for some K 1 . By the Quadratic Bound Lemma <ref> [37, p.144] </ref>, we have kf (y k i + k i d k i ) f (y k i ) rf (y k i ) T k i d k i k 2 2 2 : Since rR (y) is nonsingular, on the compact S, there exists K (S) and k
Reference: [38] <author> J.-S. Pang. </author> <title> Newton's method for B-differentiable equations. </title> <journal> Mathematics of Operations Research, </journal> <volume> 15 </volume> <pages> 331-341, </pages> <year> 1990. </year>
Reference-contexts: In [13] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in [46, 47, 48]. Since then, several approaches based on B-differentiable equations were investigated in <ref> [14, 38, 39] </ref>. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [40], as well as a Newton method with a path following technique [43, 9], and a trust region Newton method for 4 solving a nonlinear least squares reformulation of the NCP [31].
Reference: [39] <author> J.-S. Pang. </author> <title> A B-differentiable equation based, globally and locally quadrat-ically convergent algorithm for nonlinear programs, complementarity and variational inequality problems. </title> <journal> Mathematical Programming, </journal> <volume> 51 </volume> <pages> 101-131, </pages> <year> 1991. </year> <month> 94 </month>
Reference-contexts: In [13] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in [46, 47, 48]. Since then, several approaches based on B-differentiable equations were investigated in <ref> [14, 38, 39] </ref>. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [40], as well as a Newton method with a path following technique [43, 9], and a trust region Newton method for 4 solving a nonlinear least squares reformulation of the NCP [31].
Reference: [40] <author> J.-S. Pang and S.A. Gabriel. NE/SQP: </author> <title> A robust algorithm for the nonlinear complementarity problem. </title> <journal> Mathematical Programming, </journal> <volume> 60 </volume> <pages> 295-337, </pages> <year> 1993. </year>
Reference-contexts: Generalizations of the Newton method to nonsmooth equations can be found in [46, 47, 48]. Since then, several approaches based on B-differentiable equations were investigated in [14, 38, 39]. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given <ref> [40] </ref>, as well as a Newton method with a path following technique [43, 9], and a trust region Newton method for 4 solving a nonlinear least squares reformulation of the NCP [31]. <p> In Section 4.5, encouraging numerical testing results are given for positive semidef-inite linear complementarity problems up to 10,000 variables and 52 problems from the MCPLIB [10] which includes all the problems attempted in <ref> [14, 40, 9] </ref>. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model [40, 52] which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model <ref> [40, 52] </ref> which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> The details of implementing the smooth algorithm are given in Appendix C. For comparison, we also give the results for the PATH solver [9]. Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [14], <ref> [40] </ref> and [9], 51 problems are from the MCPLIB [10], and one is the generalized von Thunen model from [40] and [52]. Our smooth algorithm was run using one set of default parameters and so was the PATH solver. <p> Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [14], <ref> [40] </ref> and [9], 51 problems are from the MCPLIB [10], and one is the generalized von Thunen model from [40] and [52]. Our smooth algorithm was run using one set of default parameters and so was the PATH solver. The smooth algorithm is written in the C language and implemented by using the GAMS/CPLIB [11]. <p> The results are given in Figures 4.9 to 4.12. It can be seen that with a Newton preprocessor, the solution times are very similar for PATH and SMOOTH for larger problems, whereas PATH is still better for the smaller problems. As mentioned in <ref> [40] </ref>, the generalized von Thunen model is an NCP with 106 variables. This is a very difficult problem that has challenged many of the recently proposed algorithms [40, 52]. <p> As mentioned in [40], the generalized von Thunen model is an NCP with 106 variables. This is a very difficult problem that has challenged many of the recently proposed algorithms <ref> [40, 52] </ref>. Starting with the initial point provided by Jong-shi Pang [41], PATH failed to give a solution while SMOOTH obtained a solution with some small negative components in 27 iterations and 2.92 seconds. <p> An efficient smooth algorithm based on the Newton-Armijo approach with an adjusted smoothing parameter, is also given and its convergence is established. Very encouraging numerical testing results are given for 52 problems from the MCPLIB [10] which includes all the problems attempted in <ref> [14, 40] </ref>. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model [40, 52] which is solved here to an accuracy of 1.0e-7. <p> Very encouraging numerical testing results are given for 52 problems from the MCPLIB [10] which includes all the problems attempted in [14, 40]. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model <ref> [40, 52] </ref> which is solved here to an accuracy of 1.0e-7. For the nonlinear complementarity problem, an exact solution of our parametric smooth nonlinear equations traces a path in the interior of the feasible 87 region when the smoothing parameter is varied.
Reference: [41] <author> J.S. Pang. </author> <title> Private communication, </title> <month> October 3, </month> <year> 1994. </year>
Reference-contexts: As mentioned in [40], the generalized von Thunen model is an NCP with 106 variables. This is a very difficult problem that has challenged many of the recently proposed algorithms [40, 52]. Starting with the initial point provided by Jong-shi Pang <ref> [41] </ref>, PATH failed to give a solution while SMOOTH obtained a solution with some small negative components in 27 iterations and 2.92 seconds.
Reference: [42] <author> M. C. Pinar and S. A. Zenios. </author> <title> On smoothing exact penalty functions for convex constrained optimization. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4 </volume> <pages> 486-511, </pages> <year> 1994. </year>
Reference-contexts: With this approximation, many efficient algorithms, such as Newton and quasi-Newton methods, can be easily employed. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [24], multi-commodity flow problems <ref> [42] </ref>, non-smooth programming [54, 22], linear and convex inequalities [4], and linear complementarity problems [2, 4, 19]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> Example 2.4.2 Chen-Harker-Kanzow-Smale Smooth Plus Function [51], [19] and [2] d (x) = (x 2 + 4) 2 Here D 1 = 1; D 2 = 0; suppfd (x)g = R and ^p (x; fi) = p 2 Example 2.4.3 Pinar-Zenios Smooth Plus Function <ref> [42] </ref> Let 8 &gt; &gt; : 0 otherwise (2.21) 18 ff log (1 + e ffx ) with ff = 5 s (x; ff) 1+e ffx with ff = 5 t (x; ff) (1+e ffx ) 2 with ff = 5 19 Here D 1 = 0; D 2 = 1
Reference: [43] <author> D. Ralph. </author> <title> Global convergence of damped newton's method for nonsmooth equations via the path search. </title> <institution> Mathematics of Operations Research, </institution> <year> 1993. </year>
Reference-contexts: Since then, several approaches based on B-differentiable equations were investigated in [14, 38, 39]. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [40], as well as a Newton method with a path following technique <ref> [43, 9] </ref>, and a trust region Newton method for 4 solving a nonlinear least squares reformulation of the NCP [31]. With the exception of [31], a feature common to all these methods is that the subproblem at each Newton iteration is still a combinatorial problem.
Reference: [44] <author> J. Ren. </author> <title> Computable error bounds in mathematical programming. </title> <type> Technical Report 1173, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: By Lemma 5.2.1 of <ref> [44] </ref>, there exists a x 1 (fi) 2 X 1 such that kx 1 (fi) x 1 (fi)k 1 1 (A; b)(k (Ax 1 (fi) b) + k 1 k (Ax 1 (fi) b) + k 1 ) + (3.19) 30 In the remainder of this section, we consider the function <p> Then w = (x 2 (ff); *(ff); u (ff); v (ff)) is an approximate dual pair. By Lemma 5.3.2 of <ref> [44] </ref>, we have that kx 2 (ff) x 2 (ff)k 2 2 2 (A; b)(r (w) + s (w)) (3.22) where r (w) = k minf 0 B B B B B B B B B @ *(ff) u (ff) v (ff) *(ff) + u (ff) + v (ff) *(ff) C <p> The following error bound for the strongly monotone NCP is given as Theorem 3.2.1 in <ref> [44] </ref>. 44 Lemma 4.1.2 Let the NCP be strongly monotone and let F (x) be Lipschitz continuous. <p> Then there exists an x (fi) which is a solution of LCP (M; q) such that kx (fi) x (fi))k 2 t (M; q) n maxfD 1 ; D 2 gfi; 56 where t (M; q) is a constant, see Theorem 2.2.1 <ref> [44] </ref>. For this remaining section, we consider only the function p (x; ff) defined in Example 2.4.1. The following theorem proves that if ff is sufficiently large, then a solution of (4.2) can be purified to an exact solution of LCP (M; q).
Reference: [45] <author> S.M. Robinson. </author> <title> Application of error bounds for convex programming in a linear space. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 13 </volume> <pages> 271-273, </pages> <year> 1975. </year>
Reference-contexts: both 36 in X, such that kx 1 (fi) x 1 (fi)k 1 mC 1 (D 1 + D 2 )fi (3.32) kx 2 (fi) x 2 (fi)k 2 mC 2 (D 1 + D 2 )fi; (3.33) where C 1 and C 2 are constants dependent on g (x) <ref> [45, 26] </ref>. (ii) If the Slater constraint qualification is satisfied by g (x) 0 and the density function d (x) satisfies (A3), then there exists an fi &gt; 0 such that for any fi fi, x 1 (fi) and x 2 (fi) solve the convex inequalities (3.28) exactly.
Reference: [46] <author> S.M. Robinson. </author> <title> Generalized equations and their solution: Part I: Basic thoery. </title> <journal> Mathematical Programming Study, </journal> <volume> 10 </volume> <pages> 128-140, </pages> <year> 1979. </year>
Reference-contexts: In Chapter 4 we consider mixed complementarity problems. There are many Newton-based algorithms for solving nonlinear complementarity problems, variational inequalities and mixed complementarity problems. In [13] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in <ref> [46, 47, 48] </ref>. Since then, several approaches based on B-differentiable equations were investigated in [14, 38, 39].
Reference: [47] <author> S.M. Robinson. </author> <title> Strongly regular generalized equations. </title> <journal> Mathematics of Operations Research, </journal> <volume> 5 </volume> <pages> 43-62, </pages> <year> 1980. </year>
Reference-contexts: In Chapter 4 we consider mixed complementarity problems. There are many Newton-based algorithms for solving nonlinear complementarity problems, variational inequalities and mixed complementarity problems. In [13] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in <ref> [46, 47, 48] </ref>. Since then, several approaches based on B-differentiable equations were investigated in [14, 38, 39].
Reference: [48] <author> S.M. Robinson. </author> <title> Newton's method for a class of nonsmooth functions. </title> <journal> Set-Valued Analysis, </journal> <volume> 2 </volume> <pages> 291-305, </pages> <year> 1994. </year> <month> 95 </month>
Reference-contexts: In Chapter 4 we consider mixed complementarity problems. There are many Newton-based algorithms for solving nonlinear complementarity problems, variational inequalities and mixed complementarity problems. In [13] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in <ref> [46, 47, 48] </ref>. Since then, several approaches based on B-differentiable equations were investigated in [14, 38, 39].
Reference: [49] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year>
Reference-contexts: Let rc (g) denote the recession cone of a proper convex function g, that is rc (g) = fyj sup (g (x + y) g (x)) 0g; where dom g is the domain of g <ref> [49] </ref>. Now we will state a condition under which (3.31) has a solution. Theorem 3.4.1 Let g : R n ! R m be continuous and convex and let f (x) be defined as in (3.29) or (3.30). The following are equivalent: 1.
Reference: [50] <author> Thomas F. Rutherford. MILES: </author> <title> A mixed inequality and nonlinear equation solver. </title> <type> Working Paper, </type> <institution> Department of Economics, University of Colorado, Boulder, </institution> <year> 1993. </year>
Reference-contexts: Comparison of solution times for sparse Lemke and smooth methods. SOR method fails on all these problems. 77 note that the PATH solver [9] is faster than Josephy's Newton method [18] and Rutherford's GAMS [11] mixed inequality and linear equation solver (MILES) <ref> [50] </ref> which is also Newton-based. Figures 4.6 to 4.8 indicate that our smooth algorithm is faster than PATH solver for the larger problems, whereas PATH solver is faster on smaller problems.
Reference: [51] <author> S. Smale. </author> <title> Algorithms for solving equations. </title> <booktitle> In Proceedings of the International Congress of Mathematicians, </booktitle> <pages> pages 172-195. </pages> <address> Ameri. </address> <publisher> Math. Soc., </publisher> <address> Providence, </address> <year> 1987. </year>
Reference-contexts: Following are several other smooth plus functions based on probability den sity functions proposed by other authors. Example 2.4.2 Chen-Harker-Kanzow-Smale Smooth Plus Function <ref> [51] </ref>, [19] and [2] d (x) = (x 2 + 4) 2 Here D 1 = 1; D 2 = 0; suppfd (x)g = R and ^p (x; fi) = p 2 Example 2.4.3 Pinar-Zenios Smooth Plus Function [42] Let 8 &gt; &gt; : 0 otherwise (2.21) 18 ff log (1 <p> Hence an exact solution of (4.2) is interior to the feasible region. However the iterates of the smooth method, which are only approximate solutions of (4.2), are not necessarily feasible. For the function ^p defined in Example 2.4.2 <ref> [51, 19, 2] </ref> , the exact solution x of the equation (4.2) satisfies x &gt; 0; F (x) &gt; 0; x i F i (x) = fi 2 ; i = 1; ; n which is precisely the central path of the interior point method for solving NCP. <p> Methods that trace this path but allow iterates to be exterior to the feasible region have been proposed in <ref> [51] </ref>, [2] and [19]. In [20], the relation between Smale's method [51] and the central path was pointed out. <p> Methods that trace this path but allow iterates to be exterior to the feasible region have been proposed in <ref> [51] </ref>, [2] and [19]. In [20], the relation between Smale's method [51] and the central path was pointed out.
Reference: [52] <author> B. Xiao. </author> <title> Global newton methods for nonlinear programs and variational inequalities. </title> <type> Technical report, Ph. D. thesis, </type> <institution> Department of Decision Sciences, The Wharton School, University of Pennsylvania, </institution> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model <ref> [40, 52] </ref> which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [14], [40] and [9], 51 problems are from the MCPLIB [10], and one is the generalized von Thunen model from [40] and <ref> [52] </ref>. Our smooth algorithm was run using one set of default parameters and so was the PATH solver. The smooth algorithm is written in the C language and implemented by using the GAMS/CPLIB [11]. <p> As mentioned in [40], the generalized von Thunen model is an NCP with 106 variables. This is a very difficult problem that has challenged many of the recently proposed algorithms <ref> [40, 52] </ref>. Starting with the initial point provided by Jong-shi Pang [41], PATH failed to give a solution while SMOOTH obtained a solution with some small negative components in 27 iterations and 2.92 seconds. <p> We used three starting points. In the first, we set all variables to 1, as suggested by Michael C. Ferris; the second one is a starting point suggested in <ref> [52] </ref>, while the third is the point suggested in [52] and modified by Jong-Shi Pang. SMOOTH, with or without the Newton preprocessor, solved the problem from all the three starting points. Solution times did not change by adding the Newton preprocessor. We report times for SMOOTH with the preprocessor. <p> We used three starting points. In the first, we set all variables to 1, as suggested by Michael C. Ferris; the second one is a starting point suggested in <ref> [52] </ref>, while the third is the point suggested in [52] and modified by Jong-Shi Pang. SMOOTH, with or without the Newton preprocessor, solved the problem from all the three starting points. Solution times did not change by adding the Newton preprocessor. We report times for SMOOTH with the preprocessor. <p> Very encouraging numerical testing results are given for 52 problems from the MCPLIB [10] which includes all the problems attempted in [14, 40]. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model <ref> [40, 52] </ref> which is solved here to an accuracy of 1.0e-7. For the nonlinear complementarity problem, an exact solution of our parametric smooth nonlinear equations traces a path in the interior of the feasible 87 region when the smoothing parameter is varied.
Reference: [53] <author> Y. Ye. </author> <title> A fully polynomial-time approximation algorithm for computing a stationary point of the general linear complementarity problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 18 </volume> <pages> 334-345, </pages> <year> 1993. </year>
Reference-contexts: For example, most optimality conditions of mathematical programming [34] as well as variational inequalities [6] and extended complementarity problems <ref> [29, 12, 53] </ref> can be so formulated. In this sense, the plus function plays a key role in mathematical programming. But one big disadvantage of the plus function is that it is not smooth because it is not differentiable.
Reference: [54] <author> Israel Zang. </author> <title> A smoothing-out technique for min-max optimization. </title> <journal> Mathematical Programming, </journal> <volume> 19 </volume> <pages> 61-77, </pages> <year> 1980. </year> <month> 96 </month>
Reference-contexts: With this approximation, many efficient algorithms, such as Newton and quasi-Newton methods, can be easily employed. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [24], multi-commodity flow problems [42], non-smooth programming <ref> [54, 22] </ref>, linear and convex inequalities [4], and linear complementarity problems [2, 4, 19]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> Example 2.4.4 Zang Smooth Plus Function <ref> [54] </ref> Let 8 &gt; &gt; : 2 x 1 0 otherwise (2.23) Here D 1 = 1 8 ; D 2 = 0; suppfd (x)g = [ 1 2 ; 1 ^p (x; fi) = &gt; &gt; &gt; &gt; &gt; &gt; &lt; 0 if x &lt; fi 1 2 ) 2
References-found: 54

