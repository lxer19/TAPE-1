URL: http://www.ics.uci.edu/~cmerz/mlj.scann.ps
Refering-URL: http://www.ics.uci.edu/~cmerz/resume.html
Root-URL: 
Title: Machine Learning,  Using Correspondence Analysis to Combine Classifiers  
Author: CHRISTOPHER J. MERZ Editor: Philip Chan 
Keyword: Classification, correspondence analysis, multiple models, combining estimates  
Address: Irvine, CA 92697-3425  
Affiliation: Department of Information and Computer Science, University of California,  
Note: c 1997 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Pubnum: 0,  
Email: cmerz@uci.edu  
Date: 1-22 (1997)  Received October 1, 1997  
Abstract: Several effective methods for improving the performance of a single learning algorithm have been developed recently. The general approach is to to create a set of learned models by repeatedly applying the algorithm to different versions of the training data, and then combine the learned models' predictions according to a prescribed voting scheme. Little work has been done in combining the predictions of a collection of models generated by many learning algorithms having different representation and/or search strategies. This paper describes a method which uses the strategies of stacking and correspondence analysis to model the relationship between the learning examples and the way in which they are classified by a collection of learned models. A nearest neighbor method is then applied within the resulting representation to classify previously unseen examples. The new algorithm consistently performs as well or better than other combining techniques on a suite of data sets. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> K. Ali. </author> <title> Learning Probabilistic Relational Concept Descriptions. </title> <type> PhD thesis, </type> <institution> University of California, Irvine, </institution> <year> 1996. </year>
Reference: 2. <author> K. Ali and M. Pazzani. </author> <title> Learning multiple relational rule-based models. </title> <editor> In D. Fisher and H. Lenz, editors, </editor> <title> Learning from Data: </title> <journal> Artificial Intelligence and Statistics, </journal> <volume> Vol. 5. </volume> <publisher> Springer-Verlag, </publisher> <address> Fort Lauderdale, FL, </address> <year> 1995. </year>
Reference: 3. <author> K. Ali and M.J. Pazzani. </author> <title> Error reduction through learning multiple descriptions. </title> <booktitle> Machine Learning, </booktitle> <address> 24:173, </address> <year> 1996. </year>
Reference-contexts: The approaches taken thus far attempt to generate learned models which make uncorrelated errors by using the same algorithm and presenting different samples of the training data [8, 38], or by adjusting the search heuristic slightly <ref> [44, 3] </ref>. No single learning algorithm has the right bias for a broad selection of problems. Therefore, another way to achieve diversity in the errors of the learned models generated is to use completely different learning algorithms which vary in their method of search and/or representation.
Reference: 4. <author> Roberto Battiti and Anna Maria Colla. </author> <title> Democracy in neural nets: Voting schemes for classification. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(4) </volume> <pages> 691-707, </pages> <year> 1994. </year>
Reference: 5. <author> W. Baxt. </author> <title> Improving the accuracy of an artificial neural network using multiple differently trained networks. </title> <journal> Neural Computation, </journal> <volume> 4(5) </volume> <pages> 772-780, </pages> <year> 1992. </year>
Reference: 6. <author> M. Berry, T. Do, G. O'Brien, V. Krishna, and S. Varadhan. </author> <title> Svdpackc (version 1.0) user's guide. </title> <type> Technical Report CS-93-194, </type> <institution> Dept. of Computer Science, University of Tennessee, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Classical methods for determining the singular value decomposition of a matrix can be computation-ally expensive as the number of examples increase. One way around this is to use special computation procedures for dealing with large matrices <ref> [6] </ref>. Another approach is to train the constituent learners on subsets of the training data and then use a smaller set of validation examples for the combining phase of SCANN. Another line of future research will be to apply SCANN to model sets using a variety of model generation techniques.
Reference: 7. <author> L. Breiman. </author> <title> Heuristics of instability in model selection. </title> <type> Technical report, </type> <institution> Department of Statistics, University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: Again, the choice of dimensionality, Kfl, will help to ensure that the dimensions retained contain relevant information about the predictions of the learned models. 3. The nearest neighbor algorithm is known to be "stable." Breiman <ref> [7] </ref> defines the stability of an algorithm as its sensitivity to minor changes in the training data. Stable algorithms are not sensitive to small changes in the training data, instable algorithms are. <p> Two other methods for assigning fixed weights to each model are "bagging" <ref> [7] </ref> and "boosting" [56]. These methods are tightly coupled to the model generation phase rather than being general combining techniques. The goal is to generate a set of models which are likely to make uncorrelated errors (or to have higher variance) thus increasing the potential payoffs in the combining stage.
Reference: 8. <author> L. Breiman. </author> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24(2) </volume> <pages> 123-40, </pages> <year> 1996. </year>
Reference-contexts: 1. Introduction The machine learning and neural networks communities have recently placed a lot of attention on the task of generating and combining multiple learned models <ref> [62, 40, 48, 31, 8, 38, 57] </ref> with the goal of forming an improved estimate. The learned models may be decision/regression trees, rule lists, neural networks, etc. The challenge of this problem is to decide which models to rely upon for prediction and how much weight to give each. <p> Recently, several effective methods have been developed for improving the performance of a single learning algorithm by combining multiple learned models generated using the algorithm. Some examples include bagging <ref> [8] </ref>, boosting [18], and error correcting output codes [29]. The general approach is to use a particular learning algorithm and a model generation technique to create a set of learned models and then combine their predictions according to a prescribed voting scheme. <p> Previous work [48] has indicated that the ideal conditions for combining occur when the errors of the learned models are uncorrelated. The approaches taken thus far attempt to generate learned models which make uncorrelated errors by using the same algorithm and presenting different samples of the training data <ref> [8, 38] </ref>, or by adjusting the search heuristic slightly [44, 3]. No single learning algorithm has the right bias for a broad selection of problems. <p> This approach has frequently been used as a "straw man" combining scheme for comparing to other combining schemes [39], or as a simple combining scheme to evaluate model generation strategies <ref> [8, 33] </ref>. A more elaborate weighting scheme derived by Perrone and Cooper [47] is the general ensemble method (GEM). GEM is different from SCANN in that models are assigned fixed weights, and GEM has difficultly dealing with models that make highly correlated errors.
Reference: 9. <author> P. Chan and S. Stolfo. </author> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proceedings of the 12th International Conference on Machine Learning, </booktitle> <pages> pages 90-98. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Like bagging, boosting places more emphasis on generating a diverse model set. It is possible that a more elaborate non-constant weighting scheme like SCANN could improve upon the combining approach above. Several other resampling techniques have been explored in the literature <ref> [38, 30, 9] </ref>. However, they are not discussed in detail because the emphasis here is on the combining stage. 7.2.
Reference: 10. <author> P. Chan and S. Stolfo. </author> <title> On the accuracy of meta-learning for scalable data mining. </title> <journal> Journal Intelligent Information Systems, </journal> <year> 1996. </year>
Reference: 11. <author> P. Chan and S. Stolfo. </author> <title> Scaling learning by meta-learning over disjoint and partially replicated data. </title> <booktitle> In Proceedings of the Ninth Florida Artificial Intelligence Research Symposium, </booktitle> <pages> pages 151-155, </pages> <year> 1996. </year>
Reference: 12. <author> Peter Clark and Tim Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-283, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: A preliminary experiment was conducted for each data set (using only the training data) to determine the number of hidden units. Twenty percent of the training data was set aside as a validation set for determining when to stop training. The CN2 algorithm <ref> [12] </ref> was used to generate rule lists. Clark and Niblett's version 6.1 was used with the default parameters. Decision trees were generated using C4.5 [50], OC1 [55]. The default parameters were used for both algorithms. A second version of OC1 was run allowing only axis-parallel splits.
Reference: 13. <author> R. Clemen. </author> <title> Combining forecast: A review and annotated bibliography. </title> <journal> International Journal on Forecasting, </journal> <volume> 5 </volume> <pages> 559-583, </pages> <year> 1989. </year>
Reference: 14. <author> S. Cost, S.; Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10(1) </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: Clark and Niblett's version 6.1 was used with the default parameters. Decision trees were generated using C4.5 [50], OC1 [55]. The default parameters were used for both algorithms. A second version of OC1 was run allowing only axis-parallel splits. Two nearest neighbor approaches were used: PEBLS <ref> [14] </ref> and first nearest neighbor (1-NN). For PEBLS, numeric attributes were discretized into ten bins spanning the range of possible values. A naive Bayesian classifier [16] was also used. Numeric attributes were discretized in the same fashion as for PEBLS.
Reference: 15. <author> H. Drucker, R. Schapire, and P. Simard. </author> <title> Improving performance in neural networks using a boosting algorithm. </title> <editor> In Steven J. Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 42-49. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference: 16. <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: A second version of OC1 was run allowing only axis-parallel splits. Two nearest neighbor approaches were used: PEBLS [14] and first nearest neighbor (1-NN). For PEBLS, numeric attributes were discretized into ten bins spanning the range of possible values. A naive Bayesian classifier <ref> [16] </ref> was also used. Numeric attributes were discretized in the same fashion as for PEBLS. Depending on the data set, anywhere from five to eight instantiations of algorithms were applied. OC1 and 1-NN were run only on data sets with all numeric attributes and no missing values (see Table 3).
Reference: 17. <author> B. Efron and R. Tibshirani. </author> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman and Hall, </publisher> <address> London and New York, </address> <year> 1993. </year>
Reference-contexts: The general approach is to use a particular learning algorithm and a model generation technique to create a set of learned models and then combine their predictions according to a prescribed voting scheme. The models are typically generated by varying the training data using resampling techniques such as bootstrapping <ref> [17] </ref> or data partitioning [38].
Reference: 18. <author> Y. Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <journal> Information and Computation, </journal> <volume> 121(2) </volume> <pages> 256-285, </pages> <month> September </month> <year> 1995. </year> <note> Also appeared in COLT90. </note>
Reference-contexts: Recently, several effective methods have been developed for improving the performance of a single learning algorithm by combining multiple learned models generated using the algorithm. Some examples include bagging [8], boosting <ref> [18] </ref>, and error correcting output codes [29]. The general approach is to use a particular learning algorithm and a model generation technique to create a set of learned models and then combine their predictions according to a prescribed voting scheme.
Reference: 19. <author> Y. Freund and R. E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Proceedings of the Second European Conference on Computational Learning Theory, </booktitle> <pages> pages 23-37. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> 20 CHRISTOPHER MERZ </note>
Reference-contexts: Another resampling method has its roots in what is known as "boosting," initially developed by Schapire [56]. Boosting is based on the idea that a set of moderately inaccurate rules-of-thumb (i.e., learned models) can be generated and combined to form a very accurate prediction rule. Freund and Schapire <ref> [20, 19] </ref> have developed several algorithms for boosting. This technique assigns a weight to each example in the training data and adjusts it after learning each model. Initially, the examples are weighted uniformly. <p> The data sets for each learned model are resampled with replacement according to the weight distribution of the examples. 2 A combining strategy for boosting is described in Freund and Schapire's <ref> [19] </ref> AdaBoost.M1 algorithm. The i-th model's vote for a given class is a function of its error, * i , i.e., ff i = log * i In this scheme, learned models which make fewer errors (on the distribution of examples they see) tend to get higher weights.
Reference: 20. <author> Yoav Freund and Robert E. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Proceedings of the 13th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: The probability of an example being drawn is uniform, and the number of examples drawn is the same as the size of the original training set. The underlying theory of this approach indicates that the models should be weighted uniformly. This approach appears to be effective <ref> [20, 49] </ref>, but may be limited by the particular algorithm being "bagged." SCANN is more broadly applicable because it can work with multiple learning algorithms at the same time. Another resampling method has its roots in what is known as "boosting," initially developed by Schapire [56]. <p> Another resampling method has its roots in what is known as "boosting," initially developed by Schapire [56]. Boosting is based on the idea that a set of moderately inaccurate rules-of-thumb (i.e., learned models) can be generated and combined to form a very accurate prediction rule. Freund and Schapire <ref> [20, 19] </ref> have developed several algorithms for boosting. This technique assigns a weight to each example in the training data and adjusts it after learning each model. Initially, the examples are weighted uniformly.
Reference: 21. <author> J. Ghosh, K. Tumer, S. Beck, and L. Deuser. </author> <title> Integration of neural classifiers for passive sonar signals. </title> <editor> In C. T. Leondes, editor, </editor> <booktitle> In Digital Signal Processing Techniques and Applications. </booktitle> <publisher> Academic Press, </publisher> <year> 1995. </year>
Reference: 22. <author> Michael J. Greenacre. </author> <title> Theory and Application of Correspondence Analysis. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1984. </year>
Reference-contexts: Section 2 describes the problem and explains some of the caveats of solving it. The approach taken, called SCANN (Section 3), uses the strategies of stacking [62] and correspondence analysis <ref> [22] </ref> to model the relationship between the learning examples and the way in which they are classified by a collection of learned models. A nearest neighbor method is then applied to the resulting representation to classify previously unseen examples. <p> The SCANN Algorithm A learning algorithm can be broken down into four parts: representation, classification, search, and evaluation. Section 3.1 discusses the first two components by describing how the predictions of the learned models can be mapped to a new representation using Correspondence Analysis <ref> [22] </ref>, and how test examples can be classified using a nearest neighbor algorithm. The search and evaluation aspects of SCANN are covered in Section 3.2. 3.1. Representation and Classification The representation used in SCANN is based upon the variates derived using correspondence analysis [22]. <p> to a new representation using Correspondence Analysis <ref> [22] </ref>, and how test examples can be classified using a nearest neighbor algorithm. The search and evaluation aspects of SCANN are covered in Section 3.2. 3.1. Representation and Classification The representation used in SCANN is based upon the variates derived using correspondence analysis [22]. Sections 3.1.1 and 3.1.2 show how stacking and CA are used to generate the new representation. A nearest neighbor strategy is then used to locate and classify test examples using the new representation (Section 3.1.3). <p> A D r 1=2 (P rc T )D c 1=2 Standardized residuals. 2 A UV T SVD of A. 3 F D r 1=2 U Principal coordinates of rows. G D c 1=2 V Principal coordinates of columns. 3.1.2. Correspondence Analysis Correspondence Analysis (CA) <ref> [22] </ref> is a method for geometrically exploring the relationship between the rows and columns of a matrix whose entries are categorical. The goal here is to explore the relationship between the training examples and how they are classified by the learned models.
Reference: 23. <author> K. Ho, J. J. Hull, and S. N. Srihari. </author> <title> Decision combination in multiple classifier systems. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-16(1):66-75, </volume> <month> January </month> <year> 1994. </year>
Reference-contexts: Plugging these probabilities in as the weights, ff, the class with the highest probability is selected. To date, logistic regression has not been applied to learned models with class label output (versus class probabilities). However, Ho et al <ref> [23] </ref> have successfully applied it to learned models with class rankings with positive results. Due to the large number of free parameters (i.e., N (M + 1)), this approach is only good for a small number of learned models and classes with plenty of training data.
Reference: 24. <author> R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference-contexts: However, they are not discussed in detail because the emphasis here is on the combining stage. 7.2. Non-Constant Weighting Functions The most prevalent method in the literature for dynamically deciding how to weight a collection of classifiers is the "mixture of experts" approach <ref> [24] </ref> which consists of several different "expert" learned models (i.e., multilayer perceptrons) plus a gating network that decides which of the experts should be used for each case. Each expert 18 CHRISTOPHER MERZ reports a class probability distribution for a given example.
Reference: 25. <author> Robert A. Jacobs and Michael I. Jordan. </author> <title> A competitive modular connectionist architecture. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 767-773, </pages> <year> 1991. </year>
Reference: 26. <author> M. I. Jordan and X. Lei. </author> <title> Convergence results for the EM approach to mixtures of experts architectures. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(9) </volume> <pages> 1409-1431, </pages> <year> 1995. </year>
Reference: 27. <author> Michael I. Jordan and Robert A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: The weights of other experts which specialize in quite different cases are unmodified. The experts become "localized" because their weights are decoupled from the weights of other experts, and they will end up specializing on a small portion of the input space. Jordan and Jacobs <ref> [27] </ref> expanded on this approach allowing the learned models/experts to be generalized linear models. The experts are leaves in a tree-structured architecture whose internal nodes are gating functions. These gating functions make "soft" splits allowing data to lie simultaneously in multiple regions.
Reference: 28. <author> Keehoon Kim and Eric B. Bartlett. </author> <title> Error estimation by series association for neural network systems. </title> <journal> Neural Computation, </journal> <volume> 7(4) </volume> <pages> 799-808, </pages> <year> 1995. </year>
Reference: 29. <author> E. B. Kong and T. G. Dietterich. </author> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> In Proceedings of the 12th International Conference on Machine Learning, </booktitle> <pages> pages 313-321. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Recently, several effective methods have been developed for improving the performance of a single learning algorithm by combining multiple learned models generated using the algorithm. Some examples include bagging [8], boosting [18], and error correcting output codes <ref> [29] </ref>. The general approach is to use a particular learning algorithm and a model generation technique to create a set of learned models and then combine their predictions according to a prescribed voting scheme.
Reference: 30. <author> Anders Krogh and Jesper Vedelsby. </author> <title> Neural network ensembles, cross validation, and active learning. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 231-238. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Like bagging, boosting places more emphasis on generating a diverse model set. It is possible that a more elaborate non-constant weighting scheme like SCANN could improve upon the combining approach above. Several other resampling techniques have been explored in the literature <ref> [38, 30, 9] </ref>. However, they are not discussed in detail because the emphasis here is on the combining stage. 7.2.
Reference: 31. <author> M. Leblanc and R. Tibshirani. </author> <title> Combining estimates in regression and classification. </title> <type> Technical report, </type> <institution> Department of Statistics, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: 1. Introduction The machine learning and neural networks communities have recently placed a lot of attention on the task of generating and combining multiple learned models <ref> [62, 40, 48, 31, 8, 38, 57] </ref> with the goal of forming an improved estimate. The learned models may be decision/regression trees, rule lists, neural networks, etc. The challenge of this problem is to decide which models to rely upon for prediction and how much weight to give each.
Reference: 32. <author> W. P. Lincoln and J. Skrzypek. </author> <title> Synergy of clustering multiple backpropagation networks. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 650-657, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: 33. <author> R. Maclin and J. W. Shavlik. </author> <title> Combining the predictions of multiple classifiers: Using competitive learning to initialize neural networks. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: This approach has frequently been used as a "straw man" combining scheme for comparing to other combining schemes [39], or as a simple combining scheme to evaluate model generation strategies <ref> [8, 33] </ref>. A more elaborate weighting scheme derived by Perrone and Cooper [47] is the general ensemble method (GEM). GEM is different from SCANN in that models are assigned fixed weights, and GEM has difficultly dealing with models that make highly correlated errors.
Reference: 34. <author> Morgan Mangeas, Andreas S. Weigend, and Corinne Muller. </author> <title> Forecasting electricity demand using nonlinear mixture of experts. </title> <booktitle> In Proc. WCNN'95, World Congress on Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 48-53. </pages> <address> INNS, </address> <year> 1995. </year>
Reference: 35. <author> G. Mani. </author> <title> Lowering variance of decisions by using artificial network portfolios. </title> <journal> Neural Computation, </journal> <volume> 3(4) </volume> <pages> 484-486, </pages> <year> 1991. </year>
Reference: 36. <author> Dragos D. Margineantu and Thomas G. Dietterich. </author> <title> Pruning adaptive boosting. </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Model ^ f i (1 i 10) was set equal to f for each example with a 10% chance of being wrong, in which case one of the incorrect classes was selected at random. The examples were randomly divided into a training (2/3) and test (1/3) partition. Kappa-Error diagrams <ref> [36] </ref> were used to visualize the differences between the models (see Figure 2).
Reference: 37. <author> Marina Meila and Michael I. Jordan. </author> <title> Learning fine motion by markov mixtures of experts. </title> <editor> In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 1003-1009. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference: 38. <author> Ronny Meir. </author> <title> Bias, variance and the combination of least squares estimators. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 295-302. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: 1. Introduction The machine learning and neural networks communities have recently placed a lot of attention on the task of generating and combining multiple learned models <ref> [62, 40, 48, 31, 8, 38, 57] </ref> with the goal of forming an improved estimate. The learned models may be decision/regression trees, rule lists, neural networks, etc. The challenge of this problem is to decide which models to rely upon for prediction and how much weight to give each. <p> The models are typically generated by varying the training data using resampling techniques such as bootstrapping [17] or data partitioning <ref> [38] </ref>. <p> Previous work [48] has indicated that the ideal conditions for combining occur when the errors of the learned models are uncorrelated. The approaches taken thus far attempt to generate learned models which make uncorrelated errors by using the same algorithm and presenting different samples of the training data <ref> [8, 38] </ref>, or by adjusting the search heuristic slightly [44, 3]. No single learning algorithm has the right bias for a broad selection of problems. <p> Like bagging, boosting places more emphasis on generating a diverse model set. It is possible that a more elaborate non-constant weighting scheme like SCANN could improve upon the combining approach above. Several other resampling techniques have been explored in the literature <ref> [38, 30, 9] </ref>. However, they are not discussed in detail because the emphasis here is on the combining stage. 7.2.
Reference: 39. <author> C. J. Merz. </author> <title> Dynamical selection of learning algorithms. </title> <editor> In D. Fisher and H. Lenz, editors, </editor> <title> Learning from Data: </title> <journal> Artificial Intelligence and Statistics, </journal> <volume> 5. </volume> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: This was referred earlier as the plurality vote (PV) and is also known as the basic ensemble method 16 CHRISTOPHER MERZ (BEM) [47]. This approach has frequently been used as a "straw man" combining scheme for comparing to other combining schemes <ref> [39] </ref>, or as a simple combining scheme to evaluate model generation strategies [8, 33]. A more elaborate weighting scheme derived by Perrone and Cooper [47] is the general ensemble method (GEM).
Reference: 40. <author> C. J. Merz and M. J. Pazzani. </author> <title> Combining neural network regression estimates with regularized linear weights. </title> <editor> In M.C. Mozer, M.I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 9. </volume> <publisher> The MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: 1. Introduction The machine learning and neural networks communities have recently placed a lot of attention on the task of generating and combining multiple learned models <ref> [62, 40, 48, 31, 8, 38, 57] </ref> with the goal of forming an improved estimate. The learned models may be decision/regression trees, rule lists, neural networks, etc. The challenge of this problem is to decide which models to rely upon for prediction and how much weight to give each.
Reference: 41. <author> C.J. Merz and P.M. Murphy. </author> <title> UCI repository of machine learning databases, 1996. COMBINING CLASSIFIERS 21 </title>
Reference-contexts: Empirical Evaluation of SCANN This section contains the results of an experiment comparing SCANN to several other combining strategies on a collection of data sets. 5.1. Classification Data Sets The data sets used were taken from the UCI Machine Learning Database Repository <ref> [41] </ref>, except for the unreleased medical data sets: retardation and dementia. A description of the data sets used is given in Table 3. The data sets with missing values were run only on the constituent learners capable of handling missing values (see Section 5.2). COMBINING CLASSIFIERS 11 5.2.
Reference: 42. <author> S. J. Nowlan. </author> <title> Competing experts: An experimental investigation of associative mixture models. </title> <type> Technical Report CRG-TR-90-5, </type> <institution> Dept. of Computer Science, University of Toronto, Canada, </institution> <month> September </month> <year> 1990. </year>
Reference: 43. <author> Steven J. Nowlan and Geoffrey E. Hinton. </author> <title> Evaluation of adaptive mixtures of competing experts. </title> <editor> In Richard P. Lippmann, John E. Moody, and David S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 774-780. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1991. </year>
Reference: 44. <author> David W. Opitz and Jude W. Shavlik. </author> <title> Generating accurate and diverse members of a neural-network ensemble. </title> <editor> In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 535-541. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Existing approaches typically place more emphasis on the model generation phase rather than the combining phase (e.g., <ref> [44] </ref>). As a result, the combining method is rather limited. This paper describes a novel combining method applicable to model sets that are homogeneous or heterogeneous in their representation and/or search techniques. Section 2 describes the problem and explains some of the caveats of solving it. <p> The approaches taken thus far attempt to generate learned models which make uncorrelated errors by using the same algorithm and presenting different samples of the training data [8, 38], or by adjusting the search heuristic slightly <ref> [44, 3] </ref>. No single learning algorithm has the right bias for a broad selection of problems. Therefore, another way to achieve diversity in the errors of the learned models generated is to use completely different learning algorithms which vary in their method of search and/or representation. <p> More ambitious methods incorporate the estimated accuracy of a learned model in choosing its weight. Opitz and Shavlik <ref> [44] </ref> do so as follows, ff i = P N where E i the estimate of model i's accuracy based on performance on a validation set. Intuitively, model i gets more weight as its estimated performance increases relative to the estimated cumulative performance of the other models.
Reference: 45. <author> Fengchun Peng, R. A. Jacobs, and Martin A. Tanner. </author> <title> Bayesian inference in mixtures-of-experts and hierarchical mixtures-of-experts models with an application to speech recognition. </title> <journal> Journal of the American Statistical Association, </journal> <note> accepted for publication, </note> <year> 1995. </year>
Reference: 46. <author> M. P. Perrone. </author> <title> Improving Regression Estimation: Averaging Methods for Variance Reduction with Extensions to General Convex Measure Optimization. </title> <type> PhD thesis, </type> <institution> Brown University, Institute for Brain and Neural Systems, </institution> <month> May </month> <year> 1993. </year>
Reference: 47. <author> M. P. Perrone and L. N. Cooper. </author> <title> When networks disagree: Ensemble methods for hybrid neural networks. </title> <editor> In R. J. Mammone, editor, </editor> <booktitle> Artificial Neural Networks for Speech and Vision, </booktitle> <pages> pages 126-142, </pages> <address> London, 1993. </address> <publisher> Chapman & Hall. </publisher>
Reference-contexts: This was referred earlier as the plurality vote (PV) and is also known as the basic ensemble method 16 CHRISTOPHER MERZ (BEM) <ref> [47] </ref>. This approach has frequently been used as a "straw man" combining scheme for comparing to other combining schemes [39], or as a simple combining scheme to evaluate model generation strategies [8, 33]. A more elaborate weighting scheme derived by Perrone and Cooper [47] is the general ensemble method (GEM). <p> basic ensemble method 16 CHRISTOPHER MERZ (BEM) <ref> [47] </ref>. This approach has frequently been used as a "straw man" combining scheme for comparing to other combining schemes [39], or as a simple combining scheme to evaluate model generation strategies [8, 33]. A more elaborate weighting scheme derived by Perrone and Cooper [47] is the general ensemble method (GEM). GEM is different from SCANN in that models are assigned fixed weights, and GEM has difficultly dealing with models that make highly correlated errors. One can also combine learned models using the statistical approach of logistic regression.
Reference: 48. <author> Michael P. Perrone. </author> <title> Putting it all together: Methods for combining neural networks. </title> <editor> In Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 1188-1189. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1994. </year>
Reference-contexts: 1. Introduction The machine learning and neural networks communities have recently placed a lot of attention on the task of generating and combining multiple learned models <ref> [62, 40, 48, 31, 8, 38, 57] </ref> with the goal of forming an improved estimate. The learned models may be decision/regression trees, rule lists, neural networks, etc. The challenge of this problem is to decide which models to rely upon for prediction and how much weight to give each. <p> The goal here is to combine the predictions of the members of F so as to find the best approximation of f (x). Previous work <ref> [48] </ref> has indicated that the ideal conditions for combining occur when the errors of the learned models are uncorrelated. <p> If distinct patterns occur in the errors, e.g., ^ f i is particularly good at classifying class c, then the errors are said to be correlated. In the former case, a simple approach like PV is most effective <ref> [48] </ref>. In the latter case, a more complex combining scheme is needed. An effective combining strategy must be able to adjust for both situations. Sections 4.1 and 4.2 evaluate how SCANN handles these two scenarios.
Reference: 49. <author> J. Ross Quinlan. Bagging, </author> <title> boosting, </title> <booktitle> and C4.5. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <year> 1996. </year>
Reference-contexts: The probability of an example being drawn is uniform, and the number of examples drawn is the same as the size of the original training set. The underlying theory of this approach indicates that the models should be weighted uniformly. This approach appears to be effective <ref> [20, 49] </ref>, but may be limited by the particular algorithm being "bagged." SCANN is more broadly applicable because it can work with multiple learning algorithms at the same time. Another resampling method has its roots in what is known as "boosting," initially developed by Schapire [56].
Reference: 50. <author> Ross Quinlan. </author> <title> C4.5 Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Twenty percent of the training data was set aside as a validation set for determining when to stop training. The CN2 algorithm [12] was used to generate rule lists. Clark and Niblett's version 6.1 was used with the default parameters. Decision trees were generated using C4.5 <ref> [50] </ref>, OC1 [55]. The default parameters were used for both algorithms. A second version of OC1 was run allowing only axis-parallel splits. Two nearest neighbor approaches were used: PEBLS [14] and first nearest neighbor (1-NN). For PEBLS, numeric attributes were discretized into ten bins spanning the range of possible values.
Reference: 51. <author> Y. Raviv and N. Intrator. </author> <title> Bootstrapping with noise: An effective regularization technique. </title> <booktitle> Connection Science, </booktitle> ?(?):??-??, <year> 1996. </year>
Reference: 52. <author> Galina Rogova. </author> <title> Combining the results of neural network classifiers. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(5) </volume> <pages> 777-781, </pages> <year> 1994. </year>
Reference: 53. <author> D. L. Ruderman. </author> <title> Natural Ensembles and Sensory Signal Processing. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1994. </year>
Reference: 54. <author> David E. Rumelhart, Geoffrey E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart, J. L. McClelland, and the PDP research group., editors, </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The data sets with missing values were run only on the constituent learners capable of handling missing values (see Section 5.2). COMBINING CLASSIFIERS 11 5.2. Constituent Learners The constituent learning algorithms, A, spanned a variety of search and/or representation techniques. A standard implementation of Error Backpropagation (BP) <ref> [54] </ref> was used to generate neural network models. All networks consisted of an input layer, a single hidden layer, and an output layer. For the input layer, a single input node was assigned to each numeric attribute. Nominal attributes were allocated one input node for each possible attribute value.
Reference: 55. <author> S. Murthy; S. Kasif; S. Salzberg; and R. Beigel. </author> <title> OC1: Randomized induction of oblique decision trees. </title> <booktitle> In Proceedings of AAAI-93. AAAI Pres, </booktitle> <year> 1993. </year>
Reference-contexts: Twenty percent of the training data was set aside as a validation set for determining when to stop training. The CN2 algorithm [12] was used to generate rule lists. Clark and Niblett's version 6.1 was used with the default parameters. Decision trees were generated using C4.5 [50], OC1 <ref> [55] </ref>. The default parameters were used for both algorithms. A second version of OC1 was run allowing only axis-parallel splits. Two nearest neighbor approaches were used: PEBLS [14] and first nearest neighbor (1-NN). For PEBLS, numeric attributes were discretized into ten bins spanning the range of possible values.
Reference: 56. <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Two other methods for assigning fixed weights to each model are "bagging" [7] and "boosting" <ref> [56] </ref>. These methods are tightly coupled to the model generation phase rather than being general combining techniques. The goal is to generate a set of models which are likely to make uncorrelated errors (or to have higher variance) thus increasing the potential payoffs in the combining stage. <p> Another resampling method has its roots in what is known as "boosting," initially developed by Schapire <ref> [56] </ref>. Boosting is based on the idea that a set of moderately inaccurate rules-of-thumb (i.e., learned models) can be generated and combined to form a very accurate prediction rule. Freund and Schapire [20, 19] have developed several algorithms for boosting.
Reference: 57. <author> Volker Tresp and Michiaki Taniguchi. </author> <title> Combining estimators using non-constant weighting functions. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 419-426. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: 1. Introduction The machine learning and neural networks communities have recently placed a lot of attention on the task of generating and combining multiple learned models <ref> [62, 40, 48, 31, 8, 38, 57] </ref> with the goal of forming an improved estimate. The learned models may be decision/regression trees, rule lists, neural networks, etc. The challenge of this problem is to decide which models to rely upon for prediction and how much weight to give each. <p> These gating functions make "soft" splits allowing data to lie simultaneously in multiple regions. The "mixture of experts" approach is different than SCANN in that it is more involved in model generation phase. SCANN deals with the models after they have been learned. Tresp and Taniguchi <ref> [57] </ref> derived a collection of non-constant weighting functions which can be used to combine regressors or classifiers. The proposed methods weigh a learned model according to its reliability in the region of the given example.
Reference: 58. <author> S. R. Waterhouse and A. J. Robinson. </author> <title> Classification using hierarchical mixtures of experts. </title> <booktitle> In Proceedings of the 1994 IEEE Workshop on Neural Networks for Signal Processing IV, </booktitle> <pages> pages 177-186. </pages> <publisher> IEEE Press, </publisher> <year> 1994. </year>
Reference: 59. <author> S. R. Waterhouse and A. J. Robinson. </author> <title> Non-linear prediction of acoustic vectors using hierarchical mixtures of experts. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 835-842. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference: 60. <author> S. R. Waterhouse and A. J. Robinson. </author> <title> Constructive algorithms for hierarchical mixtures of experts. </title> <editor> In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 584-590. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year> <note> 22 CHRISTOPHER MERZ </note>
Reference: 61. <author> Steve Waterhouse, David MacKay, and Tony Robinson. </author> <title> Bayesian methods for mixtures of experts. </title> <editor> In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 351-357. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference: 62. <author> D. H. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259, </pages> <year> 1992. </year>
Reference-contexts: 1. Introduction The machine learning and neural networks communities have recently placed a lot of attention on the task of generating and combining multiple learned models <ref> [62, 40, 48, 31, 8, 38, 57] </ref> with the goal of forming an improved estimate. The learned models may be decision/regression trees, rule lists, neural networks, etc. The challenge of this problem is to decide which models to rely upon for prediction and how much weight to give each. <p> This paper describes a novel combining method applicable to model sets that are homogeneous or heterogeneous in their representation and/or search techniques. Section 2 describes the problem and explains some of the caveats of solving it. The approach taken, called SCANN (Section 3), uses the strategies of stacking <ref> [62] </ref> and correspondence analysis [22] to model the relationship between the learning examples and the way in which they are classified by a collection of learned models. A nearest neighbor method is then applied to the resulting representation to classify previously unseen examples. <p> Together, Stacking, Correspondence Analysis, and Nearest Neighbor make up the core of the SCANN algorithm which is summarized in Section 3.1.4. 3.1.1. Stacking Once a diverse set of models has been generated, the issue of how to combine them arises. Wolpert <ref> [62] </ref> provided a general framework for doing so called stacked generalization or stacking. The goal of stacking is to combine the members of F based on information learned about their particular biases with respect to L 1 .
Reference: 63. <author> D. H. Wolpert. </author> <title> Combining generealizers using partitions of the learning set. </title> <editor> In L. Nadel and D. Stein, editors, </editor> <booktitle> Lectures in Complex Systems. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference: 64. <author> Lei Xu, Michael I. Jordan, and Geoffrey E. Hinton. </author> <title> An alternative model for mixtures of experts. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 633-640. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference: 65. <author> N. Intrator Y. Shimshoni. </author> <title> Automatic discrimination between local earthquakes and quarry blasts by integrating ensembles of neural networks. </title> <booktitle> In Workshop on AI and Seismology, </booktitle> <address> Luxembourg, </address> <month> October </month> <year> 1995. </year>
Reference: 66. <author> Ying Zhao, Richard Schwartz, Jason Sroka, and John Makhoul. </author> <title> Hierarchical mixtures of experts methodology applied to continuous speech recognition. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 859-865. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
References-found: 66

