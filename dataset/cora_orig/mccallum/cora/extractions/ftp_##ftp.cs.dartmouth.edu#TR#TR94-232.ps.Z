URL: ftp://ftp.cs.dartmouth.edu/TR/TR94-232.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR94-232/
Root-URL: http://www.cs.dartmouth.edu
Email: dfk@cs.dartmouth.edu  
Author: David Kotz and Ting Cai 
Note: make this possible.  
Date: October 19, 1994 Revised February 20, 1995  
Address: Hanover, NH 03755  
Affiliation: Department of Computer Science Dartmouth College  
Pubnum: Dartmouth Technical Report PCS-TR94-232  
Abstract: Available at URL ftp://ftp.cs.dartmouth.edu/pub/CS-techreports/TR94-232.ps.Z Exploring the use of I/O Nodes for Computation Abstract As parallel systems move into the production scientific computing world, the emphasis will be on cost-effective solutions that provide high throughput for a mix of applications. Cost-effective solutions demand that a system make effective use of all of its resources. Many MIMD multiprocessors today, however, distinguish between "compute" and "I/O" nodes, the latter having attached disks and being dedicated to running the file-system server. This static division of responsibilities simplifies system management but does not necessarily lead to the best performance in workloads that need a different balance of computation and I/O. Of course, computational processes sharing a node with a file-system service may receive less CPU time, network bandwidth, and memory bandwidth than they would on a computation-only node. In this paper we examine this issue experimentally. We found that high-performance I/O does not necessarily require substantial CPU time, leaving plenty of time for application computation. There were some complex file-system requests, however, which left little CPU time available to the application. (The impact on network and memory bandwidth still needs to be determined.) For applications (or users) that cannot tolerate an occasional interruption, we recommend that they continue to use only compute nodes. For tolerant applications needing more cycles than those provided by the compute nodes, we recommend that they take full advantage of both compute and I/O nodes for computation, and that operating systems should in a MIMD Multiprocessor
Abstract-found: 1
Intro-found: 1
Reference: [ALBL91] <author> Thomas E. Anderson, Henry M. Levy, Brian N. Bershad, and Edward D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <year> 1991. </year>
Reference-contexts: The easy pattern (representing points in the upper right) distributed a one-dimensional matrix of 8-KB records cyclically among the memories (recall that 8 KB was the file-system block size). The hard pattern (representing points 1 This is a moderate context-switch time <ref> [ALBL91] </ref>, even when cache effects are considered. In any case, preliminary experiments showed that our results were not sensitive to this parameter. 7 Table 1: Parameters for simulator.
Reference: [BBS + 94] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Most are based on a fairly traditional Unix-like interface, in which individual processes make a request to the file system for each piece of the file they read 2 or write. Increasingly common, however, are specialized interfaces to support multidimensional matrices <ref> [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92] </ref>, and interfaces that support collective I/O [GGL93, BdC93, BBS + 94, Mas92]. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests. <p> Increasingly common, however, are specialized interfaces to support multidimensional matrices [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92], and interfaces that support collective I/O <ref> [GGL93, BdC93, BBS + 94, Mas92] </ref>. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests.
Reference: [BdC93] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: Most are based on a fairly traditional Unix-like interface, in which individual processes make a request to the file system for each piece of the file they read 2 or write. Increasingly common, however, are specialized interfaces to support multidimensional matrices <ref> [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92] </ref>, and interfaces that support collective I/O [GGL93, BdC93, BBS + 94, Mas92]. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests. <p> Increasingly common, however, are specialized interfaces to support multidimensional matrices [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92], and interfaces that support collective I/O <ref> [GGL93, BdC93, BBS + 94, Mas92] </ref>. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests.
Reference: [BDCW91] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrook, and William E. Weihl. Proteus: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> MIT, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: For each interruption, therefore, we deducted 50 sec. 1 Idle intervals shorter than 50 sec were therefore useless to the computation, and so were not counted. 3.5 Simulator Our traces were collected from the STARFISH parallel file-system simulator [Kot94], which ran on top of the Proteus parallel-architecture simulator <ref> [BDCW91] </ref>, which in turn ran on a DEC-5000 workstation. We configured Proteus using the parameters listed in Table 1. These parameters are not meant to reflect any particular machine, but a generic machine of current technology. 3.6 Results well as showing the I/O bandwidth achieved by the I/O application.
Reference: [CF94] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year>
Reference: [CFPB93] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, and Sandra Johnson Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year>
Reference-contexts: Most are based on a fairly traditional Unix-like interface, in which individual processes make a request to the file system for each piece of the file they read 2 or write. Increasingly common, however, are specialized interfaces to support multidimensional matrices <ref> [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92] </ref>, and interfaces that support collective I/O [GGL93, BdC93, BBS + 94, Mas92]. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests.
Reference: [DdR92] <author> Erik DeBenedictis and Juan Miguel del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC), </booktitle> <pages> pages 0117-0124, </pages> <month> April </month> <year> 1992. </year>
Reference: [Dib90] <author> Peter C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year>
Reference: [DSE88] <author> Peter Dibble, Michael Scott, and Carla Ellis. </author> <title> Bridge: A high-performance file system for parallel processors. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computer Systems, </booktitle> <pages> pages 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference: [FPD93] <author> James C. French, Terrence W. Pratt, and Mriganka Das. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference: [GGL93] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: Most are based on a fairly traditional Unix-like interface, in which individual processes make a request to the file system for each piece of the file they read 2 or write. Increasingly common, however, are specialized interfaces to support multidimensional matrices <ref> [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92] </ref>, and interfaces that support collective I/O [GGL93, BdC93, BBS + 94, Mas92]. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests. <p> Increasingly common, however, are specialized interfaces to support multidimensional matrices [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92], and interfaces that support collective I/O <ref> [GGL93, BdC93, BBS + 94, Mas92] </ref>. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests.
Reference: [GL91] <author> Andrew S. Grimshaw and Edmond C. Loyot, Jr. </author> <title> ELFS: object-oriented extensible file systems. </title> <type> Technical Report TR-91-14, </type> <institution> Univ. of Virginia Computer Science Department, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Most are based on a fairly traditional Unix-like interface, in which individual processes make a request to the file system for each piece of the file they read 2 or write. Increasingly common, however, are specialized interfaces to support multidimensional matrices <ref> [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92] </ref>, and interfaces that support collective I/O [GGL93, BdC93, BBS + 94, Mas92]. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests.
Reference: [HdC95] <author> Michael Harry, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> VIP-FS: A virtual, parallel file system for high performance parallel and distributed computing. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year> <note> To appear. 13 </note>
Reference: [HER + 95] <author> Jay Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <type> Technical Report UIUCDCS-R-95-1903, </type> <institution> University of Illinois at Urbana Champaign, </institution> <month> January </month> <year> 1995. </year>
Reference: [HPF93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <address> 1.0 edition, </address> <month> May 3 </month> <year> 1993. </year>
Reference-contexts: a single compute node (in units of array elements), and the stride (s) is the file distance between the beginning of one chunk and the next chunk destined for the same compute node, where relevant. across the 16 memories of the I/O application according to one of the HPF distributions <ref> [HPF93] </ref>, as shown in Figure 1. Each matrix element was either 8 bytes or 8 Kbytes.
Reference: [Kot94] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests. Disk-directed I/O is a promising new technique that takes advantage of a collective-I/O interface, and leads to much better performance than file systems based on traditional caching strategies <ref> [Kot94] </ref>. With disk-directed I/O, compute nodes make a collective request to the file system, which forwards the request to all I/O nodes. <p> Data transfers between compute nodes and I/O nodes use low-overhead "Memput" and "Memget" messages that move data directly to and from the application buffer. The experiments in <ref> [Kot94] </ref> show that disk-directed I/O obtains nearly the peak disk bandwidth across many data distributions and system configurations. There have been no similar studies of CPU activity on the I/O nodes of multiprocessors. A ten-year old study of diskless workstations [LZCZ86] found that file-server CPU load can be extremely high. <p> Disk-directed I/O is a new technique that takes advantage of a collective-I/O interface, and leads to much better performance than traditional caching <ref> [Kot94] </ref>. As described above, it works by giving control over the order and pace of data transfer to the I/O nodes, who optimize the transfer for maximum disk performance. <p> For each interruption, therefore, we deducted 50 sec. 1 Idle intervals shorter than 50 sec were therefore useless to the computation, and so were not counted. 3.5 Simulator Our traces were collected from the STARFISH parallel file-system simulator <ref> [Kot94] </ref>, which ran on top of the Proteus parallel-architecture simulator [BDCW91], which in turn ran on a DEC-5000 workstation. We configured Proteus using the parameters listed in Table 1. <p> The CPU overhead of traditional caching does not seem to be so bad, but this was again partially due to the poor I/O 2 In <ref> [Kot94] </ref>, the easy patterns are called rc and wc with 8-KB records, and the hard patterns are called rbc and wbc with 8-byte records. 3 We suspect the latter may be improved with a gather/scatter message-passing mechanism. 8 Table 2: Percent of CPU time available to the computational application (100% is
Reference: [Kri94] <author> Orran Krieger. </author> <title> HFS: A flexible file system for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <month> October </month> <year> 1994. </year>
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference: [LZCZ86] <author> Edward D. Lazowska, John Zahorjan, David R. Cheriton, and Willy Zwaenepoel. </author> <title> File access performance of diskless workstations. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(3) </volume> <pages> 238-268, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: The experiments in [Kot94] show that disk-directed I/O obtains nearly the peak disk bandwidth across many data distributions and system configurations. There have been no similar studies of CPU activity on the I/O nodes of multiprocessors. A ten-year old study of diskless workstations <ref> [LZCZ86] </ref> found that file-server CPU load can be extremely high. To be able to provide high performance during periods of intense I/O activity, however, a balanced multiprocessor spreads its disks across many I/O nodes so that the I/O-node CPUs will not be a performance bottleneck.
Reference: [Mas92] <institution> Parallel file I/O routines. MasPar Computer Corporation, </institution> <year> 1992. </year>
Reference-contexts: Most are based on a fairly traditional Unix-like interface, in which individual processes make a request to the file system for each piece of the file they read 2 or write. Increasingly common, however, are specialized interfaces to support multidimensional matrices <ref> [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92] </ref>, and interfaces that support collective I/O [GGL93, BdC93, BBS + 94, Mas92]. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests. <p> Increasingly common, however, are specialized interfaces to support multidimensional matrices [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92], and interfaces that support collective I/O <ref> [GGL93, BdC93, BBS + 94, Mas92] </ref>. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests.
Reference: [MCD + 91] <author> Evangelos Markatos, Mark Crovella, Prakash Das, Cezary Dubnicki, and Tom LeBlanc. </author> <title> The effects of multiprogramming on barrier synchronization. </title> <booktitle> In Proceedings of the 1991 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 662-669, </pages> <year> 1991. </year>
Reference-contexts: These interruptions slowed the computational application in two ways. First, every cycle spent servicing 3 the I/O request was another cycle delay for the interrupted application. Second, delaying one process in the computational application indirectly delayed other processes that waited for the process at a future synchronization point <ref> [MCD + 91] </ref>. In our experiments we used two different kinds of computational applications, 36 different kinds of I/O applications, and two different kinds of file systems, all on a parallel file-system simulator. 3.1 Computational applications Our two computational applications did nothing but computation.
Reference: [MS94] <author> Steven A. Moyer and V. S. Sunderam. </author> <title> PIOUS: a scalable parallel I/O system for distributed computing environments. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 71-78, </pages> <year> 1994. </year>
Reference: [Nit94] <author> Bill Nitzberg. </author> <title> Time between barriers. </title> <type> Personal communication, </type> <year> 1994. </year>
Reference-contexts: Similarly we chose a tight 5 msec interval to represent a challenging case (several NASA benchmarks on the Paragon and an SGI cluster were measured with inter-barrier times of 6, 17, or 64 msec <ref> [Nit94] </ref>). Note that our barrier experiment also represents a computational application that is running on many processors, only some of which are involved in serving I/O, while others are left to run at full speed.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference: [Roy93] <author> Paul J. Roy. </author> <title> Unix file access and caching in a multicomputer environment. </title> <booktitle> In Proceedings of the Usenix Mach III Symposium, </booktitle> <pages> pages 21-37, </pages> <year> 1993. </year>
Reference: [SW94] <author> K. E. Seamons and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array I/O. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Most are based on a fairly traditional Unix-like interface, in which individual processes make a request to the file system for each piece of the file they read 2 or write. Increasingly common, however, are specialized interfaces to support multidimensional matrices <ref> [CFPB93, SW94, GL91, GGL93, BdC93, BBS + 94, Mas92] </ref>, and interfaces that support collective I/O [GGL93, BdC93, BBS + 94, Mas92]. With a collective-I/O interface, all processes make a single joint request to the file system, rather than numerous independent requests.
Reference: [Wal94] <author> D. W. Walker. </author> <title> The design of a standard message passing interface for distributed memory concurrent computers. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 657-673, </pages> <month> April </month> <year> 1994. </year> <month> 14 </month>
Reference-contexts: Execution time (sec) Efficiency No I/O 130.8 Sequential read 219.9 59.5% Random read 181.4 72.1% Sequential write 135.1 96.8% Random write 193.8 67.5% 4 For more information see http://www.cs.dartmouth.edu/research/fleet/. 5 We used MPI <ref> [Wal94] </ref> for the communication support. 11 5 Discussion and conclusions Large multiprocessors with many processors and disks have great potential for fast computations and high I/O throughput. Since they typically cost a lot of money, it is important to utilize the resources efficiently.
References-found: 27

