URL: http://www.cs.virginia.edu/~gjf2a/AAAI_Fall98/papers/Peterson.ps
Refering-URL: http://www.cs.virginia.edu/~gjf2a/AAAI_Fall98/sched.html
Root-URL: http://www.cs.virginia.edu
Email: -gpeterso, cook@cse.uta.edu  
Title: Planning and Learning in an Adversarial Robotic Game  
Author: Gilbert Peterson and Diane Cook 
Address: Box 19015 Arlington, TX 76019-0015  
Affiliation: University of Texas at Arlington  
Abstract: 1 This paper demonstrates the tandem use of a finite automata learning algorithm and a utility planner for an adversarial robotic domain. For many applications, robot agents need to predict the movement of objects in the environment and plan to avoid them. When the robot has no reasoning model of the object, machine learning techniques can be used to generate one. In our project, we learn a DFA model of an adversarial robot and use the automaton to predict the next move of the adversary. The robot agent plans a path to avoid the adversary at the predicted location while fulfilling the goal requirements. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. </author> <year> 1987. </year> <title> Learning Regular Sets from Queries and Counterexamples. </title> <booktitle> Information and Computation 75: </booktitle> <pages> 87-106. </pages>
Reference-contexts: By learning the opponent model, the agent can gain a higher optimal performance. Learning the opponent model is being studied by Carmel and Markovitch (1994). They use a DFA learning algorithm based on Angluins L* <ref> ( Angluin, 1987) </ref>. Another approach for learning the model of the DFA is by Mor, Goldman, and Rosneschein (1996), in that the agent knows how many states exist in the DFA and uses a payoff matrix to decide how states should interconnect.
Reference: <author> Carmel, D. and Markovitch, S. </author> <year> 1994. </year> <title> Unsupervised Learning of Finite Automata: A Practical Approach. </title> <type> Technical Report CIS report 9504. </type>
Reference: <author> Carmel, D. and Markovitch, S. </author> <year> 1994. </year> <title> Learning Models of Opponents Strategies in Game Playing. </title> <type> Technical Report CIS report 9318, </type> <note> and CIS report 9305. </note>
Reference: <author> Dupont, P. </author> <year> 1994. </year> <title> Incremental Regular Inference. </title> <editor> In Miclet, L., and Higuera, C., eds., </editor> <booktitle> Proceedings of the Third ICGI-96, Lecture Notes in Artificial Intelligence 1147: </booktitle> <pages> 222-237. </pages> <address> Montpellier, France: </address> <publisher> Springer. </publisher>
Reference: <author> Freud, Y., Kearns, M., Mansour, Y. Ron, D., Rubinfeld, R., and Shapire, R. E. </author> <year> 1995. </year> <title> Efficient Algorithms for Learning to Play Repeated Games Against Computationally Bounded Adversaries. </title> <booktitle> Proceedings of the 36 th Annual Symposium on Foundations of Computer Science. </booktitle>
Reference-contexts: In learning a model of the opponent, the approach assumes the adversary uses a computationally bounded model to make move decisions instead of playing and searching for the minimax optimal move. The computational bounded model being used to make decisions is most often a finite automata <ref> (Freud, et al., 1995) </ref>. By learning the opponent model, the agent can gain a higher optimal performance. Learning the opponent model is being studied by Carmel and Markovitch (1994). They use a DFA learning algorithm based on Angluins L* ( Angluin, 1987).
Reference: <author> Gold, E. M. </author> <year> 1978. </year> <title> Complexity of automaton identification from given data. </title> <booktitle> Information and Control 37(3): </booktitle> <pages> 302-320. </pages>
Reference: <author> Oncina, J. and Garcia, P. </author> <year> 1992. </year> <title> Inferring Regular Languages in Polynomial Updated Time. Pattern Recognition and Image Analysis: </title> <booktitle> Selected Papers from the IVth Spanish Symposium: </booktitle> <pages> 49-61. </pages>
Reference-contexts: The observation table consists of the closed set of strings S, the tests E that are the suffix-closed set of strings, and a two-dimensional table T that maintains the output of the elements in the sets of S and E. The RPNI algorithm <ref> (Oncina and Garcia, 1992) </ref> and the RPNI2 algorithm (Dupont, 1992) break the example set into sets of positive and negative examples. The positive example set becomes the initial FA tree structure. The negative example set becomes a test set to reduce the positive tree to a minimal DFA.
Reference: <author> Mor, Y., Goldman, C., and Rosenchein, J. S. </author> <year> 1996. </year> <title> Learn Your Opponents Strategy (in Polynomial Time)!. </title>
Reference: <author> Moore, Andrew W., and Atkeson, Christopher G. </author> <year> 1993. </year> <title> Memory-based Reinforcement Learning: Converging with Less Data and Less Real Time. Robot Learning. </title> <type> 79-104. </type>
Reference-contexts: The other possibilities are to implement a similar utility planner with later research into controlling the size and time issues <ref> (Moore and Atkinson, 1993) </ref>. Results The first set of experiments compares the two predictive outputs of the automaton and a case with no predictive information. The tests were run with a finite automaton trained on 25 and 50 trials.
Reference: <author> Olivera, A. L. and Edwards, S. </author> <year> 1996. </year> <title> Limits of Exact Algorithms For Inference of Minimum Size Finite State Machines. </title> <booktitle> Proceedings of the Seventh International Workshop on Algorithmic Learning Theory(ALT96). </booktitle>
Reference: <author> Parekh, R. G. and Honavar, V. G.. </author> <year> 1997. </year> <title> Learning DFA from Simple Examples. </title> <booktitle> Proceedings of the Eighth International Workshop on Algorithmic Learning Theory (ALT'97), Sendai, </booktitle> <address> Japan. Oct 6-8, </address> <note> 97 (To appear) Rivest, </note> <author> R. L. and Schapire, R. E. </author> <year> 1989. </year> <title> Inference of finite automata using homing sequences. </title> <booktitle> Proceedings of the 21 st ACM Symposium on Theory and Computing: </booktitle> <pages> 411-420. </pages>
Reference: <author> Russel, S. J. and Norvig, P. </author> <year> 1995. </year> <title> Artificial Intelligence A Modern Approach. </title> <address> New Jersey: </address> <publisher> Prentice Hall. </publisher>
Reference: <author> Sahota, M. K. </author> <year> 1994. </year> <title> Reactive deliberation: An architecture for real-time intelligent control in dynamic environments. </title> <booktitle> Proceedings of the 12th National Conference on Artificial Intelligence, </booktitle> <pages> 1303. </pages>
Reference: <author> Sutton, R. S. </author> <year> 1995. </year> <title> TD Models: Modeling the World at a Mixture of Time Scales. </title> <booktitle> Proceeding of the 12 th International Conference on Machine Learning: </booktitle> <pages> 531-539. </pages>
Reference: <author> Sutton, R. S. and Barto, A. G. </author> <year> 1998. </year> <title> Reinforcment Learning: an introduction. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Whitehead, S., Karlsson, J., and Tenenberg, J.1993. </author> <title> Learning Multiple Goal Behavior via Task Decomposition and Dynamic Policy Merging. Robot Learning. </title> <type> 45-78. </type>
Reference-contexts: Each policy then creates a path to one of the gold bars or the exit, and using a nearest neighbor or search to choose that policy to follow. By separating the reward, each policy would have fewer peaks and the valleys might be avoided better <ref> (Whitehead, Karlsson, and Tenenberg, 1993) </ref>. The second set of tests compares the normalized output of the learned finite automaton with the actual behavior of the wumpus. For these tests, the learning of the finite automaton occurred over fifty trials. Each individual wumpus strategy was tested fifty times.
References-found: 16

