URL: http://www.ai.univie.ac.at/ilp_kdd/brockhausen.ps.gz
Refering-URL: http://www.ai.univie.ac.at/ilp_kdd/schedule.html
Root-URL: 
Email: fbrockh, morikg@ls8.informatik.uni-dortmund.de  
Title: Direct Access of an ILP Algorithm to a Database Management System  
Author: Peter Brockhausen and Katharina Morik 
Address: D-44221 Dortmund  
Affiliation: Univ. Dortmund, Computer Science Department, LS VIII  
Abstract: When learning from very large databases, the reduction of complexity is of highest importance. Two extremes of making knowledge discovery in databases (KDD) feasible have been put forward. One extreme is to choose a most simple hypothesis language and so to be capable of very fast learning on real-world databases. The opposite extreme is to select a small data set and be capable of learning very expressive (first-order logic) hypotheses. We have combined inductive logic programming (ILP) directly with a relational database. The tool exploits a declarative specification of the syntactic form of hypotheses. We indicate the impact of different mappings from the learner's representation to the one of the database on the complexity of learning. We demonstrate, how background knowledge can be structured and integrated into our learning framework. We conclude with discussing results from first tests.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rakesh Agrawal, Heikki Mannila, Ramakrishnan Srikant, Hannu Toivonen, and A. Inkeri Verkamo. </author> <title> Fast discovery of association rules. </title> <editor> In Usama M. Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, and Ramasamy Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, AAAI Press Series in Computer Science, chapter 12, </booktitle> <pages> pages 277-296. </pages> <publisher> A Bradford Book, The MIT Press, </publisher> <address> Cambridge Massachusetts, London England, </address> <year> 1996. </year>
Reference-contexts: The Apriori and AprioriTid algorithms find association rules that determine subsets of correlated attribute values. Attribute values are represented such that each attribute value becomes a Boolean attribute, indicating whether the value is true or false for a certain entity <ref> [1] </ref>. Rules are formed that state: If a set of attributes is true, then also another set of attributes is true. As all combinations of Boolean attributes have to be considered, the time complexity of the algorithm is exponential in the number of attributes. <p> As all combinations of Boolean attributes have to be considered, the time complexity of the algorithm is exponential in the number of attributes. However, in practice the algorithm takes only 20 seconds for 100 000 tuples 1 . 1 In <ref> [1] </ref> the authors present a series of experiments with the algorithms and give a lower bound for finding an association rule. 1 Other fast learning algorithms exploit given hierarchies and generalize attribute values by climbing the hierarchy [15], merging tuples that become identical, and drop attributes with too many distinct values <p> we enlarge the number of predicates, but we reduce constant learning. 7 Mapping 2: For each relation R with attributes A 1 ; : : : ; A n ; where the attributes A j ; : : : ; A l are the primary key, for each x 2 <ref> [1; : : : ; n] </ref>n [j; : : :; l] a predicate r AX (A j ; : : : ; A l ; A x ) is formed, where AX is the string of the attribute name.
Reference: [2] <author> Siegfried Bell. </author> <title> Deciding distinctness of query results by discovered constraints. </title> <editor> In Mark Wallace, editor, </editor> <booktitle> Proceedings of the Second International Conference on the Practical Application of Constraint Technology, </booktitle> <pages> pages 399-417, </pages> <address> Blackpool, Lancashire, FY2 9UN, UK, </address> <year> 1996. </year> <title> The Practical Application Company Ltd. </title>
Reference-contexts: One fast and simple, but nevertheless effective approach, is to plug a semantic query optimizer into our interaction model, e.g. the one described by Bell <ref> [2] </ref>.
Reference: [3] <author> Siegfried Bell and Peter Brockhausen. </author> <title> Discovery of constraints and data dependencies in databases (extended abstract). </title> <editor> In Nada Lavrac and Stefan Wrobel, editors, </editor> <booktitle> Machine Learning: ECML-95 (Proc. European Conf. on Machine Learning, 1995), Lecture Notes in Artificial Intelligence 914, </booktitle> <pages> pages 267 - 270, </pages> <address> Berlin, Heidelberg, New York, 1995. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Hence, there are only a few options left. One alternative, which we only mention here briefly, consists of extending Rdt/- db into a multistrategy learning system. It then uses two algorithms, which we have developed, that structure attributes and attribute values, respectively: Fdd (Functional Dependency Detection) <ref> [3] </ref> finds a generality order of attributes by detecting functional dependencies in the database. Num Int finds a hierarchy of intervals in numerical (linear) attributes without reference to a classification. This approach is discussed in detail in [18].
Reference: [4] <author> Ronald J. Brachman and Tej Anand. </author> <title> The process of knowledge discovery in databases: A human-centered approach. </title> <editor> In Usama M. Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, and Ramasamy Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, AAAI Press Series in Computer Science, chapter 2, </booktitle> <pages> pages 33-51. </pages> <publisher> A Bradford Book, The MIT Press, </publisher> <address> Cambridge Massachusetts, London England, </address> <year> 1996. </year>
Reference-contexts: It is an issue for discussion, whether the user should select appropriate levels from the learned hierarchies of the "service algorithms", here Stt, or not. We have adopted the position of <ref> [4] </ref> that the user should be involved in the KDD process. On the one hand, the selection of one layer as opposed to trying all combinations of all hierarchies makes learning feasible also on very large databases.
Reference: [5] <author> Y. Cai, N. Cercone, and J. Han. </author> <title> Attribute-oriented induction in relational databases. </title> <editor> In G. Piatetsky-Shapiro and W. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, pages 213 -228. </booktitle> <publisher> AAAI/MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1991. </year>
Reference-contexts: The result are rules that characterize all tuples that have a certain value of attribute A in terms of generalized values of other attributes <ref> [5] </ref>. Similarly, the Kid3 algorithm discovers dependencies between values of two attributes using hierarchies from background knowledge [19].
Reference: [6] <author> Luc De Raedt and Maurice Bruynooghe. </author> <title> A theory of clausal discovery. </title> <editor> In Stephen Muggleton, editor, </editor> <booktitle> Procs. of the 3rd International Workshop on Inductive Logic Programming, number IJS-DP-6707 in J. Stefan Institute Technical Reports, </booktitle> <pages> pages 25-40, </pages> <year> 1993. </year>
Reference-contexts: This learning task is solved by some inductive logic programming (ILP) systems (e.g., Rdt [12], Claudien <ref> [6] </ref>), Linus [13] and Index [7]). The learning task itself is not restricted to learning from databases. However, for the application to databases the selected tuples have to be re-represented as (Prolog) ground facts.
Reference: [7] <author> Peter Flach. </author> <title> Predicate invention in inductive data engineering. </title> <editor> In Pavel Brazdil, editor, </editor> <booktitle> Machine Learning - ECML'93, volume 667 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 83-94, </pages> <year> 1993. </year>
Reference-contexts: This learning task is solved by some inductive logic programming (ILP) systems (e.g., Rdt [12], Claudien [6]), Linus [13] and Index <ref> [7] </ref>). The learning task itself is not restricted to learning from databases. However, for the application to databases the selected tuples have to be re-represented as (Prolog) ground facts. This time consuming work has to be redone, whenever the chosen representation turns out to be inadequate.
Reference: [8] <author> Nicolas Helft. </author> <title> Inductive generalisation: A logical framework. </title> <booktitle> In Procs. of the 2nd European Working Session on Learning, </booktitle> <year> 1987. </year>
Reference-contexts: This option is chosen by most algorithms of inductive logic programming (ILP), which are applied to the KDD problem. The rule learning task has been stated within the ILP paradigm by Nicolas Helft <ref> [8] </ref> using the logic notion of minimal models of a theory M + (T h) M (T h): Given observations E in a representation language L E and background knowledge B in a representation language L B , find the set of hypotheses H in L H , which is a
Reference: [9] <author> M. Holsheimer and A. Siebes. </author> <title> Data mining: The search for knowledge in databases. </title> <type> Technical report, </type> <institution> CWI Amsterdam, </institution> <year> 1993. </year>
Reference-contexts: Marcel Holsheimer and Arno Siebes have shown that the complexity of finding what they call set-descriptions (i.e. disjunctions of constants) is exponential in the number of all attribute values of all relations as well as in the number of tuples of the database <ref> [9] </ref>. Hence, there are only a few options left. One alternative, which we only mention here briefly, consists of extending Rdt/- db into a multistrategy learning system.
Reference: [10] <author> Jorg-Uwe Kietz. </author> <title> Incremental and reversible acquisition of taxonomies. </title> <booktitle> In Proceedings of EKAW-88, chapter 24, </booktitle> <pages> pages 1-11. </pages> <year> 1988. </year> <note> Also as KIT-Report 66, Technical University Berlin. </note>
Reference-contexts: However, background material is often unstructured. In this case, it needs some structuring before it can be used for learning from the database. For this task, we use Stt, a tool for acquiring taxonomies from facts <ref> [10] </ref> 6 . For all predicates it forms sets for each argument position consisting of the constant values which occur at that position. The subset relations between the sets is computed.
Reference: [11] <author> Jorg Uwe Kietz. </author> <title> Induktive Analyse relationaler Daten. </title> <type> PhD thesis, </type> <year> 1996. </year> <note> to appear, in german. </note>
Reference-contexts: First, the learning task is to find all valid and nonredundant rules (rule learning). This learning task is more complex than the concept learning task as was shown by Uwe Kietz <ref> [11] </ref>. To make it even worse, second, the data sets for learning are very large. Two extremes of making KDD feasible have been put forward. One extreme is to choose a most simple hypothesis language and to be capable of very fast learning on real-world databases.
Reference: [12] <author> Jorg-Uwe Kietz and Stefan Wrobel. </author> <title> Controlling the complexity of learning in logic through syntactic and task-oriented models. </title> <editor> In Stephen Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, chapter 16, </booktitle> <pages> pages 335-360. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1992. </year> <month> 15 </month>
Reference-contexts: This learning task is solved by some inductive logic programming (ILP) systems (e.g., Rdt <ref> [12] </ref>, Claudien [6]), Linus [13] and Index [7]). The learning task itself is not restricted to learning from databases. However, for the application to databases the selected tuples have to be re-represented as (Prolog) ground facts. <p> The use of Sql as a query language gives us the opportunity to access almost any commercial database systems, without changing the learning tool. RDT/DB Rdt/db uses the same declarative specification of the hypothesis language as Rdt does in order to restrict the hypothesis space, see for details <ref> [12] </ref>. The specification is given by the user in terms of rule schemata. A rule schema is a rule with predicate variables (instead of predicate symbols). In addition, arguments of the literals can be designated for learning constant values.
Reference: [13] <author> Nada Lavrac and Saso Dzeroski. </author> <title> Inductive Logic Programming Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This learning task is solved by some inductive logic programming (ILP) systems (e.g., Rdt [12], Claudien [6]), Linus <ref> [13] </ref> and Index [7]). The learning task itself is not restricted to learning from databases. However, for the application to databases the selected tuples have to be re-represented as (Prolog) ground facts. This time consuming work has to be redone, whenever the chosen representation turns out to be inadequate.
Reference: [14] <author> R.S. Michalski and R.E. Stepp. </author> <title> Conceptual clustering: Inventing goal-oriented classifications of structured objects. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning AnArtificial Intelligence Approach Vol II, </booktitle> <pages> pages 471-498. </pages> <publisher> Tioga Publishing Company, </publisher> <address> Los Altos, </address> <year> 1986. </year>
Reference-contexts: We have to introduce descriptors into the hypothesis language that are more general than constant values. The classical approaches to overcome this problem are hardly applicable when learning from real databases. First, taxonomies of descriptors have served this purpose, for instance, in conceptual clustering <ref> [14] </ref> or the versions space approach [16]. There, however, the taxonomies were given by the user of the learning system. Here, we do not know of such hierarchies of sets of attribute values but have to find them.
Reference: [15] <author> Ryszard S. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <booktitle> In Machine Learning | An Artificial Intelligence Approach, </booktitle> <pages> pages 83-134. </pages> <publisher> Morgan Kaufman, </publisher> <address> Los Altos, CA, </address> <year> 1983. </year>
Reference-contexts: algorithm takes only 20 seconds for 100 000 tuples 1 . 1 In [1] the authors present a series of experiments with the algorithms and give a lower bound for finding an association rule. 1 Other fast learning algorithms exploit given hierarchies and generalize attribute values by climbing the hierarchy <ref> [15] </ref>, merging tuples that become identical, and drop attributes with too many distinct values that cannot be generalized. The result are rules that characterize all tuples that have a certain value of attribute A in terms of generalized values of other attributes [5].
Reference: [16] <author> Tom M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18(2) </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: The classical approaches to overcome this problem are hardly applicable when learning from real databases. First, taxonomies of descriptors have served this purpose, for instance, in conceptual clustering [14] or the versions space approach <ref> [16] </ref>. There, however, the taxonomies were given by the user of the learning system. Here, we do not know of such hierarchies of sets of attribute values but have to find them.
Reference: [17] <author> K. Morik, S. Wrobel, J.-U. Kietz, and W. Emde. </author> <title> Knowledge Acquisition and Machine Learning Theory, Methods, and Applications. </title> <publisher> Academic Press, </publisher> <address> Lon-don, </address> <year> 1993. </year>
Reference-contexts: We omit the presentation of other features of Stt. Here, we apply it as a fast tool for finding sets of entities that are described by several predicates in background knowledge. 6 A detailed description can be found in <ref> [17] </ref>. 11 We have represented textual background material as one-ary ground facts. The predicates express independent aspects of attribute values of an attribute of the database. These attribute values are at argument position. Different predicates hold for the same attribute value.
Reference: [18] <author> Katharina Morik and Peter Brockhausen. </author> <title> A multistrategy approach to relational knowledge discovery in databases. </title> <editor> In Ryszard S. Michalski and Janusz Wnek, editors, </editor> <booktitle> Proceedings of the Third International Workshop on Multistrategy Learning (MSL-96), </booktitle> <address> Palo Alto, May 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Having a table with three attributes A; B; and C, A is the key, the user might be only interested in the different combinations of 2 The learning algorithm Num Int discovers intervals of numerical values, based on a gap ap proach, for details cf. <ref> [18] </ref>. 8 B and C. Then he can specify a predicate combinations (B; C). Using this predicate in a hypothesis, the Sql generator takes care of three different things. First, a group by B, C statement will be inserted into the query. <p> Num Int finds a hierarchy of intervals in numerical (linear) attributes without reference to a classification. This approach is discussed in detail in <ref> [18] </ref>. In the following we concentrate on another alternative, namely the use of background knowledge.
Reference: [19] <author> G. Piatetsky-Shapiro. </author> <title> Discovery, analysis, and presentation of strong rules. </title> <editor> In G. Piatetsky-Shapiro and W. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, pages 229 -248. </booktitle> <publisher> AAAI/MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1991. </year>
Reference-contexts: The result are rules that characterize all tuples that have a certain value of attribute A in terms of generalized values of other attributes [5]. Similarly, the Kid3 algorithm discovers dependencies between values of two attributes using hierarchies from background knowledge <ref> [19] </ref>. The result is a set of rules of the form A = a 0 ! cond (B) where a 0 is a generalized attribute value (i.e,. it covers a set of attribute values) of attribute A.
Reference: [20] <author> S. Wrobel, D. Wettscherek, E. Sommer, and W. Emde. </author> <title> Extensibility in data mining systems. </title> <editor> In Evangelos Simoudis and Jia Wei Han, editors, </editor> <booktitle> 2nd Int. Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Menlo Park, </address> <month> August </month> <year> 1996. </year> <note> AAAI Press. submitted paper, available at http://nathan.gmd.de/projects/ml/home.html. 16 </note>
Reference-contexts: There are two main differences between our framework for KDD and the Kepler KDD workbench, which offers a variety of ILP algorithms together with a set-oriented layer for data management <ref> [20] </ref>. Kepler allows the user to call various learning tools and use the results of one as input to another one. In our learning environment, the user is involved as an oracle for selecting a part of the background knowledge, see above.
References-found: 20

