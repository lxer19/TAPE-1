URL: http://iacoma.cs.uiuc.edu/iacoma-papers/com.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: dahlgren@ce.chalmers.se  torrella@cs.uiuc.edu  
Title: Cache-Only Memory Architectures  
Author: Fredrik Dahlgren Josep Torrellas 
Date: September 20, 1998  
Web: http://iacoma.cs.uiuc.edu  
Address: S-412 96 Goteborg, Sweden  Urbana, IL 61801, U.S.A.  
Affiliation: Department of Computer Engineering Chalmers University of Technology  Computer Science Department University of Illinois at Urbana-Champaign  
Abstract: Scalable shared-memory multiprocessors are emerging as attractive high-performance platforms. While these machines are relatively easy to program thanks to their shared address space, it can be argued that, if truly high-performance is desired, they still require substantial programmer effort. This is because data must be allocated close to the processors that will use it. To relieve the programmer of this burden, researchers have proposed compiler-, operating system-, and architecture-based approaches. One of the latter approaches is the Cache Only Memory Architecture (COMA) concept. In a COMA, the hardware transparently replicates the program's data and migrates it to the memory modules of the nodes that are currently accessing it. The chances that data will be subsequently accessed locally are then increased. In this paper, we explain the functionality, architecture, performance and complexity of COMA systems. We also present a survey of different COMA designs as well as several alternative architectures that provide somewhat similar benefits. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> P. Stenstrom, </author> <title> "A Survey of Cache Coherence Schemes for Multiprocessors," </title> <journal> IEEE Computer, </journal> <volume> pp.12-24, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: A write to this block by one processor requires a mechanism to prevent other processors from reading the old value from their cached copies. This mechanism is referred to as the cache coherence protocol. A survey of cache-coherence protocols can be found in <ref> [1] </ref>. The most commonly-used technique is for the hardware to transparently invalidate all other cached copies of the same block.
Reference: 2. <author> T. Mowry, </author> <title> "Tolerating Latency through Software-Controlled Data Prefetching," </title> <type> PhD dissertation, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford, Calif., </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Prefetching. This is a technique where data is fetched by the processor in a non-blocking manner, usually into the cache, prior to the processor needing the data. The goal is that when the processor finally needs the data, it finds it in its cache. In software-controlled prefetching <ref> [2] </ref>, the compiler or programmer inserts additional prefetching instructions in the code to perform the prefetching. In hardware-controlled prefetching [3], a hardware mechanism detects memory reference patterns and automatically prefetches future data according to such patterns. Relaxed Memory Consistency.
Reference: 3. <author> F. Dahlgren and P. Stenstrom, </author> <title> "Evaluation of Hardware-Based Stride and Sequential Prefetching in Shared-Memory Multiprocessors," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 7, No. 4, </volume> <pages> pp. 385-398, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: The goal is that when the processor finally needs the data, it finds it in its cache. In software-controlled prefetching [2], the compiler or programmer inserts additional prefetching instructions in the code to perform the prefetching. In hardware-controlled prefetching <ref> [3] </ref>, a hardware mechanism detects memory reference patterns and automatically prefetches future data according to such patterns. Relaxed Memory Consistency.
Reference: 4. <author> S.V. Adve and K. Gharachorloo, </author> <title> "Shared Memory Consistency Models: A Tutorial," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 66-76, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: We can, however, relax the memory consistency model and accept more overlapping of memory accesses except at synchronization points. With this relaxed memory ordering, the latency of memory accesses can be hidden more <ref> [4] </ref>. This ultimately leads to higher performance. Examples of relaxed memory consistency models are release consistency, weak consistency, and RMO (Relaxed Memory Ordering). Multithreading.

References-found: 4

