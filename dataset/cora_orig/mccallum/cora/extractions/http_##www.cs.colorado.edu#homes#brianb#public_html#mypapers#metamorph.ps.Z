URL: http://www.cs.colorado.edu/homes/brianb/public_html/mypapers/metamorph.ps.Z
Refering-URL: http://www.cs.colorado.edu/homes/brianb/public_html/mypapers.html
Root-URL: http://www.cs.colorado.edu
Title: Metamorphosis Networks: An Alternative to Constructive Methods  
Author: Brian V. Bonnlander Michael C. Mozer 
Address: Boulder, CO 80309-0430  
Affiliation: Department of Computer Science Institute of Cognitive Science University of Colorado  
Abstract: Given a set of training examples, determining the appropriate number of free parameters is a challenging problem. Constructive learning algorithms attempt to solve this problem automatically by adding hidden units, and therefore free parameters, during learning. We explore an alternative class of algorithms|called metamorphosis algorithms|in which the number of units is fixed, but the number of free parameters gradually increases during learning. The architecture we investigate is composed of RBF units on a lattice, which imposes flexible constraints on the parameters of the network. Virtues of this approach include variable subset selection, robust parameter selection, multiresolution processing, and interpolation of sparse training data.
Abstract-found: 1
Intro-found: 1
Reference: <author> L. Breiman, J. Friedman, R. A. Olsen & C. J. Stone. </author> <title> (1984) Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: To achieve this, 2 n1 additional underlying psets, which we call the split group, are required (Figure 3). The splitting process implements a recursive partitioning strategy similar to the strategies employed in the CART <ref> (Breiman et al., 1984) </ref> and MARS (Friedman, 1991) statistical learning algorithms. Many possible rules for region splitting exist. In the simulations presented later, we consider every possible region and every possible split of the region into two subregions.
Reference: <author> S. E. Fahlman & C. Lebiere. </author> <title> (1990) The cascade-correlation learning architecture. </title> <editor> In D. S. Touretzky (ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> 524-532. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> J. Friedman. </author> <title> (1991) Multivariate Adaptive Regression Splines. </title> <journal> Annals of Statistics 19 </journal> <pages> 1-141. </pages>
Reference-contexts: To achieve this, 2 n1 additional underlying psets, which we call the split group, are required (Figure 3). The splitting process implements a recursive partitioning strategy similar to the strategies employed in the CART (Breiman et al., 1984) and MARS <ref> (Friedman, 1991) </ref> statistical learning algorithms. Many possible rules for region splitting exist. In the simulations presented later, we consider every possible region and every possible split of the region into two subregions. <p> Consequently, the learning algorithm can orient a small number of regions to fit data that is not aligned with the lattice to begin with. CART and MARS have to create many regions to fit this kind of data <ref> (Friedman, 1991) </ref>. To see if this style of learning algorithm could learn to solve a difficult problem, we trained an MRBF network on the Mackey-Glass chaotic time series.
Reference: <author> S. Geman, E. Bienenstock & R. Doursat. </author> <title> (1992) Neural networks and the bias/variance dilemma. </title> <booktitle> Neural Computation 4(1) </booktitle> <pages> 1-58. </pages>
Reference: <author> E. Hartman & J. D. Keeler. </author> <title> (1991) Predicting the future: advantages of semilocal units. </title> <booktitle> Neural Computation 3(4) </booktitle> <pages> 566-578. </pages>
Reference-contexts: However, the advantages of constructive learning with RBF networks diminish for problems with high-dimensional input spaces <ref> (Hartman & Keeler, 1991) </ref>. For these problems, a large number of RBF units are needed to cover the input space, even when the number of input dimensions relevant for the problem is small.
Reference: <author> T. Kohonen. </author> <title> (1982) Self-organized formation of topologically correct feature maps. </title> <booktitle> Biological Cybernetics 43 </booktitle> <pages> 59-69. </pages>
Reference: <author> J. Moody & C. Darken. </author> <title> (1989) Fast learning in networks of locally-tuned processing units. </title> <booktitle> Neural Computation 1(2) </booktitle> <pages> 281-294. </pages>
Reference-contexts: This idea of multiresolution processing has been studied in the context of computer vision relaxation algorithms and is a property of algorithms proposed by other authors <ref> (e.g. Moody, 1989, Platt, 1991) </ref>. two-dimensional classification task, where the goal is to classify all inputs inside the U-shape as belonging to the same category. An MRBF network is constrained using a one-dimensional lattice. Circles represent RBF widths, and squares represent the height of each RBF.
Reference: <author> J. Moody. </author> <title> (1989) Fast learning in multi-resolution hierarchies. </title> <editor> In D. S. Touretzky (ed.), </editor> <booktitle> Advances in Neural Information Processing 1, </booktitle> <pages> 29-39. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This idea of multiresolution processing has been studied in the context of computer vision relaxation algorithms and is a property of algorithms proposed by other authors <ref> (e.g. Moody, 1989, Platt, 1991) </ref>. two-dimensional classification task, where the goal is to classify all inputs inside the U-shape as belonging to the same category. An MRBF network is constrained using a one-dimensional lattice. Circles represent RBF widths, and squares represent the height of each RBF.
Reference: <author> S. J. Nowlan. </author> <title> (1990) Maximum likelihood competition in RBF networks. </title> <type> Tech. Rep. </type> <institution> CRG-TR-90-2, Department of Computer Science, University of Toronto, Toronto, Canada. </institution>
Reference-contexts: Section 4 describes the advantages of this class of algorithm. The final section suggests directions for further research. 2 RBF NETWORKS RBF networks have been used successfully for learning difficult input-output mappings such as phoneme recognition (Wettschereck & Dietterich, 1991), digit classification <ref> (Nowlan, 1990) </ref>, and time series prediction (Moody & Darken, 1989; Platt, 1991). The basic architecture is shown in Figure 1. The response properties of each RBF unit are determined by a set of parameter values, which we'll call a pset.
Reference: <author> S. J. Nowlan & G. Hinton. </author> <title> (1991) Adaptive soft weight-tying using Gaussian Mixtures. </title>
Reference: <editor> In Moody, Hanson, & Lippmann (eds.), </editor> <booktitle> Advances in Neural Information Processing 4, </booktitle> <pages> 993-1000. </pages> <address> San Mateo, CA: </address> <publisher> Morgan-Kaufmann. </publisher>
Reference: <author> J. Platt. </author> <title> (1991) A resource-allocating network for function interpolation. </title> <booktitle> Neural Computation 3(2) </booktitle> <pages> 213-225. </pages>
Reference-contexts: Consequently, the choice of initial RBF center locations is critical for constructive al for RAN and MRBF represent an average over ten and three simulation runs, respectively. The simulations used 300 training patterns and 500 test patterns as described in <ref> (Platt 1991) </ref>. Simulation parameters for RAN match those reported in (Platt 1991) with * = 0:02. (b) Gaussian noise was added to the function y = sin8x, 0 &lt; x &lt; 1, where the task was to predict y given x. <p> The simulations used 300 training patterns and 500 test patterns as described in <ref> (Platt 1991) </ref>. Simulation parameters for RAN match those reported in (Platt 1991) with * = 0:02. (b) Gaussian noise was added to the function y = sin8x, 0 &lt; x &lt; 1, where the task was to predict y given x. The horizontal axis represents the standard deviation of the Gaussian distribution.
Reference: <author> T. Poggio & F. Girosi. </author> <title> (1990) Regularization algorithms for learning that are equivalent to multilayer networks. </title> <booktitle> Science 247 </booktitle> <pages> 978-982. </pages>
Reference: <author> D. Wettschereck & T. Dietterich. </author> <title> (1991) Improving the performance of radial basis function networks by learning center locations. </title> <editor> In Moody, Hanson, & Lippmann (eds.), </editor> <booktitle> Advances in Neural Info. Processing 4, </booktitle> <pages> 1133-1140. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Section 4 describes the advantages of this class of algorithm. The final section suggests directions for further research. 2 RBF NETWORKS RBF networks have been used successfully for learning difficult input-output mappings such as phoneme recognition <ref> (Wettschereck & Dietterich, 1991) </ref>, digit classification (Nowlan, 1990), and time series prediction (Moody & Darken, 1989; Platt, 1991). The basic architecture is shown in Figure 1. The response properties of each RBF unit are determined by a set of parameter values, which we'll call a pset.
References-found: 14

