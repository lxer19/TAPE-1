URL: ftp://ftp.cs.umd.edu/pub/sel/papers/ICSE93.ps.Z
Refering-URL: http://www.cs.umd.edu/users/bthomas/abs/icse93.html
Root-URL: 
Title: Modeling and Managing Risk Early in Software Development  
Author: Lionel C. Briand, William M. Thomas and Christopher J. Hetmanski 
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland,  
Date: May, 1993  
Note: 15th International Conference on Software Engineering, Baltimore,  
Abstract: In order to improve the quality of the software development process, we need to be able to build empirical multivariate models based on data collectable early in the software process. These models need to be both useful for prediction and easy to interpret, so that remedial actions may be taken in order to control and optimize the development process. We present an automated modeling technique which can be used as an alternative to regression techniques. We show how it can be used to facilitate the identification and aid the interpretation of the significant trends which characterize "high risk" components in several Ada systems. Finally, we evaluate the effectiveness of our technique based on a comparison with logistic regression based models. 
Abstract-found: 1
Intro-found: 1
Reference: [Agr90] <author> A. </author> <title> Agresti, Categorical Data Analysis, </title> <publisher> John Wiley & Sons, </publisher> <year> 1990. </year>
Reference-contexts: In this context, we will examine the use of the following modeling approaches: Logistic regression, which is one of the most common classification techniques <ref> [Agr90] </ref>. This technique has been applied to software engineering modeling [MK92], as well as other experimental fields. Optimized Set Reduction (OSR), which is based on both statistics and machine learning principles [Qui86]. <p> Two modeling approaches were evaluated: logistic regression with a stepwise variable selection, and optimized set reduction. The characteristics of each technique are briefly described in the following paragraphs. 2.2 Logistic Regression The first technique, logistic regression analysis, is based on the following relationship equation <ref> [Agr90] </ref>: log ( 1 - p As an example, we can assume P to be the probability of a component to be in the high risk class, i.e. is likely to have at least one difficult error to correct., and the X i 's to be the design metrics included as
Reference: [AES90] <author> W. Agresti, W. Evanco, and M. Smith, </author> <title> "Early Experiences Building a Software Quality Prediction Model", </title> <booktitle> Proceedings of the Fifteenth Annual Software Engineering Workshop, </booktitle> <month> November, </month> <year> 1990. </year>
Reference-contexts: Recent studies have focused on the identification of problem areas during the design phase, noting that the software architecture is a major factor in the number of errors and rework effort found in later phases <ref> [HK81, ROM87, CA88, AES90] </ref>. If such potential problem areas can be detected during the design, as opposed to during implementation or test, the development organization may have more options available to mitigate the risk. <p> A research project at the MITRE corporation studied a number of these Ada systems, and related characteristics of software architecture to quality factors concerning the presence of defects, the difficulty in correcting defects, and the difficulty in adapting the system to changes <ref> [AES90, EA92] </ref>. Several regression-based models have been developed to predict quality factors from architectural characteristics. These models tackled issues such as identifying error-prone or difficult to modify components. We defined the notion of a high risk component based on a combination of the above two quality factors. <p> Metrics Used in the Study The metrics used in the study were obtained from a project whose goals were to build multivariate models of software quality based on architectural characteristics of Ada designs <ref> [AES90] </ref>. <p> The architectural view of the Ada system can be derived by identifying the major components of the system, and determining the relationships among them. The library unit aggregation (LUA), or the library unit and all its descendant secondary units <ref> [AES90] </ref>, has been noted as providing an interesting view of an Ada system. Example relationships between LUAs are the importer/exporter relationship and the relationship between an instantiation and its generic template. <p> The metric used is a ratio of "cascaded imports" (or declarations directly imported to a higher level unit in the LUA, and whose visibility "cascades" to the descendent units), to direct imports <ref> [AES90] </ref>. When this ratio is equal to one, it indicates that declarations are being imported directly to each compilation unit that uses them. <p> are seen as such an indicator of a high risk component - from counts of object declarations (LUOBJ) to counts of statements (LUADA) and source lines of code (LUSLOC) in the component. (3) Visibility Control: LUVCPUD [70%, 100%] DH = 0.18, PH=0.74,#PV=35 The ratio of cascaded imports to direct imports <ref> [AES90] </ref> provides a crude measure as to whether declarations are being imported directly to where they are needed.
Reference: [AE92] <author> W. Agresti and W. Evanco, </author> <title> "Projecting Software Defects from, Analyzing Ada Designs", </title> <journal> IEEE Trans. Software Eng., </journal> <volume> 18 (11), </volume> <month> November, </month> <year> 1992. </year>
Reference-contexts: Example relationships between LUAs are the importer/exporter relationship and the relationship between an instantiation and its generic template. Characteristics of the LUAs and the relationships between LUAs were used to develop multivariate statistical models of quality factors such as defect density, error correction effort, and change implementation effort <ref> [AE92, AE+92, EA92] </ref>. <p> the context of a large context coupling ratio (LUIEPUD), as is evidenced by the increased probability of being in the high risk class. (5) Context Coupling ratio: LUIEPUD [63%, 100%] DH = 0.14, PH=0.72,#PV=46 LUIEUDEC [42%,100%] OR LUIETOT [42%,100%] DH = 0.07, PH=0.66,#PV=73 One measure of design complexity suggested in <ref> [AE92] </ref> is context coupling, which measures the interconnection of compilation units.
Reference: [AE+92] <author> W. Agresti, W. Evanco, D. Murphy, W. Thomas, and B. Ulery, </author> <title> "Statistical Models for Ada Design Quality", </title> <booktitle> Proceedings of the Fourth Software Quality Workshop, Alexandria Bay, </booktitle> <address> New York, </address> <month> August, </month> <year> 1992. </year>
Reference-contexts: Example relationships between LUAs are the importer/exporter relationship and the relationship between an instantiation and its generic template. Characteristics of the LUAs and the relationships between LUAs were used to develop multivariate statistical models of quality factors such as defect density, error correction effort, and change implementation effort <ref> [AE92, AE+92, EA92] </ref>.
Reference: [Bas85] <author> V. Basili, </author> <title> "Quantitative Evaluation of Software Methodology", </title> <booktitle> Proceedings of the First Pan Pacific Computer Conference, </booktitle> <address> Australia, </address> <month> July </month> <year> 1985. </year>
Reference: [BR88] <author> V. Basili and H. Rombach,"The TAME Project: </author> <title> Towards Improvement-Oriented Software Environments", </title> <journal> IEEE Trans. Software Eng., </journal> <volume> 14 (6), </volume> <month> June, </month> <year> 1988. </year>
Reference-contexts: This is a very important point in the context of the improvement paradigm <ref> [BR88] </ref>. Feedback and therefore process improvement is only possible if the generated quantitative models are interpretable. Taking effective corrective actions is only possible when the impact of controllable factors on the parameters to be controlled (e.g. cost or quality) can be fully understood and quantified.
Reference: [BF+84] <author> L. Breiman, J. Friedman, R. Olshen and C. Stone, </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Monterey, California, </address> <year> 1984. </year>
Reference-contexts: For each component (X) in the sample, a model was developed based on the remaining components (-Sample - X-) and used to "predict" whether the component (X) is likely to be in the high risk class. This model validation method, known as V-fold cross-validation <ref> [BF+84] </ref>, is commonly used when data sets are small. Characteristics of the design were used as explanatory variables in order to build classification models of the Ada components. These design characteristics are identified in section 3 along with our definition of software "component".
Reference: [BP92] <author> L. Briand and A. Porter, </author> <title> "An Alternative Modeling Approach for Predicting Error Profiles in Ada Systems", </title> <booktitle> EUROMETRICS '92, European Conference on Quantitative Evaluation of Software and Systems, </booktitle> <address> Brussels, Belgium, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: In light of this relationship, there have been a number of studies that focus on the development and use of models to identify these "high risk" components <ref> [PA+82, SP88, BP92, MK92] </ref>. There are two different aspects to be treated when one builds a risk model. First, metrics that are good predictors of risk should be defined and validated.
Reference: [BBH92] <author> L. Briand, V. Basili and C. Hetmanski, </author> <title> "Providing an Empirical Basis for Optimizing the Verification and Testing Phases of Software Development", </title> <booktitle> I E E E International Symposium on Software Reliability Engineering, </booktitle> <address> North Carolina, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Optimized Set Reduction (OSR), which is based on both statistics and machine learning principles [Qui86]. This approach has been developed at the University of Maryland and has been applied in several software engineering applications <ref> [BBT92, BBH92] </ref>. 2 Both techniques will be evaluated with respect to their accuracy, constraints of use and ease of interpretation. <p> As shown by [DG84, MK92], this may increase the stability of the stepwise variable selection process and therefore improve the predictive result of the regression model. 2.3 Optimized Set Reduction The second approach, Optimized Set Reduction (OSR), is described in <ref> [BBT92, BBH92] </ref>. It is a modeling approach which is based on both statistical and machine learning principles [BSOF84]. Given an historical data set, OSR automatically generates (through a search algorithm) a collection of logical expressions referred to as patterns which characterize the trends observable in the data set. <p> In order to address these issues, algorithms, supported by tools, have been designed to merge "similar" patterns according to a user defined, statistically based, degree of similarity <ref> [BBH92] </ref>. These algorithms have been used in order to obtain the patterns presented in the next sections. It should be noted that in the design of OSR, we have alleviated some of the problems encountered in the logistic regression model. <p> This also confirms previous studies showing similar results for other kinds of applications <ref> [BBT92, BBH92] </ref>. (3) Patterns appear to be more stable and more interpretable structures than regression equations when the theoretical underlying assumptions are not met. This is a very important point in the context of the improvement paradigm [BR88].
Reference: [BBT92] <author> L. Briand, V. Basili and W. Thomas, </author> <title> "A Pattern Recognition Approach for Software Enginnering Data Analysis", </title> <journal> IEEE Trans. Software Eng., </journal> <volume> 18 (11), </volume> <month> November, </month> <year> 1992. </year>
Reference-contexts: Optimized Set Reduction (OSR), which is based on both statistics and machine learning principles [Qui86]. This approach has been developed at the University of Maryland and has been applied in several software engineering applications <ref> [BBT92, BBH92] </ref>. 2 Both techniques will be evaluated with respect to their accuracy, constraints of use and ease of interpretation. <p> As shown by [DG84, MK92], this may increase the stability of the stepwise variable selection process and therefore improve the predictive result of the regression model. 2.3 Optimized Set Reduction The second approach, Optimized Set Reduction (OSR), is described in <ref> [BBT92, BBH92] </ref>. It is a modeling approach which is based on both statistical and machine learning principles [BSOF84]. Given an historical data set, OSR automatically generates (through a search algorithm) a collection of logical expressions referred to as patterns which characterize the trends observable in the data set. <p> This also confirms previous studies showing similar results for other kinds of applications <ref> [BBT92, BBH92] </ref>. (3) Patterns appear to be more stable and more interpretable structures than regression equations when the theoretical underlying assumptions are not met. This is a very important point in the context of the improvement paradigm [BR88].
Reference: [CA88] <author> D. Card and W. </author> <title> Agresti, "Measuring Software Design Complexity", </title> <journal> Journal of Systems and Software, </journal> <volume> 8 (3), </volume> <month> March, </month> <year> 1988. </year>
Reference-contexts: Recent studies have focused on the identification of problem areas during the design phase, noting that the software architecture is a major factor in the number of errors and rework effort found in later phases <ref> [HK81, ROM87, CA88, AES90] </ref>. If such potential problem areas can be detected during the design, as opposed to during implementation or test, the development organization may have more options available to mitigate the risk.
Reference: [DG84] <author> W. Dillon and M. Goldstein, </author> <title> Multivariate Analysis: Methods and Applications, </title> <publisher> Wiley and Sons, </publisher> <year> 1984. </year>
Reference-contexts: In this way, we insure that undefined instances will not affect the calculation of the likelihood function. (2) Two of the dependent variables are defined on nominal scales. The only solution to deal with such variable is to use "dummy" variables <ref> [DG84] </ref>. In this case, the two nominal variables force us to generate eight dummy variables to be considered during the stepwise variable selection process. For a larger number of symbolic/nominal variables, this issue may become a serious handicap for using the logistic regression approach. <p> Before starting the stepwise logistic regression process, it is possible to reduce the dimensionality of the sample space (i.e. 68 explanatory variables in our case) by performing a principal component analysis <ref> [DG84] </ref> on the available design and size metrics. Thus, we hope to be able to extract a smaller number of variables capturing most of the variation observed in the sample space. <p> Thus, we hope to be able to extract a smaller number of variables capturing most of the variation observed in the sample space. As shown by <ref> [DG84, MK92] </ref>, this may increase the stability of the stepwise variable selection process and therefore improve the predictive result of the regression model. 2.3 Optimized Set Reduction The second approach, Optimized Set Reduction (OSR), is described in [BBT92, BBH92].
Reference: [Dou87] <author> D. </author> <title> Doubleday, "ASAP: An Ada Static Source Code Analyzer Program", </title> <institution> TR-1895, Department of Computer Science, University of Maryland, </institution> <month> August, </month> <year> 1987. </year>
Reference-contexts: The characteristics that were included in this study are described below. 3.2 Description of Design Characteristics The metrics used in this study are derived from the architecture of the system, and were obtained by an automated static analysis of the source code using the ASAP static analysis program <ref> [Dou87] </ref>, UNIX utilities, and the SAS statistical analysis system. They were generated as part of a research project performed at the MITRE Corporation whose goal was to develop models to predict various product qualities throughout the development process [AES90,AE92].
Reference: [EA92] <author> W. Evanco and W. </author> <title> Agresti, "Statistical Representations and Analyses of Software", </title> <booktitle> Proceedings of the 24th Symposium on the Interface of Computing Science and Statistics", </booktitle> <address> College Station, Texas, </address> <month> March, </month> <year> 1992. </year>
Reference-contexts: A research project at the MITRE corporation studied a number of these Ada systems, and related characteristics of software architecture to quality factors concerning the presence of defects, the difficulty in correcting defects, and the difficulty in adapting the system to changes <ref> [AES90, EA92] </ref>. Several regression-based models have been developed to predict quality factors from architectural characteristics. These models tackled issues such as identifying error-prone or difficult to modify components. We defined the notion of a high risk component based on a combination of the above two quality factors. <p> Example relationships between LUAs are the importer/exporter relationship and the relationship between an instantiation and its generic template. Characteristics of the LUAs and the relationships between LUAs were used to develop multivariate statistical models of quality factors such as defect density, error correction effort, and change implementation effort <ref> [AE92, AE+92, EA92] </ref>.
Reference: [GKB86] <author> J. Gannon, E. Katz, and V. Basili, </author> <title> "Metrics for Ada Packages: An Initial Study", </title> <journal> Communications of the ACM, </journal> <volume> 29 (7), </volume> <month> July, </month> <year> 1986. </year>
Reference-contexts: Either extreme may adversely affect quality. Visibility Control: This design characteristic attempts to capture the extent to which declarations are imported to where they are needed, as suggested in <ref> [GKB86] </ref>. The metric used is a ratio of "cascaded imports" (or declarations directly imported to a higher level unit in the LUA, and whose visibility "cascades" to the descendent units), to direct imports [AES90].
Reference: [HK81] <author> S. Henry and D. Kafura, </author> <title> "Software Structure Metrics Based on Information Flow", </title> <journal> IEEE Trans. Software Eng., </journal> <volume> 7 (5), </volume> <month> September, </month> <year> 1981. </year>
Reference-contexts: Recent studies have focused on the identification of problem areas during the design phase, noting that the software architecture is a major factor in the number of errors and rework effort found in later phases <ref> [HK81, ROM87, CA88, AES90] </ref>. If such potential problem areas can be detected during the design, as opposed to during implementation or test, the development organization may have more options available to mitigate the risk.
Reference: [MK92] <author> J. Munson and T. Khoshgoftaar, </author> <title> "The Detection of Fault-Prone Programs", </title> <journal> IEEE Trans. Software Eng., </journal> <volume> 18 (5), </volume> <month> May, </month> <year> 1992. </year>
Reference-contexts: In light of this relationship, there have been a number of studies that focus on the development and use of models to identify these "high risk" components <ref> [PA+82, SP88, BP92, MK92] </ref>. There are two different aspects to be treated when one builds a risk model. First, metrics that are good predictors of risk should be defined and validated. <p> Process improvement in terms of the prediction of defects in the delivered product is one area that has received a significant amount of attention recently <ref> [SP88, MK92] </ref>. Recent studies have focused on the identification of problem areas during the design phase, noting that the software architecture is a major factor in the number of errors and rework effort found in later phases [HK81, ROM87, CA88, AES90]. <p> In this context, we will examine the use of the following modeling approaches: Logistic regression, which is one of the most common classification techniques [Agr90]. This technique has been applied to software engineering modeling <ref> [MK92] </ref>, as well as other experimental fields. Optimized Set Reduction (OSR), which is based on both statistics and machine learning principles [Qui86]. <p> Thus, we hope to be able to extract a smaller number of variables capturing most of the variation observed in the sample space. As shown by <ref> [DG84, MK92] </ref>, this may increase the stability of the stepwise variable selection process and therefore improve the predictive result of the regression model. 2.3 Optimized Set Reduction The second approach, Optimized Set Reduction (OSR), is described in [BBT92, BBH92].
Reference: [PA+82] <author> H. Potier, J. Albin, R. Ferreol and A. Bilodeau, </author> <title> "Experiments with Computer Software Complexity and Reliability", </title> <booktitle> Proceedings of the Sixth International Conference on Software Engineering , September, </booktitle> <year> 1982. </year>
Reference-contexts: In light of this relationship, there have been a number of studies that focus on the development and use of models to identify these "high risk" components <ref> [PA+82, SP88, BP92, MK92] </ref>. There are two different aspects to be treated when one builds a risk model. First, metrics that are good predictors of risk should be defined and validated.
Reference: [Qui86] <author> J. Quinlan, </author> <title> "Induction of Decision Trees", </title> <journal> Machine Learning 1, </journal> <volume> Number 1, </volume> <year> 1986. </year>
Reference-contexts: This technique has been applied to software engineering modeling [MK92], as well as other experimental fields. Optimized Set Reduction (OSR), which is based on both statistics and machine learning principles <ref> [Qui86] </ref>. This approach has been developed at the University of Maryland and has been applied in several software engineering applications [BBT92, BBH92]. 2 Both techniques will be evaluated with respect to their accuracy, constraints of use and ease of interpretation.
Reference: [Rom87] <author> H. D. Rombach, </author> <title> "A Controlled Experiment on the Impact of Software Structure on Maintainability", </title> <journal> I E EE Trans. Software Eng., </journal> <volume> 13 (3), </volume> <month> March, </month> <year> 1987. </year>
Reference-contexts: Recent studies have focused on the identification of problem areas during the design phase, noting that the software architecture is a major factor in the number of errors and rework effort found in later phases <ref> [HK81, ROM87, CA88, AES90] </ref>. If such potential problem areas can be detected during the design, as opposed to during implementation or test, the development organization may have more options available to mitigate the risk.
Reference: [SP88] <author> R. Selby and A. Porter, </author> <title> "Learning from Examples: Generation and Evaluation of Decision Trees for Software Resource Analysis", </title> <journal> IEEE Trans. Software Eng., </journal> <volume> 14 (12), </volume> <month> December, </month> <year> 1988. </year>
Reference-contexts: In light of this relationship, there have been a number of studies that focus on the development and use of models to identify these "high risk" components <ref> [PA+82, SP88, BP92, MK92] </ref>. There are two different aspects to be treated when one builds a risk model. First, metrics that are good predictors of risk should be defined and validated. <p> Process improvement in terms of the prediction of defects in the delivered product is one area that has received a significant amount of attention recently <ref> [SP88, MK92] </ref>. Recent studies have focused on the identification of problem areas during the design phase, noting that the software architecture is a major factor in the number of errors and rework effort found in later phases [HK81, ROM87, CA88, AES90].
References-found: 21

