URL: http://www.ri.cmu.edu/afs/cs/project/cmcl/archive/Nectar-papers/96tpds.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/usr/prs/WWW/papers.html
Root-URL: 
Title: Network-Based Multicomputers: A Practical Supercomputer Architecture  
Author: Peter Steenkiste 
Note: This research was sponsored by the Defense Advanced Research Projects Agency (DOD) under con tract number MDA972-90-C-0035, in part by the National Science Foundation and the Defense Advanced Research Projects Agency under Cooperative Agreement NCR-8919038 with the Corporation for Na tional Research Initiatives. Accepted for publication in IEEE Transactions on Parallel and Distributed Systems.  
Address: Pittsburgh, Pennsylvania 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Multicomputers built around a general network are an attractive architecture for a wide class of applications. The architecture provides many benefits compared with special-purpose approaches, including heterogeneity, reuse of application and system code, and sharing of resources. The architecture also poses new challenges to both computer system implementors and users. First, traditional local-area networks do not have enough bandwidth and create a communication bottleneck, thus seriously limiting the set of applications that can be run effectively. Second, programmers have to deal with large bodies of code distributed over a variety of architectures, and work in an environment where both the network and nodes are shared with other users. Our experience in the Nectar project shows that it is possible to overcome these problems. We show how networks based on high-speed crossbar switches and efficient protocol implementations can support high bandwidth and low latency communication while still enjoying the flexibility of general networks, and we use three applications to demonstrate that network-based multicomputers are a practical architecture. We also show how the network traffic generated by this new class of applications poses severe requirements for networks. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Adams. </author> <title> Cray T3D system architecture overview. </title> <institution> Cray Research Inc., </institution> <year> 1993. </year> <note> Revision 1.C. </note>
Reference-contexts: This creates new challenges, including developing high-performance networks and providing programming tools that help users dealing with a new set of programming problems. 3 2.2.1. Multicomputer interconnect requirements For today's traditional multicomputers, the bandwidth of each interconnection link can be as high as several 100 MByte/second <ref> [31, 1] </ref> and the communication latency between processes on two processors can be as low as several 10s of microseconds [8] or lower [1]. Traditional local-area networks have much lower performance, in part because they have to operate in a much more complex environment. <p> Multicomputer interconnect requirements For today's traditional multicomputers, the bandwidth of each interconnection link can be as high as several 100 MByte/second [31, 1] and the communication latency between processes on two processors can be as low as several 10s of microseconds [8] or lower <ref> [1] </ref>. Traditional local-area networks have much lower performance, in part because they have to operate in a much more complex environment. The increased complexity shows up in two areas: the interconnection network itself, and the interface to the host.
Reference: 2. <author> Marco Annaratone, Emmanuel Arnould, Thomas Gross, H. T. Kung, Monica Lam, Onat Menzilcioglu, and Jon Webb. </author> <title> "The Warp Computer: Architecture, Implementation, and Performance". </title> <journal> IEEE Transactions on Computers 36, </journal> <month> 12 (December </month> <year> 1987), </year> <pages> 1523-1538. </pages>
Reference-contexts: Most hosts attached to Nectar were workstations, although some application experiments [33] were performed involving the Warp system <ref> [2] </ref>, iWarp [9], and the Cray-YMP. The Cray-YMP is accessible via a 26 kilometer single-mode fiber link to the Pittsburgh Supercomputing Center.
Reference: 3. <author> Jose Arabe, Adam Beguelin, Bruce Lowekamp, Eric Seligman, Mike Starkey, and Peter Stephan. Dome: </author> <title> Parallel programming in a heterogeneous multi-user environment. </title> <type> Tech. </type> <institution> Rept. CMU-CS-95-137, Computer Science Department, Carnegie Mellon University, </institution> <month> April, </month> <year> 1995. </year>
Reference-contexts: Ideally, these tasks are performed using programming tools that support distributed computing and that can significantly reduce the complexity of parallel and distributed computing. Examples of tools include parallelizing compilers [61, 15, 23], object 11 based runtime tools <ref> [16, 6, 26, 3, 48] </ref>, and languages and environments for specific application domains [39, 28, 64]. While many distributed applications benefit from the use of tools, today's tools cannot yet, in general, characterize the way irregular programs update complex data structures or the complex control flow of a substantial application.
Reference: 4. <author> Emmanuel Arnould, Francois Bitz, Eric Cooper, H. T. Kung, Robert Sansom and Peter Steenkiste. </author> <title> The Design of Nectar: A Network Backplane for Heterogeneous Multicomputers. </title> <booktitle> Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> ACM/IEEE, Boston, </address> <month> April, </month> <year> 1989, </year> <pages> pp. 205-216. </pages>
Reference-contexts: The HUBs implement a command set that allows source CABs to open and close connections through the switch <ref> [4] </ref>. These commands can be used by the CABs to implement both packet switching and circuit switching. The command set also supports multi-hop routing and multicast. <p> The virtual memory system of the host operating system can limit access of each application process running on the host to the CAB memory area that was allocated to it. For application threads executing on the CAB, the memory protection hardware on the CAB <ref> [4] </ref> can provide the protection. Computing over networks also raises the issue of security: a malicious third party could obtain the results of the computation by intercepting messages, or could even influence the result by replacing messages that are exchanged 10 by cooperating tasks.
Reference: 5. <author> D. H. Bailey, E. Barszcz, R. A. Fatoohi, H. D. Simon, and S. Weeratunga. </author> <title> Performance Results on the Intel Touchstone Gamma Prototype. </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, IEEE, </booktitle> <month> April, </month> <year> 1990, </year> <pages> pp. 1236-1245. </pages>
Reference-contexts: However, since they are system-specific, these interconnects do not have the flexibility of general networks: for example, they cannot be connected to many types of existing hosts. Moreover, traditional multicomputers typically also use a customized runtime system and programming environment <ref> [30, 5] </ref>. This paper argues that it is feasible to build high-performance network-based multicomputers that use general networks instead of system-specific interconnects. Such a multicomputer is able to use existing hosts, including workstations and special-purpose processors, as its processors. <p> In tightly coupled distributed memory systems, the CPU and network are very closely coupled, either at the chip level (e.g. iWarp [10]), or at the board level with access times to the network similar to that of cache accesses (e.g. Touchstone <ref> [5] </ref> or DASH [45]), resulting in low-overhead communication. In network-based multicomputers, however, the network enters the host through an I/O bus, i.e. one step further removed from the CPU than in tightly coupled systems, resulting in slower network access. <p> The latency is under 100 microseconds for a message sent between threads on two CABs, and about 200 microseconds between processes residing in two workstation hosts. These performance results are similar to those of traditional multicomputers using similar technology <ref> [8, 5] </ref>. Figures 2 and 3 show the application-level CAB-CAB and host-host throughput for different packet sizes [20]. In the host-host case, the throughput is limited by the bandwidth of the VME bus.
Reference: 6. <author> H. E. Bal, M. F. Kaashoek, and A. S. Tanenbaum. "Orca: </author> <title> A Language For Parallel Programming of Distributed Systems". </title> <address> TRANSSE 18, </address> <month> 3 (March </month> <year> 1992), </year> <pages> 190-205. </pages>
Reference-contexts: Ideally, these tasks are performed using programming tools that support distributed computing and that can significantly reduce the complexity of parallel and distributed computing. Examples of tools include parallelizing compilers [61, 15, 23], object 11 based runtime tools <ref> [16, 6, 26, 3, 48] </ref>, and languages and environments for specific application domains [39, 28, 64]. While many distributed applications benefit from the use of tools, today's tools cannot yet, in general, characterize the way irregular programs update complex data structures or the complex control flow of a substantial application.
Reference: 7. <author> Tom Blank. </author> <title> The MasPar MP-1 Architecture. </title> <booktitle> IEEE Compcon Spring 1990, IEEE, </booktitle> <address> San Francisco, February/March, </address> <year> 1990, </year> <pages> pp. 20-24. </pages>
Reference-contexts: commercial parallel machines cover a wide spectrum of architectures: shared-memory parallel computers such as the Alliant, Encore, Sequent, and CRAY Y-MP; and distributed-memory computers including MIMD machines such as the Transputer [29], iWarp [9], and the Paragon [31], and SIMD machines such as the Connection Machine [62], DAP, and MasPar <ref> [7] </ref>. Like SIMD machines, distributed memory MIMD computers, or multicom-puters, are inherently scalable. Multicomputers however can handle a larger set of applications than SIMD machines because they allow different programs to run on different processors. Multicomputers with over 1,000 processors have been used successfully in some application areas [27].
Reference: 8. <author> S. Bokhari. </author> <title> Communication Overhead on the Intel iPSC-860 Hypercube. </title> <type> ICASE Interim Report 10, </type> <institution> Institute for Computer Applications in Science and Engineering, NASA Langley Research Center, </institution> <month> May, </month> <year> 1990. </year> <month> 26 </month>
Reference-contexts: Multicomputer interconnect requirements For today's traditional multicomputers, the bandwidth of each interconnection link can be as high as several 100 MByte/second [31, 1] and the communication latency between processes on two processors can be as low as several 10s of microseconds <ref> [8] </ref> or lower [1]. Traditional local-area networks have much lower performance, in part because they have to operate in a much more complex environment. The increased complexity shows up in two areas: the interconnection network itself, and the interface to the host. <p> The latency is under 100 microseconds for a message sent between threads on two CABs, and about 200 microseconds between processes residing in two workstation hosts. These performance results are similar to those of traditional multicomputers using similar technology <ref> [8, 5] </ref>. Figures 2 and 3 show the application-level CAB-CAB and host-host throughput for different packet sizes [20]. In the host-host case, the throughput is limited by the bandwidth of the VME bus.
Reference: 9. <author> Shekhar Borkar, Robert Cohn, George Cox, Sha Gleason, Thomas Gross, H. T. Kung, Monica Lam, Brian Moore, Craig Peterson, John Pieper, Linda Rankin, P. S. Tseng, Jim Sutton, John Urbanski, and Jon Webb. </author> <title> iWarp: An Integrated Solution to High-Speed Parallel Computing. </title> <booktitle> Proceedings of the 1988 International Conference on Supercomputing, IEEE Computer Society and ACM SIGARCH, </booktitle> <address> Orlando, Florida, </address> <month> November, </month> <year> 1988, </year> <pages> pp. 330-339. </pages>
Reference-contexts: 1. Introduction Current commercial parallel machines cover a wide spectrum of architectures: shared-memory parallel computers such as the Alliant, Encore, Sequent, and CRAY Y-MP; and distributed-memory computers including MIMD machines such as the Transputer [29], iWarp <ref> [9] </ref>, and the Paragon [31], and SIMD machines such as the Connection Machine [62], DAP, and MasPar [7]. Like SIMD machines, distributed memory MIMD computers, or multicom-puters, are inherently scalable. <p> Most hosts attached to Nectar were workstations, although some application experiments [33] were performed involving the Warp system [2], iWarp <ref> [9] </ref>, and the Cray-YMP. The Cray-YMP is accessible via a 26 kilometer single-mode fiber link to the Pittsburgh Supercomputing Center.
Reference: 10. <author> Shekhar Borkar, Robert Cohn, George Cox, Thomas Gross, H.T. Kung, Monica Lam, Margie Levine, Brian Moore, Wire Moore, Craig Peterson, Jim Susman, Jim Sutton, John Urbanski, and Jon Webb. </author> <title> Supporting Systolic and Memory Communication in iWarp. </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <address> ACM/IEEE, Seattle, </address> <month> May, </month> <year> 1990, </year> <pages> pp. 70-81. </pages>
Reference-contexts: How tightly a node is coupled to the network has a big impact on the communication efficiency. In tightly coupled distributed memory systems, the CPU and network are very closely coupled, either at the chip level (e.g. iWarp <ref> [10] </ref>), or at the board level with access times to the network similar to that of cache accesses (e.g. Touchstone [5] or DASH [45]), resulting in low-overhead communication.
Reference: 11. <author> Bernd Bruegge, Hiroshi Nishikawa, and Peter Steenkiste. </author> <title> Computing over Networks: An Illustrated Example. </title> <booktitle> Proceedings of the Sixth Distributed Memory Computing Conference, IEEE, </booktitle> <month> April, </month> <year> 1991, </year> <pages> pp. 254-257. </pages>
Reference-contexts: Adding dynamic load balancing Unfortunately, since the distribution of work is done statically, performance degrades quickly if the amount of work assigned to each node does not match the processor speed, or if the load by competing processes on the nodes changes during the execution <ref> [11] </ref>. To avoid this degradation in performance, we added dynamic load balancing to phase two of the application. <p> Showing that the dynamic load balancer is effective in redistributing the load requires information on the dynamic behavior of the application <ref> [11] </ref>. Figure 7 shows the effect of dynamic load balancing using the BEE monitoring tool [47]. The three windows (left to right) show for each node, the simulation time reached, the load (number of particles assigned to the node), and accumulated CPU time.
Reference: 12. <author> Bernd Bruegge. </author> <title> A Portable Platform for Distributed Event Environments. </title> <booktitle> PDDEB91, ACM, </booktitle> <address> Santa Cruz, California, </address> <month> December, </month> <year> 1991, </year> <pages> pp. 184-193. </pages> . 
Reference-contexts: As described earlier in this section, programming tools for both tightly-coupled and 19 loosely-coupled distributed-memory systems are an active area of research. In the context of the Nectar system, we have demonstrated tools in three critical areas: monitoring tools that help the programmer understand the behavior of the application <ref> [47, 12] </ref>, support for data sharing across the network [48, 49], and load balancing tools that help in distributing work to make efficient use of the cycles on the nodes [67, 66, 67, 53, 54, 55]. 5.
Reference: 13. <author> Randy E. Bryant, Derek Beatty, Karl Brace, Kyeongsoon Cho, and Thomas Sheffler. COSMOS: </author> <title> A Compiled Simulator for MOS Circuits. </title> <booktitle> Proceedings of the Design Automation Conference, </booktitle> <address> ACM/IEEE, </address> <month> June, </month> <year> 1987, </year> <pages> pp. 9-16. </pages>
Reference-contexts: The implementations of the system modules on the different architectures can be optimized for the architecture. Hopefully this task-based approach is also a useful step towards a more automated approach. 4.2. COSMOS: a logic simulation application We describe a parallel implementation on Nectar of COSMOS <ref> [13] </ref>, a high-performance logic simulator developed at Carnegie Mellon by Randy Bryant and his associates. The key feature of COSMOS is that it compiles the circuit into executable code.
Reference: 14. <author> Vinton G. Cerf and Robert E. Kahn. </author> <title> "A Protocol for Packet Network Communication". </title> <journal> IEEE Transactions on Communications 22, </journal> <month> 5 (May </month> <year> 1974), </year> <pages> 637-648. </pages>
Reference-contexts: Robustness Although the error rates in modern fiber-optic networks are low, it is still necessary to implement end-to-end checksums and reliable protocols above the physical layer. These protocols are responsible for detecting corrupted and lost packets, for recovering from these errors by retransmission, and for flow and congestion control <ref> [14] </ref>. The network must be able to recover from errors quickly, otherwise the performance of the multicomputer will degrade rapidly. In contrast, traditional multicomputers typically rely on parity or error correcting codes to detect and recover from bit errors, since their interconnects have extremely low error rates.
Reference: 15. <author> M. Chen. </author> <title> A Parallel Language and Its Compilation to Multiprocessors Machines or VLSI. </title> <booktitle> Conference Record of the 13th Annual ACM Conference on Principles of Programming Languages, ACM, </booktitle> <month> January, </month> <year> 1986, </year> <pages> pp. 131-139. </pages>
Reference-contexts: Ideally, these tasks are performed using programming tools that support distributed computing and that can significantly reduce the complexity of parallel and distributed computing. Examples of tools include parallelizing compilers <ref> [61, 15, 23] </ref>, object 11 based runtime tools [16, 6, 26, 3, 48], and languages and environments for specific application domains [39, 28, 64].
Reference: 16. <author> Roger S. Chin and Samuel T. Chanson. </author> <title> "Distributed Object-Based Programming Systems". </title> <journal> ACM Computing Surveys 23, </journal> <month> 1 (March </month> <year> 1991), </year> <pages> 91-124. </pages>
Reference-contexts: Ideally, these tasks are performed using programming tools that support distributed computing and that can significantly reduce the complexity of parallel and distributed computing. Examples of tools include parallelizing compilers [61, 15, 23], object 11 based runtime tools <ref> [16, 6, 26, 3, 48] </ref>, and languages and environments for specific application domains [39, 28, 64]. While many distributed applications benefit from the use of tools, today's tools cannot yet, in general, characterize the way irregular programs update complex data structures or the complex control flow of a substantial application.
Reference: 17. <author> Young Choi. </author> <title> Vertex-based Boundary Representation of Non-Manifold Geometric Models. </title> <type> Ph.D. </type> <institution> Th., Carnegie Mellon University, </institution> <year> 1989. </year>
Reference-contexts: The main effort in Nectar-COSMOS was in implementing the system code, which communicates the signals between the nodes and supports communication between the master and the simulators for initialization and termination detection. 4.3. NOODLES: a geometric modeling application NOODLES is a geometric modeling system <ref> [17] </ref> developed by Fritz Prinz' group at the NSF Engineering Design Research Center at Carnegie Mellon University. NOODLES models objects of different dimensions as a collection of basic components such as vertices, edges, and faces. Applications of NOODLES include knowledge-driven manufacturability analysis and rapid tool manufacturing. <p> When merging two models, each with n components, the number of updates to the 2 database representing the models is O (n) or O (nlog n) <ref> [17] </ref>, while the number of geometric tests is O (n ). Thus, for large models, the speedup resulting from parallelizing geometric tests can be substantial. The goal of Nectar-NOODLES is to allow interactive use of NOODLES, even for large designs.
Reference: 18. <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen. </author> <title> "An Analysis of TCP Processing Overhead". </title> <journal> IEEE Communications Magazine 27, </journal> <month> 6 (June </month> <year> 1989), </year> <pages> 23-29. </pages>
Reference-contexts: If an uncorrectable bit error is detected, the entire system or the application fail. Error recovery in software adds overhead in a number of ways. First, there is protocol processing overhead (e.g. TCP processing); for optimized protocol implementations, this overhead can be as low as 200 instructions per packet <ref> [18, 58] </ref>. A second component is the checksum calculation; since this involves touching the data, the cost is relatively high (Figure 2), but hardware support can eliminate this cost.
Reference: 19. <author> Robert Clay and Peter Steenkiste. </author> <title> Distributing a Chemical Process Optimization Application over a Gigabit Network. </title> <booktitle> Proceedings of Supercomputing '95, </booktitle> <address> ACM/IEEE, </address> <month> December, </month> <year> 1995. </year> <note> Appeared on CD-ROM only. </note>
Reference-contexts: Finally, multicomputers make it possible to use heterogeneous architectures, which can potentially result in substantial speedups by matching application requirements with machine characteristics <ref> [40, 19] </ref>. We are exploring these issues in the context of the Gigabit Nectar [57, 36] and Credit Net [43, 37] projects at Carnegie Mellon University. Acknowledgements A large number of people at Carnegie Mellon University contributed to the Nectar system. H.T.
Reference: 20. <author> Eric Cooper, Peter Steenkiste, Robert Sansom, and Brian Zill. </author> <title> Protocol Implementation on the Nectar Communication Processor. </title> <booktitle> Proceedings of the SIGCOMM '90 Symposium on Communications Architectures and Protocols, ACM, </booktitle> <address> Philadelphia, </address> <month> September, </month> <year> 1990, </year> <pages> pp. 135-143. </pages>
Reference-contexts: Each CAB has 1 megabyte of packet memory, 512 kilobytes of program memory, a 16.5 MHz SPARC processor and hardware devices. The Nectar system software <ref> [20] </ref> consists of a CAB runtime system and libraries on the host that exchange messages with the CAB on behalf of the application. The CAB software performs protocol processing but since the system has support for multiprogramming (threads), it is possible to offload communication-oriented application tasks onto the CAB. <p> These performance results are similar to those of traditional multicomputers using similar technology [8, 5]. Figures 2 and 3 show the application-level CAB-CAB and host-host throughput for different packet sizes <ref> [20] </ref>. In the host-host case, the throughput is limited by the bandwidth of the VME bus. For TCP, when TCP checksums are not computed, the throughput between two CABs is over 80 Mbit/second for 8 kilobyte packets. When checksums are computed, TCP throughput drops to about 30 Mbit/second. <p> In Nectar, communication bypasses the host operating system, and the functions that are normally performed in the OS are now performed by the CAB. This results in more efficient communication, since the CAB is optimized to support communication <ref> [20] </ref>. In this architecture, the CAB is responsible for providing the necessary user protection, with some help from the OS. Although this was not implemented, the CAB provides the necessary support to provide protection.
Reference: 21. <author> M. de Prycker. </author> <title> Asynchronous Transfer Mode. </title> <publisher> Ellis Harwood, </publisher> <year> 1991. </year>
Reference-contexts: At its peak (1991), the Nectar system had 26 hosts, mostly Sun 4 workstations. The advantage of switch-based networks (such as Autonet [52], HPC [24], S3.mp [50], Nectar and ATM networks <ref> [21] </ref>), compared with LANs based on a shared medium (such as Ethernet, token ring, or FDDI), is that for the 5 same link technology, the aggregate bandwidth is much higher, since each attached computer has an exclusive link to the switch and can communicate with other systems attached to the same <p> Note that it would be very beneficial to be able to build networks with more predictable latency. This is however a very difficult task and the very nature of the multicomputing traffic (very bursty) makes this even harder. Some 23 emerging network technologies such as ATM <ref> [21] </ref> have the potential to make this possible, both because the small cell size makes the network more predictable, and because it allows guaranteed services using resource reservation in the network for critical data streams. 5.3.
Reference: 22. <institution> TURBOchannel Overview. Digital Equipment Corporation, </institution> <year> 1990. </year>
Reference-contexts: On more recent workstations, high-speed synchronous busses <ref> [22, 51] </ref> couple the host more tightly to the I/O bus, but they require burst transfers to get good throughput.
Reference: 23. <author> Thomas Gross, Dave O'Hallaron and Jaspal Subhlok. </author> <title> "Task Parallelism in a High Performance Fortran Framework". </title> <booktitle> IEEE Parallel & Distributed Technology 2, 3 (Fall 1994), </booktitle> <pages> 16-26. </pages>
Reference-contexts: Ideally, these tasks are performed using programming tools that support distributed computing and that can significantly reduce the complexity of parallel and distributed computing. Examples of tools include parallelizing compilers <ref> [61, 15, 23] </ref>, object 11 based runtime tools [16, 6, 26, 3, 48], and languages and environments for specific application domains [39, 28, 64].
Reference: 24. <author> R. D. Gaglianello, B. S. Robinson, T. L. Lindstrom, E. E. Sampieri. HPC/VORX: </author> <title> A Local Area Multicomputer System. </title> <booktitle> Proceedings of the Ninth International Conference on Distributed Computing Systems, IEEE, </booktitle> <month> June, </month> <year> 1989, </year> <pages> pp. 246-253. </pages>
Reference-contexts: Hosts are attached to Nectar using Communication Acceleration Boards (CABs) and the system uses 100 Mbit/second fiber links and 1616 HUBs. At its peak (1991), the Nectar system had 26 hosts, mostly Sun 4 workstations. The advantage of switch-based networks (such as Autonet [52], HPC <ref> [24] </ref>, S3.mp [50], Nectar and ATM networks [21]), compared with LANs based on a shared medium (such as Ethernet, token ring, or FDDI), is that for the 5 same link technology, the aggregate bandwidth is much higher, since each attached computer has an exclusive link to the switch and can communicate
Reference: 25. <author> G.A. Geist and V. S. Sunderam. </author> <title> The PVM System: Supercomputer Level Concurrent Computation on a Heterogeneous Network of Workstations. </title> <booktitle> Proceedings of the Sixth Distributed Memory Computing Conference, </booktitle> <month> April, </month> <year> 1991, </year> <pages> pp. 258-261. </pages>
Reference-contexts: Programming tools are needed to give programmers more uniform access to these computing resources. For example, PVM <ref> [25] </ref> provides uniform access for applications using messages passing. In Section 4 we discuss how these problems were addressed for several applications over Nectar. 3.
Reference: 26. <author> Andrew Grimshaw. </author> <title> The Mentat Run-Time System: Support for Medium Grain Parallel Computation. </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, IEEE, </booktitle> <month> April, </month> <year> 1990, </year> <pages> pp. 1064-1073. </pages>
Reference-contexts: Ideally, these tasks are performed using programming tools that support distributed computing and that can significantly reduce the complexity of parallel and distributed computing. Examples of tools include parallelizing compilers [61, 15, 23], object 11 based runtime tools <ref> [16, 6, 26, 3, 48] </ref>, and languages and environments for specific application domains [39, 28, 64]. While many distributed applications benefit from the use of tools, today's tools cannot yet, in general, characterize the way irregular programs update complex data structures or the complex control flow of a substantial application.
Reference: 27. <author> John L. Gustafson, Gary R. Montry, and Robert E. Benner. </author> <title> "Development of Parallel Methods for a 1024-processor Hypercube". </title> <journal> SIAM Journal on Scientific and Statistical Computing 9, </journal> <month> 4 (July </month> <year> 1988), </year> <pages> 609-638. </pages>
Reference-contexts: Like SIMD machines, distributed memory MIMD computers, or multicom-puters, are inherently scalable. Multicomputers however can handle a larger set of applications than SIMD machines because they allow different programs to run on different processors. Multicomputers with over 1,000 processors have been used successfully in some application areas <ref> [27] </ref>. Multicomputers are traditionally built by using proprietary interconnections to link a set of dedicated processors. Like a proprietary internal bus in a conventional machine, the interconnect is intended to connect to a small set of specially-designed processor boards, and is optimized to do so.
Reference: 28. <author> Leonard Hamey, John Webb, and I-Chen Wu. </author> <title> Low-Level Vision on Warp and the Apply Programming Model. </title> <booktitle> In Parallel Computation and Computers for Artificial Intelligence, </booktitle> <editor> J. Kowalik, Ed., </editor> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987, </year> <pages> pp. 185-199. </pages>
Reference-contexts: Examples of tools include parallelizing compilers [61, 15, 23], object 11 based runtime tools [16, 6, 26, 3, 48], and languages and environments for specific application domains <ref> [39, 28, 64] </ref>. While many distributed applications benefit from the use of tools, today's tools cannot yet, in general, characterize the way irregular programs update complex data structures or the complex control flow of a substantial application.
Reference: 29. <author> M. Homewood, D. May, D. Shepherd, and R. Shepherd. </author> <title> "The IMS T800 Transputer". </title> <booktitle> IEEE Micro 7, </booktitle> <month> 5 (October </month> <year> 1987), </year> <pages> 10-26. </pages> <month> 30. </month> <title> iPSC/2 C Programmer's Reference Manual. </title> <publisher> Intel, </publisher> <year> 1988. </year>
Reference-contexts: 1. Introduction Current commercial parallel machines cover a wide spectrum of architectures: shared-memory parallel computers such as the Alliant, Encore, Sequent, and CRAY Y-MP; and distributed-memory computers including MIMD machines such as the Transputer <ref> [29] </ref>, iWarp [9], and the Paragon [31], and SIMD machines such as the Connection Machine [62], DAP, and MasPar [7]. Like SIMD machines, distributed memory MIMD computers, or multicom-puters, are inherently scalable.
Reference: 31. <institution> Paragon X/PS Product Overview. Intel Corporation, </institution> <year> 1991. </year>
Reference-contexts: 1. Introduction Current commercial parallel machines cover a wide spectrum of architectures: shared-memory parallel computers such as the Alliant, Encore, Sequent, and CRAY Y-MP; and distributed-memory computers including MIMD machines such as the Transputer [29], iWarp [9], and the Paragon <ref> [31] </ref>, and SIMD machines such as the Connection Machine [62], DAP, and MasPar [7]. Like SIMD machines, distributed memory MIMD computers, or multicom-puters, are inherently scalable. Multicomputers however can handle a larger set of applications than SIMD machines because they allow different programs to run on different processors. <p> This creates new challenges, including developing high-performance networks and providing programming tools that help users dealing with a new set of programming problems. 3 2.2.1. Multicomputer interconnect requirements For today's traditional multicomputers, the bandwidth of each interconnection link can be as high as several 100 MByte/second <ref> [31, 1] </ref> and the communication latency between processes on two processors can be as low as several 10s of microseconds [8] or lower [1]. Traditional local-area networks have much lower performance, in part because they have to operate in a much more complex environment.
Reference: 32. <author> Tom Stricker, Jim Stichnoth, Dave O'Hallaron, Susan Hinrichs and Thomas Gross. </author> <title> Decoupling Communication Services for Compiled Parallel Programs. </title> <type> Tech. </type> <institution> Rept. CMU-CS-94-139, Carnegie Mellon University, School of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: These operations make it possible to exploit parallelism between the application and the communication operation more effectively. This type of operations is already widely accepted on tightly coupled systems (for example, the deposit model used on iWarp <ref> [32] </ref>, and active messages [63]), but their use on network-based multicomputers is less common. 6. Concluding remarks Our experience using the Nectar systems shows that network-based multicomputers are an attractive architecture for many applications. They allow applications to combine the computing power and memory (e.g.
Reference: 33. <author> R. Iyer and G. McRae. </author> <title> Parallel Strategies for Flowsheet Simulation Using Heterogeneous Distributed-Memory Computers. </title> <note> Submitted for publication. </note>
Reference-contexts: The number of nodes attached to Nectar changed over time, starting with 2 nodes in 1989 and going through a peak of 26 nodes in 1991; it was decommissioned in October 1994. Most hosts attached to Nectar were workstations, although some application experiments <ref> [33] </ref> were performed involving the Warp system [2], iWarp [9], and the Cray-YMP. The Cray-YMP is accessible via a 26 kilometer single-mode fiber link to the Pittsburgh Supercomputing Center. <p> Several other large applications, not reported in this paper, have been successfully ported to Nectar. These include a parallel solid modeler from University of Leeds (called Mistral-3), distributed algorithms of finding exact solutions of traveling salesman problems [38], image processing using Adapt [65] and a chemical flowsheeting simulation <ref> [33] </ref>. Implementing applications on a network-based multicomputer is a non-trivial effort, and programming tools that simplify that task are needed. As described earlier in this section, programming tools for both tightly-coupled and 19 loosely-coupled distributed-memory systems are an active area of research. <p> TSP [38] is a traveling salesman problem that uses dynamic load balancing to balance the load during the space search. The current best solution is broadcasted periodically to prune the search. Flow sheeting <ref> [33] </ref> simulates a simple model of a chemical plant. It uses simple-master model. BEE [47] is a monitoring tool that collects monitoring information from all the nodes in the multicom puter. Adapt [64] is a mid-level image processing language.
Reference: 34. <author> P. Janson and R. Molva. </author> <title> "Security in Open Networks and Distributed Systems". </title> <booktitle> Computer Networks and ISDN Systems 22, </booktitle> <month> 5 (October </month> <year> 1991), </year> <pages> 323-346. </pages>
Reference-contexts: In many cases this is not a concern, but for some applications security is extremely important, e.g. business applications going over a public network, and end-to-end security measures are needed. No security features were implemented in the Nectar system. An overview of the issues can be found in <ref> [34] </ref>. 3.3.4. Multicast High-bandwidth multicast is a valuable feature for multicomputers because multicomputer applications make significant use of multicast for distributing large amounts of data [42].
Reference: 35. <author> Manpreet Khaira. </author> <title> Enabling Large Scale Switch-Level Simulation. </title> <type> Internal report. </type>
Reference-contexts: Mapping COSMOS onto Nectar In the COSMOS implementation on Nectar, called Nectar-COSMOS, subcircuits are statically distributed over the Nectar nodes <ref> [35] </ref>. The connectivity information for the subcircuits is used to determine what signals have to be communicated between the nodes on every simulation step. Each node runs a copy of the simulator and the COSMOS code corresponding to the subcircuits assigned to the node. <p> During simulation, peak activity can move around in the circuit, complicating load balancing <ref> [35] </ref>. Even though COSMOS is a significant application (about 50,000 lines of code in the COSMOS compiler chain and kernel), the porting of COSMOS to Nectar was relatively easy.
Reference: 36. <author> Karl Kleinpaste, Peter Steenkiste, and Brian Zill. </author> <title> Software Support for Outboard Buffering and Checksumming. </title> <booktitle> Proceedings of the SIGCOMM '95 Symposium on Communications Architectures and Protocols, ACM, </booktitle> <address> August/September, </address> <year> 1995, </year> <pages> pp. 87-98. </pages>
Reference-contexts: Figure 5 (b) reduces the number of data transfers while maintaining an application interface where messages are specified using a pointer-length pair (copy semantics, e.g. Unix sockets [44]). This architecture is also implemented in gigabit Nectar <ref> [57, 36] </ref>. To support the host interface of Figure 5 (c), the Nectar interface implements ``buffered'' send and receive primitives [56, 60]. With buffered send and receive primitives, the application builds and receives messages in message buffers that are shared with the system. <p> Finally, multicomputers make it possible to use heterogeneous architectures, which can potentially result in substantial speedups by matching application requirements with machine characteristics [40, 19]. We are exploring these issues in the context of the Gigabit Nectar <ref> [57, 36] </ref> and Credit Net [43, 37] projects at Carnegie Mellon University. Acknowledgements A large number of people at Carnegie Mellon University contributed to the Nectar system. H.T. Kung, Eric Cooper, Robert Sansom, Brian Zill and Francois Bitz played a key role in designing and building the system.
Reference: 37. <author> Corey Kosak, David Eckhardt, Todd Mummert, Peter Steenkiste, and Allan Fisher. </author> <title> Buffer Management and Flow Control in the Credit Net ATM Host Interface. </title> <publisher> LAN20, IEEE, </publisher> <address> Minneapolis, </address> <month> October, </month> <year> 1995, </year> <pages> pp. 370-378. </pages>
Reference-contexts: Finally, multicomputers make it possible to use heterogeneous architectures, which can potentially result in substantial speedups by matching application requirements with machine characteristics [40, 19]. We are exploring these issues in the context of the Gigabit Nectar [57, 36] and Credit Net <ref> [43, 37] </ref> projects at Carnegie Mellon University. Acknowledgements A large number of people at Carnegie Mellon University contributed to the Nectar system. H.T. Kung, Eric Cooper, Robert Sansom, Brian Zill and Francois Bitz played a key role in designing and building the system.
Reference: 38. <author> Gautham K. Kudva and Joseph Pekny. </author> <title> "A Distributed Exact Algorithm for the Multiple Resource Constrained Sequencing Problem". </title> <note> Annals of Operations Research , ( 1992), </note> . 
Reference-contexts: Several other large applications, not reported in this paper, have been successfully ported to Nectar. These include a parallel solid modeler from University of Leeds (called Mistral-3), distributed algorithms of finding exact solutions of traveling salesman problems <ref> [38] </ref>, image processing using Adapt [65] and a chemical flowsheeting simulation [33]. Implementing applications on a network-based multicomputer is a non-trivial effort, and programming tools that simplify that task are needed. <p> We briefly describe the programming model used by the other applications: Simplex is a parallel simplex algorithm. Searching and updating rows is done in parallel, but the selection of the pivot row is done on a central processor. TSP <ref> [38] </ref> is a traveling salesman problem that uses dynamic load balancing to balance the load during the space search. The current best solution is broadcasted periodically to prune the search. Flow sheeting [33] simulates a simple model of a chemical plant. It uses simple-master model.
Reference: 39. <author> G. Kudva and J. F. Pekny. "DCABB: </author> <title> A Distributed Control Architecture for Branch and Bound Calculations". </title> <journal> Computers and Chemical Engineering , ( 1994), </journal> . <note> In Press. </note>
Reference-contexts: Examples of tools include parallelizing compilers [61, 15, 23], object 11 based runtime tools [16, 6, 26, 3, 48], and languages and environments for specific application domains <ref> [39, 28, 64] </ref>. While many distributed applications benefit from the use of tools, today's tools cannot yet, in general, characterize the way irregular programs update complex data structures or the complex control flow of a substantial application.
Reference: 40. <author> H.T. Kung. </author> <title> Heterogeneous Multicomputers. </title> <institution> Carnegie Mellon Computer Science: A 25-Year Commemorative, Reading, Massachusetts, </institution> <year> 1990, </year> <pages> pp. 235-251. </pages>
Reference-contexts: Finally, multicomputers make it possible to use heterogeneous architectures, which can potentially result in substantial speedups by matching application requirements with machine characteristics <ref> [40, 19] </ref>. We are exploring these issues in the context of the Gigabit Nectar [57, 36] and Credit Net [43, 37] projects at Carnegie Mellon University. Acknowledgements A large number of people at Carnegie Mellon University contributed to the Nectar system. H.T.
Reference: 41. <author> H.T. Kung, Robert Sansom, Steven Schlick, Peter Steenkiste, Matthieu Arnould, Francois J. Bitz, Fred Chris-tianson, Eric C. Cooper, Onat Menzilcioglu, Denise Ombres, and Brian Zill. </author> <title> Network-Based Multicomputers: An Emerging Parallel Architecture. </title> <booktitle> Proceedings of Supercomputing '91, IEEE, </booktitle> <address> Albequerque, </address> <month> November, </month> <year> 1991, </year> <pages> pp. 664-673. </pages>
Reference-contexts: Nectar as a network We discuss the impact of required multicomputer network features such as robustness on performance <ref> [41] </ref>. 3.3.1. Robustness Although the error rates in modern fiber-optic networks are low, it is still necessary to implement end-to-end checksums and reliable protocols above the physical layer. <p> Flexibility Since LANs typically do not have a regular topology, and nodes may be added and removed while the system is running, LANs need more flexible routing and configuration support than tightly coupled systems. Nectar uses deterministic source routing, managed in software by the CABs <ref> [41] </ref>.
Reference: 42. <author> H.T. Kung, Peter Steenkiste, Marco Gubitoso, and Manpreet Khaira. </author> <title> Parallelizing a New Class of Large Applications over High-Speed Networks. </title> <booktitle> Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, ACM, </booktitle> <month> April, </month> <year> 1991, </year> <pages> pp. 167-177. </pages>
Reference-contexts: No security features were implemented in the Nectar system. An overview of the issues can be found in [34]. 3.3.4. Multicast High-bandwidth multicast is a valuable feature for multicomputers because multicomputer applications make significant use of multicast for distributing large amounts of data <ref> [42] </ref>. Network support for multicast allows the sender to send multicast messages once instead of many times, thus reducing both the communication overhead on the sender and message latency. <p> For this reason, we use a task-based methodology based on the separation of application code and system code <ref> [42] </ref>, which is an intermediate solution between automatic parallelization and rewriting the entire application.
Reference: 43. <author> H. T. Kung. </author> <title> "Gigabit Local Area Networks: A Systems Perspective". </title> <journal> IEEE Communications Magazine 30, </journal> <month> 4 (April </month> <year> 1992), </year> . 
Reference-contexts: Finally, multicomputers make it possible to use heterogeneous architectures, which can potentially result in substantial speedups by matching application requirements with machine characteristics [40, 19]. We are exploring these issues in the context of the Gigabit Nectar [57, 36] and Credit Net <ref> [43, 37] </ref> projects at Carnegie Mellon University. Acknowledgements A large number of people at Carnegie Mellon University contributed to the Nectar system. H.T. Kung, Eric Cooper, Robert Sansom, Brian Zill and Francois Bitz played a key role in designing and building the system.
Reference: 44. <author> Samuel J. Leffler, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massa-chusetts, </address> <year> 1989. </year>
Reference-contexts: Figure 5 (b) reduces the number of data transfers while maintaining an application interface where messages are specified using a pointer-length pair (copy semantics, e.g. Unix sockets <ref> [44] </ref>). This architecture is also implemented in gigabit Nectar [57, 36]. To support the host interface of Figure 5 (c), the Nectar interface implements ``buffered'' send and receive primitives [56, 60].
Reference: 45. <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta and John Hennessy. </author> <title> The DASH Prototype: Implementation and Performance. </title> <booktitle> 19th Annual International Symposium in Computer Architecture, IEEE, </booktitle> <month> May, </month> <year> 1992, </year> <pages> pp. 92-102. </pages>
Reference-contexts: In tightly coupled distributed memory systems, the CPU and network are very closely coupled, either at the chip level (e.g. iWarp [10]), or at the board level with access times to the network similar to that of cache accesses (e.g. Touchstone [5] or DASH <ref> [45] </ref>), resulting in low-overhead communication. In network-based multicomputers, however, the network enters the host through an I/O bus, i.e. one step further removed from the CPU than in tightly coupled systems, resulting in slower network access.
Reference: 46. <author> The MPI Forum. </author> <title> MPI: A Message Passing Interface. </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <address> Oregon, </address> <month> November, </month> <year> 1993, </year> <pages> pp. 878-883. </pages>
Reference-contexts: The first three operations are similar to traditional LAN communication, while the last four are collective communication operations that are specific to multicom-puting. Some message passing libraries, e.g. MPI <ref> [46] </ref>, support collective communication as primitive operations. Table 6 lists the features of the main communication operations used in a number of Nectar application.
Reference: 47. <author> Bernd Bruegge and Peter Steenkiste. </author> <title> Supporting the Development of Network Programs. </title> <booktitle> Proceedings of the Eleventh International Conference on Distributed Computing Systems, IEEE, </booktitle> <month> May, </month> <year> 1991, </year> <pages> pp. 641-648. </pages>
Reference-contexts: Showing that the dynamic load balancer is effective in redistributing the load requires information on the dynamic behavior of the application [11]. Figure 7 shows the effect of dynamic load balancing using the BEE monitoring tool <ref> [47] </ref>. The three windows (left to right) show for each node, the simulation time reached, the load (number of particles assigned to the node), and accumulated CPU time. <p> As described earlier in this section, programming tools for both tightly-coupled and 19 loosely-coupled distributed-memory systems are an active area of research. In the context of the Nectar system, we have demonstrated tools in three critical areas: monitoring tools that help the programmer understand the behavior of the application <ref> [47, 12] </ref>, support for data sharing across the network [48, 49], and load balancing tools that help in distributing work to make efficient use of the cycles on the nodes [67, 66, 67, 53, 54, 55]. 5. <p> TSP [38] is a traveling salesman problem that uses dynamic load balancing to balance the load during the space search. The current best solution is broadcasted periodically to prune the search. Flow sheeting [33] simulates a simple model of a chemical plant. It uses simple-master model. BEE <ref> [47] </ref> is a monitoring tool that collects monitoring information from all the nodes in the multicom puter. Adapt [64] is a mid-level image processing language. It also uses a simple master-slave programming model where slaves ask for tasks.
Reference: 48. <author> Hiroshi Nishikawa and Peter Steenkiste. Aroma: </author> <title> Language Support for Distributed Objects. </title> <booktitle> International Parallel Processing Symposium, IEEE, </booktitle> <address> Los Angeles, </address> <month> April, </month> <year> 1992, </year> <pages> pp. 686-690. 28 </pages>
Reference-contexts: Ideally, these tasks are performed using programming tools that support distributed computing and that can significantly reduce the complexity of parallel and distributed computing. Examples of tools include parallelizing compilers [61, 15, 23], object 11 based runtime tools <ref> [16, 6, 26, 3, 48] </ref>, and languages and environments for specific application domains [39, 28, 64]. While many distributed applications benefit from the use of tools, today's tools cannot yet, in general, characterize the way irregular programs update complex data structures or the complex control flow of a substantial application. <p> In the context of the Nectar system, we have demonstrated tools in three critical areas: monitoring tools that help the programmer understand the behavior of the application [47, 12], support for data sharing across the network <ref> [48, 49] </ref>, and load balancing tools that help in distributing work to make efficient use of the cycles on the nodes [67, 66, 67, 53, 54, 55]. 5. Traffic characterization An important part of any system design is understanding how the system will be used. <p> Adapt [64] is a mid-level image processing language. It also uses a simple master-slave programming model where slaves ask for tasks. Results are sent back to the master at the request of the master to avoid congestion. Aroma <ref> [48] </ref> is a distributed object system for mostly data parallel computations. Most messages are updates or requests for data assigned to other processors. Initialization is a typical initialization phase that most applications go through. It consist of a master node distributing or multicasting initial data to all the nodes.
Reference: 49. <author> Hiroshi Nishikawa and Peter Steenkiste. </author> <title> A General Architecture for Load Balancing in a Distributed-Memory Environment. </title> <booktitle> Proceedings of the Thirteenth International Conference on Distributed Computing Systems, IEEE, </booktitle> <address> Pittsburgh, </address> <month> May, </month> <year> 1993, </year> <pages> pp. 47-54. </pages>
Reference-contexts: In the context of the Nectar system, we have demonstrated tools in three critical areas: monitoring tools that help the programmer understand the behavior of the application [47, 12], support for data sharing across the network <ref> [48, 49] </ref>, and load balancing tools that help in distributing work to make efficient use of the cycles on the nodes [67, 66, 67, 53, 54, 55]. 5. Traffic characterization An important part of any system design is understanding how the system will be used.
Reference: 50. <author> Andreas Nowatzyk, Gunes Aybay, Michael Browne, Edmund Kelly, David Lee, and Michael Parkin. </author> <title> The S3.mp Scalable Shared Memory Multiprocessor. </title> <booktitle> Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences - Vol. I: Architecture, </booktitle> <month> January, </month> <year> 1994, </year> <pages> pp. 144-153. </pages>
Reference-contexts: Hosts are attached to Nectar using Communication Acceleration Boards (CABs) and the system uses 100 Mbit/second fiber links and 1616 HUBs. At its peak (1991), the Nectar system had 26 hosts, mostly Sun 4 workstations. The advantage of switch-based networks (such as Autonet [52], HPC [24], S3.mp <ref> [50] </ref>, Nectar and ATM networks [21]), compared with LANs based on a shared medium (such as Ethernet, token ring, or FDDI), is that for the 5 same link technology, the aggregate bandwidth is much higher, since each attached computer has an exclusive link to the switch and can communicate with other
Reference: 51. <institution> Sbus Specification A.1. Sun Microsystems, Inc., </institution> <year> 1990. </year>
Reference-contexts: On more recent workstations, high-speed synchronous busses <ref> [22, 51] </ref> couple the host more tightly to the I/O bus, but they require burst transfers to get good throughput.
Reference: 52. <author> Michael D. Schroeder, Andrew D. Birrell, Michael Burrows, Hal Murray, Roger M. Needham, Thomas L. Rodeheffer, Edwin H. Satterthwaite, and Charles P. Thacker. Autonet: </author> <title> A High-speed, Self-configuring Local Area Network Using Point-to-point Links. </title> <type> Research Report 59, </type> <institution> DEC Systems Research Center, </institution> <month> April, </month> <year> 1990. </year>
Reference-contexts: Nectar system overview switches (HUBs). Hosts are attached to Nectar using Communication Acceleration Boards (CABs) and the system uses 100 Mbit/second fiber links and 1616 HUBs. At its peak (1991), the Nectar system had 26 hosts, mostly Sun 4 workstations. The advantage of switch-based networks (such as Autonet <ref> [52] </ref>, HPC [24], S3.mp [50], Nectar and ATM networks [21]), compared with LANs based on a shared medium (such as Ethernet, token ring, or FDDI), is that for the 5 same link technology, the aggregate bandwidth is much higher, since each attached computer has an exclusive link to the switch and <p> Neither problem was an issue in the small Nectar prototype system. The software approach used in Nectar is in contrast with, for example, Autonet <ref> [52] </ref>, which explored the problems involved in managing large, general, switch-based networks, and supports automatic reconfiguration when host or switches are added to or removed from the network.
Reference: 53. <author> Bruce Siegell and Peter Steenkiste. </author> <title> Automatic Generation of Parallel Programs with Dynamic Load Balancing. </title> <booktitle> Proceedings of the Third International Symposium on High-Performance Distributed Computing, IEEE, </booktitle> <address> San Fran-sisco, </address> <month> August, </month> <year> 1994, </year> <pages> pp. 166-175. </pages>
Reference-contexts: we have demonstrated tools in three critical areas: monitoring tools that help the programmer understand the behavior of the application [47, 12], support for data sharing across the network [48, 49], and load balancing tools that help in distributing work to make efficient use of the cycles on the nodes <ref> [67, 66, 67, 53, 54, 55] </ref>. 5. Traffic characterization An important part of any system design is understanding how the system will be used. In the case of networks this corresponds to characterizing the traffic it will carry.
Reference: 54. <author> Bruce Siegell and Peter Steenkiste. </author> <title> Controlling Application Grain Size on a Network of Workstations. </title> <booktitle> Proceedings of Supercomputing '95, </booktitle> <address> ACM/IEEE, </address> <month> December, </month> <year> 1995. </year> <note> Appeared on CD-ROM only. </note>
Reference-contexts: we have demonstrated tools in three critical areas: monitoring tools that help the programmer understand the behavior of the application [47, 12], support for data sharing across the network [48, 49], and load balancing tools that help in distributing work to make efficient use of the cycles on the nodes <ref> [67, 66, 67, 53, 54, 55] </ref>. 5. Traffic characterization An important part of any system design is understanding how the system will be used. In the case of networks this corresponds to characterizing the traffic it will carry.
Reference: 55. <author> Bruce Siegell. </author> <title> Automatic Generation of Parallel Programs with Dynamic Load Balancing for a Network of Workstations. </title> <type> Ph.D. </type> <institution> Th., Department of Computer and Electrical Enginerring, Carnegie Mellon University, </institution> <year> 1995. </year> <note> Also appeared as technical report CMU-CS-95-168. </note>
Reference-contexts: we have demonstrated tools in three critical areas: monitoring tools that help the programmer understand the behavior of the application [47, 12], support for data sharing across the network [48, 49], and load balancing tools that help in distributing work to make efficient use of the cycles on the nodes <ref> [67, 66, 67, 53, 54, 55] </ref>. 5. Traffic characterization An important part of any system design is understanding how the system will be used. In the case of networks this corresponds to characterizing the traffic it will carry.
Reference: 56. <author> Peter Steenkiste. </author> <title> A Symmetrical Communication Interface for Distributed-Memory Computers. </title> <booktitle> Proceedings of the Sixth Distributed Memory Computing Conference, IEEE, </booktitle> <address> Portland, </address> <month> April, </month> <year> 1991, </year> <pages> pp. 262-265. </pages>
Reference-contexts: Unix sockets [44]). This architecture is also implemented in gigabit Nectar [57, 36]. To support the host interface of Figure 5 (c), the Nectar interface implements ``buffered'' send and receive primitives <ref> [56, 60] </ref>. With buffered send and receive primitives, the application builds and receives messages in message buffers that are shared with the system. <p> Communication primitives that allow a looser coupling between the application and the communication operation are more attractive. Examples are asynchronous communication and buffered communication primitives <ref> [56, 60] </ref>. Asynchronous primitives allow an application to post a send or receive (for example in the form of a pointer-length pair describing a message) and then to continue processing while the communication operation is executed.
Reference: 57. <author> Peter A. Steenkiste, Brian D. Zill, H.T. Kung, Steven J. Schlick, Jim Hughes, Bob Kowalski, and John Mul-laney. </author> <title> A Host Interface Architecture for High-Speed Networks. </title> <booktitle> Proceedings of the 4th IFIP Conference on High Performance Networks, IFIP, Liege, </booktitle> <address> Belgium, </address> <month> December, </month> <year> 1992, </year> <pages> pp. A3 1-16. </pages>
Reference-contexts: The Cray-YMP is accessible via a 26 kilometer single-mode fiber link to the Pittsburgh Supercomputing Center. We are currently further exploring multicomputer issues in the gigabit Nectar project <ref> [57, 59] </ref>, and the Nectar testbed, one of the 5 national gigabit testbeds that is exploring the benefits of gigabit networks on applications such as high-performance computing. 2 The Nectar system has been used as a vehicle to study architectural issues in network-based multicomputers and as a testbed for the development <p> Figure 5 (b) reduces the number of data transfers while maintaining an application interface where messages are specified using a pointer-length pair (copy semantics, e.g. Unix sockets [44]). This architecture is also implemented in gigabit Nectar <ref> [57, 36] </ref>. To support the host interface of Figure 5 (c), the Nectar interface implements ``buffered'' send and receive primitives [56, 60]. With buffered send and receive primitives, the application builds and receives messages in message buffers that are shared with the system. <p> Finally, multicomputers make it possible to use heterogeneous architectures, which can potentially result in substantial speedups by matching application requirements with machine characteristics [40, 19]. We are exploring these issues in the context of the Gigabit Nectar <ref> [57, 36] </ref> and Credit Net [43, 37] projects at Carnegie Mellon University. Acknowledgements A large number of people at Carnegie Mellon University contributed to the Nectar system. H.T. Kung, Eric Cooper, Robert Sansom, Brian Zill and Francois Bitz played a key role in designing and building the system.
Reference: 58. <author> Peter Steenkiste. </author> <title> Analyzing Communication Latency using the Nectar Communication Processor. </title> <booktitle> Proceedings of the SIGCOMM '92 Symposium on Communications Architectures and Protocols, ACM, </booktitle> <address> Baltimore, </address> <month> August, </month> <year> 1992, </year> <pages> pp. 199-209. </pages>
Reference-contexts: Table 1 shows the latency between applications running on two CABs and on two hosts using both proprietary (datagram, request-response, Reliable Message Protocol) and standard (User Datagram Protocol Internet Protocol) protocols <ref> [58] </ref>. The latency is under 100 microseconds for a message sent between threads on two CABs, and about 200 microseconds between processes residing in two workstation hosts. These performance results are similar to those of traditional multicomputers using similar technology [8, 5]. <p> If an uncorrectable bit error is detected, the entire system or the application fail. Error recovery in software adds overhead in a number of ways. First, there is protocol processing overhead (e.g. TCP processing); for optimized protocol implementations, this overhead can be as low as 200 instructions per packet <ref> [18, 58] </ref>. A second component is the checksum calculation; since this involves touching the data, the cost is relatively high (Figure 2), but hardware support can eliminate this cost.
Reference: 59. <author> Peter A. Steenkiste, Michael Hemy, Todd Mummert, Brian Zill. </author> <title> Architecture and Evaluation of a High-Speed Networking Subsystem for Distributed-Memory Systems. </title> <booktitle> Proceedings of the 21th Annual International Symposium on Computer Architecture, IEEE, </booktitle> , <month> May, </month> <year> 1994, </year> <pages> pp. 154-163. </pages>
Reference-contexts: The Cray-YMP is accessible via a 26 kilometer single-mode fiber link to the Pittsburgh Supercomputing Center. We are currently further exploring multicomputer issues in the gigabit Nectar project <ref> [57, 59] </ref>, and the Nectar testbed, one of the 5 national gigabit testbeds that is exploring the benefits of gigabit networks on applications such as high-performance computing. 2 The Nectar system has been used as a vehicle to study architectural issues in network-based multicomputers and as a testbed for the development
Reference: 60. <author> Peter A. Steenkiste. </author> <title> "A Systematic Approach to Host Interface Design for High-Speed Networks". </title> <booktitle> IEEE Computer 26, </booktitle> <month> 3 (March </month> <year> 1994), </year> <pages> 47-57. </pages>
Reference-contexts: Unix sockets [44]). This architecture is also implemented in gigabit Nectar [57, 36]. To support the host interface of Figure 5 (c), the Nectar interface implements ``buffered'' send and receive primitives <ref> [56, 60] </ref>. With buffered send and receive primitives, the application builds and receives messages in message buffers that are shared with the system. <p> With buffered send and receive primitives, the application builds and receives messages in message buffers that are shared with the system. The advantage of buffered sends and receives over socket-like primitives is that data no longer has to be copied as part of the send and 8 receive calls <ref> [60] </ref>. Our experience with the Nectar system indicates that buffered primitives are faster for large messages, but that socket-like primitives are faster for short messages. <p> Communication primitives that allow a looser coupling between the application and the communication operation are more attractive. Examples are asynchronous communication and buffered communication primitives <ref> [56, 60] </ref>. Asynchronous primitives allow an application to post a send or receive (for example in the form of a pointer-length pair describing a message) and then to continue processing while the communication operation is executed.
Reference: 61. <author> C. Tseng and S. Hiranandani and K. Kennedy. </author> <title> Preliminary Experiences with the Fortran D Compiler. </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> Nov., </month> <year> 1993, </year> <pages> pp. </pages> <month> 338--350. </month>
Reference-contexts: Ideally, these tasks are performed using programming tools that support distributed computing and that can significantly reduce the complexity of parallel and distributed computing. Examples of tools include parallelizing compilers <ref> [61, 15, 23] </ref>, object 11 based runtime tools [16, 6, 26, 3, 48], and languages and environments for specific application domains [39, 28, 64].
Reference: 62. <author> Lewis W. Tucker and George G. Robertson. </author> <title> "Architecture and Applications of the Connection Machine". </title> <booktitle> IEEE Computer 21, </booktitle> <month> 8 (August </month> <year> 1988), </year> <pages> 26-38. </pages>
Reference-contexts: 1. Introduction Current commercial parallel machines cover a wide spectrum of architectures: shared-memory parallel computers such as the Alliant, Encore, Sequent, and CRAY Y-MP; and distributed-memory computers including MIMD machines such as the Transputer [29], iWarp [9], and the Paragon [31], and SIMD machines such as the Connection Machine <ref> [62] </ref>, DAP, and MasPar [7]. Like SIMD machines, distributed memory MIMD computers, or multicom-puters, are inherently scalable. Multicomputers however can handle a larger set of applications than SIMD machines because they allow different programs to run on different processors.
Reference: 63. <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism or Integrated Communication and Computation. </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <month> May, </month> <year> 1992, </year> <pages> pp. 256-266. </pages>
Reference-contexts: These operations make it possible to exploit parallelism between the application and the communication operation more effectively. This type of operations is already widely accepted on tightly coupled systems (for example, the deposit model used on iWarp [32], and active messages <ref> [63] </ref>), but their use on network-based multicomputers is less common. 6. Concluding remarks Our experience using the Nectar systems shows that network-based multicomputers are an attractive architecture for many applications. They allow applications to combine the computing power and memory (e.g.
Reference: 64. <author> Jon A. Webb. </author> <title> Architecture-Independent Global Image Processing. </title> <booktitle> 10th International Conference on Pattern Recognition, IEEE, </booktitle> <address> Atlantic City, NJ, </address> <month> June, </month> <year> 1990, </year> <pages> pp. 623-628. </pages>
Reference-contexts: Examples of tools include parallelizing compilers [61, 15, 23], object 11 based runtime tools [16, 6, 26, 3, 48], and languages and environments for specific application domains <ref> [39, 28, 64] </ref>. While many distributed applications benefit from the use of tools, today's tools cannot yet, in general, characterize the way irregular programs update complex data structures or the complex control flow of a substantial application. <p> The current best solution is broadcasted periodically to prune the search. Flow sheeting [33] simulates a simple model of a chemical plant. It uses simple-master model. BEE [47] is a monitoring tool that collects monitoring information from all the nodes in the multicom puter. Adapt <ref> [64] </ref> is a mid-level image processing language. It also uses a simple master-slave programming model where slaves ask for tasks. Results are sent back to the master at the request of the master to avoid congestion. Aroma [48] is a distributed object system for mostly data parallel computations.
Reference: 65. <author> Jon Webb. </author> <title> "Steps Towards Architecture-Independent Image Processing". </title> <booktitle> IEEE Computer 25, </booktitle> <month> 2 (February </month> <year> 1992), </year> <pages> 21-31. </pages>
Reference-contexts: Several other large applications, not reported in this paper, have been successfully ported to Nectar. These include a parallel solid modeler from University of Leeds (called Mistral-3), distributed algorithms of finding exact solutions of traveling salesman problems [38], image processing using Adapt <ref> [65] </ref> and a chemical flowsheeting simulation [33]. Implementing applications on a network-based multicomputer is a non-trivial effort, and programming tools that simplify that task are needed. As described earlier in this section, programming tools for both tightly-coupled and 19 loosely-coupled distributed-memory systems are an active area of research.
Reference: 66. <author> I-Chen Wu and H.T. Kung. </author> <title> Communication Complexity for Parallel Divide-and-Conquer. </title> <booktitle> 1991 Symposium on Foundations of Computer Science, </booktitle> <address> San Juan, </address> <month> October, </month> <year> 1991, </year> <pages> pp. 151-162. </pages>
Reference-contexts: we have demonstrated tools in three critical areas: monitoring tools that help the programmer understand the behavior of the application [47, 12], support for data sharing across the network [48, 49], and load balancing tools that help in distributing work to make efficient use of the cycles on the nodes <ref> [67, 66, 67, 53, 54, 55] </ref>. 5. Traffic characterization An important part of any system design is understanding how the system will be used. In the case of networks this corresponds to characterizing the traffic it will carry.
Reference: 67. <author> I-Chen Wu. </author> <title> Multilist scheduling: A New Parallel Programming Model. </title> <type> Ph.D. </type> <institution> Th., School of Computer Science, Carnegie Mellon University, </institution> <year> 1993. </year> <note> Also appeared as technical report CMU-CS-93-184. 29 </note>
Reference-contexts: we have demonstrated tools in three critical areas: monitoring tools that help the programmer understand the behavior of the application [47, 12], support for data sharing across the network [48, 49], and load balancing tools that help in distributing work to make efficient use of the cycles on the nodes <ref> [67, 66, 67, 53, 54, 55] </ref>. 5. Traffic characterization An important part of any system design is understanding how the system will be used. In the case of networks this corresponds to characterizing the traffic it will carry.
References-found: 66

