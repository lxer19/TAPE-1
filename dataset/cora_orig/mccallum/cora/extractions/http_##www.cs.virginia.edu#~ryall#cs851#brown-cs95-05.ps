URL: http://www.cs.virginia.edu/~ryall/cs851/brown-cs95-05.ps
Refering-URL: http://www.cs.virginia.edu/~ryall/cs851/
Root-URL: http://www.cs.virginia.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Ian G. Angus and Henry A. Sowizral. </author> <title> Embedding the 2d interaction metaphor in a real 3d virtual environment. </title> <type> Technical report, </type> <institution> Boeing Computer Services, </institution> <year> 1994. </year>
Reference-contexts: In Section 4 we present a number of interaction techniques that use the virtual tricorder, and in Section 5 we briefly describe our implementation. Section 6 summarizes our contribution and lists possible future work. 2 Related Work 2.1 Origins Work by Angus and Sowizral at Boeing <ref> [1] </ref> inspired the virtual tricorder. As in our project, the Boeing work displays a visual model of a six-degrees-of-freedom user input device in VR. Their virtual model is augmented with a 2D screen protruding from the front of the device.
Reference: [2] <author> Eric A. Bier, Maureen C. Stone, Ken Pier, William Buxton, and Tony DeRose. Toolglass and Magic Lenses: </author> <title> The see-through interface. </title> <editor> In James T. Kajiya, editor, </editor> <booktitle> Computer Graphics (SIG-GRAPH '93 Proceedings), </booktitle> <volume> volume 27, </volume> <pages> pages 73-80, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: As with the wire-frame lens, the user positions it by moving the tricorder through the scene. Our final example of VR interaction via the virtual tricorder metaphor generalizes 2D magic lenses <ref> [2] </ref> into immersive 3D. A magic lens as described in [2] gives the user a different view of the objects underneath (in 2D) or behind it (in 3D). We currently implement two such lenses: wire-frame and magnify, shown in Figures 8 and 9, respectively. <p> As with the wire-frame lens, the user positions it by moving the tricorder through the scene. Our final example of VR interaction via the virtual tricorder metaphor generalizes 2D magic lenses <ref> [2] </ref> into immersive 3D. A magic lens as described in [2] gives the user a different view of the objects underneath (in 2D) or behind it (in 3D). We currently implement two such lenses: wire-frame and magnify, shown in Figures 8 and 9, respectively. <p> Thus, we restrict 3D magic lenses to be screen-aligned rectangles. The wire-frame lens renders all objects in its view in wire-frame mode. The lens attaches to the top of the tricorder and the user manipulates it like a magnifying glass. Such a lens is already implemented in <ref> [2] </ref>, and our contribution lies in porting this tool to immersive VR. The added third dimension allows the user to change the lens-to-viewpoint distance and thus to adjust how much of the viewed scene is shown in wire-frame. <p> The wire-frame lens suggests other visualization tools, for example, a lens that selectively shows normally hidden pipes or electrical wires in photorealistically rendered rooms. The magnifying lens, also already mentioned in <ref> [2] </ref>, magnifies objects in its view by a user-selected factor. It is useful for viewing dense information in context and as such relates to other information-visualization work in 3D [11] and 2D environments [13]. 5 Implementation to a Logitech Flymouse.
Reference: [3] <author> Frederick P. Brooks Jr., Ming Ouh-Young, James J. Batter, and P. Jerome Kilpatrick. </author> <title> Project GROPE haptic displays for scientific visualization. </title> <booktitle> Computer Graphics (SIGGRAPH '90 Proceedings), </booktitle> <volume> 24(4) </volume> <pages> 177-185, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The haptic feedback thus relates to the sense of touch only and in particular does not provide force feedback <ref> [3] </ref>. Furthermore, the input device's inertia (caused by manipulating a real object with mass) intensifies the tactile sensation. Even though the tactile feedback provided is basic, it convincingly suggests the reality of the virtual tricorder.
Reference: [4] <author> Steve Bryson and Creon Levitt. </author> <title> The virtual windtunnel: An environment for the exploration of three-dimensional unsteady flows. </title> <booktitle> In Visualization '91, </booktitle> <pages> pages 17-24, </pages> <year> 1991. </year>
Reference-contexts: Thus, the tricorder can simulate a wide variety of tools, but due to its generality it cannot outperform input devices tailored to specific tasks (see Section 3.2). 2.3 Glove Gestures and Postures Perhaps the best-publicized user interface for immersive VR is gesturing and posturing with a glove [20] [18] <ref> [4] </ref>. The position, orientation, and flex-angles of the fingers of one or two user-worn gloves are tracked and displayed in the virtual world. The user thus controls the environment by gesturing or assuming postures. The interface is considered intuitive, because users typically use their hands to interact with real-life objects.
Reference: [5] <author> Jeff Butterworth, Andrew Davidson, Stephen Hench, and T. Marc Olano. 3DM: </author> <title> A three dimensional modeler using a head-mounted display. </title> <booktitle> Computer Graphics (1992 Symposium on Interactive 3D Graphics), </booktitle> <volume> 25(2) </volume> <pages> 135-138, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: On the other hand, a tool metaphor and thus all tricorder interaction is inherently indirect (see Section 3.3). 2.4 Floating Menus Floating menus are text- or icon-menus displayed in the virtual world <ref> [5] </ref>. Typically, the menu free-floats in space either absolutely or relative to the user's head position; only when users grab the menu can they change its position. The main disadvantage of floating menus in immersive VR is their foreign look and feel. <p> Each tool communicates its purpose through 3D graphical design as well as button labels. However, to maintain the tactile feedback, all designs must conform to the base-shape of the actual input device. This conformity and the resulting tactile feedback distinguishes the tricorder from common VR practice. Systems such as <ref> [5] </ref> give the user a collection of tools, but, since the tools' shapes and designs vary considerably, they provide no continuity and only take minimum advantage of the tactile feedback of the used input device.
Reference: [6] <author> D. Brookshire Conner, Scott S. Snibbe, Kenneth P. Herndon, Daniel C. Robbins, Robert C. Zeleznik, and Andries van Dam. </author> <title> Three-dimensional widgets. </title> <booktitle> Computer Graphics (1992 Symposium on Interactive 3D Graphics), </booktitle> <volume> 25(2) </volume> <pages> 183-188, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Furthermore, the added third dimension seems to complicate (rather than simplify) menu selection as compared to 2D environments. We believe that 2D menus are not inherently foreign to immersive VR and describe an inconspicuous integration in Section 4.1. 2.5 3D Widgets Finally, 3D widgets <ref> [6] </ref> [14] [15] [17] are graphical direct-manipulation interfaces: they attach directly to the manipulated objects and thus exist in the same 3D world as those objects. Because their graphical design occupies the same space, it must disclose the widget's function without obliterating the manipulated object. <p> In particular, investigating the use of other 3D magic lenses for immersive information visualization seems fruitful. Other tricorder-inspired interaction techniques could also be studied; for example, it might be possible to adapt some of the 3D widgets mentioned in <ref> [6] </ref> [14] to immersive VR via the tricorder metaphor. Extending this idea to combining the tricorder metaphor with other immersive interface metaphors seems intricate but ultimately rewarding. In the hardware realm, designing an input device that better represents a tricorder has much potential.
Reference: [7] <author> Kenneth P. Herndon, Andries van Dam, and Michael Gle-icher. </author> <title> Workshop on the challenges of 3d interaction. </title> <journal> SIGCHI Bulletin, </journal> <volume> 26(4), </volume> <month> October </month> <year> 1994. </year>
Reference-contexts: Two examples of this strategy are (1) using a flashlight augmented with a six-degrees-of-freedom tracker to operate a virtual flashlight <ref> [7] </ref>, and (2) using a similarly augmented tablet/tennis-ball combination to interact via the world in miniature metaphor [16]. The main advantages of this device-per-task technique are its tactile feedback and the sensation of physical manipulation of tools with mass [8].
Reference: [8] <author> Ken Hinckley, Randy Pausch, John C. Goble, and Neal F. Kassel. </author> <title> A survey of design issues in spatial input. </title> <booktitle> 1994 UIST Proceedings, </booktitle> <pages> pages 213-222, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The main advantages of this device-per-task technique are its tactile feedback and the sensation of physical manipulation of tools with mass <ref> [8] </ref>. In a sense, our virtual tricorder is a software implementation of the device-per-task philosophy. Instead of creating new hardware, usually a cumbersome process that yields a single-purpose tool, we reprogram the visual representation and behavior of the virtual tricorder. <p> Thus, the virtual tricorder amplifies immersion. This amplified immersion in turn establishes the user's body as the spatial reference to which all actions are relative. As Hinckley et al. point out, such a spatial reference point is highly desirable <ref> [8] </ref>. However, operating such a physical device with mass is also disadvantageous. Prolonged use of it causes fatigue. The tricorder in particular is suspect, because users often operate it close to their eyes, i.e., their arms are in a non-relaxed position. <p> In both cases the user receives visual, tactile, and force feedback about the task. not provide force feedback. Tools also obviate the need for hardware clutching mechanisms <ref> [8] </ref> in which the user presses a designated button (the clutch) to disconnect the tracked input device temporarily from the virtual representation. Clutching is thus crucial for avoiding uncomfortable hand positions, but destroys the user's identification of virtual with real devices, since the clutch suspends the mapping.
Reference: [9] <author> Ken Hinckley, Randy Pausch, John C. Goble, and Neal F. Kassell. </author> <title> Passive real-world interface props for neurosurgical visualization. </title> <booktitle> In Proceedings of ACM CHI'94 Conference on Human Factors in Computing Systems, </booktitle> <year> 1994. </year>
Reference-contexts: tactile feedback (Section 3.1), but do not take advantage of the full potential of a programmable 3D tool for a 3D world. 2.2 A New Device for Every New Task Some VR researchers, notably Pausch at the University of Virginia, create for every new task a new hardware input device <ref> [9] </ref> [16] that is tailored to the requirements of the particular task. Two examples of this strategy are (1) using a flashlight augmented with a six-degrees-of-freedom tracker to operate a virtual flashlight [7], and (2) using a similarly augmented tablet/tennis-ball combination to interact via the world in miniature metaphor [16]. <p> Worse, while the tricorder lets us implement and experiment with a variety of simple interaction techniques, it cannot replace task-specific, tailor-made interaction devices such as those in <ref> [9] </ref> [16]. Thus, its usefulness for specific applications is limited to early prototyping stages that require quick implementations for testing early designs. 3.3 The Tool Metaphor Direct-manipulation interfaces are considered natural because they mimic human-object interaction in the real world. However, humans also often work with tools (see Figure 2).
Reference: [10] <author> J. Liang and M. Green. JDCAD: </author> <title> A highly interactive 3d modeling system. </title> <booktitle> In 3rd International Conference on CAD and Computer Graphics, </booktitle> <pages> pages 217-222, </pages> <address> Beijing, China, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: In object selection mode, a semi-transparent cone projects from the front of the tricorder (see Figure 6). The cone determines which object is selected when the user presses the `select' button, similar to selection in <ref> [10] </ref>. By incorporating a spotlight at the apex of the cone, we enhance visual feedback by literally highlighting the objects to be selected. In addition, we ease selection by allowing the user to control the width of the cone interactively.
Reference: [11] <author> Jock D. Mackinlay, George G. Robertson, and Stuart K. Card. </author> <title> The perspective wall: Detail and context smoothly integrated. </title> <booktitle> In Proceedings of ACM CHI'91 Conference on Human Factors in Computing Systems, </booktitle> <pages> pages 173-179, </pages> <year> 1991. </year>
Reference-contexts: The magnifying lens, also already mentioned in [2], magnifies objects in its view by a user-selected factor. It is useful for viewing dense information in context and as such relates to other information-visualization work in 3D <ref> [11] </ref> and 2D environments [13]. 5 Implementation to a Logitech Flymouse. Its tracking data is processed by a two-processor SGI Onyx whose RealityEngine II renders the scene in stereo.
Reference: [12] <author> Gene Roddenberry. </author> <title> Star Trek. </title> <journal> Paramount TV Series, </journal> <note> 1966 to 1969. </note>
Reference-contexts: Spock reports: Fascinating, Captain: while there are no life-form readings other than our own, I detect rapid movement ahead. Set your phasers on stun, orders Captain Kirk as he pulls his weapon from his belt : : : The above illustrates the universal functionality of the Star Trek <ref> [12] </ref> tricorder. As depicted in Star Trek, the tricorder's functionality encompasses medical, chemical, and structural analysis, determining existence and location of life-forms, location of any kind of fl Box 1910, Department of Computer Science, Brown University, Providence, RI 02912.
Reference: [13] <author> Manojit Sarkar, Scott S. Snibbe, Oren J. Tversky, and Steven P. Reiss. </author> <title> Stretching the rubbersheet: A metaphor for viewing large layouts on small screens. </title> <booktitle> UIST Proceedings, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: The magnifying lens, also already mentioned in [2], magnifies objects in its view by a user-selected factor. It is useful for viewing dense information in context and as such relates to other information-visualization work in 3D [11] and 2D environments <ref> [13] </ref>. 5 Implementation to a Logitech Flymouse. Its tracking data is processed by a two-processor SGI Onyx whose RealityEngine II renders the scene in stereo.
Reference: [14] <author> Scott S. Snibbe, Kenneth P. Herndon, Daniel C. Robbins, D. Brookshire Conner, and Andries van Dam. </author> <title> Using deformations to explore 3d widget design. </title> <booktitle> Computer Graphics (SIGGRAPH '92 Proceedings), </booktitle> <volume> 26(2) </volume> <pages> 351-352, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Furthermore, the added third dimension seems to complicate (rather than simplify) menu selection as compared to 2D environments. We believe that 2D menus are not inherently foreign to immersive VR and describe an inconspicuous integration in Section 4.1. 2.5 3D Widgets Finally, 3D widgets [6] <ref> [14] </ref> [15] [17] are graphical direct-manipulation interfaces: they attach directly to the manipulated objects and thus exist in the same 3D world as those objects. Because their graphical design occupies the same space, it must disclose the widget's function without obliterating the manipulated object. <p> In particular, investigating the use of other 3D magic lenses for immersive information visualization seems fruitful. Other tricorder-inspired interaction techniques could also be studied; for example, it might be possible to adapt some of the 3D widgets mentioned in [6] <ref> [14] </ref> to immersive VR via the tricorder metaphor. Extending this idea to combining the tricorder metaphor with other immersive interface metaphors seems intricate but ultimately rewarding. In the hardware realm, designing an input device that better represents a tricorder has much potential.
Reference: [15] <author> Marc P. Stevens, Robert C. Zeleznik, and John F. Hughes. </author> <title> An architecture for an extensible 3d interface toolkit. </title> <booktitle> 1994 UIST Proceedings, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Furthermore, the added third dimension seems to complicate (rather than simplify) menu selection as compared to 2D environments. We believe that 2D menus are not inherently foreign to immersive VR and describe an inconspicuous integration in Section 4.1. 2.5 3D Widgets Finally, 3D widgets [6] [14] <ref> [15] </ref> [17] are graphical direct-manipulation interfaces: they attach directly to the manipulated objects and thus exist in the same 3D world as those objects. Because their graphical design occupies the same space, it must disclose the widget's function without obliterating the manipulated object.
Reference: [16] <author> Richard Stoakley, Matthew J. Conway, and Randy Pausch. </author> <title> Virtual reality on a WIM: </title> <booktitle> Interactive worlds in miniature. Proceedings of ACM SIGCHI 1995, </booktitle> <month> March </month> <year> 1995. </year>
Reference-contexts: feedback (Section 3.1), but do not take advantage of the full potential of a programmable 3D tool for a 3D world. 2.2 A New Device for Every New Task Some VR researchers, notably Pausch at the University of Virginia, create for every new task a new hardware input device [9] <ref> [16] </ref> that is tailored to the requirements of the particular task. Two examples of this strategy are (1) using a flashlight augmented with a six-degrees-of-freedom tracker to operate a virtual flashlight [7], and (2) using a similarly augmented tablet/tennis-ball combination to interact via the world in miniature metaphor [16]. <p> device [9] <ref> [16] </ref> that is tailored to the requirements of the particular task. Two examples of this strategy are (1) using a flashlight augmented with a six-degrees-of-freedom tracker to operate a virtual flashlight [7], and (2) using a similarly augmented tablet/tennis-ball combination to interact via the world in miniature metaphor [16]. The main advantages of this device-per-task technique are its tactile feedback and the sensation of physical manipulation of tools with mass [8]. In a sense, our virtual tricorder is a software implementation of the device-per-task philosophy. <p> Worse, while the tricorder lets us implement and experiment with a variety of simple interaction techniques, it cannot replace task-specific, tailor-made interaction devices such as those in [9] <ref> [16] </ref>. Thus, its usefulness for specific applications is limited to early prototyping stages that require quick implementations for testing early designs. 3.3 The Tool Metaphor Direct-manipulation interfaces are considered natural because they mimic human-object interaction in the real world. However, humans also often work with tools (see Figure 2).
Reference: [17] <author> Paul S. Strauss and Rikk Carey. </author> <title> An object-oriented 3d graphics toolkit. </title> <booktitle> Computer Graphics (SIGGRAPH '92 Proceedings), </booktitle> <volume> 26(2) </volume> <pages> 341-349, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Furthermore, the added third dimension seems to complicate (rather than simplify) menu selection as compared to 2D environments. We believe that 2D menus are not inherently foreign to immersive VR and describe an inconspicuous integration in Section 4.1. 2.5 3D Widgets Finally, 3D widgets [6] [14] [15] <ref> [17] </ref> are graphical direct-manipulation interfaces: they attach directly to the manipulated objects and thus exist in the same 3D world as those objects. Because their graphical design occupies the same space, it must disclose the widget's function without obliterating the manipulated object.
Reference: [18] <author> David J. Sturman, David Zeltzer, and Steve Piper. </author> <title> Hands-on interaction with virtual environments. </title> <booktitle> In Proceedings of the ACM SIGGRAPH Symposium on User Interface Software and Technology, </booktitle> <year> 1989. </year>
Reference-contexts: Thus, the tricorder can simulate a wide variety of tools, but due to its generality it cannot outperform input devices tailored to specific tasks (see Section 3.2). 2.3 Glove Gestures and Postures Perhaps the best-publicized user interface for immersive VR is gesturing and posturing with a glove [20] <ref> [18] </ref> [4]. The position, orientation, and flex-angles of the fingers of one or two user-worn gloves are tracked and displayed in the virtual world. The user thus controls the environment by gesturing or assuming postures.
Reference: [19] <author> Colin Ware and Steven Osborne. </author> <title> Exploration and virtual camera control in virtual three dimensional environments. </title> <booktitle> Computer Graphics (1990 Symposium on Interactive 3D Graphics), </booktitle> <volume> 24(2) </volume> <pages> 175-183, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Its main use is to correct the flight path that resulted from the throwing mechanism. Note that engaging the grappling iron in midflight acts as an instantaneous brake. Zero-gravity grappling combines a translation-only scene in hand and the flying vehicle control interaction metaphors <ref> [19] </ref>. 2 Combining these two is of advantage because scene in hand is well suited for precise short-range navigation, while flying vehicle control is good for imprecise but fast long-range navigation [19]. 4.3 Selecting and Positioning Objects tractor beam photo simplify selection the width of the cone is interactively adjustable. <p> Zero-gravity grappling combines a translation-only scene in hand and the flying vehicle control interaction metaphors <ref> [19] </ref>. 2 Combining these two is of advantage because scene in hand is well suited for precise short-range navigation, while flying vehicle control is good for imprecise but fast long-range navigation [19]. 4.3 Selecting and Positioning Objects tractor beam photo simplify selection the width of the cone is interactively adjustable. In object selection mode, a semi-transparent cone projects from the front of the tricorder (see Figure 6).
Reference: [20] <author> Thomas G. Zimmerman, Jaron Lanier, Chuck Blanchard, Steve Bryson, and Young Harvill. </author> <title> A hand gesture interface device. </title> <editor> In J. M. Carroll and P. P. Tanner, editors, </editor> <booktitle> Proceedings of Human Factors in Computing Systems and Graphics Interface '87, </booktitle> <pages> pages 189-192, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Thus, the tricorder can simulate a wide variety of tools, but due to its generality it cannot outperform input devices tailored to specific tasks (see Section 3.2). 2.3 Glove Gestures and Postures Perhaps the best-publicized user interface for immersive VR is gesturing and posturing with a glove <ref> [20] </ref> [18] [4]. The position, orientation, and flex-angles of the fingers of one or two user-worn gloves are tracked and displayed in the virtual world. The user thus controls the environment by gesturing or assuming postures.
References-found: 20

