URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/stichnot/public/www/dsl97.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/stichnot/public/www/publications.html
Root-URL: http://www.cs.cmu.edu
Title: Code Composition as an Implementation Language for Compilers  
Author: James M. Stichnoth and Thomas Gross 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Code composition is an effective technique for a compiler to implement complex high-level operations. The developer (i.e., the language designer or compiler writer) provides building blocks consisting of sequences of code written in, e.g., C, that are combined by a composition system to generate the code for such a high-level operation. The composition system can include optimizations not commonly found in compilers; e.g., it can specialize the code sequences based on loop nesting depth or procedure parameters. We describe a composition system, Catacomb, and illustrate its use for mapping array operations onto a parallel system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1986. </year>
Reference-contexts: For example, CONSTANT is used to test whether the input evaluates to a compile-time constant. There is a default set of external functions in Catacomb, and the set is easily extended (e.g., to add functions that query the distribution parameters of HPF arrays). Catacomb implements several global optimizations <ref> [1, 8] </ref>, as well as some nonstandard optimizations based on bounds analysis. Bounds analysis is based on the observation that sometimes, even though the compiler cannot determine a specific value for a variable or expression, it can determine that it must fall within a certain range of values.
Reference: [2] <author> C. Ancourt, F. Coelho, F. Irigoin, and R. Keryell. </author> <title> A linear algebra framework for static HPF code distribution. </title> <type> Technical Report A-278-CRI, </type> <institution> Cen-tre de Recherche en Informatique, Ecole Nationale Sup erieure des Mines de Paris, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Evidence of the complexity and importance of the array assignment statement can be found in the number of different algorithms that have been proposed for executing it efficiently <ref> [21, 11, 7, 13, 26, 14, 2, 25] </ref>. The compact syntax that hides the complexity of an efficient and complete implementation is what makes the array assignment statement particularly interesting.
Reference: [3] <author> J. Auslander, M. Philipose, C. Chambers, S. Eggers, and B. Bershad. </author> <title> Fast, effective dynamic compilation. </title> <booktitle> In Proceedings of the ACM SIGPLAN'96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 149159, </pages> <address> Philadelphia, Penn-sylvania, </address> <month> May </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: model presents the user with more natural semantics and ease of use, and whether there is in fact a realistic situation in which Catacomb's semantics are necessary. 7.3 Runtime code generation Dynamic approaches attempt to improve the code at run time, and dynamic methods have lately received renewed attention (e.g., <ref> [3] </ref>). If the program notices that some parameter always has the same value, a runtime optimizer can customize the program by working with the known parameter values. Since these values are known, some tests may be resolved, or special instructions chosen, and such transformations have the potential to improve performance.
Reference: [4] <author> H. Bao, J. Bielak, O. Ghattas, D.R. O'Hallaron, L.F. Kallivokas, J.R. Shewchuk, and J. Xu. </author> <title> Earthquake ground motion modeling on parallel computers. </title> <booktitle> In Supercomputing '96, </booktitle> <address> Pittsburgh, Pennsylva-nia, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: Irregular array assignment statements and irregular parallel loops are examples of complex high-level operations. Engineering difficulties arise in a compiler when we add multiple levels of indirection, and when we distribute several dimensions of a multidimensional array. 2.3 Archimedes The Quake project <ref> [4] </ref> at Carnegie Mellon focuses on predicting the ground motion during large earthquakes. At its heart is Archimedes [19], a system for compiling and executing unstructured finite element simulations on parallel computers. The compiler component of Archimedes, called Author, presents the programmer with a language nearly identical to C.
Reference: [5] <author> R. Barrett, M. Berry, T.F. Chan, J. Demmel, J. Do-nato, J Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. Van der Vorst. </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, 2nd Edition. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn-sylvania, </address> <year> 1994. </year>
Reference-contexts: These systems are generally not extensible like Catacomb, and do not offer an integrated single-phase execution model, thus precluding the use of global optimizations in the macro processing decisions. Barrett et al. <ref> [5] </ref> use the concept of templates in a numerical computation context. Templates are designed and written in a high-level language to handle specific features of iterative solvers for linear systems (e.g., sparse or dense, convergence requirements, sequential or parallel, data layout).
Reference: [6] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M.-Y. Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1):1526, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: These libraries contain sophisticated routines that help the programmer translate a sequential program into a parallel program that uses the inspector/executor approach. More recently, several parallelizing compilers <ref> [6, 30] </ref> analyze the sequential loops in a program and automatically produce parallel loops with calls to the appropriate CHAOS routines. For many kinds of simple sequential loops, this translation is fairly straightforward. However, the translation becomes more complex as more levels of indirection in the array references are added.
Reference: [7] <author> S. Chatterjee, J. Gilbert, F.J.E. Long, R. Schreiber, and S.-H. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 26(1):7284, </volume> <month> April </month> <year> 1995. </year>
Reference-contexts: Evidence of the complexity and importance of the array assignment statement can be found in the number of different algorithms that have been proposed for executing it efficiently <ref> [21, 11, 7, 13, 26, 14, 2, 25] </ref>. The compact syntax that hides the complexity of an efficient and complete implementation is what makes the array assignment statement particularly interesting.
Reference: [8] <author> F. Chow. </author> <title> A Portable Machine-Independent Global Optimizer Design and Measurements. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1984. </year>
Reference-contexts: For example, CONSTANT is used to test whether the input evaluates to a compile-time constant. There is a default set of external functions in Catacomb, and the set is easily extended (e.g., to add functions that query the distribution parameters of HPF arrays). Catacomb implements several global optimizations <ref> [1, 8] </ref>, as well as some nonstandard optimizations based on bounds analysis. Bounds analysis is based on the observation that sometimes, even though the compiler cannot determine a specific value for a variable or expression, it can determine that it must fall within a certain range of values.
Reference: [9] <author> R. Das, J. Saltz, and R. von Hanxleden. </author> <title> Slicing analysis and indirect accesses to distributed arrays. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, volume 768 of Lecture Notes in Computer Science, </booktitle> <pages> pages 152 168, </pages> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year> <note> Springer Ver-lag. </note>
Reference-contexts: However, the translation becomes more complex as more levels of indirection in the array references are added. For compiling irregular programs that use multiple levels of indirection in distributed arrays, Das, Saltz, and von Hanxleden use a technique called slicing analysis <ref> [9] </ref>. The idea behind slicing analysis is that for each loop containing a distributed array reference with multiple levels of indirection, that loop can be rewritten as several loops, each of which contains only a single level of indirection.
Reference: [10] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: Instead, a programming language like C or Fortran serves as the base and is augmented by operators or control constructs to capture specific information about a problem domain. For example, Fortran and C have been extended with array assignment statements and abstractions for computer vision <ref> [10, 28] </ref>, or parallel looping or synchronization constructs have been added. These extensions can usually be expressed in the form of high-level operators. The benefits of such an extension are obvious: the extensions address the concerns of the problem domain, and the general-purpose language can be used for everything else. <p> In this section, we briefly present three examples of complex high-level operations. They are described in more detail in Section 5. 2.1 Array assignment statement The array assignment statement, which is a key component of High Performance Fortran (HPF) <ref> [10] </ref>, effects a parallel transfer of a sequence of elements from a source array into a destination array.
Reference: [11] <author> S.K.S. Gupta, S.D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> Compiling array expressions for efficient execution on distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 32(2):155172, </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: Evidence of the complexity and importance of the array assignment statement can be found in the number of different algorithms that have been proposed for executing it efficiently <ref> [21, 11, 7, 13, 26, 14, 2, 25] </ref>. The compact syntax that hides the complexity of an efficient and complete implementation is what makes the array assignment statement particularly interesting. <p> These algorithms are known as the CMU algorithm [22], the OSU algorithm <ref> [11] </ref>, and the LSU algorithm [26]. We divided our implementation into three components: preprocessing, architecture, and algorithm. The algorithm component determines the communication sets and packs/unpacks the communication buffers.
Reference: [12] <author> N.D. Jones, C.K. Gomard, and P. Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <booktitle> International Series in Computer Science. </booktitle> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: This kind of system fits well within the code composition framework we describe. the array assignment, as well as lines of code in support libraries. 7.2 Partial evaluation Like most optimizing compilation systems, Catacomb and the concept of code composition are related to the field of partial evaluation <ref> [12] </ref>. A partial evaluation system takes as input a program in a source language, and a set of known inputs to the program, and produces a residual program specialized for those particular inputs. The code templates are similar to a two-level version of an imperative input language.
Reference: [13] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> A linear-time algorithm for computing the memory access sequence in data-parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Princi--ples and Practice of Parallel Programming, </booktitle> <pages> pages 102111, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Evidence of the complexity and importance of the array assignment statement can be found in the number of different algorithms that have been proposed for executing it efficiently <ref> [21, 11, 7, 13, 26, 14, 2, 25] </ref>. The compact syntax that hides the complexity of an efficient and complete implementation is what makes the array assignment statement particularly interesting.
Reference: [14] <author> S. Midkiff. </author> <title> Local iteration set computation for block-cyclic distributions. </title> <type> Technical Report RC-19910, </type> <institution> IBM T.J. Watson Research Center, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Evidence of the complexity and importance of the array assignment statement can be found in the number of different algorithms that have been proposed for executing it efficiently <ref> [21, 11, 7, 13, 26, 14, 2, 25] </ref>. The compact syntax that hides the complexity of an efficient and complete implementation is what makes the array assignment statement particularly interesting.
Reference: [15] <author> S. Pollack and T. Sterling. </author> <title> A Guide to PL/I and Structured Programming (Third edition). </title> <publisher> Holt, Rinehart, and Winston, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: Furthermore, its decoupling from the compiler prevents anything like the single-phase integrated execution model mentioned in this document at the end of Section 4. A consequence that many C programmers may be familiar with is the inability to perform preprocessor operations like #if sizeof (int)==4. PL/I <ref> [15] </ref> offers a more powerful preprocessor. However, it also is incapable of a single-phase execution mode, and neither it nor the C preprocessor is equipped to perform structural queries on general expressions, a feature critical to code composition.
Reference: [16] <author> D.J. Salomon. </author> <title> Using partial evaluation in support of portability, reusability, and maintainability. </title> <editor> In Tibor Gyim othy, editor, </editor> <booktitle> Proceeding of the Sixth International Conference on Compiler Construction, volume 1060 of Lecture Notes in Computer Science, pages 208222, </booktitle> <address> Link oping, Sweden, April 1996. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: While this is an interesting way to gain compile-time control over the structure of an expression, in practice the specifications end up being overly complex and unreadable. There are other macro extensions to C (e.g., Safer C <ref> [16] </ref> and Programmable Syntax Macros [29]) that offer many of the same benefits as Catacomb. These systems are generally not extensible like Catacomb, and do not offer an integrated single-phase execution model, thus precluding the use of global optimizations in the macro processing decisions.
Reference: [17] <author> J. Saltz, R. Ponnusamy, S.D. Sharma, B. Moon, Y.- S. Hwang, M. Uyasl, and R. Das. </author> <title> A manual for the CHAOS runtime library. </title> <type> Technical Report CS-TR-3437, </type> <institution> Department of Computer Science, University of Maryland, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: The executor carries out the data transfer and the computation. The most effective means of executing irregular computations is through the use of the PARTI or CHAOS runtime libraries <ref> [24, 17, 18] </ref>, developed by Saltz et al. These libraries contain sophisticated routines that help the programmer translate a sequential program into a parallel program that uses the inspector/executor approach.
Reference: [18] <author> S.D. Sharma, R. Ponnusamy, B. Moon, Y.-S. Hwang, R. Das, and J. Saltz. </author> <title> Run-time and compile-time support for adaptive irregular problems. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 97106, </pages> <address> Washington, DC, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: The executor carries out the data transfer and the computation. The most effective means of executing irregular computations is through the use of the PARTI or CHAOS runtime libraries <ref> [24, 17, 18] </ref>, developed by Saltz et al. These libraries contain sophisticated routines that help the programmer translate a sequential program into a parallel program that uses the inspector/executor approach.
Reference: [19] <author> J.R. Shewchuk and O. Ghattas. </author> <title> A compiler for parallel finite element methods with domain-decomposed unstructured meshes. </title> <editor> In David E. Keyes and Jinchao Xu, editors, </editor> <booktitle> Proceedings of the Seventh International Conference on Domain Decomposition Methods in Scientific and Engineering Computing, volume 180 of Contemporary Mathematics, </booktitle> <pages> pages 445450. </pages> <publisher> American Mathematical Society, </publisher> <month> October </month> <year> 1993. </year>
Reference-contexts: Engineering difficulties arise in a compiler when we add multiple levels of indirection, and when we distribute several dimensions of a multidimensional array. 2.3 Archimedes The Quake project [4] at Carnegie Mellon focuses on predicting the ground motion during large earthquakes. At its heart is Archimedes <ref> [19] </ref>, a system for compiling and executing unstructured finite element simulations on parallel computers. The compiler component of Archimedes, called Author, presents the programmer with a language nearly identical to C.
Reference: [20] <author> J. Stichnoth. </author> <title> Generating Code for High-Level Operations through Code Composition. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: There are several alternative template programming styles, execution models, and semantics for allowing composition decisions to be made based on the values of code variables (these are discussed in greater detail elsewhere <ref> [20] </ref>). Most are insufficient and/or have unacceptably confusing semantics under different circumstances. The best alternative, although naturally the most difficult to implement, is a single-phase execution model, in which global optimizations are performed at the same time as the control execution. <p> Regarding efficiency, Catacomb's aggressive optimization framework results in code whose quality is close to that of hand-tuned code. Because this paper focuses more on the soft ware engineering issues, we omit a discussion of the performance of the generated code; details are available elsewhere <ref> [20] </ref>. Regarding generality, the code templates make it simple and straightforward to support any regular array assignment statement, containing an arbitrary number of dimensions, and arbitrary number of right-hand side terms, and an arbitrary interleaving of scalar subscripts and subscript triplets.
Reference: [21] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1):150159, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: Evidence of the complexity and importance of the array assignment statement can be found in the number of different algorithms that have been proposed for executing it efficiently <ref> [21, 11, 7, 13, 26, 14, 2, 25] </ref>. The compact syntax that hides the complexity of an efficient and complete implementation is what makes the array assignment statement particularly interesting.
Reference: [22] <author> J.M. Stichnoth. </author> <title> Efficient compilation of array statements for private memory systems. </title> <type> Technical Report CMU-CS-93-109, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: These algorithms are known as the CMU algorithm <ref> [22] </ref>, the OSU algorithm [11], and the LSU algorithm [26]. We divided our implementation into three components: preprocessing, architecture, and algorithm. The algorithm component determines the communication sets and packs/unpacks the communication buffers.
Reference: [23] <author> J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Exploiting task and data parallelism on a multicomputer. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1322, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The breakdown shows that the control constructs are relatively evenly matched with the code constructs. In contrast, the original implementation of the CMU algorithm in the Fx par-allelizing compiler <ref> [23] </ref> required roughly 15,000 lines of compiler code. Given that the CMU algorithm itself contains less than 1,000 lines of code, we can see that the vast majority of the 15,000 lines was dedicated to compile-time control.
Reference: [24] <author> A. Sussman, G. Agrawal, and J. Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <type> Technical Report CS-TR-3070.1, </type> <institution> University of Maryland, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: The executor carries out the data transfer and the computation. The most effective means of executing irregular computations is through the use of the PARTI or CHAOS runtime libraries <ref> [24, 17, 18] </ref>, developed by Saltz et al. These libraries contain sophisticated routines that help the programmer translate a sequential program into a parallel program that uses the inspector/executor approach.
Reference: [25] <author> R. Thakur, A. Choudhary, and G. Fox. </author> <title> Runtime array redistribution in HPF programs. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <pages> pages 309316, </pages> <address> Knoxville, Tennessee, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Evidence of the complexity and importance of the array assignment statement can be found in the number of different algorithms that have been proposed for executing it efficiently <ref> [21, 11, 7, 13, 26, 14, 2, 25] </ref>. The compact syntax that hides the complexity of an efficient and complete implementation is what makes the array assignment statement particularly interesting.
Reference: [26] <author> A. Thirumalai and J. Ramanujam. </author> <title> Efficient computation of address sequences in data-parallel programs using closed forms for basis vectors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 38(2):188 203, </volume> <month> November </month> <year> 1996. </year>
Reference-contexts: Evidence of the complexity and importance of the array assignment statement can be found in the number of different algorithms that have been proposed for executing it efficiently <ref> [21, 11, 7, 13, 26, 14, 2, 25] </ref>. The compact syntax that hides the complexity of an efficient and complete implementation is what makes the array assignment statement particularly interesting. <p> These algorithms are known as the CMU algorithm [22], the OSU algorithm [11], and the LSU algorithm <ref> [26] </ref>. We divided our implementation into three components: preprocessing, architecture, and algorithm. The algorithm component determines the communication sets and packs/unpacks the communication buffers. The architecture component provides an interface to the architecture-specific communication features, such as the specific method for calling the send, receive, and synchronization primitives.
Reference: [27] <author> T. Veldhuizen. </author> <title> Expression templates. C++ Report, </title> <address> 7(5):2631, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The C++ template system provides a simple way to generate new functions and methods, tailored to a specific data type. Veldhuizen <ref> [27] </ref> has developed a mechanism called expression templates, which allows the template system to compose code in more complex ways, based on the structure of input expressions.
Reference: [28] <author> J. Webb. </author> <title> Steps toward architecture-independent image processing. </title> <journal> IEEE Computer Magazine, </journal> <volume> 25(2):2131, </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: Instead, a programming language like C or Fortran serves as the base and is augmented by operators or control constructs to capture specific information about a problem domain. For example, Fortran and C have been extended with array assignment statements and abstractions for computer vision <ref> [10, 28] </ref>, or parallel looping or synchronization constructs have been added. These extensions can usually be expressed in the form of high-level operators. The benefits of such an extension are obvious: the extensions address the concerns of the problem domain, and the general-purpose language can be used for everything else.
Reference: [29] <author> D. Weise and R. </author> <title> Crew. Programmable syntax macros. </title> <booktitle> In Proceedings of the ACM SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 156165, </pages> <address> Albuquerque, New Mexico, </address> <month> June </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: While this is an interesting way to gain compile-time control over the structure of an expression, in practice the specifications end up being overly complex and unreadable. There are other macro extensions to C (e.g., Safer C [16] and Programmable Syntax Macros <ref> [29] </ref>) that offer many of the same benefits as Catacomb. These systems are generally not extensible like Catacomb, and do not offer an integrated single-phase execution model, thus precluding the use of global optimizations in the macro processing decisions.
Reference: [30] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrota, and A. Schwald. </author> <title> Vienna Fortran a language specification version 1.1. </title> <type> Technical Report ACPC/TR 92-4, </type> <institution> Austrian Center for Parallel Computation, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: These libraries contain sophisticated routines that help the programmer translate a sequential program into a parallel program that uses the inspector/executor approach. More recently, several parallelizing compilers <ref> [6, 30] </ref> analyze the sequential loops in a program and automatically produce parallel loops with calls to the appropriate CHAOS routines. For many kinds of simple sequential loops, this translation is fairly straightforward. However, the translation becomes more complex as more levels of indirection in the array references are added.
References-found: 30

