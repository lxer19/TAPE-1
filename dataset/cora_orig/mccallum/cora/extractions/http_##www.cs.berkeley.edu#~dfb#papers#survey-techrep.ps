URL: http://www.cs.berkeley.edu/~dfb/papers/survey-techrep.ps
Refering-URL: http://www.cs.berkeley.edu/~dfb/publist.html
Root-URL: http://www.cs.berkeley.edu
Title: Compiler Transformations for High-Performance Computing  
Author: DAVID F. BACON, SUSAN L. GRAHAM, AND OLIVER J. SHARP 
Keyword: Categories and Subject Descriptors: D.3.4 [Programming Languages]: Processors-compilers; optimization; D.1.3 [Programming Techniques]: Concurrent Programming; I.2.2 [Artificial Intelligence]: Automatic Programming-program transformation General Terms: Compilation, Optimization, Parallelism Additional Key Words and Phrases: Vectorization, multiprocessors, superscalar processors, dependence analysis  
Note: Technical Report No. UCB/CSD-93-781  
Address: Berkeley, California 94720  
Affiliation: Computer Science Division, University of California,  
Abstract: In the last three decades a large number of compiler transformations for optimizing programs have been implemented. Most optimizations for uniprocessors reduce the number of instructions executed by the program, and analyze the properties of scalar quantities using flow analysis techniques. In contrast, optimizations for high-performance vector and parallel processors maximize parallelism and memory locality, mostly by tracking the properties of arrays using loop dependence analysis. In this survey we give an overview of the important high-level program restructuring techniques for imperative languages such as C and Fortran, and to describe how and when they should be applied on high-performance uniprocessors and on vector and multiprocessor machines. The basic issues involved in optimization are discussed, and the compiler analysis required for the transformations is described in some detail. A basic familiarity with modern computer architecture and program compilation is assumed. This research has been sponsored in part by the Defense Advanced Research Projects Agency (DARPA) under contract DABT63-92-C-0026, by NSF grant CDA-8722788, by an IBM Resident Study Program Fellowship to David Bacon, and by a Hertz Fellowship to Oliver Sharp. The content of the paper does not necessarily reflect the position or the policy of the Government or of IBM and no official endorsement should be inferred. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <institution> Decide upon a part of the program to optimize and a particular transformation to apply to it; </institution>
Reference: 2. <institution> Verify that the transformation either does not change the meaning of the program or changes it in a restricted way that is acceptable to the user; and </institution>

Reference: 1. <institution> Space is allocated on the stack for the procedure invocation. </institution>
Reference: 2. <editor> The values of registers that will be modified during procedure execution (and which must be preserved across the call) are saved on the stack. </editor> <title> If the procedure calls any others, the saved registers should include the return address, </title> <publisher> R31. </publisher>
Reference: 3. <institution> The procedure body is executed. </institution>
Reference: 4. <institution> The return value (if any) is stored in R1 and the registers that were saved in step 2 are restored. </institution>
Reference: 5. <institution> The frame is removed from the stack. </institution>
Reference: 6. <editor> Control is transferred to the return address. </editor> <title> Calling a procedure is a four step process: </title>
Reference: 1. <editor> The values of any of the registers R1-R15 that contain live values are saved. </editor> <title> If the values of any global variables that might be used by the callee are in a register and have been modified, the copy of those variables in memory is updated. </title>
Reference: 2. <editor> The arguments are stored in the designated registers and, </editor> <title> if necessary, on the stack. </title>
Reference: 3. <institution> A linked jump is made to the target procedure; the CPU leaves the address of the next instruction in R31. </institution>

References-found: 11

