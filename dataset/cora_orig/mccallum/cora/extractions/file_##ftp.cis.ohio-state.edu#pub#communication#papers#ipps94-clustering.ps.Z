URL: file://ftp.cis.ohio-state.edu/pub/communication/papers/ipps94-clustering.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~panda/mapping_pub.html
Root-URL: 
Email: E-mail: fradiya-v,pandag@cis.ohio-state.edu  
Title: Clustering and Intra-Processor Scheduling for Explicitly-Parallel Programs on Distributed-Memory Systems  
Author: Vibha A. Dixit-Radiya and Dhabaleswar K. Panda 
Date: 1-8.  
Note: To appear in 8th International Parallel Processing Symposium, April 1994, pp.  
Address: Columbus, OH 43210  
Affiliation: Department of Computer and Information Science The Ohio State University,  
Abstract: When mapping a parallel program onto a parallel architecture, the number of available processors is usually less than the number of tasks in the program. This gives rise to clustering and intra-processor scheduling problems. In this paper, we address these two problems for distributed-memory systems where programs are explicitly-parallel in nature. We show that existing models of program representation are insufficient to capture temporal behavior of such programs. We use a new Temporal Communication Graph model that allows identification of overlap of communication with computation and inter-task parallelism. Clustering and intra-processor scheduling heuristics, attempting to minimize program completion time, are proposed using this model. Simulation results on random task graphs show 10-25% improvement in completion time over existing heuristics. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. W. Bollinger and S. F. Midkiff, </author> <title> "Heursitic technique for processor and link assignment in multi-computers," </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. 40, No. 3, </volume> <month> Mar </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The clustering problem is defined as mapping of tasks of a parallel program onto processors of a fully-connected system such that the program executes with minimal time <ref> [1, 2] </ref>. The problem becomes difficult when the number of processors is bounded and less than the number of tasks. This forces more than one task to be mapped onto a single processor. <p> The mapping problem is then solved for the interaction graph. Other researchers like Bollinger <ref> [1] </ref>, Chaudhary and Aggarwal [2], and Lo [8], solve the mapping problem for explicitly-parallel programs using interaction graph model. The interaction graph, due to fusion of intra-task computations and communications, is devoid of temporal information like when message communication occurs and by how much.
Reference: [2] <author> V. Chaudhary and J.K. Aggarwal, </author> <title> "A generalized scheme for mapping parallel algorithms," Completion times are normalized with respect to Aggarwal's heuristic. Each graph shows plots for different TCG classes - Computation-bound-Low-parallelism (CpLp), </title> <journal> Computation-bound-High-parallelism (CpHp), Communication-bound-Low-parallelism (CmLp), and Communication-bound-High-parallelism (CmHp). IEEE Trans. on Parallel and Distributed Systems, March 1993, </journal> <volume> Vol. 4, No. 3, </volume> <pages> pp. 328-346. </pages>
Reference-contexts: 1 Introduction The clustering problem is defined as mapping of tasks of a parallel program onto processors of a fully-connected system such that the program executes with minimal time <ref> [1, 2] </ref>. The problem becomes difficult when the number of processors is bounded and less than the number of tasks. This forces more than one task to be mapped onto a single processor. <p> The mapping problem is then solved for the interaction graph. Other researchers like Bollinger [1], Chaudhary and Aggarwal <ref> [2] </ref>, and Lo [8], solve the mapping problem for explicitly-parallel programs using interaction graph model. The interaction graph, due to fusion of intra-task computations and communications, is devoid of temporal information like when message communication occurs and by how much. <p> We use a two-phase method [10], clustering and assignment, in solving the mapping problem based on TCG. This paper proposes clustering and intra-processor task scheduling heuristics for fully-connected architectures. The performance of our heuristics is compared with that of two recent interaction graph-based heuristics proposed by Aggarwal in <ref> [2] </ref> and Lo in [8]. In a separate paper [4], we have solved the task assignment problem for wormhole-routed systems that takes into account architecture topology, link contention, and routing adaptivity. This paper is organized as follows. Section 2 formally defines the TCG model and its attributes. <p> The performance of our clustering heuristics was compared with that of two representative interaction graph-based heuristics proposed by Aggarwal in <ref> [2] </ref> and Lo in [8]. All heuristics were run on 100 samples for each combination of TCG classes and (task, processor) configurations. For Aggarwal's heuristic, the following steps were performed for each TCG. First an interaction graph was generated for the TCG.
Reference: [3] <author> V. A. Dixit-Radiya and D. K. Panda, </author> <title> "Mapping and scheduling in distributed-memory systems using temporal communication graph model," </title> <type> Technical Report, </type> <institution> Ohio State Univ., OSU-CISRC-3/93-TR11, </institution> <year> 1993. </year>
Reference-contexts: Fig. 3 shows an informal method of computing completion time for the example TCG by profiling the time steps when task computation and communication occur. A complete formal algorithm is given in <ref> [3] </ref>. By taking into account communication dependencies and overlap of communication with computation, the completion time obtained is a better objective function compared to other non-temporal functions (e.g. maximize processor load balance or minimize communication cost) used by previous researchers. <p> Due to lack of space, detailed derivations are not given here and the readers are requested to refer to <ref> [3] </ref>. 5 Simulation results and analysis Simulations on randomly generated task graphs were performed to examine the performance of the proposed clustering heuristics. The (task, processor) configurations covered were (16,4), (32,8), (128,32), and (256,32).
Reference: [4] <author> V. A. Dixit-Radiya and D. K. Panda, </author> <title> "Task assignment on distributed-memory systems with adaptive wormhole routing," </title> <booktitle> Sym. on Parallel and Distributed Processing, </booktitle> <year> 1993, </year> <pages> pp. 674-681. </pages>
Reference-contexts: This paper proposes clustering and intra-processor task scheduling heuristics for fully-connected architectures. The performance of our heuristics is compared with that of two recent interaction graph-based heuristics proposed by Aggarwal in [2] and Lo in [8]. In a separate paper <ref> [4] </ref>, we have solved the task assignment problem for wormhole-routed systems that takes into account architecture topology, link contention, and routing adaptivity. This paper is organized as follows. Section 2 formally defines the TCG model and its attributes. Section 3 discusses the approach of clustering and scheduling problems. <p> We assume that the host architecture supports concurrent computation and communication. Since clustering problem assumes a fully-connected architecture, link and port contention are ignored in the clustering phase. The assignment problem that we solve in <ref> [4] </ref> takes care of these architectural issues in detail. Given a clustering of tasks onto processors, we define computation stage cost, tcomp (v xp ) = wcomp (v xp ) fi t e , as the time to compute the pth computation stage in task T x . <p> Our ongoing work includes testing our heuristics for real applications. The clustering phase must be followed by assignment phase for mapping clusters onto processors with specific topology and routing. We have solved this assignment problem using the TCG model in a separate paper <ref> [4] </ref>. Currently, the TCG can model parallel programs with forward flow only. It is interesting to extend it to model programs with conditionals and loops and programs with dynamic structures that are predictable at compile time.
Reference: [5] <author> H. El-Rewini and T. G. Lewis, </author> <title> "Scheduling parallel program tasks onto arbitrary target machines," </title> <journal> Jour. of Parallel and Distributed Computing, </journal> <volume> 9, </volume> <year> 1990, </year> <pages> pp. 138-153. </pages>
Reference-contexts: Most previous work on mapping have ignored the IPS problem. The clustering problem has received a lot of attention and is proven in literature to be NP-hard [10]. Research has therefore been focused on designing tractable heuristics that generate efficient clustering solutions. Previous work by Gerasoulis [6], El-Rewini <ref> [5] </ref>, Kim [7], Sarkar [10] Sih and Lee [11], and Yang [12], start with a precedence graph model. This model represents dependencies among computational nodes of a sequential program. <p> Previous work by Gerasoulis [6], El-Rewini [5], Kim [7], Sarkar [10] Sih and Lee [11], and Yang [12], start with a precedence graph model. This model represents dependencies among computational nodes of a sequential program. Parallelism is extracted by directly mapping independent nodes onto processors <ref> [5, 11, 12] </ref> or by generating a partitioned graph [6, 7, 10].
Reference: [6] <author> A. Gerasoulis, S. Venugopal, and T. Yang, </author> <title> "Clustering task graphs for message passing architectures," </title> <booktitle> Int. Conf. on Supercomputing, </booktitle> <month> June </month> <year> 1990, </year> <pages> pp. 447-456. </pages>
Reference-contexts: Most previous work on mapping have ignored the IPS problem. The clustering problem has received a lot of attention and is proven in literature to be NP-hard [10]. Research has therefore been focused on designing tractable heuristics that generate efficient clustering solutions. Previous work by Gerasoulis <ref> [6] </ref>, El-Rewini [5], Kim [7], Sarkar [10] Sih and Lee [11], and Yang [12], start with a precedence graph model. This model represents dependencies among computational nodes of a sequential program. <p> This model represents dependencies among computational nodes of a sequential program. Parallelism is extracted by directly mapping independent nodes onto processors [5, 11, 12] or by generating a partitioned graph <ref> [6, 7, 10] </ref>. In the latter case, the partitioned graph is converted into an interaction graph by fusing nodes within a partition into a single super-node (which we denote as a task) and summing up the amount of communication between each pair of partitions.
Reference: [7] <author> S. J. Kim and J. C. Browne, </author> <title> "A general approach to mapping of parallel computations upon multiprocessor architectures," </title> <booktitle> Int. Conf. on Parallel Processing, 1988, </booktitle> <volume> Vol III, </volume> <pages> pp. 1-8. </pages>
Reference-contexts: The clustering problem has received a lot of attention and is proven in literature to be NP-hard [10]. Research has therefore been focused on designing tractable heuristics that generate efficient clustering solutions. Previous work by Gerasoulis [6], El-Rewini [5], Kim <ref> [7] </ref>, Sarkar [10] Sih and Lee [11], and Yang [12], start with a precedence graph model. This model represents dependencies among computational nodes of a sequential program. Parallelism is extracted by directly mapping independent nodes onto processors [5, 11, 12] or by generating a partitioned graph [6, 7, 10]. <p> This model represents dependencies among computational nodes of a sequential program. Parallelism is extracted by directly mapping independent nodes onto processors [5, 11, 12] or by generating a partitioned graph <ref> [6, 7, 10] </ref>. In the latter case, the partitioned graph is converted into an interaction graph by fusing nodes within a partition into a single super-node (which we denote as a task) and summing up the amount of communication between each pair of partitions.
Reference: [8] <author> V. M. Lo et al. "OREGAMI: </author> <title> Tools for mapping parallel computations to parallel architectures," </title> <booktitle> Int'l Jour. of Parallel Programming, </booktitle> <volume> Vol. 20, No. 3, </volume> <year> 1991. </year>
Reference-contexts: The mapping problem is then solved for the interaction graph. Other researchers like Bollinger [1], Chaudhary and Aggarwal [2], and Lo <ref> [8] </ref>, solve the mapping problem for explicitly-parallel programs using interaction graph model. The interaction graph, due to fusion of intra-task computations and communications, is devoid of temporal information like when message communication occurs and by how much. This information is important to produce efficient mapping solutions for explicitly-parallel programs. <p> This information is important to produce efficient mapping solutions for explicitly-parallel programs. In this paper, we use a new Temporal Communication Graph (TCG) model which was introduced by Lo in <ref> [8] </ref>. However in [8], temporal information has been used only after the clustering and assignment phases to produce application-specific routing. In addition, only parallel programs exhibiting logically synchronous communication have been addressed. <p> This information is important to produce efficient mapping solutions for explicitly-parallel programs. In this paper, we use a new Temporal Communication Graph (TCG) model which was introduced by Lo in <ref> [8] </ref>. However in [8], temporal information has been used only after the clustering and assignment phases to produce application-specific routing. In addition, only parallel programs exhibiting logically synchronous communication have been addressed. In this paper, we take a generalized approach in defining the TCG for modeling programs with arbitrary inter-task communication patterns. <p> This paper proposes clustering and intra-processor task scheduling heuristics for fully-connected architectures. The performance of our heuristics is compared with that of two recent interaction graph-based heuristics proposed by Aggarwal in [2] and Lo in <ref> [8] </ref>. In a separate paper [4], we have solved the task assignment problem for wormhole-routed systems that takes into account architecture topology, link contention, and routing adaptivity. This paper is organized as follows. Section 2 formally defines the TCG model and its attributes. <p> The performance of our clustering heuristics was compared with that of two representative interaction graph-based heuristics proposed by Aggarwal in [2] and Lo in <ref> [8] </ref>. All heuristics were run on 100 samples for each combination of TCG classes and (task, processor) configurations. For Aggarwal's heuristic, the following steps were performed for each TCG. First an interaction graph was generated for the TCG.
Reference: [9] <author> M. G. Norman and P. Thanisch, </author> <title> "Models of machines and computation for mapping in multicom-puters," </title> <journal> ACM Comput. Surveys, </journal> <volume> Vol. 25, No. 3, </volume> <month> Sept. </month> <year> 1993, </year> <pages> pp. 263-302. </pages>
Reference-contexts: Programs for distributed-memory systems are explicitly-parallel. They are either derived from a sequential program by automatic parallelization or written by a programmer directly in languages such as Oc-cam or C with message-passing extensions <ref> [9] </ref>. The latter is more often the case since automatic paralleliza-tion methods, such as High Performance FORTRAN, are not sufficiently matured. Explicitly-parallel programs comprise of a set of communicating sequential tasks.
Reference: [10] <author> V. Sarkar, </author> <title> Partitioning and scheduling parallel programs for multiprocessors, </title> <booktitle> Research Monographs in Parallel and Distributed Computing, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Most previous work on mapping have ignored the IPS problem. The clustering problem has received a lot of attention and is proven in literature to be NP-hard <ref> [10] </ref>. Research has therefore been focused on designing tractable heuristics that generate efficient clustering solutions. Previous work by Gerasoulis [6], El-Rewini [5], Kim [7], Sarkar [10] Sih and Lee [11], and Yang [12], start with a precedence graph model. This model represents dependencies among computational nodes of a sequential program. <p> The clustering problem has received a lot of attention and is proven in literature to be NP-hard <ref> [10] </ref>. Research has therefore been focused on designing tractable heuristics that generate efficient clustering solutions. Previous work by Gerasoulis [6], El-Rewini [5], Kim [7], Sarkar [10] Sih and Lee [11], and Yang [12], start with a precedence graph model. This model represents dependencies among computational nodes of a sequential program. Parallelism is extracted by directly mapping independent nodes onto processors [5, 11, 12] or by generating a partitioned graph [6, 7, 10]. <p> This model represents dependencies among computational nodes of a sequential program. Parallelism is extracted by directly mapping independent nodes onto processors [5, 11, 12] or by generating a partitioned graph <ref> [6, 7, 10] </ref>. In the latter case, the partitioned graph is converted into an interaction graph by fusing nodes within a partition into a single super-node (which we denote as a task) and summing up the amount of communication between each pair of partitions. <p> A TCG helps in analyzing communication dependency and overlap between communication and computation. This temporal information enables us to obtain a more accurate estimate of program completion time as compared to cost functions like minimizing interprocessor communication cost used by previous researchers. We use a two-phase method <ref> [10] </ref>, clustering and assignment, in solving the mapping problem based on TCG. This paper proposes clustering and intra-processor task scheduling heuristics for fully-connected architectures. The performance of our heuristics is compared with that of two recent interaction graph-based heuristics proposed by Aggarwal in [2] and Lo in [8].
Reference: [11] <author> G. C. Sih and E. A. Lee, </author> <title> "A compile-time scheduling heuristic for interconnection-contrained heterogeneous processor architectures," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(2), </volume> <month> Feb </month> <year> 1993, </year> <pages> pp. 175-187. </pages>
Reference-contexts: The clustering problem has received a lot of attention and is proven in literature to be NP-hard [10]. Research has therefore been focused on designing tractable heuristics that generate efficient clustering solutions. Previous work by Gerasoulis [6], El-Rewini [5], Kim [7], Sarkar [10] Sih and Lee <ref> [11] </ref>, and Yang [12], start with a precedence graph model. This model represents dependencies among computational nodes of a sequential program. Parallelism is extracted by directly mapping independent nodes onto processors [5, 11, 12] or by generating a partitioned graph [6, 7, 10]. <p> Previous work by Gerasoulis [6], El-Rewini [5], Kim [7], Sarkar [10] Sih and Lee [11], and Yang [12], start with a precedence graph model. This model represents dependencies among computational nodes of a sequential program. Parallelism is extracted by directly mapping independent nodes onto processors <ref> [5, 11, 12] </ref> or by generating a partitioned graph [6, 7, 10].
Reference: [12] <author> J. Yang, L. Bic, and A. Nicolau, </author> <title> "A mapping strategy for MIMD computers," </title> <booktitle> Int. Conf. on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: The clustering problem has received a lot of attention and is proven in literature to be NP-hard [10]. Research has therefore been focused on designing tractable heuristics that generate efficient clustering solutions. Previous work by Gerasoulis [6], El-Rewini [5], Kim [7], Sarkar [10] Sih and Lee [11], and Yang <ref> [12] </ref>, start with a precedence graph model. This model represents dependencies among computational nodes of a sequential program. Parallelism is extracted by directly mapping independent nodes onto processors [5, 11, 12] or by generating a partitioned graph [6, 7, 10]. <p> Previous work by Gerasoulis [6], El-Rewini [5], Kim [7], Sarkar [10] Sih and Lee [11], and Yang [12], start with a precedence graph model. This model represents dependencies among computational nodes of a sequential program. Parallelism is extracted by directly mapping independent nodes onto processors <ref> [5, 11, 12] </ref> or by generating a partitioned graph [6, 7, 10].
References-found: 12

