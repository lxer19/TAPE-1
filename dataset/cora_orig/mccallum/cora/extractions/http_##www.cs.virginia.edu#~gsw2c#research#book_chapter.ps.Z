URL: http://www.cs.virginia.edu/~gsw2c/research/book_chapter.ps.Z
Refering-URL: http://www.cs.virginia.edu/~gsw2c/research/research.html
Root-URL: http://www.cs.virginia.edu
Email: kortenkamp@jsc.nasa.gov  
Title: Integrating a behavior-based approach to active stereo vision with an intelligent control architecture for mobile robots  
Author: David Kortenkamp, Eric Huber and Glenn Wasson 
Address: 1012 Hercules Houston TX 77058  
Affiliation: Metrica Inc. Texas Robotics and Automation Center  
Abstract: Today's robotics applications require complex, real-time, high-bandwidth sensor systems. Although many such systems have been developed, integrating them into an autonomous robot architecture remains an area of active research. In this chapter we describe our behavior-based active stereo vision system and how we integrated this system into a hybrid reactive/deliberative robot control architecture. The integrated system is used to perform tasks such as pursuing moving agents and attending to several agents at the same time.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Philip E. Agre and David Chapman. </author> <title> Pengi an implementation of a theory of activity. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <year> 1987. </year>
Reference-contexts: This architecture was called ATLANTIS [12] and contained RAPs and robot behaviors written in Gat's Alpha circuit language. Numerous other tiered architectures have been developed, Bonasso et al [4] gives a comprehensive overview of many of these and compares them to 3T. Integrating perception Agre and Chapman <ref> [1] </ref> presented a novel integration of perception and action using markers in their Pengi system. Their system and ours differ in that their agent operated from a 2-D overhead prospective with no occlusion and no early vision (they directly accessed their simulation's data structures). <p> Our system of perceptual memory is different from representation systems that operate at other levels of autonomous agent architectures [8, 9]. It is composed of small, task dependent structures called "markers" <ref> [1, 5, 23] </ref>. The key element of markers is that they represent objects in the agent's environment that are important to its current task.
Reference: 2. <author> Dana H. Ballard. </author> <title> Animate vision. </title> <journal> Artificial Intelligence, </journal> <volume> 49(1), </volume> <year> 1991. </year>
Reference-contexts: Active vision The active vision paradigm was espoused by Ballard <ref> [2] </ref> as a way to overcome the computational complexity of reconstructing a scene from a single image. <p> at integrating vision and architecture [13] within the context of a purely behaviorist system. 2 A Behavior-based Approach to Active Stereo Vision In order to efficiently process the enormous amount of information available from stereo cameras, we use techniques that have recently been developed by the active vision research community <ref> [2, 7] </ref>. In particular, we address the issue of gaze control, i.e., where to focus attention and visual resources. Figure 1 gives an overview of the software modules comprising our vision system. To summarize the figure, left and right images enter the system from the cameras.
Reference: 3. <author> R. Peter Bonasso. </author> <title> Using parallel program specifications for reactive control of underwater vehicles. </title> <journal> Journal of Applied Intelligence, </journal> <volume> 2(2), </volume> <year> 1992. </year>
Reference-contexts: This architecture was used successfully in a number of experiments with underwater robots <ref> [3] </ref>. An alternative hybrid architecture to integrate deliberation and reaction was also being proposed by Erann Gat at the NASA Jet Propulsion Laboratory for control of a Mars rover. This architecture was called ATLANTIS [12] and contained RAPs and robot behaviors written in Gat's Alpha circuit language.
Reference: 4. <author> R. Peter Bonasso, R. J. Firby, E. Gat, David Kortenkamp, D. Miller, and M. Slack. </author> <title> Experiences with an architecture for intelligent, reactive agents. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 9(2), </volume> <year> 1997. </year>
Reference-contexts: Metrica Incorporated has also developed an intelligent robot control architecture called 3T <ref> [4] </ref>. The architecture combines a reactive control subsystem with a deliberative planning system, both mediated by a middle layer sequencer based on the Reactive Action Packages (RAP) system. This allows for long-range planning to take place while, at the same time, the system can react to immediate environmental events. <p> This architecture was called ATLANTIS [12] and contained RAPs and robot behaviors written in Gat's Alpha circuit language. Numerous other tiered architectures have been developed, Bonasso et al <ref> [4] </ref> gives a comprehensive overview of many of these and compares them to 3T. Integrating perception Agre and Chapman [1] presented a novel integration of perception and action using markers in their Pengi system.
Reference: 5. <author> F.Z. Brill, G.S. Wasson, G.J. Ferrer, and W.M. Martin. </author> <title> The effective field of view paradigm: Adding representation to a reactive system. </title> <journal> Engineering Applications of Artificial Intelligence, </journal> <month> January </month> <year> 1998. </year>
Reference-contexts: Our system of perceptual memory is different from representation systems that operate at other levels of autonomous agent architectures [8, 9]. It is composed of small, task dependent structures called "markers" <ref> [1, 5, 23] </ref>. The key element of markers is that they represent objects in the agent's environment that are important to its current task.
Reference: 6. <author> Rodney A. Brooks. </author> <title> A Robust Layered Control System for a Mobile Robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 2(1), </volume> <year> 1986. </year>
Reference-contexts: Using the proximity space to focus our attention, we developed a method for moving the proximity space within the field of view. This method is inspired by recent research into behavior-based approaches <ref> [6] </ref>, which combine simple algorithms (called behaviors) in a low-cost fashion. In our system, each behavior assesses information within the proximity space in order to influence the future position of the proximity space. The information being assessed by each behavior is simply the texture-hits and texture-misses within the proximity space.
Reference: 7. <author> David J. Coombs and C. M. Brown. </author> <title> Cooperative gaze holding in binocular vision. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on Intelligent Control, </booktitle> <year> 1991. </year>
Reference-contexts: at integrating vision and architecture [13] within the context of a purely behaviorist system. 2 A Behavior-based Approach to Active Stereo Vision In order to efficiently process the enormous amount of information available from stereo cameras, we use techniques that have recently been developed by the active vision research community <ref> [2, 7] </ref>. In particular, we address the issue of gaze control, i.e., where to focus attention and visual resources. Figure 1 gives an overview of the software modules comprising our vision system. To summarize the figure, left and right images enter the system from the cameras.
Reference: 8. <author> Chris Elsaesser and Richard MacMillan. </author> <title> Representation and algorithms for mul--tiagent adversarial planning. </title> <type> Technical Report MTR-91W000207, </type> <institution> The MITRE Corporation, </institution> <year> 1991. </year>
Reference-contexts: Information about the environment beyond a certain range from the agent's position cannot easily be verified and should no longer be stored, hence it is local-space. Our system of perceptual memory is different from representation systems that operate at other levels of autonomous agent architectures <ref> [8, 9] </ref>. It is composed of small, task dependent structures called "markers" [1, 5, 23]. The key element of markers is that they represent objects in the agent's environment that are important to its current task.
Reference: 9. <author> R. James Firby. </author> <title> An investigation into reactive planning in complex domains. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI), </booktitle> <year> 1987. </year>
Reference-contexts: Our work in this proposal builds directly on the work of Nishihara. Hybrid architectures Hybrid architectures refer to the class of robot architectures that attempt to integrate reactivity and deliberation. A first step towards the integration of reaction and deliberation was the RAPs system of Jim Firby <ref> [9] </ref>. In his thesis [10], we see the first outline of an integrated, three-layer architecture. The middle layer of that architecture and the subject of the thesis was the Reactive Action Packages system (RAPs). <p> Information about the environment beyond a certain range from the agent's position cannot easily be verified and should no longer be stored, hence it is local-space. Our system of perceptual memory is different from representation systems that operate at other levels of autonomous agent architectures <ref> [8, 9] </ref>. It is composed of small, task dependent structures called "markers" [1, 5, 23]. The key element of markers is that they represent objects in the agent's environment that are important to its current task.
Reference: 10. <author> R. James Firby. </author> <title> Adaptive Execution in Complex Dynamic Worlds. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1989. </year>
Reference-contexts: Hybrid architectures Hybrid architectures refer to the class of robot architectures that attempt to integrate reactivity and deliberation. A first step towards the integration of reaction and deliberation was the RAPs system of Jim Firby [9]. In his thesis <ref> [10] </ref>, we see the first outline of an integrated, three-layer architecture. The middle layer of that architecture and the subject of the thesis was the Reactive Action Packages system (RAPs). <p> For example, exiting a room might be orchestrated through the use of Fig. 5. The 3T intelligent reactive control architecture. reactive skills for door tracking, local navigation, grasping, and pulling. We are using the Reactive Action Packages (RAPs) system <ref> [10] </ref> for this portion of the architecture. A deliberative planning capability which reasons in depth about goals, resources and timing constraints. The planning tier was not used in the work described in this chapter. The architecture works as follows.
Reference: 11. <author> R. James Firby, Roger E. Kahn, Peter N. Prokopowicz, and Michael J. Swain. </author> <title> An architecture for vision and action. </title> <booktitle> In International Joint Conference on Artificial Intelligence (to appear), </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year> <pages> IJCAI. </pages>
Reference-contexts: Our approach operates from a 3-D first person perspective and must deal with issues of occlusion, a limited field of view, and early vision. Jim Firby et al <ref> [11] </ref> have proposed an architecture for vision and action that uses the RAPs system (the middle layer in our architecture). They have successfully incorporated gesture recognition and color histogram-based object recognition into their robotic tasks.
Reference: 12. <author> Erann Gat. </author> <title> Integrating planning and reacting in a heterogeneous asynchronous architecture for controlling real-world mobile robots. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI), </booktitle> <year> 1992. </year>
Reference-contexts: This architecture was used successfully in a number of experiments with underwater robots [3]. An alternative hybrid architecture to integrate deliberation and reaction was also being proposed by Erann Gat at the NASA Jet Propulsion Laboratory for control of a Mars rover. This architecture was called ATLANTIS <ref> [12] </ref> and contained RAPs and robot behaviors written in Gat's Alpha circuit language. Numerous other tiered architectures have been developed, Bonasso et al [4] gives a comprehensive overview of many of these and compares them to 3T.
Reference: 13. <author> Ian Horswill. </author> <title> Find this title. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 9(2), </volume> <year> 1997. </year>
Reference-contexts: A three tiered architecture at the University of Virginia [23] has incorporated perceptual markers into the bottom two tiers. Their use of markers is very similar to what we propose in our architecture. Ian Horswill has also looked at integrating vision and architecture <ref> [13] </ref> within the context of a purely behaviorist system. 2 A Behavior-based Approach to Active Stereo Vision In order to efficiently process the enormous amount of information available from stereo cameras, we use techniques that have recently been developed by the active vision research community [2, 7].
Reference: 14. <author> Eric Huber. </author> <title> Object tracking with stereo vision. </title> <booktitle> In Proceedings of the AIAA/NASA Conference on Intelligent Robots in Field, Factory, Service, and Space (CIRFFSS '94), </booktitle> <year> 1994. </year>
Reference-contexts: This integration of high-bandwidth sensing and intelligent control produces a highly reactive, goal-driven robot system. 1.1 Approach overview Metrica Incorporated has, over the last several years, developed a real-time, active stereo vision software that provides fast and robust disparity information <ref> [14, 15] </ref>. Our innovative method concentrates system resources in cubic volumes of space which we call proximity spaces.
Reference: 15. <author> Eric Huber and David Kortenkamp. </author> <title> Using stereo vision to pursue moving agents with a mobile robot. </title> <booktitle> In 1995 IEEE International Conference on Robotics and Automation, </booktitle> <year> 1995. </year>
Reference-contexts: This integration of high-bandwidth sensing and intelligent control produces a highly reactive, goal-driven robot system. 1.1 Approach overview Metrica Incorporated has, over the last several years, developed a real-time, active stereo vision software that provides fast and robust disparity information <ref> [14, 15] </ref>. Our innovative method concentrates system resources in cubic volumes of space which we call proximity spaces.
Reference: 16. <author> Leslie Pack Kaelbling. </author> <title> Goals as parallel program specifications. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <year> 1988. </year>
Reference-contexts: These Rex machines guaranteed consistent semantics between the agents internal states and that of the world. The conditional sequencer was a reaction plan [22] implemented in the GAPPs language <ref> [16] </ref>, which would continuously activate and deactivate (set enabling "wires" to high states and low states) the Rex skills until the robot's task was complete. This architecture was used successfully in a number of experiments with underwater robots [3].
Reference: 17. <author> David Kortenkamp, Eric Huber, and R. Peter Bonasso. </author> <title> Recognizing and interpreting gestures on a mobile robot. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI), </booktitle> <year> 1996. </year>
Reference-contexts: The model is shown in Figure 4. This system, implemented on the Teleos PRISM 3 hardware on a mobile robot, could recognize six gestures in real time. The gesture recognition system is described in details in <ref> [17] </ref>. 3 Integrating Perception and an Intelligent Control Architecture The perception system that we outlined in the previous section, while powerful, is limited to operating on local sensory information. Proximity spaces are purely reactive "agents" that don't have knowledge about the current task or their relationship to other proximity spaces.
Reference: 18. <author> E. Kroktkov. </author> <title> Active Computer Vision by Cooperative Focus and Stereo. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: In active vision, only a small portion of the visual field of view is analyzed at any given time and this analysis is performed many times per second on successive frames of the image. Early active vision systems <ref> [18] </ref> were promising, but typically too brittle and slow for practical application. In 1992 Keith Nishihara developed the PRISM-3 high speed stereo vision system [20], an embodiment of his Laplacian of Gaussian sign correlation theory, which itself is an extension of Marr and Poggio's classical zero-crossing theory [19].
Reference: 19. <author> David Marr and T. Poggio. </author> <title> A computational theory of human stereo vision. </title> <booktitle> In Proceedings of the Royal Society of London, </booktitle> <year> 1979. </year>
Reference-contexts: In 1992 Keith Nishihara developed the PRISM-3 high speed stereo vision system [20], an embodiment of his Laplacian of Gaussian sign correlation theory, which itself is an extension of Marr and Poggio's classical zero-crossing theory <ref> [19] </ref>. Our work in this proposal builds directly on the work of Nishihara. Hybrid architectures Hybrid architectures refer to the class of robot architectures that attempt to integrate reactivity and deliberation. A first step towards the integration of reaction and deliberation was the RAPs system of Jim Firby [9].
Reference: 20. <author> H.K. Nishihara. </author> <title> Practical real-time imaging stereo matcher. </title> <journal> Optical Engineering, </journal> <volume> 23(5), </volume> <year> 1984. </year>
Reference-contexts: Early active vision systems [18] were promising, but typically too brittle and slow for practical application. In 1992 Keith Nishihara developed the PRISM-3 high speed stereo vision system <ref> [20] </ref>, an embodiment of his Laplacian of Gaussian sign correlation theory, which itself is an extension of Marr and Poggio's classical zero-crossing theory [19]. Our work in this proposal builds directly on the work of Nishihara.
Reference: 21. <author> Stan J. Rosenschein and Leslie Pack Kaelbling. </author> <title> The synthesis of digital machines with provable epistemic properties. </title> <booktitle> In Proceedings of the Conference on Theoretical Aspects of Reasoning About Knowledge, </booktitle> <pages> pages 83-98, </pages> <address> Monterey, CA, </address> <year> 1988. </year>
Reference-contexts: The middle layer of that architecture and the subject of the thesis was the Reactive Action Packages system (RAPs). Independently and simultaneously, Pete Bonasso at MITRE, unaware of Firby's work, devised an architecture that began at the bottom layer with robot behaviors programmed in the Rex <ref> [21] </ref> language as synchronous circuits. These Rex machines guaranteed consistent semantics between the agents internal states and that of the world.
Reference: 22. <author> Marcel Schoppers. </author> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> In Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI), </booktitle> <year> 1987. </year>
Reference-contexts: These Rex machines guaranteed consistent semantics between the agents internal states and that of the world. The conditional sequencer was a reaction plan <ref> [22] </ref> implemented in the GAPPs language [16], which would continuously activate and deactivate (set enabling "wires" to high states and low states) the Rex skills until the robot's task was complete. This architecture was used successfully in a number of experiments with underwater robots [3].
Reference: 23. <author> Glenn Wasson, Gabe Ferrer, and W. N. Martin. </author> <title> Systems for perception, action and effective representation. </title> <booktitle> In FLAIRS-97, </booktitle> <year> 1997. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Jim Firby et al [11] have proposed an architecture for vision and action that uses the RAPs system (the middle layer in our architecture). They have successfully incorporated gesture recognition and color histogram-based object recognition into their robotic tasks. A three tiered architecture at the University of Virginia <ref> [23] </ref> has incorporated perceptual markers into the bottom two tiers. Their use of markers is very similar to what we propose in our architecture. <p> Our system of perceptual memory is different from representation systems that operate at other levels of autonomous agent architectures [8, 9]. It is composed of small, task dependent structures called "markers" <ref> [1, 5, 23] </ref>. The key element of markers is that they represent objects in the agent's environment that are important to its current task.
References-found: 23

