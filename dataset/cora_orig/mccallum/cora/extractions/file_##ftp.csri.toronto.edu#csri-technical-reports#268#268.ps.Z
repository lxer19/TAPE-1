URL: file://ftp.csri.toronto.edu/csri-technical-reports/268/268.ps.Z
Refering-URL: http://www.eecg.toronto.edu/~okrieg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Hierarchical Clustering: A Structure for Scalable Multiprocessor Operating System Design  
Author: Ron Unrau, Michael Stumm, and Orran Krieger 
Address: Toronto, Canada M5S 1A4  
Affiliation: Computer Systems Research Institute University of Toronto  
Abstract: Technical Report CSRI-268 March, 1992 The Computer Systems Research Institute (CSRI) is an interdisciplinary group formed to conduct research and development relevant to computer systems and their application. It is an Institute within the Faculty of Applied Science and Engineering, and the Faculty of Arts and Science, at the University of Toronto, and is supported in part by the Natural Sciences and Engineering Research Council of Canada. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Ahmad and A. Ghafoor. </author> <title> Semi-distributed load balancing for massively parallel multicomputer systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(10) </volume> <pages> 987-1004, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The scheduling decisions are divided into two levels. Within a cluster, the load between the processors is balanced at a fine granularity through the dis 4 A similar structuring mechanism for scheduling has been proposed by Feitelson and Rudolph [10], and by Ahmad and Ghafoor <ref> [1] </ref>. Page 6 patcher (in the micro-kernel). This fixed scope lim-its the load on the dispatcher itself and allows local placement decisions to be made in parallel.
Reference: [2] <author> A. Barak and Y. Kornatzky. </author> <title> Design principles of operating systems for large scale multicom-puters. </title> <type> Technical Report RC 13220 (#59114), </type> <institution> IBM T.J. Watson Research Center, </institution> <month> 10 </month> <year> 1987. </year>
Reference-contexts: Bounded overhead: The overhead for each independent operating system service call must be bounded by a constant, independent of the number of processors <ref> [2] </ref>. This follows directly from requirements 1 and 3. If the overhead of each service call increases with the number of processors, the system will ultimately saturate, so the demand on any single resource cannot increase with the number of processors.
Reference: [3] <institution> BBN Advanced Computers, Inc. Overview of the Butterfly GP1000, </institution> <year> 1988. </year>
Reference-contexts: On hierarchical systems such as Cedar [12], Dash [15], KSR-1 [6], or Hector [18], a cluster might correspond to a hardware station. On a local-remote memory architecture, such as the Butterfly <ref> [3] </ref>, a smaller cluster size (perhaps even a cluster per processor), may be more appropriate; in this case, clustering can be viewed as an extension of the fully distributed structuring sometimes used on these machines.
Reference: [4] <institution> BBN Advanced Computers, Inc. </institution> <type> TC2000 Technical Product Summary, </type> <year> 1989. </year>
Reference-contexts: This paper addresses scalability in operating system design for such scalable shared memory multiprocessors as the Stan-ford Dash [15], the Kendall Square Research KSR-1 [6], the University of Toronto Hector [18], the BBN TC2000 <ref> [4] </ref>, and the IBM RP3 [17]. Typically, existing multiprocessor operating sys tems have been scaled to accommodate a large number of processors in an ad hoc manner, by repeatedly identifying and then removing the most contended bottlenecks.
Reference: [5] <author> W.J. Bolosky, R.P. Fitzgerald, </author> <title> and M.L. Scott. Simple but effective techniques for NUMA memory management. </title> <booktitle> In Proc. 12th ACM Symp. on Operating System Principles, </booktitle> <pages> pages 19-31, </pages> <year> 1989. </year> <pages> Page 11 </pages>
Reference-contexts: At this time, the default policy is to share physical pages within a cluster, but to replicate and migrate pages across clusters. Several research groups have shown that page-level replication and migration policies can reduce access latency and contention for some applications <ref> [5, 9, 13] </ref>. However, the overhead of these policies must be amortized to realize a net gain in performance. On machines where the local-remote access ratio is high, relatively few local accesses are sufficient to recoup these costs.
Reference: [6] <author> H. Burkhardt. </author> <title> KSR1 computer system. Comp.arch Netnews article, Kendall Square Research, </title> <month> February </month> <year> 1992. </year>
Reference-contexts: However, scalable multiprocessor hardware can only be cost effective for general purpose usage if the operating system is as scalable as the hardware. This paper addresses scalability in operating system design for such scalable shared memory multiprocessors as the Stan-ford Dash [15], the Kendall Square Research KSR-1 <ref> [6] </ref>, the University of Toronto Hector [18], the BBN TC2000 [4], and the IBM RP3 [17]. Typically, existing multiprocessor operating sys tems have been scaled to accommodate a large number of processors in an ad hoc manner, by repeatedly identifying and then removing the most contended bottlenecks. <p> The appropriate cluster size for a system is affected by several factors, including the machine configuration, the local-remote memory access ratio, the hardware cache size and coherence support, the topology of the interconnection backplane, etc. On hierarchical systems such as Cedar [12], Dash [15], KSR-1 <ref> [6] </ref>, or Hector [18], a cluster might correspond to a hardware station.
Reference: [7] <author> E. Chaves, T. J. LeBlanc, B. D. Marsh, and M. L. Scott. </author> <title> Kernel-kernel communication in a shared-memory multiprocessor. </title> <booktitle> In Proc. Second Symposium on Distributed and Multiprocessor Systems, </booktitle> <pages> pages 105-116, </pages> <address> Atlanta, Georgia, March 1991. </address> <publisher> Usenix. </publisher>
Reference-contexts: Finally, clustering simplifies lock structuring issues, and hence reduces code complexity, which can lead to improved performance and scalability. For example, Chaves reports that the fine-grained locking used in an unclustered system significantly increases the length of the critical path, even when there is no lock contention <ref> [7] </ref>. As well, deadlock can be a problem when several fine-grained locks must be held simultaneously. Because contention for a lock is primarily limited to the number of processors in a cluster, clus tering allows for coarser grained locking, as we show in Section 5.
Reference: [8] <author> David R. Cheriton. </author> <title> "The V Distributed System". </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: While the principles of clustering are applicable to many OS philosophies, the discussion is presented using examples from the Hurricane operating system, a prototype clustered system we have implemented for the Hector multiprocessor. The Hurricane kernel, which is similar in structure to the V kernel <ref> [8] </ref>, provides for 1) address spaces, 2) processes, and 3) message passing. The address spaces are (initially empty) containers in which an arbitrary number of processes can run.
Reference: [9] <author> A.L. Cox and R.J. Fowler. </author> <title> The implementation of a coherent memory abstraction on a NUMA multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proc. 12th ACM Symp. on Operating System Principles, </booktitle> <pages> pages 32-44, </pages> <year> 1989. </year>
Reference-contexts: At this time, the default policy is to share physical pages within a cluster, but to replicate and migrate pages across clusters. Several research groups have shown that page-level replication and migration policies can reduce access latency and contention for some applications <ref> [5, 9, 13] </ref>. However, the overhead of these policies must be amortized to realize a net gain in performance. On machines where the local-remote access ratio is high, relatively few local accesses are sufficient to recoup these costs.
Reference: [10] <author> D.G. Feitelson and L. Rudolph. </author> <title> Distributed hierarchical control for parallel processing. </title> <journal> Computer, </journal> <volume> 23(5) </volume> <pages> 65-81, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The scheduling decisions are divided into two levels. Within a cluster, the load between the processors is balanced at a fine granularity through the dis 4 A similar structuring mechanism for scheduling has been proposed by Feitelson and Rudolph <ref> [10] </ref>, and by Ahmad and Ghafoor [1]. Page 6 patcher (in the micro-kernel). This fixed scope lim-its the load on the dispatcher itself and allows local placement decisions to be made in parallel.
Reference: [11] <author> O. Krieger, M. Stumm, and R. Unrau. </author> <title> Exploiting the advantages of mapped files for stream I/O. </title> <booktitle> In Proc. 1992 Winter USENIX Conf., </booktitle> <year> 1992. </year>
Reference-contexts: In Hurricane 3 , these services are provided on a per cluster basis, and 3 A more complete description of the Hurricane file system can be found in <ref> [11] </ref> all application requests are directed to local servers. Since changes to the name space are relatively localized, file names and directories are replicated across clusters. Consistency of replicated entries is maintained through an updating mechanism (instead of invalidating).
Reference: [12] <author> D.J. Kuck, E.S. Davidson, D.H. Lawrie, and A.H. Sameh. </author> <title> Parallel suppercomputing today and the Cedar approach. </title> <booktitle> Science, </booktitle> <pages> pages 967-974, </pages> <year> 1986. </year>
Reference-contexts: The appropriate cluster size for a system is affected by several factors, including the machine configuration, the local-remote memory access ratio, the hardware cache size and coherence support, the topology of the interconnection backplane, etc. On hierarchical systems such as Cedar <ref> [12] </ref>, Dash [15], KSR-1 [6], or Hector [18], a cluster might correspond to a hardware station.
Reference: [13] <author> R.P. LaRowe Jr., C.S. Ellis, and L.S. Kaplan. </author> <title> Tuning NUMA memory management for applications and architectures. </title> <booktitle> In Proc. 13th ACM Symp. on Operating System Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: At this time, the default policy is to share physical pages within a cluster, but to replicate and migrate pages across clusters. Several research groups have shown that page-level replication and migration policies can reduce access latency and contention for some applications <ref> [5, 9, 13] </ref>. However, the overhead of these policies must be amortized to realize a net gain in performance. On machines where the local-remote access ratio is high, relatively few local accesses are sufficient to recoup these costs.
Reference: [14] <author> E.D. Lazowska, J. Zahorjan, G.S. Graham, and K.C. Sevcik. </author> <title> Quantitative System Performance. </title> <publisher> Prentice Hall, </publisher> <year> 1984. </year>
Reference-contexts: Instead of attempting to develop a definition of our own, we identify several necessary requirements for an operating system to be scalable by considering throughput and utilization formulas from elementary queueing theory <ref> [14] </ref>. If c is a type of OS service request, then c (p), the arrival rate of requests for this service, can be expected to grow linearly with the number of processors in the worst case.
Reference: [15] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In 17th Intl. Symp. on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: However, scalable multiprocessor hardware can only be cost effective for general purpose usage if the operating system is as scalable as the hardware. This paper addresses scalability in operating system design for such scalable shared memory multiprocessors as the Stan-ford Dash <ref> [15] </ref>, the Kendall Square Research KSR-1 [6], the University of Toronto Hector [18], the BBN TC2000 [4], and the IBM RP3 [17]. <p> The appropriate cluster size for a system is affected by several factors, including the machine configuration, the local-remote memory access ratio, the hardware cache size and coherence support, the topology of the interconnection backplane, etc. On hierarchical systems such as Cedar [12], Dash <ref> [15] </ref>, KSR-1 [6], or Hector [18], a cluster might correspond to a hardware station.
Reference: [16] <author> D. Nussbaum and A. Agarwal. </author> <title> Scalability of parallel machines. </title> <journal> Communications of the ACM, </journal> <volume> 34(3) </volume> <pages> 56-61, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: For example, one of the better known formal definitions is by Nussbaum and Agarwal <ref> [16] </ref> 1 , but is not applicable to operating systems for several reasons. First, they exclude the operating system from their considerations by treating it as an extension of the hardware.
Reference: [17] <author> G.F. Pister, W.C. Brantley, and George D.A. </author> <title> The IBM parallel research processor prototype. </title> <booktitle> In Proc. Intl. Conf. on Parallel Processing, </booktitle> <pages> pages 764-769, </pages> <year> 1985. </year>
Reference-contexts: This paper addresses scalability in operating system design for such scalable shared memory multiprocessors as the Stan-ford Dash [15], the Kendall Square Research KSR-1 [6], the University of Toronto Hector [18], the BBN TC2000 [4], and the IBM RP3 <ref> [17] </ref>. Typically, existing multiprocessor operating sys tems have been scaled to accommodate a large number of processors in an ad hoc manner, by repeatedly identifying and then removing the most contended bottlenecks.
Reference: [18] <author> Z.G. Vranesic, M. Stumm, D. Lewis, and R. White. Hector: </author> <title> A hierarchically structured shared-memory multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 24(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: This paper addresses scalability in operating system design for such scalable shared memory multiprocessors as the Stan-ford Dash [15], the Kendall Square Research KSR-1 [6], the University of Toronto Hector <ref> [18] </ref>, the BBN TC2000 [4], and the IBM RP3 [17]. Typically, existing multiprocessor operating sys tems have been scaled to accommodate a large number of processors in an ad hoc manner, by repeatedly identifying and then removing the most contended bottlenecks. <p> The appropriate cluster size for a system is affected by several factors, including the machine configuration, the local-remote memory access ratio, the hardware cache size and coherence support, the topology of the interconnection backplane, etc. On hierarchical systems such as Cedar [12], Dash [15], KSR-1 [6], or Hector <ref> [18] </ref>, a cluster might correspond to a hardware station. <p> All our experiments were conducted on a 16 processor Hector shared-memory multiprocessor running the Hurricane operating system. Hector is a hierarchical multiprocessor designed for scalability <ref> [18] </ref>, where a number of processing modules are connected by bus to form stations, which in turn are connected by a hierarchy of rings. The processing modules of the current implementation contain a Motorola 88000 processor, up to 128 Kbytes instruction and 128 Kbytes data cache and 16 Mbytes RAM.
Reference: [19] <author> S. Zhou and T. Brecht. </author> <title> Processor pool-based scheduling for large-scale NUMA multiprocessors. </title> <booktitle> In Proc. ACM Sigmetrics Conference, </booktitle> <month> September </month> <year> 1990. </year> <pages> Page 12 </pages>
Reference-contexts: Hence, for parallel programs with a small number of processes, all of the processes will run on the same cluster. For larger-scale parallel programs that span multiple clusters, the number of clusters spanned is minimized. These policies are motivated by simulation studies <ref> [19] </ref>, which have shown that clustering can noticeably improve overall performance 4 . The scheduling decisions are divided into two levels.
References-found: 19

