URL: http://utstat.toronto.edu/reports/tibs/lasso.ps
Refering-URL: http://utstat.toronto.edu/reports/tibs/
Root-URL: 
Title: Regression shrinkage and selection via the lasso  
Author: Robert Tibshirani 
Keyword: regression, subset selection, shrinkage, quadratic programming.  
Affiliation: Department of Statistics and Division of Biostatistics Stanford University  
Abstract: We propose a new method for estimation in linear models. The "lasso" minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly zero and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L. </author> <year> (1993), </year> <title> Better subset selection using the non-negative garotte, </title> <type> Tech nical report, </type> <institution> Univ. of Cal., Berkeley. </institution>
Reference: <author> Breiman, L., Friedman, J., Olshen, R. & Stone, C. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth. </publisher>
Reference: <author> Efron, B. & Tibshirani, R. </author> <year> (1993), </year> <title> An Introduction to the Bootstrap, </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Friedman, J. </author> <year> (1991), </year> <title> `Multivariate adaptive regression splines (with discussion)', </title> <journal> Annals of Statistics 19(1), </journal> <pages> 1-141. </pages>
Reference: <author> George, E. & McCulloch, R. </author> <year> (1993), </year> <title> `Variable selection via gibbs sampling', </title> <journal> J. Amer. Statist. Assoc. </journal> <volume> 88, </volume> <pages> 884-889. </pages>
Reference: <author> Hastie, T. & Tibshirani, R. </author> <year> (1990), </year> <title> Generalized Additive Models, </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Lawson, C. & Hansen, R. </author> <year> (1974), </year> <title> Solving least squares problems, </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: For our problem however, m = 2 p may be very large so that direct application of this procedure is not practical. However the problem can be solved by introducing the inequality constraints sequentially, seeking a feasible solution satisfying the so-called Kuhn-Tucker conditions <ref> (Lawson and Hansen, 1974) </ref>. We outline the procedure below. Let g (fi) = i=1 (y i j fi j x ij ) 2 , and let ffi i , i = 1; 2; . . .2 p be the p-tuples of the form (1; 1; . . . 1).
Reference: <author> Stein, C. </author> <year> (1981), </year> <title> `Estimation of the mean of a multivariate normal distribution', </title> <journal> Ann. Statist. </journal> <volume> 9, </volume> <pages> 1135-1151. </pages>
Reference-contexts: Suppose that z is a multivariate normal random vector with mean and variance the identity matrix. Let ^ be an estimator of , and write ^ = z+g (z) where g is an almost differential function from R p to R p <ref> (see definition 1 of Stein, 1981) </ref>. Then Stein (1981) showed that E jj ^ jj 2 = p + E jjg (z)jj 2 + 2 1 i We may apply this result to the lasso estimator (3).
References-found: 8

