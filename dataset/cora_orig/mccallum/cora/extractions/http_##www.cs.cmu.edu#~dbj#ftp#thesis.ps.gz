URL: http://www.cs.cmu.edu/~dbj/ftp/thesis.ps.gz
Refering-URL: http://www.cs.cmu.edu/~dbj/ft.html
Root-URL: 
Title: Distributed System Fault Tolerance Using Message Logging and Checkpointing  
Author: by David Bruce Johnson Robert S. Cartwright John E. Dennis 
Degree: A Thesis Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Approved, Thesis Committee: Willy Zwaenepoel, Chairman Associate Professor of Computer Science  Professor of Computer Science  Professor  
Date: December, 1989  
Address: Houston, Texas  
Affiliation: RICE UNIVERSITY  of Mathematical Sciences  
Abstract-found: 0
Intro-found: 1
Reference: [Allchin83] <author> J. E. Allchin and M. S. McKendry. </author> <title> Synchronization and recovery of actions. </title> <booktitle> In Proceedings of the Second Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 31-44. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1983. </year>
Reference-contexts: Examples of systems using atomic actions for fault toler 10 ance include ARGUS [Liskov83, Liskov87, Liskov88], TABS [Spector85b, Spector85a], Camelot [Spector86, Spector87], the original implementation of ISIS [Birman85], Clouds <ref> [LeBlanc85, Allchin83] </ref>, QuickSilver [Haskin88], and Gutenberg [Vinter86]. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them. However, many existing applications do not fit this model, and it is not always practical to program new applications in this way.
Reference: [Almes85] <author> Guy T. Almes, Andrew P. Black, and Edward D. Lazowska. </author> <title> The Eden system: A technical review. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(1):43-59, </volume> <month> January </month> <year> 1985. </year>
Reference-contexts: Some systems have used ad hoc rules to determine when to checkpoint each process, effectively causing each process to checkpoint each time it communicates with another process. Examples of these systems include Eden <ref> [Almes85, Lazowska81] </ref> and the Tandem NonStop system [Bartlett81, Dimmer85]. These systems force processes 12 to checkpoint frequently, creating a large overhead for the provision of fault tolerance, as has been recognized in these two systems [Black85, Bartlett87].
Reference: [Anderson78] <author> Thomas Anderson, Peter A. Lee, and Santosh K. Shrivastava. </author> <title> A model of recoverability in multilevel systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-4(6):486-494, </volume> <month> November </month> <year> 1978. </year> <note> Also reprinted in Reliable Computer Systems, </note> <editor> edited by Santosh K. </editor> <booktitle> Shrivastava, </booktitle> <pages> pages 381-395, </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Other more specialized models include those to support atomic transactions [Best81, Shrivastava82, Skeen83], real-time systems [Anderson83], and centralized systems providing recovery at multiple independent levels <ref> [Anderson78] </ref>. Many of these models also make assumptions about the underlying system, such as assuming that communication is reliable or that communication and execution are atomic, in order to simplify the properties of the model.
Reference: [Anderson81] <author> T. Anderson and P. A. Lee. </author> <title> Fault Tolerance: </title> <booktitle> Principles and Practice. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: Such hardware methods, though, are less flexible and cannot easily be added to existing systems. Since the fault-tolerance methods using message logging and checkpointing developed in this thesis use no specialized hardware, these hardware methods will not be discussed further. 1.4.2 Application-Specific Methods Application-specific fault-tolerance methods <ref> [Denning76, Anderson81, Shoch82] </ref> are those designed specifically for the particular program that is to use them. These designs require knowledge of both the application program and its environment. Each type of failure that can occur in the system must be anticipated, and specific solutions for each must be programmed.
Reference: [Anderson83] <author> Thomas Anderson and John C. Knight. </author> <title> A framework for software fault-tolerance in real-time systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-9(3):355-364, </volume> <month> May </month> <year> 1983. </year> <note> Also reprinted in Reliable Computer Systems, </note> <editor> edited by Santosh K. </editor> <booktitle> Shrivastava, </booktitle> <pages> pages 358-377, </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Likewise, methods for determining the new configuration of the system after a failure, such as finding a suitable node on which to restart any failed process, are not considered. The related problems of 13 providing fault tolerance for real-time systems and applications <ref> [Hecht76, Kopetz85a, Anderson83] </ref>, reintegrating the system after a network partition [Birrell82, Walker83], and maintaining the consistency and availability of static data such as file systems and databases [Gray79, Haerder83, Lampson79, Svobodova84] are also not directly addressed by this thesis. 1.6 Thesis Outline Chapter 2 presents the theoretical framework that will be <p> Other more specialized models include those to support atomic transactions [Best81, Shrivastava82, Skeen83], real-time systems <ref> [Anderson83] </ref>, and centralized systems providing recovery at multiple independent levels [Anderson78]. Many of these models also make assumptions about the underlying system, such as assuming that communication is reliable or that communication and execution are atomic, in order to simplify the properties of the model.
Reference: [Banino82] <author> J. S. Banino and J. C. Fabre. </author> <title> Distributed coupled actors: A CHORUS proposal for reliability. </title> <booktitle> In Proceedings of the 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 128-134. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1982. </year>
Reference-contexts: The methods of N-modular redundancy and N-version programming are special cases of this class of fault-tolerance methods. Examples of systems using active replication include the new ISIS system (ISIS 2 ) [Birman87], Circus [Cooper85, Cooper84], CHORUS <ref> [Banino85, Banino82] </ref>, MP [Gait85], MARS [Kopetz85b], FT Concurrent C [Cmelik88], PRIME [Fabry73], SIFT [Wensley78], and Yang and York's work on the Intel iAPX 432 [Yang85]. These methods are well suited for use in real-time systems, since failure recovery is essentially immediate.
Reference: [Banino85] <author> J. S. Banino, J. C. Fabre, M. Guillemont, G. Morisset, and M. Rozier. </author> <title> Some fault-tolerant aspects of the CHORUS distributed system. </title> <booktitle> In Proceedings of the 5th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 430-437. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1985. </year> <pages> 115 116 </pages>
Reference-contexts: The methods of N-modular redundancy and N-version programming are special cases of this class of fault-tolerance methods. Examples of systems using active replication include the new ISIS system (ISIS 2 ) [Birman87], Circus [Cooper85, Cooper84], CHORUS <ref> [Banino85, Banino82] </ref>, MP [Gait85], MARS [Kopetz85b], FT Concurrent C [Cmelik88], PRIME [Fabry73], SIFT [Wensley78], and Yang and York's work on the Intel iAPX 432 [Yang85]. These methods are well suited for use in real-time systems, since failure recovery is essentially immediate.
Reference: [Bartlett81] <author> Joel F. Bartlett. </author> <title> A NonStop kernel. </title> <booktitle> In Proceedings of the Eighth Symposium on Operating Systems Principles, </booktitle> <pages> pages 22-29. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1981. </year>
Reference-contexts: Some systems have used ad hoc rules to determine when to checkpoint each process, effectively causing each process to checkpoint each time it communicates with another process. Examples of these systems include Eden [Almes85, Lazowska81] and the Tandem NonStop system <ref> [Bartlett81, Dimmer85] </ref>. These systems force processes 12 to checkpoint frequently, creating a large overhead for the provision of fault tolerance, as has been recognized in these two systems [Black85, Bartlett87].
Reference: [Bartlett87] <author> Joel Bartlett, Jim Gray, and Bob Horst. </author> <title> Fault tolerance in Tandem computer systems. In The Evolution of Fault-Tolerant Computing, edited by A. Avizienis, </title> <editor> H. Kopetz, and J.C. Laprie, </editor> <booktitle> volume 1 of Dependable Computing and Fault-Tolerant Systems, </booktitle> <pages> pages 55-76. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Examples of these systems include Eden [Almes85, Lazowska81] and the Tandem NonStop system [Bartlett81, Dimmer85]. These systems force processes 12 to checkpoint frequently, creating a large overhead for the provision of fault tolerance, as has been recognized in these two systems <ref> [Black85, Bartlett87] </ref>. A different style of using checkpointing alone is to periodically create a global checkpoint of the entire system, using a protocol to coordinate the checkpointing of the individual component processes.
Reference: [Bernstein87] <author> Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: Each process is checkpointed individually, and no coordination is required between the checkpointing of different processes. The logged messages and checkpoints are stored in some way that survives any failures that the system is intended to recover from, such as by writing them to stable storage on disk <ref> [Lampson79, Bernstein87] </ref>. Recovery of a failed process using these logged messages and checkpoints is based on the assumption that the execution of the process is deterministic between received input messages. <p> The current state of a process is thus completely determined by its starting state and the sequence of messages it has received. * No global clock is available in the system. * The network includes a shared stable storage service <ref> [Lampson79, Bernstein87] </ref> that is always accessible to all active nodes in the system. * Packet delivery on the network need not be guaranteed, but reliable delivery of a packet can be achieved by retransmitting it a limited number of times until an acknowledgement arrives from its destination. * The network protocol <p> Although in some cases, application-specific methods can be made to be more efficient than transparent general-purpose methods, they are limited by their lack of transparency. 1.4.3 Atomic Actions The use of atomic actions to provide fault tolerance [Lomet85] is similar to the use of atomic transactions in database systems <ref> [Gray79, Lampson81, Bernstein87] </ref>, but atomic actions can perform arbitrary operations on user-defined data types. By structuring application programs as a series of possibly nested atomic actions, any failure can be recovered from by simply forcing each failed action to abort and then reinvok-ing it from its beginning. <p> This same problem of delaying output occurs in other fault-tolerance methods. In systems using atomic actions (Section 1.4.3), and in more conventional database transaction systems <ref> [Bernstein87] </ref>, the transaction or action cannot commit until all information needed to recover it has been recorded on stable storage. Thus, any side effects of the computation cannot be made permanent (and cannot be seen outside the system) until this occurs. <p> Previous message logging protocols send an extra copy of each message elsewhere for logging, either to stable storage <ref> [Lampson79, Bernstein87] </ref> on disk or to some special backup process that can survive the failure of the receiver process. <p> Some mechanisms used to support atomic actions (Section 1.4.3) resemble the use of optimistic message logging and checkpointing. Logging on stable storage is used to record state changes of modified objects during the execution of a transaction. Typically, the entire state of each object is recorded, although logical logging <ref> [Bernstein87] </ref> records only the names of operations performed and their parameters, such that they can be reexecuted during recovery, much the same as reexecuting processes based on logged messages.
Reference: [Best81] <author> Eike Best and Brian Randell. </author> <title> A formal model of atomicity in asynchronous systems. </title> <journal> Acta Informatica, </journal> <volume> 16(1) </volume> <pages> 93-124, </pages> <month> August </month> <year> 1981. </year> <note> Also reprinted in Reliable Computer Systems, </note> <editor> edited by Santosh K. </editor> <booktitle> Shrivastava, </booktitle> <pages> pages 266-297, </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Other more specialized models include those to support atomic transactions <ref> [Best81, Shrivastava82, Skeen83] </ref>, real-time systems [Anderson83], and centralized systems providing recovery at multiple independent levels [Anderson78]. Many of these models also make assumptions about the underlying system, such as assuming that communication is reliable or that communication and execution are atomic, in order to simplify the properties of the model.
Reference: [Birman85] <author> Kenneth P. Birman. </author> <title> Replication and fault-tolerance in the ISIS system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-86. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1985. </year>
Reference-contexts: Examples of systems using atomic actions for fault toler 10 ance include ARGUS [Liskov83, Liskov87, Liskov88], TABS [Spector85b, Spector85a], Camelot [Spector86, Spector87], the original implementation of ISIS <ref> [Birman85] </ref>, Clouds [LeBlanc85, Allchin83], QuickSilver [Haskin88], and Gutenberg [Vinter86]. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them. However, many existing applications do not fit this model, and it is not always practical to program new applications in this way.
Reference: [Birman87] <author> Kenneth P. Birman and Thomas A. Joseph. </author> <title> Exploiting virtual synchrony in distributed systems. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 123-138. </pages> <publisher> ACM, </publisher> <month> November </month> <year> 1987. </year>
Reference-contexts: If one replica fails, the remaining replicas of that process continue the computation without interruption. The methods of N-modular redundancy and N-version programming are special cases of this class of fault-tolerance methods. Examples of systems using active replication include the new ISIS system (ISIS 2 ) <ref> [Birman87] </ref>, Circus [Cooper85, Cooper84], CHORUS [Banino85, Banino82], MP [Gait85], MARS [Kopetz85b], FT Concurrent C [Cmelik88], PRIME [Fabry73], SIFT [Wensley78], and Yang and York's work on the Intel iAPX 432 [Yang85]. These methods are well suited for use in real-time systems, since failure recovery is essentially immediate.
Reference: [Birrell82] <author> Andrew D. Birrell, Roy Levin, Roger M. Needham, and Michael D. Schroeder. Grapevine: </author> <title> An exercise in distributed computing. </title> <journal> Communications of the ACM, </journal> <volume> 25(4) </volume> <pages> 260-274, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: The related problems of 13 providing fault tolerance for real-time systems and applications [Hecht76, Kopetz85a, Anderson83], reintegrating the system after a network partition <ref> [Birrell82, Walker83] </ref>, and maintaining the consistency and availability of static data such as file systems and databases [Gray79, Haerder83, Lampson79, Svobodova84] are also not directly addressed by this thesis. 1.6 Thesis Outline Chapter 2 presents the theoretical framework that will be used in the remainder of the thesis.
Reference: [Birrell84] <author> Andrew D. Birrell and Bruce Jay Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: The use of this piggybacking optimization for a sequence of these request-reply exchanges is illustrated in Figure 3.4. This optimization is particularly useful in systems using a remote procedure call protocol <ref> [Birrell84] </ref> or other request-response protocol [Cheriton88, Cheriton86a], since all communication takes place as a sequence of message exchanges.
Reference: [Black85] <author> Andrew P. Black. </author> <title> Supporting distributed applications: Experience with Eden. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 181-193. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1985. </year> <month> 117 </month>
Reference-contexts: Examples of these systems include Eden [Almes85, Lazowska81] and the Tandem NonStop system [Bartlett81, Dimmer85]. These systems force processes 12 to checkpoint frequently, creating a large overhead for the provision of fault tolerance, as has been recognized in these two systems <ref> [Black85, Bartlett87] </ref>. A different style of using checkpointing alone is to periodically create a global checkpoint of the entire system, using a protocol to coordinate the checkpointing of the individual component processes.
Reference: [Borg83] <author> Anita Borg, Jim Baumbach, and Sam Glazer. </author> <title> A message system supporting fault tolerance. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 90-99. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1983. </year>
Reference-contexts: These protocols are called "pessimistic" because they assume that a failure could occur at any time, possibly before the needed logging is completed. Previous pessimistic message logging protocols <ref> [Powell83, Borg83, Borg89] </ref> have achieved this guarantee by blocking a process when it receives a message, until that message has been logged. This ensures that if a process fails, all messages received by it since its last checkpoint are logged, regardless of when the failure occurs. <p> The main drawback, however, of pessimistic message logging is the performance degradation caused by the synchronization in the message logging protocol. Previous pessimistic message logging protocols <ref> [Powell83, Borg83, Borg89] </ref> have attempted to reduce this overhead by using special-purpose hardware to assist the logging. 1.1.2 Optimistic Message Logging In contrast to pessimistic protocols, optimistic message logging protocols operate asynchronously. <p> Also, sender-based message logging requires no specialized hardware to assist with logging. The TARGON/32 system [Borg89] and its predecessor Auros <ref> [Borg83] </ref> log messages at a backup node for the receiver, using specialized networking hardware that provides three-way atomic broadcast of each message. <p> All pessimistic logging protocols must prevent the system from entering a state that could cause any process other than those that failed to be rolled back during recovery. Previous pessimistic logging protocols <ref> [Borg83, Powell83, Borg89] </ref> have required each message to be logged before it is received by the destination process, blocking the receiver while the logging takes place.
Reference: [Borg89] <author> Anita Borg, Wolfgang Blau, Wolfgang Graetsch, Ferdinand Herrmann, and Wolfgang Oberle. </author> <title> Fault tolerance under UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(1) </volume> <pages> 1-24, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: These protocols are called "pessimistic" because they assume that a failure could occur at any time, possibly before the needed logging is completed. Previous pessimistic message logging protocols <ref> [Powell83, Borg83, Borg89] </ref> have achieved this guarantee by blocking a process when it receives a message, until that message has been logged. This ensures that if a process fails, all messages received by it since its last checkpoint are logged, regardless of when the failure occurs. <p> The main drawback, however, of pessimistic message logging is the performance degradation caused by the synchronization in the message logging protocol. Previous pessimistic message logging protocols <ref> [Powell83, Borg83, Borg89] </ref> have attempted to reduce this overhead by using special-purpose hardware to assist the logging. 1.1.2 Optimistic Message Logging In contrast to pessimistic protocols, optimistic message logging protocols operate asynchronously. <p> Also, sender-based message logging requires no specialized hardware to assist with logging. The TARGON/32 system <ref> [Borg89] </ref> and its predecessor Auros [Borg83] log messages at a backup node for the receiver, using specialized networking hardware that provides three-way atomic broadcast of each message. <p> With this networking hardware assistance and using available idle time on a dedicated processor of each multiprocessor node, the overhead of providing fault tolerance in TARGON/32 has been reported to be about 10 percent <ref> [Borg89] </ref>. Sender-based message logging causes less overhead for all but the most communication-intensive programs, without the use of specialized hardware. The PUBLISHING mechanism [Powell83] proposes the use a centralized logging node for all messages, which must reliably receive every network packet. <p> All pessimistic logging protocols must prevent the system from entering a state that could cause any process other than those that failed to be rolled back during recovery. Previous pessimistic logging protocols <ref> [Borg83, Powell83, Borg89] </ref> have required each message to be logged before it is received by the destination process, blocking the receiver while the logging takes place.
Reference: [Bourne78] <author> S. R. Bourne. </author> <title> The UNIX shell. </title> <journal> The Bell System Technical Journal, </journal> <volume> 57(6) </volume> <pages> 1971-1990, </pages> <month> July-August </month> <year> 1978. </year>
Reference-contexts: This includes loading new programs and terminating existing ones when requested, scheduling the execution of programs, and maintaining a directory of currently executing programs. The exec server manages one or more command interpreters known as execs, which correspond to the shell in Unix <ref> [Bourne78] </ref>. Other standard servers include a terminal server, which controls the user interaction through the keyboard, mouse, and display of the workstation; and the exception server, which uniformly handles all exceptions and traps incurred by executing processes. Process communication is structured around a synchronous request-response protocol.
Reference: [Bubenik89] <author> Rick Bubenik and Willy Zwaenepoel. </author> <title> Performance of optimistic make. </title> <booktitle> In 1989 ACM SIGMETRICS and PERFORMANCE '89 International Conference on Measurement and Modeling of Computer Systems: Proceedings, </booktitle> <pages> pages 39-48. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1989. </year>
Reference-contexts: Optimistic message logging is a type of optimistic algorithm or optimistic computation. Optimistic algorithms have also been used in other areas, including con-currency control mechanisms [Kung81, Gherfal85], the automatic parallelization of programs for multiprocessors and distributed systems [Strom87], distributed discrete event simulation [Jefferson87, Jefferson85], software development tools <ref> [Bubenik89] </ref>, and network protocols for bulk data transfer [Carter89].
Reference: [Carter85] <author> W. C. Carter. </author> <title> Hardware fault tolerance. In Resilient Computing Systems, edited by T. </title> <editor> Anderson, </editor> <volume> chapter 2, </volume> <pages> pages 11-63. </pages> <address> Collins, London, </address> <year> 1985. </year>
Reference-contexts: Some comparison is also made to the new methods developed in this thesis, although more specific comparisons with many of these systems will be made in the following chapters as these new methods are presented. 1.4.1 Hardware Fault Tolerance Fault-tolerance methods implemented entirely in hardware <ref> [Carter85, Siewiorek86] </ref> may be able to outperform those implemented in software. Two examples of large 9 systems using hardware fault-tolerance methods are the ESS electronic telephone switching systems developed by AT&T [Clement87] and the ARPANET Pluribus IMP [Katsuki78].
Reference: [Carter89] <author> John B. Carter and Willy Zwaenepoel. </author> <title> Optimistic implementation of bulk data transfer protocols. </title> <booktitle> In 1989 ACM SIGMETRICS and PERFORMANCE '89 International Conference on Measurement and Modeling of Computer Systems: Proceedings, </booktitle> <pages> pages 61-69. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1989. </year>
Reference-contexts: Optimistic algorithms have also been used in other areas, including con-currency control mechanisms [Kung81, Gherfal85], the automatic parallelization of programs for multiprocessors and distributed systems [Strom87], distributed discrete event simulation [Jefferson87, Jefferson85], software development tools [Bubenik89], and network protocols for bulk data transfer <ref> [Carter89] </ref>.
Reference: [Chandy85] <author> K. Mani Chandy and Leslie Lamport. </author> <title> Distributed snapshots: Determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: A different style of using checkpointing alone is to periodically create a global checkpoint of the entire system, using a protocol to coordinate the checkpointing of the individual component processes. Examples of this type of global checkpointing protocol are those proposed by Chandy and Lamport <ref> [Chandy85] </ref>, and by Koo and Toueg [Koo87]. Although these protocols avoid the large overhead of checkpointing on each communication, they still may be expensive to employ and may implicitly or explicitly interfere with the underlying execution of the system. <p> A system state is called consistent if it could have been seen at some instant by an outside observer during the preceding execution of the system from its initial state, regardless of the relative speeds of the component processes <ref> [Chandy85] </ref>. After recovery from a failure, the system must be recovered to a consistent system state. This ensures that the total execution of the system is equivalent to some possible failure-free execution.
Reference: [Chandy88] <author> K. Mani Chandy. </author> <title> Theorems on computations of distributed systems. </title> <type> Technical Report Caltech-CS-TR-88-6, </type> <institution> California Institute of Technology, </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: It also does not allow any process state to become stable after the state has occurred, which is required to support optimistic message logging. Another limitation of the occurrence graph model and several other general models <ref> [Russell77, Russell80, Wood85, Chandy88] </ref> is that they do not specify how a process state becomes stable, and thus cannot be used to reason about the progress of message logging and checkpointing.
Reference: [Cheriton83] <author> David R. Cheriton and Willy Zwaenepoel. </author> <title> The distributed V kernel and its performance for diskless workstations. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 129-140. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1983. </year>
Reference-contexts: This work has been carried out in the environment of the V-System, a distributed operating system originally developed at Stanford University <ref> [Cheriton83, Cheriton84, Cheriton88] </ref>. The V-System runs on a collection of diskless SUN workstations connected by a 10-megabit per second 7 Ethernet local network [Metcalfe76, Shoch80] to a shared SUN network file server. For this implementation, version 6.0 of the V-System [Stanford86] was used, which does not support demand-paged virtual memory.
Reference: [Cheriton84] <author> David R. Cheriton. </author> <title> The V kernel: A software base for distributed systems. </title> <journal> IEEE Software, </journal> <volume> 1(2) </volume> <pages> 19-42, </pages> <month> April </month> <year> 1984. </year> <month> 118 </month>
Reference-contexts: This work has been carried out in the environment of the V-System, a distributed operating system originally developed at Stanford University <ref> [Cheriton83, Cheriton84, Cheriton88] </ref>. The V-System runs on a collection of diskless SUN workstations connected by a 10-megabit per second 7 Ethernet local network [Metcalfe76, Shoch80] to a shared SUN network file server. For this implementation, version 6.0 of the V-System [Stanford86] was used, which does not support demand-paged virtual memory.
Reference: [Cheriton85] <author> David R. Cheriton and Willy Zwaenepoel. </author> <title> Distributed process groups in the V kernel. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(2) </volume> <pages> 77-107, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: These operations all use retransmissions and different forms of acknowledgements to implement reliable packet delivery over the network. The Send operation can also be used to send a message to a group of processes <ref> [Cheriton85] </ref> or to send a message as a datagram, with no guarantee of reliable transmission. In the V-System, multiple processes on the same node may share a single address space, forming a team. <p> The checkpoint server process manages the recording of new checkpoints and the reloading of processes from their checkpoints during recovery. All logging servers in the system belong to a V-System process group <ref> [Cheriton85] </ref>, and all checkpoint servers belong to a separate process group. This use of server processes limits the increase in size and complexity of the kernel. In total, only five new primitives to support message logging and three new primitives to support checkpointing were added to the kernel. <p> It contains the SSN of the message sent, the logical host identifier of the receiver, and the new RSN value returned. It is 12 bytes long. For most messages sent, only the LoggedMessage record type is used. However, a message sent to a process group <ref> [Cheriton85] </ref> is delivered reliably to only one receiver, with delivery to other members of the group not guaranteed.
Reference: [Cheriton86a] <author> David R. Cheriton. VMTP: </author> <title> A transport protocol for the next generation of communication systems. </title> <booktitle> In Proceedings of the 1986 SigComm Symposium, </booktitle> <pages> pages 406-415. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1986. </year>
Reference-contexts: The use of this piggybacking optimization for a sequence of these request-reply exchanges is illustrated in Figure 3.4. This optimization is particularly useful in systems using a remote procedure call protocol [Birrell84] or other request-response protocol <ref> [Cheriton88, Cheriton86a] </ref>, since all communication takes place as a sequence of message exchanges.
Reference: [Cheriton86b] <author> David R. Cheriton and Michael Stumm. </author> <title> The multi-satellite star: Structuring parallel computations for a workstation cluster. </title> <type> Technical report, </type> <institution> Department of Computer Science, Stanford University, Stanford, California, </institution> <year> 1986. </year>
Reference-contexts: If a process evaluating some function fails, it can simply be restarted from the beginning with the same parameters. Examples of systems using this type of functional programming for fault tolerance include DIB [Finkel87], STARDUST [Hornig84], Lin and Keller's work [Lin86], and the multi-satellite star model <ref> [Cheriton86b] </ref>. Although these methods are straightforward, they can only be used for programs that follow the functional programming model, which may not be possible or practical for some applications. Also, since failed functions must be restarted from their beginning, the amount of reexecution 11 needed for recovery may be large.
Reference: [Cheriton88] <author> David R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: This work has been carried out in the environment of the V-System, a distributed operating system originally developed at Stanford University <ref> [Cheriton83, Cheriton84, Cheriton88] </ref>. The V-System runs on a collection of diskless SUN workstations connected by a 10-megabit per second 7 Ethernet local network [Metcalfe76, Shoch80] to a shared SUN network file server. For this implementation, version 6.0 of the V-System [Stanford86] was used, which does not support demand-paged virtual memory. <p> The use of this piggybacking optimization for a sequence of these request-reply exchanges is illustrated in Figure 3.4. This optimization is particularly useful in systems using a remote procedure call protocol [Birrell84] or other request-response protocol <ref> [Cheriton88, Cheriton86a] </ref>, since all communication takes place as a sequence of message exchanges.
Reference: [Clement87] <author> George F. Clement and Paul K. Giloth. </author> <title> Evolution of fault tolerant switching systems in AT&T. In The Evolution of Fault-Tolerant Computing, edited by A. Avizienis, </title> <editor> H. Kopetz, and J.C. Laprie, </editor> <booktitle> volume 1 of Dependable Computing and Fault-Tolerant Systems, </booktitle> <pages> pages 37-54. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Two examples of large 9 systems using hardware fault-tolerance methods are the ESS electronic telephone switching systems developed by AT&T <ref> [Clement87] </ref> and the ARPANET Pluribus IMP [Katsuki78]. Such hardware methods, though, are less flexible and cannot easily be added to existing systems.
Reference: [Cmelik88] <author> R. F. Cmelik, N. H Gehani, and W. D. Roome. </author> <title> Fault Tolerant Concurrent C: A tool for writing fault tolerant distributed programs. </title> <booktitle> In The Eighteenth Annual International Symposium on Fault-Tolerant Computing: Digest of Papers, </booktitle> <pages> pages 56-61. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: The methods of N-modular redundancy and N-version programming are special cases of this class of fault-tolerance methods. Examples of systems using active replication include the new ISIS system (ISIS 2 ) [Birman87], Circus [Cooper85, Cooper84], CHORUS [Banino85, Banino82], MP [Gait85], MARS [Kopetz85b], FT Concurrent C <ref> [Cmelik88] </ref>, PRIME [Fabry73], SIFT [Wensley78], and Yang and York's work on the Intel iAPX 432 [Yang85]. These methods are well suited for use in real-time systems, since failure recovery is essentially immediate. However, this ability requires extra processors to be dedicated to each program for its replicas.
Reference: [Cooper84] <author> Eric C. Cooper. </author> <title> Replicated procedure call. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 220-232. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1984. </year>
Reference-contexts: If one replica fails, the remaining replicas of that process continue the computation without interruption. The methods of N-modular redundancy and N-version programming are special cases of this class of fault-tolerance methods. Examples of systems using active replication include the new ISIS system (ISIS 2 ) [Birman87], Circus <ref> [Cooper85, Cooper84] </ref>, CHORUS [Banino85, Banino82], MP [Gait85], MARS [Kopetz85b], FT Concurrent C [Cmelik88], PRIME [Fabry73], SIFT [Wensley78], and Yang and York's work on the Intel iAPX 432 [Yang85]. These methods are well suited for use in real-time systems, since failure recovery is essentially immediate.
Reference: [Cooper85] <author> Eric C. Cooper. </author> <title> Replicated distributed programs. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 63-78. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1985. </year>
Reference-contexts: If one replica fails, the remaining replicas of that process continue the computation without interruption. The methods of N-modular redundancy and N-version programming are special cases of this class of fault-tolerance methods. Examples of systems using active replication include the new ISIS system (ISIS 2 ) [Birman87], Circus <ref> [Cooper85, Cooper84] </ref>, CHORUS [Banino85, Banino82], MP [Gait85], MARS [Kopetz85b], FT Concurrent C [Cmelik88], PRIME [Fabry73], SIFT [Wensley78], and Yang and York's work on the Intel iAPX 432 [Yang85]. These methods are well suited for use in real-time systems, since failure recovery is essentially immediate.
Reference: [Denning76] <author> Peter J. Denning. </author> <title> Fault tolerant operating systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 8(4) </volume> <pages> 359-389, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: Such hardware methods, though, are less flexible and cannot easily be added to existing systems. Since the fault-tolerance methods using message logging and checkpointing developed in this thesis use no specialized hardware, these hardware methods will not be discussed further. 1.4.2 Application-Specific Methods Application-specific fault-tolerance methods <ref> [Denning76, Anderson81, Shoch82] </ref> are those designed specifically for the particular program that is to use them. These designs require knowledge of both the application program and its environment. Each type of failure that can occur in the system must be anticipated, and specific solutions for each must be programmed.
Reference: [Dimmer85] <author> C. I. Dimmer. </author> <title> The Tandem Non-Stop system. In Resilient Computing Systems, edited by T. </title> <editor> Anderson, </editor> <volume> chapter 10, </volume> <pages> pages 178-196. </pages> <address> Collins, London, </address> <year> 1985. </year> <month> 119 </month>
Reference-contexts: Some systems have used ad hoc rules to determine when to checkpoint each process, effectively causing each process to checkpoint each time it communicates with another process. Examples of these systems include Eden [Almes85, Lazowska81] and the Tandem NonStop system <ref> [Bartlett81, Dimmer85] </ref>. These systems force processes 12 to checkpoint frequently, creating a large overhead for the provision of fault tolerance, as has been recognized in these two systems [Black85, Bartlett87].
Reference: [Fabry73] <author> R. S. Fabry. </author> <title> Dynamic verification of operating system decisions. </title> <journal> Communications of the ACM, </journal> <volume> 16(11) </volume> <pages> 659-668, </pages> <month> November </month> <year> 1973. </year>
Reference-contexts: The methods of N-modular redundancy and N-version programming are special cases of this class of fault-tolerance methods. Examples of systems using active replication include the new ISIS system (ISIS 2 ) [Birman87], Circus [Cooper85, Cooper84], CHORUS [Banino85, Banino82], MP [Gait85], MARS [Kopetz85b], FT Concurrent C [Cmelik88], PRIME <ref> [Fabry73] </ref>, SIFT [Wensley78], and Yang and York's work on the Intel iAPX 432 [Yang85]. These methods are well suited for use in real-time systems, since failure recovery is essentially immediate. However, this ability requires extra processors to be dedicated to each program for its replicas.
Reference: [Finkel87] <author> Raphael Finkel and Udi Manber. </author> <title> DIB|A distributed implementation of backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(2) </volume> <pages> 235-256, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: If a process evaluating some function fails, it can simply be restarted from the beginning with the same parameters. Examples of systems using this type of functional programming for fault tolerance include DIB <ref> [Finkel87] </ref>, STARDUST [Hornig84], Lin and Keller's work [Lin86], and the multi-satellite star model [Cheriton86b]. Although these methods are straightforward, they can only be used for programs that follow the functional programming model, which may not be possible or practical for some applications.
Reference: [Gait85] <author> Jason Gait. </author> <title> A distributed process manager with transparent continuation. </title> <booktitle> In Proceedings of the 5th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 422-429. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1985. </year>
Reference-contexts: The methods of N-modular redundancy and N-version programming are special cases of this class of fault-tolerance methods. Examples of systems using active replication include the new ISIS system (ISIS 2 ) [Birman87], Circus [Cooper85, Cooper84], CHORUS [Banino85, Banino82], MP <ref> [Gait85] </ref>, MARS [Kopetz85b], FT Concurrent C [Cmelik88], PRIME [Fabry73], SIFT [Wensley78], and Yang and York's work on the Intel iAPX 432 [Yang85]. These methods are well suited for use in real-time systems, since failure recovery is essentially immediate.
Reference: [Gherfal85] <author> Fawzi F. Gherfal and S. Mamrak. </author> <title> An optimistic concurrency control mechanism for an object based distributed system. </title> <booktitle> In Proceedings of the 5th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 236-245. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1985. </year>
Reference-contexts: Optimistic message logging is a type of optimistic algorithm or optimistic computation. Optimistic algorithms have also been used in other areas, including con-currency control mechanisms <ref> [Kung81, Gherfal85] </ref>, the automatic parallelization of programs for multiprocessors and distributed systems [Strom87], distributed discrete event simulation [Jefferson87, Jefferson85], software development tools [Bubenik89], and network protocols for bulk data transfer [Carter89].
Reference: [Gray79] <author> J. N. Gray. </author> <title> Notes on database operating systems. In Operating Systems: An Advanced Course, edited by R. </title> <editor> Bayer, R. M. Graham, and G. Seegmuller, </editor> <booktitle> chapter 3. F., </booktitle> <pages> pages 393-481. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Although in some cases, application-specific methods can be made to be more efficient than transparent general-purpose methods, they are limited by their lack of transparency. 1.4.3 Atomic Actions The use of atomic actions to provide fault tolerance [Lomet85] is similar to the use of atomic transactions in database systems <ref> [Gray79, Lampson81, Bernstein87] </ref>, but atomic actions can perform arbitrary operations on user-defined data types. By structuring application programs as a series of possibly nested atomic actions, any failure can be recovered from by simply forcing each failed action to abort and then reinvok-ing it from its beginning. <p> The related problems of 13 providing fault tolerance for real-time systems and applications [Hecht76, Kopetz85a, Anderson83], reintegrating the system after a network partition [Birrell82, Walker83], and maintaining the consistency and availability of static data such as file systems and databases <ref> [Gray79, Haerder83, Lampson79, Svobodova84] </ref> are also not directly addressed by this thesis. 1.6 Thesis Outline Chapter 2 presents the theoretical framework that will be used in the remainder of the thesis.
Reference: [Grit84] <author> D. H. Grit. </author> <title> Towards fault tolerance in a distributed multiprocessor. </title> <booktitle> In The Fourteenth International Conference on Fault-Tolerant Computing: Digest of Papers, </booktitle> <pages> pages 272-277. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1984. </year>
Reference-contexts: are written, which is independent of the design of the application. 1.4.4 Functional Programming The programming language model of functional programming, in which all execution is performed through the application of functions that return results and produce no side effects, can be used to facilitate the provision of fault tolerance <ref> [Grit84] </ref>. In a distributed system, these "functions" may actually be processes that are invoked by a message and return their results at completion in a message, but such processes must retain no internal state between invocations.
Reference: [Haerder83] <author> Theo Haerder and Andreas Reuter. </author> <title> Principles of transaction-oriented database recovery. </title> <journal> ACM Computing Surveys, </journal> <volume> 15(4) </volume> <pages> 287-317, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: The related problems of 13 providing fault tolerance for real-time systems and applications [Hecht76, Kopetz85a, Anderson83], reintegrating the system after a network partition [Birrell82, Walker83], and maintaining the consistency and availability of static data such as file systems and databases <ref> [Gray79, Haerder83, Lampson79, Svobodova84] </ref> are also not directly addressed by this thesis. 1.6 Thesis Outline Chapter 2 presents the theoretical framework that will be used in the remainder of the thesis.
Reference: [Haskin88] <author> Roger Haskin, Yoni Malachi, Wayne Sawdon, and Gregory Chan. </author> <title> Recovery management in QuickSilver. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 82-108, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Examples of systems using atomic actions for fault toler 10 ance include ARGUS [Liskov83, Liskov87, Liskov88], TABS [Spector85b, Spector85a], Camelot [Spector86, Spector87], the original implementation of ISIS [Birman85], Clouds [LeBlanc85, Allchin83], QuickSilver <ref> [Haskin88] </ref>, and Gutenberg [Vinter86]. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them. However, many existing applications do not fit this model, and it is not always practical to program new applications in this way.
Reference: [Hecht76] <author> H. Hecht. </author> <title> Fault-tolerant software for real-time applications. </title> <journal> ACM Computing Surveys, </journal> <volume> 8(4) </volume> <pages> 391-407, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: Likewise, methods for determining the new configuration of the system after a failure, such as finding a suitable node on which to restart any failed process, are not considered. The related problems of 13 providing fault tolerance for real-time systems and applications <ref> [Hecht76, Kopetz85a, Anderson83] </ref>, reintegrating the system after a network partition [Birrell82, Walker83], and maintaining the consistency and availability of static data such as file systems and databases [Gray79, Haerder83, Lampson79, Svobodova84] are also not directly addressed by this thesis. 1.6 Thesis Outline Chapter 2 presents the theoretical framework that will be
Reference: [Hornig84] <author> David A. Hornig. </author> <title> Automatic Partitioning and Scheduling on a Network of Personal Computers. </title> <type> Ph.D. thesis, </type> <institution> Carnegie-Mellon University, Pittsburgh, Pennsylvania, </institution> <month> November </month> <year> 1984. </year> <month> 120 </month>
Reference-contexts: If a process evaluating some function fails, it can simply be restarted from the beginning with the same parameters. Examples of systems using this type of functional programming for fault tolerance include DIB [Finkel87], STARDUST <ref> [Hornig84] </ref>, Lin and Keller's work [Lin86], and the multi-satellite star model [Cheriton86b]. Although these methods are straightforward, they can only be used for programs that follow the functional programming model, which may not be possible or practical for some applications.
Reference: [Horning74] <author> J. J. Horning, H. C. Lauer, P. M. Melliar-Smith, and B. Randell. </author> <title> A program structure for error detection and recovery. In Operating Systems, edited by E. </title> <editor> Gelenbe and C. Kaiser, </editor> <booktitle> volume 16 of Lecture Notes in Computer Science, </booktitle> <pages> pages 171-187. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1974. </year> <note> Also reprinted in Reliable Computer Systems, </note> <editor> edited by Santosh K. </editor> <booktitle> Shrivastava, </booktitle> <pages> pages 53-68, </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: These designs require knowledge of both the application program and its environment. Each type of failure that can occur in the system must be anticipated, and specific solutions for each must be programmed. Implementation of these methods may follow some general structure such as the use of recovery blocks <ref> [Horning74, Lee78, Randell75] </ref>, or may be structured specially for each application program. However, these methods are nontransparent, and thus require existing programs to be carefully modified or rewritten in order to be fault-tolerant.
Reference: [Jefferson85] <author> David R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Optimistic message logging is a type of optimistic algorithm or optimistic computation. Optimistic algorithms have also been used in other areas, including con-currency control mechanisms [Kung81, Gherfal85], the automatic parallelization of programs for multiprocessors and distributed systems [Strom87], distributed discrete event simulation <ref> [Jefferson87, Jefferson85] </ref>, software development tools [Bubenik89], and network protocols for bulk data transfer [Carter89].
Reference: [Jefferson87] <author> David Jefferson, Brian Beckman, Fred Wieland, Leo Blume, Mike DiLoreto, Phil Hontalas, Pierre Laroche, Kathy Sturdevant, Jack Tupman, Van Warren, John Wedel, Herb Younger, and Steve Bellenot. </author> <title> Distributed simulation and the Time Warp operating system. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 77-93. </pages> <publisher> ACM, </publisher> <month> November </month> <year> 1987. </year>
Reference-contexts: Optimistic message logging is a type of optimistic algorithm or optimistic computation. Optimistic algorithms have also been used in other areas, including con-currency control mechanisms [Kung81, Gherfal85], the automatic parallelization of programs for multiprocessors and distributed systems [Strom87], distributed discrete event simulation <ref> [Jefferson87, Jefferson85] </ref>, software development tools [Bubenik89], and network protocols for bulk data transfer [Carter89].
Reference: [Johnson87] <author> David B. Johnson and Willy Zwaenepoel. </author> <title> Sender-based message logging. </title> <booktitle> In The Seventeenth Annual International Symposium on Fault-Tolerant Computing: Digest of Papers, </booktitle> <pages> pages 14-19. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: Strom and Yemini's Optimistic Recovery mechanism [Strom85] logs all messages on stable storage on disk, but Strom, Bacon, and Yemini have recently proposed enhancements to it using ideas from sender-based message logging <ref> [Johnson87] </ref> to avoid logging some messages on stable storage [Strom88]. Another difference between sender-based message logging and previous pessimistic logging protocols, which is not related to the logging of messages at the sender, is in the enforcement of the requirements of a pessimistic logging protocol.
Reference: [Johnson88] <author> David B. Johnson and Willy Zwaenepoel. </author> <title> Recovery in distributed systems using optimistic message logging and checkpointing. </title> <booktitle> In Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 171-181. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1988. </year>
Reference-contexts: in this thesis has been partially motivated by Strom and Yemini's Optimistic Recovery system [Strom85], and recently Sistla and Welch have proposed a new method using optimistic message logging [Sistla89], based in part on some aspects of both Strom and Yemini's system and on an earlier version of this work <ref> [Johnson88] </ref>. The system in this chapter is unique among these in that it always finds the maximum recoverable system state. Although these other systems occasionally checkpoint processes as this system does, they do not consider the checkpoints in finding the current recovery state of the system.
Reference: [Katsuki78] <author> D. Katsuki, E.S. Elsam, W.F. Mann, E.S. Roberts, J.G. Robinson, </author> <title> F.S. Skowronski, and E.W. Wolf. Pluribus|An operational fault-tolerant multiprocessor. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 66(10) </volume> <pages> 1146-1159, </pages> <month> October </month> <year> 1978. </year>
Reference-contexts: Two examples of large 9 systems using hardware fault-tolerance methods are the ESS electronic telephone switching systems developed by AT&T [Clement87] and the ARPANET Pluribus IMP <ref> [Katsuki78] </ref>. Such hardware methods, though, are less flexible and cannot easily be added to existing systems.
Reference: [Koo87] <author> Richard Koo and Sam Toueg. </author> <title> Checkpointing and rollback-recovery for distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):23-31, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: Examples of this type of global checkpointing protocol are those proposed by Chandy and Lamport [Chandy85], and by Koo and Toueg <ref> [Koo87] </ref>. Although these protocols avoid the large overhead of checkpointing on each communication, they still may be expensive to employ and may implicitly or explicitly interfere with the underlying execution of the system.
Reference: [Kopetz85a] <author> H. Kopetz. </author> <title> Resilient real-time systems. In Resilient Computing Systems, edited by T. </title> <editor> Anderson, </editor> <volume> chapter 5, </volume> <pages> pages 91-101. </pages> <address> Collins, London, </address> <year> 1985. </year> <month> 121 </month>
Reference-contexts: Likewise, methods for determining the new configuration of the system after a failure, such as finding a suitable node on which to restart any failed process, are not considered. The related problems of 13 providing fault tolerance for real-time systems and applications <ref> [Hecht76, Kopetz85a, Anderson83] </ref>, reintegrating the system after a network partition [Birrell82, Walker83], and maintaining the consistency and availability of static data such as file systems and databases [Gray79, Haerder83, Lampson79, Svobodova84] are also not directly addressed by this thesis. 1.6 Thesis Outline Chapter 2 presents the theoretical framework that will be
Reference: [Kopetz85b] <author> H. Kopetz and W. Merker. </author> <title> The architecture of MARS. </title> <booktitle> In The Fifteenth Annual International Symposium on Fault-Tolerant Computing: Digest of Papers, </booktitle> <pages> pages 274-279. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1985. </year>
Reference-contexts: The methods of N-modular redundancy and N-version programming are special cases of this class of fault-tolerance methods. Examples of systems using active replication include the new ISIS system (ISIS 2 ) [Birman87], Circus [Cooper85, Cooper84], CHORUS [Banino85, Banino82], MP [Gait85], MARS <ref> [Kopetz85b] </ref>, FT Concurrent C [Cmelik88], PRIME [Fabry73], SIFT [Wensley78], and Yang and York's work on the Intel iAPX 432 [Yang85]. These methods are well suited for use in real-time systems, since failure recovery is essentially immediate.
Reference: [Kung81] <author> H. T. Kung and J. T. Robinson. </author> <title> On optimistic methods for concur-rency control. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 2(2) </volume> <pages> 213-226, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Optimistic message logging is a type of optimistic algorithm or optimistic computation. Optimistic algorithms have also been used in other areas, including con-currency control mechanisms <ref> [Kung81, Gherfal85] </ref>, the automatic parallelization of programs for multiprocessors and distributed systems [Strom87], distributed discrete event simulation [Jefferson87, Jefferson85], software development tools [Bubenik89], and network protocols for bulk data transfer [Carter89].
Reference: [Lamport78] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Since there is no single notion of "real time" in a distributed system, these dependencies, together with the linear ordering of events within each process, define a partial ordering on the events within the system <ref> [Lamport78] </ref>. This model describes the state of a process by its current dependencies, and describes the state of the system as a collection of process states. 15 16 2.1.1 Process States The execution of each process is divided into discrete intervals by the messages that the process receives. <p> Although similar, the system history relation differs from Lamport's happened before relation <ref> [Lamport78] </ref> in that it orders the system states that result from events rather than the events themselves, and in that only process state intervals (started by the receipt of a message) constitute events. To illustrate this partial order, Figure 2.1 shows a system of four communicating processes.
Reference: [Lampson79] <author> Butler W. Lampson and Howard E. Sturgis. </author> <title> Crash recovery in a distributed data storage system. </title> <type> Technical report, </type> <institution> Xerox Palo Alto Research Center, Palo Alto, California, </institution> <month> April </month> <year> 1979. </year>
Reference-contexts: Each process is checkpointed individually, and no coordination is required between the checkpointing of different processes. The logged messages and checkpoints are stored in some way that survives any failures that the system is intended to recover from, such as by writing them to stable storage on disk <ref> [Lampson79, Bernstein87] </ref>. Recovery of a failed process using these logged messages and checkpoints is based on the assumption that the execution of the process is deterministic between received input messages. <p> The current state of a process is thus completely determined by its starting state and the sequence of messages it has received. * No global clock is available in the system. * The network includes a shared stable storage service <ref> [Lampson79, Bernstein87] </ref> that is always accessible to all active nodes in the system. * Packet delivery on the network need not be guaranteed, but reliable delivery of a packet can be achieved by retransmitting it a limited number of times until an acknowledgement arrives from its destination. * The network protocol <p> The related problems of 13 providing fault tolerance for real-time systems and applications [Hecht76, Kopetz85a, Anderson83], reintegrating the system after a network partition [Birrell82, Walker83], and maintaining the consistency and availability of static data such as file systems and databases <ref> [Gray79, Haerder83, Lampson79, Svobodova84] </ref> are also not directly addressed by this thesis. 1.6 Thesis Outline Chapter 2 presents the theoretical framework that will be used in the remainder of the thesis. <p> Previous message logging protocols send an extra copy of each message elsewhere for logging, either to stable storage <ref> [Lampson79, Bernstein87] </ref> on disk or to some special backup process that can survive the failure of the receiver process.
Reference: [Lampson81] <author> Butler W. Lampson. </author> <title> Atomic transactions. In Distributed Systems: Architecture and Implementation, edited by B. </title> <editor> W. Lampson, M. Paul, and H. J. Siegert, </editor> <volume> chapter 11, </volume> <pages> pages 246-265. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Although in some cases, application-specific methods can be made to be more efficient than transparent general-purpose methods, they are limited by their lack of transparency. 1.4.3 Atomic Actions The use of atomic actions to provide fault tolerance [Lomet85] is similar to the use of atomic transactions in database systems <ref> [Gray79, Lampson81, Bernstein87] </ref>, but atomic actions can perform arbitrary operations on user-defined data types. By structuring application programs as a series of possibly nested atomic actions, any failure can be recovered from by simply forcing each failed action to abort and then reinvok-ing it from its beginning.
Reference: [Lazowska81] <author> Edward D. Lazowska, Henry M. Levy, Guy T. Almes, Michael J. Fischer, Robert J. Fowler, and Stephen C. Vestal. </author> <title> The architecture of the Eden system. </title> <booktitle> In Proceedings of the Eighth Symposium on Operating Systems Principles, </booktitle> <pages> pages 148-159. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1981. </year>
Reference-contexts: Some systems have used ad hoc rules to determine when to checkpoint each process, effectively causing each process to checkpoint each time it communicates with another process. Examples of these systems include Eden <ref> [Almes85, Lazowska81] </ref> and the Tandem NonStop system [Bartlett81, Dimmer85]. These systems force processes 12 to checkpoint frequently, creating a large overhead for the provision of fault tolerance, as has been recognized in these two systems [Black85, Bartlett87].
Reference: [LeBlanc85] <author> Richard J. LeBlanc and C. Thomas Wilkes. </author> <title> Systems programming with objects and actions. </title> <booktitle> In Proceedings of the 5th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 132-139. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1985. </year>
Reference-contexts: Examples of systems using atomic actions for fault toler 10 ance include ARGUS [Liskov83, Liskov87, Liskov88], TABS [Spector85b, Spector85a], Camelot [Spector86, Spector87], the original implementation of ISIS [Birman85], Clouds <ref> [LeBlanc85, Allchin83] </ref>, QuickSilver [Haskin88], and Gutenberg [Vinter86]. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them. However, many existing applications do not fit this model, and it is not always practical to program new applications in this way.
Reference: [Lee78] <author> P. A. Lee. </author> <title> A reconsideration of the recovery block scheme. </title> <journal> The Computer Journal, </journal> <volume> 21(4) </volume> <pages> 306-310, </pages> <month> November </month> <year> 1978. </year> <note> Also reprinted in Reliable Computer Systems, </note> <editor> edited by Santosh K. </editor> <booktitle> Shrivastava, </booktitle> <pages> pages 69-79, </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: These designs require knowledge of both the application program and its environment. Each type of failure that can occur in the system must be anticipated, and specific solutions for each must be programmed. Implementation of these methods may follow some general structure such as the use of recovery blocks <ref> [Horning74, Lee78, Randell75] </ref>, or may be structured specially for each application program. However, these methods are nontransparent, and thus require existing programs to be carefully modified or rewritten in order to be fault-tolerant.
Reference: [Lin86] <author> Frank C. H. Lin and Robert M. Keller. </author> <title> Distributed recovery in applicative systems. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <editor> edited by Kai Hwang, Steven M. 122 Jacobs, and Earl E. </editor> <booktitle> Swartzlander, </booktitle> <pages> pages 405-412. </pages> <publisher> IEEE Computer Society, </publisher> <month> August </month> <year> 1986. </year>
Reference-contexts: If a process evaluating some function fails, it can simply be restarted from the beginning with the same parameters. Examples of systems using this type of functional programming for fault tolerance include DIB [Finkel87], STARDUST [Hornig84], Lin and Keller's work <ref> [Lin86] </ref>, and the multi-satellite star model [Cheriton86b]. Although these methods are straightforward, they can only be used for programs that follow the functional programming model, which may not be possible or practical for some applications.
Reference: [Liskov83] <author> Barbara Liskov and Robert Scheifler. </author> <title> Guardians and actions: Linguistic support for robust, distributed programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 381-404, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: By structuring application programs as a series of possibly nested atomic actions, any failure can be recovered from by simply forcing each failed action to abort and then reinvok-ing it from its beginning. Examples of systems using atomic actions for fault toler 10 ance include ARGUS <ref> [Liskov83, Liskov87, Liskov88] </ref>, TABS [Spector85b, Spector85a], Camelot [Spector86, Spector87], the original implementation of ISIS [Birman85], Clouds [LeBlanc85, Allchin83], QuickSilver [Haskin88], and Gutenberg [Vinter86]. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them.
Reference: [Liskov87] <author> Barbara Liskov, Dorothy Curtis, Paul Johnson, and Robert Scheifler. </author> <title> Implementation of Argus. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 111-122. </pages> <publisher> ACM, </publisher> <month> November </month> <year> 1987. </year>
Reference-contexts: By structuring application programs as a series of possibly nested atomic actions, any failure can be recovered from by simply forcing each failed action to abort and then reinvok-ing it from its beginning. Examples of systems using atomic actions for fault toler 10 ance include ARGUS <ref> [Liskov83, Liskov87, Liskov88] </ref>, TABS [Spector85b, Spector85a], Camelot [Spector86, Spector87], the original implementation of ISIS [Birman85], Clouds [LeBlanc85, Allchin83], QuickSilver [Haskin88], and Gutenberg [Vinter86]. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them.
Reference: [Liskov88] <author> Barbara Liskov. </author> <title> Distributed programming in Argus. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 300-312, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: By structuring application programs as a series of possibly nested atomic actions, any failure can be recovered from by simply forcing each failed action to abort and then reinvok-ing it from its beginning. Examples of systems using atomic actions for fault toler 10 ance include ARGUS <ref> [Liskov83, Liskov87, Liskov88] </ref>, TABS [Spector85b, Spector85a], Camelot [Spector86, Spector87], the original implementation of ISIS [Birman85], Clouds [LeBlanc85, Allchin83], QuickSilver [Haskin88], and Gutenberg [Vinter86]. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them.
Reference: [Lomet85] <author> D. B. Lomet. </author> <title> Process structuring, synchronization, and recovery using atomic actions. In Reliable Computer Systems, edited by Santosh K. </title> <booktitle> Shrivastava, </booktitle> <pages> pages 249-265. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year> <journal> Reprinted from ACM SIGPLAN Notices, </journal> <volume> 12(3) </volume> <pages> 128-137, </pages> <month> March </month> <year> 1977. </year>
Reference-contexts: Although in some cases, application-specific methods can be made to be more efficient than transparent general-purpose methods, they are limited by their lack of transparency. 1.4.3 Atomic Actions The use of atomic actions to provide fault tolerance <ref> [Lomet85] </ref> is similar to the use of atomic transactions in database systems [Gray79, Lampson81, Bernstein87], but atomic actions can perform arbitrary operations on user-defined data types.
Reference: [Merlin78] <author> P. M. Merlin and B. Randell. </author> <title> State restoration in distributed systems. </title> <booktitle> In The Eighth International Conference on Fault-Tolerant Computing: Digest of Papers, </booktitle> <pages> pages 129-134. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1978. </year> <note> Also reprinted in Reliable Computer Systems, </note> <editor> edited by Santosh K. </editor> <booktitle> Shrivastava, </booktitle> <pages> pages 435-447, </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: The increased generality of not requiring these properties leads to a more powerful model for reasoning about these systems. A number of other recovery models have been developed, although none explicitly for systems using message logging and checkpointing. The occurrence graph model <ref> [Merlin78] </ref> is very general, but does not include deterministic process execution. It also does not allow any process state to become stable after the state has occurred, which is required to support optimistic message logging.
Reference: [Metcalfe76] <author> Robert M. Metcalfe and David R. Boggs. </author> <title> Ethernet: Distributed packet switching for local computer networks. </title> <journal> Communications of the ACM, </journal> <volume> 19(7) </volume> <pages> 395-404, </pages> <month> July </month> <year> 1976. </year>
Reference-contexts: This work has been carried out in the environment of the V-System, a distributed operating system originally developed at Stanford University [Cheriton83, Cheriton84, Cheriton88]. The V-System runs on a collection of diskless SUN workstations connected by a 10-megabit per second 7 Ethernet local network <ref> [Metcalfe76, Shoch80] </ref> to a shared SUN network file server. For this implementation, version 6.0 of the V-System [Stanford86] was used, which does not support demand-paged virtual memory.
Reference: [Mohan86] <author> C. Mohan, B. Lindsay, and R. Obermarck. </author> <title> Transaction management in the R fl distributed database management system. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 11(4) </volume> <pages> 378-396, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: However, this extra logging can be reduced through the use of special commit protocols, such as the Presumed Commit and Presumed Abort protocols <ref> [Mohan86] </ref>. 107 4.8 Summary This chapter has presented a new transparent optimistic message logging system that guarantees to find the maximum possible recoverable state in the system. This is achieved by utilizing all logged messages and checkpoints in forming recoverable system states, using the model presented in Chapter 2.
Reference: [Pausch88] <author> Randy Pausch. </author> <title> Adding Input and Output to the Transactional Model. </title> <type> Ph.D. thesis, </type> <institution> Carnegie-Mellon Univerisity, Pittsburgh, Pennsylvania, </institution> <month> August </month> <year> 1988. </year> <note> Also available as Technical Report CMU-CS-88-171, </note> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> August </month> <year> 1988. </year> <month> 123 </month>
Reference-contexts: The treatment of input and output with the outside world in this model is similar to the treatment used in Strom and Yemini's system, and to that proposed by Pausch in extending the transactional model to include outside world interactions <ref> [Pausch88] </ref>. For each process, Strom and Yemini define an input boundary function, which logs all process input as messages, and an output boundary function, which holds all process output until it is known that the state interval from which it was sent will never be rolled back. <p> Using shorter actions or transactions can alleviate this delay to some degree, but this increases the overhead caused by recording the 32 recovery information on stable storage. In addition to output to a database or to the file system, Pausch <ref> [Pausch88] </ref> has extended transactions to support output to the outside world. In systems using global checkpointing without message logging (Section 1.4.6), a new global checkpoint must be created before any output can be committed.
Reference: [Powell83] <author> Michael L. Powell and David L. Presotto. </author> <title> Publishing: A reliable broadcast communication mechanism. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 100-109. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1983. </year>
Reference-contexts: These protocols are called "pessimistic" because they assume that a failure could occur at any time, possibly before the needed logging is completed. Previous pessimistic message logging protocols <ref> [Powell83, Borg83, Borg89] </ref> have achieved this guarantee by blocking a process when it receives a message, until that message has been logged. This ensures that if a process fails, all messages received by it since its last checkpoint are logged, regardless of when the failure occurs. <p> The main drawback, however, of pessimistic message logging is the performance degradation caused by the synchronization in the message logging protocol. Previous pessimistic message logging protocols <ref> [Powell83, Borg83, Borg89] </ref> have attempted to reduce this overhead by using special-purpose hardware to assist the logging. 1.1.2 Optimistic Message Logging In contrast to pessimistic protocols, optimistic message logging protocols operate asynchronously. <p> Sender-based message logging causes less overhead for all but the most communication-intensive programs, without the use of specialized hardware. The PUBLISHING mechanism <ref> [Powell83] </ref> proposes the use a centralized logging node for all messages, which must reliably receive every network packet. <p> All pessimistic logging protocols must prevent the system from entering a state that could cause any process other than those that failed to be rolled back during recovery. Previous pessimistic logging protocols <ref> [Borg83, Powell83, Borg89] </ref> have required each message to be logged before it is received by the destination process, blocking the receiver while the logging takes place.
Reference: [Randell75] <author> Brian Randell. </author> <title> System structure for software fault tolerance. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1(2):220-232, </volume> <month> June </month> <year> 1975. </year>
Reference-contexts: These designs require knowledge of both the application program and its environment. Each type of failure that can occur in the system must be anticipated, and specific solutions for each must be programmed. Implementation of these methods may follow some general structure such as the use of recovery blocks <ref> [Horning74, Lee78, Randell75] </ref>, or may be structured specially for each application program. However, these methods are nontransparent, and thus require existing programs to be carefully modified or rewritten in order to be fault-tolerant. <p> Thus system state R itself must remain recoverable. Since the set R forms a lattice, any new current recovery state R 0 established after state R must be greater than R. The domino effect <ref> [Randell75, Russell80] </ref> is a well known problem that can occur in attempting to recover the state of a distributed system, and must be avoided to guarantee progress in the system in spite of failures. An example of how the domino effect can occur is illustrated in Figure 2.3.
Reference: [Russell77] <author> David L. Russell. </author> <title> Process backup in producer-consumer systems. </title> <booktitle> In Proceedings of the Sixth Symposium on Operating Systems Principles, </booktitle> <pages> pages 151-157. </pages> <publisher> ACM, </publisher> <month> November </month> <year> 1977. </year>
Reference-contexts: It also does not allow any process state to become stable after the state has occurred, which is required to support optimistic message logging. Another limitation of the occurrence graph model and several other general models <ref> [Russell77, Russell80, Wood85, Chandy88] </ref> is that they do not specify how a process state becomes stable, and thus cannot be used to reason about the progress of message logging and checkpointing.
Reference: [Russell80] <author> David L. Russell. </author> <title> State restoration in systems of communicating processes. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-6(2):183-194, </volume> <month> March </month> <year> 1980. </year>
Reference-contexts: Thus system state R itself must remain recoverable. Since the set R forms a lattice, any new current recovery state R 0 established after state R must be greater than R. The domino effect <ref> [Randell75, Russell80] </ref> is a well known problem that can occur in attempting to recover the state of a distributed system, and must be avoided to guarantee progress in the system in spite of failures. An example of how the domino effect can occur is illustrated in Figure 2.3. <p> It also does not allow any process state to become stable after the state has occurred, which is required to support optimistic message logging. Another limitation of the occurrence graph model and several other general models <ref> [Russell77, Russell80, Wood85, Chandy88] </ref> is that they do not specify how a process state becomes stable, and thus cannot be used to reason about the progress of message logging and checkpointing.
Reference: [Saltzer84] <author> J. H. Saltzer, D. P. Reed, and D. D. Clark. </author> <title> End-to-end arguments in system design. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4) </volume> <pages> 277-288, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Although this logging node avoids the need to send an additional copy of each message over the network for logging, providing this reliability guarantee seems to be impractical without additional protocol complexity <ref> [Saltzer84] </ref>. Strom and Yemini's Optimistic Recovery mechanism [Strom85] logs all messages on stable storage on disk, but Strom, Bacon, and Yemini have recently proposed enhancements to it using ideas from sender-based message logging [Johnson87] to avoid logging some messages on stable storage [Strom88].
Reference: [Schlichting83] <author> Richard D. Schlichting and Fred B. Schneider. </author> <title> Fail-stop processors: An approach to designing fault-tolerant distributed computing systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(3) </volume> <pages> 222-238, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: The following assumptions about the underlying distributed system are made: * The system is composed of a network of fail-stop processors <ref> [Schlichting83] </ref>. A fail-stop processor immediately halts whenever any failure of the processor occurs; it thus never produces incorrect output because of any failure. * Processes communicate with one another only through messages. 6 * The execution of each process in the system is deterministic between received input messages.
Reference: [Schneider87] <author> Fred B. Schneider. </author> <title> The state machine approach: A tutorial. </title> <type> Technical Report TR 86-800, </type> <institution> Cornell University, </institution> <address> Ithaca, New York, </address> <month> June </month> <year> 1987. </year> <booktitle> To appear in Proceedings of a Workshop on Fault-Tolerant Distributed Computing, Lecture Notes in Computer Science series, </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: The basic assumption behind this model, that processes execute deterministically based only on their starting state and on the sequence of messages that they receive, has been called the state machine approach <ref> [Schneider87] </ref>. A state machine models a process as a set of state variables and a set of commands that operate on those variables. Each command executes deterministically, and atomically transforms the variables of the state machine to a set of new values.
Reference: [Shoch80] <author> John F. Shoch and Jon A. Hupp. </author> <title> Measured performance of an Ethernet local network. </title> <journal> Communications of the ACM, </journal> <volume> 23(12) </volume> <pages> 711-721, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: This work has been carried out in the environment of the V-System, a distributed operating system originally developed at Stanford University [Cheriton83, Cheriton84, Cheriton88]. The V-System runs on a collection of diskless SUN workstations connected by a 10-megabit per second 7 Ethernet local network <ref> [Metcalfe76, Shoch80] </ref> to a shared SUN network file server. For this implementation, version 6.0 of the V-System [Stanford86] was used, which does not support demand-paged virtual memory.
Reference: [Shoch82] <author> John F. Shoch and Jon A. Hupp. </author> <title> The "worm" programs|Early experience with a distributed computation. </title> <journal> Communications of the ACM, </journal> <volume> 25(3) </volume> <pages> 172-180, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Such hardware methods, though, are less flexible and cannot easily be added to existing systems. Since the fault-tolerance methods using message logging and checkpointing developed in this thesis use no specialized hardware, these hardware methods will not be discussed further. 1.4.2 Application-Specific Methods Application-specific fault-tolerance methods <ref> [Denning76, Anderson81, Shoch82] </ref> are those designed specifically for the particular program that is to use them. These designs require knowledge of both the application program and its environment. Each type of failure that can occur in the system must be anticipated, and specific solutions for each must be programmed.
Reference: [Shrivastava82] <author> S. K. Shrivastava. </author> <title> A dependency, commitment and recovery model for atomic actions. </title> <booktitle> In Proceedings of the Second Symposium on 124 Reliability in Distributed Software and Database Systems, </booktitle> <pages> pages 112-119. </pages> <publisher> IEEE Computer Society, </publisher> <month> July </month> <year> 1982. </year> <note> Also reprinted in Reliable Computer Systems, </note> <editor> edited by Santosh K. </editor> <booktitle> Shrivastava, </booktitle> <pages> pages 485-497, </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Other more specialized models include those to support atomic transactions <ref> [Best81, Shrivastava82, Skeen83] </ref>, real-time systems [Anderson83], and centralized systems providing recovery at multiple independent levels [Anderson78]. Many of these models also make assumptions about the underlying system, such as assuming that communication is reliable or that communication and execution are atomic, in order to simplify the properties of the model.
Reference: [Siewiorek86] <author> D. Siewiorek. </author> <title> Architecture of fault-tolerant computers. In Fault-Tolerant Computing: Theory and Techniques, edited by Dhiraj K. </title> <journal> Pradhan, </journal> <volume> volume 2, chapter 6, </volume> <pages> pages 417-466. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1986. </year>
Reference-contexts: Some comparison is also made to the new methods developed in this thesis, although more specific comparisons with many of these systems will be made in the following chapters as these new methods are presented. 1.4.1 Hardware Fault Tolerance Fault-tolerance methods implemented entirely in hardware <ref> [Carter85, Siewiorek86] </ref> may be able to outperform those implemented in software. Two examples of large 9 systems using hardware fault-tolerance methods are the ESS electronic telephone switching systems developed by AT&T [Clement87] and the ARPANET Pluribus IMP [Katsuki78].
Reference: [Sistla89] <author> A. Prasad Sistla and Jennifer L. Welch. </author> <title> Efficient distributed recovery using message logging. </title> <booktitle> In Proceedings of the Eighth Annual ACM Symposium on Principles of Distributed Computing. ACM, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: Although several previous optimistic message logging protocols have been proposed <ref> [Strom85, Strom88, Sistla89] </ref>, none of these protocols has been implemented, and thus their actual performance cannot be evaluated. Optimistic message logging is a type of optimistic algorithm or optimistic computation. <p> Thus, in their definitions of a consistent system state, Strom and Yemini [Strom85] require all messages sent to have been received, and Sistla and Welch <ref> [Sistla89] </ref> require the sequence of messages received on each channel to be a prefix of those sent on it. Since this model does not assume reliable delivery, it can be applied to distributed systems that do not guarantee reliable delivery, such as those based on an Ethernet network. <p> The work in this thesis has been partially motivated by Strom and Yemini's Optimistic Recovery system [Strom85], and recently Sistla and Welch have proposed a new method using optimistic message logging <ref> [Sistla89] </ref>, based in part on some aspects of both Strom and Yemini's system and on an earlier version of this work [Johnson88]. The system in this chapter is unique among these in that it always finds the maximum recoverable system state. <p> Sistla and Welch have proposed two alternative recovery algorithms based on optimistic message logging <ref> [Sistla89] </ref>. One algorithm tags each message sent with a transitive dependency vector as in Strom and Yemini's system, whereas the other algorithm tags each message only with the sender's current state interval index as in 106 the system of this chapter. <p> This is achieved by utilizing all logged messages and checkpoints in forming recoverable system states, using the model presented in Chapter 2. Previous systems using optimistic message logging and checkpointing <ref> [Strom85, Sistla89] </ref> have considered only logged messages in forming recoverable system states, and thus may not find the maximum possible state. Also, but utilizing the checkpointed states, some messages received by a process before it was checkpointed may not need to be logged.
Reference: [Skeen83] <author> Dale Skeen and Michael Stonebraker. </author> <title> A formal model of crash recovery in a distributed system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-9(3):219-228, </volume> <month> May </month> <year> 1983. </year>
Reference-contexts: Other more specialized models include those to support atomic transactions <ref> [Best81, Shrivastava82, Skeen83] </ref>, real-time systems [Anderson83], and centralized systems providing recovery at multiple independent levels [Anderson78]. Many of these models also make assumptions about the underlying system, such as assuming that communication is reliable or that communication and execution are atomic, in order to simplify the properties of the model.
Reference: [Spector85a] <author> Alfred Z. Spector, Jacob Butcher, Dean S. Daniels, Daniel J. Duchamp, Jeffrey L. Eppinger, Charles E. Fineman, Abdelsalam Heddaya, and Peter M. Schwarz. </author> <title> Support for distributed transactions in the TABS prototype. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(6):520-530, </volume> <month> June </month> <year> 1985. </year>
Reference-contexts: Examples of systems using atomic actions for fault toler 10 ance include ARGUS [Liskov83, Liskov87, Liskov88], TABS <ref> [Spector85b, Spector85a] </ref>, Camelot [Spector86, Spector87], the original implementation of ISIS [Birman85], Clouds [LeBlanc85, Allchin83], QuickSilver [Haskin88], and Gutenberg [Vinter86]. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them.
Reference: [Spector85b] <author> Alfred Z. Spector, Dean Daniels, Daniel Duchamp, Jeffrey L. Eppinger, and Randy Pausch. </author> <title> Distributed transactions for reliable systems. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 127-146. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1985. </year>
Reference-contexts: Examples of systems using atomic actions for fault toler 10 ance include ARGUS [Liskov83, Liskov87, Liskov88], TABS <ref> [Spector85b, Spector85a] </ref>, Camelot [Spector86, Spector87], the original implementation of ISIS [Birman85], Clouds [LeBlanc85, Allchin83], QuickSilver [Haskin88], and Gutenberg [Vinter86]. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them.
Reference: [Spector86] <author> Alfred Z. Spector, Joshua J. Bloch, Dean S. Daniels, Richard P. Draves, Dan Duchamp, Jeffrey L. Eppinger, Sherri G. Menees, and Dean S. Thompson. </author> <title> The Camelot project. </title> <type> Technical Report CMU-CS-86-166, </type> <institution> Department of Computer Science, Carnegie-Mellon University, Pittsburgh, Pennsylvania, </institution> <month> November </month> <year> 1986. </year>
Reference-contexts: Examples of systems using atomic actions for fault toler 10 ance include ARGUS [Liskov83, Liskov87, Liskov88], TABS [Spector85b, Spector85a], Camelot <ref> [Spector86, Spector87] </ref>, the original implementation of ISIS [Birman85], Clouds [LeBlanc85, Allchin83], QuickSilver [Haskin88], and Gutenberg [Vinter86]. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them.
Reference: [Spector87] <author> Alfred Z. Spector. </author> <title> Distributed transaction processing and the Camelot system. In Distributed Operating Systems: Theory and Practice, edited by Yakup Paker, </title> <editor> Jean-Pierre Banatre, and Muslim Bozyigit, </editor> <booktitle> volume 28 of NATO Advanced Science Institute Series F: Computer and Systems Sciences, </booktitle> <pages> pages 331-353. </pages> <publisher> Springer-Verlag, </publisher> <address> 125 Berlin, </address> <year> 1987. </year> <note> Also available as Technical Report CMU-CS-87-100, </note> <institution> Department of Computer Science, Carnegie-Mellon University, Pittsburgh, Pennsylvania, </institution> <month> January </month> <year> 1987. </year>
Reference-contexts: Examples of systems using atomic actions for fault toler 10 ance include ARGUS [Liskov83, Liskov87, Liskov88], TABS [Spector85b, Spector85a], Camelot <ref> [Spector86, Spector87] </ref>, the original implementation of ISIS [Birman85], Clouds [LeBlanc85, Allchin83], QuickSilver [Haskin88], and Gutenberg [Vinter86]. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them.
Reference: [Stanford86] <institution> V-System Development Group, Computer Systems Laboratory, Departments of Computer Science and Electrical Engineering, Stanford University, Stanford, California. </institution> <note> V-System 6.0 Reference Manual, </note> <month> May </month> <year> 1986. </year>
Reference-contexts: The V-System runs on a collection of diskless SUN workstations connected by a 10-megabit per second 7 Ethernet local network [Metcalfe76, Shoch80] to a shared SUN network file server. For this implementation, version 6.0 of the V-System <ref> [Stanford86] </ref> was used, which does not support demand-paged virtual memory. Although this implementation has involved a number of system-specific issues, many of these issues would be important with any choice of implementation environment, and many of the solutions used are applicable over a wide range of similar target systems.
Reference: [Strom85] <author> Robert E. Strom and Shaula Yemini. </author> <title> Optimistic recovery in distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Although several previous optimistic message logging protocols have been proposed <ref> [Strom85, Strom88, Sistla89] </ref>, none of these protocols has been implemented, and thus their actual performance cannot be evaluated. Optimistic message logging is a type of optimistic algorithm or optimistic computation. <p> Proof Follows directly from Lemma 2.1 and Definitions 2.1 and 2.5. 2.2 Related Work The dependency vectors used in this model are similar to those used by Strom and Yemini in their Optimistic Recovery system <ref> [Strom85] </ref>. However, Strom and Yemini's dependency vectors contain a complete transitive closure of all dependencies of the process, rather than just the direct dependencies from messages received by the process. <p> Previous optimistic systems have used a model in which a separate channel connects each pair of processes, such that the channel does not lose or reorder messages. Thus, in their definitions of a consistent system state, Strom and Yemini <ref> [Strom85] </ref> require all messages sent to have been received, and Sistla and Welch [Sistla89] require the sequence of messages received on each channel to be a prefix of those sent on it. <p> Although this logging node avoids the need to send an additional copy of each message over the network for logging, providing this reliability guarantee seems to be impractical without additional protocol complexity [Saltzer84]. Strom and Yemini's Optimistic Recovery mechanism <ref> [Strom85] </ref> logs all messages on stable storage on disk, but Strom, Bacon, and Yemini have recently proposed enhancements to it using ideas from sender-based message logging [Johnson87] to avoid logging some messages on stable storage [Strom88]. <p> This allows the receiver to execute based on the message data while the logging proceeds asynchronously. For example, if the message requests some service of the receiver, this service can begin while the message is being logged. Optimistic message logging methods <ref> [Strom85] </ref> have the potential to outperform pessimistic methods, since message logging proceeds asynchronously without delaying either the sender or the receiver for message logging to complete. However, these methods require significantly more complex protocols for logging. <p> The work in this thesis has been partially motivated by Strom and Yemini's Optimistic Recovery system <ref> [Strom85] </ref>, and recently Sistla and Welch have proposed a new method using optimistic message logging [Sistla89], based in part on some aspects of both Strom and Yemini's system and on an earlier version of this work [Johnson88]. <p> Also, by including these checkpointed process states, some messages received by a process before it was checkpointed may not need to be logged, as illustrated in the example of Section 4.4.3. In Strom and Yemini's Optimistic Recovery system <ref> [Strom85] </ref>, each message sent is tagged with a transitive dependency vector, which has size proportional to the number of processes in the system. <p> This is achieved by utilizing all logged messages and checkpoints in forming recoverable system states, using the model presented in Chapter 2. Previous systems using optimistic message logging and checkpointing <ref> [Strom85, Sistla89] </ref> have considered only logged messages in forming recoverable system states, and thus may not find the maximum possible state. Also, but utilizing the checkpointed states, some messages received by a process before it was checkpointed may not need to be logged.
Reference: [Strom87] <author> Robert Strom and Shaula Yemini. </author> <title> Synthesizing distributed and parallel programs through optimistic transformations. </title> <booktitle> In Current Advances in Distributed Computing and Communications, edited by Yechiam Yemini, </booktitle> <pages> pages 234-256. </pages> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland, </address> <year> 1987. </year> <note> Also available as Research Report RC 10797, </note> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, New York, </address> <month> July </month> <year> 1984. </year>
Reference-contexts: Optimistic message logging is a type of optimistic algorithm or optimistic computation. Optimistic algorithms have also been used in other areas, including con-currency control mechanisms [Kung81, Gherfal85], the automatic parallelization of programs for multiprocessors and distributed systems <ref> [Strom87] </ref>, distributed discrete event simulation [Jefferson87, Jefferson85], software development tools [Bubenik89], and network protocols for bulk data transfer [Carter89].
Reference: [Strom88] <author> Robert E. Strom, David F. Bacon, and Shaula A. Yemini. </author> <title> Volatile logging in n-fault-tolerant distributed systems. </title> <booktitle> In The Eighteenth Annual International Symposium on Fault-Tolerant Computing: Digest of Papers, </booktitle> <pages> pages 44-49. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: Although several previous optimistic message logging protocols have been proposed <ref> [Strom85, Strom88, Sistla89] </ref>, none of these protocols has been implemented, and thus their actual performance cannot be evaluated. Optimistic message logging is a type of optimistic algorithm or optimistic computation. <p> Strom and Yemini's Optimistic Recovery mechanism [Strom85] logs all messages on stable storage on disk, but Strom, Bacon, and Yemini have recently proposed enhancements to it using ideas from sender-based message logging [Johnson87] to avoid logging some messages on stable storage <ref> [Strom88] </ref>. Another difference between sender-based message logging and previous pessimistic logging protocols, which is not related to the logging of messages at the sender, is in the enforcement of the requirements of a pessimistic logging protocol.
Reference: [Svobodova84] <author> Liba Svobodova. </author> <title> File servers for network-based distributed systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 16(4) </volume> <pages> 353-398, </pages> <month> December </month> <year> 1984. </year>
Reference-contexts: The related problems of 13 providing fault tolerance for real-time systems and applications [Hecht76, Kopetz85a, Anderson83], reintegrating the system after a network partition [Birrell82, Walker83], and maintaining the consistency and availability of static data such as file systems and databases <ref> [Gray79, Haerder83, Lampson79, Svobodova84] </ref> are also not directly addressed by this thesis. 1.6 Thesis Outline Chapter 2 presents the theoretical framework that will be used in the remainder of the thesis.
Reference: [Theimer85] <author> Marvin N. Theimer, Keith A. Lantz, and David R. Cheriton. </author> <title> Preemptable remote execution facilities for the V-System. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 2-12. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1985. </year>
Reference-contexts: The logical host is then frozen while the remainder of the data is written to the file. This is similar to the technique used by Theimer for process migration in the V-System <ref> [Theimer85] </ref>. After the checkpoint has been completed, the group of logging servers is notified to remove from their logs all messages received by this host before the checkpoint.
Reference: [Vinter86] <author> Stephen Vinter, Krithi Ramamritham, and David Stemple. </author> <title> Recoverable actions in Gutenberg. </title> <booktitle> In Proceedings of the 6th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 242-249. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1986. </year>
Reference-contexts: Examples of systems using atomic actions for fault toler 10 ance include ARGUS [Liskov83, Liskov87, Liskov88], TABS [Spector85b, Spector85a], Camelot [Spector86, Spector87], the original implementation of ISIS [Birman85], Clouds [LeBlanc85, Allchin83], QuickSilver [Haskin88], and Gutenberg <ref> [Vinter86] </ref>. In general, these methods can provide fault tolerance only for application programs that are specifically designed to use them. However, many existing applications do not fit this model, and it is not always practical to program new applications in this way.
Reference: [Walker83] <author> Bruce Walker, Gerald Popek, Robert English, Charles Kline, and Greg Thiel. </author> <title> The LOCUS distributed operating system. </title> <booktitle> In 126 Proceedings of the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 49-70. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1983. </year>
Reference-contexts: The related problems of 13 providing fault tolerance for real-time systems and applications [Hecht76, Kopetz85a, Anderson83], reintegrating the system after a network partition <ref> [Birrell82, Walker83] </ref>, and maintaining the consistency and availability of static data such as file systems and databases [Gray79, Haerder83, Lampson79, Svobodova84] are also not directly addressed by this thesis. 1.6 Thesis Outline Chapter 2 presents the theoretical framework that will be used in the remainder of the thesis.
Reference: [Wensley78] <author> John H. Wensley, Leslie Lamport, Jack Goldberg, Milton W. Green, Karl N. Levitt, P. M. Melliar-Smith, Robert E. Shostak, and Charles B. Weinstock. SIFT: </author> <title> Design and analysis of a fault-tolerant computer for aircraft control. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 66(10) </volume> <pages> 1240-1255, </pages> <month> October </month> <year> 1978. </year>
Reference-contexts: The methods of N-modular redundancy and N-version programming are special cases of this class of fault-tolerance methods. Examples of systems using active replication include the new ISIS system (ISIS 2 ) [Birman87], Circus [Cooper85, Cooper84], CHORUS [Banino85, Banino82], MP [Gait85], MARS [Kopetz85b], FT Concurrent C [Cmelik88], PRIME [Fabry73], SIFT <ref> [Wensley78] </ref>, and Yang and York's work on the Intel iAPX 432 [Yang85]. These methods are well suited for use in real-time systems, since failure recovery is essentially immediate. However, this ability requires extra processors to be dedicated to each program for its replicas.
Reference: [Wood85] <author> W. G. Wood. </author> <title> Recovery control of communicating processes in a distributed system. In Reliable Computer Systems, edited by Santosh K. </title> <booktitle> Shrivastava, </booktitle> <pages> pages 448-484. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: It also does not allow any process state to become stable after the state has occurred, which is required to support optimistic message logging. Another limitation of the occurrence graph model and several other general models <ref> [Russell77, Russell80, Wood85, Chandy88] </ref> is that they do not specify how a process state becomes stable, and thus cannot be used to reason about the progress of message logging and checkpointing.
Reference: [Yang85] <author> Xiao-Zong Yang and Gary York. </author> <title> Fault recovery of triplicated software on the Intel iAPX 432. </title> <booktitle> In Proceedings of the 5th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 438-443. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1985. </year>
Reference-contexts: Examples of systems using active replication include the new ISIS system (ISIS 2 ) [Birman87], Circus [Cooper85, Cooper84], CHORUS [Banino85, Banino82], MP [Gait85], MARS [Kopetz85b], FT Concurrent C [Cmelik88], PRIME [Fabry73], SIFT [Wensley78], and Yang and York's work on the Intel iAPX 432 <ref> [Yang85] </ref>. These methods are well suited for use in real-time systems, since failure recovery is essentially immediate. However, this ability requires extra processors to be dedicated to each program for its replicas.
Reference: [Zwaenepoel84] <author> Willy Zwaenepoel. </author> <title> Message Passing on a Local Network. </title> <type> Ph.D. thesis, </type> <institution> Stanford University, Stanford, California, </institution> <month> October </month> <year> 1984. </year> <note> Also available as Technical Report STAN-CS-85-1083, </note> <institution> Department of Computer Science, Stanford University, </institution> <month> October </month> <year> 1985. </year>
Reference-contexts: RSN field in the LoggedMessage record is not used, and an AdditionalRsn record is created to hold the RSN when it arrives, if the message is received. 3.3.3 Packet Format The network packet format used by sender-based message logging is a modified form of the standard V kernel packet format <ref> [Zwaenepoel84] </ref>. Fields have been added to each packet to carry RSNs and RSN acknowledgements. Since it must be possible to piggyback these fields on any existing packet, no special control packet type is used.
Reference: [Zwaenepoel85] <author> Willy Zwaenepoel. </author> <title> Protocols for large data transfers over local area networks. </title> <booktitle> In Proceedings of the 9th Data Communications Symposium, </booktitle> <pages> pages 22-32. </pages> <publisher> IEEE Computer Society, </publisher> <month> September </month> <year> 1985. </year> <title> Also reprinted in Advances in Local Area Networks, edited by Karl Kummerle, </title> <editor> John O. Limb, and Fouad A. Tobagi, </editor> <booktitle> Frontiers in Communications series, chapter 33, </booktitle> <pages> pages 560-573, </pages> <publisher> IEEE Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: The first of these optimizations is to encode more than one RSN or RSN acknowledgement in a single packet. This optimization is effective when an uninterrupted stream of packets is received from a single sender. For example, when receiving a blast bulk data transfer <ref> [Zwaenepoel85] </ref>, the RSNs for all data packets of the blast can be returned to the sender in a single packet.
References-found: 101

