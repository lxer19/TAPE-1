URL: http://www.cs.umn.edu/Research/Agassiz/Paper/tsai.pact96.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: jytsai@csrd.uiuc.edu yew@cs.umn.edu  
Title: The Superthreaded Architecture: Thread Pipelining with Run-time Data Dependence Checking and Control Speculation  
Author: Jenn-Yuan Tsai Pen-Chung Yew 
Address: Urbana, IL 61801-1351 Minneapolis, MN 55455-0519  
Affiliation: Center for Supercomputing R D Department of Computer Science University of Illinois University of Minnesota  
Abstract: This paper presents a new concurrent multiple-threaded architectural model, called superthreading, for exploiting thread-level parallelism on a processor. This architectural model adopts a thread pipelining execution model that allows threads with data dependences and control dependences to be executed in parallel. The basic idea of thread pipelining is to compute and forward recurrence data and possible dependent store addresses to the next thread as soon as possible, so the next thread can start execution and perform run-time data dependence checking. Thread pipelining also forces contiguous threads to perform their memory write-backs in order, which enables the compiler to fork threads with control speculation. With run-time support for data dependence checking and control speculation, the superthreaded architectural model can exploit loop-level parallelism from a broad range of applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert Alverson, David Callahan, Daniel Cum-mings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera computer system. </title> <booktitle> In Conference Proceedings, 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June 11-15, </month> <year> 1990. </year>
Reference-contexts: Among them, models such as Simultaneous Multi-threading [16] and SPSM [3] allow tasks that are independent, such as the iterations of a do-all loop, to be executed in parallel. This restriction can simplify the design, but limits the exploitable parallelism. Models such as HEP [10], Tera <ref> [1] </ref>, XIMD [19], Elementary Multithreading [8], and M-machine [5, 12] allow data synchronization and communication between threads. These models rely on compilers to detect dependences between threads, and to insert explicit data synchronization and communication commands in a program. They do not support run-time dependence checking.
Reference: [2] <author> Michael Butler, Tse-Yu Yeh, Yale Patt, Mitch Al-sup, Hunter Scales, and Michael Shebanow. </author> <title> Single instruction stream parallelism is greater than two. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276-286, </pages> <month> May 27-30, </month> <year> 1991. </year>
Reference-contexts: However, it is known to be very difficult to extract enough parallelism with a single thread of control even for a small number of functional units <ref> [2, 13, 17] </ref>. A single-threaded sequencing mechanism has several major limitations. For example, independent instructions from different basic blocks need to be grouped together in a single instruction stream.
Reference: [3] <author> Pradeep K. Dubey, Kevin O'Brien, Kathryn O'Brien, and Charles Barton. </author> <title> Single-program speculative multithreading (SPSM) architecture: Compiler-assisted fine-grained multithreading. </title> <booktitle> In Proceedings of the IFIP WG 10.3 Working Conference on Parallel Architectures and Compilation Techniques, PACT '95, </booktitle> <pages> pages 109-121, </pages> <month> June 27-29, </month> <year> 1995. </year>
Reference-contexts: In this paper, we focus on models that are primarily for speeding up the execution of one single program. Among them, models such as Simultaneous Multi-threading [16] and SPSM <ref> [3] </ref> allow tasks that are independent, such as the iterations of a do-all loop, to be executed in parallel. This restriction can simplify the design, but limits the exploitable parallelism. <p> For programs with a lot of maybe data dependences between threads, the performance will suffer if those maybe data dependences do not actually occur at run-time. Some concurrent multiple-threaded models such as Elementary Multithreading [8], Multiscalar [6, 15], and SPSM <ref> [3] </ref> support control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. After the earlier threads have determined the conditions, the speculative thread is committed or squashed, accordingly. <p> After the earlier threads have determined the conditions, the speculative thread is committed or squashed, accordingly. Thread-level control speculation is very useful in exploiting parallelism from WHILE loops where a later iteration is control dependent on the exit condition from an earlier iteration. Multiscalar [6, 15] and SPSM <ref> [3] </ref> also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. In these models, the compiler speculates on maybe data dependences between threads without generating explicit data synchronization instructions to enforce them.
Reference: [4] <author> Matthew K. Farrens and Andrew R. Pleszkun. </author> <title> Strategies for achieving improved processor throughput. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 362-369, </pages> <month> May 27-30, </month> <year> 1991. </year> <month> 11 </month>
Reference-contexts: In addition, multiple functional units can be shared among threads for better utilization. Many concurrent multiple-threaded processor architectures have been proposed and studied [1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 19]. Some of them <ref> [4, 7, 11, 14] </ref> are primarily for increasing system throughput by allowing multiple programs (one program for each thread) to be run concurrently. In this paper, we focus on models that are primarily for speeding up the execution of one single program.
Reference: [5] <author> Marco Fillo, Stephen W. Keckler, Dally William J, Nicholas P. Carter, Andrew Chang, Yevgeny Gurevich, and Whay S. Lee. </author> <title> The m-machine multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Mi-croarchitecture, </booktitle> <pages> pages 146-156, </pages> <month> November 29-December 1, </month> <year> 1995. </year>
Reference-contexts: This restriction can simplify the design, but limits the exploitable parallelism. Models such as HEP [10], Tera [1], XIMD [19], Elementary Multithreading [8], and M-machine <ref> [5, 12] </ref> allow data synchronization and communication between threads. These models rely on compilers to detect dependences between threads, and to insert explicit data synchronization and communication commands in a program. They do not support run-time dependence checking. <p> The superthreaded model does not support direct data transfer between register files in two threads as in <ref> [5, 15] </ref>. However, the compiler can map the global registers, such as the stack pointer and the frame pointer, to some memory addresses. At run time, a thread can obtain the contents of the global registers from the corresponding addresses.
Reference: [6] <author> Manoj Franklin and Gurindar S. Sohi. </author> <title> The expandable split window paradigm for exploiting fine-grained parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 58-67, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: For programs with a lot of maybe data dependences between threads, the performance will suffer if those maybe data dependences do not actually occur at run-time. Some concurrent multiple-threaded models such as Elementary Multithreading [8], Multiscalar <ref> [6, 15] </ref>, and SPSM [3] support control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. After the earlier threads have determined the conditions, the speculative thread is committed or squashed, accordingly. <p> After the earlier threads have determined the conditions, the speculative thread is committed or squashed, accordingly. Thread-level control speculation is very useful in exploiting parallelism from WHILE loops where a later iteration is control dependent on the exit condition from an earlier iteration. Multiscalar <ref> [6, 15] </ref> and SPSM [3] also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. In these models, the compiler speculates on maybe data dependences between threads without generating explicit data synchronization instructions to enforce them.
Reference: [7] <author> Robert H. Halstead, Jr. and Tetsuya Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <month> May 30-June 2, </month> <year> 1988. </year>
Reference-contexts: In addition, multiple functional units can be shared among threads for better utilization. Many concurrent multiple-threaded processor architectures have been proposed and studied [1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 19]. Some of them <ref> [4, 7, 11, 14] </ref> are primarily for increasing system throughput by allowing multiple programs (one program for each thread) to be run concurrently. In this paper, we focus on models that are primarily for speeding up the execution of one single program.
Reference: [8] <author> Hiroaki Hirata, Kozo Kimura, Satoshi Nagamine, Yoshiyuki Mochizuki, Akio Nishimura, Yoshimori Nakase, and Teiji Nishizawa. </author> <title> An elementary processor architecture with simultaneous instruction issuing from multiple threads. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 136-145, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: This restriction can simplify the design, but limits the exploitable parallelism. Models such as HEP [10], Tera [1], XIMD [19], Elementary Multithreading <ref> [8] </ref>, and M-machine [5, 12] allow data synchronization and communication between threads. These models rely on compilers to detect dependences between threads, and to insert explicit data synchronization and communication commands in a program. They do not support run-time dependence checking. <p> For programs with a lot of maybe data dependences between threads, the performance will suffer if those maybe data dependences do not actually occur at run-time. Some concurrent multiple-threaded models such as Elementary Multithreading <ref> [8] </ref>, Multiscalar [6, 15], and SPSM [3] support control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. After the earlier threads have determined the conditions, the speculative thread is committed or squashed, accordingly.
Reference: [9] <author> Mike Johnson. </author> <title> Superscalar Microprocessor Design. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1991. </year>
Reference-contexts: Instruction-level speculative execution with branch prediction is also needed to move independent instructions across basic block boundaries. However, a large instruction window size requires the compiler or hardware to perform more levels of branch prediction and speculation. The accuracy of branch prediction at deeper levels will suffer quickly <ref> [9] </ref>. Also, in VLIW architectures, the code size will expand exponentially because a single-threaded code that includes all possible combinations of branch conditions needs to be generated. This problem is especially serious when a compiler attempts to software pipeline a loop with conditional branches [18]. <p> In superscalar architectures, the processor needs to perform run-time dependence checking for both register and memory accesses. The hardware overhead for such dependence checking is very high and can grow quadratically as the size of the instruction window increases <ref> [9] </ref>. In VLIW architectures, the sub-operations of each long-word are executed in a lock-step fash 1 ion to enforce the dependences between instructions.
Reference: [10] <author> Harry F. Jordan. </author> <title> Performance measurements on HEP | a pipelined MIMD computer. </title> <booktitle> In Proceedings of the 10th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 207-212, </pages> <month> June 13-17, </month> <year> 1983. </year>
Reference-contexts: Among them, models such as Simultaneous Multi-threading [16] and SPSM [3] allow tasks that are independent, such as the iterations of a do-all loop, to be executed in parallel. This restriction can simplify the design, but limits the exploitable parallelism. Models such as HEP <ref> [10] </ref>, Tera [1], XIMD [19], Elementary Multithreading [8], and M-machine [5, 12] allow data synchronization and communication between threads. These models rely on compilers to detect dependences between threads, and to insert explicit data synchronization and communication commands in a program. They do not support run-time dependence checking.
Reference: [11] <author> George E. Daddis Jr. and H.C. Torng. </author> <title> The concurrent execution of multiple instruction streams on superscalar processors. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages I:76-83, </pages> <month> August, </month> <year> 1991. </year>
Reference-contexts: In addition, multiple functional units can be shared among threads for better utilization. Many concurrent multiple-threaded processor architectures have been proposed and studied [1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 19]. Some of them <ref> [4, 7, 11, 14] </ref> are primarily for increasing system throughput by allowing multiple programs (one program for each thread) to be run concurrently. In this paper, we focus on models that are primarily for speeding up the execution of one single program.
Reference: [12] <author> Stephen W. Keckler and William J. Dally. </author> <title> Processor coupling: Integrating compile time and run-time scheduling for parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 202-213, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: This restriction can simplify the design, but limits the exploitable parallelism. Models such as HEP [10], Tera [1], XIMD [19], Elementary Multithreading [8], and M-machine <ref> [5, 12] </ref> allow data synchronization and communication between threads. These models rely on compilers to detect dependences between threads, and to insert explicit data synchronization and communication commands in a program. They do not support run-time dependence checking.
Reference: [13] <author> Monica S. Lam and Robert P. Wilson. </author> <title> Limits of control flow on parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 46-57, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: However, it is known to be very difficult to extract enough parallelism with a single thread of control even for a small number of functional units <ref> [2, 13, 17] </ref>. A single-threaded sequencing mechanism has several major limitations. For example, independent instructions from different basic blocks need to be grouped together in a single instruction stream.
Reference: [14] <author> R. Guru Prasadh and Chuang lin Wu. </author> <title> A benchmark evaluation of a multi-thread risc processor architecture. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages I:84-91, </pages> <month> August, </month> <year> 1991. </year>
Reference-contexts: In addition, multiple functional units can be shared among threads for better utilization. Many concurrent multiple-threaded processor architectures have been proposed and studied [1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 19]. Some of them <ref> [4, 7, 11, 14] </ref> are primarily for increasing system throughput by allowing multiple programs (one program for each thread) to be run concurrently. In this paper, we focus on models that are primarily for speeding up the execution of one single program.
Reference: [15] <author> Gurindar S. Sohi, Scott E. Breach, and T. N. Vi-jaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: For programs with a lot of maybe data dependences between threads, the performance will suffer if those maybe data dependences do not actually occur at run-time. Some concurrent multiple-threaded models such as Elementary Multithreading [8], Multiscalar <ref> [6, 15] </ref>, and SPSM [3] support control speculation. A thread which is control dependent on some conditions determined by earlier threads can be initiated and speculatively executed before the conditions are known. After the earlier threads have determined the conditions, the speculative thread is committed or squashed, accordingly. <p> After the earlier threads have determined the conditions, the speculative thread is committed or squashed, accordingly. Thread-level control speculation is very useful in exploiting parallelism from WHILE loops where a later iteration is control dependent on the exit condition from an earlier iteration. Multiscalar <ref> [6, 15] </ref> and SPSM [3] also support speculation on the maybe data dependences with the help of run-time address disambiguation hardware. In these models, the compiler speculates on maybe data dependences between threads without generating explicit data synchronization instructions to enforce them. <p> The superthreaded model does not support direct data transfer between register files in two threads as in <ref> [5, 15] </ref>. However, the compiler can map the global registers, such as the stack pointer and the frame pointer, to some memory addresses. At run time, a thread can obtain the contents of the global registers from the corresponding addresses.
Reference: [16] <author> Dean M. Tullsen, Susan J. Eggers, and Henry M. Levy. </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392-403, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: In this paper, we focus on models that are primarily for speeding up the execution of one single program. Among them, models such as Simultaneous Multi-threading <ref> [16] </ref> and SPSM [3] allow tasks that are independent, such as the iterations of a do-all loop, to be executed in parallel. This restriction can simplify the design, but limits the exploitable parallelism.
Reference: [17] <author> David W. Wall. </author> <title> Limits of instruction-level parallelism. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 176-188, </pages> <month> April 8-11, </month> <year> 1991. </year>
Reference-contexts: However, it is known to be very difficult to extract enough parallelism with a single thread of control even for a small number of functional units <ref> [2, 13, 17] </ref>. A single-threaded sequencing mechanism has several major limitations. For example, independent instructions from different basic blocks need to be grouped together in a single instruction stream.
Reference: [18] <author> Nancy J. Warter, Grant E. Haab, John W. Bockhaus, and Krishna Subramanian. </author> <title> Enhanced modulo scheduling for loops with conditional branches. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 170-179, </pages> <month> December 1-4, </month> <year> 1992. </year>
Reference-contexts: Also, in VLIW architectures, the code size will expand exponentially because a single-threaded code that includes all possible combinations of branch conditions needs to be generated. This problem is especially serious when a compiler attempts to software pipeline a loop with conditional branches <ref> [18] </ref>. In superscalar architectures, the processor needs to perform run-time dependence checking for both register and memory accesses. The hardware overhead for such dependence checking is very high and can grow quadratically as the size of the instruction window increases [9].
Reference: [19] <author> Andrew Wolfe and John P. Shen. </author> <title> A variable instruction stream extension to the VLIW architecture. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-14, </pages> <month> April 8-11, </month> <year> 1991. </year> <month> 12 </month>
Reference-contexts: Among them, models such as Simultaneous Multi-threading [16] and SPSM [3] allow tasks that are independent, such as the iterations of a do-all loop, to be executed in parallel. This restriction can simplify the design, but limits the exploitable parallelism. Models such as HEP [10], Tera [1], XIMD <ref> [19] </ref>, Elementary Multithreading [8], and M-machine [5, 12] allow data synchronization and communication between threads. These models rely on compilers to detect dependences between threads, and to insert explicit data synchronization and communication commands in a program. They do not support run-time dependence checking.
References-found: 19

