URL: http://www.cs.berkeley.edu/~yelick/cpwen/mplib.ps
Refering-URL: http://www.cs.berkeley.edu/~yelick/papers.html
Root-URL: 
Title: Abstract  
Abstract: This report summarizes and compares the functionality of several portable message passing libraries. A message passing library contains explicit communication primitives for the exchange of messages among computing processes. A portable message passing library further attempts to provide a uniform communication interface on different types of machines. We survey ten message passing libraries, most of which are publicly available, and summarize their basic communication interfaces in this report. We also discuss the semantics of message passing primitives, how easy they are to use, and how the interface design and implementation can affect portability and performance. 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> A. Skjellum, </author> <title> Zipcode: A Portable Communication Layer for High Performance Multicomputing Practice and Experience, </title> <month> March </month> <year> 1991. </year>
Reference-contexts: Typical performance overhead of using such libraries, based on numbers provided by the developers, is about 25% relative to the cost of native message primitives <ref> [1, 2] </ref>. Aside from reducing programming complexity, a portable message passing library may abstract the architectural details of different parallel machines. The programmer can now use the same communication interface to program distributed memory multi-computers, cache-coherent multiprocessors, or even networks of workstations. <p> PVM stands for "Parallel Virtual Machine". * TCGMSG [11]: a communication library developed and used by the Theoretical Chemistry Group at Argonne National Laboratory. * Zipcode <ref> [2, 12] </ref>: a communication library developed by the researchers at Lawrence Livermore National Laboratory. The operating environments of these libraries are summarized in table 1. Note that many different ports of these libraries are currently under development, and the entries here are likely to be out-dated.
Reference: [3] <institution> Application Portable Parallel Library version 2.0, NASA Lewis Research Center, </institution> <month> January </month> <year> 1992. </year> <note> Source code available from fsang@lerc.nasa.gov. </note>
Reference-contexts: In this report we sample ten communication libraries that are built around message passing primitives. Most of them operate on multiple types of parallel computers. A few also have the capability of running in a heterogeneous network environment. The ten communication libraries are listed below: * APPL <ref> [3] </ref>: a communication library developed at NASA Lewis Research Center. The goal of APPL is to simplify programming on the parallel machines there.
Reference: [4] <institution> CMMD User's guide, Thinking Machine Corporation. </institution>
Reference-contexts: The goal of APPL is to simplify programming on the parallel machines there. APPL stands for "Application Portable Parallel Library". * CMAM [19]: an Active Message layer for the Connection Machine (CM-5), devel oped at the University of California, Berkeley. * CMMD <ref> [4] </ref>: a specialized communication library written for the Thinking Ma chines CM-5, a multiprocessor architected around a packet switching network. * EXPRESS [1, 5]: a commercial parallel programming toolkit developed by Para soft Corp., which was founded by a group from the Caltech hypercube project. * P4 [6]: a library of
Reference: [5] <author> J. Flower and A. Kolawa, </author> <title> A "packet" History of Message Passing Systems, </title> <publisher> Parasoft Corporation. </publisher>
Reference-contexts: stands for "Application Portable Parallel Library". * CMAM [19]: an Active Message layer for the Connection Machine (CM-5), devel oped at the University of California, Berkeley. * CMMD [4]: a specialized communication library written for the Thinking Ma chines CM-5, a multiprocessor architected around a packet switching network. * EXPRESS <ref> [1, 5] </ref>: a commercial parallel programming toolkit developed by Para soft Corp., which was founded by a group from the Caltech hypercube project. * P4 [6]: a library of macros and subroutines developed at Argonne National Laboratory. <p> Another special case is the reduce operation which combines elements from all processes and ships the result to one or all of them. For example, a reduce operation using the min operator finds the minimum value in a distributed array. * crystal router: the crystal router <ref> [5] </ref> defers individual message transfers until the processes synchronize, at which point the accumulated messages are sent en masse. The operation exploits the regular communication pattern in loosely synchronous applications (such as meshes in DIME [17]).
Reference: [6] <author> R. Lusk, </author> <note> P4 Version 0.2 Documentation. Source code is available from info.mcs.anl.gov. </note>
Reference-contexts: Berkeley. * CMMD [4]: a specialized communication library written for the Thinking Ma chines CM-5, a multiprocessor architected around a packet switching network. * EXPRESS [1, 5]: a commercial parallel programming toolkit developed by Para soft Corp., which was founded by a group from the Caltech hypercube project. * P4 <ref> [6] </ref>: a library of macros and subroutines developed at Argonne National Laboratory. It is meant to support "Portable Programs for Parallel Processors", from which it also takes its name. * PARMACS [20]: a collection of macros described in the book "Portable Programs for Parallel Processors".
Reference: [7] <author> G. Geist et al., </author> <title> A User's Guide to PICL: A Portable Instrumented Communication Library, </title> <institution> Oak Ridge National Lab. </institution> <note> Report No. ORNL/TM-11616, </note> <month> February </month> <year> 1991. </year>
Reference-contexts: It is meant to support "Portable Programs for Parallel Processors", from which it also takes its name. * PARMACS [20]: a collection of macros described in the book "Portable Programs for Parallel Processors". It is one of the predecessors of P4. * PICL <ref> [7] </ref>: a "Portable, Instrumented Communication Library" developed by the researchers at Oak Ridge National Laboratory. PICL was used to aid the perfor mance characterization research at ORNL [8]. * PVM [9, 10]: a communication package developed at Oak Ridge National Laboratory to address heterogeneity in a network based environment.
Reference: [8] <author> P. Worley and M. </author> <type> Heath, </type> <institution> Performance Characterization Research at Oak Ridge National Laboratory. </institution>
Reference-contexts: It is one of the predecessors of P4. * PICL [7]: a "Portable, Instrumented Communication Library" developed by the researchers at Oak Ridge National Laboratory. PICL was used to aid the perfor mance characterization research at ORNL <ref> [8] </ref>. * PVM [9, 10]: a communication package developed at Oak Ridge National Laboratory to address heterogeneity in a network based environment.
Reference: [9] <author> V. Sunderam, </author> <title> PVM: A Framework for Parallel Distributed Computing, PVM 2.3 documentation. Source code available from netlib. </title>
Reference-contexts: It is one of the predecessors of P4. * PICL [7]: a "Portable, Instrumented Communication Library" developed by the researchers at Oak Ridge National Laboratory. PICL was used to aid the perfor mance characterization research at ORNL [8]. * PVM <ref> [9, 10] </ref>: a communication package developed at Oak Ridge National Laboratory to address heterogeneity in a network based environment.
Reference: [10] <author> A. Beguelin et al., </author> <title> A User's Guide to PVM Parallel Virtual Machine, </title> <institution> Oak Ridge National Lab. </institution> <note> Report No. ORNL/TM-11826, </note> <month> July </month> <year> 1991. </year>
Reference-contexts: It is one of the predecessors of P4. * PICL [7]: a "Portable, Instrumented Communication Library" developed by the researchers at Oak Ridge National Laboratory. PICL was used to aid the perfor mance characterization research at ORNL [8]. * PVM <ref> [9, 10] </ref>: a communication package developed at Oak Ridge National Laboratory to address heterogeneity in a network based environment.
Reference: [11] <author> R. Harrison, </author> <note> TCGMSG Version 4.0, Theoretical Chemistry Group, </note> <institution> Ar-gonne National Laboratory, </institution> <month> December </month> <year> 1991. </year> <note> Source code available from harrison.tcg.anl.gov. </note>
Reference-contexts: PICL was used to aid the perfor mance characterization research at ORNL [8]. * PVM [9, 10]: a communication package developed at Oak Ridge National Laboratory to address heterogeneity in a network based environment. PVM stands for "Parallel Virtual Machine". * TCGMSG <ref> [11] </ref>: a communication library developed and used by the Theoretical Chemistry Group at Argonne National Laboratory. * Zipcode [2, 12]: a communication library developed by the researchers at Lawrence Livermore National Laboratory. The operating environments of these libraries are summarized in table 1.
Reference: [12] <author> A. Skjellum and C. Baldwin, </author> <title> The multicomputer toolbox: Scalable Parallel Libraries for Large-Scale Concurrent Applications, LLNL Numerical Mathematics Group Report No. </title> <address> UCRL-JC-109251, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: PVM stands for "Parallel Virtual Machine". * TCGMSG [11]: a communication library developed and used by the Theoretical Chemistry Group at Argonne National Laboratory. * Zipcode <ref> [2, 12] </ref>: a communication library developed by the researchers at Lawrence Livermore National Laboratory. The operating environments of these libraries are summarized in table 1. Note that many different ports of these libraries are currently under development, and the entries here are likely to be out-dated. <p> The latency of broadcasts and scans can be made logarithmic with the number of processes by applying techniques such as recursive doubling <ref> [12, 15] </ref> . On some machines (such as the CM-5 or the shared-memory multiprocessors) broadcasts can be performed efficiently by the hardware. Synchronization primitives Synchronization plays an important role in parallel programs. Explicit synchronization primitives coordinate processes through the exchange of control information instead of data (or messages).
Reference: [13] <author> PARMAC documentation. </author> <title> Source code available from netlib. </title>
Reference: [14] <author> C. Seitz et al., </author> <title> The C Programmer's Abbreviated Guide to Multicomputer Programming, </title> <institution> it Caltech Computer Science Technical Report No. Caltech-CS-TR-88-1, </institution> <month> January </month> <year> 1988. </year>
Reference: [15] <author> J. Gistafson et al., </author> <title> Development of Parallel Methods For a 1024-Processor Hypercube, </title> <journal> SIAM Journal on Scientific and Statistical Computing, Vol.9 No.4, </journal> <month> July </month> <year> 1988. </year>
Reference-contexts: The latency of broadcasts and scans can be made logarithmic with the number of processes by applying techniques such as recursive doubling <ref> [12, 15] </ref> . On some machines (such as the CM-5 or the shared-memory multiprocessors) broadcasts can be performed efficiently by the hardware. Synchronization primitives Synchronization plays an important role in parallel programs. Explicit synchronization primitives coordinate processes through the exchange of control information instead of data (or messages).
Reference: [16] <author> L. Bomans and D. Roose, </author> <title> Benchmarking the iPSC hypercube multiprocessor, </title> <journal> Concurrency: Practice and Experience, Vol.1, </journal> <volume> pp.3-18, </volume> <month> September </month> <year> 1989. </year>
Reference: [17] <author> R. Williams, DIME: </author> <title> A Programming Environment for Unstructured Triangular Meshes on a Distributed-Memory Parallel Processor, </title> <type> C3P Report 502, </type> <year> 1988. </year>
Reference-contexts: The operation exploits the regular communication pattern in loosely synchronous applications (such as meshes in DIME <ref> [17] </ref>). These applications decompose the problem into cycles of computations, and the results produced (and distributed) by a certain compute phase are not acted upon until the next cycle.
Reference: [18] <author> G. Wilson, </author> <title> Design Principle for Message-Passing System, </title> <type> Draft paper, </type> <month> January </month> <year> 1991. </year>
Reference: [19] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser, </author> <title> Active messages: a mechanism for integrated communication and computation, </title> <booktitle> 19th Annual International Symposium on Computer Architecture, </booktitle> <address> pp.256-66, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: The ten communication libraries are listed below: * APPL [3]: a communication library developed at NASA Lewis Research Center. The goal of APPL is to simplify programming on the parallel machines there. APPL stands for "Application Portable Parallel Library". * CMAM <ref> [19] </ref>: an Active Message layer for the Connection Machine (CM-5), devel oped at the University of California, Berkeley. * CMMD [4]: a specialized communication library written for the Thinking Ma chines CM-5, a multiprocessor architected around a packet switching network. * EXPRESS [1, 5]: a commercial parallel programming toolkit developed by
Reference: [20] <editor> E. Lusk et al., </editor> <title> Portable Programs for Parallel Processors, </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <year> 1987. </year>
Reference-contexts: It is meant to support "Portable Programs for Parallel Processors", from which it also takes its name. * PARMACS <ref> [20] </ref>: a collection of macros described in the book "Portable Programs for Parallel Processors". It is one of the predecessors of P4. * PICL [7]: a "Portable, Instrumented Communication Library" developed by the researchers at Oak Ridge National Laboratory.
Reference: [21] <author> W. Gentleman, </author> <title> Administrators and multiprocessor rendezvous mechanisms., </title> <journal> Software Practice and Experience Jan. </journal> <note> 1992, vol.22, no.1, pp1-39. </note>
Reference-contexts: The process now sees its sends handled in the program order, but the ordering between messages sent by different processes is not deterministic. This scheme is well suited for the client/server type of communication <ref> [21] </ref>. The next scheme is to use asynchronous messages. In this case the sending process will not know when and in what order its outgoing messages are received. The ordering of messages depends on the use of sender ids and message tags. Active messages introduce the most nondeterminism.
References-found: 20

