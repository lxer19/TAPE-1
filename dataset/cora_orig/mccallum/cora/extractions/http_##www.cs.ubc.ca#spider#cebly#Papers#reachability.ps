URL: http://www.cs.ubc.ca/spider/cebly/Papers/reachability.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: cebly@cs.ubc.ca  brafman@cs.bgu.ac.il  geib@htc.honeywell.com  
Title: Structured Reachability Analysis for Markov Decision Processes  
Author: Craig Boutilier Ronen I. Brafman Christopher Geib 
Address: Vancouver, BC, Canada V6T 1Z4  Beer Sheva, Israel 84105  MN65-2600, 3660 Technology Dr. Minneapolis, MN, USA 55418  
Affiliation: Department of Computer Science University of British Columbia  Department of Math and CS Ben-Gurion University  Honeywell Technology Center  
Abstract: Recent research in decision theoretic planning has focussed on making the solution of Markov decision processes (MDPs) more feasible. We develop a family of algorithms for structured reachability analysis of MDPs that are suitable when an initial state (or set of states) is known. Using compact, structured representations of MDPs (e.g., Bayesian networks), our methods, which vary in the tradeoff between complexity and accuracy, produce structured descriptions of (estimated) reachable states that can be used to eliminate variables or variable values from the problem description, reducing the size of the MDP and making it easier to solve. One contribution of our work is the extension of ideas from GRAPHPLAN to deal with the distributed nature of action representations typically embodied within Bayes nets and the problem of correlated action effects. We also demonstrate that our algorithm can be made more complete by using k-ary constraints instead of binary constraints. Another contribution is the illustration of how the compact representation of reachability constraints can be exploited by several existing (exact and approximate) abstraction algorithms for MDPs.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard E. Bellman. </author> <title> Dynamic Programming. </title> <note> Princeton Uni 11 We defer full details to a longer version of this paper. versity Press, </note> <institution> Princeton, </institution> <year> 1957. </year>
Reference-contexts: One of the key drawbacks of classic algorithms such as policy iteration [16] or value iteration <ref> [1] </ref> is the need to explicitly sweep through state space, making these techniques impractical for realistic problems. Recent research on the use of MDPs for DTP has focussed on methods for solving MDPs that avoid explicit state space enumeration while constructing optimal or approximately optimal policies. <p> We conclude in Section 5 with additional discussion. 2 MDPs and Their Representation 2.1 Markov Decision Processes We assume that the system to be controlled can be described as a fully-observable, finite Markov decision process <ref> [1, 16] </ref>, with a finite set of system states S. The controlling agent has available a finite set of actions A which cause stochastic state transitions: we write Pr (s; a; t) to denote the probability action a causes a transition to state t when executed in state s. <p> A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . The optimal value function V fl is the same as the value function for any optimal policy. A number of state-based techniques for constructing optimal policies exist, including value iteration <ref> [1] </ref> and policy iteration [16]. 2.2 Structured Representation and Computation One of the key problems facing the use of MDPs for DTP is Bellman's curse of dimensionality: the number of states grows exponentially with the number of problem variables.
Reference: [2] <author> Dimitri P. Bertsekas and John. N. Tsitsiklis. </author> <title> Neuro-dynamic Programming. </title> <publisher> Athena, </publisher> <address> Belmont, MA, </address> <year> 1996. </year>
Reference-contexts: Recent research on the use of MDPs for DTP has focussed on methods for solving MDPs that avoid explicit state space enumeration while constructing optimal or approximately optimal policies. These include the use of function approximators for value functions <ref> [2] </ref>, aggregation and abstraction techniques [5, 6, 12, 8], reachability analysis [9], and decomposition techniques [11]. fl This work was supported by NSERC Research Grant OGP0121843 and IRIS-II Project IC-7. y Part of this work was undertaken at U.B.C. and was sup ported by NSERC. z Part of this work was
Reference: [3] <author> Avrim L. Blum and Merrick L. Furst. </author> <title> Fast planning through graph analysis. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1636-1642, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: This knowledge can be integrated into abstraction techniques by eliminating abstract states that contain unreachable variable combinations. The method we develop is based on the GRAPHPLAN algorithm <ref> [3] </ref>: the graph-building phase of GRAPHPLAN can be viewed as performing an approximate reachability analysis that is used to prune subsequent goal-regression search in a classical planning framework. However, we make certain modifications designed to deal with the DBN action representation. <p> If there is enough material to produce one of P4 or P5, then RdyP4 and RdyP5 might change, but the condition on P6 cannot, and again AsmP6 can have its value fixed. 3.1 Reachability Analysis without Direct Correlations Our algorithm for structured reachability analysis in inspired by GRAPHPLAN <ref> [3] </ref>, a classical planning algorithm that essentially performs a reachability analysis to construct a plan graph and then performs goal regression within that graph. Specifically, the graph building phase of GRAPH PLAN operates by alternating the construction of propositional levels and action levels.
Reference: [4] <author> Craig Boutilier. </author> <title> Correlated action effects in decision theoretic regression. </title> <booktitle> In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 30-37, </pages> <address> Providence, RI, </address> <year> 1997. </year>
Reference-contexts: and approximate) abstraction techniques can be employed to solve an MDP without explicit state space enumeration [5, 6, 12]. 1 These all rely on the identification of conditions under which a variable can influence a value function or action choice, and can be viewed as decision-theoretic generalizations of goal regression <ref> [4] </ref>. Reachability analysis allows one to determine that certain states are not reachable in an MDP given a particular initial state (or a set of possible initial states, or an initial distribution), no matter what actions are performed. <p> The fact that even simple reachability analysis can substantially reduce the effective size of an MDP seems clear; and more complex analysis will, at higher computational cost, provide deeper overall reductions. The more sophisticated techniques of <ref> [4, 5, 6] </ref> can, of course, benefit in the same way as the simple abstraction method. However, these algorithms create dynamic, nonuniform abstractions, creating and recreating tree representations of value functions and policies.
Reference: [5] <author> Craig Boutilier and Richard Dearden. </author> <title> Approximating value trees in structured dynamic programming. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 54-62, </pages> <address> Bari, Italy, </address> <year> 1996. </year>
Reference-contexts: Recent research on the use of MDPs for DTP has focussed on methods for solving MDPs that avoid explicit state space enumeration while constructing optimal or approximately optimal policies. These include the use of function approximators for value functions [2], aggregation and abstraction techniques <ref> [5, 6, 12, 8] </ref>, reachability analysis [9], and decomposition techniques [11]. fl This work was supported by NSERC Research Grant OGP0121843 and IRIS-II Project IC-7. y Part of this work was undertaken at U.B.C. and was sup ported by NSERC. z Part of this work was undertaken at U.B.C. and was <p> We assume that the MDP is described in terms of random variables using dynamic Bayes nets (DBNs) [6]; we also assume an initial state has been given. Given this representation, several useful (exact and approximate) abstraction techniques can be employed to solve an MDP without explicit state space enumeration <ref> [5, 6, 12] </ref>. 1 These all rely on the identification of conditions under which a variable can influence a value function or action choice, and can be viewed as decision-theoretic generalizations of goal regression [4]. <p> The method can be extended to deal with approximation by using degrees of relevance as well <ref> [5] </ref>. A simpler abstraction technique developed in [12] uses a static analysis of the problem (as opposed to a dynamic analysis) to delete irrelevant or marginally relevant variables in the problem description. <p> The fact that even simple reachability analysis can substantially reduce the effective size of an MDP seems clear; and more complex analysis will, at higher computational cost, provide deeper overall reductions. The more sophisticated techniques of <ref> [4, 5, 6] </ref> can, of course, benefit in the same way as the simple abstraction method. However, these algorithms create dynamic, nonuniform abstractions, creating and recreating tree representations of value functions and policies.
Reference: [6] <author> Craig Boutilier, Richard Dearden, and Moises Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1104-1111, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: Recent research on the use of MDPs for DTP has focussed on methods for solving MDPs that avoid explicit state space enumeration while constructing optimal or approximately optimal policies. These include the use of function approximators for value functions [2], aggregation and abstraction techniques <ref> [5, 6, 12, 8] </ref>, reachability analysis [9], and decomposition techniques [11]. fl This work was supported by NSERC Research Grant OGP0121843 and IRIS-II Project IC-7. y Part of this work was undertaken at U.B.C. and was sup ported by NSERC. z Part of this work was undertaken at U.B.C. and was <p> We assume that the MDP is described in terms of random variables using dynamic Bayes nets (DBNs) <ref> [6] </ref>; we also assume an initial state has been given. <p> We assume that the MDP is described in terms of random variables using dynamic Bayes nets (DBNs) [6]; we also assume an initial state has been given. Given this representation, several useful (exact and approximate) abstraction techniques can be employed to solve an MDP without explicit state space enumeration <ref> [5, 6, 12] </ref>. 1 These all rely on the identification of conditions under which a variable can influence a value function or action choice, and can be viewed as decision-theoretic generalizations of goal regression [4]. <p> Fortunately, several good representations for MDPs, suitable for DTP, have been proposed that alleviate the associated representational and computational burdens. These include stochastic STRIPS operators [12, 15] and dynamic Bayes nets <ref> [6, 10] </ref>. We will use the latter. We assume that a set of variables V = fV 1 ; V N g describes our system, with each V i having a finite domain Dom (V i ) of possible values. <p> Each post-action node has an associated conditional probability table (CPT) quantifying the influence of the action on the corresponding variable, given the value of its influences (see <ref> [6, 7] </ref> for a more detailed discussion of this representation). Figures 1 (a) and (b) illustrate network fragments for two different actions (spray painting three parts, and assembling parts P4 and P5 into P6). <p> We refer to [7] for a detailed discussion of persistence in DBNs. In <ref> [6] </ref> a structured version of modified policy iteration is developed, in which value functions and policies are represented using decision trees and the DBN representation of the MDP is exploited to build these compact policies. <p> The fact that even simple reachability analysis can substantially reduce the effective size of an MDP seems clear; and more complex analysis will, at higher computational cost, provide deeper overall reductions. The more sophisticated techniques of <ref> [4, 5, 6] </ref> can, of course, benefit in the same way as the simple abstraction method. However, these algorithms create dynamic, nonuniform abstractions, creating and recreating tree representations of value functions and policies.
Reference: [7] <author> Craig Boutilier and Moises Goldszmidt. </author> <title> The frame problem and Bayesian network action representations. </title> <booktitle> In Proceedings of the Eleventh Biennial Canadian Conference on Artificial Intelligence, </booktitle> <pages> pages 69-83, </pages> <address> Toronto, </address> <year> 1996. </year>
Reference-contexts: Each post-action node has an associated conditional probability table (CPT) quantifying the influence of the action on the corresponding variable, given the value of its influences (see <ref> [6, 7] </ref> for a more detailed discussion of this representation). Figures 1 (a) and (b) illustrate network fragments for two different actions (spray painting three parts, and assembling parts P4 and P5 into P6). <p> We refer to <ref> [7] </ref> for a detailed discussion of persistence in DBNs. In [6] a structured version of modified policy iteration is developed, in which value functions and policies are represented using decision trees and the DBN representation of the MDP is exploited to build these compact policies.
Reference: [8] <author> Thomas Dean and Robert Givan. </author> <title> Model minimization in markov decision processes. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 106-111, </pages> <address> Providence, </address> <year> 1997. </year>
Reference-contexts: Recent research on the use of MDPs for DTP has focussed on methods for solving MDPs that avoid explicit state space enumeration while constructing optimal or approximately optimal policies. These include the use of function approximators for value functions [2], aggregation and abstraction techniques <ref> [5, 6, 12, 8] </ref>, reachability analysis [9], and decomposition techniques [11]. fl This work was supported by NSERC Research Grant OGP0121843 and IRIS-II Project IC-7. y Part of this work was undertaken at U.B.C. and was sup ported by NSERC. z Part of this work was undertaken at U.B.C. and was
Reference: [9] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 574-579, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: These include the use of function approximators for value functions [2], aggregation and abstraction techniques [5, 6, 12, 8], reachability analysis <ref> [9] </ref>, and decomposition techniques [11]. fl This work was supported by NSERC Research Grant OGP0121843 and IRIS-II Project IC-7. y Part of this work was undertaken at U.B.C. and was sup ported by NSERC. z Part of this work was undertaken at U.B.C. and was sup ported by IRIS-II Project IC-7. <p> This can be used to restrict dynamic programming to reachable states, reducing the computational burden of solving an MDP (for instance, see the envelope approach of <ref> [9] </ref>). However, approaches that determine reachability using explicit transition matrix operations [13] or state-based search cannot be exploited by variable-based abstraction techniques.
Reference: [10] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intel ligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: Fortunately, several good representations for MDPs, suitable for DTP, have been proposed that alleviate the associated representational and computational burdens. These include stochastic STRIPS operators [12, 15] and dynamic Bayes nets <ref> [6, 10] </ref>. We will use the latter. We assume that a set of variables V = fV 1 ; V N g describes our system, with each V i having a finite domain Dom (V i ) of possible values.
Reference: [11] <author> Thomas Dean and Shieu-Hong Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1121-1127, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: These include the use of function approximators for value functions [2], aggregation and abstraction techniques [5, 6, 12, 8], reachability analysis [9], and decomposition techniques <ref> [11] </ref>. fl This work was supported by NSERC Research Grant OGP0121843 and IRIS-II Project IC-7. y Part of this work was undertaken at U.B.C. and was sup ported by NSERC. z Part of this work was undertaken at U.B.C. and was sup ported by IRIS-II Project IC-7.
Reference: [12] <author> Richard Dearden and Craig Boutilier. </author> <title> Abstraction and approximate decision theoretic planning. </title> <journal> Artificial Intelligence, </journal> <volume> 89 </volume> <pages> 219-283, </pages> <year> 1997. </year>
Reference-contexts: Recent research on the use of MDPs for DTP has focussed on methods for solving MDPs that avoid explicit state space enumeration while constructing optimal or approximately optimal policies. These include the use of function approximators for value functions [2], aggregation and abstraction techniques <ref> [5, 6, 12, 8] </ref>, reachability analysis [9], and decomposition techniques [11]. fl This work was supported by NSERC Research Grant OGP0121843 and IRIS-II Project IC-7. y Part of this work was undertaken at U.B.C. and was sup ported by NSERC. z Part of this work was undertaken at U.B.C. and was <p> We assume that the MDP is described in terms of random variables using dynamic Bayes nets (DBNs) [6]; we also assume an initial state has been given. Given this representation, several useful (exact and approximate) abstraction techniques can be employed to solve an MDP without explicit state space enumeration <ref> [5, 6, 12] </ref>. 1 These all rely on the identification of conditions under which a variable can influence a value function or action choice, and can be viewed as decision-theoretic generalizations of goal regression [4]. <p> Fortunately, several good representations for MDPs, suitable for DTP, have been proposed that alleviate the associated representational and computational burdens. These include stochastic STRIPS operators <ref> [12, 15] </ref> and dynamic Bayes nets [6, 10]. We will use the latter. We assume that a set of variables V = fV 1 ; V N g describes our system, with each V i having a finite domain Dom (V i ) of possible values. <p> The method can be extended to deal with approximation by using degrees of relevance as well [5]. A simpler abstraction technique developed in <ref> [12] </ref> uses a static analysis of the problem (as opposed to a dynamic analysis) to delete irrelevant or marginally relevant variables in the problem description. For instance, the fine differences in reward associated with Worn in Figure 1 (c) can be ignored by deleting this variable. <p> Generally such isomorphisms can be detected readily given canonical variable orderings. rather than the complete policy construction problem, this can offer a considerable advantage. The simple abstraction algorithm of <ref> [12] </ref> can benefit substantially from this sort or analysis. As mentioned above, we may easily determine that certain objectives are infeasible and remove them from the problem description (e.g., DrlP1; DrlP2 above). <p> Empirical Evaluation: In our manufacturing example described in the previous section, we get very substantial reductions in MDP size by first removing unreachable values and value combinations from the MDP description. Using the algorithm of <ref> [12] </ref> without reachability, assuming that only strictly irrelevant variables are removed (i.e., with no approximation), we can remove 2 of the 31 variables, reduc ing the MDP to size 2 29 .
Reference: [13] <author> B. L. Fox and D. M. Landi. </author> <title> An algorithm for identifying the ergodic subchains and transient states of a stochastic matrix. </title> <journal> Communications of the ACM, </journal> <volume> 2 </volume> <pages> 619-621, </pages> <year> 1968. </year>
Reference-contexts: This can be used to restrict dynamic programming to reachable states, reducing the computational burden of solving an MDP (for instance, see the envelope approach of [9]). However, approaches that determine reachability using explicit transition matrix operations <ref> [13] </ref> or state-based search cannot be exploited by variable-based abstraction techniques. Our aim is to develop methods that determine the set of reachable states implicitly by explicitly considering the variable values or combinations of variable values that can or cannot be realized.
Reference: [14] <author> B. Cenk Gazen and Craig A. Knoblock. </author> <title> Combining the expressiveness of UCPOP with the efficieny of Graphplan. </title> <booktitle> In Proceedings of the Fourth European Conference on Planning, </booktitle> <pages> pages ??-??, Toulouse, </pages> <year> 1997. </year>
Reference-contexts: action nodes: the fact that action a has condi tional effects is treated by creating a different action node (here a CAE node) for each condition under which a has different effects. 5 GRAPHPLAN can easily be extended to deal with conditional effects in this way in classical settings (see <ref> [14, 18] </ref> where similar suggestions are made). A more substantial difference has to do with the distributed representation of action effects. To effectively deal with this, we cre ate nodes for all possible independent effects of a, retaining the distributed flavor of the DBN model.
Reference: [15] <author> Steve Hanks and Drew V. McDermott. </author> <title> Modeling a dynamic and uncertain world I: Symbolic and probabilistic reasoning about change. </title> <journal> Artificial Intelligence, </journal> <year> 1994. </year>
Reference-contexts: However, we make certain modifications designed to deal with the DBN action representation. In particular, the distributed nature of this representation, while often more compact than (say) probabilistic STRIPS operators <ref> [15] </ref>, requires that care be taken to deal with correlated effects. A second way in which we generalize the graph-building phase of GRAPHPLAN is to consider 1 The integration of reachability with state aggregation has been considered in the verification community (e.g., see [19]). <p> Fortunately, several good representations for MDPs, suitable for DTP, have been proposed that alleviate the associated representational and computational burdens. These include stochastic STRIPS operators <ref> [12, 15] </ref> and dynamic Bayes nets [6, 10]. We will use the latter. We assume that a set of variables V = fV 1 ; V N g describes our system, with each V i having a finite domain Dom (V i ) of possible values. <p> This offers economy of representation for many types of actions. The painting action, for instance, has three independent effects on three different parts. The representation of this action in (probabilistic variants of) STRIPS <ref> [15] </ref> requires specifying all eight combinations of painted/not painted values; and the size of the STRIPS representation generally grows exponentially with the number of independent effects. A similar representation can be used to represent the reward function R, as shown in Figure 1 (c).
Reference: [16] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1960. </year>
Reference-contexts: One of the key drawbacks of classic algorithms such as policy iteration <ref> [16] </ref> or value iteration [1] is the need to explicitly sweep through state space, making these techniques impractical for realistic problems. Recent research on the use of MDPs for DTP has focussed on methods for solving MDPs that avoid explicit state space enumeration while constructing optimal or approximately optimal policies. <p> We conclude in Section 5 with additional discussion. 2 MDPs and Their Representation 2.1 Markov Decision Processes We assume that the system to be controlled can be described as a fully-observable, finite Markov decision process <ref> [1, 16] </ref>, with a finite set of system states S. The controlling agent has available a finite set of actions A which cause stochastic state transitions: we write Pr (s; a; t) to denote the probability action a causes a transition to state t when executed in state s. <p> In order to compare policies, we adopt expected total discounted reward as our optimality criterion; future rewards are discounted by rate 0 fi &lt; 1. The value of a policy can be shown to satisfy <ref> [16] </ref>: V (s) = R (s) + fi t2S The value of at any initial state s can be computed by solving this system of linear equations. A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . <p> The optimal value function V fl is the same as the value function for any optimal policy. A number of state-based techniques for constructing optimal policies exist, including value iteration [1] and policy iteration <ref> [16] </ref>. 2.2 Structured Representation and Computation One of the key problems facing the use of MDPs for DTP is Bellman's curse of dimensionality: the number of states grows exponentially with the number of problem variables.
Reference: [17] <author> Craig A. Knoblock. </author> <title> Automatically generating abstractions for planning. </title> <journal> Artificial Intelligence, </journal> <volume> 68 </volume> <pages> 243-302, </pages> <year> 1994. </year>
Reference-contexts: In this paper we address the problem of integrating reach-ability considerations into the construction of abstract MDPs. In particular, we develop techniques whereby knowledge of an initial state (or initial conditions) and the concomitant reachability considerations influence the abstractions produced for an MDP, forming what is termed by Knoblock <ref> [17] </ref> a problem specific abstraction. We assume that the MDP is described in terms of random variables using dynamic Bayes nets (DBNs) [6]; we also assume an initial state has been given.
Reference: [18] <author> J. Koehler, B. Nebel, J. Hoffman, and Y. Dimopolous. </author> <title> Extending Graphplan to a subset of ADL. </title> <booktitle> In Proceedings of the Fourth European Conference on Planning, </booktitle> <pages> pages ??-??, Toulouse, </pages> <year> 1997. </year>
Reference-contexts: action nodes: the fact that action a has condi tional effects is treated by creating a different action node (here a CAE node) for each condition under which a has different effects. 5 GRAPHPLAN can easily be extended to deal with conditional effects in this way in classical settings (see <ref> [14, 18] </ref> where similar suggestions are made). A more substantial difference has to do with the distributed representation of action effects. To effectively deal with this, we cre ate nodes for all possible independent effects of a, retaining the distributed flavor of the DBN model.
Reference: [19] <author> D. Lee and M. Yannakakis. </author> <title> Online miminization of transition systems. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing (STOC-92), </booktitle> <pages> pages 264-274, </pages> <address> Victoria, BC, </address> <year> 1992. </year>
Reference-contexts: A second way in which we generalize the graph-building phase of GRAPHPLAN is to consider 1 The integration of reachability with state aggregation has been considered in the verification community (e.g., see <ref> [19] </ref>). There explicit state space representations are used however. both simpler and more complex constraints in the construc-tion of exclusion relations. We argue that reachability can be performed with varying levels of completeness, leading to one way of addressing anytime tradeoffs.
References-found: 19

