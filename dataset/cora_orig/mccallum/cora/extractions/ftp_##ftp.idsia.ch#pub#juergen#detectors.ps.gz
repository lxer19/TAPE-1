URL: ftp://ftp.idsia.ch/pub/juergen/detectors.ps.gz
Refering-URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/index.html
Root-URL: 
Title: SEMILINEAR PREDICTABILITY MINIMIZATION PRODUCES WELL-KNOWN FEATURE DETECTORS Neural Computation, 1996 (accepted)  
Author: Jurgen Schmidhuber Martin Eldracher Bernhard Foltin 
Address: Corso Elvezia 36 6900 Lugano, Switzerland  Corso Elvezia 36 6900 Lugano, Switzerland  TUM, 80290 Munchen, Germany  
Affiliation: IDSIA,  IDSIA,  Fakultat fur Informatik  
Abstract: Predictability minimization (PM | Schmidhuber, 1992) exhibits various intuitive and theoretical advantages over many other methods for unsupervised redundancy reduction. So far, however, there were only toy applications of PM. In this paper, we apply semilinear PM to static real world images and find: without a teacher and without any significant preprocessing, the system automatically learns to generate distributed representations based on well-known feature detectors, such as orientation sensitive edge detectors and off-center-on-surround-like structures, thus extracting simple features related to those considered useful for image pre-processing and compression.
Abstract-found: 1
Intro-found: 1
Reference: <author> Atick, J. J., Li, Z., and Redlich, A. N. </author> <year> (1992). </year> <title> Understanding retinal color coding from first principles. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 559-572. </pages>
Reference: <author> Barlow, H. B., Kaushal, T. P., and Mitchison, G. J. </author> <year> (1989). </year> <title> Finding minimum entropy codes. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 412-423. </pages>
Reference-contexts: A feedforward network with n output units (or code units) sees input patterns with redundant components. Its goal is to respond with informative but less redundant output patterns, ideally by creating a factorial (statistically nonredundant) code of the input ensemble <ref> (Barlow et al., 1989) </ref>. The central idea of PM is: for each code unit, there is a predictor network that tries to predict the code unit from the remaining n 1 code units.
Reference: <author> Barrow, H. G. </author> <year> (1987). </year> <title> Learning receptive fields. </title> <booktitle> In Proceedings of the IEEE 1st Annual Conference on Neural Networks, volume IV, </booktitle> <pages> pages 115-121. </pages> <editor> IEEE. </editor> <title> 6 pattern presentations (logarithmic scale). Results are shown for various pairs of predictor learning rates P and code unit learning rates C . a : P = 0:001; C = 0:00004. b : P = 0:01; C = 0:00011. c : P = 0:1; C = 0:005. d : P = 1:0; C = 0:0042. </title>
Reference: <author> Deco, G. and Obradovic, D. </author> <year> (1996). </year> <title> An information-theoretic approach to neural computing. </title> <publisher> Springer, </publisher> <address> New York. </address>
Reference: <author> Field, D. J. </author> <year> (1994). </year> <title> What is the goal of sensory coding? Neural Computation, </title> <booktitle> 6 </booktitle> <pages> 559-601. </pages>
Reference: <author> Foldiak, P. </author> <year> (1990). </year> <title> Forming sparse representations by local anti-Hebbian learning. </title> <journal> Biological Cybernetics, </journal> <volume> 64 </volume> <pages> 165-170. </pages>
Reference: <author> Lindstadt, S. </author> <year> (1993). </year> <title> Comparison of two unsupervised neural network models for redundancy reduction. </title> <editor> In Mozer, M. C., Smolensky, P., Touretzky, D. S., Elman, J. L., and Weigend, A. S., editors, </editor> <booktitle> Proc. of the 1993 Connectionist Models Summer School, </booktitle> <pages> pages 308-315. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum Associates. </publisher>
Reference: <author> Linsker, R. </author> <year> (1986a). </year> <title> From basic network principles to neural architecture: Emergence of orientation-selective cells. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <month> 83 </month> <pages> 8779-8783. </pages>
Reference: <author> Linsker, R. </author> <year> (1986b). </year> <title> From basic network principles to neural architecture: Emergence of spatial-opponent cells. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <month> 83 </month> <pages> 8390-8394. </pages>
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <journal> IEEE Computer, </journal> <volume> 21 </volume> <pages> 105-117. </pages>
Reference: <author> MacKay, D. J. C. and Miller, K. D. </author> <year> (1990). </year> <title> Analysis of Linsker's simulation of Hebbian rules. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <month> 173-187. </month> <title> 7 Bright (dark) circles represent positive (negative) weights. The connections are divided into two groups, one with inhibitory connections, the other one with excitatory connections. Both groups are separated by a "fuzzy axis" (black division line) through the center of the receptive field. Its rotation angle determines the alignment of the edge provoking maximal response. training boundaries between inhibitory and excitatory weights are shown. Distances between field centers are "blown up" to avoid confusion caused by overlaps. </title>
Reference-contexts: Rubner and Schulten, 1990) that can lead to well-known feature detectors. For instance, in case of Gaussian input distributions, Linsker's linear approach (Linsker, 1986b; Linsker, 1986a) for single output units also generates certain kinds of orientation sensitive fields <ref> (see also MacKay and Miller, 1990) </ref>. This holds for more structured input data as well (Linsker, personal communication, 1994).
Reference: <author> Miller, K. D. </author> <year> (1994). </year> <title> A model for the development of simple cell receptive fields and the ordered arrangement of orientation columns through activity-dependent competition between on- and off-center inputs. </title> <journal> Journal of Neuroscience, </journal> <volume> 14(1) </volume> <pages> 409-441. </pages>
Reference: <author> Rubner, J. and Schulten, K. </author> <year> (1990). </year> <title> Development of feature detectors by self-organization: A network model. </title> <journal> Biological Cybernetics, </journal> <volume> 62 </volume> <pages> 193-199. </pages>
Reference-contexts: See Figure 8 for an example. 4 DISCUSSION We do not claim that PM is the only parallel method <ref> (as opposed to sequential methods, e.g. Rubner and Schulten, 1990) </ref> that can lead to well-known feature detectors. For instance, in case of Gaussian input distributions, Linsker's linear approach (Linsker, 1986b; Linsker, 1986a) for single output units also generates certain kinds of orientation sensitive fields (see also MacKay and Miller, 1990).
Reference: <author> Rubner, J. and Tavan, P. </author> <year> (1989). </year> <title> A self-organization network for principal-component analysis. </title> <journal> Europhysics Letters, </journal> <volume> 10 </volume> <pages> 693-698. </pages>
Reference: <author> Schmidhuber, J. H. </author> <year> (1992). </year> <title> Learning factorial codes by predictability minimization. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 863-879. </pages>
Reference-contexts: But how to achieve this goal in a massively parallel, local, efficient, and perhaps even biologically plausible way? Predictability minimization (PM). The simple approach in this paper is based on the recent principle of predictability minimization (PM) <ref> (Schmidhuber, 1992) </ref>. A feedforward network with n output units (or code units) sees input patterns with redundant components. Its goal is to respond with informative but less redundant output patterns, ideally by creating a factorial (statistically nonredundant) code of the input ensemble (Barlow et al., 1989). <p> This encourages high information throughput and redundancy reduction. Predictors and code generating net may have hidden units. In this paper, however, they don't. See text for details. The clue is: the code units are trained (in our experiments by online backprop) to maximize essentially the same objective function <ref> (Schmidhuber, 1992) </ref> the predictors try to minimize: V C = i;p p p Predictors and code units co-evolve by fighting each other. Justification. Let us assume that the P i never get trapped in local minima and always perfectly learn the conditional expectations. <p> Justification. Let us assume that the P i never get trapped in local minima and always perfectly learn the conditional expectations. It then turns out that the objective function V C is essentially equivalent to the following one <ref> (also given in Schmidhuber, 1992) </ref>: X V AR (y i ) i;p p where y i denotes the mean activation of unit i, and VAR denotes the variance operator.
Reference: <author> Schmidhuber, J. H. </author> <year> (1993). </year> <institution> Netzwerkarchitekturen, Zielfunktionen und Kettenregel. Habilitations-schrift, Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: The equivalence of (2) and (3) was observed by Peter Dayan, Richard Zemel and Alex Pouget (personal communication, SALK Institute, 1992 | see <ref> (Schmidhuber, 1993) </ref> for details). (3) gives some intuition about what is going on while (2) is maximized. Mazimizing the first term of (3) tends to enforce binary units, and also local maximization of information throughput (given the binary constraint).
Reference: <author> Schmidhuber, J. H. </author> <year> (1994). </year> <title> Neural predictors for detecting and removing redundant information. </title> <editor> In Cruse, H., Dean, J., and Ritter, H., editors, </editor> <booktitle> Adaptive Behavior and Learning, </booktitle> <volume> number 9, </volume> <pages> pages 135-145. </pages> <note> Center for Interdisciplinary Research, </note> <editor> Universitat Bielefeld. </editor> <title> 8 per field and a symmetric arrangement of receptive field centers (essentially, on the circular boundary of each field there are 6 other field centers), the weight patterns generated by the system tend to be either off-center-on-surround-like (see figure) or on-center-off-surround-like. </title> <type> 9 </type>
References-found: 17

