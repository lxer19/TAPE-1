URL: http://polaris.cs.uiuc.edu/reports/1341.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: ISO Series on Parallel Numerical Algorithms. On the Development of Libraries and their use in Applications  
Author: Kyle A. Gallivan, Bret A. Marsolf, William Jalby, and Ahmed H. Sameh 
Address: 1308 West Main Street Urbana, Illinois 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Date: May 1995  
Note: To appear in a forthcoming volume of The NATO  
Pubnum: CSRD Report No. 1341  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aho, A. V., Sethi, R., and Ullman, J. D., </author> <year> 1986. </year> <title> Compilers: Principles, Techniques, and Tools. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts. </address>
Reference-contexts: Current compilers perform optimizations by restructuring arithmetic operations using the algebraic rules for the scalar values <ref> (Aho, Sethi, and Ullman, 1986) </ref>. If algebraic rules are known for the higher-level objects used, such as vectors and matrices, then algebraic restructuring of these objects can also take place. For instance, by defining the algebraic rules for matrix arithmetic, the matrix operations could be restructured to improve performance.
Reference: <author> Angerson, E. et al., </author> <year> 1990. </year> <title> LAPACK: A portable linear algebra library for high-performance computers. </title> <booktitle> In Proc. of Supercomputing '90 , pp. </booktitle> <pages> 2-11. </pages>
Reference-contexts: General-Purpose Linear Algebra Libraries LINPACK (Dongarra et al., 1979) Solves linear system problems for dense, banded, tridiag onal, & triangular matrices. EISPACK (Smith et al., 1976) Solves eigenvalue & eigenvector problems for dense, banded, & tridiagonal matrices. LAPACK <ref> (Angerson et al., 1990) </ref> Solves linear system, eigenvalue, & least-square problems for dense & triangular matrices. The third set of libraries are those which solve linear systems represented by sparse matrices. <p> Specialized Matrix Classes AMR++ (Balsara, Lemke, and Quinlan, 1992) An abstraction of the M++ matrix class (by Dyad Software) for adaptive mesh refinement. LAPACK++ (Dongarra, Pozo, and Walker, 1993) A C++ extension of the LAPACK li brary <ref> (Angerson et al., 1990) </ref>. Lapack.h++ (Vermeulen, 1993) An object-oriented interface to the LAPACK library using the Math.h++ and the Matrix.h++ class libraries (by Rogue Wave Software). The third set of matrix classes are those which are defined for parallel systems.
Reference: <author> Balsara, D., Lemke, M., and Quinlan, D., </author> <year> 1992. </year> <title> AMR++, a C++ object-oriented class library for parallel adaptive mesh refinement fluid dynamics applications. Adaptive, Multilevel, and Hierarchical Computational Strategies 157 413-433. </title>
Reference-contexts: The second set of matrix classes are those which are defined for a specific problem type or interface. One of these classes is designed to support adaptive mesh refinement and the other two are designed to provide the functionality of the LAPACK library. Specialized Matrix Classes AMR++ <ref> (Balsara, Lemke, and Quinlan, 1992) </ref> An abstraction of the M++ matrix class (by Dyad Software) for adaptive mesh refinement. LAPACK++ (Dongarra, Pozo, and Walker, 1993) A C++ extension of the LAPACK li brary (Angerson et al., 1990).
Reference: <author> Barrett, R. et al., </author> <year> 1993. </year> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. </title> <journal> SIAM. </journal> <volume> 24 Beguelin, </volume> <editor> A. et al., </editor> <year> 1991. </year> <title> A User's Guide to PVM Parallel Virtual Machine. </title> <type> Tech. Rep. </type> <institution> ORNL/TM-11826, Oak Ridge Nat'l. Lab. </institution>
Reference-contexts: Other techniques have been tried that attempt to address some of the issues in library development. In order to give application programmers more flexibility with the algorithms, the approach in <ref> (Barrett et al., 1993) </ref> is to distribute only the algebraic algorithms (called templates), instead of code, and allow the implementor to make the required optimizations.
Reference: <author> Birchenhall, C. R., </author> <year> 1993. </year> <title> A Draft Guide to MatClass: A matrix class for C++. </title>
Reference-contexts: The basic arithmetic operations for matrices are provided by all of these classes, but the higher level operations supported vary between the classes. General-Purpose Matrix Classes MatClass <ref> (Birchenhall, 1993) </ref> Supports operations such as LU, Cholesky, QR, and SVD decompositions and least square methods on rectangular matrices. newmat07 (Davies, 1994) Supports operations such as inverse, SVD and Cholesky decompositions, Householder transforms, FFT, and eigenvalues on rectangular, triangular, sym metric, & banded matrices.
Reference: <author> Bodin, F. et al., </author> <year> 1993. </year> <title> Distributed pC++: Basic ideas for an object parallel language. </title> <booktitle> In OON-SKI'93 Proc. of the 1st Ann. Object-Oriented Numerics Conf. </booktitle> <pages> pp. 1-24. </pages>
Reference-contexts: In a similar manner, Cedar Fortran (Hoeflinger, 1991) was designed to utilize the clusters of the Cedar architecture. More generic languages have also been developed such as HPF (High Performance Fortran Forum, 1993) and pC++ <ref> (Bodin et al., 1993) </ref> which do not target a specific machine, but instead target a class of parallel machines. These types of languages make it easier to utilize the capabilities of the architecture, but they do not provide any additional understanding of the algorithms or the application under development.
Reference: <author> Budge, K. G., </author> <year> 1994. </year> <title> C++ optimization and excluding middle-level code. </title> <booktitle> In OON-SKI'94 Proc. of the 2nd Ann. Object-Oriented Numerics Conf. </booktitle> <pages> pp. 107-121. </pages>
Reference-contexts: Methods for improving object-oriented library performance must deal not only with the performance of the code within libraries, but also with the optimization of the method invocations. Current compilers have been found to have difficulties optimizing C++ codes which contain frequent calls to classes <ref> (Budge, 1994) </ref>. For each algebraic operation, a decision needs to be made about how to implement the code in C++, based on the operation being performed and the data types of the operands.
Reference: <author> Carr, S. and Kennedy, K., </author> <year> 1992. </year> <title> Compiler blockability of numerical algorithms. </title> <booktitle> In Proc. of Supercomputing '92 pp. </booktitle> <pages> 114-124. </pages>
Reference-contexts: This is, of course, due to the inevitable loss of key information inherent when expressing the algorithm in a particular programming language. Carr and Kennedy have worked on compiler optimizations to automatically block numerical algorithms for dense linear algebra on machines with memory hierarchies <ref> (Carr and Kennedy, 1992) </ref>. Even for this more specific restructuring task, they have encountered the fundamental difficulty of many high-performance algorithms: the best block algorithms cannot always be derived solely based on the operations and computational primitives which are present in the sequential code.
Reference: <author> Choi, J., Dongarra, J. J., and Walker, D. W, </author> <year> 1994. </year> <title> PB-BLAS: A set of parallel block basic lin ear algebra subprograms. </title> <booktitle> In Proc. of SHPCC '94: Scalable High-Performance Computing Conf. </booktitle> <pages> pp. 534-541. </pages>
Reference-contexts: The first is the development of communication/synchronization libraries based upon standardized interfaces such as the MPI standard (Walker, 1993). The second is the development of parallel numerical libraries such as PB-BLAS <ref> (Choi, Dongarra, and Walker, 1994) </ref> where communication/synchronization is hidden from the user as much as possible within the body of the library itself. Standard entry points are provided at the appropriate functional levels.
Reference: <author> Davies, R. B., </author> <year> 1994. </year> <title> Writing a matrix package in C++. </title> <booktitle> In OON-SKI'94 Proc. of the 2nd Ann. Object-Oriented Numerics Conf. </booktitle> <pages> pp. 207-213. </pages>
Reference-contexts: The basic arithmetic operations for matrices are provided by all of these classes, but the higher level operations supported vary between the classes. General-Purpose Matrix Classes MatClass (Birchenhall, 1993) Supports operations such as LU, Cholesky, QR, and SVD decompositions and least square methods on rectangular matrices. newmat07 <ref> (Davies, 1994) </ref> Supports operations such as inverse, SVD and Cholesky decompositions, Householder transforms, FFT, and eigenvalues on rectangular, triangular, sym metric, & banded matrices. Matrix (Tiller and Dantzig, 1994) Supports the Biconjugate-Gradient, Gauss, Jacobi, SOR, and YSMP linear system solvers for rectangular, banded, & sparse matrices.
Reference: <author> DeRose, L. et al., </author> <year> 1994. </year> <title> An environment for the rapid prototyping and development of numer ical programs and libraries for scientific computation. </title> <booktitle> In Proc. of the DAGS'94 Symp.: Parallel Computation and Problem Solving Environments, </booktitle> <pages> pp. 11-25. </pages>
Reference-contexts: This framework is available in an environment that is being developed for the interactive development of numerical program and libraries <ref> (DeRose et al., 1994) </ref>. In order to support the iterative process of development, the environment provides the capability to transform the code both within a specific representations and between representations.
Reference: <author> Dodson, D. S., Grimes, R. G., and Lewis, J. G., </author> <year> 1991. </year> <title> Sparse extensions to the FORTRAN basic linear algebra subprograms. </title> <journal> ACM Trans. on Math. Soft. </journal> <volume> 17, 2, </volume> <pages> pp. 253-263. </pages>
Reference-contexts: Level 2 BLAS (Dongarra et al., 1988) Provides basic linear algebra operations, typically vector-matrix operations, for dense matrices. Level 3 BLAS (Dongarra et al., 1990) Provides basic linear algebra operations, typically matrix-matrix operations, for dense matrices. Sparse BLAS <ref> (Dodson, Grimes, and Lewis, 1991) </ref> Provides level 1 BLAS operations for sparse matrices. Sparse BLAS Toolkit (Duff, Marrone, and Radicati, 1992) Provides level 3 BLAS operations for sparse matrices. The general-purpose libraries support operations that are more complex than the basic operations just described.
Reference: <author> Dongarra, J. J. et al., </author> <year> 1979. </year> <title> LINPACK: Users' Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia. </address>
Reference-contexts: Though most of these libraries are portable, a few can be optimized 4 through the use of parameters and, for a few high-performance architectures, machine-specific libraries have been written, such as CMSSL on the CM-5 (Thinking Machines Corporation, 1994). General-Purpose Linear Algebra Libraries LINPACK <ref> (Dongarra et al., 1979) </ref> Solves linear system problems for dense, banded, tridiag onal, & triangular matrices. EISPACK (Smith et al., 1976) Solves eigenvalue & eigenvector problems for dense, banded, & tridiagonal matrices. LAPACK (Angerson et al., 1990) Solves linear system, eigenvalue, & least-square problems for dense & triangular matrices.
Reference: <author> Dongarra, J. et al., </author> <year> 1988. </year> <title> An extended set of FORTRAN basic linear algebra subprograms. </title> <journal> ACM Trans. on Math. Soft. </journal> <volume> 14, 1, </volume> <pages> pp. 1-17. </pages>
Reference-contexts: Libraries at this level are sometimes optimized for specific machine architectures using methods such as assembly code, vectorization, or parallelization. Libraries of Basic Operations Level 1 BLAS (Lawson et al., 1979) Provides basic linear algebra operations, typically at a vector level. Level 2 BLAS <ref> (Dongarra et al., 1988) </ref> Provides basic linear algebra operations, typically vector-matrix operations, for dense matrices. Level 3 BLAS (Dongarra et al., 1990) Provides basic linear algebra operations, typically matrix-matrix operations, for dense matrices. Sparse BLAS (Dodson, Grimes, and Lewis, 1991) Provides level 1 BLAS operations for sparse matrices.
Reference: <author> Dongarra, J. et al., </author> <year> 1990. </year> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans. on Math. Soft. </journal> <volume> 16, 1, </volume> <pages> pp. 1-17. </pages>
Reference-contexts: Libraries of Basic Operations Level 1 BLAS (Lawson et al., 1979) Provides basic linear algebra operations, typically at a vector level. Level 2 BLAS (Dongarra et al., 1988) Provides basic linear algebra operations, typically vector-matrix operations, for dense matrices. Level 3 BLAS <ref> (Dongarra et al., 1990) </ref> Provides basic linear algebra operations, typically matrix-matrix operations, for dense matrices. Sparse BLAS (Dodson, Grimes, and Lewis, 1991) Provides level 1 BLAS operations for sparse matrices. Sparse BLAS Toolkit (Duff, Marrone, and Radicati, 1992) Provides level 3 BLAS operations for sparse matrices.
Reference: <author> Dongarra, J. J., Pozo, R., and Walker, D. W., </author> <year> 1993. </year> <title> LAPACK++: A design overview of object-oriented extensions for high-performance linear algebra. </title> <institution> University of Tennessee. </institution>
Reference-contexts: These libraries tend to be implemented in the traditional method using subroutines with arguments of a specific data type. Portable Communication Libraries PVM (Beguelin et al., 1991) Contains send, receive, and broadcast primitives for 1-D arrays; also has barrier synchronization. BLACS <ref> (Dongarra, van de Geijn, and Whaley, 1993) </ref> Contains send, receive, and broadcast primitives for rectangular and trapezoidal matrices; also has global operators for minimum, maximum, and summation. 5 MPI (Walker, 1993) Contains send, receive and broadcast primitives for user defined datatypes; also has global operations for reduction and scan. 3.3. <p> One of these classes is designed to support adaptive mesh refinement and the other two are designed to provide the functionality of the LAPACK library. Specialized Matrix Classes AMR++ (Balsara, Lemke, and Quinlan, 1992) An abstraction of the M++ matrix class (by Dyad Software) for adaptive mesh refinement. LAPACK++ <ref> (Dongarra, Pozo, and Walker, 1993) </ref> A C++ extension of the LAPACK li brary (Angerson et al., 1990). Lapack.h++ (Vermeulen, 1993) An object-oriented interface to the LAPACK library using the Math.h++ and the Matrix.h++ class libraries (by Rogue Wave Software).
Reference: <author> Dongarra, J. J., Pozo, R., and Walker, D. W., </author> <year> 1993a. </year> <title> An object oriented design for high performance linear algebra on distributed memory architectures. </title> <booktitle> In OON-SKI'93 Proc. of the 1st Ann. Object-Oriented Numerics Conf., </booktitle> <pages> pp. 257-264. </pages>
Reference-contexts: This set includes matrix classes which are designed for both general problems and specific problem types. 6 Parallel Matrix Classes P++ (Lemke and Quinlan, 1993) An architecture independent parallel abstraction of the M++ matrix class (by Dyad Software) for structured grid methods. ScaLAPACK++ <ref> (Dongarra, Pozo, and Walker, 1993a) </ref> An object-oriented extension of the LAPACK library (using LAPACK++) for distributed memory architectures. OOSkit (Yang, 1993) Supports a matrix class with operations such as linear system solving, differentiation, integration, and ODE's with message-passing parallelism. 3.4.
Reference: <author> Dongarra, J. J., van de Geijn, R. A., and Whaley, R. C., </author> <year> 1993. </year> <title> A User's Guide to the BLACS. </title> <institution> University of Tennessee. </institution>
Reference-contexts: These libraries tend to be implemented in the traditional method using subroutines with arguments of a specific data type. Portable Communication Libraries PVM (Beguelin et al., 1991) Contains send, receive, and broadcast primitives for 1-D arrays; also has barrier synchronization. BLACS <ref> (Dongarra, van de Geijn, and Whaley, 1993) </ref> Contains send, receive, and broadcast primitives for rectangular and trapezoidal matrices; also has global operators for minimum, maximum, and summation. 5 MPI (Walker, 1993) Contains send, receive and broadcast primitives for user defined datatypes; also has global operations for reduction and scan. 3.3. <p> One of these classes is designed to support adaptive mesh refinement and the other two are designed to provide the functionality of the LAPACK library. Specialized Matrix Classes AMR++ (Balsara, Lemke, and Quinlan, 1992) An abstraction of the M++ matrix class (by Dyad Software) for adaptive mesh refinement. LAPACK++ <ref> (Dongarra, Pozo, and Walker, 1993) </ref> A C++ extension of the LAPACK li brary (Angerson et al., 1990). Lapack.h++ (Vermeulen, 1993) An object-oriented interface to the LAPACK library using the Math.h++ and the Matrix.h++ class libraries (by Rogue Wave Software).
Reference: <author> Duff, I. S., Marrone, M., and Radicati, G., </author> <year> 1992. </year> <title> A proposal for user level sparse BLAS. </title> <type> Tech. rep., </type> <institution> CERFACS. </institution>
Reference-contexts: Level 3 BLAS (Dongarra et al., 1990) Provides basic linear algebra operations, typically matrix-matrix operations, for dense matrices. Sparse BLAS (Dodson, Grimes, and Lewis, 1991) Provides level 1 BLAS operations for sparse matrices. Sparse BLAS Toolkit <ref> (Duff, Marrone, and Radicati, 1992) </ref> Provides level 3 BLAS operations for sparse matrices. The general-purpose libraries support operations that are more complex than the basic operations just described.
Reference: <author> Duff, I. S. and Reid, J. K., </author> <year> 1979. </year> <title> Some design features of a sparse matrix code. </title> <journal> ACM Trans. on Math. Soft. </journal> <volume> 5, 1, </volume> <pages> pp. 18-35. </pages>
Reference-contexts: This complexity comes from the use of sparse data structures and the additional data operations sparse algorithms use in order to reduce the number of arithmetic operations. Sparse Matrix Solvers MA28 <ref> (Duff and Reid, 1979) </ref> Performs the direct solution of linear systems for unsymmetric matrices. Y12M (Zlatev, Wasniewski, and Schaumburg, 1981) Performs the direct and iterative solu tion of linear systems for unsymmetric matrices. SPARSPAK-C (George and Ng, 1987) Performs the direct solution of linear systems for unsymmetric matrices.
Reference: <author> Eaton, J. W., </author> <year> 1993. </year> <title> Octave: A high-level interactive language for numerical computations. </title>
Reference-contexts: The first set of programming environments are the general programming environments. These systems provide interactive environments for developing or solving linear algebra problems. General Programming Environments MATLAB (The Math Works Inc., 1992) An interactive computing environment for perform ing numerical linear algebra with dense & sparse matrices. Octave <ref> (Eaton, 1993) </ref> A high-level interactive environment for numerical computations with dense matrices. Khoros (Edwards and Hayes, 1993) A visual programming environment which has been combined with the ITPACK library of iterative methods for solving linear equations with dense, diagonal, triangular, & sparse matrices.
Reference: <author> Edwards, H. C. and Hayes, L. J., </author> <year> 1993. </year> <title> Visual programming of iterative methods. </title> <booktitle> In OON SKI'93 Proc. of the 1st Ann. Object-Oriented Numerics Conf., </booktitle> <pages> pp. 163-170. </pages>
Reference-contexts: General Programming Environments MATLAB (The Math Works Inc., 1992) An interactive computing environment for perform ing numerical linear algebra with dense & sparse matrices. Octave (Eaton, 1993) A high-level interactive environment for numerical computations with dense matrices. Khoros <ref> (Edwards and Hayes, 1993) </ref> A visual programming environment which has been combined with the ITPACK library of iterative methods for solving linear equations with dense, diagonal, triangular, & sparse matrices. The second set of programming environments are parallel programming environments. <p> To improve this aspect of using libraries, tools for browsing and selecting library subroutines are being developed. For instance, the visual programming environment Khoros has been combined with ITPACK <ref> (Edwards and Hayes, 1993) </ref> to present the subroutines within the library as icons. The user selects the icons from the library, graphically connects them, and the environment automatically generates the code to perform the operations.
Reference: <author> Gallivan, K. and Marsolf, B., </author> <year> 1994. </year> <title> Practical issues related to developing object-oriented numerical libraries. </title> <booktitle> In OON-SKI'94 Proc. of the 2nd Ann. Object-Oriented Numerics Conf., </booktitle> <pages> pp. 93-106. </pages>
Reference: <author> Gallivan, K. A., Plemmons, P. J., and Sameh, A. H., </author> <year> 1990. </year> <title> Parallel algorithms for dense linear algebra computations. </title> <journal> SIAM Review 32, </journal> <volume> 1, </volume> <pages> pp. 54-135. </pages>
Reference-contexts: Gallivan et al. have pointed out that it is often possible to move easily from standard algorithms with limited parallelism to highly parallel algorithms by using "higher level transformations" that exploit algebraic knowledge of the operations <ref> (Gallivan, Plemmons, and Sameh, 1990) </ref>. An alternative to detecting parallelism in existing sequential languages involves new languages designed to complement a specific class of architecture. For example, CM Fortran (Thinking Machines Corporation, 1991) was designed to utilize the data parallel capabilities of the CM-2 and CM-5.
Reference: <author> George, A. and Ng, E., </author> <year> 1987. </year> <title> Symbolic factorization for sparse gaussian elimination with partial pivoting. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6, </volume> <pages> pp. 877-898. </pages>
Reference-contexts: Sparse Matrix Solvers MA28 (Duff and Reid, 1979) Performs the direct solution of linear systems for unsymmetric matrices. Y12M (Zlatev, Wasniewski, and Schaumburg, 1981) Performs the direct and iterative solu tion of linear systems for unsymmetric matrices. SPARSPAK-C <ref> (George and Ng, 1987) </ref> Performs the direct solution of linear systems for unsymmetric matrices. SLES (Gropp and Smith, 1993) Performs the direct and iterative solution of linear systems for sparse & dense matrices. 3.2.
Reference: <author> Gropp, W. and Smith, B., </author> <year> 1993. </year> <title> Simplified Linear Equation Solvers Users Manual. </title> <type> Tech. Rep. </type> <institution> ANL-93/8, Argonne Nat'l. Lab. </institution>
Reference-contexts: Y12M (Zlatev, Wasniewski, and Schaumburg, 1981) Performs the direct and iterative solu tion of linear systems for unsymmetric matrices. SPARSPAK-C (George and Ng, 1987) Performs the direct solution of linear systems for unsymmetric matrices. SLES <ref> (Gropp and Smith, 1993) </ref> Performs the direct and iterative solution of linear systems for sparse & dense matrices. 3.2. Communication libraries The communication libraries being used each define a portable interface to message passing communication on distributed memory machines. <p> This is an example of the observation made above concerning the assumptions at the different user levels. Clearly, this approach simplifies one researchers design task by complicating someone else's. The approach in <ref> (Gropp and Smith, 1993) </ref> has concentrated on standardizing the interface to the library's algorithms and data structures. This allows both the algorithms and data structures to be interchanged with almost no changes to the calling program.
Reference: <author> Guidec, F. and Jezequel, J.-M., </author> <year> 1993. </year> <title> Numeric parallel programming with sequential object oriented languages. </title> <booktitle> In OON-SKI'93 Proc. of the 1st Ann. Object-Oriented Numerics Conf., </booktitle> <pages> pp. </pages> <month> 55-69. </month> <title> 25 High Performance Fortran Forum, 1993. High Performance Fortran Language Specification. </title> <note> Version 1.0. </note>
Reference-contexts: Parallel Programming Environments CONLAB (Jacobson, Kagstrom, and Rannar, 1992) An interactive environment for developing matrix algorithms for distributed memory multicomputers. It supports a subset of the MATLAB language with the addition of primitives for parallelism, synchronization, and communication. EPEE <ref> (Guidec and Jezequel, 1993) </ref> Allows the parallel programming of numerical problems in the Eiffel programming language. The parallelism is provided through data parallelism along with the SPMD programming model. 7 ESP (Lamb, Furnish, and Gray, 1993) Provides an environment which supports concurrent execution of object-oriented programs.
Reference: <author> Hoeflinger, J., </author> <year> 1991. </year> <title> Cedar Fortran Programmer's Handbook. </title> <type> Tech. rep. 1157, </type> <institution> CSRD, Univ. of Illinois. </institution>
Reference-contexts: An alternative to detecting parallelism in existing sequential languages involves new languages designed to complement a specific class of architecture. For example, CM Fortran (Thinking Machines Corporation, 1991) was designed to utilize the data parallel capabilities of the CM-2 and CM-5. In a similar manner, Cedar Fortran <ref> (Hoeflinger, 1991) </ref> was designed to utilize the clusters of the Cedar architecture. More generic languages have also been developed such as HPF (High Performance Fortran Forum, 1993) and pC++ (Bodin et al., 1993) which do not target a specific machine, but instead target a class of parallel machines.
Reference: <author> Houstis, E. N. et al., </author> <year> 1990. </year> <title> //ELLPACK: A numerical simulation programming environment for parallel MIMD machines. </title> <booktitle> In Proc. 1990 Int'l. Conf. on Supercomputing, </booktitle> <pages> pp. 96-107. </pages>
Reference-contexts: An alternative approach has been to provide the user with more functionality at the cost 11 of less control. This approach has resulted in problem-solving environments, libraries and languages targeted to a specific application area. For instance, //ELLPACK <ref> (Houstis et al., 1990) </ref> is used to perform numerical simulations via the solution of partial differential equations.
Reference: <author> Jacobson, P., Kagstrom, B., and Rannar, M., </author> <year> 1992. </year> <title> Algorithm development for distributed memory multicomputers using CONLAB. </title> <booktitle> Scientific Programming 1 , pp. </booktitle> <pages> 185-203. </pages>
Reference-contexts: The second set of programming environments are parallel programming environments. These environments include both interactive environments and programming systems to help with the development of parallel programs. Parallel Programming Environments CONLAB <ref> (Jacobson, Kagstrom, and Rannar, 1992) </ref> An interactive environment for developing matrix algorithms for distributed memory multicomputers. It supports a subset of the MATLAB language with the addition of primitives for parallelism, synchronization, and communication.
Reference: <author> Kohn, S. R. and Baden, S. B., </author> <year> 1994. </year> <title> A robust parallel programming model for dynamic non uniform scientific computations. </title> <booktitle> In Proc. of SHPCC '94: Scalable High-Performance Computing Conf., </booktitle> <pages> pp. 509-517. </pages>
Reference-contexts: Another example, the LPARX system <ref> (Kohn and Baden, 1994) </ref>, defines C++ classes for the run-time support of calculations using irregular, block-oriented data structures, such as adaptive meshes, which can be operated on in parallel.
Reference: <author> Lamb, S., Furnish, G., and Gray, M., </author> <year> 1993. </year> <title> A distributed environment for plasma simulation. </title> <booktitle> In OON-SKI'93 Proc. of the 1st Ann. Object-Oriented Numerics Conf., </booktitle> <pages> pp. 155-162. </pages>
Reference-contexts: EPEE (Guidec and Jezequel, 1993) Allows the parallel programming of numerical problems in the Eiffel programming language. The parallelism is provided through data parallelism along with the SPMD programming model. 7 ESP <ref> (Lamb, Furnish, and Gray, 1993) </ref> Provides an environment which supports concurrent execution of object-oriented programs. The parallelism is supported through remote object creation, concurrency control, and implicit messaging between objects and between nodes. 4.
Reference: <editor> Lawson, C. L. et al., </editor> <year> 1979. </year> <title> Basic linear algebra subprograms for Fortran usage. </title> <journal> ACM Trans. on Math. Soft. </journal> <volume> 5, 3, </volume> <pages> pp. 308-323. </pages>
Reference-contexts: Typical operations include dot-products, vector triads, matrix-vector multiplication, rank-k updates, and triangular solves, with support for either dense or sparse data structures. Libraries at this level are sometimes optimized for specific machine architectures using methods such as assembly code, vectorization, or parallelization. Libraries of Basic Operations Level 1 BLAS <ref> (Lawson et al., 1979) </ref> Provides basic linear algebra operations, typically at a vector level. Level 2 BLAS (Dongarra et al., 1988) Provides basic linear algebra operations, typically vector-matrix operations, for dense matrices. Level 3 BLAS (Dongarra et al., 1990) Provides basic linear algebra operations, typically matrix-matrix operations, for dense matrices.
Reference: <author> Lemke, M. and Quinlan, D., </author> <year> 1993. </year> <title> P++, a parallel C++ array class library for architecture independent development of numerical software. </title> <booktitle> In OON-SKI'93 Proc. of the 1st Ann. Object-Oriented Numerics Conf., </booktitle> <pages> pp. 268-269. </pages> <publisher> The Math Works Inc., </publisher> <year> 1992. </year> <title> MATLAB Reference Guide. </title>
Reference-contexts: The third set of matrix classes are those which are defined for parallel systems. All of these classes are designed to run on message passing parallel systems. This set includes matrix classes which are designed for both general problems and specific problem types. 6 Parallel Matrix Classes P++ <ref> (Lemke and Quinlan, 1993) </ref> An architecture independent parallel abstraction of the M++ matrix class (by Dyad Software) for structured grid methods. ScaLAPACK++ (Dongarra, Pozo, and Walker, 1993a) An object-oriented extension of the LAPACK library (using LAPACK++) for distributed memory architectures.
Reference: <author> Muraoka, Y. and Kuck, D. J., </author> <year> 1973. </year> <title> On the time required for a sequence of matrix products. </title> <journal> Communications of the ACM, </journal> <volume> 16, 1, </volume> <pages> pp. 22-26. </pages>
Reference-contexts: Simple algebraic restructuring has been discussed and successfully applied in the literature <ref> (Muraoka and Kuck, 1973) </ref>. It is possible, and sometimes necessary, to go beyond this level of algebraic restructuring.
Reference: <author> Padua, D. et al., </author> <year> 1993. </year> <title> Polaris: A New-Generation Parallelizing Compiler for MPP's. </title> <type> Tech. rep. 1306, </type> <institution> CSRD, Univ. of Illinois. </institution>
Reference-contexts: These range from restructuring compilers to application-specific environments. These approaches, however, tend to only focus on one aspect of a library or its development and do not address the issues involved over its life-cycle. Compiler projects, such as Polaris <ref> (Padua et al., 1993) </ref> and Parafrase-2 (Polychronopoulos et al., 1989), have been devel 10 oped to search for parallelism within sequential programs.
Reference: <author> Polychronopoulos, C. et al., </author> <year> 1989. </year> <title> Parafrase-2: A new generation parallelizing compiler. </title> <booktitle> In Proc. of 1989 Int'l. Conf. on Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 39-48. </pages>
Reference-contexts: These range from restructuring compilers to application-specific environments. These approaches, however, tend to only focus on one aspect of a library or its development and do not address the issues involved over its life-cycle. Compiler projects, such as Polaris (Padua et al., 1993) and Parafrase-2 <ref> (Polychronopoulos et al., 1989) </ref>, have been devel 10 oped to search for parallelism within sequential programs.
Reference: <author> Smith, B. T. et al., </author> <year> 1976. </year> <title> Matrix Eigensystem Routines - EISPACK Guide Second Edition, </title> <booktitle> vol. 6 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address> <institution> Thinking Machines Corporation, </institution> <year> 1991. </year> <type> CM5 Technical Summary. </type> <institution> Cambridge, Massachusetts. Thinking Machines Corporation, </institution> <year> 1994. </year> <title> CMSSL for CM Fortran, </title> <address> V3.2. Cambridge, </address> <publisher> Mas sachusetts. </publisher>
Reference-contexts: General-Purpose Linear Algebra Libraries LINPACK (Dongarra et al., 1979) Solves linear system problems for dense, banded, tridiag onal, & triangular matrices. EISPACK <ref> (Smith et al., 1976) </ref> Solves eigenvalue & eigenvector problems for dense, banded, & tridiagonal matrices. LAPACK (Angerson et al., 1990) Solves linear system, eigenvalue, & least-square problems for dense & triangular matrices. The third set of libraries are those which solve linear systems represented by sparse matrices.
Reference: <author> Tiller, M. M. and Dantzig, J. A., </author> <year> 1994. </year> <title> FEMLIB: A framework for simulation and opti mization. </title> <booktitle> In OON-SKI'94 Proc. of the 2nd Ann. Object-Oriented Numerics Conf., </booktitle> <pages> pp. 366-378. </pages>
Reference-contexts: General-Purpose Matrix Classes MatClass (Birchenhall, 1993) Supports operations such as LU, Cholesky, QR, and SVD decompositions and least square methods on rectangular matrices. newmat07 (Davies, 1994) Supports operations such as inverse, SVD and Cholesky decompositions, Householder transforms, FFT, and eigenvalues on rectangular, triangular, sym metric, & banded matrices. Matrix <ref> (Tiller and Dantzig, 1994) </ref> Supports the Biconjugate-Gradient, Gauss, Jacobi, SOR, and YSMP linear system solvers for rectangular, banded, & sparse matrices. The second set of matrix classes are those which are defined for a specific problem type or interface.
Reference: <author> Vermeulen, A., </author> <year> 1993. </year> <title> Eigenvalues in Lapack.h++: Object oriented linear algebra software. </title> <booktitle> In OON-SKI'93 Proc. of the 1st Ann. Object-Oriented Numerics Conf., </booktitle> <pages> pp. 314-317. </pages>
Reference-contexts: Specialized Matrix Classes AMR++ (Balsara, Lemke, and Quinlan, 1992) An abstraction of the M++ matrix class (by Dyad Software) for adaptive mesh refinement. LAPACK++ (Dongarra, Pozo, and Walker, 1993) A C++ extension of the LAPACK li brary (Angerson et al., 1990). Lapack.h++ <ref> (Vermeulen, 1993) </ref> An object-oriented interface to the LAPACK library using the Math.h++ and the Matrix.h++ class libraries (by Rogue Wave Software). The third set of matrix classes are those which are defined for parallel systems. All of these classes are designed to run on message passing parallel systems.
Reference: <author> Walker, D. W., </author> <year> 1993. </year> <title> The Design of a Standard Message Passing Interface for Distributed Memory Concurrent Computers. </title> <type> Tech. Rep. </type> <institution> TM-12512, Oak Ridge Nat'l. Lab. </institution>
Reference-contexts: Since the form of the parallelism revolves around two basic attributes of the algorithm, computation and communication, two general responses have evolved. The first is the development of communication/synchronization libraries based upon standardized interfaces such as the MPI standard <ref> (Walker, 1993) </ref>. The second is the development of parallel numerical libraries such as PB-BLAS (Choi, Dongarra, and Walker, 1994) where communication/synchronization is hidden from the user as much as possible within the body of the library itself. Standard entry points are provided at the appropriate functional levels. <p> BLACS (Dongarra, van de Geijn, and Whaley, 1993) Contains send, receive, and broadcast primitives for rectangular and trapezoidal matrices; also has global operators for minimum, maximum, and summation. 5 MPI <ref> (Walker, 1993) </ref> Contains send, receive and broadcast primitives for user defined datatypes; also has global operations for reduction and scan. 3.3. Object-oriented matrix classes These classes are characterized by the definition of an interface to a matrix object for use with C++. <p> One of these classes is designed to support adaptive mesh refinement and the other two are designed to provide the functionality of the LAPACK library. Specialized Matrix Classes AMR++ (Balsara, Lemke, and Quinlan, 1992) An abstraction of the M++ matrix class (by Dyad Software) for adaptive mesh refinement. LAPACK++ <ref> (Dongarra, Pozo, and Walker, 1993) </ref> A C++ extension of the LAPACK li brary (Angerson et al., 1990). Lapack.h++ (Vermeulen, 1993) An object-oriented interface to the LAPACK library using the Math.h++ and the Matrix.h++ class libraries (by Rogue Wave Software).
Reference: <author> Weatherford, S. A., </author> <year> 1994. </year> <title> High-Level Pattern-Matching Extensions to C++ for Fortran Pro gram Manipulation in Polaris. </title> <type> Master's thesis, </type> <institution> University of Illinois. </institution>
Reference-contexts: A pattern matching system is being developed to match the segments of the code being developed to the target code patterns. This system is based in part on the work done to develop a pattern matching system for Polaris by Weatherford <ref> (Weatherford, 1994) </ref>. Within both parts of the transformation there can be data declarations and MATLAB code 20 segments. Also, in the target code pattern, there can be data dependence information.
Reference: <author> Yang, L., </author> <year> 1993. </year> <title> Object-oriented techniques in the design and construction of parallel numerical application software. </title> <booktitle> In OON-SKI'93 Proc. of the 1st Ann. Object-Oriented Numerics Conf., </booktitle> <pages> pp. 303-313. </pages>
Reference-contexts: ScaLAPACK++ (Dongarra, Pozo, and Walker, 1993a) An object-oriented extension of the LAPACK library (using LAPACK++) for distributed memory architectures. OOSkit <ref> (Yang, 1993) </ref> Supports a matrix class with operations such as linear system solving, differentiation, integration, and ODE's with message-passing parallelism. 3.4. Programming environments This section deals with programming environments which help users with numerical linear algebra.
Reference: <author> Zlatev, Z., Wasniewski, J., and Schaumburg, K., </author> <year> 1981. </year> <title> Y12M: Solution of Large and Sparse Systems of Linear Algebraic Equations, </title> <booktitle> vol. 121 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address> <month> 26 </month>
Reference-contexts: This complexity comes from the use of sparse data structures and the additional data operations sparse algorithms use in order to reduce the number of arithmetic operations. Sparse Matrix Solvers MA28 (Duff and Reid, 1979) Performs the direct solution of linear systems for unsymmetric matrices. Y12M <ref> (Zlatev, Wasniewski, and Schaumburg, 1981) </ref> Performs the direct and iterative solu tion of linear systems for unsymmetric matrices. SPARSPAK-C (George and Ng, 1987) Performs the direct solution of linear systems for unsymmetric matrices.
References-found: 44

