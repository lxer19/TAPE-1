URL: http://L2R.cs.uiuc.edu/~danr/Papers/pos.ps.gz
Refering-URL: http://L2R.cs.uiuc.edu/~danr/publications.html
Root-URL: http://www.cs.uiuc.edu
Email: fdanr,zelenkog@cs.uiuc.edu  
Title: Part of Speech Tagging Using a Network of Linear Separators significance in terms of efficiency,
Author: Dan Roth and Dmitry Zelenko 
Address: 1304 W Springfield Ave., Urbana, IL 61801  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign  
Note: To Appear in COLING-ACL '98  The algorithm used is an on-line algorithm: every example is used by the algorithm only once, and is then discarded. This has  
Abstract: We present an architecture and an on-line learning algorithm and apply it to the problem of part-of-speech tagging. The architecture presented, SNOW, is a network of linear separators in the feature space, utilizing the Winnow update algorithm. Multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good behavior when applied to very high dimensional problems, and especially when the target concepts depend on only a small subset of the features in the feature space. In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction selecting the part of speech of a word. The experimental analysis presented here provides more evidence to that these algorithms are suitable for natural language problems. We present an extensive experimental study of our algorithm under various conditions; in particular, it is shown that the algorithm performs comparably to the best known algorithms for POS. 
Abstract-found: 1
Intro-found: 1
Reference: <author> E. Brill. </author> <year> 1995. </year> <title> Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. </title> <journal> Computational Linguistics, </journal> <volume> 21(4) </volume> <pages> 543-565. </pages>
Reference-contexts: This process goes on until the last rule in the list is evaluated. The last labeling is the output of the hypothesis. In its most general setting, the TBL hypothesis is not a classifier <ref> (Brill, 1995) </ref>. The reason is that, in general, the truth value of the condition of the ith rule may change while evaluating one of the preceding rules. <p> The transformation is then applied, and the process is repeated until no more mislabel-ing minimization can be achieved. For example, in POS, the consequence of a transformation labels a word with a part of speech. <ref> (Brill, 1995) </ref> uses lexicon for initial annotation of the training corpus, where each word in the lexicon has a set POS tags seen for the word in the training corpus. <p> After every prediction, the tag output by the SNOW tagger for a word is used for labeling the word in the test data. There 2 The features 1-8 are part of <ref> (Brill, 1995) </ref> features fore, the features of the following words will de-pend on the output tags of the preceding words. 5 Experimental Results The data for all the experiments was extracted from the Penn Treebank WSJ corpus. The training and test corpus consist of 600000 and 150000, respectively.
Reference: <author> E. Brill. </author> <year> 1997. </year> <title> Unsupervised learning of disambiguation rules for part of speech tagging. In Natural Language Processing Using Very Large Corpora. </title> <publisher> Kluwer Academic Press. </publisher>
Reference: <author> C. Cardie, </author> <year> 1996. </year> <title> Embedded Machine Learning Systems for natural language processing: A general framework, </title> <address> pages 315-328. </address> <publisher> Springer. </publisher>
Reference-contexts: More fundamental, we believe, are those that are concerned with the general learning paradigm the SNOW architecture proposes. A large number of different kinds of ambiguities are to be resolved simultaneously in performing any higher level natural language inference <ref> (Cardie, 1996) </ref>. Naturally, these processes, acting on the same input and using the same "memory", will interact. In SNOW, a collection of classifiers are used; all are learned from the same data, and share the same "memory".
Reference: <author> R. O. Duda and P. E. Hart. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley. </publisher>
Reference: <author> A. R. Golding and D. Roth. </author> <year> 1998. </year> <title> A winnow based approach to context-sensitive spelling correction. </title> <journal> Machine Learning. </journal> <note> Special issue on Machine Learning and Natural Language; . Preliminary version appeared in ICML-96. </note>
Reference-contexts: A target node in the network corresponds to a candidate in the disambiguation task; all subnet-works learn autonomously from the same data in an online fashion, and at run time, they compete for assigning the correct meaning. A similar architecture which includes an additional layer is described in <ref> (Golding and Roth, 1998) </ref>. The POS problem suggests a special challenge to this approach. First, the problem is a multi-class prediction problem.
Reference: <author> M. Herbster and M. Warmuth. </author> <year> 1995. </year> <title> Tracking the best expert. </title> <booktitle> In Proc. 12th International Conference on Machine Learning, </booktitle> <pages> pages 286-294. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> J. Kivinen and M. K. Warmuth. </author> <year> 1995. </year> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <note> In Proc. of STOC. Tech Report UCSC-CRL-94-16. </note>
Reference: <author> Y. Krymolowski and D. Roth. </author> <year> 1998. </year> <title> Incorporating knowledge in natural language learning: A case study. </title> <booktitle> COLING-ACL Workshop. </booktitle>
Reference: <author> J. Kupiec. </author> <year> 1992. </year> <title> Robust part-of-speech tagging using a hidden makov model. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 6 </volume> <pages> 225-242. </pages>
Reference: <author> N. Littlestone and M. K. Warmuth. </author> <year> 1994. </year> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261. </pages>
Reference-contexts: In this paper we present a learning algorithm and an architecture with properties suitable for this domain. The SNOW algorithm presented here builds on recently introduced theories of multiplicative weight-updating learning algorithms for linear functions. Multiplicative weight-updating algorithms such as Winnow (Littlestone, 1988) and Weighted Majority <ref> (Littlestone and War-muth, 1994) </ref> have been studied extensively in the COLT literature. Theoretical analysis has shown that they have exceptionally good behavior in the presence of irrelevant attributes, noise, and even a target function changing in time (Littlestone, 1988; Littlestone and War-muth, 1994; Herbster and Warmuth, 1995).
Reference: <author> N. Littlestone. </author> <year> 1988. </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318. </pages>
Reference-contexts: In this paper we present a learning algorithm and an architecture with properties suitable for this domain. The SNOW algorithm presented here builds on recently introduced theories of multiplicative weight-updating learning algorithms for linear functions. Multiplicative weight-updating algorithms such as Winnow <ref> (Littlestone, 1988) </ref> and Weighted Majority (Littlestone and War-muth, 1994) have been studied extensively in the COLT literature. <p> A local learning algorithm, Littlestone's Winnow algorithm <ref> (Littlestone, 1988) </ref>, is used at each target node to learn its dependence on other nodes. Winnow has three parameters: a threshold , and two update parameters, a promotion parameter ff &gt; 1 and a demotion parameter 0 &lt; fi &lt; 1.
Reference: <author> L. A. Ramshaw and M. P. Marcus. </author> <year> 1996. </year> <title> Exploring the nature of transformation-based learning. </title> <editor> In J. Klavans and P. Resnik, editors, </editor> <title> The Balancing Act: Combining Symbolic and Statistical Approaches to Language. </title> <publisher> MIT Press. </publisher>
Reference-contexts: TBL, on the other hand, uses a much larger set of features. Moreover, the learning and tagging mechanism in TBL relies on the interdependence between the produced labels and the features. However, <ref> (Ramshaw and Marcus, 1996) </ref> demonstrate that the inter-dependence impacts only 12% of the predictions. Since the classifier used in TBL without inter-dependence can be represented as a linear separator (Roth, 1998), it is perhaps not surprising that SNOW performs as well as TBL.
Reference: <author> D. Roth. </author> <year> 1998. </year> <title> Learning to resolve natural language ambiguities: A unified approach. </title> <booktitle> In Proc. National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: A target node in the network corresponds to a candidate in the disambiguation task; all subnet-works learn autonomously from the same data in an online fashion, and at run time, they compete for assigning the correct meaning. A similar architecture which includes an additional layer is described in <ref> (Golding and Roth, 1998) </ref>. The POS problem suggests a special challenge to this approach. First, the problem is a multi-class prediction problem. <p> Moreover, the learning and tagging mechanism in TBL relies on the interdependence between the produced labels and the features. However, (Ramshaw and Marcus, 1996) demonstrate that the inter-dependence impacts only 12% of the predictions. Since the classifier used in TBL without inter-dependence can be represented as a linear separator <ref> (Roth, 1998) </ref>, it is perhaps not surprising that SNOW performs as well as TBL. Also, the success of the adaptive SNOW taggers shows that we can alleviate the lack of the inter-dependence by adaptation to the testing corpus.
Reference: <author> H. Schmid. </author> <year> 1994. </year> <title> Part-of-speech tagging with neural networks. </title> <booktitle> In COLING-94. </booktitle>
Reference-contexts: In recent years, a number of approaches have been tried for solving the problem. The most notable methods are based on Hidden Markov Models (HMM)(Kupiec, 1992; Schutze, 1995), transformation rules (Brill, 1995; Brill, 1997), and multi-layer neural networks <ref> (Schmid, 1994) </ref>. HMM taggers use manually tagged training data to compute statistics on features. For example, they can estimate lexical probabilities P rob (wordjtag) and contextual probabilities P rob (tagjprevious n tags).
Reference: <author> H. Schutze. </author> <year> 1995. </year> <title> Distributional part-of-speech tagging. </title> <booktitle> In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics. </booktitle>
References-found: 15

