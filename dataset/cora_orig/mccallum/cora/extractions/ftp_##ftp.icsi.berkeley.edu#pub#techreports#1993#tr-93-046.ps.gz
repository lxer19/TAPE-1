URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-046.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Email: smueller@icsi.berkeley.edu  
Title: A Performance Analysis of the CNS-1 on Large, Dense Backpropagation Networks Connectionist Network Supercomputer  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Author: Silvia M. Muller 
Note: The CNS-1 project is a collaboration of the  
Date: September 1993  
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  University of California at Berkeley and the International Computer Science Institute  
Pubnum: TR-93-046  
Abstract: We determine in this study the sustained performance of the CNS-1 during training and evaluation of large multilayered feedforward neural networks. Using a sophisticated coding, the 128-node machine would achieve up to 111 Giga connections per second (GCPS) and 22 Giga connection updates per second (GCUPS). During recall the machine would archieve 87% of the peak multiply-accumulate performance. The training of large nets is less efficient than the recall but only by a factor of 1.5 to 2. The benchmark is parallelized and the machine code is optimized before analyzing the performance. Starting from an optimal parallel algorithm, CNS specific optimizations still reduce the run time by a factor of 4 for recall and by a factor of 3 for training. Our analysis also yields some strategies for code optimization. The CNS-1 is still in design, and therefore we have to model the run time behavior of the memory system and the interconnection network. This gives us the option of changing some parameters of the CNS-1 system in order to analyze their performance impact. 
Abstract-found: 1
Intro-found: 1
Reference: [ABC + 93] <author> K. Asanovic, J. Beck, T. Callahan, J. Feldman, B. Irissou, B. Kingsbury, P. Kohn, J. Lazzaro, N. Morgan, D. Stoutamire, and J. Wawrzynek. </author> <title> CNS-1 architecture specification. </title> <type> Technical Report TR-93-021, </type> <institution> International Computer Science Institute and UC Berkeley, </institution> <year> 1993. </year>
Reference-contexts: At the moment, there exists no efficient compiler for the machine, and therefore most of the code has to be hand optimized. 1.2 Structure of the analysis The design team describes the design and the functionality of the CNS-1 very detailed in the technical reports <ref> [ABC + 93, Asa93, AC93] </ref>, but until now they have only sketched out the timing behavior of the memory system and the interconnection network. Since for our analysis we also need detailed information on these two components, we model them in 1 chapter 2. <p> Callahan, Jerry Feldman, David Johnson, Brian Kingsbury, Phil Kohn, Nelson Morgan and John Wawrzynek. 2 Chapter 2 Modelling the Timing The Connectionist Network Supercomputer (CNS-1) has been designed for neurocompu-tation and should reach high performance even on sparsely connected neural networks; this at least is one goal of the project <ref> [ABC + 93] </ref>. The CNS is a multicomputer with distributed memory, based on a super-scalar design. The nodes are connected via a mesh with wraparound in one dimension. Each processor has a scalar unit and three coprocessors: a network interface, a memory unit, and a small SIMD array. <p> The scalar unit is a MIPS-based design extended by a few instructions to communicate with its coprocessors. The SIMD unit is accessed via vector operations. The details of the hardware and the ISA are described in <ref> [ABC + 93, Asa93, AC93] </ref>. 2.1 Memory system 2.1.1 Memory hierarchy The CNS-1 has a three level memory hierarchy: separate instruction and data cache on the processor chip, a second level cache in each RDRAM chip, and the RDRAM itself.
Reference: [AC93] <author> K. Asanovic and T. Callahan. </author> <title> Torrent Architecture Manual. </title> <institution> International Computer Science Institute and UC Berkeley, </institution> <year> 1993. </year> <note> Iternal document, revisions 1.5/1.9. </note>
Reference-contexts: At the moment, there exists no efficient compiler for the machine, and therefore most of the code has to be hand optimized. 1.2 Structure of the analysis The design team describes the design and the functionality of the CNS-1 very detailed in the technical reports <ref> [ABC + 93, Asa93, AC93] </ref>, but until now they have only sketched out the timing behavior of the memory system and the interconnection network. Since for our analysis we also need detailed information on these two components, we model them in 1 chapter 2. <p> The scalar unit is a MIPS-based design extended by a few instructions to communicate with its coprocessors. The SIMD unit is accessed via vector operations. The details of the hardware and the ISA are described in <ref> [ABC + 93, Asa93, AC93] </ref>. 2.1 Memory system 2.1.1 Memory hierarchy The CNS-1 has a three level memory hierarchy: separate instruction and data cache on the processor chip, a second level cache in each RDRAM chip, and the RDRAM itself. <p> That allows us to hide almost all the transfer. Otherwise the whole transfer times are visible. 60 Appendix A Assembler Code We write the assembler code in a register transfer language and not in CNS-assembler <ref> [Asa93, AC93] </ref>, but we only use CNS instructions aside from two exceptions. First, we use a multiply-accumulate instruction for the vector unit, and second, we assume that the scalars for vector operations are stored in a special register file in the vector unit.
Reference: [Asa93] <author> K. Asanovic. </author> <title> T0 Reference Manual. </title> <institution> International Computer Science Institute and UC Berkeley, </institution> <year> 1993. </year> <note> Iternal document, revision 1.5. </note>
Reference-contexts: At the moment, there exists no efficient compiler for the machine, and therefore most of the code has to be hand optimized. 1.2 Structure of the analysis The design team describes the design and the functionality of the CNS-1 very detailed in the technical reports <ref> [ABC + 93, Asa93, AC93] </ref>, but until now they have only sketched out the timing behavior of the memory system and the interconnection network. Since for our analysis we also need detailed information on these two components, we model them in 1 chapter 2. <p> The scalar unit is a MIPS-based design extended by a few instructions to communicate with its coprocessors. The SIMD unit is accessed via vector operations. The details of the hardware and the ISA are described in <ref> [ABC + 93, Asa93, AC93] </ref>. 2.1 Memory system 2.1.1 Memory hierarchy The CNS-1 has a three level memory hierarchy: separate instruction and data cache on the processor chip, a second level cache in each RDRAM chip, and the RDRAM itself. <p> That allows us to hide almost all the transfer. Otherwise the whole transfer times are visible. 60 Appendix A Assembler Code We write the assembler code in a register transfer language and not in CNS-assembler <ref> [Asa93, AC93] </ref>, but we only use CNS instructions aside from two exceptions. First, we use a multiply-accumulate instruction for the vector unit, and second, we assume that the scalars for vector operations are stored in a special register file in the vector unit.
Reference: [Cal93] <author> T. Callahan. </author> <title> Network Iinterface Manual. </title> <institution> International Computer Science Institute and UC Berkeley, </institution> <year> 1993. </year> <note> Iternal document, revision 1.4. </note>
Reference: [CSS + 91] <author> D. Culler, A. Sah, K. Schauser, T. von Eichen, and J. Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <year> 1991. </year>
Reference-contexts: The processor overhead for sending and receiving messages is fairly small, because of active messages <ref> [CSS + 91] </ref>. In our analysis, we assume that they are T p;s = T p;r = 10cc (CPU cycles). This includes the overhead of moving the data to the transfer registers.
Reference: [Gre92] <author> M.C. Greenspon. </author> <title> Ring Array Processor: Programmer's guide to the RAP libraries. </title> <type> Technical Report TR-92-060, </type> <institution> International Computer Science Institute, </institution> <year> 1992. </year>
Reference-contexts: For these reasons, both training and recall should be represented in the benchmark. 3.1 Description of the applications Our benchmark focuses on dense three-layer networks with about 10 9 connections. The first kernel deals with recall and the second with training. We use a backpropagation algorithm described in <ref> [MR88, Gre92] </ref>. The underlying neural network has one hidden layer which is completely connected to the other two layers. The network has n i inputs, n h hidden nodes, and n o outputs, and its size varies between 201.3 million and 1.34 billion connections.
Reference: [KSA93] <author> V. Kumar, S. Shekhar, and M.B. Amin. </author> <title> A scalable parallel formulation of the backpropagation algorithm for hypercubes and related architctures. </title> <type> Technical Report TR-92-54, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <address> Minneapolis, MN 55455, </address> <year> 1993. </year>
Reference-contexts: There are p h nodes involved in each of them. This transfer can widely be overlapped with computation, because of active messages. Kumar et al. analyzed in <ref> [KSA93] </ref> several parallel formulations of the backpropagation algorithm for multilayered feedforward networks. They showed that the blockdistribution we are using is optimal for hypercubes and related architectures when using per-pattern training.
Reference: [MR88] <author> J.L. McClelland and D.E. Rumelhart. </author> <title> Explorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises. </title> <publisher> The MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: For these reasons, both training and recall should be represented in the benchmark. 3.1 Description of the applications Our benchmark focuses on dense three-layer networks with about 10 9 connections. The first kernel deals with recall and the second with training. We use a backpropagation algorithm described in <ref> [MR88, Gre92] </ref>. The underlying neural network has one hidden layer which is completely connected to the other two layers. The network has n i inputs, n h hidden nodes, and n o outputs, and its size varies between 201.3 million and 1.34 billion connections.
Reference: [Mul93] <author> U. A. Muller. </author> <title> Simulation of Neural Networks on Parallel Computers, </title> <booktitle> volume 23 of Microelectronics. Hartung-Gorre Verlag, 1993. Reprint of the Ph.D Thesis: </booktitle> <volume> ETH No 10188. </volume>
Reference: [Ram92] <institution> Rambus Inc., Mountain View, California. </institution> <note> Rambus Technology Guide, 0.90 - preliminary edition, </note> <month> May </month> <year> 1992. </year> <month> 75 </month>
Reference-contexts: In the case of a cache miss, the access time also depends on the dirty flag; the write back of a dirty cache line takes some additional cycles (Table 2.2). The data sheets <ref> [Ram92, Tos92b, Tos92a] </ref> specify the memory access times in nanoseconds, but all our analysis is done in cycles.
Reference: [Tos92a] <editor> Toshiba. </editor> <booktitle> Advanced Information: </booktitle> <address> 2M fi 9 RDRAM, </address> <year> 1992. </year>
Reference-contexts: In the case of a cache miss, the access time also depends on the dirty flag; the write back of a dirty cache line takes some additional cycles (Table 2.2). The data sheets <ref> [Ram92, Tos92b, Tos92a] </ref> specify the memory access times in nanoseconds, but all our analysis is done in cycles.
Reference: [Tos92b] <editor> Toshiba. </editor> <booktitle> Advanced Information: </booktitle> <address> 512K fi 9 RDRAM, TC59R0409, </address> <year> 1992. </year> <month> 76 </month>
Reference-contexts: In the case of a cache miss, the access time also depends on the dirty flag; the write back of a dirty cache line takes some additional cycles (Table 2.2). The data sheets <ref> [Ram92, Tos92b, Tos92a] </ref> specify the memory access times in nanoseconds, but all our analysis is done in cycles.
References-found: 12

