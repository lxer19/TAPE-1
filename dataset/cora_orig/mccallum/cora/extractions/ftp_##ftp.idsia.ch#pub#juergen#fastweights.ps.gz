URL: ftp://ftp.idsia.ch/pub/juergen/fastweights.ps.gz
Refering-URL: http://www.idsia.ch/~juergen/topics.html
Root-URL: 
Email: schmidhu@tumult.informatik.tu-muenchen.de  
Title: LEARNING TO CONTROL FAST-WEIGHT MEMORIES: AN ALTERNATIVE TO DYNAMIC RECURRENT NETWORKS (Neural Computation, 4(1):131-139, 1992)  
Author: Jurgen Schmidhuber 
Address: Arcisstr. 21, 8000 Munchen 2, Germany  
Affiliation: Institut fur Informatik Technische Universitat Munchen  
Abstract: Previous algorithms for supervised sequence learning are based on dynamic recurrent networks. This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: The first net learns to produce context dependent weight changes for the second net whose weights may vary very quickly. The method offers the potential for STM storage efficiency: A single weight (instead of a full-fledged unit) may be sufficient for storing temporal information. Various learning methods are derived. Two experiments with unknown time delays illustrate the approach. One experiment shows how the system can be used for adaptive temporary variable binding.
Abstract-found: 1
Intro-found: 1
Reference: <author> Moller, K. and Thrun, S. </author> <year> (1990). </year> <title> Task modularization by network modulation. </title> <editor> In Rault, J., editor, </editor> <booktitle> Proceedings of Neuro-Nimes '90, </booktitle> <pages> pages 419-432. </pages>
Reference-contexts: Equation (1) is essentially identical to Moller and Thrun's equation (1) in <ref> (Moller and Thrun, 1990) </ref>. Unlike (Moller and Thrun, 1990), however, the current paper derives an exact gradient descent algorithm for time-varying inputs and outputs for this kind of architecture. <p> Equation (1) is essentially identical to Moller and Thrun's equation (1) in <ref> (Moller and Thrun, 1990) </ref>. Unlike (Moller and Thrun, 1990), however, the current paper derives an exact gradient descent algorithm for time-varying inputs and outputs for this kind of architecture.
Reference: <author> Pearlmutter, B. A. </author> <year> (1989). </year> <title> Learning state space trajectories in recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 263-269. </pages>
Reference-contexts: In general, this task requires storage of input events in a short-term memory. Previous solutions to this problem have employed gradient-based dynamic recurrent nets (e.g., (Robinson and Fallside, 1987), <ref> (Pearlmutter, 1989) </ref>, (Williams and Zipser, 1989)). In the next section an alternative gradient-based approach is described. For convenience, we drop the indices p which stand for the various episodes. The gradient of the error over all episodes is equal to the sum of the gradients for each episode.
Reference: <author> Robinson, A. J. and Fallside, F. </author> <year> (1987). </year> <title> The utility driven dynamic error propagation network. </title> <type> Technical Report CUED/F-INFENG/TR.1, </type> <institution> Cambridge University Engineering Department. </institution>
Reference-contexts: In general, this task requires storage of input events in a short-term memory. Previous solutions to this problem have employed gradient-based dynamic recurrent nets (e.g., <ref> (Robinson and Fallside, 1987) </ref>, (Pearlmutter, 1989), (Williams and Zipser, 1989)). In the next section an alternative gradient-based approach is described. For convenience, we drop the indices p which stand for the various episodes. <p> For t &gt; 0 we obtain the recursion @w ab (t) = @w ab (t 1) @w ij @(w ab (t 1); 2w ab (t)) @2w ab (t) : We can employ a method similar to the one described in <ref> (Robinson and Fallside, 1987) </ref> and (Williams and Zipser, 1989): For each w ab 2 W F and each w ij 2 W S we introduce a variable p ab ij (initialized to zero at the beginning of an episode) which can be updated at each time step t &gt; 0: p
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: on-line version is that we do not have to specify episode boundaries (`all episodes blend into each other' (Williams and Zipser, 1989)). 3 2.2 Unfolding in time An alternative of the method above would be to employ a method similar to the `unfolding in time'- algorithm for recurrent nets (e.g. <ref> (Rumelhart et al., 1986) </ref>). It is convenient to keep an activation stack for each unit in S. At each time step of an episode, some unit's new activation should be pushed onto its stack.
Reference: <author> Schmidhuber, J. H. </author> <year> (1990a). </year> <institution> Dynamische neuronale Netze und das fundamentale raumzeitliche Lern-problem. Dissertation, Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference: <author> Schmidhuber, J. H. </author> <year> (1990b). </year> <title> Learning algorithms for networks with internal and external feedback. </title> <editor> In Touretzky, D. S., Elman, J. L., Sejnowski, T. J., and Hinton, G. E., editors, </editor> <booktitle> Proc. of the 1990 Connectionist Models Summer School, </booktitle> <pages> pages 52-61. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This makes it easy to solve (6) in a second pass. The algorithm is local in time, its update-complexity per time step is O (j W F jj W S j). However, it is not local in space (see <ref> (Schmidhuber, 1990b) </ref> for a definition of locality in space and time). 2.1 On-Line Versus Off-Line Learning The off-line version of the algorithm would wait for the end of an episode to compute the final change of W S as the sum of all changes computed at each time step.
Reference: <author> Schmidhuber, J. H. </author> <year> (1991a). </year> <title> Learning to generate sub-goals for action sequences. </title> <editor> In Simula, O., editor, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks ICANN 91, to appear. </booktitle>
Reference-contexts: The only requirement is that the memory and retrieval functions be differentiable with respect to their internal parameters. Such systems work because of the existence of the chain rule. Results as above (as well as other novel applications of the chain rule <ref> (Schmidhuber, 1991a) </ref>(Schmidhuber, 1990a)) indicate that there may be additional interesting (yet undiscovered) ways of applying the chain rule for temporal credit assignment in adaptive systems. 5 Acknowledgements I wish to thank Klaus Bergner for conducting the experiments.
Reference: <institution> Elsevier Science Publishers B.V. </institution>
Reference: <author> Schmidhuber, J. H. </author> <year> (1991b). </year> <title> An O(n 3 ) learning algorithm for fully recurrent networks. </title> <type> Technical Report FKI-151-91, </type> <institution> Institut fur Informatik, Technische Universitat Munchen. v.d. Malsburg, </institution> <address> C. </address> <year> (1981). </year> <type> Internal Report 81-2, </type> <institution> Abteilung fur Neurobiologie, Max-Planck Institut fur Biophysik und Chemie, Gottingen. </institution>
Reference-contexts: In the experiment below S and F are non-recurrent, mainly to demonstrate that even a feed-forward system employing the principles above can solve certain tasks that only recurrent nets were supposed to solve. The method can be accelerated by a procedure analogous to the one presented in <ref> (Schmidhuber, 1991b) </ref>. 3 Experiments The following experiments were conducted in collaboration with Klaus Bergner, a student at Technische Universitat Munchen. 3.1 An Experiment With Unknown Time Delays In this experiment, the system was presented with a continuous stream of input events and F 's task was to switch on the single
Reference: <author> Werbos, P. J. </author> <year> (1974). </year> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University. </institution>
Reference-contexts: At time step t &gt; 0, the w ab (t 1) are used to compute the output of F according to the usual activation spreading rules for back-propagation networks (e.g. <ref> (Werbos, 1974) </ref>). <p> At each time step t &gt; 0, the factor ffi ab (t) = @w ab (t 1) can be computed by conventional back-propagation (e.g. <ref> (Werbos, 1974) </ref>).
Reference: <author> Williams, R. J. and Zipser, D. </author> <year> (1989). </year> <title> Experimental analysis of the real-time recurrent learning algorithm. </title> <journal> Connection Science, </journal> <volume> 1(1) </volume> <pages> 87-111. 6 </pages>
Reference-contexts: In general, this task requires storage of input events in a short-term memory. Previous solutions to this problem have employed gradient-based dynamic recurrent nets (e.g., (Robinson and Fallside, 1987), (Pearlmutter, 1989), <ref> (Williams and Zipser, 1989) </ref>). In the next section an alternative gradient-based approach is described. For convenience, we drop the indices p which stand for the various episodes. The gradient of the error over all episodes is equal to the sum of the gradients for each episode. <p> method for minimizing the error observed during one particular episode: E = t where E (t) = 1 2 i (d i (t) y i (t)) 2 . (In the practical on-line version of the algorithm below there will be no episode boundaries; one episode will 'blend' into the next <ref> (Williams and Zipser, 1989) </ref>.) fl Current address: Dept. of Computer Science, University of Colorado, Campus Box 430, Boulder, CO 80309, USA 1 2 The Architecture and the Algorithm The basic idea is to use a slowly learning feed-forward network S (with a set of randomly initialized weights W S ) whose <p> For t &gt; 0 we obtain the recursion @w ab (t) = @w ab (t 1) @w ij @(w ab (t 1); 2w ab (t)) @2w ab (t) : We can employ a method similar to the one described in (Robinson and Fallside, 1987) and <ref> (Williams and Zipser, 1989) </ref>: For each w ab 2 W F and each w ij 2 W S we introduce a variable p ab ij (initialized to zero at the beginning of an episode) which can be updated at each time step t &gt; 0: p ab @(w ab (t 1); <p> The on-line version changes W S at every time step, assuming that is small enough to avoid instabilities <ref> (Williams and Zipser, 1989) </ref>. An interesting property of the on-line version is that we do not have to specify episode boundaries (`all episodes blend into each other' (Williams and Zipser, 1989)). 3 2.2 Unfolding in time An alternative of the method above would be to employ a method similar to the <p> The on-line version changes W S at every time step, assuming that is small enough to avoid instabilities <ref> (Williams and Zipser, 1989) </ref>. An interesting property of the on-line version is that we do not have to specify episode boundaries (`all episodes blend into each other' (Williams and Zipser, 1989)). 3 2.2 Unfolding in time An alternative of the method above would be to employ a method similar to the `unfolding in time'- algorithm for recurrent nets (e.g. (Rumelhart et al., 1986)). It is convenient to keep an activation stack for each unit in S. <p> With being a sum-and-squash function, the only kind of interesting time-varying output that can be produced is in response to variations in the input; in particular, autonomous dynamical behavior like oscillations (e.g. <ref> (Williams and Zipser, 1989) </ref>) cannot be performed while the input is held fixed. It is straight-forward to extend the system above to the case where both S and F are recurrent. <p> At all other times, the output unit was to be switched off. This is the flip-flop task described in <ref> (Williams and Zipser, 1989) </ref>. One difficulty with this task is that there can be arbitrary time lags between relevant events. An additional difficulty is that no information about `episode boundaries' is given. The on-line method was employed: The activations of the networks were never reset.
References-found: 11

