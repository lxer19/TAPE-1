URL: http://www.research.att.com/library/trs/TRs/97/97.20/97.20.1.body.ps
Refering-URL: http://www.research.att.com/library/trs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: dagan@macs.biu.ac.il  llee@eecs.harvard.edu  pereira@research.att.com  
Title: Similarity-Based Methods For Word-Sense Disambiguation  
Author: Ido Dagan Lillian Lee Fernando Pereira 
Address: Ramat Gan 52900, Israel  Cambridge, MA 01238, USA  180 Park Avenue Florham Park, NJ 07932, USA  
Affiliation: Dept. of Mathematics and Computer Science Bar Ilan University  Div. of Engineering and Applied Sciences Harvard University  AT&T Labs Research  
Abstract: We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency. The similarity-based methods perform up to 40% better on this particular task. We also conclude that events that occur only once in the training set have major impact on similarity-based estimates.
Abstract-found: 1
Intro-found: 1
Reference: <author> Brown, Peter F., Vincent J. DellaPietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. </author> <year> 1992. </year> <title> Class-based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 467-479, </pages> <month> December. </month> <note> 7 Church, </note> <author> Kenneth. </author> <year> 1988. </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143. </pages>
Reference: <author> Church, Kenneth W. and William A. Gale. </author> <year> 1991. </year> <title> A comparison of the enhanced Good-Turing and deleted estimation methods for estimating proba-bilites of english bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 19-54. </pages>
Reference: <author> Cover, Thomas M. and Joy A. Thomas. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <publisher> John Wiley. </publisher>
Reference-contexts: All the similarity functions we describe below depend just on the base language model P (j), not the discounted model ^ P (j) from Section 2.1 above. 2.3.1 KL divergence Kullback-Leibler (KL) divergence is a stan dard information-theoretic measure of the dissimilarity between two probability mass functions <ref> (Cover and Thomas, 1991) </ref>.
Reference: <author> Dagan, Ido, Fernando Pereira, and Lillian Lee. </author> <year> 1994. </year> <title> Similarity-based estimation of word cooccurrence probabilities. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the ACL, </booktitle> <pages> pages 272-278, </pages> <address> Las Cruces, NM. </address>
Reference-contexts: Now, we could directly replace P r (w 2 jw 1 ) in the back-off equation (2) with P SIM (w 2 jw 1 ). However, other variations are possible, such as interpolating with the unigram probability P (w 2 ): where fl is determined experimentally <ref> (Dagan, Pereira, and Lee, 1994) </ref>. This represents, in effect, a linear combination of the similarity es timate and the back-off estimate: if fl = 1, then we have exactly Katz's back-off scheme.
Reference: <author> Essen, Ute and Volker Steinbiss. </author> <year> 1992. </year> <title> Co-occurrence smoothing for stochastic language modeling. </title> <booktitle> In Proceedings of ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 161-164. </pages>
Reference: <author> Gale, William, Kenneth Church, and David Yarowsky. </author> <year> 1992. </year> <title> Work on statistcal methods for word sense disambiguation. </title> <booktitle> In Working Notes, AAAI Fall Symposium Series, Probabilistic Approaches to Natural Language, </booktitle> <pages> pages 54-60. </pages>
Reference: <author> Good, I.J. </author> <year> 1953. </year> <title> The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40(3 and </volume> 4):237-264. Hoeffding, Wassily. 1965. Asymptotically optimal tests for multinomial distributions. Annals of Mathematical Statistics, pages 369-401. 
Reference: <author> Jelinek, Frederick, Robert L. Mercer, and Salim Roukos. </author> <year> 1992. </year> <title> Principles of lexical language modeling for speech recognition. </title> <editor> In In Sadaoki Furui and M. Mohan Sondhi, editors, </editor> <booktitle> Advances in Speech Signal Processing. </booktitle> <publisher> Mercer Dekker, Inc., </publisher> <pages> pages 651-699. </pages>
Reference: <author> Karov, Yael and Shimon Edelman. </author> <year> 1996. </year> <title> Learning similarity-based word sense disambiguation from sparse data. </title> <booktitle> In 4rth Workshop on Very Large Corpora. </booktitle>
Reference: <author> Katz, Slava M. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400-401, </volume> <month> March. </month>
Reference-contexts: Even large training sets tend to misrepresent low-probability events, since rare events may not appear in the training corpus at all. We concentrate here on the problem of estimating the probability of unseen word pairs, that is, pairs that do not occur in the training set. Katz's back-off scheme <ref> (Katz, 1987) </ref>, widely used in bigram language modeling, estimates the probability of an unseen bigram by utilizing unigram estimates. This has the undesirable result of assigning unseen bigrams the same probability if they are made up of unigrams of the same frequency. <p> The discounting approach is the one adopted by Katz (1987): ^ P (w 2 jw 1 ) = P d (w 2 jw 1 ) c (w 1 ; w 2 ) &gt; 0 ; where P d represents the Good-Turing discounted estimate <ref> (Katz, 1987) </ref> for seen word pairs, and P r denotes the model for probability redistribution among the unseen word pairs. ff (w 1 ) is a normalization factor. <p> Furthermore, we wished to investigate Katz's claim that one can delete singletons, word pairs that occur only once, from the training set without affecting model performance <ref> (Katz, 1987) </ref>; our training set contained 82407 singletons.
Reference: <author> Pereira, Fernando, Naftali Tishby, and Lillian Lee. </author> <year> 1993. </year> <title> Distributional clustering of English words. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the ACL, </booktitle> <pages> pages 183-190, </pages> <address> Columbus, OH. </address>
Reference-contexts: We will only be concerned with similarity between words in V 1 . 1 To the best of our knowledge, this is the first use of this particular distribution dissimilarity function in statistical language processing. The function itself is implicit in earlier work on distributional clustering <ref> (Pereira, Tishby, and Lee, 1993) </ref>, has been used by Tishby (p.c.) in other distributional similarity work, and, as suggested by Yoav Freund (p.c.), it is related to results of Hoeffding (1965) on the probability that a given sample was drawn from a given joint distribution. 2.1 Discounting and Redistribution Data sparseness
Reference: <author> Resnik, Philip. </author> <year> 1992. </year> <title> Wordnet and distributional analysis: A class-based approach to lexical discovery. </title> <booktitle> AAAI Workshop on Statistically-based Natural Language Processing Techniques, </booktitle> <pages> pages 56-64, </pages> <address> July. Schutze, Hinrich. </address> <year> 1992. </year> <title> Context space. </title> <booktitle> In Working Notes, AAAI Fall Symposium on Probabilistic Approaches to Natural Language. </booktitle> <pages> 8 </pages>
Reference-contexts: of this paper, P r (w 2 jw 1 ) = P SIM (w 2 jw 1 ). 2.3 Measures of Similarity We now consider several word similarity functions that can be derived automatically from the statistics of a training corpus, as opposed to functions derived from manually-constructed word classes <ref> (Resnik, 1992) </ref>.
References-found: 12

