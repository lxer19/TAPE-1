URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1992/tr-92-014.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1992.html
Root-URL: http://www.icsi.berkeley.edu
Title: Backwards Analysis of Randomized Geometric Algorithms  
Author: Raimund Seidel 
Date: August 20, 1992  
Address: Berkeley CA 94720 USA  
Affiliation: Computer Science Division University of California Berkeley  
Abstract: The theme of this paper is a rather simple method that has proved very potent in the analysis of the expected performance of various randomized algorithms and data structures in computational geometry. The method can be described as "analyze a randomized algorithm as if it were running backwards in time, from output to input." We apply this type of analysis to a variety of algorithms, old and new, and obtain solutions with optimal or near optimal expected performance for a plethora of problems in computational geometry, such as computing Delaunay triangulations of convex polygons, computing convex hulls of point sets in the plane or in higher dimensions, sorting, intersecting line segments, linear programming with a fixed number of variables, and others.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.K. Agarwal, H. Edelsbrunner, O. Schwarzkopf, and E. Welzl. </author> <title> "Euclidean Minimum Spanning Trees and Bichromatic Closest Pairs." </title> <booktitle> Proc. 6th ACM Symp. on Computational Geometry (1990), </booktitle> <pages> pp 203-210. </pages>
Reference: [2] <author> A. Aggarwal, L.J. Guibas, J. Saxe, and P.W. Shor. </author> <title> "A Linear Time Algorithm for Computing the Voronoi Diagram of a Convex Polygon." </title> <booktitle> Proc. 19th ACM Symp. on Theory of Computing (1987) pp 39-47. </booktitle>
Reference-contexts: In particular one was interested in the question whether an O (n) time bound was possible when S consists of the vertices of a convex polygon, given in order around the polygon. An affirmative answer was eventually given in 1986 by Aggarwal, Guibas, Saxe, and Shor <ref> [2] </ref> using an ingenious but rather involved algorithm. In the mean time, almost unnoticed, Paul Chew had discovered a very simple randomized algorithm for this problem with linear expected running time [14].
Reference: [3] <author> F. Aurenhammer. </author> <title> "Voronoi Diagrams | A Survey." </title> <note> To appear in ACM Computing Surveys. </note>
Reference-contexts: Their efficient construction and the recognition of their multifarious useful properties by Shamos and Hoey [53] were instrumental in getting the field of computational geometry started. By now these structures along with numerous generalizations are standard fare in the field (see <ref> [45, 25, 3, 36] </ref>). Shamos's and Hoey's big feat was an O (n log n) algorithm for the construction of Delaunay triangulations. It was also very soon realized that the O (n log n) bound was asymptotically worst case optimal for reasonable models of computation.
Reference: [4] <author> D. Avis and H. ElGindy. </author> <title> "Triangulating Simplicial Point Sets in Space." </title> <booktitle> Proc. 2nd ACM Symp. on Computational Geometry (1986) pp 133-141. </booktitle>
Reference-contexts: Deterministic algorithms for solving this problem in O (n log n) time have been presented in <ref> [4, 26] </ref>. Here we consider the following randomized incremental algorithm that was inspired by the QUICKSORT algorithm of the previous section.
Reference: [5] <author> J.L. Bentley and T.A. Ottmann. </author> <title> ``Algorithms for Reporting and Counting Geometric Intersections." </title> <note> IEEE Transactions on Computers 28 (1979) pp 643-647. </note>
Reference-contexts: We are interested in finding all intersecting pairs of segments in S. The first non-trivial algorithm for solving this problem was given by Bentley and Ottmann in 1979 <ref> [5] </ref>. It was based on the sweep paradigm and achieved a worst case running time of O ((K + n) log n), where K is the output size, namely the number of intersecting pairs of segments in S.
Reference: [6] <author> J.L. Bentley and M.I. Shamos. </author> <title> "Divide-and-Conquer for Linear Expected Time." </title> <note> Information Processing Letters 7 (1978) pp 87-91. </note>
Reference-contexts: stones: in 1972 Graham gave the first O (n log n) algorithm [29]; in 1978 Bentley and Shamos showed that for a large class of geometric distributions the convex hull of a set of n points drawn according to such a distribution could be computed in O (n) expected time <ref> [6] </ref> (here the expectation is with respect to the input distribution); in 1983 Kirkpatrick and Seidel gave an algorithm whose worst case running time also depends on the output-size and comes to O (n log H), where H is the number of corners of the output polygon [35].
Reference: [7] <author> J.D. Boissonnat, O. Devillers, R. Schott, M. Teillaud, and M. Yvinec. </author> <title> "Applications of Random Sampling to On-line Algorithms in Computational Geometry." </title> <note> INRIA Tech. Report 1285 (1990). </note>
Reference: [8] <author> J.D. Boissonnat, O. Devillers, and M. Teillaud. </author> <title> "A Randomized Incremental Algorithm for Constructing Higher Order Voronoi Diagrams." </title> <note> to appear in Algorithmica. 28 </note>
Reference: [9] <author> J.D. Boissonnat, O. Devillers, R. Schott, M. Teillaud, and M. Yvinec. </author> <title> "On-line Algorithms with Good Expected Behaviours." </title> <type> Manuscript (1991). </type>
Reference: [10] <author> B. Chazelle. </author> <title> "Reporting and Counting Segment Intersections." </title> <journal> J. </journal> <note> Computer System Science 32 (1986) pp 156-182. </note>
Reference-contexts: In 1983 Chazelle <ref> [10] </ref> came close to this goal with a rather complicated algorithm whose worst case running time was O (K + n log 2 n= log log n).
Reference: [11] <author> B. Chazelle and H. Edelsbrunner. </author> <title> "An Optimal Algorithm for Intersecting Line Segments in the Plane." </title> <booktitle> Proc. 29th IEEE Symp. on Foundations of Computer Science (1988) pp 590-600. </booktitle>
Reference-contexts: In 1983 Chazelle [10] came close to this goal with a rather complicated algorithm whose worst case running time was O (K + n log 2 n= log log n). Finally, five years later he and Edels-brunner <ref> [11] </ref> designed an even more complicated deterministic algorithm that did achieve the O (K + n log n) worst case running time.
Reference: [12] <author> B. Chazelle, H. Edelsbrunner, L.J. Guibas, and M. Sharir. </author> <title> "Computing a Face in an Arrangement of Line Segments." </title> <booktitle> Proc. 2nd ACM-SIAM Symp. on Discrete Algorithms (1991) pp 441-448. </booktitle>
Reference: [13] <author> B. Chazelle, L.J. Guibas, and D.T. Lee. </author> <title> "The Power of Geometric Duality." </title> <note> BIT 25 (1985) pp 76-90. </note>
Reference-contexts: Now we "thread" s through G S (R 0 ) in the usual way, similar to the incremental line arrangement construction algorithm <ref> [27, 13] </ref>: We walk along the segment s; assume we just entered a face f through some edge e; we determine through which edge the segment s leaves f and which new face the segment enters by simply testing all edges of f (say, in clockwise order starting after e).
Reference: [14] <author> P. Chew. </author> <title> "Building Voronoi Diagrams for Convex Polygons in Linear Expected Time." </title> <type> Manuscript (1986). </type>
Reference-contexts: In this paper we apply this "backwards analysis" to a number of problems and algorithms. We start with demonstrating the idea of backwards analysis on a simple algorithm due to Paul Chew <ref> [14] </ref> for constructing the Delaunay triangulation of the vertices of a convex polygon and show that it has linear expected running time. As far as we know Chew was the first to apply this type of analysis to a computational geometry algorithm. <p> An affirmative answer was eventually given in 1986 by Aggarwal, Guibas, Saxe, and Shor [2] using an ingenious but rather involved algorithm. In the mean time, almost unnoticed, Paul Chew had discovered a very simple randomized algorithm for this problem with linear expected running time <ref> [14] </ref>. Chew employed backwards 3 anlysis, and as far as we know this was the first time that this trick was used in computational geometry. His algorithm and analysis shall serve as a first example for the concept of backwards analysis.
Reference: [15] <author> K.L. Clarkson. </author> <title> "A Probabilistic Algorithm for the Post Office Problem." </title> <booktitle> Proc. 17th ACM Symp. on Theory of Computing (1985), </booktitle> <pages> pp 175-184. </pages>
Reference-contexts: In the mid 80's Ken Clarkson started to create and apply his random sampling technique, which in the mean time has developed into a surprising general framework with numerous applications <ref> [15, 16, 17, 18] </ref>. Around the same time Haussler and Welzl published their important paper [32] that introduced "-nets and the VC-dimension, which have become very useful and versatile tools in the design and analysis of randomized algorithms.
Reference: [16] <author> K.L. Clarkson. </author> <title> "New Applications of Random Sampling in Computational Geometry." </title> <booktitle> Discrete & Computational Geometry 2 (1987), </booktitle> <pages> pp 195-222. </pages>
Reference-contexts: In the mid 80's Ken Clarkson started to create and apply his random sampling technique, which in the mean time has developed into a surprising general framework with numerous applications <ref> [15, 16, 17, 18] </ref>. Around the same time Haussler and Welzl published their important paper [32] that introduced "-nets and the VC-dimension, which have become very useful and versatile tools in the design and analysis of randomized algorithms.
Reference: [17] <author> K.L. Clarkson and P.W. Shor. </author> <title> "Algorithms for Diametral Pairs and Convex Hulls that are Optimal, Randomized, and Incremental." </title> <booktitle> Proc. 4th ACM Symp. on Computational Geometry (1988), </booktitle> <pages> pp 12-17. </pages>
Reference-contexts: In the mid 80's Ken Clarkson started to create and apply his random sampling technique, which in the mean time has developed into a surprising general framework with numerous applications <ref> [15, 16, 17, 18] </ref>. Around the same time Haussler and Welzl published their important paper [32] that introduced "-nets and the VC-dimension, which have become very useful and versatile tools in the design and analysis of randomized algorithms.
Reference: [18] <author> K.L. Clarkson and P.W. Shor. </author> <title> Applications of Random Sampling in Computational Geometry, II." </title> <booktitle> Discrete & Computational Geometry 4 (1989), </booktitle> <pages> pp 387-421. </pages>
Reference-contexts: In the mid 80's Ken Clarkson started to create and apply his random sampling technique, which in the mean time has developed into a surprising general framework with numerous applications <ref> [15, 16, 17, 18] </ref>. Around the same time Haussler and Welzl published their important paper [32] that introduced "-nets and the VC-dimension, which have become very useful and versatile tools in the design and analysis of randomized algorithms. <p> We consider a randomized incremental algorithm and show that for d &gt; 3 there is a simple variant that via backwards analysis can easily be shown to have optimal O (n bd=2c ) expected running time. Then we consider the "conflict graph" based algorithm due to Clarkson and Shor <ref> [18] </ref> and present a new backwards analysis due to Clarkson [21] that shows that this algorithm has "optimal" expected running time for all dimensions d. 2 Delaunay Triangulations of Convex Polygons Let S be a set of n points in the plane. <p> Finally, five years later he and Edels-brunner [11] designed an even more complicated deterministic algorithm that did achieve the O (K + n log n) worst case running time. Around the same time independently Mulmuley [42] as well as Clarkson and Shor <ref> [18] </ref> developed rather simple randomized algorithms with O (K + n log n) expected running time. Clarkson and Shor based the analysis of the running time of their algorithm on the general theory of random sampling. <p> I am not sure whom to attribute this algorithm to. It certainly owes a lot to the conflict graph based algorithm of Clarkson and Shor <ref> [18] </ref>, and it seems to have grown out of discussions among several researchers at a DIMACS workshop in the fall of 1989. <p> Jir Matousek [38] managed to improve the analysis of that algorithm to the remarkable expected time bound of O ((nd + d 3 )e 4 p 9 Clarkson's Backwards Analysis of the Conflict Graph Based Convex Hull Algorithm In their landmark paper on applications of random sampling in computational geometry <ref> [18] </ref> Clarkson and Shor described a randomized algorithm for constructing convex hulls of point sets in IR d that has optimal expected running time. Their analysis of the expected running time was based on very general lemmas about random sampling. <p> of P : No visible face of P is a face of P 0 ; all obscured and all horizon faces of P are faces of P 0 ; for each horizon face G of P the pyramid 4 We actually present a slightly different version than the one in <ref> [18] </ref> in that we do not dualize and use a slightly different notion of a conflict graph. 22 conv (G [ fxg) is a face of P 0 ; this yields all faces of P 0 . <p> A solution to this "visibility problem" that performs satisfactorily in all dimensions, and not just for d &gt; 3, was invented by Clarkson and Shor <ref> [18] </ref>. <p> We will here describe a variant of this approach that was also already considered in <ref> [18] </ref>, where for each point p 2 S n S r only one representative visible facet V F (p; P r ) 2 V is (p; P r ) is maintained (provided such a facet exists at all). <p> for the case d = 2; 3. 10 Odds and Ends It should be pointed out that the type of analysis presented in the previous section is not particular to the convex hull problem but can be applied to randomized incremental construction in the formal framework of Clarkson and Shor <ref> [18] </ref>. In their terminology the generalization of the crucial insight (2) is the observation that the regions defined by R that do not conflict with object q are exactly those regions defined by R [ fqg that do not involve q.
Reference: [19] <author> K.L. Clarkson. </author> <title> "Linear Programming in O(n3 d 2 ) Time." </title> <journal> Inf. </journal> <note> Proc. Letters 22 (1986) pp 21-24. </note>
Reference-contexts: In the last decade deterministic algorithms were developed that solve such linear programs in O (m) worst case time <ref> [39, 40, 22, 23, 19] </ref>. However those algorithms are mostly of theoretical interest only since they are quite complicated and the dependence of their running times on the dimension d is exponential and has only been shown to be 3 d 2 at best.
Reference: [20] <author> K.L. Clarkson. </author> <title> "A Las Vegas Algorithm for Linear and Integer Programming when the Dimension is Small." </title> <note> Manuscript; a preliminary version appeared in Proc. 29th IEEE Symp. on Foundations of Computer Science (1988) pp 452-456. </note>
Reference-contexts: However those algorithms are mostly of theoretical interest only since they are quite complicated and the dependence of their running times on the dimension d is exponential and has only been shown to be 3 d 2 at best. More recently Ken Clarkson <ref> [20] </ref> proposed a randomized algorithm with a remarkable running time of O (d 2 m) + (log m)O (d) d=2+O (1) + O (d 4 p m log m). Here we briefly present another randomized algorithm that was first described in [50].
Reference: [21] <author> K.L. Clarkson. </author> <type> Personal Communication, </type> <month> September 10 </month> <year> (1990). </year>
Reference-contexts: Then we consider the "conflict graph" based algorithm due to Clarkson and Shor [18] and present a new backwards analysis due to Clarkson <ref> [21] </ref> that shows that this algorithm has "optimal" expected running time for all dimensions d. 2 Delaunay Triangulations of Convex Polygons Let S be a set of n points in the plane. <p> Their analysis of the expected running time was based on very general lemmas about random sampling. Recently Ken Clarkson <ref> [21] </ref> has discovered a new analysis that is completely self-contained and relies heavily on the idea of backwards analysis. In this section we give first a brief description of the convex hull algorithm 4 and then its new analysis. <p> For A S let now f (A) denote the number of facets of conv A, and for a 2 A let deg (a; A) now denote the number of facets of conv A that contain a. Here comes an ingenious observation due to Ken Clarkson <ref> [21] </ref>: vis (q; R) = f (R) f (R [ fqg) + deg (q; R [ fqg) ; (2) since the facets of conv R that are not visible from q are exactly the facets of conv (R [ fqg) that do not contain q.
Reference: [22] <author> M.E. Dyer. </author> <title> "Linear Algorithms for Two and Three-Variable Linear Programs." </title> <note> SIAM J. on Computing 13 (1984) pp 31-45. </note>
Reference-contexts: In the last decade deterministic algorithms were developed that solve such linear programs in O (m) worst case time <ref> [39, 40, 22, 23, 19] </ref>. However those algorithms are mostly of theoretical interest only since they are quite complicated and the dependence of their running times on the dimension d is exponential and has only been shown to be 3 d 2 at best.
Reference: [23] <author> M.E. Dyer. </author> <title> "On a Multidimensional Search Technique and its Applications to the Eu clidean One-Centre Problem." </title> <note> SIAM J. on Computing 15 (1986) pp 725-738. 29 </note>
Reference-contexts: In the last decade deterministic algorithms were developed that solve such linear programs in O (m) worst case time <ref> [39, 40, 22, 23, 19] </ref>. However those algorithms are mostly of theoretical interest only since they are quite complicated and the dependence of their running times on the dimension d is exponential and has only been shown to be 3 d 2 at best.
Reference: [24] <author> M.E. Dyer and A.M. </author> <title> Frieze "A Randomized Algorithm for Fixed-Dimensional Linear Programming." </title> <note> Mathematical Programming 44 (1989) pp 203-212. </note>
Reference: [25] <author> H. Edelsbrunner. </author> <title> Algorithms in Combinatorial Geometry. </title> <publisher> Springer Verlag (1987). </publisher>
Reference-contexts: Their efficient construction and the recognition of their multifarious useful properties by Shamos and Hoey [53] were instrumental in getting the field of computational geometry started. By now these structures along with numerous generalizations are standard fare in the field (see <ref> [45, 25, 3, 36] </ref>). Shamos's and Hoey's big feat was an O (n log n) algorithm for the construction of Delaunay triangulations. It was also very soon realized that the O (n log n) bound was asymptotically worst case optimal for reasonable models of computation. <p> So let S be the n vertices of a convex polygon P given in order around P . Assume that no four points in S are co-circular, a condition that could easily be simulated using standard perturbation techniques <ref> [25, pp. 185] </ref>. Chew's algorithm proceeds as follows: If S consists of only three points, then the triangle spanned by them forms DT (S). <p> As usual such non-degeneracy could be simulated by standard perturbation methods <ref> [25, pp. 185] </ref>, or also the algorithm could easily be modified so that none of these assumptions are necessary. Mulmuley's algorithm does more than just determine which pairs of segments in S intersect. It constructs what we call the trapezoidal decomposition induced by S. <p> We will assume that S is in non-degenerate position, i.e. no d + 1 of its points lie in a common hyperplane. Such non-degeneracy can easily be simulated with impunity using standard perturbation techniques <ref> [25, pp. 185] </ref>. Non-degeneracy ensures that the convex hull of any subset of S is a simplicial polytope. A few relevant basics about polytopes: Let P be a simplicial d-polytope, let V be the vertex set of P , and let m = jV j.
Reference: [26] <author> H. Edelsbrunner, </author> <title> F.P. Preparata, and D.B. West. "Tetrahedrizing Point Sets in Three Dimensions." </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-86-1310, Univ. of Illinois, Dept. Computer Science (1986). </institution>
Reference-contexts: Deterministic algorithms for solving this problem in O (n log n) time have been presented in <ref> [4, 26] </ref>. Here we consider the following randomized incremental algorithm that was inspired by the QUICKSORT algorithm of the previous section.
Reference: [27] <author> H. Edelsbrunner, J. O'Rourke, and R. Seidel. </author> <title> "Constructing Arrangements of Hyperplanes and Applications." </title> <journal> SIAM J. on Computing 15 (1986), </journal> <pages> pp 341-363. </pages>
Reference-contexts: Now we "thread" s through G S (R 0 ) in the usual way, similar to the incremental line arrangement construction algorithm <ref> [27, 13] </ref>: We walk along the segment s; assume we just entered a face f through some edge e; we determine through which edge the segment s leaves f and which new face the segment enters by simply testing all edges of f (say, in clockwise order starting after e).
Reference: [28] <author> G.H. Gonnet. </author> <title> Handbook of Algorithms and Data Structures. </title> <publisher> Addison-Wesley (1984). </publisher>
Reference-contexts: That the expected number of comparisons of QUICKSORT is 2 (n + 1)H n 4n is a well known result and has been derived before without using backwards analysis (see e.g. <ref> [28] </ref>). We will now use backwards analysis to estimate in a reasonably painless way the probability that the running time of QUICKSORT is significantly larger than its expectation (see [47]) for a similar result).
Reference: [29] <author> R.L. Graham. </author> <title> "An efficient algorithm for determining the convex hull of a finite planar set." </title> <journal> Inform. Proc. Lett., </journal> <volume> 1 (1972), </volume> <pages> pp 132-133. </pages>
Reference-contexts: The planar convex hull problem has received an extraordinary amount of attention in the computational geometry literature. We will not attempt to give a complete history here, but just list three mile stones: in 1972 Graham gave the first O (n log n) algorithm <ref> [29] </ref>; in 1978 Bentley and Shamos showed that for a large class of geometric distributions the convex hull of a set of n points drawn according to such a distribution could be computed in O (n) expected time [6] (here the expectation is with respect to the input distribution); in 1983
Reference: [30] <author> L.J. Guibas, D.E. Knuth, and M. Sharir. </author> <title> "Randomized Incremental Construction of De-launay and Voronoi Diagrams." </title> <booktitle> Proc. ICALP (1990). </booktitle>
Reference: [31] <author> T. Hagerup and C. Rub. </author> <title> "A Guided Tour of Chernoff Bounds." </title> <journal> Inform. Proc. Letters 33 (1989/90), </journal> <pages> pp 305-308. </pages>
Reference-contexts: Thus E [Y] = P P therefore E [X] the expected number of comparisons in our slowed down algorithm is 2nH n . To estimate Pr (Y &gt; c E [Y]) we can now use the well-known Chernoff bound (see <ref> [47, 31] </ref>), which in one form states that if a random variable Z is the sum of n independent 0-1 random variables and the expectation of Z is E, then for c 1 Pr (Z &gt; c E) e E (1c+c log c) : In our case the Y i 's
Reference: [32] <author> D. Haussler and E. Welzl. </author> <title> "Epsilon-Nets and Simplex Range Queries." </title> <booktitle> Discrete & Computational Geometry 2 (1987), </booktitle> <pages> pp 127-151. </pages>
Reference-contexts: In the mid 80's Ken Clarkson started to create and apply his random sampling technique, which in the mean time has developed into a surprising general framework with numerous applications [15, 16, 17, 18]. Around the same time Haussler and Welzl published their important paper <ref> [32] </ref> that introduced "-nets and the VC-dimension, which have become very useful and versatile tools in the design and analysis of randomized algorithms.
Reference: [33] <author> C.A.R. Hoare. </author> <title> "Quicksort." </title> <journal> Computer Journal 5.1 (1962), </journal> <pages> pp 10-15. </pages>
Reference-contexts: Invented by Hoare in 1960 <ref> [33] </ref>, it has since been amply analyzed (see for instance Sedgewick's book [49]) and with its various versions it has become the maybe most frequently used sorting algorithm in practice. We will consider a somewhat different version of QUICKSORT that is more amenable to backwards analysis than the usual version.
Reference: [34] <author> R.M. Karp. </author> <title> "An Introduction to Randomized Algorithms." </title> <note> To appear in Discrete Applied Mathematics. </note>
Reference-contexts: The approach has proved useful in such diverse areas as number theory, distributed computing, combinatorial algorithms, complexity theory, and others. For surveys see <ref> [34, 47, 55] </ref>. It is interesting that in Rabin's seminal 1976 paper [46] which initiated the study of algorithmic uses of randomness one of the two example problems considered was a computational geometry problem, namely fl Supported by NSF Presidential Young Investigator Award CCR-9058440.
Reference: [35] <author> D.G. Kirkpatrick, R. Seidel. </author> <title> "The Ultimate Planar Convex Hull Algorithm?" SIAM J. </title> <journal> on Comput., </journal> <volume> Vol. 15, No. 1 (1986), </volume> <pages> pp 287-299. </pages>
Reference-contexts: (n) expected time [6] (here the expectation is with respect to the input distribution); in 1983 Kirkpatrick and Seidel gave an algorithm whose worst case running time also depends on the output-size and comes to O (n log H), where H is the number of corners of the output polygon <ref> [35] </ref>. The attractiveness of the planar convex hull problem to computational geometers stems partly from the fact that most computational paradigms can be successfully applied to this problem. This is also the case with our paradigm of backwards analysis.
Reference: [36] <author> R. Klein. </author> <title> Concrete and Abstract Voronoi Diagrams. </title> <booktitle> Springer Verlag, Lecture Notes in Computer Science 400 (1989). </booktitle>
Reference-contexts: Their efficient construction and the recognition of their multifarious useful properties by Shamos and Hoey [53] were instrumental in getting the field of computational geometry started. By now these structures along with numerous generalizations are standard fare in the field (see <ref> [45, 25, 3, 36] </ref>). Shamos's and Hoey's big feat was an O (n log n) algorithm for the construction of Delaunay triangulations. It was also very soon realized that the O (n log n) bound was asymptotically worst case optimal for reasonable models of computation.
Reference: [37] <author> J. Matousek and R. Seidel. </author> <title> "On Tail Estimates for Mulmuley's Segment Intersection Algorithm." </title> <note> In preparation. </note>
Reference-contexts: For Mulmuley's algorithm of section 3 Matousek and Seidel 27 <ref> [37] </ref> have recently shown a tail estimate of O (n c ), provided K, the number of intersecting segment pairs, is not too small relative to n.
Reference: [38] <author> J. Matousek, M. Sharir, and E. Welzl. </author> <title> "A Subexponential Bound for Linear Programming." </title> <note> To appear in Proc. of 8th ACM Symp. on Computational Geometry (1992). </note>
Reference-contexts: Their method avoids the recursion on the dimension and in the case of linear programming achieves an expected time bound of O (2 d n). The complexity analysis of their new algorithm also exploits aspects of "backwards" analysis. Even more recently the same two authors together with Jir Matousek <ref> [38] </ref> managed to improve the analysis of that algorithm to the remarkable expected time bound of O ((nd + d 3 )e 4 p 9 Clarkson's Backwards Analysis of the Conflict Graph Based Convex Hull Algorithm In their landmark paper on applications of random sampling in computational geometry [18] Clarkson and
Reference: [39] <author> N. Megiddo. </author> <title> "Linear-Time Algorithms for Linear Programming in IR 3 and Related Problems." </title> <note> SIAM J. on Computing 12 (1983) pp 759-776. 30 </note>
Reference-contexts: In the last decade deterministic algorithms were developed that solve such linear programs in O (m) worst case time <ref> [39, 40, 22, 23, 19] </ref>. However those algorithms are mostly of theoretical interest only since they are quite complicated and the dependence of their running times on the dimension d is exponential and has only been shown to be 3 d 2 at best.
Reference: [40] <author> N. Megiddo. </author> <title> "Linear Programming in Linear Time when the Dimension is Fixed." </title> <note> Journal of the ACM 31 (1984) pp 114-127. </note>
Reference-contexts: In the last decade deterministic algorithms were developed that solve such linear programs in O (m) worst case time <ref> [39, 40, 22, 23, 19] </ref>. However those algorithms are mostly of theoretical interest only since they are quite complicated and the dependence of their running times on the dimension d is exponential and has only been shown to be 3 d 2 at best.
Reference: [41] <author> K. Mehlhorn. </author> <type> Personal Communication, </type> <month> October </month> <year> (1990). </year>
Reference-contexts: running times of the other algorithms presented in this paper? What is the probability that the actual running time on a problem of size n exceeds the expectation by a multiplicative factor of c? For the polygon triangulation algorithm of secion 2 one can use a general result of Mehlhorn <ref> [41] </ref> to show that this probability is at most 1 e ( e c .
Reference: [42] <author> K. Mulmuley. </author> <title> "A Fast Planar Partition Algorithm: Part I." </title> <booktitle> Proc. 29th IEEE Symp. on Foundations of Computer Science (1988), </booktitle> <pages> pp 580-589. </pages>
Reference-contexts: Around the same time Haussler and Welzl published their important paper [32] that introduced "-nets and the VC-dimension, which have become very useful and versatile tools in the design and analysis of randomized algorithms. In a series of papers <ref> [42, 43, 44] </ref> Mulmuley introduced and studied a number of probabilistic games that allow a rather tight analysis of the expected behaviour of a number of geometric algorithms. <p> As far as we know Chew was the first to apply this type of analysis to a computational geometry algorithm. Next we apply backwards analysis to an algorithm due to Mulmuley for determining all intersection pairs among a set of line segments in the plane <ref> [42, 43] </ref>. Mulmuley's original analysis of the algorithm was based on probabilistic games and was rather involved. <p> Finally, five years later he and Edels-brunner [11] designed an even more complicated deterministic algorithm that did achieve the O (K + n log n) worst case running time. Around the same time independently Mulmuley <ref> [42] </ref> as well as Clarkson and Shor [18] developed rather simple randomized algorithms with O (K + n log n) expected running time. Clarkson and Shor based the analysis of the running time of their algorithm on the general theory of random sampling.
Reference: [43] <author> K. Mulmuley. </author> <title> "A Fast Planar Partition Algorithm: Part II." </title> <booktitle> Proc. 5th ACM Symp. on Computational Geometry (1989), </booktitle> <pages> pp 33-43. </pages>
Reference-contexts: Around the same time Haussler and Welzl published their important paper [32] that introduced "-nets and the VC-dimension, which have become very useful and versatile tools in the design and analysis of randomized algorithms. In a series of papers <ref> [42, 43, 44] </ref> Mulmuley introduced and studied a number of probabilistic games that allow a rather tight analysis of the expected behaviour of a number of geometric algorithms. <p> As far as we know Chew was the first to apply this type of analysis to a computational geometry algorithm. Next we apply backwards analysis to an algorithm due to Mulmuley for determining all intersection pairs among a set of line segments in the plane <ref> [42, 43] </ref>. Mulmuley's original analysis of the algorithm was based on probabilistic games and was rather involved.
Reference: [44] <author> K. Mulmuley. </author> <title> "On Obstructions in Relation to a Fixed Viewpoint." </title> <booktitle> Proc. 30th IEEE Symp. on Foundations of Computer Science (1989), </booktitle> <pages> pp 592-597. </pages>
Reference-contexts: Around the same time Haussler and Welzl published their important paper [32] that introduced "-nets and the VC-dimension, which have become very useful and versatile tools in the design and analysis of randomized algorithms. In a series of papers <ref> [42, 43, 44] </ref> Mulmuley introduced and studied a number of probabilistic games that allow a rather tight analysis of the expected behaviour of a number of geometric algorithms.
Reference: [45] <author> F.P. Preparata and M.I. Shamos. </author> <title> Computational Geometry An Introduction. </title> <publisher> Springer Verlag (1985). </publisher>
Reference-contexts: Their efficient construction and the recognition of their multifarious useful properties by Shamos and Hoey [53] were instrumental in getting the field of computational geometry started. By now these structures along with numerous generalizations are standard fare in the field (see <ref> [45, 25, 3, 36] </ref>). Shamos's and Hoey's big feat was an O (n log n) algorithm for the construction of Delaunay triangulations. It was also very soon realized that the O (n log n) bound was asymptotically worst case optimal for reasonable models of computation.
Reference: [46] <author> M.O. Rabin. </author> <title> "Probabilistic Algorithms." </title> <editor> In J.F. Traub, editor, </editor> <title> Algorithms and Complexity, Recent Results and New Dierections. </title> <publisher> Academic Press, </publisher> <address> New York (1976), </address> <pages> pp 21-39. </pages>
Reference-contexts: The approach has proved useful in such diverse areas as number theory, distributed computing, combinatorial algorithms, complexity theory, and others. For surveys see [34, 47, 55]. It is interesting that in Rabin's seminal 1976 paper <ref> [46] </ref> which initiated the study of algorithmic uses of randomness one of the two example problems considered was a computational geometry problem, namely fl Supported by NSF Presidential Young Investigator Award CCR-9058440. Email address: seidel@cs.berkeley.edu 1 the Euclidean closest pair problem.
Reference: [47] <author> P. Raghavan. </author> <title> "Lecture Notes on Randomized Algorithms." </title> <institution> IBM T.J. Watson Research Center Computer Science Report RC 15430 (1990). </institution>
Reference-contexts: The approach has proved useful in such diverse areas as number theory, distributed computing, combinatorial algorithms, complexity theory, and others. For surveys see <ref> [34, 47, 55] </ref>. It is interesting that in Rabin's seminal 1976 paper [46] which initiated the study of algorithmic uses of randomness one of the two example problems considered was a computational geometry problem, namely fl Supported by NSF Presidential Young Investigator Award CCR-9058440. <p> We will now use backwards analysis to estimate in a reasonably painless way the probability that the running time of QUICKSORT is significantly larger than its expectation (see <ref> [47] </ref>) for a similar result). For this purpose we will artificially slow down our algorithm as follows: If in 15 round r the new pivot p r is in B 0 , then all keys in B r are compared with p r also. <p> Thus E [Y] = P P therefore E [X] the expected number of comparisons in our slowed down algorithm is 2nH n . To estimate Pr (Y &gt; c E [Y]) we can now use the well-known Chernoff bound (see <ref> [47, 31] </ref>), which in one form states that if a random variable Z is the sum of n independent 0-1 random variables and the expectation of Z is E, then for c 1 Pr (Z &gt; c E) e E (1c+c log c) : In our case the Y i 's
Reference: [48] <author> G. </author> <title> Rote. </title> <type> Personal Communication, </type> <month> October 15 </month> <year> (1990). </year>
Reference: [49] <author> R.Sedgewick. </author> <title> Quicksort. </title> <publisher> Garland, </publisher> <address> New York (1978). </address>
Reference-contexts: Invented by Hoare in 1960 [33], it has since been amply analyzed (see for instance Sedgewick's book <ref> [49] </ref>) and with its various versions it has become the maybe most frequently used sorting algorithm in practice. We will consider a somewhat different version of QUICKSORT that is more amenable to backwards analysis than the usual version.
Reference: [50] <author> R. Seidel. </author> <title> "Linear Programming and Convex Hulls Made Easy." </title> <booktitle> Proc. 6th ACM Symp. on Computational Geometry (1990), </booktitle> <pages> pp 211-215. </pages>
Reference-contexts: More recently Ken Clarkson [20] proposed a randomized algorithm with a remarkable running time of O (d 2 m) + (log m)O (d) d=2+O (1) + O (d 4 p m log m). Here we briefly present another randomized algorithm that was first described in <ref> [50] </ref>. The main virtues of this algorithm are its simplicity and its amenability to backwards analysis (well | for the purpose of this paper this is a virtue). <p> The boundedness assumption appears to be the most difficult to remove. One approach is to enforce it simply by stipulating that we are not interested in all of IR d but just some "bounding box" B (i.e. we impose explicit lower and upper bounds for the variables; see <ref> [50] </ref> for details). Another approach involves generalizing the notion of "optimum vertex:" in case of unboundedness define the "optimum" to be the unit vector in the direction of a ray in the feasibility region that maximizes the inner product with the objective direction a. <p> Let us now turn to the details of how that step can be implemented. How can one determine the set V is (p r ; P r1 ) of all facets of P r1 that are visible from p r ? As pointed out in <ref> [50] </ref> there is a simple solution for this problem that turns out to be reasonably efficient for the case d &gt; 3. <p> For the linear programming algorithm of section 7 a bound of O (c d! ) is given in <ref> [50] </ref>, where d is the dimension. A similar bound applies to the algorithm of section 8. To my knowledge no non-trivial tail estimate is known for the convex hull algorithm of section 9. 11 Acknowledgements Many people have wittingly and unwittingly contributed to this paper in one form or another.
Reference: [51] <author> R. Seidel. </author> <title> "A Simple and Fast Incremental Algorithm for Computing Trapezoidal Decompositions and for Triangulating Polygons." </title> <note> To appear in COMPUTATIONAL GEOMETRY: Theory and Applications (1991). </note>
Reference-contexts: A search structure is constructed using a randomized algorithm; query times are then random variables with respect to the "coin flips" made during the construction. Backwards analysis works very well for determining the expectation of those query times; see for instance <ref> [51] </ref>. In that paper a little twist is also added to this approach, which yields a rather straightforward randomized method for triangulating a simple polygon in nearly optimal O (n log fl n) expected time.
Reference: [52] <author> M.I. Shamos. </author> <title> Computational Geometry. </title> <type> Ph.D. thesis, </type> <institution> Dept. of Computer Science, Yale Univ. </institution> <year> (1978). </year>
Reference-contexts: Email address: seidel@cs.berkeley.edu 1 the Euclidean closest pair problem. Of course computational geometry as a field came about a number of years later. Shamos's thesis <ref> [52] </ref> appeared in 1978.
Reference: [53] <author> M.I. Shamos and D. Hoey. </author> <title> "Closest Point Problems." </title> <booktitle> Proc. 16th IEEE Symp. on Foundations of Computer Science (1975) pp 151-162. </booktitle>
Reference-contexts: Delaunay triangulations along with their dual structures, which are known as Voronoi diagrams, have been studied intensively in computational geometry. Their efficient construction and the recognition of their multifarious useful properties by Shamos and Hoey <ref> [53] </ref> were instrumental in getting the field of computational geometry started. By now these structures along with numerous generalizations are standard fare in the field (see [45, 25, 3, 36]). Shamos's and Hoey's big feat was an O (n log n) algorithm for the construction of Delaunay triangulations.
Reference: [54] <author> M. Sharir and E. Welzl. </author> <title> "A Combinatorial Bound for Linear Programming and Related Problems." </title> <booktitle> Proc. of 9th Symp. on theoretical Aspects of Computer Science (STACS 1992). </booktitle>
Reference-contexts: It is possible to unify these algorithms by considering the relevant problems as special instances of a suitably axiomatized abstract optimization problem. 3 Very recently Micha Sharir and Emo Welzl <ref> [54] </ref> proposed a new axiomatic framework along with a new randomized algorithm for the linear programming type of problems considered here 3 Previous versions of this paper contained an attempt of such an axiomization.
Reference: [55] <author> J.S. Vitter and Ph. Flajolet. </author> <title> "Average-Case Analysis of Algorithms and Data Structures." </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science: Algorithms and Complexity. </booktitle> <publisher> Elsevier (1990), </publisher> <pages> pp 431-524. 31 </pages>
Reference-contexts: The approach has proved useful in such diverse areas as number theory, distributed computing, combinatorial algorithms, complexity theory, and others. For surveys see <ref> [34, 47, 55] </ref>. It is interesting that in Rabin's seminal 1976 paper [46] which initiated the study of algorithmic uses of randomness one of the two example problems considered was a computational geometry problem, namely fl Supported by NSF Presidential Young Investigator Award CCR-9058440.
Reference: [56] <author> E. Welzl. </author> <title> "Smallest Enclosing Disks (Balls and Ellipsoids)." </title> <editor> In H. Maurer, editor, </editor> <booktitle> New Results and New Trends in Computer Science. Springer Lecture Notes in Computer Science 555 (1991), </booktitle> <pages> pp 359-370. 32 </pages>
Reference-contexts: We present a very simple algorithm for solving linear programs with m constraints in d variables that has expected running time O (d!m). Again the analysis via the backwards view is very straightforward. We then describe an adaption of this method due to Welzl <ref> [56] </ref> for the problem of findingthe smallest enclosing balls for a finite set of points in IR d . Finally we turn our attention to the problem of constructing the convex hull of n points in IR d . <p> Both approaches require slight changes in the "bottoming-out" part of our procedure. See the next section for an abstract description. 19 8 Welzl's Minidisk Algorithm Here we present an algorithm due to Emo Welzl <ref> [56] </ref> for constructing the smallest enclosing ball for a finite point set T IR d . The algorithm is very similar to the linear programming algorithm of the previous section.
References-found: 56

