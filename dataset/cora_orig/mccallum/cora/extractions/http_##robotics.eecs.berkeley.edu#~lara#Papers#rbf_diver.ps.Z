URL: http://robotics.eecs.berkeley.edu/~lara/Papers/rbf_diver.ps.Z
Refering-URL: http://robotics.eecs.berkeley.edu/~lara/publications.html
Root-URL: 
Title: Learning Controllers for Complex Behavioral Systems  and  
Author: Lara S. Crawford S. Shankar Sastry 
Address: Berkeley  
Note: in Biophysics  This research was supported in part by ARO under grants DAAL03-91-G0171, DAAH04-94-G0211, DAAH04-95-05888, and MURI DAAH04-96-1-0341.  
Affiliation: Graduate Group  Department of Electrical Engineering and Computer Sciences University of California at  
Abstract: Biological control systems routinely guide complex dynamical systems through complicated tasks such as running or diving. Conventional control techniques, however, stumble with these problems, which have complex dynamics, many degrees of freedom, and a task which is often only partially specified (e.g., "move forward fast," or "execute a one-and-one-half-somersault dive"). To address problems like these, we are using a biologically-inspired, hierarchical control structure, in which controllers composed of radial basis function networks learn the controls required at each level of the hierarchy. Through learning and proper encoding of behaviors and controls, some of these difficulties in controlling complex systems can be overcome. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72 </volume> <pages> 81-138, </pages> <year> 1995. </year>
Reference-contexts: We are currently implementing this reinforcement learning algorithm for training the coordinating controller. For systems in which the state space is discrete, and the Q values can be stored in a table, if the states contain sufficient information that the system is Markov, then the Q-learning algorithm converges (see <ref> [1] </ref>, [20], [29]). For a system like the diver, however, where we require a function approximator, convergence is more problematic. Boyan and Moore [4] give simple examples in which substituting a function approximator for a lookup table results in loss of convergence.
Reference: [2] <author> C. Batterman. </author> <title> The Techniques of Springboard Diving. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1968. </year>
Reference-contexts: The control problem for the diver is as follows: given fixed initial conditions (after leaving the board), execute a certain maneuver (a full twisting one-and-a-half somersault, for example), and then enter the water in a fully extended, vertical position (see <ref> [2] </ref>). There is no particular desired trajectory specified. This problem has several interesting features. After the diver has left the board, he is subject to angular momentum conservation, which creates a nonholonomic constraint. The diver leaves the board with some initial (non-zero) angular momentum, however, so the system has drift.
Reference: [3] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Neuro-dynamic programming: an overview. </title> <booktitle> In Proceedings of the 34th Conference on Decision and Control, </booktitle> <pages> pages 560-564, </pages> <year> 1995. </year>
Reference-contexts: There are some learning approaches to problems of this type in the literature as well: Gorinevsky, Kapitanovsky, and Goldenberg [13] use radial basis functions to learn the controls for steering a space platform with an arm, and Bertsekas and Tsitsiklis <ref> [3] </ref> use neurodynamic programming to learn to control discrete systems. We have tested some conventional techniques on a planar, two-joint simplification of the diver model, but these proved unsatisfactory even for the simplified system. <p> Thus the values of the final states are propagated backward to earlier states. Such an approach, using dynamic programming ideas with a network approximation of the Q or value function, has been called neuro-dynamic programming <ref> [3] </ref>. A separate action selector would select the 11 shoulder abduction/adduction. The top plot shows the squared errors in u before squashing (with fi = :8). After squashing, each control is a scaled value in (0; 1).
Reference: [4] <author> J. A. Boyan and A. W. Moore. </author> <title> Generalization in reinforcement learning: safely approximating the value function. </title> <booktitle> In Advances in Neural Information Processing 7, </booktitle> <pages> pages 369-376, </pages> <year> 1995. </year>
Reference-contexts: For a system like the diver, however, where we require a function approximator, convergence is more problematic. Boyan and Moore <ref> [4] </ref> give simple examples in which substituting a function approximator for a lookup table results in loss of convergence. These examples can be made to converge by changing slightly the methods used, however; see [22] for a summary.
Reference: [5] <author> R. Brockett. </author> <title> On the computer control of movement. </title> <booktitle> In Proceedings of the IEEE Conference on Robotics and Automation, </booktitle> <year> 1988. </year>
Reference-contexts: The controllers described here have certain similarities to Brockett's (u,k,T) hybrid motor control system <ref> [5] </ref>, but here all dynamics are encapsulated in the lower level pattern generators, so the controls are simply vectors rather than time trajectories. Also, the controllers are feed-forward (except for the limited use of PD controllers mentioned above) at this time, though we plan to add feedback in the future.
Reference: [6] <author> R. W. Brockett. </author> <title> Systems theory on group manifolds and coset spaces. </title> <journal> SIAM Journal of Control, </journal> <volume> 10(2) </volume> <pages> 265-284, </pages> <year> 1972. </year> <month> 14 </month>
Reference: [7] <author> R. W. Brockett. </author> <title> Analog and digital computing. </title> <booktitle> In Future Tendencies in Computer Science, Control and Applied Mathematics. International Conference on the Occasion of the 25th Anniversary of INRIA, Proceedings, </booktitle> <pages> pages 279-289, </pages> <year> 1992. </year>
Reference-contexts: Other variations on this type of parametrization are possible, of course (c.f. <ref> [7] </ref>). In the future, we may be required to add more parameters for stylistic considerations or to reduce the number of possible solutions.
Reference: [8] <author> L. S. Crawford and S. S. Sastry. </author> <title> Biological motor approaches for a planar diver. </title> <booktitle> In Proceedings of the 34th IEEE Conference on Decision and Control, </booktitle> <pages> pages 3881-3886, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: We have tested some conventional techniques on a planar, two-joint simplification of the diver model, but these proved unsatisfactory even for the simplified system. A simple learning algorithm applied to the planar diver was more promising (see <ref> [8] </ref>), and led to our continued work on learning controllers described here. We are also involved in an effort to develop new path-planning methods for nonholonomic systems with drift [11]. 3 Learning Controller Architecture In designing a controller, we have taken some inspiration from biological systems, the original learning controllers.
Reference: [9] <author> C. </author> <title> Frohlich. </title> <journal> Do springboard divers violate angular momentum conservation? American Journal of Physics, </journal> <volume> 47(7) </volume> <pages> 583-592, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: Since the diver is falling while executing the maneuver, there is a predetermined length of time in which the controls can act. Since the diver generally starts with his momentum totally in the somersault direction, he needs to execute a "throwing" maneuver with his arms to initiate twisting (see <ref> [9] </ref>). We are currently simulating a diver with ten degrees of freedom in the joints: three in each shoulder, one at each elbow, and one at each hip.
Reference: [10] <author> P. Di Giamberardino, S. Monaco, and D. Normand-Cyrot. </author> <title> Digital control through finite feedback discretizability. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <volume> volume 4, </volume> <pages> pages 3141-3146, </pages> <year> 1996. </year>
Reference-contexts: Although much work has been done recently on the control and steering of nonholonomic systems, most of it has been for drift-free systems (for a survey, see [23]). Some specific cases with drift have been addressed ([6]; [24]; <ref> [10] </ref>, e.g.), but very little work exists concerning general systems with drift.
Reference: [11] <author> J-M. Godhavn, A. Balluchi, L. S. Crawford, and S. S. Sastry. </author> <title> Path planning for non-holonomic systems with drift. </title> <booktitle> Submitted to the 1997 American Control Conference., </booktitle> <year> 1996. </year>
Reference-contexts: A simple learning algorithm applied to the planar diver was more promising (see [8]), and led to our continued work on learning controllers described here. We are also involved in an effort to develop new path-planning methods for nonholonomic systems with drift <ref> [11] </ref>. 3 Learning Controller Architecture In designing a controller, we have taken some inspiration from biological systems, the original learning controllers. For example, biological systems deal with dynamic complexity through hierarchical organization, with different parts of the motor control system performing different functions (see Figure 1).
Reference: [12] <author> G. J. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <institution> CMU-CS-95-103, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: Certain types of function approximators have been shown to guarantee convergence when combined with dynamic programming techniques; in particular, neural network approximators may not converge, but certain linear interpolation approimators will <ref> [12] </ref>, as will some feature-based methods (including radial basis function networks) satisfying certain properties, under a modified dynamic programming algorithm [30].
Reference: [13] <author> D. Gorinevsky, A. Kapitanovsky, and A. Goldenberg. </author> <title> Radial basis function network architecture for nonholonomic motion planning and control of free-flying manipulators. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 12(3), </volume> <month> June </month> <year> 1996. </year>
Reference-contexts: There are some learning approaches to problems of this type in the literature as well: Gorinevsky, Kapitanovsky, and Goldenberg <ref> [13] </ref> use radial basis functions to learn the controls for steering a space platform with an arm, and Bertsekas and Tsitsiklis [3] use neurodynamic programming to learn to control discrete systems.
Reference: [14] <author> G. L. Gottlieb, D. M. Corcos, and G. C. Agarwal. </author> <title> Strategies for the control of voluntary movements with one mechanical degree of freedom. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 12 </volume> <pages> 189-210, </pages> <year> 1989. </year>
Reference-contexts: More recently, several investigators have found evidence for the existence of low-level controllers in fast, goal-directed, single-joint movements (see, for example, <ref> [14] </ref>). In the model proposed by Gottlieb, Corcos, and Agarwal in [14], the low-level controller is a pulse generator which produces square activation pulses as inputs to the motoneuron pool. <p> More recently, several investigators have found evidence for the existence of low-level controllers in fast, goal-directed, single-joint movements (see, for example, <ref> [14] </ref>). In the model proposed by Gottlieb, Corcos, and Agarwal in [14], the low-level controller is a pulse generator which produces square activation pulses as inputs to the motoneuron pool. Such a controller would produce the stereotypical patterns often seen in fast, single-joint movements, which are identifiable by their torque, velocity, and double 5 or triple-burst EMG profiles. <p> Based on the model above, we have chosen torque profiles consisting of two square pulses, as indicated in Figure 3. As these torque profiles have four obvious parameters, namely the pulse heights and widths for the two pulses, we use another idea from <ref> [14] </ref> to restrict the control family. There, it is hypothesized that the motor control system uses two different control schemes in different conditions, pulse height modulation (PHM) and pulse width modulation (PWM).
Reference: [15] <author> S. Grillner. </author> <title> Locomotion in vertebrates: central mechanisms and reflex interaction. </title> <journal> Physiological Reviews, </journal> <volume> 55(2) </volume> <pages> 247-304, </pages> <month> April </month> <year> 1975. </year>
Reference-contexts: It has been known for some time that rhythmic movements like walking, swimming, breathing, and chewing are controlled in many animals by periodic pattern generators (for a review, see <ref> [15] </ref>). More recently, several investigators have found evidence for the existence of low-level controllers in fast, goal-directed, single-joint movements (see, for example, [14]).
Reference: [16] <author> S. S. Haykin. </author> <title> Neural Networks: A Comprehensive Foundation. </title> <publisher> MacMillan, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Our current implementation uses networks of radial basis functions for both the single-DOF controllers and the coordinating controller (see <ref> [16] </ref> for an introduction).
Reference: [17] <author> J. Hertz, Anders Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: With a radial basis function architecture, only one layer is required to approximate any function, whereas with a conventional neural network architecture, two are needed if the function is discontinuous (see <ref> [17] </ref> for a brief summary). Thus, if the centers and the functions themselves are fixed, a linear algorithm such as recursive least squares can be applied to the weights. In our implementation, the situation is a bit more complicated.
Reference: [18] <author> J. K. Hodgins and M. H. </author> <title> Raibert. </title> <journal> Biped gymnastics. International Journal of Robotics Research, </journal> <volume> 9(2) </volume> <pages> 115-132, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Some specific cases with drift have been addressed ([6]; [24]; [10], e.g.), but very little work exists concerning general systems with drift. Other control approaches to problems like this include that of Hodgins and Raibert <ref> [18] </ref> and Wooten and Hodgins [32], who divide complicated movements into states of a finite state machine; within each state, motions are regulated by PD controllers.
Reference: [19] <author> M. G. Hollars, D. E. Rosenthal, and M. A. Sherman. </author> <title> SD/FAST user's manual. </title> <address> Mountain View, CA, </address> <year> 1991. </year>
Reference-contexts: As the equations of motion for these systems are prohibitively long and complex, we have used the SD/FAST software package (Symbolic Dynamics, Inc. <ref> [19] </ref>), which uses Kane's formulation, for our simulations. Our approach for dealing with the complexity of the control problem is inspired by biological systems. First, our controller design has a hierarchical structure which simplifies the control task at each level. <p> If the joint runs up against the joint limits during on-line learning, a virtual error estimating the overshoot prevented by the joint limit is added to the position error, to facilitate the learning. Our simulations use the SD/FAST software package <ref> [19] </ref> with a three-dimensional diver model generously shared with us by Jessica Hodgins (see [32]). Preliminary simulations on the single-DOF controllers are promising.
Reference: [20] <author> T. Jaakkola, M. I. Jordan, and S. P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 1185-1201, </pages> <year> 1994. </year> <month> 15 </month>
Reference-contexts: For systems in which the state space is discrete, and the Q values can be stored in a table, if the states contain sufficient information that the system is Markov, then the Q-learning algorithm converges (see [1], <ref> [20] </ref>, [29]). For a system like the diver, however, where we require a function approximator, convergence is more problematic. Boyan and Moore [4] give simple examples in which substituting a function approximator for a lookup table results in loss of convergence.
Reference: [21] <author> M. Jeannerod. </author> <title> The Neural and Behavioral Organization of Goal-Directed Movements. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1988. </year>
Reference-contexts: Various researchers have shown that the most important piece of information for humans learning new, complex tasks is the relative timing between the different movement segments or the phasing between continuous movements (see, e.g., [31]; [28]; <ref> [21] </ref>, Ch. 1). Thus, complex skills can be learned by combining more basic movement building blocks in an appropriate way.
Reference: [22] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement learning: a survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: However, a more fruitful approach may be that of reinforcement learning. Good surveys of reinforcement learning can be found in [26] and <ref> [22] </ref>. In reinforcement learning, the output error jjy p y d jj is minimized directly, and the learning can be distributed over sequences of actions. One variant which does not require a system model is Q-learning. <p> Boyan and Moore [4] give simple examples in which substituting a function approximator for a lookup table results in loss of convergence. These examples can be made to converge by changing slightly the methods used, however; see <ref> [22] </ref> for a summary.
Reference: [23] <author> I. Kolmanovsky and N. H. McClamroch. </author> <title> Developments in nonholonomic control problems. </title> <journal> IEEE Control Systems, </journal> <volume> 15(6) </volume> <pages> 20-36, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Although much work has been done recently on the control and steering of nonholonomic systems, most of it has been for drift-free systems (for a survey, see <ref> [23] </ref>). Some specific cases with drift have been addressed ([6]; [24]; [10], e.g.), but very little work exists concerning general systems with drift.
Reference: [24] <author> I. V. Kolmanovsky, N. H. McClamroch, and V. T. Coppola. </author> <title> Controllability of a class of nonlinear systems with drift. </title> <booktitle> In Proceedings of the 33rd Conference on Decision and Control, </booktitle> <pages> pages 1254-1255, </pages> <year> 1994. </year>
Reference-contexts: Although much work has been done recently on the control and steering of nonholonomic systems, most of it has been for drift-free systems (for a survey, see [23]). Some specific cases with drift have been addressed ([6]; <ref> [24] </ref>; [10], e.g.), but very little work exists concerning general systems with drift.
Reference: [25] <author> A. C. Pil and H. Asada. </author> <title> Recursive experimental structure re-design of a robot arm using rapid prototyping. </title> <booktitle> In Proceedings of the 1994 IEEE International Conference on Robotics and Automation, </booktitle> <volume> volume 2, </volume> <pages> pages 1094-1099, </pages> <year> 1994. </year>
Reference-contexts: Also, the controllers are feed-forward (except for the limited use of PD controllers mentioned above) at this time, though we plan to add feedback in the future. Our control system design has several features similar to Pil and Asada's recursive structure redesign algorithm <ref> [25] </ref> as well. 4 Learning Algorithms In biological systems, when the structure of the system is not known a priori, an internal model can be built up through learning. The learned model will allow the system to generalize from known tasks to new tasks.
Reference: [26] <author> S. J. Russell and P. Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1995. </year>
Reference-contexts: However, a more fruitful approach may be that of reinforcement learning. Good surveys of reinforcement learning can be found in <ref> [26] </ref> and [22]. In reinforcement learning, the output error jjy p y d jj is minimized directly, and the learning can be distributed over sequences of actions. One variant which does not require a system model is Q-learning.
Reference: [27] <author> R. A. Schmidt. </author> <title> A schema theory of discrete motor skill learning. </title> <journal> Psychological Review, </journal> <volume> 82(4) </volume> <pages> 225-260, </pages> <year> 1975. </year>
Reference-contexts: In the behavioral literature, controllers like these might be viewed as schemas. The original definition of a schema is simply a learned relationship between the input and required output vectors of the controller <ref> [27] </ref>. Here, this would correspond to a learned model of the inverse relationship governing the lower levels of the system.
Reference: [28] <author> Schoner and Kelso. </author> <title> A synergetic theory of environmentally-specified and learned patterns of movement coordination: I. relative phase dynamics. </title> <journal> Biological Cybernetics, </journal> <volume> 58 </volume> <pages> 71-80, </pages> <year> 1988. </year>
Reference-contexts: Various researchers have shown that the most important piece of information for humans learning new, complex tasks is the relative timing between the different movement segments or the phasing between continuous movements (see, e.g., [31]; <ref> [28] </ref>; [21], Ch. 1). Thus, complex skills can be learned by combining more basic movement building blocks in an appropriate way.
Reference: [29] <author> J. N. Tsitsiklis. </author> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16 </volume> <pages> 185-202, </pages> <year> 1994. </year>
Reference-contexts: For systems in which the state space is discrete, and the Q values can be stored in a table, if the states contain sufficient information that the system is Markov, then the Q-learning algorithm converges (see [1], [20], <ref> [29] </ref>). For a system like the diver, however, where we require a function approximator, convergence is more problematic. Boyan and Moore [4] give simple examples in which substituting a function approximator for a lookup table results in loss of convergence.
Reference: [30] <author> J. N. Tsitsiklis and B. Van Roy. </author> <title> Feature-based methods for large scale dynamic programming. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 59-94, </pages> <year> 1996. </year>
Reference-contexts: types of function approximators have been shown to guarantee convergence when combined with dynamic programming techniques; in particular, neural network approximators may not converge, but certain linear interpolation approimators will [12], as will some feature-based methods (including radial basis function networks) satisfying certain properties, under a modified dynamic programming algorithm <ref> [30] </ref>.
Reference: [31] <author> B. Vereijken, H. T. A. Whiting, and Beek. </author> <title> A dynamical systems approach to skill acquisition. </title> <journal> Quarterly Journal of Experimental Psychology Section A Human Experimental Psychology, </journal> <volume> 45(2) </volume> <pages> 323-344, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Various researchers have shown that the most important piece of information for humans learning new, complex tasks is the relative timing between the different movement segments or the phasing between continuous movements (see, e.g., <ref> [31] </ref>; [28]; [21], Ch. 1). Thus, complex skills can be learned by combining more basic movement building blocks in an appropriate way.
Reference: [32] <author> W. L. Wooten and J. K. Hodgins. </author> <title> Animation of human diving. </title> <journal> Computer Graphics Forum, </journal> <volume> 15(1) </volume> <pages> 3-13, </pages> <month> March </month> <year> 1996. </year> <month> 16 </month>
Reference-contexts: Some specific cases with drift have been addressed ([6]; [24]; [10], e.g.), but very little work exists concerning general systems with drift. Other control approaches to problems like this include that of Hodgins and Raibert [18] and Wooten and Hodgins <ref> [32] </ref>, who divide complicated movements into states of a finite state machine; within each state, motions are regulated by PD controllers. <p> Our simulations use the SD/FAST software package [19] with a three-dimensional diver model generously shared with us by Jessica Hodgins (see <ref> [32] </ref>). Preliminary simulations on the single-DOF controllers are promising. Figure 6 shows the squared errors in the controls u and the plant output v p for online training of a PHM controller for shoulder 10 abduction/adduction with v s = (0; 0).
References-found: 32

