URL: http://www.isle.org/~langley/papers/flex.uai95.ps
Refering-URL: http://www.isle.org/~langley/pubs.html
Root-URL: 
Email: gjohn@CS.Stanford.EDU  langley@CS.Stanford.EDU  
Title: Estimating Continuous Distributions in Bayesian Classifiers  
Author: George H. John Pat Langley 
Note: In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Mateo, 1995  
Web: http://robotics.stanford.edu/~gjohn/  http://robotics.stanford.edu/~langley/  
Address: Stanford, CA 94305  Stanford, CA 94305  
Affiliation: Computer Science Dept. Stanford University  Robotics Laboratory Stanford University  
Abstract: When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparamet-ric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Buntine, W. L. </author> <year> (1994), </year> <title> "Operations for learning with graphical models", </title> <journal> Journal of Artificial Intelligence Research 2, </journal> <pages> 159-225. </pages>
Reference-contexts: Thus, when depicted graphically, a naive Bayesian classifier has the form shown in Figure 1, in which all arcs are directed from the class attribute to the observable, predictive attributes <ref> (Buntine 1994) </ref>. These assumptions support very efficient algorithms for both classification and learning. To see this, let C be the random variable denoting the class of an instance and let X be a vector of random variables denoting the observed attribute values.
Reference: <author> Casella, G. & Berger, R. L. </author> <year> (1990), </year> <title> Statistical Inference, </title> <publisher> Wadsworth & Brooks/Cole. </publisher>
Reference-contexts: Let n j = P number of samples of value j. Then n j =n is a strongly consistent estimator of p j . Proof: This is a direct instantiation of the strong law of large numbers <ref> (Casella & Berger 1990) </ref>. Theorem 2 (Strong Consistency for Reals) Due to Devroye (1983).
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W. & Freeman, D. </author> <year> (1988), </year> <title> Autoclass: A Bayesian classification system, </title> <booktitle> in "Machine Learning: Proceedings of the Fifth International Workshop", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 54-64. </pages>
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1989), </year> <title> "The CN2 induction algorithm", </title> <booktitle> Machine Learning 3(4), </booktitle> <pages> 261-83. </pages>
Reference: <author> Cooper, G. F. & Herskovits, E. </author> <year> (1992), </year> <title> "A Bayesian method for the induction of probabilistic networks from data", </title> <booktitle> Machine Learning 9(4), </booktitle> <pages> 309-347. </pages>
Reference: <author> Dempster, A. P., Laird, N. M. & Rubin, D. B. </author> <year> (1977), </year> <title> "Maximum likelihood from incomplete data via the EM algorithm", </title> <journal> Journal of the Royal Statistical Society B 39, </journal> <pages> 1-38. </pages>
Reference-contexts: A further possibility is borrow Cheeseman et al.'s (1988) Bayesian approach to density estimation with Gaussian mixtures using the EM algorithm <ref> (Dempster, Laird & Rubin 1977) </ref>. 7 Conclusion In this paper we reviewed the naive Bayesian classifier and the assumptions on which it relies, including the common use of a single Gaussian distribution for each predictive attribute.
Reference: <author> Devroye, L. </author> <year> (1983), </year> <title> "The equivalence of weak, strong, and complete convergence in l 1 for kernel density estimates", </title> <journal> The Annals of Statistics 11, </journal> <pages> 896-904. </pages>
Reference: <author> Dougherty, J., Kohavi, R. & Sahami, M. </author> <year> (1995), </year> <title> Supervised and unsupervised discretization of continuous features, </title> <booktitle> in "Machine Learning: Proceedings of the Twelfth International Conference", </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Geiger, D. & Heckerman, D. </author> <year> (1994), </year> <title> Learning Gaussian networks, </title> <booktitle> in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. 235-243. </pages>
Reference: <author> Hastie, T. J. & Tibshirani, R. J. </author> <year> (1990), </year> <title> Generalized Additive Models, </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Heckerman, D., Geiger, D. & Chickering, D. </author> <year> (1994), </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data, </title> <booktitle> in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. 293-301. </pages>
Reference: <author> Izenman, A. J. </author> <year> (1991), </year> <title> "Recent developments in non-parametric density estimation", </title> <journal> Journal of the Am. Stat. Assoc. </journal> <volume> 86(413), </volume> <pages> 205-223. </pages>
Reference-contexts: In this section we discuss the theoretical properties of kernel density estimation and their implications for the Flexible Bayes algorithm. Statisticians are principally concerned with the consistency of a density estimate <ref> (Izenman 1991) </ref>.
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages>
Reference: <author> Kibler, D. & Langley, P. </author> <year> (1988), </year> <title> Machine learning as an experimental science, </title> <booktitle> in "Proceedings of the Third European Working Session on Learning", </booktitle> <publisher> Pitman Publishing, </publisher> <address> London, UK, </address> <pages> pp. 81-92. </pages>
Reference-contexts: Within machine learning, the standard experimental method <ref> (Kibler & Langley 1988) </ref> involves running a learning algorithm on a set of training data, then using the induced model to make predictions about separate test cases and measuring the accuracy.
Reference: <author> Kononenko, I. </author> <year> (1991), </year> <title> Semi-naive Bayesian classifier, </title> <booktitle> in "Proceedings of the Sixth European Working Session on Learning", Pittman, Porto, Portugal, </booktitle> <pages> pp. 206-219. </pages>
Reference: <author> Kononenko, I. </author> <year> (1993), </year> <title> "Inductive and Bayesian learning in medical diagnosis", </title> <booktitle> Applied Artificial Intelligence 7, </booktitle> <pages> 317-337. </pages>
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994), </year> <title> Induction of selective Bayesian classifiers, </title> <booktitle> in "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Seattle, WA, </address> <pages> pp. 399-406. </pages>
Reference: <author> Langley, P., Iba, W. & Thompson, K. </author> <year> (1992), </year> <title> An analysis of Bayesian classifiers, </title> <booktitle> in "Proceedings of the Tenth National Conference on Artificial Intelligence", </booktitle> <pages> pp. 223-228. </pages>
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <note> "UCI repository of machine learning databases", Available by anonymous ftp to ics.uci.edu in the pub/machine- learning-databases directory. </note>
Reference-contexts: To evaluate the behavior of the flexible Bayesian classifier, we designed and carried out a number of experimental studies along these lines. 5.1 Experiments on Natural Data To determine the relevance of our approach to real-world problems, we first selected 11 databases from the UCI machine learning repository <ref> (Murphy & Aha 1994) </ref> and elsewhere. Table 2 summarizes the number of instances, the number of classes, and the number of nominal and numeric attributes in each data set.
Reference: <author> Pazzani, M. </author> <year> (1995), </year> <title> Searching for attribute dependencies in Bayesian classifiers, </title> <booktitle> in "Fifth International Workshop on Artificial Intelligence and Statistics", </booktitle> <pages> pp. 424-429. </pages>
Reference: <author> Provan, G. M. & Singh, M. </author> <year> (1995), </year> <title> Learning Bayesian networks using feature selection, </title> <booktitle> in "Fifth International Workshop on Artificial Intelligence and Statistics", </booktitle> <pages> pp. 450-456. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1995), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schalkoff, R. </author> <year> (1992), </year> <title> Pattern Recognition: Statistical, Structural, and Neural Approaches, </title> <publisher> Wiley. </publisher>
Reference-contexts: The maximum likelihood estimates of the mean and standard deviation of a normal distribution are the sample average and the sample standard deviation <ref> (Schalkoff 1992) </ref>. To clarify the estimation process, consider a small data set in which there are two classes (+ and ), a nominal attribute X 1 which takes values a and b, and a continuous attribute X 2 .
Reference: <author> Silverman, B. W. </author> <year> (1986), </year> <title> Density estimation for statistics and data analysis, </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Specht, D. F. & Romsdahl, H. </author> <year> (1994), </year> <title> Experience with adaptive pobabilistic neural networks and adaptive general regression neural networks, </title> <booktitle> in "IEEE International Conference on Neural Networks", </booktitle> <address> Orlando, FL. </address>
Reference: <author> Venables, W. N. & Ripley, B. D. </author> <year> (1994), </year> <title> Modern Applied Statistics with S-Plus, </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The Bayesian classifier encounters this problem whenever it must estimate p (XjC) for some continuous attribute X. This is a general problem in statistics, and a variety of methods are available for solving it <ref> (Venables & Ripley 1994, Silverman 1986) </ref>. In this section we discuss the theoretical properties of kernel density estimation and their implications for the Flexible Bayes algorithm. Statisticians are principally concerned with the consistency of a density estimate (Izenman 1991).
References-found: 26

