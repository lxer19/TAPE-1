URL: http://www.cs.princeton.edu/~ristad/papers/pu-544-97.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/papers/pu-544-97.html
Root-URL: http://www.cs.princeton.edu
Title: Hierarchical Non-Emitting Markov Models  
Author: Eric Sven Ristad Robert G. Thomas 
Keyword: Markov model, interpolated Markov model, hidden Markov model, mixture modeling, non-emitting state transitions, state-conditional interpolation, statistical language model, discrete time series, Brown corpus, Wall Street  
Note: Journal.  
Abstract: Department of Computer Science Princeton University Research Report CS-TR-544-97 May 1997; Revised January 1998 Abstract We describe a simple variant of the interpolated Markov model with non-emitting state transitions and prove that it is strictly more powerful than any Markov model. More importantly, the non-emitting model outperforms the classic interpolated model on natural language texts under a wide range of experimental conditions, with only a modest increase in computational requirements. The non-emitting model is also much less prone to overfitting. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bahl, L. R., Brown, P. F., de Souza, P. V., Mercer, R. L., and Nahamoo, D. </author> <title> A fast algorithm for deleted interpolation. </title> <booktitle> In Proc. </booktitle> <address> EU-ROSPEECH '91 (Genoa, </address> <year> 1991), </year> <pages> pp. 1209-1212. </pages>
Reference-contexts: A basic Markov model = hA; n; ffi n i consists of an alphabet A, a model order n, n 0, and the state transition probabilities ffi n : A n fi A ! <ref> [0; 1] </ref>. With probability ffi n (yjx n ), a Markov model in the state x n will emit the symbol y and transition to the state x n 2 y. <p> Formally, an interpolated Markov model = hA; n; ffi; i consists of a finite alphabet A, a maximal model order n, the state transition probabilities ffi = ffi 0 : : : ffi n , ffi i : A i fi A ! <ref> [0; 1] </ref>, and the state-conditional interpolation parameters : A n fi [0; n] ! [0; 1]. The state order is a hidden variable. <p> n; ffi; i consists of a finite alphabet A, a maximal model order n, the state transition probabilities ffi = ffi 0 : : : ffi n , ffi i : A i fi A ! <ref> [0; 1] </ref>, and the state-conditional interpolation parameters : A n fi [0; n] ! [0; 1]. The state order is a hidden variable. <p> While parameter tying can improve performance, reducing state-conditional interpolation to state-independent in terpolation results in poor performance. 3 A hierarchical parameterization of the full state-conditional interpolation is more effective. Let i : A i ! <ref> [0; 1] </ref> be the set of i th order state interpolation parameters, where i (x i ) is the probability of using the i th order state transition probability ffi i (jx i ), conditioned on the decision not to use any higher order state transition probability. (ijx n ) = <p> A non-emitting mixture Markov model = hA; n; ffi; i consists of a finite alphabet A, a maximal model order n, the emitting state transition probabilities ffi i : A i fi A ! <ref> [0; 1] </ref>, and the non-emitting state transition probabilities i : 4 A i fi [0; n] ! [0; 1]. The non-emitting model alternates between non-emitting and emitting transitions according to the and ffi parameters, respectively. <p> non-emitting mixture Markov model = hA; n; ffi; i consists of a finite alphabet A, a maximal model order n, the emitting state transition probabilities ffi i : A i fi A ! <ref> [0; 1] </ref>, and the non-emitting state transition probabilities i : 4 A i fi [0; n] ! [0; 1]. The non-emitting model alternates between non-emitting and emitting transitions according to the and ffi parameters, respectively. The parameter (jjx i ) specifies the probability that the model will transition from the state x i to the state x j without emitting a symbol. <p> The expectation-step () algorithm requires O (nb) time and space for an order n non-emitting model on a string x b of length b. A comparable interpolated model can take an expectation step in O (nb) time and O (1) space <ref> [1] </ref>. While the difference between O (nb) and O (1) space can be considerable, the additional space requirements of the non-emitting algorithm are small when compared to the cost of storing all the model parameters.
Reference: [2] <author> Baum, L., and Eagon, J. </author> <title> An inequality with applications to statistical estimation for probabilistic functions of a Markov process and to models for ecology. Bull. </title> <booktitle> AMS 73 (1967), </booktitle> <pages> 360-363. </pages>
Reference-contexts: (EM) algorithm to optimize the parameters of a hierarchical non-emitting mixture model on data. 6 An EM algorithm iteratively maximizes the probability of the training data ac-cording to the model by computing the expectation of model parameters on the data and then updating the model parameters to maximize those expectations <ref> [2, 3, 6] </ref>. The non-emitting mixture model is sufficiently expressive that any maximum likelihood estimator will overfit its parameters to the training corpus. Unseen events will be assigned zero probability, and the overfit model will fail to accurately predict the future.
Reference: [3] <author> Baum, L., Petrie, T., Soules, G., and Weiss, N. </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Ann. Math. Stat. </journal> <volume> 41 (1970), </volume> <pages> 164-171. </pages>
Reference-contexts: (EM) algorithm to optimize the parameters of a hierarchical non-emitting mixture model on data. 6 An EM algorithm iteratively maximizes the probability of the training data ac-cording to the model by computing the expectation of model parameters on the data and then updating the model parameters to maximize those expectations <ref> [2, 3, 6] </ref>. The non-emitting mixture model is sufficiently expressive that any maximum likelihood estimator will overfit its parameters to the training corpus. Unseen events will be assigned zero probability, and the overfit model will fail to accurately predict the future.
Reference: [4] <author> Brown, P., Pietra, V. D., Pietra, S. D., Lai, J., and Mercer, R. </author> <title> An estimate of an upper bound for the entropy of English. </title> <booktitle> Computational Linguistics 18 (1992), </booktitle> <pages> 31-40. </pages>
Reference: [5] <author> Cleary, J., and Witten, I. </author> <title> Data compression using adaptive coding and partial string matching. </title> <journal> IEEE Trans. Comm. COM-32, </journal> <volume> 4 (1984), </volume> <pages> 396-402. </pages>
Reference-contexts: Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state order distribution. Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model [19, 20, 26], the backoff model <ref> [5, 13] </ref>, and the interpolated Markov model [12, 14]. More importantly, the non-emitting model consistently outperforms the best Markov models on natural language texts, under a wide range of experimental conditions. The non-emitting model is also nearly as computationally efficient and easy to implement as the interpolated Markov model.
Reference: [6] <author> Dempster, A., Laird, N., and Rubin, D. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statist. Soc. Ser. B (methodological) 39 (1977), </journal> <pages> 1-38. </pages>
Reference-contexts: (EM) algorithm to optimize the parameters of a hierarchical non-emitting mixture model on data. 6 An EM algorithm iteratively maximizes the probability of the training data ac-cording to the model by computing the expectation of model parameters on the data and then updating the model parameters to maximize those expectations <ref> [2, 3, 6] </ref>. The non-emitting mixture model is sufficiently expressive that any maximum likelihood estimator will overfit its parameters to the training corpus. Unseen events will be assigned zero probability, and the overfit model will fail to accurately predict the future.
Reference: [7] <author> Francis, W. N., and Kucera, H. </author> <title> Frequency analysis of English usage: lexicon and grammar. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, </address> <year> 1982. </year>
Reference-contexts: To take the true measure of non-emitting models for natural language texts, we evaluate their performance as character models on the Brown corpus <ref> [7] </ref> and as word models on the Wall Street Journal. Our results show that the non-emitting Markov model consistently gives better predictions than the traditional interpolated Markov model under equivalent experimental conditions. <p> The non-emitting model consistently outperformed the interpolated model on both corpora for all ten parameter tying schemes. Thomas shows that our frequency-diversity parameter tying scheme is one of the more effective parameter schemes. 6.1 Brown Corpus Our first set of experiments were with character models on the Brown corpus <ref> [7] </ref>. The Brown corpus is an eclectic collection of English prose, containing 6,004,032 characters partitioned into 500 files. We performed 10 iterations of cross estimation on 21 blocks. Results are reported as per-character test message entropies (bits/char), 1 v log 2 p (y v jv).
Reference: [8] <author> Good, I. J. </author> <title> The population frequencies of species and the estimation of population parameters. </title> <booktitle> Biometrika 40 (1953), </booktitle> <pages> 237-264. </pages>
Reference: [9] <author> Good, I. J. </author> <title> The Estimation of Probabilities. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1965. </year>
Reference: [10] <author> H. Ney, U. Essen, R. K. </author> <title> On the estimation of small probabilities by leaving-one-out. </title> <journal> IEEE Trans. </journal> <volume> PAMI 17, 12 (1995), </volume> <pages> 1202-1212. </pages>
Reference: [11] <author> Howard, P., and Vitter, J. </author> <title> Practical implementations of arithmetic coding. In Image and Text Compression, </title> <editor> J. Storer, Ed. </editor> <publisher> Kluwer Academic, Norwell, </publisher> <address> MA, </address> <year> 1992, </year> <pages> pp. 85-112. </pages>
Reference: [12] <author> Jelinek, F., and Mercer, R. L. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Pattern Recognition in Practice (Amsterdam, </booktitle> <month> May 21-23 </month> <year> 1980), </year> <editor> E. S. Gelsema and L. N. Kanal, Eds., </editor> <publisher> North Holland, </publisher> <pages> pp. 381-397. </pages>
Reference-contexts: Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model [19, 20, 26], the backoff model [5, 13], and the interpolated Markov model <ref> [12, 14] </ref>. More importantly, the non-emitting model consistently outperforms the best Markov models on natural language texts, under a wide range of experimental conditions. The non-emitting model is also nearly as computationally efficient and easy to implement as the interpolated Markov model. <p> In the interpolated Markov model, the transition probabilities from states of different orders are combined using state-conditional mixing parameters. The mixing parameters smooth the transition probabilities from higher order states with those from lower order states <ref> [12] </ref>. Mixing the transition probabilities from states of different orders results in more accurate predictions than can be obtained from any fixed model order. <p> Unseen events will be assigned zero probability, and the overfit model will fail to accurately predict the future. The traditional solution to this problem for interpolated Markov models is cross-estimation <ref> [12] </ref>. Cross-estimation repeatedly partitions the training data into two blocks and optimizes the mixing parameters on one block after initializing the state transition parameters on the other block. We present a traditional cross-estimation algorithm for hierarchical non-emitting models. <p> In the Potamianos-Jelinek experiments, the test block consisted of complete sentences chosen uniformly from the entire (modified) Brown corpus. To this comparison, we added the original interpolation schemes of Je-linek and Mercer <ref> [12] </ref> under 10 iterations of forward-estimation (DI-FE) and cross-estimation (DI-CE). Both models used hierarchical state-conditional interpolation (x i ) and straight frequency fi diversity parameter tying. We also added the hierarchical non-emitting model with straight frequency fi diversity parameter tying, and 10 iterations of forward-estimation (NE-FE) and cross-optimization (NE-CE).
Reference: [13] <author> Katz, S. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Trans. ASSP 35 (1987), </journal> <pages> 400-401. </pages>
Reference-contexts: Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state order distribution. Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model [19, 20, 26], the backoff model <ref> [5, 13] </ref>, and the interpolated Markov model [12, 14]. More importantly, the non-emitting model consistently outperforms the best Markov models on natural language texts, under a wide range of experimental conditions. The non-emitting model is also nearly as computationally efficient and easy to implement as the interpolated Markov model.
Reference: [14] <author> MacKay, D. J., and Peto, L. C. B. </author> <title> A hierarchical Dirichlet language model. </title> <booktitle> Natural Language Engineering 1, </booktitle> <month> 1 </month> <year> (1994). </year>
Reference-contexts: Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model [19, 20, 26], the backoff model [5, 13], and the interpolated Markov model <ref> [12, 14] </ref>. More importantly, the non-emitting model consistently outperforms the best Markov models on natural language texts, under a wide range of experimental conditions. The non-emitting model is also nearly as computationally efficient and easy to implement as the interpolated Markov model.
Reference: [15] <author> Moffat, A. </author> <title> Implementing the PPM data compresion scheme. </title> <journal> IEEE Trans. Communications 38, </journal> <volume> 11 (1990), </volume> <pages> 1917-1921. </pages>
Reference: [16] <author> Potamianos, G., and Jelinek, F. </author> <title> The study of n-gram and decision tree letter language modeling methods. </title> <booktitle> CLSP Research Notes 13, </booktitle> <institution> The Johns Hopkins University, Baltimore, MD, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: Non-Emitting order untied tied untied tied 1 3.602 3.602 3.602 3.602 3 2.490 2.486 2.473 2.473 5 2.149 2.112 2.076 2.075 7 2.212 2.077 2.015 2.008 9 2.334 2.093 2.009 1.996 We also compared the performance of our techniques with two new interpolation schemes recently proposed by Potamianos and Jelinek <ref> [16] </ref>. Their DI-TD scheme uses hierarchical state-conditional interpolation (x i ), variable-width frequency fi order parameter tying, and "top-down optimization" on one withheld block. Their DI-BU scheme uses general state-conditional interpolation (jjx i ), variable-width frequency fi order parameter tying, and bottom-up optimization on one withheld block.
Reference: [17] <author> Riccardi, G., Pieraccini, R., and Bocchieri, E. </author> <title> Stochastic automata for language modeling. </title> <booktitle> Computer Speech and Language 10 (1996), </booktitle> <pages> 265-293. </pages>
Reference: [18] <author> Rissanen, J. </author> <title> Probability estimation for symbols observed or not. </title> <booktitle> In International Symposium on Information Theory (Quebec, </booktitle> <address> CN, </address> <month> September 26-30 </month> <year> 1983), </year> <note> IEEE. </note>
Reference: [19] <author> Rissanen, J. </author> <title> A universal data compression system. </title> <journal> IEEE Trans. Inform. Theory IT-29, </journal> <volume> 5 (1983), </volume> <pages> 656-664. </pages>
Reference-contexts: Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state order distribution. Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model <ref> [19, 20, 26] </ref>, the backoff model [5, 13], and the interpolated Markov model [12, 14]. More importantly, the non-emitting model consistently outperforms the best Markov models on natural language texts, under a wide range of experimental conditions.
Reference: [20] <author> Rissanen, J. </author> <title> Complexity of strings in the class of Markov sources. </title> <journal> IEEE Trans. Inform. Theory IT-32, </journal> <volume> 4 (1986), </volume> <pages> 526-532. </pages>
Reference-contexts: Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state order distribution. Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model <ref> [19, 20, 26] </ref>, the backoff model [5, 13], and the interpolated Markov model [12, 14]. More importantly, the non-emitting model consistently outperforms the best Markov models on natural language texts, under a wide range of experimental conditions.
Reference: [21] <author> Ristad, E. S. </author> <title> A natural law of succession. </title> <type> Tech. Rep. </type> <institution> CS-TR-495-95, Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The diversity q (x i ) : = jfy : c (x i y) &gt; 0gj of a state is the number of distinct symbols observed in the state. Experience with multinomial prediction suggests that frequency and diversity are necessary to accurately estimate the likelihood of novel symbols <ref> [21] </ref>. In related work [25], Thomas compares the performance of the interpolated and non-emitting models on the Brown corpus and Wall Street Journal with ten different parameter tying schemes. His experiments confirm that some parameter tying schemes improve model performance, although to a lesser degree when cross-estimation is used.
Reference: [22] <author> Ristad, E. S., and Thomas, R. G. </author> <title> New techniques for context modeling. </title> <booktitle> In Proc. 33rd Annual Meeting of the ACL (Cambridge, </booktitle> <address> MA, </address> <month> June </month> <year> 1995). </year>
Reference: [23] <author> Ristad, E. S., and Thomas, R. G. </author> <title> Hierarchical non-emitting Markov models. </title> <booktitle> In Proc. 35th Annual Meeting of the ACL (Madrid, </booktitle> <month> July 7-11 </month> <year> 1997), </year> <pages> pp. 381-385. </pages>
Reference: [24] <author> Ristad, E. S., and Yianilos, P. N. </author> <title> Library of practical abstractions, release 1.2, </title> <year> 1997. </year> <month> ftp://ftp.cs.princeton.edu/pub/packages/libpa. </month>
Reference-contexts: For this reason, our implementation used an extended exponent representation from the library of practical abstractions <ref> [24] </ref>. This balanced_t module provides single precision floating point numbers with 32 bit exponents. It is 1.5 to 3.0 times faster than the logarithmic representation, depending on the machine.
Reference: [25] <author> Thomas, R. G. </author> <title> Mixture Models for Natural Language Processing. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> January </month> <year> 1998. </year>
Reference-contexts: Experience with multinomial prediction suggests that frequency and diversity are necessary to accurately estimate the likelihood of novel symbols [21]. In related work <ref> [25] </ref>, Thomas compares the performance of the interpolated and non-emitting models on the Brown corpus and Wall Street Journal with ten different parameter tying schemes. His experiments confirm that some parameter tying schemes improve model performance, although to a lesser degree when cross-estimation is used.
Reference: [26] <author> Willems, F. M. J., Shtarkov, Y. M., and Tjalkens, T. J. </author> <title> The context-tree weighting method: basic properties. </title> <journal> IEEE Trans. Inform. Theory 41, </journal> <volume> 3 (1995), </volume> <pages> 653-664. 21 </pages>
Reference-contexts: Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state order distribution. Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model <ref> [19, 20, 26] </ref>, the backoff model [5, 13], and the interpolated Markov model [12, 14]. More importantly, the non-emitting model consistently outperforms the best Markov models on natural language texts, under a wide range of experimental conditions.
References-found: 26

