URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/DamKroGro94b.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: CNN: a Neural Architecture that Learns Multiple Transformations of Spatial Representations  
Author: J.W.M. van Dam B.J.A. Krose, F.C.A. Groen 
Address: Kruislaan 403 NL 1098 SJ Amsterdam  
Affiliation: Faculty of Mathematics and Computer Science University of Amsterdam  
Abstract-found: 0
Intro-found: 1
Reference: [Bullock et al., (1993)] <author> D. Bullock, D. Greve, S. Grossberg, </author> <year> (1993), </year> <title> "A Self-organizing Neural Network for Learning A Body-centered Invariant Representation of 3-D Target Position". </title> <booktitle> Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> 91-95. </pages>
Reference-contexts: The environment is represented in a "neural map", a collection of neurons organised in a lattice. Many solutions to the problem of the transformation of such neural maps have been described if the maps contain a single peaking unit (see, e.g., <ref> [Bullock et al., (1993)] </ref>). The peaking unit can, e.g., be the target for a saccadic eye movement. Much less attention has been paid to the same problem when all neurons in the map have independent activation values.
Reference: [Groh and Sparks (1992)] <author> J.M. Groh, D.L. Sparks, </author> <year> (1992), </year> <title> "Two models for transforming auditory signals from head-centered to eye-centered coordinates". </title> <journal> Biological Cybernetics, </journal> <volume> 67, </volume> <pages> 291-302 </pages>
Reference-contexts: These activation values can, among others, be grey values, motion values, or probabilities that the corresponding spatial positions are occupied by some obstacle (occupancy grids, see [Van Dam et al., (1993)]). Recently, [Hartman (1993)] and <ref> [Groh and Sparks (1992)] </ref> have introduced models to perform transformations of such neural maps. They both adopt the principle that activation values are gated through lateral connections. Although the models are biologically plausible, they have the obvious drawback of severe blurring of the spatial representation.
Reference: [Hartman (1993)] <author> Georg Hartmann, </author> <year> (1993), </year> <title> "Architectural Consequences of Mapping 3D Space Representations onto 2D", </title> <booktitle> Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> 898-902. </pages>
Reference-contexts: These activation values can, among others, be grey values, motion values, or probabilities that the corresponding spatial positions are occupied by some obstacle (occupancy grids, see [Van Dam et al., (1993)]). Recently, <ref> [Hartman (1993)] </ref> and [Groh and Sparks (1992)] have introduced models to perform transformations of such neural maps. They both adopt the principle that activation values are gated through lateral connections. Although the models are biologically plausible, they have the obvious drawback of severe blurring of the spatial representation.
Reference: [Van Dam et al., (1993)] <author> J.W.M. van Dam. B.J.A. Krose, F.C.A. </author> <month> Groen </month> <year> (1993), </year> <title> "Transforming Occupancy Grids Under Robot Motion", </title> <booktitle> Proceedings of the International Conference on Artificial Neural Networks, </booktitle> <address> Amsterdam, </address> <month> 318. </month>
Reference-contexts: Much less attention has been paid to the same problem when all neurons in the map have independent activation values. These activation values can, among others, be grey values, motion values, or probabilities that the corresponding spatial positions are occupied by some obstacle (occupancy grids, see <ref> [Van Dam et al., (1993)] </ref>). Recently, [Hartman (1993)] and [Groh and Sparks (1992)] have introduced models to perform transformations of such neural maps. They both adopt the principle that activation values are gated through lateral connections. <p> With the activation function given by (2), it follows the weights should be updated according to: w ij = fl 2@w ij i y i ) 2 = fl (y fl Y (1 w ik x k ) (3) More details on this method of estimating correlations are given in <ref> [Van Dam et al., (1993)] </ref>. 3 Learning multiple transformations of neural maps In the previous section it was discussed how weights w fl ij can be learned for a single transformation of neural maps. Obviously, for a different transformation, we will have different values w fl ij .
Reference: [Van Dam et al., (1994)] <author> J.W.M. van Dam. B.J.A. Krose, F.C.A. </author> <month> Groen </month> <year> (1994), </year> <title> "Optimising Local Hebbian Learning: Use the ffi-rule". </title> <booktitle> Submitted to International Conference on Artificial Neural Networks, </booktitle> <address> Sorrento. </address>
Reference-contexts: Traditionally, Hebbian learning is used to train such a network. However, in <ref> [Van Dam et al., (1994)] </ref> it is shown, that the ffi-rule is preferable, if the activation function of the output neurons is chosen to suit the learning samples. In our application, we have binary learning samples. <p> All neurons not set to 1 are set to 0. Note that these learning samples obey E [y fl i x j ] = w fl In <ref> [Van Dam et al., (1994)] </ref> it is shown, that for binary samples we must use the activation function: y i = 1 j This activation function is given by combinatorial probability theory; it com putes the probability that y fl i is set to 1 by at least one x j
References-found: 5

