URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1992/tr-92-052.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1992.html
Root-URL: http://www.icsi.berkeley.edu
Title: Improved Parallel Computations with Toeplitz-like and Hankel-like Matrices  
Author: Dario Bini Victor Pan 
Note: as supported by NSF Grants CCR 8805782 and CCR 9020690, and by MPI (40% funds). Lehman College, CUNY, and ICSI. Victor Pan was supported by NSF grants CCR 8805782, CCR 9020690 and by PSC CUNY Awards #661340, #668541, #669210 and #662478.  
Affiliation: Department of Mathematics, University of Pisa. Dario Bini w  
Date: September 1992  
Pubnum: TR-92-052  
Abstract: The known parallel algorithms for computations with general Toeplitz, Hankel, Toeplitz-like, and Hankel-like matrices are inherently sequential. We develop some new techniques in order to devise fast parallel algorithms for such computations, including the evaluation of Krylov sequences for such matrices, traces of their power sums, characteristic polynomials and generalized inverses. This has further extensions to computing the solution or a least-squares solution to a linear system of equations with such a matrix and to several polynomial evaluations (such as computing gcd, lcm, Pade approximation and extended Euclidean scheme for two polynomials), as well as to computing the minimum span of a linear recurrence sequence. The algorithms can be applied over any field of constants, with the resulting advantages of using modular arithmetic. The algorithms consist of simple computational blocks (mostly reduced to fast Fourier transforms, FFT's) and have potential practical value. We also develop the techniques for extending all our results to the case of matrices representable as the sums of Toeplitz-like and Hankel-like matrices and in addition show some more minor innovations, such as an improvement of the transition to the solution to a Toeplitz linear system T x = b from two computed columns of T 1 . 
Abstract-found: 1
Intro-found: 1
Reference: [AG] <author> G. S. Ammar, P. Gader, </author> <title> A Variant of the Gohberg-Semencul Formula Involving Cir-culant Matrices, </title> <journal> SIAM J. on Matrix Analysis and Its Applications, </journal> <volume> 12, 3, </volume> <pages> 534-541, </pages> <year> 1991. </year>
Reference-contexts: In particular, by choosing V = C 0 , with C 0 being the unit circulant matrix [with the first row e (n1) ], the reader may deduce useful formulae for the inverse of a Toeplitz matrix, similar to ones of <ref> [AG] </ref>. 4. Extension of algorithms to Toeplitz-like + Hankel-like matrices. In this section we apply the operators F + , F , and F as the operator F and the matrices of the class t , to extend the results of section 2 to Toeplitz-like and Hankel-like matrices. <p> In the case of a Hermitian and positive definite Toeplitz matrix A, the known best way for the recovery of A 1 w from A 1 e (n1) is shown in <ref> [AG] </ref> (also compare our Remark 3.1). This approach is reduced to the computation of two vectors, each taking the form L (u)Cw ; (B:2) where we are given a circulant matrix C and two vectors u and w.
Reference: [BC] <author> D. Bini, M. Capovani, </author> <title> Spectral and Computational Properties of Band Symmetric Toeplitz Matrices, </title> <journal> Lin. Alg. and its Applics., </journal> <volume> 52/53, </volume> <pages> 99-126, </pages> <year> 1983. </year>
Reference: [BGP] <author> D. Bini, L Gemignani, V. Pan, </author> <title> Improved parallel computations with matrices and polynomials, </title> <booktitle> Proc. 18-th Intern. Symposium on Automata, Languages and Programming, Lectures Notes in Computer Science, </booktitle> <volume> 510, </volume> <pages> 520-531, </pages> <publisher> Springer 1991. </publisher>
Reference: [B83] <author> D. Bini, </author> <title> On a Class of Matrices Related to Toeplitz Matrices, </title> <type> Tech. Rep. TR 83-5, </type> <institution> Computer Sci. Dept., SUNYA, Albany, </institution> <address> NY, </address> <year> 1983. </year>
Reference-contexts: In appendix C we display some correlations between such operators and the two classical displacement operators of [KKM], [CKL-A]. Some further details can be found in our original technical reports [P90b] (on the algorithms for the computations with Toeplitz and Toeplitz-like matrices) and <ref> [B83] </ref> (on the operators associated with the sums of Toeplitz-like and Hankel-like matrices, on their main properties and on some related results). 2. Improved parallel computations with Toeplitz matrices. <p> Operators of Toeplitz and Hankel type. In this section we will introduce some machinery that we will use in the next section in order to extend the algorithms of section 2 to a more general class of matrices. In particular, we will follow the line of <ref> [B83] </ref> to define this class of matrices in terms of the associated displacement operators. Our study of these classes of matrices and operators extends the theory developed in [KKM], [CKL-A]. <p> To prove part (c), we follow <ref> [B83] </ref> and apply the matrix representation (3.5) of the operator, that is, we rewrite F + (A) = P d i in matrix form, thus obtaining the block tridiagonal system of linear equations: 0 B B B Z I O . . . . . .
Reference: [CKLA] <author> J. Chun, T. Kailath, M. Lev-Ari, </author> <title> Fast Parallel Algorithm for QR-factorization of Structured Matrices, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8, 6, </volume> <pages> 899-913, </pages> <year> 1987. </year>
Reference: [FMKL] <author> B. Friedlander, M. Morf, T. Kailath, L. Ljung, </author> <title> New Inversion Formulas for Matrices Classified in Terms of their Distances from Toeplitz Matrices, </title> <journal> Lin. Alg. and its Applics., </journal> <volume> 27, </volume> <pages> 31-60, </pages> <year> 1979. </year>
Reference-contexts: In this case we may express the matrix polynomial e S i = S i mod 2 i = via its first and last columns, by applying the Gohberg-Semencul formula for the inverse of a Toeplitz matrix (see [GS], <ref> [FMKL] </ref>, [T]): e S i = u 0 L (u (i) )L T (Jv (i) ) L (Zv (i) )L T (ZJu (i) ) mod 2 i where u (i) 0 = ( e S i ) 00 = 1 mod , u (i) and v (i) are the first and
Reference: [GS] <author> I. C. Gohberg, A. A. Semencul, </author> <title> On the Inversion of Finite Toeplitz Matrices and their Continuous Analogs, </title> <journal> Math. Issl., </journal> <volume> 2, </volume> <booktitle> 201-233 (in Russian), </booktitle> <year> 1972. </year>
Reference-contexts: In this case we may express the matrix polynomial e S i = S i mod 2 i = via its first and last columns, by applying the Gohberg-Semencul formula for the inverse of a Toeplitz matrix (see <ref> [GS] </ref>, [FMKL], [T]): e S i = u 0 L (u (i) )L T (Jv (i) ) L (Zv (i) )L T (ZJu (i) ) mod 2 i where u (i) 0 = ( e S i ) 00 = 1 mod , u (i) and v (i) are the first
Reference: [JJ] <author> J. Ja Ja, </author> <title> An Introduction to Parallel Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Our algorithms satisfy the stated complexity bounds under any model of parallel computing that supports the cost bounds of Table 1.1 (listed for some fundamental problems of parallel computations). We refer the reader to [Q], <ref> [JJ] </ref> and [L] on verification of these simple cost bounds under some realistic models of parallel computing.
Reference: [KKM] <author> T. Kailath, S.-Y. Kung, M. Morf, </author> <title> Displacement Rank of Matrices and Linear Equations, </title> <journal> J. Math. Anal. Applics., </journal> <volume> 68, 2, </volume> <pages> 395-407, </pages> <year> 1979. </year>
Reference-contexts: Such an extension requires developing some special techniques of independent interest, which we present in section 3. In particular, we study the properties of some displacement operators associated with matrices of the latter class, thus extending the theory of <ref> [KKM] </ref>, [CKL-A]. In appendix C we display some correlations between such operators and the two classical displacement operators of [KKM], [CKL-A]. <p> In particular, we study the properties of some displacement operators associated with matrices of the latter class, thus extending the theory of <ref> [KKM] </ref>, [CKL-A]. In appendix C we display some correlations between such operators and the two classical displacement operators of [KKM], [CKL-A]. Some further details can be found in our original technical reports [P90b] (on the algorithms for the computations with Toeplitz and Toeplitz-like matrices) and [B83] (on the operators associated with the sums of Toeplitz-like and Hankel-like matrices, on their main properties and on some related results). 2. <p> In particular, we will follow the line of [B83] to define this class of matrices in terms of the associated displacement operators. Our study of these classes of matrices and operators extends the theory developed in <ref> [KKM] </ref>, [CKL-A]. <p> Appendix C. Correlations between F + ; F and the classical displacement operators. The classical displacement operators F + and F of <ref> [KKM] </ref>, [CKL-A], such that F + (A) = A ZAZ T ; are related to operators F + and F of sections 3 and 4 via the following equations, which hold for any n fi n matrix A ([P90b]): F + (A)Z T = F + (A) Ae (0) e (0)T
Reference: [KPa] <author> E. Kaltofen, V. Pan, </author> <title> Processor Efficient Parallel Solution of Linear Systems II: Positive Characteristic and Singular Cases, </title> <booktitle> Proc. 33-rd Ann. IEEE Symp. on Foundations of Computer Science, </booktitle> <year> 1992. </year>
Reference-contexts: Alternate techniques of <ref> [KPa] </ref> use randomization to ensure such a transition over any field at the computation cost O log 2 n d (n; p); n 2 log log n = d (n; p) log n) where p is the characteristic of the field of constants, d (n; p) = dlog n= log pe
Reference: [KP] <author> E. Kaltofen, V. Pan, </author> <title> Processor Efficient Parallel Solution of Linear Systems over an Abstract Field, </title> <booktitle> Proc. 3-rd Ann. ACM Symp. on Parallel Computers and Architectures, </booktitle> <pages> 180-191, </pages> <year> 1991. </year>
Reference-contexts: The results can be further extended to such computational problems as fast parallel evaluation of rank T , of null space of T , of the minimum span of a linear recurrence sequence (Berlekamp-Massey problem), of polynomial gcd and lcm, Pade approximation and extended Euclidean scheme for polynomials ([BGP], <ref> [KP] </ref>, [P90b]). Many of our algorithms can be immediately extended to computations with other dense structured matrices, such as Hilbert-like and Vandermonde-like matrices, by applying the techniques of [P90a].
Reference: [KR] <author> R. Karp, V. Ramachandran, </author> <title> A Survey of Parallel Algorithms for Shared Memory Computers, </title> <booktitle> Handbook for Theoretical Computer Science, </booktitle> <publisher> North-Holland, </publisher> <editor> Amsterdam (J. van Leeuwen, ed.), </editor> <month> 869-941, </month> <year> 1990. </year>
Reference-contexts: We deduced our estimates assuming Brent's modified principle <ref> [KR] </ref>, [P90b], according to which the number of processors required for the implementation of a parallel algorithm can be decreased by the factor of s, 1 s p, at the cost of slowing down the algorithm by O (s) times.
Reference: [L] <author> H. T. Leighton, </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees and Hypercubes, </title> <publisher> Morgan Kauffman, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Our algorithms satisfy the stated complexity bounds under any model of parallel computing that supports the cost bounds of Table 1.1 (listed for some fundamental problems of parallel computations). We refer the reader to [Q], [JJ] and <ref> [L] </ref> on verification of these simple cost bounds under some realistic models of parallel computing. <p> A more intricate application of Brent's principle enables us to simplify some parallel computations, improving the straightforward bounds on the complexity of the summation of n numbers from O (log n; n) to O (log n; n= log n) ([Q], <ref> [L] </ref>) and similarly for our algorithm of this paper, from O (log 2 n; n 2 ) to O (log 2 n; n 2 = log n) (see section 2).
Reference: [P] <author> V. Pan, </author> <title> Concurrent Iterative Algorithm for Toeplitz-like Linear Systems, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <note> to appear. </note>
Reference-contexts: require at least n parallel steps for the computations with nfin Toeplitz matrices (although there are faster parallel algorithms, specially devised for computations with well-conditioned Toeplitz matrices [P89], [P92]; and for rapid refinement of an already rather close initial approximation to the solution to a Toeplitz or Toeplitz-like linear system <ref> [P] </ref>).
Reference: [P90a] <author> V. Pan, </author> <title> On Computations with Dense Structured Matrices, </title> <journal> Math. of Computation, </journal> <volume> 55, 191, </volume> <pages> 179-190, </pages> <year> 1990. </year>
Reference-contexts: Many of our algorithms can be immediately extended to computations with other dense structured matrices, such as Hilbert-like and Vandermonde-like matrices, by applying the techniques of <ref> [P90a] </ref>. The algorithms work over (or can be extended to) any field of constants, which enables us to take advantage of using the techniques of residue (modular) arithmetic.
Reference: [P89] <author> V. Pan, </author> <title> Parallel Inversion of Toeplitz and Block Toeplitz Matrices, Operator Theory: </title> <booktitle> Advances and Applics., </booktitle> <volume> 40, </volume> <pages> 359-389, </pages> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference-contexts: Toeplitz and Hankel computations are inherently sequential: they either recursively reduce the dimension of the problem by 1 or, for some similar reasons, require at least n parallel steps for the computations with nfin Toeplitz matrices (although there are faster parallel algorithms, specially devised for computations with well-conditioned Toeplitz matrices <ref> [P89] </ref>, [P92]; and for rapid refinement of an already rather close initial approximation to the solution to a Toeplitz or Toeplitz-like linear system [P]).
Reference: [P90] <author> V. Pan, </author> <title> Parallel Least-Squares Solution of General and Toeplitz-like Linear Systems, </title> <booktitle> Proc. 2-nd Ann. ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> 244-253, </pages> <year> 1990. </year> <month> 18 </month>
Reference-contexts: For the converse computation, however, a simpler algorithm is available, due to [S] (see also <ref> [P90] </ref>).
Reference: [P92] <author> V. Pan, </author> <title> Parallel Solution of Toeplitzlike Linear Systems, </title> <journal> J. Complexity, </journal> <volume> 8, </volume> <pages> 1-21, </pages> <year> 1992. </year>
Reference-contexts: and Hankel computations are inherently sequential: they either recursively reduce the dimension of the problem by 1 or, for some similar reasons, require at least n parallel steps for the computations with nfin Toeplitz matrices (although there are faster parallel algorithms, specially devised for computations with well-conditioned Toeplitz matrices [P89], <ref> [P92] </ref>; and for rapid refinement of an already rather close initial approximation to the solution to a Toeplitz or Toeplitz-like linear system [P]).
Reference: [P90b] <author> V. Pan, </author> <title> Parametrization of Newton's Iteration for Computations with Structured Matrices and Applications, </title> <type> Tech. Rep. </type> <institution> CUCS-031-90, Columbia Univ., Computer Sci. Dept., </institution> <address> NY, </address> <year> 1990. </year>
Reference-contexts: The results can be further extended to such computational problems as fast parallel evaluation of rank T , of null space of T , of the minimum span of a linear recurrence sequence (Berlekamp-Massey problem), of polynomial gcd and lcm, Pade approximation and extended Euclidean scheme for polynomials ([BGP], [KP], <ref> [P90b] </ref>). Many of our algorithms can be immediately extended to computations with other dense structured matrices, such as Hilbert-like and Vandermonde-like matrices, by applying the techniques of [P90a]. <p> We deduced our estimates assuming Brent's modified principle [KR], <ref> [P90b] </ref>, according to which the number of processors required for the implementation of a parallel algorithm can be decreased by the factor of s, 1 s p, at the cost of slowing down the algorithm by O (s) times. <p> In appendix C we display some correlations between such operators and the two classical displacement operators of [KKM], [CKL-A]. Some further details can be found in our original technical reports <ref> [P90b] </ref> (on the algorithms for the computations with Toeplitz and Toeplitz-like matrices) and [B83] (on the operators associated with the sums of Toeplitz-like and Hankel-like matrices, on their main properties and on some related results). 2. Improved parallel computations with Toeplitz matrices.
Reference: [PP] <author> V. Pan, F. Preparata, </author> <title> Supereffective Slow-down of Parallel Computations, </title> <booktitle> Proc. 4-th Ann. ACM Symp. on Parallel Algorithms & Architectures, </booktitle> <pages> 402-409, </pages> <year> 1992. </year>
Reference-contexts: The latter bounds also imply the bounds O (s log 2 n; n 2 =(s log n)) for any s, 1 s n 2 = log n, due to Brent's principle. Moreover, the general techniques of supereffective slowdown of parallel computations <ref> [PP] </ref> enable us to implement our algorithms 3 so as to arrive at the bounds O (n 1a log 2 n; n 2a = log n), for any a, 0 &lt; a 1, that is, we may make our algorithms run in O (n 1a log 2 n) time using O
Reference: [Pe] <author> M. Pease, </author> <title> An Adaptation of the Fast Fourier Transform for Parallel Procelssing, </title> <journal> J. ACM, </journal> <volume> 15, </volume> <pages> 252-284, </pages> <year> 1968. </year>
Reference-contexts: In our case, the constants hidden in this "O" notation are quite small: in particular, at most 3 log n arithmetic time-steps and 2n processors are needed to support FFT on n points <ref> [Pe] </ref>, and 2dlog ne steps and dn= log ne processors suffice in order to sum n numbers.
Reference: [PFTV] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, W. T. Vetterling, </author> <title> Numerical Recipes, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge 1986. </address>
Reference-contexts: Performing the sine transform of a real vector involves by factor 1/2 fewer operations than performing its FFT <ref> [PFTV] </ref>. The above formulae enable us to compute the product of a matrix A 2 t and of a vector by means of three sine transforms at the sequential cost of O (n log n) arithmetic operations and at the parallel cost O (log n; n), (see appendix B).
Reference: [Q] <author> M. J. Quinn, </author> <title> Designing Efficient Algorithms for Parallel Computers, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Our algorithms satisfy the stated complexity bounds under any model of parallel computing that supports the cost bounds of Table 1.1 (listed for some fundamental problems of parallel computations). We refer the reader to <ref> [Q] </ref>, [JJ] and [L] on verification of these simple cost bounds under some realistic models of parallel computing.
Reference: [S] <author> A. Schonhage, </author> <title> The Fundamental Theorem of Algebra in Terms of Computational Complexity, </title> <type> unpublished manuscript, </type> <year> 1982. </year>
Reference-contexts: For the converse computation, however, a simpler algorithm is available, due to <ref> [S] </ref> (see also [P90]).
Reference: [T] <author> W. F. Trench, </author> <title> A Note on a Toeplitz Inversion Formula, Linear Alg. </title> <journal> and its Applics., </journal> <volume> 129, </volume> <pages> 55-61, </pages> <year> 1990 </year>
Reference-contexts: In this case we may express the matrix polynomial e S i = S i mod 2 i = via its first and last columns, by applying the Gohberg-Semencul formula for the inverse of a Toeplitz matrix (see [GS], [FMKL], <ref> [T] </ref>): e S i = u 0 L (u (i) )L T (Jv (i) ) L (Zv (i) )L T (ZJu (i) ) mod 2 i where u (i) 0 = ( e S i ) 00 = 1 mod , u (i) and v (i) are the first and last
Reference: [B83] <author> D. Bini, </author> <title> On a Class of Matrices Related to Toeplitz Matrices, </title> <type> Tech. Rep. TR 83-5, </type> <institution> Computer Sci. Dept., SUNYA, Albany, </institution> <address> NY, </address> <year> 1983. </year>
Reference-contexts: In appendix C we display some correlations between such operators and the two classical displacement operators of [KKM], [CKL-A]. Some further details can be found in our original technical reports [P90b] (on the algorithms for the computations with Toeplitz and Toeplitz-like matrices) and <ref> [B83] </ref> (on the operators associated with the sums of Toeplitz-like and Hankel-like matrices, on their main properties and on some related results). 2. Improved parallel computations with Toeplitz matrices. <p> Operators of Toeplitz and Hankel type. In this section we will introduce some machinery that we will use in the next section in order to extend the algorithms of section 2 to a more general class of matrices. In particular, we will follow the line of <ref> [B83] </ref> to define this class of matrices in terms of the associated displacement operators. Our study of these classes of matrices and operators extends the theory developed in [KKM], [CKL-A]. <p> To prove part (c), we follow <ref> [B83] </ref> and apply the matrix representation (3.5) of the operator, that is, we rewrite F + (A) = P d i in matrix form, thus obtaining the block tridiagonal system of linear equations: 0 B B B Z I O . . . . . .
Reference: [BC] <author> D. Bini, M. Capovani, </author> <title> Spectral and Computational Properties of Band Symmetric Toeplitz Matrices, </title> <journal> Lin. Alg. and its Applics., </journal> <volume> 52/53, </volume> <pages> 99-126, </pages> <year> 1983. </year>
Reference: [BGP] <author> D. Bini, L. Gemignani, V. Pan, </author> <title> Improved Parallel Computations with Matrices and Polynomials, </title> <booktitle> Proceedings 18th Intern. SYMP&gt; on Automata, Llanguages and Programming, Lecture Notes in Computer Science, </booktitle> <pages> 520-531, </pages> <publisher> Springer, </publisher> <year> 1991. </year>
Reference: [GS] <author> I. C. Gohberg, A. A. Semencul, </author> <title> On the Inversion of Finite Toeplitz Matrices and their Continuous Analogs, </title> <journal> Math. Issl., </journal> <volume> 2, </volume> <booktitle> 201-233 (in Russian), </booktitle> <year> 1972. </year>
Reference-contexts: In this case we may express the matrix polynomial e S i = S i mod 2 i = via its first and last columns, by applying the Gohberg-Semencul formula for the inverse of a Toeplitz matrix (see <ref> [GS] </ref>, [FMKL], [T]): e S i = u 0 L (u (i) )L T (Jv (i) ) L (Zv (i) )L T (ZJu (i) ) mod 2 i where u (i) 0 = ( e S i ) 00 = 1 mod , u (i) and v (i) are the first
Reference: [FMKL] <author> B. Friedlander, M. Morf, T. Kailath, L. Ljung, </author> <title> New Inversion Formulas for Matrices Classified in Terms of their Distances from Toeplitz Matrices, </title> <journal> Lin. Alg. and its Applics., </journal> <volume> 27, </volume> <pages> 31-60, </pages> <year> 1979. </year>
Reference-contexts: In this case we may express the matrix polynomial e S i = S i mod 2 i = via its first and last columns, by applying the Gohberg-Semencul formula for the inverse of a Toeplitz matrix (see [GS], <ref> [FMKL] </ref>, [T]): e S i = u 0 L (u (i) )L T (Jv (i) ) L (Zv (i) )L T (ZJu (i) ) mod 2 i where u (i) 0 = ( e S i ) 00 = 1 mod , u (i) and v (i) are the first and
Reference: [KP] <author> E. Kaltofen, V. Pan, </author> <title> Processor Efficient Parallel Solution of Linear Systems over an Abstract Field, </title> <booktitle> Proc. 3-rd Ann. ACM Symp. on Parallel Computers and Architectures, </booktitle> <pages> 180-191, </pages> <year> 1991. </year>
Reference-contexts: The results can be further extended to such computational problems as fast parallel evaluation of rank T , of null space of T , of the minimum span of a linear recurrence sequence (Berlekamp-Massey problem), of polynomial gcd and lcm, Pade approximation and extended Euclidean scheme for polynomials ([BGP], <ref> [KP] </ref>, [P90b]). Many of our algorithms can be immediately extended to computations with other dense structured matrices, such as Hilbert-like and Vandermonde-like matrices, by applying the techniques of [P90a].
Reference: [KPa] <author> E.Kaltofen, V.Pan, </author> <title> Processor Efficient Parallel Solution of Linear Systems II: Positive Characteristic and Singular Cases, </title> <booktitle> Proc. 33-rd Ann. IEEE Symp. on Foundations of Computer Science, </booktitle> <year> 1992. </year>
Reference-contexts: Alternate techniques of <ref> [KPa] </ref> use randomization to ensure such a transition over any field at the computation cost O log 2 n d (n; p); n 2 log log n = d (n; p) log n) where p is the characteristic of the field of constants, d (n; p) = dlog n= log pe
Reference: [KKM] <author> T. Kailath, S.-Y. Kung, M. Morf, </author> <title> Displacement Rank of Matrices and Linear Equations, </title> <journal> J. Math. Anal. Applics., </journal> <volume> 68, 2, </volume> <pages> 395-407, </pages> <year> 1979. </year>
Reference-contexts: Such an extension requires developing some special techniques of independent interest, which we present in section 3. In particular, we study the properties of some displacement operators associated with matrices of the latter class, thus extending the theory of <ref> [KKM] </ref>, [CKL-A]. In appendix C we display some correlations between such operators and the two classical displacement operators of [KKM], [CKL-A]. <p> In particular, we study the properties of some displacement operators associated with matrices of the latter class, thus extending the theory of <ref> [KKM] </ref>, [CKL-A]. In appendix C we display some correlations between such operators and the two classical displacement operators of [KKM], [CKL-A]. Some further details can be found in our original technical reports [P90b] (on the algorithms for the computations with Toeplitz and Toeplitz-like matrices) and [B83] (on the operators associated with the sums of Toeplitz-like and Hankel-like matrices, on their main properties and on some related results). 2. <p> In particular, we will follow the line of [B83] to define this class of matrices in terms of the associated displacement operators. Our study of these classes of matrices and operators extends the theory developed in <ref> [KKM] </ref>, [CKL-A]. <p> Appendix C. Correlations between F + ; F and the classical displacement operators. The classical displacement operators F + and F of <ref> [KKM] </ref>, [CKL-A], such that F + (A) = A ZAZ T ; are related to operators F + and F of sections 3 and 4 via the following equations, which hold for any n fi n matrix A ([P90b]): F + (A)Z T = F + (A) Ae (0) e (0)T
Reference: [CKLA] <author> J. Chun, T. Kailath, M. Lev-Ari, </author> <title> Fast Parallel Algorithm for QR-factorization of Structured Matrices, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8, 6, </volume> <pages> 899-913, </pages> <year> 1987. </year> <month> 19 </month>
Reference: [KR] <author> R. Karp, V. Ramachandran, </author> <title> A Survey of Parallel Algorithms for Shared Memory Computers, </title> <booktitle> Handbook for Theoretical Computer Science, </booktitle> <publisher> North-Holland, </publisher> <editor> Amsterdam (J. van Leeuwen, ed.), </editor> <month> 869-941, </month> <year> 1990. </year>
Reference-contexts: We deduced our estimates assuming Brent's modified principle <ref> [KR] </ref>, [P90b], according to which the number of processors required for the implementation of a parallel algorithm can be decreased by the factor of s, 1 s p, at the cost of slowing down the algorithm by O (s) times.
Reference: [JJ] <author> J. Ja Ja, </author> <title> An Introduction to Parallel Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Our algorithms satisfy the stated complexity bounds under any model of parallel computing that supports the cost bounds of Table 1.1 (listed for some fundamental problems of parallel computations). We refer the reader to [Q], <ref> [JJ] </ref> and [L] on verification of these simple cost bounds under some realistic models of parallel computing.
Reference: [L] <author> H. T. Leighton, </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees and Hypercubes, </title> <publisher> Morgan Kauffman, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Our algorithms satisfy the stated complexity bounds under any model of parallel computing that supports the cost bounds of Table 1.1 (listed for some fundamental problems of parallel computations). We refer the reader to [Q], [JJ] and <ref> [L] </ref> on verification of these simple cost bounds under some realistic models of parallel computing. <p> A more intricate application of Brent's principle enables us to simplify some parallel computations, improving the straightforward bounds on the complexity of the summation of n numbers from O (log n; n) to O (log n; n= log n) ([Q], <ref> [L] </ref>) and similarly for our algorithm of this paper, from O (log 2 n; n 2 ) to O (log 2 n; n 2 = log n) (see section 2).
Reference: [Pe] <author> M. Pease, </author> <title> An Adaptation of the Fast Fourier Transform for Parallel Procelssing, </title> <journal> J. ACM, </journal> <volume> 15, </volume> <pages> 252-284, </pages> <year> 1968. </year>
Reference-contexts: In our case, the constants hidden in this "O" notation are quite small: in particular, at most 3 log n arithmetic time-steps and 2n processors are needed to support FFT on n points <ref> [Pe] </ref>, and 2dlog ne steps and dn= log ne processors suffice in order to sum n numbers.
Reference: [Q] <author> M. J. Quinn, </author> <title> Designing Efficient Algorithms for Parallel Computers, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Our algorithms satisfy the stated complexity bounds under any model of parallel computing that supports the cost bounds of Table 1.1 (listed for some fundamental problems of parallel computations). We refer the reader to <ref> [Q] </ref>, [JJ] and [L] on verification of these simple cost bounds under some realistic models of parallel computing.
Reference: [P] <author> V. Pan, </author> <title> Concurrent Iterative Algorithm for Toeplitz-like Linear Systems, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <note> to appear. </note>
Reference-contexts: require at least n parallel steps for the computations with nfin Toeplitz matrices (although there are faster parallel algorithms, specially devised for computations with well-conditioned Toeplitz matrices [P89], [P92]; and for rapid refinement of an already rather close initial approximation to the solution to a Toeplitz or Toeplitz-like linear system <ref> [P] </ref>).
Reference: [T] <author> W. F. Trench, </author> <title> A Note on a Toeplitz Inversion Formula, Linear Alg. </title> <journal> and its Applics., </journal> <volume> 129, </volume> <pages> 55-61, </pages> <year> 1990. </year>
Reference-contexts: In this case we may express the matrix polynomial e S i = S i mod 2 i = via its first and last columns, by applying the Gohberg-Semencul formula for the inverse of a Toeplitz matrix (see [GS], [FMKL], <ref> [T] </ref>): e S i = u 0 L (u (i) )L T (Jv (i) ) L (Zv (i) )L T (ZJu (i) ) mod 2 i where u (i) 0 = ( e S i ) 00 = 1 mod , u (i) and v (i) are the first and last
Reference: [P89] <author> V. Pan, </author> <title> Parallel Inversion of Toeplitz and Block Toeplitz Matrices, Operator Theory: </title> <booktitle> Advances and Applics., </booktitle> <volume> 40, </volume> <pages> 359-389, </pages> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference-contexts: Toeplitz and Hankel computations are inherently sequential: they either recursively reduce the dimension of the problem by 1 or, for some similar reasons, require at least n parallel steps for the computations with nfin Toeplitz matrices (although there are faster parallel algorithms, specially devised for computations with well-conditioned Toeplitz matrices <ref> [P89] </ref>, [P92]; and for rapid refinement of an already rather close initial approximation to the solution to a Toeplitz or Toeplitz-like linear system [P]).
Reference: [P90] <author> V. Pan, </author> <title> Parallel Least-Squares Solution of General and Toeplitz-like Linear Systems, </title> <booktitle> Proc. 2-nd Ann. ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> 244-253, </pages> <year> 1990. </year>
Reference-contexts: For the converse computation, however, a simpler algorithm is available, due to [S] (see also <ref> [P90] </ref>).
Reference: [P90a] <author> V. Pan, </author> <title> On Computations with Dense Structured Matrices, </title> <journal> Math. of Computation, </journal> <volume> 55, 191, </volume> <pages> 179-190, </pages> <year> 1990. </year>
Reference-contexts: Many of our algorithms can be immediately extended to computations with other dense structured matrices, such as Hilbert-like and Vandermonde-like matrices, by applying the techniques of <ref> [P90a] </ref>. The algorithms work over (or can be extended to) any field of constants, which enables us to take advantage of using the techniques of residue (modular) arithmetic.
Reference: [P90b] <author> V. Pan, </author> <title> Parametrization of Newton's Iteration for Computations with Structured Matrices and Applications, </title> <type> Tech. Rep. </type> <institution> CUCS-031-90, Columbia Univ., Computer Sci. Dept., </institution> <address> NY, </address> <year> 1990. </year>
Reference-contexts: The results can be further extended to such computational problems as fast parallel evaluation of rank T , of null space of T , of the minimum span of a linear recurrence sequence (Berlekamp-Massey problem), of polynomial gcd and lcm, Pade approximation and extended Euclidean scheme for polynomials ([BGP], [KP], <ref> [P90b] </ref>). Many of our algorithms can be immediately extended to computations with other dense structured matrices, such as Hilbert-like and Vandermonde-like matrices, by applying the techniques of [P90a]. <p> We deduced our estimates assuming Brent's modified principle [KR], <ref> [P90b] </ref>, according to which the number of processors required for the implementation of a parallel algorithm can be decreased by the factor of s, 1 s p, at the cost of slowing down the algorithm by O (s) times. <p> In appendix C we display some correlations between such operators and the two classical displacement operators of [KKM], [CKL-A]. Some further details can be found in our original technical reports <ref> [P90b] </ref> (on the algorithms for the computations with Toeplitz and Toeplitz-like matrices) and [B83] (on the operators associated with the sums of Toeplitz-like and Hankel-like matrices, on their main properties and on some related results). 2. Improved parallel computations with Toeplitz matrices.
Reference: [P92] <author> V. Pan, </author> <title> Parallel Solution of Toeplitzlike Linear Systems, </title> <journal> J. Complexity, </journal> <volume> 8, </volume> <pages> 1-21, </pages> <year> 1992. </year>
Reference-contexts: and Hankel computations are inherently sequential: they either recursively reduce the dimension of the problem by 1 or, for some similar reasons, require at least n parallel steps for the computations with nfin Toeplitz matrices (although there are faster parallel algorithms, specially devised for computations with well-conditioned Toeplitz matrices [P89], <ref> [P92] </ref>; and for rapid refinement of an already rather close initial approximation to the solution to a Toeplitz or Toeplitz-like linear system [P]).
Reference: [PP] <author> V. Pan, F. Preparata, </author> <title> Supereffective Slow-down of Parallel Computations, </title> <booktitle> Proc. 4-th Ann. ACM Symp. on Parallel Algorithms & Architectures, </booktitle> <pages> 402-409, </pages> <year> 1992. </year>
Reference-contexts: The latter bounds also imply the bounds O (s log 2 n; n 2 =(s log n)) for any s, 1 s n 2 = log n, due to Brent's principle. Moreover, the general techniques of supereffective slowdown of parallel computations <ref> [PP] </ref> enable us to implement our algorithms 3 so as to arrive at the bounds O (n 1a log 2 n; n 2a = log n), for any a, 0 &lt; a 1, that is, we may make our algorithms run in O (n 1a log 2 n) time using O

References-found: 47

