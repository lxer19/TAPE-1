URL: http://simon.cs.cornell.edu/Info/Projects/Bernoulli/papers/lcpc4.ps
Refering-URL: 
Root-URL: 
Title: An Executable Representation of Distance and Direction  
Author: Richard Johnson Wei Li Keshav Pingali 
Affiliation: Cornell University  
Abstract: The dependence flow graph is a novel intermediate representation for optimizing and parallelizing compilers that can be viewed as an executable representation of program dependences. The execution model, called dependence-driven execution, is a generalization of the tagged-token dataflow model that permits imperative updates to memory. The dependence flow graph subsumes other representations such as continuation-passing style [12], data dependence graphs [13], and static single assignment form [8]. In this paper, we show how dependence distance and direction information can be represented in this model using dependence operators. From a functional perspective, these operators can be viewed as functions on streams [4].
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction The growing complexity of optimizing and parallelizing compilers has led the compiler com munity to re-examine the design of intermediate program representations. Traditionally, compilers have used the control-flow graph augmented with dependence information such as def-use chains <ref> [1] </ref>, data dependences [13] and control dependences [10]. <p> The integration of control and data dependences enables us to give a parallel, composi 3 No knowledge of streams is needed to read this paper, since we have tried to avoid such `cultural' references. S 1 : A <ref> [1] </ref> := 1 S 3 : A [I+1] := A [I]+1 (a) Source Program for I := 1,N-1 A [1] := 1 1 loop loop -1 sync P := A [Q] A [1] := 1 (b) PDG Representation (c) DFG Representation tional execution semantics to dependence flow graphs. <p> S 1 : A <ref> [1] </ref> := 1 S 3 : A [I+1] := A [I]+1 (a) Source Program for I := 1,N-1 A [1] := 1 1 loop loop -1 sync P := A [Q] A [1] := 1 (b) PDG Representation (c) DFG Representation tional execution semantics to dependence flow graphs. <p> S 1 : A <ref> [1] </ref> := 1 S 3 : A [I+1] := A [I]+1 (a) Source Program for I := 1,N-1 A [1] := 1 1 loop loop -1 sync P := A [Q] A [1] := 1 (b) PDG Representation (c) DFG Representation tional execution semantics to dependence flow graphs. To introduce this parallel semantics, we first consider the sequential execution of statements in a control-flow graph modeled in a dataflow style as follows. <p> In Figure 6c, we represent more detailed information about the flow dependence; in particular, the producer-consumer relationship between iteration i of the I loop and iteration i+1 of the J loop is indicated explicitly. Note that S 2 executing in the first iteration (J = 2) uses A <ref> [1] </ref> which is not written by any instance of S 1 , and thus depends on some prior write to A. Thus, some prior barrier synchronization provides token ffi + to the loop 1 node, thereby initializing S 2 in iteration 1.
Reference: [2] <author> Arvind, K. P. Gostelow, and W. Plouffe. </author> <title> An asynchronous programming language and computing machine. </title> <type> Technical Report 114a, </type> <institution> Univ. of Calif., Irvine, </institution> <month> Dec. </month> <year> 1978. </year>
Reference-contexts: The loop 1 operator increments the token's tag by 1, producing the token ffi:i + 1 which flows to S 3 in iteration i+1. (The reader familiar with the tagged-token dataflow model will recognize that the loop 1 operator implements the function of the D operator in that model <ref> [2] </ref>.) Since we do not know which element of array A is read by statement S 4 , we conservatively say S 4 depends on all instances of S 3 . <p> Although we have described the behavior of dependence operators using the ^ -pointer, readers familiar with the concept of streams in non-strict functional languages such as Id <ref> [2] </ref> should note the correspondence with streams and stream operators. The tokens flowing down a dependence arc can viewed as elements of a non-strict data structure called a stream whose elements can be used before the entire data structure is produced.
Reference: [3] <author> R. A. Ballance, A. B. Maccabe, and K. J. Ottenstein. </author> <title> The Program Dependence Web: A representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> In Proc. of the 1990 SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The dependence flow graph is compact | it is asymptotically smaller than the data dependence graph. Interestingly, dependence flow graphs incorporate all the advantages of recently proposed representations such as the program dependence graph and web <ref> [10, 3] </ref>, program representation graph [5], static single assignment form [8] and continuation-passing style [12]. In our earlier presentation, we considered only scalars, and arrays were handled by treating them as scalars (that is, an assignment to an array element was treated as an assignment to the entire array).
Reference: [4] <author> W. H. Burge. </author> <title> Recursive Programming Techniques. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1975. </year>
Reference-contexts: For readers familiar with the concept of streams in functional languages, our results can be interpreted as a generalization of the ideas of Landin who first used streams to model functionally the execution of loops in imperative languages <ref> [4] </ref> 3 . The rest of the paper is organized as follows. In Section 2, we introduce dependence flow graphs and their execution model. In Section 3, we include distance and direction vector information in our execution model.
Reference: [5] <author> R. Cartwright and M. Felleisen. </author> <title> The semantics of program dependence. </title> <booktitle> In Proc. of the 1989 SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 13-27, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: A first step towards a semantic account of dependences has been taken by Selke [16] and Cartwright and Felleison <ref> [5] </ref>, who give a -calculus based execution model for the special case of program dependence graphs arising from a structured programming language without aliasing or arrays. Their approach is limited by the simple execution model that underlies it. <p> The dependence flow graph is compact | it is asymptotically smaller than the data dependence graph. Interestingly, dependence flow graphs incorporate all the advantages of recently proposed representations such as the program dependence graph and web [10, 3], program representation graph <ref> [5] </ref>, static single assignment form [8] and continuation-passing style [12]. In our earlier presentation, we considered only scalars, and arrays were handled by treating them as scalars (that is, an assignment to an array element was treated as an assignment to the entire array).
Reference: [6] <author> J.-D. Choi. </author> <type> personal communication, </type> <year> 1991. </year>
Reference-contexts: These tests and redefinitions can significantly increase code size <ref> [6] </ref>.
Reference: [7] <author> P. Cousout and R. Cousout. </author> <title> Abstract Interpretation: A unified lattice model for static analysis of programs by construction of approximations of fixpoints. </title> <booktitle> Proc. of the 4th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <month> Jan. </month> <year> 1977. </year>
Reference-contexts: Dependence flow graphs have the following advantages: 1. Since the representation is executable, we can use abstract interpretation to design optimization algorithms, facilitating systematic algorithm development and proof of correctness <ref> [7] </ref>. From a software engineering perspective, this is advantageous because algorithms based on abstract interpretation have the same structure, which permits code sharing. 2. Algorithms based on abstract interpretation of dependence flow graphs are efficient.
Reference: [8] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Proc. of the 16th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 25-35, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: In a previous paper [14], we showed that all these problems can be avoided by using an executable representation of dependences called the dependence flow graph. The 2 A similar problem is encountered in using the static single assignment form <ref> [8] </ref>. dependence flow graph can be viewed either as a data structure incorporating depen-dence information or as a program that can be executed. Our execution model is called dependence-driven execution and is a generalization of the tagged-token dataflow model of computation; the generalization permits imperative updates to memory locations. <p> The dependence flow graph is compact | it is asymptotically smaller than the data dependence graph. Interestingly, dependence flow graphs incorporate all the advantages of recently proposed representations such as the program dependence graph and web [10, 3], program representation graph [5], static single assignment form <ref> [8] </ref> and continuation-passing style [12]. In our earlier presentation, we considered only scalars, and arrays were handled by treating them as scalars (that is, an assignment to an array element was treated as an assignment to the entire array).
Reference: [9] <author> E. W. Dijkstra. </author> <title> A Discipline of Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1976. </year>
Reference-contexts: Second, like all semantic descriptions, it has prescriptive value in identifying and fixing weaknesses since constructs that are difficult to model semantically are usually difficult to use in practice | for example, consider the unstructured goto in high-level programming languages <ref> [9] </ref>. A first step towards a semantic account of dependences has been taken by Selke [16] and Cartwright and Felleison [5], who give a -calculus based execution model for the special case of program dependence graphs arising from a structured programming language without aliasing or arrays.
Reference: [10] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependency graph and its uses in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: 1 Introduction The growing complexity of optimizing and parallelizing compilers has led the compiler com munity to re-examine the design of intermediate program representations. Traditionally, compilers have used the control-flow graph augmented with dependence information such as def-use chains [1], data dependences [13] and control dependences <ref> [10] </ref>. <p> The dependence flow graph is compact | it is asymptotically smaller than the data dependence graph. Interestingly, dependence flow graphs incorporate all the advantages of recently proposed representations such as the program dependence graph and web <ref> [10, 3] </ref>, program representation graph [5], static single assignment form [8] and continuation-passing style [12]. In our earlier presentation, we considered only scalars, and arrays were handled by treating them as scalars (that is, an assignment to an array element was treated as an assignment to the entire array).
Reference: [11] <author> S. Horwitz. </author> <title> Identifying the semantic and textual differences between two versions of a program. </title> <booktitle> In Proc. of the 1990 SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 234-245, </pages> <year> 1990. </year>
Reference-contexts: Nevertheless, we note that difficulties in modeling conditional assignments in program dependence graphs has led to the proposal of the program representation graph which has turned out to be better suited than the program dependence graph for problems such as integration of program versions <ref> [11] </ref>; this illustrates the prescriptive power of giving semantics to dependence information. In a previous paper [14], we showed that all these problems can be avoided by using an executable representation of dependences called the dependence flow graph.
Reference: [12] <author> G. L. S. Jr. and G. J. Sussman. </author> <title> Scheme: An interpreter for extended lambda calculus. </title> <type> Technical Report Memo 349, </type> <institution> M.I.T. Artificial Intellegence Laboratory, </institution> <year> 1975. </year>
Reference-contexts: Interestingly, dependence flow graphs incorporate all the advantages of recently proposed representations such as the program dependence graph and web [10, 3], program representation graph [5], static single assignment form [8] and continuation-passing style <ref> [12] </ref>. In our earlier presentation, we considered only scalars, and arrays were handled by treating them as scalars (that is, an assignment to an array element was treated as an assignment to the entire array).
Reference: [13] <author> D. J. Kuck. </author> <title> The Structure of Computers and Computations, volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: 1 Introduction The growing complexity of optimizing and parallelizing compilers has led the compiler com munity to re-examine the design of intermediate program representations. Traditionally, compilers have used the control-flow graph augmented with dependence information such as def-use chains [1], data dependences <ref> [13] </ref> and control dependences [10].
Reference: [14] <author> K. Pingali, M. Beck, R. Johnson, M. Moudgill, and P. Stodghill. </author> <title> Dependence Flow Graphs: An algebraic approach to program dependencies. </title> <booktitle> In Proc. of the 18th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 67-78, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: In a previous paper <ref> [14] </ref>, we showed that all these problems can be avoided by using an executable representation of dependences called the dependence flow graph. <p> We have used this idea to design a simple global constant propagation algorithm that is as powerful as the complex one of Wegman and Zadeck that uses both the control-flow graph and def-use chains <ref> [14] </ref>. 3. The dependence flow graph is compact | it is asymptotically smaller than the data dependence graph. <p> The reader who is interested in a more detailed account is referred to our earlier paper <ref> [14] </ref>. We assume that the reader is familiar with the program dependence graph and with the tagged-token dataflow model. Hereafter, we abbreviate dependence flow graph as DFG and program dependence graph as PDG. DFG representations. The PDG consists of control and data dependences. <p> Similarly, the dependence from S 3 inside the loop to S 4 outside the loop passes through a sync operator. Thus, traditional data dependence edges become dependence paths in our representation; perhaps surprisingly, this results in an asymptotically smaller representation, since these paths may share vertices <ref> [14] </ref>. The integration of control and data dependences enables us to give a parallel, composi 3 No knowledge of streams is needed to read this paper, since we have tried to avoid such `cultural' references.
Reference: [15] <author> B. Rosen. </author> <title> Linear time is sometimes quadratic. </title> <booktitle> In Proc. of the 8th ACM Symposium on Principles of Programming Languages, </booktitle> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: The separation of execution semantics from dependence information results in a number of problems. When a program is transformed, dependence information may need to be modified, but it is usually difficult to do this incrementally; for example, it is hard to update def-use chains after eliminating unreachable code <ref> [15] </ref>. Therefore, the full benefit of program optimization must be obtained in one of two ways. One way is to perform repeated passes of program analysis and transformation.
Reference: [16] <author> R. P. Selke. </author> <title> A rewriting semantics for program dependence graphs. </title> <booktitle> In Proc. of the 16th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 12-24, </pages> <year> 1989. </year>
Reference-contexts: A first step towards a semantic account of dependences has been taken by Selke <ref> [16] </ref> and Cartwright and Felleison [5], who give a -calculus based execution model for the special case of program dependence graphs arising from a structured programming language without aliasing or arrays. Their approach is limited by the simple execution model that underlies it.
Reference: [17] <author> M. N. Wegman and F. K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <booktitle> In Proc. of the 11th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 291-299, </pages> <year> 1984. </year>
Reference-contexts: Alternatively, the problem can be circumvented through the use of complex algorithms such as the global constant propagation algorithm of Wegman and Zadeck, which combines constant propagation with unreachable code elimination. This algorithm requires simultaneous traversals of both the control-flow graph and def-use chains <ref> [17] </ref>. Neither approach is satisfactory; the first is expensive and the second does not solve the problem of out-of-date dependence information. The separation of execution semantics and dependence information has also inhibited the development of an adequate semantic account of dependences. Such a semantic account is useful for two reasons.
Reference: [18] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman Publishing, </publisher> <address> Lon-don, </address> <year> 1989. </year>
Reference-contexts: However, over the last fifteen years, a number of subscript analysis tests, such as the GCD test and Banerjee test <ref> [18] </ref>, have been developed, which provide finer-grain dependence information. These tests not only yield information about the existence of dependences, but they also give semantic information such as dependence directions and distances needed to parallelize programs more effectively. <p> As a bonus, we show that problems faced by conventional dependence representations in dealing with reduction operations, imperfectly nested loops and non-nested loops <ref> [18] </ref> pose no problem for us. For readers familiar with the concept of streams in functional languages, our results can be interpreted as a generalization of the ideas of Landin who first used streams to model functionally the execution of loops in imperative languages [4] 3 . <p> For example, statement S 1 in Figure 5a is a reduction statement; there are output and flow dependences from S 1 to itself, each having distance 1. Recognizing reductions is essential to generating good vector and concurrent code <ref> [18] </ref>. Many vector machines support vectorization of reductions; without special recognition, reductions appear unvectorizable. Recognizing commutative reduction operators allows more flexible scheduling of iterations on multiprocessors and transformations such as loop interchange which appear illegal otherwise. Thus, reductions may often be handled specially, allowing their loop-carried dependences to be ignored.
References-found: 18

