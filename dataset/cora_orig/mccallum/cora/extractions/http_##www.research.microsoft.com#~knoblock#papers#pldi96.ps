URL: http://www.research.microsoft.com/~knoblock/papers/pldi96.ps
Refering-URL: http://www.research.microsoft.com/~knoblock/
Root-URL: http://www.research.microsoft.com
Email: ftoddk, erikrufg@microsoft.com  
Title: Data Specialization  
Author: Todd B. Knoblock and Erik Ruf 
Note: Paper to appear in ACM SIGPLAN '96 Conference on Programming Language Design and Implementation  
Date: February 5, 1996  May 1996.  
Address: One Microsoft Way, Redmond, WA 98052 USA  One Microsoft Way Redmond, WA 98052  (PLDI'96), Philadelphia, PA,  
Affiliation: Microsoft Research  Microsoft Research Advanced Technology Division Microsoft Corporation  
Abstract: Technical Report MSR-TR-96-04 
Abstract-found: 1
Intro-found: 1
Reference: [And94] <author> Lars Ole Andersen. </author> <title> Program Analysis and Specialization for the C Programming Language. </title> <type> PhD thesis, </type> <institution> DIKU, University of Copenhagen, Denmark, </institution> <year> 1994. </year> <note> DIKU Research Report 94/19. </note>
Reference-contexts: We initially assume that all terms other than the varying inputs are independent, then iteratively propagate the dependence constraints listed above. Other techniques for binding time analysis of imperative programs such as program slicing [DRH95] and type inference <ref> [And94] </ref> could also be applied here. In the example program of Figure 1, the references to variables z1 and z2 are marked as dependent, as are the multiplication z1*z2 and the surrounding addition and division. <p> Keppel et al. [KEH93] com-pared assembly-level and compiler intermediate representation (IR) level template compilers for a variety of workloads, and computed conservative amortization intervals of 10-1000 uses for assembly templates and 1000-infinite uses for IR templates. General partial evaluators <ref> [JGS93, Ruf93, And94, Osg93] </ref> can yield highly specialized code, but existing systems are impractical for runtime use due both to their slowness and to the cost of dynamically compiling the high-level code they generate. <p> For example, our caching analysis can label a term as dynamic without forcing its consumers to be dynamic, while a BTA-based approach (in which dependent dynamic) would unnecessarily force all of the term's consumers into the reader. We believe that current partial evaluators for imperative languages <ref> [Osg93, And94] </ref> have not yet experienced this problem because they do not perform flow-sensitive binding time analysis.
Reference: [APC + 96] <author> Joel Auslander, Matthai Phillipose, Craig Chambers, Susan J. Eggers, and Brian N. Bershad. </author> <title> Fast, effective dynamic compilation. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Programming Language Design and Implementation. ACM, </booktitle> <month> May </month> <year> 1996. </year> <note> This volume. </note>
Reference-contexts: It is the programmer's responsibility to enforce the invariant that fixed inputs will not change across invocations of the late phase; the early phase must be reinvoked whenever the fixed inputs are altered. A number of staging techniques <ref> [MP89, KEH93, LL94, EP94, CN96, APC + 96] </ref> are based on dynamic compilation. These approaches range from runtime instantiation of manually generated machine-code templates to runtime execution of full-blown optimizers and code generators, with a variety of template- and compilation-based mechanisms lying between these two extremes. <p> In unstructured code, this problem becomes significantly more difficult; Auslander et al. <ref> [APC + 96] </ref> handle this case by performing a reachability analysis concurrently with dependence analysis. Our implementation of dependence analysis is a straightforward, worst-case quadratic-time solution based on abstract interpretation. <p> Massalin and Pu [MP89] used hand-generated assembly templates; Consel and Noel [CN96] automatically generate portable templates at the source code level and extract them from the resulting compiled code using machine-specific techniques. Auslander et al. <ref> [APC + 96] </ref> present a template-based compiler for an annotated version of C; this system uses analyses similar to ours to construct an early phase that fills in a "run-time constants table" similar to our cache.
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ull-man. </author> <booktitle> Compilers; principles, techniques, and tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Doing so improves performance when the time savings from repeated execution of the optimized computation exceed the cost of performing the optimization dynamically. In simple cases of staging, such as loop invariant code motion <ref> [ASU86] </ref>, a compiler can automatically recognize the frequency with which the inputs (variable definitions) to a computation (loop body) are altered, determine which sub-computations depend only on invariant inputs, and hoist them to an appropriate location. In more complex situations, some degree of programmer assistance is required. <p> Our implementation optionally reassociates expressions to maximize the size of independent terms, increasing the number of computations that can be performed in the loader. 2 This operation is similar to "binding time improvement" techniques used in o*ine partial evaluation, and to the rank-ordered reassoci-ation used in code motion optimizations <ref> [ASU86, BC94] </ref>. 4.3 Cache size limiting Each program term annotated as cached represents an intermediate result value that will be computed by the loader, placed in the cache, and later used by the reader instead of executing the term.
Reference: [BC94] <author> Preston Briggs and Keith D. Cooper. </author> <title> Effective partial redundancy elimination. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 159-170, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Our implementation optionally reassociates expressions to maximize the size of independent terms, increasing the number of computations that can be performed in the loader. 2 This operation is similar to "binding time improvement" techniques used in o*ine partial evaluation, and to the rank-ordered reassoci-ation used in code motion optimizations <ref> [ASU86, BC94] </ref>. 4.3 Cache size limiting Each program term annotated as cached represents an intermediate result value that will be computed by the loader, placed in the cache, and later used by the reader instead of executing the term.
Reference: [Bul84] <author> M.A. Bulyonkov. </author> <title> Polyvariant mixed computation for analyzer programs. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 473-484, </pages> <year> 1984. </year>
Reference-contexts: Other dynamic compilation systems [DS84, Cha92] concentrate more on optimizing language features such as dynamic dispatch than on staging user-level computations. An alternate formulation of partial evaluation known as mixed computation <ref> [Ers77, Bul84] </ref> has a signature somewhat similar to that of data specialization, in that the transformation emits both specialized code and specialized data.
Reference: [CD90] <author> Charles Consel and Olivier Danvy. </author> <title> From interpreting to compiling binding times. </title> <editor> In N. Jones, editor, </editor> <booktitle> Proceedings of the 3rd European Symposium on Programming, </booktitle> <pages> pages 88-105. </pages> <publisher> Springer-Verlag, LNCS 432, </publisher> <year> 1990. </year>
Reference-contexts: Our splitting pass, which traverses the annotated program fragment and emits the loader and reader code, can be viewed as two nonstandard semantic interpretations of an action tree <ref> [CD90, CN96] </ref>. Consel's "evaluate" action denotes maximal-sized independent subtrees and thus corresponds (in the absence of speculation avoidance and cache size limiting) to our cached annotation.
Reference: [CFR + 91] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: If slot1 is filled at all, it is filled with the same value as is slot2. To avoid this problem, we preprocess the program to produce unique definitions at the join points of the control flow graph. The result is analogous to static single assignment (SSA) form <ref> [CFR + 91] </ref>. In particular, we share the property with SSA that every variable reference except for those in the introduced assignments (our analog of the "phi" nodes of SSA) have exactly one reaching definition.
Reference: [Cha92] <author> Craig Chambers. </author> <title> The Design and Implementation of the Self Compiler, an Optimizing Compiler for Object-Oriented Programming Languages. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1992. </year> <note> Published as technical report STAN-CS-92-1420. </note>
Reference-contexts: General partial evaluators [JGS93, Ruf93, And94, Osg93] can yield highly specialized code, but existing systems are impractical for runtime use due both to their slowness and to the cost of dynamically compiling the high-level code they generate. Other dynamic compilation systems <ref> [DS84, Cha92] </ref> concentrate more on optimizing language features such as dynamic dispatch than on staging user-level computations. An alternate formulation of partial evaluation known as mixed computation [Ers77, Bul84] has a signature somewhat similar to that of data specialization, in that the transformation emits both specialized code and specialized data.
Reference: [CN96] <author> Charles Consel and Francois Noel. </author> <title> A general approach to run-time specialization and its application to C. </title> <booktitle> In Proceedings 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 145-156. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1996. </year>
Reference-contexts: It is the programmer's responsibility to enforce the invariant that fixed inputs will not change across invocations of the late phase; the early phase must be reinvoked whenever the fixed inputs are altered. A number of staging techniques <ref> [MP89, KEH93, LL94, EP94, CN96, APC + 96] </ref> are based on dynamic compilation. These approaches range from runtime instantiation of manually generated machine-code templates to runtime execution of full-blown optimizers and code generators, with a variety of template- and compilation-based mechanisms lying between these two extremes. <p> We also relate several of our implementation techniques to their counterparts in partial evaluation. 6.1 Runtime Code Generation Several approaches to dynamic optimization rely on the fast instantiation of precomputed object code templates. Massalin and Pu [MP89] used hand-generated assembly templates; Consel and Noel <ref> [CN96] </ref> automatically generate portable templates at the source code level and extract them from the resulting compiled code using machine-specific techniques. <p> Our splitting pass, which traverses the annotated program fragment and emits the loader and reader code, can be viewed as two nonstandard semantic interpretations of an action tree <ref> [CD90, CN96] </ref>. Consel's "evaluate" action denotes maximal-sized independent subtrees and thus corresponds (in the absence of speculation avoidance and cache size limiting) to our cached annotation.
Reference: [DNBDV91] <author> Anne De Niel, Eddy Bevers, and Karel De Vlam-inck. </author> <title> Program bifurcation for a polymorphically typed functional language. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, </title> <journal> Connecticut (Sigplan Notices, </journal> <volume> vol. 26, no. 9, </volume> <month> September </month> <year> 1991), </year> <pages> pages 142-153. </pages> <address> New York: </address> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: We believe that current partial evaluators for imperative languages [Osg93, And94] have not yet experienced this problem because they do not perform flow-sensitive binding time analysis. Program bifurcation <ref> [Mog89, DNBDV91] </ref> factors each of a program's functions into two new functions: (1) a function which takes as input only independent values, and produces the independent portion of the result, and (2) a function which takes both independent and dependent inputs and produces only the dependent portion of the result.
Reference: [DRH95] <author> Manuvir Das, Thomas Reps, and Pascal Van Hen-tenryck. </author> <title> Semantic foundations of binding time analysis for imperative programs. In Partial Evaluation and Semantics-Based Program Manipulation, </title> <address> La Jolla, California, June 1995. New York: </address> <publisher> ACM, </publisher> <year> 1995. </year>
Reference-contexts: Our implementation of dependence analysis is a straightforward, worst-case quadratic-time solution based on abstract interpretation. We initially assume that all terms other than the varying inputs are independent, then iteratively propagate the dependence constraints listed above. Other techniques for binding time analysis of imperative programs such as program slicing <ref> [DRH95] </ref> and type inference [And94] could also be applied here. In the example program of Figure 1, the references to variables z1 and z2 are marked as dependent, as are the multiplication z1*z2 and the surrounding addition and division.
Reference: [DS84] <author> L. Peter Deutsch and A. M. Schiffman. </author> <title> Efficient implementation of the Smalltalk-80 system. </title> <booktitle> In Proceedings of the Eleventh Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 297-302. </pages> <publisher> ACM, </publisher> <year> 1984. </year>
Reference-contexts: General partial evaluators [JGS93, Ruf93, And94, Osg93] can yield highly specialized code, but existing systems are impractical for runtime use due both to their slowness and to the cost of dynamically compiling the high-level code they generate. Other dynamic compilation systems <ref> [DS84, Cha92] </ref> concentrate more on optimizing language features such as dynamic dispatch than on staging user-level computations. An alternate formulation of partial evaluation known as mixed computation [Ers77, Bul84] has a signature somewhat similar to that of data specialization, in that the transformation emits both specialized code and specialized data.
Reference: [EHK96] <author> Dawson Engler, Wilson C. Hsieh, and M. Frans Kaashoek. </author> <title> `C, a language for high-level, efficient, and machine-independent dynamic code generation. </title> <booktitle> In Proceedings 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 131-144. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1996. </year> <month> 10 </month>
Reference-contexts: Non-template systems optimize and generate code from an intermediate form at runtime. Tools for this purpose include DCG [EP94] and `C <ref> [EHK96] </ref>, which have achieved speedups as high as 50x, but require tens to hundreds of dynamic instructions to emit a single optimized instruction.
Reference: [EP94] <author> Dawson R. Engler and Todd A Proebsting. </author> <title> DCG: An efficient, retargetable dynamic code generation system. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 263-272, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: It is the programmer's responsibility to enforce the invariant that fixed inputs will not change across invocations of the late phase; the early phase must be reinvoked whenever the fixed inputs are altered. A number of staging techniques <ref> [MP89, KEH93, LL94, EP94, CN96, APC + 96] </ref> are based on dynamic compilation. These approaches range from runtime instantiation of manually generated machine-code templates to runtime execution of full-blown optimizers and code generators, with a variety of template- and compilation-based mechanisms lying between these two extremes. <p> Our systems differ in that we construct a single reader that references the cache, while Auslander et al. use the cached data to instantiate immediate values in dynamically generated code. Non-template systems optimize and generate code from an intermediate form at runtime. Tools for this purpose include DCG <ref> [EP94] </ref> and `C [EHK96], which have achieved speedups as high as 50x, but require tens to hundreds of dynamic instructions to emit a single optimized instruction.
Reference: [Ers77] <author> Andrei P. Ershov. </author> <title> On the partial computation principle. </title> <journal> Information Processing Letters, </journal> <volume> 6(2) </volume> <pages> 38-41, </pages> <month> April </month> <year> 1977. </year>
Reference-contexts: Other dynamic compilation systems [DS84, Cha92] concentrate more on optimizing language features such as dynamic dispatch than on staging user-level computations. An alternate formulation of partial evaluation known as mixed computation <ref> [Ers77, Bul84] </ref> has a signature somewhat similar to that of data specialization, in that the transformation emits both specialized code and specialized data.
Reference: [GKR95] <author> Brian Guenter, Todd B. Knoblock, and Erik Ruf. Specializing shaders. </author> <booktitle> In ACM SIGGRAPH'95 (Computer Graphics Proceedings, Annual Conference Series), </booktitle> <pages> pages 343-349, </pages> <year> 1995. </year>
Reference-contexts: All measurements were conducted using the Microsoft Visual C++ compiler version 4.0 on an Intel Pentium/100 CPU with 64 megabytes of physical memory. Our benchmarks are shading procedures, or shaders, belonging to the interactive graphics rendering system described in <ref> [GKR95] </ref>. A shader computes the color value for 6 procedures. Each shader is specialized on multiple input partitions (one per control parameter); these are displayed in the y direction above each shader number. <p> We construct, compile, and link this code statically at the time a shader is installed, an operation that takes only a few seconds per input partition. In the sections that follow, we present results for ten shading procedures (some derived from examples in <ref> [Ups89, Smi90, GKR95] </ref>, the remainder written by the authors) representing a variety of styles and complexity levels. These range in size from 50 to 150 lines of C code, and invoke a small mathematical library that supports vector and matrix operations as well as noise functions.
Reference: [Hoo92] <author> Roger Hoover. Alphonse: </author> <title> incremental computation as a programming abstraction. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 261-272, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Systems that cope with arbitrary input changes by dynamically checking dependences <ref> [PT89, Hoo92] </ref> avoid more computations than data specialization does, but they lose the efficiency we gain from "compiling away" the dependences in advance. Liu and Teitelbaum [LT95a, LT95b] present algorithms for statically deriving an incremental version of a pure functional program under some input change.
Reference: [JGS93] <author> Neil D. Jones, Carsten K. Gomard, and Peter Ses-toft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: The caching analysis ensures that the requisite context will be available by caching the value or delaying the effects of such referenced terms. This concern is similar to the congruence criterion in o*ine partial evaluation <ref> [JGS93] </ref>. 3 Any consistent cache labeling must satisfy the following system of constraints, where the functions Dependent , Static, Cached , and Dynamic are predicates on terms. Other predicates will be defined in the text accompanying each rule. 1. Dependent (t) ! Dynamic (t) 2. <p> Keppel et al. [KEH93] com-pared assembly-level and compiler intermediate representation (IR) level template compilers for a variety of workloads, and computed conservative amortization intervals of 10-1000 uses for assembly templates and 1000-infinite uses for IR templates. General partial evaluators <ref> [JGS93, Ruf93, And94, Osg93] </ref> can yield highly specialized code, but existing systems are impractical for runtime use due both to their slowness and to the cost of dynamically compiling the high-level code they generate. <p> Our dependence annotation is similar to the binding time attribute computed by o*ine partial evaluators <ref> [JGS93] </ref>, in that both involve transitive data dependence on distinguished inputs. The approaches differ in that we separate semantic (dependence) information from policy (caching) information, while binding time analyzers typically mix both in the binding time attribute. We have found that the latter approach can introduce false dependences.
Reference: [JS86] <author> Ulrik Jtrring and William L. Scherlis. </author> <title> Compilers and staging transformations. </title> <booktitle> In Thirteenth ACM Symposium on Principles of Programming Languages, </booktitle> <address> St. Petersburg, Florida, </address> <pages> pages 86-96. </pages> <address> New York: </address> <publisher> ACM, </publisher> <year> 1986. </year>
Reference-contexts: cannot always be achieved; for example, in the specialization in which ringscale is varying, reducing the cache size limit from 16 bytes to 12 bytes leads to a 95% reduction in speedup independent of which values are cached. 6 Related Work Data specialization can be viewed as a staging transformation <ref> [JS86] </ref> that moves computations to contexts where they will be executed less often. In this section, we describe two other staging strategies: runtime code generation and incremental program execution.
Reference: [KEH93] <author> David Keppel, Susan J. Eggers, and Robert R. Henry. </author> <title> Evaluating runtime-compiled value-specific optimizations. </title> <type> Technical Report UWCSE 93-11-02, </type> <institution> University of Washington Department of Computer Science and Engineering, </institution> <year> 1993. </year>
Reference-contexts: It is the programmer's responsibility to enforce the invariant that fixed inputs will not change across invocations of the late phase; the early phase must be reinvoked whenever the fixed inputs are altered. A number of staging techniques <ref> [MP89, KEH93, LL94, EP94, CN96, APC + 96] </ref> are based on dynamic compilation. These approaches range from runtime instantiation of manually generated machine-code templates to runtime execution of full-blown optimizers and code generators, with a variety of template- and compilation-based mechanisms lying between these two extremes. <p> The error appears artificially large for parameters lightx, lighty, and lightz in Figure 10 because their speedup range is smaller than those of other parameters. 8 that directly emits object code. Keppel et al. <ref> [KEH93] </ref> com-pared assembly-level and compiler intermediate representation (IR) level template compilers for a variety of workloads, and computed conservative amortization intervals of 10-1000 uses for assembly templates and 1000-infinite uses for IR templates.
Reference: [LL94] <author> Mark Leone and Peter Lee. </author> <title> Lightweight run-time code generation. In Partial Evaluation and Semantics-Based Program Manipulation, </title> <address> Orlando, Florida, </address> <note> June 1994 (Technical Report 94/9, </note> <institution> Department of Computer Science, University of Mel-bourne), </institution> <address> pages 97-106, </address> <year> 1994. </year>
Reference-contexts: It is the programmer's responsibility to enforce the invariant that fixed inputs will not change across invocations of the late phase; the early phase must be reinvoked whenever the fixed inputs are altered. A number of staging techniques <ref> [MP89, KEH93, LL94, EP94, CN96, APC + 96] </ref> are based on dynamic compilation. These approaches range from runtime instantiation of manually generated machine-code templates to runtime execution of full-blown optimizers and code generators, with a variety of template- and compilation-based mechanisms lying between these two extremes. <p> Non-template systems optimize and generate code from an intermediate form at runtime. Tools for this purpose include DCG [EP94] and `C [EHK96], which have achieved speedups as high as 50x, but require tens to hundreds of dynamic instructions to emit a single optimized instruction. Leone and Lee <ref> [LL94] </ref> used partial evaluation to "compile out" the intermediate form, generating a custom optimizer 4 The few small increases in speedup as cache size is decreased have been verified to be due to timing imprecision and processor cache effects rather than poor choices made by the heuristic, as the generated code
Reference: [LT95a] <author> Yanhong A. Liu and Tim Teitelbaum. </author> <title> Caching intermediate results for program improvement. In Partial Evaluation and Semantics-Based Program Manipulation, </title> <address> La Jolla, California, </address> <month> June </month> <year> 1995, </year> <pages> pages 190-201. </pages> <publisher> ACM, </publisher> <year> 1995. </year>
Reference-contexts: Systems that cope with arbitrary input changes by dynamically checking dependences [PT89, Hoo92] avoid more computations than data specialization does, but they lose the efficiency we gain from "compiling away" the dependences in advance. Liu and Teitelbaum <ref> [LT95a, LT95b] </ref> present algorithms for statically deriving an incremental version of a pure functional program under some input change.
Reference: [LT95b] <author> Yanhong A. Liu and Tim Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 24 </volume> <pages> 1-39, </pages> <year> 1995. </year>
Reference-contexts: Systems that cope with arbitrary input changes by dynamically checking dependences [PT89, Hoo92] avoid more computations than data specialization does, but they lose the efficiency we gain from "compiling away" the dependences in advance. Liu and Teitelbaum <ref> [LT95a, LT95b] </ref> present algorithms for statically deriving an incremental version of a pure functional program under some input change.
Reference: [Mog89] <author> Torben Mogensen. </author> <title> Separating binding times in language specifications. </title> <booktitle> In Fourth International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <address> London, England, </address> <month> Septem-ber </month> <year> 1989, </year> <pages> pages 14-25. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: We believe that current partial evaluators for imperative languages [Osg93, And94] have not yet experienced this problem because they do not perform flow-sensitive binding time analysis. Program bifurcation <ref> [Mog89, DNBDV91] </ref> factors each of a program's functions into two new functions: (1) a function which takes as input only independent values, and produces the independent portion of the result, and (2) a function which takes both independent and dependent inputs and produces only the dependent portion of the result.
Reference: [MP89] <author> Henry Massalin and Calton Pu. </author> <title> Threads and input/output in the Synthesis kernel. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 191-201, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: It is the programmer's responsibility to enforce the invariant that fixed inputs will not change across invocations of the late phase; the early phase must be reinvoked whenever the fixed inputs are altered. A number of staging techniques <ref> [MP89, KEH93, LL94, EP94, CN96, APC + 96] </ref> are based on dynamic compilation. These approaches range from runtime instantiation of manually generated machine-code templates to runtime execution of full-blown optimizers and code generators, with a variety of template- and compilation-based mechanisms lying between these two extremes. <p> We also relate several of our implementation techniques to their counterparts in partial evaluation. 6.1 Runtime Code Generation Several approaches to dynamic optimization rely on the fast instantiation of precomputed object code templates. Massalin and Pu <ref> [MP89] </ref> used hand-generated assembly templates; Consel and Noel [CN96] automatically generate portable templates at the source code level and extract them from the resulting compiled code using machine-specific techniques.
Reference: [Osg93] <author> Nathaniel David Osgood. </author> <title> PARTICLE: an automatic program specialization system for imperative and low-level languages. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <month> Septem-ber </month> <year> 1993. </year>
Reference-contexts: Keppel et al. [KEH93] com-pared assembly-level and compiler intermediate representation (IR) level template compilers for a variety of workloads, and computed conservative amortization intervals of 10-1000 uses for assembly templates and 1000-infinite uses for IR templates. General partial evaluators <ref> [JGS93, Ruf93, And94, Osg93] </ref> can yield highly specialized code, but existing systems are impractical for runtime use due both to their slowness and to the cost of dynamically compiling the high-level code they generate. <p> For example, our caching analysis can label a term as dynamic without forcing its consumers to be dynamic, while a BTA-based approach (in which dependent dynamic) would unnecessarily force all of the term's consumers into the reader. We believe that current partial evaluators for imperative languages <ref> [Osg93, And94] </ref> have not yet experienced this problem because they do not perform flow-sensitive binding time analysis.
Reference: [PT89] <author> William Pugh and T. Teitelbaum. </author> <title> Incremental computation via function caching. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 315-328, </pages> <address> Austin, TX, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: Systems that cope with arbitrary input changes by dynamically checking dependences <ref> [PT89, Hoo92] </ref> avoid more computations than data specialization does, but they lose the efficiency we gain from "compiling away" the dependences in advance. Liu and Teitelbaum [LT95a, LT95b] present algorithms for statically deriving an incremental version of a pure functional program under some input change.
Reference: [Ruf93] <author> Erik Ruf. </author> <title> Topics in Online Partial Evaluation. </title> <type> PhD thesis, </type> <institution> Stanford University, California, </institution> <month> April </month> <year> 1993. </year> <note> Published as technical report CSL-TR-93-563. </note>
Reference-contexts: Keppel et al. [KEH93] com-pared assembly-level and compiler intermediate representation (IR) level template compilers for a variety of workloads, and computed conservative amortization intervals of 10-1000 uses for assembly templates and 1000-infinite uses for IR templates. General partial evaluators <ref> [JGS93, Ruf93, And94, Osg93] </ref> can yield highly specialized code, but existing systems are impractical for runtime use due both to their slowness and to the cost of dynamically compiling the high-level code they generate.
Reference: [SH91] <author> R.S. Sundaresh and Paul Hudak. </author> <title> A theory of incremental computation and its application. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-13, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: For example, using the cached value of the expression fibonacci (x 1) in place of the expression fibonacci (x 2) under the input change (x:x + 1) yields a form of finite differencing. Sundaresh and Hudak <ref> [SH91] </ref> derive incremental versions of functional programs by expressing the program as the composition of a number of residual program fragments, each of which depends on a different projection of the program's inputs.
Reference: [Smi90] <author> Alvy Ray Smith. </author> <note> Unpublished notes. </note> <year> 1990. </year>
Reference-contexts: We construct, compile, and link this code statically at the time a shader is installed, an operation that takes only a few seconds per input partition. In the sections that follow, we present results for ten shading procedures (some derived from examples in <ref> [Ups89, Smi90, GKR95] </ref>, the remainder written by the authors) representing a variety of styles and complexity levels. These range in size from 50 to 150 lines of C code, and invoke a small mathematical library that supports vector and matrix operations as well as noise functions.
Reference: [Ups89] <author> Steve Upstill. </author> <title> The RenderMan Companion. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: We construct, compile, and link this code statically at the time a shader is installed, an operation that takes only a few seconds per input partition. In the sections that follow, we present results for ten shading procedures (some derived from examples in <ref> [Ups89, Smi90, GKR95] </ref>, the remainder written by the authors) representing a variety of styles and complexity levels. These range in size from 50 to 150 lines of C code, and invoke a small mathematical library that supports vector and matrix operations as well as noise functions.
Reference: [WMGH94] <author> Tim A. Wagner, Vance Maverick, Susan Graham, and Michael A. Harrison. </author> <title> Accurate static estimators for program optimization. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 85-96, </pages> <month> June </month> <year> 1994. </year> <month> 11 </month>
Reference-contexts: Further complicating this decision is that the cost and utility of the cache elements are not independent, but depend upon what is already being cached versus computed dynamically. Our heuristic begins by statically approximating the execution cost of every program term (c.f., <ref> [WMGH94] </ref>), combining the following factors: * a static cost value for the term's operator (for example, the cost of + is 1, the cost of / is 9), * the sum of the costs of computing all subterms, * for terms in loops, a multiplier (5), * for terms guarded by
References-found: 32

