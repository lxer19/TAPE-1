URL: http://ciir.cs.umass.edu/info/psfiles/irpubs/callancroftsigir93.ps.gz
Refering-URL: http://ciir.cs.umass.edu/info/psfiles/irpubs/irnew.html
Root-URL: 
Email: callan@cs.umass.edu, croft@cs.umass.edu  
Title: An Evaluation of Query Processing Strategies Using the TIPSTER Collection  
Author: James P. Callan and W. Bruce Croft 
Address: Amherst, MA 01003, USA  
Affiliation: Computer Science Department University of Massachusetts,  
Abstract: The TIPSTER collection is unusual because of both its size and detail. In particular, it describes a set of information needs, as opposed to traditional queries. These detailed representations of information need are an opportunity for research on different methods of formulating queries. This paper describes several methods of constructing queries for the INQUERY information retrieval system, and then evaluates those methods on the TIPSTER document collection. Both AdHoc and Routing query processing methods are evaluated. 
Abstract-found: 1
Intro-found: 1
Reference: [BCCC93] <author> N. J. Belkin, C. Cool, W. B. Croft, and J. P. Callan. </author> <title> The effect of multiple query representations on information retrieval system performance. </title> <booktitle> In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1993. </year>
Reference-contexts: Some documents that were ranked highly by the description-only queries had no relevance judgements, so it is unclear whether the documents were relevant (but not judged), or not relevant. A similar phenomenon has been identified with short Boolean queries <ref> [BCCC93] </ref>. 3.2 Multiple Sources of Information The multiple-field approach to query processing applied different types of processing to different fields. Experiments were conducted with a variety of processing combinations. Results for six of these combinations are reported below.
Reference: [CCH92] <author> James P. Callan, W. Bruce Croft, and Stephen M. Harding. </author> <title> The INQUERY retrieval system. </title> <booktitle> In Proceedings of the Third International Conference on Database and Expert Systems Applications, </booktitle> <pages> pages 78-83. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Documents are indexed by the word stems and numbers that occur in the text. Stopwords are discarded. Documents are also indexed automatically by a small number of features that provide a controlled indexing vocabulary <ref> [CCH92] </ref>. 2 For example, when a document refers to a company by name, the document is indexed by both the company name (words in the text) and the feature #company. 3 IN-QUERY includes company [Rau91], country, U.S. city, 2 We define a feature to be any generalization of words in a <p> We have sometimes called these generalizations concepts (e.g., <ref> [CCH92] </ref>). 3 INQUERY distinguishes among features and words by prefixing features with the `#' character. number and date [Mau89], and person name recogniz-ers. The set used for a particular collection can be controlled easily, and new, domain-specific recogniz-ers can be incorporated easily [CCH92]. <p> We have sometimes called these generalizations concepts (e.g., <ref> [CCH92] </ref>). 3 INQUERY distinguishes among features and words by prefixing features with the `#' character. number and date [Mau89], and person name recogniz-ers. The set used for a particular collection can be controlled easily, and new, domain-specific recogniz-ers can be incorporated easily [CCH92]. It remains an open question how to determine the `right' mix of feature recognizers for a document collection. The query language contains about a dozen operators [TC91; Tur91]. Feature operators match features that were recognized when the document was parsed. For example, the #company operator matches the #company feature.
Reference: [CD90] <author> W. B. Croft and R. Das. </author> <title> Experiments with query acquisition and use in document retrieval systems. </title> <booktitle> In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 349-368, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction One approach to improving the effectiveness of an information retrieval (IR) system is to use sophisticated methods of gathering and representing information from a user. Techniques include automatic or interactive introduction of synonyms [Har88], forms-based interfaces <ref> [CD90] </ref>, automatic recognition of phrases [CTL91], and relevance feedback [SB90]. All of these techniques have shown promise on standard test collections, but it was not clear how they would scale up to much larger and more heterogeneous document collections. <p> The research results obtained are suggestive of what might work well in an interactive interface with a human user. Previous research with other collections suggested that combining different representations of an information need can yield an improvement in both recall and precision <ref> [KMT + 82; CD90; Tur91] </ref>. However, it has been difficult to do systematic research on this subject because different representations of information needs have not been available generally. The TIPSTER project also distinguishes among two different types of queries: AdHoc and Routing [Har92a]. <p> It is also unclear how and how many thesaurus words and phrases to add to the query. A combination of methods Q-1 and Q-6 produced a 2.1% average improvement over either method alone. This result is confirmation of previous research <ref> [KMT + 82; CD90; Tur91] </ref> in two ways. First, it shows that combining different representations of an information need is helpful. Second, it shows that Q-1 and Q-6, which are similar, retrieve different sets of documents.
Reference: [CGHH91] <author> Kenneth Church, William Gale, Patrick Hanks, and Donald Hindle. </author> <title> Using statistics in lexical analysis. </title> <editor> In U. Zernick, editor, </editor> <title> Lexical Acquisition: Using Online Resources to Build the Lexicon. </title> <publisher> Lawrence Erlbaum, </publisher> <year> 1991. </year>
Reference-contexts: Although some phrases were more common in the collection than others, we do not believe the phrases themselves were the problem. The INQUERY phrase operators treat as individual words any phrases that they consider to be `low quality', based upon Mutual Information Measure <ref> [CGHH91] </ref>, frequency, or other statistics.
Reference: [Chu88] <author> Kenneth Church. </author> <title> A stochastic parts pro gram and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the 2nd Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143, </pages> <year> 1988. </year>
Reference-contexts: It also includes a small number of phrases that do not occur in the topics, but that would be likely to occur in an interactive system. 2.2 Phrase Recognition Phrases are recognized in the query by applying a stochastic part of speech tagger <ref> [Chu88] </ref>, and then using rules to identify noun phrases. For example, "monthly short interest" is transformed into "#phrase (monthly short interest)".
Reference: [CTL91] <author> W. B. Croft, H.R. Turtle, and D.D. Lewis. </author> <title> The use of phrases and structured queries in information retrieval. </title> <booktitle> In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 32-45, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction One approach to improving the effectiveness of an information retrieval (IR) system is to use sophisticated methods of gathering and representing information from a user. Techniques include automatic or interactive introduction of synonyms [Har88], forms-based interfaces [CD90], automatic recognition of phrases <ref> [CTL91] </ref>, and relevance feedback [SB90]. All of these techniques have shown promise on standard test collections, but it was not clear how they would scale up to much larger and more heterogeneous document collections. <p> The TREC1 relevance judgements were used to determine relevance. The difference between the performance of methods Q-1 and Q-3 shows that phrases, proper names and proximity operators were useful. This result confirms previous research showing that phrases improved performance <ref> [CTL91] </ref>. However, most of the improvement occurred at low recall, resulting in a small average improvement (1.6%). Experiments with different phrase operators produced only small (usually negative) changes in recall and precision. The reason for these results is unclear.
Reference: [Har88] <author> D. Harman. </author> <title> Towards interactive query ex pansion. </title> <editor> In Y. Chiaramella, editor, </editor> <booktitle> Proceedings of the 11 th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 321-332. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: 1 Introduction One approach to improving the effectiveness of an information retrieval (IR) system is to use sophisticated methods of gathering and representing information from a user. Techniques include automatic or interactive introduction of synonyms <ref> [Har88] </ref>, forms-based interfaces [CD90], automatic recognition of phrases [CTL91], and relevance feedback [SB90]. All of these techniques have shown promise on standard test collections, but it was not clear how they would scale up to much larger and more heterogeneous document collections.
Reference: [Har92a] <author> D. Harman. </author> <title> The DARPA Tipster project. </title> <journal> SIGIR Forum, </journal> <volume> 26(2) </volume> <pages> 26-28, </pages> <year> 1992. </year>
Reference-contexts: All of these techniques have shown promise on standard test collections, but it was not clear how they would scale up to much larger and more heterogeneous document collections. A large and heterogeneous document collection for IR research became available recently as a result of the DARPA/SISTO TIPSTER project <ref> [Har92a] </ref>. The first two volumes of the TIPSTER collection contain the full text of about 750,000 newspaper articles, newswire articles, magazine articles, Federal Register announcements, and Department of Energy technical abstracts. These two volumes occupy about two gigabytes of disk space. <p> However, it has been difficult to do systematic research on this subject because different representations of information needs have not been available generally. The TIPSTER project also distinguishes among two different types of queries: AdHoc and Routing <ref> [Har92a] </ref>. AdHoc queries are intended for a single use to satisfy an immediate need for information.
Reference: [Har92b] <editor> D. Harman, editor. </editor> <booktitle> The First Text RE trieval Conference (TREC1). National Institute of Standards and Technology Special Publication 200-207, </booktitle> <address> Gaithersburg, MD, </address> <year> 1992. </year>
Reference-contexts: Relevance judgements for the TIPSTER collection were obtained by having trained analysts evaluate the top documents retrieved for each topic by a variety of different information retrieval systems. In our experiments, we used the relevance judgements from the first Text REtrieval and Evaluation (TREC1) conference <ref> [Har92b] </ref>. The relevance judgements should be considered incomplete, because most documents were not evaluated for relevance to any topic. The TIPSTER collection differs in many characteristics from the standard test collections available previously. One difference is the set of TIPSTER topics, which each contain varying representations of an information need.
Reference: [HC93] <author> David Haines and W. B. Croft. </author> <title> Relevance feedback and inference networks. </title> <booktitle> In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1993. </year>
Reference-contexts: Relevance feedback is described below. Relevance feedback was conducted on the Q-1 query set and the TIPSTER Volume 1 documents, using all of the TREC1 relevance judgements. The hypothesis was that relevance feedback on Volume 1 would produce queries suitable for use on Volume 2. The rdfidf method <ref> [HC93] </ref> was used to select five terms to add to each query. Term weights for all terms were determined by the rtfidf method [HC93]. This approach to query creation is called Q-R in this paper. The results from the experiments are summarized in Table 7. <p> The hypothesis was that relevance feedback on Volume 1 would produce queries suitable for use on Volume 2. The rdfidf method <ref> [HC93] </ref> was used to select five terms to add to each query. Term weights for all terms were determined by the rtfidf method [HC93]. This approach to query creation is called Q-R in this paper. The results from the experiments are summarized in Table 7. The results were obtained by creating queries for TIPSTER topics 51-100, and using them to retrieve documents from Volume 2 of the TIPSTER data.
Reference: [KMT + 82] <author> J. Katzer, M. J. McGill, J. A. Tessier, W. Frakes, and P. DasGupta. </author> <title> A study of the overlap among document representations. </title> <journal> Information Technology: Research and Development, </journal> <volume> 1 </volume> <pages> 261-274, </pages> <year> 1982. </year>
Reference-contexts: The research results obtained are suggestive of what might work well in an interactive interface with a human user. Previous research with other collections suggested that combining different representations of an information need can yield an improvement in both recall and precision <ref> [KMT + 82; CD90; Tur91] </ref>. However, it has been difficult to do systematic research on this subject because different representations of information needs have not been available generally. The TIPSTER project also distinguishes among two different types of queries: AdHoc and Routing [Har92a]. <p> It is also unclear how and how many thesaurus words and phrases to add to the query. A combination of methods Q-1 and Q-6 produced a 2.1% average improvement over either method alone. This result is confirmation of previous research <ref> [KMT + 82; CD90; Tur91] </ref> in two ways. First, it shows that combining different representations of an information need is helpful. Second, it shows that Q-1 and Q-6, which are similar, retrieve different sets of documents.
Reference: [Mau89] <author> Michael Loren Mauldin. </author> <title> Information re trieval by text skimming. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburg, </institution> <address> PA, </address> <year> 1989. </year>
Reference-contexts: We have sometimes called these generalizations concepts (e.g., [CCH92]). 3 INQUERY distinguishes among features and words by prefixing features with the `#' character. number and date <ref> [Mau89] </ref>, and person name recogniz-ers. The set used for a particular collection can be controlled easily, and new, domain-specific recogniz-ers can be incorporated easily [CCH92]. It remains an open question how to determine the `right' mix of feature recognizers for a document collection.
Reference: [O'C80] <author> John O'Connor. </author> <title> Answer-passage re trieval by text searching. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 31(4) </volume> <pages> 227-239, </pages> <year> 1980. </year>
Reference-contexts: The modifications permitted were restricted to adding words from the Narrative field, deleting words or phrases from the query, and indicating that certain words or phrases should occur near each other within a document. The distance restriction was introduced to simulate paragraph-level retrieval <ref> [O'C80] </ref>. Table 5 summarizes the results of experiments with two slightly different methods of interactive query processing. The differences between the methods are described below. Q-M: Manual addition of words or phrases from the Narrative, and manual deletion of words or phrases from the query.
Reference: [Rau91] <author> Lisa F. Rau. </author> <title> Extracting company names from text. </title> <booktitle> In Proceedings of the Sixth IEEE Conference on Artificial Intelligence Applications, </booktitle> <year> 1991. </year>
Reference-contexts: are also indexed automatically by a small number of features that provide a controlled indexing vocabulary [CCH92]. 2 For example, when a document refers to a company by name, the document is indexed by both the company name (words in the text) and the feature #company. 3 IN-QUERY includes company <ref> [Rau91] </ref>, country, U.S. city, 2 We define a feature to be any generalization of words in a document. We have sometimes called these generalizations concepts (e.g., [CCH92]). 3 INQUERY distinguishes among features and words by prefixing features with the `#' character. number and date [Mau89], and person name recogniz-ers.
Reference: [SB90] <author> Gerard Salton and Chris Buckley. </author> <title> Im proving retrieval performance by relevance feedback. </title> <journal> JASIS, </journal> <volume> 41 </volume> <pages> 288-297, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction One approach to improving the effectiveness of an information retrieval (IR) system is to use sophisticated methods of gathering and representing information from a user. Techniques include automatic or interactive introduction of synonyms [Har88], forms-based interfaces [CD90], automatic recognition of phrases [CTL91], and relevance feedback <ref> [SB90] </ref>. All of these techniques have shown promise on standard test collections, but it was not clear how they would scale up to much larger and more heterogeneous document collections.
Reference: [TC91] <author> Howard Turtle and W. Bruce Croft. </author> <title> Eval uation of an inference network-based retrieval model. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 9(3), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Section 3 describes techniques for creating `AdHoc' queries. Section 4 describes techniques for creating `Routing' queries. Section 5 summarizes the results, and concludes. 2 The INQUERY Information Re trieval System INQUERY is a probabilistic information retrieval system based upon a Bayesian inference network model <ref> [TC91; Tur91] </ref>. Documents are indexed by the word stems and numbers that occur in the text. Stopwords are discarded. <p> The set used for a particular collection can be controlled easily, and new, domain-specific recogniz-ers can be incorporated easily [CCH92]. It remains an open question how to determine the `right' mix of feature recognizers for a document collection. The query language contains about a dozen operators <ref> [TC91; Tur91] </ref>. Feature operators match features that were recognized when the document was parsed. For example, the #company operator matches the #company feature. Proximity operators require their arguments to occur either in order, within some distance of each other, or within some window.
Reference: [Tur91] <author> Howard Robert Turtle. </author> <title> Inference net works for document retrieval. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, University of Massachusetts, Amherst, </institution> <year> 1991. </year>
Reference-contexts: The research results obtained are suggestive of what might work well in an interactive interface with a human user. Previous research with other collections suggested that combining different representations of an information need can yield an improvement in both recall and precision <ref> [KMT + 82; CD90; Tur91] </ref>. However, it has been difficult to do systematic research on this subject because different representations of information needs have not been available generally. The TIPSTER project also distinguishes among two different types of queries: AdHoc and Routing [Har92a]. <p> Section 3 describes techniques for creating `AdHoc' queries. Section 4 describes techniques for creating `Routing' queries. Section 5 summarizes the results, and concludes. 2 The INQUERY Information Re trieval System INQUERY is a probabilistic information retrieval system based upon a Bayesian inference network model <ref> [TC91; Tur91] </ref>. Documents are indexed by the word stems and numbers that occur in the text. Stopwords are discarded. <p> The set used for a particular collection can be controlled easily, and new, domain-specific recogniz-ers can be incorporated easily [CCH92]. It remains an open question how to determine the `right' mix of feature recognizers for a document collection. The query language contains about a dozen operators <ref> [TC91; Tur91] </ref>. Feature operators match features that were recognized when the document was parsed. For example, the #company operator matches the #company feature. Proximity operators require their arguments to occur either in order, within some distance of each other, or within some window. <p> It is also unclear how and how many thesaurus words and phrases to add to the query. A combination of methods Q-1 and Q-6 produced a 2.1% average improvement over either method alone. This result is confirmation of previous research <ref> [KMT + 82; CD90; Tur91] </ref> in two ways. First, it shows that combining different representations of an information need is helpful. Second, it shows that Q-1 and Q-6, which are similar, retrieve different sets of documents.
References-found: 17

