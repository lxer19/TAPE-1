URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/SmaDevGro95.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: fsmagt, anuj, groeng@fwi.uva.nl  
Title: A visually guided robot and a neural network join to grasp slanted objects  Novel Functions  
Author: P. van der Smagt, A. Dev, and F. C. A. Groen RWCP 
Address: Kruislaan 403, NL-1098 SJ Amsterdam  
Affiliation: Faculty of Mathematics and Computer Science, University of Amsterdam  
Pubnum: SNN Laboratory  
Abstract: In this paper we introduce a method for model-free monocular visual guidance of a robot arm. The robot arm, with a single camera in its end-effector, should be positioned above a target, with a changing pan and tilt, which is placed against a textured background. It is shown that a trajectory can be planned in visual space by using components of the optic flow, and this trajectory can be translated to joint torques by a self-learning neural network. No model of the robot, camera, or environment is used. The method reaches a high grasping accuracy after only a few trials.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Cipolla and A. Blake. </author> <title> Surface orientation and time to contact from image divergence and deformation. </title> <editor> In G. Sandini, editor, </editor> <booktitle> Computer Vision|ECCV '92, </booktitle> <pages> pages 187-202. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Thus it is possible that the eye-in-hand robot arm exactly stops on an observed object by use of optic flow, without having absolute visual depth information. The use of optic flow for robot navigation is no novelty. Cipolla and Blake <ref> [1] </ref>, for example, describe a system for estimating time-to-contact from the flow-field, and use this for obstacle avoidance of a robot arm with known kinematics. Sharma [2] uses time-to-contact derived directly from the optic flow as a basis for mobile robot trajectory planning. <p> If we relax the restriction on the direction of the object plane, the relative distance to the object cannot be measured from the object area alone <ref> [1] </ref>; the slant and tilt of the object plane are needed to compute the relative distance ~ z (t). We therefore will use another visual cue: the induced optic flow.
Reference: [2] <author> R. Sharma. </author> <title> Active vision in robot navigation: Monitoring time-to-collision while tracking. </title> <booktitle> In Proceedings of the 1992 IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <pages> pages 2203-2208. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: The use of optic flow for robot navigation is no novelty. Cipolla and Blake [1], for example, describe a system for estimating time-to-contact from the flow-field, and use this for obstacle avoidance of a robot arm with known kinematics. Sharma <ref> [2] </ref> uses time-to-contact derived directly from the optic flow as a basis for mobile robot trajectory planning. Vernon and Tistarelli [3] use visual velocity vectors to estimate depth in a complex visual scene, but again for a robot with known kinematics.
Reference: [3] <author> D. Vernon and M. Tistarelli. </author> <title> Using camera motion to estimate range for robotic parts manipulation. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 7(5) </volume> <pages> 509-521, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Cipolla and Blake [1], for example, describe a system for estimating time-to-contact from the flow-field, and use this for obstacle avoidance of a robot arm with known kinematics. Sharma [2] uses time-to-contact derived directly from the optic flow as a basis for mobile robot trajectory planning. Vernon and Tistarelli <ref> [3] </ref> use visual velocity vectors to estimate depth in a complex visual scene, but again for a robot with known kinematics.
Reference: [4] <author> Patrick van der Smagt. </author> <title> Visual Robot Arm Guidance using Neural Networks. </title> <type> PhD thesis, </type> <institution> Dept of Computer Systems, University of Amsterdam, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: The proof of this theorem is given in <ref> [4] </ref>. We will refer to (5) as the time-dependent constraints. Theorem 1 shows that the trajectory of any n-order system can be described by a single constraint on the ratio of position and velocity of that system. <p> The neural network. The neural network that is used consists of a feed-forward neural network with six hidden units <ref> [4] </ref>. The neural network is implemented on two computers. One computer is used to control the robot. The learning samples it constructs are sent to a second computer, which does the actual learning. Weight matrices are periodically sent back to the controlling computer.
Reference: [5] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> Second-order optic flow. </title> <journal> Optical Society of America A, </journal> <volume> 9(4), </volume> <year> 1992. </year>
Reference-contexts: From the optic flow, ideally a projection of the 3D motion field onto the 2D image plane, we want to compute the orientation of a planar surface on which the object lies or the planar surface of the top of the object. It can be shown <ref> [5] </ref> that every planar surface induces a second order optic flow v = (u; v) and can be written as u (~) = i+j2 x ~ j X v ij ~ i y : (11) From the second order optic flow expansion coefficients (u ij ; v ij )j ~=0 the
Reference: [6] <author> A. M. Waxman and S. Ullman. </author> <title> Surface structure and three-dimensional motion from image flow kinematics. </title> <journal> The International Journal of Robotics Research, </journal> <volume> 4(3), </volume> <year> 1985. </year>
Reference-contexts: can be written as u (~) = i+j2 x ~ j X v ij ~ i y : (11) From the second order optic flow expansion coefficients (u ij ; v ij )j ~=0 the relative velocities of the camera, and the normal of the object plane can be computed <ref> [6] </ref> and are given by: u 00 = t x u 10 = t z x t x u 01 = y t x u 20 = x t z (12) with t i = _ d i (t)=d z (t), where i indicates x, y, or z.
References-found: 6

