URL: http://www.cse.unsw.edu.au/~nickt/doc/casperp_class.ps.gz
Refering-URL: http://www.cse.unsw.edu.au/~nickt/index.html
Root-URL: http://www.cse.unsw.edu.au
Email: -@cse.unsw.edu.au  
Title: Extending and Benchmarking the CasPer Algorithm  
Author: N.K. Treadgold and T.D. Gedeon nickt tom 
Keyword: Neural, Network, Constructive, Cascade, RPROP.  
Web: http://www.cse.unsw.edu.au/-~nickt ~tom  
Address: Sydney N.S.W. 2052 AUSTRALIA  
Affiliation: Department of Information Engineering School of Computer Science Engineering The University of New South Wales  
Abstract: The CasPer algorithm is a constructive neural network algorithm. CasPer creates cascade network architectures in a similar manner to Cascade Correlation. CasPer, however, uses a modified form of the RPROP algorithm, termed Progressive RPROP, to train the whole network after the addition of each new hidden neuron. Previous work with CasPer has shown that it builds networks which generalise better than CasCor, often using less hidden neurons. This work adds two extensions to CasPer. First, an enhancement to the RPROP algorithm, SARPROP, is used to train newly installed hidden neurons. The second extension involves the use of a pool of hidden neurons, each trained using SARPROP, with the best performing selected for insertion into the network. These extensions are shown to result in CasPer producing more compact networks which often generalise better than those produced by the original CasPer algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Treadgold, </author> <title> N.K. and Gedeon, T.D. A Cascade Network Employing Progressive RPROP, </title> <booktitle> Int. Work Conf. on Artificial and Natural Neural Networks, </booktitle> <pages> pp. 733-742, </pages> <year> 1997. </year>
Reference-contexts: 1 INTRODUCTION The CasPer <ref> [1] </ref> algorithm has been shown to be a powerful method for training neural networks. CasPer is a constructive algorithm which inserts hidden neurons one at a time to form a cascade architecture, similar to Cascade Correlation (CasCor) [2]. <p> CasPer is a constructive algorithm which inserts hidden neurons one at a time to form a cascade architecture, similar to Cascade Correlation (CasCor) [2]. CasPer has been shown to produce networks with fewer hidden neurons than CasCor, while also improving the resulting network generalisation, especially with regression tasks <ref> [1] </ref>. The reasons for CasPers improved performance is that it does not use either Cascors correlation measure, which can cause poor generalisation performance [3], or weight freezing, which can lead to oversize networks [4].
Reference: [2] <author> Fahlman, S.E. and Lebiere, C. </author> <booktitle> The cascade-correlation learning architecture, Advances in Neural Information Processing, </booktitle> <volume> vol. 2, </volume> <editor> D.S. Touretzky, (Ed.) </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kauffman, </publisher> <pages> pp. 524-532, </pages> <year> 1990. </year>
Reference-contexts: 1 INTRODUCTION The CasPer [1] algorithm has been shown to be a powerful method for training neural networks. CasPer is a constructive algorithm which inserts hidden neurons one at a time to form a cascade architecture, similar to Cascade Correlation (CasCor) <ref> [2] </ref>. CasPer has been shown to produce networks with fewer hidden neurons than CasCor, while also improving the resulting network generalisation, especially with regression tasks [1]. <p> The CasCor algorithm used for benchmarking was obtained from the public domain Carnegie Mellon University (CMU) AI Repository. For all comparisons, a pool of eight candidate neurons were used and a maximum learning iteration of 100 was set for both the hidden and output neurons, as used by Fahlman <ref> [2] </ref> for the two spirals data set. <p> Each training pattern consists of two inputs (the x,y coordinates) and a single output (the spiral classification). This problem was used by Fahlman <ref> [2] </ref> to demonstrate the effectiveness of the CasCor algorithm on a problem known to be very difficult for traditional Back Propagation to solve. In order to compare CasPer and CasCor on this problem, 100 independent runs were performed using each algorithm. <p> At this point the mean, standard deviation and median for the following characteristics were measured: epochs trained, hidden neurons inserted, number of connection crossings and percentage correct on the test set. Fahlman <ref> [2] </ref> defines the term connection crossings as the number of multiply-accumulate steps to propagate activation values forward through the network and error values backward. <p> This largely overcomes the additional time required to train a pool of neurons. The ability to create cascade networks with smaller numbers of hidden neurons is especially relevant to the area of VLSI implementation of these networks. Cascade networks result in deep networks with large fan-in and propagation delays <ref> [2] </ref>. Smaller networks reduce these difficulties. One problem not addressed by these improvements to CasPer is that the size of the constructed network is difficult to estimate prior to training. A VLSI implementation, however, will need to set an upper bound on both fan in and network depth.
Reference: [3] <author> J. Hwang, S. You, S. Lay, and I. Jou, </author> <title> The Cascade-Correlation Learning: A Projection Pursuit Learning Perspective, </title> <journal> IEEE Trans. Neural Networks 7(2), </journal> <pages> pp. 278-289, </pages> <year> 1996. </year>
Reference-contexts: CasPer has been shown to produce networks with fewer hidden neurons than CasCor, while also improving the resulting network generalisation, especially with regression tasks [1]. The reasons for CasPers improved performance is that it does not use either Cascors correlation measure, which can cause poor generalisation performance <ref> [3] </ref>, or weight freezing, which can lead to oversize networks [4]. A difficult problem faced by both CasPer and CasCor is that the newly created hidden neuron may have difficulty in converging to a good solution on the error surface.
Reference: [4] <author> T. Kwok and D. Yeung, </author> <title> Experimental Analysis of Input Weight Freezing in Constructive Neural Networks, </title> <booktitle> Proc IEEE Int. Conf. On Neural Networks, </booktitle> <pages> pp. 511-516, </pages> <year> 1993. </year>
Reference-contexts: The reasons for CasPers improved performance is that it does not use either Cascors correlation measure, which can cause poor generalisation performance [3], or weight freezing, which can lead to oversize networks <ref> [4] </ref>. A difficult problem faced by both CasPer and CasCor is that the newly created hidden neuron may have difficulty in converging to a good solution on the error surface. One main cause for this poor convergence may be the presence of local minima.
Reference: [5] <author> Treadgold, </author> <title> N.K. and Gedeon, T.D. A Simulated Annealing Enhancement to Resilient Backpropagation, </title> <booktitle> Proc. Int. Panel Conf. Soft and Intelligent Computing, Budapest, </booktitle> <pages> pp. 293-298, </pages> <year> 1996. </year>
Reference-contexts: In order to improve the convergence ability of CasPer, two extensions are proposed. The first is to employ the SARPROP algorithm <ref> [5] </ref> to train the newly inserted hidden neuron. SARPROP is based on the RPROP algorithm [6], and uses Simulated Annealing to enhance the convergence properties of RPROP. SARPROP has been shown to be successful in escaping local minima [5], a property which will enable a better search of the error surface <p> The first is to employ the SARPROP algorithm <ref> [5] </ref> to train the newly inserted hidden neuron. SARPROP is based on the RPROP algorithm [6], and uses Simulated Annealing to enhance the convergence properties of RPROP. SARPROP has been shown to be successful in escaping local minima [5], a property which will enable a better search of the error surface by the new hidden neuron. The second extension involves CasPer training a pool of hidden neurons, as is done in CasCor. <p> CasPer also makes use of weight decay as a means to improve the generalisation properties of the constructed network. After some experimentation it was found that the addition of a Simulated Annealing (SA) term applied to the weight decay, as used in the SARPROP algorithm <ref> [5] </ref>, often improved convergence and generalisation. Each time a new hidden neuron is inserted, the weight decay begins with a large magnitude, which is then reduced by the SA term. The amount of weight decay is proportional to the square of the weight magnitude. <p> The amount of noise added falls as training continues via a Simulated Annealing term. This combination of techniques has been shown to improve RPROPs ability to escape local minima <ref> [5] </ref>.
Reference: [6] <author> Riedmiller, M. and Braun, H. </author> <title> A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm, </title> <booktitle> Proc IEEE Int. Conf. on Neural Networks, </booktitle> <pages> pp. 586-591, </pages> <year> 1993. </year>
Reference-contexts: In order to improve the convergence ability of CasPer, two extensions are proposed. The first is to employ the SARPROP algorithm [5] to train the newly inserted hidden neuron. SARPROP is based on the RPROP algorithm <ref> [6] </ref>, and uses Simulated Annealing to enhance the convergence properties of RPROP. SARPROP has been shown to be successful in escaping local minima [5], a property which will enable a better search of the error surface by the new hidden neuron.
Reference: [7] <author> Fahlman, </author> <title> S.E. An empirical study of learning speed in backpropagation networks, </title> <type> Technical Report CMU-CS-88-162, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1988. </year>
Reference-contexts: The following (standard) RPROP values were used: h + - 50, D min = 1x10 . A constant value of 0.0001 was added to the derivative of the sigmoid in order to overcome the flat spot problem, and the hyperbolic arctan error function was used <ref> [7] </ref>. Weights in the initial network were initialised to evenly distributed random values in the range -0.7 to 0.7. All weights associated with newly inserted hidden neurons were initialised in the range -0.1 to 0.1. Training of the initial network used the initial update value D 0 = 0.2. <p> CasPer halts training when network outputs for all patterns are within 0.2 of the required training outputs, in which case the training set was considered completely learnt. This more restricted value (0.4 being the more traditional value <ref> [7] </ref>) was chosen since it was found that it improved CasPers performance on the test set, although it did result in additional training time, and sometimes in larger networks. S_CasPer and SP_CasPer use the same criterion. For judging success on the test set for classification problems the traditional criterion [7] was <p> value <ref> [7] </ref>) was chosen since it was found that it improved CasPers performance on the test set, although it did result in additional training time, and sometimes in larger networks. S_CasPer and SP_CasPer use the same criterion. For judging success on the test set for classification problems the traditional criterion [7] was used: a pattern was considered correct if all its outputs were within 0.4 of the required outputs. For S_CasPer and SP_CasPer, the SARPROP parameters used were: k 1 = 1, and k 2 = 0.4. SP_CasPer used a pool of eight hidden neurons.
Reference: [8] <author> Murphy, P.M. and Aha, D.W. </author> <title> UCI Repository of machine learning databases, </title> <address> [http://www.ics.uci.edu/~mlearn/MLRepository.html], Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1994. </year>
Reference-contexts: The Iris data set was obtained from the UCI database <ref> [8] </ref>, and consists of 150 patterns, of which 120 were randomly selected as training patterns, leaving 30 as test patterns. Each Iris pattern consists of 4 input and 3 output values.
Reference: [9] <author> Treadgold, </author> <title> N.K. and Gedeon, T.D. Extending CasPer: A Regression Survey, </title> <note> Int. Conf. On Neural Information Processing, to appear, </note> <year> 1997. </year>
Reference-contexts: The SP_CasPer extension, which uses of a pool of hidden neurons trained by SARPROP, results in further improvements in network size, although there is an increased computational cost. Additional comparisons between CasPer, S_CasPer and SP_CasPer using regression benchmarks <ref> [9] </ref> support the conclusions drawn here.
References-found: 9

