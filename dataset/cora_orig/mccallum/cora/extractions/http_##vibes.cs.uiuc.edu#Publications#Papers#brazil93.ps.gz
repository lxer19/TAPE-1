URL: http://vibes.cs.uiuc.edu/Publications/Papers/brazil93.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Publications/publications.htm
Root-URL: http://www.cs.uiuc.edu
Email: E-mail: mendes@cs.uiuc.edu  
Title: Performance Prediction by Trace Transformation  
Author: Celso L. Mendes 
Note: Supported by the Brazilian Institute of Space Research (INPE) and by a scholarship from the Brazilian Ministry of Education, Process CAPES-913/89-2  
Address: Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: Performance stability is an essential feature for the widespread adoption of multicomputers. In this paper, we report the preliminary steps of our research in performance prediction and extrapolation. Performance tuning, guided by extrapolation, may help achieve a substantial fraction of peak performance rates across a broader range of applications while providing guidance for code porting. We introduce a methodology for assessing stability of parallel programs, based on stability of the program execution graph, using time perturbation analysis. For programs with stable behavior, we present a model for performance prediction under architecture variations, by transformation of the execution traces with parameters that reflect the differences in architecture between two systems. We illustrate the use of this transformation with an example of a parallel PDE solver executing on a multicomputer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adve, V. S., and Vernon, M. K. </author> <title> The influence of random delays on parallel execution times. </title> <booktitle> In Sigmetrics Proceedings (San Diego, </booktitle> <month> May </month> <year> 1993). </year>
Reference-contexts: They modeled a parallel computation as a task system with precedence relationships expressed as a series-parallel directed acyclic graph and machine resources as service centers in a queueing network model. On several test cases, they obtained very accurate predictions. However, Adve and Vernon <ref> [1] </ref> suggested recently that stochastic models may create unnecessary modeling complexity. Lyon et al [6] made another claim against stochastic models, by proposing performance analysis at a macro level, thus ignoring particular details in the systems or in the applications.
Reference: [2] <author> A.G.Mohamed, G.C.Fox, von Laszewski, G., M.Parashar, T.Haupt, K.Mills, Y.Lu, N.Lin, and N.Yeh. </author> <title> Application benchmark set for Fortran-D and High Performance Fortran. </title> <type> Tech. Rep. </type> <institution> SCCS-327, Northeast Parallel Architectures Center, </institution> <year> 1992. </year>
Reference-contexts: We will use these tools to study the behavior of time perturbed versions of programs, in which delays are inserted in systematic patterns. In addition to the PDE program, we will be using message-passing codes from the HPF/Fortran D Benchmark Suite <ref> [2] </ref>, a ray-tracing program, which has a more dynamic behavior and thus becomes a potential candidate to instability, and a variety of other parallel codes.
Reference: [3] <author> Jain, R. </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The design of the required experiments includes the selection of code locations where delays will be inserted; each such location constitutes a factor. For each experiment, factors are set at one of their possible levels (e.g., delay or no-delay). In a full factorial design <ref> [3] </ref>, with p locations selected, we must conduct 2 p experiments to determine the influence of each of the p perturbations. However, in practice, a much smaller number is needed, because interactions between distinct perturbations are not always significant.
Reference: [4] <author> Lamport, L. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM 21, </journal> <month> 7 (July </month> <year> 1978), </year> <pages> 558-565. </pages>
Reference-contexts: We refer to this graph as the program execution graph. A particular execution of the program, represented by the program execution graph, defines a relation on the set of events in the execution. This is Lamport's happens before relation <ref> [4] </ref>, denoted by &lt;. It has the following properties: 1. If E m i are events that occur on the same task i, and E m i occurs before E n i , then E m i &lt; E n 2.
Reference: [5] <author> Levi, G. </author> <title> A note on the derivation of maximal common subgraphs of two directed or undirected graphs. </title> <booktitle> Calcolo 9 (1972), </booktitle> <pages> 341-352. </pages>
Reference-contexts: It can be used to compare graphs G 1 and G 2 , and provides a quantitative measure of similarity. Algorithms for direct computation of d are known <ref> [5] </ref>. However, the time complexity of these algorithms is O (n!) for an n node graph.
Reference: [6] <author> Lyon, G., Snelick, R., and Kacker, R. </author> <title> Synthetic-perturbation tuning of MIMD programs. </title> <type> Tech. Rep. NISTIR 5131, </type> <institution> National Institute of Standards and Technology, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: On several test cases, they obtained very accurate predictions. However, Adve and Vernon [1] suggested recently that stochastic models may create unnecessary modeling complexity. Lyon et al <ref> [6] </ref> made another claim against stochastic models, by proposing performance analysis at a macro level, thus ignoring particular details in the systems or in the applications. They inserted synthetic perturbations in a program, and measured their effect on global performance. <p> If the execution graph does not change significantly across machines, we can confidently use the event order on the first machine as a basis for prediction. One possible way to assess program stability relies on time perturbation analysis <ref> [6] </ref>. The idea is to perturb the original program and verify the effect of such perturbation. Several instrumented versions of the program are executed, each with a specific set of time delays inserted in the code.
Reference: [7] <author> Mak, V. W., and Lundstrom, S. F. </author> <title> Predicting performance of parallel computations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1, </journal> <month> 3 (July </month> <year> 1990), </year> <pages> 257-270. </pages>
Reference-contexts: The third reason is that, in general, there is a bigger semantic gap between high-level program code and compiled code on parallel systems than in sequential ones. In another approach, Mak and Lundstrom <ref> [7] </ref> presented a method for predicting performance of parallel computations. They modeled a parallel computation as a task system with precedence relationships expressed as a series-parallel directed acyclic graph and machine resources as service centers in a queueing network model. On several test cases, they obtained very accurate predictions.
Reference: [8] <author> Read, R. C., and Corneil, D. G. </author> <title> The graph isomorphism disease. </title> <journal> Journal of Graph Theory 1 (1977), </journal> <pages> 339-363. </pages>
Reference-contexts: Although it is not known whether graph isomorphism is an NP-complete problem, no polynomial time algorithm is known <ref> [8] </ref>. In our case, testing for isomorphism is insufficient | two execution graphs might be similar, but not isomorphic. We need to determine how "similar" they are. In other words, we need a metric to compare graphs. Under this metric, isomorphism means complete similarity.
Reference: [9] <author> Reed, D. A., Aydt, R. A., Madhyastha, T. M., Noe, R. J., Shields, K. A., and Schwartz, B. W. </author> <title> The Pablo Performance Analysis Environment. </title> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: We conducted the following prediction experiment: based on the traces obtained from execution of the PDE program, instrumented with the Pablo tracing library <ref> [9] </ref>, on four nodes of an iPSC/2 and with a 64x64 grid, we applied our transformation model to compute the predicted traces for an iPSC/860. The first step in the prediction was to execute the communication benchmarks on both the iPSC/2 and iPSC/860, to characterize their communication performance.
Reference: [10] <author> Saavedra-Barrera, R. H., Smith, A. J., and Miya, E. </author> <title> Performance prediction by benchmark and machine characterization. </title> <journal> IEEE Transactions on Computers 38, </journal> <month> 12 (December </month> <year> 1989), </year> <pages> 1659-1679. </pages>
Reference-contexts: Conversely, applications with a deterministic nature are easier to model, making performance prediction simpler and more reliable. 2.3 Related Work Performance prediction and the interaction of hardware, software and application variations have been widely studied for both sequential and parallel systems. As an example, Saavedra-Barrera and Smith <ref> [10] </ref> proposed a model for performance evaluation and prediction on uniprocessors. In this model, they identified standard operations and constructs in Fortran and characterized application programs by the number and type of these operations that were executed.
Reference: [11] <author> SPEC. </author> <title> SPEC benchmark suite release 1.0. SPEC Newsletter 2, </title> <booktitle> 2 (1990), </booktitle> <pages> 3-4. </pages>
Reference-contexts: Comparing two machines that use different processors is, however, a nontrivial task, even for uniprocessors. Several other factors, in addition to the processor type, play an important role in overall performance: cache organization, compiler quality and I/O bandwidth. The SPEC benchmarks <ref> [11] </ref> are a recent attempt to address this problem: performance data are reported for each of the individual benchmarks in the suite. Users then compare two machines by considering only those benchmarks that most closely resemble their typical application. In parallel systems, interconnection network differences also affect overall performance.
Reference: [12] <author> Zelinka, B. </author> <title> On a certain distance between isomorphism classes of graphs. </title> <booktitle> Casopis pro pestovani matem-atiky 100 (1975), </booktitle> <pages> 371-373. </pages>
Reference-contexts: We also define the distance d between graphs G 1 and G 2 by d = n s. Under these definitions, the following two statements are equivalent for any graphs G 1 and G 2 with n vertices <ref> [12] </ref>: 1. There exist isomorphic graphs H 1 and H 2 , each with at least n d vertices, such that H 1 is an induced subgraph of G 1 and H 2 is an induced subgraph of G 2 . 2. <p> Use the notation d (G i ; G j ) to represent the distance between graphs G i and G j and the symbol = to represent graph isomorphism; the following properties hold for any graphs G i , G j and G k of degree n <ref> [12] </ref>: * G i = G j , d (G i ; G j ) = 0 * d (G i ; G j ) = d (G j ; G i ) * 0 d (G i ; G j ) n 1 Thus, d is a metric in the
References-found: 12

