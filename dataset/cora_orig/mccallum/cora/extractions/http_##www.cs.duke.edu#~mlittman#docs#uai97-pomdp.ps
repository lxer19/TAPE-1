URL: http://www.cs.duke.edu/~mlittman/docs/uai97-pomdp.ps
Refering-URL: http://www.cs.duke.edu/~mlittman/topics/pomdp-page.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: arc@cs.brown.edu  mlittman@cs.duke.edu  lzhang@cs.ust.hk  
Title: Incremental Pruning: A Simple, Fast, Exact Method for Partially Observable Markov Decision Processes  
Author: Anthony Cassandra Michael L. Littman Nevin L. Zhang 
Address: Providence, RI 02912  Durham, NC 27708-0129  HK  
Affiliation: Computer Science Dept. Brown University  Dept. of Computer Science Duke University  Computer Science Dept. The Hong Kong U. of Sci. Tech. Clear Water Bay, Kowloon,  
Abstract: Most exact algorithms for general partially observable Markov decision processes (pomdps) use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another. We examine variations of the "incremental pruning" method for solving this problem and compare them to earlier algorithms from theoretical and empirical perspectives. We find that incremental pruning is presently the most efficient ex act method for solving pomdps.
Abstract-found: 1
Intro-found: 1
Reference: <author> Boutilier, C., and Poole, D. </author> <year> 1996. </year> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 1168-1175. </pages> <publisher> AAAI Press/The MIT Press. </publisher>
Reference-contexts: This includes algorithms that solve pomdps via value iteration (Sawaki & Ichikawa 1978; Cassandra, Kaelbling, & Littman 1994), policy iteration (Sondik 1978), accelerated value iteration (White & Scherer 1989), structured representations <ref> (Boutilier & Poole 1996) </ref>, and approximation (Zhang & Liu 1996). Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial.
Reference: <author> Cassandra, A. R.; Kaelbling, L. P.; and Littman, M. L. </author> <year> 1994. </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 1023-1028. </pages>
Reference: <author> Cheng, H.-T. </author> <year> 1988. </year> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> Ph.D. Dissertation, </type> <institution> University of British Columbia, British Columbia, Canada. </institution>
Reference-contexts: Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass (Sondik 1971), exhaustive (Monahan 1982), linear support <ref> (Cheng 1988) </ref>, and witness (Littman, Cassandra, & Kaelbling 1996). Cheng (1988) gave experimental evidence that the linear support algorithm is more efficient than the one-pass algorithm.
Reference: <author> Chrisman, L. </author> <year> 1992. </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 183-188. </pages> <address> San Jose, California: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Hansen, E. A. </author> <year> 1994. </year> <title> Cost-effective sensing during plan execution. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/The MIT Press. </publisher> <pages> 1029-1035. </pages>
Reference: <author> Kushmerick, N.; Hanks, S.; and Weld, D. S. </author> <year> 1995. </year> <title> An algorithm for probabilistic planning. </title> <journal> Artificial Intelligence 76(1-2):239-286. </journal>
Reference: <author> Littman, M. L.; Cassandra, A. R.; and Kaelbling, L. P. </author> <year> 1996. </year> <title> Efficient dynamic-programming updates in partially observable Markov decision processes. </title> <type> Technical Report CS-95-19, </type> <institution> Brown University, Providence, RI. </institution>
Reference-contexts: Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass (Sondik 1971), exhaustive (Monahan 1982), linear support (Cheng 1988), and witness <ref> (Littman, Cassandra, & Kaelbling 1996) </ref>. Cheng (1988) gave experimental evidence that the linear support algorithm is more efficient than the one-pass algorithm. <p> These sets have a unique representation of minimum size <ref> (Littman, Cas-sandra, & Kaelbling 1996) </ref>, and we assume that the symbols S a z , S a , and S 0 refer to the minimum-size sets. Here is a brief description of the set and vector nota tion we will be using. <p> Using the definition of R, we can define purge (A) = fffjff 2 A; R (ff; A) 6= ;g; it is the set of vectors in A that have non-empty witness regions and is precisely the minimum-size set for representing the piecewise-linear convex function given by A <ref> (Littman, Cassandra, & Kaelbling 1996) </ref> 1 . a set of vectors F , Filter (F ) returns the vectors in F that have non-empty witness regions, thereby "purging" or "filtering" or "pruning" out the unnecessary vectors. <p> For the remainder of this paper, we assume that jW j &gt; 1, since the case of jW j = 1 is trivial. The witness algorithm has been analyzed previously <ref> (Littman, Cassandra, & Kaelbling 1996) </ref>, and we list the basic results here for easy comparison. The total number of linear programs solved by witness is ( P z j jZj)jS a j + jS a j 1; asymptotically, this is fi (jS a j P z j).
Reference: <author> McCallum, R. A. </author> <year> 1993. </year> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 190-196. </pages> <address> Amherst, Massachusetts: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Monahan, G. E. </author> <year> 1982. </year> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <booktitle> Management Science 28(1) </booktitle> <pages> 1-16. </pages>
Reference-contexts: Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass (Sondik 1971), exhaustive <ref> (Monahan 1982) </ref>, linear support (Cheng 1988), and witness (Littman, Cassandra, & Kaelbling 1996). Cheng (1988) gave experimental evidence that the linear support algorithm is more efficient than the one-pass algorithm.
Reference: <author> Parr, R., and Russell, S. </author> <year> 1995. </year> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Platzman, L. K. </author> <year> 1981. </year> <title> A feasible computational approach to infinite-horizon partially-observed Markov decision problems. </title> <type> Technical report, </type> <institution> Georgia Institute of Technology, </institution> <address> Atlanta, GA. </address>
Reference-contexts: There are many ways to approach this problem based on checking which information states can be reached (Washington 1996; Hansen 1994), searching for good controllers <ref> (Platzman 1981) </ref>, and using dynamic programming (Smallwood & Sondik 1973; Cheng 1988; Monahan 1982; Littman, Cassandra, & Kaelbling 1996). Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another.
Reference: <author> Russell, S. J., and Norvig, P. </author> <year> 1994. </year> <title> Artificial Intelligence: A Modern Approach. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Sawaki, K., and Ichikawa, A. </author> <year> 1978. </year> <title> Optimal control for partially observable Markov decision processes over an infinite horizon. </title> <journal> Journal of the Operations Research Society of Japan 21(1) </journal> <pages> 1-14. </pages>
Reference: <author> Smallwood, R. D., and Sondik, E. J. </author> <year> 1973. </year> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research 21 </journal> <pages> 1071-1088. </pages>
Reference: <author> Sondik, E. </author> <year> 1971. </year> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University. </institution>
Reference-contexts: Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass <ref> (Sondik 1971) </ref>, exhaustive (Monahan 1982), linear support (Cheng 1988), and witness (Littman, Cassandra, & Kaelbling 1996). Cheng (1988) gave experimental evidence that the linear support algorithm is more efficient than the one-pass algorithm.
Reference: <author> Sondik, E. J. </author> <year> 1978. </year> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research 26(2) </journal> <pages> 282-304. </pages>
Reference-contexts: Most exact algorithms for general pomdps use a form of dynamic programming in which a piecewise-linear and convex representation of one value function is transformed into another. This includes algorithms that solve pomdps via value iteration (Sawaki & Ichikawa 1978; Cassandra, Kaelbling, & Littman 1994), policy iteration <ref> (Sondik 1978) </ref>, accelerated value iteration (White & Scherer 1989), structured representations (Boutilier & Poole 1996), and approximation (Zhang & Liu 1996). Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial.
Reference: <author> Washington, R. </author> <year> 1996. </year> <title> Incremental Markov-model planning. </title> <booktitle> In Proceedings of TAI-96, Eighth IEEE International Conference on Tools With Artificial Intelligence, </booktitle> <pages> 41-47. </pages>
Reference: <author> White, III, C. C., and Scherer, W. T. </author> <year> 1989. </year> <title> Solution procedures for partially observed Markov decision processes. </title> <journal> Operations Research 37(5) </journal> <pages> 791-797. </pages>
Reference-contexts: This includes algorithms that solve pomdps via value iteration (Sawaki & Ichikawa 1978; Cassandra, Kaelbling, & Littman 1994), policy iteration (Sondik 1978), accelerated value iteration <ref> (White & Scherer 1989) </ref>, structured representations (Boutilier & Poole 1996), and approximation (Zhang & Liu 1996). Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial.
Reference: <author> White, III, C. C. </author> <year> 1991. </year> <title> Partially observed Markov decision processes: A survey. </title> <note> Annals of Operations Research 32. </note>
Reference-contexts: The algorithm is due to Lark <ref> (White 1991) </ref>; Littman, Cassandra, & Kaelbling (1996) analyze the algorithm and describe the way that the argmax operators need to be implemented for the analysis to hold (ties must be broken lexicographically). <p> The lemma follows. 2 Different choices of D result in different incremental pruning algorithms. In general, the smaller the D set, the more efficient the algorithm. Equation 13 is equiv alent to using Monahan's (1982) filtering algorithm in IncPrune, Equation 15 is equivalent to using Lark's filtering algorithm <ref> (White 1991) </ref> in IncPrune (i.e., IP, as described earlier). We refer to variations of the incremental pruning method using a combination of Equations 16 and 17 as the restricted region (RR) algorithm.
Reference: <author> Zhang, N. L., and Liu, W. </author> <year> 1996. </year> <title> Planning in stochastic domains: Problem characteristics and approximation. </title> <type> Technical Report HKUST-CS96-31, </type> <institution> Department of Computer Science, Hong Kong University of Science and Technology. </institution>
Reference-contexts: This includes algorithms that solve pomdps via value iteration (Sawaki & Ichikawa 1978; Cassandra, Kaelbling, & Littman 1994), policy iteration (Sondik 1978), accelerated value iteration (White & Scherer 1989), structured representations (Boutilier & Poole 1996), and approximation <ref> (Zhang & Liu 1996) </ref>. Because dynamic-programming updates are critical to such a wide array of pomdp algorithms, identifying fast algorithms is crucial. Several algorithms for dynamic-programming updates have been proposed, such as one pass (Sondik 1971), exhaustive (Monahan 1982), linear support (Cheng 1988), and witness (Littman, Cassandra, & Kaelbling 1996). <p> IncPrune (S a z 1 ; : : : ; S a 1 W Filter (S a z 1 S a 2 for i 3 to k 3 do W Filter (W S a z i ) 4 return W 4 INCREMENTAL PRUNING This section describes the incremental pruning method <ref> (Zhang & Liu 1996) </ref>, which computes S a efficiently from the S a z sets. Recall the definition for S a in Equation 7: S a = purge z2Z z = purge (S a z 1 S a z k ); here, k = jZj.
References-found: 20

