URL: http://kmi.open.ac.uk/techreports/papers/kmi-tr-43.ps.gz
Refering-URL: http://kmi.open.ac.uk/techreports/kmi-tr-list.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. L. Buntine. </author> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 159-225, </pages> <year> 1994. </year> <title> 14 Learning Bayesian Networks from Incomplete Databases Generating Structure M 4 log(^p(D inc jM)) Run Time (sec) 100 - % -40125.37 275 X 5 X 3 X 1 ! X 2 X 4 60 - " % -42145.68 278 X 5 X 3 X 4 X 1 ! X 2 20 - " % -39952.47 285 Table 11: Models induced from the database generated from M 4 for different percentages of available data. </title>
Reference-contexts: Early results in this quest toward an efficient method to learn bbns from databases were based on non Bayesian approaches [11], but almost immediately a seminal paper by Cooper and Herskovitz [4] gave rise to a stream of research within a Bayesian framework <ref> [1, 5] </ref>. Along this second approach, there are two main tasks involved in the learning process of a bbn from a database: the induction of the graphical model best fitting the database at hand, and the extraction of the conditional probabilities defining the dependencies in a given graphical model.
Reference: [2] <author> D. M. Chickering and D. Heckerman. </author> <title> Learning Bayesian networks is NP-hard. </title> <type> Tech nical Report MSR-TR-94-17, </type> <institution> Microsoft Research, Microsoft Corporation, </institution> <year> 1994. </year>
Reference-contexts: Current approaches exploit heuristics to reduce the search space and use the scoring metric to drive the search process. Although the process of extracting bbns from databases in known to be NP-Hard for the general case <ref> [2] </ref>, under certain assumptions, these methods are able to extract quite large bbns from databases of thousands of cases. One of these assumptions is that the database is complete, that is, no entry in the database is unknown.
Reference: [3] <author> D. M. Chickering and D. Heckerman. </author> <title> Efficient approximations for the marginal likeli hood of incomplete data given a Bayesian network. </title> <type> Technical Report MSR-TR-96-08, </type> <institution> Microsoft Research, Microsoft Corporation, </institution> <year> 1996. </year>
Reference-contexts: Well-known methods typically involve the use of the em algorithm or Markof Chain Monte Carlo methods, such as Gibbs sampling <ref> [3] </ref>.
Reference: [4] <author> G.F. Cooper and E. Herskovitz. </author> <title> A bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: Early results in this quest toward an efficient method to learn bbns from databases were based on non Bayesian approaches [11], but almost immediately a seminal paper by Cooper and Herskovitz <ref> [4] </ref> gave rise to a stream of research within a Bayesian framework [1, 5]. <p> Clearly the valuation can be done independently of the marginal probability p (D), by just considering the joint probability of the model and the data: p (M; D). It is well known <ref> [4] </ref>, that p (M; D) can be easily computed if the conditional probabilities defining M are regarded as random variables ijk whose prior distribution represents the observer's beliefs before seen any data. <p> The database is complete; 2. The cases are independent, given the parameter associated to M; 3. The prior distribution of the parameters is conjugate to the sampling model p (Dj); 4. The parameters are marginally independent. Details of the calculations can be found for instance in <ref> [4] </ref>. <p> The probability (3) is the base for an algorithm proposed by <ref> [4] </ref> to induce the model from a database. Suppose we have a partial order on the variables so that X i X j if X i cannot be parent of X j . <p> D (^ff ij ^p ij1 ; :::; ^ff ij ^p ijc i ): (7) From (7) we can then derive an estimate of (3): ^p (M; D) = p (M) i=1 j=1 ( ^ff ij ) k=1 (ff ijk ) which can be also used to extend the algorithm in <ref> [4] </ref> to the induction of Bayesian networks from incomplete databases. <p> The system performs two tasks: i) extracts a graphical model from the available information in the database using the method described in Section 3 and ii) assesses the conditional probabilities for the extracted graphical model using the bc method. The first task uses the search procedure devised by <ref> [4] </ref>, and replaces the measure 6 to estimate the local contribution of a node X i and its parents ij to the joint probability of 8 Learning Bayesian Networks from Incomplete Databases Generating Structure Variables n M 1 X 1 ! X 2 ! X 3 all binary 1,000 M 2
Reference: [5] <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning bayesian networks: The combinations of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: Early results in this quest toward an efficient method to learn bbns from databases were based on non Bayesian approaches [11], but almost immediately a seminal paper by Cooper and Herskovitz [4] gave rise to a stream of research within a Bayesian framework <ref> [1, 5] </ref>. Along this second approach, there are two main tasks involved in the learning process of a bbn from a database: the induction of the graphical model best fitting the database at hand, and the extraction of the conditional probabilities defining the dependencies in a given graphical model.
Reference: [6] <author> R.J.A. Little and D.B. Rubin. </author> <title> Statistical Analysis with Missing Data. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Well-known methods typically involve the use of the em algorithm or Markof Chain Monte Carlo methods, such as Gibbs sampling [3]. The basic strategy underlying these methods is based on the Missing Information Principle <ref> [6] </ref>: fill in the missing observations on the basis of the available information. em performs this task by replacing the missing entries with the maximum likelihood estimates extracted from the available data and proceeds by iteratively estimating and replacing until stability is reached within a certain threshold. <p> missing at random, that is: ff ijk + n (x ik j ij ) + n * ff ij + h n (x ih j ij ) + n * : A remarkable property is that, if ff ijk = 0, then 14 is the maximum likelihood estimate of ijk <ref> [6] </ref>. The method is not limited to the assumption that data are missing at random. For instance, when no information on the mechanism generating the missing data is available and therefore any pattern of missing data is equally likely, then ^ ijk = 1=c i .
Reference: [7] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of plausible inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: 1. Introduction A Bayesian Belief Network (bbn) <ref> [7] </ref> is direct acyclic graph where nodes represent stochastic variables and arcs represent conditional dependencies among these variables.
Reference: [8] <author> M. Ramoni and P. Sebastiani. </author> <title> Robust learning with missing data. </title> <type> Technical Report KMi-TR-28, </type> <institution> Knowledge Media Institute, The Open University, </institution> <year> 1996. </year> <note> Available at http://kmi.open.ac.uk/techreports/KMi-TR-28. </note>
Reference-contexts: j ij ; D inc ); p * (x ik j ij ; D inc )] contains all posterior estimates of ijk that would be obtained from the possible completions of the database and therefore it provides a measure of the quality of information conveyed by D inc about ijk <ref> [8] </ref>. 3.2 Step 2: Posterior Precision The value in (11) is an estimate of E ( ijk jD inc ). We now derive an estimate of the posterior precision of ij . <p> variable X i is done by evaluating the estimate ^g (X i ; P i ) = j=1 k=1 ( ^ff ij )(ff ijk ) The second task is performed using the method described in [9] and it is implemented using a slightly modified version of the algorithms described in <ref> [8] </ref>. The system has been developed in Common Lisp and CLOS under Machintosh Common Lisp v.4. Being entirely CLtL2 compliant, the system should be easily portable to any platform where a CLtL2 development environment is available. 5. <p> The number of missing data affects only the storage procedure described in <ref> [8] </ref> but its effect is limited by taking advantage of the local independence of the bbn and by using discrimination trees to store the counters of observed data and keep track of the possible completions.
Reference: [9] <author> M. Ramoni and P. Sebastiani. </author> <title> The use of exogenous knowledge to learn Bayesian networks from incomplete databases. Technical Report KMi-TR 15 Learning Bayesian Networks from Incomplete Databases 44, </title> <institution> Knowledge Media Institute, The Open University, </institution> <year> 1996. </year> <note> Available at http://kmi.open.ac.uk/techreports/KMi-TR-44. </note>
Reference-contexts: Unfortunately, these processes are usually highly resource demanding, their convergence rates may be slow, and their execution time heavily depends on the number of missing data. Ramoni and Sebastiani <ref> [9] </ref> introduced a deterministic method to estimate the conditional probabilities defining the dependencies in a bbn which does not rely on the Missing Information Principle. <p> parents ij to the joint probability of (M; D) defined for the complete databases by 6 can be estimated as q i Y (ff ij ) c i Y ( ^ff ij ^p (x ik j ij )) : (9) 3.1 Step 1: Posterior Expectation The bc method proposed by <ref> [9] </ref> is a technique to estimate the conditional probabilities defining a bbn from an incomplete database. Suppose we have a model M of conditional dependencies, specifying for each X i the parent variable ij . <p> The method is not limited to the assumption that data are missing at random. For instance, when no information on the mechanism generating the missing data is available and therefore any pattern of missing data is equally likely, then ^ ijk = 1=c i . Experimental comparisons <ref> [9] </ref> have shown that, when data are missing at random, the estimates computed by the bc method are equivalent to those obtained using stochastic methods based on the Missing Information Principle, as the Gibbs Sampling, are are more robust to departures from the true pattern of missing data. <p> Therefore, the choice on whether adding a parent node to a variable X i is done by evaluating the estimate ^g (X i ; P i ) = j=1 k=1 ( ^ff ij )(ff ijk ) The second task is performed using the method described in <ref> [9] </ref> and it is implemented using a slightly modified version of the algorithms described in [8]. The system has been developed in Common Lisp and CLOS under Machintosh Common Lisp v.4.
Reference: [10] <author> D.J. Spiegelhalter and S.L. Lauritzen. </author> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 157-224, </pages> <year> 1990. </year>
Reference-contexts: It is well known <ref> [10] </ref> that, under the assumptions 1 - 4, the posterior distribution of is still a product of Dirichlet distributions, and Thus an estimate of p (x ik j ij ) is the posterior expectation of ijk : ff ijk + n (x ijk j ij ) ; and the posterior precision
Reference: [11] <author> P. Sprites, C. Glymour, and R. Scheines. </author> <title> Causation, Prediction, and Search. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1993. </year> <month> 16 </month>
Reference-contexts: Early results in this quest toward an efficient method to learn bbns from databases were based on non Bayesian approaches <ref> [11] </ref>, but almost immediately a seminal paper by Cooper and Herskovitz [4] gave rise to a stream of research within a Bayesian framework [1, 5].
References-found: 11

