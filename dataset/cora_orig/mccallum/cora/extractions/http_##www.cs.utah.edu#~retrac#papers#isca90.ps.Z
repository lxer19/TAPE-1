URL: http://www.cs.utah.edu/~retrac/papers/isca90.ps.Z
Refering-URL: http://www.cs.utah.edu/~retrac/papers.html
Root-URL: 
Title: Adaptive Software Cache Management for Distributed Shared Memory Architectures  
Author: John K. Bennett John B. Carter flfl Willy Zwaenepoel flfl 
Address: Houston, TX 77251-1892  
Affiliation: Department of Electrical and Computer Engineering flfl Department of Computer Science Rice University  
Abstract: An adaptive cache coherence mechanism exploits semantic information about the expected or observed access behavior of particular data objects. We contend that, in distributed shared memory systems, adaptive cache coherence mechanisms will outperform static cache coherence mechanisms. We have examined the sharing and synchronization behavior of a variety of shared memory parallel programs. We have found that the access patterns of a large percentage of shared data objects fall in a small number of categories for which efficient software coherence mechanisms exist. In addition, we have performed a simulation study that provides two examples of how an adaptive caching mechanism can take advantage of semantic information. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Richard L. Sites, and Mark Horowitz. ATUM: </author> <title> A new technique for capturing address traces using microcode. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 119-127, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: An important difference between our approach and previous methods <ref> [1, 16] </ref> is that we only log accesses to shared memory, not all accesses to memory. Non-shared memory, such as program code and local variables, generally does not require special handling in a distributed shared memory system. <p> Logging in software during program execution combines many of the benefits of software simulation [16] and built-in tracing mechanisms <ref> [1] </ref>, without some of the problems associated with these techniques. As with software simulation, with software logging it is easy to change the information that is collected during a particular run of the program. <p> The flexibility, power and low overhead of our system does not come without cost. Only accesses to shared memory performed by the applications program and run-time system are collected, so our system suffers from what Agarwal refers to as omission distortion <ref> [1] </ref>, the inability of a system to record the complete address stream of a running program.
Reference: [2] <author> James Archibald. </author> <title> A cache coherence approach for large multiprocessor systems. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 337-345, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: write-invalidate cache does not needlessly broadcast the new value during subsequent writes. 8 Archibald described a cache coherence protocol that attempts to adapt to the current reference pattern and dynamically choose to update or invalidate the other copies of a shared data object depending on how they are being used <ref> [2] </ref>. His protocol is designed for hardware implementation, and therefore is fairly simple and not as aggressive in its attempts to adapt to the expected access behavior as what we propose. Nevertheless, his simulation study indicates that even a simple adaptive protocol can enhance performance.
Reference: [3] <author> James Archibald and Jean-Loup Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: provide evidence that adaptive cache coherence mechanisms can significantly improve upon the performance of standard cache coherence mechanisms in a distributed shared memory system where all coherence is performed in software and network latencies are relatively high. 5 Related Work Archibald and Baer discuss a variety of cache coherence protocols <ref> [3] </ref>, most of which are variations of write-invalidate and write-update. Each works well in some instances and poorly in others.
Reference: [4] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the 1990 Conference on Principles and Practice of Parallel Programming, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: 1 Introduction We are developing Munin <ref> [4] </ref>, a system that will allow programs written for shared memory multiprocessors to be executed efficiently on distributed memory machines. What distinguishes Munin from previous distributed shared memory systems [6, 12, 14] is the means by which memory coherence is achieved. <p> For adaptive caching to perform well, it must be possible to characterize a large percentage of all accesses to shared data objects by a small number of categories of access patterns for which efficient coherence mechanisms can be developed. In a previous paper <ref> [4] </ref>, we have identified a number of categories, and described the design of efficient coherence mechanism for each. In this paper, we show that these categories capture the vast majority of the accesses to shared data objects in a number of shared memory parallel programs. <p> We also show, through simulation, the potential for performance improvement of adaptive caching compared to static coherence mechanisms. In Section 2 of this paper, we briefly reiterate the main results of our previous paper <ref> [4] </ref>. We describe the categories of access patterns, provide examples, and give a brief description of how each category can be handled efficiently. <p> A brief description of these mechanisms may provide insight into why these particular categories are chosen. A separate paper describes our complete set of coherence mechanisms in more detail <ref> [4] </ref>. Write-Many objects appear in many parallel programs wherein several threads simultaneously access and modify a single shared data object between explicit synchronization points in the program. <p> The programmer uses explicit synchronization (such as a lock or monitor) to denote the points in the program execution at which such inconsistencies are not tolerable. We refer to this controlled inconsistency as loose coherence <ref> [4] </ref>, as contrasted with strict coherence, in which no inconsistency is allowed. Strict and loose coherence are closely related to the concepts of strong and weak ordering of events as described by Dubois, et al [7].
Reference: [5] <author> Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> PRESTO: A system for object-oriented parallel programming. </title> <journal> Software| 9 Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Result objects are handled by maintaining a single copy and propagating updates to this copy. Finally, General Read-Write objects are handled by a standard coherence mechanism. 3 Logging Study 3.1 Programs We have studied six shared memory parallel programs written in C++ [17] using the Presto programming system <ref> [5] </ref> on the Sequent Symmetry shared memory multiprocessor [13].
Reference: [6] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Lit-tlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> De-cember </month> <year> 1989. </year>
Reference-contexts: 1 Introduction We are developing Munin [4], a system that will allow programs written for shared memory multiprocessors to be executed efficiently on distributed memory machines. What distinguishes Munin from previous distributed shared memory systems <ref> [6, 12, 14] </ref> is the means by which memory coherence is achieved. Instead of a single memory coherence mechanism for all shared data objects, Munin will employ several different mechanisms, each appropriate for a different category of shared data object.
Reference: [7] <author> Michel Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: We refer to this controlled inconsistency as loose coherence [4], as contrasted with strict coherence, in which no inconsistency is allowed. Strict and loose coherence are closely related to the concepts of strong and weak ordering of events as described by Dubois, et al <ref> [7] </ref>. Strong and weak ordering define a relation on the ordering of events (such as accesses to shared memory) in a system, while strict and loose coherence are operational definitions of the coherence guarantees that a system provides. Maintaining strict coherence unnecessarily is inefficient and introduces false sharing.
Reference: [8] <author> Susan J. Eggers and Randy H. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 373-383, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: For example, the log entry for a read of an element of a matrix object indicates only that the matrix was read at object granularity, but indicates the specific element that was read at element granularity. Our study of sharing in parallel programs distinguishes itself from similar work <ref> [8, 15, 18] </ref> in that it studies sharing at the programming language level, and hence is relatively architecture-independent, and in that our selection of parallel programs embodies a wider variation in programming and synchronization styles. <p> Migratory objects are accessed in long runs. A run or write-run <ref> [8] </ref> is a sequence of accesses to a single object by a single thread. For the purposes of our analysis, the minimum length of a long run is variable (default: 8). <p> Our approach is more aggressive. We have identified two types of shared data objects (Write-Many and Producer-Consumer) that would fall into Weber-Gupta's Frequently read/written category, yet can be handled efficiently by an appropriate protocol. Eggers and Katz analyze the sharing characteristics of four parallel programs <ref> [8] </ref>. Two of the applications exhibited a high percentage of sequential sharing while the other two exhibited a high degree of fine-grained sharing. This indicates that neither write-broadcast nor write-invalidate is clearly better for all applications.
Reference: [9] <author> James R. Goodman, Mary K. Vernon, and Philip J. Woest. </author> <title> Efficient synchronization primitives for large-scale cache-coherent multiprocessor. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <month> April </month> <year> 1989. </year>
Reference-contexts: Each lock has a queue associated with it that contains a list of the servers that need the lock. This queue facilitates efficient exchange of lock ownership. This mechanism is similar to that proposed by Goodman, et al <ref> [9] </ref>. Several categories of shared data objects can be handled in a straightforward fashion. Private objects are only accessed by one thread, so keeping them coherent is trivial. Replication is used for Write-once objects. Read-mostly objects are also candidates for replication since reads predominate writes.
Reference: [10] <author> A. R. Karlin, M. S. Manasse, L. Rudolph, and D.D. Sleator. </author> <title> Competitive snoopy caching. </title> <booktitle> In Proceedings of the 16th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 244-254, </pages> <year> 1986. </year>
Reference-contexts: Nevertheless, his simulation study indicates that even a simple adaptive protocol can enhance performance. Other adaptive caching schemes have been proposed, including competitive snoopy caching <ref> [10] </ref> and read-broadcast [11]. Each appears to be appropriate for particular types of sharing behavior, and we plan to examine them in more detail as our work continues. Weber and Gupta attempt to link the observed invalidation patterns back to high-level applications program objects [18].
Reference: [11] <author> Kai Li. </author> <title> Private communication. </title> <month> March </month> <year> 1990. </year>
Reference-contexts: Nevertheless, his simulation study indicates that even a simple adaptive protocol can enhance performance. Other adaptive caching schemes have been proposed, including competitive snoopy caching [10] and read-broadcast <ref> [11] </ref>. Each appears to be appropriate for particular types of sharing behavior, and we plan to examine them in more detail as our work continues. Weber and Gupta attempt to link the observed invalidation patterns back to high-level applications program objects [18].
Reference: [12] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: 1 Introduction We are developing Munin [4], a system that will allow programs written for shared memory multiprocessors to be executed efficiently on distributed memory machines. What distinguishes Munin from previous distributed shared memory systems <ref> [6, 12, 14] </ref> is the means by which memory coherence is achieved. Instead of a single memory coherence mechanism for all shared data objects, Munin will employ several different mechanisms, each appropriate for a different category of shared data object.
Reference: [13] <author> Tom Lovett and Shreekant Thakkar. </author> <title> The Symmetry multiprocessor system. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 303-310, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Finally, General Read-Write objects are handled by a standard coherence mechanism. 3 Logging Study 3.1 Programs We have studied six shared memory parallel programs written in C++ [17] using the Presto programming system [5] on the Sequent Symmetry shared memory multiprocessor <ref> [13] </ref>. The selected programs are written specifically for a shared memory multiprocessor so that our results are not influenced by the program being written with distribution in mind and accurately reflect the memory access behavior that occurs when programmers do not expend special effort towards distributing the data across processors.
Reference: [14] <author> Umakishore Ramachandran and M. Yousef A. Khalidi. </author> <title> An implementation of distributed shared memory. </title> <booktitle> Distributed and Multiprocessor Systems Workshop, </booktitle> <pages> pages 21-38, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction We are developing Munin [4], a system that will allow programs written for shared memory multiprocessors to be executed efficiently on distributed memory machines. What distinguishes Munin from previous distributed shared memory systems <ref> [6, 12, 14] </ref> is the means by which memory coherence is achieved. Instead of a single memory coherence mechanism for all shared data objects, Munin will employ several different mechanisms, each appropriate for a different category of shared data object.
Reference: [15] <author> Richard L. Sites and Anant Agarwal. </author> <title> Multiprocessor cache analysis using ATUM. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 186-195, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: For example, the log entry for a read of an element of a matrix object indicates only that the matrix was read at object granularity, but indicates the specific element that was read at element granularity. Our study of sharing in parallel programs distinguishes itself from similar work <ref> [8, 15, 18] </ref> in that it studies sharing at the programming language level, and hence is relatively architecture-independent, and in that our selection of parallel programs embodies a wider variation in programming and synchronization styles.
Reference: [16] <author> K. So, F. Darema-Rogers, D. George, V.A. Nor-ton, and G.F. Pfister. PSIMUL: </author> <title> A system for parallel simulation of the execution of parallel programs. </title> <type> Technical Report RC11674, </type> <institution> IBM Research, </institution> <year> 1986. </year>
Reference-contexts: An important difference between our approach and previous methods <ref> [1, 16] </ref> is that we only log accesses to shared memory, not all accesses to memory. Non-shared memory, such as program code and local variables, generally does not require special handling in a distributed shared memory system. <p> This allows us to log the shared memory accesses of relatively long-running programs in their entirety, which is important because the access patterns during initialization are significantly different from those during computation. Logging in software during program execution combines many of the benefits of software simulation <ref> [16] </ref> and built-in tracing mechanisms [1], without some of the problems associated with these techniques. As with software simulation, with software logging it is easy to change the information that is collected during a particular run of the program.
Reference: [17] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: Result objects are handled by maintaining a single copy and propagating updates to this copy. Finally, General Read-Write objects are handled by a standard coherence mechanism. 3 Logging Study 3.1 Programs We have studied six shared memory parallel programs written in C++ <ref> [17] </ref> using the Presto programming system [5] on the Sequent Symmetry shared memory multiprocessor [13].
Reference: [18] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year> <month> 10 </month>
Reference-contexts: For example, the log entry for a read of an element of a matrix object indicates only that the matrix was read at object granularity, but indicates the specific element that was read at element granularity. Our study of sharing in parallel programs distinguishes itself from similar work <ref> [8, 15, 18] </ref> in that it studies sharing at the programming language level, and hence is relatively architecture-independent, and in that our selection of parallel programs embodies a wider variation in programming and synchronization styles. <p> Each appears to be appropriate for particular types of sharing behavior, and we plan to examine them in more detail as our work continues. Weber and Gupta attempt to link the observed invalidation patterns back to high-level applications program objects <ref> [18] </ref>. They distinguished several distinct types of shared data objects: Code and read-only, Mostly-read, Migratory, Synchronization, and Frequently read/written. The first four of their categories have corresponding categories in our classification and are handled similarly.
References-found: 18

