URL: http://www.cs.umn.edu/Research/Agassiz/Paper/cho.icpp98.ps.gz
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Title: High-Level Information An Approach for Integrating Front-End and Back-End Compilers  
Author: Sangyeun Cho, Jenn-Yuan Tsai Yonghong Song Bixia Zheng, Stephen J. Schwinn Xin Wang, Qing Zhao, Zhiyuan Li David J. Lilja and Pen-Chung Yew 
Web: http://www.cs.umn.edu/Research/Agassiz  
Address: Minneapolis, MN 55455 Urbana, IL 61801 West Lafayette, IN 47907 Minneapolis, MN 55455  
Affiliation: Dept. of Comp. Sci. and Eng. Dept. of Comp. Sci. Dept. of Comp. Sci Dept. of Elec. and Comp. Eng. Univ. of Minnesota Univ. of Illinois Purdue University Univ. of Minnesota  
Abstract: We propose a new universal High-Level Information (HLI) format to effectively integrate front-end and back-end compilers by passing front-end information to the back-end compiler. Importing this information into an existing back-end leverages the state-of-the-art analysis and transformation capabilities of existing front-end compilers to allow the back-end greater optimization potential than it has when relying on only locally-extracted information. A version of the HLI has been implemented in the SUIF parallelizing compiler and the GCC back-end compiler. Experimental results with the SPEC benchmarks show that HLI can provide GCC with substantially more accurate data dependence information than it can obtain on its own. Our results show that the number of dependence edges in GCC can be reduced by an average of 48% for the integer benchmark programs and an average of 54% for the floating-point benchmark programs studied, which provides greater flexibility to GCC's code scheduling pass. Even with the scheduling optimization limited to basic blocks, the use of HLI produces moderate speedups compared to using only GCC's dependence tests when the optimized programs are executed on MIPS R4600 and R10000 processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic Translation of FORTRAN Programs to Vector Form, </title> <journal> ACM Trans. on Prog. Lang. and Sys., </journal> <volume> 9(4): 491 542, </volume> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: Based on the results, a sequential program is transformed into a parallel program containing program constructs such as DOALL. Alternatively, the compiler may insert a directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [14], PFC <ref> [1] </ref>, Parafrase-2 [18], Po-laris [3], Panorama [11], and PTRAN [19], and commercial Fortran compilers, such as KAP [13] and VAST [25], have taken such a source-to-source approach.
Reference: [2] <author> E. Ayguade et al. </author> <title> A Uniform Internal Representation for High-Level and Instruction-Level Transformations, </title> <type> TR 1434, </type> <institution> CSRD, Univ. of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: It also maintains a low-level intermediate representation that is close to the machine code. As another example, the Polaris parallelizing compiler has recently incorporated a low-level representation to enable low-level compiler techniques <ref> [2] </ref>. Nonetheless, results showing how high-level analysis benefits the low-level analysis and optimizations are largely unavailable today.
Reference: [3] <author> W. Blume et al. </author> <title> Parallel Programming with Polaris, </title> <journal> IEEE Computer, pp. </journal> <volume> 78 82, </volume> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: Alternatively, the compiler may insert a directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [14], PFC [1], Parafrase-2 [18], Po-laris <ref> [3] </ref>, Panorama [11], and PTRAN [19], and commercial Fortran compilers, such as KAP [13] and VAST [25], have taken such a source-to-source approach.
Reference: [4] <author> S. Cho, J.-Y. Tsai, Y. Song, B. Zheng, S. J. Schwinn, X. Wang, Q. Zhao, Z. Li, D. J. Lilja, and P.-C. Yew. </author> <title> High-Level Information An Approach for Integrating Front-End and Back-End Compilers, </title> <type> TR #98-008, </type> <institution> Dept. of Computer Sci. and Eng., Univ. of Minnesota, </institution> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: Because it is both back-end compiler and machine dependent, separating the HLI generation into these two phases allows us to reuse the code for TBLCONST across different back-end compilers or target machines. 2 Readers are referred to <ref> [4] </ref> for a more complete description. 3 RTL (Register Transfer Language) is an intermediate representation used by GCC that resembles Lisp lists [22].
Reference: [5] <author> S. Cho and Y. Song. </author> <title> The HLI Implementor's Guide (v0.1), Agassiz Project Internal Document, </title> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: To provide a common interface across different back-ends, the stored HLI can be retrieved only via a set of query functions. There are five basic query functions that can be used to construct more complex query functions <ref> [5] </ref>. There are another set of utility functions that simplify the implementation of the query and maintenance functions (Section 3.2.3) by hiding the low-level details of the target compiler. Two examples are given in this section to show how the query functions can be used in GCC. <p> The entire HLI components (tables) must be reconstructed using old infor mation, and the old information must be discarded. loop unrolling. The HLI maintenance functions have been written to provide a means to update the HLI in response to these changes <ref> [5] </ref>. The functions allow a back-end compiler to generate or delete items, inherit the attributes of one item to another, insert an item into a region, and update the HLI tables.
Reference: [6] <author> F. C. Chow. </author> <title> A Portable Machine-Independent Global Optimizer Design and Measurements, </title> <type> Ph.D. Thesis, </type> <institution> Stanford Univ., </institution> <month> Dec. </month> <year> 1983. </year>
Reference-contexts: Once the thread assignment to individual processors has been determined, par-allelizing compilers have little control over the execution of the code by each processor. Over the past years, both machine independent and machine specific compiler techniques have been developed to enhance the performance of uniprocessors <ref> [17, 6, 12, 7, 16] </ref>. These compiler techniques rely primarily on dataflow analysis for symbolic registers or simple scalars that are not aliased. Advanced data dependence analysis and data flow analysis regarding array references and pointer dereferences are generally not available to current uniprocessor compilers.
Reference: [7] <author> J. C. Dehnert and R. A. Towle. </author> <title> Compiling for the Cydra 5, </title> <journal> J. of Supercomputing, </journal> <volume> 7(1/2): 181 227, </volume> <year> 1993. </year>
Reference-contexts: Furthermore, the scope of the optimizations the back-end can perform, such as improving instruction issuing rates through architecture-aware code scheduling <ref> [7, 10, 12, 16] </ref>, is limited to only short-range, local transformations. Another consequence of this split is that it is not uncommon for transformations performed in the front-end to be ignored, or even undone, in the back-end. <p> Once the thread assignment to individual processors has been determined, par-allelizing compilers have little control over the execution of the code by each processor. Over the past years, both machine independent and machine specific compiler techniques have been developed to enhance the performance of uniprocessors <ref> [17, 6, 12, 7, 16] </ref>. These compiler techniques rely primarily on dataflow analysis for symbolic registers or simple scalars that are not aliased. Advanced data dependence analysis and data flow analysis regarding array references and pointer dereferences are generally not available to current uniprocessor compilers.
Reference: [8] <author> M. Emami, R. Ghiya and L. J. Hendren. </author> <title> Context-Sensitive Interprocedural Points-to Analysis in the Presence of Function Pointers, </title> <booktitle> Proc. of the ACM SIGPLAN `94 Conf. on PLDI, </booktitle> <pages> pp. 242 256, </pages> <month> June </month> <year> 1994. </year> <note> 5 See http://www.cs.umn.edu/Research/Agassiz/. </note>
Reference-contexts: With the increased demand for ILP, the importance of incorporating high-level analysis into uniprocessor compilers has been generally recognized. Recent work on pointer and structure analysis aims at accurate recognition of aliases due to pointer dereferences and pointer arguments <ref> [8, 27] </ref>. Experimental results in this area have been limited to reporting the accuracy of recognizing aliases. Compared with these studies, this paper presents new data showing how high-level array and pointer analysis can improve data dependence analysis in a common uniprocessor compiler.
Reference: [9] <author> C. Fraser and D. Hanson. </author> <title> A Retargetable C Compiler: Design and Implementation, </title> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, CA, </address> <year> 1995. </year>
Reference-contexts: These compiler techniques rely primarily on dataflow analysis for symbolic registers or simple scalars that are not aliased. Advanced data dependence analysis and data flow analysis regarding array references and pointer dereferences are generally not available to current uniprocessor compilers. The publically available GCC [22] and LCC <ref> [9] </ref> compilers exemplify the situation. They both maintain low-level IRs of the input programs, keeping no high-level program constructs for array data dependence and pointer-structure analysis. With the increased demand for ILP, the importance of incorporating high-level analysis into uniprocessor compilers has been generally recognized.
Reference: [10] <author> J. R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: Furthermore, the scope of the optimizations the back-end can perform, such as improving instruction issuing rates through architecture-aware code scheduling <ref> [7, 10, 12, 16] </ref>, is limited to only short-range, local transformations. Another consequence of this split is that it is not uncommon for transformations performed in the front-end to be ignored, or even undone, in the back-end.
Reference: [11] <author> J. Gu, Z. Li, and G. Lee. </author> <title> Experience with Efficient Array Data Flow Analysis for Array Privatization, </title> <booktitle> Proc. of the 6th ACM SIGPLAN Symp. on PPOPP, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: The back-end, however, needs machine details to perform its finer-grained optimizations. The penalty for this split, though, is that the back-end misses potential optimization opportunities due to its lack of high-level information. For example, optimizations involving loops, such as scalar promotion or array privati-zation <ref> [11] </ref>, are difficult to perform in the back-end since complex loop structures can be difficult to identify with the limited information available in the back-end, especially for nested loops. <p> Alternatively, the compiler may insert a directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [14], PFC [1], Parafrase-2 [18], Po-laris [3], Panorama <ref> [11] </ref>, and PTRAN [19], and commercial Fortran compilers, such as KAP [13] and VAST [25], have taken such a source-to-source approach.
Reference: [12] <author> W. W. Hwu et al. </author> <title> The Superblock: An Effective Technique for VLIW and Superscalar Compilation, </title> <journal> J. of Supercomputing, </journal> <volume> 7(1/2): 229 248, </volume> <year> 1993. </year>
Reference-contexts: Furthermore, the scope of the optimizations the back-end can perform, such as improving instruction issuing rates through architecture-aware code scheduling <ref> [7, 10, 12, 16] </ref>, is limited to only short-range, local transformations. Another consequence of this split is that it is not uncommon for transformations performed in the front-end to be ignored, or even undone, in the back-end. <p> Once the thread assignment to individual processors has been determined, par-allelizing compilers have little control over the execution of the code by each processor. Over the past years, both machine independent and machine specific compiler techniques have been developed to enhance the performance of uniprocessors <ref> [17, 6, 12, 7, 16] </ref>. These compiler techniques rely primarily on dataflow analysis for symbolic registers or simple scalars that are not aliased. Advanced data dependence analysis and data flow analysis regarding array references and pointer dereferences are generally not available to current uniprocessor compilers.
Reference: [13] <institution> KAP User's Guide, </institution> <type> Tech. Report (Doc. No. 8811002), </type> <institution> Kuck & Associates, Inc. </institution>
Reference-contexts: Alternatively, the compiler may insert a directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [14], PFC [1], Parafrase-2 [18], Po-laris [3], Panorama [11], and PTRAN [19], and commercial Fortran compilers, such as KAP <ref> [13] </ref> and VAST [25], have taken such a source-to-source approach. Computer vendors generally provide their own compilers to take a source program, which has been parallelized by programmers or by a parallelizing compiler, and generate multithreaded machine code, i.e., machine code embedded with thread library calls.
Reference: [14] <author> D. J. Kuck et al. </author> <title> The Structure of an Advanced Vectorizer for Pipelined Processors, </title> <booktitle> Proc. of the 4th Int'l Computer Software and Application Conf., </booktitle> <pages> pp. 709 715, </pages> <month> Oct. </month> <year> 1980. </year>
Reference-contexts: Based on the results, a sequential program is transformed into a parallel program containing program constructs such as DOALL. Alternatively, the compiler may insert a directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase <ref> [14] </ref>, PFC [1], Parafrase-2 [18], Po-laris [3], Panorama [11], and PTRAN [19], and commercial Fortran compilers, such as KAP [13] and VAST [25], have taken such a source-to-source approach.
Reference: [15] <author> M. Lam. </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines, </title> <booktitle> Proc. of the ACM SIGPLAN '88 Conf. on PLDI, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: Accurate data dependence information allows aggressive scheduling of a memory reference across other memory references, for example. Additionally, LCDD information is indispensable for a cyclic scheduling algorithm such as software pipelining <ref> [15] </ref>. In loop invariant code removal, a memory reference can be moved out of a loop only when there remains no other memory reference in the loop that can possibly alias the memory reference.
Reference: [16] <author> P. G. Lowney et al. </author> <title> The Multiflow Trace Scheduling Compiler, </title> <journal> J. of Supercomputing, </journal> <volume> 7(1/2): 51 142, </volume> <year> 1993. </year>
Reference-contexts: Furthermore, the scope of the optimizations the back-end can perform, such as improving instruction issuing rates through architecture-aware code scheduling <ref> [7, 10, 12, 16] </ref>, is limited to only short-range, local transformations. Another consequence of this split is that it is not uncommon for transformations performed in the front-end to be ignored, or even undone, in the back-end. <p> Once the thread assignment to individual processors has been determined, par-allelizing compilers have little control over the execution of the code by each processor. Over the past years, both machine independent and machine specific compiler techniques have been developed to enhance the performance of uniprocessors <ref> [17, 6, 12, 7, 16] </ref>. These compiler techniques rely primarily on dataflow analysis for symbolic registers or simple scalars that are not aliased. Advanced data dependence analysis and data flow analysis regarding array references and pointer dereferences are generally not available to current uniprocessor compilers.
Reference: [17] <author> S. S. Muchnick. </author> <title> Advanced Compiler Design and Implementation, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1997. </year>
Reference-contexts: Once the thread assignment to individual processors has been determined, par-allelizing compilers have little control over the execution of the code by each processor. Over the past years, both machine independent and machine specific compiler techniques have been developed to enhance the performance of uniprocessors <ref> [17, 6, 12, 7, 16] </ref>. These compiler techniques rely primarily on dataflow analysis for symbolic registers or simple scalars that are not aliased. Advanced data dependence analysis and data flow analysis regarding array references and pointer dereferences are generally not available to current uniprocessor compilers.
Reference: [18] <author> C. D. Polychronopoulos et al. </author> <title> Parafrase-2: An Environment for Parallelizing, Partitioning, Synchronizing and Scheduling Programs on Multiprocessors, </title> <booktitle> Proc. of the ICPP, </booktitle> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: Alternatively, the compiler may insert a directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [14], PFC [1], Parafrase-2 <ref> [18] </ref>, Po-laris [3], Panorama [11], and PTRAN [19], and commercial Fortran compilers, such as KAP [13] and VAST [25], have taken such a source-to-source approach.
Reference: [19] <author> V. Sarkar. </author> <title> The PTRAN Parallel Programming System, Parallel Functional Programming Languages and Compilers, </title> <editor> B. Szymanski, Ed., </editor> <publisher> ACM Press, </publisher> <pages> pp. 309 391, </pages> <year> 1991. </year>
Reference-contexts: Alternatively, the compiler may insert a directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [14], PFC [1], Parafrase-2 [18], Po-laris [3], Panorama [11], and PTRAN <ref> [19] </ref>, and commercial Fortran compilers, such as KAP [13] and VAST [25], have taken such a source-to-source approach.
Reference: [20] <author> V. Sarkar. </author> <title> Automatic Selection of High-Order Transformations in the IBM XL FORTRAN Compilers, </title> <journal> IBM J. of Research and Development, </journal> <volume> 41(3): 233 264, </volume> <month> May </month> <year> 1997. </year>
Reference-contexts: There have been continued efforts to incorporate uniprocessor parameters and knowledge about low-level code generation strategies into the high-level decisions about program transformations. The ASTI optimizer for the IBM XL Fortran compilers <ref> [20] </ref> is a good example. Nonetheless, the register allocator and instruction scheduler of the uniprocessor compiler still lacks direct information about data dependences concerning complex memory references. New efforts on integrating parallelizing compilers with uniprocessor compilers also have emerged recently.
Reference: [21] <author> S. J. Schwinn. </author> <title> The HLI Interface Specification for Back-End Compilers (v0.1), Agassiz Project Internal Document, </title> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: This section discusses some of the implementation details. 2 Note, however, that the HLI format is platform-independent, and many of the implemented functions are portable to other compilers <ref> [21] </ref>. Figure 3 shows an overview of our HLI implementation in the SUIF compiler and GCC. 3.1 Front-end implementation The HLI generation in the front-end contains two major phases memory access item generation (ITEMGEN) and HLI table construction (TBLCONST).
Reference: [22] <author> R. M. Stallman. </author> <title> Using and Porting GNU CC (version 2.7), Free Software Foundation, </title> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Section 3 then describes our implementation of this HLI into the SUIF front-end paral-lelizing compiler [26] and the GCC back-end optimizing compiler <ref> [22] </ref>. Experiments with the SPEC benchmark programs [23] and a GNU utility are presented in Section 4. <p> the front-end can pass interprocedural data-flow information to the back-end to enable the back-end to move instructions around a function call, for instance. 3 Implementation Issues A version of the HLI described in the previous section has been implemented in the SUIF parallelizing compiler [26] and the GCC back-end compiler <ref> [22] </ref>. This section discusses some of the implementation details. 2 Note, however, that the HLI format is platform-independent, and many of the implemented functions are portable to other compilers [21]. <p> separating the HLI generation into these two phases allows us to reuse the code for TBLCONST across different back-end compilers or target machines. 2 Readers are referred to [4] for a more complete description. 3 RTL (Register Transfer Language) is an intermediate representation used by GCC that resembles Lisp lists <ref> [22] </ref>. An RTL chain is the linked list of low-level instructions in the RTL format. 3.1.1 Memory access item generation (ITEMGEN) The ITEMGEN phase traverses the SUIF internal representation (IR) to generate memory access items. <p> These compiler techniques rely primarily on dataflow analysis for symbolic registers or simple scalars that are not aliased. Advanced data dependence analysis and data flow analysis regarding array references and pointer dereferences are generally not available to current uniprocessor compilers. The publically available GCC <ref> [22] </ref> and LCC [9] compilers exemplify the situation. They both maintain low-level IRs of the input programs, keeping no high-level program constructs for array data dependence and pointer-structure analysis. With the increased demand for ILP, the importance of incorporating high-level analysis into uniprocessor compilers has been generally recognized.
Reference: [23] <institution> The Standard Performance Evaluation Corporation, </institution> <note> http://www.specbench.org. </note>
Reference-contexts: Section 3 then describes our implementation of this HLI into the SUIF front-end paral-lelizing compiler [26] and the GCC back-end optimizing compiler [22]. Experiments with the SPEC benchmark programs <ref> [23] </ref> and a GNU utility are presented in Section 4.
Reference: [24] <author> J.-Y. Tsai. </author> <title> High-Level Information Format for Integrating Front-End and Back-End Compilers (v0.2), Agassiz Project Internal Document, </title> <month> March </month> <year> 1997. </year>
Reference-contexts: Related work is discussed in Section 5, with our results and conclusions summarized in Section 6. 2 High-Level Information Definition A High-Level Information (HLI) file for a program includes information that is important for back-end optimizations, but is only available or computable in the front-end <ref> [24] </ref>. As shown in Figure 1, an HLI file contains a number of HLI entries.
Reference: [25] <institution> VAST-2 for XL FORTRAN, </institution> <note> User's Guide, Edition 1.2, Tech. Report (Doc. No. </note> <institution> VA061), Pacific-Sierra Research Co., </institution> <year> 1994. </year>
Reference-contexts: Several research parallelizing Fortran compilers, including Parafrase [14], PFC [1], Parafrase-2 [18], Po-laris [3], Panorama [11], and PTRAN [19], and commercial Fortran compilers, such as KAP [13] and VAST <ref> [25] </ref>, have taken such a source-to-source approach. Computer vendors generally provide their own compilers to take a source program, which has been parallelized by programmers or by a parallelizing compiler, and generate multithreaded machine code, i.e., machine code embedded with thread library calls.
Reference: [26] <author> R. P. Wilson et al. </author> <title> SUIF: An Infrastructure for Research on Parallelizing and Optimizing Compilers, </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29 (12): 31 37, </volume> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: Section 3 then describes our implementation of this HLI into the SUIF front-end paral-lelizing compiler <ref> [26] </ref> and the GCC back-end optimizing compiler [22]. Experiments with the SPEC benchmark programs [23] and a GNU utility are presented in Section 4. <p> With this table, the front-end can pass interprocedural data-flow information to the back-end to enable the back-end to move instructions around a function call, for instance. 3 Implementation Issues A version of the HLI described in the previous section has been implemented in the SUIF parallelizing compiler <ref> [26] </ref> and the GCC back-end compiler [22]. This section discusses some of the implementation details. 2 Note, however, that the HLI format is platform-independent, and many of the implemented functions are portable to other compilers [21]. <p> Nonetheless, the register allocator and instruction scheduler of the uniprocessor compiler still lacks direct information about data dependences concerning complex memory references. New efforts on integrating parallelizing compilers with uniprocessor compilers also have emerged recently. The SUIF tool <ref> [26] </ref>, for instance, maintains a high-level intermediate representation that is close to the source program to support high-level analysis and transformations. It also maintains a low-level intermediate representation that is close to the machine code.
Reference: [27] <author> R. P. Wilson and M. S. Lam. </author> <title> Efficient Context-Sensitive Pointer Analysis for C Programs, </title> <booktitle> Proc. of the ACM SIG-PLAN `95 Conf. on PLDI, </booktitle> <pages> pp. 1 12, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: With the increased demand for ILP, the importance of incorporating high-level analysis into uniprocessor compilers has been generally recognized. Recent work on pointer and structure analysis aims at accurate recognition of aliases due to pointer dereferences and pointer arguments <ref> [8, 27] </ref>. Experimental results in this area have been limited to reporting the accuracy of recognizing aliases. Compared with these studies, this paper presents new data showing how high-level array and pointer analysis can improve data dependence analysis in a common uniprocessor compiler.
References-found: 27

