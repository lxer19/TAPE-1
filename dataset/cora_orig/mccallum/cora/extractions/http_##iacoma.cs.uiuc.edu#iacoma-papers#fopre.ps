URL: http://iacoma.cs.uiuc.edu/iacoma-papers/fopre.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: dkoufaty@ichips.intel.com torrella@cs.uiuc.edu  
Title: Comparing Data Forwarding and Prefetching for Communication-Induced Misses in Shared-Memory MPs 1  
Author: David Koufaty and Josep Torrellas 
Web: http://iacoma.cs.uiuc.edu  
Address: IL 61801  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign,  
Abstract: As the difference in speed between processor and memory system continues to increase, it is becoming crucial to develop and refine techniques that enhance the effectiveness of cache hierarchies. Two such techniques are data prefetching and data forwarding. With prefetching, a processor hides the latency of cache misses by requesting the data before it actually needs it. With forwarding, a producer processor hides the latency of communication-induced cache misses in the consumer processors by sending the data to the caches of the latter. These two techniques are complementary approaches to hiding the latency of communication-induced misses. This paper compares the effectiveness of data forwarding and data prefetching to hide communication-induced misses. Although both techniques require comparable hardware support, forwarding usually has a lower instruction overhead. We evaluate prefetching and forwarding algorithms in a paralleliz-ing compiler using execution-driven simulations of a shared-memory multiprocessor. Both data forwarding and prefetch-ing reduce the execution time of applications significantly (30-40% on average). Forwarding performs better on average, while prefetching is more robust to changes in cache and memory parameters. Finally, we propose two ways of integrating the two techniques. The integration of the two techniques reduces the execution time even more (43-48% on average) and is very robust. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Abdel-Shafi, J. Hall, S. V. Adve, and V. Adve. </author> <title> An Evaluation of Fine-Grain Producer-Initiated Communication in Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on High-Performance Computer Architecture, </booktitle> <pages> pages 204-215, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: To tolerate this communication, popularly known as data sharing, techniques that overlap processor computation with the communication have been proposed. Two of these techniques are data forwarding and data prefetching. In data forwarding <ref> [1, 8, 10] </ref>, producer processors send the data to the caches of the processors that will use the data 1 This work was supported in part by the National Science Foundation under grants NSF Young Investigator Award MIP-9457436, ASC-9612099 and MIP-9619351, DARPA Contract DABT63-95-C-0097, NASA Contract NAG-1-613 and gifts from Intel
Reference: [2] <author> D. H. Bailey and J. T. Barton. </author> <title> The NAS Kernel Benchmark Program. </title> <type> Technical report, </type> <institution> Numerial Aerodynamic Simulations Systems Division, NASA Ames Research Center, </institution> <month> June </month> <year> 1986. </year>
Reference-contexts: Prefetching, however, always brings the data to the first-level cache. To drive the simulations, we use six Polaris-parallelized Fortran applications from a wide variety of sources to cover as many cases as possible: emit and gmtry from the NAS benchmark set <ref> [2] </ref>, swim and tomcatv from SPEC95, fft from the CMU suite [5], and lu, a custom-developed LU factorization. For the SPEC95 applications, we use the test data set. The test and the reference data sets differ only in the number of iterations executed, not in the data set size.
Reference: [3] <author> W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoe-flinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, B. Pottenger, L. Rauchwerger, and P. Tu. </author> <title> Parallel Programming with Po-laris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: In this paper, we compare the effectiveness of data forwarding and software-controlled data prefetching to hide communication-induced misses. We study full-scale implementations of both techniques in the Polaris parallelizing compiler <ref> [3] </ref>. We evaluate scenarios with different cache and memory parameters. Based on this comparison, we propose integrated approaches that combine data forwarding and data prefetching to deliver the highest performance and robustness. <p> Data prefetches that would overflow the prefetch buffer are discarded. 3 Compiler Support To insert the forwarding and prefetching primitives automatically in realistic codes, we have taken the Polaris parallelizing compiler <ref> [3] </ref> and added a pass. Polaris takes a Fortran 77 program as input and generates a transformed Fortran program annotated with directives. These directives specify the parallelism in the code and are used to run the code efficiently on a parallel computer. The model of parallelism in Polaris is loop-based.
Reference: [4] <author> T. F. Chen and J. L. Baer. </author> <title> A Performance Study of Software and Hardware Data Prefetching Schemes. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 223-232, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This is true even if multiple processors will consume the data. Finally, forwarding minimizes the use of network and directory resources, since the producer simply transfers the data to the consumer in one transaction. In data prefetching <ref> [4, 9, 11] </ref>, processors request in advance data that they will use later. This triggers a fetch to the memory system that brings the data to the local caches. Previous work by many researchers has conclusively shown that prefetching is highly effective.
Reference: [5] <author> P. Dinda, T. Gross, D. O'Hallaron, E. Segall, J. Stichnoth, J. Subhlok, J. Webb, and B. Yang. </author> <title> The CMU Task Parallel Program Suite. </title> <type> Technical Report CMU-CS-94-131, </type> <institution> Carnagie Mellon University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: To drive the simulations, we use six Polaris-parallelized Fortran applications from a wide variety of sources to cover as many cases as possible: emit and gmtry from the NAS benchmark set [2], swim and tomcatv from SPEC95, fft from the CMU suite <ref> [5] </ref>, and lu, a custom-developed LU factorization. For the SPEC95 applications, we use the test data set. The test and the reference data sets differ only in the number of iterations executed, not in the data set size.
Reference: [6] <author> S. Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Compiler Support Forwarding requires more sophisticated com piler support. Hardware Support Both strategies require comparable hardware support. Overhead Both strategies can cause cache conflicts and have instruction overhead. Data forwarding often has less instruction overhead. 5 Simulation Environment To evaluate our compiler algorithm, we use TangoLite <ref> [6] </ref> execution-driven simulations of applications parallelized by Polaris and annotated by our forwarding and prefetching passes. The architecture model that we simulate is a scalable Flat COMA multiprocessor [12] with 32 processors cycling at 300 MHz. Each processor has a two-level cache hierarchy.
Reference: [7] <author> D. Koufaty. </author> <title> Architectural and Compiler Support to Hide Coherence Misses in Distributed Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana Champaign, </institution> <month> October </month> <year> 1997. </year>
Reference-contexts: Data forwarding is supported with a four-primitive framework. The choice of the latter is based on the insight that we gained from doing a complete data forwarding compiler implementation <ref> [7] </ref>. We now describe each of the primitives. Prefetch. It is a non-blocking data read. If the data is found in the local caches, the instruction completes. Otherwise, the data is requested from the right directory and is finally deposited in the local caches. Write-Forward. <p> Second, a Polaris pass is used to identify the array sections that are read and those that are written in each node. Finally, data flow analysis is used to compute the set of definitions that reach each node of the graph. The details of this algorithm are described in <ref> [7] </ref>. The reaching definitions are then intersected with the array sections that each node reads to generated a set of potential producer-consumer pairs. Each pair is composed of a write and a read reference in the source code that produce and consume a given array section, respectively. <p> The criteria used includes the location of the consumer read (parallel or serial section), complexity of the consumer expression, number of consumer expressions, independence of the consumer expression from the enclosing parallel loop and ability of the compiler to analyze both expressions <ref> [7] </ref>. Before generating the final code, the compiler must handle the case where an access belongs to several producer-consumer pairs. Figure 2-(a) shows the two possible cases. A consumer read that belongs to several producer-consumer pairs poses no problem: each producer write is handled separately. <p> ENDDO ENDDO DOALL j = 2, n-1, 1 DO k = 0, 7 DIR$ PREFETCH (rx (2+k,j)) ENDDO DO i = 2, n-1, 1 DIR$ PREFETCH (rx (i+8,j)) ... = rx (i, j) ... ENDDO ENDDO (b) data prefetching (b). useful. The compiler algorithms are described in detail in <ref> [7] </ref>. Figure 3-(a) shows the example code segment after data forwarding has been added. <p> These two cache sizes and their miss rates are shown in Table 3. For a given application, we find these sizes by measuring the change in the miss rate as we vary the size of the cache <ref> [7] </ref>. Table 3: Applications studied. <p> We qualitatively assign priorities to the factors in the following order: serial sections, instrumentation, conflict misses, data size, distance and multicast limits. The exact details of the scheme used can be found in <ref> [7] </ref>. 7.3 Performance Results naive combining (C-Naive) and the base architecture with smart combining (C-Smart). Let us focus first on C-Naive. Naive combining is able to reduce the execution time of four applications by 2%-8% over the previous best case, either prefetching or forwarding alone.
Reference: [8] <author> D. Koufaty, X. Chen, D. Poulsen, and J. Torrellas. </author> <title> Data Forwarding in Scalable Shared-Memory Multiprocessors. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 7(12) </volume> <pages> 1250-1264, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: To tolerate this communication, popularly known as data sharing, techniques that overlap processor computation with the communication have been proposed. Two of these techniques are data forwarding and data prefetching. In data forwarding <ref> [1, 8, 10] </ref>, producer processors send the data to the caches of the processors that will use the data 1 This work was supported in part by the National Science Foundation under grants NSF Young Investigator Award MIP-9457436, ASC-9612099 and MIP-9619351, DARPA Contract DABT63-95-C-0097, NASA Contract NAG-1-613 and gifts from Intel <p> In our analysis, we focus only on accesses to shared arrays. This is because scalars have very little weight in the codes considered. In addition, since it has been suggested that it is necessary to analyze the codes interprocedurally <ref> [8] </ref>, and Polaris lacks good interprocedural analysis, we inline the programs. First, we build a data flow graph (Figure 1). Each node is a serial or a parallel section of the program and edges correspond to the flow between one section and another.
Reference: [9] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: This is true even if multiple processors will consume the data. Finally, forwarding minimizes the use of network and directory resources, since the producer simply transfers the data to the consumer in one transaction. In data prefetching <ref> [4, 9, 11] </ref>, processors request in advance data that they will use later. This triggers a fetch to the memory system that brings the data to the local caches. Previous work by many researchers has conclusively shown that prefetching is highly effective. <p> Once a regular read or write reaches the directory entry, the U bit is reset. 2.3 Data Prefetching The microarchitecture supports non-binding data prefetch-ing <ref> [9] </ref>. The prefetched data remains in the processor's cache until it is referenced. If, in the meantime, another processor writes to the data, the line is invalidated. This frees the compiler from the task of preserving correctness. Binding prefetches are not considered is this study. <p> For each producer-consumer pair, we mark the corresponding read to be prefetched. After all the reads have been marked, we proceed to schedule the prefetches for each reference. We schedule the prefetches using software-pipelining <ref> [9] </ref>. Prefetches must be issued early enough to hide the latency of the accesses. We use an estimate of the number of cycles needed to execute one loop iteration, time.
Reference: [10] <author> D. K. Poulsen and P.-C. Yew. </author> <title> Data Prefetching and Data Forwarding in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 276-280, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: To tolerate this communication, popularly known as data sharing, techniques that overlap processor computation with the communication have been proposed. Two of these techniques are data forwarding and data prefetching. In data forwarding <ref> [1, 8, 10] </ref>, producer processors send the data to the caches of the processors that will use the data 1 This work was supported in part by the National Science Foundation under grants NSF Young Investigator Award MIP-9457436, ASC-9612099 and MIP-9619351, DARPA Contract DABT63-95-C-0097, NASA Contract NAG-1-613 and gifts from Intel
Reference: [11] <author> V. Santhanam, E. Gornish, and W.-C. Hsu. </author> <title> Data Prefetching on the HP PA-8000. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 264-273, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: This is true even if multiple processors will consume the data. Finally, forwarding minimizes the use of network and directory resources, since the producer simply transfers the data to the consumer in one transaction. In data prefetching <ref> [4, 9, 11] </ref>, processors request in advance data that they will use later. This triggers a fetch to the memory system that brings the data to the local caches. Previous work by many researchers has conclusively shown that prefetching is highly effective.
Reference: [12] <author> P. Stenstrom, T. Joe, and A. Gupta. </author> <title> Comparative Performance Evaluation of Cache-Coherent NUMA and COMA Architectures. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Data forwarding often has less instruction overhead. 5 Simulation Environment To evaluate our compiler algorithm, we use TangoLite [6] execution-driven simulations of applications parallelized by Polaris and annotated by our forwarding and prefetching passes. The architecture model that we simulate is a scalable Flat COMA multiprocessor <ref> [12] </ref> with 32 processors cycling at 300 MHz. Each processor has a two-level cache hierarchy. Table 2 shows the latencies for a round trip to different levels of the memory hierarchy. To schedule prefetches, the memory latency is taken to be 500 cycles.
References-found: 12

