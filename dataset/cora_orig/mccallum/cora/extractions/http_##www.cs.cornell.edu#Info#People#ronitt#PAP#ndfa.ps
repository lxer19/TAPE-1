URL: http://www.cs.cornell.edu/Info/People/ronitt/PAP/ndfa.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/ronitt/papers.html
Root-URL: http://www.cs.brown.edu/
Email: danar@cs.huji.ac.il  RONITT RUBINFELD ronitt@cs.cornell.edu  
Title: Learning Fallible Deterministic Finite Automata Keywords: PAC Learning under the Uniform Distribution, Persistent Errors, Fallible
Author: DANA RON Editor: Sally A. Goldman 
Address: Jerusalem 91904, Israel  Ithaca, NY 14853, U.S.A.  
Affiliation: Computer Science Institute, Hebrew University,  Computer Science Department, Cornell University,  
Note: 1-38 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> D. Angluin. </author> <year> (1981). </year> <title> A note on the number of queries needed to identify regular languages. </title> <journal> Information and Control, </journal> <pages> 51 , 76-87. </pages>
Reference-contexts: Related Results In the error free case of learning DFAs, Kearns and Valiant [24] use the prediction preserving reductions of Pitt and Warmuth [28] to show that under cryptographic assumptions, the problem of predicting the class of DFAs is hard when only given access to random examples. Angluin <ref> [1] </ref> shows that (exact) learning using only membership queries is also hard. She describes a family of automata that cannot be identified in less than exponential time when the learner can only observe the behavior of the machine on inputs of the learner's own choosing. <p> We would also like to thank the referees of this paper for their careful reading and very helpful comments. 36 DANA RON AND RONITT RUBINFELD Notes 1. [11] actually mention that the adversary argument in <ref> [1] </ref> showing that DFAs cannot be learned using membership queries only can be modified so that the target DFAs all have distinguishing sequences. This implies that even in the error-free case one cannot hope to find a distinguishing sequence in polynomial time by exploration only.
Reference: 2. <author> D. Angluin. </author> <year> (1987). </year> <title> Learning regular sets from queries and counterexamples. </title> <booktitle> Information and Computation, </booktitle> <pages> 75 87-106. </pages>
Reference-contexts: Specifically, we consider the problem in which the true target concept is a Deterministic Finite Automaton (DFA). Angluin and Laird [3] propose to explore the effect of noise in the case of queries, and specifically ask if Angluin's algorithm <ref> [2] </ref> for learning DFAs in the error-free case can be modified to handle errors both in the answers to the queries and in the random examples. We answer this question by presenting a polynomial time algorithm for learning fallible DFAs under the uniform distribution on inputs. <p> Using additional properties of the partition, we show how to correct an arbitrarily large fraction of the expert's errors and thus receive a more refined labeled partition on which we base the construction of our hypothesis automaton. Parts of our algorithm rely on a version of Angluin's algorithm <ref> [2] </ref> for learning finite automata in the error-free case. This version is presented preceding the description of our algorithm. 2. <p> She describes a family of automata that cannot be identified in less than exponential time when the learner can only observe the behavior of the machine on inputs of the learner's own choosing. However, as mentioned earlier, Angluin <ref> [2] </ref> describes an algorithm for learning DFAs given access both to random examples and to membership queries. Rivest and Schapire [30], [31], [33] present algorithms for inferring DFAs from input/output behavior in the absence of a means of resetting the machine to a start state. <p> Learning automata from an infallible expert In this section we give a version of Angluin's algorithm for PAC learning deterministic finite automata, given access to random labeled examples (distributed according to an arbitrary distribution), and membership queries <ref> [2] </ref>. We assume all examples and queried strings are labeled correctly. In this version the learning algorithm is given an upper bound n b on n, the number of states in the target automaton.
Reference: 3. <author> D. Angluin and P. Laird. </author> <year> (1988). </year> <title> Learning from noisy examples. </title> <booktitle> Machine Learning, </booktitle> <pages> 2 , 343-370. </pages>
Reference-contexts: Specifically, we consider the problem in which the true target concept is a Deterministic Finite Automaton (DFA). Angluin and Laird <ref> [3] </ref> propose to explore the effect of noise in the case of queries, and specifically ask if Angluin's algorithm [2] for learning DFAs in the error-free case can be modified to handle errors both in the answers to the queries and in the random examples. <p> Several results have been obtained for learning in the presence of errors in the Probably Approximately Correct model introduced by Valiant [38]. These include results for learning in the presence of malicious and random noise in classification and attributes [39], <ref> [3] </ref>, [25], [22], [20], [34], [35], [37], [36]. In recent work Kearns [21] identifies and formalizes a sufficient condition on learning algorithms in Valiant's model that permits the immediate derivation of noise-tolerant learning algorithms.
Reference: 4. <author> D. Angluin, M. Frazier and L. Pitt. </author> <year> (1992). </year> <title> Learning conjunctions of Horn Clauses. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 147-164. </pages>
Reference-contexts: Further Research An interesting direction for further research is to try and modify additional PAC learning algorithms that use membership queries to the case in which the queries might be answered erroneously. Such algorithms exist for learning monotone DNF [38], read-once Boolean functions [5], Horn Clauses <ref> [4] </ref>, among others. A more general direction that can be pursued is to study the possible relationship between learning from fallible experts and the area of self-correcting [9], [26].
Reference: 5. <author> D. Angluin, L. Hellerstein and M. Karpinski. </author> <year> (1993). </year> <title> Learning read-once formulas with queries. </title> <journal> Journal of the Association for Computing Machinery, </journal> <pages> 48 , 185-210. </pages>
Reference-contexts: Further Research An interesting direction for further research is to try and modify additional PAC learning algorithms that use membership queries to the case in which the queries might be answered erroneously. Such algorithms exist for learning monotone DNF [38], read-once Boolean functions <ref> [5] </ref>, Horn Clauses [4], among others. A more general direction that can be pursued is to study the possible relationship between learning from fallible experts and the area of self-correcting [9], [26].
Reference: 6. <author> N. Alon and J. H. Spencer. </author> <year> (1991). </year> <title> The Probabilistic Method. </title> <publisher> Wiley Interscience. </publisher>
Reference-contexts: Let p = P Then we have the following two inequalities. The first inequality (additive form) isreferences. are they ok? usually credited to Hoeffding [16] and the second inequality (multiplicative form) is usually credited to Chernoff [10]. The versions below were taken from <ref> [6] </ref>. references. are they ok? Inequality 1 For 0 &lt; ff 1, P r [ i=1 X i p &gt; ff] &lt; e 2ff 2 M P r [p i=1 X i &gt; ff] &lt; e 2ff 2 M LEARNING FALLIBLE DETERMINISTIC FINITE AUTOMATA 5 Inequality 2 For 0 &lt; ff
Reference: 7. <author> D. Angluin and D. Slonim. </author> <year> (1994). </year> <title> Randomly fallible teachers: Learning monotone DNF with an incomplete membership oracle. </title> <booktitle> Machine Learning, </booktitle> <pages> 14 , 7-26. </pages>
Reference-contexts: They present algorithms for exactly identifying different circuits under fixed distributions, and show that their algorithms can be modified to handle large rates of randomly chosen, though persistent, misclassification noise in the queries. Angluin and Slonim <ref> [7] </ref> consider a more benign model of incomplete membership queries in which with some probability the teacher may answer "I don't know". For more work in this model see [15]. 3. Preliminaries Let A be the deterministic finite state automaton we would like to learn.
Reference: 8. <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Occam's Razor. </author> <year> (1987). </year> <journal> Information Processing Letters, </journal> <pages> 27 , 377-380. </pages>
Reference-contexts: With probability at least 1 ffi after time polynomial in n b , L, jj, 1 * it must output a hypothesis automaton A 0 such that P r D ( A 0 (x) 6= A (x)) *. By Occam's Razor Cardinality Lemma <ref> [8] </ref>, in order to output such a hypothesis with probability at least 1 ffi, it suffices to find an automaton A 0 with n 0 states (where n 0 = poly (n b )) which agrees with A on a set of sample strings of size at least 1 ffi ),
Reference: 9. <author> M. Blum, M. Luby, and R. Rubinfeld. </author> <year> (1993). </year> <title> Self-Testing/Correcting with applications to numerical problems. </title> <journal> Journal of Computer and System Sciences, </journal> <pages> 47 , 549-595. </pages>
Reference-contexts: Such algorithms exist for learning monotone DNF [38], read-once Boolean functions [5], Horn Clauses [4], among others. A more general direction that can be pursued is to study the possible relationship between learning from fallible experts and the area of self-correcting <ref> [9] </ref>, [26]. A simple observation is that any family of functions that has both a known learning algorithm (with or without membership queries), and a self corrector, has a learning algorithm with membership queries that works when queries might be answered erroneously. <p> This algorithm is composed of an algorithm for learning parity functions [13], [19] by solving a system of linear equations over the field of integers modulo 2, and a self-correcting algorithm <ref> [9] </ref>, [26] for the same family of functions. We do not know of any other self-correcting algorithm that has been directly applied to a related learning problem, but the possibility exists that techniques used in one field may be useful in the other.
Reference: 10. <author> H. Chernoff. </author> <year> (1952). </year> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Annals of the Mathematical Statistics, </journal> <pages> 23 , 493-507. </pages>
Reference-contexts: Let p = P Then we have the following two inequalities. The first inequality (additive form) isreferences. are they ok? usually credited to Hoeffding [16] and the second inequality (multiplicative form) is usually credited to Chernoff <ref> [10] </ref>.
Reference: 11. <author> T. Dean, D. Angluin, K. Basye, S. Engelson, L. Kaelbling, E. Kokkevis, and O. Maron. </author> <year> (1992). </year> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <booktitle> Proceedings of the 10th National Conference on Artificial Intelligence (pp. </booktitle> <month> 208-214). </month> <title> LEARNING FALLIBLE DETERMINISTIC FINITE AUTOMATA 37 </title>
Reference-contexts: Therefore, existing learning algorithms can be modified and then used in this model of random noise. Dean et. al. <ref> [11] </ref> study Rivest and Schapire's model [30] for learning DFAs when the learner has no means of resetting the machine and thus can be viewed as a robot learning an environment. They investigate the case in which the output of each state may be erroneous with some fixed probability. <p> Moreover, even if the target automaton is known to have a distinguishing sequence, then there is not necessarily an efficient procedure for finding one such sequence. 1 Thus the solution described in <ref> [11] </ref> is not applicable in the general case, even when the learner does have means of resetting the machine (as our learner does). Goldman, Kearns, and Schapire [14] consider a model of persistent noise in membership queries which is very similar to the one used in this paper. <p> We would also like to thank the referees of this paper for their careful reading and very helpful comments. 36 DANA RON AND RONITT RUBINFELD Notes 1. <ref> [11] </ref> actually mention that the adversary argument in [1] showing that DFAs cannot be learned using membership queries only can be modified so that the target DFAs all have distinguishing sequences.
Reference: 12. <author> Y. Freund, M. J. Kearns, D. Ron, R. Rubinfeld, R. E. Schapire, and L. Sellie. </author> <year> (1993). </year> <title> Efficient learning of typical finite automata from random walks. </title> <booktitle> Proceedings of the 25th Annual ACM Symposium on Theory of Computing (pp. </booktitle> <pages> 315-324). </pages> <address> San-Diego, CA: </address> <institution> The Association for Computing Machinery. </institution>
Reference-contexts: Rivest and Schapire [30], [31], [33] present algorithms for inferring DFAs from input/output behavior in the absence of a means of resetting the machine to a start state. Freund et al. <ref> [12] </ref> present efficient algorithms for learning typical DFAs from random walks without membership queries, both when the learner is provided with the means of resetting the machine and when it is not.
Reference: 13. <author> Paul Fischer and Hans Ulrich Simon. </author> <title> (1992) On learning ring-sum-expansions. </title> <journal> SIAM Journal on Computing, </journal> <pages> 21 , 181-192. </pages>
Reference-contexts: Instead, it always queries the corrector. A simple example of an application of the above observation is a learning algorithm (using membership queries) for noisy parity functions. This algorithm is composed of an algorithm for learning parity functions <ref> [13] </ref>, [19] by solving a system of linear equations over the field of integers modulo 2, and a self-correcting algorithm [9], [26] for the same family of functions.
Reference: 14. <author> S. A. Goldman, M. J. Kearns, and R. E. Schapire. </author> <year> (1993). </year> <title> Exact identification of read-once formulas using fixed points of amplification functions. </title> <journal> SIAM Journal on Computing, </journal> <pages> 22 , 705-726. </pages>
Reference-contexts: Goldman, Kearns, and Schapire <ref> [14] </ref> consider a model of persistent noise in membership queries which is very similar to the one used in this paper.
Reference: 15. <author> S. A. Goldman and H. D. Mathias. </author> <year> (1992). </year> <title> Learning k-term DNF formulas with an incomplete membership oracle. </title> <booktitle> Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory (pp. </booktitle> <pages> 85-92). </pages> <address> Pittsburgh, PA: </address> <institution> The Association for Computing Machinery. </institution>
Reference-contexts: Angluin and Slonim [7] consider a more benign model of incomplete membership queries in which with some probability the teacher may answer "I don't know". For more work in this model see <ref> [15] </ref>. 3. Preliminaries Let A be the deterministic finite state automaton we would like to learn.
Reference: 16. <author> W. Hoeffding. </author> <year> (1963). </year> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <pages> 58 , 13-30. </pages>
Reference-contexts: Let p = P Then we have the following two inequalities. The first inequality (additive form) isreferences. are they ok? usually credited to Hoeffding <ref> [16] </ref> and the second inequality (multiplicative form) is usually credited to Chernoff [10].
Reference: 17. <author> J. E. Hopcroft. </author> <year> (1971). </year> <title> An n log n algorithm for minimizing the states in a finite automaton. </title> <booktitle> The Theory of Machines and Computations, </booktitle> <pages> 189-196. </pages> <publisher> Academic Press, New-York. </publisher>
Reference-contexts: The resulting automaton might be much larger than the minimal equivalent automaton and so we apply an algorithm for minimizing DFAs [18], [27], <ref> [17] </ref>, and find the smallest equivalent automaton. The final partition is defined in the following simple manner. For any given string r 2 R, let r = r p r s where jr s j = l 2 .
Reference: 18. <author> D. A. Huffman. </author> <year> (1954). </year> <title> The synthesis of sequential switching circuits. </title> <journal> J. Franklin Institute, </journal> <volume> 257, </volume> <pages> 161-190, 275-303. </pages>
Reference-contexts: The resulting automaton might be much larger than the minimal equivalent automaton and so we apply an algorithm for minimizing DFAs <ref> [18] </ref>, [27], [17], and find the smallest equivalent automaton. The final partition is defined in the following simple manner. For any given string r 2 R, let r = r p r s where jr s j = l 2 .
Reference: 19. <author> D. Helmbold, R. Sloan, and M. K. Warmuth. </author> <year> (1992). </year> <title> Learning integer lattices. </title> <journal> SIAM Journal on Computing, </journal> <pages> 21 , 240-266. </pages>
Reference-contexts: Instead, it always queries the corrector. A simple example of an application of the above observation is a learning algorithm (using membership queries) for noisy parity functions. This algorithm is composed of an algorithm for learning parity functions [13], <ref> [19] </ref> by solving a system of linear equations over the field of integers modulo 2, and a self-correcting algorithm [9], [26] for the same family of functions.
Reference: 20. <author> M. Kearns. </author> <year> (1990). </year> <title> The Computational Complexity of Machine Learning. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Several results have been obtained for learning in the presence of errors in the Probably Approximately Correct model introduced by Valiant [38]. These include results for learning in the presence of malicious and random noise in classification and attributes [39], [3], [25], [22], <ref> [20] </ref>, [34], [35], [37], [36]. In recent work Kearns [21] identifies and formalizes a sufficient condition on learning algorithms in Valiant's model that permits the immediate derivation of noise-tolerant learning algorithms.
Reference: 21. <author> M. Kearns. </author> <year> (1993). </year> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> Proceedings of the 25th Annual ACM Symposium on Theory of Computing (pp. </booktitle> <pages> 392-401). </pages> <institution> San-Diego: The Association for Computing Machinery. </institution>
Reference-contexts: These include results for learning in the presence of malicious and random noise in classification and attributes [39], [3], [25], [22], [20], [34], [35], [37], [36]. In recent work Kearns <ref> [21] </ref> identifies and formalizes a sufficient condition on learning algorithms in Valiant's model that permits the immediate derivation of noise-tolerant learning algorithms.
Reference: 22. <author> M. Kearns and M. Li. </author> <year> (1993). </year> <title> Learning in the presence of malicious errors. </title> <journal> Siam Journal on Computing, </journal> <pages> 22 , 807-837. </pages>
Reference-contexts: Several results have been obtained for learning in the presence of errors in the Probably Approximately Correct model introduced by Valiant [38]. These include results for learning in the presence of malicious and random noise in classification and attributes [39], [3], [25], <ref> [22] </ref>, [20], [34], [35], [37], [36]. In recent work Kearns [21] identifies and formalizes a sufficient condition on learning algorithms in Valiant's model that permits the immediate derivation of noise-tolerant learning algorithms.
Reference: 23. <author> M. Kearns and R. Schapire. </author> <title> (1990) Efficient distribution-free learning of probabilistic concepts. </title> <booktitle> Proceedings of the 31st Annual Symposium on Foundations of Computer Science (pp. </booktitle> <pages> 382-391). </pages> <note> To appear in JCSS. </note>
Reference-contexts: The general problem (in which for every string u the expert has a (possibly different) error probability (u)) seems hard. It might be argued that the natural problem in this case is to learn the corresponding probabilistic concept <ref> [23] </ref>. What we would like to know is if there are other (reasonable) special cases for which our algorithm can be adapted.
Reference: 24. <author> M. Kearns and L. Valiant. </author> <year> (1994). </year> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <journal> Journal of the Association for Computing Machinery, </journal> <pages> 41 , 67-95. </pages>
Reference-contexts: Parts of our algorithm rely on a version of Angluin's algorithm [2] for learning finite automata in the error-free case. This version is presented preceding the description of our algorithm. 2. Related Results In the error free case of learning DFAs, Kearns and Valiant <ref> [24] </ref> use the prediction preserving reductions of Pitt and Warmuth [28] to show that under cryptographic assumptions, the problem of predicting the class of DFAs is hard when only given access to random examples. Angluin [1] shows that (exact) learning using only membership queries is also hard.
Reference: 25. <author> P. Laird. </author> <year> (1988). </year> <title> Learning From Good Data and Bad. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Several results have been obtained for learning in the presence of errors in the Probably Approximately Correct model introduced by Valiant [38]. These include results for learning in the presence of malicious and random noise in classification and attributes [39], [3], <ref> [25] </ref>, [22], [20], [34], [35], [37], [36]. In recent work Kearns [21] identifies and formalizes a sufficient condition on learning algorithms in Valiant's model that permits the immediate derivation of noise-tolerant learning algorithms.
Reference: 26. <author> R. Lipton. </author> <year> (1991). </year> <title> New directions in testing. </title> <journal> Distributed Computing and Cryptography, DIMACS Series in Discrete Math and Theoretical Computer Science, American Mathematical Society, </journal> <pages> 2 , 191-202. </pages>
Reference-contexts: Such algorithms exist for learning monotone DNF [38], read-once Boolean functions [5], Horn Clauses [4], among others. A more general direction that can be pursued is to study the possible relationship between learning from fallible experts and the area of self-correcting [9], <ref> [26] </ref>. A simple observation is that any family of functions that has both a known learning algorithm (with or without membership queries), and a self corrector, has a learning algorithm with membership queries that works when queries might be answered erroneously. <p> This algorithm is composed of an algorithm for learning parity functions [13], [19] by solving a system of linear equations over the field of integers modulo 2, and a self-correcting algorithm [9], <ref> [26] </ref> for the same family of functions. We do not know of any other self-correcting algorithm that has been directly applied to a related learning problem, but the possibility exists that techniques used in one field may be useful in the other.
Reference: 27. <author> E. F. Moore. </author> <year> (1956). </year> <title> Gedanken experiments on sequential machines. </title> <booktitle> Automata Studies, </booktitle> <pages> 129-153. </pages> <publisher> Princeton Univ. Press, </publisher> <address> Princeton, N.J. </address>
Reference-contexts: The resulting automaton might be much larger than the minimal equivalent automaton and so we apply an algorithm for minimizing DFAs [18], <ref> [27] </ref>, [17], and find the smallest equivalent automaton. The final partition is defined in the following simple manner. For any given string r 2 R, let r = r p r s where jr s j = l 2 .
Reference: 28. <author> L. Pitt and M. Warmuth. </author> <title> (1990) Prediction-preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <pages> 41 , 430-467. </pages>
Reference-contexts: This version is presented preceding the description of our algorithm. 2. Related Results In the error free case of learning DFAs, Kearns and Valiant [24] use the prediction preserving reductions of Pitt and Warmuth <ref> [28] </ref> to show that under cryptographic assumptions, the problem of predicting the class of DFAs is hard when only given access to random examples. Angluin [1] shows that (exact) learning using only membership queries is also hard.
Reference: 29. <author> D. Ron and R. Rubinfeld. </author> <year> (1993). </year> <title> Learning fallible finite state automata. </title> <booktitle> Proceedings of the 6th Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> (pp. 218-227). </pages> <address> Santa-Cruz, CA: </address> <institution> The Association for Computing Machinery. </institution>
Reference-contexts: If the initial partition in fact corresponds to the target automaton, then this labeling is correct with high probability. We would also like to note that in the preliminary version of this paper <ref> [29] </ref> we added an additional stage to the algorithm (between the initial and final partitioning) which treated this special case in a different manner. 7. <p> We feel that a more careful (though perhaps more complicated) analysis might yield better bounds. We would like to point out that we were able to improve these bounds with respect to the ones presented in the preliminary version of this paper <ref> [29] </ref>. 9.5. Further Research An interesting direction for further research is to try and modify additional PAC learning algorithms that use membership queries to the case in which the queries might be answered erroneously.
Reference: 30. <author> R. Rivest and R. Schapire. </author> <year> (1987). </year> <title> Diversity-based inference of finite automata. </title> <booktitle> Proceedings of the 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> (pp. 78-87). </pages> <note> To appear in JACM. </note>
Reference-contexts: However, as mentioned earlier, Angluin [2] describes an algorithm for learning DFAs given access both to random examples and to membership queries. Rivest and Schapire <ref> [30] </ref>, [31], [33] present algorithms for inferring DFAs from input/output behavior in the absence of a means of resetting the machine to a start state. <p> Therefore, existing learning algorithms can be modified and then used in this model of random noise. Dean et. al. [11] study Rivest and Schapire's model <ref> [30] </ref> for learning DFAs when the learner has no means of resetting the machine and thus can be viewed as a robot learning an environment. They investigate the case in which the output of each state may be erroneous with some fixed probability.
Reference: 31. <author> R. Rivest and R. Schapire. </author> <year> (1993). </year> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103, </volume> <pages> 299-347. </pages>
Reference-contexts: However, as mentioned earlier, Angluin [2] describes an algorithm for learning DFAs given access both to random examples and to membership queries. Rivest and Schapire [30], <ref> [31] </ref>, [33] present algorithms for inferring DFAs from input/output behavior in the absence of a means of resetting the machine to a start state.
Reference: 32. <author> Y. Sakakibara. </author> <year> (1991). </year> <title> On learning from queries and counterexamples in the presence of noise. </title> <journal> Information Processing Letters, </journal> <pages> 37 , 279-284. </pages>
Reference-contexts: He introduces a new model of learning from statistical queries and shows that any class efficiently learnable from statistical queries is also learnable with random classification noise in the random examples. There are fewer results when generalizing PAC learning to learning with membership queries. Sakakibara <ref> [32] </ref> shows that if for each query there is some independent probability to receive an incorrect answer, and these errors are not persistent then LEARNING FALLIBLE DETERMINISTIC FINITE AUTOMATA 3 queries can be repeated until the confidence in the correct answer is high enough.
Reference: 33. <author> R. E. Schapire. </author> <year> (1991). </year> <title> The Design and Analysis of Efficient Learning Algorithms. </title> <publisher> MIT Press. 38 DANA RON AND RONITT RUBINFELD </publisher>
Reference-contexts: However, as mentioned earlier, Angluin [2] describes an algorithm for learning DFAs given access both to random examples and to membership queries. Rivest and Schapire [30], [31], <ref> [33] </ref> present algorithms for inferring DFAs from input/output behavior in the absence of a means of resetting the machine to a start state.
Reference: 34. <author> R. H. Sloan. </author> <year> (1988). </year> <title> Types of noise in data for concept learning. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory (pp. </booktitle> <pages> 91-96). </pages> <address> Santa-Cruz, CA: </address> <institution> The Association for Computing Machinery. </institution>
Reference-contexts: Several results have been obtained for learning in the presence of errors in the Probably Approximately Correct model introduced by Valiant [38]. These include results for learning in the presence of malicious and random noise in classification and attributes [39], [3], [25], [22], [20], <ref> [34] </ref>, [35], [37], [36]. In recent work Kearns [21] identifies and formalizes a sufficient condition on learning algorithms in Valiant's model that permits the immediate derivation of noise-tolerant learning algorithms.
Reference: 35. <author> R. H. Sloan. </author> <year> (1989). </year> <title> Computational Learning Theory: New Models and Algorithms. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Science, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address> <note> Issued as MIT/LCS/TR-448. </note>
Reference-contexts: Several results have been obtained for learning in the presence of errors in the Probably Approximately Correct model introduced by Valiant [38]. These include results for learning in the presence of malicious and random noise in classification and attributes [39], [3], [25], [22], [20], [34], <ref> [35] </ref>, [37], [36]. In recent work Kearns [21] identifies and formalizes a sufficient condition on learning algorithms in Valiant's model that permits the immediate derivation of noise-tolerant learning algorithms.
Reference: 36. <author> Y. Sakakibaraand Rani Siromoney. </author> <year> (1992). </year> <title> A noise model on learning sets of strings. </title> <booktitle> Proceedings of the 5th Annual Workshop on Computational Learning Theory (pp. </booktitle> <pages> 295-302). </pages> <address> Pittsburgh, PA: </address> <institution> The Association for Computing Machinery. </institution>
Reference-contexts: Several results have been obtained for learning in the presence of errors in the Probably Approximately Correct model introduced by Valiant [38]. These include results for learning in the presence of malicious and random noise in classification and attributes [39], [3], [25], [22], [20], [34], [35], [37], <ref> [36] </ref>. In recent work Kearns [21] identifies and formalizes a sufficient condition on learning algorithms in Valiant's model that permits the immediate derivation of noise-tolerant learning algorithms.
Reference: 37. <author> G. Shackelford and D. Volper. </author> <year> (1988). </year> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory (pp. </booktitle> <pages> 97-103). </pages> <address> Santa-Cruz, CA: </address> <institution> The Association for Computing Machinery. </institution>
Reference-contexts: Several results have been obtained for learning in the presence of errors in the Probably Approximately Correct model introduced by Valiant [38]. These include results for learning in the presence of malicious and random noise in classification and attributes [39], [3], [25], [22], [20], [34], [35], <ref> [37] </ref>, [36]. In recent work Kearns [21] identifies and formalizes a sufficient condition on learning algorithms in Valiant's model that permits the immediate derivation of noise-tolerant learning algorithms.
Reference: 38. <author> L. G. Valiant. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <pages> 27 , 1134-1142. </pages>
Reference-contexts: Several results have been obtained for learning in the presence of errors in the Probably Approximately Correct model introduced by Valiant <ref> [38] </ref>. These include results for learning in the presence of malicious and random noise in classification and attributes [39], [3], [25], [22], [20], [34], [35], [37], [36]. <p> Further Research An interesting direction for further research is to try and modify additional PAC learning algorithms that use membership queries to the case in which the queries might be answered erroneously. Such algorithms exist for learning monotone DNF <ref> [38] </ref>, read-once Boolean functions [5], Horn Clauses [4], among others. A more general direction that can be pursued is to study the possible relationship between learning from fallible experts and the area of self-correcting [9], [26].
Reference: 39. <author> L. G. Valiant. </author> <title> (1985) Learning disjunctions of conjunctions. </title> <booktitle> Proceedings of the 9th International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 560-566). </pages> <address> Los Ange-les, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Several results have been obtained for learning in the presence of errors in the Probably Approximately Correct model introduced by Valiant [38]. These include results for learning in the presence of malicious and random noise in classification and attributes <ref> [39] </ref>, [3], [25], [22], [20], [34], [35], [37], [36]. In recent work Kearns [21] identifies and formalizes a sufficient condition on learning algorithms in Valiant's model that permits the immediate derivation of noise-tolerant learning algorithms.
References-found: 39

