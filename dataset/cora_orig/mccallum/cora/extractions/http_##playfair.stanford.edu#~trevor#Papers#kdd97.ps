URL: http://playfair.stanford.edu/~trevor/Papers/kdd97.ps
Refering-URL: http://playfair.stanford.edu/~trevor/Papers/
Root-URL: 
Email: ruby@stat.stanford.edu trevor@stat.stanford.edu  
Title: Discriminative vs Informative Learning Automatic classification is among the main goals of data mining systems
Author: Y. Dan Rubinstein and Trevor Hastie (Fayyad, Piatetsky-Shapiro, Smyth ). 
Note: KDD and Classification  
Address: Stanford, CA 94305  
Affiliation: Department of Statistics Stanford University  
Abstract: The goal of pattern classification can be approached from two points of view: informative where the classifier learns the class densities, or discriminative where the focus is on learning the class boundaries without regard to the underlying class densities. We review and synthesize the tradeoffs between these two approaches for simple classifiers, and extend the results to modern techniques such as Naive Bayes and Generalized Additive Models. Data mining applications often operate in the domain of high dimensional features where the tradeoffs between informative and discriminative classifiers are especially relevant. Experimental results are provided for simulated and real data. 1 
Abstract-found: 1
Intro-found: 1
Reference: <author> Byth, K., and McLachlan, G. J. </author> <year> 1980. </year> <title> Logistic regression compared to normal discrimination for non-normal populations. </title> <journal> The Australian Journal of Statistics 22 </journal> <pages> 188-196. </pages>
Reference-contexts: Even when the class densities are not gaussian there are circumstances such as when the classes are well separated when informative training does about as well as discriminative <ref> (Byth & McLachlan 1980) </ref>. The informative approach requires estimating class means and a pooled covariance which requires only a single sweep through the data. The discriminative approach requires an iterative optimization via a gradient descent of the conditional likelihood. data.
Reference: <author> Duda, R. O., and Hart., P. E. </author> <year> 1973. </year> <title> Pattern classification and scene analysis. </title> <publisher> Wiley. </publisher>
Reference-contexts: Underlying the observations is a true joint density p (x; y) = p (yjx)p (x) = p (xjy)p (y) which is unknown. The goal is to minimize the total cost of errors, known as the overall risk and this is achieved by the Bayes classifier <ref> (Duda & Hart. 1973) </ref> fl (x) = min 1 m=1 = max 1 p (y = kjx) (0/1 loss): (2) For 0/1 loss this reduces to classifying x to the class k for which the class posterior probability p (y = kjx) is maximum.
Reference: <author> Efron, B. </author> <year> 1975. </year> <title> The efficiency of logistic regression compared to normal discriminant analysis. </title> <journal> Journal of the American Statistical Association 70(352) </journal> <pages> 892-898. </pages>
Reference: <author> Fayyad, U. M.; Piatetsky-Shapiro, G.; and Smyth, P. </author> <year> 1996. </year> <title> From data mining to knowledge discovery: An overview. In Advances in Knowledge Discovery and Data Mining. </title> <publisher> The Mit Press. </publisher> <pages> 1-31. </pages>
Reference: <author> Friedman, J. </author> <year> 1996. </year> <title> On bias, variance, 0/1-loss and the curse of dimensionality. </title> <type> Technical report, </type> <institution> Dept. of Statistics, Stanford University. </institution>
Reference-contexts: Here then is an instance where informative training actually does slightly better than discriminative training, even though the discriminative model is correct and the informative one is not! Conclusion Recently, Friedman <ref> (Friedman 1996) </ref> has shown that when it comes to classification, bias in the class posteriors is not so critical because of the discretization of the assignment rule.
Reference: <author> Hastie, T., and Tibshirani, R. </author> <year> 1990. </year> <title> Generalized Additive Models. </title> <publisher> Chapman Hall. </publisher>
Reference-contexts: Langley (John & Lang-ley 1995) considered class densities that are products of univariate gaussians as well as "flexible" gaussian kernel densities. The corresponding discriminative procedure is known as a Generalized Additive Model (GAM) <ref> (Hastie & Tibshirani 1990) </ref>.
Reference: <author> John, G. H., and Langley, P. </author> <year> 1995. </year> <title> Estimating continuous distributions in bayesian classifiers. </title> <booktitle> In Proceedings of the Eleventh Conference on Uncertainty in Artifical Intelligence. </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The class densities assume independence among the predictors p (xjy = k) = j ) log p (xjy = k) = j = j and are naive for this reason. Langley <ref> (John & Lang-ley 1995) </ref> considered class densities that are products of univariate gaussians as well as "flexible" gaussian kernel densities. The corresponding discriminative procedure is known as a Generalized Additive Model (GAM) (Hastie & Tibshirani 1990).
Reference: <author> Langley, P., and Sage, S. </author> <year> 1994. </year> <title> Induction of selective bayesian classifers. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artifical Intelligence. </booktitle> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Asymptotically, the Naive Bayes (NB) classifier does worse (9.0%) than GAM, since the class densities are not a product form. However, when only a finite sample of training observations is available, the Naive Bayes classifier does surprisingly well (this behavior has been noted by Langley <ref> (Langley & Sage 1994) </ref>). In simulation experiments, 25 training sets each containing 25 observations from each class were used to train both NB and GAM classifiers. The average error rates were 11.1% for NB and 11.4% for GAM with standard errors of 0.05 % and 0.06% respectively.
Reference: <author> Michie, D.; Spiegelhalter, D. J.; and Taylor, C. </author> <year> 1994. </year> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood. </publisher>
Reference-contexts: StatLog data The StatLog experiments compared several classification techniques on various datasets. For most of the datasets, logistic discrimination did better than the corresponding informative approach of LDA <ref> (Michie, Spiegelhalter, & Taylor 1994) </ref>. However, there were several cases, such as the chromosome dataset, in which LDA did better than logistic discrimination. For these cases, the informative model apparently makes use of important information in the marginal density p (x).
Reference: <author> Ney, H. </author> <year> 1995. </year> <title> On the probabilistic interpretation of neural network classifiers and discriminative training criteria. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 17(2) </journal> <pages> 107-119. </pages>
Reference-contexts: Many classification techniques seek to estimate the class posterior probabilities p (y = kjx), since we see in (2) that optimal classification can be achieved if these are known perfectly (for a discussion on the relationship between class posteriors and neural net outputs see <ref> (Ney 1995) </ref>). For convenience in what follows, we will make use of the discriminant function k (x) = log p (y = Kjx) This discriminant preserves the ordering of the class posterior probabilities and can be used instead of them for classification.
Reference: <author> O'Neill, T. J. </author> <year> 1980. </year> <title> The general distribution of the error rate of a classification procedure with application to logistic regression discrimination. </title> <journal> Journal of the American Statistical Association 75(369) </journal> <pages> 154-160. </pages>
Reference: <author> Ruiz-Velasco, S. </author> <year> 1991. </year> <title> Asymptotic efficiency of logistic regression relative to linear discriminant analysis. </title> <journal> Biometrika 78 </journal> <pages> 235-243. </pages>
Reference: <author> Stone, C. J.; Hansen, M.; Kooperberg, C.; and Truong, Y. K. </author> <title> to appear. Polynomial splines and their tensor products in extended linear modeling. </title> <journal> Annals of Statistics. </journal>
References-found: 13

