URL: http://www.ius.cs.cmu.edu/IUS/inf07/tsato/spotslide/nameit.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/tsato/www/pubcmu.html
Root-URL: http://www.cs.cmu.edu
Email: email: satoh@rd.nacsis.ac.jp  
Title: Network-Centric Computing (NCC) Special Issue Name-It: Naming and Detecting Faces in News Video Running head:
Author: Shin'ichi Satoh Toshio Sato Michael A. Smith Yuichi Nakamura Takeo Kanade Contact: Shin'ichi Satoh, ph: +--- 
Date: March 1996 to December 1996.  
Note: 1 The author had been a visiting scientist at CMU from April 1995 to April 1997. 2 The author had been a visiting scientist at CMU from  
Address: 3-29-1 Otsuka, Bunkyo-ku, Tokyo 112, Japan.  
Affiliation: National Center for Science Information Systems (NACSIS)  School of Computer Science, Carnegie Mellon University  School of Computer Science, Carnegie Mellon University  Institute of Information Sciences and Electronics, University of Tsukuba  School of Computer Science, Carnegie Mellon University  R&D Dept., NACSIS,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Satoh and T. Kanade, Name-It: </author> <title> Association of face and name in video, </title> <booktitle> in Proceedings, Computer Vision and Pattern Recognition, </booktitle> <year> 1997. </year>
Reference-contexts: Video data is multi-modal, i.e., it may include image sequences, audio (including speech), closed-captions, transcripts, etc. Therefore, a multi-modal approach is an effective means to obtain the contents of video data. Vision/image processing and natural language processing should play an important role. We propose Name-It <ref> [1, 2] </ref>, a system that associates names and faces in news videos, as a part of the Informedia project [3] at Carnegie Mellon University. It's basic function is to guess which face corresponds to which name in news videos. <p> We use the co-occurrence factor <ref> [1, 2] </ref> taking advantage of face extraction/identification, name extraction, and video caption recognition. Let N and 18 F be a name and a face, respectively. The co-occurrence factor C fl (N; F ) mea-sures the degree of how well face F will match name N .
Reference: [2] <author> S. Satoh, Y. Nakamura, and T. Kanade, Name-It: </author> <title> Naming and detecting faces in video by the integration of image and natural language processing, </title> <booktitle> in Proceedings, International Joint Conference on Artificial Intelligence, </booktitle> <year> 1997. </year>
Reference-contexts: Video data is multi-modal, i.e., it may include image sequences, audio (including speech), closed-captions, transcripts, etc. Therefore, a multi-modal approach is an effective means to obtain the contents of video data. Vision/image processing and natural language processing should play an important role. We propose Name-It <ref> [1, 2] </ref>, a system that associates names and faces in news videos, as a part of the Informedia project [3] at Carnegie Mellon University. It's basic function is to guess which face corresponds to which name in news videos. <p> We use the co-occurrence factor <ref> [1, 2] </ref> taking advantage of face extraction/identification, name extraction, and video caption recognition. Let N and 18 F be a name and a face, respectively. The co-occurrence factor C fl (N; F ) mea-sures the degree of how well face F will match name N .
Reference: [3] <author> H. D. Wactlar, T. Kanade, M. A. Smith, and S. M. Stevens, </author> <title> Intelligent access to digital video: The informedia project, </title> <journal> IEEE Computer, </journal> <volume> 29, </volume> <year> 1996, </year> <pages> 46-52. 23 </pages>
Reference-contexts: Therefore, a multi-modal approach is an effective means to obtain the contents of video data. Vision/image processing and natural language processing should play an important role. We propose Name-It [1, 2], a system that associates names and faces in news videos, as a part of the Informedia project <ref> [3] </ref> at Carnegie Mellon University. It's basic function is to guess which face corresponds to which name in news videos. In other words, Name-It detects faces with corresponding names in news videos as content information.
Reference: [4] <author> A. Pentland, R. W. Picard, and S. Schlaroff, Photobook: </author> <title> Content-based ma-nipulation of image databases, </title> <journal> International Journal of Computer Vision, </journal> <volume> 18, </volume> <year> 1996, </year> <pages> 233-254. </pages>
Reference-contexts: There are image database systems which can perform face similarity matching; they include MIT Photobook <ref> [4] </ref> and the Virage system. These two systems use the eigenvector-based method for face similarity matching [5]. It is noteworthy that Photobook was applied to more than 7,500 face images of about 3,000 people to obtain successful results.
Reference: [5] <author> M. Turk and A. Pentland, </author> <title> Eigenfaces for recognition, </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3, </volume> <year> 1991, </year> <pages> 71-86. </pages>
Reference-contexts: There are image database systems which can perform face similarity matching; they include MIT Photobook [4] and the Virage system. These two systems use the eigenvector-based method for face similarity matching <ref> [5] </ref>. It is noteworthy that Photobook was applied to more than 7,500 face images of about 3,000 people to obtain successful results. The results reveal that eigenvector-based face similarity matching works well to some extent. Video analysis is one of the most popular research topics today. <p> The system chooses the face having the largest F r to be the most frontal face of the face sequence. Figure 5 shows example faces, extracted face skin regions, and frontal factors. 10 4.2.2 Eigenface-Based Face Identification We employ the eigenface-based method <ref> [5] </ref> to evaluate face identification. Each of the most frontal faces is normalized into a 64fi64 image using the eye positions, then converted into a point in the 16-dimensional eigenface space.
Reference: [6] <author> D. Swanberg, C. F. Shu, and R. Jain, </author> <title> Knowledge guided parsing in video database, </title> <booktitle> in Proceedings, Symposium on Electric Imaging, Science and Technology, </booktitle> <address> IS&T/SPIE, </address> <year> 1993. </year>
Reference-contexts: The results reveal that eigenvector-based face similarity matching works well to some extent. Video analysis is one of the most popular research topics today. Among researches in this area, video parsing <ref> [6, 7] </ref> is more related to our approach. In their 5 approach, once a news video is given as a target, the video is decomposed into seg-ments or shots, then these shots are classified based on the structure of the video. <p> In closed-caption texts of CNN Headline News, the components can easily be distinguished; a topic is led by &gt;>>, and a paragraph is led by &gt;> (See Figure 8.). To discriminate an anchor/live video shot from videos, we use this literal information, instead of news video structuring techniques <ref> [6, 7] </ref>. A typical paragraph at the beginning of the 12 topic is an anchor paragraph, in which an anchor person gives an overview of the topic.
Reference: [7] <author> S. W. Smoliar and H. Zhang, </author> <title> Content-based video indexing and retrieval, </title> <booktitle> IEEE Multimedia, Summer 1994, </booktitle> <pages> 62-72. </pages>
Reference-contexts: The results reveal that eigenvector-based face similarity matching works well to some extent. Video analysis is one of the most popular research topics today. Among researches in this area, video parsing <ref> [6, 7] </ref> is more related to our approach. In their 5 approach, once a news video is given as a target, the video is decomposed into seg-ments or shots, then these shots are classified based on the structure of the video. <p> In closed-caption texts of CNN Headline News, the components can easily be distinguished; a topic is led by &gt;>>, and a paragraph is led by &gt;> (See Figure 8.). To discriminate an anchor/live video shot from videos, we use this literal information, instead of news video structuring techniques <ref> [6, 7] </ref>. A typical paragraph at the beginning of the 12 topic is an anchor paragraph, in which an anchor person gives an overview of the topic.
Reference: [8] <author> R. Chopra and R. K. Srihari, </author> <title> Control structures for incorporating picture specific context in image interpretation, </title> <booktitle> in Proceedings, International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: Although videos can be parsed, i.e., the structure of videos can be analyzed, these methods do not provide content information such as object identification or topic classification in news videos. Piction system <ref> [8] </ref> identifies faces within given captioned photos, typically in newspapers. The system extracts faces from a photo and analyzes captions to obtain geometric constraints among faces which will appear in the photo, then labels each face with each name.
Reference: [9] <editor> Proceedings, </editor> <booktitle> Sixth Message Understanding Conference, </booktitle> <year> 1995. </year>
Reference-contexts: Meanwhile, to extract names of persons of interest, in-depth semantic analysis of the transcript is necessary. This is not achieved, even though selecting names in text is achieved with sufficient accuracy <ref> [9] </ref>. Therefore, we extract faces and names which are likely to 6 correspond to persons of interest. The system employs face detection and track-ing to extract face sequences, and natural language processing techniques using a dictionary, thesaurus, and parser to locate names in transcripts (See Figure 3.).
Reference: [10] <author> H. Rowley, S. Baluja, and T. Kanade, </author> <title> Human face detection in visual scenes, </title> <type> Tech. Rep. </type> <institution> CMU-CS-95-158, School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: This interval should be small enough not to miss important face sequences, yet large enough to ensure reasonable processing time. Optimally, we apply the face detector at intervals of 10 frames. The system uses the neural network-based 8 face detector <ref> [10] </ref>, which detects mostly frontal faces at various sizes and loca-tions. The detected face is output as a rectangular region that includes most of the skin, but excludes the hair and the background.
Reference: [11] <author> J. Yang and A. Waibel, </author> <title> Tracking human faces in real-time, </title> <type> Tech. Rep. </type> <institution> CMU-CS-95-210, School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: In several cases, researchers used the Gaussian model in (r; g) space (r = R=(R + G + B); g = G=(R + G + B)) as a general skin color model for face tracking <ref> [11, 12] </ref>. Instead, for our research, the Gaussian model in (R; G; B) space is used because this model is more sensitive to brightness of skin color, and thus is much more suitable for the model tailored for each face.
Reference: [12] <author> H. M. Hunke, </author> <title> Locating and tracking of human faces with neural networks, </title> <type> Tech. Rep. </type> <institution> CMU-CS-94-155, School of Computer Science, Carnegie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: In several cases, researchers used the Gaussian model in (r; g) space (r = R=(R + G + B); g = G=(R + G + B)) as a general skin color model for face tracking <ref> [11, 12] </ref>. Instead, for our research, the Gaussian model in (R; G; B) space is used because this model is more sensitive to brightness of skin color, and thus is much more suitable for the model tailored for each face.
Reference: [13] <author> M. Smith and T. Kanade, </author> <title> Video skimming for quick browsing based on audio and image characterization, </title> <type> Tech. Rep. </type> <institution> CMU-CS-95-186, School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: The overlap between each of these regions and each of the face regions of the previous frame is evaluated to decide whether one of the skin candidate regions is the succeeding face region. In addition, the scene change detection method based on the sub-region color histogram matching <ref> [13] </ref> is applied. Face region tracking is continued until a scene change is encountered or until no succeeding face region is found. 9 4.2 Face Identification To evaluate face identification, we employed a face similarity measurement based on the eigenface method.
Reference: [14] <institution> The Oxford Text Archive. </institution> <note> http://ota.ox.ac.uk/. </note>
Reference-contexts: The person appearing in a live video rarely mentions his/her own name. Instead, just before the live video, an anchor person tends to introduce him/her (See The system evaluates these conditions for each word in the transcripts by using a dictionary (the Oxford Advanced Learner's Dictionary <ref> [14] </ref>), thesaurus (WordNet [15]), and parser (Link Parser [16]).
Reference: [15] <author> G. Miller, </author> <title> WordNet: An on-line lexical database, </title> <journal> International Journal of Lexicography, </journal> <volume> 3, </volume> <year> 1990. </year> <month> 24 </month>
Reference-contexts: The person appearing in a live video rarely mentions his/her own name. Instead, just before the live video, an anchor person tends to introduce him/her (See The system evaluates these conditions for each word in the transcripts by using a dictionary (the Oxford Advanced Learner's Dictionary [14]), thesaurus (WordNet <ref> [15] </ref>), and parser (Link Parser [16]). Then, the system outputs the three-tuple list: a word, timing information (frame), and a normalized score reflecting the above conditions. 13 5.3 Score Calculation Referring to the dictionaries and the parsing results, the system calculates the score for each word in transcripts.
Reference: [16] <author> D. Sleator, </author> <title> Parsing english with a link grammar, </title> <booktitle> in Proceedings, Third Inter--national Workshop on Parsing Technologies, </booktitle> <year> 1993. </year>
Reference-contexts: Instead, just before the live video, an anchor person tends to introduce him/her (See The system evaluates these conditions for each word in the transcripts by using a dictionary (the Oxford Advanced Learner's Dictionary [14]), thesaurus (WordNet [15]), and parser (Link Parser <ref> [16] </ref>). Then, the system outputs the three-tuple list: a word, timing information (frame), and a normalized score reflecting the above conditions. 13 5.3 Score Calculation Referring to the dictionaries and the parsing results, the system calculates the score for each word in transcripts.

References-found: 16

