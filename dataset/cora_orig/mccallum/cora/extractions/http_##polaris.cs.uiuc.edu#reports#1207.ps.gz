URL: http://polaris.cs.uiuc.edu/reports/1207.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Microarchitecture Support for Dynamic Scheduling of Acyclic Task Graphs 1  
Author: Carl J. Beckmann Constantine D. Polychronopoulos 
Keyword: - Functional parallelism, fine-grain parallelism, microarchitecture, dynamic schedul ing, parallelizing compiler.  
Note: and control dependences, and describes a microarchitecture which implements these algorithms efficiently.  
Address: St. Urbana, IL. 61801  2630 Walsh Ave. Santa Clara, CA. 95051-0905  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign 305 Talbot Lab, 104 S. Wright  Center for Supercomputing Research and Development and Kubota Pacific Computer, Inc.  
Email: beckmann@csrd.uiuc.edu  cdp@kpc.com  
Phone: tel. 217-244-0052  tel. 408-987-3330  
Abstract: It can be shown that any program can be broken into its loop structure, plus acyclic these acyclic graphs augments the loop-level parallelism available in the program. This paper presents two algorithms for dynamic scheduling of such acyclic task graphs containing both data dependence graphs representing the body of each loop or subroutine. The parallelism inherent in
Abstract-found: 1
Intro-found: 1
Reference: [AhSU86] <author> A. Aho, R. Sethi, J. Ullman, </author> <booktitle> Compilers Principles, Techniques, and Tools, </booktitle> <publisher> Addison-Wesley 1986. </publisher>
Reference-contexts: The Hierarchical Task Graph The loop structure of the program is extracted by constructing its control flow graph (CFG), computing a dominator tree based on the CFG, and finding the back-edges in the CFG based on the dominator tree <ref> [AhSU86] </ref>. By removing the back-edges and collapsing each loop into a compound node, the body of each loop thus becomes an acyclic CFG, each of whose nodes is either a node in the original CFG, or a nested loop node [Girk91]. Using the acyclic CFG and data flow analysis [AhSU86], control <p> tree <ref> [AhSU86] </ref>. By removing the back-edges and collapsing each loop into a compound node, the body of each loop thus becomes an acyclic CFG, each of whose nodes is either a node in the original CFG, or a nested loop node [Girk91]. Using the acyclic CFG and data flow analysis [AhSU86], control and data dependences between nodes at a given level of the HTG are then computed. 2.2. Data Dependence Data dependences represent the implied ordering of conflicting data accesses to program variables.
Reference: [BCKK88] <author> M. Berry, et al., </author> <title> "The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers," </title> <type> CSRD report #827, </type> <month> November </month> <year> 1988. </year>
Reference-contexts: Task Graphs in Scientific Codes We have analyzed a number of the Perfect Club benchmarks <ref> [BCKK88] </ref> to gather data about ATGs in typical numeric programs. The codes were examined at two different levels: the - 23 - statement level and the instruction level. Parafrase-2 is used to extract an HTG for each program [PGHL89].
Reference: [CSSE91] <author> D. Culler, A. Sah, K. Schauser, T. von Eiken, J. Wawrzynek, </author> <title> "Fine-grain Parallel ism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine", </title> <booktitle> ASPLOS 91, </booktitle> <pages> pp. 164-175 </pages>
Reference-contexts: This allows the compiler to choose the desired task granularity and to optimize away many trivial synchronizations. Unlike various hybrid dataflow approaches [Iann88] [NiAr89] [PaCu90] [PaTr91] <ref> [CSSE91] </ref>, our hardware scheme does not employ separate synchronization or thread management instructions, and in principle achieves zero scheduling overhead (as long as there is sufficient work to keep the pipeline busy). - 27 - Table 6.
Reference: [DeHB89] <author> J. Dehnert, P. Hsu, J. Bratt, </author> <title> "Overlapped Loop Support in the Cydra 5", </title> <booktitle> ASPLOS 89, </booktitle> <pages> pp. 26-38 </pages>
Reference-contexts: Since scheduling is fully dynamic, our scheme has the advantages over static software pipe-lining schemes [RaGP82] [Lam88] <ref> [DeHB89] </ref>, that it readily handles both conditional branches, and long and unpredictable latencies caused by cache misses. - 29 - 8. Conclusions This paper has presented two algorithms for dynamic scheduling of acyclic task graphs.
Reference: [FeOW87] <author> J. Ferrante, K.J. Ottenstein and J.D. Warren, </author> <title> "The Program Dependence Graph and its Use in Optimization," </title> <journal> ACM Transactions of Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The formal definition is based on the post dominance relationship in the CFG [GiPo90] <ref> [FeOW87] </ref>. Let v STOP be a unique node in the CFG to which control eventually flows from any other node.
Reference: [FrPe88] <author> E. Freudenthal, O. Peze, </author> <title> "Efficient Synchronization Algorithms Using Fetch&Add on Multiple Bitfield Integers", Ultracomputer Note #148, </title> <month> February </month> <year> 1988. </year>
Reference-contexts: If, after general optimizations, all the necessary counters can be placed in a single machine - 11 - word, then the exit code of any task can update all of its successors' counters in a single instruction using fetch-and-sub with appropriate constants <ref> [FrPe88] </ref>. For the task graph in Figure 2, only tasks 4, 8 and 9 require counters. Since these are initialized to 2, 2, and 3, respectively, each one requires a 2-bit field, for a total of 6 bits.
Reference: [GiPo90] <author> M. Girkar, C. Polychronopoulos, </author> <title> "Automatic Detection and Generation of Unstructured Parallelism in Ordinary Programs", </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> April </month> <year> 1992. </year>
Reference-contexts: The properties and derivation of these task graphs is discussed in the remainder of this section. While the focus of this paper is on algorithms and implementation in hardware and - 3 - software, this theory, based on the work of Girkar and Polychronopoulos [Poly88] <ref> [GiPo90] </ref> [Poly90] [Girk91], is presented below as background. 2.1. The Hierarchical Task Graph The loop structure of the program is extracted by constructing its control flow graph (CFG), computing a dominator tree based on the CFG, and finding the back-edges in the CFG based on the dominator tree [AhSU86]. <p> The formal definition is based on the post dominance relationship in the CFG <ref> [GiPo90] </ref> [FeOW87]. Let v STOP be a unique node in the CFG to which control eventually flows from any other node. <p> A CFG edge a -b is said to satisfy a dependence v 1 d c v 2 if control is guaranteed to flow to v 2 when v 1 takes the branch a -b . The following definitions are indirectly related to control dependence <ref> [GiPo90] </ref>: Neg (v j ) denotes the set of control flow edges which, when taken, prevent v j from being executed. <p> However, they can be computed from the CFG in a straightforward manner during the control dependence calculation <ref> [GiPo90] </ref>. 2.4. Acyclic Task Graphs A acyclic task graph (ATG) is obtained by merging the (acyclic) DDG with the (acyclic) CDG. <p> General Optimizations Optimizations of the graph [Poly88] <ref> [GiPo90] </ref> [Girk91] can be performed prior generating exit code implementing either of the above algorithms. Various general optimizations can then be applied to reduce the number of execution condition tests [Poly90].
Reference: [Girk91] <author> M. B. Girkar, </author> <title> "Functional Parallelism: Theoretical Foundations and Implementation," </title> <type> Ph.D. thesis, CSRD report #1182, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: 1. Introduction Traditional approaches to parallel processing have focused largely on loop-level parallelism. Another source of parallelism in programs is non-loop, or functional, parallelism <ref> [Girk91] </ref>. While the amount of functional parallelism available in programs is typically much lower than what is available from loops, it may still be very important for highly pipelined, multithreaded, or superscalar architectures, and for exploiting parallelism in sequential loops. <p> The properties and derivation of these task graphs is discussed in the remainder of this section. While the focus of this paper is on algorithms and implementation in hardware and - 3 - software, this theory, based on the work of Girkar and Polychronopoulos [Poly88] [GiPo90] [Poly90] <ref> [Girk91] </ref>, is presented below as background. 2.1. The Hierarchical Task Graph The loop structure of the program is extracted by constructing its control flow graph (CFG), computing a dominator tree based on the CFG, and finding the back-edges in the CFG based on the dominator tree [AhSU86]. <p> By removing the back-edges and collapsing each loop into a compound node, the body of each loop thus becomes an acyclic CFG, each of whose nodes is either a node in the original CFG, or a nested loop node <ref> [Girk91] </ref>. Using the acyclic CFG and data flow analysis [AhSU86], control and data dependences between nodes at a given level of the HTG are then computed. 2.2. Data Dependence Data dependences represent the implied ordering of conflicting data accesses to program variables. <p> General Optimizations Optimizations of the graph [Poly88] [GiPo90] <ref> [Girk91] </ref> can be performed prior generating exit code implementing either of the above algorithms. Various general optimizations can then be applied to reduce the number of execution condition tests [Poly90].
Reference: [Iann88] <author> R. </author> <title> Iannucci, "Toward a Dataflow/Von Neumann Hybrid Architecture" , Proc. </title> <booktitle> Intl. Symp. on Computer Architecture 1988, </booktitle> <pages> pp. 131-140 </pages>
Reference-contexts: Our scheme in a sense goes one step beyond the explicit token store concept [PaCu90] by making synchronization explicit and separating it from data transfer. This allows the compiler to choose the desired task granularity and to optimize away many trivial synchronizations. Unlike various hybrid dataflow approaches <ref> [Iann88] </ref> [NiAr89] [PaCu90] [PaTr91] [CSSE91], our hardware scheme does not employ separate synchronization or thread management instructions, and in principle achieves zero scheduling overhead (as long as there is sufficient work to keep the pipeline busy). - 27 - Table 6.
Reference: [Kuck78] <author> D. J. Kuck, </author> <title> The Structure of Computers and Computations, </title> <publisher> John Wiley & Sons, </publisher> <year> 1978 </year>
Reference-contexts: c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c Furthermore, our approach is based on the theory of dependences <ref> [Kuck78] </ref> and the control flow execution model used by imperative languages such as C and Fortran. It does not require single-assignment properties, and readily handles anti- and output-dependences. Our hardware approach can also be thought of as an alternative to dynamic instruction issue architectures [Thor64] [Toma67].
Reference: [Lam88] <author> M. Lam, </author> <title> "Software Pipelining: An Effective Scheduling Technique for VLIW Machines" , SIGPLAN Conf. </title> <booktitle> on Programming Language Design and Implementa tion, ACM (June), </booktitle> <address> Atlanta, Ga. 318-328. (p. </address> <month> 340) </month>
Reference-contexts: Since scheduling is fully dynamic, our scheme has the advantages over static software pipe-lining schemes [RaGP82] <ref> [Lam88] </ref> [DeHB89], that it readily handles both conditional branches, and long and unpredictable latencies caused by cache misses. - 29 - 8. Conclusions This paper has presented two algorithms for dynamic scheduling of acyclic task graphs.
Reference: [Moto89] <author> Motorola, Inc., </author> <title> MC88100 RISC Microprocessor User's Manual, second edition, ISBN 0-13-567090-X, </title> <publisher> Prentice-Hall, </publisher> <year> 1989 </year>
Reference: [NiAr89] <author> R. Nikhil, Arvind, </author> <title> "Can Dataflow subsume von Neumann computing?", </title> <booktitle> Proc. Intl. Symp. on Computer Architecture 1989, </booktitle> <pages> pp. 262-272 </pages>
Reference-contexts: Our scheme in a sense goes one step beyond the explicit token store concept [PaCu90] by making synchronization explicit and separating it from data transfer. This allows the compiler to choose the desired task granularity and to optimize away many trivial synchronizations. Unlike various hybrid dataflow approaches [Iann88] <ref> [NiAr89] </ref> [PaCu90] [PaTr91] [CSSE91], our hardware scheme does not employ separate synchronization or thread management instructions, and in principle achieves zero scheduling overhead (as long as there is sufficient work to keep the pipeline busy). - 27 - Table 6.
Reference: [PGHL89] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C. L. Lee, B. Leung, D. Schouten, </author> <title> "Parafrase-2: An Environment for Parallelizing, Partitioning, Synchronizing, and Scheduling Programs on Multiprocessors," </title> <booktitle> Proc. Intl. Conf. Parallel Processing 1989, </booktitle> <volume> vol. II, </volume> <pages> pp. 39-48. </pages>
Reference-contexts: The codes were examined at two different levels: the - 23 - statement level and the instruction level. Parafrase-2 is used to extract an HTG for each program <ref> [PGHL89] </ref>. HTG extraction in Parafrase-2 requires induction variable substitution to be performed first. Dead code elimination is also performed to clean up the output of induction variable substitution. For statement level analysis, no other transforms are applied.
Reference: [Poly88] <author> C. Polychronopoulos, </author> <title> "Toward Autoscheduling Compilers", </title> <type> CSRD report #789, </type> <institution> and Journal of Supercomputing, </institution> <month> Nov. </month> <year> 1988, </year> <pages> pp. 297-330. </pages>
Reference-contexts: The properties and derivation of these task graphs is discussed in the remainder of this section. While the focus of this paper is on algorithms and implementation in hardware and - 3 - software, this theory, based on the work of Girkar and Polychronopoulos <ref> [Poly88] </ref> [GiPo90] [Poly90] [Girk91], is presented below as background. 2.1. The Hierarchical Task Graph The loop structure of the program is extracted by constructing its control flow graph (CFG), computing a dominator tree based on the CFG, and finding the back-edges in the CFG based on the dominator tree [AhSU86]. <p> General Optimizations Optimizations of the graph <ref> [Poly88] </ref> [GiPo90] [Girk91] can be performed prior generating exit code implementing either of the above algorithms. Various general optimizations can then be applied to reduce the number of execution condition tests [Poly90].
Reference: [Poly90] <author> C. Polychronopoulos, </author> <title> "Auto-Scheduling: Control Flow and Data Flow Come Together", </title> <type> CSRD report #1058, </type> <year> 1990. </year>
Reference-contexts: The properties and derivation of these task graphs is discussed in the remainder of this section. While the focus of this paper is on algorithms and implementation in hardware and - 3 - software, this theory, based on the work of Girkar and Polychronopoulos [Poly88] [GiPo90] <ref> [Poly90] </ref> [Girk91], is presented below as background. 2.1. The Hierarchical Task Graph The loop structure of the program is extracted by constructing its control flow graph (CFG), computing a dominator tree based on the CFG, and finding the back-edges in the CFG based on the dominator tree [AhSU86]. <p> General Optimizations Optimizations of the graph [Poly88] [GiPo90] [Girk91] can be performed prior generating exit code implementing either of the above algorithms. Various general optimizations can then be applied to reduce the number of execution condition tests <ref> [Poly90] </ref>. In particular, any task without predecessors does not require an execution condition test since it is unconditionally queued upon starting the task graph. Also, any task with only a single predecessor does not require an execution condition test since its sole predecessor unconditionally queues it.
Reference: [PaCu90] <author> G. Papadopoulos, D. Culler, "Monsoon: </author> <title> an Explicit Token-Store Architecture", </title> <booktitle> Proc. Intl. Symp. on Computer Architecture 1990, </booktitle> <pages> pp. 82-91 </pages>
Reference-contexts: In dataflow this is signalled by the arrival of tokens carrying data as well as synchronization information. Our scheme in a sense goes one step beyond the explicit token store concept <ref> [PaCu90] </ref> by making synchronization explicit and separating it from data transfer. This allows the compiler to choose the desired task granularity and to optimize away many trivial synchronizations. Unlike various hybrid dataflow approaches [Iann88] [NiAr89] [PaCu90] [PaTr91] [CSSE91], our hardware scheme does not employ separate synchronization or thread management instructions, and <p> Our scheme in a sense goes one step beyond the explicit token store concept <ref> [PaCu90] </ref> by making synchronization explicit and separating it from data transfer. This allows the compiler to choose the desired task granularity and to optimize away many trivial synchronizations. Unlike various hybrid dataflow approaches [Iann88] [NiAr89] [PaCu90] [PaTr91] [CSSE91], our hardware scheme does not employ separate synchronization or thread management instructions, and in principle achieves zero scheduling overhead (as long as there is sufficient work to keep the pipeline busy). - 27 - Table 6.
Reference: [PaTr91] <author> G. Papadopoulos, K. Traub, </author> <title> "Multithreading: A Revisionist View of Dataflow Architectures", </title> <booktitle> Proc. Intl. Symp. on Computer Architecture 1991, </booktitle> <pages> pp. 342-351 </pages>
Reference-contexts: This allows the compiler to choose the desired task granularity and to optimize away many trivial synchronizations. Unlike various hybrid dataflow approaches [Iann88] [NiAr89] [PaCu90] <ref> [PaTr91] </ref> [CSSE91], our hardware scheme does not employ separate synchronization or thread management instructions, and in principle achieves zero scheduling overhead (as long as there is sufficient work to keep the pipeline busy). - 27 - Table 6.
Reference: [RaGP82] <author> B. Rau, C. Glaeser, R. </author> <title> Picard, "Efficient Code Generation for Horizontal Archi tectures: </title> <booktitle> Compiler Techniques and Architectural Support", Proc. Intl. Symp. on Computer Architecture 1982, </booktitle> <pages> pp. 131-139 </pages>
Reference-contexts: Since scheduling is fully dynamic, our scheme has the advantages over static software pipe-lining schemes <ref> [RaGP82] </ref> [Lam88] [DeHB89], that it readily handles both conditional branches, and long and unpredictable latencies caused by cache misses. - 29 - 8. Conclusions This paper has presented two algorithms for dynamic scheduling of acyclic task graphs.
Reference: [Smit78] <author> B. Smith, </author> <title> "A Pipelined, Shared Resource MIMD Computer", </title> <booktitle> Proc. Intl. Conf. Parallel Processing 1978, </booktitle> <pages> pp. 6-8 </pages>
Reference-contexts: Instructions from independent tasks may thus be presented to the functional unit on successive clock cycles, as in other multithreaded architectures <ref> [Smit78] </ref> [ThSm88]. Each instruction word contains two fields: a function unit instruction, which is like an ordinary instruction in a conventional architecture (typically 32 bits wide), and a short ATG instruction (typically 8 bits wide). The ATG instruction is also passed through the pipeline along with the data.
Reference: [ThSm88] <author> M. Thistle, B. Smith, </author> <title> "A Processor Architecture for Horizon", </title> <booktitle> Proc. Supercomputing '88, </booktitle> <pages> pp. 35-41 </pages>
Reference-contexts: Instructions from independent tasks may thus be presented to the functional unit on successive clock cycles, as in other multithreaded architectures [Smit78] <ref> [ThSm88] </ref>. Each instruction word contains two fields: a function unit instruction, which is like an ordinary instruction in a conventional architecture (typically 32 bits wide), and a short ATG instruction (typically 8 bits wide). The ATG instruction is also passed through the pipeline along with the data.
Reference: [Thor64] <author> J. E. Thornton, </author> <title> "Parallel Operations in the Control Data 6600", </title> <booktitle> AFIPS Proceedings FJCC, </booktitle> <volume> pt 2, Vol 26, </volume> <year> 1964, </year> <pages> pp. 33-40 </pages>
Reference-contexts: It does not require single-assignment properties, and readily handles anti- and output-dependences. Our hardware approach can also be thought of as an alternative to dynamic instruction issue architectures <ref> [Thor64] </ref> [Toma67]. Given a sufficiently large instruction scheduling window and hardware register renaming or loop unrolling by the compiler, these architectures could exploit the same parallelism as our hardware.
Reference: [Toma67] <author> R. M. Tomasulo, </author> <title> "An Efficient Algorithm for Exploiting Multiple Arithmetic Units", </title> <journal> IBM Journal, </journal> <volume> No. 11, </volume> <year> 1967, </year> <pages> pp. 25-33 </pages>
Reference-contexts: It does not require single-assignment properties, and readily handles anti- and output-dependences. Our hardware approach can also be thought of as an alternative to dynamic instruction issue architectures [Thor64] <ref> [Toma67] </ref>. Given a sufficiently large instruction scheduling window and hardware register renaming or loop unrolling by the compiler, these architectures could exploit the same parallelism as our hardware.
Reference: [Veid85] <author> A. Veidenbaum, </author> <title> "Compiler Optimization and Architecture Design Issues for Multiprocessors", </title> <type> CSRD report #520, </type> <month> May </month> <year> 1985 </year>
Reference-contexts: Finally, it would be interesting to explore the effects of more aggressive compiler optimizations such as tree height reduction <ref> [Veid85] </ref> for increasing the parallelism in instruction-level ATGs.
References-found: 24

