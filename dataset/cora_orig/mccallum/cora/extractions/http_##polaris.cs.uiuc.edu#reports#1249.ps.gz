URL: http://polaris.cs.uiuc.edu/reports/1249.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: SUCCESS AND LIMITATIONS IN AUTOMATIC PARALLELIZATION OF THE PERFECT BENCHMARKS TM PROGRAMS  
Author: BY WILLIAM JOSEPH BLUME 
Degree: 1989 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1992 Urbana, Illinois  
Affiliation: B.S., University of Illinois,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Clifford N. Arnold. </author> <title> Performance Evaluation of three Automatic Vectorizer Packages. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 235-242, </pages> <year> 1982. </year>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [1, 2, 3, 7, 17, 18] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [5, 10, 22].
Reference: [2] <author> Robert N. Braswell and Malcolm S. Keech. </author> <title> An Evaluation of Vector Fortran 200 Generated by Cyber 205 and ETA-10 Pre-Compilation Tools. </title> <booktitle> In Proc. Supercomputing `88, </booktitle> <pages> pages 106-113, </pages> <year> 1988. </year>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [1, 2, 3, 7, 17, 18] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [5, 10, 22]. <p> Nobayashi and Eoyang [18] extended the work of Callahan, Dongarra, and Levine with measurements of three more machines and additional kernels. They also made some performance measurements for these machines. Braswell and Keech <ref> [2] </ref> determined the effectiveness of the vectorizing compilers for the CDC Cyber 205 and ETA-10 by measuring the speedup of execution times on a set of kernels. Three of the compilers measured were Kap and two variants of Vast.
Reference: [3] <author> David Callahan, Jack Dongarra, and David Levine. </author> <title> Vectorizing Compilers: A Test Suite and Results. </title> <booktitle> In Proc. Supercomputing `88, </booktitle> <pages> pages 98-105, </pages> <year> 1988. </year>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [1, 2, 3, 7, 17, 18] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [5, 10, 22]. <p> Kuck, Kuhn, Leasure, and Wolfe [15] collected frequencies of the use of individual restructuring techniques in vectorizing a set of kernels. Callahan, Dongarra, and Levine <ref> [3] </ref> evaluated the effectiveness of vectorizers for 19 supercomputers by counting the number of partially and fully vectorized loops in a carefully selected test suite of 100 loops. The loops were chosen to stress particular aspects of a vectorizing compiler.
Reference: [4] <author> Ding-Kai Chen and Pen-Chung Yew. </author> <title> An Empirical Study of DOACROSS Loops. </title> <booktitle> Proceedings of Supercomputing'91, </booktitle> <address> Albuquerque, NM, </address> <pages> pages 630-632, </pages> <month> November 18-, </month> <year> 1991. </year>
Reference-contexts: There has also been studies that determine the effectiveness of individual restructuring techniques on parallelizing codes. Cytron, Kuck, and Veidenbaum [6] measure the impact of several restructuring techniques on the speed of a set of kernals running on a simulator. Chen and Yew <ref> [4] </ref> determine the importance of synchronized concurrent loops for parallelizing codes. 1.3 Caveats As in all benchmarking reports, our measurements will be biased toward the machine and the compilers used, and inaccuracies from run-to-run variations.
Reference: [5] <author> Doreen Y. Cheng and Douglas M. Pase. </author> <title> An Evaluation of Automatic and Interactive Parallel Programming Tools. </title> <booktitle> In Proc. Supercomputing '91, </booktitle> <pages> pages 412-422, </pages> <year> 1991. </year>
Reference-contexts: The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs <ref> [5, 10, 22] </ref>. However, very few papers have reported evaluation measures of individual restructuring techniques [6, 10]. This thesis will measure both overall performance improvements from automatic parallelization and the 3 contribution of individual restructuring techniques. <p> More recently, several effectiveness studies of parallelizing compilers that generate concurrent code have been made. Lee, Kruskal, and Kuck [16] measure and analyzed the speedups of a set of automatically parallelized nonnumeric algorithms running on a simulator. Cheng and Pase <ref> [5] </ref> measured the speedups in execution times of vectorization and concurrent execution of 25 programs including the Perfect Benchmarks. The measurements were made on a Cray Y-MP and several compilers, including Kap. An interactive compiler is also evaluated.
Reference: [6] <author> Ron Cytron, David J. Kuck, and Alex V. Veidenbaum. </author> <title> The Effect of Restructuring Compilers on Program Performance for High-Speed Computers. </title> <booktitle> Special Issue of Computer Physics Communications devoted to the Proceedings of the Conference on Vector and Parallel Processors in Computational Science II, </booktitle> <volume> 37 </volume> <pages> 39-48, </pages> <year> 1985. </year>
Reference-contexts: The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [5, 10, 22]. However, very few papers have reported evaluation measures of individual restructuring techniques <ref> [6, 10] </ref>. This thesis will measure both overall performance improvements from automatic parallelization and the 3 contribution of individual restructuring techniques. The measurements are taken on an Alliant FX/80 machine using the Perfect Benchmarks programs and the two compilers, Kap and Vast. <p> Petersen and Padua [22] determined the effectiveness of Kap on the Perfect Benchmarks and several linear algebra routines by comparing the achieved and optimal speedups on an ideal machine. There has also been studies that determine the effectiveness of individual restructuring techniques on parallelizing codes. Cytron, Kuck, and Veidenbaum <ref> [6] </ref> measure the impact of several restructuring techniques on the speed of a set of kernals running on a simulator.
Reference: [7] <author> Ulrich Detert. </author> <title> Programmiertechniken fur die Vektorisierung. </title> <booktitle> In Proc. Supercomputer `87, Mannheim, </booktitle> <address> Germany, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [1, 2, 3, 7, 17, 18] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [5, 10, 22].
Reference: [8] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, Zhiyuan Li, and D. Padua. </author> <title> Restructuring Fortran Programs for Cedar. </title> <note> to appear in Concurrency: Practice and Experience, </note> <year> 1992. </year>
Reference-contexts: Solutions to these problems seem feasible, possibly at the cost of increased compilation time. Further evidence for potential improvement was delivered by the compiler group at CSRD which has been working on manually parallelizing the Perfect Benchmarks programs, applying mostly automatable techniques <ref> [8, 11] </ref>. The attained speedups ranged from 4 to 17, using the same measures as in Figure 3.1. 78 In this chapter, we will look at the manual parallelization of the code adm. We believe that all the transformations applied to this code are automatable. <p> We believe that all the transformations applied to this code are automatable. We also believe that from our observations in previous chapters and from the work done in <ref> [8] </ref> and [11], that these transformations will be useful for other codes as well. 6.1 A case study: parallelization of the code adm Adm simulates air pollution concentration and deposition patterns for lake shore environments [23]. It is 6104 lines long and consists of 97 subroutines. <p> We have derived this from manually restructuring the Perfect code adm using techniques that we believe to be automatable. The findings for adm are similar to those for other Perfect codes, reported elsewhere <ref> [8, 11] </ref>. Among the areas we identified are: certain implementation issues, advances of existing techniques, new analysis and transformation techniques, improved drivers for techniques and compiler passes, interprocedural optimization, and run-time driven optimization.
Reference: [9] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> Cedar Fortran and Its Compiler. </title> <booktitle> Lecture Notes in Computer Science: Proceedings of the Joint Conference on Vector and Parallel Processing, </booktitle> <address> Zurich, Switzerland, </address> <month> 457 </month> <pages> 288-300, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: This compiler was modified as part of the Cedar project conducted at CSRD <ref> [9] </ref>. As a byproduct, an Alliant/FX80 version of the restructurer has been developed. It contains a number of improvements over the original Kap version. For example, we have added strip-mining capabilities to allow single loops to be vectorized and executed concurrently. We also added switches to disable individual restructuring techniques.
Reference: [10] <author> Rudolf Eigenmann and William Blume. </author> <title> An Effectiveness Study of Parallelizing Compiler Techniques. </title> <booktitle> Proceedings of ICPP'91, </booktitle> <address> St. Charles, IL, II:17-25, </address> <month> August 12-16, </month> <year> 1991. </year>
Reference-contexts: The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs <ref> [5, 10, 22] </ref>. However, very few papers have reported evaluation measures of individual restructuring techniques [6, 10]. This thesis will measure both overall performance improvements from automatic parallelization and the 3 contribution of individual restructuring techniques. <p> The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [5, 10, 22]. However, very few papers have reported evaluation measures of individual restructuring techniques <ref> [6, 10] </ref>. This thesis will measure both overall performance improvements from automatic parallelization and the 3 contribution of individual restructuring techniques. The measurements are taken on an Alliant FX/80 machine using the Perfect Benchmarks programs and the two compilers, Kap and Vast.
Reference: [11] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 65-83, </pages> <month> August </month> <year> 1991. </year> <month> 86 </month>
Reference-contexts: For adm, manually parallelizing these loop nests achieved a speedup of 6.6 from serial execution time. Impressive speedups of the other three codes were also achieved either by parallelizing loops with call statements or using inline expansion <ref> [11, 13] </ref>. Other techniques that were used to achieve these improvements are described in section 6. The effect of I/O statements on program performance is less pronounced. In bdna, the important loop nest containing an I/O statement accounted for only 7.8% of the vector-concurrent execution time. <p> While this shows that this technique is not applied frequently in available compilers, there is an indication that it may become more important in the future: induction variable recognition was critical in the manual parallelization of trfd and mdg <ref> [11] </ref>. We pick loop interchanging to illustrate the applicability of a technique. Table 4.3 displays the percentage of loop nests that are parallelizable and at least doubly nested. Examining the table, it can be seen that on average, at most 45% of the loop nests can use loop interchanging. <p> Solutions to these problems seem feasible, possibly at the cost of increased compilation time. Further evidence for potential improvement was delivered by the compiler group at CSRD which has been working on manually parallelizing the Perfect Benchmarks programs, applying mostly automatable techniques <ref> [8, 11] </ref>. The attained speedups ranged from 4 to 17, using the same measures as in Figure 3.1. 78 In this chapter, we will look at the manual parallelization of the code adm. We believe that all the transformations applied to this code are automatable. <p> We believe that all the transformations applied to this code are automatable. We also believe that from our observations in previous chapters and from the work done in [8] and <ref> [11] </ref>, that these transformations will be useful for other codes as well. 6.1 A case study: parallelization of the code adm Adm simulates air pollution concentration and deposition patterns for lake shore environments [23]. It is 6104 lines long and consists of 97 subroutines. <p> We have derived this from manually restructuring the Perfect code adm using techniques that we believe to be automatable. The findings for adm are similar to those for other Perfect codes, reported elsewhere <ref> [8, 11] </ref>. Among the areas we identified are: certain implementation issues, advances of existing techniques, new analysis and transformation techniques, improved drivers for techniques and compiler passes, interprocedural optimization, and run-time driven optimization.
Reference: [12] <author> Chuigang Fu. </author> <title> Evaluating the Effectiveness of Fortran Vectorizers by Measuring Total Parallelism. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Braswell and Keech [2] determined the effectiveness of the vectorizing compilers for the CDC Cyber 205 and ETA-10 by measuring the speedup of execution times on a set of kernels. Three of the compilers measured were Kap and two variants of Vast. Fu <ref> [12] </ref> compared the Kap's and Vast's performance on LINPACK and three codes from the Perfect Benchmarks. The execution time of the transformed codes on a simulated vector machine were used for the comparisons. More recently, several effectiveness studies of parallelizing compilers that generate concurrent code have been made.
Reference: [13] <author> Jay Hoeflinger. </author> <title> QCD Optimization Report. </title> <type> Technical Report 1115, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomp. R&D, </institution> <year> 1991. </year>
Reference-contexts: For adm, manually parallelizing these loop nests achieved a speedup of 6.6 from serial execution time. Impressive speedups of the other three codes were also achieved either by parallelizing loops with call statements or using inline expansion <ref> [11, 13] </ref>. Other techniques that were used to achieve these improvements are described in section 6. The effect of I/O statements on program performance is less pronounced. In bdna, the important loop nest containing an I/O statement accounted for only 7.8% of the vector-concurrent execution time.
Reference: [14] <institution> Kuck & Associates, Inc., Champaign, Illinois. </institution> <note> KAP User's Guide, </note> <year> 1988. </year>
Reference-contexts: The figures will help us later in interpreting the performance gains we get in large programs. 2 The Conjugate Gradient Algorithm 9 2.3 Compilers used The parallelizing compiler we used for our measurements is a modified version of Kap, the source-to-source restructurer developed at Kuck & Associates <ref> [14] </ref>. This compiler was modified as part of the Cedar project conducted at CSRD [9]. As a byproduct, an Alliant/FX80 version of the restructurer has been developed. It contains a number of improvements over the original Kap version.
Reference: [15] <author> D. J. Kuck, R. H. Kuhn, B. Leasure, and M. Wolfe. </author> <title> The Structure of an Advanced Vectorizer for Pipelined Processors. </title> <booktitle> Proc. of COMPSAC 80, The 4th Int'l. Computer Software and Applications Conf., </booktitle> <pages> pages 709-715, </pages> <month> Oct., </month> <year> 1980. </year>
Reference-contexts: Suggestions for improvements are given in section 6. Finally, section 7 summarizes the results of this thesis. 4 1.2 Related work There has been much work on evaluating the performance of vectorizing compilers. Kuck, Kuhn, Leasure, and Wolfe <ref> [15] </ref> collected frequencies of the use of individual restructuring techniques in vectorizing a set of kernels. Callahan, Dongarra, and Levine [3] evaluated the effectiveness of vectorizers for 19 supercomputers by counting the number of partially and fully vectorized loops in a carefully selected test suite of 100 loops.
Reference: [16] <author> Ghungho Lee, Clyde P. Kruskal, and David J. Kuck. </author> <title> The Effectiveness of Automatic Restructuring on Nonnumerical Programs. </title> <booktitle> Proceedings of the 1985 Int'l. Conference on Parallel Processing, </booktitle> <pages> pages 607-613, </pages> <month> August 20-23, </month> <year> 1985. </year>
Reference-contexts: The execution time of the transformed codes on a simulated vector machine were used for the comparisons. More recently, several effectiveness studies of parallelizing compilers that generate concurrent code have been made. Lee, Kruskal, and Kuck <ref> [16] </ref> measure and analyzed the speedups of a set of automatically parallelized nonnumeric algorithms running on a simulator. Cheng and Pase [5] measured the speedups in execution times of vectorization and concurrent execution of 25 programs including the Perfect Benchmarks.
Reference: [17] <author> G.R. Luecke, J. Coyle, W. Haque, J. Hoekstra, H. Jespersen, and R. Schmidt. </author> <title> A comparative study of KAP and VAST: two automatic preprocessors with Fortran 8x Output. Supercomputer 28, </title> <address> V(6):15-25, </address> <year> 1988. </year>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [1, 2, 3, 7, 17, 18] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [5, 10, 22]. <p> Callahan, Dongarra, and Levine [3] evaluated the effectiveness of vectorizers for 19 supercomputers by counting the number of partially and fully vectorized loops in a carefully selected test suite of 100 loops. The loops were chosen to stress particular aspects of a vectorizing compiler. Lueke et al. <ref> [17] </ref> performed a similar, although more qualitative, study of the Kap and Vast vectorizers. Nobayashi and Eoyang [18] extended the work of Callahan, Dongarra, and Levine with measurements of three more machines and additional kernels. They also made some performance measurements for these machines.
Reference: [18] <author> H. Nobayashi and C. Eoyang. </author> <title> A Comparison Study of Automatically Vectorizing Fortran Compilers. </title> <booktitle> Proc. Supercomputing '89, </booktitle> <year> 1989. </year>
Reference-contexts: These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness. Many early studies measured the success rate of automatic vectorizers on a suite of test loops <ref> [1, 2, 3, 7, 17, 18] </ref>. The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs [5, 10, 22]. <p> The loops were chosen to stress particular aspects of a vectorizing compiler. Lueke et al. [17] performed a similar, although more qualitative, study of the Kap and Vast vectorizers. Nobayashi and Eoyang <ref> [18] </ref> extended the work of Callahan, Dongarra, and Levine with measurements of three more machines and additional kernels. They also made some performance measurements for these machines.
Reference: [19] <author> D. Padua and M. Wolfe. </author> <title> Advanced Compiler Optimization for Supercomputers. </title> <journal> CACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December, </month> <year> 1986. </year>
Reference-contexts: INTRODUCTION 1.1 Motivation and goals Over the years a large number of techniques have been developed to transform a sequential program so that it runs efficiently on a parallel architecture <ref> [19] </ref>. Many of these techniques have been incorporated into the compilers of these machines. These compilers are known as parallelizing compilers. Despite the wealth of research on new restructuring techniques, little work has been done on evaluating their effectiveness.
Reference: [20] <author> David A. Padua and Michael J. Wolfe. </author> <title> Advanced Compiler Optimizations for Supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: We will use these performance gains and losses to evaluate the effectiveness of these techniques. We will also investigate the underlying reasons for these successes or failures by examining whether and how the transformations were applied in important program sections. We refer to <ref> [20] </ref> for an introduction to restructuring techniques.
Reference: [21] <author> Paul Petersen and David Padua. </author> <title> Experimental Evaluation of Some Data Dependence Tests (Extended Abstract). </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> February </month> <year> 1991. </year> <type> CSRD Report 1080. </type>
Reference-contexts: In addition, there are complementing techniques not commonly called transformations, but which are significant parts of restructuring compilers. Examples are last-value assignments and loop-normalizations. We did not evaluate data dependence tests; such measurements are being done in a complementary project <ref> [21] </ref>. Another important basis of restructuring compilers that we have not covered is the set of analysis techniques, such as the life-time analysis.
Reference: [22] <author> Paul Petersen and David Padua. </author> <title> Machine-Independent Evaluation of Parallelizing Compilers. In Advanced Compilation Techniques for Novel Architectures, </title> <month> January </month> <year> 1992. </year>
Reference-contexts: The need for more comprehensive studies has been pointed out, and more recent work has measured performance results of automatic parallelizers on a representative set of real programs <ref> [5, 10, 22] </ref>. However, very few papers have reported evaluation measures of individual restructuring techniques [6, 10]. This thesis will measure both overall performance improvements from automatic parallelization and the 3 contribution of individual restructuring techniques. <p> An interactive compiler is also evaluated. Singh and Hennessy [24] reported the performance of three programs transformed by Vast for 5 vector-concurrent execution and run on an Alliant FX/8. They also measured the impact of varying amounts of user intervention on the speeds of the codes. Petersen and Padua <ref> [22] </ref> determined the effectiveness of Kap on the Perfect Benchmarks and several linear algebra routines by comparing the achieved and optimal speedups on an ideal machine. There has also been studies that determine the effectiveness of individual restructuring techniques on parallelizing codes.
Reference: [23] <author> M. Berry; D. Chen; P. Koss; D. Kuck; L. Pointer, S. Lo; Y. Pang; R. Roloff; A. Sameh; E. Clementi, S. Chin; D. Schneider; G. Fox; P. Messina; D. Walker, C. Hsiung; J. Schwarzmeier; K. Lue; S. Orszag; F. Seidl, O. Johnson; G. Swanson; R. Goodrum, and J. Martin. </author> <title> The Perfect Club Benchmarks: Effective Performance Evalution of Supercomputers. </title> <booktitle> Int'l. Journal of Supercomputer Applications, Fall 1989, </booktitle> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: The program suite that we use as a representation of the "real world" is the Perfect Benchmarks T M suite <ref> [23] </ref>, and we are subject to all its caveats. These are: missing I/O information, compromised data sets, and missing throughput measures. Furthermore, one can always question the representativeness of these codes. There is no ideal way to avoid these problems. <p> also believe that from our observations in previous chapters and from the work done in [8] and [11], that these transformations will be useful for other codes as well. 6.1 A case study: parallelization of the code adm Adm simulates air pollution concentration and deposition patterns for lake shore environments <ref> [23] </ref>. It is 6104 lines long and consists of 97 subroutines. The execution time is spread evenly throughout the program; 90% of the execution time is spent in 23 subroutines. Almost all of these subroutines contain 1-3 loop nests, all of which are important.
Reference: [24] <author> J.P. Singh and J.L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Cheng and Pase [5] measured the speedups in execution times of vectorization and concurrent execution of 25 programs including the Perfect Benchmarks. The measurements were made on a Cray Y-MP and several compilers, including Kap. An interactive compiler is also evaluated. Singh and Hennessy <ref> [24] </ref> reported the performance of three programs transformed by Vast for 5 vector-concurrent execution and run on an Alliant FX/8. They also measured the impact of varying amounts of user intervention on the speeds of the codes.
References-found: 24

