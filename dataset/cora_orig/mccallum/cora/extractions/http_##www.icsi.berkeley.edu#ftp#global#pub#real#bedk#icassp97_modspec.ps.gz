URL: http://www.icsi.berkeley.edu/ftp/global/pub/real/bedk/icassp97_modspec.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/ftp/global/pub/real/bedk/
Root-URL: http://www.icsi.berkeley.edu
Email: fsteveng,bedkg@icsi.berkeley.edu  
Title: THE MODULATION SPECTROGRAM: IN PURSUIT OF AN INVARIANT REPRESENTATION OF SPEECH  
Author: Steven Greenberg and Brian E. D. Kingsbury 
Address: 1947 Center Street, Suite 600, Berkeley, CA 94704, USA  Berkeley, Berkeley, CA 94704, USA  
Affiliation: International Computer Science Institute,  Department of Linguistics Department of Electrical Engineering and Computer Sciences University of California at  
Abstract: Understanding the human ability to reliably process and decode speech across a wide range of acoustic conditions and speaker characteristics is a fundamental challenge for current theories of speech perception. Conventional speech representations such as the sound spectrogram emphasize many spectro-temporal details that are not directly germane to the linguistic information encoded in the speech signal and which consequently do not display the perceptual stability characteristic of human listeners. We propose a new representational format, the modulation spectrogram, that discards much of the spectro-temporal detail in the speech signal and instead focuses on the underlying, stable structure incorporated in the low-frequency portion of the modulation spectrum distributed across critical-band-like channels. We describe the representation and illustrate its stability with color-mapped displays and with results from automatic speech recognition experiments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Homer Dudley. </author> <title> Remaking speech. </title> <journal> JASA, </journal> <volume> 11(2) </volume> <pages> 169-177, </pages> <month> October </month> <year> 1939. </year>
Reference-contexts: In the late 1930's the developers of the vocoder found that it was possible to synthesize intelligible, high-quality speech based on a ten-channel spectral estimate with roughly 300-Hz resolution that was low-pass filtered at 25 Hz <ref> [1] </ref>. More recently, in a study on the intelligibility of temporally-smeared speech, Drullman and colleagues have demonstrated that modulations at rates above 16 Hz are not required for speech intelligibility [2].
Reference: [2] <author> Rob Drullman, Joost M. Festen, and Reinier Plomp. </author> <title> Effect of temporal envelope smearing on speech reception. </title> <journal> JASA, </journal> <volume> 95(2) </volume> <pages> 1053-1064, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: More recently, in a study on the intelligibility of temporally-smeared speech, Drullman and colleagues have demonstrated that modulations at rates above 16 Hz are not required for speech intelligibility <ref> [2] </ref>. A representation that focuses on slow modulations in speech also has compelling parallels to the dynamics of speech production, in which the articulators move at rates of 2-12 Hz [3], and to the sensitivity of auditory cortical neurons to amplitude-modulations at rates below 20 Hz [4]. 2.
Reference: [3] <author> Caroline L. Smith, Catherine P. Browman, Richard S. McGowan, and Bruce Kay. </author> <title> Extracting dynamic parameters from speech movement data. </title> <journal> JASA, </journal> <volume> 93(3) </volume> <pages> 1580-1588, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: A representation that focuses on slow modulations in speech also has compelling parallels to the dynamics of speech production, in which the articulators move at rates of 2-12 Hz <ref> [3] </ref>, and to the sensitivity of auditory cortical neurons to amplitude-modulations at rates below 20 Hz [4]. 2.

Reference: [5] <author> Donald D. Greenwood. </author> <title> Critical bandwidth and the frequency coordinates of the basilar membrane. </title> <journal> JASA, </journal> <volume> 33 </volume> <pages> 1344-1356, </pages> <year> 1961. </year>
Reference-contexts: The modulation spectrogram represents modulation frequencies in the speech signal between 0 and 8 Hz, with a peak sensitivity at 4 Hz, corresponding closely to the long-term modulation spectrum of speech. The modulation spectrogram is computed in critical-band-wide channels <ref> [5] </ref> to match the frequency resolution of the auditory system, incorporates a simple automatic gain control and emphasizes spectro-temporal peaks. to produce the modulation spectrogram. Incoming speech, sampled at 8 kHz, is analyzed into approximately critical-band-wide channels via an FIR filter bank. <p> Both the modulation spectrograms and narrow-band spectrograms cover approximately the same range of frequencies. However, the modulation spectrogram frequency axis is nonlinear in accordance with the human spatial frequency coordinates described in <ref> [5] </ref>. While the narrow-band spectrogram of the clean speech sample clearly portrays features of the speech signal such as onsets, formant trajectories, and harmonic structure, these features are all but lost in the narrow-band spectrogram of the noisy speech, where only a few spectro-temporal peaks stand out above the noise.
Reference: [6] <author> Brian E. D. Kingsbury and Nelson Morgan. </author> <title> Recognizing reverberant speech with RASTA-PLP. </title> <booktitle> In Proc. ICASSP-97. IEEE, </booktitle> <year> 1997. </year>
Reference-contexts: Aside from the front-end processing, the recognizers are identical, using similarly-sized MLPs for phonetic probability estimation, and the same HMM word models and class bigram grammar for speech decoding. Further details on the recognition experiments are provided in <ref> [6] </ref>. Table 1 compares the performance of a recognizer using features based on the modulation spectrogram 2 with the performance of a recognizer that uses PLP features [7]. 5.
Reference: [7] <author> Hynek Hermansky. </author> <title> Perceptual linear predictive (PLP) analysis of speech. </title> <journal> JASA, </journal> <volume> 87(4) </volume> <pages> 1738-1752, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Further details on the recognition experiments are provided in [6]. Table 1 compares the performance of a recognizer using features based on the modulation spectrogram 2 with the performance of a recognizer that uses PLP features <ref> [7] </ref>. 5. THE IMPORTANCE OF THE SYLLABLE IN SPEECH RECOGNITION A central problem in speech science is the explication of the process by which the brain is able to go from sound to meaning.
Reference: [8] <author> Tammo Houtgast and Herman J. M. Steeneken. </author> <title> A review of the MTF concept in room acoustics and its use for estimating speech intelligibility. </title> <journal> JASA, </journal> <volume> 77(3) </volume> <pages> 1069-1077, </pages> <month> March </month> <year> 1985. </year>
Reference-contexts: It has been previously suggested that the broad peak at 4 Hz in the modulation spectrum corresponds to the average syllable rate <ref> [8] </ref>. Recently, we have found a more specific correlation between the distribution of low-frequency modulations in speech and the statistical distribution of syllable durations in spoken discourse [9]. It has also been shown that the concentrations of energy in the modulation spectrographic display correspond to syllabic nuclei.
Reference: [9] <author> Steven Greenberg, Joy Hollenback, and Dan Ellis. </author> <title> Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus. </title> <booktitle> In Proc. </booktitle> <address> ICSLP-96, </address> <year> 1996. </year>
Reference-contexts: It has been previously suggested that the broad peak at 4 Hz in the modulation spectrum corresponds to the average syllable rate [8]. Recently, we have found a more specific correlation between the distribution of low-frequency modulations in speech and the statistical distribution of syllable durations in spoken discourse <ref> [9] </ref>. It has also been shown that the concentrations of energy in the modulation spectrographic display correspond to syllabic nuclei.
Reference: [10] <author> Su-Lin Wu, Michael L. Shire, Steven Greenberg, and Nelson Morgan. </author> <title> Integrating syllable boundary information into speech recognition. </title> <booktitle> In Proc. ICASSP-97. IEEE, </booktitle> <year> 1997. </year>
Reference-contexts: Thus, it appears that the modulation spectrogram robustly extracts information pertaining to the syllabic segmentation of speech, and that this information is of some utility in recognizing speech under adverse acoustic conditions <ref> [10] </ref>. Two common objections to a syllabic representation of English are the relatively complex and heterogeneous syllable structure of English and the large number of syllables required to cover the lexical inventory. However, these theoretical concerns are not borne out in practice.
Reference: [11] <author> Godfrey Dewey. </author> <title> Relative Frequency of English Speech Sounds, </title> <booktitle> volume 4 of Harvard Studies in Education. </booktitle> <publisher> Harvard University Press, </publisher> <address> Cambridge, </address> <month> 1023. </month>
Reference-contexts: In written English, only 12 syllables comprise over 25% of all syllable occurrences, and 339 syllables account for 75% of all syllable occurrences <ref> [11] </ref>. Spoken English employs a similarly reduced syllabic inventory [12, 13].
Reference: [12] <author> Norman R. French, Charles W. Carter, Jr., and Walter Koenig, Jr. </author> <title> The words and sounds of telephone conversations. </title> <journal> The Bell System Technical Journal, </journal> <volume> IX:290-325, </volume> <month> April </month> <year> 1930. </year>
Reference-contexts: In written English, only 12 syllables comprise over 25% of all syllable occurrences, and 339 syllables account for 75% of all syllable occurrences [11]. Spoken English employs a similarly reduced syllabic inventory <ref> [12, 13] </ref>.
Reference: [13] <author> Steven Greenberg, Joy Hollenback, and Dan Ellis. </author> <title> The Switchboard transcription project. </title> <type> Technical report, </type> <institution> International Computer Science Institute, </institution> <year> 1997. </year>
Reference-contexts: In written English, only 12 syllables comprise over 25% of all syllable occurrences, and 339 syllables account for 75% of all syllable occurrences [11]. Spoken English employs a similarly reduced syllabic inventory <ref> [12, 13] </ref>.
Reference: [14] <author> Steven Greenberg. </author> <title> Understanding speech understanding: Towards a unified theory of speech perception. </title> <editor> In William Ainsworth and Steven Greenberg, editors, </editor> <booktitle> Proc. of the ESCA Workshop on the Auditory Basis of Speech Perception, </booktitle> <pages> pages 1-8. ESCA, </pages> <year> 1996. </year>
Reference-contexts: This model is described in more detail in <ref> [14] </ref>. 6.
References-found: 13

