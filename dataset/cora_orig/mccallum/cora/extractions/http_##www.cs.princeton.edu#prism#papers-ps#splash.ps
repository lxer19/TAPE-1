URL: http://www.cs.princeton.edu/prism/papers-ps/splash.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Title: SPLASH: Stanford Parallel Applications for Shared-Memory  
Author: Jaswinder Pal Singh, Wolf-Dietrich Weber and Anoop Gupta 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: We present the Stanford Parallel Applications for Shared-Memory (SPLASH), a set of parallel applications for use in the design and evaluation of shared-memory multiprocessing systems. Our goal is to provide a suite of realistic applications that will serve as a well-documented and consistent basis for evaluation studies. We describe the applications currently in the suite in detail, discuss some of their important characteristics, and explore their behavior by running them on a real multiprocessor as well as on a simulator of an idealized parallel architecture. We expect the current set of applications to act as a nucleus for a suite that will grow with time.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.J. Dongarra, J.L. Martin and J. Worlton, </author> <title> "Evaluating Computers and Their Performance: Perspectives, Pitfalls, and Paths," </title> <type> IBM Research Report 12904, </type> <month> April, </month> <year> 1987. </year> <title> [2] "SPEC Benchmark Suite Release 1.0," </title> <month> October, </month> <year> 1989. </year>
Reference-contexts: Drawn from several scientific and engineering problem domains, the applications are intended as a design aid for architects and software people working in the area of shared-memory multiprocessing. The use of real applications for studying system performance, however is not without pitfalls. Dongarra et al. <ref> [1] </ref> discuss some of these in the context of sequential and vector computing. <p> This information should help the user to select the applications appropriate for her studies, and it provides some idea of the coverage achieved with SPLASH. Some basic information about the different applications is presented in Table 1. Following the terminology of Dongarra et al. <ref> [1] </ref>, our applications can be characterized as whole applications.
Reference: [3] <editor> E.L. Lusk and R.A. Overbeek, </editor> <title> "Use of Monitors in FORTRAN: A Tutorial on the Barrier, Self-scheduling DO-Loop, and Askfor Monitors," </title> <type> Tech. Report No. ANL-84-51, Rev. 1, </type> <institution> Argonne National Laboratory, </institution> <month> June </month> <year> 1987. </year> <title> [4] "Using the Encore Multimax," </title> <type> Tech. Mem. No. 65, Rev. 1, </type> <institution> Math. and Comp. Sci. Division, Argonne National Laboratory, </institution> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: and people who want to be added to or deleted from the mailing list for updates should send electronic mail to that account. 3 The Programming Model Most of the programs in this suite are written in C (one is in FORTRAN), using the parmacs macros from Ar-gonne National Laboratory <ref> [3] </ref> for parallel constructs. The programs assume a number of tasks (Unix processes) operating on a single shared address space. Typically, the initial or parent process spawns off a number of child processes, one per additional processor to be used 1 .
Reference: [5] <author> J.J. Dongarra, J. Bunch, C. Moler and G. Stewart, </author> <title> "LINPACK Users' Guide," </title> <publisher> SIAM Pub., </publisher> <address> Philadelphia, </address> <year> 1976. </year>
Reference: [6] <author> H. Davis, S. Goldschmidt and J.L. Hennessy, </author> <title> "Tango: a Multiprocessor Simulation and Tracing System," </title> <type> Tech. Report No. </type> <institution> CSL-TR-90-439, Stanford University, </institution> <year> 1990. </year>
Reference-contexts: Timing measurements were made with no other user applications running on the machine. 5.2 The Simulator There are two parts to the simulator we use: the Tango reference generator <ref> [6] </ref> which runs the application and produces a parallel memory reference stream, and a memory system simulator which processes these references and feeds timing information back to the reference generator. The simulator runs on a DECstation 5000.
Reference: [7] <author> J.P. Singh and J.L. Hennessy, </author> <title> "Finding and Exploiting Parallelism in an Ocean Simulation Program: Experience, Results and Implications," </title> <note> to appear in Journal of Parallel and Distributed Computing. Also Tech. Report No. </note> <institution> CSL-TR-89-388, Stanford University, </institution> <month> Aug. </month> <year> 1989. </year> <month> 38 </month>
Reference-contexts: However, the grid-based application is well suited to parallelism. The work done every time-step essentially involves setting up and solving a set of spatial partial differential equations, details of which can be found in <ref> [7] </ref>. The continuous functions are transformed into discrete counterparts by second-order finite-differencing, and the resulting difference equations set up and solved on two-dimensional fixed-size grids representing horizontal cross-sections of the ocean basin. <p> We simulate a square grid of size 98-by-98 points, and we use the same (constant) resolution in both dimensions. The original sequential program used a block cyclic reduction algorithm to solve the elliptic equations (see <ref> [7] </ref>); the parallel programs use an iterative method: Gauss-Seidel with Successive Over Relaxation (SOR) [7, 8]. This iterative solver works well for coarse grid resolutions such as the one we use. For finer resolutions, a more sophisticated solver may be necessary. We have developed a multigrid solver for this purpose. <p> We simulate a square grid of size 98-by-98 points, and we use the same (constant) resolution in both dimensions. The original sequential program used a block cyclic reduction algorithm to solve the elliptic equations (see [7]); the parallel programs use an iterative method: Gauss-Seidel with Successive Over Relaxation (SOR) <ref> [7, 8] </ref>. This iterative solver works well for coarse grid resolutions such as the one we use. For finer resolutions, a more sophisticated solver may be necessary. We have developed a multigrid solver for this purpose. <p> After this, the outermost loop of the program iterates over a fixed number of time-steps, each performing a number of computations on entire grids. Parallelism is afforded at a hierarchy of levels, both across and within grid computations (see <ref> [7] </ref>). The high-level structure of the parallel program within a time-step is shown in Figure 1. Grid computations in the same horizontal section in this figure are independent of one another. Those in the same vertical section follow a thread of dependence. <p> The Jacobians and Laplacians are near-neighbor computations (9-point and 5-point stencils, respectively) with different input and output arrays, while the equation solution is an in-place near-neighbor iteration to convergence with a 5-point stencil. An understanding of the equation system and solution method can be obtained from <ref> [7] </ref>.
Reference: [8] <author> G.H. Golub and C.F. Van Loan, </author> <title> Matrix Computations, Second Edition, </title> <journal> Chap. </journal> <volume> 10, </volume> <publisher> The Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: We simulate a square grid of size 98-by-98 points, and we use the same (constant) resolution in both dimensions. The original sequential program used a block cyclic reduction algorithm to solve the elliptic equations (see [7]); the parallel programs use an iterative method: Gauss-Seidel with Successive Over Relaxation (SOR) <ref> [7, 8] </ref>. This iterative solver works well for coarse grid resolutions such as the one we use. For finer resolutions, a more sophisticated solver may be necessary. We have developed a multigrid solver for this purpose.
Reference: [9] <author> C.W.Gear, </author> <title> Numerical Initial Value Problems in Ordinary Differential Equations, </title> <publisher> Prentice-Hall, </publisher> <address> New Jersey, </address> <year> 1971. </year>
Reference-contexts: The computation is performed over a user-specified number of time-steps, hopefully allowing the system to reach a steady state. Every time-step involves setting up and solving the Newtonian equations of motion for water molecules in a cubical box with periodic boundary conditions, using Gear's sixth-order predictor-corrector method <ref> [9] </ref>. The total potential is computed as the sum of intra- and intermolecular potentials. To avoid computing all the n 2 2 pairwise interactions among molecules, a spherical cutoff range is used with radius equal to half the box length.
Reference: [10] <author> J.P. Singh and J.L. Hennessy, </author> <title> "Data Locality and Memory System Performance in the Parallel Simulation of Ocean Eddy Currents," </title> <booktitle> Proceedings of the Second Symposium on High Performance Computing, </booktitle> <address> Montpelier, France, </address> <month> October </month> <year> 1991. </year> <note> Also Tech. Report. No. </note> <institution> CSL-TR-91-490, Stanford University, </institution> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Finally, the near-neighbor communication patterns also allow convenient exploitation of geographic locality in an interconnection network with nonuniform distances between processors (for example, a mesh or hypercube). A more detailed discussion of data locality and memory system performance in this application can be found in <ref> [10] </ref>. 7.2.2 Synchronization and Granularity Mutual exclusion, enforced with locks, is required in obtaining a process identifier and in two other situations in this application: when every process accumulates its private sum into a shared sum in computing a matrix integral, and when processors communicate through a shared convergence flag in
Reference: [11] <author> J.P. Singh and J.L. Hennessy, </author> <title> "Automatic and Explicit Parallelization of an N-body Simulation," </title> <note> submitted for publication. </note>
Reference-contexts: The box length is computed by the program to be large enough to hold all the molecules. Double-precision accuracy is required for this simulation, which can be used to predict a variety of static and dynamic properties of liquid water. Further documentation of the program can be found in <ref> [11] </ref>, and details of the physical models in [12, 13, 14]. The sequential program, written in FORTRAN, is one of the Perfect Club set of supercomputing bench marks [15].
Reference: [12] <author> G.C. Lie and E.Clementi, </author> <title> "Molecular-Dynamics Simulation of Liquid Water with an ab initio Flexible Water-Water Interaction Potential," </title> <journal> Physical Review, </journal> <volume> Vol. A33, </volume> <pages> pp. 2679 ff., </pages> <year> 1986. </year>
Reference-contexts: Double-precision accuracy is required for this simulation, which can be used to predict a variety of static and dynamic properties of liquid water. Further documentation of the program can be found in [11], and details of the physical models in <ref> [12, 13, 14] </ref>. The sequential program, written in FORTRAN, is one of the Perfect Club set of supercomputing bench marks [15].
Reference: [13] <author> O. Matsuoka, E.Clementi and M. Yoshimine, </author> <title> "CI Study of the Water Dimer Potential Surface," </title> <journal> Journal of Chemical Physics, </journal> <volume> Vol. 64, No. 4, </volume> <pages> pp. 1351-61, </pages> <month> Feb. </month> <year> 1976. </year>
Reference-contexts: Double-precision accuracy is required for this simulation, which can be used to predict a variety of static and dynamic properties of liquid water. Further documentation of the program can be found in [11], and details of the physical models in <ref> [12, 13, 14] </ref>. The sequential program, written in FORTRAN, is one of the Perfect Club set of supercomputing bench marks [15].
Reference: [14] <author> R. Bartlett, I. Shavitt and G. Purvis, </author> <title> "The Quartic Force Field of H 2 O Determined by Many-Body Methods that Include Quadruple Excitation Effects," </title> <journal> Journal of Chemical Physics, </journal> <volume> Vol. 71, No. 1, </volume> <pages> pp. 281-291, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: Double-precision accuracy is required for this simulation, which can be used to predict a variety of static and dynamic properties of liquid water. Further documentation of the program can be found in [11], and details of the physical models in <ref> [12, 13, 14] </ref>. The sequential program, written in FORTRAN, is one of the Perfect Club set of supercomputing bench marks [15].
Reference: [15] <author> M. Berry et. al., </author> <title> "The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers," </title> <type> CSRD Report No. 827, </type> <institution> Center for Supercomputing Research and Develpment, Urbana, Illinois, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Further documentation of the program can be found in [11], and details of the physical models in [12, 13, 14]. The sequential program, written in FORTRAN, is one of the Perfect Club set of supercomputing bench marks <ref> [15] </ref>. The parallel program is written in C, with significantly modified data structures. 8.1 Principal Data Structures The main data structure used in the Perfect Club benchmark is a large, one-dimensional array called VAR.
Reference: [16] <author> J.E. Barnes and P. Hut, </author> <title> "A Hierarchical O(N log N) Force Calculation Algorithm", </title> <journal> Nature, </journal> <volume> Vol. 324, No. 4, </volume> <pages> pp. 446-449, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Since an O (n 2 ) complexity makes simulating large systems impractical, hierarchical tree-based methods have been developed that reduce the complexity to O (n log n) <ref> [16] </ref> for general distributions, or even O (n) for uniform distributions [20]. This application uses the O (n log n) Barnes-Hut algorithm 6 . The Barnes-Hut algorithm is based on a hierarchical octree representation of space in three dimensions (in two dimensions, a quadtree representation is used) 7 .
Reference: [17] <author> G.C. Fox, </author> <title> "A Graphical Approach to Load Balancing and Sparse Matrix Vector Multiplication on the Hypercube", in Numerical Algorithms for Modern Parallel Computer Architectures, </title> <editor> ed. M. Schultz, </editor> <publisher> Springer-Verlag, </publisher> <year> 1988, </year> <pages> pp. 37-62. </pages>
Reference-contexts: 17 20 29 32 2726 22 23 34 35 46 47 64615249 53 56 57 60 4342 38 39 A split partition assigned to the same processor Costzones ORB Proc 1 Proc 2 Proc 3 Proc 4 Proc 5 Proc 6 Proc 7 Proc 8 Orthogonal Recursive Bisection (ORB) ORB <ref> [17] </ref> is a more robust technique for spatial locality which actually partitions space rather than the Barnes-Hut tree. The tree is not used in the partitioning process at all.
Reference: [18] <author> J.K. Salmon, </author> <title> "Parallel Hierarchical N-body Methods", </title> <type> Ph.D. Thesis, </type> <institution> California Insitute of Technology, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Further details of implementing ORB are omitted for reasons of space; a greater description of its application to this problem can be found in <ref> [18] </ref>. ORB introduces several new data structures|including a separate binary ORB tree whose nodes are the recursively subdivided subspaces with their processor subsets, and whose leaves are the final spatial partitions|has a lot more runtime overhead, and is far more complex to implement than the tree-partitioning scheme we described above.
Reference: [19] <author> J.P. Singh, J.L. Hennessy and A. Gupta, </author> <title> "Implications of Hierarchical N-Body Techniques for Multiprocessor Architecture", </title> <type> Technical Report CSL-TR-92-506, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: These parameters make the following contributions to 22 Table 7: Barnes-Hut Information (Simulator). Number of Miss Rate Synchronization Processors (%) Waiting Time (%) 1 0.00 0.00 8 0.11 1.32 32 0.24 3.32 the total simulation error <ref> [19] </ref>: * n: The error from the increased relaxation rate due to Monte Carlo sampling scales as 1 p n ; thus, an increase in n by a factor of k leads to a decrease in simulation error by a factor of p * t: The leap-frog method used to integrate <p> Results for how communication to computation scale under different scaling models using this rule can be found in <ref> [19] </ref>. 10 MP3D MP3D solves a problem in rarefied fluid flow simulation. Rarefied flow problems are of interest to aerospace researchers who study the forces exerted on space vehicles as they pass through the upper atmosphere at hypersonic speeds.
Reference: [20] <author> L. Greengard and V. Rokhlin, </author> <title> "A Fast Algorithm for Particle Simulation", </title> <journal> Journal of Computational Physics, </journal> <volume> Vol. 73, No. 325, </volume> <year> 1987. </year>
Reference-contexts: Since an O (n 2 ) complexity makes simulating large systems impractical, hierarchical tree-based methods have been developed that reduce the complexity to O (n log n) [16] for general distributions, or even O (n) for uniform distributions <ref> [20] </ref>. This application uses the O (n log n) Barnes-Hut algorithm 6 . The Barnes-Hut algorithm is based on a hierarchical octree representation of space in three dimensions (in two dimensions, a quadtree representation is used) 7 . <p> We look forward to expanding the set with other parallel applications from the user community. In the near future, we will add the following applications or versions of applications: * A galactic simulation using the Fast Multipole Method <ref> [20] </ref>. * A radiosity application from computer graphics that uses a hierarchical solution method. * A version of the Ocean application with a multigrid solver instead of SOR. * A version of the Water application with the spatial data structure to make the intermolecular force calculation algorithm O (n).
Reference: [21] <author> J.P. Singh, C. Holt, T. Totsuka, A. Gupta and J.L. Hennessy, </author> <title> "Load Balancing and Data Locality in Hierarchical N-body Methods", </title> <type> Technical Report CSL-TR-92-505, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: the nonuniformity of the domain, which leads to highly nonuniform distributions of workload and communication among the units of parallelism (particles); the dynamically changing nature of the particle distribution; the need for unstructured, long-range communication; and the fact that different phases of computation (see Figure 7) have different preferred partitionings <ref> [21] </ref>. This program focuses its partitioning efforts on the force-computation phase, since it is the most time-consuming. The partitioning is not modified for other phases since the overhead of doing so (both in partitioning and in the loss of locality) outweighs the potential benefits. <p> The program uses profiled load balancing combined with one of the following two techniques (determined by a compile-time flag, see Section 9.3) to obtain data locality <ref> [21] </ref>. Costzones The Barnes-Hut algorithm already has a representation of the spatial distribution encoded in its tree data structure. In this partitioning scheme, the tree is conceptually laid out in a two-dimensional plane, with a cell's children laid out from left to right in increasing order of child number.
Reference: [22] <author> David R. Cheriton, Hendrik A. Goosen, and Philip Machanick, </author> <title> "Restructuring a parallel simulation to improve cache behavior in a shared-memory multiprocessor: A first experience, </title> <booktitle> 1990," to appear in Proc. International Symposium on Shared-Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: MP3D was developed and initially parallelized in the Aeronautics and Astronautics department at Stanford. Several enhanced versions have since been developed, and a restructuring study is described in <ref> [22] </ref>. 10.1 Principal Data Structures Two large arrays of structures account for more than 99% of the static data space used by MP3D. The first one stores the state information for each molecule, and occupies 36 bytes per molecule. <p> In the 64-processor run, the overall miss rate was 22.1%. These misses are almost entirely invalidation misses. Since each molecule is assigned to a fixed processor, the space array is responsible for most of the misses. Assigning regions of the space array to different processors, as was done in <ref> [22] </ref>, is one approach to reducing the number of space array misses. With a realistic problem, we would expect the total data space used by the application to be much larger than the available cache space.
Reference: [23] <author> Jeffrey D. McDonald, </author> " <title> A direct particle simulation method for hypersonic rarified flow," </title> <type> CS 411 Final Project Report, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: After a steady-state is reached, statistical analysis of the trajectory data produces an estimated flow field for the configuration under study. To obtain accurate results, such methods require large amounts of computation. Vectorized and parallelized codes have, therefore, been developed <ref> [23] </ref>. MP3D employs five degree-of-freedom simulation of idealized diatomic molecules in a three-dimensional active space. There are three translational freedoms and two rotational energy modes. The active space is a rectangular tunnel with openings at each end and reflecting walls on the remaining sides.
Reference: [24] <author> J.S. Rose, "LocusRoute: </author> <title> a parallel global router for standard cells," </title> <booktitle> Proc. 25th Design Automation Conference, </booktitle> <pages> pages 189-195, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: With a realistic problem, we would expect the total data space used by the application to be much larger than the available cache space. This means that the application basically sweeps the caches during each time-step, resulting in high miss rates due to replacement. 11 LocusRoute LocusRoute <ref> [24, 25, 26] </ref> is a commercial quality VLSI standard cell router. It is used to evaluate standard cell circuit placements by routing them efficiently and determining the area of the resulting layout.
Reference: [25] <author> J.S. Rose, </author> <title> "The parallel decomposition and implementation of an integrated circuit global router," </title> <booktitle> ACM Sigplan Symposium on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <pages> pages 138-145, </pages> <month> July </month> <year> 1988. </year> <month> Sep. </month> <year> 1990. </year>
Reference-contexts: With a realistic problem, we would expect the total data space used by the application to be much larger than the available cache space. This means that the application basically sweeps the caches during each time-step, resulting in high miss rates due to replacement. 11 LocusRoute LocusRoute <ref> [24, 25, 26] </ref> is a commercial quality VLSI standard cell router. It is used to evaluate standard cell circuit placements by routing them efficiently and determining the area of the resulting layout.
Reference: [26] <author> J.S. Rose, </author> <title> "Parallel global routing for standard cells", </title> <journal> IEEE Trans. Computer-Aided Design of Circuits and Systems, </journal> <month> September </month> <year> 1990. </year> <month> 39 </month>
Reference-contexts: With a realistic problem, we would expect the total data space used by the application to be much larger than the available cache space. This means that the application basically sweeps the caches during each time-step, resulting in high miss rates due to replacement. 11 LocusRoute LocusRoute <ref> [24, 25, 26] </ref> is a commercial quality VLSI standard cell router. It is used to evaluate standard cell circuit placements by routing them efficiently and determining the area of the resulting layout.
Reference: [27] <author> K. M. Chandy and J. Misra, </author> <title> "Asynchronous Distributed Simulation Via a Sequence of Parallel Computa--tions," </title> <journal> Comm of the ACM, </journal> <volume> 24:11, </volume> <pages> pages 198-206, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: Number of Miss Rate Synchronization Processors (%) Waiting Time (%) 1 0.20 0.00 4 1.42 0.00 16 3.17 0.00 64 5.08 0.02 PTHOR uses a variant of the Chandy-Misra <ref> [27] </ref> distributed-time algorithm (denoted CM). The CM algorithm will be described only briefly here; for an in-depth treatment see [28].
Reference: [28] <author> Larry Soule and Anoop Gupta. </author> <title> "Analysis of parallelism and deadlocks in distributed-time logic simulation," </title> <type> Technical Report CSL-TR-89-378, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Number of Miss Rate Synchronization Processors (%) Waiting Time (%) 1 0.20 0.00 4 1.42 0.00 16 3.17 0.00 64 5.08 0.02 PTHOR uses a variant of the Chandy-Misra [27] distributed-time algorithm (denoted CM). The CM algorithm will be described only briefly here; for an in-depth treatment see <ref> [28] </ref>. While the standard event-driven algorithm maintains a common value of the current simulated time for the entire circuit, CM allows every element to advance its own value of time independently of other elements. As a result, different elements might have different notions of the current simulated time.
Reference: [29] <author> I. Duff, R. Grimes, and J. Lewis, </author> <title> "Sparse matrix test problems," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: The program also verifies that the computed factor is correct once the factorization is complete. Two input matrices are provided. Both come from the Boeing/Harwell sparse matrix test set <ref> [29] </ref>. bcsttk14 is a 1806-by-1806 matrix with 30,824 non-zeros in the matrix and 110,461 in the factor; it has 503 distinct supernodes, the largest of which contains 135 columns. The corresponding numbers for the larger matrix bcsttk15 are 3948-by-3948, 56934, 647274, 1295 and 211, respectively.
Reference: [30] <author> A. George, M. Heath, J. Liu, and E. Ng, </author> <title> "Solution of sparse positive definite systems on a hypercube," </title> <type> Technical Report TM-10865, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1988. </year>
Reference-contexts: This step is typically the most time-consuming, and is parallelized in our program. The numerical factorization approach we use is very efficient, due to the use of supernodal elimination techniques. The approach is a dynamic version of the supernodal fan-out method [32], an enhancement of the fan-out method of <ref> [30] </ref>. Supernodes are sets of columns with nearly identical non-zero structures, and a factor matrix will typically contain a number of often very large supernodes. 13.1 Principal Data Structures The primary data structure in this program is the representation of the sparse matrix itself.
Reference: [31] <author> A. George and J. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: The row number of a particular non-zero is available through the row [] and startrow [] fields. Row numbers are stored in a compressed manner in order to conserve space. Details of the compression and other issues relating to the data structure can be found in <ref> [31] </ref>. <p> The corresponding numbers for the larger matrix bcsttk15 are 3948-by-3948, 56934, 647274, 1295 and 211, respectively. Both matrices have been reordered using the minimum degree heuristic <ref> [31] </ref>. 36 The program prints some numbers describing the matrix and the execution. It also outputs the execution time and MFLOPS rate of the factorization. 13.5 Results The results we present are for the factorization of the two Boeing/Harwell matrices included with the program.
Reference: [32] <author> E. Rothberg and A. Gupta, </author> <title> "Techniques for improving the performance of sparse factorization on multiprocessor workstations," </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <month> November, </month> <year> 1990. </year> <month> 40 </month>
Reference-contexts: This step is typically the most time-consuming, and is parallelized in our program. The numerical factorization approach we use is very efficient, due to the use of supernodal elimination techniques. The approach is a dynamic version of the supernodal fan-out method <ref> [32] </ref>, an enhancement of the fan-out method of [30].
References-found: 30

