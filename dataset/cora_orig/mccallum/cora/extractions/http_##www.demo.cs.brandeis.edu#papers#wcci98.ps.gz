URL: http://www.demo.cs.brandeis.edu/papers/wcci98.ps.gz
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Title: A Gradient Descent Method for a Neural Fractal Memory  
Author: Ofer Melnik and Jordan Pollack, Volen 
Keyword: Recurrent Neural Networks, Dynamical Systems, Fractals, Iterated Function Systems, Inverse Fractal Problem, Learning Rules, Gradient Descent.  
Address: MA, USA  
Affiliation: Center for Complex Systems, Brandeis University, Waltham,  
Abstract: It has been demonstrated that higher order recurrent neural networks exhibit an underlying fractal attractor as an artifact of their dynamics. These fractal attractors offer a very efficent mechanism to encode visual memories in a neural substrate, since even a simple twelve weight network can encode a very large set of different images. The main problem in this memory model, which so far has remained unaddressed, is how to train the networks to learn these different attractors. Following other neural training methods this paper proposes a Gradient Descent method to learn the attractors. The method is based on an error function which examines the effects of the current network transform on the desired fractal attractor. It is tested across a bank of different target fractal attractors and at different noise levels. The results show positive performance across three error measures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.B. Pollack, </author> <title> "The induction of dynamical recognizers," </title> <journal> Machine Learning, </journal> <volume> vol. 7, </volume> <pages> pp. 227-252, </pages> <year> 1991. </year>
Reference: [2] <author> C.L. Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee, </author> <title> "Learning and extracting finite state automata with second-order recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 393-405, </pages> <year> 1992. </year>
Reference: [3] <author> M. Casey, </author> <title> "The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction," </title> <journal> Neural Computation, </journal> <volume> vol. 8, no. 6, </volume> <year> 1996. </year>
Reference: [4] <author> F.S. Tsung and G.W. Cottrell, </author> <title> "Phase-space learning for recurrent networks," </title> <type> Tech. Rep. </type> <institution> CS93-285, Dept. Computer Science and Engineering, University of California, San-Diego, </institution> <year> 1993. </year>
Reference: [5] <author> J.B. Pollack, </author> <title> "Recursive distributed representations," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 46, </volume> <pages> pp. 77-105, </pages> <year> 1990. </year>
Reference: [6] <author> D.J. Stucki and J.B. Pollack, </author> <title> "Fractal (reconstructive analogue) memory," </title> <booktitle> in Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society. Cognitive Science Society, </booktitle> <year> 1992, </year> <pages> pp. 118-123. </pages>
Reference: [7] <author> M.F. </author> <title> Barnsley, Fractals Everywhere, </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1988. </year>
Reference: [8] <author> M.F. </author> <title> Barnsley and L.P. Hurd, Fractal Image Compression, </title> <type> AK Peters, Wellesley, </type> <year> 1992. </year>
Reference: [9] <author> E.R. Vrscay and C. J. Roehrig, </author> <title> Iterated Function Systems and the Inverse Problem of Fractal Construction Using Moments, </title> <journal> pp. </journal> <pages> 250-259, </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference: [10] <author> R. Shonkwiler, F. Mendivil, and A. Deliu, </author> <title> "Genetic algorithms for the 1-d fractal inverse problem," </title> <booktitle> in Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <address> San Diego, </address> <year> 1991. </year>
Reference: [11] <author> A. Jacquin, </author> <title> "A novel fractal block-coding technique for digital images," </title> <booktitle> in IEEE ICASSP Proc. 4. IEEE, </booktitle> <year> 1990, </year> <pages> pp. 2225-2228. </pages>
Reference: [12] <author> Y. Fisher, </author> <title> Fractal Image Compression, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference: [13] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams, </author> <title> Learning Internal Representations by Error Propagation, </title> <journal> vol. </journal> <volume> 1, chapter 8, </volume> <publisher> MIT Press, Cambride, </publisher> <address> MA, </address> <year> 1986. </year>
References-found: 13

