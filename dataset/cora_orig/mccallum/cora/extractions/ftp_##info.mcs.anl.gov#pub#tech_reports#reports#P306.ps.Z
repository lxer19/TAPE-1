URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P306.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts92.htm
Root-URL: http://www.mcs.anl.gov
Title: A Compiler Approach to Scalable Concurrent Program Design 1  
Author: Ian Foster Stephen Taylor 
Affiliation: Argonne National Laboratory and  California Institute of Technology  
Abstract: The programmer's most powerful tool for controlling complexity in program design is abstraction. We seek to use abstraction in the design of concurrent programs, so as to separate design decisions concerned with decomposition, communication, synchronization, mapping, granularity, and load balancing. This paper describes programming and compiler techniques intended to facilitate this design strategy. The programming techniques are based on a core programming notation with two important properties: the ability to separate concurrent programming concerns, and extensibility with reusable programmer-defined abstractions. The compiler techniques are based on a simple transformation system together with a set of compilation transformations and portable run-time support. The transformation system allows programmer-defined abstractions to be defined as source-to-source transformations that convert abstractions into the core notation. The same transformation system is used to apply compilation transformations that incrementally transform the core notation toward an abstract concurrent machine. This machine can be implemented on a variety of concurrent architectures using simple run-time support. The transformation, compilation, and run-time system techniques have been implemented and are incorporated in a public-domain program development toolkit. This toolkit operates on a wide variety of networked workstations, multicomputers, and shared-memory multiprocessors. It includes a program transformer, concurrent compiler, syntax checker, debugger, performance analyzer, and execution animator. A variety of substantial applications have been developed using the toolkit, in areas such as climate modeling and fluid dynamics. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agha, G., </author> <title> Actors, </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Alternatively, specialized versions of the system can be developed for architectures of particular interest, by retargeting the final stage of the compiler. 2 Related Work The benefits of an architecturally independent model of parallel computation have been widely recognized in the computer science community <ref> [29, 28, 25, 1, 7] </ref>. The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming [11, 24], functional programming [28, 26, 9], and object-oriented programming [1]. <p> The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming [11, 24], functional programming [28, 26, 9], and object-oriented programming <ref> [1] </ref>. Similarly, concurrent composition underlies such diverse approaches as CSP [29], concurrent logic programming, functional programming, and Unity [7]. Unfortunately, these models either do not support concurrent source-to-source transformations or embed the basic ideas in complex language designs and programming paradigms that have little to do with concurrent programming.
Reference: [2] <author> Babb, R., </author> <title> Parallel processing with large grain data flow techniques, </title> <journal> IEEE Computer, </journal> <volume> 17(7), </volume> <pages> 55-61, </pages> <year> 1984. </year>
Reference-contexts: We consider the basic ideas to be sufficient in and of themselves and have worked to develop them as a practical basis for concurrent programming [19]. The integration of sequential and concurrent programs has been the focus of a number of other systems, notably large-grain dataflow and Linda <ref> [2, 6] </ref>. However, we insist upon a clear separation of sequential and concurrent components in order to conveniently apply source-to-source transformation techniques and build programming abstractions. Previous work on reusable abstractions in parallel program design include the Argonne monitor macros [4] and Schedule package [17], and Cole's algorithmic skeletons [14].
Reference: [3] <author> Backus, J., </author> <title> Can programming be liberated from the von Neumann style? A functional style and its algebra of processes, </title> <journal> CACM, </journal> <volume> 21, </volume> <pages> 613-41, </pages> <year> 1978. </year>
Reference-contexts: However, we prefer to use compile-time methods based on source-to-source transformations so as to avoid run-time overheads and achieve our goals of 5 efficient communication, synchronization, and sequential execution. The use of "meta--programs" to specify program transformations is common in declarative programming <ref> [3, 28, 38, 12, 5, 42] </ref>. Novel features of our approach include the integration of a programmable transformer into the compilation pipeline, linguistic support for invocation of transformations, and the use of set-oriented abstractions for specifying transformations.
Reference: [4] <author> Boyle, J., Butler, R., Disz, T., Glickfeld, B., Lusk, E., Overbeek, R., Patterson, J., and Stevens, R., </author> <title> Portable Programs for Parallel Processors, </title> <publisher> Holt, Rinehart, and Winston, </publisher> <year> 1987. </year>
Reference-contexts: However, we insist upon a clear separation of sequential and concurrent components in order to conveniently apply source-to-source transformation techniques and build programming abstractions. Previous work on reusable abstractions in parallel program design include the Argonne monitor macros <ref> [4] </ref> and Schedule package [17], and Cole's algorithmic skeletons [14]. However, in none of these approaches is support for abstractions incorporated into a compiler. An alternative to our compiler techniques is to use run-time techniques such as higher-order functions [28, 31].
Reference: [5] <author> Boyle, J., and Muralidharan, M., </author> <title> Program reusability through program transformations, </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> SE-10(5), </volume> <pages> 574-588, </pages> <year> 1984. </year>
Reference-contexts: However, we prefer to use compile-time methods based on source-to-source transformations so as to avoid run-time overheads and achieve our goals of 5 efficient communication, synchronization, and sequential execution. The use of "meta--programs" to specify program transformations is common in declarative programming <ref> [3, 28, 38, 12, 5, 42] </ref>. Novel features of our approach include the integration of a programmable transformer into the compilation pipeline, linguistic support for invocation of transformations, and the use of set-oriented abstractions for specifying transformations.
Reference: [6] <author> Carriero, N., and Gelernter, D., </author> <title> How to Write Parallel Programs, </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: We consider the basic ideas to be sufficient in and of themselves and have worked to develop them as a practical basis for concurrent programming [19]. The integration of sequential and concurrent programs has been the focus of a number of other systems, notably large-grain dataflow and Linda <ref> [2, 6] </ref>. However, we insist upon a clear separation of sequential and concurrent components in order to conveniently apply source-to-source transformation techniques and build programming abstractions. Previous work on reusable abstractions in parallel program design include the Argonne monitor macros [4] and Schedule package [17], and Cole's algorithmic skeletons [14].
Reference: [7] <author> Chandy, K. M., and Misra, J. </author> <title> Parallel Program Design, </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year> <month> 34 </month>
Reference-contexts: Alternatively, specialized versions of the system can be developed for architectures of particular interest, by retargeting the final stage of the compiler. 2 Related Work The benefits of an architecturally independent model of parallel computation have been widely recognized in the computer science community <ref> [29, 28, 25, 1, 7] </ref>. The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming [11, 24], functional programming [28, 26, 9], and object-oriented programming [1]. <p> The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming [11, 24], functional programming [28, 26, 9], and object-oriented programming [1]. Similarly, concurrent composition underlies such diverse approaches as CSP [29], concurrent logic programming, functional programming, and Unity <ref> [7] </ref>. Unfortunately, these models either do not support concurrent source-to-source transformations or embed the basic ideas in complex language designs and programming paradigms that have little to do with concurrent programming. Furthermore, few approaches are developed to the point where they can be used to develop large-scale applications.
Reference: [8] <author> Chandy, K. M., and Taylor, S., </author> <title> An Introduction to Parallel Programming, </title> <editor> Jones and Bartlett, </editor> <year> 1991. </year>
Reference-contexts: These concepts are language independent and have been incorporated into a commercially available programming system, Strand [21]. In this paper, we work with a second-generation system in which programs are expressed in a program composition notation (PCN) <ref> [8] </ref>. This notation provides a uniform treatment of concurrent composition, non-deterministic choice, and sequential programming. In addition, a simple syntax and the use of recursively-defined data structures allows PCN programs to be represented concisely as data structures. <p> In particular, procedures can be executed in any order or in parallel. A consequence of monotonicity and concurrent execution is that it is not important where and when procedures execute. Hence, decisions concerning partitioning, mapping, 7 and granularity can be isolated from the rest of the program design process <ref> [8] </ref>. Choice. Programs must inevitably choose between alternative actions; this choice is based on the values of variables. We adopt a simple method of specifying program actions that makes such choices explicit and avoids overspecification [16]. This is illustrated in the minimum procedure. <p> We employ these concepts under the constraint that shared variables are constant, or monotone, during concurrent execution. This constraint can be enforced by the programmer [21] or by the compiler using copying <ref> [8] </ref>. In this context, a procedure expressed in a conventional language such as C, C++, or Fortran can be viewed as an atomic black box. This box simply computes an input-output relation. <p> These techniques support the organization of arbitrary communication protocols, termination detection in distributed computations, the construction of distributed data structures, and the implementation of atomic transactions <ref> [21, 8] </ref>. 4 Example Programming Problem Throughout the rest of this paper, we will repeatedly return to a single example program to demonstrate our programming, compilation, and run-time techniques. This program is a simplied implementation of an application developed to simulate the atmospheric circulation over the globe [10]. <p> Core PCN programs simply receive messages in the guard, modify local state and/or spawn more processes; process synchronization occurs only in the guard components of a program. 21 The operational semantic of a Core PCN program consists of a subset of the semantic for PCN programs <ref> [8] </ref>; it is identical to that of Strand [21] except that atomic actions may modify data structures. If any guard G i is true, the associated atomic actions are executed, and concurrent processes are then spawned. If all guards are false, then the default action is executed.
Reference: [9] <author> Chen, M., Choo, Y., and Li, J., </author> <title> Compiling parallel programs by optimizing performance, </title> <journal> J. Supercomputing, </journal> <volume> 1(2), </volume> <pages> 171-207, </pages> <year> 1988. </year>
Reference-contexts: The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming [11, 24], functional programming <ref> [28, 26, 9] </ref>, and object-oriented programming [1]. Similarly, concurrent composition underlies such diverse approaches as CSP [29], concurrent logic programming, functional programming, and Unity [7].
Reference: [10] <author> Chern, I., and Foster, I., </author> <title> Design and parallel implementation of two methods for solving PDEs on the sphere, </title> <booktitle> Proc. Conf. on Parallel Computational Fluid Dynamics, </booktitle> <address> Stuttgart, Germany, </address> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1991. </year>
Reference-contexts: Currently, the emulator operates on Sun, Next, IBM, DEC, SGI, and HP workstations, on Intel iPSC 386/860/Delta and Symult S2010 multicomputers, and on Sequent Symmetry and Sun shared-memory multiprocessors. The resulting programs have impressive and predictable performance characteristics across a variety of architectures <ref> [10, 27] </ref>. An alternative abstract machine implementation technique further compiles the encoded abstract machine instructions to make use of specific architectural features. For example, most machines provide high-performance floating point accelerators. <p> This program is a simplied implementation of an application developed to simulate the atmospheric circulation over the globe <ref> [10] </ref>. The actual code comprises approximately 750 lines of PCN code, 1,400 lines of Fortran, and 870 lines of C. It executes at 2.5 Gflops on the 528-computer Intel Delta and is portable across a wide range of architectures with predictable performance characteristics [10]. <p> simulate the atmospheric circulation over the globe <ref> [10] </ref>. The actual code comprises approximately 750 lines of PCN code, 1,400 lines of Fortran, and 870 lines of C. It executes at 2.5 Gflops on the 528-computer Intel Delta and is portable across a wide range of architectures with predictable performance characteristics [10]. The code is typical of other application codes developed at Argonne National Laboratory and Caltech (e.g., [27]). These codes involve both substantial computational components, requiring efficient uniprocessor computation, and complex communication protocols, requiring efficient communication and synchronization. <p> The library, given in Program 4, incorporates solutions to three distinct problems: the partitioning of the data domain into disjoint subdomains, the organization of communication between subdomains to exchange boundary values, and the mapping of subdomains to processors in a parallel computer. As described in <ref> [10] </ref>, this code is developed by a series of refinement steps, each introducing a solution to one of these problems. The library code creates a process structure comprising 4c 2 subdomain processes. <p> It includes tools for defining program transformations, compiling concurrent programs, checking programs, debugging, performance analysis, and program animation. The toolkit has been used to design and implement substantial applications in several domains, including climate modeling and fluid dynamics <ref> [10, 27] </ref>. These programs use abstractions to coordinate the execution of thousands of lines of pre-existing C and Fortran code. Experimental studies show that the codes operate with predictable and impressive performance on a wide range of parallel computers. The toolkit can be obtained by anonymous FTP.
Reference: [11] <author> Clark, K., and Gregory, S., </author> <title> A relational language for parallel programming, </title> <booktitle> Proc. 1981 ACM Conf. on Functional Programming Languages and Computer Architectures, </booktitle> <year> 1981, </year> <pages> 171-178. </pages>
Reference-contexts: The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming <ref> [11, 24] </ref>, functional programming [28, 26, 9], and object-oriented programming [1]. Similarly, concurrent composition underlies such diverse approaches as CSP [29], concurrent logic programming, functional programming, and Unity [7].
Reference: [12] <author> Clocksin, W., and Mellish, C., </author> <title> Programming in Prolog, </title> <publisher> Springer-Verlag, </publisher> <year> 1981. </year>
Reference-contexts: However, we prefer to use compile-time methods based on source-to-source transformations so as to avoid run-time overheads and achieve our goals of 5 efficient communication, synchronization, and sequential execution. The use of "meta--programs" to specify program transformations is common in declarative programming <ref> [3, 28, 38, 12, 5, 42] </ref>. Novel features of our approach include the integration of a programmable transformer into the compilation pipeline, linguistic support for invocation of transformations, and the use of set-oriented abstractions for specifying transformations.
Reference: [13] <author> Cohen, J., </author> <title> Garbage collection of linked data structures, </title> <journal> Computing Surveys, </journal> <volume> 13(3), </volume> <pages> 341-367, </pages> <year> 1981. </year>
Reference-contexts: Although it is possible to write programs that execute without consuming memory, a garbage collection algorithm is required in the general case. This algorithm reclaims memory occupied by data structures that are no longer accessible by any active process <ref> [13] </ref>. The current PCN implementation uses a simple asynchronous garbage collection technique for memory management. This technique allows computers to col 30 lect independently by maintaining tables of remote references. These tables decouple the address spaces on different computers [22].
Reference: [14] <author> Cole, M., </author> <title> Algorithmic Skeletons: Structured Management of Parallel Computation, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: However, we insist upon a clear separation of sequential and concurrent components in order to conveniently apply source-to-source transformation techniques and build programming abstractions. Previous work on reusable abstractions in parallel program design include the Argonne monitor macros [4] and Schedule package [17], and Cole's algorithmic skeletons <ref> [14] </ref>. However, in none of these approaches is support for abstractions incorporated into a compiler. An alternative to our compiler techniques is to use run-time techniques such as higher-order functions [28, 31].
Reference: [15] <author> Dally, W. J., et al., </author> <title> The J-Machine: A fine-grain concurrent computer, Information Processing 89, </title> <editor> G. X. Ritter (ed.), </editor> <publisher> Elsevier Science Publishers B.V., North Holland, IFIP, </publisher> <year> 1989. </year>
Reference-contexts: The J-machine also provides high performance variable and code-manipulation hardware <ref> [15] </ref>. All of these features may be used to replace unique components of the emulator design, providing high-performance, native-code versions of the system. Implementations of this type are currently under construction. 1.5 Summary The important characteristics of this approach are as follows.
Reference: [16] <author> Dijkstra, E. W., </author> <title> Guarded commands, nondeterminacy and the formal derivation of programs, </title> <journal> CACM, </journal> <volume> 18, </volume> <pages> 453-457, </pages> <year> 1975. </year>
Reference-contexts: Choice. Programs must inevitably choose between alternative actions; this choice is based on the values of variables. We adopt a simple method of specifying program actions that makes such choices explicit and avoids overspecification <ref> [16] </ref>. This is illustrated in the minimum procedure. Informally, the two rules in this program specify two alternative actions, each with an associated condition. <p> This notation is simply PCN augmented with operations for the manipulation of sets of programs. These operations provide building blocks that are used to construct libraries of reusable transformations. All of the transformation, compilation and run-time system techniques described in 31 compute (step,mesh,ni,ei,si,wi,no,eo,so,wo, DE) double mesh [], edge <ref> [16] </ref>; f ? step &lt; 1000 &gt; f ; c get edge (0,edge,mesh), no=[edge j no1], : : :, compute.1 (step,mesh,ni,ei,si,wi,no1,eo1,so1,wo1, DE) g, default &gt; f ; c dump mesh (mesh), DE = [] g g compute/11: = * R0 = step, R1 = mesh, R2-9 = ni-wo, R10 = DE
Reference: [17] <author> Dongarra, J. and Sorenson, D., </author> <title> Schedule: Tools for developing and analyzing parallel Fortran programs, The Characteristics of Parallel Algorithms, </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: However, we insist upon a clear separation of sequential and concurrent components in order to conveniently apply source-to-source transformation techniques and build programming abstractions. Previous work on reusable abstractions in parallel program design include the Argonne monitor macros [4] and Schedule package <ref> [17] </ref>, and Cole's algorithmic skeletons [14]. However, in none of these approaches is support for abstractions incorporated into a compiler. An alternative to our compiler techniques is to use run-time techniques such as higher-order functions [28, 31].
Reference: [18] <author> Foster, I., </author> <title> Automatic generation of self-scheduling programs, </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> 2(1), </volume> <pages> 68-78, </pages> <year> 1991. </year>
Reference-contexts: The implementation of an abstraction is combined with problem-specific information to form a complete application. In previous work, we have explored these ideas in the context of mapping [39], self-scheduling computations <ref> [18] </ref>, and tree reduction problems [20].
Reference: [19] <author> Foster, I., Kesselman, C., and Taylor, S., </author> <title> Concurrency: Simple concepts and powerful tools, </title> <journal> Computer Journal, </journal> <volume> 33(6), </volume> <pages> 501-507, </pages> <year> 1990. </year>
Reference-contexts: This architectural independence can be achieved by using a programming model based on four simple concepts: monotonicity, concurrent composition, choice between alternatives, 1 and separation of sequential code <ref> [19] </ref>. The notion of monotonicity provides an abstract model of communication and synchronization. Concurrent composition is used to specify opportunities for parallel execution. Choice is used to select between alternative program actions. Finally, separation of sequential code simplifies the use of state change and sequencing. <p> Furthermore, few approaches are developed to the point where they can be used to develop large-scale applications. We consider the basic ideas to be sufficient in and of themselves and have worked to develop them as a practical basis for concurrent programming <ref> [19] </ref>. The integration of sequential and concurrent programs has been the focus of a number of other systems, notably large-grain dataflow and Linda [2, 6]. However, we insist upon a clear separation of sequential and concurrent components in order to conveniently apply source-to-source transformation techniques and build programming abstractions. <p> However, we find the complexity of this approach unnecessary and prefer to implement transformations directly. The abstract machine design that we employ builds on our previous work in run-time support for concurrent programming <ref> [19, 39] </ref>. Unlike our previous designs and other uniprocessor systems [25, 30, 40], the PCN abstract machine emphasizes mutable data structures and the integration of sequential procedures, written in languages such as C, C++, and Fortran, into concurrent programs. <p> This machine comprises a number of computers connected via an interconnection network. Each computer is organized as shown in Figure 5 and is responsible for process scheduling, intercomputer communication, and memory management. The machine also incorporates facilities for performance evaluation <ref> [19, 32] </ref>. The abstract machine executes sequences of simple instructions that encode process control, guard evaluation, and data structure manipulation. In all, there are 33 instructions whose arguments are typically registers (R i ), program names (P), the number of arguments in a process (N), etc.
Reference: [20] <author> Foster, I., and Stevens, R., </author> <title> Parallel programming with algorithmic motifs, </title> <booktitle> Proc. Intl Conf. on Parallel Processing, </booktitle> <address> Penn. </address> <publisher> State Univ. Press, </publisher> <year> 1989. </year>
Reference-contexts: The implementation of an abstraction is combined with problem-specific information to form a complete application. In previous work, we have explored these ideas in the context of mapping [39], self-scheduling computations [18], and tree reduction problems <ref> [20] </ref>. In this paper, we show how the specification and implementation of such abstractions can be incorporated into the compilation process. 1.4 Compiler Techniques We seek techniques that permit efficient implementation of concurrent programs, expressed using the concepts described in previous sections, on a wide range of parallel 2 architectures.
Reference: [21] <author> Foster, I. and Taylor, S., Strand: </author> <title> New Concepts in Parallel Programming, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J. </address> <year> 1989. </year>
Reference-contexts: Concurrent composition is used to specify opportunities for parallel execution. Choice is used to select between alternative program actions. Finally, separation of sequential code simplifies the use of state change and sequencing. These concepts are language independent and have been incorporated into a commercially available programming system, Strand <ref> [21] </ref>. In this paper, we work with a second-generation system in which programs are expressed in a program composition notation (PCN) [8]. This notation provides a uniform treatment of concurrent composition, non-deterministic choice, and sequential programming. <p> We employ these concepts under the constraint that shared variables are constant, or monotone, during concurrent execution. This constraint can be enforced by the programmer <ref> [21] </ref> or by the compiler using copying [8]. In this context, a procedure expressed in a conventional language such as C, C++, or Fortran can be viewed as an atomic black box. This box simply computes an input-output relation. <p> These techniques support the organization of arbitrary communication protocols, termination detection in distributed computations, the construction of distributed data structures, and the implementation of atomic transactions <ref> [21, 8] </ref>. 4 Example Programming Problem Throughout the rest of this paper, we will repeatedly return to a single example program to demonstrate our programming, compilation, and run-time techniques. This program is a simplied implementation of an application developed to simulate the atmospheric circulation over the globe [10]. <p> receive messages in the guard, modify local state and/or spawn more processes; process synchronization occurs only in the guard components of a program. 21 The operational semantic of a Core PCN program consists of a subset of the semantic for PCN programs [8]; it is identical to that of Strand <ref> [21] </ref> except that atomic actions may modify data structures. If any guard G i is true, the associated atomic actions are executed, and concurrent processes are then spawned. If all guards are false, then the default action is executed.
Reference: [22] <author> Foster, I. Tuecke, S., and Taylor, S., </author> <title> A portable run-time system for PCN, </title> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Tech. Rept. ANL/MCS-TM-137, </institution> <month> July </month> <year> 1991. </year> <month> 35 </month>
Reference-contexts: Section 1.2). The second stage applies a set of compilation transformations to the entire program produced by the first stage. These transformations incrementally transform PCN programs toward a simple canonical form called Core PCN <ref> [22] </ref>. This canonical form is a high-level representation of a fine-grain, concurrent programming model in which processes receive messages, make simple decisions, perform atomic actions to modify memory, and spawn additional processes. The third stage translates Core PCN programs into the instruction set of an abstract, fine-grain, concurrent machine. <p> The abstract machine can be implemented in a variety of ways that trade off efficiency and portability. A general-purpose run-time system, or emulator, has been produced that executes the instruction set of the abstract machine directly <ref> [22] </ref>. This emulator is written in a portable subset of C that allows it to operate on a wide class of architectures; it typically compiles to a binary image of less than 100 Kbytes. <p> The compilation transformations incrementally transform programs into a canonical form that can be directly encoded into machine instructions. We term this canonical form Core PCN since it reflects the core ideas of the underlying implementation strategy, namely, fine-grain concurrent processes that communicate and synchronize through message passing <ref> [22] </ref>. <p> The current PCN implementation uses a simple asynchronous garbage collection technique for memory management. This technique allows computers to col 30 lect independently by maintaining tables of remote references. These tables decouple the address spaces on different computers <ref> [22] </ref>. We are currently investigating programming and compiler techniques that will allow programs to be refined so as to avoid the need for garbage collection.
Reference: [23] <author> Gajski, D., Padua, D., Kuck, D., and Kuhn, R. </author> <title> A second opinion on dataflow ma-chines and languages, </title> <journal> IEEE Computer, </journal> <volume> 15(2), </volume> <pages> 58-69, </pages> <year> 1982. </year>
Reference-contexts: Separation of Sequential Code. State change and sequencing are familiar concepts from sequential programming. State change permits efficient management of memory via destructive operations to storage locations; sequencing permits state changes to be organized without the overhead of explicit synchronization operations on each access to data <ref> [23] </ref>. Although these concepts are valuable from a programming perspective, they are dangerous in parallel programs if used in an unrestricted manner, because of the possibility of race conditions. We employ these concepts under the constraint that shared variables are constant, or monotone, during concurrent execution.
Reference: [24] <author> Gregory, S., </author> <title> Parallel Logic Programming in PARLOG, </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming <ref> [11, 24] </ref>, functional programming [28, 26, 9], and object-oriented programming [1]. Similarly, concurrent composition underlies such diverse approaches as CSP [29], concurrent logic programming, functional programming, and Unity [7].
Reference: [25] <author> Gregory, S., Foster, I., Burt, A., and Ringwood, G., </author> <title> An abstract machine for the implementation of PARLOG on uniprocessors, </title> <journal> New Generation Computing, </journal> <volume> 6, </volume> <pages> 389-420, </pages> <year> 1989. </year>
Reference-contexts: Alternatively, specialized versions of the system can be developed for architectures of particular interest, by retargeting the final stage of the compiler. 2 Related Work The benefits of an architecturally independent model of parallel computation have been widely recognized in the computer science community <ref> [29, 28, 25, 1, 7] </ref>. The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming [11, 24], functional programming [28, 26, 9], and object-oriented programming [1]. <p> However, we find the complexity of this approach unnecessary and prefer to implement transformations directly. The abstract machine design that we employ builds on our previous work in run-time support for concurrent programming [19, 39]. Unlike our previous designs and other uniprocessor systems <ref> [25, 30, 40] </ref>, the PCN abstract machine emphasizes mutable data structures and the integration of sequential procedures, written in languages such as C, C++, and Fortran, into concurrent programs.
Reference: [26] <author> Halstead, R., </author> <title> Multilisp A language for concurrent symbolic computation, </title> <journal> ACM Trans. Prog. Lang. Syst., </journal> <volume> 7(4), </volume> <pages> 501-538, </pages> <year> 1985. </year>
Reference-contexts: The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming [11, 24], functional programming <ref> [28, 26, 9] </ref>, and object-oriented programming [1]. Similarly, concurrent composition underlies such diverse approaches as CSP [29], concurrent logic programming, functional programming, and Unity [7].
Reference: [27] <author> Harrar, H., Keller, H., Lin, D., and Taylor, S., </author> <title> Parallel computation of Taylor-vortex flows, </title> <booktitle> Proc. Conf. on Parallel Computational Fluid Dynamics, </booktitle> <address> Stuttgart, Germany, </address> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1991. </year>
Reference-contexts: Currently, the emulator operates on Sun, Next, IBM, DEC, SGI, and HP workstations, on Intel iPSC 386/860/Delta and Symult S2010 multicomputers, and on Sequent Symmetry and Sun shared-memory multiprocessors. The resulting programs have impressive and predictable performance characteristics across a variety of architectures <ref> [10, 27] </ref>. An alternative abstract machine implementation technique further compiles the encoded abstract machine instructions to make use of specific architectural features. For example, most machines provide high-performance floating point accelerators. <p> It executes at 2.5 Gflops on the 528-computer Intel Delta and is portable across a wide range of architectures with predictable performance characteristics [10]. The code is typical of other application codes developed at Argonne National Laboratory and Caltech (e.g., <ref> [27] </ref>). These codes involve both substantial computational components, requiring efficient uniprocessor computation, and complex communication protocols, requiring efficient communication and synchronization. The application involves the parallel implementation of a control volume method for solving partial differential equations on a sphere. <p> It includes tools for defining program transformations, compiling concurrent programs, checking programs, debugging, performance analysis, and program animation. The toolkit has been used to design and implement substantial applications in several domains, including climate modeling and fluid dynamics <ref> [10, 27] </ref>. These programs use abstractions to coordinate the execution of thousands of lines of pre-existing C and Fortran code. Experimental studies show that the codes operate with predictable and impressive performance on a wide range of parallel computers. The toolkit can be obtained by anonymous FTP.
Reference: [28] <author> Henderson, P., </author> <title> Functional Programming, </title> <publisher> Prentice-Hall, </publisher> <year> 1980. </year>
Reference-contexts: Alternatively, specialized versions of the system can be developed for architectures of particular interest, by retargeting the final stage of the compiler. 2 Related Work The benefits of an architecturally independent model of parallel computation have been widely recognized in the computer science community <ref> [29, 28, 25, 1, 7] </ref>. The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming [11, 24], functional programming [28, 26, 9], and object-oriented programming [1]. <p> The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming [11, 24], functional programming <ref> [28, 26, 9] </ref>, and object-oriented programming [1]. Similarly, concurrent composition underlies such diverse approaches as CSP [29], concurrent logic programming, functional programming, and Unity [7]. <p> However, in none of these approaches is support for abstractions incorporated into a compiler. An alternative to our compiler techniques is to use run-time techniques such as higher-order functions <ref> [28, 31] </ref>. However, we prefer to use compile-time methods based on source-to-source transformations so as to avoid run-time overheads and achieve our goals of 5 efficient communication, synchronization, and sequential execution. The use of "meta--programs" to specify program transformations is common in declarative programming [3, 28, 38, 12, 5, 42]. <p> However, we prefer to use compile-time methods based on source-to-source transformations so as to avoid run-time overheads and achieve our goals of 5 efficient communication, synchronization, and sequential execution. The use of "meta--programs" to specify program transformations is common in declarative programming <ref> [3, 28, 38, 12, 5, 42] </ref>. Novel features of our approach include the integration of a programmable transformer into the compilation pipeline, linguistic support for invocation of transformations, and the use of set-oriented abstractions for specifying transformations.
Reference: [29] <author> Hoare, C., </author> <title> Communicating sequential processes, </title> <journal> CACM, </journal> <volume> 21(8), </volume> <pages> 666-677, </pages> <year> 1978. </year>
Reference-contexts: Alternatively, specialized versions of the system can be developed for architectures of particular interest, by retargeting the final stage of the compiler. 2 Related Work The benefits of an architecturally independent model of parallel computation have been widely recognized in the computer science community <ref> [29, 28, 25, 1, 7] </ref>. The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming [11, 24], functional programming [28, 26, 9], and object-oriented programming [1]. <p> The notion of monotonicity is at the heart of several such programming models, notably concurrent logic programming [11, 24], functional programming [28, 26, 9], and object-oriented programming [1]. Similarly, concurrent composition underlies such diverse approaches as CSP <ref> [29] </ref>, concurrent logic programming, functional programming, and Unity [7]. Unfortunately, these models either do not support concurrent source-to-source transformations or embed the basic ideas in complex language designs and programming paradigms that have little to do with concurrent programming.
Reference: [30] <author> Houri, A. and Shapiro, E., </author> <title> A sequential abstract machine for Flat Concurrent Prolog, </title> <type> Weizmann Institute Technical Report CS86-19, </type> <institution> Rehovot, </institution> <year> 1986. </year>
Reference-contexts: However, we find the complexity of this approach unnecessary and prefer to implement transformations directly. The abstract machine design that we employ builds on our previous work in run-time support for concurrent programming [19, 39]. Unlike our previous designs and other uniprocessor systems <ref> [25, 30, 40] </ref>, the PCN abstract machine emphasizes mutable data structures and the integration of sequential procedures, written in languages such as C, C++, and Fortran, into concurrent programs.
Reference: [31] <author> Kelly, </author> <title> P, Functional Programming for Loosely-Coupled Multiprocessors, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: However, in none of these approaches is support for abstractions incorporated into a compiler. An alternative to our compiler techniques is to use run-time techniques such as higher-order functions <ref> [28, 31] </ref>. However, we prefer to use compile-time methods based on source-to-source transformations so as to avoid run-time overheads and achieve our goals of 5 efficient communication, synchronization, and sequential execution. The use of "meta--programs" to specify program transformations is common in declarative programming [3, 28, 38, 12, 5, 42].
Reference: [32] <author> Kesselman, C., </author> <title> Integrating Performance Analysis with Performance Improvement in Parallel Programs, </title> <type> Ph.D. thesis, </type> <institution> UCLA, </institution> <year> 1991. </year>
Reference-contexts: This machine comprises a number of computers connected via an interconnection network. Each computer is organized as shown in Figure 5 and is responsible for process scheduling, intercomputer communication, and memory management. The machine also incorporates facilities for performance evaluation <ref> [19, 32] </ref>. The abstract machine executes sequences of simple instructions that encode process control, guard evaluation, and data structure manipulation. In all, there are 33 instructions whose arguments are typically registers (R i ), program names (P), the number of arguments in a process (N), etc.
Reference: [33] <author> Martin, A., </author> <title> The torus: An exercise in constructing a processing surface, </title> <booktitle> Proc. Conf. on VLSI, Caltech, </booktitle> <pages> 52-57, </pages> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: This interleaving at a single computer allows overlapping of communication and computation. If the location annotations are present, they indicate that a process should execute at an alternative computer within some virtual machine <ref> [33] </ref>. Virtual machines play two primary roles in program design: to reshape the physical machine to a form more 8 convenient for programming, and to provide scalability by expanding and contracting the physical machine to employ any arbitrary number of computers.
Reference: [34] <author> Parnas, D., </author> <title> On the criteria to be used in decomposing systems into modules, </title> <journal> CACM, </journal> <volume> 15(12), </volume> <pages> 1053-1058, </pages> <year> 1972. </year>
Reference-contexts: Modern computer science has given us two basic methods by which to use abstraction in program design: information hiding <ref> [34] </ref> and stepwise refinement [41]. Both of these development methodologies attempt to separate concerns and place implementation details in unique components of a program. These strategies improve program clarity, localize change thus improving maintainability, and isolate system dependencies, thus improving portability.
Reference: [35] <author> Safra, S., and Shapiro, S., </author> <title> Meta-interpreters for real, Concurrent Prolog: Collected Papers, </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: An alternative approach to the implementation of compile-time transformation uses meta-interpreters to specify transformations and partial evaluators to compile away the overhead of interpretation <ref> [35] </ref>. However, we find the complexity of this approach unnecessary and prefer to implement transformations directly. The abstract machine design that we employ builds on our previous work in run-time support for concurrent programming [19, 39].
Reference: [36] <author> Seitz, C. L., </author> <title> Multicomputers, Developments in Concurrency and Communication, C.A.R. </title> <editor> Hoare (ed.), </editor> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Application PredefinedExisting Fortran C, C++, Linking/Assemble PCN Abstract Program Canonical Form Core - PCN Encoding Object Code Object Code Networks iPSC 860 J-Machine Mosaic Portable Emulator Abstract CodeNative Code Compilation Transformations New Compilers Standard Abstractions Abstractions Abstraction Removal PCN + Abstractions 4 architecture provides high-performance message-handling and fine-grain process schedul-ing <ref> [36] </ref>. The J-machine also provides high performance variable and code-manipulation hardware [15]. All of these features may be used to replace unique components of the emulator design, providing high-performance, native-code versions of the system.
Reference: [37] <author> Smith, R. D., Dukowicz, J. K., and Malone, R. C., </author> <title> Parallel ocean general circulation modeling, </title> <note> Physica D (to appear). </note>
Reference-contexts: This may be specified as follows. map (c,r,i,j,locn) fj j locn = r*c*c + i*c + j g An alternative approach is to fold the octahedral mesh so as to ensure nearest-neighbor communications <ref> [37] </ref>. In this approach, each processor is allocated four subdomains. This constrains scalability, but is useful when remote communication is expensive. The alternative can be implemented simply by redefining the map procedure.
Reference: [38] <author> Steele, G., Rabbit: </author> <title> A compiler for Scheme, </title> <institution> MIT AI Lab TR/474, </institution> <year> 1978. </year>
Reference-contexts: However, we prefer to use compile-time methods based on source-to-source transformations so as to avoid run-time overheads and achieve our goals of 5 efficient communication, synchronization, and sequential execution. The use of "meta--programs" to specify program transformations is common in declarative programming <ref> [3, 28, 38, 12, 5, 42] </ref>. Novel features of our approach include the integration of a programmable transformer into the compilation pipeline, linguistic support for invocation of transformations, and the use of set-oriented abstractions for specifying transformations.
Reference: [39] <author> Taylor, S., </author> <title> Parallel Logic Programming Techniques, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1989. </year>
Reference-contexts: These capture, in a reusable form, application-independent aspects of program design such as scalability constraints, partitioning, mapping, and granularity. The implementation of an abstraction is combined with problem-specific information to form a complete application. In previous work, we have explored these ideas in the context of mapping <ref> [39] </ref>, self-scheduling computations [18], and tree reduction problems [20]. <p> However, we find the complexity of this approach unnecessary and prefer to implement transformations directly. The abstract machine design that we employ builds on our previous work in run-time support for concurrent programming <ref> [19, 39] </ref>. Unlike our previous designs and other uniprocessor systems [25, 30, 40], the PCN abstract machine emphasizes mutable data structures and the integration of sequential procedures, written in languages such as C, C++, and Fortran, into concurrent programs. <p> Second, the concept is trivial to implement efficiently: it maps directly to pointers within a single computer and to message passing between computers. Once available, the value of a variable can be propagated throughout a parallel machine without concern for consistency of copies <ref> [39] </ref>. Hence, programs can operate on distributed shared data without locking protocols or complex synchronization schemes. Concurrent Execution. Procedure calls in concurrent compositions are able to execute when their data is available; if data is available, a procedure is guaranteed to execute eventually. <p> Virtual machines may also be used to decompose a physical machine into a collection of submachines, each of which may be allocated a different computation. The combination of location annotations and virtual machines allows concurrent programs to be written that recursively unravel over a parallel architecture <ref> [39] </ref>. Programming Techniques. Extensive use of these programming ideas has convinced us that they are sufficient for all practical purposes. In particular, it has proved possible to develop a small set of concurrent programming techniques that address the vast majority of issues that arise in concurrent programming. <p> The algorithms used to implement communication follow from the representation chosen for monotone variables in a parallel computer network. Each variable is located at a single computer; all other instances of the variable are represented by intercomputer pointers termed remote references <ref> [39] </ref>. Intercomputer communication is necessary whenever a guard or assignment operation encounters a remote reference.
Reference: [40] <author> Warren, D.H.D., </author> <title> Applied logic | its use and implementation as a programming tool, </title> <type> SRI International Tech. Rep. 290, </type> <year> 1983. </year>
Reference-contexts: However, we find the complexity of this approach unnecessary and prefer to implement transformations directly. The abstract machine design that we employ builds on our previous work in run-time support for concurrent programming [19, 39]. Unlike our previous designs and other uniprocessor systems <ref> [25, 30, 40] </ref>, the PCN abstract machine emphasizes mutable data structures and the integration of sequential procedures, written in languages such as C, C++, and Fortran, into concurrent programs.
Reference: [41] <author> Wirth, N., </author> <title> Program development by stepwise refinement, </title> <journal> CACM, </journal> <volume> 14, </volume> <pages> 221-227, </pages> <year> 1971. </year>
Reference-contexts: Modern computer science has given us two basic methods by which to use abstraction in program design: information hiding [34] and stepwise refinement <ref> [41] </ref>. Both of these development methodologies attempt to separate concerns and place implementation details in unique components of a program. These strategies improve program clarity, localize change thus improving maintainability, and isolate system dependencies, thus improving portability.
Reference: [42] <author> Yang, J., and Choo, Y., </author> <title> Parallel program transformation using a metalanguage, </title> <booktitle> Proc. Conf. on Principles of Programming Languages, </booktitle> <pages> 11-20, </pages> <year> 1991. </year> <month> 37 </month>
Reference-contexts: However, we prefer to use compile-time methods based on source-to-source transformations so as to avoid run-time overheads and achieve our goals of 5 efficient communication, synchronization, and sequential execution. The use of "meta--programs" to specify program transformations is common in declarative programming <ref> [3, 28, 38, 12, 5, 42] </ref>. Novel features of our approach include the integration of a programmable transformer into the compilation pipeline, linguistic support for invocation of transformations, and the use of set-oriented abstractions for specifying transformations.
References-found: 42

