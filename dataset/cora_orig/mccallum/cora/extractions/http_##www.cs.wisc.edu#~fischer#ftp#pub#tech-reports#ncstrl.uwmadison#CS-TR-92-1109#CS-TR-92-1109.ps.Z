URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-92-1109/CS-TR-92-1109.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-92-1109/
Root-URL: http://www.cs.wisc.edu
Title: Bilinear Separation of Two Sets in n-Space  
Author: Kristin P. Bennett O. L. Mangasarian 
Abstract: The NP-complete problem of determining whether two disjoint point sets in the n-dimensional real space R n can be separated by two planes is cast as a bilinear program, that is minimizing the scalar product of two linear functions on a polyhedral set. The bilinear program, which has a vertex solution, is processed by an iterative linear programming algorithm that terminates in a finite number of steps at a point satisfying a necessary optimality condition or at a global minimum. Encouraging computational experience on a number of test problems is reported.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F.A. Al-Khayyal and J.E. Falk. </author> <title> Jointly constrained biconvex programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 8(2) </volume> <pages> 273-286, </pages> <year> 1983. </year>
Reference-contexts: Because of the special property of a zero minimum for bilinearly separable problems, we have opted for the simpler Franke-Wolfe type algorithms [10], rather than the more complex algorithms that have been given for bilinear programs <ref> [1, 30, 27, 28] </ref>. <p> The first case will have uncoupled constraints while the second case will have coupled constraints. We note that there are many papers on bilinear programs such as [27, 28] with uncoupled constraints, and <ref> [1, 30] </ref> with coupled constraints. However, none of the papers on coupled constraints appear to exploit zeroness of the minimum as we do here for the case of bilin-early separable sets. This fact allows us to conclude that a vertex solution exists for such problems. <p> If the algorithm does not terminate, let fx i j g ! x and without loss of generality, let v i j = v, some fixed vertex of X . Then for 2 <ref> [0; 1] </ref> f ((1 )x i j + v) f (x i j ) min f (x) f (x i j ) = f (x i j +1 ) f (x i j ): Since ff (x i )g is a nonincreasing sequence bounded below it converges. <p> Hence the limit of the last difference above is zero, and we have in the limit f ((1 )x + v) f (x) 0 8 2 <ref> [0; 1] </ref> Letting ! 0 and invoking the differentiability of f gives 5f (x)(v x) 0: But by algorithm construction 5f (x i j )(x x i j ) 5f (x i j )(v x i j ) 8 x 2 X and in the limit 5f (x)(x x) 5f (x)(v
Reference: [2] <author> E.B. Baum. </author> <title> Private communication, </title> <year> 1992. </year>
Reference-contexts: In order to investigate the effectiveness of the linear programming approach on high-dimensional bilinearly separable problems, we randomly generated test problems as follows <ref> [2] </ref>. Two points were randomly generated on an n-dimensional unit sphere.
Reference: [3] <author> E.B. Baum and D. Haussler. </author> <title> What size net gives valid generalization. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 81-90, </pages> <address> San Mateo, California, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However if more training examples are used the error decreases. This is indicated by lower curves in Figure 8 for the 1000-point training set compared to the curves for the 500-point training set. These trends agree with computational learning theory results <ref> [3] </ref> that provide necessary and sufficient conditions for valid generalization (that is correctness on testing sets) which are dependent on problem dimension and number of points in the training set.
Reference: [4] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <year> 1992. </year> <month> 18 </month>
Reference-contexts: These are clearly undesirable solutions that are easily detected in practice. The w 1 = w 2 case for the uncoupled problem was practically avoided by starting with a good initial solution found by using the first two planes of the Multisurface-Method-Tree algorithm <ref> [4, 6] </ref>. The problem of w 1 = 0 and w 2 = 0 was tackled by detecting when this occurred and then adding a linear constraint to the problem which made the zero solution infeasible. This did not affect the convergence proof of the algorithm. <p> This is because 500 is not a sufficient number of training set examples to adequately represent a problem in a 100-dimensional space. Nine out of ten generated training sets were linearly separable, and were quickly separated linearly by the Multisurface-Method-Tree algorithm <ref> [4, 6] </ref>. We also compared the performance of the UBPA 2.1 and BPA 2.3 with that of the backpropagation (BP) algorithm [25], the standard algorithm for training neural networks.
Reference: [5] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67, </pages> <address> Amsterdam, 1992. </address> <publisher> North Holland. </publisher>
Reference-contexts: Bilinear separation is a natural extension of linear separation which, for a long time, has been known to be equivalent to the polynomial-time solution of a single linear program <ref> [9, 13, 26, 5] </ref>. Linear separation is also equivalent to separation by Rosenblatt's perceptron or linear threshold unit (LTU) [24, 25, 11] (see Figure 2). However most problems are not linearly separable. For example the simple Minsky-Papert exclusive-or classical problem [20], is not linearly separable, but is bilinearly separable. <p> It can be solved by a neural network with 2 layers of linear threshold units [25, 18](see Figure 3). Other methods of separation by more than one plane, for example multisurface methods (MSM) of pattern separation, have also been proposed <ref> [14, 5, 6] </ref> and extensively used for medical diagnosis [29, 17, 5]. MSM which has been shown to be equivalent to a feed-forward neural network with a single hidden layer [5], can be trained by a greedy algorithm using linear programming [14, 5, 6]. <p> It can be solved by a neural network with 2 layers of linear threshold units [25, 18](see Figure 3). Other methods of separation by more than one plane, for example multisurface methods (MSM) of pattern separation, have also been proposed [14, 5, 6] and extensively used for medical diagnosis <ref> [29, 17, 5] </ref>. MSM which has been shown to be equivalent to a feed-forward neural network with a single hidden layer [5], can be trained by a greedy algorithm using linear programming [14, 5, 6]. <p> MSM which has been shown to be equivalent to a feed-forward neural network with a single hidden layer <ref> [5] </ref>, can be trained by a greedy algorithm using linear programming [14, 5, 6]. Thus bilinear and MSM separation can be thought of as alternative linear-programming-based methods for solving problems that are usually solved by neural networks. <p> methods (MSM) of pattern separation, have also been proposed <ref> [14, 5, 6] </ref> and extensively used for medical diagnosis [29, 17, 5]. MSM which has been shown to be equivalent to a feed-forward neural network with a single hidden layer [5], can be trained by a greedy algorithm using linear programming [14, 5, 6]. Thus bilinear and MSM separation can be thought of as alternative linear-programming-based methods for solving problems that are usually solved by neural networks. <p> By using the duality theory of linear programming, this can be shown to be equivalent to the existence of a plane wx = fl strictly separating A from B (see Figure 2 (a)) which is equivalent to <ref> [12, 13, 26, 5] </ref> Aw + efl + e 0; Bw efl + e 0; for some w 2 R n ; fl 2 R: (5) Based on the above definition of linear separability, we now define bilinear separability as follows: 7 Definition 3.1 (Bilinear separability definition (See Figure 5)) The
Reference: [6] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: It can be solved by a neural network with 2 layers of linear threshold units [25, 18](see Figure 3). Other methods of separation by more than one plane, for example multisurface methods (MSM) of pattern separation, have also been proposed <ref> [14, 5, 6] </ref> and extensively used for medical diagnosis [29, 17, 5]. MSM which has been shown to be equivalent to a feed-forward neural network with a single hidden layer [5], can be trained by a greedy algorithm using linear programming [14, 5, 6]. <p> methods (MSM) of pattern separation, have also been proposed <ref> [14, 5, 6] </ref> and extensively used for medical diagnosis [29, 17, 5]. MSM which has been shown to be equivalent to a feed-forward neural network with a single hidden layer [5], can be trained by a greedy algorithm using linear programming [14, 5, 6]. Thus bilinear and MSM separation can be thought of as alternative linear-programming-based methods for solving problems that are usually solved by neural networks. <p> These are clearly undesirable solutions that are easily detected in practice. The w 1 = w 2 case for the uncoupled problem was practically avoided by starting with a good initial solution found by using the first two planes of the Multisurface-Method-Tree algorithm <ref> [4, 6] </ref>. The problem of w 1 = 0 and w 2 = 0 was tackled by detecting when this occurred and then adding a linear constraint to the problem which made the zero solution infeasible. This did not affect the convergence proof of the algorithm. <p> This is because 500 is not a sufficient number of training set examples to adequately represent a problem in a 100-dimensional space. Nine out of ten generated training sets were linearly separable, and were quickly separated linearly by the Multisurface-Method-Tree algorithm <ref> [4, 6] </ref>. We also compared the performance of the UBPA 2.1 and BPA 2.3 with that of the backpropagation (BP) algorithm [25], the standard algorithm for training neural networks.
Reference: [7] <author> C. Berge and A. Ghouila-Houri. </author> <title> Programming, Games and Transportation Networks. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1965. </year>
Reference-contexts: We also establish finite termination when the sequence of points generated by the algorithm tend to a zero minimum. These results, which do not seem to be readily available in the stated form, are needed for our bilinear program algorithm BPA 2.3. Our proofs are based on those of <ref> [7] </ref> for the convex case. We consider the following problem and assumptions.
Reference: [8] <author> A. Blum and R.L. Rivest. </author> <title> Training a 3-node neural network is np-complete. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 494-501, </pages> <address> San Mateo, California, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction The problem we wish to consider is the following: Given two disjoint points sets A and B in the n-dimensional real space R n , can they be (strictly) separated by two planes? This is a fundamental NP-complete problem <ref> [19, 8] </ref> that is depicted in Figure 1 for the 2-dimensional real space R 2 . The configurations (a) and (b) of Figure 1 are equivalent as can easily be seen if the roles of A and B are interchanged. <p> Our computational experience, summarized in Section 4, indicates that the proposed algorithms are effective ones, especially in view of the fact that the underlying problem is an NP-complete problem. (More precisely only the bilinear separability problem corresponding to Figure 1 (a)-1 (b) has been shown to be NP-complete <ref> [19, 8] </ref>. That the corresponding problem to Figure 1 (c) is NP-complete as well, can be deduced from Theorem 3.1 below by noting that the bilinear program (13) is a special instance of the bilinear program (15).) A word about our notation now.
Reference: [9] <author> A. Charnes. </author> <title> Some fundamental theorems of perceptron theory and their geometry. </title> <editor> In J. T. Lou and R. H. Wilcox, editors, </editor> <booktitle> Computer and Information Sciences, </booktitle> <pages> pages 67-74, </pages> <address> Washington, 1964. </address> <publisher> Spartan Books. </publisher>
Reference-contexts: Bilinear separation is a natural extension of linear separation which, for a long time, has been known to be equivalent to the polynomial-time solution of a single linear program <ref> [9, 13, 26, 5] </ref>. Linear separation is also equivalent to separation by Rosenblatt's perceptron or linear threshold unit (LTU) [24, 25, 11] (see Figure 2). However most problems are not linearly separable. For example the simple Minsky-Papert exclusive-or classical problem [20], is not linearly separable, but is bilinearly separable.
Reference: [10] <author> M. Frank and P. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year>
Reference-contexts: Because of the special property of a zero minimum for bilinearly separable problems, we have opted for the simpler Franke-Wolfe type algorithms <ref> [10] </ref>, rather than the more complex algorithms that have been given for bilinear programs [1, 30, 27, 28]. <p> Proof. Since the quadratic objective function is bounded below by zero on the polyhedral feasible region, it must have a solution (x; y; r; s) <ref> [10] </ref>. <p> 10 10 1000 10 of 10 25 1000 10 of 10 Table 2: Performance of Coupled Bilinear Programming Algorithm BPA on Training Set 14 15 16 A Frank Wolfe Algorithm for Nonconvex and Bilinear Programs For convenience and the sake of completeness, we give simple convergence proofs for a Frank-Wolfe <ref> [10] </ref> algorithm without any convexity assumptions. We also establish finite termination when the sequence of points generated by the algorithm tend to a zero minimum. These results, which do not seem to be readily available in the stated form, are needed for our bilinear program algorithm BPA 2.3.
Reference: [11] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: Bilinear separation is a natural extension of linear separation which, for a long time, has been known to be equivalent to the polynomial-time solution of a single linear program [9, 13, 26, 5]. Linear separation is also equivalent to separation by Rosenblatt's perceptron or linear threshold unit (LTU) <ref> [24, 25, 11] </ref> (see Figure 2). However most problems are not linearly separable. For example the simple Minsky-Papert exclusive-or classical problem [20], is not linearly separable, but is bilinearly separable. It can be solved by a neural network with 2 layers of linear threshold units [25, 18](see Figure 3).
Reference: [12] <author> W. H. Highleyman. </author> <title> A note on linear separation. </title> <journal> IRE Transactions on Electronic Computers, </journal> <volume> 10 </volume> <pages> 777-778, </pages> <year> 1961. </year>
Reference-contexts: By using the duality theory of linear programming, this can be shown to be equivalent to the existence of a plane wx = fl strictly separating A from B (see Figure 2 (a)) which is equivalent to <ref> [12, 13, 26, 5] </ref> Aw + efl + e 0; Bw efl + e 0; for some w 2 R n ; fl 2 R: (5) Based on the above definition of linear separability, we now define bilinear separability as follows: 7 Definition 3.1 (Bilinear separability definition (See Figure 5)) The
Reference: [13] <author> O.L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference-contexts: Bilinear separation is a natural extension of linear separation which, for a long time, has been known to be equivalent to the polynomial-time solution of a single linear program <ref> [9, 13, 26, 5] </ref>. Linear separation is also equivalent to separation by Rosenblatt's perceptron or linear threshold unit (LTU) [24, 25, 11] (see Figure 2). However most problems are not linearly separable. For example the simple Minsky-Papert exclusive-or classical problem [20], is not linearly separable, but is bilinearly separable. <p> By using the duality theory of linear programming, this can be shown to be equivalent to the existence of a plane wx = fl strictly separating A from B (see Figure 2 (a)) which is equivalent to <ref> [12, 13, 26, 5] </ref> Aw + efl + e 0; Bw efl + e 0; for some w 2 R n ; fl 2 R: (5) Based on the above definition of linear separability, we now define bilinear separability as follows: 7 Definition 3.1 (Bilinear separability definition (See Figure 5)) The
Reference: [14] <author> O.L. Mangasarian. </author> <title> Multi-surface method of pattern separation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:801-807, </volume> <year> 1968. </year>
Reference-contexts: It can be solved by a neural network with 2 layers of linear threshold units [25, 18](see Figure 3). Other methods of separation by more than one plane, for example multisurface methods (MSM) of pattern separation, have also been proposed <ref> [14, 5, 6] </ref> and extensively used for medical diagnosis [29, 17, 5]. MSM which has been shown to be equivalent to a feed-forward neural network with a single hidden layer [5], can be trained by a greedy algorithm using linear programming [14, 5, 6]. <p> methods (MSM) of pattern separation, have also been proposed <ref> [14, 5, 6] </ref> and extensively used for medical diagnosis [29, 17, 5]. MSM which has been shown to be equivalent to a feed-forward neural network with a single hidden layer [5], can be trained by a greedy algorithm using linear programming [14, 5, 6]. Thus bilinear and MSM separation can be thought of as alternative linear-programming-based methods for solving problems that are usually solved by neural networks.
Reference: [15] <author> O.L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1969. </year>
Reference-contexts: We shall solve this bilinear program by a finite sequence of linear programs (Algorithms 2.1, 2.2 and 2.3) which terminate either at a solution of the bilinear program or at a point satisfying the minimum principle necessary optimality conditions <ref> [15, Theorem 9.3.3] </ref>. In Section 2 we begin with some basic results on the existence of vertex solutions to bilinear programs as well as some linear-programming-based finite algorithms for their solution. <p> Then UBPA 2.1 terminates in a finite number of steps at a global solution or a point (x i+1 ; y i ; r i+1 ; s i ) that satisfies the minimum principle necessary optimality condition <ref> [15] </ref> y i (x x i+1 ) + x i+1 (y y i ) 0 8 (x; r) 2 X ; 8 (y; s) 2 Y: (3) Proof.
Reference: [16] <author> O.L. Mangasarian. </author> <title> Characterization of linear complementarity problems as linear programs. </title> <journal> Mathematical Programming Study, </journal> <volume> 7 </volume> <pages> 74-87, </pages> <year> 1978. </year>
Reference-contexts: Since we are minimizing a concave function on a polyhedral set not containing lines going to infinity in both directions, it must have a vertex solution [23, Corollary 32.3.4]. The above proof is based on the proofs of Lemmas 1 and 2 of <ref> [16] </ref> which show that every solvable linear complementarity problem, monotonic or not, has a vertex solution. With the above theo rems we can formulate finite algorithms for each of the uncoupled and coupled bilinear programs: algorithms UBPA 2.1, UBPA1 2.2 and BPA 2.3.
Reference: [17] <author> O.L. Mangasarian, R. Setiono, </author> <title> and W.H. Wolberg. Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Proceedings of the Workshop on Large-Scale Numerical Optimization, </booktitle> <institution> Cornell University, </institution> <address> Ithaca, New York, </address> <month> October 19-20, </month> <year> 1989, </year> <pages> pages 22-31, </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1990. </year> <note> SIAM. </note>
Reference-contexts: It can be solved by a neural network with 2 layers of linear threshold units [25, 18](see Figure 3). Other methods of separation by more than one plane, for example multisurface methods (MSM) of pattern separation, have also been proposed [14, 5, 6] and extensively used for medical diagnosis <ref> [29, 17, 5] </ref>. MSM which has been shown to be equivalent to a feed-forward neural network with a single hidden layer [5], can be trained by a greedy algorithm using linear programming [14, 5, 6].
Reference: [18] <author> J.L. McClelland and D.E. Rummelhart. </author> <title> Explorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: However most problems are not linearly separable. For example the simple Minsky-Papert exclusive-or classical problem [20], is not linearly separable, but is bilinearly separable. It can be solved by a neural network with 2 layers of linear threshold units <ref> [25, 18] </ref>(see Figure 3). Other methods of separation by more than one plane, for example multisurface methods (MSM) of pattern separation, have also been proposed [14, 5, 6] and extensively used for medical diagnosis [29, 17, 5].
Reference: [19] <author> N. Megiddo. </author> <title> On the complexity of polyhedral separability. </title> <journal> Discrete and Computational Geometry, </journal> <volume> 3 </volume> <pages> 325-337, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction The problem we wish to consider is the following: Given two disjoint points sets A and B in the n-dimensional real space R n , can they be (strictly) separated by two planes? This is a fundamental NP-complete problem <ref> [19, 8] </ref> that is depicted in Figure 1 for the 2-dimensional real space R 2 . The configurations (a) and (b) of Figure 1 are equivalent as can easily be seen if the roles of A and B are interchanged. <p> Our computational experience, summarized in Section 4, indicates that the proposed algorithms are effective ones, especially in view of the fact that the underlying problem is an NP-complete problem. (More precisely only the bilinear separability problem corresponding to Figure 1 (a)-1 (b) has been shown to be NP-complete <ref> [19, 8] </ref>. That the corresponding problem to Figure 1 (c) is NP-complete as well, can be deduced from Theorem 3.1 below by noting that the bilinear program (13) is a special instance of the bilinear program (15).) A word about our notation now.
Reference: [20] <author> M. Minsky and S. Papert. </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1972. </year> <month> 19 </month>
Reference-contexts: Linear separation is also equivalent to separation by Rosenblatt's perceptron or linear threshold unit (LTU) [24, 25, 11] (see Figure 2). However most problems are not linearly separable. For example the simple Minsky-Papert exclusive-or classical problem <ref> [20] </ref>, is not linearly separable, but is bilinearly separable. It can be solved by a neural network with 2 layers of linear threshold units [25, 18](see Figure 3).
Reference: [21] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.0 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: The linear programming package MINOS 5.4 <ref> [21] </ref> was used to solve the linear subproblems. Initial experimentation with UBPA 2.1, UBPA1 2.2, and BPA 2.3 showed that they were prone to halting at solutions with w 1 = 0, w 2 = 0, or w 1 = w 2 which satisfied the minimum principle necessary optimality condition.
Reference: [22] <author> K.G. Murty. </author> <title> Linear Programming. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, New York, </address> <year> 1983. </year>
Reference-contexts: Hence the linear program min xy; X = f Cx + Er g; (x; r) 0g has a vertex (^x; ^r) of its feasible region X as solution <ref> [22] </ref>, and such that ^xy = xy: Similarly the linear program min ^xy; Y = f Dy + F s h; (y; s) 0g has a vertex (^y; ^s) of its feasible region Y as solution, and such that ^x^y = ^xy = xy: Hence ((^x; ^r); (^y; ^s)) is a
Reference: [23] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year>
Reference-contexts: Since we are minimizing a concave function on a polyhedral set not containing lines going to infinity in both directions, it must have a vertex solution <ref> [23, Corollary 32.3.4] </ref>. The above proof is based on the proofs of Lemmas 1 and 2 of [16] which show that every solvable linear complementarity problem, monotonic or not, has a vertex solution.
Reference: [24] <author> F. Rosenblatt. </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan Books, </publisher> <address> New York, New York, </address> <year> 1959. </year>
Reference-contexts: Bilinear separation is a natural extension of linear separation which, for a long time, has been known to be equivalent to the polynomial-time solution of a single linear program [9, 13, 26, 5]. Linear separation is also equivalent to separation by Rosenblatt's perceptron or linear threshold unit (LTU) <ref> [24, 25, 11] </ref> (see Figure 2). However most problems are not linearly separable. For example the simple Minsky-Papert exclusive-or classical problem [20], is not linearly separable, but is bilinearly separable. It can be solved by a neural network with 2 layers of linear threshold units [25, 18](see Figure 3).
Reference: [25] <author> D.E. Rumelhart, G.E. Hinton, and J.L. McClelland. </author> <title> Learning internal representations. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> pages 318-362, </pages> <address> Cambridge, Massachusetts, 1986. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Bilinear separation is a natural extension of linear separation which, for a long time, has been known to be equivalent to the polynomial-time solution of a single linear program [9, 13, 26, 5]. Linear separation is also equivalent to separation by Rosenblatt's perceptron or linear threshold unit (LTU) <ref> [24, 25, 11] </ref> (see Figure 2). However most problems are not linearly separable. For example the simple Minsky-Papert exclusive-or classical problem [20], is not linearly separable, but is bilinearly separable. It can be solved by a neural network with 2 layers of linear threshold units [25, 18](see Figure 3). <p> However most problems are not linearly separable. For example the simple Minsky-Papert exclusive-or classical problem [20], is not linearly separable, but is bilinearly separable. It can be solved by a neural network with 2 layers of linear threshold units <ref> [25, 18] </ref>(see Figure 3). Other methods of separation by more than one plane, for example multisurface methods (MSM) of pattern separation, have also been proposed [14, 5, 6] and extensively used for medical diagnosis [29, 17, 5]. <p> Nine out of ten generated training sets were linearly separable, and were quickly separated linearly by the Multisurface-Method-Tree algorithm [4, 6]. We also compared the performance of the UBPA 2.1 and BPA 2.3 with that of the backpropagation (BP) algorithm <ref> [25] </ref>, the standard algorithm for training neural networks. BP was used to train neural networks configured as in Figure 3 (b) and Figure 4 (b) to solve the bilinear separability problem. We found that BP could consistently solve only small dimensional problems with the topology of Figure 1 (a).
Reference: [26] <author> F. W. Smith. </author> <title> Pattern classifier design by linear programming. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-17:367-372, </volume> <year> 1968. </year>
Reference-contexts: Bilinear separation is a natural extension of linear separation which, for a long time, has been known to be equivalent to the polynomial-time solution of a single linear program <ref> [9, 13, 26, 5] </ref>. Linear separation is also equivalent to separation by Rosenblatt's perceptron or linear threshold unit (LTU) [24, 25, 11] (see Figure 2). However most problems are not linearly separable. For example the simple Minsky-Papert exclusive-or classical problem [20], is not linearly separable, but is bilinearly separable. <p> By using the duality theory of linear programming, this can be shown to be equivalent to the existence of a plane wx = fl strictly separating A from B (see Figure 2 (a)) which is equivalent to <ref> [12, 13, 26, 5] </ref> Aw + efl + e 0; Bw efl + e 0; for some w 2 R n ; fl 2 R: (5) Based on the above definition of linear separability, we now define bilinear separability as follows: 7 Definition 3.1 (Bilinear separability definition (See Figure 5)) The
Reference: [27] <author> T. V. Thieu. </author> <title> A note on the solution of bilinear programming problems by reduction to concave minimization. </title> <journal> Mathematical Programming, </journal> <volume> 41 </volume> <pages> 249-260, </pages> <year> 1988. </year>
Reference-contexts: Because of the special property of a zero minimum for bilinearly separable problems, we have opted for the simpler Franke-Wolfe type algorithms [10], rather than the more complex algorithms that have been given for bilinear programs <ref> [1, 30, 27, 28] </ref>. <p> We shall consider two categories of bilinear programs corresponding to cases (a)-(b) and case (c) of Figure 1. The first case will have uncoupled constraints while the second case will have coupled constraints. We note that there are many papers on bilinear programs such as <ref> [27, 28] </ref> with uncoupled constraints, and [1, 30] with coupled constraints. However, none of the papers on coupled constraints appear to exploit zeroness of the minimum as we do here for the case of bilin-early separable sets. <p> This fact allows us to conclude that a vertex solution exists for such problems. This in turn leads to finite termination for the proposed algorithms. For uncoupled constraints, the existence of a vertex solution is known and has been exploited algorithmically <ref> [27, 28] </ref>. For completeness and contrast with the proof for the case of coupled bilinear programs, we begin by a simple proof of the existence of a vertex solution for uncoupled bilinear programs.
Reference: [28] <author> D. J. White. </author> <title> A linear programming approach to solving bilinear programs. </title> <journal> Mathematical Programming, </journal> <volume> 56 </volume> <pages> 45-50, </pages> <year> 1992. </year>
Reference-contexts: Because of the special property of a zero minimum for bilinearly separable problems, we have opted for the simpler Franke-Wolfe type algorithms [10], rather than the more complex algorithms that have been given for bilinear programs <ref> [1, 30, 27, 28] </ref>. <p> We shall consider two categories of bilinear programs corresponding to cases (a)-(b) and case (c) of Figure 1. The first case will have uncoupled constraints while the second case will have coupled constraints. We note that there are many papers on bilinear programs such as <ref> [27, 28] </ref> with uncoupled constraints, and [1, 30] with coupled constraints. However, none of the papers on coupled constraints appear to exploit zeroness of the minimum as we do here for the case of bilin-early separable sets. <p> This fact allows us to conclude that a vertex solution exists for such problems. This in turn leads to finite termination for the proposed algorithms. For uncoupled constraints, the existence of a vertex solution is known and has been exploited algorithmically <ref> [27, 28] </ref>. For completeness and contrast with the proof for the case of coupled bilinear programs, we begin by a simple proof of the existence of a vertex solution for uncoupled bilinear programs.
Reference: [29] <author> W. H. Wolberg and O.L. Mangasarian. </author> <title> Multisurface method of pattern separation for medical diagnosis applied to breast cytology. </title> <booktitle> Proceedings of the National Academy of Sciences,U.S.A., </booktitle> <volume> 87 </volume> <pages> 9193-9196, </pages> <year> 1990. </year>
Reference-contexts: It can be solved by a neural network with 2 layers of linear threshold units [25, 18](see Figure 3). Other methods of separation by more than one plane, for example multisurface methods (MSM) of pattern separation, have also been proposed [14, 5, 6] and extensively used for medical diagnosis <ref> [29, 17, 5] </ref>. MSM which has been shown to be equivalent to a feed-forward neural network with a single hidden layer [5], can be trained by a greedy algorithm using linear programming [14, 5, 6].
Reference: [30] <author> Y. Yajima and H. Konno. </author> <title> Efficient algorithms for solving rank two and rank three bilinear programming problems. </title> <journal> Journal of Global Optimization, </journal> <volume> 1 </volume> <pages> 155-171, </pages> <year> 1991. </year> <month> 20 </month>
Reference-contexts: Because of the special property of a zero minimum for bilinearly separable problems, we have opted for the simpler Franke-Wolfe type algorithms [10], rather than the more complex algorithms that have been given for bilinear programs <ref> [1, 30, 27, 28] </ref>. <p> The first case will have uncoupled constraints while the second case will have coupled constraints. We note that there are many papers on bilinear programs such as [27, 28] with uncoupled constraints, and <ref> [1, 30] </ref> with coupled constraints. However, none of the papers on coupled constraints appear to exploit zeroness of the minimum as we do here for the case of bilin-early separable sets. This fact allows us to conclude that a vertex solution exists for such problems.
References-found: 30

