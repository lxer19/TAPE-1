URL: http://www.cs.utk.edu/~library/TechReports/1995/ut-cs-95-267.ps.Z
Refering-URL: http://www.cs.rice.edu:80/~clayb/work.html
Root-URL: 
Title: Parallel Benchmarks and Comparison-Based Computing  
Author: Clay P. Breshears Michael A. Langston 
Address: Hattiesburg, MS 39406 USA  Knoxville, TN 37996 USA  
Affiliation: Department of Computer Science University of Southern Mississippi  Department of Computer Science University of Tennessee  
Abstract: Non-numeric algorithms have been largely ignored in parallel benchmarking suites. Prior studies have concentrated mainly on the computational speed of processors within very regular and structured numeric codes. In this paper, we survey the current state of non-numeric benchmark algorithms and investigate the use of in-place merging as a suitable candidate for this role. In-place merging enjoys several important advantages, including the scalability of efficient memory utilization, the generality of comparison-based computing and the representativeness of near-random data access patterns. Experimental results over several families of parallel architectures are presented. fl A preliminary version of a portion of this paper was presented at the International Conference on Parallel Computing held in Gent, Belgium, in September, 1995. This research has been supported in part by the National Science Foundation under grant CDA-9115428 and by the Office of Naval Research under contract N00014-90-J-1855. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. A. Addison, V. S. Getov, A. J. G. Hey, R. W. Hockney and I. C. Walton, </author> <title> "The GENESIS Distributed-Memory Benchmarks," </title> <booktitle> in Computer Benchmarks (Advances in Parallel Computing 8), </booktitle> <editor> J. J. Dongarra and W. Gentzsch, eds., </editor> <publisher> Elsevier Science Publishers B.V., </publisher> <address> Amsterdam, </address> <year> 1993. </year>
Reference-contexts: Bhuyan and Zhang bring several of these examples together in [5]. Attempts to develop and make available a standardized parallel benchmarking suite are in their infancy. These attempts include the EuroBEN Group [25], the GENESIS Project <ref> [1] </ref>, PERFECT Club Benchmarks [4], NAS Parallel Benchmarks [2] and the vendor-supported SPEC Benchmarks [8]. Recently, a number of researchers interested in benchmarking met and formed the PARKBENCH Committee [14]. <p> Numbers in brackets represent relative rankings. Among other things, Tables 2 and 4 both illustrate that SIMD architectures can be extremely unfriendly to non-numeric algorithms. Number of LINPACK Merge Machine processors R max (Gflop/s) (Mrec/s) Intel iPSC/860 32 .64 [2] 1.428 <ref> [1] </ref> MasPar MP-2 4096 .374 [3] 0.236 [2] Thinking Machines CM-5 (SIMD) 32 1.9 [1] 0.001 [3] Table 4: Comparison of LINPACK and In-Place Merge Benchmarks We also note differences between our results and those of the NAS Parallel Benchmark Integer Sort kernel. <p> Among other things, Tables 2 and 4 both illustrate that SIMD architectures can be extremely unfriendly to non-numeric algorithms. Number of LINPACK Merge Machine processors R max (Gflop/s) (Mrec/s) Intel iPSC/860 32 .64 [2] 1.428 <ref> [1] </ref> MasPar MP-2 4096 .374 [3] 0.236 [2] Thinking Machines CM-5 (SIMD) 32 1.9 [1] 0.001 [3] Table 4: Comparison of LINPACK and In-Place Merge Benchmarks We also note differences between our results and those of the NAS Parallel Benchmark Integer Sort kernel. We have taken from [3] the raw execution times for 2 23 keys and converted them to Mrec/s. See Table 5. <p> We have taken from [3] the raw execution times for 2 23 keys and converted them to Mrec/s. See Table 5. Number of NAS IS Merge Machine processors (Mrec/s) (Mrec/s) IBM SP2 16 3.077 <ref> [1] </ref> 4.877 [1] Intel iPSC/860 32 0.326 [4] 1.630 [4] Intel Paragon (OSF1.2) 32 1.074 [3] 4.039 [2] Table 5: Comparison of NAS Integer Sort and In-Place Merge Benchmarks In addition to the differences between our rankings and those of LINPACK and NAS, we observe a quantitative difference between relative machine <p> We have taken from [3] the raw execution times for 2 23 keys and converted them to Mrec/s. See Table 5. Number of NAS IS Merge Machine processors (Mrec/s) (Mrec/s) IBM SP2 16 3.077 <ref> [1] </ref> 4.877 [1] Intel iPSC/860 32 0.326 [4] 1.630 [4] Intel Paragon (OSF1.2) 32 1.074 [3] 4.039 [2] Table 5: Comparison of NAS Integer Sort and In-Place Merge Benchmarks In addition to the differences between our rankings and those of LINPACK and NAS, we observe a quantitative difference between relative machine performances.
Reference: [2] <author> D. H. Bailey, J. T. Barton, T. A. Lasinski and H. D. Simon, </author> <title> "The NAS Parallel Benchmarks," </title> <type> RNR Technical Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Only in recent years have non-numeric algorithms been considered for inclusion in parallel and supercomputer benchmarks <ref> [2, 8, 22] </ref>. Past benchmarking suites have been primarily concerned with a parallel system's aggregate speed of performing floating-point operations. As the field of parallel computation matures, however, the range of codes executed on parallel machines will likely include a wide assortment of non-numeric routines. <p> As the field of parallel computation matures, however, the range of codes executed on parallel machines will likely include a wide assortment of non-numeric routines. Counting sort and radix sort have been proposed as non-numeric benchmark candidates <ref> [2, 22] </ref>. Unfortunately, these simple algorithms are not very representative of operations carried out in wide assortments of non-numeric applications. Both of these sorts exhibit completely predictable data access patterns and, even worse, squander an unbounded amount of unnecessary memory. <p> Bhuyan and Zhang bring several of these examples together in [5]. Attempts to develop and make available a standardized parallel benchmarking suite are in their infancy. These attempts include the EuroBEN Group [25], the GENESIS Project [1], PERFECT Club Benchmarks [4], NAS Parallel Benchmarks <ref> [2] </ref> and the vendor-supported SPEC Benchmarks [8]. Recently, a number of researchers interested in benchmarking met and formed the PARKBENCH Committee [14]. <p> All of these benchmarking programs and suites are geared toward scientific computations. This, in turn, translates to codes devoted to computations involving single and multiple precision floating-point operations. There are, however, some non-numeric exceptions included within two of the suites. The NAS Parallel Benchmarks <ref> [2] </ref> is a collection of different algorithms found in actual computational fluid dynamics calculations. The entire suite includes five parallel kernels and three complete applications. <p> Numbers in brackets represent relative rankings. Among other things, Tables 2 and 4 both illustrate that SIMD architectures can be extremely unfriendly to non-numeric algorithms. Number of LINPACK Merge Machine processors R max (Gflop/s) (Mrec/s) Intel iPSC/860 32 .64 <ref> [2] </ref> 1.428 [1] MasPar MP-2 4096 .374 [3] 0.236 [2] Thinking Machines CM-5 (SIMD) 32 1.9 [1] 0.001 [3] Table 4: Comparison of LINPACK and In-Place Merge Benchmarks We also note differences between our results and those of the NAS Parallel Benchmark Integer Sort kernel. <p> Numbers in brackets represent relative rankings. Among other things, Tables 2 and 4 both illustrate that SIMD architectures can be extremely unfriendly to non-numeric algorithms. Number of LINPACK Merge Machine processors R max (Gflop/s) (Mrec/s) Intel iPSC/860 32 .64 <ref> [2] </ref> 1.428 [1] MasPar MP-2 4096 .374 [3] 0.236 [2] Thinking Machines CM-5 (SIMD) 32 1.9 [1] 0.001 [3] Table 4: Comparison of LINPACK and In-Place Merge Benchmarks We also note differences between our results and those of the NAS Parallel Benchmark Integer Sort kernel. <p> See Table 5. Number of NAS IS Merge Machine processors (Mrec/s) (Mrec/s) IBM SP2 16 3.077 [1] 4.877 [1] Intel iPSC/860 32 0.326 [4] 1.630 [4] Intel Paragon (OSF1.2) 32 1.074 [3] 4.039 <ref> [2] </ref> Table 5: Comparison of NAS Integer Sort and In-Place Merge Benchmarks In addition to the differences between our rankings and those of LINPACK and NAS, we observe a quantitative difference between relative machine performances.
Reference: [3] <author> D. H. Bailey, E. Barszcz, L. Dagum and H. D. Simon, </author> <title> "NAS Parallel Benchmark Results 10-94," </title> <type> NAS Technical Report NAS-94-001, </type> <institution> NASA Ames Research Center, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: Numbers in brackets represent relative rankings. Among other things, Tables 2 and 4 both illustrate that SIMD architectures can be extremely unfriendly to non-numeric algorithms. Number of LINPACK Merge Machine processors R max (Gflop/s) (Mrec/s) Intel iPSC/860 32 .64 [2] 1.428 [1] MasPar MP-2 4096 .374 <ref> [3] </ref> 0.236 [2] Thinking Machines CM-5 (SIMD) 32 1.9 [1] 0.001 [3] Table 4: Comparison of LINPACK and In-Place Merge Benchmarks We also note differences between our results and those of the NAS Parallel Benchmark Integer Sort kernel. We have taken from [3] the raw execution times for 2 23 keys <p> Number of LINPACK Merge Machine processors R max (Gflop/s) (Mrec/s) Intel iPSC/860 32 .64 [2] 1.428 [1] MasPar MP-2 4096 .374 <ref> [3] </ref> 0.236 [2] Thinking Machines CM-5 (SIMD) 32 1.9 [1] 0.001 [3] Table 4: Comparison of LINPACK and In-Place Merge Benchmarks We also note differences between our results and those of the NAS Parallel Benchmark Integer Sort kernel. We have taken from [3] the raw execution times for 2 23 keys and converted them to Mrec/s. See Table 5. <p> .64 [2] 1.428 [1] MasPar MP-2 4096 .374 <ref> [3] </ref> 0.236 [2] Thinking Machines CM-5 (SIMD) 32 1.9 [1] 0.001 [3] Table 4: Comparison of LINPACK and In-Place Merge Benchmarks We also note differences between our results and those of the NAS Parallel Benchmark Integer Sort kernel. We have taken from [3] the raw execution times for 2 23 keys and converted them to Mrec/s. See Table 5. Number of NAS IS Merge Machine processors (Mrec/s) (Mrec/s) IBM SP2 16 3.077 [1] 4.877 [1] Intel iPSC/860 32 0.326 [4] 1.630 [4] Intel Paragon (OSF1.2) 32 1.074 [3] 4.039 [2] Table 5: Comparison <p> We have taken from <ref> [3] </ref> the raw execution times for 2 23 keys and converted them to Mrec/s. See Table 5. Number of NAS IS Merge Machine processors (Mrec/s) (Mrec/s) IBM SP2 16 3.077 [1] 4.877 [1] Intel iPSC/860 32 0.326 [4] 1.630 [4] Intel Paragon (OSF1.2) 32 1.074 [3] 4.039 [2] Table 5: Comparison of NAS Integer Sort and In-Place Merge Benchmarks In addition to the differences between our rankings and those of LINPACK and NAS, we observe a quantitative difference between relative machine performances.
Reference: [4] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsuing, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodrum and J. Martin, </author> <title> "The PERFECT Club Benchmarks: Effective Performance Evaluation of Computers," </title> <booktitle> International Journal of Supercomputer Applications 3 (1989), </booktitle> <pages> 5-40. </pages>
Reference-contexts: Bhuyan and Zhang bring several of these examples together in [5]. Attempts to develop and make available a standardized parallel benchmarking suite are in their infancy. These attempts include the EuroBEN Group [25], the GENESIS Project [1], PERFECT Club Benchmarks <ref> [4] </ref>, NAS Parallel Benchmarks [2] and the vendor-supported SPEC Benchmarks [8]. Recently, a number of researchers interested in benchmarking met and formed the PARKBENCH Committee [14]. <p> We have taken from [3] the raw execution times for 2 23 keys and converted them to Mrec/s. See Table 5. Number of NAS IS Merge Machine processors (Mrec/s) (Mrec/s) IBM SP2 16 3.077 [1] 4.877 [1] Intel iPSC/860 32 0.326 <ref> [4] </ref> 1.630 [4] Intel Paragon (OSF1.2) 32 1.074 [3] 4.039 [2] Table 5: Comparison of NAS Integer Sort and In-Place Merge Benchmarks In addition to the differences between our rankings and those of LINPACK and NAS, we observe a quantitative difference between relative machine performances. <p> We have taken from [3] the raw execution times for 2 23 keys and converted them to Mrec/s. See Table 5. Number of NAS IS Merge Machine processors (Mrec/s) (Mrec/s) IBM SP2 16 3.077 [1] 4.877 [1] Intel iPSC/860 32 0.326 <ref> [4] </ref> 1.630 [4] Intel Paragon (OSF1.2) 32 1.074 [3] 4.039 [2] Table 5: Comparison of NAS Integer Sort and In-Place Merge Benchmarks In addition to the differences between our rankings and those of LINPACK and NAS, we observe a quantitative difference between relative machine performances.
Reference: [5] <author> L. N. Bhuyan and X. Zhang, eds., </author> <title> Multi-Processor Performance Measurement and Evaluation, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1995. </year>
Reference-contexts: Francis and Mathieson [10] put forth a parallel sort algorithm for benchmarking shared memory machines. There are more individual benchmarking examples used to rate 4 the performance of specific hardware systems or architectures in the literature. Bhuyan and Zhang bring several of these examples together in <ref> [5] </ref>. Attempts to develop and make available a standardized parallel benchmarking suite are in their infancy. These attempts include the EuroBEN Group [25], the GENESIS Project [1], PERFECT Club Benchmarks [4], NAS Parallel Benchmarks [2] and the vendor-supported SPEC Benchmarks [8].
Reference: [6] <author> C. P. Breshears and M. A. Langston, </author> <title> "MIMD Versus SIMD Computation: Experience with Non-Numeric Parallel Algorithms," </title> <booktitle> Parallel Algorithms and Applications 2 (1994), </booktitle> <pages> 123-138. </pages>
Reference-contexts: Note that the x axis uses a logarithmic scale. Memory capacity is another critical measure. We have observed before <ref> [6] </ref> that memory management schemes can exhibit unexpected behavior at or near capacity. Table 3 lists maximum data memory capacity and corresponding machine performance for three sample 16 configurations: MasPar MP-2 with 4096 processors; Thinking Machines CM-5 (MIMD) with 32 processors; Intel iPSC/860 32 processors.
Reference: [7] <author> D. J. DeWitt, </author> <title> "The Wisconsin Benchmark: Past, Present, and Future," in The Benchmark Handbook for Database and Transaction Processing Systems, </title> <editor> J. Gray, editor, </editor> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Integer or character computations are used for evaluation and often involve comparisons of quantities or movement of data. Such computations may be repeated in the processing of a single transaction or query. Two of the better-known database benchmarks are the Wisconsin benchmark <ref> [7] </ref> and the TPC benchmark [20]. The former uses a synthetic database with a set of queries designed to measure performance of decision support systems; the latter measures transaction processing using a large number of small transactions. <p> Both benchmarks were originally designed for serial systems, but are adaptable for use on parallel ones. The performance evaluation of the Gamma parallel database machine under the Wisconsin benchmark is included in <ref> [7] </ref>.
Reference: [8] <author> K. M. Dixit, </author> <title> "The SPEC Benchmarks," </title> <booktitle> Parallel Computing 17 (1991), </booktitle> <pages> 1195-1209. </pages>
Reference-contexts: 1 Introduction Only in recent years have non-numeric algorithms been considered for inclusion in parallel and supercomputer benchmarks <ref> [2, 8, 22] </ref>. Past benchmarking suites have been primarily concerned with a parallel system's aggregate speed of performing floating-point operations. As the field of parallel computation matures, however, the range of codes executed on parallel machines will likely include a wide assortment of non-numeric routines. <p> Attempts to develop and make available a standardized parallel benchmarking suite are in their infancy. These attempts include the EuroBEN Group [25], the GENESIS Project [1], PERFECT Club Benchmarks [4], NAS Parallel Benchmarks [2] and the vendor-supported SPEC Benchmarks <ref> [8] </ref>. Recently, a number of researchers interested in benchmarking met and formed the PARKBENCH Committee [14]. <p> They propose instead a radix sort, and present performance results using a Connection Machine CM-5 with varying numbers of processors and several data distributions. Within the integer computation suite, CINT92, of the SPEC Benchmarks <ref> [8] </ref> are routines for generating and optimizing Programmable Logic Arrays, solving a nine queens' problem with a Lisp interpreter and compressing input files with the Lempel-Ziv encoding.
Reference: [9] <author> J. J. Dongarra, </author> <title> "Performance of Various Computers Using Standard Linear Equations Software," </title> <note> University of Tennessee Technical Report CS-89-85 (updated May 18, </note> <year> 1994). </year>
Reference-contexts: Using the notation of <ref> [9] </ref>, we let R max denote billions of floating-point operations per second (Gflop/s) measured for the largest problem run on each machine. For comparison, we use Mrec/s values. The results from 17 both benchmarks are shown in Table 4 y . Numbers in brackets represent relative rankings.
Reference: [10] <author> R. Francis and I. Mathieson, </author> <title> "A Benchmark Parallel Sort for Shared Memory Multiprocessors", </title> <journal> IEEE Transactions on Computing 37 (1988), </journal> <pages> 1619-1626. </pages>
Reference-contexts: This algorithm, along with others used for comparison, were written in the parallel language Id90. Francis and Mathieson <ref> [10] </ref> put forth a parallel sort algorithm for benchmarking shared memory machines. There are more individual benchmarking examples used to rate 4 the performance of specific hardware systems or architectures in the literature. Bhuyan and Zhang bring several of these examples together in [5].
Reference: [11] <author> R. D. Grappel and J. E. Hemenway, </author> <title> "A Tale of Four Ps: Benchmarks Quantify Performance," </title> <address> EDN (April 1, </address> <year> 1981), </year> <pages> 179-265. </pages>
Reference-contexts: These benchmarking results and routines are available through several on-line sources. 2.1.4 EDN Benchmarks This collection of programs was developed at Carnegie Mellon University and published by EDN in 1981 <ref> [11] </ref>. The intent of the benchmarks was to measure the computational speed of microprocessors without measuring the quality of the compiler. For that reason, the original codes were written directly in assembly languages of various microprocessors, though a subset of the original benchmarks are now available in C.
Reference: [12] <author> J. Gray, </author> <title> The Benchmark Handbook for Database and Transaction Processing Systems, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1993. </year> <month> 20 </month>
Reference-contexts: We have selected a parallel, in-place method as our algorithm of choice. An explanation of this selection follows in the next section. Details on the inner-workings of the algorithm itself are contained in an appendix. 7 3 Defining a Suitable Benchmark Algorithm Gray <ref> [12] </ref>, lists four criteria for domain-specific benchmarks. Though this list was aimed at database systems, we argue that it can easily be extended to more general non-numeric parallel environments.
Reference: [13] <author> X. Guan and M. A. Langston, </author> <title> "Time-Space Optimal Parallel Merging and Sorting," </title> <journal> IEEE Transactions on Computers 40 (1991), </journal> <pages> 596-602. </pages>
Reference-contexts: Thus, in order to predict more accurately the performance of non-numeric algorithms, a comparison-based, in-place benchmark may be more telling. In the sequel, we examine the effectiveness of the parallel merge (and hence sort-by-merging) approach first devised in <ref> [13] </ref>. We report the results of large collections of experiments conducted on a wide range of parallel machines, including both SIMD versus MIMD and shared versus distributed memory designs. We also compare these results to those obtained for numeric codes. <p> In this case, however, memory access and communication patterns are predictable and unlikely to be representative of arbitrary computations. 3.5 In-Place Merging The algorithm of <ref> [13] </ref> covers the five desired operations very well, and is easily scalable to any number of processors or memory size.
Reference: [14] <author> R. W. Hockney and M. Berry, eds., </author> <title> "Public International Benchmarks for Parallel Computers," PARKBENCH Committee: </title> <address> Report-1, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: These attempts include the EuroBEN Group [25], the GENESIS Project [1], PERFECT Club Benchmarks [4], NAS Parallel Benchmarks [2] and the vendor-supported SPEC Benchmarks [8]. Recently, a number of researchers interested in benchmarking met and formed the PARKBENCH Committee <ref> [14] </ref>. The goal of this group is to make available codes that test the capabilities of scalable massively-parallel computers while maintaining strict guidelines on an acceptable benchmarking methodology to ensure that performance evaluations are meaningful across a variety of machines.
Reference: [15] <author> R. W. Hockney, </author> <title> "A Framework for Benchmark Performance Analysis," </title> <booktitle> in Computer Benchmarks (Advances in Parallel Computing 8), </booktitle> <editor> J. J. Dongarra and W. Gentzsch, eds., </editor> <publisher> Elsevier Science Publishers B.V., </publisher> <address> Amsterdam, </address> <year> 1993. </year>
Reference-contexts: For non-numeric computations such a measure is meaningless. Also, exact counts for numbers of statements executed are not easily calculated a priori and rely entirely on the initial distribution of data. Based on the performance metrics outlined in <ref> [15] </ref>, a more useful measure for non-numeric algorithms such as ours is the number of records handled per second. Thus we use the average number of records merged per second (rec/s). This value is computed by dividing the total number of records by the total time taken by the merge.
Reference: [16] <author> B-C Huang and M. A. Langston, </author> <title> "Practical In-Place Merging," </title> <booktitle> Communications of the ACM 31 (1988), </booktitle> <pages> 348-352. </pages>
Reference: [17] <author> M. A. Kronrod, </author> <title> "An Optimal Ordering Algorithm Without a Field of Operation," </title> <journal> Dok. Akad, Nauk SSSR 186 (1969), </journal> <pages> 1256-1258. </pages>
Reference: [18] <author> V. Kumar, A. Grama, G. Anshul and G. Karypis, </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms, </title> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: There is no direct comparison of keys (ranking is accomplished by counting); data movement is not even timed; no global scan/reduction operations are involved. 3.3 Radix Sort Parallel radix sort <ref> [18] </ref> implemented on a finite number of processors first evenly divides the records to be sorted among the processors.
Reference: [19] <author> J. A. McCann and D. A. Bell, </author> <title> "A Hybrid Benchmarking Model for Database Machine Performance Studies," </title> <booktitle> in Computer Benchmarks (Advances in Parallel Computing 8), </booktitle> <editor> J. J. Dongarra and W. Gentzsch, eds., </editor> <publisher> Elsevier Science Publishers B.V., </publisher> <address> Amsterdam, </address> <year> 1993. </year>
Reference-contexts: Citing the need for a finer level of granularity than is generally available from serial benchmarks, McCann and Bell <ref> [19] </ref> have developed a hybrid benchmarking model. There are two stages to the model: serial and parallel. The serial stage measures resource consumption by modeling the system in terms of input, output, comparison and other low level operations.
Reference: [20] <author> O. </author> <title> Serlin, "The History of DebitCredit and the TPC," in The Benchmark Handbook for Database and Transaction Processing Systems, </title> <editor> J. Gray, editor, </editor> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Integer or character computations are used for evaluation and often involve comparisons of quantities or movement of data. Such computations may be repeated in the processing of a single transaction or query. Two of the better-known database benchmarks are the Wisconsin benchmark [7] and the TPC benchmark <ref> [20] </ref>. The former uses a synthetic database with a set of queries designed to measure performance of decision support systems; the latter measures transaction processing using a large number of small transactions. Both benchmarks were originally designed for serial systems, but are adaptable for use on parallel ones.
Reference: [21] <author> E. Spertus, S. C. Goldstein, K. E. Schauser, T. von Eiken, D. E. Cutter and W. J. Dally, </author> <title> "Evaluation of Mechanisms for Fine-Grained Parallel Programs in the J-Machine and the CM-5," </title> <booktitle> Proceedings, 20th Annual International Symposium on Computer Architecture (1993), </booktitle> <pages> 302-313. </pages>
Reference-contexts: This subset includes routines for string search, linked list insertion and quicksort. There is no formal mechanism for distribution. 2.2 Parallel Benchmarks In testing their Threaded Abstract Parallel Machine model of computation on the Thinking Machines CM-5 and the MIT J-machine, Spertus et al <ref> [21] </ref> make use of a simple quicksort. This algorithm, along with others used for comparison, were written in the parallel language Id90. Francis and Mathieson [10] put forth a parallel sort algorithm for benchmarking shared memory machines.
Reference: [22] <author> K. Thearling and S. Smith, </author> <title> "An Improved Supercomputer Benchmark," </title> <booktitle> Proceedings, Supercomputing (1992), </booktitle> <pages> 14-19. </pages>
Reference-contexts: 1 Introduction Only in recent years have non-numeric algorithms been considered for inclusion in parallel and supercomputer benchmarks <ref> [2, 8, 22] </ref>. Past benchmarking suites have been primarily concerned with a parallel system's aggregate speed of performing floating-point operations. As the field of parallel computation matures, however, the range of codes executed on parallel machines will likely include a wide assortment of non-numeric routines. <p> As the field of parallel computation matures, however, the range of codes executed on parallel machines will likely include a wide assortment of non-numeric routines. Counting sort and radix sort have been proposed as non-numeric benchmark candidates <ref> [2, 22] </ref>. Unfortunately, these simple algorithms are not very representative of operations carried out in wide assortments of non-numeric applications. Both of these sorts exhibit completely predictable data access patterns and, even worse, squander an unbounded amount of unnecessary memory. <p> This benchmark is intended to measure integer computation and communication performance. In <ref> [22] </ref>, Thearling and Smith note that the NAS sorting benchmark does not go far enough with either the size of the data file to be sorted or the characterization of the data set used.
Reference: [23] <institution> The Industrial Liaison Program at University of California at Berkeley, Eqntott #V9, </institution> <month> released </month> <year> 1985. </year>
Reference-contexts: Within the integer computation suite, CINT92, of the SPEC Benchmarks [8] are routines for generating and optimizing Programmable Logic Arrays, solving a nine queens' problem with a Lisp interpreter and compressing input files with the Lempel-Ziv encoding. Another code, eqntott <ref> [23] </ref>, that translates logical Boolean expressions into truth table equivalents, makes use of a sorting procedure. 2.3 Database Benchmarking In contrast to benchmarks targeted at specific types of computations or performance of particular hardware subsystems (e.g., I/O latency, cache access, page swapping), benchmarks for database management systems use higher level measures
Reference: [24] <author> L. G. Valiant, </author> <title> "Parallelism in Comparison Problems," </title> <journal> SIAM Journal of Computing 4 (1975), </journal> <pages> 348-355. </pages>
Reference-contexts: Furthermore, global data movement is really nothing more than the permutation of data through a network. More effective algorithms [29] are available if one wishes to measure this very limited type of data movement. 3.4 Valiant's Parallel Merge Converting the algorithm of Valiant <ref> [24] </ref> to a computational model with a fixed number, k, of processors is fairly simple.
Reference: [25] <author> A. J. van der Steen, </author> <title> "The Benchmark of the EuroBen Group," </title> <booktitle> Parallel Computing 17 (1991), </booktitle> <pages> 1211-1221. </pages>
Reference-contexts: Bhuyan and Zhang bring several of these examples together in [5]. Attempts to develop and make available a standardized parallel benchmarking suite are in their infancy. These attempts include the EuroBEN Group <ref> [25] </ref>, the GENESIS Project [1], PERFECT Club Benchmarks [4], NAS Parallel Benchmarks [2] and the vendor-supported SPEC Benchmarks [8]. Recently, a number of researchers interested in benchmarking met and formed the PARKBENCH Committee [14].
Reference: [26] <author> R. P. Weicker, "Dhrystone: </author> <title> A Synthetic Systems Programming Benchmark," </title> <booktitle> Communications of the ACM 27 (1984), </booktitle> <pages> 1013-1030. 21 </pages>
Reference-contexts: We know of no parallel implementations or performance results for any of the codes described within this section. 2.1.1 Dhrystone The Dhrystone benchmark <ref> [26] </ref> is a synthetic collection of operations based upon a literature survey of the distribution of language features most used in non-numeric, system-type programming. The major computational concentration is on string functions. Originally implemented in Ada, more recent versions have been coded and distributed in C [27].
Reference: [27] <author> R. P. Weicker, </author> <title> "Dhrystone Benchmark: Rationale for Version 2 and Measurement Rules," </title> <booktitle> SIGPLAN Notices 9 (1989), </booktitle> <pages> 60-82. </pages>
Reference-contexts: The major computational concentration is on string functions. Originally implemented in Ada, more recent versions have been coded and distributed in C <ref> [27] </ref>. The Dhrystone benchmark is intended to measure integer performance on small machines with simple architectures. 2.1.2 Stanford Small Programs Benchmark Set Weicker [28] makes mention of the Stanford Small Programs Benchmark Set.
Reference: [28] <author> R. P. Weicker, </author> <title> "A Detailed Look at Some Popular Benchmarks," </title> <booktitle> Parallel Computing 17 (1991), </booktitle> <pages> 1153-1172. </pages>
Reference-contexts: The major computational concentration is on string functions. Originally implemented in Ada, more recent versions have been coded and distributed in C [27]. The Dhrystone benchmark is intended to measure integer performance on small machines with simple architectures. 2.1.2 Stanford Small Programs Benchmark Set Weicker <ref> [28] </ref> makes mention of the Stanford Small Programs Benchmark Set. Used for the first comparisons between RISC and CISC processors, this benchmark is a collection of small programs brought together at Stanford University by John Hennessy and Peter Nye.
Reference: [29] <author> N. Nupairoj and L. Ni, </author> <title> "Performance Evaluation of Some MPI Implementations on Workstation Clusters," </title> <booktitle> Proceedings, Scalable Parallel Library Conference 1994, </booktitle> <pages> 98-105. </pages>
Reference-contexts: Records whose keys are determined from a combination of two or more data fields may not be handled at all. Furthermore, global data movement is really nothing more than the permutation of data through a network. More effective algorithms <ref> [29] </ref> are available if one wishes to measure this very limited type of data movement. 3.4 Valiant's Parallel Merge Converting the algorithm of Valiant [24] to a computational model with a fixed number, k, of processors is fairly simple.
References-found: 29

