URL: ftp://ai.iit.nrc.ca/pub/iit-papers/NRC-35074.ps.Z
Refering-URL: http://ai.iit.nrc.ca/cgi-bin/ftpsearch/?turney
Root-URL: 
Email: turney@ai.iit.nrc.ca  
Title: 268 ROBUST CLASSIFICATION WITH CONTEXT-SENSITIVE FEATURES  
Author: Peter D. Turney 
Address: Ottawa, Ontario, Canada, K1A 0R6  
Affiliation: Knowledge Systems Laboratory Institute for Information Technology National Research Council Canada  
Abstract: This paper addresses the problem of classifying observations when features are context-sensitive, especially when the testing set involves a context that is different from the training set. The paper begins with a precise definition of the problem, then general strategies are presented for enhancing the performance of classification algorithms on this type of problem. These strategies are tested on three domains. The first domain is the diagnosis of gas turbine engines. The problem is to diagnose a faulty engine in one context, such as warm weather, when the fault has previously been seen only in another context, such as cold weather. The second domain is speech recognition. The context is given by the identity of the speaker. The problem is to recognize words spoken by a new speaker, not represented in the training set. The third domain is medical prognosis. The problem is to predict whether a patient with hepatitis will live or die. The context is the age of the patient. For all three domains, exploiting context results in substantially more accurate classification. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Aha, D.W., Kibler, D., and Albert, </author> <title> M.K., Instance-based learning algorithms, </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> pp. 37-66, </pages> <year> 1991. </year>
Reference-contexts: The strategies are tested on three domains. First, the paper shows how contextual information can improve the diagnosis of faults in an aircraft gas turbine engine. The classification algorithms used on the engine data were a form of instance-based learning (IBL) <ref> [1, 2, 3] </ref> and a form of multivariate linear regression (MLR) [4]. Both algorithms benefit from contextual information. Second, the paper shows how context can be used to improve speech recognition. The speech recognition data were classified using IBL and cascade-correlation (CC) [5]. Again, both algorithms benefit from exploiting context. <p> Contextual weighting has a different purpose: to prefer some features over other features, if they may improve accuracy. THE CLASSIFICATION ALGORITHMS To demonstrate the generality of the above strategies, three different classification algorithms were used, a form of instance-based learning (IBL) <ref> [1, 2, 3] </ref>, multivariate linear regression (MLR) [4], and cascade-correlation (CC) [5]. Instance-based learning [1, 2] is closely related to the nearest neighbor pattern recognition paradigm [3]. Predictions are made by matching new data to stored data, using a measure of similarity to find the best matches [3]. <p> THE CLASSIFICATION ALGORITHMS To demonstrate the generality of the above strategies, three different classification algorithms were used, a form of instance-based learning (IBL) [1, 2, 3], multivariate linear regression (MLR) [4], and cascade-correlation (CC) [5]. Instance-based learning <ref> [1, 2] </ref> is closely related to the nearest neighbor pattern recognition paradigm [3]. Predictions are made by matching new data to stored data, using a measure of similarity to find the best matches [3]. The algorithm used here is a simple form of IBL, known as single-nearest neighbor pattern recognition. <p> If and are two feature vectors, then the similarity between and is defined as: (9) In this paper, IBL will be used to refer to this simple form of instance-based learning. IBL easily handles both symbolic <ref> [1] </ref> and real-valued [2] features and classes. Multivariate linear regression [4] models data with a system of linear equations. Like IBL, MLR easily handles both symbolic and real-valued features and classes.
Reference: 2. <author> Kibler, D., Aha, D.W., and Albert, </author> <title> M.K., Instance-based prediction of real-valued attributes, </title> <journal> Computational Intelligence, </journal> <volume> 5, </volume> <pages> pp. 51-57, </pages> <year> 1989. </year> <note> MACHINE LEARNING 276 </note>
Reference-contexts: The strategies are tested on three domains. First, the paper shows how contextual information can improve the diagnosis of faults in an aircraft gas turbine engine. The classification algorithms used on the engine data were a form of instance-based learning (IBL) <ref> [1, 2, 3] </ref> and a form of multivariate linear regression (MLR) [4]. Both algorithms benefit from contextual information. Second, the paper shows how context can be used to improve speech recognition. The speech recognition data were classified using IBL and cascade-correlation (CC) [5]. Again, both algorithms benefit from exploiting context. <p> Contextual weighting has a different purpose: to prefer some features over other features, if they may improve accuracy. THE CLASSIFICATION ALGORITHMS To demonstrate the generality of the above strategies, three different classification algorithms were used, a form of instance-based learning (IBL) <ref> [1, 2, 3] </ref>, multivariate linear regression (MLR) [4], and cascade-correlation (CC) [5]. Instance-based learning [1, 2] is closely related to the nearest neighbor pattern recognition paradigm [3]. Predictions are made by matching new data to stored data, using a measure of similarity to find the best matches [3]. <p> THE CLASSIFICATION ALGORITHMS To demonstrate the generality of the above strategies, three different classification algorithms were used, a form of instance-based learning (IBL) [1, 2, 3], multivariate linear regression (MLR) [4], and cascade-correlation (CC) [5]. Instance-based learning <ref> [1, 2] </ref> is closely related to the nearest neighbor pattern recognition paradigm [3]. Predictions are made by matching new data to stored data, using a measure of similarity to find the best matches [3]. The algorithm used here is a simple form of IBL, known as single-nearest neighbor pattern recognition. <p> If and are two feature vectors, then the similarity between and is defined as: (9) In this paper, IBL will be used to refer to this simple form of instance-based learning. IBL easily handles both symbolic [1] and real-valued <ref> [2] </ref> features and classes. Multivariate linear regression [4] models data with a system of linear equations. Like IBL, MLR easily handles both symbolic and real-valued features and classes. The algorithm used here is a form of MLR that is suitable for symbolic classes, known as linear discriminant analysis.
Reference: 3. <author> Dasarathy, </author> <title> B.V. Nearest Neighbor Pattern Classification Techniques, (edited collection), </title> <publisher> Los Alamitos, </publisher> <address> CA: </address> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference-contexts: The strategies are tested on three domains. First, the paper shows how contextual information can improve the diagnosis of faults in an aircraft gas turbine engine. The classification algorithms used on the engine data were a form of instance-based learning (IBL) <ref> [1, 2, 3] </ref> and a form of multivariate linear regression (MLR) [4]. Both algorithms benefit from contextual information. Second, the paper shows how context can be used to improve speech recognition. The speech recognition data were classified using IBL and cascade-correlation (CC) [5]. Again, both algorithms benefit from exploiting context. <p> Contextual weighting has a different purpose: to prefer some features over other features, if they may improve accuracy. THE CLASSIFICATION ALGORITHMS To demonstrate the generality of the above strategies, three different classification algorithms were used, a form of instance-based learning (IBL) <ref> [1, 2, 3] </ref>, multivariate linear regression (MLR) [4], and cascade-correlation (CC) [5]. Instance-based learning [1, 2] is closely related to the nearest neighbor pattern recognition paradigm [3]. Predictions are made by matching new data to stored data, using a measure of similarity to find the best matches [3]. <p> THE CLASSIFICATION ALGORITHMS To demonstrate the generality of the above strategies, three different classification algorithms were used, a form of instance-based learning (IBL) [1, 2, 3], multivariate linear regression (MLR) [4], and cascade-correlation (CC) [5]. Instance-based learning [1, 2] is closely related to the nearest neighbor pattern recognition paradigm <ref> [3] </ref>. Predictions are made by matching new data to stored data, using a measure of similarity to find the best matches [3]. The algorithm used here is a simple form of IBL, known as single-nearest neighbor pattern recognition. <p> Instance-based learning [1, 2] is closely related to the nearest neighbor pattern recognition paradigm <ref> [3] </ref>. Predictions are made by matching new data to stored data, using a measure of similarity to find the best matches [3]. The algorithm used here is a simple form of IBL, known as single-nearest neighbor pattern recognition. The algorithm is given, as input, an observation (a feature vector) in the testing set.
Reference: 4. <author> Draper, N.R. and Smith, H., </author> <title> Applied Regression Analysis, (second edition), </title> <address> New York, NY: </address> <publisher> John Wiley & Sons, </publisher> <year> 1981. </year>
Reference-contexts: First, the paper shows how contextual information can improve the diagnosis of faults in an aircraft gas turbine engine. The classification algorithms used on the engine data were a form of instance-based learning (IBL) [1, 2, 3] and a form of multivariate linear regression (MLR) <ref> [4] </ref>. Both algorithms benefit from contextual information. Second, the paper shows how context can be used to improve speech recognition. The speech recognition data were classified using IBL and cascade-correlation (CC) [5]. Again, both algorithms benefit from exploiting context. <p> Contextual weighting has a different purpose: to prefer some features over other features, if they may improve accuracy. THE CLASSIFICATION ALGORITHMS To demonstrate the generality of the above strategies, three different classification algorithms were used, a form of instance-based learning (IBL) [1, 2, 3], multivariate linear regression (MLR) <ref> [4] </ref>, and cascade-correlation (CC) [5]. Instance-based learning [1, 2] is closely related to the nearest neighbor pattern recognition paradigm [3]. Predictions are made by matching new data to stored data, using a measure of similarity to find the best matches [3]. <p> If and are two feature vectors, then the similarity between and is defined as: (9) In this paper, IBL will be used to refer to this simple form of instance-based learning. IBL easily handles both symbolic [1] and real-valued [2] features and classes. Multivariate linear regression <ref> [4] </ref> models data with a system of linear equations. Like IBL, MLR easily handles both symbolic and real-valued features and classes. The algorithm used here is a form of MLR that is suitable for symbolic classes, known as linear discriminant analysis. <p> Otherwise, is set to 0. The in the equation for class are selected from among the features available in the feature space. MLR uses the forward selection procedure to select the <ref> [4] </ref>. Standard linear regression techniques are used to find the best values for the constant coefficients in the linear equation [7]. In the testing phase, MLR is given the values of the variables for each observation in the testing set.
Reference: 5. <author> Fahlman, S.E. and Lebiere, C., </author> <title> The Cascade-Correlation Learning Architecture, </title> <type> (technical report), </type> <institution> CMU-CS-90-100, Pittsburgh, PA: Carnegie-Mellon University, </institution> <year> 1991. </year>
Reference-contexts: Both algorithms benefit from contextual information. Second, the paper shows how context can be used to improve speech recognition. The speech recognition data were classified using IBL and cascade-correlation (CC) <ref> [5] </ref>. Again, both algorithms benefit from exploiting context. Third, the paper shows how context can be used to improve the accuracy of medical prognosis. Hepatitis data were classified using IBL. <p> THE CLASSIFICATION ALGORITHMS To demonstrate the generality of the above strategies, three different classification algorithms were used, a form of instance-based learning (IBL) [1, 2, 3], multivariate linear regression (MLR) [4], and cascade-correlation (CC) <ref> [5] </ref>. Instance-based learning [1, 2] is closely related to the nearest neighbor pattern recognition paradigm [3]. Predictions are made by matching new data to stored data, using a measure of similarity to find the best matches [3]. <p> To predict the class of an observation, MLR calculates the values of the linear equations. This yields values for , one value for each of the classes. The predicted class of the observation is the class with the largest calculated value. Cascade-correlation (CC) <ref> [5] </ref> is a form of neural network algorithm. Like IBL and MLR, CC easily handles both symbolic and real-valued features and classes. The CC algorithm is similar to feed-forward neural networks trained with back-propagation. <p> The three strategies were also tested with cascade-correlation <ref> [5] </ref>. Because of the time required for training CC, results were gathered for only two cases: With no preprocessing, cascade-correlation correctly classified 216 observations (47%). With preprocessing by all three strategies, cascade-correlation correctly classified 236 observations (51%). <p> ACKNOWLEDGMENTS The gas turbine engine data and engine expertise were provided by the Engine Laboratory of the NRC, with funding from DND. The vowel data and the hepatitis data were obtained from the University of California data repository (ftp ics.uci.edu, directory /pub/machine-learning-databases) [10]. The cascade-correlation <ref> [5] </ref> software was obtained from Carnegie-Mellon University (ftp pt.cs.cmu.edu, directory /afs/cs/project/connect/code). The author wishes to thank Rob Wylie and Peter Clark of the NRC and two anonymous referees of IEA/AIE-93 for their helpful comments on this paper.
Reference: 6. <author> Katz, A.J., Gately, </author> <title> M.T., and Collins, D.R., Robust classifiers without robust features, </title> <journal> Neural Computation, </journal> <volume> 2, </volume> <pages> pp. 472-479, </pages> <year> 1990. </year>
Reference-contexts: Examples of this use of background knowledge will be presented later in the paper. STRATEGIES FOR EXPLOITING CONTEXT Katz et al. <ref> [6] </ref> list four strategies for using contextual information when classifying: 1. Contextual normalization: The contextual features can be used to normalize the context-sensitive primary features, prior to classification. The intent is to process context-sensitive features in a way that reduces their sensitivity to the context. 2. <p> Thus the synergy found in the experiments reported here is to be expected. RELATED WORK The work described here is most closely related to <ref> [6] </ref>. However, [6] did not give a precise definition of the distinction between contextual features (their terminology: parameters or global features) and primary features (their terminology: features). They examined only contextual classifier selection, using neural networks to classify images, with context such as lighting. <p> Thus the synergy found in the experiments reported here is to be expected. RELATED WORK The work described here is most closely related to <ref> [6] </ref>. However, [6] did not give a precise definition of the distinction between contextual features (their terminology: parameters or global features) and primary features (their terminology: features). They examined only contextual classifier selection, using neural networks to classify images, with context such as lighting.
Reference: 7. <author> Turney, P.D. and Halasz, M., </author> <title> Contextual normalization applied to aircraft gas turbine engine diagnosis, (in press), </title> <journal> Journal of Applied Intelligence, </journal> <year> 1993. </year>
Reference-contexts: The in the equation for class are selected from among the features available in the feature space. MLR uses the forward selection procedure to select the [4]. Standard linear regression techniques are used to find the best values for the constant coefficients in the linear equation <ref> [7] </ref>. In the testing phase, MLR is given the values of the variables for each observation in the testing set. To predict the class of an observation, MLR calculates the values of the linear equations. This yields values for , one value for each of the classes. <p> The observations fall in eight classes: seven classes of deliberately implanted faults and a healthy class <ref> [7] </ref>. The amount of thrust produced by an engine is a primary feature for diagnosing faults in the engine. The exterior air temperature is a contextual feature, since the engines performance is sensitive to the exterior air temperature. <p> Thus the sample size for testing purposes is 242. The data were analyzed using two classification algorithms, IBL and MLR. IBL and MLR were also used to preprocess the data by contextual normalization <ref> [7] </ref>. <p> The values of and were estimated using IBL and MLR, trained with healthy observations (spanning a range of ambient conditions) <ref> [7] </ref>. Table 2 (derived from Table 5 in [7]) shows the results of this experiment. For IBL, the average score without contextual normalization is 42% and the average score with contextual normalization is 55%, an improvement of 13%. <p> The values of and were estimated using IBL and MLR, trained with healthy observations (spanning a range of ambient conditions) <ref> [7] </ref>. Table 2 (derived from Table 5 in [7]) shows the results of this experiment. For IBL, the average score without contextual normalization is 42% and the average score with contextual normalization is 55%, an improvement of 13%. <p> IBL avg/dev baseline 111 46 IBL IBL 139 57 MLR none 100 41 MLR min/max train 100 41 MLR avg/dev train 100 41 MLR percentile train 74 31 MLR avg/dev baseline 100 41 MLR IBL 103 43 MACHINE LEARNING 272 cantly better than all of the alternatives that were examined <ref> [7] </ref>. SPEECH RECOGNITION This section examines strategies 1, 2, and 5: contextual normalization, contextual expansion, and contextual weighting. The problem is to recognize a vowel spoken by an arbitrary speaker. There are ten continuous primary features (derived from spectral data) and two discrete contextual features (the speakers identity and sex). <p> The values of and were estimated by taking the average and standard deviation of for each interval . This is different from the method used for contextual normalization with the continuous contextual features in gas turbine engine diagnosis <ref> [7] </ref>. Note that equation (11) does not require continuous features; it works well with the boolean features in the hepatitis data, when true and false are represented by one and zero. Contextual expansion: The age of the patient was treated as another feature.
Reference: 8. <author> Deterding, D., </author> <title> Speaker Normalization for Automatic Speech Recognition, </title> <type> (Ph.D. thesis), </type> <institution> Cambridge, UK: University of Cambridge, Department of Engineering, </institution> <year> 1989. </year>
Reference-contexts: The problem is to recognize a vowel spoken by an arbitrary speaker. There are ten continuous primary features (derived from spectral data) and two discrete contextual features (the speakers identity and sex). The observations fall in eleven classes (eleven different vowels) <ref> [8] </ref>. For speech recognition, spectral data is a primary feature for recognizing a vowel. The sex of the speaker is a contextual feature, since we can achieve better recognition by exploiting the fact that a mans voice tends to sound different from a womans voice. <p> It is the average, for all speakers in the training set and all classes, of the standard deviation of the feature, for a given speaker and a given class. Let , where and , be the standard deviations of for each of Table 3: Robinsons <ref> (1989) </ref> results with the vowel data. classifier no. of hidden units no. correct (of 462) percent correct Single-layer perceptron - 154 33 Multi layer perceptron 88 234 51 Multi-layer perceptron 22 206 45 Multi-layer perceptron 11 203 44 Modified Kanerva Model 528 231 50 Modified Kanerva Model 88 197 43 Radial <p> They found that contextual classifier selection resulted in increased accuracy and efficiency. They did not address the difficulties that arise when the context in the testing set is different from the context in the training set. This work is also related to work in speech recognition on speaker normalization <ref> [8] </ref>. However, the work on speaker normalization tends to be specific to speech recognition. Here, the concern is with general-purpose strategies for exploiting context.
Reference: 9. <author> Robinson, A.J., </author> <title> Dynamic Error Propagation Networks, </title> <type> (Ph.D. thesis), </type> <institution> Cambridge, UK: University of Cambridge, Department of Engineering, </institution> <year> 1989. </year>
Reference-contexts: Each of the eleven vowels was spoken six times by each speaker. The training set is from four male and four female speakers ( observations). The testing set is from four new male and three new female speakers ( observations). Using a wide variety of neural network algorithms, Robinson <ref> [9] </ref> achieved accuracies ranging from 33% to 56% correct on the testing set. The mean score was 49%, with a standard deviation of 6%. Table 3 summarizes Robinsons results. <p> It is the average, for all speakers in the training set and all classes, of the standard deviation of the feature, for a given speaker and a given class. Let , where and , be the standard deviations of for each of Table 3: Robinsons <ref> (1989) </ref> results with the vowel data. classifier no. of hidden units no. correct (of 462) percent correct Single-layer perceptron - 154 33 Multi layer perceptron 88 234 51 Multi-layer perceptron 22 206 45 Multi-layer perceptron 11 203 44 Modified Kanerva Model 528 231 50 Modified Kanerva Model 88 197 43 Radial
Reference: 10. <author> Murphy, P.M. and Aha, D.W., </author> <title> UCI Repository of Machine Learning Databases, </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1991. </year>
Reference-contexts: There are two contextual features, of which one is discrete (patients sex) and one is continuous (patients age). The patients sex was not used in the following experiments, since 90% of the patients were male. The observations fall in two classes (live or die) <ref> [10] </ref>. There are many missing values in the hepatitis data. These were filled in by using the single nearest neighbor algorithm with the training data. For hepatitis prognosis, bilirubin level is a primary feature for determining whether the patient will die from the disease. <p> ACKNOWLEDGMENTS The gas turbine engine data and engine expertise were provided by the Engine Laboratory of the NRC, with funding from DND. The vowel data and the hepatitis data were obtained from the University of California data repository (ftp ics.uci.edu, directory /pub/machine-learning-databases) <ref> [10] </ref>. The cascade-correlation [5] software was obtained from Carnegie-Mellon University (ftp pt.cs.cmu.edu, directory /afs/cs/project/connect/code). The author wishes to thank Rob Wylie and Peter Clark of the NRC and two anonymous referees of IEA/AIE-93 for their helpful comments on this paper.
Reference: 11. <author> Diaconis, P. and Efron, B., </author> <title> Computer-intensive methods in statistics, </title> <journal> Scientific American, </journal> <volume> 248, </volume> <month> (May), </month> <pages> pp. 116-131, </pages> <year> 1983. </year>
Reference-contexts: This may be due to the fact that there is no systematic difference between the training and testing sets in the hepatitis data, while the testing set for the vowel data uses different speakers from the training set. For comparison, other researchers have reported accuracies of 80% <ref> [11] </ref> and 83% [12] on the hepatitis data. It is interesting that a single-nearest neighbor algorithm can match or surpass these results, when strategies are employed to use the contextual information contained in the data.
Reference: 12. <author> Cestnik, G., Konenenko, I., and Bratko, I., Assistant-86: </author> <title> a knowledge-elicitation tool for sophisticated users, in Progress in Machine Learning, edited by I. </title> <editor> Bratko and N. </editor> <booktitle> Lavrac, </booktitle> <pages> pp. 31-45, </pages> <address> Wilmslow, England: </address> <publisher> Sigma Press, </publisher> <year> 1987. </year>
Reference-contexts: For comparison, other researchers have reported accuracies of 80% [11] and 83% <ref> [12] </ref> on the hepatitis data. It is interesting that a single-nearest neighbor algorithm can match or surpass these results, when strategies are employed to use the contextual information contained in the data.
References-found: 12

