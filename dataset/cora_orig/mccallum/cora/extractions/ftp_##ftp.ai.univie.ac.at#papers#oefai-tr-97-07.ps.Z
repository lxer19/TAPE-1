URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-97-07.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+97-07
Root-URL: 
Email: E-mail: juffi@ai.univie.ac.at  
Title: Noise-Tolerant Windowing  
Author: Johannes Furnkranz 
Web: OEFAI-TR-97-07  
Address: Schottengasse 3, A-1010 Wien, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: Windowing has been proposed as a procedure for efficient memory use in the ID3 decision tree learning algorithm. However, previous work has shown that it may often lead to a decrease in performance, in particular in noisy domains. Following up on previous work, where we have shown that the ability of separate-and-conquer rule learning algorithms to learn rules independently can be exploited for more efficient windowing procedures, we demonstrate in this paper how this property can be exploited to achieve noise-tolerance in windowing.
Abstract-found: 1
Intro-found: 1
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> Megainduction: A test flight. </title> <editor> In Birnbaum, L., & Collins, G. (Eds.), </editor> <booktitle> Proceedings of the 8th International Workshop on Machine Learning (ML-91), </booktitle> <pages> pp. </pages> <address> 596-599 Evanston, IL. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This hypothesis is consistent with the results of (Wirth & Catlett, 1988) and <ref> (Catlett, 1991) </ref>, where the sensitivity of windowing to noisy data sets has been shown empirically. 5 A Noise-Tolerant Version of Windowing The windowing algorithm described in (Furnkranz, 1997), which is only applicable to noise-free domains, is based on the observation that rule learning algorithms will re-discover good rules again and again
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 (4), </volume> <pages> 261-283. </pages>
Reference-contexts: However, in noisy domains, noise-tolerant learning algorithms will typically produce rules that are not consistent with the training data. Thus, more elaborate criteria must be used. We have experimented with a variety of criteria known from the literature (like e.g. CN2's likelihood ratio significance test <ref> (Clark & Niblett, 1989) </ref>), but found that they are insufficient for our purposes.
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Fast effective rule induction. </title> <editor> In Prieditis, A., & Russell, S. (Eds.), </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML-95), </booktitle> <pages> pp. </pages> <address> 115-123 Lake Tahoe, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The basic learning algorithm we use, I-RIP, is based on I-REP (Furnkranz & Widmer, 1994) and its successor RIPPER <ref> (Cohen, 1995) </ref>. However, the algorithms presented in this paper do not depend on this choice; any other noise-tolerant rule learning algorithm could be used in I-RIP's place. <p> The resulting rule is added to the theory, and all examples that it covers are removed from the training set. The remaining training examples are used to learn another rule until no more meaningful rules can be discovered. In <ref> (Cohen, 1995) </ref> it was shown that some of the parameters of the I-REP algorithm, like the pruning and stopping criteria, were not chosen optimally. We have implemented the I-REP algorithm as described in (Furnkranz & Widmer, 1994), but using RIPPER's rule-value-metric pruning criterion and its 0:5-rule-accuracy stopping criterion. <p> We have not implemented RIPPER's rule optimization heuristics. Thus our I-RIP algorithm is half-way between I-REP and RIPPER. As such, it is quite similar to I-REP*, which is also described in <ref> (Cohen, 1995) </ref>, but it differs from it as its implementation is closer to the original I-REP. For example, I-RIP considers every condition in a rule for pruning, while I-REP* only considers to delete a final sequence of conditions.
Reference: <author> Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. </author> <year> (1996). </year> <title> From data mining to knowledge discovery in databases. </title> <journal> AI Magazine, </journal> <volume> 17 (3), </volume> <pages> 37-54. </pages>
Reference-contexts: One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent developments in the areas of Knowledge Discovery in Databases <ref> (Fayyad, Piatetsky-Shapiro, & Smyth, 1996) </ref> and Intelligent Information Retrieval (Hearst & Hirsh, 1996) have again shown the limits of conventional machine learning algorithms. Dimensionality reduction using subsampling procedures has been recognized as a promising field of research (Lewis & Catlett, 1994; Yang, 1996).
Reference: <author> Furnkranz, J. </author> <year> (1996). </year> <title> Separate-and-conquer rule learning. </title> <type> Tech. rep. </type> <institution> OEFAI-TR-96-25, Austrian Research Institute for Artificial Intelligence. 12 Furnkranz, J. </institution> <year> (1997). </year> <title> More efficient windowing. </title> <type> Tech. rep. </type> <institution> OEFAI-TR-97-01, Austrian Research Institute for Artificial Intelligence. </institution>
Reference-contexts: In this paper we will show how this property can be exploited in order to achieve noise-tolerance. 2 The I-RIP algorithm We have conducted our study in the framework of separate-and-conquer rule learning algorithms that has recently gained in popularity <ref> (Furnkranz, 1996) </ref>. The basic learning algorithm we use, I-RIP, is based on I-REP (Furnkranz & Widmer, 1994) and its successor RIPPER (Cohen, 1995). However, the algorithms presented in this paper do not depend on this choice; any other noise-tolerant rule learning algorithm could be used in I-RIP's place.
Reference: <author> Furnkranz, J., & Widmer, G. </author> <year> (1994). </year> <title> Incremental Reduced Error Pruning. </title>
Reference-contexts: The basic learning algorithm we use, I-RIP, is based on I-REP <ref> (Furnkranz & Widmer, 1994) </ref> and its successor RIPPER (Cohen, 1995). However, the algorithms presented in this paper do not depend on this choice; any other noise-tolerant rule learning algorithm could be used in I-RIP's place. <p> In (Cohen, 1995) it was shown that some of the parameters of the I-REP algorithm, like the pruning and stopping criteria, were not chosen optimally. We have implemented the I-REP algorithm as described in <ref> (Furnkranz & Widmer, 1994) </ref>, but using RIPPER's rule-value-metric pruning criterion and its 0:5-rule-accuracy stopping criterion. We have not implemented RIPPER's rule optimization heuristics. Thus our I-RIP algorithm is half-way between I-REP and RIPPER. <p> In terms of accuracy, no significant differences can be observed between I-RIP, Win-RIP, and I-Win (0.0), although the latter is able to compensate some of the weakness of I-RIP at low example set sizes that is due to its internal split of the data <ref> (Furnkranz & Widmer, 1994) </ref>. I-Win with ff = 0:5 and ff = 1:0 has a significantly worse performance, because these versions are often content with slightly over-general rules, which is detrimental in this noise-free domain.
Reference: <editor> In Cohen, W., & Hirsh, H. (Eds.), </editor> <booktitle> Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 70-77 New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <editor> Hearst, M. A., & Hirsh, H. (Eds.). </editor> <booktitle> (1996). Proceedings of the AAAI Spring Symposium on Machine Learning in Information Access. </booktitle> <publisher> AAAI Press. Technical Report SS-96-05. </publisher>
Reference-contexts: One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent developments in the areas of Knowledge Discovery in Databases (Fayyad, Piatetsky-Shapiro, & Smyth, 1996) and Intelligent Information Retrieval <ref> (Hearst & Hirsh, 1996) </ref> have again shown the limits of conventional machine learning algorithms. Dimensionality reduction using subsampling procedures has been recognized as a promising field of research (Lewis & Catlett, 1994; Yang, 1996).
Reference: <author> Lewis, D. D., & Catlett, J. </author> <year> (1994). </year> <title> Heterogeneous uncertainty sampling for supervised learning. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning (ML-94). </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine Learning. An Artificial Intelligence Approach, </booktitle> <pages> pp. 463-482. </pages> <publisher> Tioga, </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: The gain in efficiency is obtained by identifying an appropriate subset of the given training examples, from which a theory of sufficient quality can be induced. Such procedures are also known as subsam-pling. Windowing has been proposed in <ref> (Quinlan, 1983) </ref> as a supplement to the inductive decision tree learner ID3 to enable it to tackle tasks which would otherwise have exceeded the memory capacity of the computers of those days. Despite first successful experiments in the KRKN chess endgame domain (Quinlan, 1983), windowing has not played a major role <p> Windowing has been proposed in <ref> (Quinlan, 1983) </ref> as a supplement to the inductive decision tree learner ID3 to enable it to tackle tasks which would otherwise have exceeded the memory capacity of the computers of those days. Despite first successful experiments in the KRKN chess endgame domain (Quinlan, 1983), windowing has not played a major role in machine learning research. One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling.
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference-contexts: I-REP achieves noise-tolerance by first learning a single, complete and consistent rule on two thirds of the training data and then pruning this rule on the remaining third using a form of reduced error pruning <ref> (Quinlan, 1987) </ref>. The resulting rule is added to the theory, and all examples that it covers are removed from the training set. The remaining training examples are used to learn another rule until no more meaningful rules can be discovered.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: For example, in (Wirth & Catlett, 1988) windowing with the decision tree learner ID3 could not achieve significant run-time gains over pure ID3, while the slightly modified version of windowing used in C4.5 is able to achieve a run-time improvement of only about 15% (p. 59 of <ref> (Quinlan, 1993) </ref>). three versions of I-Win, each one using a different setting of its ff parameter.
Reference: <author> Wirth, J., & Catlett, J. </author> <year> (1988). </year> <title> Experiments on the costs and benefits of windowing in ID3. </title> <editor> In Laird, J. (Ed.), </editor> <booktitle> Proceedings of the 5th International Conference on Machine Learning (ML-88), </booktitle> <pages> pp. </pages> <address> 87-99 Ann Arbor, MI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Dimensionality reduction using subsampling procedures has been recognized as a promising field of research (Lewis & Catlett, 1994; Yang, 1996). A good deal of the lack of interest in windowing can be attributed to an empirical study <ref> (Wirth & Catlett, 1988) </ref> that showed that windowing is unlikely to gain any efficiency. The authors studied windowing with ID3 in various domains and concluded that windowing cannot be recommended as a procedure for improving efficiency. <p> This hypothesis is consistent with the results of <ref> (Wirth & Catlett, 1988) </ref> and (Catlett, 1991), where the sensitivity of windowing to noisy data sets has been shown empirically. 5 A Noise-Tolerant Version of Windowing The windowing algorithm described in (Furnkranz, 1997), which is only applicable to noise-free domains, is based on the observation that rule learning algorithms will re-discover <p> First we have evaluated our algorithms on the 8124 example Mushroom database. Although this database is known to be noise-free, it forms an interesting test-bed for our algorithms, because it allows a rough comparison to previous results. For example, in <ref> (Wirth & Catlett, 1988) </ref> windowing with the decision tree learner ID3 could not achieve significant run-time gains over pure ID3, while the slightly modified version of windowing used in C4.5 is able to achieve a run-time improvement of only about 15% (p. 59 of (Quinlan, 1993)). three versions of I-Win, each
Reference: <author> Yang, Y. </author> <year> (1996). </year> <title> Sampling strategies and learning efficiency in text categorization. </title> <editor> In Hearst, M., & Hirsh, H. (Eds.), </editor> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning in Information Access, </booktitle> <pages> pp. 88-95. </pages> <note> AAAI Press. Technical Report SS-96-05. 13 </note>
References-found: 14

