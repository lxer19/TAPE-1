URL: http://www.cs.berkeley.edu/~brewer/cs262/rosenblum.ps
Refering-URL: http://www.cs.berkeley.edu/~brewer/cs262/fall95/fall95.html
Root-URL: 
Abstract: Computer systems are rapidly changing. Over the next few years, we will see wide-scale deployment of dynamically-scheduled processors that can issue multiple instructions every clock cycle, execute instructions out of order, and overlap computation and cache misses. We also expect clock-rates to increase, caches to grow, and multiprocessors to replace uniprocessors. Using SimOS, a complete machine simulation environment, this paper explores the impact of the above architectural trends on operating system performance. We present results based on the execution of large and realistic workloads (program development, transaction processing, and engineering compute-server) running on the IRIX 5.3 operating system from Silicon Graphics Inc. Looking at uniprocessor trends, we find that disk I/O is the first-order bottleneck for workloads such as program development and transaction processing. Its importance continues to grow over time. Ignoring I/O, we find that the memory system is the key bottleneck, stalling the CPU for over 50% of the execution time. Surprisingly, however, our results show that this stall fraction is unlikely to increase on future machines due to increased cache sizes and new latency hiding techniques in processors. We also find that the benefits of these architectural trends spread broadly across a majority of the important services provided by the operating system. We find the situation to be much worse for multiprocessors. Most operating systems services consume 30-70% more time than their uniprocessor counterparts. A large fraction of the stalls are due to coherence misses caused by communication between processors. Because larger caches do not reduce coherence misses, the performance gap between uniprocessor and multiprocessor performance will increase unless operating system developers focus on kernel restructuring to reduce unnecessary communication. The paper presents a detailed decomposition of execution time (e.g., instruction execution time, memory stall time separately for instructions and data, synchronization time) for important kernel services in the three workloads. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> James Bennett and Mike Flynn, </author> <title> Performance Factors for Superscalar Processors, </title> <type> Technical report CSL-TR-95-661, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: This ability to hide cache miss latency is potentially a large performance win for programs with poor memory system locality, a characteristic frequently attributed to operating system kernels. We model these next-generation processors using the MXS CPU simulator <ref> [1] </ref>. We configure the MXS pipeline and caches to model the MIPS R10000, the successor to the MIPS R4400 due out in early 1996. The MXS simulator models a processor built out of decoupled fetch, execution, and graduation units.
Reference: [2] <author> John Chapin, Stephen A. Herrod, Mendel Rosenblum, and Anoop Gupta, </author> <title> Memory System Performance of UNIX on CC-NUMA Multiprocessors, </title> <booktitle> In Proceedings of SIGMETRICS/ PERFORMANCE 95, </booktitle> <pages> pp. 1-13, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: This kind of error is more difficult to detect since it is likely the workload will continue to run correctly. To validate the timings and statistic reporting, we configure SimOS to look like the one-cluster DASH multiprocessor used in a previous operating system characterization study <ref> [2] </ref> and examine the cache and profile statistics of a parallel compilation workload. Statistics in [2] were obtained with a bus monitor, and are presented in Table 2.1. Although the sources of these statistics are completely different, the system behavior is quite similar. <p> To validate the timings and statistic reporting, we configure SimOS to look like the one-cluster DASH multiprocessor used in a previous operating system characterization study <ref> [2] </ref> and examine the cache and profile statistics of a parallel compilation workload. Statistics in [2] were obtained with a bus monitor, and are presented in Table 2.1. Although the sources of these statistics are completely different, the system behavior is quite similar. The absence of existing systems with dynamically-scheduled processors makes validation of the next-generation machine model difficult. <p> For the uniprocessor case, we use a parallel make utility con Execution profile Fraction of misses in kernel mode Kernel User Idle SimOS 25% 53% 22% 52% Bus monitor <ref> [2] </ref> 24% 48% 28% 49% TABLE 2.1. SimOS validation results. We compare several coarse statistics from SimOS to a published operating system characterization.
Reference: [3] <author> J. Bradley Chen and Brian N. Bershad, </author> <title> The Impact of Operating System Structure on Memory System Performance, </title> <booktitle> In Proceedings of the 16th International Symposium on Operating System Principles, </booktitle> <pages> pp. 120-133, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: In fact, their memory system behavior is quite comparable to that of the kernel. Other studies have concluded that the memory system behavior of the kernel was worse than that of application programs <ref> [3] </ref>. We find that this is true for smaller programs, but does not hold for large applications. The implication is that processor improvements targeted at large applications will likely benefit kernel performance as well. <p> One interesting point of comparison between these studies and ours is the methodology used to observe system behavior. Previous studies were based on the analysis of traces either using hardware monitors [2][8][13] or through software instrumentation <ref> [3] </ref>. To these traditional methodologies, we add the use of complete machine simulation for operating system characterization. We believe that our approach has several advantages over the previous techniques. <p> SimOS can model any hardware platform that would successfully execute the workloads. Studies often use software instrumentation to annotate a work-loads code and to improve the visibility of hardware monitors. Unfortunately, this instrumentation is intrusive. For example, Chen <ref> [3] </ref> had to deal with both time and memory dilations. Although this was feasible for a uniprocessor memory system behavior study, it becomes significantly more difficult on multiprocessors. The SimOS annotation mechanism allows non-intrusive system observation at levels of detail not previously obtainable. Our results confirm numerous studies [2][3][8][13]: memory system
Reference: [4] <author> Jim Gray, Ed., </author> <title> The Benchmark Handbook for Database and Transaction Processing systems, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Database Workload. As our second workload, we examine the performance impact of a Sybase SQL Server (version 10 for SGI IRIX) supporting a transaction processing workload. This workload is a bank/customer transaction suite modeled after the TPC-B transaction processing benchmark <ref> [4] </ref>. The database consists of 63 Mbytes of data and 570 Kbytes of indexes. The data and the transaction logs are stored on separate disk devices. This workload makes heavy use of the operating system, specifically inter-processor communication.
Reference: [5] <author> Mark Horowitz, </author> <title> Stanford University, </title> <type> Personal Communication, </type> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Processors like the MIPS R10000 have significantly increased the complexity of the design while holding the clock rate relatively constant. The next challenge appears to be increasing the clock rate without sacrificing advanced processor features <ref> [5] </ref>. We assume that a 1998 microprocessor will contain the latency tolerating features of the 1996 model, but will run at a 500Mhz clock rate and contain larger caches. We also allow for 5 small improvements in cache and memory system miss times.
Reference: [6] <author> David Kotz, Song Bac Toh, and Sriram Radhakrishnan, </author> <title> A Detailed Simulation of the HP 97560 Disk Drive, </title> <type> Technical Report PCS-TR94-20, </type> <institution> Dartmouth College, </institution> <year> 1994. </year>
Reference-contexts: While we vary several machine parameters, there are others that remain constant. All simulated machines contain 128 Mbytes of main memory, support multiple disks, and have a single console device. The timing of the disk device is modeled using a validated simulator of the HP 97560 disk 1 <ref> [6] </ref>. Data from the disk is transferred to memory using cache-coherent DMA. No input is given to the console and the ethernet controller during the measurement runs. The CPU models support the MIPS-2 1.
Reference: [7] <author> Jeff Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John L. Hennessy, </author> <title> The Stanford FLASH multiprocessor, </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pp. 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The multiprocessor database is striped across 4 disks to improve throughput. Engineering Workload. The final workload we use represents an engineering development environment. Our workload combines instances of a large memory system simulation (we simulate the memory system of the Stanford FLASH machine <ref> [7] </ref> using the FlashLite simulator) along with verilog simulation runs (we simulate the verilog of the FLASH MAGIC chip using the Chronologics VCS simulator).
Reference: [8] <author> Ann M. G. Maynard, Collette M. Donnelly, and Bret R. Olszewski, </author> <title> Contrasting Characteristics and Cache Performance of Technical and Multi-User Commercial Workloads, </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 145-156, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: The SimOS annotation mechanism allows non-intrusive system observation at levels of detail not previously obtainable. Our results confirm numerous studies [2][3]<ref> [8] </ref>[13]: memory system performance, block copy, and instruction miss stall time are important components of operating system performance. Like Maynard [8] and Torrellas [13], who used hardware-based traces, we were able to examine large applications and confirm their results.
Reference: [9] <institution> MIPS Technologies, Inc., </institution> <note> R10000 Microprocessor Users Manual - 2nd edition, </note> <month> June </month> <year> 1995. </year>
Reference-contexts: Cache misses in this simulator stall the CPU for the duration of the cache miss. Cache size, organization, and miss penalties were chosen based on the SGI workstation parameters. 1 3.3 1996 Model Next-generation microprocessors such as the MIPS R10000 <ref> [9] </ref>, Intel P6, and Sun UltraSPARC, will incorporate several new features including multiple instruction issue, dynamic scheduling, and non-blocking caches. The multiple instruction issue feature allows these processors to issue multiple consecutive instructions every clock cycle.
Reference: [10] <author> John Ousterhout, </author> <title> Why Arent Operating Systems Getting Faster as Fast as Hardware?, </title> <booktitle> In Proceedings of the Summer 1990 USENIX Conference, </booktitle> <pages> pp. 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Program Development Workload. A common use of todays machines is as a platform for program development. This type of workload typically includes many small, short-lived processes that rely significantly on operating system services. We use a variant of the compile phase of the Modified Andrew Benchmark <ref> [10] </ref>. The Modified Andrew Benchmark uses the gcc compiler to compile 17 files with an average length of 427 lines each.
Reference: [11] <author> Mendel Rosenblum, Stephen A. Herrod, Emmett Witchel, and Anoop Gupta, </author> <title> Fast and Accurate Multiprocessor Simulation: The SimOS Approach, </title> <booktitle> In IEEE Parallel and Distributed Technology, </booktitle> <volume> Volume 3, Number 4, </volume> <month> Fall </month> <year> 1995. </year>
Reference-contexts: to three years from now; (iii) we present results for both uniprocessor and multiprocessor configurations, comparing their relative performance; and finally (iv) we present detailed performance data of specific operating system services (e.g. file I/0, process creation, page fault handling, etc.) The technology used to gather these results is SimOS <ref> [11] </ref>, a comprehensive machine and operating system simulation environment. SimOS simulates the hardware of modern uniprocessor and multiprocessor computer systems in enough detail to boot and run a commercial operating system. SimOS also contains features which enable non-intrusive yet highly detailed study of kernel execution. <p> Finally, Section 6 discusses related work and Section 7 concludes. 2 Experimental Environment In this section, we present the SimOS environment, describe our data collection methodology, and present the workloads used throughout this study. 2.1 The SimOS Simulation Environment SimOS <ref> [11] </ref> is a machine simulation environment that simulates the hardware of uniprocessor and multiprocessor computer systems in enough detail to boot, run, and study a commercial operating system.
Reference: [12] <author> Mendel Rosenblum and John Ousterhout, </author> <title> The Design and Implementation of a Log-structured File System, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 25-52, </pages> <month> Feb. </month> <year> 1992 </year>
Reference-contexts: These solutions range from special-casing the /tmp directory and making it a memory-based file system to adding write-ahead logging to file systems <ref> [12] </ref>. 1 OS events Prog-Dev Database Eng Duration 8.5 secs 7.6 secs 4.1 secs Process creations 11 &lt; 1 &lt; 1 Context switches 92 847 34 Interrupts 162 753 133 System calls 1133 4632 18 TLB refills 87 x 10 3 425 x 10 3 486 x 10 3 VM faults
Reference: [13] <author> Josep Torrellas, Anoop Gupta, and John L. Hennessy, </author> <title> Characterizing the Cache Performance and Synchronization Behavior of a Multiprocessor Operating System, </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 162-174, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: The SimOS annotation mechanism allows non-intrusive system observation at levels of detail not previously obtainable. Our results confirm numerous studies [2][3][8]<ref> [13] </ref>: memory system performance, block copy, and instruction miss stall time are important components of operating system performance. Like Maynard [8] and Torrellas [13], who used hardware-based traces, we were able to examine large applications and confirm their results.
Reference: [14] <author> Josep Torrellas, Chun Xia, and Russel Daigle, </author> <title> Optimizing Instruction Cache Performance for Operating System Intensive Workloads, </title> <booktitle> In Proceedings of the First IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pp. 360-369, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: Any improvements will necessarily have to come from improved memory systems. However, operating system designers can address KERN-self cache misses. For example, recent work has shown how code re-organization can reduce these instruction misses <ref> [14] </ref>. Reducing the KERN-other and the USER-other misses is more problematic. Most of the USER-other misses are due to interference between the user and kernel rather than interference between two user processes. In fact, these two miss types are quite complementary.
References-found: 14

