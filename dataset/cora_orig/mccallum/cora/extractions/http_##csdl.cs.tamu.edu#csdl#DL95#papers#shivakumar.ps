URL: http://csdl.cs.tamu.edu/csdl/DL95/papers/shivakumar.ps
Refering-URL: http://csdl.cs.tamu.edu/csdl/DL95/contents.html
Root-URL: http://www.cs.tamu.edu
Email: fshiva, hectorg@cs.stanford.edu  
Title: SCAM: A Copy Detection Mechanism for Digital Documents  
Author: Narayanan Shivakumar, Hector Garcia-Molina 
Keyword: Copy Detection, Plagiarism, Registration Server, Databases.  
Address: Stanford, CA 94305-2140  
Affiliation: Department of Computer Science Stanford University  
Abstract: Copy detection in Digital Libraries may provide the necessary guarantees for publishers and newsfeed services to offer valuable on-line data. We consider the case for a registration server that maintains registered documents against which new documents can be checked for overlap. In this paper we present a new scheme for detecting copies based on comparing the word frequency occurrences of the new document against those of registered documents. We also report on an experimental comparison between our proposed scheme and COPS [6], a detection scheme based on sentence overlap. The tests involve over a million comparisons of netnews articles and show that in general the new scheme performs better in detecting documents that have partial overlap. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Aho and M. Corasick. </author> <title> Efficient string matching: An aid to bibliographic search. </title> <journal> Communications of the ACM, </journal> <volume> 18(6), </volume> <year> 1975. </year>
Reference-contexts: Several schemes have been proposed to enhance IR schemes, such as use of signature files [8], lexical analysis <ref> [1] </ref>, stoplists [13, 9], stemming algorithms [12, 15], thesaurus [21] and ranking algorithms [19]. Since our approach is based on IR, such schemes are orthogonal to our model, and one or more of these schemes could be used to enhance our document comparison mechanism.
Reference: 2. <author> C. Anderson. Robocops: </author> <title> Stewart and Feder's mechanized misconduct search. </title> <journal> Nature, </journal> <volume> 350(6318) </volume> <pages> 454-455, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Signature schemes have two weaknesses: (a) the signatures often can be removed automatically, leading to untraceable documents, and (b) they are not useful for detecting partial overlap. For these reasons we advocate registration based copy detection schemes. With these schemes original documents are registered and stored in a repository <ref> [17, 2] </ref>. Subsequent documents that are produced are compared against the pre-registered documents for partial or complete overlap.
Reference: 3. <author> D. Boneh and J. Shaw. </author> <title> Collusion-secure fingerprinting for digital data. </title> <type> Technical Report 468, </type> <institution> Computer Science Department, Princeton University, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: In signature based schemes, a signature is added to the document, and this signature can be used to trace the origins of the document. For example, one popular approach is to incorporate watermarks such as word spacings and checksum into documents <ref> [5, 4, 22, 7, 3] </ref>. Signature schemes have two weaknesses: (a) the signatures often can be removed automatically, leading to untraceable documents, and (b) they are not useful for detecting partial overlap. For these reasons we advocate registration based copy detection schemes.
Reference: 4. <author> J. Brassil, S. Low, N. Maxemchuk, and L.O'Gorman. </author> <title> Document marking and identification using both line and word shifting. </title> <type> Technical report, </type> <institution> AT&T Bell Labratories, </institution> <year> 1994. </year> <note> May be obtained from ftp://ftp.research.att.com/dist/brassil/docmark2.ps. </note>
Reference-contexts: In signature based schemes, a signature is added to the document, and this signature can be used to trace the origins of the document. For example, one popular approach is to incorporate watermarks such as word spacings and checksum into documents <ref> [5, 4, 22, 7, 3] </ref>. Signature schemes have two weaknesses: (a) the signatures often can be removed automatically, leading to untraceable documents, and (b) they are not useful for detecting partial overlap. For these reasons we advocate registration based copy detection schemes.
Reference: 5. <author> J. Brassil, S. Low, N. Maxemchuk, and L.O'Gorman. </author> <title> Electronic marking and identification techniques to discourage document copying. </title> <type> Technical report, </type> <institution> AT&T Bell Labratories, </institution> <year> 1994. </year>
Reference-contexts: In signature based schemes, a signature is added to the document, and this signature can be used to trace the origins of the document. For example, one popular approach is to incorporate watermarks such as word spacings and checksum into documents <ref> [5, 4, 22, 7, 3] </ref>. Signature schemes have two weaknesses: (a) the signatures often can be removed automatically, leading to untraceable documents, and (b) they are not useful for detecting partial overlap. For these reasons we advocate registration based copy detection schemes.
Reference: 6. <author> S. Brin, J. Davis, and H. Garcia-Molina. </author> <title> Copy detection mechanisms for digital documents. </title> <booktitle> In Proceedings of the ACM SIGMOD Annual Conference, </booktitle> <address> San Francisco, CA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: We believe that prevention techniques may be cumbersome, may get in the way of the honest user <ref> [6] </ref>, and may make it difficult to share information. Furthermore, prevention schemes are not always bulletproof since documents may be recorded by using software emulators [6]. The other approach is not to place restrictions on the distribution of documents, but to detect illegal copies. <p> We believe that prevention techniques may be cumbersome, may get in the way of the honest user <ref> [6] </ref>, and may make it difficult to share information. Furthermore, prevention schemes are not always bulletproof since documents may be recorded by using software emulators [6]. The other approach is not to place restrictions on the distribution of documents, but to detect illegal copies. Detection fl This material is based upon work supported by the National Science Foundation under Cooperative Agreement IRI-9411306. <p> The repository of registered documents can be compacted in a variety of ways <ref> [6] </ref> and periodically distributed to mail gateways and bulletin boards so that checks can be done locally. Another application of registration copy detection is for filtering duplicate messages often found in newsgroups and mailing lists [25]. There are a number of ways to detect duplication with registered documents. In COPS [6], <p> <ref> [6] </ref> and periodically distributed to mail gateways and bulletin boards so that checks can be done locally. Another application of registration copy detection is for filtering duplicate messages often found in newsgroups and mailing lists [25]. There are a number of ways to detect duplication with registered documents. In COPS [6], registered documents are broken up into sentences or sequences of sentences, and are stored in the registration server. Subsequent query documents are broken up in the same way and are compared against the registered documents. <p> However, this requires substantial manual work, and for this reason we believe registration based copy detection is superior to signature based schemes. Although COPS has been shown to work well <ref> [6] </ref>, it does have some problems. In particular, it has some difficulties in detecting sentences. Often equations, figures, and abbreviations confuse it. Also, checking for overlap involves many random probes into the registration database, and is expensive. For these reasons, we have explored alternative schemes. <p> However, before we can use word chunking, we need to determine a good scheme for comparing documents. Recall that for sentence chunking, comparison was straightforward: if X of the Y sentences in document D 1 appear in D 2 then the overlap is X fl 100=Y <ref> [6] </ref>. Unfortunately, this simple scheme breaks down for words: the fact that D 2 has many of the words of D 1 does not necessarily mean they overlap. In the next section, we propose one scheme based on relative frequency of words that we have empirically found to be effective. <p> COPS has problems with small sentences and cannot handle in its current implementation documents that have multiple copies of the same sentence. Also COPS has the classic sentence boundary problem since it is hard to detect when a sentence ends <ref> [6] </ref>. While there are some fairly sophisticated mechanisms [16] for detecting sentence boundaries, SCAM has the clear advantage since it uses word chunking. COPS sometimes gets confused due to :signature" files at the bottom of news messages while SCAM does not (again due to word chunking). <p> We have presented a new similarity measure based solely on word frequencies in documents. We presented results of some initial comparisons of SCAM (Stanford Copy Analysis Mechanism), our current prototype, against COPS <ref> [6] </ref>, another approach based on sentence comparisons. The results demonstrate the general advantages of word chunking for detecting document overlap, while also highlighting that using sentence chunking reduces the number of false positives. Our comparisons have used relatively small netnews articles due to easy availability of several related document sets.
Reference: 7. <author> A. Choudhury, N. Maxemchuk, S. Paul, and H. Schulzrinne. </author> <title> Copyright protection for electronic publishing over computer networks. </title> <type> Technical report, </type> <institution> AT&T Bell Labratories, </institution> <year> 1994. </year> <note> Submitted to IEEE Network Magazine June 1994. </note>
Reference-contexts: In signature based schemes, a signature is added to the document, and this signature can be used to trace the origins of the document. For example, one popular approach is to incorporate watermarks such as word spacings and checksum into documents <ref> [5, 4, 22, 7, 3] </ref>. Signature schemes have two weaknesses: (a) the signatures often can be removed automatically, leading to untraceable documents, and (b) they are not useful for detecting partial overlap. For these reasons we advocate registration based copy detection schemes.
Reference: 8. <author> C. Faloutsos. </author> <title> Description and performance analysis of signature file methods. </title> <journal> ACM Transactions on Office Information Systems, </journal> <volume> 2(4), </volume> <year> 1984. </year>
Reference-contexts: Several schemes have been proposed to enhance IR schemes, such as use of signature files <ref> [8] </ref>, lexical analysis [1], stoplists [13, 9], stemming algorithms [12, 15], thesaurus [21] and ranking algorithms [19]. Since our approach is based on IR, such schemes are orthogonal to our model, and one or more of these schemes could be used to enhance our document comparison mechanism.
Reference: 9. <author> W. Francis and H. Kucera. </author> <title> Frequency analysis of English Usage. </title> <publisher> Houghton Mifflin, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: Several schemes have been proposed to enhance IR schemes, such as use of signature files [8], lexical analysis [1], stoplists <ref> [13, 9] </ref>, stemming algorithms [12, 15], thesaurus [21] and ranking algorithms [19]. Since our approach is based on IR, such schemes are orthogonal to our model, and one or more of these schemes could be used to enhance our document comparison mechanism.
Reference: 10. <author> G. N. Griswold. </author> <title> A method for protecting copyright on networks. </title> <booktitle> In Joint Harvard MIT Workshop on Technology Strategies for Protecting Intellectual Propertyin the Networked Multimedia Environment, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Copy prevention schemes include physical isolation of the information (e.g., by placing it on a stand-alone CD-ROM system), use of special-purpose hardware for authorization [18], and active documents that are essentially documents encapsulated by programs <ref> [10] </ref>. We believe that prevention techniques may be cumbersome, may get in the way of the honest user [6], and may make it difficult to share information. Furthermore, prevention schemes are not always bulletproof since documents may be recorded by using software emulators [6].
Reference: 11. <author> W. Li. </author> <title> Random texts exhibit zipf's law-like word frequency distribution. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(6):1842, </volume> <year> 1992. </year>
Reference-contexts: However, we see one advantage for small chunking units. A small chunking unit increases locality. That is most documents will have a relatively small working set of words rather than sentences. Consider the frequency distribution of N words to follow Zipf's Law <ref> [26, 23, 11] </ref>.
Reference: 12. <author> J.B. Lovins. </author> <title> Development of a stemming algorithm. </title> <journal> Mechanical Translation and Computational Linguistics, </journal> <pages> 11(1-2), </pages> <year> 1968. </year>
Reference-contexts: Several schemes have been proposed to enhance IR schemes, such as use of signature files [8], lexical analysis [1], stoplists [13, 9], stemming algorithms <ref> [12, 15] </ref>, thesaurus [21] and ranking algorithms [19]. Since our approach is based on IR, such schemes are orthogonal to our model, and one or more of these schemes could be used to enhance our document comparison mechanism.
Reference: 13. <author> H.P. Luhn. </author> <title> A statistical approach to mechanized encoding and searching of literary information. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 1(4), </volume> <year> 1957. </year>
Reference-contexts: Several schemes have been proposed to enhance IR schemes, such as use of signature files [8], lexical analysis [1], stoplists <ref> [13, 9] </ref>, stemming algorithms [12, 15], thesaurus [21] and ranking algorithms [19]. Since our approach is based on IR, such schemes are orthogonal to our model, and one or more of these schemes could be used to enhance our document comparison mechanism.
Reference: 14. <author> U. Manber. </author> <title> Finding similar files in a large file system. </title> <booktitle> In USENIX, </booktitle> <pages> pages 1-10, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Subsequent query documents are broken up in the same way and are compared against the registered documents. If a query document shares more than a given threshold of matching sentences (or sequences of sentences) with a registered document, the user is notified. Another scheme is presented in <ref> [14] </ref>, where the problem of finding similar files is addressed. The mechanism works by selecting a few words as anchors and computing checksums of a following window of characters for comparison.
Reference: 15. <author> C. Paice. </author> <title> Another stemmer. </title> <journal> ACM SIGIR Forum, </journal> <volume> 24(3), </volume> <year> 1990. </year>
Reference-contexts: Several schemes have been proposed to enhance IR schemes, such as use of signature files [8], lexical analysis [1], stoplists [13, 9], stemming algorithms <ref> [12, 15] </ref>, thesaurus [21] and ranking algorithms [19]. Since our approach is based on IR, such schemes are orthogonal to our model, and one or more of these schemes could be used to enhance our document comparison mechanism.
Reference: 16. <author> D.D. Palmer and M.A. Hearst. </author> <title> Adaptive sentence boundary disambiguation. </title> <booktitle> In Proceedings of ANLP, </booktitle> <address> Stuttgart, Germany, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: COPS has problems with small sentences and cannot handle in its current implementation documents that have multiple copies of the same sentence. Also COPS has the classic sentence boundary problem since it is hard to detect when a sentence ends [6]. While there are some fairly sophisticated mechanisms <ref> [16] </ref> for detecting sentence boundaries, SCAM has the clear advantage since it uses word chunking. COPS sometimes gets confused due to :signature" files at the bottom of news messages while SCAM does not (again due to word chunking).
Reference: 17. <author> A. Parker and J. O. Hamblen. </author> <title> Computer algorithms for plagiarism detection. </title> <journal> IEEE Trasnactions on Education, </journal> <volume> 32(2) </volume> <pages> 94-99, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Signature schemes have two weaknesses: (a) the signatures often can be removed automatically, leading to untraceable documents, and (b) they are not useful for detecting partial overlap. For these reasons we advocate registration based copy detection schemes. With these schemes original documents are registered and stored in a repository <ref> [17, 2] </ref>. Subsequent documents that are produced are compared against the pre-registered documents for partial or complete overlap.
Reference: 18. <author> G.J. Popek and C.S. Kline. </author> <title> Encryption and secure computer networks. </title> <journal> ACM Computing Surveys, </journal> <volume> 11(4) </volume> <pages> 331-356, </pages> <month> Decem-ber </month> <year> 1979. </year>
Reference-contexts: Most existing techniques that address this problem fall into two categories, those of copy prevention and copy detection. Copy prevention schemes include physical isolation of the information (e.g., by placing it on a stand-alone CD-ROM system), use of special-purpose hardware for authorization <ref> [18] </ref>, and active documents that are essentially documents encapsulated by programs [10]. We believe that prevention techniques may be cumbersome, may get in the way of the honest user [6], and may make it difficult to share information.
Reference: 19. <author> G. Salton and C. Buckley. </author> <booktitle> Term-weighting approaches in authomatic text retrieval. Information Processing and Management, </booktitle> <volume> 24(5), </volume> <year> 1988. </year>
Reference-contexts: Several schemes have been proposed to enhance IR schemes, such as use of signature files [8], lexical analysis [1], stoplists [13, 9], stemming algorithms [12, 15], thesaurus [21] and ranking algorithms <ref> [19] </ref>. Since our approach is based on IR, such schemes are orthogonal to our model, and one or more of these schemes could be used to enhance our document comparison mechanism.
Reference: 20. <author> G. Salton. </author> <title> The state of retrieval system evaluation. </title> <booktitle> Information processing & management., </booktitle> <address> 28(4):441, </address> <year> 1992. </year>
Reference-contexts: Conceptually, we compute a vector that gives the frequency with which each possible word occurs in the new document. Then we compare this vector against similar vectors in the database of registered documents. This is very similar to how Information Retrieval (IR) systems compute document similarities <ref> [20] </ref>, except that we use a new similarity measure that more accurately characterizes copy overlap, while traditional IR systems look for semantic similarity. <p> Let w i refer to the i th chunk in the vocabulary. Let the size of the vocabulary (number of distinct chunks) be N . Inverted Index Storage We propose using an inverted index structure (as in traditional IR systems <ref> [20] </ref>) for storing chunks of the registered documents. An index of the chunks in the vocabulary is constructed and maintained at registration time. Each entry for a chunk points to a set of postings that indicate the documents where the chunk occurs. <p> In the next section, we propose one scheme based on relative frequency of words that we have empirically found to be effective. OVERLAP MEASURES In traditional IR schemes <ref> [20] </ref>, when queries arrive from users, the query is compared" against the documents, and some measure of relevance between the document and the query is obtained. Similarly, we need to establish a metric that measures the overlap between an incoming document and a pre-registered document. <p> Similarly, we need to establish a metric that measures the overlap between an incoming document and a pre-registered document. In this section, we consider a popular model used in IR systems termed the Vector Space Model (VSM) <ref> [20] </ref> that relates documents to queries, and see why it is not directly applicable for copy detection. We then propose the Relative Frequency Model (RFM) that presents a better framework for detecting overlaps. For our discussion, let D refer to a generic document (registered or new). <p> Vector Space Model A popular model in the IR domain <ref> [20] </ref>, is the VSM model. Given a query with its corresponding weights, a dot product of the weighted occurrence vector of the query with a stored document is computed: if the dot product value exceeds a certain threshold, the document is flagged to match the query. <p> Since the overlap of R with S 2 and S 4 is more significant than that with S 1 or S 3 , the overlap values reported are not acceptable. Another popular weighting measure used is the cosine similarity measure <ref> [20] </ref> which defines relevance of document R to query Q to be sim (R; Q) = i=1 ff 2 q i=1 ff 2 i (R)fl i=1 ff 2 i (Q) where ff i is the weight associated with the occurence of the i th chunk.
Reference: 21. <author> Y-C. Wang, J. Vandendorpe, and M. Evens. </author> <title> Relationship thesauri in information retrieval. </title> <journal> J. American Society of Information Science, </journal> <year> 1985. </year>
Reference-contexts: Several schemes have been proposed to enhance IR schemes, such as use of signature files [8], lexical analysis [1], stoplists [13, 9], stemming algorithms [12, 15], thesaurus <ref> [21] </ref> and ranking algorithms [19]. Since our approach is based on IR, such schemes are orthogonal to our model, and one or more of these schemes could be used to enhance our document comparison mechanism.
Reference: 22. <author> D. Wheeler. </author> <title> Computer networks are said to offer new opportunities for plagarists. </title> <booktitle> The Chronicle of Higher Education, </booktitle> <pages> pages 17, 19, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: In signature based schemes, a signature is added to the document, and this signature can be used to trace the origins of the document. For example, one popular approach is to incorporate watermarks such as word spacings and checksum into documents <ref> [5, 4, 22, 7, 3] </ref>. Signature schemes have two weaknesses: (a) the signatures often can be removed automatically, leading to untraceable documents, and (b) they are not useful for detecting partial overlap. For these reasons we advocate registration based copy detection schemes.
Reference: 23. <author> T. Yan and H. Garcia-Molina. </author> <title> Index structures for selective dissemination of information under the boolean model. </title> <journal> IEEE Transactions on Database Systems, </journal> <month> June </month> <year> 1994. </year>
Reference-contexts: However, we see one advantage for small chunking units. A small chunking unit increases locality. That is most documents will have a relatively small working set of words rather than sentences. Consider the frequency distribution of N words to follow Zipf's Law <ref> [26, 23, 11] </ref>. <p> If the words are ranked in non-increasing order of frequencies, then the probability that a word w of rank r occurs is P (w) = 1 P N If we assume a vocabulary of about 1.8 million words <ref> [23] </ref>, about 40,000 (about 2% of 1.8 million) words constitute nearly 75% of the actual occurrences of words thereby increasing the effects of cacheing.
Reference: 24. <author> T. Yan and H. Garcia-Molina. </author> <title> Duplicate detection in information dissemination. </title> <booktitle> In Proceedings of Very Large Databases (VLDB'95) Conference, </booktitle> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: In this case, it is useful to have a human check the detected documents to eliminate false positives. On the other hand, if overlap detection is to be completely automated, as for duplicate removal in netnews articles <ref> [24] </ref>, COPS has the advantage. In Figures 6 and 7, we report tables similar to Figure 4 and 5, comparing our new document comparison measure to the traditional IR cosine measure.
Reference: 25. <author> T. Yan and H. Garcia-Molina. </author> <title> Sift a tool for wide-area information dissemination. </title> <booktitle> In Proceedings of USENIX, </booktitle> <year> 1995. </year>
Reference-contexts: The repository of registered documents can be compacted in a variety of ways [6] and periodically distributed to mail gateways and bulletin boards so that checks can be done locally. Another application of registration copy detection is for filtering duplicate messages often found in newsgroups and mailing lists <ref> [25] </ref>. There are a number of ways to detect duplication with registered documents. In COPS [6], registered documents are broken up into sentences or sequences of sentences, and are stored in the registration server.
Reference: 26. <author> G.K. Zipf. </author> <title> Human Behavior and the Principle of Least Effort. </title> <publisher> Addison-Wesley Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1949. </year>
Reference-contexts: However, we see one advantage for small chunking units. A small chunking unit increases locality. That is most documents will have a relatively small working set of words rather than sentences. Consider the frequency distribution of N words to follow Zipf's Law <ref> [26, 23, 11] </ref>.
References-found: 26

