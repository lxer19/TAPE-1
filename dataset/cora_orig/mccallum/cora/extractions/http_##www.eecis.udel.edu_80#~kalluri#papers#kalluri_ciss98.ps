URL: http://www.eecis.udel.edu:80/~kalluri/papers/kalluri_ciss98.ps
Refering-URL: http://www.eecis.udel.edu:80/~kalluri/resume.html
Root-URL: http://www.cis.udel.edu
Email: kalluri@ee.udel.edu  
Title: A General Class of Nonlinear Normalized LMS-type Adaptive Algorithms  
Author: Sudhakar Kalluri and Gonzalo R. Arce 
Address: 19716  
Affiliation: Department of Electrical and Computer Engineering University of Delaware, Newark, DE  
Abstract: The Normalized Least Mean Square (NLMS) algorithm is an important variant of the classical LMS algorithm for adaptive linear FIR filtering. It provides an automatic choice for the LMS step-size parameter which affects the stability, convergence speed and steady-state performance of the algorithm. In this paper, we generalize the NLMS algorithm by deriving a class of Nonlinear Normalized LMS-type (NLMS-type) Algorithms that are applicable to a wide variety of nonlinear filters. These algorithms are developed by choosing an optimal time-varying step-size in the class of LMS-type adaptive nonlinear filtering algorithms. An auxiliary fixed step-size can be introduced in the NLMS-type algorithm. However, unlike in the LMS-type algorithm, the bounds on this new step-size for algorithm stability are independent of the input signal statistics. Computer simulations demonstrate that these NLMS-type algorithms have a potentially faster convergence than their LMS-type counterparts. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Haykin, </author> <title> Adaptive Filter Theory. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction The Least Mean Square (LMS) algorithm <ref> [1] </ref> is widely used for adapting the weights of a linear FIR filter that minimizes the mean square error (MSE) between the filter output and a desired signal. <p> In an environment of unknown or changing signal statistics, the LMS algorithm <ref> [1] </ref> attempts to minimize the MSE by continually updating the weights as w (n + 1) = w (n) e (n) x (n); (1) fl This work was supported by the National Science Foundation under Grant MIP-9530923. where &gt; 0 is the so-called step-size of the update. <p> Further, its implementation requires the choice of an appropriate step-size which affects the stability, steady-state MSE and convergence speed of the algorithm. The stability region for mean-square convergence of the LMS algorithm is given by 0 &lt; &lt; (2 = trace (R)) <ref> [1, 2] </ref>, where R 4 = Efx (n)x T (n)g is the autocorrelation matrix of the input vector x (n). When the input signal statistics are unknown or time-varying, it is difficult to choose a step-size that is guaranteed to lie within the stability region. <p> When the input signal statistics are unknown or time-varying, it is difficult to choose a step-size that is guaranteed to lie within the stability region. The so-called Normalized LMS (NLMS) algorithm <ref> [1] </ref> addresses the problem of step-size design in (1) by choos ing a time-varying step-size (n) that minimizes the next-step MSE, J n+1 4 = Efe 2 (n + 1)g. <p> The theoretical bounds on the stability of the NLMS algorithm are given by 0 &lt; ~ &lt; 2 <ref> [1] </ref>. Unlike the LMS step-size of (1), the auxiliary step-size ~ is dimensionless and the stability region for ~ is independent of the signal statistics. This allows for an easier step-size design with guaranteed stability of the algorithm.
Reference: [2] <author> V. Solo and X. Kong, </author> <title> Adaptive Signal Processing Algorithms: Stability and Performance. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: Further, its implementation requires the choice of an appropriate step-size which affects the stability, steady-state MSE and convergence speed of the algorithm. The stability region for mean-square convergence of the LMS algorithm is given by 0 &lt; &lt; (2 = trace (R)) <ref> [1, 2] </ref>, where R 4 = Efx (n)x T (n)g is the autocorrelation matrix of the input vector x (n). When the input signal statistics are unknown or time-varying, it is difficult to choose a step-size that is guaranteed to lie within the stability region.
Reference: [3] <author> D. T. M. Slock, </author> <title> "On the convergence behavior of the LMS and the normalized LMS algorithms," </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> vol. 41, </volume> <pages> pp. 2811-2825, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: This allows for an easier step-size design with guaranteed stability of the algorithm. Further, the NLMS algorithm is known to converge much faster than the LMS algorithm <ref> [3, 4] </ref>. We can also interpret (2) as a modified LMS algorithm, where the update term in (1) is divided (normalized) by the squared-norm kx (n)k 2 , to ensure stability under large excursions of the input vector x (n).
Reference: [4] <author> M. Rupp, </author> <title> "The behavior of LMS and NLMS algorithms in the presence of spherically invariant processes," </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> vol. 41, </volume> <pages> pp. 1149-1160, </pages> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: This allows for an easier step-size design with guaranteed stability of the algorithm. Further, the NLMS algorithm is known to converge much faster than the LMS algorithm <ref> [3, 4] </ref>. We can also interpret (2) as a modified LMS algorithm, where the update term in (1) is divided (normalized) by the squared-norm kx (n)k 2 , to ensure stability under large excursions of the input vector x (n).
Reference: [5] <author> S. Kalluri and G. R. Arce, </author> <title> "A general class of nonlinear normalized LMS-type adaptive filtering algorithms," </title> <journal> IEEE Transactions on Signal Processing. </journal> <note> In preparation. </note>
Reference-contexts: We see from (27) that our remaining task is to evaluate F 0 j (0). The required expression is derived in <ref> [5] </ref>, and is given by F 0 k=1 @ 2 y (n) @y (n) @w j we omit the derivation here due to lack of space. We can now substitute (29) and (28) into (27) and obtain an expression for the optimal step-size o (n).
Reference: [6] <author> S. Kalluri and G. R. Arce, </author> <title> "Adaptive weighted myriad filter algorithms for robust signal processing in ff-stable noise environments," </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> vol. 46, </volume> <pages> pp. 322-334, </pages> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: For the nonlinear high-pass filter, we chose the so-called Weighted Myriad Filter <ref> [6, 7] </ref>, which has recently been proposed for robust signal processing in impulsive noise environments.
Reference: [7] <author> S. Kalluri and G. R. Arce, </author> <title> "Robust frequency-selective filtering using generalized weighted myriad filters admitting real-valued weights," </title> <journal> IEEE Transactions on Signal Processing. </journal> <note> In preparation. </note>
Reference-contexts: For the nonlinear high-pass filter, we chose the so-called Weighted Myriad Filter <ref> [6, 7] </ref>, which has recently been proposed for robust signal processing in impulsive noise environments.
Reference: [8] <author> C. L. Nikias and M. Shao, </author> <title> Signal Processing with Alpha-Stable Distributions and Applications. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1995. </year>
Reference-contexts: The additive noise process v (n) was chosen to have a zero-mean symmetric ff-stable distribution <ref> [8] </ref> with characteristic exponent ff = 1:6 and dispersion fl = 0:02. Impulsive noise is well-modeled by the heavy-tailed class of ff-stable distributions, which includes the Gaussian distribution as the special case when ff = 2.
References-found: 8

