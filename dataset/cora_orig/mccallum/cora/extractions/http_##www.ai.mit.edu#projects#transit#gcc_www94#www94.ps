URL: http://www.ai.mit.edu/projects/transit/gcc_www94/www94.ps
Refering-URL: http://www.ai.mit.edu/projects/iiip/colab/gcc-abstract.html
Root-URL: 
Title: Global Cooperative Computing  
Author: Andre DeHon Jeremy Brown Ian Eslick Jake Harris Lara Karbiner Thomas F. Knight, Jr. 
Note: Acknowledgments: This research is supported in part by the Advanced Research Projects Agency under contract N00014-91-J-1698.  
Date: September 15, 1994  
Abstract: Pervasive network connectivity changes our computational landscape. Each computer now resides within a global computer web rather than existing in isolation. Until recently, use of the network has been manual and primitive, requiring sophistication and effort on the part of the user to exploit the resource. The World-Wide Web transcends this traditional model, providing native support for the global computer web. To date, however, the WWW has been used primarily to publish information. Global cooperative computing represents the next step in exploiting wide-scale network connectivity wherein distributed users and computers globally cooperate on common computing problems. In this paper we focus on how the World-Wide Web can be used to rapidly advance the state of the art in computation and change the way software is developed, maintained, debugged, optimized, and used. The cooperative computing model we advocate supports systems which continually adapt to user needs and global information. This adaptability delivers considerable performance and convenience advantages to both developers and end users. 
Abstract-found: 1
Intro-found: 1
Reference: [BHK + 94] <author> Jeremy Brown, Jake Harris, Lara Karbiner, Massimiliano Poletto, Andr'e DeHon, and Jr. Thomas F. Knight. HyperCode. </author> <booktitle> In The Second International World-Wide Web Conference 1994, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: This model, however, has made less progress towards providing a global computation web which brings direct benefits to individual computational processes. 4 HyperCode We use HyperCode <ref> [BHK + 94] </ref> as a springboard to facilitate migration from a global web of information to a global web of computation. At the entry level, HyperCode brings world-wide, distributed, HyperText technology to program source code.
Reference: [FS93] <author> Jay Fenlason and Richard Stallman. </author> <title> GNU gprof. Free Software Foundation, </title> <publisher> Inc., </publisher> <address> 675 Mas-sachusetts Avenue, Cambridge, MA 02139, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Further, by normalizing aggregate crash incidents for a program against aggregate runtime or invocations, one can determine crude measures of robustness. 3. Profiling Profiling information, such as prof or gprof [GKM82] <ref> [FS93] </ref>, can be collected and reported, providing a more fine-grained view on program usage and bottlenecks. Such profiling information is useful in improving code. Aggregated across wide-spread use, developers or performance tuners get a fairly comprehensive view of deployed performance.
Reference: [GK92] <author> Torbjorn Granlund and Richard Kenner. </author> <title> Eliminating Branches using a Superoptimizer and the GNU C Compiler. </title> <booktitle> In Proceedings of the ACM SIGPLAN 1992 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 341-352. </pages> <booktitle> ACM SIGPLAN, </booktitle> <publisher> ACM Press, </publisher> <month> June </month> <year> 1992. </year> <journal> SIGPLAN Notices, </journal> <volume> Volume 27, Number 7. </volume>
Reference-contexts: Highly optimized versions of heavily used subroutines are especially good candidates. Work on superoptimization has demonstrated the opportunity for gains on short code segments [Mas87] <ref> [GK92] </ref>. With feedback on code usage, computers can automatically determine the most benefitial sequences to optimize. A large user base collectively searching a huge space such as this can reasonably attack far larger problems than a single, isolated computer. In a similar manner, many optimization problems (e.g.
Reference: [GKM82] <author> Susan L. Graham, Peter B. Kessler, and Marshall K. McKusick. </author> <title> gprof: a Call Graph Execution Profiler. </title> <booktitle> In Proceedings of the SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <pages> pages 120-126. </pages> <booktitle> ACM SIGPLAN, ACM, </booktitle> <month> June </month> <year> 1982. </year> <journal> SIGPLAN Notices, </journal> <volume> Volume 17, Number 6. </volume>
Reference-contexts: Further, by normalizing aggregate crash incidents for a program against aggregate runtime or invocations, one can determine crude measures of robustness. 3. Profiling Profiling information, such as prof or gprof <ref> [GKM82] </ref> [FS93], can be collected and reported, providing a more fine-grained view on program usage and bottlenecks. Such profiling information is useful in improving code. Aggregated across wide-spread use, developers or performance tuners get a fairly comprehensive view of deployed performance.
Reference: [HKM + 88] <author> J. H. Howard, M. J. Kazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, R. N. Sidebotham, and M. J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6 </volume> <pages> 55-81, </pages> <year> 1988. </year>
Reference-contexts: The user must be very aware of specific features of the network and the individual computers in it to get any benefits. Predominant examples of network use include file transfer, remote login, and electronic mail. Remote file-systems - e.g. NFS, AFS <ref> [HKM + 88] </ref>, printers, and job servers provided slightly more automatic use of the network in local-area domains.
Reference: [Lam84] <author> Butler W. Lampson. </author> <title> Hints for Computer System Design. </title> <journal> IEEE Software, </journal> <volume> 1(1) </volume> <pages> 11-28, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Gross Dataset Statistics Datasets can be monitored to determine the real common cases seen in deployed applications. Again, developers and compilers can produce more highly optimized code when they know the cases of typical use <ref> [Lam84] </ref>. Further, gross differences in dataset statistics may suggest the need for different versions of code optimized and specialized to different users and usages.
Reference: [Leu94] <author> Kristin Leutwyler. </author> <title> Superhack. </title> <journal> Scientific American, </journal> <volume> 271(1) </volume> <pages> 17-18, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Formally, often NP -hard) * Verifying a solution is much easier (e.g. Formally, often P ) We further assume that individuals are willing to dedicate their "idle" computer time to solving problems which benefit them. Compare, for example, the recent collaborative effort to factor RSA-129 <ref> [Leu94] </ref>. In these cases, the global computer can co-opt volunteered resources to solve hard problems for everyone's benefit. The fact that solutions are verifiable in reasonable time allow individual computers to double-check the solutions they get from others to assure that the proposed solutions are, in fact, correct.
Reference: [Mas87] <author> Henry Massalin. </author> <title> Superoptimizer A Look at the Smallest Program. </title> <booktitle> In Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 122-126. </pages> <publisher> ACM, IEEE Computer Society Press, </publisher> <month> October </month> <year> 1987. </year>
Reference-contexts: Highly optimized versions of heavily used subroutines are especially good candidates. Work on superoptimization has demonstrated the opportunity for gains on short code segments <ref> [Mas87] </ref> [GK92]. With feedback on code usage, computers can automatically determine the most benefitial sequences to optimize. A large user base collectively searching a huge space such as this can reasonably attack far larger problems than a single, isolated computer. In a similar manner, many optimization problems (e.g.

References-found: 8

