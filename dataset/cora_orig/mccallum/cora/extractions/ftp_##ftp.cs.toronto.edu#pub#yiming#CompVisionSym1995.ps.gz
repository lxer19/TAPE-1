URL: ftp://ftp.cs.toronto.edu/pub/yiming/CompVisionSym1995.ps.gz
Refering-URL: http://www.cs.toronto.edu/~yiming/Research.html
Root-URL: http://www.cs.toronto.edu
Title: Where to Look Next in 3D Object Search  
Author: Yiming Ye John K. Tsotsos 
Address: Toronto, Ontario, Canada M5S 1A4  
Affiliation: Department of Computer Science University of Toronto  
Abstract: The task of sensor planning for object search is formulated and a mechanism for "where to look next" for this task is presented. The searcher is assumed to be a mobile platform equipped with an active camera and a method of calculating depth, like stereo or a laser range finder. The formulation casts sensor planning as an optimization problem: the goal is to maximize the probability of detecting the target object with minimal cost. The search space is thus characterized by the probability distribution of the presence of the target. The control of the sensing parameters depends on the current state of the search space and the detecting ability of the recognition algorithm. In order to represent the environment and to efficiently determine the sensing parameters over time, a concept called the sensed sphere is proposed and its construction, using a laser range finder, is derived. The result of each sensing operation is used to update the status of the search space. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. </author> <title> Bajcsy. Active perception vs. passive perception. </title> <booktitle> In Third IEEE Workshop on Vision, </booktitle> <pages> pages 55-59, </pages> <address> Bellaire, </address> <year> 1985. </year>
Reference: [2] <author> Connell. </author> <title> An Artificial Creature. </title> <type> Ph.D thesis, </type> <institution> AI Lab, MIT, </institution> <year> 1989. </year>
Reference-contexts: Although sensor planning for object search is very important if a robot wants to interact intelligently and effectively with its environment, there is little research within the computer vision community ([9], [12], [3], [6], [8]). Connell <ref> [2] </ref> constructed a robot that roams an area searching for and collecting soda cans. The planning is very simple since the robot just follows the walls of the room and the sensor only searches the area immediately in front of the robot.
Reference: [3] <author> T. D. Garvey. </author> <title> Perceptual strategies for purposive vision. </title> <type> Technical Report Note 117, </type> <institution> SRI International, </institution> <year> 1976. </year>
Reference-contexts: Although sensor planning for object search is very important if a robot wants to interact intelligently and effectively with its environment, there is little research within the computer vision community ([9], [12], <ref> [3] </ref>, [6], [8]). Connell [2] constructed a robot that roams an area searching for and collecting soda cans. The planning is very simple since the robot just follows the walls of the room and the sensor only searches the area immediately in front of the robot. <p> Probability of presence is used in their system, but the detection ability of the sensor is not considered and the purpose of the sensor planning is mainly verification instead of searching. The indirect search mechanism proposed by Garvey <ref> [3] </ref> is to first direct the sensor to search for an "intermediate" object that commonly participates in a spatial relationship with the target and then direct the sensor to examine the restricted region specified by this relationship.
Reference: [4] <author> P. Jasiobedzki etc.. </author> <title> Laser Eye anew 3D sensor for active vision. In Intelligent Robotics and Computer Vision: Sensor Fusion VI. </title> <booktitle> Proc of SPIE. </booktitle> <volume> vol. </volume> <year> 2059, </year> <pages> pages 316-321, </pages> <address> Boston, </address> <year> 1993. </year>
Reference-contexts: The searcher model is based on the ARK robot, which is a mobile platform equipped with a special sensor: the Laser Eye <ref> [4] </ref>. The Laser Eye is mounted on a robotic head with pan and tilt capabilities. It consists of a camera with controllable focal length (zoom), a laser range-finder and a mirror. The mirror is used to ensure collinearity of effective optical axes of the camera lens and the range finder.
Reference: [5] <author> B. O. Koopman. </author> <title> Search and Screen: general principles with historical applications. </title> <publisher> Pergaman Press, </publisher> <address> Elmsford, N.Y, </address> <year> 1980. </year>
Reference-contexts: It is interesting to note that the operational research community has done a lot of research on optimal search <ref> [5] </ref>. Their purpose is to determine how to allocate effort to search for a target, such as a lost submarine in the ocean or an oil field within a certain region.
Reference: [6] <author> J. Maver and R. </author> <title> Bajcsy. How to decide from the first view where to look next. </title> <booktitle> In Proceedings of the DARPA Image Understanding Workshop, </booktitle> <year> 1990. </year>
Reference-contexts: Although sensor planning for object search is very important if a robot wants to interact intelligently and effectively with its environment, there is little research within the computer vision community ([9], [12], [3], <ref> [6] </ref>, [8]). Connell [2] constructed a robot that roams an area searching for and collecting soda cans. The planning is very simple since the robot just follows the walls of the room and the sensor only searches the area immediately in front of the robot.
Reference: [7] <author> D. Reece and S. Shafer. </author> <title> Using active vision to simplify perception for robot driving. </title> <type> Technical Report CMU-CS-91-199, </type> <institution> Comp. Sci., Carnegie Mel-lon, </institution> <year> 1992. </year>
Reference: [8] <author> R.D. Rimey and C.M. Brown. </author> <title> Where to look next using a bayes net: incorporating geometric relations. </title> <booktitle> In Second European Conference on Computer Vision, </booktitle> <pages> pages 542-550, </pages> <address> Italy, </address> <year> 1992. </year>
Reference-contexts: Although sensor planning for object search is very important if a robot wants to interact intelligently and effectively with its environment, there is little research within the computer vision community ([9], [12], [3], [6], <ref> [8] </ref>). Connell [2] constructed a robot that roams an area searching for and collecting soda cans. The planning is very simple since the robot just follows the walls of the room and the sensor only searches the area immediately in front of the robot. <p> This may not be very efficient since the likely presence of the target is not considered when the robot is roaming. Rimey and Brown <ref> [8] </ref> used a composite Bayes net and utility decision rule to plan the sensor action in their task-oriented system TEA. The sensor is directed to the center of mass of the expected area for a certain object based on the belief value of the net.
Reference: [9] <author> J.K. Tsotsos. </author> <title> 3D Search Strategy. Internal ARK Working Paper, </title> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1992. </year>
Reference-contexts: The environment can thus be represented by the union of these solid angles. This representation is called the sensed sphere <ref> [9] </ref>. We can use a laser range finder to construct the sensed sphere. First, we need to tessellate the surface of the unit sphere centered at the camera center into a set of surface patches.
Reference: [10] <author> J.K. Tsotsos. </author> <title> Active verses passive perception, which is more efficient? IJCV, </title> <type> 7(2), </type> <year> 1992. </year>
Reference-contexts: It is clear that the probability of detecting the target by this allocation is: P [F] = P (f 1 ) + : : : + f i=1 The total cost for applying this allocation is (fol lowing <ref> [10] </ref>): T [F] = i=1 Suppose K is the total time that can be allowed in the search, then the task of sensor planning for object search can be defined as finding an allocation F ae O , which satisfies T (F) K and maximizes P [F].
Reference: [11] <author> D. Wilkes and J.K. Tsotsos. </author> <title> Active object recognition. </title> <booktitle> In CVPR, </booktitle> <pages> pages 136-141, </pages> <address> USA, </address> <year> 1992. </year>
Reference-contexts: The search region is shown in Fig. 2 (b). Only two angle sizes are needed to examine the search region with respect to the first robot position. They are 4 fi and 0:376 fi 0:376. Their effective ranges are <ref> [11; 27] </ref> and [27; 66] respectively. We assume the outside prob ability is 0:5 and the distribution within the room is uniform at the beginning. Fig. 3 (b) shows that the actions are selected by our algorithm to only examine the unoccluded region.
Reference: [12] <author> L. Wixson. </author> <title> Gaze Selection for Visual Search. </title> <type> Ph.D thesis, </type> <institution> Comp. Sci. Dept., Univ. of Rochester, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Although sensor planning for object search is very important if a robot wants to interact intelligently and effectively with its environment, there is little research within the computer vision community ([9], <ref> [12] </ref>, [3], [6], [8]). Connell [2] constructed a robot that roams an area searching for and collecting soda cans. The planning is very simple since the robot just follows the walls of the room and the sensor only searches the area immediately in front of the robot. <p> The indirect search mechanism proposed by Garvey [3] is to first direct the sensor to search for an "intermediate" object that commonly participates in a spatial relationship with the target and then direct the sensor to examine the restricted region specified by this relationship. Wixson <ref> [12] </ref> presented a mathematical model of search efficiency and predicted that indirect search can improve efficiency in many situations. <p> We do not have space to address the "where to move next" problem in this paper. But it is interesting to note that Wixson <ref> [12] </ref> has done 2D simulation experi ments and that his results are actually consistent with our results.
Reference: [13] <author> Y. Ye and J. Tsotsos. </author> <title> Sensor Planning for Object Search. </title> <type> Technical Report RBCV-TR-94-47, </type> <institution> Comp. Sci. Dept., Univ. of Toronto, </institution> <year> 1994. </year>
Reference-contexts: In the rest of the paper, we assume the search region is an office-like environment, and we tessellate the space into little cubes of the same size. Usually the size of the cube is determined by the size of the environment and the size of the target <ref> [13] </ref>. An operation f = f (x c ; y c ; z c ; p; t; w; h; a) is an action of the searcher within the region . Where a is the recognition algorithm used to detect the target. <p> In general <ref> [13] </ref>, b (c i ; f ) is determined by various factors, such as intensity, occlusion, and orientation etc. <p> limited space, in this paper we only address the "where to look next " problem: how to select w; h; p; t; a of f so as to maximize E (f ) for a fixed camera position (For the discussion of the "where to move next" problem, please refer to <ref> [13] </ref>). 3 Detection Function We briefly discuss the detection function in this section. For details, please refer to [13]. <p> w; h; p; t; a of f so as to maximize E (f ) for a fixed camera position (For the discussion of the "where to move next" problem, please refer to <ref> [13] </ref>). 3 Detection Function We briefly discuss the detection function in this section. For details, please refer to [13]. <p> h 0 (7) 0 = arctan [tan () tan ( w 0 tan ( w ] (8) ffi 0 = arctan [tan (ffi) tan ( w 0 tan ( w ] (9) When the configurations of two operations are very similar, they might be correlated with each other (refer to <ref> [13] </ref> for detail). Repeated actions are avoided during the search process. When independence is assumed, b (c i ; f ) is calculated as follows. First, calculate the corresponding (; ffi; l) of the center of c i with respect to operation f . <p> Note, the sensed sphere representation is similar to a radially organized occu-pancy grid representation <ref> [13] </ref>. 5 Where to look next We need to select w; h; p; t; a for the next action. First, we select w; h; p; t for each given recognition algorithm. <p> We do not have space to address the "where to move next" problem in this paper. But it is interesting to note that Wixson [12] has done 2D simulation experi ments and that his results are actually consistent with our results. Please refer to <ref> [13] </ref> for detail. 7 Conclusion In this paper, we formulate the sensor planning task for object search and present a practical strategy for the "where to look next" problem of this task.

References-found: 13

