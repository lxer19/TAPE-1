URL: http://www.cs.uchicago.edu/publications/tech-reports/TR-97-05.ps
Refering-URL: http://cs-www.uchicago.edu/~burke/writing.html
Root-URL: 
Email: kulyuking@cs.uchicago.edu  flytinen, cphdnt, sschoenbg@cs.depaul.edu  
Title: Question Answering from Frequently-Asked Question Files: Experiences with the FAQ Finder System  
Author: Robin D. Burke, Kristian J. Hammond, Vladimir A. Kulyukin fburke, kris, Steven L. Lytinen, Noriko Tomuro, Scott Schoenberg 
Note: supported this work.  
Date: June 1997  
Address: 1100 E. 58th St., Chicago, IL 60637  243 S. Wabash, Chicago, IL 60604  1100 East 58th Street Chicago, Illinois 60637  
Affiliation: Intelligent Information Laboratory, University of Chicago  School of Computer Science, DePaul University  The University of Chicago Computer Science Department  The University of Chicago Computer Science Department  
Pubnum: Technical Report TR-97-05  
Abstract: This technical report describes FAQ Finder, a natural language question-answering system that uses files of frequently-asked questions as its knowledge base. Unlike AI question-answering systems that focus on the generation of new answers, FAQ Finder retrieves existing ones found in frequently-asked question files. Unlike information retrieval approaches that rely on a purely lexical metric of similarity between query and document, FAQ Finder uses a semantic knowledge base (WordNet) to improve its ability to match question and answer. We describe the design and the current implementation of the system and its support components, including results from an evaluation of the system's performance against a corpus of user questions. An important finding was that a combination of semantic and statistical techniques works better than any single approach. We analyze failures of the system and discuss future research aimed at addressing them. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bookstein, A., Klein, S., Raita, T. </author> <year> 1995. </year> <title> Detecting Content-Bearing Words by Serial Clustering Extended Abstract. </title> <booktitle> In ACM SIGIR 1995, </booktitle> <pages> pp. 319-327. </pages>
Reference-contexts: Since FAQ files have several structural constraints on their texts, it makes sense to experiment with term selection techniques that are more sensitive to the document structure than tfidf. One technique that seems particularly suitable for our purposes is condensation clustering <ref> (Bookstein, Klein, & Raita, 1995) </ref>. The basic idea is to look at the distribution of a term over a set of receptacles, i.e. sentences, paragraphs, Q&A pairs, chapters, etc, and see if the distribution has a statistically significant deviation from the random distribution.
Reference: <author> Buckley, C. </author> <year> 1985. </year> <title> Implementation of the SMART Information Retrieval Retrieval [sic] System. </title> <type> Technical Report 85-686, </type> <institution> Cornell University. </institution>
Reference-contexts: Finally, the system's Q&A matcher returns a small set of Q&A pairs to the user. For the first stage of processing, FAQ Finder uses standard information retrieval technology, the public domain SMART information retrieval system <ref> (Buckley, 1985) </ref>, to perform the initial step of narrowing the focus to a small subset of the FAQ files. The user's question is treated as a query to be matched against the library of FAQ files.
Reference: <author> Burke, R., Hammond, K., & Cooper, E. </author> <year> 1996. </year> <title> Knowledge-based information retrieval from semi-structured text. </title> <booktitle> In AAAI Workshop on Internet-based Information Systems, </booktitle> <pages> pp. 9-15. </pages> <publisher> AAAI. </publisher>
Reference-contexts: This version of the system was also demoed at several conferences 2 and described in workshop publications <ref> (Burke, Hammond & Cooper, 1996) </ref>. Local use of the system enabled us to gather a better test corpus. In the Fall of 1996, we undertook to reimplement the system in order to increase its performance and stability for public web release. <p> Therefore, the task of "tagging," identifying important structural elements in FAQ files is an important research area within the FAQ Finder project. We have experimented with various techniques for question and answer tagging. The FAQ Minder system was developed as a comprehensive approach to analyzing text file structure <ref> (Kulyukin, Hammond & Burke, 1996) </ref>, and in its semi-automated mode is our most accurate tagging technique. However, we reserve this labor-intensive process for only those files with obscure or irregular organization. The majority of FAQ Finder files are tagged automatically using a regular expression-based perl program called FAQ Grinder. <p> such sections match poorly with questions like "What books are there on antique radios?" theword book might not even appear in a list of titles and authors, but if the section of the FAQ could be reliably identified as a bibliography, the possibility of such matches would be greatly enhanced <ref> (Kulyukin, Hammond & Burke, 1996) </ref>. We are investigating ways to improve the system's response time. A significant component of the matching time is the input of FAQ file-specific auxiliary files. There are two mechanisms that may improve our handling of these files.
Reference: <author> Callan, J. P. </author> <year> 1994. </year> <title> Passage-Level Evidence in Document Retrieval. </title> <booktitle> In ACM SIGIR 1994, </booktitle> <pages> pp. 302-309. </pages>
Reference-contexts: that underlying database could be recovered, it would be possible to answer questions such as "What Star Trek episodes included the Minos Korva system?" or "What star system does the Bread and Circuses episode occur in?" A somewhat less ambitious approach to these files is to use passage-based retrieval techniques <ref> (Callan, 1994) </ref>. A document can be split into chunks of text. 13 Each chunk is treated as a separate document. An answer to the user query, in this case, is a place in the FAQ that is likely to contain relevant information.
Reference: <author> Collins, A. M. and Quillian, M. R. </author> <year> 1972. </year> <title> How to Make a Language User. </title> <editor> In E. Tulving and W. Donaldson, </editor> <booktitle> Organization of Memory. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: We found that unrestricted marker passing, using all networks in which a term appears, led to too many spurious matches, a common problem in marker passing systems in general <ref> (Collins & Quillian, 1972) </ref>. We tried several approaches to disambiguate terms to a single WordNet network. Our first attempt was to use an existing part-of-speech tagger, the Xerox Tagger (Cutting, et al., 1992).
Reference: <author> Cutting, D., Kupiec, J., Pederson, J., & Sibun, P. </author> <year> 1992. </year> <title> A Practical Part-of-Speech Tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing. ACL. </booktitle>
Reference-contexts: We tried several approaches to disambiguate terms to a single WordNet network. Our first attempt was to use an existing part-of-speech tagger, the Xerox Tagger <ref> (Cutting, et al., 1992) </ref>. This system uses a hidden Markov model learned from a large corpus of English text to statistically determine the most likely sense for any given word in the context of a sentence.
Reference: <author> Grady Ward, </author> <year> 1993. </year> <title> Moby Part-of-Speech II. Computer file. </title> <address> Arcata, CA: </address> <publisher> Grady Ward. </publisher>
Reference: <author> Hearst, M. A. </author> <year> 1993. </year> <title> TextTiling: A quantitative approach to discourse segmentation. </title> <type> Technical Report Sequoia 93/94, </type> <institution> Computer Science Department, University of California, Berke-ley. </institution>
Reference-contexts: The FAQ Finder project has shown that when there is an existing collection of questions and answers, as found in the FAQ files, question answering can be reduced 12 Non-frivolous examples of such files also exist in the RTFM archive. 13 The TextTiling technique proposed in <ref> (Hearst, 1993) </ref> seems particularly suitable here. 34 ------------------------------------------------------- SOLAR/STAR SYSTEMS ------------------------------------------------------- Acamar TNG "The Vengeance Factor" Alpha Centauri TOS "Metamorphosis" ...
Reference: <author> Hornby, A. S. </author> <year> 1984. </year> <title> Oxford Student's Dictionary of Current English. </title> <publisher> Oxford, UK: Oxford University Press. </publisher>
Reference-contexts: Toward that end, we have compiled a list of 226 most common irregular verbs on the basis of A. S. Hornby's Oxford Student's Dictionary of Current English <ref> (Hornby, 1984) </ref>. Thus, before the 32 rules are applied, the word is looked up in the table of irregular verbs. We plan to compile a similar list of irregular adjectives like "good", "better", "best", etc.
Reference: <author> Kolodner, J. </author> <year> 1993. </year> <title> Case-Based Reasoning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Question relevance: The question half of the Q&A pair is the most relevant for deter mining the match to a user's question. General knowledge: Broad, shallow knowledge of language is sufficient for question match ing. We see assumptions 1-3 as leading to an essentially case-based <ref> (Kolodner, 1993) </ref> view of the FAQ retrieval problem. A Q&A pair might be loosely considered a kind of case: it is a piece of knowledge that has been considered useful enough to be codified for reuse. The question serves as an index to the knowledge contained in the answer.
Reference: <author> Kulyukin, V., Hammond, K. & Burke, R. </author> <year> 1996. </year> <title> Automated analysis of structured on-line documents. </title> <booktitle> In AAAI Workshop on Internet-based Information Systems, </booktitle> <pages> pp. 78-86. </pages> <publisher> AAAI, </publisher> <month> 36 </month> <year> 1996. </year>
Reference-contexts: Therefore, the task of "tagging," identifying important structural elements in FAQ files is an important research area within the FAQ Finder project. We have experimented with various techniques for question and answer tagging. The FAQ Minder system was developed as a comprehensive approach to analyzing text file structure <ref> (Kulyukin, Hammond & Burke, 1996) </ref>, and in its semi-automated mode is our most accurate tagging technique. However, we reserve this labor-intensive process for only those files with obscure or irregular organization. The majority of FAQ Finder files are tagged automatically using a regular expression-based perl program called FAQ Grinder. <p> such sections match poorly with questions like "What books are there on antique radios?" theword book might not even appear in a list of titles and authors, but if the section of the FAQ could be reliably identified as a bibliography, the possibility of such matches would be greatly enhanced <ref> (Kulyukin, Hammond & Burke, 1996) </ref>. We are investigating ways to improve the system's response time. A significant component of the matching time is the input of FAQ file-specific auxiliary files. There are two mechanisms that may improve our handling of these files.
Reference: <author> Lang, K. L.; Graesser, A. C.; Dumais, S. T. and Kilman, D. </author> <year> 1992. </year> <title> Question Asking in Human-Computer Interfaces. </title> <editor> In T. Lauer, E. Peacock and A. C. </editor> <booktitle> Graesser Questions and Information Systems (pp. </booktitle> <pages> 131-165). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Assoc. </publisher>
Reference-contexts: We do not need our systems to actually comprehend the queries they receive <ref> (Lang, et al. 1992) </ref> or to generate new text that explain the answer (Souther, et al. 1989).
Reference: <author> Lenhert, W. </author> <year> 1978. </year> <title> The Process of Question Answering. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Assoc. </publisher>
Reference: <author> Miller, G. A. </author> <year> 1995. </year> <title> WordNet: A Lexical Database for English. </title> <journal> Communications of the ACM, </journal> <volume> 38(11). </volume>
Reference-contexts: Since FAQ Finder is intended to encompass the whole gamut of USENET topics, not just computers, it is impractical to expect even this simple level of domain-specific knowledge representation. FAQ Finder obtains its knowledge of shallow lexical semantics from WordNet, a semantic network of English words <ref> (Miller, 1995) </ref>. The WordNet system provides a system of relations between words and "synonym sets," and between synonym sets themselves. The level of knowledge representation does not go much deeper than the words themselves, but there is an impressive coverage of basic lexical relations.
Reference: <author> Norvig, P. </author> <year> 1987. </year> <title> Unified Theory of Inference for Text Understanding. </title> <type> Technical Report No. </type> <institution> UCB/CSD 87/339, University of California at Berkeley. </institution>
Reference-contexts: The second, perhaps more serious, concern is the translation of natural language inputs into the right knowledge representation language that the inference mechanism supports <ref> (Norvig, 1987) </ref>: the natural language understanding problem which we do not seek to tackle head-on. The real question for this is whether one can approximate inference without building an inference mechanism. We believe that the co-occurrence of terms may give a simple affirmative answer to this question.
Reference: <author> Ogden, W. C. </author> <year> 1988. </year> <title> Using natural language interfaces. </title> <editor> In M. Helander (Ed.), </editor> <booktitle> Handbook of human-computer interaction (pp. </booktitle> <pages> 205-235). </pages> <address> New York: </address> <publisher> North-Holland. </publisher>
Reference-contexts: What is needed is a centralized means of access to these answers. We believe that the most natural kind of interface to a database of answers is the question, stated in natural language <ref> (Ogden, 1988) </ref>. While the general problem of understanding questions stated in natural language remains open, we believe that the simpler task of matching questions to corresponding question-answer pairs is feasible and practical.
Reference: <author> Quillian, M. R. </author> <year> 1968. </year> <title> Semantic Memory. In Semantic Information Processing, Marvin Minsky, </title> <publisher> ed., </publisher> <pages> pp. 216-270. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Salton, G., & McGill, M. </author> <year> 1983. </year> <title> Introduction to modern information retrieval. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: A Q&A pair is represented by a term vector that associates a significance value with each term in the Q&A pair. The significance value that we use is commonly known as tfidf, which stands for term frequency times log of inverse document frequency <ref> (Salton & McGill, 1983) </ref>.
Reference: <author> Souther, A.; Acker, L.; Lester, J. and Porter, B. </author> <year> 1989. </year> <title> Using view types to generate explanations in intelligent tutoring systems. </title> <booktitle> In Proceedings of the Eleventh Annual conference of the Cognitive Science Society, </booktitle> <pages> pp. 123-130. </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Assoc. </publisher> <pages> 37 </pages>
Reference-contexts: We do not need our systems to actually comprehend the queries they receive (Lang, et al. 1992) or to generate new text that explain the answer <ref> (Souther, et al. 1989) </ref>. They only have to identify the files that are relevant to the query and then match against the segments of text that are used to organize the files themselves (e.g., questions, section headings, key words, etc.).
References-found: 19

