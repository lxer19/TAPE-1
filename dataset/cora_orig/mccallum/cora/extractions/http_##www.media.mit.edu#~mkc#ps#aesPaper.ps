URL: http://www.media.mit.edu/~mkc/ps/aesPaper.ps
Refering-URL: http://www.media.mit.edu/~mkc/research.html
Root-URL: http://www.media.mit.edu
Email: mkc,billg,sbasu@media.mit.edu  
Title: Vision-Steered Beam Forming and Transaural Rendering for the Artificial Life Interactive Video Environment, (ALIVE)  
Author: Michael A. Casey, William G. Gardner, Sumit Basu 
Address: Cambridge, USA  
Affiliation: MIT Media Laboratory,  
Pubnum: Perceptual Computing Technical Report #352  
Abstract: This paper describes the audio component of an interactive video system that uses remote sensing to free the user from body-mounted tracking equipment. Position information is obtained from a camera and used to constrain a beam-forming microphone array, for far-field speech input, and a two-speaker transaural audio system for rendering 3D audio.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ANSI. </author> <title> S3.5-1969,American National Standard Methods for the Calculation of the Articulation Index. </title> <institution> American National Standards Institute, </institution> <address> New York, </address> <year> 1969. </year>
Reference-contexts: One such metric is the intelligibility-weighted directivity index [7] in which the directivity index is weighted by a set of frequency-dependent coefficients provided by the ANSI standard for the speech articulation index <ref> [1] </ref>. This metric weights the directivity index in fourteen one-third-octave bands spanning 180 to 4500 Hz [7]. 1.7 Designing the Array An important first consideration is the choice of array geometry. Two possible architectures are considered; endfire arrangement, Figure 3, and broadside arrangement, Figure 4.
Reference: [2] <author> H. Cox. </author> <title> "Robust Adaptive Beamforming" IEEE Transactions on Acoustics, </title> <booktitle> Speech and Signal Processing, </booktitle> <volume> 35(10) </volume> <pages> 1365-1376, </pages> <year> 1987. </year>
Reference-contexts: The problem with adaptive strategies for the ALIVE space is that there are typically many observers around the space and the level of ambient speech-like sound is very high as a result. Adaptive algorithms do not perform well when multiple sources arrive simultaneously from different directions <ref> [2] </ref>. 1.4 Fixed Beamforming Fixed array strategies optimize the microphone filtering for a particular direction and don't change with varying incident source direction. Thus the directional response of the array is fixed to a particular azimuth and elevation.
Reference: [3] <author> F. Khalil, J.P. Jullien, and A. Gilloire. </author> <title> "Microphone Array for Sound Pickup in Teleconference Systems". </title> <journal> Journal of the Audio Engineering Society, </journal> <volume> 42(9) </volume> <pages> 691-699, </pages> <year> 1994. </year> <month> 15 </month>
Reference-contexts: This is the well-known principle of 4 pattern multiplication <ref> [3] </ref> [7]. For omnidirectional microphones, the gain pat-terns for the two layouts are identical but for a rotation. The gain patterns for an endfire array centered along the = 0 axis with omnidirectional microphones steered at 15, 45, and 75 degrees is shown in Figure 5.
Reference: [4] <author> P. Maes, T. Darrell, B. Blumberg, and A. Pentland. </author> <title> "The ALIVE Sys--tem: Full-body Interaction with Autonomous Agents". </title> <booktitle> Proceedings of the Computer Animation Conference, </booktitle> <address> Switzerland, </address> <publisher> IEEE Press, </publisher> <year> 1995. </year>
Reference-contexts: 1 Vision-Steered Beam Forming 1.1 Introduction The Media Lab's ALIVE project, Artificial Life Interactive Video Environment, is a testbed for research into remote-sensing, full-body, interactive interfaces for virtual environments, <ref> [4] </ref>. A plan view of the ALIVE space is shown in Figure 1. A camera on top of the large video projection screen captures images of the user in the active zone, these images are fed to a visual recognition system where various features are tracked. <p> to be modified in real-time in order to beamform in the direction of the user. 1.5 A Visually Constrained Beamformer The ALIVE space utilizes a visual recognition system called Pfinder, short for person finder, developed at the Media Lab for tracking a person's hands, face or any other color-discriminable feature <ref> [4] </ref>. Pfinder uses an intensity-normalized color representation of each pixel in the camera image and multi-way Gaussian classifiers to decide which of several classes each pixel belongs to. Examples of classes are background, left hand, right hand and head. <p> The mean of each cluster gives the coordinates of the class, and the eigenvalues give the orientation. Pfinder provides updates on each class roughly 6 times a second. Further details on the visual recognition system can be found in <ref> [4] </ref>. The information from the visual recognition system is used to steer a fixed beamforming algorithm. Azimuth calculations are performed from the 3-space coordinate data provided by the mean of the head class. <p> sensory input are paramount in this system since the desirable properties of fixed arrays are combined with the steerability of an adaptive system. 5 2 Visually Steered 3-D Audio 2.1 Introduction This section discusses the 3-D audio system that has been developed for the ALIVE project at the Media Lab <ref> [4] </ref>. The audio system can position sounds at arbitrary azimuths and elevations around a listener's head. The system uses stereo loudspeakers arranged conventionally (at 30 degrees with respect to the listener). <p> As discussed earlier, the ALIVE project uses video cameras to track people using the system. The Pfinder program can track various features of the human body, including the head and hands <ref> [4] </ref>. With a single camera viewing a standing 13 person, the distance between the camera and the person is calculated by finding the position of the feet relative to the bottom of the image.
Reference: [5] <author> R.J. Mailloux. </author> <title> Phased Array Antenna Handbook. </title> <publisher> Artech House, </publisher> <address> Boston, London, </address> <year> 1994. </year>
Reference: [6] <author> W. Soede, A.J. Berkhout, and F.A. Bilsen. </author> <title> "Development of a Directional Hearing Instrument Based on Array Technology". </title> <journal> Journal of the Aoustical Society of America, </journal> <volume> 94(2) </volume> <pages> 785-798, </pages> <year> 1993. </year>
Reference: [7] <author> R.W. Stadler and W.M. Rabinowitz. </author> <title> "On the Potential of Fixed Arrays for Hearing Aids". </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 94(3) </volume> <pages> 1332-1342, </pages> <year> 1993. </year>
Reference-contexts: Note, however, that this is not the only set of weights that can be used for W. For example, Stadler and Rabinowitz present a method of obtaining the weights with a parameter fi that arbitrates high directivity and uncorrelated noise gain <ref> [7] </ref>. This method, when used to obtain maximum directivity, yields gain patterns that are slightly more directional than the basic weights described above. The standard performance metric for the directionality of a fixed array is the directivity index which is shown in Equation 3, [7]. <p> high directivity and uncorrelated noise gain <ref> [7] </ref>. This method, when used to obtain maximum directivity, yields gain patterns that are slightly more directional than the basic weights described above. The standard performance metric for the directionality of a fixed array is the directivity index which is shown in Equation 3, [7]. <p> In order to assess an array for use in speech enhancement a broad-band performance metric must be used. One such metric is the intelligibility-weighted directivity index <ref> [7] </ref> in which the directivity index is weighted by a set of frequency-dependent coefficients provided by the ANSI standard for the speech articulation index [1]. This metric weights the directivity index in fourteen one-third-octave bands spanning 180 to 4500 Hz [7]. 1.7 Designing the Array An important first consideration is the <p> One such metric is the intelligibility-weighted directivity index <ref> [7] </ref> in which the directivity index is weighted by a set of frequency-dependent coefficients provided by the ANSI standard for the speech articulation index [1]. This metric weights the directivity index in fourteen one-third-octave bands spanning 180 to 4500 Hz [7]. 1.7 Designing the Array An important first consideration is the choice of array geometry. Two possible architectures are considered; endfire arrangement, Figure 3, and broadside arrangement, Figure 4. A second factor is the choice of microphone gain pattern for the individual microphone elements, F (). <p> This is the well-known principle of 4 pattern multiplication [3] <ref> [7] </ref>. For omnidirectional microphones, the gain pat-terns for the two layouts are identical but for a rotation. The gain patterns for an endfire array centered along the = 0 axis with omnidirectional microphones steered at 15, 45, and 75 degrees is shown in Figure 5.
Reference: [8] <author> Ali Azarbayejani, Thad Starner, Bradley Horowitz, and Alex Pentland. </author> <title> Visually controlled graphics. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 15(6) </volume> <pages> 602-605, </pages> <month> June </month> <year> 1993. </year> <title> (Special Section on 3-D Modeling in Image Analysis and Synthesis). </title>
Reference-contexts: However, orientation of the head can be estimated from a closeup view of the face. This is accomplished by obtaining templates for the eyes, nose and mouth, recovering these feature positions via normalized correlation, and assuming an elliptical head shape <ref> [8] </ref>. 2.9 Preliminary results In order to experiment with head tracking in the context of transaural 3-D audio, we are currently using a Polhemus tracking system. This system returns the position and orientation of a sensor with respect to a transmitter (6 degrees of freedom).
Reference: [9] <author> Durand R. Begault. </author> <title> 3-D Sound for Virtual Reality and Multimedia. </title> <publisher> Academic Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Finally we will discuss how the head tracking information can be used, and give preliminary results. 2.2 Principles of binaural spatial synthesis A binaural spatializer simulates the auditory experience of one or more sound sources arbitrarily located around a listener <ref> [9] </ref>. The basic idea is to reproduce the acoustical signals at the two ears that would occur in a normal listening situation.
Reference: [10] <author> Duane H. Cooper and Jerald L. Bauck. </author> <title> "Prospects for Transaural Recording". </title> <journal> J. Audio Eng. Soc., </journal> 37(1/2):3-19, 1989. 
Reference-contexts: The basic idea is to filter the binaural signal such that the subsequent stereo presentation produces the binaural signal at the ears of the listener. The technique was first put into practice by Schroeder and Atal [22, 21] and later refined by Cooper and Bauck <ref> [10] </ref>, who referred to it as "transaural audio". The stereo listening situation is shown in figure 12, where ^x L and ^x R are the signals sent to the speakers, and y L and y R are the signals at the listener's ears. <p> be specified in terms of the ipsilateral (H i = H LL = H RR ) and contralateral (H c = H LR = H RL ) responses: H 1 = H 2 c H i H c Cooper and Bauck proposed using a "shu*er" implementation of the transaural filter <ref> [10] </ref>, which involves forming the sum and difference of x L and x R , filtering these signals, and then undoing the sum and difference operation. <p> * The ipsilateral response is taken to be unity, and the contralateral response is modeled as a delay and attenuation [21]. * Same as above, but the contralateral response is modeled as a delay, at tenuation, and lowpass filter 2 . * The head is modeled as a rigid sphere <ref> [10] </ref>. * The head is modeled as a generic human head without pinna. At high frequencies, where pinna response becomes important (&gt; 8 kHz), the head effectively blocks the crosstalk between channels. Furthermore, the variation in head response for different people is greatest at high frequencies [19].
Reference: [11] <author> P. Damaske. </author> <title> "Head-Related Two-Channel Stereophony with Loudspeaker Reproduction". </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 50(4) </volume> <pages> 1109-1115, </pages> <year> 1971. </year>
Reference-contexts: The calibration procedure involves adjusting the parameters such that single sided noises are located as close as possible to their corresponding ears and the stereo noise is maximally enveloping <ref> [11] </ref>. The in-teraural delay parameter has the most effect of steering the signal and changing the timbre, provided the gain parameter is sufficiently close to 1. The lowpass cutoff has the most subtle effect.
Reference: [12] <author> N. I. Durlach, A. Rigopulos, X. D. Pang, W. S. Woods, A. Kulkarni, H. S. Colburn, and E. M. Wenzel. </author> <title> "On the Externalization of Auditory Images". </title> <journal> Presence, </journal> <volume> 1(2) </volume> <pages> 251-257, </pages> <year> 1992. </year>
Reference-contexts: Externaliza-tion can be improved by using the listener's own head responses, adding reverberation, and adding dynamic cues <ref> [12] </ref>. * Frontal sounds are localized between the ears or on top ot the head, rather than in front of the listener.
Reference: [13] <author> W. G. Gardner and K. D. Martin. </author> <title> "HRTF measurements of a KEMAR". </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 97(6) </volume> <pages> 3907-3908, </pages> <year> 1995. </year>
Reference-contexts: The HRTFs were measured using a KEMAR (Knowles Electronics Mannequin for 7 Acoustics Research), which is a high quality dummy-head microphone. The HRTFs were measured in 10 degree elevation increments from -40 to +90 degrees <ref> [13] </ref>. In the horizontal plane (0 degrees elevation), measurements were made every 5 degrees of azimuth. In total, 710 directions were measured. The sampling density was chosen to be roughly in accordance with the localization resolution of humans. The HRTFs were measured at a 44.1 kHz sampling rate.
Reference: [14] <author> D. Griesinger. </author> <title> "Equalization and Spatial Equalization of Dummy-Head Recordings for Loudspeaker Reproduction". </title> <journal> J. Audio Eng. Soc., </journal> 37(1/2):20-29, 1989. 
Reference-contexts: However, it may be 14 that small time adjustments are in fact unnecessary, judging from the in-sensitivity to small head rotations with a static transaural system. This is also suggested in the literature on equalizing dummy-head recordings for loudspeaker reproduction <ref> [14] </ref>. Using the static, symmetrical transaural system described earlier, the head tracking information was also used to update the positions of 3-D sounds so that the auditory scene remained fixed as the listener's head rotated.
Reference: [15] <author> J. M. Jot, Veronique Larcher, and Olivier Warusfel. </author> <title> "Digital signal processing issues in the context of binaural and transaural stereophony". </title> <journal> In Proc. Audio Eng. Soc. </journal> <volume> Conv., </volume> <year> 1995. </year>
Reference-contexts: The interaural delay can be included in the filter responses directly as leading zero coefficients, or can be factored out in an effort to shorten the filter lengths. It is also possible to use mimimum phase filters derived from the HRTFs <ref> [15] </ref>, since these will in general be shorter than the original HRTFs. This is somewhat risky because the resulting interaural phase may be completely distorted. It would appear, however, that interaural amplitudes as a function of frequency encode more useful directional information than interaural phase [16]. <p> One way to eliminate all factors which do not vary as a function of direction is to equalize the HRTFs to a diffuse-field reference <ref> [15] </ref>. <p> cropping (rectangular windowing). * Reduce the filter size by factoring out the interaural delay and implement ing this separately from the convolution. * Reduce the filter size by using minimum phase filters. * Model the HRTFs using infinite impulse response (IIR) filters. 9 Many of these strategies are discussed in <ref> [15] </ref>. To obtain the best price to performance ratio, commercial spatializers attempt to be as efficient as possible, and usually run on dedicated DSPs. Consequently, the filters are modeled as efficiently as possible and the algorithms are hand-coded.
Reference: [16] <author> Keith D. Martin. </author> <title> A computational model of spatial hearing. </title> <type> Master's thesis, </type> <institution> MIT Dept. of Elec. Eng., </institution> <year> 1995. </year> <month> 16 </month>
Reference-contexts: This is somewhat risky because the resulting interaural phase may be completely distorted. It would appear, however, that interaural amplitudes as a function of frequency encode more useful directional information than interaural phase <ref> [16] </ref>. There are several problems common to headphone spatializers: * The HRTFs used for sythesis are often a generic set and not the specific HRTFs of the listener. This can cause localization performance to suffer [24, 25], particularly in regards to front-back discrimination, elevation perception, and externalization.
Reference: [17] <author> Henrik Moller, Dorte Hammershoi, Clemen Boje Jensen, and Michael Fris Sorensen. </author> <title> "Transfer Characteristics of Headphones Measured on Human Ears". </title> <journal> J. Audio Eng. Soc., </journal> <volume> 43(4) </volume> <pages> 203-217, </pages> <year> 1995. </year>
Reference-contexts: In this paper, we will use the term HRTF to refer to both the time and frequency domain representation. 6 to ear frequency response <ref> [26, 17] </ref>. A schematic diagram of a single source system is shown in figure 9. The direction of the source ( = azimuth, = elevation) determines which pair of HRTFs to use, and the distance (r) determines the gain.
Reference: [18] <author> Henrik Moller, Clemen Boje Jensen, Dorte Hammershoi, and Michael Fris Sorensen. </author> <title> "Design Criteria for Headphones". </title> <journal> J. Audio Eng. Soc., </journal> <volume> 43(4) </volume> <pages> 218-232, </pages> <year> 1995. </year>
Reference-contexts: A constant amount of reverberation can be mixed into the final output using an external reverberator as shown in figure 10. The spatializer was evaluated using headphones (AKG-K240, which are diffuse-field equalized <ref> [23, 18] </ref>). The input sound, usually music or sound effects, was taken from one channel of a compact disc player. The spatializer worked quite well for lateral and rear directions for all listeners. As expected, some listeners had problems with front-back reversals.
Reference: [19] <author> Henrik Moller, Michael Fris Sorensen, Dorte Hammershoi, and Clemen Boje Jensen. </author> <title> "Head-Related Transfer Functions of Human Subjects". </title> <journal> J. Audio Eng. Soc., </journal> <volume> 43(5) </volume> <pages> 300-321, </pages> <year> 1995. </year>
Reference-contexts: At high frequencies, where pinna response becomes important (&gt; 8 kHz), the head effectively blocks the crosstalk between channels. Furthermore, the variation in head response for different people is greatest at high frequencies <ref> [19] </ref>. Consequently, there is little point in modeling pinna response when constructing a transaural filter. 2.6 Implementation of transaural filter Our transaural filter is based on a simplified head model suggested by David Griesinger.
Reference: [20] <author> Alan V. Oppenheim and Ronald W. Schafer. </author> <title> Discrete Time Signal Processing. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: The spatializer convolves a monophonic input signal with a pair of HRTFs to produce a stereophonic (binaural) output. The HRTFs that are closest to the desired azimuth and elevation are used. For efficiency, the convolution is accomplished using an overlap-save block convolver <ref> [20] </ref> based on the fast Fourier transform (FFT). Because the impulse response is 128 points long, the convolution is performed in 128-point blocks, using a 256-point real FFT. The forward transforms of all HRTFs are pre-computed.
Reference: [21] <author> M. R. Schroeder. </author> <title> "Digital simulation of sound transmission in reverberant spaces". </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 47(2) </volume> <pages> 424-431, </pages> <year> 1970. </year>
Reference-contexts: The basic idea is to filter the binaural signal such that the subsequent stereo presentation produces the binaural signal at the ears of the listener. The technique was first put into practice by Schroeder and Atal <ref> [22, 21] </ref> and later refined by Cooper and Bauck [10], who referred to it as "transaural audio". <p> In practice, the transaural filters are often based on a simplified head model. Here we list a few possible models in order of increasing complexity: * The ipsilateral response is taken to be unity, and the contralateral response is modeled as a delay and attenuation <ref> [21] </ref>. * Same as above, but the contralateral response is modeled as a delay, at tenuation, and lowpass filter 2 . * The head is modeled as a rigid sphere [10]. * The head is modeled as a generic human head without pinna. <p> The general form of the transaural filter (equation 7) may be used instead, but at much greater computational cost. It may be better to abandon the simplified IIR model and use an FIR implementation based on a more realistic head model <ref> [21] </ref>. * To compensate for head rotations, the general form of the transaural filter (equation 7) was implemented with the simplified head model (equation 11). The resulting dynamic filter compensated for the changing path lengths between the speakers and the ears in order to keep the cancellation signals aligned properly.
Reference: [22] <author> M. R. Schroeder and B. S. Atal. </author> <title> "Computer simulation of sound transmission in rooms". </title> <journal> IEEE Conv. Record, </journal> <volume> 7 </volume> <pages> 150-155, </pages> <year> 1963. </year>
Reference-contexts: The basic idea is to filter the binaural signal such that the subsequent stereo presentation produces the binaural signal at the ears of the listener. The technique was first put into practice by Schroeder and Atal <ref> [22, 21] </ref> and later refined by Cooper and Bauck [10], who referred to it as "transaural audio".
Reference: [23] <author> G. Theile. </author> <title> "On the Standardization of the Frequency Response of High-Quality Studio Headphones". </title> <journal> J. Audio Eng. Soc., </journal> <volume> 34 </volume> <pages> 956-969, </pages> <year> 1986. </year>
Reference-contexts: A constant amount of reverberation can be mixed into the final output using an external reverberator as shown in figure 10. The spatializer was evaluated using headphones (AKG-K240, which are diffuse-field equalized <ref> [23, 18] </ref>). The input sound, usually music or sound effects, was taken from one channel of a compact disc player. The spatializer worked quite well for lateral and rear directions for all listeners. As expected, some listeners had problems with front-back reversals.
Reference: [24] <author> E. M. Wenzel. </author> <title> "Localization in Virtual Acoustic Displays". </title> <journal> Presence, </journal> <volume> 1(1) </volume> <pages> 80-107, </pages> <year> 1992. </year>
Reference-contexts: There are several problems common to headphone spatializers: * The HRTFs used for sythesis are often a generic set and not the specific HRTFs of the listener. This can cause localization performance to suffer <ref> [24, 25] </ref>, particularly in regards to front-back discrimination, elevation perception, and externalization. When the listener's own head responses are used, their localization performance is comparable to natural listening [26]. * The auditory scene created moves with the head. <p> This can be fixed by dynamically tracking the orientation of the head and updating the HRTFs appropriately. Localization performance and realism should both improve when dynamic cues are added <ref> [24] </ref>. * The auditory images created are not perceived as being external to the head, but rather are localized at the head or inside the head.
Reference: [25] <author> E. M. Wenzel, M. Arruda, D. J. Kistler, and F. L. Wightman. </author> <title> "Localization using nonindividualized head-related transfer functions". </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 94(1) </volume> <pages> 111-123, </pages> <year> 1993. </year>
Reference-contexts: There are several problems common to headphone spatializers: * The HRTFs used for sythesis are often a generic set and not the specific HRTFs of the listener. This can cause localization performance to suffer <ref> [24, 25] </ref>, particularly in regards to front-back discrimination, elevation perception, and externalization. When the listener's own head responses are used, their localization performance is comparable to natural listening [26]. * The auditory scene created moves with the head.
Reference: [26] <author> F. L. Wightman and D. J. Kistler. </author> <title> "Headphone simulation of free-field listening". </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 85 </volume> <pages> 858-878, </pages> <booktitle> 1989. </booktitle> <volume> 17 18 19 20 21 22 23 24 25 26 27 arrangement. </volume> <pages> 28 </pages>
Reference-contexts: In this paper, we will use the term HRTF to refer to both the time and frequency domain representation. 6 to ear frequency response <ref> [26, 17] </ref>. A schematic diagram of a single source system is shown in figure 9. The direction of the source ( = azimuth, = elevation) determines which pair of HRTFs to use, and the distance (r) determines the gain. <p> This can cause localization performance to suffer [24, 25], particularly in regards to front-back discrimination, elevation perception, and externalization. When the listener's own head responses are used, their localization performance is comparable to natural listening <ref> [26] </ref>. * The auditory scene created moves with the head. This can be fixed by dynamically tracking the orientation of the head and updating the HRTFs appropriately.
References-found: 26

