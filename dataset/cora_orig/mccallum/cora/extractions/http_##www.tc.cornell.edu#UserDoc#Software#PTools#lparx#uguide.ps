URL: http://www.tc.cornell.edu/UserDoc/Software/PTools/lparx/uguide.ps
Refering-URL: http://www.tc.cornell.edu/UserDoc/Software/PTools/lparx/
Root-URL: http://www.tc.cornell.edu
Email: fbaden,skohn,silvia,sfinkg@cs.ucsd.edu  
Title: The LPARX User's Guide  
Author: v. Scott B. Baden Scott R. Kohn Silvia M. Figueira Stephen J. Fink 
Address: La Jolla, CA 92093-0114 USA  
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Date: 9 November 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. Agrawal, A. Sussman, and J. Saltz, </author> <title> Compiler and runtime support for structured and block structured applications, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: In contrast to HPF, LPARX decompositions are first class language objects which can be created and manipulated at run-time. This enables LPARX to exploit the structure in blocked irregular decompositions such as recursive bisection, and in adaptive grid hierarchies. Block structured PARTI/CHAOS <ref> [1] </ref> provides facilities that are similar to LPARX, though for a different problem domain: irregularly coupled regular meshes, in which the blocks are static and relatively large. LPARX does not apply to unstructured problems such as the finite element method.
Reference: [2] <author> A. Almgren, T. Buttke, and P. Colella, </author> <title> A fast vortex method in three dimensions, </title> <booktitle> in Proceedings of the 10th AIAA Computational Fluid Dynamics Conference, </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1991, </year> <pages> pp. 446-455. </pages>
Reference-contexts: The computation repeats for the desired number of time steps. The force calculation typically dominates the computation time. In general, each particle may be influenced by every other particle. A naive implementation would require O (n 2 ) particle-particle interactions to determine the force on every particle. Approximation methods <ref> [2] </ref> [8] [16] trade some accuracy for speed by dividing the computation into two components: direct particle-particle interactions and far-field calculations. Direct interactions are calculated only for those particles lying within a specified cut-off distance r, as shown in Figure 9. That is, particle-particle interactions are spatially localized.
Reference: [3] <author> M. J. Berger and S. H. Bokhari, </author> <title> A partitioning strategy for nonuniform problems on multiprocessors, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36 (1987), </volume> <pages> pp. 570-580. </pages>
Reference-contexts: Perform calculations on the Grids in the XArray in parallel using the forall loop. The decomposition in (1) may be managed explicitly by the application, such as in the generation of refinement regions, or through partitioning routines such as recursive bisection <ref> [3] </ref>, uniform block partitioning, and inverse space partitioning [14], which are provided by LPARX's standard library. If LPARX is used in the back-end of a compiler, the data decomposition may be automatically generated by the compiler. <p> Such a uniform decomposition does not efficiently distribute workloads; for example, no work is assigned to processors p4, p5, p10, and p11. A better method for decomposing non-uniform workloads is shown in Figure 11, which gives two irregular block assignments rendered using recursive bisection <ref> [3] </ref>. In these decompositions, each 11 Creating Grid and XArray classes for user-defined types is discussed in Appendix B. 12 We "square" off the circular dependence region for programming convenience. 18 distribution. Workload is related to particle density. processor receives approximately the same amount of work.
Reference: [4] <author> M. J. Berger and P. Colella, </author> <title> Local adaptive mesh refinement for shock hydrodynamics, </title> <journal> JCP, </journal> <volume> 82 (1989), </volume> <pages> pp. 64-84. </pages>
Reference-contexts: Because we partition each level of the multigrid hierarchy independently, each mesh may communicate with several others at the next level. The second distributed loop in Coarsen () copies the averaged data to all overlapping meshes. 4.3 Adaptive Mesh Refinement Adaptive Mesh Refinement (AMR) <ref> [4] </ref> is another important class of dynamic, irregular scientific computations. One instance of an AMR application is the calculation of the electronic wavefunction for a system such as the C 20 H 20 molecule shown in Figure 16. Such computations arise in ab initio molecular dynamics.
Reference: [5] <author> M. J. Berger and J. Oliger, </author> <title> Adaptive mesh refinement for hyperbolic partial differential equations, </title> <journal> Journal of Computational Physics, </journal> <volume> 53 (1984), </volume> <pages> pp. 484-512. </pages>
Reference-contexts: We may reduce the error by uniformly decreasing the spacing of the mesh. A more efficient, though also more elaborate, method is to selectively refine the mesh according to a local estimate of the error <ref> [5] </ref>. The cost of uniformly refining a d-dimensional mesh by a factor k is substantial: memory usage scales as k d and computation time at least as fast 14 as k d .
Reference: [6] <author> A. Brandt, </author> <title> Multigrid techniques: 1984 guide, </title> <type> Tech. Rep. </type> <institution> GMD Studien 85, Gesselschaft fuer Mathematik un Datenverarbeitung, </institution> <address> St. Augustin, Germany, </address> <year> 1984. </year>
Reference-contexts: Of course, the run-time system must elimenate pointers when moving particle data between different processors' memory address spaces, as pointers apply only to a local address space; these details are hidden from the programmer. 21 4.2 Multigrid Multigrid <ref> [6] </ref> is a fast method for solving partial differential equations and sparse systems of linear equations. It represents a problem over a hierarchy of successively coarsened meshes to accelerate the communication of information across the computational domain and hence reduce the time to solution.
Reference: [7] <author> W. L. Briggs, </author> <title> Multigrid Tutorial, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1987. </year>
Reference-contexts: Since multigrid is simpler to describe, we use it to illustrate the management of multilevel mesh structures in LPARX. We will only discuss the communication of information between meshes; details about the numerical calculations can be found in Briggs's informative tutorial <ref> [7] </ref>. We consider here a typical multigrid application, solving the 2-d Poisson's equation, although the ideas readily generalize to higher dimensions and other multilevel calculations. The problem is initially posed on a base mesh of spacing N fi N .
Reference: [8] <author> J. Carrier, L. Greengard, and V. Rokhlin, </author> <title> A fast adaptive multipole algorithm for particle simulations, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <year> (1988). </year> <month> 28 </month>
Reference-contexts: The force calculation typically dominates the computation time. In general, each particle may be influenced by every other particle. A naive implementation would require O (n 2 ) particle-particle interactions to determine the force on every particle. Approximation methods [2] <ref> [8] </ref> [16] trade some accuracy for speed by dividing the computation into two components: direct particle-particle interactions and far-field calculations. Direct interactions are calculated only for those particles lying within a specified cut-off distance r, as shown in Figure 9. That is, particle-particle interactions are spatially localized.
Reference: [9] <author> S. Deshpande, P. Delisle, and A. G. Daghi, </author> <title> A communication facility for distributed object-oriented applications, </title> <booktitle> in USENIX C++ Conference Proceedings, </booktitle> <year> 1992. </year>
Reference-contexts: DPO does not enforce any particular form of synchronization protocol between primary and secondary copies. The Distributed Parallel Object design is based in part on the distributed object-oriented communication facilities described by Deshpande et al. <ref> [9] </ref>, which are intended for distributed systems. DPO differs in that inter-object communication is non-blocking and asynchronous, as compared to the blocking remote procedure call (RPC) communication typically used in distributed systems. At the top of the LPARX software hierarchy are the LPARX classes.
Reference: [10] <author> P. N. Hilfinger and P. Colella, Fidil: </author> <title> A language for scientific programming, </title> <type> Tech. Rep. </type> <institution> UCRL-98057, Lawrence Livermore National Laboratory, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: On parallel processors, the underlying interprocessor communication model is hidden from the programmer. However, the programmer is able to manage locality within the memory hierarchy to avoid high overhead costs. This is accomplished through LPARX's domain calculus, which first appeared in the FIDIL programming language <ref> [10] </ref>. <p> Often P 0 = P . 2.3 The Domain Calculus To simplify Region manipulation, LPARX borrows FIDIL's <ref> [10] </ref> domain calculus. The domain calculus provides geometric operators over Regions such as grow and intersect which help the programmer coordinate data motion among block-structured objects.
Reference: [11] <author> R. W. Hockney and J. W. Eastwood, </author> <title> Computer Simulation Using Particles, </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: For example, in addition to representing a finite difference mesh of floating point numbers, the Grid may also be used to implement the spatial data structures <ref> [11] </ref> common in particle calculations. Associated with each Grid is its Region; the bounding box over which the Grid's storage is to be defined. <p> That is, particle-particle interactions are spatially localized. Although far-field calculations 17 typically involve some form of global communication, most far-field computations are also spatially localized. A common method for maintaining neighbor relationships for interacting particles is to sort the particles into a binning mesh <ref> [11] </ref>, which avoids the costly O (n 2 ) search for neighbors. Each bin of this mesh contains a list of the particles assigned to the corresponding region of space.
Reference: [12] <author> S. R. Kohn and S. B. Baden, </author> <title> Asynchronous object-oriented parallel programming with distributed parallel objects. </title> <note> Submitted to Supercomputing '94, </note> <month> April </month> <year> 1994. </year>
Reference-contexts: Asynchronous Message Streams (AMS), built on top of the MP ++ layer, is a new communication paradigm which combines abstractions from Active Messages [17] and the C ++ I/O stream library <ref> [12] </ref>. AMS extends Active Messages to the user-level and frees the programmer from many message passing details. In AMS, a processor sends a message "stream" to a user-defined function handler on a different processor. <p> This handler is awakened by AMS, consumes the message stream, takes some appropriate action, and then exits. AMS hides all details of buffer management and buffer packing and unpacking; communication is specified at a high-level and resembles standard C ++ I/O (see Figure 18). The Distributed Parallel Object (DPO) <ref> [12] </ref> layer builds further object-oriented abstractions on top of AMS. DPO manipulates physically distributed C ++ objects in a shared name space.
Reference: [13] <author> Message Passing Interface Forum, </author> <title> Document for a Standard Message-Passing Interface (Draft), </title> <month> November </month> <year> 1993. </year>
Reference-contexts: At the bottom of the LPARX software hierarchy is MP ++ , an architecture independent portable message passing layer which provides facilities for asynchronous and synchronous message passing, barrier synchronization, broadcasts, global reductions, and all-to-all broadcasts. When the Message Passing Interface (MPI) <ref> [13] </ref> has been standardized and is readily available, we will replace the MP ++ layer with MPI. MP ++ is currently running on the CM-5, Paragon, iPSC/860, nCUBE/2, KSR-1, Cray C-90 (single processor), single processor workstations, and networks of workstations under PVM.
Reference: [14] <author> J. R. Pilkington and S. B. Baden, </author> <title> Dynamic non-uniform mesh partitioning with space-filling curves. </title> <note> To be submitted to IEEE Transactions on Parallel and Distributed Systems, </note> <month> October </month> <year> 1993. </year>
Reference-contexts: Perform calculations on the Grids in the XArray in parallel using the forall loop. The decomposition in (1) may be managed explicitly by the application, such as in the generation of refinement regions, or through partitioning routines such as recursive bisection [3], uniform block partitioning, and inverse space partitioning <ref> [14] </ref>, which are provided by LPARX's standard library. If LPARX is used in the back-end of a compiler, the data decomposition may be automatically generated by the compiler.
Reference: [15] <author> V. S. Sunderam, </author> <title> Pvm: A framework for parallel distributed computing, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2 (1990), </volume> <pages> pp. 315-339. </pages>
Reference-contexts: LPARX, implemented as a C ++ class library, requires only basic message passing support and is currently running on several MIMD computers|Intel Paragon and iPSC/860, Thinking Machines CM-5, IBM SP-1, Kendall Square Research KSR-1, nCUBE nCUBE/2|single processor workstations, Cray C-90 (single processor and macrotasking), and networks of workstations under PVM <ref> [15] </ref>. LPARX software may invoke subroutines written in languages other than C ++ such as C or Fortran. 2 Overview In this section, we give an overview of LPARX's facilities. We introduce LPARX's representation of block irregular decompositions and then discuss our view of coarse-grain data parallelism.
Reference: [16] <author> P. Tamayo, J. P. Mesirov, and B. M. Boghosian, </author> <title> Parallel approaches to short range molecular dynamics simulations, </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: The force calculation typically dominates the computation time. In general, each particle may be influenced by every other particle. A naive implementation would require O (n 2 ) particle-particle interactions to determine the force on every particle. Approximation methods [2] [8] <ref> [16] </ref> trade some accuracy for speed by dividing the computation into two components: direct particle-particle interactions and far-field calculations. Direct interactions are calculated only for those particles lying within a specified cut-off distance r, as shown in Figure 9. That is, particle-particle interactions are spatially localized.
Reference: [17] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser, </author> <title> Active messages: A mechanism for integrated communication and computation, </title> <booktitle> in Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year> <month> 29 </month>
Reference-contexts: Asynchronous Message Streams (AMS), built on top of the MP ++ layer, is a new communication paradigm which combines abstractions from Active Messages <ref> [17] </ref> and the C ++ I/O stream library [12]. AMS extends Active Messages to the user-level and frees the programmer from many message passing details. In AMS, a processor sends a message "stream" to a user-defined function handler on a different processor.
References-found: 17

