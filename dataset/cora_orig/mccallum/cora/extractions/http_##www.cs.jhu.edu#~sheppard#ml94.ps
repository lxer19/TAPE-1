URL: http://www.cs.jhu.edu/~sheppard/ml94.ps
Refering-URL: http://www.cs.jhu.edu/~sheppard/pubs.html
Root-URL: 
Email: Email: lastname@cs.jhu.edu  
Title: Bootstrapping Memory-Based Learning with Genetic Algorithms  
Author: John W. Sheppard and Steven L. Salzberg 
Address: Baltimore, Maryland 21218  
Affiliation: Department of Computer Science The Johns Hopkins University  
Abstract: A number of special-purpose learning techniques have been developed in recent years to address the problem of learning with delayed reinforcement. This category includes numerous important control problems that arise in robotics, planning, and other areas. However, very few researchers have attempted to apply memory-based techniques to these tasks. We explore the performance of a common memory-based technique, nearest neighbor learning, on a non-trivial delayed reinforcement task. The task requires the machine to take the role of an airplane that must learn to evade pursuing missiles. The goal of learning is to find a relatively small number of exemplars that can be used to perform the task well. Because a prior study showed that nearest neighbor had great difficulty performing this task, we decided to use genetic algorithms as a bootstrapping method to provide the examples. We then edited the examples further to reduce the size of memory. Our new experiments demonstrate that the bootstrapping method resulted in a dramatic improvement in the performance of the memory-based approach, in terms of both overall accuracy and the size of memory. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., and Salzberg, S. </author> <year> 1993. </year> <title> Learning to catch: Applying nearest neighbor algorithms to dynamic control tasks. </title> <booktitle> In Proceedings of the Fourth International Workshop on AI and Statistics. </booktitle>
Reference-contexts: More recently, Moore and Atkeson (Moore & Atkeson 1993) developed an algorithm called "prioritized sweeping" in which "interesting" examples in a Q table are the focus of updating. In another study, Aha and Salzberg <ref> (Aha & Salzberg 1993) </ref> used nearest-neighbor techniques to train a simulated robot to catch a ball. In their study, they provided an agent that knew the correct behavior for the robot, and therefore provided corrected actions when the robot made a mistake.
Reference: <author> Atkeson, C. </author> <year> 1989. </year> <title> Using local models to control movement. </title> <booktitle> In Neural Information Systems Conference. </booktitle>
Reference-contexts: The idea of using memory-based methods for delayed reinforcement tasks has only very recently been considered by a small number of researchers. Atkeson <ref> (Atkeson 1989) </ref> employed a memory-based technique to train a robot arm to follow a prespecified trajectory. More recently, Moore and Atkeson (Moore & Atkeson 1993) developed an algorithm called "prioritized sweeping" in which "interesting" examples in a Q table are the focus of updating.
Reference: <author> Barto, A.; Sutton, R.; and Anderson, C. </author> <year> 1983. </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 13 </journal> <pages> 835-846. </pages>
Reference-contexts: He also applies case based reasoning in combination with reinforcement learning on the same domain (Ram & Santamaria 1993), both approaches yielding excellent performance. Some investigators are also exploring the use of teachers to improve reinforcement learning applications. For example, Barto's ACE/ASE <ref> (Barto, Sut-ton, & Anderson 1983) </ref> incorporates a teaching mechanism with one connectionist network providing reinforcement to another.
Reference: <author> Clouse, J., and Utgoff, P. </author> <year> 1992. </year> <title> A teaching method for reinforcement learning. </title> <booktitle> In Proceedings of the Machine Learning Conference. </booktitle>
Reference-contexts: Some investigators are also exploring the use of teachers to improve reinforcement learning applications. For example, Barto's ACE/ASE (Barto, Sut-ton, & Anderson 1983) incorporates a teaching mechanism with one connectionist network providing reinforcement to another. Clouse and Utgoff <ref> (Clouse & Utgoff 1992) </ref>, who also used ACE/ASE, monitor the overall progress of the learning agent, "reset" the eligibility traces of the two learning elements when the performance fails to improve, and then provide explicit actions from an external teacher to alter the direction of learning.
Reference: <author> Grefenstette, J.; Ramsey, C.; and Schultz, A. </author> <year> 1990. </year> <title> Learning sequential decision rules using simulation models and competition. </title> <booktitle> Machine Learning 5 </booktitle> <pages> 355-381. </pages>
Reference-contexts: One purpose of this study is to show how to overcome those difficulties and put nearest-neighbor on an equal footing with other methods. For our study, we considered a reinforcement learning problem that was posed, in simpler form, by Grefenstette et al. <ref> (Grefenstette, Ramsey, & Schultz 1990) </ref>. The original work showed that this task, known as evasive maneuvers, can be solved by a genetic algorithm (GA). In the basic problem, a guided missile is fired at an airplane, which must develop a strategy for evading the missile. <p> The Evasive Maneuvers Task Grefenstette et al. <ref> (Grefenstette, Ramsey, & Schultz 1990) </ref> introduced the evasive maneuvers task to demonstrate the ability of genetic algorithms to solve complex sequential decision making tasks. In their 2-D simulation, a single aircraft attempts to evade a single missile.
Reference: <author> Grefenstette, J. </author> <year> 1991. </year> <title> Lamarckian learning in multi-agent environments. </title> <booktitle> In Proceedings of the Fourth International Conference of Genetic Algorithms, </booktitle> <pages> 303-310. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This approach is typical in nearest-neighbor applications that rely on determining "good" actions before storing examples. Genetic algorithms have also been applied to perform delayed reinforcement problems. In addition to studying the evasive maneuvers task, Grefenstette <ref> (Grefenstette 1991) </ref> applied genetic algorithms to aerial dogfighting and target tracking. Ram applies genetic algorithms to learning navigation strategies for a robot in an obstacle field (Ram et al. 1994).
Reference: <author> Moore, A., and Atkeson, C. </author> <year> 1993. </year> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <booktitle> Machine Learning 13 </booktitle> <pages> 103-130. </pages>
Reference-contexts: The idea of using memory-based methods for delayed reinforcement tasks has only very recently been considered by a small number of researchers. Atkeson (Atkeson 1989) employed a memory-based technique to train a robot arm to follow a prespecified trajectory. More recently, Moore and Atkeson <ref> (Moore & Atkeson 1993) </ref> developed an algorithm called "prioritized sweeping" in which "interesting" examples in a Q table are the focus of updating. In another study, Aha and Salzberg (Aha & Salzberg 1993) used nearest-neighbor techniques to train a simulated robot to catch a ball.
Reference: <author> Ram, A., and Santamaria, J. C. </author> <year> 1993. </year> <title> Multistrategy learning in reactive control systems for autonomous robot navigation. </title> <journal> Informatica 17(4) </journal> <pages> 347-369. </pages>
Reference-contexts: Ram applies genetic algorithms to learning navigation strategies for a robot in an obstacle field (Ram et al. 1994). He also applies case based reasoning in combination with reinforcement learning on the same domain <ref> (Ram & Santamaria 1993) </ref>, both approaches yielding excellent performance. Some investigators are also exploring the use of teachers to improve reinforcement learning applications. For example, Barto's ACE/ASE (Barto, Sut-ton, & Anderson 1983) incorporates a teaching mechanism with one connectionist network providing reinforcement to another.
Reference: <author> Ram, A.; Arkin, R.; Boone, G.; and Pearce, M. </author> <year> 1994. </year> <title> Using genetic algorithms to learn reactive control parameters for autonomous robot navigation. Adaptive Behavior 2(3). </title>
Reference-contexts: Genetic algorithms have also been applied to perform delayed reinforcement problems. In addition to studying the evasive maneuvers task, Grefenstette (Grefenstette 1991) applied genetic algorithms to aerial dogfighting and target tracking. Ram applies genetic algorithms to learning navigation strategies for a robot in an obstacle field <ref> (Ram et al. 1994) </ref>. He also applies case based reasoning in combination with reinforcement learning on the same domain (Ram & Santamaria 1993), both approaches yielding excellent performance. Some investigators are also exploring the use of teachers to improve reinforcement learning applications.
Reference: <author> Ritter, G.; Woodruff, H.; Lowry, S.; and Isenhour, T. </author> <year> 1975. </year> <title> An algorithm for a selective nearest neighbor decision rule. </title> <journal> IEEE Transactions on Information Theory 21(6) </journal> <pages> 665-669. </pages>
Reference-contexts: Those points that are incorrectly classified are deleted from the example set. Tomek (Tomek 1975) modified this approach by taking a sample of the examples and classifying them with the remaining examples. Editing then proceeds as in Wil-son editing. Ritter et al. <ref> (Ritter et al. 1975) </ref> developed another editing method, which differs from Wilson in that points that are correctly classified are discarded.
Reference: <author> Salzberg, S.; Delcher, A.; Heath, D.; and Kasif, S. </author> <year> 1991. </year> <title> Learning with a helpful teacher. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 705-711. </pages> <address> Sydney, Australia: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: It might be possible with careful editing to reduce the size of memory even further. This question is related to theoretical work by Salzberg et al. <ref> (Salzberg et al. 1991) </ref> that studies the question of how to find a minimal-size training set through the use of a "helpful teacher", which explicitly provides very good examples. We note that when nearest neighbor began, its performance exceeded that of its teacher (the genetic algorithm).
Reference: <author> Sheppard, J., and Salzberg, S. </author> <year> 1993. </year> <title> Sequential decision making: An empirical analysis of three learning algorithms. </title> <type> Technical Report JHU-93/02, </type> <institution> Dept. of Computer Science, Johns Hopkins University, Balti-more, Maryland. </institution>
Reference-contexts: In our modified problem, two guided missiles are fired at the airplane. In a preliminary study comparing nearest-neighbor (NN), GAs, and Q-learning, we found that NN was by far the worst method in its performance on this problem <ref> (Sheppard & Salzberg 1993) </ref>. As a result, we sought to develop an approach that would improve the overall performance of nearest neighbor on this task. We found that one idea was key to our success: the use of an already-trained GA to generate examples. <p> Even after a successful evasion, though, we cannot be sure that the action at every time step was the correct one. To illustrate the problems that k-NN has with the evasive maneuvering task, we briefly describe some findings of our earlier study <ref> (Sheppard & Salzberg 1993) </ref>. At first, the nearest-neighbor learner generated actions randomly until the aircraft evaded the missiles for a complete engagement. The corresponding state-action pairs for that engagement were then stored. Once some examples were stored, k-NN used its memory to guide its actions. <p> In fact, we quickly found that NN had difficulty achieving a level of performance above 45%. This indicated the two missile problem is significantly more difficult for our approach to learn. The Genetic Algorithm For details of our GA implementation, see <ref> (Sheppard & Salzberg 1993) </ref>. We show the results of the GA experiments in Figure 2. As with NN, the GA performs very well when evading one missile. In fact, it is able to achieve near perfect performance after 15,000 engagements and very good performance (above 90%) after only 5,000 engagements.
Reference: <author> Sutton, R. </author> <year> 1988. </year> <title> Learning to predict by methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44. </pages>
Reference-contexts: While memory-based learning is not generally considered to be a reinforcement learning technique, it is an elegantly simple algorithm and exhibits some marked similarities to the reinforcement learning method known as Q-learning <ref> (Sutton 1988) </ref>. However, as we show below, nearest-neighbor has inherent difficulties with reinforcement learning problems. One purpose of this study is to show how to overcome those difficulties and put nearest-neighbor on an equal footing with other methods.
Reference: <author> Tomek, I. </author> <year> 1975. </year> <title> An experiment with the edited nearest-neighbor rule. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 6(6) </journal> <pages> 448-452. </pages>
Reference-contexts: Wilson's algorithm was to use each point in the example set as a point to be classified and then classify the point with k-NN using the remaining examples. Those points that are incorrectly classified are deleted from the example set. Tomek <ref> (Tomek 1975) </ref> modified this approach by taking a sample of the examples and classifying them with the remaining examples. Editing then proceeds as in Wil-son editing. Ritter et al. (Ritter et al. 1975) developed another editing method, which differs from Wilson in that points that are correctly classified are discarded.
Reference: <author> Wilson, D. </author> <year> 1972. </year> <title> Asymptotic properties of nearest neighbor rules using edited data. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 2(3) </journal> <pages> 408-421. </pages>
Reference-contexts: We therefore modified an existing editing algorithm for our problem. We call the resulting system GABED for GA Bootstrapping EDited nearest neighbor. Early work by Wilson <ref> (Wilson 1972) </ref> showed that examples could be removed from a set used for classification, and that this simple editing could further improve classification accuracy.
References-found: 15

