URL: http://www.cs.uni-bonn.de/III/lehre/seminare/ServiceRobotic/WS97/GESTURE/chip-gesture-recognition.ps.gz
Refering-URL: http://www.cs.uni-bonn.de/III/lehre/seminare/ServiceRobotic/WS97/
Root-URL: http://cs.uni-bonn.de
Email: kahn@cs.uchicago.edu, swain@cs.uchicago.edu, peterp@cs.uchicago.edu, and firby@cs.uchicago.edu  
Title: Gesture Recognition Using the Perseus Architecture  
Author: Roger E. Kahn, Michael J. Swain, Peter N. Prokopowicz, and R. James Firby 
Address: 1100 East 58th Street Chicago, IL 60637  
Affiliation: Department of Computer Science University of Chicago  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. L. Barron, D. J. Fleet, S. S. Beauchemin, and T. A. Burkitt. </author> <title> Performance of optical flow techniques. </title> <address> CVPR, </address> <year> 1992. </year>
Reference-contexts: feature maps are computed based on the parameters specified by the ORs and markers. 5 Object Representations The pointing task requires representations for various objects including the person and the item pointed Map Description Parameters Intensity Greyscale image Intensity range, Resolution Edge Thresholded Sobel Sobel threshold, Resolution Motion Normal flow <ref> [1] </ref> Edge, Motion sensitivity Disparity Stereo disparity [10] Disparity range, Sub-pixel accuracy, Resolution Color Color histogram backprojection [15] Color space, Histogram, Resolution to. The information represented for these objects can be classified into two categories: data describing the object and methods for examining the scene.
Reference: [2] <author> D. Chapman. </author> <title> Vision, Instruction, and Action. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: We present an overview of each component, later sections describe them in detail. A reactive execution system [4] interfaces to Perseus via visual routines <ref> [17, 2] </ref>. These are the top level structures in Perseus, addressing complex visual tasks like waiting for a person to enter the scene or finding the area a person points to. The higher level system specifies which objects are important to the task by passing parameters to visual routines. <p> An important part of the pointing task is tracking; the person's head and hands are tracked to recognize when the pointing gesture occurs. When an OR needs to track something it instantiates a marker <ref> [2] </ref> and provides it with a tracking method. Visual routines and ORs query the marker to find its location. Segmentation is important part of visually understanding objects. For instance, to find the person's hands we analyze their segmentation. <p> These properties include the regions height, width, aspect ratio, edge density, and intensity [5]. The locate method places a marker on the region parameterized with a tracking method that re-locates the object and checks that the above properties remain consistent. 6 Markers Markers <ref> [2] </ref> track an object or a subpart of an object. They are created by ORs and used by ORs and visual routines when an object's location is needed. When a marker is created, it is instantiated with a method for tracking the object.
Reference: [3] <author> T. Darrell, P. Maes, B. Blumberg, and A. Pentland. </author> <title> A novel environment for situated vision and behavior. Workshop On Visual Behaviors: </title> <address> CVPR, </address> <year> 1994. </year>
Reference-contexts: When Perseus found the wrong object or reported the person as pointing at nothing, human observers were also unable to determine what was pointed at. The object pointed to is successfully found 91% of the time. 11 Related work Gesture recognition has received much attention recently. The ALIVE system <ref> [3] </ref> provides a "magic-mirror" that allows the user to see himself in a simulated environment with artificial agents. ALIVE represents people as a segmentation found via background subtraction. This representation is being extended in the Pfinder project [19] to model people as connected sets of Gaussian blobs.
Reference: [4] <author> R. J. Firby. </author> <title> Adaptive Execution in Complex Dynamic Worlds. </title> <type> PhD thesis, </type> <institution> Yale, </institution> <year> 1989. </year>
Reference-contexts: Furthermore, reuse is a major issue in the design of Perseus. Information about the environment and task is explicitly represented so it can easily be re-used in tasks other than pointing. A clean interface to Perseus is provided for symbolic higher level systems like the RAP reactive execution system <ref> [4] </ref>. In this paper we describe Perseus in detail and show how it is used to locate objects pointed to by people. 2 The Pointing Task The task being addressed is recognizing when people point and finding the object pointed to. <p> We present an overview of each component, later sections describe them in detail. A reactive execution system <ref> [4] </ref> interfaces to Perseus via visual routines [17, 2]. These are the top level structures in Perseus, addressing complex visual tasks like waiting for a person to enter the scene or finding the area a person points to. <p> Symbolic systems call visual routines with these symbols. This interface is essential for Perseus be used by a symbolic planning and execution systems like the RAP system <ref> [4] </ref>. To create ORs the higher level system, visual routines, and other ORs send a message to the LTVM asking it to instantiate an OR of a specified type.
Reference: [5] <author> R. J. Firby, R. E. Kahn, P. N. Prokopowicz, and M. J. Swain. </author> <title> Collecting trash: A test of purposive vision. </title> <booktitle> Workshop on Vision for Robots, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: This is accomplished by locating small closed regions in the edge map. Once an object is located, certain properties of the region are measured and stored in the OR. These properties include the regions height, width, aspect ratio, edge density, and intensity <ref> [5] </ref>. The locate method places a marker on the region parameterized with a tracking method that re-locates the object and checks that the above properties remain consistent. 6 Markers Markers [2] track an object or a subpart of an object. <p> The location of the head and pointing hand are returned. 10 Experiments We have successfully found objects pointed to by numerous naive users in varying environments. The objects found include soda cans of various brands, paper and plastic cups of differing colors, and crumpled pieces of paper <ref> [5] </ref>. People using the system varied in race, sex, clothing, and height. Subjects were told to put an object on the ground. They were asked to stand anywhere in the field of view and point at it, as long as it was not between themselves and the robot.
Reference: [6] <author> R. J. Firby, P. N. Prokopowicz, M. J. Swain, R. E. Kahn, and D. Franklin. </author> <title> Programming chip for the ijcai-95 robot competition. </title> <journal> AI Magazine, </journal> <month> Spring </month> <year> 1996. </year>
Reference-contexts: One of the most frequently used and expressively powerful gestures is pointing. It is far easier and more accurate to point to an object than give a verbal description of its location. To produce a more efficient, accurate, and natural human-machine interface we use the Perseus architecture <ref> [11, 6] </ref> to interpret the pointing gesture. Perseus uses a variety of techniques to reliably solve this complex visual problem in non-engineered worlds. Knowledge about the task and environment is used at all stages of processing to best interpret the scene for the current situation.
Reference: [7] <author> W. T. Freeman and C. D. Weissman. </author> <title> Television control by hand gestures. </title> <type> Technical Report TR-94-24, </type> <institution> Mitsubishi Electronic Research Laboratories, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The background OR uses the intensity feature map to represent permanent objects in the scene. This is accomplished by histogramming the intensity at each point over time. Any point in the current frame with the same intensity as the mode of its corresponding histogram is counted as background <ref> [7] </ref>. This OR is only useful when the robot is stationary. The floor OR [8] segments the floor as each point in the edge feature map below the lowest edge in the scene. The lights OR segments the lights as each point in the intensity feature map above a threshold.
Reference: [8] <author> I. Horswill. </author> <title> Specialization of Perceptual Processes. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1993. </year>
Reference-contexts: This is accomplished by histogramming the intensity at each point over time. Any point in the current frame with the same intensity as the mode of its corresponding histogram is counted as background [7]. This OR is only useful when the robot is stationary. The floor OR <ref> [8] </ref> segments the floor as each point in the edge feature map below the lowest edge in the scene. The lights OR segments the lights as each point in the intensity feature map above a threshold. Each of these segmentations can be performed at low resolution (64 fi 60).
Reference: [9] <author> D. P. Huttenlocher and W. J. Rucklidge. </author> <title> A multi-resolution technique for comparing images using the haussdorf distance. </title> <type> Technical Report CUCS TR 92-1321, </type> <institution> Department of Computer Science Cornell University, </institution> <year> 1992. </year>
Reference-contexts: More sophisticated human-robot interactions such as handing objects to the robot are being investigated. In addition to increasing the number of gestures Perseus can recognize, the number of ORs is being increased so it can identify more than just small isolated objects. We currently have Haussdorf <ref> [9] </ref> based ORs for recognizing trash bins, doors and signs. Other useful ORs may recognize other robots, chairs, and desks. 13 Conclusion The Perseus system is a purposive visual architecture that has been used to recognize the pointing gesture.
Reference: [10] <author> K. Ikeuchi, H. K. Nishihara, B. K. P. Horn, P. Sobalvarro, and S. Nagata. </author> <title> Determining grasp configurations using photometric stereo and the prism binocular stereo system. </title> <journal> IJRR, </journal> <volume> 5(1), </volume> <year> 1986. </year>
Reference-contexts: parameters specified by the ORs and markers. 5 Object Representations The pointing task requires representations for various objects including the person and the item pointed Map Description Parameters Intensity Greyscale image Intensity range, Resolution Edge Thresholded Sobel Sobel threshold, Resolution Motion Normal flow [1] Edge, Motion sensitivity Disparity Stereo disparity <ref> [10] </ref> Disparity range, Sub-pixel accuracy, Resolution Color Color histogram backprojection [15] Color space, Histogram, Resolution to. The information represented for these objects can be classified into two categories: data describing the object and methods for examining the scene. Perseus encapsulates this knowledge with object representations (ORs).
Reference: [11] <author> R. E. Kahn and M. J. Swain. </author> <title> Understanding people pointing: The perseus system. </title> <address> ISCV, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: One of the most frequently used and expressively powerful gestures is pointing. It is far easier and more accurate to point to an object than give a verbal description of its location. To produce a more efficient, accurate, and natural human-machine interface we use the Perseus architecture <ref> [11, 6] </ref> to interpret the pointing gesture. Perseus uses a variety of techniques to reliably solve this complex visual problem in non-engineered worlds. Knowledge about the task and environment is used at all stages of processing to best interpret the scene for the current situation.
Reference: [12] <author> D. Kortenkamp, E. Huber, and R. P. Bonasso. </author> <title> Recognizing and interpreting gestures on a mobile robot. </title> <publisher> AAAI, </publisher> <year> 1996. </year>
Reference: [13] <author> P. N. Prokopowicz, R. E. Kahn, R. J. Firby, and M. J. Swain. Gargoyle: </author> <title> Context-sensitive active vision for mobile robots. </title> <publisher> AAAI, </publisher> <year> 1996. </year>
Reference-contexts: Perseus encapsulates this knowledge with object representations (ORs). Visual routines use ORs to understand objects in the world required in a task by calling their methods and accessing their data. When a method is invoked, it produces a set of visual operators that interpret the scene <ref> [13] </ref> and store results in the OR's data. Since the methods modify the data in an OR, ORs are not static representations; the data they contain changes over time.
Reference: [14] <author> P. N. Prokopowicz, M. J. Swain, and R. E. Kahn. </author> <title> Task and environment-sensitive tracking. Workshop On Visual Behaviors: </title> <address> CVPR, </address> <year> 1994. </year>
Reference-contexts: For instance, knowledge of how far a person is from the cameras affects the disparities of interest. By focusing only on the relevant depths, the disparity map can be found more quickly, and at higher resolution. To improve the usefulness of feature maps, ORs and markers fine tune them <ref> [14] </ref> based on the system's goals and knowledge by specifying a set of parameters describing the feature values of interest.
Reference: [15] <author> M. J. Swain and D. H. Ballard. </author> <title> Color indexing. </title> <journal> IJCV, </journal> <volume> 7, </volume> <year> 1991. </year>
Reference-contexts: This parameterization not only increases the map's sensitivity to certain feature values; it also makes some feature maps possible that do not make sense without parameterization such as the color histogram backprojection <ref> [15] </ref> map. In this map features are points whose colors signify the existence of a particular object of interest. This map cannot be computed unless it is parameterized with these colors. The color map is used to better locate a person after the color of their clothes is known. <p> The pointing task requires representations for various objects including the person and the item pointed Map Description Parameters Intensity Greyscale image Intensity range, Resolution Edge Thresholded Sobel Sobel threshold, Resolution Motion Normal flow [1] Edge, Motion sensitivity Disparity Stereo disparity [10] Disparity range, Sub-pixel accuracy, Resolution Color Color histogram backprojection <ref> [15] </ref> Color space, Histogram, Resolution to. The information represented for these objects can be classified into two categories: data describing the object and methods for examining the scene. Perseus encapsulates this knowledge with object representations (ORs).
Reference: [16] <author> A. Treisman and G. Gelade. </author> <title> A feature-integration theory of attention. </title> <journal> Cognitive Psychology, </journal> <volume> 12, </volume> <year> 1980. </year>
Reference-contexts: These few types of features are used by the many ORs and markers in this task. To efficiently share information about features, the lowest level of the Perseus system is a set of feature maps <ref> [16] </ref>. These maps are retinotopic interpretations of the scene that describe the location of features.
Reference: [17] <author> S. Ullman. </author> <title> Visual routines. </title> <journal> Cognition, </journal> <volume> 18, </volume> <year> 1984. </year>
Reference-contexts: We present an overview of each component, later sections describe them in detail. A reactive execution system [4] interfaces to Perseus via visual routines <ref> [17, 2] </ref>. These are the top level structures in Perseus, addressing complex visual tasks like waiting for a person to enter the scene or finding the area a person points to. The higher level system specifies which objects are important to the task by passing parameters to visual routines. <p> As each OR is specialized the description of which real-world object it represents becomes more precisely defined. 9 Visual Routines The interface between Perseus and the higher level system is a library of visual routines <ref> [17] </ref>. Visual routines use ORs, markers, and other visual routines to perform visual tasks that understand the scene with respect to multiple objects. These tasks include waiting for the pointing gesture and selecting an area to search for an object in.
Reference: [18] <author> A. Wilson and A. Bobick. </author> <title> Configuration states for the representation and recognition of gesture. </title> <booktitle> International Workshop on Automatic Face and Gesture Recognition, </booktitle> <year> 1995. </year>
Reference-contexts: Perseus does not detect pointing toward the camera. Perseus, this method does not use contextual information to simplify the task. Wilson and Bobick <ref> [18] </ref> model gestures as view-point dependent motions of the body. Since gestures are intended to communicate information, they assume people will actively make them easy to understand by orienting them in a standard way. A Hidden Markov Model is used to learn the gesture.
Reference: [19] <author> C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pfinder: </author> <title> Real-time tracking of the human body. Media Lab 353, </title> <publisher> MIT, </publisher> <year> 1995. </year>
Reference-contexts: A color histogram is taken of the hand and stored in the person OR for further specialization. Currently this histogram is not used, but we plan to use it with the color feature map to track the hand when it passes over the body <ref> [19] </ref>. 5.2.3 The person find hands method The segmentation does not always find both hands. If either hand has not been found or the marker on a hand is lost, this method can be called to try to locate them. <p> The ALIVE system [3] provides a "magic-mirror" that allows the user to see himself in a simulated environment with artificial agents. ALIVE represents people as a segmentation found via background subtraction. This representation is being extended in the Pfinder project <ref> [19] </ref> to model people as connected sets of Gaussian blobs. Pfinder relies on a number of constraints imposed on the environment such as a static the background and the user must stand in a predetermined "star-fish" position during initialization.
References-found: 19

