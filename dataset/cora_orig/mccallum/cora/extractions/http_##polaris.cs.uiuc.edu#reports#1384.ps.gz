URL: http://polaris.cs.uiuc.edu/reports/1384.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: ADAPTIVE AND INTEGRATED DATA CACHE PREFETCHING FOR SHARED-MEMORY MULTIPROCESSORS  
Author: BY EDWARD H. GORNISH 
Degree: THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Date: 1989  
Address: 1986 M.S., University of Illinois,  1995 Urbana, Illinois  
Affiliation: B.S., Massachusetts Institute of Technology,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Kendall Square Research Corporation, KSR1 Principles of Operation, </institution> <year> 1991. </year>
Reference-contexts: This is particularly true as the gap between processor and memory speeds continues to grow. For example, in a 32 processor KSR1, a global access takes 126 cycles, and in a system with more than 32 processors, latencies can be as high as 600 cycles <ref> [1] </ref>. To fully utilize such systems, it is essential to use the memory hierarchy effectively to reduce memory latency. We address this issue in this dissertation. Placing caches between memory modules and processors is a first step towards solving the data access penalty problem. <p> Chi suggests how a larger lookahead can be used, but does not fully describe such an implementation. In addition, no performance results are presented. We note that this prefetching method was published simultaneously with our integrated scheme [39]. 2.3.4 Commercial Systems Several commercial systems have prefetching capabilities. The KSR1 <ref> [1] </ref> supports software prefetching of 128-byte subpage units into the second level cache. Kahhaleh [40] uses prefetching on the KSR1 to effectively speed up common operations found in iterative sparse solvers. Many current microprocessors support a software prefetching instruction.
Reference: [2] <author> D. Kroft, </author> <title> "Lockup-free instruction fetch/prefetch cache organization," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 81-87, </pages> <year> 1981. </year>
Reference-contexts: This is especially true in multiprocessor systems, since parallelizing a program often results in a loss of data locality. Memory latencies also tend to be larger in multiprocessor systems than in uniprocessor systems. 1 One method for improving cache performance is to make the caches lockup-free <ref> [2] </ref> (i.e., allow for multiple outstanding memory requests 1 ). Write buffers [4] are small FIFO queues that hold outstanding write requests while the processor proceeds with its execution. <p> This write policy, in conjunction with the software coherence policy described above, allows multiple processors to write to different words in the same cache line, during a parallel task, while still insuring correctness. The caches are lockup-free <ref> [2] </ref> (i.e., they can support multiple outstanding requests 2 ). When a request misses in the cache, a cache line is reserved and the request is sent to the fetch unit. Each line has an additional bit called the pending bit. <p> These registers are similar to the miss information/hold registers described in <ref> [2] </ref>. If a cache hit, at time t, triggers a prefetch, the cache is checked, at time t + 1, to see if it already contains the desired data. If so, the prefetch is discarded.
Reference: [3] <author> T.-F. Chen, </author> <title> Data Prefetching for High-Performance Processors. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science and Engineering, University of Washington, </institution> <year> 1993. </year>
Reference-contexts: Data prefetching|moving data from a higher level of the memory hierarchy to a lower level of the memory hierarchy before the data is actually accessed|is a method that can 1 The term lockup-free cache is used differently than its use in <ref> [3] </ref>. 2 help reduce these remaining memory access stalls. Data prefetching is the focus of this dissertation. <p> When the line is reserved, the pending bit is turned on. Therefore, additional requests for the same cache line are not duplicated. Information for these additional requests, such as which word in 2 The term lockup-free cache is used differently than its use in <ref> [3] </ref>. 29 the cache line was requested and which register the word should be forwarded to, is stored in registers in the fetch unit. These registers are similar to the miss information/hold registers described in [2].
Reference: [4] <author> A. J. Smith, </author> <title> "Cache memories," </title> <journal> Computing Surveys, </journal> <volume> vol. 14, </volume> <pages> pp. 473-530, </pages> <month> Sept. </month> <year> 1982. </year>
Reference-contexts: Memory latencies also tend to be larger in multiprocessor systems than in uniprocessor systems. 1 One method for improving cache performance is to make the caches lockup-free [2] (i.e., allow for multiple outstanding memory requests 1 ). Write buffers <ref> [4] </ref> are small FIFO queues that hold outstanding write requests while the processor proceeds with its execution. Supporting multiple outstanding reads is more difficult, since the processor needs to make sure that required data is actually available when needed. <p> We describe these software prefetching schemes in Section 2.3.2. 2.1.3 Hardware Prefetching The data elements in a loop such as the one in Example 2.1 can also be prefetched using hardware prefetching. Some of the simpler hardware prefetching schemes that have been proposed are one block lookahead (OBL) schemes <ref> [4] </ref>. In an OBL scheme line l + 1 is considered for prefetching upon the reference to line l.
Reference: [5] <author> M. Wolfe, </author> <title> "Iteration space tiling for memory hierarchies," </title> <booktitle> in Proceedings of the 3rd SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pp. 357-361, </pages> <year> 1987. </year>
Reference-contexts: Locality transformations first detect potential spatial and temporal locality in a given program. They then transform the program in an attempt to exploit the potential locality for a given architecture. Examples of locality optimizations include loop tiling <ref> [5, 6] </ref> and loop interchange [7, 6]. In this dissertation, we use PARAFRASE [8, 9, 10, 11] to perform locality transformations. Even with the optimizations described above, however, memory access stall time can still account for a large fraction of a program's execution time.
Reference: [6] <author> M. E. Wolf and M. Lam, </author> <title> "A data locality optimizing problem," </title> <booktitle> in Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Locality transformations first detect potential spatial and temporal locality in a given program. They then transform the program in an attempt to exploit the potential locality for a given architecture. Examples of locality optimizations include loop tiling <ref> [5, 6] </ref> and loop interchange [7, 6]. In this dissertation, we use PARAFRASE [8, 9, 10, 11] to perform locality transformations. Even with the optimizations described above, however, memory access stall time can still account for a large fraction of a program's execution time. <p> Locality transformations first detect potential spatial and temporal locality in a given program. They then transform the program in an attempt to exploit the potential locality for a given architecture. Examples of locality optimizations include loop tiling [5, 6] and loop interchange <ref> [7, 6] </ref>. In this dissertation, we use PARAFRASE [8, 9, 10, 11] to perform locality transformations. Even with the optimizations described above, however, memory access stall time can still account for a large fraction of a program's execution time. <p> Their scheme is based on determining which accesses are likely to be misses and therefore need to be prefetched. Code transformations then isolate these 19 misses in order to eliminate redundant prefetches. Their method uses the mathematical formulation of data reuse and locality developed in <ref> [6] </ref>. Selvidge [27] presents a prefetching scheme similar to Mowry, Lam and Gupta's. The main difference is that his scheme is based on a model where a reference is statically designated as either generating hits or misses. <p> For example, consider two references (using the notation shown in Example 4.6) a (~-F 1 + ~c 1 ) and a (~-F 2 + ~c 2 ). Furthermore, assume that F 1 = F 2 . Wolf refers to such references as uniformly generated references <ref> [6] </ref>. Bodin et al. use the concept of uniformly generated references for estimating reference windows [55]. Wolf shows that such references access the same data if: 3 2 We assume a no-fetch on write cache policy.
Reference: [7] <author> M. J. Wolfe, </author> <title> Optimizing Compilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: Locality transformations first detect potential spatial and temporal locality in a given program. They then transform the program in an attempt to exploit the potential locality for a given architecture. Examples of locality optimizations include loop tiling [5, 6] and loop interchange <ref> [7, 6] </ref>. In this dissertation, we use PARAFRASE [8, 9, 10, 11] to perform locality transformations. Even with the optimizations described above, however, memory access stall time can still account for a large fraction of a program's execution time.
Reference: [8] <author> D. J. Kuck, R. H. Kuhn, B. Leasure, and M. Wolfe, </author> <title> "The structure of an advanced vectorizer for pipelined processors," </title> <booktitle> in Fourth International Computer Software and Applications Conference, </booktitle> <month> Oct. </month> <year> 1980. </year>
Reference-contexts: They then transform the program in an attempt to exploit the potential locality for a given architecture. Examples of locality optimizations include loop tiling [5, 6] and loop interchange [7, 6]. In this dissertation, we use PARAFRASE <ref> [8, 9, 10, 11] </ref> to perform locality transformations. Even with the optimizations described above, however, memory access stall time can still account for a large fraction of a program's execution time. <p> The Sun 22 Viking [44] supports the always prefetch scheme (described in Section 2.1.3). The PA--RISC supports an integrated approach, where stride based prefetching is combined with a post-increment load instruction [45]. 2.4 Methodology Our experimental methodology has two main components: compilation We use the PARAFRASE <ref> [8, 9, 10, 11] </ref> compiler to produce pseudo assembly code. simulation We simulate the execution of this code with a shared-memory multiprocessor simulator that we developed. In the sections below, we describe these two components in more detail.
Reference: [9] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe, </author> <title> "Dependence graphs and compiler optimizations," </title> <booktitle> in Proceedings of the ACM Symposium on Principles of Programming Languages, </booktitle> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: They then transform the program in an attempt to exploit the potential locality for a given architecture. Examples of locality optimizations include loop tiling [5, 6] and loop interchange [7, 6]. In this dissertation, we use PARAFRASE <ref> [8, 9, 10, 11] </ref> to perform locality transformations. Even with the optimizations described above, however, memory access stall time can still account for a large fraction of a program's execution time. <p> The Sun 22 Viking [44] supports the always prefetch scheme (described in Section 2.1.3). The PA--RISC supports an integrated approach, where stride based prefetching is combined with a post-increment load instruction [45]. 2.4 Methodology Our experimental methodology has two main components: compilation We use the PARAFRASE <ref> [8, 9, 10, 11] </ref> compiler to produce pseudo assembly code. simulation We simulate the execution of this code with a shared-memory multiprocessor simulator that we developed. In the sections below, we describe these two components in more detail.
Reference: [10] <author> D. J. Kuck, A. Sameh, R. Cytron, A. Veidenbaum, C. Polychronopoulos, G. Lee, T. McDaniel, B. Leasure, C. Beckman, J. Davies, and C. Kruskal, </author> <title> "The effects of program restructuring, algorithm change, and architecture choice on program performance," </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: They then transform the program in an attempt to exploit the potential locality for a given architecture. Examples of locality optimizations include loop tiling [5, 6] and loop interchange [7, 6]. In this dissertation, we use PARAFRASE <ref> [8, 9, 10, 11] </ref> to perform locality transformations. Even with the optimizations described above, however, memory access stall time can still account for a large fraction of a program's execution time. <p> The Sun 22 Viking [44] supports the always prefetch scheme (described in Section 2.1.3). The PA--RISC supports an integrated approach, where stride based prefetching is combined with a post-increment load instruction [45]. 2.4 Methodology Our experimental methodology has two main components: compilation We use the PARAFRASE <ref> [8, 9, 10, 11] </ref> compiler to produce pseudo assembly code. simulation We simulate the execution of this code with a shared-memory multiprocessor simulator that we developed. In the sections below, we describe these two components in more detail.
Reference: [11] <author> D. A. Padua, D. J. Kuck, and D. H. Lawrie, </author> <title> "High-speed multiprocessors and compilation techniques," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-24, </volume> <pages> pp. 763-776, </pages> <month> Sept. </month> <year> 1980. </year>
Reference-contexts: They then transform the program in an attempt to exploit the potential locality for a given architecture. Examples of locality optimizations include loop tiling [5, 6] and loop interchange [7, 6]. In this dissertation, we use PARAFRASE <ref> [8, 9, 10, 11] </ref> to perform locality transformations. Even with the optimizations described above, however, memory access stall time can still account for a large fraction of a program's execution time. <p> The Sun 22 Viking [44] supports the always prefetch scheme (described in Section 2.1.3). The PA--RISC supports an integrated approach, where stride based prefetching is combined with a post-increment load instruction [45]. 2.4 Methodology Our experimental methodology has two main components: compilation We use the PARAFRASE <ref> [8, 9, 10, 11] </ref> compiler to produce pseudo assembly code. simulation We simulate the execution of this code with a shared-memory multiprocessor simulator that we developed. In the sections below, we describe these two components in more detail.
Reference: [12] <author> E. H. Gornish, </author> <title> "Compile time analysis for data prefetching," </title> <type> Master's thesis, </type> <institution> Uni--versity of Illinois at Urbana-Champaign, </institution> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: In particular, we study the benefits of prefetching data into the first level cache, thereby eliminating cache misses; however our techniques are also applicable to data prefetching in other levels of the memory hierarchy. We have previously investigated the potential of data prefetching by studying the following questions <ref> [12] </ref>: * How far ahead in the execution of a program can a datum be prefetched? * What is the potential improvement from data prefetching? Our results showed that data prefetching has tremendous potential for speeding up a program's execution. <p> In both cases, successive cache lines are prefetched only if previous cache lines have been accessed. When considering a loop body that consists of more than one basic block, it is necessary to determine which statements' executions are actually control dependent. We have previously shown <ref> [12] </ref> that a statement will be executed during every iteration of a loop, if: 1. it is in a basic block that dominates the loop's enddo statement 2. the loop has no branches into or out from the loop.
Reference: [13] <author> T. C. Mowry, M. S. Lam, and A. Gupta, </author> <title> "Design and evaluation of a compiler algorithm for prefetching," </title> <booktitle> in Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 62-73, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Many software prefetching schemes have been proposed that are similar to the above approach. The most extensive such study was done by Mowry, Lam and Gupta <ref> [13] </ref>. We describe these software prefetching schemes in Section 2.3.2. 2.1.3 Hardware Prefetching The data elements in a loop such as the one in Example 2.1 can also be prefetched using hardware prefetching. <p> Most of these schemes are similar and only differ in their level of sophistication and in where prefetched data is placed. The most extensive software prefetching study is by Mowry, Lam and Gupta <ref> [13] </ref>. They develop an extensive compiler algorithm for prefetching data into both levels of a two-level cache hierarchy. Their scheme is based on determining which accesses are likely to be misses and therefore need to be prefetched. Code transformations then isolate these 19 misses in order to eliminate redundant prefetches. <p> In Example 4.8, a (i + 1) and a (i) have group-temporal reuse. Example 4.8 do i = 1, n = a (i) enddo Mowry refers to a (i + 1) as the leading reference, since it is the reference that will actually suffer the cache misses <ref> [13] </ref>. In cases such as the one in Example 4.8, only the leading reference is a candidate for forming an access stream. Similar analysis is used in software prefetching schemes [13]. 4.3.2.2 Computation of Stride and End Stride To determine whether a candidate reference forms a CSAS and the extent of <p> a (i + 1) as the leading reference, since it is the reference that will actually suffer the cache misses <ref> [13] </ref>. In cases such as the one in Example 4.8, only the leading reference is a candidate for forming an access stream. Similar analysis is used in software prefetching schemes [13]. 4.3.2.2 Computation of Stride and End Stride To determine whether a candidate reference forms a CSAS and the extent of the CSAS, the compiler needs to compute both the stride of a reference and what we refer to as the end stride (ES). <p> Problems can arise in compile time prediction of cache usage in non-fully-associative caches, due to the potential for mapping conflicts. In general, the relative addresses of different data is unknown at compile time. To offset this problem, we take the approach described in <ref> [13] </ref>. We set the variable CacheSize in Figure 4.5 to be a fraction of the actual cache size. In [13], this "effective" cache size is set to one sixteenth of the actual cache size. <p> In general, the relative addresses of different data is unknown at compile time. To offset this problem, we take the approach described in <ref> [13] </ref>. We set the variable CacheSize in Figure 4.5 to be a fraction of the actual cache size. In [13], this "effective" cache size is set to one sixteenth of the actual cache size. <p> We use the following approximation to obtain results for software prefetching. We start with the integrated prefetching results, and we compute the number of additional software prefetches that would need to be prefetched in a software prefetching scheme. 111 We then refer to the software prefetching study described in <ref> [13] </ref>. We take the average prefetching overhead computed in this study, and use this to compute the additional overhead for software prefetching relative to integrated prefetching. In Section 4.2, we presented two possible implementations for the hardware support for integrated prefetching, the stream table and the instruction table.
Reference: [14] <author> F. Dahlgren, M. Dubois, and P. Stenstrom, </author> <title> "Fixed and adaptive sequential prefetch-ing in shared memory multiprocessors," </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <year> 1993. </year>
Reference-contexts: This is clarified by Example 2.5. Example 2.5 do i = 1, n = a (i) 1 The term prefetching degree is used differently from its use in <ref> [14] </ref>. 13 enddo do j = 1, n = c (j) enddo Assume that the execution time for one iteration of loop i is T I and that the execution time for one iteration of loop j is 2 fl T I . <p> We divide the related schemes into four categories: 1. hardware prefetching 2. software prefetching 3. integrated prefetching 4. prefetching in commercial systems. 2.3.1 Hardware Prefetching We first discuss hardware prefetching schemes for scalar machines. We then examine hardware schemes for vector machines. 2.3.1.1 Scalar Machines Dahlgren, Dubois and Stenstrom <ref> [14] </ref> present a multiprocessor hardware prefetching method based on the prefetch on miss paradigm (discussed in Section 2.1.3). They propose a hardware implementation that can detect if prefetched data is actually being used. <p> However, this study relies on instruction lookahead which is more difficult and costly for memory latencies common today. In summary, none of the previous hardware prefetching schemes meet the prefetching adaptivity goals that we previously specified. Three of the schemes <ref> [14, 17, 25] </ref> address the issue of adaptivity, but only [17] bases adaptivity on the performance of the memory subsystem. However, this latter scheme can miss prefetching opportunities and generate unnecessary prefetches, due to inaccurate branch prediction. Furthermore, this scheme does not adapt on a reference by reference basis. <p> In our scheme, this is not a major problem. The memory issue rate increases only briefly when the prefetching degree is increased, after which it returns to its previous value. In contrast, consider the method presented in <ref> [14] </ref>. (We discussed this approach in Section 2.3). In this scheme, the size of the block that is prefetched on a cache miss is increased and decreased in a uniform manner, based on the measured usefulness of prefetched data.
Reference: [15] <author> P. Stenstrom, </author> <title> "A survey of cache coherence schemes for multiprocessors," </title> <booktitle> IEEE Computer, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: In addition, many systems have both global and local memory, where access times between the two types of memory differ significantly. In a hardware coherent multiprocessor, global accesses can have different latencies depending on whether a clean or dirty block is being accessed <ref> [15] </ref>. Turner [16] showed that even in a uniform memory access (UMA) system, there can be tremendous memory latency variability within the same program for different accesses. Therefore, it is advantageous to enable the hardware to adapt the prefetching degree based on network and memory characteristics.
Reference: [16] <author> S. W. Turner, </author> <title> "Shared memory and interconnection network performance for vector multiprocessors," </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: In addition, many systems have both global and local memory, where access times between the two types of memory differ significantly. In a hardware coherent multiprocessor, global accesses can have different latencies depending on whether a clean or dirty block is being accessed [15]. Turner <ref> [16] </ref> showed that even in a uniform memory access (UMA) system, there can be tremendous memory latency variability within the same program for different accesses. Therefore, it is advantageous to enable the hardware to adapt the prefetching degree based on network and memory characteristics.
Reference: [17] <author> J.-L. Baer and T.-F. Chen, </author> <title> "An effective on-chip preloading scheme to reduce data access penalty," </title> <booktitle> in Supercomputing, </booktitle> <pages> pp. 176-186, </pages> <year> 1991. </year>
Reference-contexts: Instead, they adapt based on the characteristics of the program as a whole. Finally, their adaptivity is based on whether or not prefetching is useful, not on the rate that prefetched data is being delivered and consumed. Baer and Chen <ref> [17, 18] </ref> describe a multiprocessor prefetching method based on branch prediction and reference prediction. Branch prediction is used to predict what instructions will be executed, and reference prediction is used to predict what data will be accessed by a given load instruction. <p> Palacharla and Kessler [23] extend the work of Jouppi. They evaluate stream buffers as an off-chip replacement for second level cache. They also show how to enhance the stream buffer operation by eliminating useless prefetches and extending prefetching to include LCSASs. The prefetching for LCSASs differs from that in <ref> [17, 19, 20, 21] </ref> in not using load instruction addresses to predict what data to prefetch. Varma and Sinha [24] present a method for prefetching SCSASs by providing two additional copies of the cache tag-RAM. <p> However, this study relies on instruction lookahead which is more difficult and costly for memory latencies common today. In summary, none of the previous hardware prefetching schemes meet the prefetching adaptivity goals that we previously specified. Three of the schemes <ref> [14, 17, 25] </ref> address the issue of adaptivity, but only [17] bases adaptivity on the performance of the memory subsystem. However, this latter scheme can miss prefetching opportunities and generate unnecessary prefetches, due to inaccurate branch prediction. Furthermore, this scheme does not adapt on a reference by reference basis. <p> In summary, none of the previous hardware prefetching schemes meet the prefetching adaptivity goals that we previously specified. Three of the schemes [14, 17, 25] address the issue of adaptivity, but only <ref> [17] </ref> bases adaptivity on the performance of the memory subsystem. However, this latter scheme can miss prefetching opportunities and generate unnecessary prefetches, due to inaccurate branch prediction. Furthermore, this scheme does not adapt on a reference by reference basis. <p> The actions occurring on read operations would then be similar to the previous scheme. We refer to this latter table as an instruction table, to distinguish it from the previously discussed stream table. This latter approach is used in the prefetching schemes discussed in <ref> [17, 19, 20, 21] </ref>. Similar to the situation with the stream table, a small number of entries for the instruction table is probably sufficient to handle most programs. There are several advantages and disadvantages for the instruction table implementation relative to the stream table implementation. <p> In this section, we show how to modify 69 the hardware adaptivity scheme so that the stride is calculated and prefetching occurs in an LCSAS. The scheme presented here is similar to those presented in <ref> [17, 19, 20, 21] </ref>. We begin with the tagged (2) prefetching scheme that was presented in the previous section and present the necessary additions. The major difference is that stride prefetching requires the instruction table implementation as opposed to the stream table implementation.
Reference: [18] <author> T.-F. Chen and J.-L. Baer, </author> <title> "Reducing memory latency via non-blocking and prefetching caches," </title> <booktitle> in Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 51-61, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Instead, they adapt based on the characteristics of the program as a whole. Finally, their adaptivity is based on whether or not prefetching is useful, not on the rate that prefetched data is being delivered and consumed. Baer and Chen <ref> [17, 18] </ref> describe a multiprocessor prefetching method based on branch prediction and reference prediction. Branch prediction is used to predict what instructions will be executed, and reference prediction is used to predict what data will be accessed by a given load instruction.
Reference: [19] <author> I. Sklenar, </author> <title> "Prefetch unit for vector operations on scalar computers," Computer Architecture News, </title> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: In addition, following each incorrect branch prediction the LAPC needs to reset and start building up the distance between itself and the main program counter. During this period, prefetches might not be issued early enough. 17 Sklenar <ref> [19] </ref>, Fu, Patel and Janssens [20] and Jegou and Temam [21] present schemes similar to the one described above. However, their schemes do not include branch prediction; rather, the schemes simply prefetch data one iteration ahead. Therefore, these schemes are very limited in the amount of adaptivity they can perform. <p> Palacharla and Kessler [23] extend the work of Jouppi. They evaluate stream buffers as an off-chip replacement for second level cache. They also show how to enhance the stream buffer operation by eliminating useless prefetches and extending prefetching to include LCSASs. The prefetching for LCSASs differs from that in <ref> [17, 19, 20, 21] </ref> in not using load instruction addresses to predict what data to prefetch. Varma and Sinha [24] present a method for prefetching SCSASs by providing two additional copies of the cache tag-RAM. <p> The actions occurring on read operations would then be similar to the previous scheme. We refer to this latter table as an instruction table, to distinguish it from the previously discussed stream table. This latter approach is used in the prefetching schemes discussed in <ref> [17, 19, 20, 21] </ref>. Similar to the situation with the stream table, a small number of entries for the instruction table is probably sufficient to handle most programs. There are several advantages and disadvantages for the instruction table implementation relative to the stream table implementation. <p> In this section, we show how to modify 69 the hardware adaptivity scheme so that the stride is calculated and prefetching occurs in an LCSAS. The scheme presented here is similar to those presented in <ref> [17, 19, 20, 21] </ref>. We begin with the tagged (2) prefetching scheme that was presented in the previous section and present the necessary additions. The major difference is that stride prefetching requires the instruction table implementation as opposed to the stream table implementation.
Reference: [20] <author> J. W. Fu, J. H. Patel, and B. L. Janssens, </author> <title> "Stride directed prefetching in scalar processors," </title> <booktitle> in International Symposium on Microarchitecture, </booktitle> <pages> pp. 102-110, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: In addition, following each incorrect branch prediction the LAPC needs to reset and start building up the distance between itself and the main program counter. During this period, prefetches might not be issued early enough. 17 Sklenar [19], Fu, Patel and Janssens <ref> [20] </ref> and Jegou and Temam [21] present schemes similar to the one described above. However, their schemes do not include branch prediction; rather, the schemes simply prefetch data one iteration ahead. Therefore, these schemes are very limited in the amount of adaptivity they can perform. <p> Palacharla and Kessler [23] extend the work of Jouppi. They evaluate stream buffers as an off-chip replacement for second level cache. They also show how to enhance the stream buffer operation by eliminating useless prefetches and extending prefetching to include LCSASs. The prefetching for LCSASs differs from that in <ref> [17, 19, 20, 21] </ref> in not using load instruction addresses to predict what data to prefetch. Varma and Sinha [24] present a method for prefetching SCSASs by providing two additional copies of the cache tag-RAM. <p> The actions occurring on read operations would then be similar to the previous scheme. We refer to this latter table as an instruction table, to distinguish it from the previously discussed stream table. This latter approach is used in the prefetching schemes discussed in <ref> [17, 19, 20, 21] </ref>. Similar to the situation with the stream table, a small number of entries for the instruction table is probably sufficient to handle most programs. There are several advantages and disadvantages for the instruction table implementation relative to the stream table implementation. <p> In this section, we show how to modify 69 the hardware adaptivity scheme so that the stride is calculated and prefetching occurs in an LCSAS. The scheme presented here is similar to those presented in <ref> [17, 19, 20, 21] </ref>. We begin with the tagged (2) prefetching scheme that was presented in the previous section and present the necessary additions. The major difference is that stride prefetching requires the instruction table implementation as opposed to the stream table implementation.
Reference: [21] <author> Y. Jegou and O. Temam, </author> <title> "Speculative prefetching," </title> <booktitle> in International Conference on Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: In addition, following each incorrect branch prediction the LAPC needs to reset and start building up the distance between itself and the main program counter. During this period, prefetches might not be issued early enough. 17 Sklenar [19], Fu, Patel and Janssens [20] and Jegou and Temam <ref> [21] </ref> present schemes similar to the one described above. However, their schemes do not include branch prediction; rather, the schemes simply prefetch data one iteration ahead. Therefore, these schemes are very limited in the amount of adaptivity they can perform. <p> Palacharla and Kessler [23] extend the work of Jouppi. They evaluate stream buffers as an off-chip replacement for second level cache. They also show how to enhance the stream buffer operation by eliminating useless prefetches and extending prefetching to include LCSASs. The prefetching for LCSASs differs from that in <ref> [17, 19, 20, 21] </ref> in not using load instruction addresses to predict what data to prefetch. Varma and Sinha [24] present a method for prefetching SCSASs by providing two additional copies of the cache tag-RAM. <p> The actions occurring on read operations would then be similar to the previous scheme. We refer to this latter table as an instruction table, to distinguish it from the previously discussed stream table. This latter approach is used in the prefetching schemes discussed in <ref> [17, 19, 20, 21] </ref>. Similar to the situation with the stream table, a small number of entries for the instruction table is probably sufficient to handle most programs. There are several advantages and disadvantages for the instruction table implementation relative to the stream table implementation. <p> In this section, we show how to modify 69 the hardware adaptivity scheme so that the stride is calculated and prefetching occurs in an LCSAS. The scheme presented here is similar to those presented in <ref> [17, 19, 20, 21] </ref>. We begin with the tagged (2) prefetching scheme that was presented in the previous section and present the necessary additions. The major difference is that stride prefetching requires the instruction table implementation as opposed to the stream table implementation.
Reference: [22] <author> N. P. Jouppi, </author> <title> "Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 364-373, </pages> <year> 1990. </year>
Reference-contexts: However, their schemes do not include branch prediction; rather, the schemes simply prefetch data one iteration ahead. Therefore, these schemes are very limited in the amount of adaptivity they can perform. Jouppi <ref> [22] </ref> presents a scheme based on multiple prefetch buffers that prefetches data for SCSASs. This scheme uses a fixed prefetching degree; therefore, it is not an adaptive scheme. Palacharla and Kessler [23] extend the work of Jouppi. They evaluate stream buffers as an off-chip replacement for second level cache.
Reference: [23] <author> S. Palacharla and R. E. Kessler, </author> <title> "Evaluating stream buffers as a secondary cache replacement," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: Therefore, these schemes are very limited in the amount of adaptivity they can perform. Jouppi [22] presents a scheme based on multiple prefetch buffers that prefetches data for SCSASs. This scheme uses a fixed prefetching degree; therefore, it is not an adaptive scheme. Palacharla and Kessler <ref> [23] </ref> extend the work of Jouppi. They evaluate stream buffers as an off-chip replacement for second level cache. They also show how to enhance the stream buffer operation by eliminating useless prefetches and extending prefetching to include LCSASs.
Reference: [24] <author> A. Varma and G. Sinha, </author> <title> "A class of prefetch schemes for on-chip data caches," </title> <type> tech. rep., </type> <institution> Computer Engineering Dept., University of Santa Cruz, </institution> <year> 1992. </year>
Reference-contexts: They also show how to enhance the stream buffer operation by eliminating useless prefetches and extending prefetching to include LCSASs. The prefetching for LCSASs differs from that in [17, 19, 20, 21] in not using load instruction addresses to predict what data to prefetch. Varma and Sinha <ref> [24] </ref> present a method for prefetching SCSASs by providing two additional copies of the cache tag-RAM. When an access is made to line l, the additional tag-RAM copies allow comparisons to be made for lines l + 1 and l 1.
Reference: [25] <author> J. W. C. Fu and J. Patel, </author> <title> "Data prefetching in multiprocessor vector cache memories," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 54-63, </pages> <year> 1991. </year> <month> 146 </month>
Reference-contexts: If a sequential access pattern is detected, prefetching is initiated. This approach, however, can only support a prefetching degree of 1. 2.3.1.2 Vector Machines Fu and Patel <ref> [25] </ref> present a multiprocessor bus-based vector prefetching scheme that attempts to differentiate between SCSASs and LCSASs. This gives the scheme the ability to adapt on a reference by reference basis. However, the scheme does not adapt to the 18 performance of the memory subsystem. <p> However, this study relies on instruction lookahead which is more difficult and costly for memory latencies common today. In summary, none of the previous hardware prefetching schemes meet the prefetching adaptivity goals that we previously specified. Three of the schemes <ref> [14, 17, 25] </ref> address the issue of adaptivity, but only [17] bases adaptivity on the performance of the memory subsystem. However, this latter scheme can miss prefetching opportunities and generate unnecessary prefetches, due to inaccurate branch prediction. Furthermore, this scheme does not adapt on a reference by reference basis.
Reference: [26] <author> R. Lee, P.-C. Yew, and D. H. Lawrie, </author> <title> "Data Prefetching in Shared Memory Multi--processors," </title> <booktitle> Proceedings of 1987 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <pages> pp. 28-31, </pages> <month> August 19-, </month> <year> 1987. </year>
Reference-contexts: However, the scheme does not adapt to the 18 performance of the memory subsystem. Their scheme is also based on the prefetch on miss paradigm; therefore, it will miss some prefetching opportunities. Lee, Yew and Lawrie <ref> [26] </ref> present a multiprocessor vector prefetching method that uses instruction lookahead. This scheme achieves promising results for a latency of 20 cycles. However, this study relies on instruction lookahead which is more difficult and costly for memory latencies common today.
Reference: [27] <author> C. W. Selvidge, </author> <title> Compilation-Based Prefetching for Memory Latency Tolerance. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Their scheme is based on determining which accesses are likely to be misses and therefore need to be prefetched. Code transformations then isolate these 19 misses in order to eliminate redundant prefetches. Their method uses the mathematical formulation of data reuse and locality developed in [6]. Selvidge <ref> [27] </ref> presents a prefetching scheme similar to Mowry, Lam and Gupta's. The main difference is that his scheme is based on a model where a reference is statically designated as either generating hits or misses. Prefetches will then be generated for each instance of a reference designated as a miss. <p> Determining at compile time what data to prefetch becomes more complicated in the context of non-scientific ap 138 plications. For example, many types of pointer-based codes could benefit from data prefetching. One approach to prefetching in such codes was presented by Selvidge <ref> [27] </ref>. This technique combines data profiling [60] for predicting what references generate misses, with trace scheduling [61] for enlarging the segment of code in which a prefetch can be scheduled.
Reference: [28] <author> D. Callahan, K. Kennedy, and A. Porterfield, </author> <title> "Software prefetching," </title> <booktitle> in Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 40-52, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: By prefetching for each instance of reference designated as a miss, many unnecessary prefetches might be generated. Callahan, Kennedy and Porterfield <ref> [28] </ref> also present a compiler algorithm for inserting prefetches. Their method only prefetches data in loops, one iteration in advance; therefore, latencies might not be fully eliminated. Their method tries to eliminate some unnecessary prefetches; however, similar to Selvidge's scheme, it does not take cache line reuse into account. <p> Therefore the compiler in turn does not know what elements need to be prefetched by the software mechanism. A simple approach in this case could 5 This technique is adapted from similar approaches used by Porterfield <ref> [28] </ref> and Mowry [57] in software prefetching schemes. 102 be to generate prefetch instructions for each cache line of array a, outside of the if statement. This would guarantee that those elements of a that are actually accessed are also prefetched. Porterfield [28] takes the approach that prefetch instructions should be <p> is adapted from similar approaches used by Porterfield <ref> [28] </ref> and Mowry [57] in software prefetching schemes. 102 be to generate prefetch instructions for each cache line of array a, outside of the if statement. This would guarantee that those elements of a that are actually accessed are also prefetched. Porterfield [28] takes the approach that prefetch instructions should be placed inside the if statement. This is based on the assumption that if iteration i follows a certain path through the loop, then iteration i + 1 will follow the same path.
Reference: [29] <author> W. Y. Chen, R. A. Bringmann, S. A. Mahlke, R. E. Hank, and J. E. Sicolo, </author> <title> "An efficient architecture for loop based data preloading," </title> <booktitle> in International Symposium on Microarchitecture, </booktitle> <pages> pp. 92-100, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Their method tries to eliminate some unnecessary prefetches; however, similar to Selvidge's scheme, it does not take cache line reuse into account. There have been several schemes that have explored a separate storage for prefetched data. Chen et al. <ref> [29] </ref> proposed prefetching data into a preload buffer that acts as a producer/consumer queue. Chen et al. [30] and Klaiber and Levy [31] proposed prefetching into a separate prefetch buffer in order to reduce cache contention. Several studies look at software prefetching in multiprocessors.
Reference: [30] <author> W. Y. Chen, S. A. Mahlke, P. P. Chang, and W. mei W. Hwu, </author> <title> "Data access mi-croarchitectures for superscalar processors with compiler-assisted data prefetching," </title> <booktitle> in International Symposium on Microarchitecture, </booktitle> <pages> pp. 69-73, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: There have been several schemes that have explored a separate storage for prefetched data. Chen et al. [29] proposed prefetching data into a preload buffer that acts as a producer/consumer queue. Chen et al. <ref> [30] </ref> and Klaiber and Levy [31] proposed prefetching into a separate prefetch buffer in order to reduce cache contention. Several studies look at software prefetching in multiprocessors. Mowry and Gupta [32] study prefetching in the DASH multiprocessor.
Reference: [31] <author> A. C. Klaiber and H. M. Levy, </author> <title> "An architecture for software-controlled data prefething," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 43-53, </pages> <year> 1991. </year>
Reference-contexts: There have been several schemes that have explored a separate storage for prefetched data. Chen et al. [29] proposed prefetching data into a preload buffer that acts as a producer/consumer queue. Chen et al. [30] and Klaiber and Levy <ref> [31] </ref> proposed prefetching into a separate prefetch buffer in order to reduce cache contention. Several studies look at software prefetching in multiprocessors. Mowry and Gupta [32] study prefetching in the DASH multiprocessor. They do not actually develop a compiler algorithm for generating prefetches; rather, they insert them by hand.
Reference: [32] <author> T. Mowry and A. Gupta, </author> <title> "Tolerating latency through software-controlled prefetching in shared-memory multiprocessors," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1991. </year>
Reference-contexts: Chen et al. [30] and Klaiber and Levy [31] proposed prefetching into a separate prefetch buffer in order to reduce cache contention. Several studies look at software prefetching in multiprocessors. Mowry and Gupta <ref> [32] </ref> study prefetching in the DASH multiprocessor. They do not actually develop a compiler algorithm for generating prefetches; rather, they insert them by hand. They investigate prefetching into the remote access (secondary) cache and prefetching into the primary 20 cache. The latter produces the best results.
Reference: [33] <author> D. M. Tullsen and S. J. Eggers, </author> <title> "Limitattion of cache prefetching on a bus-based multiprocessor," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: They do not actually develop a compiler algorithm for generating prefetches; rather, they insert them by hand. They investigate prefetching into the remote access (secondary) cache and prefetching into the primary 20 cache. The latter produces the best results. Tullsen and Eggers <ref> [33] </ref> show that prefetch--ing in a multiprocessor with a limited bandwidth is not very effective. They simulated prefetching on a bus-based system and achieve only limited performance improvement. Poulsen and Yew [34] compare prefetching to data forwarding.
Reference: [34] <author> D. K. Poulsen and P.-C. Yew, </author> <title> "Data prefetching and data forwarding in shared memory multiprocessors," </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: The latter produces the best results. Tullsen and Eggers [33] show that prefetch--ing in a multiprocessor with a limited bandwidth is not very effective. They simulated prefetching on a bus-based system and achieve only limited performance improvement. Poulsen and Yew <ref> [34] </ref> compare prefetching to data forwarding. This technique allows a processor to forward data to other processors, when performing a write operation. They show that data forwarding helps reduce latencies in programs containing many communication-related misses.
Reference: [35] <author> E. H. Gornish, E. D. Granston, and A. V. Veidenbaum, </author> <title> "Compiler-directed data prefetching in multiprocessors with memory hierarchies," </title> <booktitle> in International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Poulsen and Yew [34] compare prefetching to data forwarding. This technique allows a processor to forward data to other processors, when performing a write operation. They show that data forwarding helps reduce latencies in programs containing many communication-related misses. Gornish, Granston and Veidenbaum <ref> [35] </ref> develop a compiler algorithm to determine the earliest time in a program's execution when data can be prefetched.
Reference: [36] <author> T.-F. Chen and J.-L. Baer, </author> <title> "A performance study of software and hardware data prefetching schemes," </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <pages> pp. 223-232, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: They show that prefetching has tremendous potential in a multiprocessor system. 2.3.3 Integrated Prefetching In addition to the integrated prefetching scheme we present in Chapter 4, we have found, in the literature, three other prefetching schemes that could be classified as an integrated method. Chen and Baer <ref> [36] </ref> describe a scheme where software prefetching is used to prefetch large chunks of data from global memory to the second level cache. Chen's original hardware algorithm ([17] described above) is then used to prefetch data from the second level cache to the first level cache.
Reference: [37] <author> O. Temam and Y. Jegou, </author> <title> "Using virtual lines to enhance locality exploitation," </title> <booktitle> in International Conference on Supercomputing, </booktitle> <pages> pp. 344-353, </pages> <year> 1994. </year>
Reference-contexts: Also, they just briefly touch on this notion of an integrated scheme, and they do not go into much detail. This scheme differs from our scheme in that we combine software and hardware prefetching at the same level of the memory hierarchy. Temam and Jegou <ref> [37] </ref> present a scheme similar to our integrated prefetching method. They first present the virtual line scheme, where several physical lines can be fetched on a data access. They describe how the virtual line scheme can be directed by software instructions.
Reference: [38] <author> C.-H. Chi, </author> <title> "Compiler optimization technique for data cache prefetching using a small cam array," </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <year> 1994. </year> <month> 147 </month>
Reference-contexts: The key difference between their scheme and ours is that, in their scheme, the prefetching degree is fixed by the degree of interleaving in the cache. Another scheme similar to our integrated prefetching method is presented by Chi in <ref> [38] </ref>. This scheme is less flexible than ours because it cannot handle NCSASs, and it only uses a lookahead of one iteration. Chi suggests how a larger lookahead can be used, but does not fully describe such an implementation. In addition, no performance results are presented.
Reference: [39] <author> E. H. Gornish and A. Veidenbaum, </author> <title> "An integrated hardware/software data prefetch--ing scheme for shared-memory multiprocessors," </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Chi suggests how a larger lookahead can be used, but does not fully describe such an implementation. In addition, no performance results are presented. We note that this prefetching method was published simultaneously with our integrated scheme <ref> [39] </ref>. 2.3.4 Commercial Systems Several commercial systems have prefetching capabilities. The KSR1 [1] supports software prefetching of 128-byte subpage units into the second level cache. Kahhaleh [40] uses prefetching on the KSR1 to effectively speed up common operations found in iterative sparse solvers.
Reference: [40] <author> B. Kahhaleh, </author> <title> "Analysis of memory latency factors and their impact on KSR1 mpp performance," </title> <type> tech. rep., </type> <institution> Department of Electrical Engineering and Computer Science, </institution> <year> 1993. </year>
Reference-contexts: In addition, no performance results are presented. We note that this prefetching method was published simultaneously with our integrated scheme [39]. 2.3.4 Commercial Systems Several commercial systems have prefetching capabilities. The KSR1 [1] supports software prefetching of 128-byte subpage units into the second level cache. Kahhaleh <ref> [40] </ref> uses prefetching on the KSR1 to effectively speed up common operations found in iterative sparse solvers. Many current microprocessors support a software prefetching instruction. These include the Alpha [41], the PowerPC [42] and the Sparc Version 9 [43].
Reference: [41] <institution> Digital Equipment Organization, Alpha Architecture Handbook, </institution> <year> 1992. </year>
Reference-contexts: The KSR1 [1] supports software prefetching of 128-byte subpage units into the second level cache. Kahhaleh [40] uses prefetching on the KSR1 to effectively speed up common operations found in iterative sparse solvers. Many current microprocessors support a software prefetching instruction. These include the Alpha <ref> [41] </ref>, the PowerPC [42] and the Sparc Version 9 [43]. The Sun 22 Viking [44] supports the always prefetch scheme (described in Section 2.1.3).
Reference: [42] <author> J. Shipnes and M. Phillip, </author> <title> "A modular approach to motorola powerpc compilers," </title> <journal> Communications of the ACM, </journal> <volume> vol. 37, </volume> <pages> pp. 56-63, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The KSR1 [1] supports software prefetching of 128-byte subpage units into the second level cache. Kahhaleh [40] uses prefetching on the KSR1 to effectively speed up common operations found in iterative sparse solvers. Many current microprocessors support a software prefetching instruction. These include the Alpha [41], the PowerPC <ref> [42] </ref> and the Sparc Version 9 [43]. The Sun 22 Viking [44] supports the always prefetch scheme (described in Section 2.1.3).
Reference: [43] <author> D. L. Weaver and T. </author> <title> Germond, The SPARC Architecture Manual. SPARC International, </title> <publisher> Inc. </publisher>
Reference-contexts: Kahhaleh [40] uses prefetching on the KSR1 to effectively speed up common operations found in iterative sparse solvers. Many current microprocessors support a software prefetching instruction. These include the Alpha [41], the PowerPC [42] and the Sparc Version 9 <ref> [43] </ref>. The Sun 22 Viking [44] supports the always prefetch scheme (described in Section 2.1.3).
Reference: [44] <author> Sun Microsystems Team, </author> <title> "A three-million-transistor microprocessor," </title> <booktitle> in Proceedings of ISSC, </booktitle> <year> 1992. </year>
Reference-contexts: Kahhaleh [40] uses prefetching on the KSR1 to effectively speed up common operations found in iterative sparse solvers. Many current microprocessors support a software prefetching instruction. These include the Alpha [41], the PowerPC [42] and the Sparc Version 9 [43]. The Sun 22 Viking <ref> [44] </ref> supports the always prefetch scheme (described in Section 2.1.3).
Reference: [45] <author> L. Gwennap, </author> <title> "Pa-7200 enables inexpensive mp systems," </title> <type> Microprocessor Report, </type> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: These include the Alpha [41], the PowerPC [42] and the Sparc Version 9 [43]. The Sun 22 Viking [44] supports the always prefetch scheme (described in Section 2.1.3). The PA--RISC supports an integrated approach, where stride based prefetching is combined with a post-increment load instruction <ref> [45] </ref>. 2.4 Methodology Our experimental methodology has two main components: compilation We use the PARAFRASE [8, 9, 10, 11] compiler to produce pseudo assembly code. simulation We simulate the execution of this code with a shared-memory multiprocessor simulator that we developed.
Reference: [46] <author> E. Granston, </author> <title> Strplp/Codegn User's Manual. </title> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research & Development, </institution> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: In the next stage, prefetch statements are generated based on the framework developed in Chapter 4. Prefetch generation is optional, allowing us to compare the relative performance of code with and without prefetching. PARAFRASE then generates pseudo-assembly level output based on the method described in <ref> [46] </ref>. A peephole optimizer applies some basic optimizations to the pseudo-code, and the optimized code is fed into a simulator that we have developed. Table 2.1 lists the benchmarks we use in this study.
Reference: [47] <author> H. Cheong, </author> <title> Compiler-Directed Cache Coeherence Strategies for Large-Scaled Shared-Memory Multiprocessor Systems. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: All other instructions execute in one cycle. All instructions are assumed to hit in the instruction cache. 28 Operation # of cycles FADD 2 FSUB 2 FMUL 4 FDIV 12 Table 2.3: Timings for Floating-Point Operations A simple software mechanism is used to maintain cache coherence <ref> [47] </ref>. Each processor's cache is flushed at the end of a parallel task (i.e. when it executes a COEND statement) and the serial processor's cache is flushed at the end of a serial task (i.e. when it executes a COBEGIN instruction).
Reference: [48] <author> Y.-C. Chen and A. Veidenbaum, </author> <title> "Comparison and analysis of software and directory coherence schemes," </title> <booktitle> in Supercomputing, </booktitle> <pages> pp. 818-829, </pages> <year> 1991. </year>
Reference-contexts: Chen and Veidenbaum showed that simple software coherence schemes perform well on large scientific codes, similar to the ones used in this study <ref> [48] </ref>. The cache uses the write-through write-allocate no-fetch policy [49]. Each word in a cache line has a separate valid bit. An entire line is brought into the cache on a read miss, but only a single word is written to memory on a write access.
Reference: [49] <author> Y.-C. Chen, </author> <title> Cache Design and Performace in a Large-Scale Shared-Memory Multiprocessor System. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: Chen and Veidenbaum showed that simple software coherence schemes perform well on large scientific codes, similar to the ones used in this study [48]. The cache uses the write-through write-allocate no-fetch policy <ref> [49] </ref>. Each word in a cache line has a separate valid bit. An entire line is brought into the cache on a read miss, but only a single word is written to memory on a write access.
Reference: [50] <author> L. </author> <title> Geppert, </title> <journal> "The new contenders," IEEE Spectrum, </journal> <volume> vol. 30, </volume> <pages> pp. 20-25, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Most current commercial microprocessors exhibit some form of instruction level parallelism and have a specific set of rules for the combinations of instructions that may be issued during the same cycle <ref> [50] </ref>. Rather than model any one specific type of multiple-instruction-issue processor (e.g. superscalar, superpipelined, VLIW), we modify our simulator to model an ideal multiple-instruction-issue processor with no restrictions on what instructions can be issued together in the same cycle.
Reference: [51] <author> O. Temam and W. Jalby, </author> <title> "Characterizing the behavior of sparse algorithms on caches," </title> <booktitle> in Supercomputing, </booktitle> <year> 1992. </year>
Reference-contexts: While this prefetched line might be unnecessary, it is also possible that the NCSAS exhibits some degree of spatial locality, as evidenced by the multiple accesses to the same line. For example, Temam and Jalby show examples of spatial locality in sparse computations <ref> [51] </ref>.
Reference: [52] <author> M. E. Benitez and J. W. Davidson, </author> <title> "Code generation for streaming: an access/execute mechanism," </title> <booktitle> in Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991. </year>
Reference-contexts: It is this phase that determines what data will be prefetched by the hardware mechanism and how to generate prefetches for the remaining data. Previous work in access stream detection includes that of Benitez and Davidson <ref> [52] </ref>, who identify access streams for the purpose of code generation for access/execute architectures. However, they only address the case of a singly nested loop. In addition, their method takes place at assembly level, while our method is targeted at the source level.
Reference: [53] <author> S. A. Moyer, </author> <title> Access Ordering and Effective Memory Bandwidth. </title> <type> PhD thesis, </type> <institution> University of Virginia, </institution> <year> 1993. </year> <month> 148 </month>
Reference-contexts: However, they only address the case of a singly nested loop. In addition, their method takes place at assembly level, while our method is targeted at the source level. Moyer <ref> [53] </ref> uses access stream detection for optimizing the ordering of accesses to better utilize memory system resources. For example, he shows how access reordering can be beneficial when the memory system is constructed from page-mode DRAMs. Moyer does not actually show how to detect access streams.
Reference: [54] <author> A. V. Aho and J. D. Ullman, </author> <booktitle> Principles of Compiler Design. </booktitle> <address> Reading, MA: </address> <publisher> Addison--Wesley Publishing Company, </publisher> <year> 1977. </year>
Reference-contexts: If it can be determined at compile time that the data for an array reference will already be locally available then that reference is not a candidate for forming an access stream. For example, references that are not upwards exposed uses <ref> [54, page 491] </ref>, do not need to be prefetched. Example 4.7 illustrates such a case. We assume that the data used by r 2 will be available in registers, based on the assignment to a in r 1 2 .
Reference: [55] <author> F. Bodin, W. Jalby, D. Windheiser, and C. Eisenbeis, </author> <title> "A quantitative algorithm for data locality optimization," </title> <type> tech. rep., </type> <institution> IRISA, University of Rennes, France, </institution> <year> 1992. </year>
Reference-contexts: Furthermore, assume that F 1 = F 2 . Wolf refers to such references as uniformly generated references [6]. Bodin et al. use the concept of uniformly generated references for estimating reference windows <ref> [55] </ref>. Wolf shows that such references access the same data if: 3 2 We assume a no-fetch on write cache policy. <p> The algorithm in algorithms in <ref> [58, 59, 55] </ref>. In Figure 4.4, we designate u j as the upperbound of loop j. Since we assume normalized loops, u j is equal to the number of iterations in loop j. A running total is associated with each reference.
Reference: [56] <author> U. Banerjee, </author> <title> Dependence Analysis for Supercomputing. </title> <booktitle> The Kluwer International Series in Engineering and Computer Science, </booktitle> <address> Boston: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Wolf shows that such references access the same data if: 3 2 We assume a no-fetch on write cache policy. Had we assumed a fetch on write policy, prefetches could have taken place for writes, also. 3 This test is similar to the gcd test <ref> [56] </ref> 82 and refers to such references as having group-temporal reuse. Even if two references do not access the same data, they might access the same cache line.
Reference: [57] <author> T. C. Mowry, </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Systems Laboratory, Stanford, </institution> <address> CA 94305, </address> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Therefore the compiler in turn does not know what elements need to be prefetched by the software mechanism. A simple approach in this case could 5 This technique is adapted from similar approaches used by Porterfield [28] and Mowry <ref> [57] </ref> in software prefetching schemes. 102 be to generate prefetch instructions for each cache line of array a, outside of the if statement. This would guarantee that those elements of a that are actually accessed are also prefetched. <p> In staying consistent with our prefetch scheduling strategy, we set the value of T I , in Equation 4.4, equal to the execution time of the shortest path through the loop body. This is the same strategy used by Mowry <ref> [57] </ref> for software 103 prefetching. Mowry also shows an algorithm for computing the shortest path through a loop body. 4.3.4.4 Prefetch Buffer Size Constraints Recall from Section 2.4.2.2 that a processing node supports a maximum number of outstanding prefetch requests, designated M P . <p> Under a hardware coherence protocol, however, it is not always possible to predict coherence misses. This in turn makes it more difficult to decide what data needs to be prefetched. Mowry <ref> [57] </ref> takes the conservative approach that any loop containing explicit synchronization is not localized. Prefetching can also have an important impact on the false sharing problem. Accurate prefetching can help minimize the effect of false sharing, but prefetching too aggressively can potentially exacerbate the problem.
Reference: [58] <author> A. K. Porterfield, </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: The algorithm in algorithms in <ref> [58, 59, 55] </ref>. In Figure 4.4, we designate u j as the upperbound of loop j. Since we assume normalized loops, u j is equal to the number of iterations in loop j. A running total is associated with each reference.
Reference: [59] <author> J. Ferrante, V. Sarkar, and W. Thrash, </author> <title> "On estimating and enhancing cache effectiveness," </title> <booktitle> in Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: The algorithm in algorithms in <ref> [58, 59, 55] </ref>. In Figure 4.4, we designate u j as the upperbound of loop j. Since we assume normalized loops, u j is equal to the number of iterations in loop j. A running total is associated with each reference.
Reference: [60] <institution> MIPS Computer Systems, Inc., Mips Language Programmer's Guide, </institution> <year> 1986. </year>
Reference-contexts: Determining at compile time what data to prefetch becomes more complicated in the context of non-scientific ap 138 plications. For example, many types of pointer-based codes could benefit from data prefetching. One approach to prefetching in such codes was presented by Selvidge [27]. This technique combines data profiling <ref> [60] </ref> for predicting what references generate misses, with trace scheduling [61] for enlarging the segment of code in which a prefetch can be scheduled. The problem with this approach is that in order to accurately perform data profiling for a program, a reasonable size data set must be used.
Reference: [61] <author> J. Fisher, </author> <title> "Trace scheduling," </title> <journal> IEEE Transactions on Computers, </journal> <year> 1981. </year>
Reference-contexts: For example, many types of pointer-based codes could benefit from data prefetching. One approach to prefetching in such codes was presented by Selvidge [27]. This technique combines data profiling [60] for predicting what references generate misses, with trace scheduling <ref> [61] </ref> for enlarging the segment of code in which a prefetch can be scheduled. The problem with this approach is that in order to accurately perform data profiling for a program, a reasonable size data set must be used.
Reference: [62] <author> E. P. Markatos and T. J. LeBlanc, </author> <title> "Using processor affinity in loop scheduling on shared-memory multiprocessors," </title> <booktitle> in Supercomputing, </booktitle> <pages> pp. 104-113, </pages> <year> 1992. </year> <note> 149 VITA Edward H. Gornish was born on February 23, 1964 in Philadelphia, </note> <institution> Pennsylvania. He graduated from Akiba Hebrew Academy High School in 1982. He obtained his Bachelor of Science degree in Computer Science, from the Massachusetts Institute of Technology, </institution> <note> in 1986. In 1989, </note> <institution> he obtained his Master of Science degree from the University of Illinois. After completing the Doctor of Philosophy degree at the University of Illinois, he will join Hewlett-Packard in Cupertino, California. </institution> <month> 150 </month>
Reference-contexts: This could, in turn, affect the performance of prefetching. For example, our hardware adaptive scheme could spend a larger percentage of the time adapting the prefetching degree to the appropriate level. Affinity scheduling <ref> [62] </ref> is a technique that can alleviate this problem.
References-found: 62

