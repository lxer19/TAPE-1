URL: ftp://ftp.cs.brown.edu/pub/techreports/95/cs95-28.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-95-28.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Black, E., Jelinek, F., Lafferty, J., Magerman, D., Mercer, R. and Roukos, S. </author> <title> Towards history-based grammars: using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics. </booktitle> <year> 1993, </year> <pages> 31-37. </pages>
Reference-contexts: Our PCFG results are also in line with those reported in [15] though their method of reporting results do not allow them to be shown in a graph line that of Figure 7. The work most similar to our is that of Black et. al. <ref> [1] </ref> and Schabes and Waters [16]. These papers like ours are concerned with using lexical information together with probabilities on rules to improve parsing performance. <p> Schabes and Waters present a scheme related to Tree-adjoining Grammars, and the burden of their paper is primarily the description of the grammatical formalism and an n-cubed parsing algorithm. No experimental results are given. On the other hand, the the "History-based Grammar" (HBG) model presented by Black et. al. <ref> [1] </ref> does have related experimental results, so we will concentrate on this paper. The main difference between our model and HBG is relative complexity. <p> Smoothing over this number of conditioning events is done with a combination of decision trees and back-off methods. One extra complexity in HBG that we hope to include in subsequent versions of our model is word clustering. The figure of merit used in <ref> [1] </ref> is the percentage of parse trees identical to those in the test corpus except for pre-terminal labels. As such it is hard to directly compare their results and ours. Furthermore the technical manual corpus they used (restricted to the most common 3000 words) is different. <p> Note that the simpler model "No h 2 (c)" (i.e., the head of the grand-parent is not used) performs almost as well as the full model, and in general the degree of performance improvement decreases as we add more conditioning information, seemingly opposite to the results reported in <ref> [1] </ref>.
Reference: 2. <author> Brent, M. R. </author> <title> Automatic acquisition of subcategorization frames from untagged text. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics. </booktitle> <year> 1991, </year> <pages> 209-214. </pages>
Reference: 3. <author> Brill, E. </author> <title> Automatic grammar induction and parsing free text: a transformation-based approach. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics. </booktitle> <year> 1993, </year> <pages> 259-265. </pages>
Reference-contexts: In Figure 7 we plot accuracy vs average sentence length for four systems, our PCFG, the transformational parsing systems of <ref> [3] </ref>, the PCFG of [13], and, for completeness, the results for the complete model presented in this paper. It should be clear that the accuracy of our PCFG is in line with these systems. <p> Their sentences ranged in length from 7 to 17, rather than our maximum of 40. They 11 10 15 20 90 100 fi 4 4 2 2 4| Our complete model fi | Our PCFG 2 | Transformation Parser of <ref> [3] </ref> 3 | PCFG of [13] report that their pure PCFG gave a success rate of 59.8% and their complete model a success rate of 74.6%. Given their much stricter definition of success their results, both for the PCFG and for the enriched model, are better than ours.
Reference: 4. <author> Brown, P. F., Pietra, V. J. D., DeSouza, P. V., Lai, J. C. and Mercer, R. L. </author> <title> Class-based n-gram models of natural language. </title> <booktitle> Computational Linguistics 18 4 (1992), </booktitle> <pages> 467-479. </pages>
Reference: 5. <author> Carroll, G. and Charniak, E. </author> <title> Two experiments on learning probabilistic dependency grammars from corpora. </title> <booktitle> In Workshop Notes, Statistically-Based NLP Techniques. AAAI, </booktitle> <year> 1992, </year> <pages> 1-13. </pages>
Reference: 6. <author> Charniak, E. and Carroll, G. </author> <title> Context-Sensitive Statistics for Improved Grammatical Language Models. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <year> 1994. </year>
Reference: 7. <author> Dagan, I., Pereira, F. and Lee, L. </author> <title> Similarity-based estimation of word cooccurence probabilities. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics. </booktitle> <year> 1994, </year> <pages> 272-278. </pages>
Reference: 8. <author> Dunning, T. </author> <title> Accurate methods for the statistics of surprise and coincidence. </title> <booktitle> Computational Linguistics 121 (1993), </booktitle> <pages> 61-74. </pages>
Reference-contexts: Indeed, we pulled out rule/verb pairs where the probability of the rule given the verb was significantly higher than the prior on the rule. (The exact measure used here is that described in <ref> [8] </ref>. ) The results looked much like the studies directed at finding verb case-frames such as [2,12]. Figure 2 shows the four rules for "gave" and the seven for "tell" which differ most from the original PCFG probabilities for the rules.
Reference: 9. <author> Hindle, D. and Rooth, M. </author> <title> Structural ambiguity and lexical relations. </title> <booktitle> In Proceedings of the Association for Computational Linguistics. ACL, 1991, </booktitle> <volume> 229 - 236. </volume>
Reference-contexts: However, Equation 3 works for any method of calculating the parse probabilities, and if basing the probability on only the rules is insufficient then adding word information might improve things. One example of how adding word-level information can aid disambiguation is shown in <ref> [9] </ref>, which deals with prepositional phrase attachment in cases where the pp can attach to either a verb, or the direct object of the verb. <p> The most important step is conditioning on the head of the parent as well: p (h (c) j t (c); h (c)). It is this step that gives the prepositional-phrase attachment work of <ref> [9] </ref> its power. For example, should the pp "over rocks" in Figure 1 attached to "pulled" or "sleds?" Equation 4 applied to the pp in effect contrasts the two conditional probabilities p (over j pp; pulled) and p (over j pp; sleds). This is exactly what [9] does with its counts <p> prepositional-phrase attachment work of <ref> [9] </ref> its power. For example, should the pp "over rocks" in Figure 1 attached to "pulled" or "sleds?" Equation 4 applied to the pp in effect contrasts the two conditional probabilities p (over j pp; pulled) and p (over j pp; sleds). This is exactly what [9] does with its counts of how often a particular pp attaches to either the preceding noun or verb. <p> This latter figure represents at 41.1% reduction in the number of bracket-crossing errors when compared to the pure PCFG. We give the results for the model without conditioning on h 2 (c) because the prepositional-phrase attachment model of <ref> [9] </ref> does not use this information. Furthermore, (Hindle, personal communication) reports that adding this information actual made their performance worse! Our results paint a different picture. While the improvement shown by adding h 2 (c) may not be statistically significant, at least things did not worsen.
Reference: 10. <author> Jones, M. A. and Eisner, J. M. </author> <title> A probabilistic parser and its applictions. </title> <booktitle> In AAAI-92 Worshop on Statistically-Based NLP Techniques. AAAI, </booktitle> <year> 1992, </year> <pages> 20-27. </pages>
Reference: 11. <author> Magerman, D. M. and Weir, C. </author> <title> Efficiency, robustness and accuracy in Picky chart parsing. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics. </booktitle> <year> 1992, </year> <pages> 40-47. </pages>
Reference: 12. <author> Manning, C. D. </author> <title> Automatic acquisition of a large subcategorization dictionary from corpora. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics. </booktitle> <year> 1993, </year> <pages> 235-242. </pages>
Reference: 13. <author> Pereira, F. and Schabes, Y. </author> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In 27th Annual Meeting of the Association for Com-putaitonal Linguistics. ACL, </booktitle> <year> 1992, </year> <pages> 128-135. </pages>
Reference-contexts: In Figure 7 we plot accuracy vs average sentence length for four systems, our PCFG, the transformational parsing systems of [3], the PCFG of <ref> [13] </ref>, and, for completeness, the results for the complete model presented in this paper. It should be clear that the accuracy of our PCFG is in line with these systems. <p> Their sentences ranged in length from 7 to 17, rather than our maximum of 40. They 11 10 15 20 90 100 fi 4 4 2 2 4| Our complete model fi | Our PCFG 2 | Transformation Parser of [3] 3 | PCFG of <ref> [13] </ref> report that their pure PCFG gave a success rate of 59.8% and their complete model a success rate of 74.6%. Given their much stricter definition of success their results, both for the PCFG and for the enriched model, are better than ours.
Reference: 14. <author> Pereira, F., Tishby, N. and Lee, L. </author> <title> Distributional clustering of English words. </title> <booktitle> In Proceedings of the Association for Computational Linguistics. ACL, </booktitle> <year> 1993. </year>
Reference-contexts: They have been manually grouped into intuitive categories. While we have grouped these manually, work such as <ref> [14] </ref> suggests how to find such groupings automatically given data such as ours. To give another example of direct objects, Figure 4 gives all of the heads of noun-phrases following "built" such that their probability given "built" is greater than .002.
Reference: 15. <author> Schabes, Y., Roth, M. and Osborne, R. </author> <title> Parsing the Wall Street Journal with the Inside-Outside Algorithm. </title> <booktitle> In Proceedings of the Sixth Meeting of the European Chaapter of the Association for Computational Linguistics. </booktitle> <year> 1993, </year> <pages> 341-347. </pages>
Reference-contexts: It should be clear that the accuracy of our PCFG is in line with these systems. Our PCFG results are also in line with those reported in <ref> [15] </ref> though their method of reporting results do not allow them to be shown in a graph line that of Figure 7. The work most similar to our is that of Black et. al. [1] and Schabes and Waters [16].
Reference: 16. <author> Schabes, Y. and Waters, R. C. </author> <title> Stochastic Lexicalized Context-Free Grammar. </title> <booktitle> In Proceedings of the Third International Workshop on Parsing Technologies. </booktitle> <year> 1993, </year> <pages> 257-266. 19 </pages>
Reference-contexts: The work most similar to our is that of Black et. al. [1] and Schabes and Waters <ref> [16] </ref>. These papers like ours are concerned with using lexical information together with probabilities on rules to improve parsing performance.
References-found: 16

