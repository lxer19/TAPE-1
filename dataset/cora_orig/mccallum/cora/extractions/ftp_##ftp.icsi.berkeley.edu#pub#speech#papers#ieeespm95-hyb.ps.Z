URL: ftp://ftp.icsi.berkeley.edu/pub/speech/papers/ieeespm95-hyb.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/real/papers.html
Root-URL: http://www.icsi.berkeley.edu
Title: An Introduction to Hybrid HMM/Connectionist Continuous Speech Recognition  
Author: Nelson Morgan and Herve Bourlard 
Address: Berkeley, CA 94704, USA  
Affiliation: International Computer Science Institute  
Abstract: Since 1988 there has been a significant body of work, both theoretical and experimental, that has established the viability of Artificial Neural Networks (ANNs) as a useful technology to assist in statistical speech recognition. ANNs are sometimes called connectionist due to their representation of information in the connections between units computing simple functions of their inputs. Researchers at a number of laboratories have shown that these nets can be used to estimate probabilities that are useful in statistical pattern recognition, and in particular for speech recognition. Simple systems based on this hybrid approach have performed very well on large vocabulary continuous speech recognition. Research is continuing on extending these results to more complex systems. In this tutorial paper, we will briefly introduce the theoretical and practical underpinnings of this approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amari, </author> <title> "A theory of adaptive pattern classifiers," </title> <journal> IEEE Trans. on Elec. Com., </journal> <volume> vol. EC16, </volume> <pages> pp. 279-307, </pages> <year> 1967. </year>
Reference-contexts: MLP parameters (the elements of the weight matrices) are trained to associate a "desired" output vector with an input vector. This is achieved via the Error Back-Propagation (EBP) algorithm (see [58], [59], [70] and [81] for multilayer networks; <ref> [1] </ref>, [69] and [83] for single layer networks; [15] for a control theory version) that uses a steepest descent procedure to iteratively minimize a cost function. 8 consists of a series of units represented by circles.
Reference: [2] <author> G. Zavaliagkos, Y. Zhao, R. Schwartz, and J. Makhoul, </author> <title> "A hybrid segmental neural net/hidden markov model system for continuous speech recognition" IEEE Trans. </title> <booktitle> on Speech and Audio Processing, </booktitle> <volume> vol. 2, no. 1, </volume> <pages> pp. 151-160, </pages> <year> 1994. </year>
Reference-contexts: Any system can be improved in a variety of ways, and so there is no conclusive evidence that connectionist speech recognition algorithms are better than other approaches. Nonetheless, in a number of controlled experiments, the use of connectionist elements in a larger statistical system has significantly improved performance <ref> [2, 47, 65] </ref>. Further, connectionist speech research can provide the opportunity to develop a new paradigm that also has a firm theoretical footing, and which may lead to newer innovations that will improve the state of the art. <p> As research continues, hopefully the models can represent speech more faithfully. However, most laboratories that have published reports of work in continuous speech recognition using ANNs are using them to generate probabilities (or distances) that are used with an underlying model that is explicitly or implicitly an HMM <ref> [2, 7, 19, 24, 33, 45, 47] </ref>. 1 For more on this class of hybrid systems, see [9]. 2 Technology Background 2.1 Automatic Speech Recognition The basic task of Automatic Speech Recognition (ASR) is to derive a sequence of words from a stream of acoustic information. <p> Thus, on quite different tasks studied by unrelated laboratories, it was observed that a very good HMM-based system using Gaussian mixture estimators could be further improved by smoothing with estimates from an ANN. Similar results have been observed in other laboratories as well. 5. In work at BBN <ref> [2] </ref>, the subsystems were combined in a different way (by taking a list of the most likely N sentences as estimated by a pure HMM system, and reordering them based on phonetic segment probabilities as estimated by an MLP), but they too reported consistent improvements over the simpler system. 6 Current <p> The secondary system then rescores the candidate list or reprocesses the lattice hypotheses with some new criterion. For instance, in the case of the Segmental Neural Network <ref> [2] </ref>, networks are trained on phonetic segments as determined from Viterbi alignments in the training set, and then are run to generate probabilities for sequences of segments in each hypothesized utterance.
Reference: [3] <author> J. Baker, </author> <title> "The DRAGON System An overview," </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 23, no. 1, </volume> <pages> pp. 24-29, </pages> <year> 1975. </year>
Reference-contexts: Further, this technology is now reasonably mature, having been built up over the last twenty years, based on the pioneering work of Baker <ref> [3] </ref> and Jelinek [36]. Additionally, many of the engineering issues were addressed by speech researchers in the late 1980's and early 1990's under the auspices of the ARPA program [16, 18, 42].
Reference: [4] <author> L. Baum, </author> <title> "An inequality and associated maximization techniques in statistical estimation of probabilistic functions of Markov processes," </title> <journal> Inequalities, </journal> <volume> no. 3, </volume> <pages> pp. 1-8, </pages> <year> 1972. </year>
Reference-contexts: There exist efficient training algorithms to learn the parameters of the probability estimators. The most common form of this procedure is often called the forward-backward algorithm, in which the estimators for the data likelihoods conditioned on each word model (P (XjM )) are iteratively trained <ref> [4] </ref>. For the Viterbi approximation, the full likelihood is approximated by the likelihood of the most probable path through the states in the models, as given by the dynamic programming procedure.
Reference: [5] <author> Y. Bengio, R. De Mori, G. Flammia, & R. Kompe, </author> <title> "Global optimization of a neural network-Hidden Markov Model hybrid," </title> <journal> IEEE Trans, on Neural Networks, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 252-259, </pages> <year> 1992. </year>
Reference-contexts: This permits a global optimization of the input transformation together with a global training of the HMMs <ref> [5] </ref>. * Preprocessing Many researchers have used feature map representations, related to one of the formulations from Kohonen and collaborators [40], to generate feature representations for a speech recognizer.
Reference: [6] <author> H. Bourlard, Y. Konig and N. Morgan, </author> <title> "REMAP: recursive estimation and maximization of a posterior probabilities Application to transition-based connectionist speech recognition," </title> <type> ICSI Technical Report TR-94-064, </type> <year> 1994. </year>
Reference-contexts: This approximation is sometimes more sensitive to poor initializations, but with a good initialization can be particularly straightforward to implement, and ultimately is more convenient for the approaches described in this article. One form of this iteration, then, could be as follows: 3 However, see <ref> [6] </ref> for some theoretical work that suggests an approach to training and recognition with a global P (MjX) criterion. 4 This actually explains the lack of discrimination mentioned in Table 1, as systems trained to maximize P (XjM) for the correct model are not trained to minimize that quantity for the <p> it can also be shown that, in theory, HMMs can be trained using local posterior probabilities as emission probabilities [9], resulting in models that are both locally and globally discriminant. (See Section 6.3 for a brief description of some current work in this area, which is described more fully in <ref> [6] </ref>). For current systems, there are generally mismatches between the prior class probabilities implicit to the training data and the priors that are implicit to the lexical and syntactic models that are used in recognition. <p> A neural network can then be trained to approximate the mapping to this probability. Algorithms to do this and a proof of their convergence can be found in <ref> [6] </ref>. 6.4 Merging Neural Networks as Statistical Experts Multiple networks can, in principle, be used together in order to improve performance over a single "expert." Additionally, this kind of approach can be used to speed training; this is the application that is discussed here. <p> Such models may be necessary in order to determine the ultimate utility of neural networks for speech recognition. 12 Some initial theory for this extension is described in [55], and the mathematical basis for training of such systems with a global maximum a posteriori (MAP) criterion as described in <ref> [6] </ref>. 34
Reference: [7] <author> H. Bourlard and N. Morgan, </author> <title> "A continuous speech recognition system embedding MLP into HMM," </title> <booktitle> in Advances in Neural Information Processing Systems 2 (D. </booktitle> <editor> S. Touretzky, </editor> <publisher> Ed.), </publisher> <pages> pp. 413-416. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1990. </year>
Reference-contexts: As research continues, hopefully the models can represent speech more faithfully. However, most laboratories that have published reports of work in continuous speech recognition using ANNs are using them to generate probabilities (or distances) that are used with an underlying model that is explicitly or implicitly an HMM <ref> [2, 7, 19, 24, 33, 45, 47] </ref>. 1 For more on this class of hybrid systems, see [9]. 2 Technology Background 2.1 Automatic Speech Recognition The basic task of Automatic Speech Recognition (ASR) is to derive a sequence of words from a stream of acoustic information. <p> We and others have performed numerous experiments that have verified these two points. In some of them, a fixed HMM was used and alternate probability estimators were substituted <ref> [7, 56, 9, 65, 67, 47] </ref>. When these experiments were controlled for the number of parameters, there have been significant improvements using the approaches described here.
Reference: [8] <author> H. Bourlard and N. Morgan, N. </author> <title> "CDNN: A context dependent neural network for nontinuous speech recognition," </title> <booktitle> IEEE Proc. Intl. Conf. on Acoustics, Speech, and Signal Processing (San Francisco, </booktitle> <address> CA), </address> <pages> pp. </pages> <address> II:349-352, </address> <year> 1992. </year>
Reference-contexts: If there are J constraint classes, this will require K fi J output units for an ANN estimator. However, using the definition of conditional probability, the desired expression can be broken down in two ways as follows <ref> [8, 19] </ref>: Thus, the desired probability is the product of an unconstrained posterior probability and a new conditional. The former can be realized with a network like that of figure 6 (though for the second case the output targets are context classes).
Reference: [9] <author> H. Bourlard and N. Morgan, </author> <title> Connectionist Speech Recognition A Hybrid Approach, </title> <publisher> Kluwer Academic Press, </publisher> <year> 1994. </year>
Reference-contexts: have published reports of work in continuous speech recognition using ANNs are using them to generate probabilities (or distances) that are used with an underlying model that is explicitly or implicitly an HMM [2, 7, 19, 24, 33, 45, 47]. 1 For more on this class of hybrid systems, see <ref> [9] </ref>. 2 Technology Background 2.1 Automatic Speech Recognition The basic task of Automatic Speech Recognition (ASR) is to derive a sequence of words from a stream of acoustic information. <p> Typically, MLPs have a layered feedforward architecture with an input layer (consisting of the input variables), zero or more hidden (intermediate) layers, and an output layer, as shown in followed by a nonlinear function, which is often a sigmoid function f (x) = 1 + exp (x) As discussed in <ref> [9] </ref>, this nonlinear function performs a different role for the hidden and the output units. On the hidden units, it serves to generate high order moments of the input; this can be done effectively by many nonlinear functions, not only by sigmoids. <p> This can be converted to emission probabilities using Bayes' rule. Several authors have shown that the outputs of ANNs used in classification mode can be interpreted as estimates of a posteriori probabilities of output classes conditioned on the input <ref> [9] </ref>, [10], [27], [66]. Before we repeat this proof, the general principle can be geometrically motivated (see Figure 4). The figure shows that an equilibrium that is reached in the ideal case does in fact correspond to the network output g being equal to the posterior probability p. <p> error minimum (where mean squared error and relative entropy are error criteria that will work for this purpose) It has been experimentally observed that, for systems trained on a large amount of speech, the outputs of a properly trained MLP do in fact approximate posterior probabilities (see Figure 6.1 in <ref> [9] </ref>), even for error values that are not precisely the global minimum. 7 Thus, emission probabilities can be estimated by applying Bayes' rule to the ANN outputs. <p> We and others have performed numerous experiments that have verified these two points. In some of them, a fixed HMM was used and alternate probability estimators were substituted <ref> [7, 56, 9, 65, 67, 47] </ref>. When these experiments were controlled for the number of parameters, there have been significant improvements using the approaches described here. <p> Despite this apparent simplicity, there are some significant characteristics of this system that have appeared to be necessary for good performance. The major points are summarized in the following sections; for further explanation, see <ref> [9] </ref>. 4.2 Training We and others have used on-line training instead of off-line (true gradient) backpropagation. In this approach, the weights are adjusted in the direction of the error gradient with respect to the weight vector, as estimated from a single pattern. <p> However, it can also be shown that, in theory, HMMs can be trained using local posterior probabilities as emission probabilities <ref> [9] </ref>, resulting in models that are both locally and globally discriminant. (See Section 6.3 for a brief description of some current work in this area, which is described more fully in [6]). <p> Run the training data forward through the network, generating posterior probabilities. Use either these probabilities 10 or scaled likelihoods (after division by priors) in a dynamic 10 As suggested in <ref> [9] </ref>, our results in these experiments confirmed that division by priors was not actually necessary 26 programming procedure (forced Viterbi) to determine the alignment between word models and speech. This will result in the recognition of pronunciations in the data. 4.
Reference: [10] <author> H. Bourlard and C. J. Wellekens, </author> <title> "Links between Markov models and multilayer perceptrons," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 12, </volume> <pages> pp. 1167-1178, </pages> <year> 1990. </year>
Reference-contexts: This can be converted to emission probabilities using Bayes' rule. Several authors have shown that the outputs of ANNs used in classification mode can be interpreted as estimates of a posteriori probabilities of output classes conditioned on the input [9], <ref> [10] </ref>, [27], [66]. Before we repeat this proof, the general principle can be geometrically motivated (see Figure 4). The figure shows that an equilibrium that is reached in the ideal case does in fact correspond to the network output g being equal to the posterior probability p.
Reference: [11] <author> J. S. Bridle, </author> <title> "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition," in Neurocomputing: Algorithms, Architectures and Applications, </title> <editor> F. Fogelman Soulie and J. Herault (Eds.), </editor> <booktitle> NATO ASI Series, </booktitle> <pages> pp. 227-236, </pages> <year> 1990. </year>
Reference-contexts: This network does implement the core of the dynamic time warp algorithm, but it does not replace the pointer bookkeeping that is required for a practical Viterbi implementation. In another connectionist formulation of what had previously been considered an entirely non-connectionist algorithm, the Alpha-Net <ref> [11] </ref> was introduced to simulate the forward recurrence of the forward-backward HMM algorithm.
Reference: [12] <author> J. S. Bridle, "Alpha-Nets: </author> <title> a recurrent neural network architecture with a hidden Markov model interpretation," </title> <journal> Speech Communication, </journal> <volume> vol. 9, </volume> <pages> pp. 83-92, </pages> <year> 1990. </year>
Reference: [13] <author> D. Broomhead, & D. Lowe, </author> <title> "Multi-variable functional interpolation and adaptive networks," </title> <journal> Complex Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 321-355, </pages> <year> 1988. </year>
Reference-contexts: of the other kinds of network that can be used in an ANN/HMM hybrid besides the MLP described above. * Radial Basis Functions (RBFs)- many researchers use a variant of a layered feedforward network in which a hidden layer with a fixed-variance Gaussian is used instead of one with sigmoids <ref> [13] </ref>. Further, these networks commonly use a single Gaussian layer and an output layer that is strictly linear. Once reasonable positions are found for the hidden Gaussians, the optimal output layer weights can be determined analytically with a matrix inversion.
Reference: [14] <author> P. F. Brown, </author> <title> "The Acoustic-Modelling Problem in Automatic Speech Recognition," </title> <type> PhD Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1987. </year>
Reference: [15] <author> A. Bryson and Yu Chi Ho, </author> <title> Applied Optimal Control, </title> <publisher> Blaisdel Publishing Company, </publisher> <year> 1969. </year> <month> 35 </month>
Reference-contexts: MLP parameters (the elements of the weight matrices) are trained to associate a "desired" output vector with an input vector. This is achieved via the Error Back-Propagation (EBP) algorithm (see [58], [59], [70] and [81] for multilayer networks; [1], [69] and [83] for single layer networks; <ref> [15] </ref> for a control theory version) that uses a steepest descent procedure to iteratively minimize a cost function. 8 consists of a series of units represented by circles.
Reference: [16] <author> Y. Chow, M. Dunham, O. Kimball, M. Krasner, G. Kubala, J. Makhoul, P. Price, S. Roucos, and R. Schwarz, </author> <title> "BYBLOS: The BBN continuous speech recognition system," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoustic, Speech, and Signal Processing Dallas, Texas, </booktitle> <pages> pp. 89-92, </pages> <year> 1987. </year>
Reference-contexts: Additionally, many of the engineering issues were addressed by speech researchers in the late 1980's and early 1990's under the auspices of the ARPA program <ref> [16, 18, 42] </ref>. It is now feasible to develop new applications by following mathematical prescriptions that have been developed under these previous programs. Many people have been able to use HMM-based approaches as the core of impressive recognition systems.
Reference: [17] <author> M. Cohen, </author> <title> "Phonological Structures for Speech Recognition," </title> <type> PhD Thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1989. </year>
Reference-contexts: We note briefly that related work is also being done to apply hybrid HMM/ANN methods to non-speech applications such as handwriting recognition (see, e.g., [72]). 6.1 Learning Pronunciations with Nets The quality of phonological models have sometimes been observed to have a significant effect on ASR performance <ref> [17] </ref>.
Reference: [18] <author> M. Cohen, H. Murveit, J. Bernstein, P. Price, and M. Weintraub, </author> <title> "The DECIPHER speech recognition system, </title> " <booktitle> in Proc. IEEE Intl. Conf. on Acoustic, Speech, and Signal Processing (Albuquerque, </booktitle> <address> NM), </address> <pages> pp. 77-80, </pages> <year> 1990. </year>
Reference-contexts: Additionally, many of the engineering issues were addressed by speech researchers in the late 1980's and early 1990's under the auspices of the ARPA program <ref> [16, 18, 42] </ref>. It is now feasible to develop new applications by following mathematical prescriptions that have been developed under these previous programs. Many people have been able to use HMM-based approaches as the core of impressive recognition systems.
Reference: [19] <author> M. Cohen, H. Franco, N. Morgan, D. Rumelhart, and V. Abrash, </author> <title> "Context-dependent multiple distribution phonetic modeling," </title> <booktitle> in Advances in Neural Information Processing Systems 5 (S.J. </booktitle> <editor> Hanson, J.D. Cowan, and C.L. Giles, </editor> <booktitle> Eds.), </booktitle> <pages> pp. 649-657, </pages> <year> 1993. </year>
Reference-contexts: As research continues, hopefully the models can represent speech more faithfully. However, most laboratories that have published reports of work in continuous speech recognition using ANNs are using them to generate probabilities (or distances) that are used with an underlying model that is explicitly or implicitly an HMM <ref> [2, 7, 19, 24, 33, 45, 47] </ref>. 1 For more on this class of hybrid systems, see [9]. 2 Technology Background 2.1 Automatic Speech Recognition The basic task of Automatic Speech Recognition (ASR) is to derive a sequence of words from a stream of acoustic information. <p> If there are J constraint classes, this will require K fi J output units for an ANN estimator. However, using the definition of conditional probability, the desired expression can be broken down in two ways as follows <ref> [8, 19] </ref>: Thus, the desired probability is the product of an unconstrained posterior probability and a new conditional. The former can be realized with a network like that of figure 6 (though for the second case the output targets are context classes). <p> These inputs can either be connected to a hidden layer, or, for greater computational simplicity, directly to an output layer. This latter architecture, as applied to phonetic context, is illustrated in Figure 9. In collaborative work with SRI International <ref> [19] </ref> this class of approach was tested for left and right generalized biphone context. <p> The right context-dependent output layer is trained similarly. 23 stopping) determined a smooth compromise between fully relying on the context-dependent data (which is sparser for each class) and the context-independent training <ref> [19] </ref>. These experiments showed significant improvements over the simpler context-independent approach, eliminating roughly one-fifth of the errors in a speaker-independent Resource Management test set [23]. <p> The resulting system was comparable in accuracy to an HMM system that used many more parameters and which modeled much more detailed context, including word-specific models for the most common words <ref> [19] </ref>. Researchers at the Oregon Graduate Institute have also recently reported very large improvements for one of their tasks, using an architecture and method similar to that used by SRI [38]. Thus, statistical factorization of ANN probabilistic estimators appears to have practical significance.
Reference: [20] <author> L. Deng, </author> <title> "A generalized hidden markov model with state-conditioned trend functions of time for the speech signal." </title> <booktitle> Signal Processing, </booktitle> <volume> 27 </volume> <pages> 65-78, </pages> <year> 1992. </year>
Reference-contexts: For these reasons, a number of researchers have recently focused on statistical representations of complete segments, as opposed to i.i.d. statistical estimates for sub-segmental frames <ref> [20, 21, 26] </ref>. In general, these approaches are used to estimate segment likelihoods. An alternate approach would be to estimate segment posteriors, and to incorporate some representation of the segment dynamics into the model.
Reference: [21] <author> V.V. Digalakis, J.R. Rohlicek, and M. Ostendorf. </author> <title> "Segment-based stochastic models of spectral dynamics for continuous speech recognition." </title> <journal> IEEE trans. on Speech and Audio Processing, </journal> <volume> 1(4) </volume> <pages> 431-442, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: For these reasons, a number of researchers have recently focused on statistical representations of complete segments, as opposed to i.i.d. statistical estimates for sub-segmental frames <ref> [20, 21, 26] </ref>. In general, these approaches are used to estimate segment likelihoods. An alternate approach would be to estimate segment posteriors, and to incorporate some representation of the segment dynamics into the model.
Reference: [22] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley Interscience, </publisher> <address> New York, </address> <year> 1973. </year>
Reference: [23] <author> H. Franco, M. Cohen, N. Morgan, D. Rumelhart, and V. Abrash, </author> <title> "Context-dependent connectionist probability estimation in a hybrid hidden Markov model-neural net speech recognition system," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol. 8, no. 3, </volume> <pages> pp. 211-222, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: These experiments showed significant improvements over the simpler context-independent approach, eliminating roughly one-fifth of the errors in a speaker-independent Resource Management test set <ref> [23] </ref>. The resulting system was comparable in accuracy to an HMM system that used many more parameters and which modeled much more detailed context, including word-specific models for the most common words [19].
Reference: [24] <author> M. Franzini, K.F. Lee, and A. Waibel, </author> <title> "Connectionist Viterbi training: a new hybrid method for continuous speech recognition," </title> <booktitle> IEEE Proc. Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. 425-428, </pages> <address> Albuquerque, NM, </address> <year> 1990. </year>
Reference-contexts: As research continues, hopefully the models can represent speech more faithfully. However, most laboratories that have published reports of work in continuous speech recognition using ANNs are using them to generate probabilities (or distances) that are used with an underlying model that is explicitly or implicitly an HMM <ref> [2, 7, 19, 24, 33, 45, 47] </ref>. 1 For more on this class of hybrid systems, see [9]. 2 Technology Background 2.1 Automatic Speech Recognition The basic task of Automatic Speech Recognition (ASR) is to derive a sequence of words from a stream of acoustic information. <p> However, all of the basic conclusions about the utility of these structures for estimating probabilities or local costs for an HMM will also hold for other structures such as a recurrent network, as used in [33], or a Time-Delay-Neural-Network (TDNN), as used in <ref> [24] </ref>. These alternative structures will be briefly discussed in Section 2.3.2. <p> This feedforward approach was shown to be useful with a single convolutional layer for consonant recognition in 1983 by Makino [48], and later by Waibel et al for a similar task using multiple convolutional layers [79] and finally for hybrid systems such as those described in this article <ref> [24] </ref>. It is a flexible structure, and in practice many researchers (including us) have used a compromise between the simple MLP and a full TDNN in which delays are only used at the input layer.
Reference: [25] <author> S. Furui, </author> <title> "Speaker independent isolated word recognizer using dynamic features of speech spectrum," </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 34, no. 1, </volume> <pages> pp. 52-59, </pages> <year> 1986. </year>
Reference-contexts: They thus have little effect on performance when there is sufficient training data." 8 Of course, ANNs are not the only way to incorporate such context. Many current systems use first and second time derivatives <ref> [25, 62] </ref> computed over a span of a few frames, allowing very limited acoustical context modelling. <p> One approach to this is to use time derivative features <ref> [25] </ref>. In general, though, the when the pronunciations were learned from the data. <p> Another is to model speech as a succession of auditory events or avents, separated by relatively stationary periods (ca. 50-150 ms). Avents correspond to times when the spectrum and amplitude are rapidly changing, which are believed to be the most important regions for phonetic discrimination <ref> [25] </ref>. The stationary periods are mapped to a single tied state, and so modeling power is focused on regions of significant change. This is described further in [55]. It is implicit in transition classification that the transition occurs at some unique time (e.g., for a single frame).
Reference: [26] <author> O. Ghitza and M.M. Sondhi. </author> <title> "Hidden markov models with templates as non-stationary states: an application to speech recognition." </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 2 </volume> <pages> 101-119, </pages> <year> 1993. </year>
Reference-contexts: For these reasons, a number of researchers have recently focused on statistical representations of complete segments, as opposed to i.i.d. statistical estimates for sub-segmental frames <ref> [20, 21, 26] </ref>. In general, these approaches are used to estimate segment likelihoods. An alternate approach would be to estimate segment posteriors, and to incorporate some representation of the segment dynamics into the model. <p> Finally, attention can be focused on the transitions rather than the relatively stationary portions of segments in an attempt to model the perceptual sensitivity to such regions. This has been done for diphone likelihoods <ref> [26] </ref>, and a posterior model focusing on transitions is described in [55]. In all of these cases, there is an attempt to better model the nonstationary nature of speech. All of this work is at an early stage, both for connectionist and non-connectionist approaches.
Reference: [27] <author> H. Gish, </author> <title> "A probabilistic approach to the understanding and training of neural network classifiers," </title> <booktitle> in Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (Albuquerque, </booktitle> <address> NM), </address> <pages> pp. 1361-1364, </pages> <year> 1990. </year>
Reference-contexts: This can be converted to emission probabilities using Bayes' rule. Several authors have shown that the outputs of ANNs used in classification mode can be interpreted as estimates of a posteriori probabilities of output classes conditioned on the input [9], [10], <ref> [27] </ref>, [66]. Before we repeat this proof, the general principle can be geometrically motivated (see Figure 4). The figure shows that an equilibrium that is reached in the ideal case does in fact correspond to the network output g being equal to the posterior probability p.
Reference: [28] <author> R. Haeb-Umbach and H. Ney, </author> <title> "Linear discriminant analysis for improved large vocabulary continuous speech recognition," </title> <booktitle> IEEE Proc. Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. </pages> <address> I-13-16, San Francisco, CA, </address> <year> 1992. </year>
Reference-contexts: Some systems transform a context window of a few adjacent frames (typically 3-5 frames in total) with Linear Discriminant Analysis (LDA), which finds a linear transformation that maximizes the between-class variance while minimizing the within-class variance (see, e.g., <ref> [28] </ref>). The neural network can be seen as a generalization of these approaches that permits arbitrary weights and a nonlinear transformation of the input data. 14 net (Figure 3). This generates local probabilities that are used, after division by priors, as local scaled likelihoods in a Viterbi dynamic programming algorithm.
Reference: [29] <author> J. Hampshire and A. Waibel, </author> <title> "Connectionist architectures for multi-speaker phoneme recognition," </title> <booktitle> in Advances in Neural Information Processing Systems 2 (D. </booktitle> <editor> S. Touretzky, Ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> CA, </address> <year> 1990. </year> <month> 36 </month>
Reference-contexts: Other categories, such as rate of speech, dialect region, or unsupervised data clusters could be used to define a partition of the data for the training of multiple networks. An approach of this kind was proposed in <ref> [29] </ref>. Speaker-dependent estimators for voiced stop consonant probabilities were weighted and summed with gating elements trained with error back-propagation. More generally, a number of authors have recently discussed the mixture of information from separately-trained experts [35].
Reference: [30] <author> H. Hermansky, </author> <title> "Perceptual Linear Predictive (PLP) analysis of speech," </title> <journal> Journal of the Acoust. Soc. Am., </journal> <volume> vol. 87, no. 4, </volume> <year> 1990. </year>
Reference-contexts: It is deceptively simple, consisting of a single large hidden layer, typically with between 500 and 4000 hidden units that receive input from several hundred acoustic variables (e.g., 9 frames of acoustic context consisting of 12th order Perceptual Linear Prediction coefficients (PLP-12) <ref> [30] </ref> and log energy, along with their derivatives, or 26 features per frame). 9 The output typically corresponds to simple context-independent acoustic classes such as phones defined for the TIMIT phonetic database, using 61 phones.
Reference: [31] <author> H. Hermansky and N. Morgan, </author> <title> "RASTA processing of speech", </title> <journal> IEEE Transactions on Speech and Audio Processing, special issue on Robust Speech Recognition, </journal> <volume> vol.2 no. 4, </volume> <pages> pp. 578-589, </pages> <month> Oct., </month> <year> 1994 </year>
Reference-contexts: Another step in this direc-tion is to use highpass or bandpass filtering of critical band trajectories (RASTA processing) to emphasize transitions <ref> [31] </ref>. While this is sometimes helpful in reducing errors due to mismatches between training and testing conditions, the resulting observation sequence is a representation that has emphasized the regions of strong change and de-emphasized temporal regions without significant spectral change.
Reference: [32] <author> J. Hertz, A. Krogh, and R. Palmer, </author> <title> Introduction to the Theory of Neural Networks, </title> <publisher> Addison Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Consequently we are not giving a significant treatment here of the possible neural networks that could be used for this purpose (see [51] or <ref> [32] </ref> for more about the possible architectures).
Reference: [33] <author> M. Hochberg, S. Renals, and A. Robinson, </author> <title> "ABBOT: The CUED hybrid connectionist-HMM large-vocabulary recognition system," </title> <booktitle> in Proc. ARPA Spoken Language Technology Workshop, </booktitle> <year> 1994. </year>
Reference-contexts: As research continues, hopefully the models can represent speech more faithfully. However, most laboratories that have published reports of work in continuous speech recognition using ANNs are using them to generate probabilities (or distances) that are used with an underlying model that is explicitly or implicitly an HMM <ref> [2, 7, 19, 24, 33, 45, 47] </ref>. 1 For more on this class of hybrid systems, see [9]. 2 Technology Background 2.1 Automatic Speech Recognition The basic task of Automatic Speech Recognition (ASR) is to derive a sequence of words from a stream of acoustic information. <p> The reader should keep in mind that most of the basic ideas should still hold for other hybrid implementations, e.g., using a recurrent net for the probability estimates as done in <ref> [33] </ref>. 4 many local matches. The local match does not typically produce a single hard choice of the closest speech class, but rather a group of distances or probabilities corresponding to possible sounds. <p> However, all of the basic conclusions about the utility of these structures for estimating probabilities or local costs for an HMM will also hold for other structures such as a recurrent network, as used in <ref> [33] </ref>, or a Time-Delay-Neural-Network (TDNN), as used in [24]. These alternative structures will be briefly discussed in Section 2.3.2.
Reference: [34] <author> X.D. Huang, K.F. Lee, and A. Waibel, </author> <title> "Connectionist speaker normalization and its application to speech recognition," </title> <booktitle> Proc. of IEEE Workshop on Neural Networks for Signal Processing, </booktitle> <pages> pp. 357-366, </pages> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference-contexts: In other designs, researchers have experimented with networks to provide mappings from noisy to clean data [75] or from a new speaker to an old speaker <ref> [34] </ref>. * Postprocessing Because of interest in more complex models of speech and the difficulty in training such models (such as the segmental models mentioned above), some researchers have used more computationally demanding approaches as a postprocessing step in speech recognition.
Reference: [35] <author> R. Jacobs and M. Jordan M. </author> <title> "Linear piecewize control strategies in a modular neural network architecture", </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, March/April 1993, </journal> <volume> vol. 23, nr. 2, </volume> <pages> pp. 337-345, </pages> <year> 1993 </year>
Reference-contexts: An approach of this kind was proposed in [29]. Speaker-dependent estimators for voiced stop consonant probabilities were weighted and summed with gating elements trained with error back-propagation. More generally, a number of authors have recently discussed the mixture of information from separately-trained experts <ref> [35] </ref>. During recognition, two approaches can be taken: either weight each network with probabilities from the gating network on every frame, or perform multiple recognitions based on the assumption of one partition being the "correct" one for the duration of the utterance.
Reference: [36] <author> F. Jelinek, </author> <title> "Continuous speech recognition by statistical methods," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 64, no. 4, </volume> <pages> pp. 532-555, </pages> <year> 1976. </year>
Reference-contexts: Further, this technology is now reasonably mature, having been built up over the last twenty years, based on the pioneering work of Baker [3] and Jelinek <ref> [36] </ref>. Additionally, many of the engineering issues were addressed by speech researchers in the late 1980's and early 1990's under the auspices of the ARPA program [16, 18, 42]. It is now feasible to develop new applications by following mathematical prescriptions that have been developed under these previous programs.
Reference: [37] <author> F. </author> <title> Jelinek "Self-organized modelling for speech recognition," in Readings in Speech Recognition, </title> <editor> A. Waibel and K. Lee (eds.), </editor> <booktitle> pp. </booktitle> <pages> 450-503, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [38] <author> L. Jiang and E. Barnard, </author> <title> "Choosing contexts for neural networks," </title> <institution> Oregon Graduate Institute Technical Report, </institution> <year> 1994. </year>
Reference-contexts: Researchers at the Oregon Graduate Institute have also recently reported very large improvements for one of their tasks, using an architecture and method similar to that used by SRI <ref> [38] </ref>. Thus, statistical factorization of ANN probabilistic estimators appears to have practical significance. The approach has also been applied in other ways, for instance for gender and combinations of speaker-specific models [41]. See Figure 10 for an illustration of a network trained with gender dependency.
Reference: [39] <author> D. Jurafsky, C. Wooters, G. Tajchman, J. Segal, A. Stolcke, and N. Morgan, </author> <title> "The Berkeley restaurant project," </title> <booktitle> in Proc. Intl. Conf. on Spoken Language Processing (Yokohama, </booktitle> <address> Japan), </address> <publisher> In Press, </publisher> <year> 1994. </year>
Reference-contexts: In recent work at ICSI on a restaurant query speech understanding system called the Berkeley Restaurant Project (BeRP) <ref> [39] </ref>, Chuck Wooters developed an approach for generating models of multiple pronunciations using a modification of our iterated training. Briefly, this procedure consisted of the following steps (sketched in Figure 11): 1.
Reference: [40] <author> T. Kohonen, </author> <title> "The `neural' phonetic typewriter," </title> <booktitle> IEEE Computer: </booktitle> <pages> 11-22, </pages> <year> 1988. </year>
Reference-contexts: This permits a global optimization of the input transformation together with a global training of the HMMs [5]. * Preprocessing Many researchers have used feature map representations, related to one of the formulations from Kohonen and collaborators <ref> [40] </ref>, to generate feature representations for a speech recognizer.
Reference: [41] <author> Y. Konig, N. Morgan, C. Wooters, V. Abrash, M. Cohen, and H. Franco, </author> <title> "Modeling consistency in a speaker independent continuous speech recognition system," </title> <booktitle> in Advances in Neural Information Processing Systems 5 (S.J. </booktitle> <editor> Hanson, J.D. Cowan, and C.L. Giles, </editor> <booktitle> Eds.), </booktitle> <pages> pp. 682-687, </pages> <year> 1993. </year>
Reference-contexts: Thus, statistical factorization of ANN probabilistic estimators appears to have practical significance. The approach has also been applied in other ways, for instance for gender and combinations of speaker-specific models <ref> [41] </ref>. See Figure 10 for an illustration of a network trained with gender dependency. This is also an example of the incorporation of long-term consistency constraints, as the input in this case is constrained to either be male or female for the entire utterance. presented to the network.
Reference: [42] <author> K. F. Lee, </author> <title> Large vocabulary speaker-independent continuous speech recognition: The SPHINX system, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Additionally, many of the engineering issues were addressed by speech researchers in the late 1980's and early 1990's under the auspices of the ARPA program <ref> [16, 18, 42] </ref>. It is now feasible to develop new applications by following mathematical prescriptions that have been developed under these previous programs. Many people have been able to use HMM-based approaches as the core of impressive recognition systems.
Reference: [43] <author> E. Levin, </author> <title> "Speech recognition using hidden control neural network architecture," </title> <booktitle> in Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (Albuquerque, </booktitle> <address> NM), </address> <pages> pp. 433-436, </pages> <year> 1990. </year>
Reference-contexts: This could either be done using multiple networks (one per state) [78], or with one network that uses the state identity as a control input <ref> [43] </ref>. Such systems have been successfully trained to do connected digit recognition. They also have sometimes been used for larger tasks, but apparently have not been as successful as classification-based hybrids.
Reference: [44] <author> R. P. Lippmann, </author> <title> "Review of neural networks for speech recognition," </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 1-38, </pages> <year> 1989. </year> <month> 37 </month>
Reference-contexts: In some other work, researchers have shown that connectionist structures can be used to represent standard HMM-based algorithms. Once such subsystem was the Viterbi network <ref> [44] </ref>. In this case, each HMM is associated with a neural network in which each output corresponds to a HMM state.
Reference: [45] <author> R. P. Lippmann and E. Singer, </author> <title> "Hybrid neural-network/HMM approaches to wordspotting," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing Minneapolis, Minn., </booktitle> <pages> pp. </pages> <address> I-565-568, </address> <year> 1993. </year>
Reference-contexts: As research continues, hopefully the models can represent speech more faithfully. However, most laboratories that have published reports of work in continuous speech recognition using ANNs are using them to generate probabilities (or distances) that are used with an underlying model that is explicitly or implicitly an HMM <ref> [2, 7, 19, 24, 33, 45, 47] </ref>. 1 For more on this class of hybrid systems, see [9]. 2 Technology Background 2.1 Automatic Speech Recognition The basic task of Automatic Speech Recognition (ASR) is to derive a sequence of words from a stream of acoustic information.
Reference: [46] <author> R. Lippmann, </author> <type> personal communication, </type> <year> 1994. </year>
Reference-contexts: This provides a simple mechanism for incorporating acoustic context into the statistical formulation. 8 7 Recently, Lippmann <ref> [46] </ref> has noted that "local minima usually represent alternative solutions which change decision regions and posterior probabilities only far from the training data. They thus have little effect on performance when there is sufficient training data." 8 Of course, ANNs are not the only way to incorporate such context.
Reference: [47] <author> D.M. Lubensky, A.O. Asadi, and J.M. Naik, </author> <title> "Connected digit recognition using connectionist probability estimators and mixture-gaussian densities," </title> <booktitle> IEEE Proc. of the Intl. Conf. on Spoken Language Processing, </booktitle> <address> pp.295-298, Yokohama, Japan, </address> <year> 1994. </year>
Reference-contexts: Any system can be improved in a variety of ways, and so there is no conclusive evidence that connectionist speech recognition algorithms are better than other approaches. Nonetheless, in a number of controlled experiments, the use of connectionist elements in a larger statistical system has significantly improved performance <ref> [2, 47, 65] </ref>. Further, connectionist speech research can provide the opportunity to develop a new paradigm that also has a firm theoretical footing, and which may lead to newer innovations that will improve the state of the art. <p> As research continues, hopefully the models can represent speech more faithfully. However, most laboratories that have published reports of work in continuous speech recognition using ANNs are using them to generate probabilities (or distances) that are used with an underlying model that is explicitly or implicitly an HMM <ref> [2, 7, 19, 24, 33, 45, 47] </ref>. 1 For more on this class of hybrid systems, see [9]. 2 Technology Background 2.1 Automatic Speech Recognition The basic task of Automatic Speech Recognition (ASR) is to derive a sequence of words from a stream of acoustic information. <p> Some of the results cited in Section 5 appear to support the notion that fewer parameters are required for these systems than in non-discriminative systems with similar performance (see, in particular, <ref> [47] </ref>). We and others have performed numerous experiments that have verified these two points. In some of them, a fixed HMM was used and alternate probability estimators were substituted [7, 56, 9, 65, 67, 47]. <p> We and others have performed numerous experiments that have verified these two points. In some of them, a fixed HMM was used and alternate probability estimators were substituted <ref> [7, 56, 9, 65, 67, 47] </ref>. When these experiments were controlled for the number of parameters, there have been significant improvements using the approaches described here. <p> For further discussion about this with more results, see [68]. In <ref> [47] </ref>, similar conclusions were also drawn for quite a different example, connected digit recognition for a standard TI database. In this case, string error for a moderate-sized MLP (about 11000 parameters) was about 2.5%, while the string error for a 28,000 parameter tied mixture system was 3.8%. <p> Again, the HMM systems are frequently much more complicated, with many parameters and complex forms of smoothing; frequently the improvement over the simpler hybrid system is small. For instance, in the connected digit case described above <ref> [47] </ref>, by expanding to over 200,000 parameters, Lubensky et al were able to achieve a 2% string error, which is an error rate reduction of 25% relative to the system incorporating the MLP estimator. For large vocabulary recognition this has also been true as of this writing. 25 4.
Reference: [48] <author> S. Makino, T. Kawabata, and K. Kido, </author> <title> "Recognition of consonant based on the Perceptron model," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Boston, Mass., </address> <pages> pp. 738-741, </pages> <year> 1983. </year>
Reference-contexts: This feedforward approach was shown to be useful with a single convolutional layer for consonant recognition in 1983 by Makino <ref> [48] </ref>, and later by Waibel et al for a similar task using multiple convolutional layers [79] and finally for hybrid systems such as those described in this article [24].
Reference: [49] <author> M. Minsky, & S. Papert, </author> <title> Perceptrons, </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference: [50] <author> Mirghafori, N., Morgan, N., and Bourlard, H., </author> <title> "Parallel training of MLP probability estimators for speech recognition: </title> <booktitle> a gender-based approach" IEEE Workshop on Neural Networks for Signal Processing, </booktitle> <address> Greece, pp.289-298, </address> <year> 1994. </year>
Reference: [51] <author> D. P. Morgan and C. L. Scofield, </author> <title> Neural Networks and Speech Processing, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Consequently we are not giving a significant treatment here of the possible neural networks that could be used for this purpose (see <ref> [51] </ref> or [32] for more about the possible architectures).
Reference: [52] <author> N. Morgan, </author> <title> "Big Dumb Deural Nets (BDNN): a working brute force approach to speech recognition", </title> <booktitle> Proceedings of the ICNN, vol. VII, </booktitle> <address> pp.4462-4465, </address> <year> 1994. </year>
Reference-contexts: It is possible to train networks that are quite large (over a million weights) on millions of speech training patterns with very few epochs; the resulting networks can be used to estimate emission probabilities for HMMs in large and difficult tasks in continuous speech recognition. This was demonstrated in <ref> [52] </ref>, where we described a 1.6 million-weight network that was trained on 6 million frames of speech from the Wall Street Journal pilot data. This simple estimator was then used to get 16% error on the 5000-word vocabulary evaluation set using a standard bigram grammar.
Reference: [53] <author> N. Morgan, J. Beck, P. Kohn, J. Bilmes, E. Allman, and J. Beer, </author> <title> "The Ring Array Processor (RAP): a multiprocessing peripheral for connectionist applications," </title> <journal> Journal of Parallel and Distributed Computing, Special Issue on Neural Networks, </journal> <volume> vol. 14, pp.248-259, </volume> <year> 1992. </year>
Reference-contexts: Because of this, connectionist speech researchers have found it necessary to use fast computational hardware. In our own laboratory, we have worked on developing computational systems (both hardware and software) to facilitate the training <ref> [53] </ref>. These computers need to be fully programmable machines, since general programs must run on them as we alter our training paradigms in the course of research.
Reference: [54] <author> N. Morgan and H. Bourlard, </author> <title> "Generalization and parameter estimation in feedforward nets: some experiments, </title> " <booktitle> in Advances in Neural Information Processing Systems 2 (D.S. </booktitle> <editor> Touret-zky, Ed.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 630-637, </pages> <year> 1990. </year>
Reference-contexts: In addition to merely halting the training based on performance for an independent validation set, a training procedure can be used in which the learning rate is also adjusted to improve generalization <ref> [54] </ref>. Specifically, the learning rate is reduced (typically by a factor of 2) when cross-validation indicates that a given rate is no longer useful. Additionally, we have empirically noted that after the first reduction, only a single epoch at each rate is useful.
Reference: [55] <author> N. Morgan, H. Bourlard, S. Greenberg, and H. Hermansky, </author> <title> "Stochastic Perceptual Auditory-Event-Based Models (SPAM) for speech recognition", </title> <booktitle> Intl. Conference on Spoken Language Processing, </booktitle> <pages> pp. 1943-1946, </pages> <year> 1994. </year>
Reference-contexts: Finally, attention can be focused on the transitions rather than the relatively stationary portions of segments in an attempt to model the perceptual sensitivity to such regions. This has been done for diphone likelihoods [26], and a posterior model focusing on transitions is described in <ref> [55] </ref>. In all of these cases, there is an attempt to better model the nonstationary nature of speech. All of this work is at an early stage, both for connectionist and non-connectionist approaches. <p> The stationary periods are mapped to a single tied state, and so modeling power is focused on regions of significant change. This is described further in <ref> [55] </ref>. It is implicit in transition classification that the transition occurs at some unique time (e.g., for a single frame). This is difficult for a classifier to learn, as the neighboring acoustic vectors are often extracted from time regions with very similar acoustics. <p> Such models may be necessary in order to determine the ultimate utility of neural networks for speech recognition. 12 Some initial theory for this extension is described in <ref> [55] </ref>, and the mathematical basis for training of such systems with a global maximum a posteriori (MAP) criterion as described in [6]. 34
Reference: [56] <author> N. Morgan, H. Hermansky, H. Bourlard, P. Kohn, and C. Wooters, </author> <title> "Continuous speech recognition using PLP analysis with multilayer perceptrons," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoustic, Speech, and Signal Processing, </booktitle> <pages> pp. 49-52, </pages> <address> Toronto, Canada, </address> <year> 1991. </year>
Reference-contexts: is a search for the best sequence of words (in the sense of the best match to the data), and is determined by integrating 1 Unavoidably, we will tend to give a description that is close to our own implementation (for instance, to use a multi-layered perceptron, as described in <ref> [56] </ref>). The reader should keep in mind that most of the basic ideas should still hold for other hybrid implementations, e.g., using a recurrent net for the probability estimates as done in [33]. 4 many local matches. <p> We and others have performed numerous experiments that have verified these two points. In some of them, a fixed HMM was used and alternate probability estimators were substituted <ref> [7, 56, 9, 65, 67, 47] </ref>. When these experiments were controlled for the number of parameters, there have been significant improvements using the approaches described here. <p> In fact, it can be beneficial to have more noise 9 Many experiments have been done in our lab that resulted in this choice of input features. Some of these are reported in <ref> [56] </ref>. 16 by a single large hidden layer. The output corresponds to phonetic categories used for labeling of the TIMIT database. 17 (larger steps) initially, in order to escape from potentially poor local solutions.
Reference: [57] <author> H. Ney, </author> <title> "The use of a one-stage dynamic programming algorithm for connected word recognition," </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> 32 </volume> <pages> 263-271, </pages> <year> 1984. </year>
Reference-contexts: Another key function of this global decoding block is to compensate for temporal distortions that occur in normal speech. For instance, vowels can be shortened in rapid speech, while some consonants may remain nearly the same length. The most common global decoding approach is some form of dynamic programming <ref> [57] </ref>, in which time warping of the input against possible speech representations results in the most likely sequence of sound categories to match the input. There are many variations to this process, but in general the local computation consists of finding the lowest cost path through possible representations by: 1.
Reference: [58] <author> D. Parker, </author> <note> Invention Report S81-64, File 1, </note> <institution> Office of Technology Licensing, Stanford University, </institution> <year> 1982. </year>
Reference-contexts: MLP parameters (the elements of the weight matrices) are trained to associate a "desired" output vector with an input vector. This is achieved via the Error Back-Propagation (EBP) algorithm (see <ref> [58] </ref>, [59], [70] and [81] for multilayer networks; [1], [69] and [83] for single layer networks; [15] for a control theory version) that uses a steepest descent procedure to iteratively minimize a cost function. 8 consists of a series of units represented by circles.
Reference: [59] <author> D. Parker, </author> <title> "Learning logic," </title> <type> Technical Report TR-47, </type> <institution> Center for Computational Research in Economics and Management Science, MIT, </institution> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: MLP parameters (the elements of the weight matrices) are trained to associate a "desired" output vector with an input vector. This is achieved via the Error Back-Propagation (EBP) algorithm (see [58], <ref> [59] </ref>, [70] and [81] for multilayer networks; [1], [69] and [83] for single layer networks; [15] for a control theory version) that uses a steepest descent procedure to iteratively minimize a cost function. 8 consists of a series of units represented by circles.
Reference: [60] <author> S.M. Peeling & R.K. Moore, </author> <title> "Isolated digit recognition experiments using the multi-layer perceptron," </title> <journal> Speech Communication, </journal> <volume> vol. 7, </volume> <pages> pp. 403-409, </pages> <year> 1988. </year> <month> 38 </month>
Reference-contexts: This is the way ANNs were initially used on simple speech recognition problems (see, e.g., <ref> [60, 80, 79] </ref>). However, ANNs classifying complete temporal sequences have not been successful for continuous speech recognition. In fact, used as such they are not likely to work well for continuous speech, since the number of possible word sequences in an utterance is generally infinite.
Reference: [61] <author> A. Poritz, </author> <title> "Linear predictive Hidden Markov Models and the speech signal," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoustic, Speech, and Signal Processing, </booktitle> <pages> pp. 1291-1294, </pages> <address> Paris, </address> <year> 1982. </year>
Reference-contexts: In any event, the predictive systems are also used as hybrids, in that a search such as the Viterbi is used to integrate the information from the network, which in this case is a prediction error signal. These networks are the nonlinear equivalent of autoregressive HMMs <ref> [61] </ref>.
Reference: [62] <author> A. B. Poritz and A.L. Richter, </author> <title> "On hidden Markov models in isolated word recognition", </title> <booktitle> IEEE Proc. Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. </pages> <address> 14.3.1-4, Tokyo, Japan, </address> <year> 1986. </year>
Reference-contexts: They thus have little effect on performance when there is sufficient training data." 8 Of course, ANNs are not the only way to incorporate such context. Many current systems use first and second time derivatives <ref> [25, 62] </ref> computed over a span of a few frames, allowing very limited acoustical context modelling.
Reference: [63] <author> L. R. Rabiner, </author> <title> "A tutorial on hidden Markov models and selected applications in speech recognition," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 77, no. 2, </volume> <pages> pp. 257-285, </pages> <year> 1989. </year>
Reference-contexts: Word models consist of concatenations of phone or phoneme models (constrained by pronunciations from a lexicon), and sentence models consist of concatenations of word models (constrained by a grammar). Theory and methodology for HMMs are described in many sources, including <ref> [63] </ref>. <p> This step is sometimes called a forced Viterbi alignment (determining the alignment of the sequence of acoustic training vectors with the corresponding phonetic labels). This procedure, sometimes called embedded Viterbi learning (as used in the segmental k-means algorithm <ref> [63] </ref>, for instance), can be proved to converge to a local optimum; in practice it is repeated until some stopping criterion has been reached. 2.3 Artificial Neural Networks (ANNs) 2.3.1 Multilayer Perceptrons (MLPs) Our discussion of neural networks for speech will be focused on Multilayer Perceptrons (MLPs), which are the most
Reference: [64] <author> S. Renals, N. Morgan, H. Bourlard, M. Cohen, and H. Franco, </author> <title> "Connectionist optimization of tied mixture Hidden Markov Models," </title> <booktitle> in Advances in Neural Information Processing Systems 4 (J. </booktitle> <editor> Moody, S. Hanson, and R. Lippmann, Eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 167-174, </pages> <year> 1992. </year>
Reference-contexts: These systems can be trained with much less computation than sigmoid or softmax-based MLPs. The outputs can also be used as probabilities and integrated into a hybrid system <ref> [64, 76] </ref>.
Reference: [65] <author> S. Renals, N. Morgan, H. Bourlard, M. Cohen, and H. Franco, </author> <title> "Connectionist probability estimators in HMM speech recognition," </title> <journal> IEEE Trans. on Speech and Audio Processing, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 161-174, </pages> <year> 1994. </year>
Reference-contexts: Any system can be improved in a variety of ways, and so there is no conclusive evidence that connectionist speech recognition algorithms are better than other approaches. Nonetheless, in a number of controlled experiments, the use of connectionist elements in a larger statistical system has significantly improved performance <ref> [2, 47, 65] </ref>. Further, connectionist speech research can provide the opportunity to develop a new paradigm that also has a firm theoretical footing, and which may lead to newer innovations that will improve the state of the art. <p> We and others have performed numerous experiments that have verified these two points. In some of them, a fixed HMM was used and alternate probability estimators were substituted <ref> [7, 56, 9, 65, 67, 47] </ref>. When these experiments were controlled for the number of parameters, there have been significant improvements using the approaches described here. <p> For equivalent performance, the classical HMM system must be made much more complicated (for instance, typically using context-dependent models and many more parameters). For instance, for the 1000 word vocabulary, perplexity 60 wordpair grammar Resource Management continuous speech recognition task, it was reported in <ref> [65] </ref> that a context-independent version of SRI's DECIPHER system had 11% word error on a particular Feb 1991 evaluation test set. The same system using MLP outputs as probability estimates achieved 5.8% errors using a similar number of parameters. <p> For instance, this was shown for the connected digits example given above. In that case, combining emission probability estimates for the best Gaussian mixture system with those from the MLP gave roughly a 1.7% string error, which was the best performance reported. Similarly, in <ref> [65] </ref>, the same MLP estimator that yielded a 5.8% word error on the Resource Management task was used to improve a complex Gaussian mixture system from 3.8% error to 3.2% error. In both experiments, the researchers smoothed together log emission probability estimates.
Reference: [66] <author> M. D. Richard and R. P. Lippmann, </author> <title> "Neural network classifiers estimate Bayesian a posteriori probabilities." </title> <journal> Neural Computation, </journal> <volume> no. 3, </volume> <pages> pp. 461-483, </pages> <year> 1991. </year>
Reference-contexts: This can be converted to emission probabilities using Bayes' rule. Several authors have shown that the outputs of ANNs used in classification mode can be interpreted as estimates of a posteriori probabilities of output classes conditioned on the input [9], [10], [27], <ref> [66] </ref>. Before we repeat this proof, the general principle can be geometrically motivated (see Figure 4). The figure shows that an equilibrium that is reached in the ideal case does in fact correspond to the network output g being equal to the posterior probability p. <p> The downward force is similarly defined, and will balance for the equilibrated case. This will only occur when the network output is equal to the posterior probability 6 . A more formal proof, originally given in <ref> [66] </ref>, is repeated here. <p> This shows that a discriminant function obtained by minimizing the MSE retains the essential property of being the best approximation to the Bayes probabilities in the sense of mean square error. A similar proof was given in <ref> [66] </ref> for the relative entropy cost function. Since these proofs are only based on the minimized criterion (and not on the architecture of the network), they are valid for any of the ANNs discussed in Section 2.3.2, given two conditions: 1.
Reference: [67] <author> T. Robinson and F. Fallside, </author> <title> "A recurrent error propagation network speech recognition system," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> no. 5, </volume> <pages> pp. 259-274, </pages> <year> 1991. </year>
Reference-contexts: performance is somewhat poorer than with the far more computationally expensive MLP. * Recurrent Neural Network the group at Cambridge University Engineering Dept (CUED) has developed an approach that is very similar to the one described here, except that the emission probability is generated by a recurrent neural network (RNN) <ref> [67] </ref>. The network uses a set of state units that takes the acoustic input and has a recurrent connection from the output of the state layer back to its input. The state layer output along with the acoustic input is connected to the output layer. <p> We and others have performed numerous experiments that have verified these two points. In some of them, a fixed HMM was used and alternate probability estimators were substituted <ref> [7, 56, 9, 65, 67, 47] </ref>. When these experiments were controlled for the number of parameters, there have been significant improvements using the approaches described here.
Reference: [68] <author> T. Robinson, L. Almeida, J.M. Boite, H. Bourlard, F. Fallside, M. Hochberg, D. Kershaw, P. Kohn, Y. Konig, N. Morgan, J.P. Neto, S. Renals, M. Saerens, & C. Wooters, </author> <title> "A neural network based, speaker independent, large vocabulary, continuous speech recognition system: The WERNICKE Project," </title> <booktitle> Proc. </booktitle> <address> EUROSPEECH'93 (Berlin, Germany), </address> <pages> pp. 1941-1944, </pages> <year> 1993. </year>
Reference-contexts: For further discussion about this with more results, see <ref> [68] </ref>. In [47], similar conclusions were also drawn for quite a different example, connected digit recognition for a standard TI database. In this case, string error for a moderate-sized MLP (about 11000 parameters) was about 2.5%, while the string error for a 28,000 parameter tied mixture system was 3.8%.
Reference: [69] <author> F. Rosenblatt, </author> <title> Principles of Neurodynamics. Perceptrons and the Theory of Brain Mechanisms, </title> <publisher> Spartan Books, </publisher> <address> Washington, </address> <year> 1962. </year>
Reference-contexts: On the output units, the nonlinearity can be viewed as a differentiable approximation to the decision threshold of a threshold logic unit or perceptron <ref> [69] </ref>, i.e., essentially to count errors. For this purpose, the output nonlinearity should be a sigmoid or sigmoid-like function. Alternatively, a function called the softmax can be used, as it approximates a statistical sigmoid function. <p> MLP parameters (the elements of the weight matrices) are trained to associate a "desired" output vector with an input vector. This is achieved via the Error Back-Propagation (EBP) algorithm (see [58], [59], [70] and [81] for multilayer networks; [1], <ref> [69] </ref> and [83] for single layer networks; [15] for a control theory version) that uses a steepest descent procedure to iteratively minimize a cost function. 8 consists of a series of units represented by circles.
Reference: [70] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> "Learning internal representations by error propagation," in Parallel Distributed Procressing (D. </title> <editor> E. Rumelhart and J.L. McClelland, Eds.), </editor> <volume> vol. 1, </volume> <pages> pp. 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1986. </year>
Reference-contexts: MLP parameters (the elements of the weight matrices) are trained to associate a "desired" output vector with an input vector. This is achieved via the Error Back-Propagation (EBP) algorithm (see [58], [59], <ref> [70] </ref> and [81] for multilayer networks; [1], [69] and [83] for single layer networks; [15] for a control theory version) that uses a steepest descent procedure to iteratively minimize a cost function. 8 consists of a series of units represented by circles.
Reference: [71] <author> R. Schaefer and L. Rabiner, </author> <title> "Digital representations of speech signals," </title> <booktitle> in Proceedings of the IEEE, </booktitle> <volume> vol. 63, no. 4, </volume> <pages> pp. 662-667, </pages> <year> 1975. </year>
Reference-contexts: The next two blocks in Figure 1 illustrate the core acoustic pattern matching operations of speech recognition. In nearly all ASR systems, a representation of speech, such as a spectral or cepstral representation <ref> [71] </ref> is computed over successive intervals, e.g., 100 times per second. These representations or speech frames are then compared (in some sense) to the spectra or cepstra for speech that were used for training, using some measure of similarity or distance.
Reference: [72] <author> M. Schenkel, I. Guyon, I., & D. Henderson, </author> <title> "On-line cursive script recognition using neural networks and hidden Markov models," </title> <booktitle> Proc. of Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. </pages> <address> II.637-640, Adelaide, Australia, </address> <year> 1994. </year>
Reference-contexts: We note briefly that related work is also being done to apply hybrid HMM/ANN methods to non-speech applications such as handwriting recognition (see, e.g., <ref> [72] </ref>). 6.1 Learning Pronunciations with Nets The quality of phonological models have sometimes been observed to have a significant effect on ASR performance [17].
Reference: [73] <author> R. Schwartz, </author> <title> Oral presentation, </title> <booktitle> Speech Research Symposium XIII, </booktitle> <publisher> Johns Hopkins, </publisher> <year> 1993. </year>
Reference: [74] <author> R. Salomon, </author> <title> Oral presentation, </title> <type> ICSI, </type> <month> July, </month> <year> 1994. </year>
Reference-contexts: However, it is possible that breaking up the probability estimation into several pieces can ultimately be more computationally efficient. In some preliminary experiments with small networks that discriminate between each class and all others, we have observed comparable performance to what we achieve with a monolithic network <ref> [74] </ref>.
Reference: [75] <author> H. Sorenson, </author> <title> "A cepstral noise reduction multi-layer network," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoustic, Speech, and Signal Processing Toronto, Canada, </booktitle> <pages> pp. 933-936, </pages> <year> 1991. </year> <month> 39 </month>
Reference-contexts: In other designs, researchers have experimented with networks to provide mappings from noisy to clean data <ref> [75] </ref> or from a new speaker to an old speaker [34]. * Postprocessing Because of interest in more complex models of speech and the difficulty in training such models (such as the segmental models mentioned above), some researchers have used more computationally demanding approaches as a postprocessing step in speech recognition.
Reference: [76] <author> E. Singer and R. Lippmann, </author> <title> "A speech recognizer using radial basis function neural networks in an HMM framework," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoustic, Speech, and Signal Processing (San Francisco, </booktitle> <address> CA), </address> <pages> pp. 629-632, </pages> <year> 1992. </year>
Reference-contexts: These systems can be trained with much less computation than sigmoid or softmax-based MLPs. The outputs can also be used as probabilities and integrated into a hybrid system <ref> [64, 76] </ref>.
Reference: [77] <author> A. Stolcke and S. Omohundro, </author> <title> "Hidden Markov model induction by Bayesian model merging," </title> <booktitle> in Advances in Neural Information Processing Systems 5 (S.J. </booktitle> <editor> Hanson, J.D. Cowan, and C.L. Giles, Eds.), </editor> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Training techniques are the same as those described earlier in this article. 5. Generate new word models by including all examples of pronunciations as labeled in the training data. Merge them using the Stolcke-Omohundro HMM merging technique <ref> [77] </ref>, which uses a Bayesian criterion to create a HMM from examples. 6. Run steps 3 through 5 until performance on an independent cross-validation set does not improve appreciably.
Reference: [78] <author> J. Tebelskis and A. Waibel, </author> <title> "Large vocabulary recognition using linked predictive neural networks," </title> <booktitle> in Proc. IEEE Intl. Conf. on Acoustic, Speech, and Signal Processing (Albuquerque, </booktitle> <address> NM), </address> <pages> pp. 437-440, </pages> <year> 1990. </year>
Reference-contexts: This can be shown to be a variant of the maximum likelihood estimate in which the dynamic of the system (acoustic correlation) is taken into account. This could either be done using multiple networks (one per state) <ref> [78] </ref>, or with one network that uses the state identity as a control input [43]. Such systems have been successfully trained to do connected digit recognition. They also have sometimes been used for larger tasks, but apparently have not been as successful as classification-based hybrids.
Reference: [79] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang, </author> <title> "Phoneme recognition: neural networks vs. hidden Markov models," </title> <booktitle> in Proc. IEEE Intl. Conf. on Acoustic, Speech, and Signal Processing (NY, </booktitle> <address> NY), </address> <pages> pp. 107-110, </pages> <year> 1988. </year>
Reference-contexts: This feedforward approach was shown to be useful with a single convolutional layer for consonant recognition in 1983 by Makino [48], and later by Waibel et al for a similar task using multiple convolutional layers <ref> [79] </ref> and finally for hybrid systems such as those described in this article [24]. It is a flexible structure, and in practice many researchers (including us) have used a compromise between the simple MLP and a full TDNN in which delays are only used at the input layer. <p> This is the way ANNs were initially used on simple speech recognition problems (see, e.g., <ref> [60, 80, 79] </ref>). However, ANNs classifying complete temporal sequences have not been successful for continuous speech recognition. In fact, used as such they are not likely to work well for continuous speech, since the number of possible word sequences in an utterance is generally infinite.
Reference: [80] <author> R. Watrous and L. Shastri, </author> <title> "Learning phonetic features using connectionist networks: an experiment in speech recognition," </title> <booktitle> in Proc. First Intl. Conf. on Neural Networks, </booktitle> <address> (San Diego, CA), </address> <booktitle> vol. </booktitle> <volume> 2, </volume> <pages> pp. 619-627, </pages> <year> 1987. </year>
Reference-contexts: This is the way ANNs were initially used on simple speech recognition problems (see, e.g., <ref> [60, 80, 79] </ref>). However, ANNs classifying complete temporal sequences have not been successful for continuous speech recognition. In fact, used as such they are not likely to work well for continuous speech, since the number of possible word sequences in an utterance is generally infinite.
Reference: [81] <author> P. Werbos, </author> <title> "Beyond regression: new tools for prediction and analysis in the behavioral sciences," </title> <type> PhD Thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, MA, </address> <year> 1974. </year>
Reference-contexts: MLP parameters (the elements of the weight matrices) are trained to associate a "desired" output vector with an input vector. This is achieved via the Error Back-Propagation (EBP) algorithm (see [58], [59], [70] and <ref> [81] </ref> for multilayer networks; [1], [69] and [83] for single layer networks; [15] for a control theory version) that uses a steepest descent procedure to iteratively minimize a cost function. 8 consists of a series of units represented by circles.
Reference: [82] <author> P. J. Werbos, </author> <title> "Backpropagation through time: what it does and how to do it," </title> <journal> Proceedings IEEE, </journal> <volume> vol. 78, </volume> <pages> pp. 1150-1160, </pages> <year> 1990. </year>
Reference: [83] <author> B. Widrow and M. Hoff, </author> <title> "Adaptive Switching Circuits," </title> <type> Technical Reports 1553-1, </type> <institution> Stanford University, Electron. Labs., Stanford, </institution> <address> CA, </address> <year> 1960. </year>
Reference-contexts: MLP parameters (the elements of the weight matrices) are trained to associate a "desired" output vector with an input vector. This is achieved via the Error Back-Propagation (EBP) algorithm (see [58], [59], [70] and [81] for multilayer networks; [1], [69] and <ref> [83] </ref> for single layer networks; [15] for a control theory version) that uses a steepest descent procedure to iteratively minimize a cost function. 8 consists of a series of units represented by circles.
Reference: [84] <author> P. Woodland, and S. Young, </author> <title> "The HTK tied-state continuous speech recognizer," </title> <booktitle> Eurospeech '93, </booktitle> <pages> pp. 2207-2210, </pages> <year> 1993. </year>
Reference-contexts: It is now feasible to develop new applications by following mathematical prescriptions that have been developed under these previous programs. Many people have been able to use HMM-based approaches as the core of impressive recognition systems. Recently, one such system, the HMM Tool Kit (HTK) from Cambridge <ref> [84] </ref> has been widely distributed so that students and developers can build up speech recognition systems based on some of the best known statistical approaches. However, speech recognition is not a solved problem in any fundamental sense.
Reference: [85] <author> C. Wooters, </author> <title> "Lexical modeling in a speaker-independent speech understanding system," </title> <type> ICSI Technical Report TR-93-068, also a UC Berkeley PhD Thesis. 40 </type>
References-found: 85

