URL: http://www.cs.cornell.edu/Info/People/ergun/colt.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/ergun/research.html
Root-URL: http://www.cs.cornell.edu
Title: On Learning Bounded-Width Branching Programs  
Author: Funda Erg un S Ravi Kumar Ronitt Rubinfeld 
Address: Ithaca, NY 14853.  
Affiliation: Department of Computer Science Cornell University  
Abstract: In this paper, we study PAC-learning algorithms for specialized classes of deterministic finite automata (DFA). In particular, we study branching programs, and we investigate the influence of the width of the branching program on the difficulty of the learning problem. We first present a distribution-free algorithm for learning width-2 branching programs. We also give an algorithm for the proper learning of width-2 branching programs under uniform distribution on labeled samples. We then show that the existence of an efficient algorithm for learning width-3 branching programs would imply the existence of an efficient algorithm for learning DNF, which is not known to be the case. Finally, we show that the existence of an algorithm for learning width-3 branching programs would also yield an algorithm for learning a very restricted version of parity with noise.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Abe and M. Warmuth. </author> <title> On the Computational Complexity of Approximating Distributions by Probabilistic Automata. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 205-260, </pages> <year> 1992. </year>
Reference: [2] <author> D. Angluin. </author> <title> On the Complexity of Minimum Inference of Regular Sets. </title> <journal> Information and Control, </journal> <volume> 39 </volume> <pages> 337-350, </pages> <year> 1978. </year>
Reference-contexts: the i-th column depends on an arbitrary x j and more than one column may depend on any particular x j . of finding the smallest automaton consistent with a given set of samples, and even approximating the number of states in the automaton by a polynomial, is NP-hard ([9], <ref> [2] </ref>, [15]). Even if the condition on the representation of the hypothesis is relaxed, the problem does not become easier: In [13], prediction-preserving reductions of [14] are used to show that (under cryptographic assumptions), predicting the class by any reasonable representation using random examples is hard.
Reference: [3] <author> D. Angluin. </author> <title> A Note on the Number of Queries Needed to Identify Regular Languages. </title> <journal> Information and Control, </journal> <volume> 51 </volume> <pages> 76-87, </pages> <year> 1981. </year>
Reference: [4] <author> D. Angluin. </author> <title> Learning Regular Sets from Queries and Counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: It is shown that learning k-BPs is equivalent to learning k-state automata (over a polynomial size alphabet). In the stronger model of learning finite automata with membership queries, the task seems to be less difficult. In <ref> [4] </ref>, an algorithm is given which learns DFA, given access to a teacher that answers questions. This algorithm assumes that the automaton is reset between queries.
Reference: [5] <author> D. Angluin and P. Laird. </author> <title> Learning from Noisy Examples. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: Proof: We use similar ideas from <ref> [5] </ref>, [19]. Let M 00 2 W 2 with l-stages be an *-bad hypothesis with respect to M fl . Then, given a random set of samples S, the expected number of samples on which M 00 disagrees with M fl is *jSj.
Reference: [6] <author> D. Barrington. </author> <title> Bounded-Width Polynomial-Size Branching Programs Recognize Exactly Those Languages in NC 1 . JCSS, </title> <booktitle> 38 </booktitle> <pages> 150-164. </pages> <year> 1989. </year>
Reference-contexts: The third author was also supported by the United States - Israel Binational Science Foundation Grant 92-00226. of DFA referred to as bounded-width branching programs. We use the following definition of bounded-width branching programs which is similar to that given in <ref> [6] </ref>, and is a subclass of the more traditional notion of width-w branching programs defined in [7]. <p> The problems of learning polynomial size automata and learning polynomial size BPs are reducible to each other by prediction-preserving reductions. The languages accepted by 5-BPs have been shown to contain all of NC 1 <ref> [6] </ref>. Thus, by the results of [13], it is NP-hard to learn the class of 5-BPs. On the other hand, in this paper we give an algorithm to learn the class of 2-BPs in the PAC-setting. We then prove that learning 3-BPs is as hard as learning DNF. <p> In the case of learning from examples over which the learner has no control, it has been shown that the problem 1 This is more restrictive than the definition in <ref> [6] </ref>, where the i-th column depends on an arbitrary x j and more than one column may depend on any particular x j . of finding the smallest automaton consistent with a given set of samples, and even approximating the number of states in the automaton by a polynomial, is NP-hard
Reference: [7] <author> A. Borodin, D. Dolev, F. Fich, and W. Paul. </author> <title> Bounds for Width Two Branching Programs. </title> <journal> SIAM J. Computing. </journal> <volume> 15 </volume> <pages> 549-560. </pages> <year> 1986. </year>
Reference-contexts: We use the following definition of bounded-width branching programs which is similar to that given in [6], and is a subclass of the more traditional notion of width-w branching programs defined in <ref> [7] </ref>.
Reference: [8] <author> Y. Freund, M. Kearns, D. Ron, R. Rubinfeld, R. Schapire, and L. Sellie. </author> <title> Efficient Learning of Typical Finite Automata from Random Walks. </title> <booktitle> In Proc. 25th STOC, </booktitle> <pages> pages 315-324. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: Even if the condition on the representation of the hypothesis is relaxed, the problem does not become easier: In [13], prediction-preserving reductions of [14] are used to show that (under cryptographic assumptions), predicting the class by any reasonable representation using random examples is hard. However, in <ref> [8] </ref>, algorithms are given for efficient learning of typical DFA (automata for which the underlying graph is chosen adversarially but the accept/reject labels at each state are chosen randomly) from random examples, even when there is no means of resetting the machine.
Reference: [9] <author> E. Gold. </author> <title> Complexity of Automaton Identification from Given Data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference: [10] <author> D. Helmbold, R. Sloan, and M. K. Warmuth. </author> <title> Learning Integer Lattices. </title> <journal> SIAM J. Computing, </journal> <volume> 21 </volume> <pages> 240-266, </pages> <year> 1992. </year>
Reference-contexts: We use M fl (~x) to denote the function computed by M fl , i.e., M fl (~x) = 1 if M fl accepts on input ~x and M fl (~x) = 0 otherwise. Linear functions can be learned by solving systems of equations <ref> [10] </ref>. It is also easy to construct a linear automaton from a linear function. Let the algorithm for learning linear automata be linear-explain (i; S), where S is a labeled set of examples.
Reference: [11] <author> M. Kearns, M. Li, L. Pitt, and L. Valiant. </author> <title> On the Learn-ability of Boolean Formulae. </title> <booktitle> In Proc. 19th STOC, </booktitle> <pages> pages 285-295. </pages> <publisher> ACM, </publisher> <year> 1987. </year>
Reference-contexts: The exact complexity of learning DNF (without queries) is not known. However, under uniform distribution on labeled samples, DNF are efficiently learnable with queries. In <ref> [11] </ref>, it is shown that the class of k-term DNF is not properly learnable unless NP = RP ([16], [11]). 3 Definitions Let W k denote the concept class of width-k branching programs. Let l denote the length of the branching program. <p> The exact complexity of learning DNF (without queries) is not known. However, under uniform distribution on labeled samples, DNF are efficiently learnable with queries. In <ref> [11] </ref>, it is shown that the class of k-term DNF is not properly learnable unless NP = RP ([16], [11]). 3 Definitions Let W k denote the concept class of width-k branching programs. Let l denote the length of the branching program. Let ~x = x 1 ; x 2 ; : : : ; x l be the input.
Reference: [12] <author> M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. Schapire, and L. Sellie. </author> <title> Efficient Learning of Typical Finite Automata from Random Walks. </title> <booktitle> In Proc. 26th STOC, </booktitle> <pages> pages 273-282. </pages> <publisher> ACM, </publisher> <year> 1994. </year>
Reference-contexts: A walk on the automaton follows edges leaving the current state, chosen according to the probability labels, and outputs the alphabet character labeling that edge. The problem of learning PFA is hard ([1], <ref> [12] </ref>). In fact, even learning width-2 PFA is known to be hard ([12]), based on the hardness of parity with noise which is the following problem: Let f be the parity function computed by a parity automaton, define a parameter 0 &lt; 1=2 called the noise rate.
Reference: [13] <author> M. Kearns and L. Valiant. </author> <title> Cryptographic Limitations on Learning Boolean Formulae and Finite Automata. </title> <booktitle> In Proc. 21st STOC, </booktitle> <pages> pages 433-444. </pages> <publisher> ACM. </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction The problem of learning deterministic finite state automata (DFA) has been well studied in recent years. In general, it is hard to learn the class of DFA in the PAC-learning model ([14], <ref> [13] </ref>). However, there are PAC-learning algorithms for specialized classes of DFA. The techniques used to design them have been adapted for use in algorithms for several applications including text correction, DNA sequencing, part-of-speech tagging, and handwriting recognition [20], [24], and [21]. <p> The problems of learning polynomial size automata and learning polynomial size BPs are reducible to each other by prediction-preserving reductions. The languages accepted by 5-BPs have been shown to contain all of NC 1 [6]. Thus, by the results of <ref> [13] </ref>, it is NP-hard to learn the class of 5-BPs. On the other hand, in this paper we give an algorithm to learn the class of 2-BPs in the PAC-setting. We then prove that learning 3-BPs is as hard as learning DNF. <p> Even if the condition on the representation of the hypothesis is relaxed, the problem does not become easier: In <ref> [13] </ref>, prediction-preserving reductions of [14] are used to show that (under cryptographic assumptions), predicting the class by any reasonable representation using random examples is hard.
Reference: [14] <author> L. Pitt and M. Warmuth. </author> <title> Prediction-preserving Reducibility. </title> <journal> JCSS, </journal> <volume> 41 </volume> <pages> 430-467, </pages> <year> 1990. </year>
Reference-contexts: Even if the condition on the representation of the hypothesis is relaxed, the problem does not become easier: In [13], prediction-preserving reductions of <ref> [14] </ref> are used to show that (under cryptographic assumptions), predicting the class by any reasonable representation using random examples is hard. <p> Our reduction is similar to the reduction in <ref> [14] </ref>. Our original reduction showed that learning W 3 is hard as learning decision trees. Rob Schapire ([22]) has pointed out that a similar reduction can be used to relate W 3 and DNF. <p> The instance transformation ~x 7! ~x 0 is clearly polynomial (squares the input length) and the size of the image concept in W 3 is linear in the size of the concept in F . Hence, our reduction is compliant with the notion of reduction as defined in <ref> [14] </ref>. 5.2 An Application Consider the class of probabilistic finite automata (PFA), which are automata in which each edge is labeled with a probability and an alphabet character.
Reference: [15] <author> L. Pitt and M. Warmuth. </author> <title> The Minimum Consistent DFA Problem Cannot be Approximated Within Any Polynomial. </title> <journal> J. ACM, </journal> <volume> 40 </volume> <pages> 95-142, </pages> <year> 1993. </year>
Reference-contexts: i-th column depends on an arbitrary x j and more than one column may depend on any particular x j . of finding the smallest automaton consistent with a given set of samples, and even approximating the number of states in the automaton by a polynomial, is NP-hard ([9], [2], <ref> [15] </ref>). Even if the condition on the representation of the hypothesis is relaxed, the problem does not become easier: In [13], prediction-preserving reductions of [14] are used to show that (under cryptographic assumptions), predicting the class by any reasonable representation using random examples is hard.
Reference: [16] <author> L. Pitt and L. Valiant. </author> <title> Computational Limitations on Learning from Examples. </title> <journal> J. ACM, </journal> <volume> 35 </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference: [17] <author> R. Rivest and S. Schapire. </author> <title> Diversity-based Inference of Finite Automata. </title> <booktitle> In Proc. 28th FOCS, </booktitle> <pages> pages 78-87. </pages> <publisher> IEEE, </publisher> <year> 1987. </year>
Reference-contexts: In the stronger model of learning finite automata with membership queries, the task seems to be less difficult. In [4], an algorithm is given which learns DFA, given access to a teacher that answers questions. This algorithm assumes that the automaton is reset between queries. In <ref> [17] </ref> and [18], this assumption is discarded and the algorithms presented learn automata from input/output behavior, in the absence of a means of resetting the machine to a start state. The exact complexity of learning DNF (without queries) is not known.
Reference: [18] <author> R. Rivest and S. Schapire. </author> <title> Inference of Finite Automata Using Homing Sequences. </title> <booktitle> In Proc. 21st STOC, </booktitle> <pages> pages 411-420. </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: In the stronger model of learning finite automata with membership queries, the task seems to be less difficult. In [4], an algorithm is given which learns DFA, given access to a teacher that answers questions. This algorithm assumes that the automaton is reset between queries. In [17] and <ref> [18] </ref>, this assumption is discarded and the algorithms presented learn automata from input/output behavior, in the absence of a means of resetting the machine to a start state. The exact complexity of learning DNF (without queries) is not known.
Reference: [19] <author> D. Ron and R. Rubinfeld. </author> <title> Learning Fallible Finite State Automata. </title> <booktitle> In Proc. 6th COLT, </booktitle> <pages> pages 218-227. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: Proof: We use similar ideas from [5], <ref> [19] </ref>. Let M 00 2 W 2 with l-stages be an *-bad hypothesis with respect to M fl . Then, given a random set of samples S, the expected number of samples on which M 00 disagrees with M fl is *jSj.
Reference: [20] <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> The Power of Amnesia. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> Volume 6, </volume> <publisher> Morgan Kauffman, </publisher> <year> 1993. </year>
Reference-contexts: However, there are PAC-learning algorithms for specialized classes of DFA. The techniques used to design them have been adapted for use in algorithms for several applications including text correction, DNA sequencing, part-of-speech tagging, and handwriting recognition <ref> [20] </ref>, [24], and [21]. In this paper, we focus on learning algorithms for a subclass fl This work was supported by ONR Young Investigator Award N00014-93-1-0590.
Reference: [21] <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> On the Learnability and Usage of Acyclic Probabilistic Finite Automata. </title> <type> Manuscript, </type> <month> January </month> <year> 1995. </year>
Reference-contexts: However, there are PAC-learning algorithms for specialized classes of DFA. The techniques used to design them have been adapted for use in algorithms for several applications including text correction, DNA sequencing, part-of-speech tagging, and handwriting recognition [20], [24], and <ref> [21] </ref>. In this paper, we focus on learning algorithms for a subclass fl This work was supported by ONR Young Investigator Award N00014-93-1-0590. The third author was also supported by the United States - Israel Binational Science Foundation Grant 92-00226. of DFA referred to as bounded-width branching programs.
Reference: [22] <author> R. Schapire. </author> <type> Personal Communication. </type> <year> 1995. </year>
Reference: [23] <author> R. Schapire and M. Warmuth. </author> <type> Personal Communication. </type> <year> 1990. </year>
Reference-contexts: However, in [8], algorithms are given for efficient learning of typical DFA (automata for which the underlying graph is chosen adversarially but the accept/reject labels at each state are chosen randomly) from random examples, even when there is no means of resetting the machine. In <ref> [23] </ref>, the problem of learning automata with a very small number of states (where the alphabet size is not constant) is investigated. It is shown that learning k-BPs is equivalent to learning k-state automata (over a polynomial size alphabet).
Reference: [24] <author> H. Schutze and Y. Singer. </author> <title> Part-of-Speech Tagging Using a Variable Memory Markov Model, </title> <booktitle> In Proc. 32nd ACL, </booktitle> <year> 1994. </year>
Reference-contexts: However, there are PAC-learning algorithms for specialized classes of DFA. The techniques used to design them have been adapted for use in algorithms for several applications including text correction, DNA sequencing, part-of-speech tagging, and handwriting recognition [20], <ref> [24] </ref>, and [21]. In this paper, we focus on learning algorithms for a subclass fl This work was supported by ONR Young Investigator Award N00014-93-1-0590. The third author was also supported by the United States - Israel Binational Science Foundation Grant 92-00226. of DFA referred to as bounded-width branching programs.
References-found: 24

