URL: http://www-wavelet.eecs.berkeley.edu/~vkgoyal/technical/it98.ps
Refering-URL: http://www-wavelet.eecs.berkeley.edu/~vkgoyal/technical/it98.html
Root-URL: 
Title: Quantized Overcomplete Expansions in R N Analysis, Synthesis, and Algorithms of quantized matching pursuit to
Author: Vivek K Goyal, Student Member, IEEE, Martin Vetterli, Fellow, IEEE, and Nguyen T. Thao, Member, IEEE 
Keyword: MSE reduction obtained with a nonlinear (consistent) reconstruction algorithm, and generally competitive performance at low bit rates. Keywords| quantization, source coding, frames, matching pursuit, consistent reconstruction, optimal reconstruction, overcomplete representations, MSE bounds  
Date: 1, JANUARY 1998  
Note: 16 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO.  Applicability  is inconsistent, the  
Abstract: Coefficient quantization has peculiar qualitative effects on representations of vectors in R N with respect to overcomplete sets of vectors. These effects are investigated in two settings: frame expansions (representations obtained by forming inner products with each element of the set) and matching pursuit expansions (approximations obtained by greedily forming linear combinations). In both cases, based on the concept of consistency, it is shown that traditional linear reconstruction methods are suboptimal, and better consistent reconstruction algorithms are given. The proposed consistent reconstruction algorithms were in each case implemented, and experimental results are included. For frame expansions, results are proven to bound distortion as a function of frame redundancy r and quantization step size for linear, consistent, and optimal reconstruction methods. Taken together, these suggest that optimal reconstruction methods will yield O(1=r 2 ) MSE, and that consistency is sufficient to insure this asymptotic behavior. A result on the asymptotic tightness of random frames is also proven. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. T. Thao and M. Vetterli, </author> <title> "Reduction of the MSE in R-times oversampled A/D conversion from O(1=R) to O(1=R 2 )," IEEE Trans. </title> <booktitle> Signal Proc., </booktitle> <volume> vol. SP-42, no. 1, </volume> <pages> pp. 200-203, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: Both fixed and adaptive basis methods are studied. Although it represents an input vector as a linear combination of elements from a representation set, the adaptive basis method is in fact a nonlinear mapping. While many other issues are explored, the unifying theme is that consistent reconstruction methods <ref> [1] </ref> give considerable improvement over linear reconstruction methods. Consider the expansion-quantization-reconstruction scenario depicted in Figure 1. A vector x 2 C N is left multiplied by a matrix F 2 C MfiN of rank N to get y 2 C M . <p> For example, Munch [11] considered a particular type of frame and assumed the coefficients were subject to a stationary noise. This paper, on the other hand, is in the same spirit as [12], <ref> [1] </ref>, [13], [14], [15] in that it utilizes the deterministic qualities of quantization. A. Frames A.1 Definitions and basics This subsection is largely adapted from [10, Ch. 3]. Some definitions and notations have been simplified because we are limiting our attention to H = R N or C N . <p> Oversampling of a periodic, bandlimited signal can be viewed as a frame operator applied to the signal, where the frame operator is associated with a tight frame. If the samples are quantized, this is exactly the situation of oversam-pled A/D conversion <ref> [1] </ref>. Let x = [x 1 x 2 x N ] T 2 R N , with N odd. Define a corresponding continuous-time signal by W X p 2kt + x 2k+1 2 sin T ; where W = N1 2 . <p> The subsequent sections consider the problem of reconstructing an estimate of an original signal from quantized frame coefficients. Classical methods are limited by the assumption that the quantization noise is white. Our approach uses deterministic qualities of quantization to arrive at the concept of consistent reconstruction <ref> [1] </ref>. Consistent reconstruction methods yield smaller reconstruction errors than classical methods. B.1 Unquantized case Let be a frame and assume the notation of xII-A.1. In this subsection, we consider the problem of recovering x from fhx; ' k ig k2K . <p> Strictly speaking, however, the assumptions on which this reconstruction is based are not valid because ^y y is a deterministic quantity depending on y, with interplay between the components. Fig. 3. Illustration of consistent reconstruction B.3 Consistent reconstruction Definition 1 (Consistency <ref> [1] </ref>) Let f : X ! Y . Let x 2 X and y = f (x). If f (^x) = y then ^x 2 X is called a consistent estimate of x from y. An algorithm that produces consistent estimates is called a consistent reconstruction algorithm. <p> For example, we can start with an orthonormal basis and increase r by adding copies of vectors already in the frame. Putting aside such pathological cases, simulations for quantization of a source uniformly distributed on <ref> [1; 1] </ref> N support this conjecture. Simulations were performed with three types of frame sequences: I. A sequence of frames corresponding to oversampled A/D conversion, as given by (8). This is the case in which we have proven an O (1=r 2 ) SE upper bound. II. <p> the four element dictionary D = cos 8 (2k 1) T k=1 Even if the distribution of the source is known, it is difficult to find analytical expressions for optimal quantizers. (The issue of optimal quantizer design is considered for the case of a source with a uniform distribution on <ref> [1; 1] </ref> 2 in [28, x3.3.2].) Since we wish to use fixed, untrained quantizers, we will use uniform quantizers for ff 0 and ff 1 . <p> &gt; 0 and r 0 1 depending only on x c (t) such that 8 M = rn r 0 n, whenever x c (t) and x 0 c (t) have the same quantized sampled versions, 1 R c (t)j 2 dt &lt; c Proof: This is a version of <ref> [1, Thm. 4.1] </ref> for real-valued signals. -b y (t) t Fig. 13. One period of the signal used in the proof of Lemma 2.
Reference: [2] <author> S. G. Mallat and Z. Zhang, </author> <title> "Matching pursuits with time frequency dictionaries," </title> <journal> IEEE Trans. Signal Proc., </journal> <volume> vol. SP-41, no. 12, </volume> <pages> pp. 3397-3415, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: In Section III the use of a greedy successive approxima GOYAL, ET AL.: QUANTIZED OVERCOMPLETE EXPANSIONS IN R N 17 tion algorithm for finding sparse linear representations with respect to an overcomplete set is studied. This algorithm, called matching pursuit (MP) <ref> [2] </ref>, has recently been applied to image coding [3], [4] and video coding [5], [6], which inherently require coarse coefficient quantization. However, to the best of our knowledge the present work is the first to describe the qualitative effects of coefficient quantization in matching pursuit. <p> More fundamentally, we can leave the usual framework of static orthogonal transform coding and consider the application of adaptive and nonlinear transforms. The matching pursuit algorithm <ref> [2] </ref>, described in xIII-A, has both adaptive and nonlinear aspects. Given a source vector x and a frame f' k g M k=1 , it produces an approximate signal representation x P n1 i=0 ff i ' k i . <p> For storage and transmission purposes, we will have to account for the indices. Matching pursuit was introduced to the signal processing community in the context of time-frequency analysis by Mallat and Zhang <ref> [2] </ref>. Mallat and his coworkers have uncovered many of its properties [23], [21], [24]. A.2 Discussion Since ff i is determined by projection, ff i ' k i ? R i+1 x. <p> MP is often said to have flexibility to differing signal structures. The archetypal illustration is that a Fourier basis provides a poor representation of functions well localized in time, while wavelet bases are not well suited to representing functions whose Fourier transforms have narrow, high frequency support <ref> [2] </ref>. The implication is that MP, with a dictionary including a Fourier basis and a wavelet basis, would avoid these difficulties. Looking at the energy compaction properties of MP gives a more extensive view of the potential of MP.
Reference: [3] <author> F. Bergeaud and S. Mallat, </author> <title> "Matching pursuit of images," </title> <booktitle> in Proc. IEEE Int. Conf. Image Proc., </booktitle> <address> Washington, DC, </address> <month> Oct. </month> <journal> 1995, </journal> <volume> vol. I, </volume> <pages> pp. 53-56. </pages>
Reference-contexts: In Section III the use of a greedy successive approxima GOYAL, ET AL.: QUANTIZED OVERCOMPLETE EXPANSIONS IN R N 17 tion algorithm for finding sparse linear representations with respect to an overcomplete set is studied. This algorithm, called matching pursuit (MP) [2], has recently been applied to image coding <ref> [3] </ref>, [4] and video coding [5], [6], which inherently require coarse coefficient quantization. However, to the best of our knowledge the present work is the first to describe the qualitative effects of coefficient quantization in matching pursuit.
Reference: [4] <author> V. K Goyal and M. Vetterli, </author> <title> "Dependent coding in quantized matching pursuit," </title> <booktitle> in Proc. SPIE Conf. on Vis. Commun. and Image Proc., </booktitle> <address> San Jose, California, </address> <month> Feb. </month> <journal> 1997, </journal> <volume> vol. 3024, </volume> <pages> pp. 2-14. </pages>
Reference-contexts: This algorithm, called matching pursuit (MP) [2], has recently been applied to image coding [3], <ref> [4] </ref> and video coding [5], [6], which inherently require coarse coefficient quantization. However, to the best of our knowledge the present work is the first to describe the qualitative effects of coefficient quantization in matching pursuit.
Reference: [5] <author> R. Neff, A. Zakhor, and M. Vetterli, </author> <title> "Very low bit rate video coding using matching pursuits," </title> <booktitle> in Proc. SPIE Conf. on Vis. Commun. and Image Proc., </booktitle> <address> Chicago, IL, </address> <month> Sept. </month> <journal> 1994, </journal> <volume> vol. 2308, </volume> <pages> pp. 47-60. </pages>
Reference-contexts: This algorithm, called matching pursuit (MP) [2], has recently been applied to image coding [3], [4] and video coding <ref> [5] </ref>, [6], which inherently require coarse coefficient quantization. However, to the best of our knowledge the present work is the first to describe the qualitative effects of coefficient quantization in matching pursuit. In particular, as in Section II, we will find that reconstruction can be improved by consistent reconstruction techniques. <p> The use of the quantized value in the residual calculation reduces the propagation of the quantization error to subsequent iterations. Although quantized matching pursuit has been applied to low bit rate compression problems <ref> [5] </ref>, [25], [6], which inherently require coarse coefficient quantization, little work has been done to understand the qualitative effects of coefficient quantization in matching pursuit. In this section we explore some of these effects. The relationship between quantized matching pursuit and other vector quantization (VQ) methods is discussed in xIII-B.1.
Reference: [6] <author> M. Vetterli and T. Kalker, </author> <title> "Matching pursuit for compression and application to motion compensated video coding," </title> <booktitle> in Proc. IEEE Int. Conf. Image Proc., </booktitle> <address> Austin, TX, </address> <month> Nov. </month> <journal> 1994, </journal> <volume> vol. 1, </volume> <pages> pp. 725-729. </pages>
Reference-contexts: This algorithm, called matching pursuit (MP) [2], has recently been applied to image coding [3], [4] and video coding [5], <ref> [6] </ref>, which inherently require coarse coefficient quantization. However, to the best of our knowledge the present work is the first to describe the qualitative effects of coefficient quantization in matching pursuit. In particular, as in Section II, we will find that reconstruction can be improved by consistent reconstruction techniques. <p> Since orthog-onalized matching pursuit does not converge significantly faster than the non-orthogonalized version for a small number of iterations [21], [25], <ref> [6] </ref>, non-orthogonalized matching pursuit is not considered hereafter. <p> The use of the quantized value in the residual calculation reduces the propagation of the quantization error to subsequent iterations. Although quantized matching pursuit has been applied to low bit rate compression problems [5], [25], <ref> [6] </ref>, which inherently require coarse coefficient quantization, little work has been done to understand the qualitative effects of coefficient quantization in matching pursuit. In this section we explore some of these effects. The relationship between quantized matching pursuit and other vector quantization (VQ) methods is discussed in xIII-B.1.
Reference: [7] <author> R. J. Duffin and A. C. Schaeffer, </author> <title> "A class of nonharmonic Fourier series," </title> <journal> Trans. Amer. Math. Soc., </journal> <volume> vol. 72, </volume> <pages> pp. 341-366, </pages> <year> 1952. </year>
Reference-contexts: The term mean squared error (MSE) is reserved for the ensemble average of SE or expected SE. II. Nonadaptive Expansions This section describes frames, which provide a general framework for studying nonadaptive linear transforms. Frames were introduced by Duffin and Schaeffer <ref> [7] </ref> in the context of non-harmonic Fourier series. Recent interest in frames has been spurred by their utility in analyzing discrete wavelet transforms [8], [9], [10] and time-frequency decompositions [11]. We are motivated by a desire to understand quantization effects and efficient representations.
Reference: [8] <author> C. Heil and D. Walnut, </author> <title> "Continuous and discrete wavelet trans forms," </title> <journal> SIAM Rev., </journal> <volume> vol. 31, </volume> <pages> pp. 628-666, </pages> <year> 1989. </year> <editor> GOYAL, ET AL.: </editor> <title> QUANTIZED OVERCOMPLETE EXPANSIONS IN R N 31 </title>
Reference-contexts: II. Nonadaptive Expansions This section describes frames, which provide a general framework for studying nonadaptive linear transforms. Frames were introduced by Duffin and Schaeffer [7] in the context of non-harmonic Fourier series. Recent interest in frames has been spurred by their utility in analyzing discrete wavelet transforms <ref> [8] </ref>, [9], [10] and time-frequency decompositions [11]. We are motivated by a desire to understand quantization effects and efficient representations. Section II-A begins with definitions and examples of frames. It concludes with a theorem on the tightness of random frames and a discussion of that result.
Reference: [9] <author> I. Daubechies, </author> <title> "The wavelet transform, time-frequency localiza tion and signal analysis," </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> vol. IT-36, </volume> <pages> pp. 961-1005, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: II. Nonadaptive Expansions This section describes frames, which provide a general framework for studying nonadaptive linear transforms. Frames were introduced by Duffin and Schaeffer [7] in the context of non-harmonic Fourier series. Recent interest in frames has been spurred by their utility in analyzing discrete wavelet transforms [8], <ref> [9] </ref>, [10] and time-frequency decompositions [11]. We are motivated by a desire to understand quantization effects and efficient representations. Section II-A begins with definitions and examples of frames. It concludes with a theorem on the tightness of random frames and a discussion of that result.
Reference: [10] <author> I. Daubechies, </author> <title> Ten Lectures on Wavelets, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: II. Nonadaptive Expansions This section describes frames, which provide a general framework for studying nonadaptive linear transforms. Frames were introduced by Duffin and Schaeffer [7] in the context of non-harmonic Fourier series. Recent interest in frames has been spurred by their utility in analyzing discrete wavelet transforms [8], [9], <ref> [10] </ref> and time-frequency decompositions [11]. We are motivated by a desire to understand quantization effects and efficient representations. Section II-A begins with definitions and examples of frames. It concludes with a theorem on the tightness of random frames and a discussion of that result. <p> This paper, on the other hand, is in the same spirit as [12], [1], [13], [14], [15] in that it utilizes the deterministic qualities of quantization. A. Frames A.1 Definitions and basics This subsection is largely adapted from <ref> [10, Ch. 3] </ref>. Some definitions and notations have been simplified because we are limiting our attention to H = R N or C N . <p> This is the problem that we address presently. In xII-B.1, we review the basic properties of reconstructing from (unquantized) frame coefficients. This material is adapted from <ref> [10] </ref>. The subsequent sections consider the problem of reconstructing an estimate of an original signal from quantized frame coefficients. Classical methods are limited by the assumption that the quantization noise is white. Our approach uses deterministic qualities of quantization to arrive at the concept of consistent reconstruction [1]. <p> In this subsection, we consider the problem of recovering x from fhx; ' k ig k2K . Let ~ F : H ! C M be the frame operator associated with ~ . It can be shown <ref> [10, Prop. 3.2.3] </ref> that ~ F fl F = I N . Thus a possible reconstruction formula is given by x = ~ F fl F x = X hx; ' k i ~' k . <p> In the DFT and Inverse DFT, one set of vectors plays the roles of both and ~ because it is a tight frame in C N . Other reconstruction formulas are possible; for details the reader is referred to <ref> [10, x3.2] </ref>. B.2 Classical method We now turn to the question of reconstructing when the frame coefficients fhx; ' k ig k2K are degraded in some way. Any mode of degradation is possible, but the most practical situations are additive noise due to measurement error or quantization. <p> Hence the component of fi perpendicular to F H should not hinder our approximation, and the best approximation is the projection of F x + fi onto Ran (F ). By <ref> [10, Prop. 3.2.3] </ref>, this approximation is given by ^x = ~ F fl (F x + fi): (9) Furthermore, because the component of fi orthogonal to Ran (F ) does not contribute, we expect kx ^xk = k ~ F fl fik to be smaller than kfik.
Reference: [11] <author> N. J. Munch, </author> <title> "Noise reduction in tight Weyl-Heisenberg frames," </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> vol. IT-38, no. 2, </volume> <pages> pp. 608-616, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Frames were introduced by Duffin and Schaeffer [7] in the context of non-harmonic Fourier series. Recent interest in frames has been spurred by their utility in analyzing discrete wavelet transforms [8], [9], [10] and time-frequency decompositions <ref> [11] </ref>. We are motivated by a desire to understand quantization effects and efficient representations. Section II-A begins with definitions and examples of frames. It concludes with a theorem on the tightness of random frames and a discussion of that result. <p> The remainder of the section gives new results on reconstruction from quantized frame coefficients. Most previous work on frame expansions is predicated either on exact knowledge of coefficients or on coefficient degradation by white additive noise. For example, Munch <ref> [11] </ref> considered a particular type of frame and assumed the coefficients were subject to a stationary noise. This paper, on the other hand, is in the same spirit as [12], [1], [13], [14], [15] in that it utilizes the deterministic qualities of quantization. A.
Reference: [12] <author> N. T. Thao, </author> <title> Deterministic Analysis of Oversampled A/D Con version and Sigma-Delta Modulation, and Decoding Improvements using Consistent Estimates, </title> <type> Ph.D. thesis, </type> <institution> Columbia Univ., </institution> <year> 1993. </year>
Reference-contexts: For example, Munch [11] considered a particular type of frame and assumed the coefficients were subject to a stationary noise. This paper, on the other hand, is in the same spirit as <ref> [12] </ref>, [1], [13], [14], [15] in that it utilizes the deterministic qualities of quantization. A. Frames A.1 Definitions and basics This subsection is largely adapted from [10, Ch. 3].
Reference: [13] <author> N. T. Thao and M. Vetterli, </author> <title> "Deterministic analysis of over sampled A/D conversion and decoding improvement based on consistent estimates," </title> <journal> IEEE Trans. Signal Proc., </journal> <volume> vol. SP-42, no. 3, </volume> <pages> pp. 519-531, </pages> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: For example, Munch [11] considered a particular type of frame and assumed the coefficients were subject to a stationary noise. This paper, on the other hand, is in the same spirit as [12], [1], <ref> [13] </ref>, [14], [15] in that it utilizes the deterministic qualities of quantization. A. Frames A.1 Definitions and basics This subsection is largely adapted from [10, Ch. 3]. Some definitions and notations have been simplified because we are limiting our attention to H = R N or C N .
Reference: [14] <author> N. T. Thao and M. Vetterli, </author> <title> "Lower bound on the mean-squared error in oversampled quantization of periodic signals using vector quantization analysis," </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> vol. IT-42, no. 2, </volume> <pages> pp. 469-479, </pages> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: For example, Munch [11] considered a particular type of frame and assumed the coefficients were subject to a stationary noise. This paper, on the other hand, is in the same spirit as [12], [1], [13], <ref> [14] </ref>, [15] in that it utilizes the deterministic qualities of quantization. A. Frames A.1 Definitions and basics This subsection is largely adapted from [10, Ch. 3]. Some definitions and notations have been simplified because we are limiting our attention to H = R N or C N . <p> This type of partition was studied in <ref> [14] </ref> and is called a "hyperplane wave partition." The number 1 ` i represents the density of hyperplanes, or the number of hyperplanes per unit length in their orthogonal direction. <p> Thus quantization of y k gives a set of parallel hyperplanes spaced by , called a hyperplane single wave. The M hyperplane single waves give a partition with a particular structure called a hyper-plane wave partition <ref> [14] </ref>. Examples of hyperplane wave partitions are shown in Figure 14. In each figure, a set of vectors comprising a frame in R 2 is shown superimposed on the hyperplane wave partition induced by quantized frame expansion with that frame.
Reference: [15] <author> Z. Cvetkovic and M. Vetterli, </author> <title> "Error analysis in oversampled A/D conversion and quantization of Weyl-Heisenberg frame expansions," </title> <journal> IEEE Trans. Info. Theory, </journal> <note> 1996, submitted. </note>
Reference-contexts: For example, Munch [11] considered a particular type of frame and assumed the coefficients were subject to a stationary noise. This paper, on the other hand, is in the same spirit as [12], [1], [13], [14], <ref> [15] </ref> in that it utilizes the deterministic qualities of quantization. A. Frames A.1 Definitions and basics This subsection is largely adapted from [10, Ch. 3]. Some definitions and notations have been simplified because we are limiting our attention to H = R N or C N .
Reference: [16] <author> D. C. Youla, </author> <title> "Mathematical theory of image restoration by the method of convex projections," in Image Recovery: Theory and Application, </title> <editor> H. Stark, Ed. </editor> <publisher> Academic Press, </publisher> <year> 1987. </year>
Reference-contexts: As illustrated, this type of reconstruction is not necessarily consistent. Further geometric interpretations of quantized frame expansions are given in Appendix B. With no assumptions on Q other than that the partition regions be convex, a consistent estimate can be determined using the projection onto convex sets (POCS) algorithm <ref> [16] </ref>. In this case that implies generating a sequence of estimates by alternately projecting on F (R N ) and Q 1 (^y). When Q is a scalar quantizer and each component quan-tizer is uniform, a linear program can be used to find consistent estimates. <p> Similar experiments with different sources and dictionaries were reported in [28]. As noted earlier, the cells of the partition generated by QMP are convex or the union of two convex cells that share one point. This fact allows the computation of consistent estimates through the method of alternating projections <ref> [16] </ref>. One would normally start with an initial estimate given by (13). Given an estimate ^x, the algorithm given in Table II performs the one "most needed" projection; namely, the first projection needed in enforcing (15)-(18).
Reference: [17] <author> G. Strang, </author> <title> Introduction to Applied Mathematics, </title> <publisher> Wellesley Cambridge Press, </publisher> <year> 1986. </year>
Reference-contexts: These inequalities can be combined into F ^x 2 + ^y 2 ^y : (10) The formulation (10) shows that ^x can be determined through linear programming <ref> [17] </ref>. The feasible set of the linear program is exactly the set of consistent estimates, so an arbitrary cost function can be used. This is summarized in Table I. <p> The feasible set of the linear program is exactly the set of consistent estimates, so an arbitrary cost function can be used. This is summarized in Table I. A linear program always returns a corner of the feasible set <ref> [17, x8:1] </ref>, so this type of reconstruction will not be close to the centroid of the partition cell. Since the cells are convex, one could use several cost functions to (presumably) get different corners of the feasible set and average the results.
Reference: [18] <author> R. H. Hardin, N. J. A. Sloane, and W. D. Smith, </author> <title> "Li brary of best ways known to us to pack n points on sphere so that minimum separation is maximized," </title> <address> URL: http://www.research.att.com/ ~ njas/packings. </address>
Reference-contexts: II. For N = 3, 4, and 5, Hardin, Sloane and Smith have numerically found arrangements of up to 130 points on N -dimensional unit spheres that maximize the minimum Euclidean norm separation <ref> [18] </ref>. III. Frames generated by randomly choosing points on the unit sphere according to a uniform distribution. Simulation results are given in Figure 4. The dashed, dotted, and solid curves correspond to frame types I, II, and III, respectively. <p> Furthermore, the quantization stepsize is constant across iterations. This is consistent with equal weighting of error in each direction. C.1 Basic experimental results In the first experiment, N = 4 and the dictionary was composed of M = 11 maximally spaced points on the unit sphere <ref> [18] </ref>. Rate was measured by summing the (scalar) sample entropies of k 0 , k 1 , : : :, k p1 and ^ff 0 , ^ff 1 , : : :, ^ff p1 , where p is the number of iterations. The results are shown in Figure 9. <p> Performance comparison between reconstruction based on (13) and consistent reconstruction. N = 4 and the dictionary is composed of M = 11 maximally spaced points on the unit sphere <ref> [18] </ref>. C.2 Improved reconstruction using consistency Continuing the experiment above, the degree of improvement obtained by using a consistent reconstruction algorithm was ascertained. Using consistent reconstruction gives the performance shown by the dashed curve in Figure 9. <p> The results, shown in Figure 10, indicate a sizable improvement at low bit rates. Fig. 10. Performance comparison between a fixed number of itera tions and a simple stopping criterion. N = 4 and the dictionary is composed of M = 11 maximally spaced points on the unit sphere <ref> [18] </ref>. Fig. 11. Performance of QMP as the dictionary size is varied (solid curves, labelled by M) compared to the performance of independent uniform quantization of each sample (dotted curve).
Reference: [19] <author> H. P. Kramer and M. V. Mathews, </author> <title> "A linear coding for trans mitting a set of correlated signals," </title> <journal> IRE Trans. Info. Theory, </journal> <volume> vol. 23, </volume> <pages> pp. 41-46, </pages> <month> Sept. </month> <year> 1956. </year>
Reference-contexts: This is because many of the M additional numbers give little or no information on x. This is discussed further in Appendix B. III. Adaptive Expansions Transform coding theory, as introduced in <ref> [19] </ref> and analyzed in detail in [20], is predicated on fine quantization approximations and assuming that signals are Gaussian. For most practical coding applications, these assumptions do not hold, so the wisdom of maximizing coding gain|which leads to the optimality of the Karhunen-Loeve transform| has been questioned.
Reference: [20] <author> J.-Y. Huang and P. M. Schultheiss, </author> <title> "Block quantization of cor related Gaussian random variables," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 11, </volume> <pages> pp. 289-296, </pages> <month> Sept. </month> <year> 1963. </year>
Reference-contexts: This is because many of the M additional numbers give little or no information on x. This is discussed further in Appendix B. III. Adaptive Expansions Transform coding theory, as introduced in [19] and analyzed in detail in <ref> [20] </ref>, is predicated on fine quantization approximations and assuming that signals are Gaussian. For most practical coding applications, these assumptions do not hold, so the wisdom of maximizing coding gain|which leads to the optimality of the Karhunen-Loeve transform| has been questioned.
Reference: [21] <author> G. Davis, </author> <title> Adaptive Nonlinear Approximations, </title> <type> Ph.D. thesis, </type> <institution> New York Univ., </institution> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The use of a greedy algorithm is justified by the computational intractability of finding the optimal subset of the original frame <ref> [21, Ch. 2] </ref>. In our finite dimensional setting, this is very similar to the problem of finding sparse approximate solutions to linear systems. In that context, this greedy heuristic is well-known and performance bounds have been proven [22]. <p> For storage and transmission purposes, we will have to account for the indices. Matching pursuit was introduced to the signal processing community in the context of time-frequency analysis by Mallat and Zhang [2]. Mallat and his coworkers have uncovered many of its properties [23], <ref> [21] </ref>, [24]. A.2 Discussion Since ff i is determined by projection, ff i ' k i ? R i+1 x. Thus we have the "energy conservation" equation kR i xk = kR i+1 xk + ff 2 2 The usage of additivity is not obvious. <p> Since orthog-onalized matching pursuit does not converge significantly faster than the non-orthogonalized version for a small number of iterations <ref> [21] </ref>, [25], [6], non-orthogonalized matching pursuit is not considered hereafter.
Reference: [22] <author> B. K. Natarajan, </author> <title> "Sparse approximate solutions to linear sys tems," </title> <journal> SIAM J. Computing, </journal> <volume> vol. 24, no. 2, </volume> <pages> pp. 227-234, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: In our finite dimensional setting, this is very similar to the problem of finding sparse approximate solutions to linear systems. In that context, this greedy heuristic is well-known and performance bounds have been proven <ref> [22] </ref>. Quantization of coefficients in matching pursuit leads to many interesting issues; some of these are discussed in xIII-B. Along with exploring general properties of matching pursuit, we are interested in its application to compressing data vectors in R N .
Reference: [23] <author> Z. Zhang, </author> <title> Matching Pursuit, </title> <type> Ph.D. thesis, </type> <institution> New York Univ., </institution> <year> 1993. </year>
Reference-contexts: For storage and transmission purposes, we will have to account for the indices. Matching pursuit was introduced to the signal processing community in the context of time-frequency analysis by Mallat and Zhang [2]. Mallat and his coworkers have uncovered many of its properties <ref> [23] </ref>, [21], [24]. A.2 Discussion Since ff i is determined by projection, ff i ' k i ? R i+1 x. Thus we have the "energy conservation" equation kR i xk = kR i+1 xk + ff 2 2 The usage of additivity is not obvious.
Reference: [24] <author> G. Davis, S. Mallat, and Z. Zhang, </author> <title> "Adaptive time-frequency approximations with matching pursuits," </title> <type> Tech. Rep. 657, </type> <institution> New York Univ., </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: For storage and transmission purposes, we will have to account for the indices. Matching pursuit was introduced to the signal processing community in the context of time-frequency analysis by Mallat and Zhang [2]. Mallat and his coworkers have uncovered many of its properties [23], [21], <ref> [24] </ref>. A.2 Discussion Since ff i is determined by projection, ff i ' k i ? R i+1 x. Thus we have the "energy conservation" equation kR i xk = kR i+1 xk + ff 2 2 The usage of additivity is not obvious.
Reference: [25] <author> T. Kalker and M. Vetterli, </author> <title> "Projection methods in motion es timation and compensation," </title> <booktitle> in Proc. IS & T/SPIE, </booktitle> <address> San Jose, CA, </address> <month> Feb. </month> <journal> 1995, </journal> <volume> vol. 2419, </volume> <pages> pp. 164-175. </pages>
Reference-contexts: Since orthog-onalized matching pursuit does not converge significantly faster than the non-orthogonalized version for a small number of iterations [21], <ref> [25] </ref>, [6], non-orthogonalized matching pursuit is not considered hereafter. <p> The use of the quantized value in the residual calculation reduces the propagation of the quantization error to subsequent iterations. Although quantized matching pursuit has been applied to low bit rate compression problems [5], <ref> [25] </ref>, [6], which inherently require coarse coefficient quantization, little work has been done to understand the qualitative effects of coefficient quantization in matching pursuit. In this section we explore some of these effects. The relationship between quantized matching pursuit and other vector quantization (VQ) methods is discussed in xIII-B.1.
Reference: [26] <author> A. Gersho and R. M. Gray, </author> <title> Vector Quantization and Signal Compression, </title> <publisher> Kluwer Acad. Pub., </publisher> <address> Boston, MA, </address> <year> 1992. </year>
Reference-contexts: In orthogonal transform coding, getting high energy compaction is dependent on designing the transform based on knowledge of source statistics; for fine quantization of a stationary Gaussian source the Karhunen-Loeve Transform is optimal <ref> [26] </ref>. Although both produce an approximation for a source vector which is a linear combination of basis elements, orthogonal transform coding contrasts sharply with MP in that the basis elements are chosen a priori and hence at best one can make the optimum basis choice on average. <p> For each discrete vector ^y 2 Q (F R N ), it maps all vectors of the subset F 1 Q 1 (^y) of R N into the single vector ^x = R (^y) of R N . According to the terminology in vector quantization <ref> [26] </ref>, F 1 Q 1 (^y) is a cell of the partition defined by the vector quantizer V Q in R N , and ^x = R (^y) is the corresponding code vector. Let C be the number of cells that can be found in the region B.
Reference: [27] <author> A. Buzo, A. H. Gray, Jr., R. M. Gray, and J. D. Markel, </author> <title> "Speech coding based upon vector quantization," </title> <journal> IEEE Trans. Acoust. Speech Signal Proc., </journal> <volume> vol. ASSP-28, </volume> <pages> pp. 562-574, </pages> <month> Oct. </month> <year> 1980. </year>
Reference-contexts: B.1 Relationship to other vector quantization methods A single iteration of matching pursuit is very similar to shape-gain VQ, which was introduced in <ref> [27] </ref>. In shape-gain VQ, a vector x 2 R N is separated into a gain, g = kxk and a shape, s = x=g. A shape ^s is chosen from a shape codebook C s to maximize hx; ^si.
Reference: [28] <author> V. K Goyal, </author> <title> "Quantized overcomplete expansions: Anal ysis, synthesis and algorithms," </title> <type> Tech. Rep. </type> <institution> M95/57, UC-Berkeley/ERL, </institution> <month> July </month> <year> 1995, </year> <note> URL: http://lcavwww.epfl.ch/-~ goyal/erl-95-57.html. </note>
Reference-contexts: The results are shown in Figure 7. The probability of inconsistency goes to zero for very coarse quantization and goes to one for fine quantization. The dependence of dictionary size and lack of monotonicity indicate complicated geometric factors. Similar experiments with different sources and dictionaries were reported in <ref> [28] </ref>. As noted earlier, the cells of the partition generated by QMP are convex or the union of two convex cells that share one point. This fact allows the computation of consistent estimates through the method of alternating projections [16]. <p> D = cos 8 (2k 1) T k=1 Even if the distribution of the source is known, it is difficult to find analytical expressions for optimal quantizers. (The issue of optimal quantizer design is considered for the case of a source with a uniform distribution on [1; 1] 2 in <ref> [28, x3.3.2] </ref>.) Since we wish to use fixed, untrained quantizers, we will use uniform quantizers for ff 0 and ff 1 . <p> Such a line segment crossing a cell boundary indicates a case of (13) giving an inconsistent estimate. 5 The partition is somewhat different when the quantizer has different decision points, e.g. f (m + 1 2 )g m2Z <ref> [28, x3.3.2] </ref>. The ensuing conclusions are qualitatively unchanged. 6 Optimality is with respect to a uniform source distribution. Fig. 8. Partitioning of first quadrant of R 2 by matching pursuit with four element dictionary (heavy lines). <p> In order to reveal qualitative properties most clearly, very simple dictionaries and synthetic sources are used in the experiments. Experiments with other dictionaries and sources appear in <ref> [28] </ref>. We do not explore the design of a dictionary or scalar quantizers for a particular application. Dictionary structure has a great impact on the computational complexity of QMP as demonstrated, for example, in [29]. For simplicity, rate and distortion are measured by sample entropy and MSE per component, respectively. <p> The experimental results that have been presented are based on entropy coding each ^ff i independently of the indices, which are in turn coded separately; there are other possibilities. Joint entropy coding of indices was explored in [30], <ref> [28] </ref>. Also, conditional entropy coding could exploit 8 Of course, N = 1 gives independent uniform scalar quantization of each sample. the likelihood of consecutively chosen dictionary vectors being orthogonal or nearly orthogonal.
Reference: [29] <author> M. Goodwin, </author> <title> "Matching pursuit with damped sinusoids," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, and Signal Proc., </booktitle> <address> Mu-nich, Germany, </address> <month> Apr. </month> <year> 1997, </year> <pages> pp. 2037-2040. </pages>
Reference-contexts: Experiments with other dictionaries and sources appear in [28]. We do not explore the design of a dictionary or scalar quantizers for a particular application. Dictionary structure has a great impact on the computational complexity of QMP as demonstrated, for example, in <ref> [29] </ref>. For simplicity, rate and distortion are measured by sample entropy and MSE per component, respectively. The sources used are multidimensional Gaussian with zero mean and independent components. The inner product quantization is uniform with midpoint reconstruction values at fmg m2Z . Furthermore, the quantization stepsize is constant across iterations.
Reference: [30] <author> V. K Goyal, M. Vetterli, and N. T. Thao, </author> <title> "Quantization of overcomplete expansions," </title> <booktitle> in Proc. IEEE Data Compression Conf., </booktitle> <editor> J. A. Storer and M. Cohn, Eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <year> 1995, </year> <pages> pp. 13-22, </pages> <publisher> IEEE Comp. Soc. Press. </publisher>
Reference-contexts: The experimental results that have been presented are based on entropy coding each ^ff i independently of the indices, which are in turn coded separately; there are other possibilities. Joint entropy coding of indices was explored in <ref> [30] </ref>, [28]. Also, conditional entropy coding could exploit 8 Of course, N = 1 gives independent uniform scalar quantization of each sample. the likelihood of consecutively chosen dictionary vectors being orthogonal or nearly orthogonal.
Reference: [31] <author> A. Papoulis, </author> <title> Probability, Random Variables, and Stochastic Pro cesses (Third Edition), </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: j), E ( M Var ( M 1 E (' k ) 2 j : (25) Noting that 2 and 4 are independent of M , (23) shows that Var fi M F fl F ) ii ! 0 as M ! 1, so ( 1 in the mean-squared sense <ref> [31, x8-4] </ref>. Similarly, (24) and (25) show that for i 6= j, ( 1 M F fl F ) ij ! 0 in the mean-squared sense. This completes the proof, provided 2 = 1 N .
Reference: [32] <author> S. M. Selby, Ed., </author> <title> Standard Mathematical Tables (Eighteenth Edition), </title> <publisher> CRC Press, </publisher> <year> 1970. </year>
Reference-contexts: The final simplification is due to a standard integration formula <ref> [32, #323] </ref>.
Reference: [33] <author> P. L. Zador, </author> <title> Development and evaluation of procedures for quan tizing multivariate distributions, </title> <type> Ph.D. thesis, </type> <institution> Stanford Univ., </institution> <year> 1963. </year>
Reference-contexts: Let C be the number of cells that can be found in the region B. It was proved by Zador <ref> [33] </ref>, [34] (see also [35]) that there exists a coefficient c (N; p) which only depends on N and p, such that, for C large enough E kV Q (x) xk 2 c (N; p) To obtain a lower bound in terms of r, let us calculate an upper bound on
Reference: [34] <author> P. L. Zador, </author> <title> "Asymptotic quantization error of continuous sig nals and the quantization dimension," </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> vol. IT-28, </volume> <pages> pp. 139-148, </pages> <month> Mar. </month> <year> 1982. </year>
Reference-contexts: Let C be the number of cells that can be found in the region B. It was proved by Zador [33], <ref> [34] </ref> (see also [35]) that there exists a coefficient c (N; p) which only depends on N and p, such that, for C large enough E kV Q (x) xk 2 c (N; p) To obtain a lower bound in terms of r, let us calculate an upper bound on C

References-found: 34

