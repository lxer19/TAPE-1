URL: http://www.cs.caltech.edu/~john-t/research/conference_papers/triada94.ps
Refering-URL: http://www.cs.caltech.edu/~john-t/research/conference_papers/
Root-URL: http://www.cs.caltech.edu
Email: john-t@cs.caltech.edu  
Title: Integrating Parallel Dataflow Programming with the Ada Tasking Model  
Author: John Thornley 
Address: California 91125, USA  
Affiliation: Computer Science Department California Institute of Technology  
Abstract: This paper describes how parallel dataflow programming can be simply and efficiently integrated with the Ada tasking model. Three extensions to standard Ada are proposed: parallel composition of statements, a parallel for-loop statement, and single-assignment types. Using these constructs, parallel Ada programs can be written without explicit tasking, that are identical| except for two new reserved words|to sequential Ada programs that satisfy the same specifications. These programs can be developed as sequential programs, then executed as deterministic parallel programs for high performance. The proposed extensions are defined by transformation into standard Ada tasking, and can efficiently be implemented by straightforward preprocessing, without any change to the underlying Ada compiler or run-time system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> American National Standards Institute, Inc. </author> <title> The Programming Language Ada Reference Manual. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1983. </year> <month> ANSI/MIL-STD-1815A. </month>
Reference-contexts: Parallel composition of statements is defined as a sequence of entry-less task declarations, the parallel for-loop statement is defined as the declaration of an array of tasks, and single-assignment types can be defined as either passive task types (in Ada 83 <ref> [1] </ref>) or protected types (in Ada 9X [2]). This gives precise meaning to the legality and effect of the new constructs in all contexts, without making any fundamental changes to the Ada language definition. Essentially, we are defining a high-level notation for disciplined use of standard Ada tasking.
Reference: [2] <author> Ada 9X Mapping/Revision Team. </author> <title> Ada 9X Reference Manual, </title> <type> Draft Version 4.0. </type> <institution> Intermetrics, Inc., Cambridge, Massachusetts, </institution> <month> September </month> <year> 1993. </year> <month> IR-MA-1363-3. </month>
Reference-contexts: Parallel composition of statements is defined as a sequence of entry-less task declarations, the parallel for-loop statement is defined as the declaration of an array of tasks, and single-assignment types can be defined as either passive task types (in Ada 83 [1]) or protected types (in Ada 9X <ref> [2] </ref>). This gives precise meaning to the legality and effect of the new constructs in all contexts, without making any fundamental changes to the Ada language definition. Essentially, we are defining a high-level notation for disciplined use of standard Ada tasking. <p> The method of representing distributed computation using the proposed extensions depends on the model of distribution chosen for the entire Ada language. For example: * With the Ada 9X Distributed Systems Annex <ref> [2, Annex I] </ref>, the statements of a parallel composition of statements could be remote subprogram calls ex ecuting in different active partitions. * With the Verdix Distributed Application Development System (DADS) [25], the statements of a parallel composition of statements could be subprogram calls or entry calls executing on different stations
Reference: [3] <author> E. W. Dijkstra. </author> <title> Co-operating sequential processes. </title> <editor> In F. Genuys, editor, </editor> <booktitle> Programming Languages, </booktitle> <pages> pages 43-112. </pages> <publisher> Academic Press, Inc., </publisher> <address> New York, New York, </address> <year> 1968. </year>
Reference-contexts: Parallel composition was suggested in 1966 by Dijkstra <ref> [3] </ref>, as the parbegin-parend construct. The first major language to support this construct was Algol 68 [4]. Our parallel for-loop is simply a quantified form of parallel composition. It is similar to the Oc-cam [5] parallel replicator construct.
Reference: [4] <author> A. van Wijngaarden et al. </author> <title> Revised report on the algorithmic language ALGOL 68. </title> <journal> Acta Informatica, </journal> <volume> 5(1-3):1-236, </volume> <year> 1975. </year>
Reference-contexts: Parallel composition was suggested in 1966 by Dijkstra [3], as the parbegin-parend construct. The first major language to support this construct was Algol 68 <ref> [4] </ref>. Our parallel for-loop is simply a quantified form of parallel composition. It is similar to the Oc-cam [5] parallel replicator construct. Single-assignment variables for synchronization were reported in 1968 by Tesler and Enea [6].
Reference: [5] <author> D. </author> <month> May. </month> <title> Occam. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 17(4) </volume> <pages> 69-79, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: Parallel composition was suggested in 1966 by Dijkstra [3], as the parbegin-parend construct. The first major language to support this construct was Algol 68 [4]. Our parallel for-loop is simply a quantified form of parallel composition. It is similar to the Oc-cam <ref> [5] </ref> parallel replicator construct. Single-assignment variables for synchronization were reported in 1968 by Tesler and Enea [6]. They have been incorporated in parallel dataflow languages, including Id [7], Val [8], and Sisal [9], and parallel logic programming languages, including Concurrent Prolog [10] and Parlog [11].
Reference: [6] <author> L. G. Tesler and H. J. Enea. </author> <title> A language design for concurrent processes. </title> <booktitle> In Proceedings of the 1968 AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 403-408, </pages> <address> Atlantic City, New Jersey, </address> <month> April 30-May 2 </month> <year> 1968. </year>
Reference-contexts: The first major language to support this construct was Algol 68 [4]. Our parallel for-loop is simply a quantified form of parallel composition. It is similar to the Oc-cam [5] parallel replicator construct. Single-assignment variables for synchronization were reported in 1968 by Tesler and Enea <ref> [6] </ref>. They have been incorporated in parallel dataflow languages, including Id [7], Val [8], and Sisal [9], and parallel logic programming languages, including Concurrent Prolog [10] and Parlog [11].
Reference: [7] <author> Arvind, K. P. Gostelow, and W. Plouffe. </author> <title> An asynchronous programming language and computing machine. </title> <type> Technical Report TR-114a, </type> <institution> Department of Information and Computer Science, University of California, Irvine, </institution> <month> December </month> <year> 1978. </year>
Reference-contexts: Our parallel for-loop is simply a quantified form of parallel composition. It is similar to the Oc-cam [5] parallel replicator construct. Single-assignment variables for synchronization were reported in 1968 by Tesler and Enea [6]. They have been incorporated in parallel dataflow languages, including Id <ref> [7] </ref>, Val [8], and Sisal [9], and parallel logic programming languages, including Concurrent Prolog [10] and Parlog [11]. Integration of parallel and sequential composition, parallel and sequential for-loops, and single and multiple assignment variables was described in 1977 by Kessels [12].
Reference: [8] <author> W. B. Ackerman and J. B. Dennis. </author> <title> VAL|a value-oriented algorithmic language: Preliminary reference manual. </title> <type> Technical Report TR-218, </type> <institution> MIT Laboratory for Computer Science, </institution> <address> Cambridge, Mas-sachusetts, </address> <month> June </month> <year> 1979. </year>
Reference-contexts: Our parallel for-loop is simply a quantified form of parallel composition. It is similar to the Oc-cam [5] parallel replicator construct. Single-assignment variables for synchronization were reported in 1968 by Tesler and Enea [6]. They have been incorporated in parallel dataflow languages, including Id [7], Val <ref> [8] </ref>, and Sisal [9], and parallel logic programming languages, including Concurrent Prolog [10] and Parlog [11]. Integration of parallel and sequential composition, parallel and sequential for-loops, and single and multiple assignment variables was described in 1977 by Kessels [12].
Reference: [9] <author> J. R. McGraw, S. Allan, J. Glauert, and I. Dobes. </author> <title> SISAL: Streams and iteration in a single-assignment language, language reference manual. </title> <type> Technical Report M-146, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1983. </year>
Reference-contexts: Our parallel for-loop is simply a quantified form of parallel composition. It is similar to the Oc-cam [5] parallel replicator construct. Single-assignment variables for synchronization were reported in 1968 by Tesler and Enea [6]. They have been incorporated in parallel dataflow languages, including Id [7], Val [8], and Sisal <ref> [9] </ref>, and parallel logic programming languages, including Concurrent Prolog [10] and Parlog [11]. Integration of parallel and sequential composition, parallel and sequential for-loops, and single and multiple assignment variables was described in 1977 by Kessels [12].
Reference: [10] <author> E. Shapiro. </author> <title> A subset of Concurrent Prolog and its interpreter. </title> <type> Technical Report TR-003, </type> <institution> ICOT, Institute for New Generation Computer Technology, </institution> <address> Tokyo, Japan, </address> <year> 1983. </year>
Reference-contexts: It is similar to the Oc-cam [5] parallel replicator construct. Single-assignment variables for synchronization were reported in 1968 by Tesler and Enea [6]. They have been incorporated in parallel dataflow languages, including Id [7], Val [8], and Sisal [9], and parallel logic programming languages, including Concurrent Prolog <ref> [10] </ref> and Parlog [11]. Integration of parallel and sequential composition, parallel and sequential for-loops, and single and multiple assignment variables was described in 1977 by Kessels [12].
Reference: [11] <author> K. L. Clark and S. Gregory. </author> <title> PARLOG: Parallel programming in logic. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(1) </volume> <pages> 1-49, </pages> <month> Jan-uary </month> <year> 1986. </year>
Reference-contexts: Single-assignment variables for synchronization were reported in 1968 by Tesler and Enea [6]. They have been incorporated in parallel dataflow languages, including Id [7], Val [8], and Sisal [9], and parallel logic programming languages, including Concurrent Prolog [10] and Parlog <ref> [11] </ref>. Integration of parallel and sequential composition, parallel and sequential for-loops, and single and multiple assignment variables was described in 1977 by Kessels [12].
Reference: [12] <author> J. L. W. Kessels. </author> <title> A conceptual framework for a nonprocedural programming language. </title> <journal> Communications of the ACM, </journal> 20(12) 906-913, December 1977. 
Reference-contexts: Integration of parallel and sequential composition, parallel and sequential for-loops, and single and multiple assignment variables was described in 1977 by Kessels <ref> [12] </ref>. The PCN language [13] supports parallel and sequential composition, single and multiple assignment variables, and an interface to code written in other languages, e.g., Fortran and C. A formal operational semantic and proof rules have been developed for PCN [14].
Reference: [13] <author> K. M. Chandy and S. Taylor. </author> <title> A primer for Program Composition Notation. </title> <type> Technical Report CS-TR-90-10, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: Integration of parallel and sequential composition, parallel and sequential for-loops, and single and multiple assignment variables was described in 1977 by Kessels [12]. The PCN language <ref> [13] </ref> supports parallel and sequential composition, single and multiple assignment variables, and an interface to code written in other languages, e.g., Fortran and C. A formal operational semantic and proof rules have been developed for PCN [14].
Reference: [14] <author> K. M. Chandy and S. Taylor. </author> <title> An Introduction to Parallel Programming. </title> <publisher> Jones and Bartlett, </publisher> <address> Boston, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: The PCN language [13] supports parallel and sequential composition, single and multiple assignment variables, and an interface to code written in other languages, e.g., Fortran and C. A formal operational semantic and proof rules have been developed for PCN <ref> [14] </ref>. Recently, CC++ [15] extends C++ with parallel composition, a parallel for-loop, single-assignment variables, atomicity, and distributed address spaces.
Reference: [15] <author> K. M. Chandy and C. Kesselman. </author> <title> CC++: A declarative concurrent object oriented programming language. </title> <type> Technical Report CS-TR-92-01, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: The PCN language [13] supports parallel and sequential composition, single and multiple assignment variables, and an interface to code written in other languages, e.g., Fortran and C. A formal operational semantic and proof rules have been developed for PCN [14]. Recently, CC++ <ref> [15] </ref> extends C++ with parallel composition, a parallel for-loop, single-assignment variables, atomicity, and distributed address spaces.
Reference: [16] <author> J. Thornley. </author> <title> Parallel programming with Declarative Ada. </title> <type> Technical Report CS-TR-93-03, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: In earlier work <ref> [16] </ref>, we investigated the integration of parallel dataflow programming with a Pascal-like subset of Ada. In this paper, we show that the same parallel programming model can be elegantly integrated with the complete Ada language, including strong typing, packages, exception handling, generics, and tasking. <p> Much of what we cover in this section is discussed in more detail in <ref> [16] </ref>. 6.1 Methodology The proposed extensions are intended to be used for problems that are specified as a mapping from inputs onto outputs, i.e., as opposed to problems that explicitly specify concurrency.
Reference: [17] <author> A. Burns. </author> <title> Efficient initialisation routines for multiprocessor systems programmed in Ada. </title> <journal> ACM Ada Letters, </journal> <volume> 5(1) </volume> <pages> 55-60, </pages> <month> July/August </month> <year> 1985. </year>
Reference-contexts: For more efficient parallel initialization of the loop parameter values than the sequential for-loop statement, an equivalent transformation could be based on Burns' recursive algorithm <ref> [17] </ref>. With Ada 9X, an equivalent transformation could use task type discriminants, instead of rendezvous, to initialize the loop parameter values.
Reference: [18] <author> A. N. Habermann and I. R. Nassi. </author> <title> Efficient implementation of Ada tasks. </title> <type> Technical Report CMU-CS-80-103, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> January </month> <year> 1980. </year>
Reference: [19] <author> P. N. Hilfinger. </author> <title> Implementation strategies for Ada tasking idioms. </title> <booktitle> In Proceedings of the ACM AdaTec Conference on Ada, </booktitle> <pages> pages 26-30, </pages> <address> Arlington, Vir-ginia, </address> <month> October </month> <year> 1982. </year>
Reference: [20] <author> J. Thornley. </author> <title> Integrating functional and imperative parallel programming: CC++ solutions to the Sal-ishan problems. </title> <booktitle> In Proceedings of the 8th IEEE International Parallel Processing Symposium, </booktitle> <pages> pages 61-67, </pages> <address> Cancun, Mexico, </address> <month> April 26-29 </month> <year> 1994. </year>
Reference-contexts: Programs can be designed and developed as sequential programs, with a programmer-maintained discipline of single-assignment on some variables. After initial development and testing, the sequential programs can be converted into parallel programs by the simple addition of the reserved words parallel and single. In other work <ref> [20] </ref>, we have used this methodology to develop solutions to the Salishan Problems. 6.2 Reasoning and Debugging Reasoning about programs that use the proposed extensions is very similar to reasoning about sequential programs.
Reference: [21] <author> J. V. Guttag, J. J. Horning, and J. M. Wing. </author> <title> The Larch family of specification languages. </title> <journal> IEEE Software, </journal> <volume> 2(5) </volume> <pages> 24-36, </pages> <month> September </month> <year> 1985. </year>
Reference: [22] <author> G. Guaspari, C. Marceau, and W. Polak. </author> <title> Formal verification of Ada programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(9) </volume> <pages> 1058-1075, </pages> <month> September </month> <year> 1990. </year>
Reference: [23] <author> D. Luckham. </author> <title> Programming with Specifications: An Introduction to ANNA, A Language for Specifying Ada Programs. </title> <publisher> Springer-Verlag, </publisher> <address> New York, New York, </address> <year> 1990. </year>
Reference-contexts: Assertions can be used for specification, verification, and debugging as they are in systems such as Larch [21][22] and Anna <ref> [23] </ref>. Some additional proof obligations are required: 1. It must be shown that single-assignment variables are assigned to at most once. 2. It must be shown that multiple-assignment vari ables are not erroneously shared in parallel. 3.
Reference: [24] <author> D. Cann. </author> <title> Retire Fortran? A debate rekindled. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 81-89, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: A great deal of relevant work has been done on the optimization of single-assignment languages. A list of references to optimization techniques used in the implementation of Sisal is given by Cann <ref> [24] </ref>. 7.4 Distribution Distribution of the execution of a program across multiple processors and multiple address spaces is a very important issue, but is mostly orthogonal to the defini-tion of the proposed extensions.
Reference: [25] <author> Verdix Corporation, Herndon, Virginia. </author> <title> Distributed Application System Development Guide, </title> <note> Version 6.2.5, </note> <month> June 30 </month> <year> 1993. </year>
Reference-contexts: For example: * With the Ada 9X Distributed Systems Annex [2, Annex I], the statements of a parallel composition of statements could be remote subprogram calls ex ecuting in different active partitions. * With the Verdix Distributed Application Development System (DADS) <ref> [25] </ref>, the statements of a parallel composition of statements could be subprogram calls or entry calls executing on different stations and could access data residing on different stations. 8 Conclusion In this paper, we have proposed three extensions to Ada that provide a means of writing structured parallel programs that are
References-found: 25

