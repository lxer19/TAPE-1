URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P552.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Email: bouarich@mcs.anl.gov.  
Title: Tensor Methods for Large, Sparse Nonlinear Least Squares Problems  
Author: Ali Bouaricha Robert B. Schnabel 
Keyword: Key Words. tensor methods, sparse problems, large-scale optimization, rank-deficient matrices  
Address: 60439,  
Note: AMS(MOS) subject classification. 65F20, 65F50, 65H10, 65K05, 65K10  Research supported in part by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38.  
Affiliation: Argonne National Laboratory and  University of Colorado  Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Illinois  
Abstract: This paper introduces tensor methods for solving large, sparse nonlinear least squares problems where the Jacobian either is analytically available or is computed by finite difference approximations. Tensor methods have been shown to have very good computational performance for small to medium-sized, dense nonlinear least squares problems. In this paper we consider the application of tensor methods to large, sparse nonlinear least squares problems. This involves an entirely new way of solving the tensor model that is efficient for sparse problems. A number of interesting linear algebraic implementation issues are addressed. The test results of the tensor method applied to a set of sparse nonlinear least squares problems compared with those of the standard Gauss-Newton method reveal that the tensor method is significantly more robust and efficient than the standard Gauss-Newton method. y Department of Computer Science, University of Colorado, Boulder, Colorado 80309-0430, bobby@cs.colorado.edu. Research supported by AFOSR Grants No. AFOSR-90-0109 and F49620-94-1-0101, ARO Grants No. DAAL03-91-G-0151 and DAAH04-94-G-0228, and NSF Grant No. CCR-9101795. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Al-Baali and R. Fletcher. </author> <title> Variational methods for nonlinear least squares. </title> <type> Technical report NA/71, </type> <institution> Department of Mathematical Sciences, University of Dundee, </institution> <year> 1983. </year>
Reference-contexts: Perform a standard backtracking line search [14] on d n to obtain x n + return x + = x n + endif endif 18 6.2 Test Results We have run the sparse tensor and Gauss-Newton codes on versions of the nonlinear least squares problems described by Al-Baali and Fletcher <ref> [1] </ref> and singular modifications of these problems.
Reference: [2] <author> M. Arioli, I. S. Duff, and P. P. M. Rijk. </author> <title> On the augmented system approach to sparse least-squares problems. </title> <journal> Numer. Math., </journal> <volume> 55 </volume> <pages> 667-684, </pages> <year> 1989. </year>
Reference-contexts: Arioli et al. <ref> [2] </ref> demonstrated that conditioning can be greatly improved by a scaling similar to that of Bjorck; they suggested an automatic technique for selecting the best scaling factor ff.

Reference: [4] <author> A. Bjorck. </author> <title> Iterative refinement of linear least squares solutions. </title> <journal> BIT, </journal> <volume> 7 </volume> <pages> 257-278, </pages> <year> 1967. </year>
Reference-contexts: With u set to 20 they found essentially no growth in the size of the largest matrix element (and on some problems they found no such growth with u as high as 10 5 ). The conditioning of the augmented system (4.1) was studied by Bjorck <ref> [4] </ref>. He considered the slightly generalized system obtained by scaling the augmented system with a scaling factor ff.
Reference: [5] <author> A. Bjorck and I. S. Duff. </author> <title> A direct method for the solution of sparse linear least squares problems. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 34 </volume> <pages> 43-67, </pages> <year> 1980. </year>
Reference-contexts: If the Jacobian matrix is rank deficient, we use an extension of the method of Peters and Wilkinson [22], developed by Bjorck and Duff <ref> [5] </ref>, to find some solution d to (5.1). <p> = ^ d + ffi. (Any values of the form ^ J + v or ( ^ J T ^ J) 1 v are computed using the augmented matrix approaches described in Section 4.2.) Else Step 4.1 Calculate the singular Gauss-Newton step d n by the Bjorck and Duff method <ref> [5] </ref> (see Section 5). Step 4.2 Select the next iterate x + using a standard backtracking line search strategy [14], where d n is the search direction; go to Step 6. Step 5 Select the next iterate x + using the global framework described in Algorithm 6.2.
Reference: [6] <author> A. Bouaricha. </author> <title> Solving large sparse systems of nonlinear equations and nonlinear least squares problems using tensor methods on sequential and parallel computers. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, University of Colorado at Boulder, </institution> <year> 1992. </year>
Reference-contexts: least squares problems Jx j = a j ; j = 1; : : :; p, then kn (fi) k 2 T Afi 2 + 4 1 fi 2 R 2 k 2 Now we can give an efficient algorithm that solves problem 3.1. (An alternative algorithm was considered in <ref> [6] </ref> but proved to be less efficient.) Algorithm 3.1 Minimization of the Tensor Model When the Jacobian Has Full Rank Let J 2 &lt; mfin be sparse, F 2 &lt; m , S 2 &lt; nfip , and A 2 &lt; mfip , m &gt; n. <p> These methods form the tensor model in the same way as in the tensor methods developed for small to medium-sized dense nonlinear least squares <ref> [6, 8] </ref>, with the exception that only one past point is used in the interpolation process. <p> This new solution approach is the main contribution of this paper and is essential because the approach for small dense problems used in <ref> [6, 8] </ref> destroys the sparsity of the Jacobian matrix as a result of orthogonal transformations of the variable and function spaces.
Reference: [7] <author> A. Bouaricha and R. B. Schnabel. </author> <title> Tensor methods for large, sparse systems of nonlinear equations. </title> <type> Preprint MCS-P473-1094, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: This section presents such a method that works both when the Jacobian is nonsingular and when it is rank-deficient. The basic approach is related to the approach given in <ref> [7] </ref> for solving the tensor model for sparse nonlinear equations, but the situation for nonlinear least squares is considerably different and more complex, as are the linear algebraic issues (considered in the following sections). <p> If rank (J) n p, usually ^ J will have full column rank (a necessary and sufficient condition for ^ J to be nonsingular is similar to that for sparse nonlinear equations <ref> [7] </ref>). Thus, if ^ J has full rank, we can use Algorithm 3.1 to obtain the value of ffi, but we use a special procedure (described in Section 4.2) to efficiently solve the linear least squares problems involving ^ J + .
Reference: [8] <author> A. Bouaricha and R. B. Schnabel. TENSOLVE: </author> <title> A software package for solving systems of nonlinear equations and nonlinear least squares problems using tensor methods. </title> <type> Technical Report CU-CS-735-94, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <year> 1994. </year>
Reference-contexts: The tensor model came initially from the research of [23] for nonlinear equations and was extended to nonlinear least squares in <ref> [8] </ref>. <p> The additional storage required is 4p m-vectors, which is small in comparison with the storage for the Jacobian. 4 After the tensor model (2.4) is formed, the least squares solution of the model, minimize d2&lt; n is computed. In <ref> [8] </ref>, we showed that the solution to (2.5) can be reduced to the solution of a small number (m n + q) quadratic equations in p unknowns, plus the solution of n q linear equations in n p unknowns. <p> Otherwise the tensor step is chosen. For further details, see <ref> [8] </ref>. 3 Solving the Tensor Model for Sparse Nonlinear Least Squares The major challenge in constructing an efficient tensor method for sparse nonlinear least squares problems is finding a way to obtain the root or minimizer of the tensor model that preserves and effectively uses the sparsity of the Jacobian matrix. <p> A stable solution procedure is given in <ref> [8] </ref> for dense nonlinear least squares. This section gives a method that is efficient when the Jacobian is large and sparse. If the Jacobian matrix is rank deficient, we can solve the tensor model by building upon the process just described. <p> These methods form the tensor model in the same way as in the tensor methods developed for small to medium-sized dense nonlinear least squares <ref> [6, 8] </ref>, with the exception that only one past point is used in the interpolation process. <p> This new solution approach is the main contribution of this paper and is essential because the approach for small dense problems used in <ref> [6, 8] </ref> destroys the sparsity of the Jacobian matrix as a result of orthogonal transformations of the variable and function spaces.
Reference: [9] <author> J. R. Bunch. </author> <title> Partial pivoting strategies for symmetric matrices. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 11 </volume> <pages> 521-528, </pages> <year> 1974. </year>
Reference-contexts: three back solves (which is the number of back solves required by the tensor method when p = 1 and the tensor model has no root, or when p = 2 and the tensor model has a root), and (2) one symmetric indefinite decomposition obtained by the method of Bunch <ref> [9] </ref> using 2 fi 2 pivots, plus three back solves. These computations showed that the unsymmetric decomposition is approximately 25% more efficient than the symmetric one. For greater values of p, the unsymmetric decomposition has an even larger advantage over the symmetric one.
Reference: [10] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More. Fortran subroutines for estimating sparse Jacobian matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 </volume> <pages> 346-347, </pages> <year> 1984. </year>
Reference-contexts: First, tensor methods require that the Jacobian matrix be 2 available at each iteration. It turns out that this requirement is not a problem in the large, sparse case because derivatives usually are computed by using efficient sparse finite differences (see <ref> [10, 11, 12, 13] </ref>) or automatic differentiation (see, e.g., [3]). Second, the method for forming the tensor model must be modified to adapt to the large, sparse case.
Reference: [11] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More. Software for estimating sparse Jacobian matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 </volume> <pages> 329-345, </pages> <year> 1984. </year>
Reference-contexts: First, tensor methods require that the Jacobian matrix be 2 available at each iteration. It turns out that this requirement is not a problem in the large, sparse case because derivatives usually are computed by using efficient sparse finite differences (see <ref> [10, 11, 12, 13] </ref>) or automatic differentiation (see, e.g., [3]). Second, the method for forming the tensor model must be modified to adapt to the large, sparse case.
Reference: [12] <author> T. F. Coleman and J. J. </author> <title> More. Estimation of sparse Jacobian matrices and graph coloring problems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 </volume> <pages> 187-207, </pages> <year> 1983. </year>
Reference-contexts: First, tensor methods require that the Jacobian matrix be 2 available at each iteration. It turns out that this requirement is not a problem in the large, sparse case because derivatives usually are computed by using efficient sparse finite differences (see <ref> [10, 11, 12, 13] </ref>) or automatic differentiation (see, e.g., [3]). Second, the method for forming the tensor model must be modified to adapt to the large, sparse case. <p> If the last global step fails to locate a point lower than x c in the line search global strategy, the method stops and reports this condition; this may indicate either success or failure. We use the graph coloring algorithm of Coleman and More <ref> [12] </ref> to compute the sparse finite difference approximation of the Jacobian matrix.
Reference: [13] <author> A. M. Curtis, M. J. D. Powell, and J. K. Reid. </author> <title> On the estimation of sparse Jacobian matrices. </title> <journal> Inst. Math. Applics., </journal> <volume> 13 </volume> <pages> 117-120, </pages> <year> 1974. </year>
Reference-contexts: First, tensor methods require that the Jacobian matrix be 2 available at each iteration. It turns out that this requirement is not a problem in the large, sparse case because derivatives usually are computed by using efficient sparse finite differences (see <ref> [10, 11, 12, 13] </ref>) or automatic differentiation (see, e.g., [3]). Second, the method for forming the tensor model must be modified to adapt to the large, sparse case.
Reference: [14] <author> J. E. Dennis and R. B. Schnabel. </author> <title> Numerical methods for unconstrained optimization and nonlinear equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: Step 4.2 Select the next iterate x + using a standard backtracking line search strategy <ref> [14] </ref>, where d n is the search direction; go to Step 6. Step 5 Select the next iterate x + using the global framework described in Algorithm 6.2. Step 6 Set x c x + , J (x c ) J (x + ), and go to Step 0. <p> Test if the tensor step is sufficiently descent if g T d t &lt; 10 4 jjgjj 2 jjd t jj 2 then Find an acceptable x t + in the tensor direction d t , by performing a standard backtracking line search <ref> [14] </ref> return x + = x t + else Perform a standard backtracking line search [14] on d n to obtain x n + return x + = x n + endif endif 18 6.2 Test Results We have run the sparse tensor and Gauss-Newton codes on versions of the nonlinear <p> if g T d t &lt; 10 4 jjgjj 2 jjd t jj 2 then Find an acceptable x t + in the tensor direction d t , by performing a standard backtracking line search <ref> [14] </ref> return x + = x t + else Perform a standard backtracking line search [14] on d n to obtain x n + return x + = x n + endif endif 18 6.2 Test Results We have run the sparse tensor and Gauss-Newton codes on versions of the nonlinear least squares problems described by Al-Baali and Fletcher [1] and singular modifications of these problems.
Reference: [15] <author> I. S. Duff. MA28: </author> <title> A set of Fortran subroutines for for sparse unsymmetric linear equations. </title> <type> Technical Report R-8730, </type> <institution> AERE Harwell Laboratory, </institution> <year> 1977. </year>
Reference-contexts: Step 1 Calculate the second-order term of the tensor model so that the tensor model interpo lates F (x) at the most recent past point (i.e., p = 1.) Step 2 Form the augmented matrix M 1 , and factorize it using the MA28 software package <ref> [15] </ref>, employing the Markowitz pivoting strategy oriented toward efficiently computing an expression of the form (J T J) 1 v, where v 2 &lt; n (see Section 4.1.) Step 3 If M 1 has full rank, use Algorithm 3.1 to solve the tensor model M (x c + d) = F <p> Step 3.2.2 Factor the lower right square (m + n ae + 2p) fi (m + n ae + 2p) sub matrix using the MA28 software package <ref> [15] </ref>. 17 Step 3.2.3 Update M 2 by combining the LU factorization of the submatrix in Step 3.2.2, the updated submatrices in Step 3.2.1, and the LU factorization of M 1 , into one LU factorization of M 2 .
Reference: [16] <author> I. S. Duff and J. K. Reid. </author> <title> A comparison of some methods for the solution of sparse overde-termined system. </title> <journal> J. Inst. Math. Applics, </journal> <volume> 17 </volume> <pages> 267-280, </pages> <year> 1975. </year>
Reference-contexts: The system (4.1) is symmetric and indefinite and can be solved by sparse techniques. One reason that we use (4.1) is the efficiency that this approach has been shown to have. Duff and Reid <ref> [16] </ref> compared four methods for the solution for sparse linear least squares problems: the direct formation and solution of normal equations, orthogonal reduction to upper triangular form [19, 18], LU factorization of A [22], and Hachtel's augmented matrix method (4.1). <p> Because of ill conditioning with the use of normal equations however, they recommend the augmented matrix method. A second reason we use (4.1) is that various research has shown the method to be quite accurate for large, sparse linear least squares. Duff and Reid <ref> [16] </ref> conducted tests on the augmented matrix method, avoiding any multiplier bigger in modulus (or norm) than a limit u and making this stability requirement override sparsity considerations. <p> By making this judicious choice of pivots and discarding the portions of the factors that are not required, Duff and Reid <ref> [16] </ref> obtained substantial savings in the number of operations for the triangular solves. Therefore Duff and Reid recommend using the unsymmetric factorization when there are multiple right-hand sides. 11 Using the collection of problems in [16], we computed the number of operations required for (1) one unsymmetric decomposition that gives a <p> pivots and discarding the portions of the factors that are not required, Duff and Reid <ref> [16] </ref> obtained substantial savings in the number of operations for the triangular solves. Therefore Duff and Reid recommend using the unsymmetric factorization when there are multiple right-hand sides. 11 Using the collection of problems in [16], we computed the number of operations required for (1) one unsymmetric decomposition that gives a slight bias toward early pivots in late rows and late pivots in late columns, plus three back solves (which is the number of back solves required by the tensor method when p = 1 and <p> Since the latter choice results in a larger savings, we bias the first m pivots to come from the first m rows. 12 We use a sparse variant of Gaussian elimination on the augmented matrix (4.4), using the following Markowitz pivot selection strategy, which is based on a strategy in <ref> [16] </ref>.
Reference: [17] <author> D. Feng, P. Frank, and R. B. Schnabel. </author> <title> Local convergence analysis of tensor methods for nonlinear equations. </title> <journal> Math. Prog., </journal> <volume> 62 </volume> <pages> 427-459, </pages> <year> 1993. </year>
Reference-contexts: On rank n 1 problems this is due in part to the tensor method achieving superlinear convergence on zero residual problems, whereas the Gauss Newton method is linearly convergent at best on these problems. The analysis in <ref> [17] </ref> shows that tensor methods for nonlinear equations have at least a 3-step order 1.5 rate of convergence on a class of problems with rank deficiency one at the solution, whereas Newton's method is linearly convergent with constant 0.5 on the same problems.
Reference: [18] <author> W. M. Gentleman. </author> <title> Least squares computation by givens transformations without squares roots. </title> <journal> J. Inst. Math. Applics., </journal> <volume> 12 </volume> <pages> 329-336, </pages> <year> 1973. </year>
Reference-contexts: One reason that we use (4.1) is the efficiency that this approach has been shown to have. Duff and Reid [16] compared four methods for the solution for sparse linear least squares problems: the direct formation and solution of normal equations, orthogonal reduction to upper triangular form <ref> [19, 18] </ref>, LU factorization of A [22], and Hachtel's augmented matrix method (4.1). The criteria used for comparison were the number of operations for matrix decomposition, storage, and number of operations required for subsequent solutions.
Reference: [19] <author> G. H. Golub. </author> <title> Numerical methods for solving linear least squares problems. </title> <journal> Numer. Math., </journal> <volume> 7 </volume> <pages> 206-216, </pages> <year> 1965. </year>
Reference-contexts: One reason that we use (4.1) is the efficiency that this approach has been shown to have. Duff and Reid [16] compared four methods for the solution for sparse linear least squares problems: the direct formation and solution of normal equations, orthogonal reduction to upper triangular form <ref> [19, 18] </ref>, LU factorization of A [22], and Hachtel's augmented matrix method (4.1). The criteria used for comparison were the number of operations for matrix decomposition, storage, and number of operations required for subsequent solutions.
Reference: [20] <author> G. D. Hachtel. </author> <title> Extended applications of the sparse tableau approach-finite elements and least squares. </title> <editor> In W. R. Spillers, editor, </editor> <title> Basic questions of design theory. </title> <publisher> North Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1974. </year>
Reference-contexts: The method that we use for solving these linear least squares problems (i.e., minimize x2&lt; n kJx b k 2 ) is based on the augmented system approach first proposed by Hachtel <ref> [20] </ref>, 2 6 6 6 I J 3 7 7 5 6 6 6 r 3 7 7 5 2 6 6 4 0 7 7 7 ; (4:1) where I is the m fi m identity matrix.
Reference: [21] <author> C. Moler. </author> <title> Private communication. </title> <year> 1991. </year>
Reference-contexts: Unfortunately, both Bjorck and Arioli et al. use a condition number estimator to calculate ff, which would make a tensor iteration very expensive. We decided to use a heuristic scaling strategy by Cleve Moler <ref> [21] </ref>, which sets the value of ff to the maximum element of the matrix in absolute value divided by 1000. A third reason we use the augmented system approach is that it can be extended to solve the tensor model when the Jacobian matrix is rank deficient.
Reference: [22] <author> G. Peters and J. H. Wilkinson. </author> <title> The least squares problem and pseudo-inverses. </title> <journal> Computer J., </journal> <volume> 13 </volume> <pages> 309-316, </pages> <year> 1970. </year>
Reference-contexts: Duff and Reid [16] compared four methods for the solution for sparse linear least squares problems: the direct formation and solution of normal equations, orthogonal reduction to upper triangular form [19, 18], LU factorization of A <ref> [22] </ref>, and Hachtel's augmented matrix method (4.1). The criteria used for comparison were the number of operations for matrix decomposition, storage, and number of operations required for subsequent solutions. <p> If J has full rank, the solution to the linear least squares problem (5.1) is already available from the computation of J + F from Step 0 of Algorithm 3.1. If the Jacobian matrix is rank deficient, we use an extension of the method of Peters and Wilkinson <ref> [22] </ref>, developed by Bjorck and Duff [5], to find some solution d to (5.1).
Reference: [23] <author> R. B. Schnabel and P. D. Frank. </author> <title> Tensor methods for nonlinear equations. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 21 </volume> <pages> 815-843, </pages> <year> 1984. </year>
Reference-contexts: The tensor model came initially from the research of <ref> [23] </ref> for nonlinear equations and was extended to nonlinear least squares in [8]. <p> The procedure of finding linearly independent directions is implemented easily by using a modified Gram-Schmidt algorithm, and usually results in p = 1 or 2. After the linearly independent past directions s k are selected, the tensor term is chosen by the procedure of Schnabel and Frank <ref> [23] </ref>, which generalizes in a straightforward way to nonlinear least squares. <p> We also tested a modified version of the Trigonometric problem where the residual at the solution (1; : : :; 1) is zero. We then created singular test problems as proposed in Schnabel and Frank <ref> [23] </ref> by modifying both the zero residual and the nonzero residual cases of the nonsingular test problems described above to the form ^ F (x) = F (x) F 0 (x fl )A (A T A) 1 A T (x x fl ); (6:7) where F (x) is the standard nonsingular
Reference: [24] <author> R. B. Schnabel, J. E. Koontz, and B. E. Weiss. </author> <title> A modular system of algorithms of unconstrained minimization. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 11 </volume> <pages> 419-440, </pages> <year> 1985. </year> <month> 29 </month>
Reference-contexts: A nonlinear unconstrained optimization software package, UNCMIN <ref> [24] </ref>, is used to minimize the l 2 norm of the m n + q quadratic equations in the p unknowns ^ d 2 . (If p = 1, this procedure is done analytically instead.) 4. <p> Then premultiply (J T J) 1 S by S T to obtain W . Step 2 Perform a Cholesky decomposition of W (i.e., W = LL T ) to obtain L 2 &lt; pfip , a lower triangular matrix. Step 3 Use UNCMIN <ref> [24] </ref>, an unconstrained minimization software package, to solve minimize fi2&lt; p fkL 1 q (fi) k 2 2 g; (3:14) where n (fi) = R 1 + 1 2 fi 2 R 2 , or solve (3.14) in closed form if p = 1.
References-found: 23

