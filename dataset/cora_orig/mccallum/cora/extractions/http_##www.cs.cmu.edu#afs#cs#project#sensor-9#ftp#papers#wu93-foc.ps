URL: http://www.cs.cmu.edu/afs/cs/project/sensor-9/ftp/papers/wu93-foc.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/project/sensor-9/ftp/papers/
Root-URL: 
Title: Range from focus-error Conventional imaging captures a two dimensional projection of the scene; the third
Author: M. W. Siegel and M. L. Leary 
Note: 2. BACKGROUND  5 between corresponding  
Address: Pittsburgh PA 15213  
Affiliation: Measurement and Control Laboratory The Robotics Institute School of Computer Science Carnegie Mellon University  
Abstract: We derive theoretically and demonstrate experimentally an approach to range-from-focus with an important improvement over all previous methods. Previous methods rely on subjective measures of sharpness to focus a selected locale of the image. Our method uses measured physical features of the optical signal to generate an objective focus-error distance map. To compute range-from-focus-error distance it is not necessary to focus any part of the image: range is calculated directly from the lens formula by substituting the difference between the lens-to-sensor distance and the focus-error distance for the usual lens-to-image distance. Our method senses focus-error distance in parallel for all locales of the image, thus providing a complete range image. The method is based on our recognition that when an image sensor is driven in longitudinal oscillation ("dithered") the Fourier amplitude of the first harmonic component of the signal is proportional to the first power of the ratio of dither amplitude to focus-error distance, whereas the Fourier amplitude of the second harmonic component is proportional to the square of this ratio. The ratio of the first harmonic sin wt amplitude A to the second harmonic cos 2wt w amplitude B is thus a constant (-4) multiple of the ratio of the focus-error distance to the dither amplitude. The 2w focus-error distance measurement via the ratio of the first-to-second harmonic amplitudes is extremely robust in the sense that the scene's gray level structure, the spatial and temporal structure of the illumination, and technical noise sources (most of which affect the Fourier amplitudes multiplicatively) all appear identically in both amplitudes, thus cancelling in the ratio. Extracting the two Fourier amplitudes and taking their ratio could be accomplished, pixel-by-pixel, by some ambitious but not outrageous analog computing circuitry that we describe. We derive the method for a point scene model, and we demonstrate the method with apparatus that instantiates this model. 4 in range accuracy, density of range data, aliasing difficulties with the correlation methods, etc, are common. While the problems differ for each method they do have one common source: the range is found off-line, not captured in intimate connection with recording the image. We describe a new on-line approach to range-from-focus that overcomes this and several related difficulties with current range-from-focus techniques. 3 Image focusing is conventionally regarded as a spatial-domain activity: the focus-controlling parameter (lens-to-sensor distance in a camera, focal length in the eye) is presumed to be adjusted with the goal of maximizing the amplitudes of the high spatial frequency image components. The amplitudes of the appropriate spatial frequencies are derived from pixel-to-pixel signal differences. The focusing information available from these differences is in reality weak and noise prone. Thus most practical focusing, e.g., in film and video photography, is done indirectly, without reference to the image, by an open-loop method using a rangefinder (e.g., a parallax based split-image method) coupled to the lens-to-sensor distance by a calibration scale. In humans, depth perception is known to be derived from the fusion of focus and binocular parallax cues. However the focusing cue is easily discounted: most people have no trouble understanding 3D-stereoscopic photos even though focus is confined to the screen-plane, while the conflicting convergence cues are controlled by the offset 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. H. Ballard and C. M. Brown, </author> <title> Computer Vision, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference: 2. <author> R. A. Jarvis, </author> <title> ``A Perspective on Range Finding Techniques for Computer Vision'', </title> <journal> IEEE Trans. </journal> <volume> PAMI-5(2), </volume> <month> Mar </month> <year> 1983, </year> <pages> pp. 122-139. </pages>
Reference: 3. <author> E. Krotkov, </author> <title> ``Focusing'', </title> <journal> International Journal of Computer Vision, </journal> <volume> Vol. 1, No. 3, </volume> <month> October </month> <year> 1987, </year> <pages> pp. 223-37. </pages>
Reference: 4. <author> P. R. Cohen and E. A. Feigenbaum, </author> <booktitle> Handbook of Artificial Intelligence, Chap. XIII, </booktitle> <publisher> William Kaufman, </publisher> <address> Los Altos, CA, </address> <booktitle> Vol. IV, </booktitle> <year> 1982. </year>
Reference: 5. <author> Reuel A. Sherman, </author> <title> ``Benefits to Vision Through Stereoscopic Films'', </title> <journal> Journal of the SMPTE, </journal> <volume> Vol. 61, </volume> <month> September </month> <year> 1953, </year> <pages> pp. 294-308. </pages>
Reference: 6. <author> Shoei Kataoka, </author> <title> ``An Attempt Towards an Artificial Retina: 3-D IC Technology for an Intelligent Image Sensor'', Transducers '85, </title> <publisher> IEEE, IEEE, </publisher> <address> Piscataway, NJ 08854, </address> <month> June </month> <year> 1985, </year> <pages> pp. 440-5. </pages>
Reference: 7. <author> B. H. Wilcox, et al., </author> <title> ``A Vision System for the Mars Rover'', </title> <booktitle> Mobile Robots II, SPIE, </booktitle> <volume> Vol. 852, </volume> <year> 1987. </year>
Reference: 8. <institution> Conceptually the signals are available to us from all pixels continuously in parallel; the method would be applicable to a scanned CCD, </institution> <month> vidicon, </month> <title> etc., only if the dither frequency were many times the frame frequency, e.g., 1 Hz. We envisage fabricating sensor arrays with the necessary signal processing replicated behind each pixel, so scanning pixels would deliver combined gray level and range images at the normal frame rate. </title>
Reference: 9. <author> M. W. Siegel, </author> <title> ``Image Focusing in Space and Time'', </title> <type> Technical Report CMU-RI-TR-88-2, </type> <institution> The Robotics Institute, Carnegie Mellon University, </institution> <month> February </month> <year> 1988. </year>

Reference: 11. <editor> Taking the ratio in software is appropriate in the present context of demonstrating the concept while retaining diagnostic access to the lowest level measurements. </editor> <title> We would want a fieldable sensor to take the ratio using an analog dividing circuit whose output we could digitize. </title>
Reference: 12. <editor> In fact this assumption is violated by the third exemplary case, </editor> <title> wherein z is nearly or exactly zero, but this o treatment is intended to be intuition building, not mathematically rigorous. </title>
References-found: 11

