URL: http://www.icsi.berkeley.edu/~luby/PAPERS/linprog.ps
Refering-URL: http://www.icsi.berkeley.edu/~luby/parallel.html
Root-URL: http://www.icsi.berkeley.edu
Title: A Parallel Approximation Algorithm for Positive Linear Programming  
Author: Michael Luby Noam Nisan 
Abstract: We introduce a fast parallel approximation algorithm for the positive linear programming optimization problem, i.e. the special case of the linear programming optimization problem where the input constraint matrix and constraint vector consist entirely of positive entries. The algorithm is elementary, and has a simple parallel implementation that runs in polylog time using a linear number of processors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Berger, B., Rompel, J., Shor, P., </author> <title> "Efficient NC Algorithms for Set Cover with Applications to Learning and Geometry", </title> <booktitle> 30 th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 54-59, </pages> <year> 1989. </year>
Reference-contexts: We first introduce an elementary (but unimplementable) continuous algorithm that produces optimal primal and dual feasible solutions to the problem, and based on this we describe a fast parallel approximation algorithm. We use ideas that were previously employed in similar contexts by <ref> [1, Berger, Rompel, Shor] </ref>, [5, Plotkin, Shmoys, Tardos] and [2, Chazelle, Friedman]. <p> We use ideas that were previously employed in similar contexts by [1, Berger, Rompel, Shor], [5, Plotkin, Shmoys, Tardos] and [2, Chazelle, Friedman]. We use the general idea also used in <ref> [1] </ref> of incrementing the values of many variables in parallel, and we use the general idea also used in [5] and [2] of changing the weight function on the constraints by an amount exponential in the change in the variables. <p> This program is positive and thus our algorithm can be used to approximate the size of the set cover to within a factor of (1 + *) log (). This is essentially optimal (up to NP-completeness [4, Lund, Yannakakis]) and matches results of <ref> [1, Berger, Rompel, Shor] </ref>. 1 In this case finding the set cover itself is also possible using e.g. ideas found in [6, Raghavan]. 1 Our results are a slight improvement over those in [1] in the sense that our multiplicative constant is 1 + *, where * is an input parameter, <p> This is essentially optimal (up to NP-completeness [4, Lund, Yannakakis]) and matches results of [1, Berger, Rompel, Shor]. 1 In this case finding the set cover itself is also possible using e.g. ideas found in [6, Raghavan]. 1 Our results are a slight improvement over those in <ref> [1] </ref> in the sense that our multiplicative constant is 1 + *, where * is an input parameter, whereas their multiplicative constant is fixed to something like 2. 2 2 The problem Throughout this paper, n is the number of variables and m is the number of constraints (not including constraints
Reference: [2] <author> Chazelle, B., Friedman, J., </author> <title> "A Deterministic View of Random Sampling and Its Use in Geometry", </title> <type> Princeton Technical Report No. </type> <institution> CS-TR-436, </institution> <month> September </month> <year> 1988. </year> <note> A preliminary version appears in FOCS 1988. </note>
Reference-contexts: We use ideas that were previously employed in similar contexts by [1, Berger, Rompel, Shor], [5, Plotkin, Shmoys, Tardos] and <ref> [2, Chazelle, Friedman] </ref>. <p> We use the general idea also used in [1] of incrementing the values of many variables in parallel, and we use the general idea also used in [5] and <ref> [2] </ref> of changing the weight function on the constraints by an amount exponential in the change in the variables.
Reference: [3] <author> Cohen, E., </author> <title> "Approximate max flow on small depth networks", </title> <booktitle> FOCS, </booktitle> <year> 1992, </year> <pages> pp. 648-658. </pages>
Reference-contexts: This program is positive and thus our algorithm can be used to approximate the size of the largest matching in a bipartite graph. This essentially matches the results of <ref> [3, Cohen] </ref>, except that we do not know how to find the matching itself. The second example is that of set-cover.
Reference: [4] <author> Lund, C., Yannakakis, M., </author> <title> "On the Hardness of Approximating Minimization Problems", </title> <type> preprint. </type>
Reference-contexts: This program is positive and thus our algorithm can be used to approximate the size of the set cover to within a factor of (1 + *) log (). This is essentially optimal (up to NP-completeness <ref> [4, Lund, Yannakakis] </ref>) and matches results of [1, Berger, Rompel, Shor]. 1 In this case finding the set cover itself is also possible using e.g. ideas found in [6, Raghavan]. 1 Our results are a slight improvement over those in [1] in the sense that our multiplicative constant is 1 +
Reference: [5] <author> Plotkin, S., Shmoys, D., Tardos, E., </author> <title> "Fast Approximation Algorithms for Fractional Packing and Covering Problems", </title> <type> Stanford Technical Report No. </type> <institution> STAN-CS-92-1419, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Supported by USA-Israel BSF 89-00126 and by a Wolfson research award. Research partially done while visiting the International Computer Science Institute. 1 Previously, <ref> [5, Plotkin, Shmoys, Tardos] </ref> have developed fast sequential algorithms for both the primal and dual versions of the positive problem which they call fractional packing and covering problems, (as well as for some generalizations of this problem). <p> They introduce algorithms that are much simpler and far superior in terms of running times than known algorithms for the general linear programming optimization problem. However, the algorithms in <ref> [5] </ref> do not have fast parallel implementations. The algorithm we introduce is competitive with their algorithms in terms of running times when implemented sequentially. <p> We first introduce an elementary (but unimplementable) continuous algorithm that produces optimal primal and dual feasible solutions to the problem, and based on this we describe a fast parallel approximation algorithm. We use ideas that were previously employed in similar contexts by [1, Berger, Rompel, Shor], <ref> [5, Plotkin, Shmoys, Tardos] </ref> and [2, Chazelle, Friedman]. <p> We use the general idea also used in [1] of incrementing the values of many variables in parallel, and we use the general idea also used in <ref> [5] </ref> and [2] of changing the weight function on the constraints by an amount exponential in the change in the variables.

References-found: 5

