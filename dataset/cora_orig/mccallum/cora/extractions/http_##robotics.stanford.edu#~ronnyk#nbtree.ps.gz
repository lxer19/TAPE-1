URL: http://robotics.stanford.edu/~ronnyk/nbtree.ps.gz
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: ronnyk@sgi.com  
Title: Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid  
Author: Ron Kohavi 
Address: 2011 N. Shoreline Blvd Mountain View, CA 94043-1389  
Affiliation: Data Mining and Visualization Silicon Graphics, Inc.  
Abstract: Naive-Bayes induction algorithms were previously shown to be surprisingly accurate on many classification tasks even when the conditional independence assumption on which they are based is violated. However, most studies were done on small databases. We show that in some larger databases, the accuracy of Naive-Bayes does not scale up as well as decision trees. We then propose a new algorithm, NBTree, which induces a hybrid of decision-tree classifiers and Naive-Bayes classifiers: the decision-tree nodes contain uni-variate splits as regular decision-trees, but the leaves contain Naive-Bayesian classifiers. The approach retains the interpretability of Naive-Bayes and decision trees, while resulting in classifiers that frequently outperform both constituents, especially in the larger databases tested. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Brachman, R. J., and Anand, T. </author> <year> 1996. </year> <title> The process of knowledge discovery in databases. In Advances in Knowledge Discovery and Data Mining. </title> <publisher> AAAI Press and the MIT Press. </publisher> <pages> chapter 2, 37-57. </pages>
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Dougherty, J.; Kohavi, R.; and Sahami, M. </author> <year> 1995. </year> <title> Supervised and unsupervised discretization of continuous features. </title> <editor> In Prieditis, A., and Russell, S., eds., </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> 194-202. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fayyad, U. M., and Irani, K. B. </author> <year> 1993. </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1022-1027. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Fayyad, U. M.; Piatetsky-Shapiro, G.; and Smyth, P. </author> <year> 1996. </year> <title> From data mining to knowledge discovery: An overview. In Advances in Knowledge Discovery and Data Mining. </title> <publisher> AAAI Press and the MIT Press. </publisher> <pages> chapter 1, 1-34. </pages>
Reference-contexts: For example, loan applications can be classified into either 'approve' or 'disapprove' classes. A classifier provides a function that maps (classifies) a data item (instance) into one of several predefined classes <ref> (Fayyad, Piatetsky-Shapiro, & Smyth 1996) </ref>. The automatic induction of classifiers from data not only provides a classifier that can be used to map new instances into their classes, but may also provide a human-comprehensible characterization of the classes.
Reference: <author> Friedman, N., and Goldszmidt, M. </author> <year> 1996. </year> <title> Building classifiers using bayesian networks. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence. To appear. </booktitle>
Reference: <author> Good, I. J. </author> <year> 1965. </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods. </title> <publisher> M.I.T. Press. </publisher>
Reference: <author> Gordon, L., and Olshen, R. A. </author> <year> 1984. </year> <title> Almost sure consistent nonparametric regression from recursive partitioning schemes. </title> <journal> Journal of Multivariate Analysis 15 </journal> <pages> 147-163. </pages>
Reference-contexts: However, even with continuous data, the discretization is global and cannot take into account attribute interactions. Decision-trees are non-parametric estimators and can approximate any "reasonable" function as the database size grows <ref> (Gordon & Olshen 1984) </ref>. This theoretical result, however, may not be very comforting if the database size required to reach the asymptotic performance is more than the number of atoms in the universe, as is sometimes the case.
Reference: <author> Hamel, G., and Prahalad, C. K. </author> <year> 1994. </year> <title> Competing for the Future. </title> <publisher> Harvard Business School Press and McGraw Hill. </publisher>
Reference: <author> Kohavi, R.; John, G.; Long, R.; Manley, D.; and Pfleger, K. </author> <year> 1994. </year> <title> MLC++: A machine learning library in C++. </title> <booktitle> In Tools with Artificial Intelligence, </booktitle> <pages> 740-743. </pages> <publisher> IEEE Computer Society Press. </publisher> <address> http://www.sgi.com/Technology/mlc. </address>
Reference-contexts: Naive-Bayes (Good 1965; Langley, Iba, & Thomp-son 1992) uses Bayes rule to compute the probability of each class given the instance, assuming the attributes are conditionally independent given the label. The version of Naive-Bayes we use in our experiments was implemented in MLC ++ <ref> (Kohavi et al. 1994) </ref>. The data is pre-discretized using the an entropy-based algorithm (Fayyad & Irani 1993; Dougherty, Kohavi, & Sahami 1995). The probabilities are estimated directly from data based directly on counts (without any corrections, such as Laplace or m-estimates).
Reference: <author> Kohavi, R. </author> <year> 1995. </year> <title> Wrappers for Performance Enhancement and Oblivious Decision Graphs. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University, Computer Science department. ftp://starry.stanford.edu/pub/ronnyk/teza.ps. </institution>
Reference: <author> Kononenko, I. </author> <year> 1991. </year> <title> Semi-naive bayesian classifiers. </title> <booktitle> In Proceedings of the sixth European Working Session on Learning, </booktitle> <pages> 206-219. </pages>
Reference: <author> Kononenko, I. </author> <year> 1993. </year> <title> Inductive and bayesian learning in medical diagnosis. </title> <booktitle> Applied Artificial Intelligence 7 </booktitle> <pages> 317-337. </pages>
Reference: <author> Langley, P.; Iba, W.; and Thompson, K. </author> <year> 1992. </year> <title> An analysis of bayesian classifiers. </title> <booktitle> In Proceedings of the tenth national conference on artificial intelligence, </booktitle> <pages> 223-228. </pages> <publisher> AAAI Press and MIT Press. </publisher>
Reference-contexts: In many cases, interpretability|the ability to understand the output of the induction algorithm|is a crucial step in the design and analysis cycle. Some classifiers are naturally easier to interpret than others; for example, decision-trees (Quinlan 1993) are easy to visualize, while neural-networks are much harder. Naive-Bayes classifiers <ref> (Langley, Iba, & Thompson 1992) </ref> are generally easy to understand and the induction of these classifiers is extremely fast, requiring only a single pass through the data if all attributes are discrete. Naive-Bayes classifiers are also very simple and easy to understand.
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1996. </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/~mlearn. </note>
Reference: <author> Pazzani, M. </author> <year> 1995. </year> <title> Searching for attribute dependencies in bayesian classifiers. </title> <booktitle> In Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> 424-429. </pages>
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> Los Altos, California: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: In many cases, interpretability|the ability to understand the output of the induction algorithm|is a crucial step in the design and analysis cycle. Some classifiers are naturally easier to interpret than others; for example, decision-trees <ref> (Quinlan 1993) </ref> are easy to visualize, while neural-networks are much harder. Naive-Bayes classifiers (Langley, Iba, & Thompson 1992) are generally easy to understand and the induction of these classifiers is extremely fast, requiring only a single pass through the data if all attributes are discrete. <p> The data is then divided according to the test, and the process repeats recursively for each child. After a full tree is built, a pruning step is executed, which reduces the tree size. In the experiments, we compared our results with the C4.5 decision-tree induction algorithm <ref> (Quinlan 1993) </ref>, which is a state-of-the-art algorithm. Naive-Bayes (Good 1965; Langley, Iba, & Thomp-son 1992) uses Bayes rule to compute the probability of each class given the instance, assuming the attributes are conditionally independent given the label.
Reference: <author> Utgoff, P. E. </author> <year> 1988. </year> <title> Perceptron trees: a case study in hybrid concept representation. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> 601-606. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A decision-tree is built with univariate splits at each node, but with Naive-Bayes classifiers at the leaves. The final classifier resembles Utgoff's Perceptron trees <ref> (Utgoff 1988) </ref>, but the induction process is very different and geared toward larger datasets. The resulting classifier is as easy to interpret as decision-trees and Naive-Bayes.
Reference: <author> Wolpert, D. H. </author> <year> 1994. </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework. </title> <editor> In Wolpert, D. H., ed., </editor> <title> The Mathemtatics of Generalization. </title> <publisher> Addison Wesley. </publisher>
Reference-contexts: C4.5, and the lower six graphs show datasets where C4.5 outperformed Naive-Bayes. The error bars are 95% confidence intervals on the accuracy. data, the learning curves will not cross. While it is well known that no algorithm can outperform all others in all cases <ref> (Wolpert 1994) </ref>, our world does tend to have some smoothness conditions and algorithms can be more successful than others in practice. In the next section we show that a hybrid approach can improve both algorithms in important practical datasets.
References-found: 19

