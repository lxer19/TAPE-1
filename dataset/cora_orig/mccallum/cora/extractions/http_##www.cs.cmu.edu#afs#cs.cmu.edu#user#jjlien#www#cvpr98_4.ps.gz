URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jjlien/www/cvpr98_4.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jjlien/www/index.html
Root-URL: 
Email: jjlien@cs.cmu.edu  jeffcohn@vms.cis.pitt.edu  tk@cs.cmu.edu  ccl@vms.cis.pitt.edu  
Title: Subtly Different Facial Expression Recognition And Expression Intensity Estimation  
Author: , James Jenn-Jier Lien Jeffrey F. Cohn Takeo Kanade Ching-Chung Li 
Web: http://www.cs.cmu.edu/~jjlien  
Address: Pittsburgh Pittsburgh, PA 15260  Pittsburgh  Pittsburgh, PA 15213  Pittsburgh  
Affiliation: 1 Department of Electrical Engineering University of  Department of Psychology University of  Vision and Autonomous Systems Center The Robotics Institute Carnegie Mellon University  Department of Electrical Engineering University of  
Abstract: We have developed a computer vision system, including both facial feature extraction and recognition, that automatically discriminates among subtly different facial expressions. Expression classification is based on Facial Action Coding System (FACS) action units (AUs), and discrimination is performed using Hidden Markov Models (HMMs). Three methods are developed to extract facial expression information for automatic recognition. The first method is facial feature point tracking using a coarse-to-fine pyramid method. This method is sensitive to subtle feature motion and is capable of handling large displacements with sub-pixel accuracy. The second method is dense flow tracking together with principal component analysis (PCA), where the entire facial motion information per frame is compressed to a low-dimensional weight vector. The third method is high gradient component (i.e., furrow) analysis in the spatio-temporal domain, which exploits the transient variation associated with the facial expression. Upon extraction of the facial information, non-rigid facial expression is separated from the rigid head motion component, and the face images are automatically aligned and normalized using an affine transformation. This system also provides expression intensity estimation, which has significant effect on the actual meaning of the expression. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.S. Bartlett, et al., </author> <title> "Classifying Facial Action," </title> <booktitle> Adv. in Neural Info. Proc. Sys. </booktitle> <volume> 8, </volume> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Since our goal is to recognize expression rather than identify individuals or objects [10, 18, 24], facial motion is analyzed using dense flow - not gray value - to ignore differences across individual subjects (compared with <ref> [1] </ref>). To ensure that the pixel-wise flows of each frame have relative geometric correspondence, an affine transformation is used to automatically warp the pixel-wise flows of each frame to the 2-D face model.
Reference: [2] <author> J.N. Bassili, </author> <title> Emotion Recognition: The Role of Facial Movement and the Relative Importance of Upper and Lower Areas of the Face, </title> <journal> J. of Personality and Social Psy., </journal> <volume> Vol. 37, </volume> <pages> pp. 2049-2059, </pages> <year> 1979. </year>
Reference-contexts: The facial motion associated with a furrow produces gray-value change in the face image, which can be extracted by use of high gradient component detectors. Because analysis of dynamic images produces more accurate and robust recognition than that of a single static image <ref> [2] </ref>, expressions are recognized in the context of entire image sequences of arbitrary length. Hidden Markov Models (HMMs) [21] are used for facial expression recognition in image sequences of arbitrary length because they perform well in the spatio-temporal domain, robustly deal with the time warping problem (compared with [15]).
Reference: [3] <author> M.J. Black and Y. Yacoob, </author> <title> "Recognizing Facial Expressions under Rigid and Non-Rigid Facial Motions," </title> <booktitle> Intl. Workshop on Automatic Face and Gesture Recognition, </booktitle> <address> Zurich, </address> <pages> pp. 12-17, </pages> <year> 1995. </year>
Reference-contexts: These approaches have been shown to track motion and classify prototypic emotion expressions [3,4,16,22,26]. A problem, however, is that the flow direction of each individual local face region is changed to conform to the flow plurality of the region <ref> [3, 22, 26] </ref> or averaged over an entire region [15, 16]. These systems are often insensitive to subtle motion because information about small deviations is lost. The recognition ability and accuracy of these systems may be reduced further when presented with less stylized expressions.
Reference: [4] <author> M.J. Black, et al., </author> <title> "Learning Parameterized Models of Image Motion," </title> <address> CVPR, </address> <year> 1997. </year>
Reference: [5] <author> J.F. Cohn and M. </author> <title> Elmore, Effect of Contingent Changes in Mothers Affective Expression on the Organization of Behavior in 3-Month-Old Infants, </title> <booktitle> Infant Behavior and Development, </booktitle> <volume> Vol. 11, </volume> <pages> pp. 493-505, </pages> <year> 1988. </year>
Reference-contexts: 1. Introduction The face is a rich source of information about human behavior. Facial expression displays emotion [7], regulates social behavior <ref> [5] </ref>, signals communicative intent [9], is computationally related to speech production [17], and reveals brain function and pathology [20]. To make use of the information afforded by facial expression, automated reliable and valid measurement is critical.
Reference: [6] <author> P. Ekman and W.V. Friesen, </author> <title> "The Facial Action Coding System," </title> <publisher> Consulting Psy. Press, </publisher> <address> CA, </address> <year> 1978. </year>
Reference-contexts: Furthermore, the structure of HMMs provides a natural description for time dependent actions (e.g., for facial expression [11,12], gesture [27] and speech recognition [21]). 2.1 Facial Action Coding System (FACS) Our approach to facial expression analysis is based on the Facial Action Coding System (FACS) <ref> [6] </ref>, which is an anatomically based coding system that enables discrimination between closely related expressions. FACS divides the face into upper and lower regions and subdivides motion into action units (AUs). AUs are the smallest visibly discriminable muscle actions that combine to perform expressions. <p> Facial Action Coding System Action Units <ref> [6] </ref>. Affine Transformation (0,0) (0,0) Original Face Image Face Model Medial Canthus Philtrum displacements well. In addition, it is useful to measure the motion of the entire face, including the forehead, cheek and chin regions.
Reference: [7] <author> P. Ekman, </author> <title> Facial Expression and Emotion, </title> <journal> American Psychologist, </journal> <volume> Vol. 48, </volume> <pages> pp. 384-392, </pages> <year> 1993. </year>
Reference-contexts: 1. Introduction The face is a rich source of information about human behavior. Facial expression displays emotion <ref> [7] </ref>, regulates social behavior [5], signals communicative intent [9], is computationally related to speech production [17], and reveals brain function and pathology [20]. To make use of the information afforded by facial expression, automated reliable and valid measurement is critical.
Reference: [8] <author> I.A. Essa, </author> <title> "Analysis, Interpretation and Synthesis of Facial Expressions," </title> <type> Perceptual Computing TR 303, </type> <institution> MIT Media Laboratory, </institution> <month> Feb. </month> <year> 1995. </year>
Reference: [9] <author> A.J. </author> <title> Fridlund Human Facial Expression: An Evolutionary View, </title> <publisher> Academic Press, </publisher> <address> CA, </address> <year> 1994. </year>
Reference-contexts: 1. Introduction The face is a rich source of information about human behavior. Facial expression displays emotion [7], regulates social behavior [5], signals communicative intent <ref> [9] </ref>, is computationally related to speech production [17], and reveals brain function and pathology [20]. To make use of the information afforded by facial expression, automated reliable and valid measurement is critical.
Reference: [10] <author> M. Kirby and L. Sirovich, </author> <title> "Application of the Karhuneh-Loeve Procedure for the Characterization of Human Faces," </title> <journal> IEEE Trans. on PAMI 12, </journal> <volume> No. 1, </volume> <year> 1990. </year>
Reference-contexts: PCA has excellent properties for our purposes, including image data compression and maintenance of a strong correlation between two consecutive motion frames. Since our goal is to recognize expression rather than identify individuals or objects <ref> [10, 18, 24] </ref>, facial motion is analyzed using dense flow - not gray value - to ignore differences across individual subjects (compared with [1]).
Reference: [11] <author> J.J. Lien, T. Kanade, A.J. Zlochower, J.F. Cohn, and C.C. Li, </author> <title> "Automatically Recognizing Facial Expressions in the Spatio-Temporal Domain," </title> <booktitle> Workshop on Perceptual User Interfaces, </booktitle> <pages> pp. 94-97, </pages> <address> Banff, Alberta, Canada, </address> <month> October 19-21, </month> <year> 1997. </year>
Reference: [12] <author> J.J. Lien, T. Kanade, J.F. Cohn, and C.C. Li, </author> <title> "Automated Facial Expression Recognition Based on FACS Action Units," </title> <booktitle> Third IEEE International Conference on Automatic Face And Gesture Recognition, </booktitle> <address> Nara, Japan, </address> <month> April 14-16, </month> <year> 1998. </year>
Reference: [13] <author> Y. Linde, A. Buzo, and R. Gray, </author> <title> "An Algorithm for Vector Quantizer Design," </title> <journal> IEEE Trans. on Communications, </journal> <volume> Vol. COM-28, NO. 1, </volume> <year> 1980. </year>
Reference-contexts: Expression Recognition and Expression Intensity Estimation The 12- and 20-dimensional training displacement vectors from feature point tracking, the 20-dimensional training weight vectors from the dense flow tracking together with PCA, and the 32- and 32-dimensional training mean-variance vectors from the high gradient component detection are each vector quantized <ref> [13] </ref>. HMMs are then trained. Because the HMM set represents the most likely individual action unit (AU) or AU combinations, it can be employed to evaluate the test-input sequence. The test-input sequence is evaluated by selecting the maximum likelihood decision value from the HMM set.
Reference: [14] <author> B.D. Lucas and T. Kanade, </author> <title> "An Iterative Image Registration Technique with an Application to Stereo Vision," </title> <booktitle> Proc. of the 7th Intl. Joint Conf. on AI, </booktitle> <year> 1981. </year>
Reference-contexts: Each feature point is the center of a 13 x 13 flow window which is used to compute the horizontal and vertical flow of the feature. The movement of facial feature points is automatically tracked across an image sequence using Lucas-Kanades optical flow algorithm, which has high tracking accuracy <ref> [14] </ref> (Figure 3). The pyramidal (5 level) optical flow method [19] is used for tracking because it robustly manages large facial feature motion displacement, such as mouth opening or brows raised suddenly.
Reference: [15] <author> K. Mase and A. Pentland, </author> <title> "Automatic Lipreading by Optical-Flow Analysis," </title> <journal> Systems and Computers in Japan, </journal> <volume> Vol. 22, No. 6, </volume> <year> 1991. </year>
Reference-contexts: These approaches have been shown to track motion and classify prototypic emotion expressions [3,4,16,22,26]. A problem, however, is that the flow direction of each individual local face region is changed to conform to the flow plurality of the region [3, 22, 26] or averaged over an entire region <ref> [15, 16] </ref>. These systems are often insensitive to subtle motion because information about small deviations is lost. The recognition ability and accuracy of these systems may be reduced further when presented with less stylized expressions. <p> Hidden Markov Models (HMMs) [21] are used for facial expression recognition in image sequences of arbitrary length because they perform well in the spatio-temporal domain, robustly deal with the time warping problem (compared with <ref> [15] </ref>).
Reference: [16] <author> K. Mase, </author> <title> "Recognition of Facial Expression from Optical Flow," </title> <journal> IEICE Trans., </journal> <volume> Vol. E74, </volume> <pages> pp. 3474-3483, </pages> <year> 1991. </year>
Reference-contexts: These approaches have been shown to track motion and classify prototypic emotion expressions [3,4,16,22,26]. A problem, however, is that the flow direction of each individual local face region is changed to conform to the flow plurality of the region [3, 22, 26] or averaged over an entire region <ref> [15, 16] </ref>. These systems are often insensitive to subtle motion because information about small deviations is lost. The recognition ability and accuracy of these systems may be reduced further when presented with less stylized expressions.
Reference: [17] <author> D. McNeil, </author> <title> "So you think gestures are nonverbal?" Psychological Review, </title> <booktitle> 92, </booktitle> <pages> 350-371, </pages> <year> 1985. </year>
Reference-contexts: 1. Introduction The face is a rich source of information about human behavior. Facial expression displays emotion [7], regulates social behavior [5], signals communicative intent [9], is computationally related to speech production <ref> [17] </ref>, and reveals brain function and pathology [20]. To make use of the information afforded by facial expression, automated reliable and valid measurement is critical.
Reference: [18] <author> H. Murase and S.K. Nayar, </author> <title> "Visual Learning and Recognition of 3-D Objects from Appearance," </title> <journal> IJCV, </journal> <volume> 14, </volume> <pages> pp. 5-24, </pages> <year> 1995. </year>
Reference-contexts: PCA has excellent properties for our purposes, including image data compression and maintenance of a strong correlation between two consecutive motion frames. Since our goal is to recognize expression rather than identify individuals or objects <ref> [10, 18, 24] </ref>, facial motion is analyzed using dense flow - not gray value - to ignore differences across individual subjects (compared with [1]).
Reference: [19] <author> C.J. Poelman, </author> <title> The Paraperspective and Projective Factorization Methods for Recovering Shape and Motion, </title> <type> Ph.D. dissertation, </type> <institution> Carnegie Mellon University, CMU-CS-95-173, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: The movement of facial feature points is automatically tracked across an image sequence using Lucas-Kanades optical flow algorithm, which has high tracking accuracy [14] (Figure 3). The pyramidal (5 level) optical flow method <ref> [19] </ref> is used for tracking because it robustly manages large facial feature motion displacement, such as mouth opening or brows raised suddenly. This method deals well with large feature point movement (100 pixel displacement between two frames) while maintaining its sensitivity to subtle (sub-pixel) facial motion.
Reference: [20] <author> W.E. Rinn,. </author> <title> "The neuropsychology of facial expression: A review of the neurological and psychological mechanisms for producing facial expressions." </title> <journal> Psychological Bulletin, </journal> <volume> 95, </volume> <pages> pp. 52-77, </pages> <year> 1984. </year>
Reference-contexts: 1. Introduction The face is a rich source of information about human behavior. Facial expression displays emotion [7], regulates social behavior [5], signals communicative intent [9], is computationally related to speech production [17], and reveals brain function and pathology <ref> [20] </ref>. To make use of the information afforded by facial expression, automated reliable and valid measurement is critical.
Reference: [21] <author> L.R. Rabiner, </author> <title> "An Introduction to Hidden Markov Models," </title> <journal> IEEE ASSP Magazine, </journal> <pages> pp. 4-16, </pages> <month> Jan. </month> <year> 1986. </year>
Reference-contexts: Because analysis of dynamic images produces more accurate and robust recognition than that of a single static image [2], expressions are recognized in the context of entire image sequences of arbitrary length. Hidden Markov Models (HMMs) <ref> [21] </ref> are used for facial expression recognition in image sequences of arbitrary length because they perform well in the spatio-temporal domain, robustly deal with the time warping problem (compared with [15]). <p> Furthermore, the structure of HMMs provides a natural description for time dependent actions (e.g., for facial expression [11,12], gesture [27] and speech recognition <ref> [21] </ref>). 2.1 Facial Action Coding System (FACS) Our approach to facial expression analysis is based on the Facial Action Coding System (FACS) [6], which is an anatomically based coding system that enables discrimination between closely related expressions.
Reference: [22] <author> M. Rosenblum, Y. Yacoob and L.S. Davis, </author> <title> "Human Emotion Recognition from Motion Using a Radial Basis Function Network Architecture," </title> <booktitle> Proc. of the Workshop on Motion of Non-rigid and Articulated Objects, </booktitle> <address> Austin, TX, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: These approaches have been shown to track motion and classify prototypic emotion expressions [3,4,16,22,26]. A problem, however, is that the flow direction of each individual local face region is changed to conform to the flow plurality of the region <ref> [3, 22, 26] </ref> or averaged over an entire region [15, 16]. These systems are often insensitive to subtle motion because information about small deviations is lost. The recognition ability and accuracy of these systems may be reduced further when presented with less stylized expressions.
Reference: [23] <author> D. Terzopoulos and K. Waters, </author> <title> "Analysis of Facial Images Using Physical and Anatomical Models," </title> <booktitle> ICCV, </booktitle> <pages> pp. 727-732, </pages> <month> Dec. </month> <year> 1990. </year>
Reference: [24] <author> M. Turk and A. Pentland, </author> <title> "Eigenfaces for Recognition," </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> Vol. 3, No. 1, </volume> <pages> pp. 71-86, </pages> <year> 1991. </year>
Reference-contexts: PCA has excellent properties for our purposes, including image data compression and maintenance of a strong correlation between two consecutive motion frames. Since our goal is to recognize expression rather than identify individuals or objects <ref> [10, 18, 24] </ref>, facial motion is analyzed using dense flow - not gray value - to ignore differences across individual subjects (compared with [1]).
Reference: [25] <author> Y.T. Wu, T. Kanade, J. F. Cohn, and C.C. Li, </author> <title> Optical Flow Estimation Using Wavelet Motion Model, </title> <address> ICCV, </address> <year> 1998. </year>
Reference-contexts: In addition, it is useful to measure the motion of the entire face, including the forehead, cheek and chin regions. To include this detailed motion information, each pixel of the entire face image is tracked using dense flow <ref> [25] </ref> (Figure 4). Because we have a large image database in which the motion of consecutive frames in a sequence is strongly correlated, the high-dimensional pixel-wise flows of each frame need to be compressed to their low-dimensional representations without losing significant characteristics and inter-frame correlation.
Reference: [26] <author> J. Yacoob and L. Davis, </author> <title> "Computing Spatio-Temporal Representations of Human Faces," </title> <booktitle> CVPR, </booktitle> <pages> pp. 70-75, </pages> <year> 1994. </year>
Reference-contexts: These approaches have been shown to track motion and classify prototypic emotion expressions [3,4,16,22,26]. A problem, however, is that the flow direction of each individual local face region is changed to conform to the flow plurality of the region <ref> [3, 22, 26] </ref> or averaged over an entire region [15, 16]. These systems are often insensitive to subtle motion because information about small deviations is lost. The recognition ability and accuracy of these systems may be reduced further when presented with less stylized expressions.
Reference: [27] <author> J. Yang, </author> <title> "Hidden Markov Model for Human Performance Modeling," </title> <type> Ph.D. Dissertation, </type> <institution> University of Akron, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Furthermore, the structure of HMMs provides a natural description for time dependent actions (e.g., for facial expression [11,12], gesture <ref> [27] </ref> and speech recognition [21]). 2.1 Facial Action Coding System (FACS) Our approach to facial expression analysis is based on the Facial Action Coding System (FACS) [6], which is an anatomically based coding system that enables discrimination between closely related expressions.
References-found: 27

