URL: file://ftp.cis.ohio-state.edu/pub/hpce/tensor/Papers/PPL94-fftrd.ps.gz
Refering-URL: http://www.cis.ohio-state.edu/~chh/Publication/tensor-papers.html
Root-URL: 
Title: IMPLEMENTING FAST FOURIER TRANSFORMS ON DISTRIBUTED-MEMORY MULTIPROCESSORS USING DATA REDISTRIBUTIONS 1  
Author: S. K. S. GUPTA, C.-H. HUANG, P. SADAYAPPAN and R. W. JOHNSON 
Keyword: fast Fourier transform, parallel algorithm, distributed-memory multiprocessor, High Performance Fortran, data distribution, communication optimization.  
Note: Received (received date) Revised (revised date) Communicated by (Name of Editor)  
Address: Columbus, OH 43210, U.S.A.  St. Cloud, MN 56301, U.S.A.  
Affiliation: Department of Computer and Information Science, The Ohio State University  Department of Computer Science, St. Cloud State University  
Abstract: Implementations of various fast Fourier transform (FFT) algorithms are presented for distributed-memory multiprocessors. These algorithms use data redistribution to localize the computation. The goal is to optimize communication cost by using a minimum number of redistribution steps. Both analytical and experimental performance results on the Intel iPSC/860 system are presented. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. R. Johnson, R. W. Johnson, D. Rodriguez, and R. Tolimieri, </author> <title> A methodology for designing, modifying, and implementing Fourier transform algorithms on various architectures, Circuits, </title> <booktitle> Systems, and Signal Processing, </booktitle> <month> 9 </month> <year> (1990) </year> <month> 449-500. </month>
Reference-contexts: 1. Introduction This paper presents distributed-memory parallel programs for the fast Fourier transform (FFT) <ref> [1, 2] </ref> which use data redistribution to localize the computation. In a distributed-memory multiprocessor like the Intel iPSC/860 and the Thinking Machines CM5, shared data is distributed across the local memories of interconnected processors. The most commonly used distributions are the block, cyclic, and block-cyclic distributions.
Reference: 2. <author> C. Van Loan, </author> <title> Computational frameworks for the fast Fourier transform. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: 1. Introduction This paper presents distributed-memory parallel programs for the fast Fourier transform (FFT) <ref> [1, 2] </ref> which use data redistribution to localize the computation. In a distributed-memory multiprocessor like the Intel iPSC/860 and the Thinking Machines CM5, shared data is distributed across the local memories of interconnected processors. The most commonly used distributions are the block, cyclic, and block-cyclic distributions. <p> The twiddle factors will be assumed to be replicated on all processors. Furthermore, we will present only radix-2 decimation-in-time FFT algorithms. However, the techniques presented in this paper are also applicable to mixed-radix and decimation-in-frequency FFTs <ref> [2] </ref>. We will use N (= 2 n ) to denote the size of the FFT being performed and P (= 2 p ; p n) to denote the number of processors being used to perform the FFT.
Reference: 3. <author> High Performance Fortran Forum, </author> <title> High Performance Fortran language specification, </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <year> 1993. </year>
Reference-contexts: In a distributed-memory multiprocessor like the Intel iPSC/860 and the Thinking Machines CM5, shared data is distributed across the local memories of interconnected processors. The most commonly used distributions are the block, cyclic, and block-cyclic distributions. These distributions are also proposed in High Performance Fortran (HPF) <ref> [3] </ref>. Communication is needed when a processor requires data from another processor's local memory. In most distributed-memory multiprocessors the cost of communicating a data value is considerably larger than the cost of a primitive arithmetic computation on the data element.
Reference: 4. <author> S. K. S. Gupta, S. D. Kaushik, C.-H. Huang, J. R. Johnson, R. W. Johnson, and P. Sadayappan, </author> <title> A methodology for the generation of data distributions to optimize communication, </title> <booktitle> Proc. Fourth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> Dec. </month> <year> 1992, </year> <pages> 436-441. </pages>
Reference-contexts: Performance results are given in Section 6. Conclusions are included in Section 7. 2. Semantics of Data Distribution Various data distributions of an array correspond to mapping certain bits of the address (binary representation of the array index) of an element to represent the processor address <ref> [4] </ref>. One such data distribution is the block-cyclic distribution. A block-cyclic distribution partitions an array into equal sized blocks of consecutive elements and maps them to the processors in a cyclic manner. The elements mapped to a processor are stored in increasing order of their indices in its local memory.
Reference: 5. <author> J. W. Cooley and J. W. Tukey, </author> <title> An algorithm for the machine calculation of complex Fourier series. </title> <booktitle> Mathematics of Computation. </booktitle> <month> 19 </month> <year> (1965) </year> <month> 297-301. </month>
Reference-contexts: An FFT is an O (N log N ) algorithm to compute the matrix-vector product: B (i) = j=0 ! N A (j); where 0 i &lt; N and ! N = e 2 p In a 2 n point Cooley-Tukey FFT <ref> [5] </ref>, there are n steps of computation.
Reference: 6. <author> M. C. Pease, </author> <title> An adaptation of the fast Fourier transform for parallel processing, </title> <journal> J. ACM, </journal> <month> 15(2) </month> <year> (1968) </year> <month> 252-264. </month>
Reference-contexts: The input array initially has a block distribution which is changed to a cyclic distribution after the third computation step. 4. Pease FFT Pease proposed an FFT algorithm suitable for parallel processing <ref> [6] </ref>. In this algorithm, the array of intermediate results is permuted (using the perfect shuffle) after the butterfly computation at each step so that the two data elements involved in a butterfly computation at the next step are moved adjacent to each other.
Reference: 7. <author> D. G. Korn and J. J. Lambiotte, </author> <title> Computing the fast Fourier transform on a vector computer, </title> <booktitle> Mathematics of Computation, </booktitle> <month> 33 </month> <year> (1979) </year> <month> 977-992. </month>
Reference-contexts: The input array has a block distribution for the first three steps and a cyclic distribution for the remaining two steps. Communication is only needed to change the distribution from block to cyclic, after the third step. The Korn-Lambiotte FFT <ref> [7] </ref>, which was developed for vector processing, can be similarly implemented on a distributed-memory vector multiprocessor. The Korn-Lambiotte FFT performs a shuffle permutation at the beginning of each step so that a fixed vector length of 2 n1 is achieved.
Reference: 8. <author> T. G. Stockham, </author> <title> High speed convolution and correlation, </title> <booktitle> Proc. Spring Joint Computer Conf., AFIPS, </booktitle> <year> 1966, </year> <pages> 229-233. </pages>
Reference-contexts: Using a technique similar to the one used for the Pease FFT, the Korn-Lambiotte FFT can be modified so that only a single redistribution is required when n 2p. Further, a fixed vector length of 2 np1 is achieved on each node. 5. Stockham FFT In the Stockham FFT <ref> [8] </ref>, at step i, 1 i n, a permutation corresponding to a right cyclic shift of the most significant i address bits is performed on the input vector.
Reference: 9. <author> P. N. Swarztrauber, </author> <title> Multiprocessor FFT's, </title> <booktitle> Parallel Computing, </booktitle> <month> 5 </month> <year> (1987) </year> <month> 197-210. </month>
Reference-contexts: The redistribution requirements for the Cooley-Tukey FFT, Pease FFT, and Stockham FFT are summarized in Table 1. Parallel programs which use point-to-point message-passing were implemented for the Cooley-Tukey FFT (CT-PP) and the Stockham FFT (ST-PP). These programs require log (P ) communication steps <ref> [9] </ref>. All the communication in CT-PP is nearest-neighbor. PS-PP was not implemented as its performance is certain to be worse than CT-PP, due to the log (N ) communication steps needed. In all programs except ST-1R and ST-PP, there is an initial bit-reversal permutation step which requires one additional redistribution.
Reference: 10. <author> S. H. Bokhari, </author> <title> Complete Exchange on the iPSC-860. </title> <type> ICASE Report 91-4, </type> <year> 1991. </year> <month> 12 </month>
Reference-contexts: We now compare the communication requirements of the CT-PP and the CT-1R algorithms. CT-PP requires log (P ) messages, each of size N=P . CT-1R requires a block to cyclic redistribution involving an all-to-all personalized communication, which can be performed using the pairwise exchange algorithm <ref> [10] </ref>. The pairwise exchange algorithm is contention-free on hypercubes. In each of the P 1 steps of this algorithm, each processor sends and receives a message of size N=P 2 . <p> The communication time (in sec.) for a buffered (UNFORCED) message of size m bytes over a distance of d is approximately 164 + 0:398m + 29:9d <ref> [10] </ref>. Therefore, t s = 164 sec. and t p = 8 fl 0:398 = 3:184 sec. (because one complex number has eight bytes). For CT-PP, all the messages are nearest-neighbor.
References-found: 10

