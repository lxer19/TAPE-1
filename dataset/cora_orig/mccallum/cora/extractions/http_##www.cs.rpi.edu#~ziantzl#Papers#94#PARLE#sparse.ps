URL: http://www.cs.rpi.edu/~ziantzl/Papers/94/PARLE/sparse.ps
Refering-URL: http://www.cs.rpi.edu/~ziantzl/Papers/papers.html
Root-URL: http://www.cs.rpi.edu
Title: Run-Time Optimization of Sparse Matrix-Vector Multiplication on SIMD Machines  
Author: Louis H. Ziantz, Can C. Ozturan, and Boleslaw K. Szymanski 
Address: Troy, New York 12180-3590 USA  
Affiliation: Department of Computer Science, Rensselaer Polytechnic Institute  
Date: July, 1994  
Note: published in Proc. Int. Conf. Parallel Architectures and Languages, Athens,  Lecture Notes in Computer Science, Vol. 817, Springer-Verlag, Berlin, 1994, pp. 313-322  
Abstract: Sparse matrix-vector multiplication forms the heart of iterative linear solvers used widely in scientific computations (e.g., finite element methods). In such solvers, the matrix-vector product is computed repeatedly, often thousands of times, with updated values of the vector until convergence is achieved. In an SIMD architecture, each processor has to fetch the updated off-processor vector elements while computing its share of the product. In this paper, we report on run-time optimization of array distribution and off-processor data fetching to reduce both the communication and computation time. The optimization is applied to a sparse matrix stored in a compressed sparse row-wise format. Actual runs on test matrices produced up to a 35 percent relative improvement over a block distribution with a naive multiplication algorithm while simulations over a wider range of processors indicate that up to a 60 percent improvement may be possible in some cases. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <institution> High Performance Fortran Language Specification (version 1.0): High Performance Fortran Forum. Center for Reaserch on Parallel Computation, Rice University, Rice 1993. </institution>
Reference-contexts: Next, we present a parallel heuristic for redistributing matrix A and the aligned vector x among the individual processors to improve the total execution time. The new generation of parallelizing Fortran compilers, e.g., Fortran D [7], Vienna Fortran [19] and High Performance Fortran <ref> [1] </ref>, provide two classes of directives for the layout of arrays: Alignment directives that describe how arrays should be aligned with respect to one another, both within and across array dimensions. Distribution directives that define how arrays should be broken up and distributed onto the processors of the target architecture.
Reference: 2. <author> S. H. Bokhari: </author> <title> On the mapping problem. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-30, </volume> <month> 3-30 </month> <year> (1981). </year>
Reference-contexts: The node exchange idea can be traced back to Rosen's [15] node ordering algorithm for reducing the band-width of a sparse matrix. Bokhari <ref> [2] </ref> used a sequential algorithm based on the same node exchange idea to partition and map FEM meshes onto an array of processors. Hammond [8] used the pairwise exchange heuristic on the massively parallel CM2 system to improve communication.
Reference: 3. <author> H.A. Choi and B. Narahari: </author> <title> Algorithms for mapping and partitioning chain-structured parallel computations. </title> <booktitle> In: Proc. of the Int. Conference on Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: This assumption is justified when the sparse matrix has already been permuted by either a straightforward or an advanced scheme like reverse Cuthill and McKee [4]. In the literature (c.f. <ref> [3] </ref> and [12]), algorithms have been presented for a similar problem involving only the computational load as the cost function. <p> This fact is utilized in <ref> [3] </ref> and [12] to come up with algorithms of complexity O (kn) and O (n + (klogn) 2 ), respectively, for the optimal solution. However, when we incorporate the off-processor reference cost into the cost function, this function is no longer monotonical in partition size.
Reference: 4. <author> E. Cuthill and J. McKee: </author> <title> Several strategies for reducing the bandwidth of matrices. </title> <editor> In: Rose and Willoughby (eds.): </editor> <title> Symposium on Sparse Matrices and Their Applications. </title> <address> New York: </address> <publisher> Plenum Press 1972, </publisher> <pages> pp. 157-166. </pages>
Reference-contexts: Hence, this is a simpler and more restricted problem than an arbitrary partitioning during which the rows can be permuted. This assumption is justified when the sparse matrix has already been permuted by either a straightforward or an advanced scheme like reverse Cuthill and McKee <ref> [4] </ref>. In the literature (c.f. [3] and [12]), algorithms have been presented for a similar problem involving only the computational load as the cost function.
Reference: 5. <author> I. Duff, R. Grimes, and J. Lewis: </author> <title> Sparse matrix test problems. </title> <journal> ACM transactions on Mathematical Software, </journal> <volume> 15, </volume> <month> 1-14 </month> <year> (1989). </year>
Reference-contexts: The versions were run on several benchmarks including meshes originally used by Hammond [8] and test cases from the Harwell-Boeing Sparse Matrix Collection <ref> [5, 6] </ref>. The characteristics of the tests are given in Table 1. The first test case is an unstructured triangular mesh around a 3-component airfoil, while the second test is a portion of a larger mesh representing an unstructured tetrahedral mesh about a Lockheed S-3A Viking aircraft.
Reference: 6. <author> I. Duff, R. Grimes, and J. Lewis: </author> <title> User's Guide for the Harwell-Boeing Sparse Matrix Collection. CERFACS, </title> <address> Toulouse, France: Cedex 1992. </address>
Reference-contexts: The versions were run on several benchmarks including meshes originally used by Hammond [8] and test cases from the Harwell-Boeing Sparse Matrix Collection <ref> [5, 6] </ref>. The characteristics of the tests are given in Table 1. The first test case is an unstructured triangular mesh around a 3-component airfoil, while the second test is a portion of a larger mesh representing an unstructured tetrahedral mesh about a Lockheed S-3A Viking aircraft.
Reference: 7. <author> G. Fox, S. Hiranandani, K Kennedy, C. Koelbel, U. Kremer, C. Tseng, and W. Wu': </author> <title> Fortran D language specification. </title> <type> Technical Report COMP TR90079. </type> <institution> Department of Computer Science, Rice University, </institution> <address> Houston 1991. </address>
Reference-contexts: Next, we present a parallel heuristic for redistributing matrix A and the aligned vector x among the individual processors to improve the total execution time. The new generation of parallelizing Fortran compilers, e.g., Fortran D <ref> [7] </ref>, Vienna Fortran [19] and High Performance Fortran [1], provide two classes of directives for the layout of arrays: Alignment directives that describe how arrays should be aligned with respect to one another, both within and across array dimensions. <p> Because of the use of indirection in subscript expressions, such an analysis is useless for sparse matrix computations. The distribution directives BLOCK and CYCLIC provided by Fortran compilers are not sufficient for effective run-time distribution of sparse matrices. Vienna Fortran [19] and Fortran D <ref> [7] </ref> employ mapping arrays and corresponding routines to support arbitrary distributions of sparse matrices. The mapping array has the same size as the mapped matrix and each location holds the identifier of the processor that stores the matrix element with the corresponding index.
Reference: 8. <author> S. W. Hammond: </author> <title> Mapping Unstructured Grid Computations to Massively Parallel Computers. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rensselaer Polytechnic Institue, </institution> <address> Troy 1991. </address>
Reference-contexts: The redistribution heuristic presented in this paper generalizes the cost function to include communication costs. 3 Implementation and Results This section describes different versions of the multiplication implementation and the redistribution heuristics. The versions were run on several benchmarks including meshes originally used by Hammond <ref> [8] </ref> and test cases from the Harwell-Boeing Sparse Matrix Collection [5, 6]. The characteristics of the tests are given in Table 1. <p> The node exchange idea can be traced back to Rosen's [15] node ordering algorithm for reducing the band-width of a sparse matrix. Bokhari [2] used a sequential algorithm based on the same node exchange idea to partition and map FEM meshes onto an array of processors. Hammond <ref> [8] </ref> used the pairwise exchange heuristic on the massively parallel CM2 system to improve communication. Execution Results To obtain comparative data we have implemented and timed a matrix-vector multiplication with: (i) block distribution, (ii) computational load balance (assuming consecutive allocation of rows), and (iii) the parallel heuristic.
Reference: 9. <author> S. Hiranandani, J. Saltz, M. Piyush, and H. Berryman: </author> <title> Performance of hashed cache data migration schemes on multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12, </volume> <month> 315-422 </month> <year> (1991). </year>
Reference-contexts: The results from implementations and simulations of the multiplication and the heuristic on benchmark test meshes are also given. Concluding remarks are presented in Section 4. The preprocessing of off-processor references follows the methods proposed in <ref> [9] </ref> for MIMD machines. The heuristics for run-time redistribution of sparse matrices on SIMD machines is an original contribution of this paper.
Reference: 10. <author> D. R. Kincaid, J.R. Respess, D.M. Young, and R.G Grimes: </author> <title> ITPACK 2C: A Fortran package for solving large sparse linear systems by adaptive accelerated iterative methods. </title> <type> Technical report, </type> <institution> University of Texas at Austin, Austin. </institution>
Reference-contexts: In this paper, we consider the problem of distributing sparse matrices with the above representation over parallel machines. In Fortran programs, the JA array is usually accessed via an indirection in a loop. As an example, consider the Fortran code (Figure 2 (a)) for matrix-vector multiplication taken from ITPACK <ref> [10] </ref> which is used in many sparse linear solvers. If both VA and JA are distributed by rows and aligned with the multiplied vector x, then each processor is responsible for multiplying the rows allocated to it. <p> The run-time redistribution of a sparse matrix used repeatedly in a matrix vector multiplication should become a common option in the standard library of sparse matrix operations, such as ITPACK <ref> [10] </ref>. We hope that the algorithms presented in this paper will enlarge the repertoire of run-time distribution algorithms available to the users of parallel machines. Acknowledgement The authors would like to thank Steve Hammond and Iain S.
Reference: 11. <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis: </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Redwood City: </address> <publisher> Benjamin Cummings 1994. </publisher>
Reference-contexts: In this paper, we describe algorithms for run-time optimization of iterative sparse matrix-vector multiplication on SIMD machines. Let A be an n fi n sparse matrix that is block-distributed in a compressed sparse row-wise format <ref> [11] </ref> over the processors of an SIMD machine. Let x be a vector aligned with the columns of A and iteratively updated according to the equation x j+1 = Ax j . <p> Here, E might be the ELEMENT array and it might point to another entity array E 0 i which stores the element vertices. Other examples of formats for the storage of sparse matrices may be found in <ref> [11] </ref>. In a compressed sparse row-wise format, a sparse matrix A is represented as a vector of its nonzero elements VA [fl], the corresponding vector of column indexes of these elements JA [fl], and the vector storing the cumulative number of nonzeroes in each row of the matrix, IA [fl].
Reference: 12. <author> D. M. Nicol: </author> <title> Rectilinear partitioning of irregular data parallel computations. </title> <type> Technical Report 91-55, </type> <institution> ICASE, </institution> <month> Hamp-ton </month> <year> 1991. </year>
Reference-contexts: This assumption is justified when the sparse matrix has already been permuted by either a straightforward or an advanced scheme like reverse Cuthill and McKee [4]. In the literature (c.f. [3] and <ref> [12] </ref>), algorithms have been presented for a similar problem involving only the computational load as the cost function. The redistribution heuristic presented in this paper generalizes the cost function to include communication costs. 3 Implementation and Results This section describes different versions of the multiplication implementation and the redistribution heuristics. <p> This fact is utilized in [3] and <ref> [12] </ref> to come up with algorithms of complexity O (kn) and O (n + (klogn) 2 ), respectively, for the optimal solution. However, when we incorporate the off-processor reference cost into the cost function, this function is no longer monotonical in partition size.
Reference: 13. <author> C. Ozturan, B.K. Szymanski, and J. E. Flaherty: </author> <title> Adaptive methods and rectangular partitioning problem. </title> <booktitle> In: Proc. Scalable High Performance Computing Conference 1992, </booktitle> <address> Williamsburg. Washington DC: </address> <publisher> IEEE Computer Science Press 1992, </publisher> <pages> pp. 409-415. </pages>
Reference-contexts: A tradeoff between load and communication is included in the formulation. Hence, the cost might decrease as the partition size is increased because of newly localized off-processor references. As a result, the complexity of the solution increases. Based on the algorithm presented in <ref> [13] </ref>, we found an O (n 2 m) algorithm for an optimal partitioning of a sparse matrix among k processors for the asynchronous cost function defined as: C = max (E (i) + c fl D (i)) : The repartitioning must take place at run-time, when the values of a sparse
Reference: 14. <author> D. Patterson: </author> <title> Massively parallel computer architecture: observations and ideas on a new theoretical model. </title> <type> Technical Report, </type> <institution> Department of Computer Science, University of California at Berkeley, </institution> <month> Berkley </month> <year> 1992. </year>
Reference-contexts: The tests were run for communication-to-computation ratios ranging from 3 to 40, the range suggested as the most typical for the the computer architectures of today and the next decade <ref> [14, 17] </ref>. The number of processors simulated ranged from 16 to 2048 (or to 512 for smaller meshes). In the following subsections, we discuss various plots from the simulation runs.
Reference: 15. <author> R. Rosen: </author> <title> Matrix band width minimization. </title> <booktitle> In: Proc. of the 23rd ACM National Conference. </booktitle> <publisher> Brandon Systems Press 1968, </publisher> <pages> pp. 585-595. </pages>
Reference-contexts: The node exchange idea can be traced back to Rosen's <ref> [15] </ref> node ordering algorithm for reducing the band-width of a sparse matrix. Bokhari [2] used a sequential algorithm based on the same node exchange idea to partition and map FEM meshes onto an array of processors.
Reference: 16. <author> J. Saltz, R. Crowley, R. Mirchandaney, and H. Berryman: </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8, </volume> <month> 303-312 </month> <year> (1990). </year>
Reference-contexts: We believe that effective support of run-time optimization for sparse matrices can be provided through libraries as illustrated by the PARTI primitives <ref> [16, 18] </ref>. We hope that the algorithms presented in this paper will enlarge the repertoire of run-time distribution algorithms available to the users of SIMD machines. The paper is organized as follows. Section 2 describes array data structures commonly used to represent FEM meshes and sparse matrices in Fortran programs.
Reference: 17. <author> R. Schreiber: </author> <title> Scalability of sparse direct solvers. </title> <type> Technical Report, </type> <institution> RIACS, Moffet Field 1992. </institution>
Reference-contexts: The tests were run for communication-to-computation ratios ranging from 3 to 40, the range suggested as the most typical for the the computer architectures of today and the next decade <ref> [14, 17] </ref>. The number of processors simulated ranged from 16 to 2048 (or to 512 for smaller meshes). In the following subsections, we discuss various plots from the simulation runs.
Reference: 18. <author> J. Wu, J. Saltz, H. Berryman, and S. Hiranandani: </author> <title> Distributed memory compiler design for sparse problems. </title> <type> Technical Report TR91-13, </type> <institution> ICASE, Hampton, </institution> <year> 1991. </year>
Reference-contexts: We believe that effective support of run-time optimization for sparse matrices can be provided through libraries as illustrated by the PARTI primitives <ref> [16, 18] </ref>. We hope that the algorithms presented in this paper will enlarge the repertoire of run-time distribution algorithms available to the users of SIMD machines. The paper is organized as follows. Section 2 describes array data structures commonly used to represent FEM meshes and sparse matrices in Fortran programs.
Reference: 19. <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald: </author> <title> Vienna Fortran a language specification version 1.1. </title> <type> Technical Report Interim 21, </type> <institution> ICASE, NASA, </institution> <month> Hampton </month> <year> 1992. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Next, we present a parallel heuristic for redistributing matrix A and the aligned vector x among the individual processors to improve the total execution time. The new generation of parallelizing Fortran compilers, e.g., Fortran D [7], Vienna Fortran <ref> [19] </ref> and High Performance Fortran [1], provide two classes of directives for the layout of arrays: Alignment directives that describe how arrays should be aligned with respect to one another, both within and across array dimensions. <p> Because of the use of indirection in subscript expressions, such an analysis is useless for sparse matrix computations. The distribution directives BLOCK and CYCLIC provided by Fortran compilers are not sufficient for effective run-time distribution of sparse matrices. Vienna Fortran <ref> [19] </ref> and Fortran D [7] employ mapping arrays and corresponding routines to support arbitrary distributions of sparse matrices. The mapping array has the same size as the mapped matrix and each location holds the identifier of the processor that stores the matrix element with the corresponding index.
References-found: 19

