URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P379.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/preprints.htm
Root-URL: http://www.mcs.anl.gov
Title: EVALUATION OF LARGE-SCALE OPTIMIZATION PROBLEMS ON VECTOR AND PARALLEL ARCHITECTURES  
Author: BRETT M. AVERICK AND JORGE J. MOR E 
Keyword: Key words. optimization, large-scale, limited memory, variable metric, performance evaluation, vector architecture, parallel architecture.  
Note: AMS subject classifications. 65Y05, 65Y20, 65K05, 65K10, 90C06, 90C30  
Abstract: We examine the importance of problem formulation for the solution of large-scale optimization problems on high-performance architectures. We use limited memory variable metric methods to illustrate performance issues. We show that the performance of these algorithms is drastically affected by application implementation. Model applications are drawn from the MINPACK-2 test problem collection, with numerical results from a super-scalar architecture (IBM RS6000/370), a vector architecture (CRAY-2), and a massively parallel architecture (Intel DELTA). 1. Introduction. Our aim is to explore performance issues associated with the 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. M. Averick, R. G. Carter, J. J. Mor e, and G.-L. Xue, </author> <title> The MINPACK-2 test problem collection, </title> <type> Preprint MCS-P153-0692, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1992. </year>
Reference-contexts: This collection is representative of large-scale optimization problems arising from applications in elasticity, combustion, lubrication, optimal design, superconductivity, and other fields of interest. Additional information on the MINPACK-2 problems can be found in Averick et al. <ref> [1] </ref>. Performance issues are illustrated with limited memory variable metric algorithms. These methods require minimal storage, lend themselves naturally to vector and parallel implementations, and work well on a wide range of optimization problems. See, for example, the comparisons of Nash and Nocedal [17], and Zou et al. [20]. <p> Our problems are taken from the collection described in Averick et al. <ref> [1] </ref>. This report contains additional information on these problems; in particular, parameter values are chosen as in this report. Steady State Combustion (SSC).
Reference: [2] <author> B. M. Averick, J. J. Mor e, C. H. Bischof, A. Carle, and A. Griewank, </author> <title> Computing large sparse Jacobian matrices using automatic differentiation, </title> <type> Preprint MCS-P348-0193, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1993. </year>
Reference-contexts: include Coleman, Garbow, and More [4] on software for the partition problem, Gold-farb and Toint [7] on the partition problem for matrices that arise from finite element approximations, Jones and Plassmann [10] on the use of partitioning techniques for the iterative solution of sparse linear systems, and Averick et al. <ref> [2] </ref> on computing large sparse Jacobian matrices using automatic differentiation. 5. Performance evaluation on vector architectures. We have shown that partitioning techniques can be used to improve the performance of the function and gradient evaluation algorithms on vector architectures.
Reference: [3] <author> W. J. Cody and W. Waite, </author> <title> Software Manual for the Elementary Functions, </title> <publisher> Prentice Hall, </publisher> <year> 1980. </year>
Reference-contexts: An implementation of the exponential function is likely to require at least 30 floating point operations. See, for example, the implementation described by Cody and Waite <ref> [3] </ref>. Thus, the number of operations on the CRAY-2 look about right, while the number for the IBM probably reflects an extremely careful implementation. The DELTA is an experimental architecture, so the implementation of the intrinsic functions may not have received sufficient attention.
Reference: [4] <author> T. F. Coleman, B. S. Garbow, and J. J. Mor e, </author> <title> Software for estimating sparse Jacobian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 (1984), </volume> <pages> pp. 329-345. </pages> <publisher> 14 B. </publisher> <editor> M. AVERICK AND J. J. </editor> <address> MOR E </address>
Reference-contexts: For general partially separable functions, the partitioning problem can be formulated as a graph coloring problem. For details and references, see Coleman and More [5]. Other references in this area include Coleman, Garbow, and More <ref> [4] </ref> on software for the partition problem, Gold-farb and Toint [7] on the partition problem for matrices that arise from finite element approximations, Jones and Plassmann [10] on the use of partitioning techniques for the iterative solution of sparse linear systems, and Averick et al. [2] on computing large sparse Jacobian
Reference: [5] <author> T. F. Coleman and J. J. Mor e, </author> <title> Estimation of sparse Jacobian matrices and graph coloring problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 (1983), </volume> <pages> pp. 187-209. </pages>
Reference-contexts: For general partially separable functions, the partitioning problem can be formulated as a graph coloring problem. For details and references, see Coleman and More <ref> [5] </ref>.
Reference: [6] <author> J. J. Dongarra and H. A. van der Vorst, </author> <title> Performance of various computers using standard sparse linear equation solving techniques, in Computer Benchmarks, </title> <editor> J. J. Dongarra and H. A. van der Vorst, eds., </editor> <publisher> North-Holland, </publisher> <year> 1994, </year> <pages> pp. 177-191. </pages>
Reference-contexts: The overall Mflop rates are quite satisfactory. For example, Dongarra and van der Vorst <ref> [6] </ref> mention that an unpreconditioned conjugate gradient iteration on a sytem of linear equations with n = 10 6 variables achieves 21.1 Mflops on an IBM RS6000/550 and 149 Mflops on the CRAY-2. 6. Evaluating functions and gradients on distributed memory architectures.
Reference: [7] <author> D. Goldfarb and P. L. Toint, </author> <title> Optimal estimation of Jacobian and Hessian matrices that arise in finite difference calculations, </title> <journal> Math. Comp., </journal> <volume> 43 (1984), </volume> <pages> pp. 69-88. </pages>
Reference-contexts: For general partially separable functions, the partitioning problem can be formulated as a graph coloring problem. For details and references, see Coleman and More [5]. Other references in this area include Coleman, Garbow, and More [4] on software for the partition problem, Gold-farb and Toint <ref> [7] </ref> on the partition problem for matrices that arise from finite element approximations, Jones and Plassmann [10] on the use of partitioning techniques for the iterative solution of sparse linear systems, and Averick et al. [2] on computing large sparse Jacobian matrices using automatic differentiation. 5.
Reference: [8] <author> W. Gropp and B. Smith, </author> <title> Users manual for the Chameleon parallel programming tools, </title> <type> Report ANL-93/23, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1993. </year>
Reference-contexts: These data must be communicated to the processor before it can compute the functions or gradient components that depend on the ghostpoints. Communication is handled by the Chameleon parallel programming tools developed by Gropp and Smith <ref> [8] </ref>. These tools can be used to develop code that is portable to a variety of computing environments. G GGG G GG Fig. 6.2. <p> Thus, each iteration of the VMLM code requires 2 (m + 1) global dot products. These computations are performed with the Chameleon package of Gropp and Smith <ref> [8] </ref>. The line search routine requires very few flops except for function and gradient evaluations, which we have already shown to be amenable to parallel implementation.
Reference: [9] <author> M. Iri, </author> <title> History of automatic differentiation and rounding error estimation, in Automatic Differentiation of Algorithms, </title> <editor> A. Griewank and G. F. Corliss, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <year> 1992, </year> <pages> pp. 3-16. </pages>
Reference-contexts: This is typical for our applications, and should hold in all cases. For a discussion of this point, in connection with automatic differentiation, see Iri <ref> [9] </ref>. As a final remark we note that the evaluation of Hessian-vector products on distributed memory architectures can also be done with the techniques discussed in Section 6, and that results were obtained for the SSC problem that were similar to those shown in Figure 6.3. Acknowledgments.
Reference: [10] <author> M. T. Jones and P. E. Plassmann, </author> <title> Scalable iterative solution of sparse linear systems, </title> <type> Preprint MCS-P277-1191, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1991. </year> <title> [11] , Computation of equilibrium vortex structures for type-II superconductors, </title> <journal> Internat. J. Supercomputing Applications, </journal> <note> 7 (1993). To appear. </note>
Reference-contexts: For details and references, see Coleman and More [5]. Other references in this area include Coleman, Garbow, and More [4] on software for the partition problem, Gold-farb and Toint [7] on the partition problem for matrices that arise from finite element approximations, Jones and Plassmann <ref> [10] </ref> on the use of partitioning techniques for the iterative solution of sparse linear systems, and Averick et al. [2] on computing large sparse Jacobian matrices using automatic differentiation. 5. Performance evaluation on vector architectures.
Reference: [12] <author> D. C. Liu and J. Nocedal, </author> <title> On the limited memory BFGS method for large scale optimization, </title> <journal> Math. Programming, </journal> <volume> 45 (1989), </volume> <pages> pp. 503-528. </pages>
Reference-contexts: See, for example, the comparisons of Nash and Nocedal [17], and Zou et al. [20]. Our implementation of a limited memory variable metric algorithm, VMLM, follows the approach of Liu and Nocedal <ref> [12] </ref>. In Section 3 we describe the key features of this code and its performance on the model applications of Section 2. The most noticeable aspect of these results is that on all the architectures under consideration, the cost of evaluating functions and gradients dominates the overall solution time. <p> In our implementation of the limited memory method, the search parameter ff k is determined by the csrch subroutine of More and Thuente [15]. The matrix H k+1 is defined, as in Liu and Nocedal <ref> [12] </ref>, in terms of a scaling parameter oe k and information gathered during the previous m iterations.
Reference: [13] <author> R. S. Maier, </author> <title> Large-scale minimization on the CM-200, Optimization Methods and Software, </title> <booktitle> 1 (1992), </booktitle> <pages> pp. 55-69. </pages>
Reference-contexts: In Section 7 we also compare the performance of the VMLM code on all three architectures by determining when the DELTA is faster than the other two machines. Performance evaluations of limited memory codes have also been performed by Nocedal [18] and Maier <ref> [13] </ref>. The issues brought out in these studies differ, in particular, because Nocedal deals only with shared memory architectures, while Maier's evaluation on the SIMD architecture of the CM-200 does not address scalability issues. We end the paper by discussing possible extensions of this work.
Reference: [14] <author> J. J. Mor e, </author> <title> On the performance of algorithms for large-scale bound constrained problems, in Large-Scale Numerical Optimization, </title> <editor> T. F. Coleman and Y. Li, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <year> 1991, </year> <pages> pp. 31-45. </pages>
Reference-contexts: More specifically, a vector dependency occurs because several elements try to write into the same component of the gradient. The vector dependency can be eliminated by splitting the functions into groups such that no functions in a group depend on the same variable. This idea was suggested by More <ref> [14] </ref> for treating synchronization issues on shared memory architectures, but can easily be extended to vector architectures. <p> Evaluating functions and gradients on distributed memory architectures. Evaluation of the function and the gradient of a partially separable function on shared memory architectures has been discussed by More <ref> [14] </ref>. The issues are more complicated on distributed memory architectures; the discussion below follows the work of Jones and Plassmann [11]. The first issue that must be considered is that the work must be distributed among the processors so as to obtain good load balancing.
Reference: [15] <author> J. J. Mor e and D. J. Thuente, </author> <title> Line search algorithms with guaranteed sufficient decrease, </title> <type> Preprint MCS-P330-1092, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1992. </year>
Reference-contexts: In a limited memory method the matrix H k is not stored explicitly; instead, it is defined implicitly in terms of a fixed number of vectors. In our implementation of the limited memory method, the search parameter ff k is determined by the csrch subroutine of More and Thuente <ref> [15] </ref>. The matrix H k+1 is defined, as in Liu and Nocedal [12], in terms of a scaling parameter oe k and information gathered during the previous m iterations.
Reference: [16] <author> S. G. Nash, </author> <title> Preconditioning of truncated Newton methods, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 6 (1985), </volume> <pages> pp. 599-616. </pages>
Reference-contexts: Concluding remarks. The performance issues that we have raised apply to truncated Newton methods because for these methods the cost of evaluating the function and gradient is likely to dominate the computing time. Nash and Nodedal [17] mention that a typical cost of the TN code of Nash <ref> [16] </ref> is 325n flops per iteration. This cost is certainly higher than for limited memory variable metric methods, but we still expect it to be a small fraction of the total computing cost for many applications.
Reference: [17] <author> S. G. Nash and J. Nocedal, </author> <title> A numerical study of the limited memory BFGS method and the truncated Newton method for large scale optimization, </title> <journal> SIAM J. Optimization, </journal> <volume> 1 (1991), </volume> <pages> pp. 358-372. </pages>
Reference-contexts: Performance issues are illustrated with limited memory variable metric algorithms. These methods require minimal storage, lend themselves naturally to vector and parallel implementations, and work well on a wide range of optimization problems. See, for example, the comparisons of Nash and Nocedal <ref> [17] </ref>, and Zou et al. [20]. Our implementation of a limited memory variable metric algorithm, VMLM, follows the approach of Liu and Nocedal [12]. In Section 3 we describe the key features of this code and its performance on the model applications of Section 2. <p> Time (seconds) per iteration of VMLM as a function of n 8. Concluding remarks. The performance issues that we have raised apply to truncated Newton methods because for these methods the cost of evaluating the function and gradient is likely to dominate the computing time. Nash and Nodedal <ref> [17] </ref> mention that a typical cost of the TN code of Nash [16] is 325n flops per iteration. This cost is certainly higher than for limited memory variable metric methods, but we still expect it to be a small fraction of the total computing cost for many applications.
Reference: [18] <author> J. Nocedal, </author> <title> The performance of several algorithms for large-scale unconstrained optimization, in Large-Scale Numerical Optimization, </title> <editor> T. F. Coleman and Y. Li, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <year> 1991, </year> <pages> pp. 138-151. </pages>
Reference-contexts: In Section 7 we also compare the performance of the VMLM code on all three architectures by determining when the DELTA is faster than the other two machines. Performance evaluations of limited memory codes have also been performed by Nocedal <ref> [18] </ref> and Maier [13]. The issues brought out in these studies differ, in particular, because Nocedal deals only with shared memory architectures, while Maier's evaluation on the SIMD architecture of the CM-200 does not address scalability issues. We end the paper by discussing possible extensions of this work.
Reference: [19] <author> T. Schlick and A. Fogelson, </author> <title> TNPACK A truncated Newton minimization package for large-scale problems: I. Algorithms and usage, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 18 (1992), </volume> <pages> pp. 46-70. </pages>
Reference-contexts: This cost is certainly higher than for limited memory variable metric methods, but we still expect it to be a small fraction of the total computing cost for many applications. The cost of the truncated Newton method of Schlick and Fogelson <ref> [19] </ref>) depends on OPTIMIZATION PROBLEMS ON PARALLEL ARCHITECTURES 13 Table 8.1 Evaluation of r 2 f (x)v (seconds) for the SSC problem on the CRAY-2 n t s t p 40,000 .70 .015 640,000 11.20 .244 the preconditioner, so the situation is less clear with this code.
Reference: [20] <author> X. Zou, I. M. Navon, M. Berger, K. H. Phua, T. Schlick, and F. X. Le Dimet, </author> <title> Numerical experience with limited-memory quasi-Newton and truncated Newton methods, </title> <journal> SIAM J. Optimization, </journal> <volume> 3 (1993), </volume> <pages> pp. 582-608. </pages>
Reference-contexts: Performance issues are illustrated with limited memory variable metric algorithms. These methods require minimal storage, lend themselves naturally to vector and parallel implementations, and work well on a wide range of optimization problems. See, for example, the comparisons of Nash and Nocedal [17], and Zou et al. <ref> [20] </ref>. Our implementation of a limited memory variable metric algorithm, VMLM, follows the approach of Liu and Nocedal [12]. In Section 3 we describe the key features of this code and its performance on the model applications of Section 2.
References-found: 19

