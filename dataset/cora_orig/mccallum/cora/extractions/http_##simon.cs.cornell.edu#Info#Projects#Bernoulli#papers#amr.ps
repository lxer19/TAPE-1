URL: http://simon.cs.cornell.edu/Info/Projects/Bernoulli/papers/amr.ps
Refering-URL: 
Root-URL: 
Email: fnikosc,prakas,pingalig@cs.cornell.edu  
Title: Parallel, structured and adaptive mesh refinement codes Compiler support for easing the programmer's burden 1)
Author: Nikos Chrisochoides, Induprakas Kodukula, Keshav Pingali 
Note: Outline:  5) Conclusions Phases of the solver  
Date: June 24, 1997  
Address: Ithaca, NY 14853.  
Affiliation: Department of Computer Science, Cornell University,  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> On efficient runtime support for multiblock and multigrid applications: Regular section analysis. </title> <institution> Technical Report CS-TR-3140 and UMIACS-TR-93-92, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Due to the irregular nature of this connectivity, communication patterns are irregular. Because of the inadequacy of present compiler technology, several libraries have been developed to make the application programmer's job easier. The most important of these are Multi-block PARTI <ref> [1] </ref>, P++/AMR++ [3], LPARX [2], DAGH [7]. All these libraries hide the nature of the data distribution from the application programmer as much as possible. They provide constructs such as forall loops to enable application writing at a high level, and provide library calls to take care of parallelization issues. <p> They provide constructs such as forall loops to enable application writing at a high level, and provide library calls to take care of parallelization issues. Each of these libraries makes certain assumptions regarding how the underlying data is distributed, which affects 1 application performance. Multi-block PARTI <ref> [1] </ref> uses block-cyclic distributions as in HPF, but allows data arrays to be mapped to a subspace of all the computing processors. This works well for Multigrid codes, but for adaptive mesh refinement, the limitations of HPF apply here as well.
Reference: [2] <author> Scott B. Baden, Scott R. Kohn, and Stephen J. Fink. </author> <title> Programming with lparx. </title> <type> Technical Report CS94-377, </type> <institution> Department of Computer Science, University of San Diego, </institution> <year> 1994. </year>
Reference-contexts: Due to the irregular nature of this connectivity, communication patterns are irregular. Because of the inadequacy of present compiler technology, several libraries have been developed to make the application programmer's job easier. The most important of these are Multi-block PARTI [1], P++/AMR++ [3], LPARX <ref> [2] </ref>, DAGH [7]. All these libraries hide the nature of the data distribution from the application programmer as much as possible. They provide constructs such as forall loops to enable application writing at a high level, and provide library calls to take care of parallelization issues. <p> AMR++ treats P++ as a black box and uses the distributions that P++ provides. These distributions reduce load-imbalance by allowing arrays to be distributed by columns of variable size, but are still limited in expressiveness. LPARX <ref> [2] </ref> allows data arrays to be distributed in irregularly shaped and irregularly sized blocks onto processors. While in principle it is possible in LPARX for multiple blocks of data to be assigned to one processor, in practice there is only one data block per processor.
Reference: [3] <author> D Balsara, M Lemke, and D Quinlan. AMR++, </author> <title> a c++ object oriented class library for parallel adaptive refinement fluid dynamics applications. </title> <booktitle> In American Society of Mechanical Engineers, Winter Annual Meeting, Annaheim, CA, </booktitle> <volume> volume 157, </volume> <pages> pages 413-433, </pages> <year> 1992. </year>
Reference-contexts: Due to the irregular nature of this connectivity, communication patterns are irregular. Because of the inadequacy of present compiler technology, several libraries have been developed to make the application programmer's job easier. The most important of these are Multi-block PARTI [1], P++/AMR++ <ref> [3] </ref>, LPARX [2], DAGH [7]. All these libraries hide the nature of the data distribution from the application programmer as much as possible. They provide constructs such as forall loops to enable application writing at a high level, and provide library calls to take care of parallelization issues. <p> Multi-block PARTI [1] uses block-cyclic distributions as in HPF, but allows data arrays to be mapped to a subspace of all the computing processors. This works well for Multigrid codes, but for adaptive mesh refinement, the limitations of HPF apply here as well. AMR++ <ref> [3] </ref> is an AMR class library layered on top of a parallel array library P++. AMR++ treats P++ as a black box and uses the distributions that P++ provides. These distributions reduce load-imbalance by allowing arrays to be distributed by columns of variable size, but are still limited in expressiveness.
Reference: [4] <author> Nikos Chrisochoides, Induprakas Kodukula, and Keshav Pingali. </author> <title> Compiler and run-time support for semi-structured applications. </title> <booktitle> In International Conference on Supercomputing. ACM SIGARCH, </booktitle> <year> 1997. </year>
Reference-contexts: With each array is an associated data structure called the distribution descriptor that keeps track of the blocks of the underlying array on each processor. More details on this can be found in <ref> [4] </ref>. * The second step is also specified with the distribution descriptor. We note that each block is completely specified by the indices of two diagonally opposite corners of the block, which can be thought of as an integer tuple. <p> We note that the above sequence of steps merely constitutes a specification of the desired code to be executed. Details of how executable code can be produced is described in detail in <ref> [4, 6] </ref>. 5 Data-centric Compilation support for AMR In this section, we discuss the plan of attack for the problems related to adaptive mesh refinement mentioned earlier using the data-centric approach. <p> In previous work <ref> [4] </ref>, we have demonstrated how to provide efficient solutions to the parallelization process in the presence of fully generalized block distributions.
Reference: [5] <author> High Performance Fortran Forum. </author> <title> High performance fortran language specification, </title> <note> version 2, specification. http://www.crpc.rice.edu/HPFF/hpf2/index.html, October 1996. </note>
Reference-contexts: makes the programmer's job considerably easier. 5.1 Support for general Distributions As mentioned earlier, one of the key problems to be solved when generating efficient code for adaptive mesh refinement, is to be able to generate parallel code in the presence of more complex distributions than currently available in HPF2 <ref> [5] </ref>. In previous work [4], we have demonstrated how to provide efficient solutions to the parallelization process in the presence of fully generalized block distributions.
Reference: [6] <author> Induprakas Kodukula, Ahmed Nawaaz, and Keshav Pingali. </author> <title> Data-centric multi-level blocking. </title> <booktitle> In Programming Languages, Design and Implementation. ACM SIGPLAN, </booktitle> <year> 1997. </year>
Reference-contexts: Specifically, we believe that the data-centric approach <ref> [6] </ref> to compilation that we have developed in the context of locality enhancement for memory hierarchies is the key to our proposed compiler support. 4.1 Data-centric Transformations The ultimate result of the orchestration is, of course, a transformed program with the desired data reuse, but to get that program, the tool <p> We note that the above sequence of steps merely constitutes a specification of the desired code to be executed. Details of how executable code can be produced is described in detail in <ref> [4, 6] </ref>. 5 Data-centric Compilation support for AMR In this section, we discuss the plan of attack for the problems related to adaptive mesh refinement mentioned earlier using the data-centric approach. <p> compiler (with the cooperation of an underlying run-time system) manipulates the complex forms of this distribution at run time with no intervention from the programmer. 4 5.2 Memory management for Deep Hierarchies We developed the data-centric approach in our attempt to exploit locality in deep memory hierarchies for scientific programs <ref> [6] </ref>. AMR applications certainly fall in th class of programs that we can handle using our approach (since the programs have dense loops with affine array access functions). However, there are two additional sources of complexity for AMR applications. * One is the presence of parallelism.
Reference: [7] <author> Manish Parashar and James C. Browne. </author> <title> Distributed dynamic data-structures for parallel adaptive mesh refinement. </title> <booktitle> In HiPC, </booktitle> <year> 1995. </year> <month> 5 </month>
Reference-contexts: Due to the irregular nature of this connectivity, communication patterns are irregular. Because of the inadequacy of present compiler technology, several libraries have been developed to make the application programmer's job easier. The most important of these are Multi-block PARTI [1], P++/AMR++ [3], LPARX [2], DAGH <ref> [7] </ref>. All these libraries hide the nature of the data distribution from the application programmer as much as possible. They provide constructs such as forall loops to enable application writing at a high level, and provide library calls to take care of parallelization issues. <p> For adaptive mesh refinement, LPARX helps in reducing the communication between grid components at the same level, but the communication between grid components at different levels increases. Finally, DAGH <ref> [7] </ref> uses a space-filling curve enumeration to distribute the blocks of an array onto processors. Space filling curves ensure spatial locality, which means that this distribution policy reduces communication between grid components at different levels of the grid hierarchy.
References-found: 7

