URL: http://www.umiacs.umd.edu/users/vishkin/PUBLICATIONS/ipps97.ps
Refering-URL: http://www.umiacs.umd.edu/users/vishkin/PUBLICATIONS/papers.html
Root-URL: 
Email: smueller@cs.uni-sb.de  vishkin@umiacs.umd.edu  
Title: Conflict-Free Access to Multiple Single-Ported Register Files  
Author: Silvia M. Mueller Uzi Vishkin 
Address: PF 151150, 66041 Saarbruecken, Germany  College Park, MD 20742-3251, USA  
Affiliation: University of Saarland Dept. 14: Computer Science  University of Maryland Institute for Advanced Studies  
Abstract: The paper presents a novel static algorithm for mapping values to multiple register files. The algorithm is based on the edge-coloring of a bipartite graph. It allows the migration of values among the register files to keep the number of RAMs as small as possible. By comparison with the register file design used in the Cydra 5 mini-supercomputer, our approach substantially reduces the number of RAMs. This reduction actually grows with the issue rate. For a system with an issue rate of 6 instructions per cycle, the cost (gate count) of the register files are already cut by half. On a numerical workload, like the Livermore Loops, both designs achieve roughly the same performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Beck, D. Yen, and T. Anderson. </author> <title> The Cydra 5 minisupercomputer: Architecture and implementation. </title> <journal> The Journal of Supercomputing, </journal> 7(1/2):143-180, 1993. 
Reference-contexts: In general, the schedule and the scratch pad allocation have to be adapted iteratively. fl Supported by DFG y Also affiliated with Tel Aviv University. Partially supported by NSF grant CCR-9416890 The scratch pad design implemented in the processors of the Cydra 5 mini-supercomputer <ref> [1, 4] </ref> uses a fixed allocation scheme (Section 2). The results of a certain function unit are always mapped to the same scratch pads. Conflict free accesses are ensured by storing the values several times. That results in a simple but very storage intensive register file design. <p> However, it can be computed efficiently by an edge-coloring algorithm. Section 4 compares the performance and the hardware complexity of the two designs, based on the formal hardware model of [9]. 2. The Context Register Matrix CRM The processors of the Cydra 5 supercomputer <ref> [1] </ref> implement a VLIW architecture which requires a register file with 12 read and 6 write ports. Instead of a multi-port RAM, the processors use a scratch pad design to provide sufficient read/write bandwidth. The allocation scheme of the scratch pads is fixed.
Reference: [2] <author> G. Chaitin, M. Auslander, A. Chandra, J. Cocke, M. Hopkins, and P. Markstein. </author> <title> Register alloctaion via coloring. </title> <journal> Computer Languages, </journal> <volume> 6 </volume> <pages> 47-57, </pages> <year> 1981. </year>
Reference-contexts: This paper is not the first to present an application of graph coloring allocating temporary values to registers. Following <ref> [2] </ref>, the use of vertex-coloring for modeling such a problem has gotten a lot of attention in the literature (for instance, see the standard text [7]) even though vertex-coloring is NP-complete.
Reference: [3] <author> D. Christie. </author> <title> Developing the AMD-K5 architecture. </title> <journal> IEEE Micro, </journal> <volume> 16(2) </volume> <pages> 16-26, </pages> <year> 1996. </year>
Reference-contexts: N denotes the data width of the RAMs and A the number of entries per RAM. The ranges are as follows, where the underlined value denotes the default: N 2 f32; 64g f = R=W 2 f2; 3g: For current generation superscalar designs <ref> [3, 13, 11] </ref>, f lies between 1 and 3. The SPV design needs additional write ports for the migration of frequently used results. According to Section 4.3, those results account for at most 30% of the read accesses, at least on the benchmark application.
Reference: [4] <author> J. Dehnert and R. Towle. </author> <title> Compiling for the Cydra 5. </title> <journal> The Journal of Supercomputing, </journal> 7(1/2):181-228, 1993. 
Reference-contexts: In general, the schedule and the scratch pad allocation have to be adapted iteratively. fl Supported by DFG y Also affiliated with Tel Aviv University. Partially supported by NSF grant CCR-9416890 The scratch pad design implemented in the processors of the Cydra 5 mini-supercomputer <ref> [1, 4] </ref> uses a fixed allocation scheme (Section 2). The results of a certain function unit are always mapped to the same scratch pads. Conflict free accesses are ensured by storing the values several times. That results in a simple but very storage intensive register file design. <p> Changing the scratch pad design can have an impact on both, the cycle count and the cycle time. Impact on the Cycle Count The Livermore Fortran Kernels (LFKs) have been compiled for the Cydra 5 with its CRM scratch pads <ref> [4] </ref>, but cycle counts have not been published. For the SPV based design, these data are also missing. Thus, we can only estimate the impact on the cycle count. However, switching to the SPV design can only increase the cycle count due to the following two problems: 1.
Reference: [5] <author> A. Formella. </author> <title> Performance and Quality of Vectorcom-puters for Numerical Applications. </title> <type> PhD thesis, </type> <institution> University of Saarland, Germany, Computer Science Depart., </institution> <year> 1992. </year>
Reference-contexts: Since both design use a static instruction schedule, indexing slows down the SPV based design as much as the CRM based design. Data Migration Only results used by more than one instruction are migrated. The data flow graphs of the LFKs (e.g., as given in <ref> [5] </ref>) indicate that, with respect to frequently used results, the kernels fall into one of two classes: * Each vector element is only referenced once. The pointers to the vectors can be updated whenever they are used for referencing a vector element.
Reference: [6] <author> H. Gabow and O. Kariv. </author> <title> Algorithm for edge coloring bipartite graphs and multigraphs. </title> <journal> SIAM J. Comput., </journal> <volume> 11 </volume> <pages> 117-129, </pages> <year> 1982. </year>
Reference-contexts: Allocation Scheme The second step of the algorithm computes an edge-coloring for the access graph, i.e., it assigns colors (labels) to the edges so that any two edges which share an end-point get a different color. Since the access graph is bipartite, an algorithm like the one of <ref> [6] </ref> can color the edges with labels 1; : : : ; d, where d denotes the degree of G (P ). 2 Table A I 1 I 2 I 3 I 4 I 5 operands I/O a, a, b b, c, c a, b, e b, d, f results a,
Reference: [7] <author> J. Hennessy and D. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, INC., </publisher> <address> San Mateo, CA, 2nd edition, </address> <year> 1996. </year>
Reference-contexts: 1. Introduction Superscalar and very large instruction word (VLIW) architectures achieve a high performance by issuing multiple instructions per cycle, but they require high-bandwidth register files. Current systems have an issue rate of 2 to 6 instructions per cycle, and the rate is still growing (as per <ref> [7] </ref>, for example). That makes the design of fast memories, such as register files, even more important and challenging. In [10], Rau et al. suggested to store the temporary results in multiple register files, so called scratch pads. <p> This paper is not the first to present an application of graph coloring allocating temporary values to registers. Following [2], the use of vertex-coloring for modeling such a problem has gotten a lot of attention in the literature (for instance, see the standard text <ref> [7] </ref>) even though vertex-coloring is NP-complete. While edge-coloring in bipartite graphs can also be used for modeling, it has an important practical advantage: providing efficient algorithms. Acknowledgment Numerous comments by Shlomo Weiss are gratefully acknowledged.
Reference: [8] <author> F. McMahon. </author> <title> The Livermore Fortran Kernels test of the numerical performance range. </title> <type> Technical report, </type> <institution> Lawrence Livermore National Laboratory, </institution> <address> CA, </address> <year> 1988. </year>
Reference-contexts: Performance Evaluation In order to quantify the performance impact of the new scratch pad design, we analyze the run time of the Livermore Fortran Kernels <ref> [8] </ref> on both designs. Note that the run time of a program on a machine M is the product of a cycle count and the cycle time of M . Changing the scratch pad design can have an impact on both, the cycle count and the cycle time.
Reference: [9] <author> S. Muller and W. Paul. </author> <title> The Complexity of Simple Computer Architectures. </title> <booktitle> Lecture Notes in Computer Science 995. </booktitle> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: Thus, the allocation scheme becomes more complicated again. However, it can be computed efficiently by an edge-coloring algorithm. Section 4 compares the performance and the hardware complexity of the two designs, based on the formal hardware model of <ref> [9] </ref>. 2. The Context Register Matrix CRM The processors of the Cydra 5 supercomputer [1] implement a VLIW architecture which requires a register file with 12 read and 6 write ports. Instead of a multi-port RAM, the processors use a scratch pad design to provide sufficient read/write bandwidth. <p> One possibility is that the compiler could extend our solution for pre-fetching memory data. 4. Quantitative Comparison This section compares the storage capacity, the hardware complexity and the performance of the two designs. Cost (gate count) and cycle time are evaluated based on the formal hardware model of <ref> [9] </ref>. Due to lack of space, we must omit most the hardware details, but they can be found on a Web-page 1 . We confine the analysis to reasonable values for the size of the register files and the ratio f = R=W of read and write ports. <p> For a given system size (R; W; B), it therefore depends on the cost of a RAM and a multiplexer, whether the SPV design is cheaper than the CRM design. We now quantify the cost impact within the framework of <ref> [9] </ref>. Figure 5 depicts the cost (gate count) of the CRM and SPV scratch pad designs as a function of the number of write ports W . The other parameters have their default value, i.e, N = 32, A = 32, and f = 2.
Reference: [10] <author> B. Rau, C. Glaeser, and R. </author> <title> Picard. Efficient code generation for horizontal architectures: </title> <booktitle> Compiler techniques and architectural support. In Proc. 9th International Symposium on Computer Architecture, </booktitle> <pages> pages 131-139, </pages> <year> 1982. </year>
Reference-contexts: Current systems have an issue rate of 2 to 6 instructions per cycle, and the rate is still growing (as per [7], for example). That makes the design of fast memories, such as register files, even more important and challenging. In <ref> [10] </ref>, Rau et al. suggested to store the temporary results in multiple register files, so called scratch pads. A viable scratch pad allocation must ensure that values which are accessed simultaneously reside in distinct scratch-pads.
Reference: [11] <author> M. Tremblay and J. O'Connor. UltraSPARC I: </author> <title> A four-issue processor supporting multimedia. </title> <journal> IEEE Micro, </journal> <volume> 16(2) </volume> <pages> 42-49, </pages> <year> 1996. </year>
Reference-contexts: N denotes the data width of the RAMs and A the number of entries per RAM. The ranges are as follows, where the underlined value denotes the default: N 2 f32; 64g f = R=W 2 f2; 3g: For current generation superscalar designs <ref> [3, 13, 11] </ref>, f lies between 1 and 3. The SPV design needs additional write ports for the migration of frequently used results. According to Section 4.3, those results account for at most 30% of the read accesses, at least on the benchmark application.
Reference: [12] <author> U. Vishkin and A. Wigderson. </author> <title> Dynamic parallel memories. </title> <journal> Information and Control, </journal> <volume> 56 </volume> <pages> 174-182, </pages> <year> 1983. </year>
Reference-contexts: The Scratch Pad Vector SPV After describing how to model the scratch pad allocation by graph coloring, we optimize the number of scratch pads and sketch a hardware realization. Our solution builds on the ideas for the implementation of a degenerate family of parallel algorithms in <ref> [12] </ref>. 3.1. General Principles Given the schedule of an instruction sequence P = I 1 ; : : : ; I p , our approach proceeds in three steps. It first translates the instruction schedule into a bipartite graph G (P ).
Reference: [13] <author> K. Yeager. </author> <title> The MIPS R10000 superscalar microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 16(2) </volume> <pages> 28-40, </pages> <year> 1996. </year> <month> 7 </month>
Reference-contexts: N denotes the data width of the RAMs and A the number of entries per RAM. The ranges are as follows, where the underlined value denotes the default: N 2 f32; 64g f = R=W 2 f2; 3g: For current generation superscalar designs <ref> [3, 13, 11] </ref>, f lies between 1 and 3. The SPV design needs additional write ports for the migration of frequently used results. According to Section 4.3, those results account for at most 30% of the read accesses, at least on the benchmark application.
References-found: 13

