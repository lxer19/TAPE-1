URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/barua-paging-full.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/
Root-URL: 
Title: Addressing Partitioned Arrays in Distributed Memory Multiprocessors the Software Virtual Memory Approach  
Author: Rajeev Barua David Kranz Anant Agarwal 
Keyword: multiprocessors, compilers, addressing, data partitioning, loop partitioning, pages, virtual memory, locality.  
Date: May 6, 1996  
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Partitioning distributed arrays to ensure locality of reference is widely recognized as being critical in obtaining good performance on distributed memory multiprocessors. Data partitioning is the process of tiling data arrays and placing the tiles in memory such that a maximum number of data accesses are satisfied from local memory. Unfortunately, data partitioning makes it difficult to physically locate an element of a distributed array. Data tiles with complicated shapes, such as hyperparallelepipeds, exacerbate this addressing problem. In this paper we propose a simple scheme called software virtual memory that allows flexible addressing of partitioned arrays with low runtime overhead. Software virtual memory implements address translation in software using small, one-dimensional pages, and a compiler-generated software page map. Because page sizes are chosen by the compiler, arbitrarily complex data tiles can be used to maximize locality, and because the pages are one-dimensional, runtime address computations are simple and efficient. One-dimensional pages also ensure that software virtual memory is more efficient than simple blocking for rectangular data tiles. Software virtual memory provides good locality for complicated compile-time partitions, thus enabling the use of sophisticated partitioning schemes appearing in recent literature. Software virtual memory can also be used in systems that provide hardware support for virtual memory. Although hardware virtual memory, when used exclusively, eliminates runtime overhead for addressing, we demonstrate that it does not preserve locality of reference to the same extent as software virtual memory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. G. Abraham and D. E. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to reduce cache coherency traffic. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 318-328, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse Authors' e-mail: fbarua,kranz,agarwalg@lcs.mit.edu. <p> Section 6 contains some experimental results on the locality/addressing tradeoff. We conclude in Section 7 . 2 Related work There has been surprisingly little related work in this area. While many works have addressed the problems of loop and data partitioning <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>, the problem of addressing data partitioned arrays, which is orthogonal, has been assumed to be system dependent.
Reference: [2] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Kenneth Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture (ISCA'95), </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Note that in distributed memory machines without a shared address space, software virtual memory has more to offer because hardware virtual memory is not supported. We have implemented the software virtual memory scheme in the compiler and runtime system for the Alewife machine <ref> [2] </ref>, a globally cache-coherent distributed-memory multiprocessor. We use the method of loop and data partitioning described in [3]. In this paper we demonstrate that: * The overhead of software virtual memory is small in general. <p> We ran two small programs on a simulator for the MIT Alewife <ref> [2] </ref> multiprocessor: a Jacobi relaxation that can achieve good locality with rectangular data partitions and a synthetic application that needs a parallelogram data partition for good locality, (pgram).
Reference: [3] <author> Anant Agarwal, David Kranz, and Venkat Natarajan. </author> <title> Automatic Partitioning of Parallel Loops for Cache-Coherent Multiprocessors. </title> <booktitle> In 22nd International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year> <note> IEEE. To appear in IEEE TPDS. </note>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse Authors' e-mail: fbarua,kranz,agarwalg@lcs.mit.edu. <p> We have implemented the software virtual memory scheme in the compiler and runtime system for the Alewife machine [2], a globally cache-coherent distributed-memory multiprocessor. We use the method of loop and data partitioning described in <ref> [3] </ref>. In this paper we demonstrate that: * The overhead of software virtual memory is small in general. <p> Section 6 contains some experimental results on the locality/addressing tradeoff. We conclude in Section 7 . 2 Related work There has been surprisingly little related work in this area. While many works have addressed the problems of loop and data partitioning <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>, the problem of addressing data partitioned arrays, which is orthogonal, has been assumed to be system dependent. <p> These methods only work well when the granularity of the computation is large and regular. 3 Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches <ref> [3, 6] </ref>. In these machines, each processor controls a local portion of the global memory; references to the local portion have lower latency than references that access remote data over the communication network. <p> Another software approach is to allocate data tiles of complex shapes, for example, parallelograms, to each processor. This is a generalization of blocking. As shown in <ref> [3] </ref>, parallelogram partitions are often required to ensure optimal locality when array accesses contain affine index functions. Let us illustrate the difficulty of addressing parallelogram data tiles with the following example. Consider the nested Doall loop in Figure 2.
Reference: [4] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The sequence adds an overhead of only four instructions (the lddf would be done anyway). On the Sparcle processor <ref> [4] </ref> used in Alewife, these 4 instructions require 5 cycles, assuming all instructions and the page table lookup hit in the cache. We expect that the cache hit rate will not degrade significantly even for small page sizes, because the page table entries are small compared to a page.
Reference: [5] <author> Saman P. Amarasinghe and Monica S. Lam. </author> <title> Communication Optimization and Code Generation for Distributed Memory Machines. </title> <booktitle> In Proceedings of SIGPLAN '93, Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 10, 11, 15, 14, 16, 17, 19, 21] </ref>. These methods only work well when the granularity of the computation is large and regular. 3 Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [6] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of SIGPLAN '93 Conference on Programming Languages Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse Authors' e-mail: fbarua,kranz,agarwalg@lcs.mit.edu. <p> Section 6 contains some experimental results on the locality/addressing tradeoff. We conclude in Section 7 . 2 Related work There has been surprisingly little related work in this area. While many works have addressed the problems of loop and data partitioning <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>, the problem of addressing data partitioned arrays, which is orthogonal, has been assumed to be system dependent. <p> These methods only work well when the granularity of the computation is large and regular. 3 Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches <ref> [3, 6] </ref>. In these machines, each processor controls a local portion of the global memory; references to the local portion have lower latency than references that access remote data over the communication network.
Reference: [7] <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic Data Layout Using 0-1 Integer Programming. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <pages> pages 111-122, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse Authors' e-mail: fbarua,kranz,agarwalg@lcs.mit.edu. <p> Section 6 contains some experimental results on the locality/addressing tradeoff. We conclude in Section 7 . 2 Related work There has been surprisingly little related work in this area. While many works have addressed the problems of loop and data partitioning <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>, the problem of addressing data partitioned arrays, which is orthogonal, has been assumed to be system dependent.
Reference: [8] <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tseng. </author> <title> Compiler Optimization for Improving Data Locality. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 252-262, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse Authors' e-mail: fbarua,kranz,agarwalg@lcs.mit.edu. <p> Section 6 contains some experimental results on the locality/addressing tradeoff. We conclude in Section 7 . 2 Related work There has been surprisingly little related work in this area. While many works have addressed the problems of loop and data partitioning <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>, the problem of addressing data partitioned arrays, which is orthogonal, has been assumed to be system dependent.
Reference: [9] <author> M. Cierniak and W. Li. </author> <title> Unifying Data and Control Transformations for Distributed Shared-Memory Machines. </title> <booktitle> Proceedings of the SIGPLAN PLDI, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse Authors' e-mail: fbarua,kranz,agarwalg@lcs.mit.edu. <p> Section 6 contains some experimental results on the locality/addressing tradeoff. We conclude in Section 7 . 2 Related work There has been surprisingly little related work in this area. While many works have addressed the problems of loop and data partitioning <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>, the problem of addressing data partitioned arrays, which is orthogonal, has been assumed to be system dependent.
Reference: [10] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On Estimating and Enhancing Cache Effectiveness, </title> <address> pages 328-341. </address> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year> <booktitle> Lecture Notes in Computer Science: Languages and Compilers for Parallel Computing. </booktitle> <editor> Editors U. Banerjee and D. Gelernter and A. Nicolau and D. </editor> <address> Padua. </address>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 10, 11, 15, 14, 16, 17, 19, 21] </ref>. These methods only work well when the granularity of the computation is large and regular. 3 Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [11] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 10, 11, 15, 14, 16, 17, 19, 21] </ref>. These methods only work well when the granularity of the computation is large and regular. 3 Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [12] <author> E.D. Granston and H. Wijshoff. </author> <title> Managing Pages in Shared Virtual Memory Systems : Getting the Compiler into the Game. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 11-20, </pages> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: In one sense, this is not so surprising, as software virtual memory is really a very specialized method of software address calculation. Note, software virtual memory is not to be confused with shared virtual memory, which is often also abbreviated to SVM in literature, as in <ref> [12] </ref>. Shared virtual memory is hardware virtual memory as adapted to shared memory multiprocessors. 3 Loop and Data Partitioning Most existing work in compilers for parallel machines has focused on parallelizing sequential code and executing it on machines where each processor has a separate address space, e.g.
Reference: [13] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of Automatic Data Partitioning Techniques for Parallelizing Compilers on Multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year> <month> 13 </month>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse Authors' e-mail: fbarua,kranz,agarwalg@lcs.mit.edu. <p> Section 6 contains some experimental results on the locality/addressing tradeoff. We conclude in Section 7 . 2 Related work There has been surprisingly little related work in this area. While many works have addressed the problems of loop and data partitioning <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>, the problem of addressing data partitioned arrays, which is orthogonal, has been assumed to be system dependent.
Reference: [14] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD Distributed Memory Machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 10, 11, 15, 14, 16, 17, 19, 21] </ref>. These methods only work well when the granularity of the computation is large and regular. 3 Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [15] <author> F. Irigoin and R. Triolet. </author> <title> Supernode Partitioning. </title> <booktitle> In 15th Symposium on Principles of Programming Languages (POPL XV), </booktitle> <pages> pages 319-329, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 10, 11, 15, 14, 16, 17, 19, 21] </ref>. These methods only work well when the granularity of the computation is large and regular. 3 Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [16] <author> Kathleen Knobe, Joan Lukas, and Guy Steele Jr. </author> <title> Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 10, 11, 15, 14, 16, 17, 19, 21] </ref>. These methods only work well when the granularity of the computation is large and regular. 3 Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [17] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling Global Name-Space Parallel Loops for Distributed Execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> October </month> <year> 1991. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 10, 11, 15, 14, 16, 17, 19, 21] </ref>. These methods only work well when the granularity of the computation is large and regular. 3 Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [18] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-Time Techniques for Data Distribution in Distributed Memory Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The problem of loop and data partitioning for distributed memory multiprocessors with global address spaces has been studied by many researchers <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>. The goal of loop partitioning for applications with nested loops that access data arrays is to divide the iteration space among the processors to get maximum reuse Authors' e-mail: fbarua,kranz,agarwalg@lcs.mit.edu. <p> Section 6 contains some experimental results on the locality/addressing tradeoff. We conclude in Section 7 . 2 Related work There has been surprisingly little related work in this area. While many works have addressed the problems of loop and data partitioning <ref> [1, 3, 6, 18, 9, 8, 7, 13] </ref>, the problem of addressing data partitioned arrays, which is orthogonal, has been assumed to be system dependent.
Reference: [19] <author> Anne Rogers and Keshav Pingali. </author> <title> Process Decomposition through Locality of Reference. </title> <booktitle> In SIGPLAN '89, Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 10, 11, 15, 14, 16, 17, 19, 21] </ref>. These methods only work well when the granularity of the computation is large and regular. 3 Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
Reference: [20] <author> Madhusudhan Talluri, Shing Kong, Mark D. Hill, and David A. Patterson. </author> <title> Tradeoffs in Supporting Two Page Sizes. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 415-424, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: On the other hand, hardware virtual memory systems that support multiple page sizes might reduce some of the problems with fixed page sizes. Although multiple-page-size systems merit further exploration, they do not appear very promising because only the simplest of these solutions are practical to build <ref> [20] </ref>, and the need to support very small pages further complicates the hardware.
Reference: [21] <author> M. Wolfe. </author> <title> More Iteration Space Tiling. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 655-664, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: CM-5 or Intel iPSC. It is usually assumed that the programmer specifies how data is distributed and the compiler tries to optimize communication by grouping references to remote data so the high cost of remote accesses can be amortized <ref> [5, 10, 11, 15, 14, 16, 17, 19, 21] </ref>. These methods only work well when the granularity of the computation is large and regular. 3 Some recent work has looked at compilation for machines with a shared address space, physically distributed memory and globally coherent caches [3, 6].
References-found: 21

