URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/src-tr-91-042.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: A Parallelizable Eigensolver for Real Diagonalizable Matrices with Real Eigenvalues  
Author: Steven Huss-Lederman Anna Tsao Thomas Turnbull 
Abstract: In this paper, preliminary research results on a new algorithm for finding all the eigenvalues and eigenvectors of a real diagonalizable matrix with real eigenvalues are presented. The basic mathematical theory behind this approach is reviewed and is followed by a discussion of the numerical considerations of the actual implementation. The numerical algorithm has been tested on thousands of matrices on both a Cray-2 and an IBM RS/6000 Model 580 workstation. The results of these tests are presented. Finally, issues concerning the parallel implementation of the algorithm are discussed. The algorithm's heavy reliance on matrix-matrix multiplication, coupled with the divide and conquer nature of this algorithm, should yield a highly parallelizable algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anderson, E., Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, & D. Sorensen, </author> <title> LAPACK: A portable linear algebra library for high-performance computers, </title> <booktitle> Proceedings, Supercomputing `90, </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1990, </year> <pages> pp. 2-11. </pages>
Reference-contexts: We borrow the term "smoothing" from digital filter theory [17]. The Scaling and Eigenvalue Smoothing steps proceed as follows: Scaling: Compute bounds on the spectrum (A) of A and use these bounds to compute ff and fi such that for `(x) = ffx + fi, (`(A)) <ref> [0; 1] </ref>, with the mean eigenvalue of A being mapped to 1=2. Eigenvalue Smoothing: Let p i (x), i = 1; 2; : : : be polynomials such that in the limit values in [0; 1=2) are mapped near 0 and values in (1=2; 1] are mapped near 1. <p> Eigenvalue Smoothing: Let p i (x), i = 1; 2; : : : be polynomials such that in the limit values in <ref> [0; 1=2) are mapped near 0 and values in (1=2; 1] </ref> are mapped near 1. <p> In other words, ^ a is the composition p K ffi p 1 ffi `. 2.1. Scaling scheme. The requirement that the polynomial ` map (A) into <ref> [0; 1] </ref> is just a convenience. Note, however, that in order for ^ a to map half the spectrum of A near 0, ` must map roughly half the eigenvalues of A into [0; 1=2). <p> Let ! and be a lower and upper bound on (A), respectively. In our implementation, we use the bounds provided by Gershgorin disks [16] as ! and . Then we let ` be the linear map that maps (A) into as large a subinterval of <ref> [0; 1] </ref> as possible so that `() = 1=2. That is, `(x) = &lt; 1 i + 1 ; if !+ 1 i ! + 1 ; if &gt; !+ The behavior of ` is illustrated in Figure 2.1. 2.2. Eigenvalue Smoothing. 2.2.1. Iteration scheme. <p> Note that for each j, B j is a polynomial of degree 2j + 1 that increases on <ref> [0; 1] </ref> and has fixed points at 0, 1=2, and 1. Let O be the function defined on [0; 1] by 8 &gt; : 2 ; 2 ; if x = 1 1; if 1 An obvious approach is to let p i = B i (i = 1; 2; 3; <p> Note that for each j, B j is a polynomial of degree 2j + 1 that increases on <ref> [0; 1] </ref> and has fixed points at 0, 1=2, and 1. Let O be the function defined on [0; 1] by 8 &gt; : 2 ; 2 ; if x = 1 1; if 1 An obvious approach is to let p i = B i (i = 1; 2; 3; : : :) since for x 2 [0; 1], lim B j (x) = O (x): It is <p> Let O be the function defined on <ref> [0; 1] </ref> by 8 &gt; : 2 ; 2 ; if x = 1 1; if 1 An obvious approach is to let p i = B i (i = 1; 2; 3; : : :) since for x 2 [0; 1], lim B j (x) = O (x): It is clear that in this approach, K would need to be prohibitively high, making this approach infeasible. <p> A better approach is to simply choose one polynomial in the family given by (2.1) and apply it recursively, i.e., since for fixed k 2 N and x 2 <ref> [0; 1] </ref>, lim B k (x) = O (x): (2.2) Here B k (x) = B k (B k ( (B k (x)))) i times : In our implementation, we choose k = 1. Note that B 1 (x) = 3x 2 2x 3 . <p> Although the table indicates that B 2 may be preferable to B 1 , B 1 was chosen over B 2 because it has a local minimum and maximum at 0 and 1, respectively. This property ensures that eigenvalues mapped outside <ref> [0; 1] </ref> because of machine roundoff will tend to be mapped back into [0; 1] by subsequent applications of B 1 . k N approximate degree of B (N) k # matrix multiplications 1 86 1:1 fi 10 41 172 3 45 1:1 fi 10 38 180 5 36 3:1 fi <p> This property ensures that eigenvalues mapped outside <ref> [0; 1] </ref> because of machine roundoff will tend to be mapped back into [0; 1] by subsequent applications of B 1 . k N approximate degree of B (N) k # matrix multiplications 1 86 1:1 fi 10 41 172 3 45 1:1 fi 10 38 180 5 36 3:1 fi 10 37 216 Table 2.1. <p> the Scaling step and "stretch" it so that its eigenvalues now lie over some interval, say [s; 1 + s], where 0 &lt; s ae 1, then the eigenvalues of `(A) near 1=2 are moved further away from 1=2 and B 1 will still map the eigenvalues of `(A) into <ref> [0; 1] </ref>. By "stretching", we mean to apply a linear function that maps 0 and 1 to 1 s and s, respectively, leaving 1=2 fixed. <p> As noted in [15], if ^ a (A) has a large gap in the singular values, then QR factorization with column pivoting should generally perform well at detecting rank deficiency and as a means of computing R ( ^ a (A)). We used the routine xGEQPF in LAPACK <ref> [1] </ref> for this computation. Rather surprisingly, our experiments showed that requiring a gap larger than p u produced a less effective algorithm. 2.4. Decoupling problem. The computations in the Decoupling and Invariant Subspace Accumulation steps are straightforward. <p> Since, in our testing, accuracy in the residuals was comparable for the dense and sparse forms, we present only results for dense matrices. A large suite of test matrices were generated using the LAPACK test generation routines xLATME (nonsymmetric) and xLATMS (symmetric) <ref> [1] </ref> . xLATME allows one to generate matrices of the form A = (U t V ) 1 D (U t V ); where U; V are random orthogonal matrices and D; are diagonal matrices. <p> In Figures 3.3-3.4 we give plots of the maximum residual versus matrix dimension for dense symmetric matrices for both ISDA and SSYEV in LAPACK <ref> [1] </ref>. Figures 3.5 and 3.6 show plots of the departure from orthogonality residuals for both ISDA and SSYEV plotted versus matrix dimension.
Reference: [2] <author> Auslander, L. & A. Tsao, </author> <title> On parallelizable eigensolvers, </title> <journal> Adv. Appl. Math. </journal> <volume> 13 (1992), </volume> <pages> 253-261. </pages>
Reference-contexts: Although this class of matrices is not completely general, it includes the important class of real symmetric matrices. Our algorithm is based on theoretical ideas of Auslander and Tsao <ref> [2] </ref>. They propose an algorithm for approximating invariant subspaces of a matrix through the computation of matrix polynomials with special properties. This, in turn, would allow block triangularization of the matrix into two independent subproblems of smaller size via a suitably chosen orthogonal similarity transformation.
Reference: [3] <author> Bai, Z. & J. Demmel, </author> <title> On a block implementation of Hessenberg multishift QR iteration, </title> <journal> International Journal of High Speed Computing 1 (1989), </journal> <volume> no. 1, </volume> <pages> 97-112, </pages> <note> (also LAPACK Working Note #8). </note>
Reference-contexts: Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm <ref> [3, 32] </ref>, Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces [6, 11, 12, 19, 26, 4].
Reference: [4] <author> Bai, Z. & J. Demmel, </author> <title> Design of parallel nonsymmetric eigenroutine toolbox, Part I, </title> <type> Research report 92-09, </type> <institution> University of Kentucky (Dec. </institution> <year> 1992), </year> <note> (also PRISM Working Note #5). </note>
Reference-contexts: Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces <ref> [6, 11, 12, 19, 26, 4] </ref>. The purpose of this paper is to present preliminary research results on a new algorithm for finding all the eigenvalues and eigenvectors of a real diagonalizable matrix with real eigenvalues. <p> We therefore feel that the Beta function approach promises more robust, scalable performance than the matrix sign approach for the matrices we are considering. However, for the general nonsymmetric eigenvalue problem where the matrices may have complex eigenvalues, the matrix sign approach is quite promising <ref> [6, 11, 12, 19, 26, 4] </ref>. 3. Test cases. Testing of the algorithm described was performed on both nonsymmetric and symmetric matrices.
Reference: [5] <author> Bai, Z., J. Demmel, & A. McKenney, </author> <title> On the conditioning the nonsymmetric eigenproblem: </title> <journal> theory and software, </journal> <note> LAPACK Working Note 13, </note> <institution> Courant Institute (1989). </institution>
Reference-contexts: The performance of ISDA for both dense and upper Hessenberg matrices was compared to the LAPACK implementations (Release 1.1) of the QR algorithm for dense (xGEEV) and upper Hessenberg matrices (xHSEQR), respectively. Since the eigenvalues are somewhat insensitive to perturbation under these conditions <ref> [5] </ref>, it was reasonable A PARALLELIZABLE EIGENSOLVER 9 to rely on xGEEV or xHSEQR to filter out cases with complex eigenvalues. Our algorithm was only applied to those matrices where the eigenvalues were "close" to real according to xGEEV or xHSEQR.
Reference: [6] <author> Beavers, A. N., Jr. & E. D. Denman, </author> <title> A computational method for eigenvalues and eigenvectors of a matrix with real eigenvalues, </title> <journal> Num. Math. </journal> <volume> 21 (1973), </volume> <pages> 389-96. </pages>
Reference-contexts: Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces <ref> [6, 11, 12, 19, 26, 4] </ref>. The purpose of this paper is to present preliminary research results on a new algorithm for finding all the eigenvalues and eigenvectors of a real diagonalizable matrix with real eigenvalues. <p> The preponderance of "fast" parallel primitives, such as matrix-matrix multiplication and solving systems of equations, coupled with the divide and conquer nature of the block triangularization, yields a highly parallelizable algorithm, in principle. A similar divide and conquer algorithm using rational functions can be found in <ref> [6] </ref>. fl This paper is PRISM Working Note #20, available via anonymous ftp to ftp.super.org in the directory pub/prism. y Center for Computing Sciences, 17100 Science Drive, Bowie, MD 20715 z Current address: Computer Science Dept., Univ. of Wisconsin-Madison, 1210 W. <p> We therefore feel that the Beta function approach promises more robust, scalable performance than the matrix sign approach for the matrices we are considering. However, for the general nonsymmetric eigenvalue problem where the matrices may have complex eigenvalues, the matrix sign approach is quite promising <ref> [6, 11, 12, 19, 26, 4] </ref>. 3. Test cases. Testing of the algorithm described was performed on both nonsymmetric and symmetric matrices.
Reference: [7] <author> Berry, M. & A. Sameh, </author> <title> Parallel algorithms for the singular value and dense symmetric eigenvalue problem, </title> <journal> CSRD (1988), </journal> <volume> no. </volume> <pages> 761. </pages>
Reference-contexts: For the symmetric eigenvalue problem, promising algorithms that have been investigated include bisection/multisection, followed by inverse iteration [21, 22, 20], Cuppen's divide and conquer algorithm [9, 14, 28], Jacobi methods <ref> [29, 7, 10, 30] </ref>, and homotopy methods [25]. Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces [6, 11, 12, 19, 26, 4].
Reference: [8] <author> Bischof, C., </author> <title> A parallel QR factorization with controlled local pivoting, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 12 (1991), no. 1, </volume> <pages> 36-57. </pages>
Reference-contexts: For larger values of N , this percentage will, of course, increase, but at the expense of greater total work. Additionally, even though the QR with column pivoting in Invariant Subspace Computation is not included as being matrix multiplication-based, Bischof <ref> [8] </ref> has shown it can be run in parallel with controlled local pivoting. Thus, subproblems of sufficient size should run efficiently on a multiprocessor due to the large fraction of matrix multiplications and the existence of a parallel QR algorithm.
Reference: [9] <author> Cuppen, J. J. M., </author> <title> A divide and conquer method for the symmetric tridiagonal eigenproblem, </title> <journal> Numer. Math. </journal> <volume> 36 (1981), </volume> <pages> 177-95. </pages>
Reference-contexts: For the symmetric eigenvalue problem, promising algorithms that have been investigated include bisection/multisection, followed by inverse iteration [21, 22, 20], Cuppen's divide and conquer algorithm <ref> [9, 14, 28] </ref>, Jacobi methods [29, 7, 10, 30], and homotopy methods [25].
Reference: [10] <author> Demmel, J. & K. Veselic, </author> <title> Jacobi's method is more accurate than QR, </title> <institution> Department of Computer Science, </institution> <address> New York University 468 (1989). </address>
Reference-contexts: For the symmetric eigenvalue problem, promising algorithms that have been investigated include bisection/multisection, followed by inverse iteration [21, 22, 20], Cuppen's divide and conquer algorithm [9, 14, 28], Jacobi methods <ref> [29, 7, 10, 30] </ref>, and homotopy methods [25]. Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces [6, 11, 12, 19, 26, 4].
Reference: [11] <author> Denman, E. D. & A. N. Beavers, Jr., </author> <title> The matrix sign function and computations in systems, </title> <journal> Appl. Math. Comp. </journal> <volume> 2 (1976), </volume> <pages> 63-94. </pages>
Reference-contexts: Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces <ref> [6, 11, 12, 19, 26, 4] </ref>. The purpose of this paper is to present preliminary research results on a new algorithm for finding all the eigenvalues and eigenvectors of a real diagonalizable matrix with real eigenvalues. <p> We therefore feel that the Beta function approach promises more robust, scalable performance than the matrix sign approach for the matrices we are considering. However, for the general nonsymmetric eigenvalue problem where the matrices may have complex eigenvalues, the matrix sign approach is quite promising <ref> [6, 11, 12, 19, 26, 4] </ref>. 3. Test cases. Testing of the algorithm described was performed on both nonsymmetric and symmetric matrices.
Reference: [12] <author> Denman, E. D. & J. Leyva-Ramos, </author> <title> Spectral decomposition of a matrix using the generalized sign matrix, </title> <journal> Appl. Math. Comp. </journal> <volume> 8 (1981), </volume> <pages> 237-50. </pages>
Reference-contexts: Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces <ref> [6, 11, 12, 19, 26, 4] </ref>. The purpose of this paper is to present preliminary research results on a new algorithm for finding all the eigenvalues and eigenvectors of a real diagonalizable matrix with real eigenvalues. <p> We therefore feel that the Beta function approach promises more robust, scalable performance than the matrix sign approach for the matrices we are considering. However, for the general nonsymmetric eigenvalue problem where the matrices may have complex eigenvalues, the matrix sign approach is quite promising <ref> [6, 11, 12, 19, 26, 4] </ref>. 3. Test cases. Testing of the algorithm described was performed on both nonsymmetric and symmetric matrices.
Reference: [13] <author> Dongarra, J., C. B. Moler, J. R. Bunch, & G. W. Stewart, </author> <title> LINPACK User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: In general, if K is the smallest positive integer for which (2.3) is satisfied, we verify that the resulting matrix, B K , does, indeed, have a large gap in its singular values. This was done by computing its QR factorization with column pivoting <ref> [13] </ref>, given by B K = QR; (2.5) where is a permutation matrix, Q is an orthogonal matrix, and R = [ R ij ] is an upper triangular matrix whose diagonal elements are arranged in order of decreasing absolute value.
Reference: [14] <author> Dongarra, J. & D. Sorensen, </author> <title> A fully parallel algorithm for the symmetric eigenvalue problem, </title> <journal> SIAM J. Sci. Statist. Comput. </journal> <note> 8 (1987), s139-54. </note>
Reference-contexts: For the symmetric eigenvalue problem, promising algorithms that have been investigated include bisection/multisection, followed by inverse iteration [21, 22, 20], Cuppen's divide and conquer algorithm <ref> [9, 14, 28] </ref>, Jacobi methods [29, 7, 10, 30], and homotopy methods [25].
Reference: [15] <author> Golub, G., V. Klema, & G. W. Stewart, </author> <title> Rank degeneracy and least squares problems, </title> <type> Technical Report TR-456, </type> <institution> University of Maryland (1976). </institution>
Reference-contexts: We declare the matrix B K to have rank r if jR r+1;r+1 j ujR rr j: (2.6) We then let ^ a (A) = B K and perform the orthogonal change of basis given by Q. As noted in <ref> [15] </ref>, if ^ a (A) has a large gap in the singular values, then QR factorization with column pivoting should generally perform well at detecting rank deficiency and as a means of computing R ( ^ a (A)). We used the routine xGEQPF in LAPACK [1] for this computation.
Reference: [16] <author> Golub, G. & C. F. Van Loan, </author> <title> Matrix Computations, 2nd ed., </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: Let ! and be a lower and upper bound on (A), respectively. In our implementation, we use the bounds provided by Gershgorin disks <ref> [16] </ref> as ! and . Then we let ` be the linear map that maps (A) into as large a subinterval of [0; 1] as possible so that `() = 1=2. <p> The analysis below is for the nonsymmetric problem; the symmetric case is analogous. Also, a straightforward unblocked implementation of the ISDA is analyzed in which Q in the Invariant Subspace Computation is explicitly formed at each stage. We follow Golub and Van Loan <ref> [16] </ref> in presenting our operations counts. An operation is defined as one floating point computation, e.g., squaring a matrix of order n takes 2n 3 operations. We let m represent the size of the subproblem, ^ A, to be divided, and n is the size of the initial problem, A. <p> In particular, ISDA requires 14 HUSS-LEDERMAN ET AL. roughly four to five times as many operations as the nonsymmetric QR algorithm, assuming that the nonsymmetric QR algorithm performs roughly 25n 3 operations <ref> [16] </ref>. But even sequentially, we see why dense matrix multiplication is such a desirable primitive. For matrices with uniformly distributed eigenvalues, ISDA is an average of 1:9 times slower than the QR algorithm on the RS/6000 and is about 2:2 times slower than the QR algorithm on the Cray-2.
Reference: [17] <author> Hamming, R. W., </author> <title> Digital Filters, 2nd ed., </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983. </year>
Reference-contexts: Ideally, one would like the matrix ^ a (A) to map approximately half the eigenvalues of A near 0. Our algorithm constructs ^ a by first performing a Scaling step followed by an Eigenvalue Smoothing step. We borrow the term "smoothing" from digital filter theory <ref> [17] </ref>. The Scaling and Eigenvalue Smoothing steps proceed as follows: Scaling: Compute bounds on the spectrum (A) of A and use these bounds to compute ff and fi such that for `(x) = ffx + fi, (`(A)) [0; 1], with the mean eigenvalue of A being mapped to 1=2. <p> Eigenvalue Smoothing. 2.2.1. Iteration scheme. We now consider construction of the polynomials p i (i = 1; 2; 3; : : :). The suitably normalized incomplete Beta functions <ref> [17] </ref> (Section 7.2) given by B j (x) = 0 Z 1 t j (1 t) j dt j X j k j + k 4 HUSS-LEDERMAN ET AL. x ! 0 2 x ! 0 2 . . . . . . . . . . . . . .
Reference: [18] <author> Hoffman, K. & R. Kunze, </author> <title> Linear Algebra, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1971. </year>
Reference-contexts: We now describe the method proposed by Auslander and Tsao for computing invariant subspaces of A. Assume that A has eigenvalues 1 ; : : : ; n . Consider a matrix polynomial a (A), where a 2 R [x]. It is well known <ref> [18] </ref> that a (A) has eigenvalues a ( 1 ); : : : ; a ( n ). Suppose that R (a (A)) is a nonempty proper subspace of R n of dimension r, i.e., a maps exactly n r eigenvalues of A to 0, counting multiplicities.
Reference: [19] <author> Howland, J. L., </author> <title> The sign matrix and the separation of matrix eigenvalues, </title> <journal> Lin. Alg. Appl. </journal> <volume> 49 (1983), </volume> <pages> 221-32. </pages>
Reference-contexts: Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces <ref> [6, 11, 12, 19, 26, 4] </ref>. The purpose of this paper is to present preliminary research results on a new algorithm for finding all the eigenvalues and eigenvectors of a real diagonalizable matrix with real eigenvalues. <p> We therefore feel that the Beta function approach promises more robust, scalable performance than the matrix sign approach for the matrices we are considering. However, for the general nonsymmetric eigenvalue problem where the matrices may have complex eigenvalues, the matrix sign approach is quite promising <ref> [6, 11, 12, 19, 26, 4] </ref>. 3. Test cases. Testing of the algorithm described was performed on both nonsymmetric and symmetric matrices.
Reference: [20] <author> Huo, Y. & R. Schreiber, </author> <title> Efficient, massively parallel eigenvalue computation, </title> <booktitle> The International J. of Supercomputer Applications 7 (Winter 1993), </booktitle> <volume> no. 4, </volume> <pages> 292-303. </pages>
Reference-contexts: Indeed, it appears likely that algorithms such as the QR algorithm, which has been so effective on serial machines, must be supplanted by algorithms that map more readily onto parallel architectures. For the symmetric eigenvalue problem, promising algorithms that have been investigated include bisection/multisection, followed by inverse iteration <ref> [21, 22, 20] </ref>, Cuppen's divide and conquer algorithm [9, 14, 28], Jacobi methods [29, 7, 10, 30], and homotopy methods [25].
Reference: [21] <author> Ipsen, I. & E. Jessup, </author> <title> Solving the symmetric tridiagonal eigenvalue problem on the hypercube, </title> <type> Tech. Rep. </type> <institution> RR-548, Yale University (1987). </institution>
Reference-contexts: Indeed, it appears likely that algorithms such as the QR algorithm, which has been so effective on serial machines, must be supplanted by algorithms that map more readily onto parallel architectures. For the symmetric eigenvalue problem, promising algorithms that have been investigated include bisection/multisection, followed by inverse iteration <ref> [21, 22, 20] </ref>, Cuppen's divide and conquer algorithm [9, 14, 28], Jacobi methods [29, 7, 10, 30], and homotopy methods [25].
Reference: [22] <author> Ipsen, I. & E. Jessup, </author> <title> Improving the accuracy of inverse iteration (1991) (to appear). </title>
Reference-contexts: Indeed, it appears likely that algorithms such as the QR algorithm, which has been so effective on serial machines, must be supplanted by algorithms that map more readily onto parallel architectures. For the symmetric eigenvalue problem, promising algorithms that have been investigated include bisection/multisection, followed by inverse iteration <ref> [21, 22, 20] </ref>, Cuppen's divide and conquer algorithm [9, 14, 28], Jacobi methods [29, 7, 10, 30], and homotopy methods [25].
Reference: [23] <author> Kenney, C. & A. J. Laub, </author> <title> Rational iterative methods for the matrix sign function, </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 12 (1990), no. 2, </volume> <pages> 273-91. </pages>
Reference-contexts: We note that B 1 (x) = (n (2x 1) + 1)=2, where n is the Newton-Schulz iteration given by n (x) = (3x x 3 )=2. A discussion of the behavior of the Newton-Schulz iteration can be found in <ref> [23] </ref>. In particular, the discussion in [23] illustrates the difficulties of extending our methodology to the complex case. <p> We note that B 1 (x) = (n (2x 1) + 1)=2, where n is the Newton-Schulz iteration given by n (x) = (3x x 3 )=2. A discussion of the behavior of the Newton-Schulz iteration can be found in <ref> [23] </ref>. In particular, the discussion in [23] illustrates the difficulties of extending our methodology to the complex case.
Reference: [24] <author> Li, T. Y., Z. Zeng, & L. Cong, </author> <title> Solving eigenvalue problems of real nonsymmetric matrices with real homotopies, </title> <institution> Dept. of Mathematics, Michigan State University (1990), </institution> <type> preprint. </type>
Reference-contexts: Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods <ref> [24] </ref>, and the matrix sign function approach to computing invariant subspaces [6, 11, 12, 19, 26, 4]. The purpose of this paper is to present preliminary research results on a new algorithm for finding all the eigenvalues and eigenvectors of a real diagonalizable matrix with real eigenvalues.
Reference: [25] <author> Li, T.-Y., H. Zhang, & X.-H. Sun, </author> <title> Parallel homotopy algorithm for symmetric tridiagonal eigenvalue problems, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 12 (1991), no. 3, </volume> <pages> 469-87. </pages>
Reference-contexts: For the symmetric eigenvalue problem, promising algorithms that have been investigated include bisection/multisection, followed by inverse iteration [21, 22, 20], Cuppen's divide and conquer algorithm [9, 14, 28], Jacobi methods [29, 7, 10, 30], and homotopy methods <ref> [25] </ref>. Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces [6, 11, 12, 19, 26, 4].
Reference: [26] <author> Lin, C.-C. & E. Zmijewski, </author> <title> A parallel algorithm for computing the eigenvalues of an unsymmetric matrix on a SIMD mesh of processors, </title> <type> TRCS 91-15, </type> <institution> Dept. of Computer Science, Univ. of Calif., Santa Barbara (1991). </institution>
Reference-contexts: Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces <ref> [6, 11, 12, 19, 26, 4] </ref>. The purpose of this paper is to present preliminary research results on a new algorithm for finding all the eigenvalues and eigenvectors of a real diagonalizable matrix with real eigenvalues. <p> We therefore feel that the Beta function approach promises more robust, scalable performance than the matrix sign approach for the matrices we are considering. However, for the general nonsymmetric eigenvalue problem where the matrices may have complex eigenvalues, the matrix sign approach is quite promising <ref> [6, 11, 12, 19, 26, 4] </ref>. 3. Test cases. Testing of the algorithm described was performed on both nonsymmetric and symmetric matrices.
Reference: [27] <author> Pan, V. & R. Schreiber, </author> <title> An improved Newton iteration for the generalized inverse of a matrix, with applications, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 12 (1991), no. 5, </volume> <pages> 1109-1129. </pages>
Reference-contexts: B 1 takes on the value 1=2 three times: at 1=2, ae, and 1 ae, where ae = (1 + p propose the following scheme, which is a slight modification of a technique suggested by Pan and Schreiber <ref> [27] </ref>.
Reference: [28] <author> Rutter, J., </author> <title> A serial implementation of Cuppen's Divide and Conquer Algorithm for the symmetric eigenvalue problem, </title> <type> UCB/CSD 94/799, </type> <institution> University of California at Berkeley (1994), Berkeley, California. </institution>
Reference-contexts: For the symmetric eigenvalue problem, promising algorithms that have been investigated include bisection/multisection, followed by inverse iteration [21, 22, 20], Cuppen's divide and conquer algorithm <ref> [9, 14, 28] </ref>, Jacobi methods [29, 7, 10, 30], and homotopy methods [25].
Reference: [29] <author> Schreiber, R., </author> <title> Solving eigenvalue and singular value problems on an undersized systolic array, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 7 (1986), no. 2, </volume> <pages> 441-451. </pages>
Reference-contexts: For the symmetric eigenvalue problem, promising algorithms that have been investigated include bisection/multisection, followed by inverse iteration [21, 22, 20], Cuppen's divide and conquer algorithm [9, 14, 28], Jacobi methods <ref> [29, 7, 10, 30] </ref>, and homotopy methods [25]. Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces [6, 11, 12, 19, 26, 4].
Reference: [30] <author> Shroff, G. & R. Schreiber, </author> <title> On the convergence of the cyclic Jacobi method for parallel block orderings, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 10 (1989), no. 3, </volume> <pages> 326-346. </pages>
Reference-contexts: For the symmetric eigenvalue problem, promising algorithms that have been investigated include bisection/multisection, followed by inverse iteration [21, 22, 20], Cuppen's divide and conquer algorithm [9, 14, 28], Jacobi methods <ref> [29, 7, 10, 30] </ref>, and homotopy methods [25]. Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces [6, 11, 12, 19, 26, 4].
Reference: [31] <author> Stewart, G. W., </author> <title> A Jacobi-like algorithm for computing the Schur decomposition of a nonhermitian matrix, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 6 (1985), no. 4, </volume> <pages> 853-864. </pages>
Reference-contexts: Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm [3, 32], Jacobi-like methods <ref> [31] </ref>, homotopy methods [24], and the matrix sign function approach to computing invariant subspaces [6, 11, 12, 19, 26, 4]. The purpose of this paper is to present preliminary research results on a new algorithm for finding all the eigenvalues and eigenvectors of a real diagonalizable matrix with real eigenvalues.
Reference: [32] <author> Van de Geijn, R. A., </author> <title> Deferred shifting schemes for parallel QR methods, </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 14 (1993), no. 1, </volume> <pages> 180-194. </pages>
Reference-contexts: Parallelizable algorithms for dense nonsymmetric matrices that have been investigated include the QR algorithm <ref> [3, 32] </ref>, Jacobi-like methods [31], homotopy methods [24], and the matrix sign function approach to computing invariant subspaces [6, 11, 12, 19, 26, 4].
References-found: 32

