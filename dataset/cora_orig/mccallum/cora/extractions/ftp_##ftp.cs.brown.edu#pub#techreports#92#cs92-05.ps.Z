URL: ftp://ftp.cs.brown.edu/pub/techreports/92/cs92-05.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-92-05.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Aggarwal, B. Alpern, A. K. Chandra, and M. Snir, </author> <title> "A Model for Hierarchical Memory," </title> <institution> IBM Watson Research Center, </institution> <type> Technical Report RC 15118, </type> <month> October </month> <year> 1989, </year> <booktitle> also appears in Proceedings of 19th Annual ACM Symposium on Theory of Computing, </booktitle> <address> New York (May 1987), </address> <pages> 305-314. </pages>
Reference-contexts: 1 Introduction Large-scale computer systems contain many levels of memory|ranging from very small but very fast registers, to successively larger but slower memories, such as multiple levels of cache, primary memory, magnetic disks, and archival storage. An elegant hierarchical memory model was introduced by Aggarwal, Alpern, Chandra, and Snir <ref> [1] </ref> and further developed by Aggarwal, Chandra, and Snir [2] to take into account block transfer. All computation takes place in the central processing unit (CPU), one instruction per unit time. <p> Conclusions and open problems are discussed in Section 11. 2 Parallel Hierarchical Memory Models A hierarchical memory model is a uniform model consisting of memory whose locations take different amounts of time to access. The basic unit of transfer in the hierarchical memory model HMM <ref> [1] </ref> is the record; access to location x takes time f (x). <p> The lower bounds for P-HMM and P-BT are based upon the approach used in <ref> [1] </ref> and [2]. Theorem 1 In the P-HMM model, the time for sorting and the FFT is fi N log N log log N !! fi N ff+1 N log N if f (x) = x ff , ff &gt; 0. <p> We prove that the randomized distribution sort algorithm we develop is simultaneously optimal for all well-behaved access cost functions, as defined in Section 2. The sorting algorithm is a modified version of the optimal distribution sort algorithm for the one-hierarchy HMM <ref> [1] </ref>. For simplicity, we assume that records have distinct key values; 6 5 SORTING IN P-HMM this assumption is satisfied, for example, if we append to the key field of each record the original global memory location of the record. <p> Let us define the "sequential time" of A to be the sum of its time costs for each of the P hierarchies; the sequential time of A is at most P times its running time. Following the approach in <ref> [1] </ref>, we can imagine superimposing onto the P-HMM-type hierarchical memory a sequence of two-level memories. For each M in the range P M &lt; N, we superimpose on the P-HMM an internal memory of size M and one infinite-sized disk. <p> The second term follows from the N log N lower bound for sorting in the comparison model of computation. 2 The next theorem shows that the sorting algorithm given earlier in Section 5 is uniformly optimal (in the language of <ref> [1] </ref>) in that it is optimal for all well-behaved access costs f (x), as defined in Section 2. <p> The following theorem, when combined with Theorem 4, also gives an alternate proof of Theorem 3. Theorem 6 The algorithm given at the beginning of Section 5 is optimal for all well-behaved access costs f (x). Proof : We use a parallelized version of the approach of <ref> [1] </ref>, combined with the lower bound strategy of Theorems 4 and 5.
Reference: [2] <author> A. Aggarwal, A. Chandra, and M. Snir, </author> <title> "Hierarchical Memory with Block Transfer," </title> <booktitle> Proceedings of 28th Annual IEEE Symposium on Foundations of Computer Science (October 1987), </booktitle> <pages> 204-216. </pages>
Reference-contexts: An elegant hierarchical memory model was introduced by Aggarwal, Alpern, Chandra, and Snir [1] and further developed by Aggarwal, Chandra, and Snir <ref> [2] </ref> to take into account block transfer. All computation takes place in the central processing unit (CPU), one instruction per unit time. Access to memory takes a varying amount of time, depending on how low in the memory hierarchy the memory access is. <p> The basic unit of transfer in the hierarchical memory model HMM [1] is the record; access to location x takes time f (x). The BT model <ref> [2] </ref> represents a notion of block transfer applied to HMM; in the BT model, access to the t + 1 records at locations x t, x t + 1, . . . , x takes time f (x) + t. <p> The lower bounds for P-HMM and P-BT are based upon the approach used in [1] and <ref> [2] </ref>. Theorem 1 In the P-HMM model, the time for sorting and the FFT is fi N log N log log N !! fi N ff+1 N log N if f (x) = x ff , ff &gt; 0. <p> It follows that the sorting algorithm is optimal. 2 6 Sorting in P-BT In this section we show the matching upper and lower bounds quoted in Theorem 2 in Section 4 for sorting in the P-BT model. The following useful lemma is a parallel version of a theorem in <ref> [2] </ref>. <p> individual hierarchy with an amortized cost per operation of O log fl n O log log P if f (x) = x ff , 0 &lt; ff &lt; 1; n O n ff1 if f (x) = x ff , ff &gt; 1, where n is the number of operations <ref> [2] </ref>. The cost of merging two lists of P elements in base memory is log P . 2 6.1 Access Cost f (x) = x ff , where 1 Let us first consider the access cost function f (x) = x ff , where 1 2 ff &lt; 1. <p> We can access the levels within our hierarchies optimally if we read and write (x=P ) ff elements at a time when we access global location x. Our optimal P-BT sorting algorithm is a modified version of the one-hierarchy algorithm presented in <ref> [2] </ref>. The key component of the algorithm is our use of the partitioning technique of [12] to spread the records in each bucket evenly among the P hierarchies. <p> The time for touching each G i and accumulating the N= log N elements of set A in Step 3 is O ((N=P ) log log (N=P )) <ref> [2] </ref>. Using Lemma 1, we can show that the time for the two-way merge sort used in Step 3 to sort n elements is O ((n=P ) log n (log log n + log P )). <p> The data movement to reposition the buckets in Step 8 can be done by the same method used by the one-hierarchy algorithm <ref> [2] </ref>, that is, by computing the generalized matrix transposition for each hierarchy independently; the time needed is thus O ((N=P )(log log (N=P )) 4 ) with high probability. <p> We sort by a simple application of divide-and-conquer merge sort. The upper bound of Theorem 2 follows by using the algorithm of Lemma 1 for merging two sorted lists. The lower bound in Theorem 2 for the case ff = 1 follows by a modification of the argument in <ref> [2] </ref> for P = 1. <p> The lower bounds for sorting apply also to FFT. The FFT algorithm that meets these bounds is based on the FFT algorithm of Section 7. The shu*ing is done by touching the records in each group, using the touching algorithm of <ref> [2] </ref> applied to the hierarchies independently. <p> Two matrices can be added by touching the corresponding elements of the matrices simultaneously, using the touching algorithm of <ref> [2] </ref> applied to the hierarchies independently. Once two elements are in base memory level together, they can be added. <p> The upper bounds for the remaining cases of Theorem 2 follow by applying the other cases of Lemma 3. The lower bound for the access cost function f (x) = x ff , where ff = 3=2, for the BT model <ref> [2] </ref> can be modified for the P-BT model.
Reference: [3] <author> A. Aggarwal and J. S. Vitter, </author> <title> "The Input/Output Complexity of Sorting and Related Problems," </title> <journal> Communications of the ACM (September 1988), </journal> <pages> 1116-1127, </pages> <booktitle> also appears in Proceedings of 14th Annual International Colloquium on Automata, Languages, and Programming (ICALP), Lecture Notes in Computer Science 267, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1987. </year>
Reference-contexts: Following the approach in [1], we can imagine superimposing onto the P-HMM-type hierarchical memory a sequence of two-level memories. For each M in the range P M &lt; N, we superimpose on the P-HMM an internal memory of size M and one infinite-sized disk. By <ref> [3] </ref>, the I/O complexity of sorting N records with one disk, no blocking, and an internal memory of size M is T M (N ) = N log N M : (1) The " M " term permits M records to reside initially in the internal memory.
Reference: [4] <author> B. Alpern, L. Carter, and E. Feig, </author> <title> "Uniform Memory Hierarchies," </title> <booktitle> Proceedings of 31st Annual IEEE Symposium on Foundations of Computer Science (October 1990). </booktitle> <pages> 19 </pages>
Reference-contexts: Subsequently, Nodine and Vitter developed optimal sorting algorithms for P-HMM and P-BT based on distribution sort that are deterministic [7,8]. Another interesting type of hierarchical memory is introduced in <ref> [4] </ref>. Parallel hierarchies of this type are studied in [6]. Acknowledgments. We thank Mark Nodine for several helpful comments.
Reference: [5] <author> F. Luccio and L. Pagli, </author> <title> "A Model of Sequential Computation Based on a Pipelined Access to Memory," </title> <booktitle> Proceedings of the 27th Annual Allerton Conference on Communication, Control, and Computing (September 1989). </booktitle>
Reference-contexts: Typical access cost functions are f (x) = log x and f (x) = x ff , for some ff &gt; 0. A model similar to the BT model that allows pipelined access to memory in O (log n) time was developed independently by Luccio and Pagli <ref> [5] </ref>. We can think of a memory hierarchy as being organized into discrete levels, as shown in Figure 1 for HMM; for each k 1, level k contains the 2 k1 locations at addresses 2 k1 , 2 k1 + 1, . . . , 2 k 1.
Reference: [6] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Large-Scale Sorting in Uniform Memory Hierarchies," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> special issue. </note>
Reference-contexts: Subsequently, Nodine and Vitter developed optimal sorting algorithms for P-HMM and P-BT based on distribution sort that are deterministic [7,8]. Another interesting type of hierarchical memory is introduced in [4]. Parallel hierarchies of this type are studied in <ref> [6] </ref>. Acknowledgments. We thank Mark Nodine for several helpful comments.
Reference: [7] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Optimal Deterministic Sorting on Parallel Memory Hierarchies," </title> <institution> Department of Computer Science, Brown University, </institution> <note> Technical Report , August 1992. </note>
Reference: [8] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Optimal Deterministic Sorting on Parallel Disks," </title> <institution> Department of Computer Science, Brown University, </institution> <note> Technical Report , August 1992. </note>
Reference: [9] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Large-Scale Sorting in Parallel Memories," </title> <booktitle> Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures (July 1991). </booktitle>
Reference-contexts: Addendum. Recently an alternative two-level sorting algorithm that is both optimal and deterministic was developed by Nodine and Vitter <ref> [9] </ref>. The algorithm is based on merge sort and thus does not seem to provide optimal P-HMM and P-BT algorithms when applied to hierarchical memory. Subsequently, Nodine and Vitter developed optimal sorting algorithms for P-HMM and P-BT based on distribution sort that are deterministic [7,8].
Reference: [10] <author> J. H. Reif and L. G. Valiant, </author> <title> "A Logarithmic Time Sort on Linear Size Networks," </title> <journal> Journal of the ACM 34 (January 1987), </journal> <pages> 60-76, </pages> <booktitle> also appears in Proceedings of the 15th Annual ACM Symposium on Theory of Computing, </booktitle> <address> Boston (April 1983), </address> <pages> 10-16. </pages>
Reference-contexts: We assume that the P base memory level locations are interconnected via a network such as a hypercube or cube-connected cycles so that the P records in the base memory level can be sorted in O (log P ) time (perhaps via a randomized algorithm <ref> [10] </ref>) and so that two p p P=2 matrices stored in the base memory level can be multiplied in O ( p P ) time. We denote by P-HMM and P-BT the P -hierarchy variants of the hierarchical memory models HMM and BT, as described above.
Reference: [11] <author> J. Savage and J. S. Vitter, </author> <title> "Parallelism in Space-Time Tradeoffs," </title> <booktitle> in Advances in Computing Research, </booktitle> <volume> Volume 4 , F. </volume> <editor> P. Preparata, ed., </editor> <publisher> JAI Press, </publisher> <year> 1987, </year> <pages> 117-146, </pages> <note> also appears in Proceedings of the International Workshop on Parallel Computing and VLSI, Amalfi, Italy (May 1984), </note> <editor> P. Bertolazzi and F. Luccio, ed., </editor> <publisher> Elsevier Science Press, </publisher> <year> 1985, </year> <pages> 49-58. </pages>
Reference-contexts: This can be proved using the same approach as in Theorem 6. By <ref> [11] </ref>, the I/O complexity of multipling two k fi k matrices with one disk, no blocking, and an internal memory of size M is p M : (6) Let T M;P (N) be the average number of I/O steps done by the standard matrix multiplication algorithm with respect to an internal
Reference: [12] <author> J. S. Vitter and E. A. Shriver, </author> <title> "Algorithms for Parallel Memory I: Two-Level Memories," </title> <institution> Department of Computer Science, Brown University, </institution> <type> Technical Report CS-90-21, </type> <month> September </month> <year> 1990. </year>
Reference-contexts: For each model, we develop matching upper and lower bounds for the problems of sorting, FFT, and matrix multiplication, which are defined in Section 3. The algorithms that realize the optimal bounds for sorting are applications of the optimal disk sorting algorithm developed in the companion paper <ref> [12] </ref> for a two-level memory model with parallel block transfer. We apply the partitioning technique of [12] to the one-hierarchy sorting algorithms of [1,2]. Intuitively, the hierarchical algorithms are optimal because the internal processing in the corresponding two-level algorithms is efficient. <p> The algorithms that realize the optimal bounds for sorting are applications of the optimal disk sorting algorithm developed in the companion paper <ref> [12] </ref> for a two-level memory model with parallel block transfer. We apply the partitioning technique of [12] to the one-hierarchy sorting algorithms of [1,2]. Intuitively, the hierarchical algorithms are optimal because the internal processing in the corresponding two-level algorithms is efficient. The main results are given in Section 4 and are proved in Sections 5-10. <p> We shall refer to the P locations, one per hierarchy, at the same relative position in each of the P hierarchies as a track, by analogy with the two-level disk model <ref> [12] </ref>. The ith track, for i 1, consists of location i from each of the P hierarchies. In this terminology, the base memory level is the track at location 1. The global memory locations (which refer collectively to the P hierarchies combined) are numbered track by track. <p> . . . , P ; the global memory locations in track 2 are numbered P + 1, P + 2, . . . , 2P ; and so on. 3 Problem Definitions The following definitions of sorting, FFT, and standard matrix multiplication are essentially those of the companion paper <ref> [12] </ref>. 3 either of type HMM or of type BT. The P CPUs can communicate among one another via the network; we assume, for example, that the P records stored in their accumulators can be sorted in O (log P ) time. <p> memory locations. 4 Main Results The fundamental problem that arises in trying to take full advantage of parallel transfer in these models is how to distribute records among the P memory hierarchies so that each hierarchy is kept "busy." We shall show later how the randomized distribution sort algorithm of <ref> [12] </ref> for a two-level memory model with parallel block transfer can be used as a basic building block to get optimal sorting algorithms for the hierarchical models. The lower bounds for P-HMM and P-BT are based upon the approach used in [1] and [2]. <p> Our techniques can be extended to get optimal algorithms for other problems of [1,2,12], such as searching, generating permutations, and permutation networks. Our optimal P-HMM and P-BT algorithms for sorting and FFT are applications of the optimal algorithms of <ref> [12] </ref> for the two-level model with parallel block transfer, applied to the HMM and BT algorithms given in [1,2]. <p> Each bucket is then sorted recursively. The final sorted order is the concatenation of the S sorted buckets. The key component of our P-HMM algorithm is the partitioning technique of <ref> [12] </ref>, which we use to spread the records in each bucket evenly among the P memory hierarchies so that the next level of recursion can proceed optimally. The partitioning technique is actually two techniques|Phase 1 and Phase 2|each with its own range of applicability. <p> In this case, we use the Phase 2 partitioning technique, motivated by routing on the FFT. Both Phase 1 and Phase 2 work with overwhelming probability in their respective ranges of applicability. First we develop some useful notation akin to that of <ref> [12] </ref>, but simplified for our purpose: Hierarchy striping is a programming technique in which the P hierarchies are coordinated so that at any given time the memory locations accessed by the P hierarchies form a track. <p> The pointer last write j;k points to the last location in the kth hierarchy written to by the jth bucket. The pointer next write k points to the next unwritten location in the kth hierarchy. The final carryover we use from <ref> [12] </ref> is a simplified notion of diagonal, for use in Phase 2, when N &lt; P 3=2 = ln P . For simplicity of exposition, let us assume that N and P are powers of 2. <p> The value of x in Step 3, which determines the number of partitioning elements, is chosen so that the partitioning analysis from the companion paper <ref> [12] </ref> can be modified to show that the records with high probability are distributed evenly among the buckets. 5.1 Logarithmic Access Cost Let us first consider the case f (x) = log x of Theorem 1. <p> The method of choosing the partitioning elements guarantees that the size N j of the jth bucket is at most 2N=(S 1), for each 1 j S; the proof is along the lines of Lemmas 3 and 4 in the companion paper <ref> [12] </ref>. The time needed to partition the file in Steps 5, 6, and 7 is O ((N=P ) log (N=P ) + (N=P ) log P ) = O ((N=P ) log N ). The analysis in the companion paper [12] can be modified to show that with high probability the <p> lines of Lemmas 3 and 4 in the companion paper <ref> [12] </ref>. The time needed to partition the file in Steps 5, 6, and 7 is O ((N=P ) log (N=P ) + (N=P ) log P ) = O ((N=P ) log N ). The analysis in the companion paper [12] can be modified to show that with high probability the records in each bucket are distributed evenly over the P hierarchies, so that the time for sorting the buckets recursively in Step 8 is with high probability X T (N j ) + O N log P ; where P <p> This yields as desired the time bound T (N) = O N log N log log N !! with high probability. The probability bounds quoted in Theorem 3 follow from those in <ref> [12] </ref>. 2 The following lower bound matches the the algorithm's running time in Theorem 3, and thus the algorithm is optimal. <p> Our optimal P-BT sorting algorithm is a modified version of the one-hierarchy algorithm presented in [2]. The key component of the algorithm is our use of the partitioning technique of <ref> [12] </ref> to spread the records in each bucket evenly among the P hierarchies. For brevity we present only the portion of the P-BT sorting algorithm whose description differs from the P-HMM sorting algorithm of Section 5. 2. <p> The analysis in the companion paper <ref> [12] </ref> can be modified to show that with high probability the records in each bucket are distributed evenly over the P hierarchies, so that the time for sorting the buckets recursively in Step 8 is with high probability X T (N j ) + O N log log N ; where <p> This yields as desired the time bound T (N ) = O N log N with high probability. The probability bounds follow from those derived in <ref> [12] </ref>. 2 A lower bound of ((N=P ) log N) time for sorting with f (x) = x ff , for 1 2 ff &lt; 1, follows from the well-known lower bound for sorting in a RAM under the comparison model of computation. 6.2 Other Access Cost Functions Since the above <p> We can perform the FFT when N P 2 using the following well-known technique that mimics somewhat the recursive decomposition used in Theorem 3. (The reader is referred to Section 5 in <ref> [12] </ref> for a discussion of FFT and the shu*e-merge technique.) 1. We start by computing p N FFTs. The ith FFT is computed on the ith contiguous group of p N =P tracks. 2. We shu*e-merge the records to form p N new contiguous groups of p N =P tracks. <p> Once two elements are in base memory level together, they can be added. 2 16 9 STANDARD MATRIX MULTIPLICATION IN P-HMM We use the following divide-and-conquer algorithm, as for two-level memories <ref> [12] </ref>: 1. If k M , we multiply the matrices internally. Otherwise we do the following steps: 2. We subdivide A and B into 8 k=2 fi k=2 submatrices: A 1 A 4 and B 1 -B 4 . <p> The sorting algorithm is a randomized version of distribution sort, using the partitioning technique of the companion paper <ref> [12] </ref>, which was developed for optimal sorting on two-level memories with parallel block transfer. Addendum. Recently an alternative two-level sorting algorithm that is both optimal and deterministic was developed by Nodine and Vitter [9].
References-found: 12

