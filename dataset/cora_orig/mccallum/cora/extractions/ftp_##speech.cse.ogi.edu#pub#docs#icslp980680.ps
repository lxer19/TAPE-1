URL: ftp://speech.cse.ogi.edu/pub/docs/icslp980680.ps
Refering-URL: http://www.cse.ogi.edu/cslu/publications/abstracts/icslp98/abstracts.html
Root-URL: http://www.cse.ogi.edu
Title: Optimized Stopping Criteria for Tree-Based Unit Selection in Concatenative Synthesis  for Spoken Language Understanding  
Author: Andrew Cronk and Michael Macon 
Web: http://cslu.cse.ogi.edu/tts  
Address: Portland, Oregon U.S.A.  
Affiliation: Center  Oregon Graduate Institute,  
Abstract: In this study, a unit selection method based on treestructured clustering of data is implemented and evaluated. This approach to tree construction differs from similar approaches used in both synthesis and recognition in that a rightsized tree is found automatically rather than using hand-tuned stopping criteria. The tree is grown to its maximum size, and its leaves are systematically recombined in order to determine the most suitable subtree. Trees are grown using the automatic stopping method and compared with those grown using thresholds. Cross validation shows that trees grown to their maximum size and systematically recombined produce fuller clusters with lower objective distortion measures than trees whose growth is arrested by a threshold. The study concludes with a discussion of how these results may affect the perceptual quality of a speech synthesizer. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. W. Black and N. Campbell. </author> <title> Optimising selection of units from speech databases for concatenative synthesis. </title> <journal> Eurospeech95, </journal> <volume> vol. 1, </volume> <pages> pp. 581-584, </pages> <address> Madrid, Spain, </address> <year> 1995. </year>
Reference-contexts: Once a sequence of candidate pools or clusters has been selected, a dynamic programming (Viterbi) search is performed to find the sequence of database units that minimize the concatenation cost of the overall sequence. This process is depicted in Figure 1. In <ref> [1] </ref>, the concepts of target and concatenation cost are presented. Target cost reflects how well the linguistic and contextual features a given database unit resemble the ideal target. On the other hand, the concatentation cost reflects how a unit will join with the previously selected unit.
Reference: 2. <author> A. W. Black and P. Taylor. </author> <title> Automatically clustering similar units for unit selection in speech synthesis. </title> <journal> Eurospeech97, </journal> <volume> vol. 2, </volume> <pages> pp. 601-604, </pages> <address> Rhodes, Greece, </address> <year> 1997 </year>
Reference-contexts: However, increasing the size of the speech database mandates that more sophisticated techniques be employed to select the most appropriate unit. As discussed in <ref> [2] </ref>, one approach to concatentative speech synthesis employs a large database of prerecorded, subphonetic units. In order to synthesize any target unit sequence, a candidate pool of units is selected from the speech database for each target. <p> The stopping criterion halts the propagation of a tree branch. Various thresholds have been proposed, such as the minimum improvement of some impurity measure and the minimum number of units per cluster <ref> [2, 3] </ref>. Stopping criteria involving thresholds may lead to trees larger than the data warrants. Such trees may not be able to generalize well to non-training data. In [3], it is proposed that growing the tree to the maximum size and then systematically pruning leaves produces a more reliable classifier. <p> Stopping Criteria Critical to the producing the right tree is the ability to stop its growth at the appropriate time. In several other published works, the predominant method of halting tree growth is by setting thresholds <ref> [2, 3, 4, 6, 7] </ref>. The exact threshold varies according to the task, but minimum node occupancy (of units or frames) or minimum improvement of some measurement (such as impurity) are common. Finding the ideal stopping threshold is difficult, often requiring much trial-and-error.
Reference: 3. <author> L. Breiman et al. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Monterey, California, </address> <year> 1984. </year>
Reference-contexts: The tree offers a means to find the optimal cluster at runtime, and the dynamic programming search determines the concatenation cost. 1.1. Classification and Regression Trees The work of Breiman et al. on classification and regression trees (CART) <ref> [3] </ref> provides the theoretical framework for developing phonetic decision trees. The basic classification tree is defined by four elements: 1. A set of binary splitting questions 2. A goodness-of-split criterion 3. A stopsplitting rule 4. <p> The stopping criterion halts the propagation of a tree branch. Various thresholds have been proposed, such as the minimum improvement of some impurity measure and the minimum number of units per cluster <ref> [2, 3] </ref>. Stopping criteria involving thresholds may lead to trees larger than the data warrants. Such trees may not be able to generalize well to non-training data. In [3], it is proposed that growing the tree to the maximum size and then systematically pruning leaves produces a more reliable classifier. <p> Stopping criteria involving thresholds may lead to trees larger than the data warrants. Such trees may not be able to generalize well to non-training data. In <ref> [3] </ref>, it is proposed that growing the tree to the maximum size and then systematically pruning leaves produces a more reliable classifier. The fourth CART specification assigns terminal nodes to a classification. This specification does not apply to the synthesis problem, as the classifications are not known a priori. <p> Stopping Criteria Critical to the producing the right tree is the ability to stop its growth at the appropriate time. In several other published works, the predominant method of halting tree growth is by setting thresholds <ref> [2, 3, 4, 6, 7] </ref>. The exact threshold varies according to the task, but minimum node occupancy (of units or frames) or minimum improvement of some measurement (such as impurity) are common. Finding the ideal stopping threshold is difficult, often requiring much trial-and-error.
Reference: 4. <author> R. E. Donovan. </author> <title> Trainable Speech Synthesis, </title> <type> Ph.D Thesis. </type> <institution> Cambridge University, </institution> <year> 1996 </year>
Reference-contexts: Stopping Criteria Critical to the producing the right tree is the ability to stop its growth at the appropriate time. In several other published works, the predominant method of halting tree growth is by setting thresholds <ref> [2, 3, 4, 6, 7] </ref>. The exact threshold varies according to the task, but minimum node occupancy (of units or frames) or minimum improvement of some measurement (such as impurity) are common. Finding the ideal stopping threshold is difficult, often requiring much trial-and-error.
Reference: 5. <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recogition. </title> <publisher> Academic Press, </publisher> <month> secondedition , </month> <year> 1990. </year>
Reference-contexts: Since any of the units contained in this leaf may be selected by the dynamic programming search, the evaluation of the tree should take into consideration not only how well the most appropriate unit matches the target, but also how poorly the worst unit matches. In <ref> [5] </ref>, Fukunaga provides insight into how a good evaluation criterion should behave. In essence, an ideal measure of classification goodness should decrease as the number of categories (in this case leaves) increases.
Reference: 6. <author> H. J. Nock, M. J. F. Gales, and S. J. Young. </author> <title> A comparative study of methods for phonetic decision-tree state clustering. </title> <booktitle> Eurospeech 97, </booktitle> <volume> vol. 1, </volume> <pages> pp. 111-114. </pages>
Reference-contexts: Stopping Criteria Critical to the producing the right tree is the ability to stop its growth at the appropriate time. In several other published works, the predominant method of halting tree growth is by setting thresholds <ref> [2, 3, 4, 6, 7] </ref>. The exact threshold varies according to the task, but minimum node occupancy (of units or frames) or minimum improvement of some measurement (such as impurity) are common. Finding the ideal stopping threshold is difficult, often requiring much trial-and-error.
Reference: 7. <author> S. Nakajima. </author> <title> Automatic synthesis unit generation for English speech synthesis based on multilayered context oriented clustering. </title> <journal> Speech Communication, </journal> <volume> vol. 14, </volume> <pages> pp. 313-324, </pages> <year> 1994. </year>
Reference-contexts: Stopping Criteria Critical to the producing the right tree is the ability to stop its growth at the appropriate time. In several other published works, the predominant method of halting tree growth is by setting thresholds <ref> [2, 3, 4, 6, 7] </ref>. The exact threshold varies according to the task, but minimum node occupancy (of units or frames) or minimum improvement of some measurement (such as impurity) are common. Finding the ideal stopping threshold is difficult, often requiring much trial-and-error.
Reference: 8. <author> W. J. Wang et al. </author> <title> Tree-based unit selection for English speech synthesis. </title> <journal> ICASSP-93, </journal> <volume> vol. 2, </volume> <pages> pp. 191-194, </pages> <address> Minneapolis, </address> <year> 1993. </year>
References-found: 8

