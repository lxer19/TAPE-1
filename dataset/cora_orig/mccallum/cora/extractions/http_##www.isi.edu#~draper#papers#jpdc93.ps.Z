URL: http://www.isi.edu/~draper/papers/jpdc93.ps.Z
Refering-URL: http://www.isi.edu/~draper/papers/papers.html
Root-URL: http://www.isi.edu
Title: Performance Evaluation of a Parallel I/O Subsystem for Hypercube Multicomputers  
Author: Joydeep Ghosh, Kelvin D. Goveas and Jeffrey T. Draper 
Address: Austin, TX 78712-1084.  
Affiliation: Department of Electrical and Computer Engineering, University of Texas,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> B. Agarwal. </author> <title> Parallel I/O subsystems for multicomputers. </title> <type> Master's thesis, </type> <institution> Department of ECE, University of Texas at Austin, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: A PE is said to be balanced with respect to an algorithm (or computation) if the total I/O time, t io equals the computation time, t comp [22]. The same definition also holds for a multicomputer system. It is shown in <ref> [1] </ref> that a balanced architecture implies a scalable one and vice versa. Let C be the computational bandwidth, in MIPS, of a single PE, IO be its external I/O bandwidth in MBytes/second and M the size of its local memory in MBytes. <p> A similar analysis is given in <ref> [1] </ref> to show that the scaling factor for matrix multiplication is p For a general purpose computer system, it is not possible to predict the job mix that will be used.
Reference: [2] <author> J. Akella and D. P. Siewiorek. </author> <title> Modelling and measurement of the impact of input/output on system performance. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 390-399, </pages> <year> 1991. </year> <month> 19 </month>
Reference-contexts: Regarding the volume of I/O activity, recent studies indicate that the long-honored Amdahl-Case rule of one bit of I/O required per instruction understates the I/O demands <ref> [2] </ref>. In fact, for distributed memory machines, I/O traffic is expected to be much higher because of the limited local memory on each processing node. A further increase in I/O traffic is observed in multicomputer systems with virtual memory, where several processes can be running in a time-shared manner.
Reference: [3] <author> W. C. Athas and C. Seitz. </author> <title> Multicomputers: message-passing concurrent computers. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 9-25, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The resulting mismatch now limits system performance for many I/O intensive applications. In most first generation message-passing parallel computers, also known as multicomputers <ref> [3] </ref>, the parallel computer is installed as a "back-end" to a host machine which serves all I/O requests. For example, the Intel iPSC/1 has a single ethernet connecting the host machine with all the processing nodes. The Symult 2010 is typically hosted from a Sun-3 workstation. <p> The Symult 2010 is typically hosted from a Sun-3 workstation. While second generation multicomputers exhibit an order of magnitude improvement in I/O capability through the use of concurrent I/O, the I/O mismatch continues to be observed for a wide class of problems <ref> [3, 4] </ref>. The importance of balancing I/O bandwidth with computational power was highlighted in [22] where it is shown that for some applications, the I/O problem cannot be alleviated by simply adding more memory at each processor node. Current research on I/O problems is reflected in [4]. <p> Moreover, blocking circuit-switched routing mechanisms such as wormhole routing [7] are becoming the norm in current-generation multicomputers with direct-connect topologies because of the availability of fast routing automata <ref> [3, 9] </ref>. In such scenarios, if both I/O and interprocessor communication use the same set of links, the transfer of a large file can block communication links for extended periods and can cause high variability even in the latencies seen by shorter messages exchanged amongst other processing nodes. <p> This facilitates the use of this memory as a disk cache, besides its use in file management duties. System parameters that are used for simulation studies are given in Table 1, and they reflect typical values observed in current-generation multicomputers <ref> [3, 15] </ref>. Files larger than 32 blocks are broken up into smaller files for simulation purposes. <p> In the near future, designers will be able to build multicomputer systems with supercomputer performance by using off-the-shelf microprocessors and support hardware <ref> [3] </ref>. Simultaneous development of the I/O system will be essential to extract optimal performance from such systems. Further extensions of this work include an efficient implementation of a distributed file system that uses the independent I/O network.
Reference: [4] <institution> Computer Architecture News, </institution> <month> September </month> <year> 1989. </year> <note> Special issue on I/O Architecture. </note>
Reference-contexts: The Symult 2010 is typically hosted from a Sun-3 workstation. While second generation multicomputers exhibit an order of magnitude improvement in I/O capability through the use of concurrent I/O, the I/O mismatch continues to be observed for a wide class of problems <ref> [3, 4] </ref>. The importance of balancing I/O bandwidth with computational power was highlighted in [22] where it is shown that for some applications, the I/O problem cannot be alleviated by simply adding more memory at each processor node. Current research on I/O problems is reflected in [4]. <p> The importance of balancing I/O bandwidth with computational power was highlighted in [22] where it is shown that for some applications, the I/O problem cannot be alleviated by simply adding more memory at each processor node. Current research on I/O problems is reflected in <ref> [4] </ref>. An extensive taxonomy and historical perspective is given in [35], while state-of-the-art I/O systems and future trends are highlighted in [15].
Reference: [5] <author> Intel Corporation. </author> <title> iPSC/2 I/O facilities. Order number 280120-001, </title> <year> 1988. </year>
Reference-contexts: Thus, links labelled b, r, and n would be absent from the interface diagram shown in Fig. 2 for the control hypercube. The internal architecture of an I/O node is similar to I/O nodes used in the Intel iPSC/2 <ref> [5] </ref>. The local memory is dual ported so that it can be accessed by both the internal bus and the I/O bus which interfaces with the disk controllers. This facilitates the use of this memory as a disk cache, besides its use in file management duties.
Reference: [6] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview. </title> <address> Portland, OR, </address> <year> 1991. </year>
Reference-contexts: This is motivated by the observation that in recent machines such as the iWARP [23] and the Intel Paragon <ref> [6] </ref>, the compute nodes provide matching bandwidths even if all channels are active concurrently. Similarly, the initial experiments ensure that the coupling between an I/O node and the local PE is not a bottleneck by choosing appropriate high values for n and m.
Reference: [7] <author> W. J. Dally and C. L. Seitz. </author> <title> Deadlock-free message routing in multiprocessor interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):547-553, </volume> <year> 1987. </year>
Reference-contexts: Since the local memory at each processing node is limited, there is typically a considerable amount of paging between main memory and secondary storage (disks). Moreover, blocking circuit-switched routing mechanisms such as wormhole routing <ref> [7] </ref> are becoming the norm in current-generation multicomputers with direct-connect topologies because of the availability of fast routing automata [3, 9]. <p> Thus they offset the average latency figures by (miss rate) fi (av. disk access time). 4.2 Routing Mechanism The flow control of PE-PE messages as well as I/O block transfers are governed by a pipelined circuit-switched routing technique called wormhole routing <ref> [7] </ref>. This technique can be considered as a blocking variant of virtual cut-through routing [17], and is employed in several multicomputers including the iWARP, Symult 2010 and Intel Paragon.
Reference: [8] <author> P.C. </author> <title> Dibble and M.L. Scott. Beyond striping: the Bridge multiprocessor file system. </title> <journal> Computer Architecture News, </journal> <volume> 17(5) </volume> <pages> 32-39, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Using disk striping, one can interleave data across independent disks under the control of a single file system [34]. This technique is able to provide adequate bandwidth for most uniprocessor systems. For multiprocessors, a scheme for having parallel interleaved files has been proposed in <ref> [8] </ref> so that the file system is run on multiple processors. A simpler mechanism, namely the interleaving of data bits across multiple processors, can be used for fine-grain SIMD machines.
Reference: [9] <author> J. Draper, J. Ghosh, and W.C. Athas. </author> <title> The M-Cache: a message-retrieving mechanism for multicomputer systems. </title> <booktitle> In Proceedings of the IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 258-265, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Moreover, blocking circuit-switched routing mechanisms such as wormhole routing [7] are becoming the norm in current-generation multicomputers with direct-connect topologies because of the availability of fast routing automata <ref> [3, 9] </ref>. In such scenarios, if both I/O and interprocessor communication use the same set of links, the transfer of a large file can block communication links for extended periods and can cause high variability even in the latencies seen by shorter messages exchanged amongst other processing nodes.
Reference: [10] <author> G. Fox et al. </author> <title> Solving Problems on Concurrent Processors (I): General Techniques and Regular Problems. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: The first method is to increase the size of the data set of the problem. This will increase the number of computational steps by a larger factor than it will increase the number of I/O steps. This is often the preferred method in practice <ref> [10] </ref>. However, it may not be possible to increase the problem size indefinitely because of application characteristics or memory limitations. The second solution is to increase the external I/O bandwidth of the computer system.
Reference: [11] <author> J. Ghosh and B. Agarwal. </author> <title> Parallel I/O subsystems for hypercube multicomputers. </title> <booktitle> In Proceedings of the Fifth International Parallel Processing Symposium, </booktitle> <pages> pages 381-384, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: It has been observed however, that there can be substantial overlap of these two distinct types of traffic in second generation multicomputer systems that have virtual memory and can run a number of processes in a time-shared manner <ref> [11] </ref>. Since the local memory at each processing node is limited, there is typically a considerable amount of paging between main memory and secondary storage (disks). <p> detailed analytical or simulation studies that quantify the advantages of, and the cost/performance tradeoffs involved in using an independent network. 1 Preliminary studies performed by us recently showed that a separate I/O network can not only decrease average network latency, but also reduce its sensitivity to locality of I/O traffic <ref> [11] </ref>. In that study, packet-switching was used in conjunction with a simple model for computation and communication.
Reference: [12] <author> S. Goldstein. </author> <title> Storage performance an eight year outlook. </title> <type> Technical Report TR 03.308, </type> <institution> IBM Corp., </institution> <address> San Jose, </address> <year> 1987. </year>
Reference-contexts: While there have been some studies on the nature and volume of I/O activity for multi-user environments on mainframes <ref> [12] </ref> and workstations [16], almost no I/O traces are publicly available at present for applications on distributed memory machines with concurrent I/O facilities. Regarding the volume of I/O activity, recent studies indicate that the long-honored Amdahl-Case rule of one bit of I/O required per instruction understates the I/O demands [2].
Reference: [13] <author> H. Hadimioglu and R. J. Flynn. </author> <title> The architectural design of a tightly-coupled hypercube file system. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 147-150, </pages> <year> 1989. </year>
Reference-contexts: Some other researchers have also argued the need for such a network for improved scalability, configurability, connectivity, and for separation of the application software from the software responsible for I/O operation which can be primarily located on the I/O nodes <ref> [13, 25, 37] </ref>. Among the commercially available systems, the NCUBE multicomputer can be purchased with attached I/O nodes that can be connected separately in a ring network.
Reference: [14] <author> K. Hwang and J. Ghosh. Hypernet: </author> <title> A communication-efficient architecture for constructing massively parallel computers. </title> <journal> IEEE Trans. Computers, </journal> <volume> C-36:1450-1466, </volume> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: Also, in this paper the I/O subsystem augments the multicomputer, as opposed to architectures such as the hypernet where the I/O nodes are an integral part of system design <ref> [14] </ref>. 2 The Case for Parallel I/O The capacity of a single large magnetic I/O disk is limited by the number of bits that can be stored per square inch. <p> For the analysis and simulation study reported in this paper, a spherical model of locality <ref> [14, 32] </ref> is used for I/O requests, which occurs concurrently with a globally uniform PE-PE traffic generated by short messages among the PEs. Various mixtures of broadcast, multicast and individual I/O requests can be well approximated with the spherical locality model by choosing an appropriate value for the data locality.
Reference: [15] <author> R. Katz, G. Gibson, and D. Patterson. </author> <title> Disk system architectures for high performance computing. </title> <booktitle> In Proceedings IEEE, </booktitle> <pages> pages 1842-1858, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Current research on I/O problems is reflected in [4]. An extensive taxonomy and historical perspective is given in [35], while state-of-the-art I/O systems and future trends are highlighted in <ref> [15] </ref>. A pioneering study of disk I/O architectures for hypercube systems was done by Reddy and Banerjee [29, 30], who simulated the performance of several algorithms under varying degrees of disk synchronization and declustering. <p> Also, since the data is split between a number of disks, the individual capacity of each disk can be less. This helps in bringing down both disk costs and seek time. Studies have shown that this strategy speeds up performance by 10% to 70% <ref> [15] </ref>. Further improvement is limited largely because the dominating seek and rotational latency delays are not reduced substantially. In a multiprocessor system with shared memory, a number of concurrently active I/O channels 2 can be used to directly access the main memory. <p> This facilitates the use of this memory as a disk cache, besides its use in file management duties. System parameters that are used for simulation studies are given in Table 1, and they reflect typical values observed in current-generation multicomputers <ref> [3, 15] </ref>. Files larger than 32 blocks are broken up into smaller files for simulation purposes. <p> They thus bypass the local I/O node. Block transfers are referred to as "IO-PE" messages. For both local and remote I/O nodes, the startup and access latencies used are given in Table 1, and are representative of current technology <ref> [15] </ref>. resides at the local node, either in the disk cache or in the disk, is P l = h.
Reference: [16] <author> R. H. Katz, J. K. Ousterhout, D. A. Patterson, and M. R. Stonebraker. </author> <title> A project on high performance I/O subsystems. </title> <journal> IEEE Database Engineering Bulletin, </journal> <volume> 11(1) </volume> <pages> 40-47, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: While there have been some studies on the nature and volume of I/O activity for multi-user environments on mainframes [12] and workstations <ref> [16] </ref>, almost no I/O traces are publicly available at present for applications on distributed memory machines with concurrent I/O facilities. Regarding the volume of I/O activity, recent studies indicate that the long-honored Amdahl-Case rule of one bit of I/O required per instruction understates the I/O demands [2].
Reference: [17] <author> P. Kermani and L. Kleinrock. </author> <title> Virtual cut-through: a new computer communication switching technique. </title> <journal> Computer Networks, </journal> <volume> 3 </volume> <pages> 267-286, </pages> <year> 1979. </year>
Reference-contexts: This technique can be considered as a blocking variant of virtual cut-through routing <ref> [17] </ref>, and is employed in several multicomputers including the iWARP, Symult 2010 and Intel Paragon. This technique can be efficiently supported in hardware, and at low loads, is characterized by very low network latencies which makes the network transit time almost independent of the number of hops taken.
Reference: [18] <author> J. Kim and C.R. Das. </author> <title> Modelling wormhole routing in a hypercube. </title> <booktitle> In International Conference on Distributed Computing Systems, </booktitle> <pages> pages 386-393, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: An exact analysis of wormhole routing is very involved due to its blocking nature which forces one to use a recursive formulation wherein one calculates the blocking probability for messages only a hop away from their destination, and works backwards <ref> [18] </ref>. We use a more tractable queuing model by observing that a blocked message can be considered as being queued up at the blocking channel (and associated flit buffer) for analysis purposes, even though different flits are at physically different locations along the path of that message [?]. <p> The simulation indicates the network saturates around 22.2 generations/ms. By applying the queuing model described above, one can obtain an estimate of the waiting times, latencies and saturation levels. More importantly, this model avoids the high complexity of more detailed analysis of wormhole-type routing <ref> [18] </ref> and provides a quick overview of the behavior of the coupled networks that have quite different characteristics. For the test case, the model indicates that the b = 1 case saturates at a lower traffic rate than the b = 0 case, which agrees with simulation results.
Reference: [19] <author> M. Y. Kim. </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(11):978-988, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: The issues explored in this paper are largely orthogonal to approaches that use arrays of I/O disks for improved performance [26], or to methods such as disk striping [34] and disk synchronization <ref> [19] </ref>. These techniques can be used to increase bandwidth and decrease the observed latency at each I/O node, and are thus complementary to the approach taken by us.
Reference: [20] <author> L. Kleinrock. </author> <title> Queueing Systems. </title> <publisher> John Wiley & Sons, </publisher> <year> 1975. </year> <month> 20 </month>
Reference-contexts: Thus, waiting times for these channels are therefore approximated as zero. The latency penalty due to contention on other channels is found by using the previously mentioned queuing models. The average waiting time in queue for an M/G/1 queue is given by <ref> [20] </ref> W M=G=1 = b ) = x b = b where is the mean arrival rate, x is the mean service time, and 2 b is the variance of the service time distribution.
Reference: [21] <author> D.F. Kotz and C.S. Ellis. </author> <title> Prefetching in file systems for MIMD multiprocessors. </title> <booktitle> IEEE Trans--actions on Parallel and Distributed Systems, </booktitle> <pages> pages 218-230, </pages> <year> 1990. </year>
Reference-contexts: A further increase in I/O traffic is observed in multicomputer systems with virtual memory, where several processes can be running in a time-shared manner. Also, typical frequency of I/O messages, as a fraction of the regular communication messages, is in the range of 3 to 20% <ref> [21, 29] </ref>. The nature of I/O activity varies widely with the application mix. <p> Finally, due to the big disparity between access times for primary and secondary memory, disk cache hit rate is still the primary determinant of performance, and efficient cache management [31] and file prefetching techniques <ref> [21] </ref> are of utmost importance. The independent I/O network provides another way of achieving higher disk cache hit rates through file migration. This avenue can also be studied further.
Reference: [22] <author> H.T. Kung. </author> <title> Memory requirements for balanced computer architecture. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 49-54, </pages> <year> 1986. </year>
Reference-contexts: While second generation multicomputers exhibit an order of magnitude improvement in I/O capability through the use of concurrent I/O, the I/O mismatch continues to be observed for a wide class of problems [3, 4]. The importance of balancing I/O bandwidth with computational power was highlighted in <ref> [22] </ref> where it is shown that for some applications, the I/O problem cannot be alleviated by simply adding more memory at each processor node. Current research on I/O problems is reflected in [4]. <p> This means that the system achieves linear speedup for that algorithm. The idea of balancing, proposed in <ref> [22] </ref>, is used to determine the scaling desired for the I/O subsystem. A PE is said to be balanced with respect to an algorithm (or computation) if the total I/O time, t io equals the computation time, t comp [22]. The same definition also holds for a multicomputer system. <p> The idea of balancing, proposed in <ref> [22] </ref>, is used to determine the scaling desired for the I/O subsystem. A PE is said to be balanced with respect to an algorithm (or computation) if the total I/O time, t io equals the computation time, t comp [22]. The same definition also holds for a multicomputer system. It is shown in [1] that a balanced architecture implies a scalable one and vice versa. <p> Note that t comp includes the time needed for interprocessor communication during sorting. In the second phase, the sorted subsets are merged together using an M-way merge algorithm. It can be shown <ref> [22] </ref> that the time requirements for the second phase are the same as for the first phase.
Reference: [23] <author> H.T. Kung. </author> <title> Network-based multicomputers: </title> <booktitle> redefining high performance computing in the 1990s. In Decennial Caltech Conference on VLSI, </booktitle> <year> 1989. </year>
Reference-contexts: In the preliminary experiments, the sourcing/sinking capabilities of the PE and I/O nodes, denoted in Fig. 2 by k and r respectively, are made equal to the net bandwidth of the node. This is motivated by the observation that in recent machines such as the iWARP <ref> [23] </ref> and the Intel Paragon [6], the compute nodes provide matching bandwidths even if all channels are active concurrently. Similarly, the initial experiments ensure that the coupling between an I/O node and the local PE is not a bottleneck by choosing appropriate high values for n and m.
Reference: [24] <author> Thinking Machines. </author> <title> Connection machine model CM-2 technical summary. </title> <type> Technical Report HA87-4, </type> <institution> Thinking Machines, Inc., </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: A simpler mechanism, namely the interleaving of data bits across multiple processors, can be used for fine-grain SIMD machines. This mechanism is implemented in the Data Vault of the Connection Machine 2 <ref> [24] </ref>, where each word can be written or read in parallel. For any of the schemes for data storage and access outlined above, an immediate question that arises is how many I/O nodes should be used, and where should they be located.
Reference: [25] <author> U. Nagaraj, U.S. Shukla, and A. Paulraj. </author> <title> Design and evaluation of a high performance file system for message-passing parallel computers. </title> <booktitle> In Fifth International Parallel Processing Symposium, </booktitle> <pages> pages 549-554, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Some other researchers have also argued the need for such a network for improved scalability, configurability, connectivity, and for separation of the application software from the software responsible for I/O operation which can be primarily located on the I/O nodes <ref> [13, 25, 37] </ref>. Among the commercially available systems, the NCUBE multicomputer can be purchased with attached I/O nodes that can be connected separately in a ring network.
Reference: [26] <author> D. Patterson, G. Gibson, and R. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM SIGMOD Conference '88, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The throughput and capacity of I/O subsystems have not kept in step with the enormous increase in the speed and capacity of main memory and of processor power in the past two decades <ref> [26] </ref>. The resulting mismatch now limits system performance for many I/O intensive applications. In most first generation message-passing parallel computers, also known as multicomputers [3], the parallel computer is installed as a "back-end" to a host machine which serves all I/O requests. <p> The results quantify the need for an independent I/O network, and demonstrate its superiority over a system without an I/O network under various scenarios. The issues explored in this paper are largely orthogonal to approaches that use arrays of I/O disks for improved performance <ref> [26] </ref>, or to methods such as disk striping [34] and disk synchronization [19]. These techniques can be used to increase bandwidth and decrease the observed latency at each I/O node, and are thus complementary to the approach taken by us. <p> Its speed is limited by the seek and rotational latency delays, which are mechanical and have been decreasing at a rate of only 7% per year <ref> [26] </ref>. Recently, optical disk technology for massive data storage and retrieval is becoming attractive [36]. Optical disks have greater data storage capabilities, but are slower, with seek times of 60-90 milliseconds (ms) as compared to 14 ms for the fastest Winchester drives.
Reference: [27] <author> P. Pierce. </author> <title> A concurrent file system for a highly parallel mass storage subsystem. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: In this study, I/O nodes are embedded in selected nodes of a hypercube-based multicomputer, and the interprocessor communication links are used for routing both interprocessor and I/O traffic. Similarly, the Concurrent File System of the Intel iPSC/2 does not utilize a separate I/O network <ref> [27] </ref>. If there is little overlap between interprocessor communication and I/O traffic, then there is little degradation in using a common network for both purposes.
Reference: [28] <author> T.W. Pratt, J.C. French, P.M. Dickens, and S.A. Janet, Jr. </author> <title> A comparison of the architecture and performance of two parallel file systems. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 161-166, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: These I/O nodes provide access to I/O devices such as RAID arrays through standard interfaces such as SCSI and HIPPI, or through a disk controller <ref> [28] </ref>. They can also function as device/file servers, and use part of their local memory as a disk cache. A good comparative description of the I/O nodes used in the Intel iPSC/2 and NCUBE/9 multicomputers can be found in [28]. <p> interfaces such as SCSI and HIPPI, or through a disk controller <ref> [28] </ref>. They can also function as device/file servers, and use part of their local memory as a disk cache. A good comparative description of the I/O nodes used in the Intel iPSC/2 and NCUBE/9 multicomputers can be found in [28]. The performance of an I/O subsystem using multiple I/O nodes is strongly influenced by the schemes used to store, maintain and access data. Using disk striping, one can interleave data across independent disks under the control of a single file system [34].
Reference: [29] <author> A. Reddy and P. Banerjee. </author> <title> Design, analysis and simulation of I/O architectures for hypercube multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 140-151, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Current research on I/O problems is reflected in [4]. An extensive taxonomy and historical perspective is given in [35], while state-of-the-art I/O systems and future trends are highlighted in [15]. A pioneering study of disk I/O architectures for hypercube systems was done by Reddy and Banerjee <ref> [29, 30] </ref>, who simulated the performance of several algorithms under varying degrees of disk synchronization and declustering. In this study, I/O nodes are embedded in selected nodes of a hypercube-based multicomputer, and the interprocessor communication links are used for routing both interprocessor and I/O traffic. <p> A further increase in I/O traffic is observed in multicomputer systems with virtual memory, where several processes can be running in a time-shared manner. Also, typical frequency of I/O messages, as a fraction of the regular communication messages, is in the range of 3 to 20% <ref> [21, 29] </ref>. The nature of I/O activity varies widely with the application mix. <p> The block access locality, which gives the probability of finding a requested block at a local I/O node, is thus affected by the extent of declustering <ref> [29] </ref>. Overall, I/O requests for file accesses can be categorized as broadcast, signifying that a block is required for all nodes; gather and scatter operations that involve a subset of the nodes, and independent file requests from single nodes [37].
Reference: [30] <author> A. Reddy, P. Banerjee, and S. Abraham. </author> <title> I/O embedding in hypercubes. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 331-338, </pages> <year> 1988. </year>
Reference-contexts: Current research on I/O problems is reflected in [4]. An extensive taxonomy and historical perspective is given in [35], while state-of-the-art I/O systems and future trends are highlighted in [15]. A pioneering study of disk I/O architectures for hypercube systems was done by Reddy and Banerjee <ref> [29, 30] </ref>, who simulated the performance of several algorithms under varying degrees of disk synchronization and declustering. In this study, I/O nodes are embedded in selected nodes of a hypercube-based multicomputer, and the interprocessor communication links are used for routing both interprocessor and I/O traffic. <p> The other PE nodes serviced by this I/O node will see a network latency corresponding to a small number of hops depending on the allocation algorithm. An optimal allocation is one in which every PE is either 1 or 2 links away from exactly one I/O node. In <ref> [30] </ref>, a scheme based on Hamming codes is used to show that an optimal allocation exists for a binary hypercube if and only if the number of dimensions in the cube, n, is given by n = 2 m 1. <p> Finding a minimum allocation is analogous to the dominating set problem in graph theory, and is known to be NP-complete. We propose to follow the Hamming code based embedding scheme of <ref> [30] </ref> that was outlined above, with the added observation that this scheme meets the desired scaling factor as determined in the previous section. The coupling between the PE and I/O subsystems, described in Section 4, however differs from that proposed in [30]. <p> to follow the Hamming code based embedding scheme of <ref> [30] </ref> that was outlined above, with the added observation that this scheme meets the desired scaling factor as determined in the previous section. The coupling between the PE and I/O subsystems, described in Section 4, however differs from that proposed in [30]. An n-dimensional hypercube is augmented with an I/O system by first extending it to the (n + 1) th dimension. About N log N of the 2 n added positions are used for placing the I/O nodes, in accordance with the scalability results of the previous section.
Reference: [31] <author> A.L.N. Reddy. </author> <title> A study of I/O system organizations. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 308-317, </pages> <year> 1992. </year>
Reference-contexts: Finally, due to the big disparity between access times for primary and secondary memory, disk cache hit rate is still the primary determinant of performance, and efficient cache management <ref> [31] </ref> and file prefetching techniques [21] are of utmost importance. The independent I/O network provides another way of achieving higher disk cache hit rates through file migration. This avenue can also be studied further.
Reference: [32] <author> D. A. Reed and R. M. Fujimoto. </author> <title> Multicomputer Networks: Message-Based Parallel Processing. </title> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: For the analysis and simulation study reported in this paper, a spherical model of locality <ref> [14, 32] </ref> is used for I/O requests, which occurs concurrently with a globally uniform PE-PE traffic generated by short messages among the PEs. Various mixtures of broadcast, multicast and individual I/O requests can be well approximated with the spherical locality model by choosing an appropriate value for the data locality.
Reference: [33] <author> T. Saaty. </author> <title> Elements of Queueing Theory with Applications. </title> <publisher> McGraw-Hill, </publisher> <year> 1961. </year>
Reference-contexts: The average waiting time for an M/M/m queue is <ref> [33] </ref> W M=M=m = m!(1 ) 2 m1 X (m) k + m!(1 ) = m The average latency is given by: latency = X P i (W i + 1) where P i is the probability that channel i is traversed, W i is the average waiting time for channel
Reference: [34] <author> K. Salem and H. Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In IEEE 1986 Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference-contexts: The issues explored in this paper are largely orthogonal to approaches that use arrays of I/O disks for improved performance [26], or to methods such as disk striping <ref> [34] </ref> and disk synchronization [19]. These techniques can be used to increase bandwidth and decrease the observed latency at each I/O node, and are thus complementary to the approach taken by us. <p> They also lack a direct overwrite capability at present. At present, the most feasible solution for obtaining a high performance I/O system is to resort to parallelization at various levels. A number of projects such as RAID ([26], <ref> [34] </ref>) have proposed a low level parallelization of I/O by dividing data blocks into smaller sub-blocks and storing these sub-blocks in different disks which are organized in an array. This strategy speeds up the transfer rate of the system by an order of magnitude and improves reliability. <p> The performance of an I/O subsystem using multiple I/O nodes is strongly influenced by the schemes used to store, maintain and access data. Using disk striping, one can interleave data across independent disks under the control of a single file system <ref> [34] </ref>. This technique is able to provide adequate bandwidth for most uniprocessor systems. For multiprocessors, a scheme for having parallel interleaved files has been proposed in [8] so that the file system is run on multiple processors.
Reference: [35] <author> M. Smotherman. </author> <title> A sequencing-based taxonomy of I/O systems and review of historical machines. </title> <journal> Computer Architecture News, </journal> <volume> 17(5) </volume> <pages> 5-15, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Current research on I/O problems is reflected in [4]. An extensive taxonomy and historical perspective is given in <ref> [35] </ref>, while state-of-the-art I/O systems and future trends are highlighted in [15]. A pioneering study of disk I/O architectures for hypercube systems was done by Reddy and Banerjee [29, 30], who simulated the performance of several algorithms under varying degrees of disk synchronization and declustering.
Reference: [36] <author> J. Voelcker. Peripherals: </author> <title> Higher capacity in smaller packages. </title> <journal> IEEE Spectrum, </journal> <pages> pages 28-30, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Its speed is limited by the seek and rotational latency delays, which are mechanical and have been decreasing at a rate of only 7% per year [26]. Recently, optical disk technology for massive data storage and retrieval is becoming attractive <ref> [36] </ref>. Optical disks have greater data storage capabilities, but are slower, with seek times of 60-90 milliseconds (ms) as compared to 14 ms for the fastest Winchester drives. They also lack a direct overwrite capability at present.

References-found: 36

