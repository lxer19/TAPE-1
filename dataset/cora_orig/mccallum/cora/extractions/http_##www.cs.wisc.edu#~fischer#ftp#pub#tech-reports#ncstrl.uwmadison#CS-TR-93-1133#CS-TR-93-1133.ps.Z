URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1133/CS-TR-93-1133.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1133/
Root-URL: http://www.cs.wisc.edu
Email: hollings@cs.wisc.edu  bart@cs.wisc.edu  
Title: Dynamic Control of Performance Monitoring on Large Scale Parallel Systems  
Author: Jeffrey K. Hollingsworth Barton P. Miller Jeffrey K. Hollingsworth and Barton P. Miller 
Note: 3 1993  This work was supported in part by National Science Foundation grants CCR-8815928 and CCR-9100968, Office of Naval Research grant N00014-89-J-1222, and a grant from Sequent Computer Systems Inc.  
Address: 1210 W. Dayton Street Madison, Wisconsin 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: Performance monitoring of large scale parallel computers creates a dilemma: we need to collect detailed information to find performance bottlenecks, yet collecting all this data can introduce serious data collection bottlenecks. At the same time, users are being inundated with volumes of complex graphs and tables that require a performance expert to interpret. We present a new approach called the W Search Model, that addresses both these problems by combining dynamic on-the-fly selection of what performance data to collect with decision support to assist users with the selection and presentation of performance data. Our goal is to provide users with answers to their performance questions and at the same time dramatically reduce the volume of performance data we need to collect. We present a case study describing how a prototype implementation of our technique was able to identify the bottlenecks in three real programs. In addition, we were able to reduce the amount of performance data collected by a factor ranging from 13 to 700 compared to traditional sampling and trace based instrumentation techniques.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> T. E. Anderson and E. D. Lazowska, "Quartz: </author> <title> A Tool for Tuning Parallel Program Performance", </title> <booktitle> Proc. of the 1990 SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Boston, </address> <month> May </month> <year> 1990, </year> <pages> pp. 115-125. </pages>
Reference-contexts: Performance metrics address the user side of the performance problem by reducing large volumes of performance data into single values or tables of values. Many metrics have been proposed for parallel programs: Critical Path [23], NPT <ref> [1] </ref>, MTOOL [8], Gprof [9]. Each of these metrics can provide useful information, however in an earlier paper [12] we compared several of these metrics (and a few variations) and concluded that no single metric was optimal for all programs.
Reference: 2. <author> P. C. Bates and J. C. Wileden, "EDL: </author> <title> A Basis For Distributed System Debugging Tools", </title> <booktitle> 15th Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1982, </year> <pages> pp. 86-93. </pages>
Reference-contexts: Several approaches have been proposed to address the problem of how to efficiently collect performance data. One approach is to define a set of predicates that describe the interesting events in a program, and only collect data for those events that satisfy the predicate. EDL <ref> [2] </ref>, ISSOS [17], and BEE [4] use this approach. The first two use a static set of predicates for an entire program's execution and lack the fine granularity of control of our approach.
Reference: 3. <author> T. Bemmerl, A. Bode, P. Braum, O. Hansen, T. Tremi and R. Wismuller, </author> <title> The Design and Implementation of TOPSYS, </title> <institution> TUM-INFO-07-71-440, Technische Universitat Munchen, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Examples of predicates are the first time a selected procedure is called, or when the synchronization wait time is above a selected threshold. This method requires less direct involvement by the user. Existing correctness debuggers (e.g., Spider [20], and TOPSYS <ref> [3] </ref>) use similar predicates. A third approach is to have the programmer annotate their program with calls to a library routine to indicate major parts of the computation. This approach can be quite effective, but is not very elegant because it requires the programmer to modify their code.
Reference: 4. <author> B. Bruegge, </author> <title> "A Portable Platform for Distributed Event Environments", </title> <booktitle> Proc. of the 1991 ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <address> Santa Cruz, CA, </address> <month> May 20-21, </month> <year> 1991, </year> <pages> pp. 184-193. </pages> <note> appears as SIGPLAN Notices, </note> <month> December </month> <year> 1991. </year>
Reference-contexts: One approach is to define a set of predicates that describe the interesting events in a program, and only collect data for those events that satisfy the predicate. EDL [2], ISSOS [17], and BEE <ref> [4] </ref> use this approach. The first two use a static set of predicates for an entire program's execution and lack the fine granularity of control of our approach. BEE permits dynamic control of the predicates, however it does not provide any guidance of what predicates to select.
Reference: 5. <institution> UNICOS Performance Utilities References Manual, SR-2040 6.0, Cray Research Inc. </institution>
Reference-contexts: However, it makes it easy to generate so much trace data that it swamps any file system or data reduction station. One system that does try to provide high level decision support about the performance of parallel programs is Atexpert <ref> [5] </ref> from Cray Research. It uses rules to recognize performance problems in Fortran programs. This tool solves a special case of the problem we are addressing, Fortran programs that have been automatically parallelized by the compiler.
Reference: 6. <institution> UNICOS File Formats and Special Files Reference Manual, SR-2014 5.0, Cray Research Inc. </institution>
Reference-contexts: BEE permits dynamic control of the predicates, however it does not provide any guidance of what predicates to select. Another approach is to build special hardware to collect performance data. The Sequent Symmetry [21], and the Cray Y-MP <ref> [6] </ref> provide a set of programable counters to collect performance data. However, since the systems can collect more data than they have of counters, the user is left to select what to collect. In addition, not all interesting events are visible to hardware data collectors.
Reference: 7. <author> D. DeWitt and R. Gerber, </author> <title> "Multiprocessor Hash-Based Join Algorithms", </title> <booktitle> Proc. of the 1985 VLDB Conference, </booktitle> <address> Stockholm, Sweden, </address> <month> August </month> <year> 1985, </year> <pages> pp. 151-164. </pages>
Reference-contexts: Shared Memory Join The shared memory join application is an implementation of the join function for a relational database. It implements a hash-join algorithm <ref> [7] </ref> using shared memory for inter-process communication. The program was written to study shared-memory and shared-nothing join algorithms. We ran the program on a dedicated four processor Sequent Symmetry. Our test case ran for 93 seconds. The Performance Consultant identified one bottleneck in the program due to excessive page faults.
Reference: 8. <author> A. J. Goldberg and J. L. Hennessy, </author> <title> "Performance Debugging Shared Memory Multiprocessor Programs with MTOOL", </title> <booktitle> Proc. </booktitle> <address> of Supercomputing'91 , Albuquerque, NM, </address> <month> Nov. </month> <pages> 18-22, </pages> <year> 1991, </year> <pages> pp. 481-490. </pages>
Reference-contexts: Performance metrics address the user side of the performance problem by reducing large volumes of performance data into single values or tables of values. Many metrics have been proposed for parallel programs: Critical Path [23], NPT [1], MTOOL <ref> [8] </ref>, Gprof [9]. Each of these metrics can provide useful information, however in an earlier paper [12] we compared several of these metrics (and a few variations) and concluded that no single metric was optimal for all programs.
Reference: 9. <author> S. L. Graham, P. B. Kessler and M. K. McKusick, </author> <title> "gprof: a Call Graph Execution Profiler", </title> <booktitle> SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <address> Boston, </address> <month> June </month> <year> 1982, </year> <pages> pp. 120-126. </pages>
Reference-contexts: Performance metrics address the user side of the performance problem by reducing large volumes of performance data into single values or tables of values. Many metrics have been proposed for parallel programs: Critical Path [23], NPT [1], MTOOL [8], Gprof <ref> [9] </ref>. Each of these metrics can provide useful information, however in an earlier paper [12] we compared several of these metrics (and a few variations) and concluded that no single metric was optimal for all programs.
Reference: 10. <author> M. T. Heath and J. A. Etheridge, </author> <title> Visualizing Performance of Parallel Programs, </title> <institution> TM-11813, Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Visualization presents large amounts of performance data in a graphical or aural way. The problem with most visualizations is that they are only useful for finding a specific type of bottleneck and so most tools provide a rich - -- library of different visualizations. For example Paragraph <ref> [10] </ref> provides over twenty different visualizations and many of these displays can be configured to plot values for different resources (e.g. CPU and disk utilization). Unfortunately the user is left with the formidable task of selecting appropriate visualizations and resources to display.
Reference: 11. <author> J. K. Hollingsworth, R. B. Irvin and B. P. Miller, </author> <title> "The Integration of Application and System Based Metrics in A Parallel Program Performance Tool", </title> <booktitle> Proc. of the 1991 ACM SIGPLAN Symposium on Principals and Practice of Parallel Programming , Williamsburg, </booktitle> <address> VA, </address> <month> April 21-24 </month> <year> 1991, </year> <pages> pp. 189-200. </pages> <note> appears as SIGPLAN Notices, </note> <month> July </month> <year> 1991. </year>
Reference-contexts: IPS-2 records event traces during a program's execution. Each event (e.g. procedure call or synchronization operation) contains both wall-clock and process time-stamps in addition to some event specific data. In addition to normal IPS-2 instrumentation we ran the programs with two External Data Collectors <ref> [11] </ref>. External Data Collectors are dedicated sampling processes that collect additional information not available via tracing. One collector gathered information about the behavior of the operating system (e.g., page faults, context switch rate, etc). The other collected data about the hardware (e.g., cache miss rates, and bus utilization). <p> This flexible approach to finding bottlenecks is an important feature of our work. To validate this result, we again used the IPS-2 performance tools. Since we had previously studied this program <ref> [11] </ref>, we recognized the page fault problem as one of the major problems in this program. The problem was due to the creation of new user data in the program.
Reference: 12. <author> J. K. Hollingsworth and B. P. Miller, </author> <title> "Parallel Program Performance Metrics: A Comparison and Validation", </title> <booktitle> Supercomputing 1992, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992, </year> <pages> pp. 4-13. </pages>
Reference-contexts: Many metrics have been proposed for parallel programs: Critical Path [23], NPT [1], MTOOL [8], Gprof [9]. Each of these metrics can provide useful information, however in an earlier paper <ref> [12] </ref> we compared several of these metrics (and a few variations) and concluded that no single metric was optimal for all programs. However, we did discover several factors that can be used to help select appropriate metrics.
Reference: 13. <author> T. Lehr, Z. Segall, D. F. Vrsalovic, E. Caplan, A. L. Chung and C. E. Fineman, </author> <title> "Visualizing Performance Debugging", </title> <booktitle> IEEE Computer 21, </booktitle> <month> 10 (October </month> <year> 1989), </year> <pages> pp. 38-51. </pages>
Reference: 14. <author> A. D. Malony and D. A. Reed, </author> <title> "A Hardware-Based Performance Monitor for the Intel iPSC/2 Hypercube", </title> <booktitle> 1990 International Conference on Supercomputing, </booktitle> <address> Amsterdam, </address> <month> June 11-15, </month> <year> 1990, </year> <pages> pp. 213-226. </pages>
Reference-contexts: However, since the systems can collect more data than they have of counters, the user is left to select what to collect. In addition, not all interesting events are visible to hardware data collectors. Another approach used in MultiKron [16], TMP [22] and HYPERMON <ref> [14] </ref> is to build hardware that can generate trace data and send it to a data reduction node (or file). Hardware assisted trace generation eliminates most of the pertibation of the CPU and inter-connection network.
Reference: 15. <author> B. P. Miller, M. Clark, J. Hollingsworth, S. Kierstead, S. Lim and T. Torzewski, "IPS-2: </author> <title> The Second Generation of a Parallel Program Measurement System", </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1, </journal> <month> 2 (April </month> <year> 1990), </year> <pages> pp. 206-217. </pages>
Reference-contexts: In the future we plan to expand our interface to incorporate realtime profile tables and visualizations of the performance data. Since no available system provided the necessary infrastructure for dynamic control of the instrumentation, we used trace data generated by the IPS-2 <ref> [15] </ref> performance tool and simulated dynamic data selection. A benefit of this approach is that it permitted us to compare the quality of guidance supplied using dynamic selection to that of full tracing.
Reference: 16. <author> A. Mink, R. Carpenter, G. Nacht and J. Roberts, </author> <title> "Multiprocessor Performance Measurement Instrumentation", </title> <booktitle> IEEE Computer 23, </booktitle> <month> 9 (September </month> <year> 1990), </year> <pages> pp. 63-75. </pages>
Reference-contexts: However, since the systems can collect more data than they have of counters, the user is left to select what to collect. In addition, not all interesting events are visible to hardware data collectors. Another approach used in MultiKron <ref> [16] </ref>, TMP [22] and HYPERMON [14] is to build hardware that can generate trace data and send it to a data reduction node (or file). Hardware assisted trace generation eliminates most of the pertibation of the CPU and inter-connection network.
Reference: 17. <author> K. Schwan, R. Ramnath, S. Vasudevan and D. M. Ogle, </author> <title> "A language and system for parallel programming", </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> April </month> <year> 1988, </year> <pages> pp. 455-471. </pages> - -- 
Reference-contexts: Several approaches have been proposed to address the problem of how to efficiently collect performance data. One approach is to define a set of predicates that describe the interesting events in a program, and only collect data for those events that satisfy the predicate. EDL [2], ISSOS <ref> [17] </ref>, and BEE [4] use this approach. The first two use a static set of predicates for an entire program's execution and lack the fine granularity of control of our approach. BEE permits dynamic control of the predicates, however it does not provide any guidance of what predicates to select.
Reference: 18. <author> Z. Segall and L. Rudolph, "PIE: </author> <title> A programming and instrumentation environment for parallel processing", </title> <booktitle> IEEE Software 2, </booktitle> <month> 6 (November </month> <year> 1985), </year> <pages> pp. 22-37. </pages>
Reference: 19. <author> J. P. Singh, W. Weber and A. Gupta, </author> <title> "SPLASH: Stanford Parallel Applications for Shared-Memory", Computer Architecture News 20, </title> <month> 1 (March </month> <year> 1992), </year> <pages> pp. 5-44. </pages>
Reference-contexts: To validate the guidance supplied by the Performance Consultant, we also studied the application programs using the IPS-2 performance tools and compared results. We used our prototype to study three applications: two are from the Splash <ref> [19] </ref> benchmark suite and one is a database application. 3.1. Experimental Method Our test implementation includes 15 hypotheses (shown in Figure 9) dealing with I/O, virtual memory, synchronization, and CPU bottlenecks.
Reference: 20. <author> E. T. Smith, </author> <title> Debugging Techniques for Communicating, Loosely-Coupled Processes, </title> <type> PhD Thesis, </type> <institution> University of Rochester, </institution> <month> December </month> <year> 1981. </year>
Reference-contexts: Examples of predicates are the first time a selected procedure is called, or when the synchronization wait time is above a selected threshold. This method requires less direct involvement by the user. Existing correctness debuggers (e.g., Spider <ref> [20] </ref>, and TOPSYS [3]) use similar predicates. A third approach is to have the programmer annotate their program with calls to a library routine to indicate major parts of the computation.
Reference: 21. <author> S. S. Thakkar, </author> <type> Personal Communication. </type>
Reference-contexts: BEE permits dynamic control of the predicates, however it does not provide any guidance of what predicates to select. Another approach is to build special hardware to collect performance data. The Sequent Symmetry <ref> [21] </ref>, and the Cray Y-MP [6] provide a set of programable counters to collect performance data. However, since the systems can collect more data than they have of counters, the user is left to select what to collect. In addition, not all interesting events are visible to hardware data collectors.
Reference: 22. <author> D. Wybranietz and D. Haban, </author> <title> "Monitoring and Performance Measuring Distributed Systems during Operation", </title> <booktitle> SIGMETRICS, </booktitle> <address> Santa Fe, New Mexico, </address> <month> May </month> <year> 1988, </year> <pages> pp. 197-206. </pages>
Reference-contexts: However, since the systems can collect more data than they have of counters, the user is left to select what to collect. In addition, not all interesting events are visible to hardware data collectors. Another approach used in MultiKron [16], TMP <ref> [22] </ref> and HYPERMON [14] is to build hardware that can generate trace data and send it to a data reduction node (or file). Hardware assisted trace generation eliminates most of the pertibation of the CPU and inter-connection network.
Reference: 23. <author> C. Yang and B. P. Miller, </author> <title> "Critical Path Analysis for the Execution of Parallel and Distributed Programs", </title> <booktitle> 8th Int'l Conf. on Distributed Computing Systems, </booktitle> <address> San Jose, Calif., </address> <month> June </month> <year> 1988, </year> <pages> pp. 366-375. </pages>
Reference-contexts: Performance metrics address the user side of the performance problem by reducing large volumes of performance data into single values or tables of values. Many metrics have been proposed for parallel programs: Critical Path <ref> [23] </ref>, NPT [1], MTOOL [8], Gprof [9]. Each of these metrics can provide useful information, however in an earlier paper [12] we compared several of these metrics (and a few variations) and concluded that no single metric was optimal for all programs.
Reference: 24. <author> D. Zernik and L. Rudolph, </author> <title> "Animating Work and Time for Debugging Parallel Programs Foundation and Experience", </title> <booktitle> Proc. of the 1991 ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <address> Santa Cruz, CA, </address> <month> May 20-21, </month> <year> 1991, </year> <pages> pp. 46-56. </pages> <note> appears as SIGPLAN Notices, </note> <month> December </month> <year> 1991. </year> - --
References-found: 24

