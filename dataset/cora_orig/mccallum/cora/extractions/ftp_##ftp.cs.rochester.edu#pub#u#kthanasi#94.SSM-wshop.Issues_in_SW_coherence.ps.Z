URL: ftp://ftp.cs.rochester.edu/pub/u/kthanasi/94.SSM-wshop.Issues_in_SW_coherence.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/kthanasi/
Root-URL: 
Email: fkthanasi,scottg@cs.rochester.edu  
Title: Issues in Software Cache Coherence  
Author: Leonidas I. Kontothanassis and Michael L. Scott 
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: Large scale multiprocessors can provide the computational power needed to solve some of the larger problems of science and engineering today. Shared memory provides an attractive and intuitive programming model that makes good use of programmer time and effort. Shared memory however requires a coherence mechanism to allow caching for performance and to ensure that processors do not use stale data in their computation. Directory-based coherence, which is the hardware mechanism of choice for large scale multiprocessors, can be expensive both in terms of hardware cost and in terms of the intellectual effort needed to design a correct, efficient protocol. For scalable multiprocessor designs with network-based interconnects, software-based coherence schemes provide an attractive alternative. In this paper we evaluate a new adaptive software coherence protocol, and demonstrate that smart software coherence protocols can be competitive with hardware-based coherence for a large variety of programs. We then discuss issues that affect the performance of software coherence protocols and proceed to suggest algorithmic and architectural enhancements that can help improve software coherence performance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal and others. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <editor> In M. Dubois and S. S. Thakkar, editors, </editor> <booktitle> Scalable Shared Memory Multiprocessors, </booktitle> <pages> pages 239-261. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Shared memory machines use caches to reduce memory latencies, and thus introduce the coherence problemthe need to ensure that processors do not use stale data in their caches. For large scale multiprocessors with network interconnects, the hardware alternative of choice seems to be directory-based coherence <ref> [1, 10] </ref>, but it is expensive both in terms of hardware cost and in terms of design time and intellectual effort required to produce a correct and efficient implementation. It is therefore not uncommon for technological progress to have rendered a machine outdated by the time it is completed.
Reference: [2] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS Parallel Bench marks. </title> <type> Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: The total cost for the transaction is well over 300 cycles. We report results for six parallel programs. Three are best described as computational kernels: Gauss, sor, and fft. The remaining are complete applications: mp3d, water [15], and appbt <ref> [2] </ref>. Due to simulation constraints the input data sizes for all programs are smaller than what would be run on a real machine, a fact that may cause us to see unnaturally high degrees of sharing.
Reference: [3] <author> B. N. Bershad and M. J. Zekauskas. Midway: </author> <title> Shared Memory Parallel Programming with Entry Consistency for Distributed Memory Multiprocessors. </title> <institution> CMU-CS-91-170, Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Under our scheme coherence could be under compiler or runtime control, annotations could have a correctness impact or be purely performance oriented, or in the worst case the absence of annotations could have a correctness impact on program execution. The entry consistency of the Midway system <ref> [3] </ref> employs annotations that fall in this category. The final dimension that we view as important in the domain of software coherence is the choice of coherence mechanism. Write-invalidate has dominated in both software- and hardware-coherent systems.
Reference: [4] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Delayed write notices have been introduced in the context of Munin <ref> [4] </ref> but the benefits of their usage is not obvious in our environment. <p> We have also examined architectural alternatives and program-structuring issues that were not addressed by Petersen and Li. Our work resembles Munin <ref> [4] </ref> and lazy release consistency [7] in its use of delayed write notices, but we take advantage of the globally accessible physical address space for cache fills and for access to the coherent map and the local weak lists.
Reference: [5] <author> A. L. Cox and R. J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Our use of remote reference to reduce coherence management overhead can also be found in work on NUMA memory management <ref> [5] </ref>. However relaxed consistency greatly reduces the opportunities for profitable remote data reference. In fact, early experiments we have conducted with on-line NUMA policies and relaxed consistency have failed badly to determine when to use remote reference.
Reference: [6] <author> N. Jouppi. </author> <title> Cache Write Policies and Performance. </title> <booktitle> In Proceedings of the Twentieth Inter national Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Per-word dirty bits for write-back caches suffice to perform this function with only 3% cache space overhead. The other alternatives include write-through caches, which keep main memory consistent all the time but may cause high memory and interconnect traffic, and write-through caches with a write-collect buffer <ref> [6] </ref>. The latter provide performance competitive to that of write-back caches with reduced hardware cost. Performance is also highly dependent on the sharing behavior exhibited by the program.
Reference: [7] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Dis tributed Shared Memory. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: We have also examined architectural alternatives and program-structuring issues that were not addressed by Petersen and Li. Our work resembles Munin [4] and lazy release consistency <ref> [7] </ref> in its use of delayed write notices, but we take advantage of the globally accessible physical address space for cache fills and for access to the coherent map and the local weak lists.
Reference: [8] <author> Leonidas I. Kontothanassis and Michael L. Scott. </author> <title> Software Cache Coherence for Large Scale Multiprocessors. </title> <type> TR 513, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> March </month> <year> 1994. </year> <note> Submitted for publication. </note>
Reference-contexts: A processor knows which of its pages are unsafe by maintaining a local list of such pages when it first maps them for access. Full details for the protocol can be found in a technical report <ref> [8] </ref>. We apply one additional optimization to the basic protocol. <p> We have seen reductions in running time of well over 50% for our protocol when compared with similar software coherence protocols that do not perform these optimizations [11]. A more detailed comparison of software coherence protocols can be found in <ref> [8] </ref>. We have found the choice of cache architecture to also have a significant impact on the performance of software cache coherence. Write-back caches provide the best performance for almost all cases, however they require additional hardware for correctness.
Reference: [9] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: A protocol co-processor could also help reduce coherence management overhead, but in this case the classification of the system as hardware or software coherent is no longer clear. Several recent research projects <ref> [9, 12] </ref> are taking this intermediate approach, combining the flexibility of software systems with the speed and asynchrony of hardware implementations. We see the choice between static and dynamic coherence management as an important dimension that affects large scale multiprocessor performance.
Reference: [10] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Shared memory machines use caches to reduce memory latencies, and thus introduce the coherence problemthe need to ensure that processors do not use stale data in their caches. For large scale multiprocessors with network interconnects, the hardware alternative of choice seems to be directory-based coherence <ref> [1, 10] </ref>, but it is expensive both in terms of hardware cost and in terms of design time and intellectual effort required to produce a correct and efficient implementation. It is therefore not uncommon for technological progress to have rendered a machine outdated by the time it is completed. <p> However relaxed consistency greatly reduces the opportunities for profitable remote data reference. In fact, early experiments we have conducted with on-line NUMA policies and relaxed consistency have failed badly to determine when to use remote reference. On the hardware side our work bears resemblance to the Stanford Dash project <ref> [10] </ref> in the use of a relaxed consistency model, and to the Georgia Tech Beehive project [14] in the use of relaxed consistency and per-word dirty bits for successful merging of inconsistent cache lines.
Reference: [11] <author> K. Petersen and K. Li. </author> <title> Cache Coherence for Shared Memory Multiprocessors Based on Virtual Memory Support. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <address> Newport Beach, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: We have seen reductions in running time of well over 50% for our protocol when compared with similar software coherence protocols that do not perform these optimizations <ref> [11] </ref>. A more detailed comparison of software coherence protocols can be found in [8]. We have found the choice of cache architecture to also have a significant impact on the performance of software cache coherence. <p> The flexibility of software coherence provides designers with the ability to incorporate all mechanisms in a single protocol and choose the one that best fits the sharing pattern at hand. 5 Related work Our work is most closely related to that of Petersen and Li <ref> [11] </ref>: we both use the notion of weak pages, and purge caches on acquire operations.
Reference: [12] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level Shared-Memory. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: A protocol co-processor could also help reduce coherence management overhead, but in this case the classification of the system as hardware or software coherent is no longer clear. Several recent research projects <ref> [9, 12] </ref> are taking this intermediate approach, combining the flexibility of software systems with the speed and asynchrony of hardware implementations. We see the choice between static and dynamic coherence management as an important dimension that affects large scale multiprocessor performance.
Reference: [13] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> San Jose, CA, </address> <note> October 1994 (to appear). </note>
Reference-contexts: Additional methods that can help designers use smaller coherence blocks include subpage valid bits, and injection of consistency code before write instructions in the executable <ref> [13] </ref>. The latter can provide very fine grained control at the expense of higher software overhead, since consistency code has to run for all memory reference instructions as opposed to the ones that violate consistency.
Reference: [14] <author> G. Shah and U. Ramachandran. </author> <title> Towards Exploiting the Architectural Features of Beehive. </title> <institution> GIT-CC-91/51, College of Computing, Georgia Institute of Technology, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: On the hardware side our work bears resemblance to the Stanford Dash project [10] in the use of a relaxed consistency model, and to the Georgia Tech Beehive project <ref> [14] </ref> in the use of relaxed consistency and per-word dirty bits for successful merging of inconsistent cache lines.
Reference: [15] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The total cost for the transaction is well over 300 cycles. We report results for six parallel programs. Three are best described as computational kernels: Gauss, sor, and fft. The remaining are complete applications: mp3d, water <ref> [15] </ref>, and appbt [2]. Due to simulation constraints the input data sizes for all programs are smaller than what would be run on a real machine, a fact that may cause us to see unnaturally high degrees of sharing.
Reference: [16] <author> J. E. Veenstra. </author> <title> Mint Tutorial and User Manual. </title> <type> TR 452, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: They reduce the number of invalidations needed in acquire operations, especially for applications with false sharing. 3 Performance Results 3.1 Methodology We use execution driven simulation to simulate a mesh-connected multiprocessor with up to 64 nodes. Our simulator consists of two parts: a front end, Mint <ref> [16] </ref>, which simulates the execution of the processors, and a back end that simulates the memory system. The front end calls the back end on every data reference (instruction fetches are assumed to always be cache hits).
References-found: 16

