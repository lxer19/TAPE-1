URL: file://ftp.cc.gatech.edu/pub/groups/architecture/TASS/git.cc.93.03.ps.Z
Refering-URL: http://www.cs.gatech.edu/grads/s/Gautam.Shah/homepage.html
Root-URL: 
Email: e-mail: rama@cc.gatech.edu  
Phone: Ph: (404) 894-5136  
Title: Scalability Study of the KSR-1  
Author: Umakishore Ramachandran Gautam Shah S. Ravikumar Jeyakumar Muthukumarasamy GIT-CC / 
Keyword: Key Words: Shared memory multiprocessors, Scalability, Synchronization, Ring interconnect, Latency, NAS benchmarks, Performance  
Note: This work is supported in part by an NSF PYI Award MIP-9058430 and an NSF Grant MIP-9200005  
Address: Atlanta, GA 30332 USA  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: Scalability of parallel architectures is an interesting area of current research. Shared memory parallel programming is attractive stemming from its relative ease in transitioning from sequential programming. However, there has been concern in the architectural community regarding the scalability of shared memory parallel architectures owing to the potential for large latencies for remote memory accesses. KSR-1 is a recently introduced commercial shared memory parallel architecture, and the scalability of KSR-1 is the focus of this research. The study is conducted using a range of experiments spanning latency measurements, synchronization, and analysis of parallel algorithms for two computational kernels. The key conclusions from this study are as follows: The communication network of KSR-1, a pipelined unidirectional ring, is fairly resilient in supporting simultaneous remote memory accesses from several processors. The multiple communication paths realized through this pipelining help in the efficient implementation of tournament-style barrier synchronization algorithms. Parallel algorithms that have fairly regular and contiguous data access patterns scale well on this architecture. The architectural features of KSR-1 such as the poststore and prefetch are useful for boosting the performance of parallel applications. The sizes of the caches available at each node may be too small for efficiently implementing large data structures. The network does saturate when there are simultaneous remote memory accesses from a fully populated (32 node) ring. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson. </author> <title> The performance of spin lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year> <month> 19 </month>
Reference-contexts: The hardware primitive essentially serializes all lock requests regardless of whether they are shared or exclusive. We have implemented a simple read-write lock using the KSR-1 exclusive lock primitive. Our algorithm is a modified version of Anderson's ticket lock <ref> [1] </ref>. A shared data structure can be acquired in read-shared mode or in a 3 Note that accessing any remote processor would be equivalent to accessing the neighboring processor in terms of latency owing to the unidirectional ring topology. 6 write-exclusive mode.
Reference: [2] <author> D. H. Bailey, E. Barszcz, and J. T. Barton et al. </author> <title> The NAS parallel benchmarks summary and preliminary results. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 158-65, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Thus the synchronization experiments on the KSR-1 demonstrate that the slotted pipelined ring overcomes the communication disadvantage observed on the Symmetry and the hardware cache coherence overcomes the inability to perform global broadcast using shared variables on the BBN Butterfly. 3.3 NAS Kernels The Numerical Aerodynamic Simulation (NAS) parallel benchmark <ref> [3, 2] </ref> consists of five kernels and three applications which are considered to be representative of several scientific and numerical applications. We have implemented three of the five kernels on the KSR-1 as part of our scalability study.
Reference: [3] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report Report RNR-91-002, </type> <institution> NAS Systems Division, Applied Research Branch, NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: Thus the synchronization experiments on the KSR-1 demonstrate that the slotted pipelined ring overcomes the communication disadvantage observed on the Symmetry and the hardware cache coherence overcomes the inability to perform global broadcast using shared variables on the BBN Butterfly. 3.3 NAS Kernels The Numerical Aerodynamic Simulation (NAS) parallel benchmark <ref> [3, 2] </ref> consists of five kernels and three applications which are considered to be representative of several scientific and numerical applications. We have implemented three of the five kernels on the KSR-1 as part of our scalability study.
Reference: [4] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <month> October </month> <year> 1991. </year>
Reference-contexts: Thus there is a tendency to believe that message passing architectures may be more scalable than its architectural rival as is evident from the number of commercially available message passing machines <ref> [4, 10, 14] </ref>. Yet, there is considerable interest in the architectural community toward realizing scalable shared memory multiprocessors. Indeed the natural progression from sequential programming to shared memory style parallel programming is one of the main reasons for such a trend.
Reference: [5] <author> Steve Breit et al. </author> <title> Implementation of EP, SP and BT on the KSR-1. Summer work at KSR, </title> <month> September </month> <year> 1992. </year>
Reference-contexts: The first one is the Embarrassingly Parallel (EP) kernel, which evaluates integrals by means of pseudo-random trials and is used in many Montecarlo simulations. As the name suggests, it is highly suited for parallel machines, since there is virtually no communication among the parallel tasks. Our implementation <ref> [5] </ref> showed linear speedup, and given the limited communication requirements of this kernel, this result was not surprising.
Reference: [6] <author> Steve Frank. </author> <type> Personal communication, </type> <month> January </month> <year> 1993. </year>
Reference-contexts: This result is surprising considering the software overhead (maintaining the queue of requestors) in our queue-based lock algorithm. The result can be partly explained due to the interaction of the operating system with the application as follows <ref> [6] </ref>. While threads of a parallel program can be bound to distinct processors, there is no 4 Each processor repeatedly accesses data in read or write mode, with a delay of 10000 local operations between successive lock requests.
Reference: [7] <author> John L. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: A drawback with this definition is that an architecture would be considered non-scalable if an algorithm running on it has a large sequential part. There have been several recent attempts at refining this notion and define new scalability metrics (see for instance <ref> [11, 7, 13] </ref>).
Reference: [8] <author> D. P. Helmbold and C. E. McDowell. </author> <title> Modelling speedup (n) greater than n. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2):.250-6, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: Figure 7 shows the corresponding speedup curve for the algorithm. Up to about 4 processors, the insufficient sizes of the sub-cache and local-cache inhibits achieving very good speedups. However, notice that relative to the 4 processor performance the 8 and 16 processor executions exhibit superunitary 6 <ref> [8] </ref> speedup. This superunitary speedup can be explained by the fact that the amount of data that each processor has to deal with fits in the respective local-caches, and as we observed earlier the algorithm design ensures that there is very limited synchronization among the processors.
Reference: [9] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two algorithms for barrier synchronization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: In each round a total of P messages are exchanged. The communication pattern for these message exchanges is such that (see Reference <ref> [9] </ref> for details) after the log 2 P rounds are over all the processors are aware of barrier completion. Algorithm 4 (labeled tournament in Figure 4) is a tournament barrier (another tree-style algorithm similar to Algorithm 2) in which the winner in each round is determined statically.
Reference: [10] <author> Intel Corporation, </author> <title> Beaverton, Oregon. Touchstone Delta System User's Guide, </title> <year> 1991. </year>
Reference-contexts: Thus there is a tendency to believe that message passing architectures may be more scalable than its architectural rival as is evident from the number of commercially available message passing machines <ref> [4, 10, 14] </ref>. Yet, there is considerable interest in the architectural community toward realizing scalable shared memory multiprocessors. Indeed the natural progression from sequential programming to shared memory style parallel programming is one of the main reasons for such a trend.
Reference: [11] <author> Alan H. Karp and Horace P. Flatt. </author> <title> Measuring parallel processor performance. </title> <journal> Communications of the ACM, </journal> <volume> 33(5) </volume> <pages> 539-543, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: A drawback with this definition is that an architecture would be considered non-scalable if an algorithm running on it has a large sequential part. There have been several recent attempts at refining this notion and define new scalability metrics (see for instance <ref> [11, 7, 13] </ref>). <p> Table 1 gives the speedup, efficiency, and the measured serial fraction 5 <ref> [11] </ref> for the CG algorithm on the KSR-1. Figure 7 shows the corresponding speedup curve for the algorithm. Up to about 4 processors, the insufficient sizes of the sub-cache and local-cache inhibits achieving very good speedups.
Reference: [12] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: While this reason may explain why the software algorithm may do as well as the system lock, it is still unclear why it does better as can be seen in the results. We have implemented five barrier synchronization algorithms <ref> [12] </ref> on the KSR-1. In our implementation, we have aligned (whenever possible) mutually exclusive parts of shared data structures on separate cache lines so that there is no false sharing. The results are shown in Figure 4. There are two steps to barrier synchronization: arrival and completion. <p> We do observe it to perform better than the counter algorithm. But since this algorithm requires mutual exclusion lock for the fetch and operation, the performance degrades quickly as the number of processors is increased. However, modifying the completion notification (as suggested in <ref> [12] </ref>) by spinning on a global wakeup flag (instead of tree-based notification) set by the last arriving processor produces remarkable performance enhancement (see line labeled tree (M) in Figure 4). <p> Its performance is almost similar to that of the dynamic-tree barrier with global wakeup flag. Although it is stated that the MCS algorithm may be the best for large-scale cache coherent multiprocessors in Reference <ref> [12] </ref>, the detrimental effects discussed above and our results on the KSR-1 indicate that the tournament (M) algorithm is better than the MCS (M) algorithm. <p> It is illustrative to compare the performance of the barriers on the KSR-1 with their performance on the Sequent Symmetry and the BBN Butterfly reported in Reference <ref> [12] </ref>. The Symmetry is a bus-based shared memory multiprocessor with invalidation-based coherent caches. The Butterfly is 10 a distributed shared memory multiprocessor with a butterfly multistage interconnection network, and no caches. The counter algorithm performs the best on the Symmetry. <p> So the determinant here in deciding the winner is the number of rounds of communication (or the critical path for the arrival + wakeup tree) as pointed out by Mellor-Crummey and Scott <ref> [12] </ref>. Thus the dissemination algorithm does the best ((log 2 P ) rounds of communication), followed by the tournament algorithm (2log 2 P rounds of communication), and then the MCS algorithm ((log 4 P + log 2 P ) rounds of communication).
Reference: [13] <author> D. Nussbaum and A. Agarwal. </author> <title> Scalability of parallel machines. </title> <journal> Communications of the ACM, </journal> <volume> 34(3) </volume> <pages> 56-61, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: A drawback with this definition is that an architecture would be considered non-scalable if an algorithm running on it has a large sequential part. There have been several recent attempts at refining this notion and define new scalability metrics (see for instance <ref> [11, 7, 13] </ref>).
Reference: [14] <author> J. F. Palmer and G. Fox. </author> <title> The NCUBE family of high-performance parallel computer systems. </title> <booktitle> In Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 847-51 vol.1, </pages> <year> 1988. </year>
Reference-contexts: Thus there is a tendency to believe that message passing architectures may be more scalable than its architectural rival as is evident from the number of commercially available message passing machines <ref> [4, 10, 14] </ref>. Yet, there is considerable interest in the architectural community toward realizing scalable shared memory multiprocessors. Indeed the natural progression from sequential programming to shared memory style parallel programming is one of the main reasons for such a trend.
Reference: [15] <institution> Kendall Square Research. KSR1 Principles of Operations, </institution> <year> 1992. </year>
Reference-contexts: Concluding remarks are presented in Section 4. 2 Architecture of the KSR-1 The KSR-1 is a 64-bit cache-only memory architecture (COMA) based on an interconnection of a hierarchy of rings <ref> [15, 16] </ref>. The ring (see Figure 1) at the lowest level in the hierarchy can contain up to 32 processors. These "leaf" rings connect to rings of higher bandwidth through a routing unit (ARD).
Reference: [16] <institution> Kendall Square Research. Technical summary, </institution> <year> 1992. </year> <month> 20 </month>
Reference-contexts: Concluding remarks are presented in Section 4. 2 Architecture of the KSR-1 The KSR-1 is a 64-bit cache-only memory architecture (COMA) based on an interconnection of a hierarchy of rings <ref> [15, 16] </ref>. The ring (see Figure 1) at the lowest level in the hierarchy can contain up to 32 processors. These "leaf" rings connect to rings of higher bandwidth through a routing unit (ARD).
References-found: 16

