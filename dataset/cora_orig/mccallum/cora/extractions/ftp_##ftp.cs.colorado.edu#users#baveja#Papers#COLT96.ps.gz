URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/COLT96.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: http://www.cs.colorado.edu
Email: flksaul,singhg@psyche.mit.edu  
Title: Learning Curve Bounds for Markov Decision Processes with Undiscounted Rewards  
Author: Lawrence K. Saul and Satinder P. Singh 
Address: 79 Amherst Street, E10-243 Cambridge, MA 02139  
Affiliation: Center for Biological and Computational Learning Massachusetts Institute of Technology  
Abstract: Markov decision processes (MDPs) with undis-counted rewards represent an important class of problems in decision and control. The goal of learning in these MDPs is to find a policy that yields the maximum expected return per unit time. In large state spaces, computing these averages directly is not feasible; instead, the agent must estimate them by stochastic exploration of the state space. In this case, longer exploration times enable more accurate estimates and more informed decision-making. The learning curve for an MDP measures how the agent's performance depends on the allowed exploration time, T . In this paper we analyze these learning curves for a simple control problem with undiscounted rewards. In particular, methods from statistical mechanics are used to calculate lower bounds on the agent's performance in the thermodynamic limit T ! 1, N ! 1, ff = T =N (finite), where T is the number of time steps allotted per policy evaluation and N is the size of the state space. In this limit, we provide a lower bound on the return of policies that appear optimal based on imperfect statistics.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <booktitle> Artificial Intelligence 72: </booktitle> <pages> 81-138, </pages> <year> 1995. </year>
Reference-contexts: In these cases, one may devise stochastic approximations <ref> [1, 11, 13] </ref> to these algorithms that estimate the required statistics rather than computing them exactly. These approximations rely on stochastic exploration of the state space to measure the returns attached to particular courses of action. <p> To illustrate this, let us calculate v fl for the MDP whose mining rewards r i are uniformly distributed over <ref> [0; 1] </ref>: u (r) = 1 for 0 r 1: 0 otherwise (19) For this reward distribution, it is straightforward to evaluate the integrals in eqs. (16-17); substituting the results into eq. (18) gives: v fl = 2 1 r 2 1 r c : (20) Maximizing this with respect to
Reference: [2] <author> D. P. Bertsekas. </author> <title> Dynamic programming and optimal control, </title> <journal> vols. </journal> <volume> 1 & 2. </volume> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1995. </year>
Reference-contexts: Finally, section 6 contains our conclusions, as well as issues for future research. 2 Markov Decision Processes This section presents a brief review of MDPs, concentrating on those aspects most relevant to our work. A more thorough introduction may be found in ref. <ref> [2] </ref>. 2.1 Background A Markov decision process (MDP) models an agent's environment by a set of N states. In each of these states, the agent is required to choose from a set of possible actions. <p> The Gibbs algorithm is highly idealized in that it performs an exhaustive search over all policies 2 f0; 1g N . Direct methods based on policy iteration <ref> [2] </ref> are more practical for MDPs with large state spaces; roughly speaking, they search through policy space in a stepwise manner, favoring moves that lead to policies with higher returns.
Reference: [3] <author> J. A. Bucklew. </author> <title> Large deviation techniques in decision, simulation, and estimation. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: The goal of this section is to provide a lower bound on " and thus an upper bound on the "confusion" probability that appears in eq. (27). 4.1 Large deviations Our starting point is the following basic theorem from the large deviation theory of Markov processes <ref> [3] </ref>. Let P ij be an N fi N ergodic transition matrix with stationary distribution i , and let X i be a real-valued function over its state space. Without loss of generality, take 1 P i i X i = 0. <p> Borrowing terminology from statistical mechanics, we will refer to matrices of the form e X i P ij as transfer matrices [6]. The rate of decay "(ffi) is also known as the large deviation rate function <ref> [3] </ref>. A straightforward extension of this theorem is to con sider pairs of independent Markov processes.
Reference: [4] <author> C. N. Fiechter. </author> <title> Efficient reinforcement learning. </title> <booktitle> In Proceedings of the 7th Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 88-97. </pages> <publisher> Morgan Kauffman, </publisher> <address> San Mateo, CA 1994. </address>
Reference-contexts: If we demand a fixed level of performance and allow fl to vary, the required exploration times scale as T ~ t fl . Likewise, for fixed fl, one can relate the agent's performance to the allowed exploration time. This was done in a PAC framework by Fiechter <ref> [4] </ref>. Less is known about MDPs with undiscounted rewards. In this case, the goal of the agent is to find the policy that yields the maximum expected return per unit time.
Reference: [5] <author> D. Haussler, M. Kearns, H. S. Seung, and N. Tishby. </author> <title> Rigorous learning curve bounds from statistical mechanics. </title> <booktitle> In Proceedings of the 7th Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 76-87. </pages> <publisher> Morgan Kauffman, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: For MDPs, this is the combined limit that the allowed exploration time, T , and the size of the state space, N , grow to infinity at a fixed rate: T ! 1; N ! 1; T =N = ff (finite). Ref. <ref> [5] </ref> gives a rigorous treatment of this method from the viewpoint of computational learning theory. Though formulated originally for problems in supervised learning, it can also be used to study problems in decision and control. Of course, important differences between these two types of problems must be kept in mind. <p> Since it is this second goal we wish to focus on, we will study an algorithm that has unlimited resources for search, but limited resources for policy evaluation. The so-called Gibbs algorithm <ref> [5] </ref> works as follows. <p> Large values of ff ensure that each policy evaluation is based on enough steps to perform this exploration. Let us sketch why the combined limit in eq. (39) is necessary for interesting learning behavior <ref> [5] </ref>. Recall that the Gibbs algorithm estimates each value function v by a random walk of length T , then outputs the policy with the best empirical return, ^v .
Reference: [6] <author> K. Huang. </author> <title> Statistical Mechanics. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: The entropy can be calculated from eq. (24) by rewriting the sum as an integral and using the method of saddle-point integration <ref> [6] </ref>. A calculation similar to the one in ref. [8] gives: s (; !) = min Z fi (26) Given a reward distribution (r), eq. (26) can be solved numerically for s (; !). <p> Borrowing terminology from statistical mechanics, we will refer to matrices of the form e X i P ij as transfer matrices <ref> [6] </ref>. The rate of decay "(ffi) is also known as the large deviation rate function [3]. A straightforward extension of this theorem is to con sider pairs of independent Markov processes.
Reference: [7] <author> M. Marcus and H. </author> <title> Minc. A survey of matrix theory and matrix inequalities. </title> <publisher> Dover, </publisher> <address> New York, </address> <year> 1992. </year>
Reference: [8] <author> L. K. Saul and S. P. Singh. </author> <title> Markov decision pro cesses in large state spaces. </title> <booktitle> In Proceedings of the 8th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 281-288. </pages> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: This limit seems appropriate, since it is precisely for large state spaces that stochastic approximations are necessary to solve MDPs. This paper extends earlier work <ref> [8] </ref> in which we introduced a thermodynamic limit for MDPs with discounted rewards. Focusing on undiscounted rewards has enabled us to derive much stronger results. <p> The entropy can be calculated from eq. (24) by rewriting the sum as an integral and using the method of saddle-point integration [6]. A calculation similar to the one in ref. <ref> [8] </ref> gives: s (; !) = min Z fi (26) Given a reward distribution (r), eq. (26) can be solved numerically for s (; !). Figure 2 shows some cross-sectional plots of s (; !) for the uniform distribution of rewards in eq. (19).
Reference: [9] <author> H. S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statis tical mechanics of learning from examples. </title> <journal> Physical Review A 45: </journal> <pages> 6056-6091, </pages> <year> 1992. </year>
Reference-contexts: Hence, the effective number of accessible states may be much smaller than the size of the state space. These considerations do not apply to MDPs with undiscounted rewards. Our analysis employs a particular limiting method|the so-called thermodynamic limit|developed in the statistical physics literature <ref> [9, 12] </ref>. For MDPs, this is the combined limit that the allowed exploration time, T , and the size of the state space, N , grow to infinity at a fixed rate: T ! 1; N ! 1; T =N = ff (finite).
Reference: [10] <author> S. P. Singh. </author> <title> Reinforcement learning algorithms for average-payoff markovian decision problems. </title> <booktitle> In Proceedings of the 12th National Conference on Ar tificial Intelligence, </booktitle> <pages> pages 700-706. </pages> <publisher> AAAI Press, </publisher> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: This detailed picture of learning is to be contrasted with the much weaker statement that optimal control emerges in the limit that the agent visits each state infinitely often. Yet even this weaker statement remains an open question for many simulation-based algorithms used to solve MDPs with undiscounted rewards <ref> [10] </ref>. An important lesson from previous work in supervised learning is that the shapes of learning curves are not universal and vary from problem to problem.
Reference: [11] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: In these cases, one may devise stochastic approximations <ref> [1, 11, 13] </ref> to these algorithms that estimate the required statistics rather than computing them exactly. These approximations rely on stochastic exploration of the state space to measure the returns attached to particular courses of action.
Reference: [12] <author> T. Watkin, A. Rau, and M. Biehl. </author> <title> The statistical mechanics of learning a rule. </title> <journal> Reviews of Modern Physics 65 </journal> <pages> 499-556, </pages> <year> 1993. </year>
Reference-contexts: Hence, the effective number of accessible states may be much smaller than the size of the state space. These considerations do not apply to MDPs with undiscounted rewards. Our analysis employs a particular limiting method|the so-called thermodynamic limit|developed in the statistical physics literature <ref> [9, 12] </ref>. For MDPs, this is the combined limit that the allowed exploration time, T , and the size of the state space, N , grow to infinity at a fixed rate: T ! 1; N ! 1; T =N = ff (finite).
Reference: [13] <author> C. Watkins and P. </author> <title> Dayan. </title> <booktitle> Q-learning. Machine Learning 8: </booktitle> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: In these cases, one may devise stochastic approximations <ref> [1, 11, 13] </ref> to these algorithms that estimate the required statistics rather than computing them exactly. These approximations rely on stochastic exploration of the state space to measure the returns attached to particular courses of action.
References-found: 13

