URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/files/ciNnw96.ps.gz
Refering-URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: LOCAL ADAPTIVE LEARNING ALGORITHMS FOR BLIND SEPARATION OF NATURAL IMAGES  
Author: Andrzej CICHOCKI, W lodzimierz KASPRZAK 
Note: Appeared in journal: Neural Network World Vol.6, No.4, 1996, pp.515-523. Published by: IDG Co., Prague, Czech Republic.  
Abstract: In this paper a neural network approach for reconstruction of natural highly correlated images from linear (additive) mixture of them is proposed. A multi-layer architecture with local on-line learning rules is developed to solve the problem of blind separation of sources. The main motivation for using a multi-layer network instead of a single-layer one is to improve the performance and robustness of separation, while applying a very simple local learning rule, which is biologically plausible. Moreover such architecture with on-chip learning is relatively easy implementable using VLSI electronic circuits. Furthermore it enables the extraction of source signals sequentially one after the other, starting from the strongest signal and finishing with the weakest one. The experimental part focuses on separating highly correlated human faces from mixture of them, with additive noise and under unknown number of sources. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.-I. Amari, A. Cichocki, and H.H. Yang. </author> <title> A new learning algorithm for blind signal separation. </title> <booktitle> In NIPS'95, </booktitle> <volume> vol. </volume> <pages> 8, </pages> <address> 1996, </address> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <note> (in print). </note>
Reference-contexts: 1. Introduction Blind signal processing and especially blind separation of sources <ref> [1, 2, 3, 4, 5, 6, 7] </ref>, is a new emerging field of research with many potential applications in science and technology. <p> The synaptic weight matrix in each layer can be updated according to the on-line nonlinear global learning rule <ref> [1, 2, 8] </ref>: or alternatively by the simplified local learning rule [9]: W (t + 1) = W (t) + (t)[I f (y (t))g T (y (t))]; (2) which can be written in an equivalent scalar form as: w ij (t + 1) = w ij (t) + (t)[ffi ij f <p> In order to improve the performance of separation (reduction of cross-talking and noise) we iterate the separation, but applying different activation functions in next steps than in the first (multi-)layer. Typical activation functions are <ref> [1, 4, 12] </ref>: 1. for separation of signals with negative kurtosis: f (y) = y 3 g (y) = y; f (y) = y g (y) = tanh (10y); f (y) = y 3 g (y) = tanh (10y); f (y) = 1=6y 7 0:5y 5 0:5y 3 g (y) =
Reference: [2] <author> S.-I. Amari, A. Cichocki, and H.H. Yang. </author> <title> Recurrent neural networks for blind separation of sources. </title> <booktitle> In Proceedings. NOLTA'95, NTA Research Society of IEICE, </booktitle> <address> Tokyo, Japan, </address> <year> 1995, </year> <month> 37-42. </month> <title> 522 Cichocki: Local adaptive learning algorithms </title>
Reference-contexts: 1. Introduction Blind signal processing and especially blind separation of sources <ref> [1, 2, 3, 4, 5, 6, 7] </ref>, is a new emerging field of research with many potential applications in science and technology. <p> Most of the developed methods provide large cross-talking or even fail to separate sources if many signals of similar distributions are mixed together or when the problem is ill-posed (i.e. the mixing matrix is near singular). In such a case even robust algorithms (e.g. <ref> [2, 3, 8] </ref>) may not be able to separate the sources with considerable quality. The main assumption of this paper is that a single layer is not able optimally to solve this difficult problem and therefore a multi-layer architecture is required instead. <p> The synaptic weight matrix in each layer can be updated according to the on-line nonlinear global learning rule <ref> [1, 2, 8] </ref>: or alternatively by the simplified local learning rule [9]: W (t + 1) = W (t) + (t)[I f (y (t))g T (y (t))]; (2) which can be written in an equivalent scalar form as: w ij (t + 1) = w ij (t) + (t)[ffi ij f <p> Four face images have been successfully separated in first three layers with a highly nonlinear activation function. In the next two layers the two activation functions f ; g have been replaced by more linear ones. This allows an improvement of the performance index P I <ref> [2, 6] </ref>. Already after two such layers the best possible separation is nearly reached. In the second experiment four original images Susie, Lenna, Ali, Cindy, were mixed by ill-conditioned matrices A, which are assumed to be unknown.
Reference: [3] <author> J.F. Cardoso, A. Belouchrani, and B. Laheld. </author> <title> A new composite criterion for adaptive and iterative blind source separation. </title> <booktitle> In Proceedings ICASSP-94, vol.4, </booktitle> <year> 1994, </year> <pages> 273-276. </pages>
Reference-contexts: 1. Introduction Blind signal processing and especially blind separation of sources <ref> [1, 2, 3, 4, 5, 6, 7] </ref>, is a new emerging field of research with many potential applications in science and technology. <p> Most of the developed methods provide large cross-talking or even fail to separate sources if many signals of similar distributions are mixed together or when the problem is ill-posed (i.e. the mixing matrix is near singular). In such a case even robust algorithms (e.g. <ref> [2, 3, 8] </ref>) may not be able to separate the sources with considerable quality. The main assumption of this paper is that a single layer is not able optimally to solve this difficult problem and therefore a multi-layer architecture is required instead. <p> On basis of signals extracted in the first layer one usually has some rough information about source signals. By estimating probability density functions (pdf) of the output signals one could choose suboptimal nonlinearities for use in the next layer <ref> [3, 5, 6] </ref>. Due to limit of space details are out of scope of this paper. The time of learning is steadily decreasing from layer to layer. Actually the same effect can be achieved if the learning rate is faster decreasing to zero from layer to layer.
Reference: [4] <author> Ch. Jutten and J. Herault. </author> <title> Blind separation of sources. An adaptive algorithm based on neuromimetic architecture. </title> <booktitle> Signal Processing, </booktitle> <volume> 24(1991), No. 1, </volume> <pages> 1-31. </pages>
Reference-contexts: 1. Introduction Blind signal processing and especially blind separation of sources <ref> [1, 2, 3, 4, 5, 6, 7] </ref>, is a new emerging field of research with many potential applications in science and technology. <p> In order to improve the performance of separation (reduction of cross-talking and noise) we iterate the separation, but applying different activation functions in next steps than in the first (multi-)layer. Typical activation functions are <ref> [1, 4, 12] </ref>: 1. for separation of signals with negative kurtosis: f (y) = y 3 g (y) = y; f (y) = y g (y) = tanh (10y); f (y) = y 3 g (y) = tanh (10y); f (y) = 1=6y 7 0:5y 5 0:5y 3 g (y) =
Reference: [5] <author> A.J. Bell and T.J. Sejnowski. </author> <title> An information-maximization approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7(1995), </volume> <pages> 1129-1159. </pages>
Reference-contexts: 1. Introduction Blind signal processing and especially blind separation of sources <ref> [1, 2, 3, 4, 5, 6, 7] </ref>, is a new emerging field of research with many potential applications in science and technology. <p> On basis of signals extracted in the first layer one usually has some rough information about source signals. By estimating probability density functions (pdf) of the output signals one could choose suboptimal nonlinearities for use in the next layer <ref> [3, 5, 6] </ref>. Due to limit of space details are out of scope of this paper. The time of learning is steadily decreasing from layer to layer. Actually the same effect can be achieved if the learning rate is faster decreasing to zero from layer to layer.
Reference: [6] <author> E. Moreau and O. Macchi. </author> <title> A complex self-adaptive algorithm for source separation based on high order contrasts. </title> <booktitle> In Proceedings. </booktitle> <institution> EUSIPCO-94, Lausanne, EUSIP Association, </institution> <year> 1994, </year> <pages> 1157-1160. </pages>
Reference-contexts: 1. Introduction Blind signal processing and especially blind separation of sources <ref> [1, 2, 3, 4, 5, 6, 7] </ref>, is a new emerging field of research with many potential applications in science and technology. <p> On basis of signals extracted in the first layer one usually has some rough information about source signals. By estimating probability density functions (pdf) of the output signals one could choose suboptimal nonlinearities for use in the next layer <ref> [3, 5, 6] </ref>. Due to limit of space details are out of scope of this paper. The time of learning is steadily decreasing from layer to layer. Actually the same effect can be achieved if the learning rate is faster decreasing to zero from layer to layer. <p> Four face images have been successfully separated in first three layers with a highly nonlinear activation function. In the next two layers the two activation functions f ; g have been replaced by more linear ones. This allows an improvement of the performance index P I <ref> [2, 6] </ref>. Already after two such layers the best possible separation is nearly reached. In the second experiment four original images Susie, Lenna, Ali, Cindy, were mixed by ill-conditioned matrices A, which are assumed to be unknown.
Reference: [7] <author> A. Cichocki and R. Unbehauen. </author> <title> Neural Networks for Optimization and Signal Processing. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: 1. Introduction Blind signal processing and especially blind separation of sources <ref> [1, 2, 3, 4, 5, 6, 7] </ref>, is a new emerging field of research with many potential applications in science and technology.
Reference: [8] <author> A. Cichocki, R. Unbehauen, L. Moszczynski, and E. Rummert. </author> <title> A new on-line adaptive learning algorithm for blind separation of source signals. </title> <booktitle> In Proceedings. </booktitle> <address> ISANN-94, Taiwan, </address> <year> 1994, </year> <pages> 406-411. </pages>
Reference-contexts: Most of the developed methods provide large cross-talking or even fail to separate sources if many signals of similar distributions are mixed together or when the problem is ill-posed (i.e. the mixing matrix is near singular). In such a case even robust algorithms (e.g. <ref> [2, 3, 8] </ref>) may not be able to separate the sources with considerable quality. The main assumption of this paper is that a single layer is not able optimally to solve this difficult problem and therefore a multi-layer architecture is required instead. <p> The synaptic weight matrix in each layer can be updated according to the on-line nonlinear global learning rule <ref> [1, 2, 8] </ref>: or alternatively by the simplified local learning rule [9]: W (t + 1) = W (t) + (t)[I f (y (t))g T (y (t))]; (2) which can be written in an equivalent scalar form as: w ij (t + 1) = w ij (t) + (t)[ffi ij f
Reference: [9] <author> A. Cichocki, W. Kasprzak, and S. Amari. </author> <title> Multi-layer neural networks with a local adaptive learning rule for blind separation of source signals. </title> <booktitle> In Proceedings. NOLTA'95, NTA Research Society of IEICE, </booktitle> <address> Tokyo, Japan, </address> <year> 1995, </year> <pages> 61-65. </pages>
Reference-contexts: We propose and test two types of learning algorithms for a multi-layer artificial neural network (ANN). The first algorithm is based on a recently developed adaptive local learning rule <ref> [9] </ref>. <p> The synaptic weight matrix in each layer can be updated according to the on-line nonlinear global learning rule [1, 2, 8]: or alternatively by the simplified local learning rule <ref> [9] </ref>: W (t + 1) = W (t) + (t)[I f (y (t))g T (y (t))]; (2) which can be written in an equivalent scalar form as: w ij (t + 1) = w ij (t) + (t)[ffi ij f (y i (t))g (y j (t))]: (3) In these formulas (t) <p> Four original images - Susie, Gaussian noise, Stripes, Baboon, given in top row of Fig. 3 were mixed by ill-conditioned matrices A which are assumed to be unknown. Quantitative results are provided in Tab. 2. It contains performance factors for signal separation if a multi-layer local rule learning method <ref> [9] </ref> with a post-processing redundancy elimination layer was applied. After PI Susie Stripes Baboon layer [] NMSE PSNR NMSE PSNR NMSE PSNR L1 3.194 - 0.2120 6.73 0.0385 23.50 L3 0.532 0.0116 30.18 0.0016 37.20 0.0171 17.68 Tab. 2.
Reference: [10] <author> J. Karhunen and J. Joutsensalo. </author> <title> Representation and separation of signals using nonlinear pca type learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(1994), No. 1, </volume> <pages> 113-127. </pages>
Reference-contexts: For this task we propose an additional post-processing layer with a suitable local learning rule. 3. Multi-Layer Neural Network 3.1 Learning rules for independent component analysis It is well known that pre-whitening, i.e. preprocessing or decorrelation, is usually necessary for any nonlinear PCA or PSA based separation <ref> [10, 11] </ref>. In this section we consider two different learning algorithms where each of them is at the same time performing a generalized (nonlinear) pre-whitening (called also sphering or normalized orthogonalization) and the required blind separation.
Reference: [11] <author> E. Oja and J. Karhunen. </author> <title> Signal separation by nonlinear hebbian learning. </title> <booktitle> In Proceedings ICNN-95, </booktitle> <address> Perth, Australia, </address> <year> 1995, </year> <pages> 417-421. </pages>
Reference-contexts: For this task we propose an additional post-processing layer with a suitable local learning rule. 3. Multi-Layer Neural Network 3.1 Learning rules for independent component analysis It is well known that pre-whitening, i.e. preprocessing or decorrelation, is usually necessary for any nonlinear PCA or PSA based separation <ref> [10, 11] </ref>. In this section we consider two different learning algorithms where each of them is at the same time performing a generalized (nonlinear) pre-whitening (called also sphering or normalized orthogonalization) and the required blind separation.
Reference: [12] <author> A. Cichocki, R. Bogner, and L. Moszczynski. </author> <title> Improved adaptive algorithms for blind separation of sources. </title> <booktitle> In Proceedings of 18-th Conference on Circuit Theory and Electronic Circuits (KKTOiUE-95), </booktitle> <address> Polana Zgorzelisko, Poland, </address> <year> 1995, </year> <pages> 648-652. 523 </pages>
Reference-contexts: In order to improve the performance of separation (reduction of cross-talking and noise) we iterate the separation, but applying different activation functions in next steps than in the first (multi-)layer. Typical activation functions are <ref> [1, 4, 12] </ref>: 1. for separation of signals with negative kurtosis: f (y) = y 3 g (y) = y; f (y) = y g (y) = tanh (10y); f (y) = y 3 g (y) = tanh (10y); f (y) = 1=6y 7 0:5y 5 0:5y 3 g (y) =
References-found: 12

