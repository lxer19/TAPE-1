URL: http://www.cs.helsinki.fi/~tirri/ml96.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~tirri/publications.html
Root-URL: 
Email: fHenry.Tirri,Petri.Kontkanen,Petri.Myllymakig@cs.Helsinki.FI  
Phone: 26,  
Title: Probabilistic Instance-Based Learning  
Author: Henry Tirri Petri Kontkanen Petri Myllymaki 
Note: Pp. 507-515 in Machine Learning: Proceedings of the Thirteenth International Conference, edited by L. Saitta. Morgan Kaufmann Publishers,  
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  San Francisco, CA, 1996.  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Abstract: Traditional instance-based learning methods base their predictions directly on (training) data that has been stored in the memory. The predictions are based on weighting the contributions of the individual stored instances by a distance function implementing a domain-dependent similarity metrics. This basic approach suffers from three drawbacks: com-putationally expensive prediction when the database grows large, overfitting in the presence of noisy data, and sensitivity to the selection of a proper distance function. We address all these issues by giving a probabilistic interpretation to instance-based learning, where the goal is to approximate predictive distributions of the attributes of interest. In this probabilistic view the instances are not individual data items but probability distributions, and we perform Bayesian inference with a mixture of such prototype distributions. We demonstrate the feasibility of the method empirically for a wide variety of public domain classification data sets.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. </author> <year> (1990). </year> <title> A Study of Instance-Based Algorithms for Supervised Learning Tasks: Mathematical, Empirical, an Psychological Observations. </title> <type> PhD thesis, </type> <institution> University of California, Irvine. </institution>
Reference-contexts: Finally, the performance of the instance-based method is sensitive to the selection of a proper, possibly varying, distance function (Friedman, 1994; Atkeson et al., 1995). The high run-time costs due to storing of all the data has lead to methods that attempt to find a smaller set of "prototypes" <ref> (Aha, 1990) </ref> that represent the data set without sacrificing prediction accuracy. This "reference selection problem" has been addressed by pruning (i.e., storing typical instances) (Zhang, 1992), by exploiting domain knowledge (Kurtzberg, 1987) and by stochastic techniques that perform search in the space of sets of prototypes (Skalak, 1994). <p> Attempts to avoid overfitting have been based on pruning reference items that cause misclassifications and by storing abstractions of instances <ref> (Aha, 1990) </ref>. The problem of proper selection of a distance function is extensively studied in (Friedman, 1994). <p> Therefore usually the joint distribution can be approximated by a simpler mixture of distributions by giving a weighted sum of "cluster distributions", each of which gives the marginal attribute probability distributions conditioned by the cluster index. In fact, construction of prototypes by averaging <ref> (Aha, 1990) </ref> can be understood as coarse grain approximations for such simpler mixture structures.

Reference: <author> Atkeson, C., Moore, A., and Schaal, S. </author> <year> (1995). </year> <title> Locally weighted learning. </title> <note> AI Review to appear. </note>
Reference-contexts: This type of algorithms are often called "lazy" as they defer all the essential computation until the prediction phase. Examples of instance-based learning methods are k-nearest neighbor (Cover and Hart, 1967), kernel regression (Franke, 1982) and locally weighted regression (for a recent survey see <ref> (Atkeson et al., 1995) </ref>). Some neural network approaches can also be viewed as instance-based methods, e.g., the family of Radial Basis Function networks (Moody and Darken, 1989) and Probabilistic networks (Specht, 1990). This basic approach of storing all the data items at the learning phase suffers from several drawbacks.
Reference: <author> Cestnik, B. and Bratko, I. </author> <year> (1991). </year> <title> On estimating probabilities in tree pruning. </title> <editor> In Kodratoff, Y., editor, </editor> <booktitle> Machine Learning EWSL-91, </booktitle> <pages> pages 138-150. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: (% correct) GLASS 1Rw in (7) 62.2 Flexible Bayes in (2) 66.2 Naive Bayes in (10) 71.5 C4.5 in (11) 71.6 D-SIDE 87.4 references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = <ref> (Cestnik and Bratko, 1991) </ref>, (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 511 50 55 60 65 70 75 80 85 90 95 Success <p> in (7) 78.4 Assistant in (7) 79.0 MSG in (9) 80.4 TAN in (9) 85.0 Naive Bayes in (7) 85.1 D-SIDE 86.6 The references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = <ref> (Cestnik and Bratko, 1991) </ref>, (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 512 Table 1: The datasets and testing methods used in our experiments.
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., and Freeman, D. </author> <year> (1988). </year> <title> Autoclass: A Bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 54-64, </pages> <address> Ann Arbor. </address>
Reference-contexts: This combination of finite mixtures with Bayesian model selection is akin to the approach adopted in the Autoclass system <ref> (Cheeseman et al., 1988) </ref> with the notable difference in our focus to prediction rather than latent class analysis.
Reference: <author> Cover, T. and Hart, P. </author> <year> (1967). </year> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13 </volume> <pages> 21-27. </pages>
Reference-contexts: The predictions of the individual items are then combined, for example by using averaging. This type of algorithms are often called "lazy" as they defer all the essential computation until the prediction phase. Examples of instance-based learning methods are k-nearest neighbor <ref> (Cover and Hart, 1967) </ref>, kernel regression (Franke, 1982) and locally weighted regression (for a recent survey see (Atkeson et al., 1995)). Some neural network approaches can also be viewed as instance-based methods, e.g., the family of Radial Basis Function networks (Moody and Darken, 1989) and Probabilistic networks (Specht, 1990).
Reference: <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1) </volume> <pages> 1-38. </pages>
Reference-contexts: c k ) = K 1 P (A i = a il jC = c k ) = n i 1 : Naturally, the parameters h k and f kil are not known, but they can be regarded as missing data and we estimate them by using the EM algorithm <ref> (Dempster et al., 1977) </ref>. The EM algorithm is an iterative algorithm, which monotonically increases the expected value of the posterior corresponding to incomplete data.
Reference: <author> Deng, K. and Moore, A. </author> <year> (1995). </year> <title> Multiresolution instance-based learning. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1233-1239. </pages>
Reference-contexts: This "reference selection problem" has been addressed by pruning (i.e., storing typical instances) (Zhang, 1992), by exploiting domain knowledge (Kurtzberg, 1987) and by stochastic techniques that perform search in the space of sets of prototypes (Skalak, 1994). A somewhat different approach is presented in <ref> (Deng and Moore, 1995) </ref> where the data items are grouped by using kd-trees thus allowing predictions with costs proportional to the number of groups instead of the number of individual ele 507 ments.
Reference: <author> Everitt, B. and Hand, D. </author> <year> (1981). </year> <title> Finite Mixture Distributions. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Franke, R. </author> <year> (1982). </year> <title> Scattered data interpolation: Test of some methods. </title> <journal> Mathematics of Computation, </journal> <volume> 38(157). </volume>
Reference-contexts: The predictions of the individual items are then combined, for example by using averaging. This type of algorithms are often called "lazy" as they defer all the essential computation until the prediction phase. Examples of instance-based learning methods are k-nearest neighbor (Cover and Hart, 1967), kernel regression <ref> (Franke, 1982) </ref> and locally weighted regression (for a recent survey see (Atkeson et al., 1995)). Some neural network approaches can also be viewed as instance-based methods, e.g., the family of Radial Basis Function networks (Moody and Darken, 1989) and Probabilistic networks (Specht, 1990).
Reference: <author> Friedman, J. </author> <year> (1994). </year> <title> Flexible metric nearest neighbor classification. </title> <type> Unpublished manuscript. </type> <note> Available by anonymous ftp from Stanford Research Institute (Menlo Park, CA) at playfair.stanford.edu. </note>
Reference-contexts: Attempts to avoid overfitting have been based on pruning reference items that cause misclassifications and by storing abstractions of instances (Aha, 1990). The problem of proper selection of a distance function is extensively studied in <ref> (Friedman, 1994) </ref>. The approach presented in this paper is based on the probabilistic viewpoint, where the attributes A i are interpreted as random variables, and the given set of training instances is used to approximate the underlying joint probability distribution of the attributes.
Reference: <author> Friedman, N. and Goldszmidt, M. </author> <year> (1996a). </year> <title> Building classifiers using Bayesian networks. </title> <note> In Proceedings of AAAI-96 (to appear). </note>
Reference-contexts: in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = <ref> (Friedman and Goldszmidt, 1996a) </ref>, (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 511 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in (1) 85.4 ITrule in (1) 86.5 Cal5 in (1) 86.9 SMART in (1) 88.5 NewID in (1) 90.0 <p> in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = <ref> (Friedman and Goldszmidt, 1996a) </ref>, (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 512 Table 1: The datasets and testing methods used in our experiments. name size #attrs #classes #clusters test method default Australian 690 15 2 17 10-fold CV 56.0 Breast cancer 286 10 2 21 11-fold CV <p> An interesting observation is that the probabilistic instance-based method outperforms also all other Bayesian approaches present in the StatLog comparison as well as more recent Naive Bayes related algorithms TAN (Tree Augmented Naive Bayes) and NBC FSS introduced in <ref> (Friedman and Goldszmidt, 1996a) </ref>. This supports the common hypothesis that many real data distributions can be naturally modeled as a sum of several component distributions. Table 2 summarizes the performance of those methods for which we could find results in 4 or more data sets.
Reference: <author> Friedman, N. and Goldszmidt, M. </author> <year> (1996b). </year> <title> Discretiz-ing continuous attributes while learning Bayesian networks. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference (to appear). </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = <ref> (Friedman and Goldszmidt, 1996b) </ref> and (11) = (Quinlan, 1996). 511 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in (1) 85.4 ITrule in (1) 86.5 Cal5 in (1) 86.9 SMART in (1) 88.5 NewID in (1) 90.0 AC2 in (1) 90.0 Bayes tree <p> (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = <ref> (Friedman and Goldszmidt, 1996b) </ref> and (11) = (Quinlan, 1996). 512 Table 1: The datasets and testing methods used in our experiments. name size #attrs #classes #clusters test method default Australian 690 15 2 17 10-fold CV 56.0 Breast cancer 286 10 2 21 11-fold CV 70.3 Diabetes 768 9 2 20
Reference: <author> Gelman, A., Carlin, J., Stern, H., and Rubin, D. </author> <year> (1995). </year> <title> Bayesian Data Analysis. </title> <publisher> Chapman & Hall. </publisher>
Reference-contexts: In addition, such a prediction computation can be performed efficiently (Myl-lymaki and Tirri, 1995; Myllymaki and Tirri, 1994; Myllymaki and Tirri, 1993). For learning the component distributions from the instance set, we have adopted the Bayesian approach (see e.g., <ref> (Gelman et al., 1995) </ref>) which allows us to make a tradeoff between the complexity of our distribution structure and fit to the data thus resolving the overfitting problem of the traditional instance-based approaches.
Reference: <author> Heckerman, D., Geiger, D., and Chickering, D. </author> <year> (1995). </year> <title> Learning Bayesian networks: The combination of 514 knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243. </pages>
Reference-contexts: From the properties of Dirichlet density and the independence assumptions it follows that the maximal probability values for the parameters fi can be ob 2 For the justification of Dirichlet distributions as priors see e.g., <ref> (Heckerman et al., 1995) </ref>. 509 tained by setting P (C = c k ) = K 1 P (A i = a il jC = c k ) = n i 1 : Naturally, the parameters h k and f kil are not known, but they can be regarded as missing
Reference: <author> Holte, R. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-91. </pages>
Reference-contexts: 71.5 C4.5 in (11) 71.6 D-SIDE 87.4 references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = <ref> (Holte, 1993) </ref>, (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 511 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in (1) 85.4 ITrule in (1) 86.5 Cal5 in (1) 86.9 <p> Bayes in (7) 85.1 D-SIDE 86.6 The references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = <ref> (Holte, 1993) </ref>, (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 512 Table 1: The datasets and testing methods used in our experiments. name size #attrs #classes #clusters test method default Australian 690 15 2 17 10-fold CV 56.0
Reference: <author> John, G. and Langley, P. </author> <year> (1995). </year> <title> Estimating continuous distributions in Bayesian classifiers. </title> <editor> In Be-snard, P. and Hanks, S., editors, </editor> <booktitle> Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 338-345. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS 1Rw in (7) 62.2 Flexible Bayes in (2) 66.2 Naive Bayes in (10) 71.5 C4.5 in (11) 71.6 D-SIDE 87.4 references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = <ref> (John and Langley, 1995) </ref>, (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). <p> in (7) 72.9 C4.5 in (11) 77.0 NBCFSS in (9) 77.7 ID3 in (7) 78.4 Assistant in (7) 79.0 MSG in (9) 80.4 TAN in (9) 85.0 Naive Bayes in (7) 85.1 D-SIDE 86.6 The references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = <ref> (John and Langley, 1995) </ref>, (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996).
Reference: <author> Kass, R. and Raftery, A. </author> <year> (1994). </year> <title> Bayes factors. </title> <type> Technical Report 254, </type> <institution> Department of Statistics, University of Washington. </institution>
Reference-contexts: As discussed in (Rissanen, 1989), the evidence can also be understood as an information theoretic measure called stochastic complexity. This evidence integral is hard to evaluate due to the very large dimensionality of the parameter space, but the evidence can be approximated by using e.g., Laplace's method <ref> (Kass and Raftery, 1994) </ref>. In the experimental results presented in Section 5 this automatic model class selection has not yet been used, instead in the search process the model classes were selected by manual search in the model class space.
Reference: <author> Kononenko, I. </author> <year> (1993). </year> <title> Successive naive Bayesian classifier. </title> <journal> Informatica, </journal> <volume> 17 </volume> <pages> 167-174. </pages>
Reference-contexts: Sometimes these structures coincide, but in the general case the number of components does not need to match the number of values of the class attribute, a difference that distinguishes our approach from the Naive Bayes classifier (see e.g., <ref> (Kononenko, 1993) </ref>) in classification tasks. In this paper we describe a methodology for probabilistic instance-based learning for discrete domains. <p> Naive Bayes in (10) 71.5 C4.5 in (11) 71.6 D-SIDE 87.4 references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = <ref> (Kononenko, 1993) </ref>, (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 511 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in (1) 85.4 ITrule in (1) 86.5 <p> in (9) 85.0 Naive Bayes in (7) 85.1 D-SIDE 86.6 The references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = <ref> (Kononenko, 1993) </ref>, (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 512 Table 1: The datasets and testing methods used in our experiments. name size #attrs #classes #clusters test method default Australian 690 15 2
Reference: <author> Kononenko, I. and Bratko, I. </author> <year> (1991). </year> <title> Information-based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 67-80. </pages>
Reference-contexts: 62.2 Flexible Bayes in (2) 66.2 Naive Bayes in (10) 71.5 C4.5 in (11) 71.6 D-SIDE 87.4 references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = <ref> (Kononenko and Bratko, 1991) </ref>, (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 511 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <p> 79.0 MSG in (9) 80.4 TAN in (9) 85.0 Naive Bayes in (7) 85.1 D-SIDE 86.6 The references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = <ref> (Kononenko and Bratko, 1991) </ref>, (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 512 Table 1: The datasets and testing methods used in our experiments. name size #attrs #classes #clusters test
Reference: <author> Kontkanen, P., Myllymaki, P., and Tirri, H. </author> <year> (1996a). </year> <title> Comparing Bayesian model class selection criteria by discrete finite mixtures. </title> <booktitle> In Proceedings of the ISIS (Information, Statistics and Induction in Science) Conference, </booktitle> <address> Melbourne, Australia. </address> <note> (To appear.). </note>
Reference-contexts: In the experimental results presented in Section 5 this automatic model class selection has not yet been used, instead in the search process the model classes were selected by manual search in the model class space. For more discussion on estimating the evidence in the finite mixture context, see <ref> (Kontkanen et al., 1996a) </ref>. In the second phase of the model construction process, we wish to find the optimal set of parameters for the selected model class M k by maximizing the posterior probability P (fijD).
Reference: <author> Kontkanen, P., Myllymaki, P., and Tirri, H. </author> <year> (1996b). </year> <title> Constructing Bayesian finite mixture models by the EM algorithm. </title> <type> Technical Report C-1996-9, </type> <institution> University of Helsinki, Department of Computer Science. </institution>
Reference-contexts: The EM algorithm is an iterative algorithm, which monotonically increases the expected value of the posterior corresponding to incomplete data. The derivation of the update formulas in our mixture case can be found in <ref> (Kontkanen et al., 1996b) </ref>. 4 Bayesian inference with the mixture model Let us assume that the mixture model M (fi) for the instance space has been constructed by the method described above.
Reference: <author> Kurtzberg, J. </author> <year> (1987). </year> <title> Feature analysis for symbol recognition by elastic matching. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 31 </volume> <pages> 91-95. </pages>
Reference-contexts: This "reference selection problem" has been addressed by pruning (i.e., storing typical instances) (Zhang, 1992), by exploiting domain knowledge <ref> (Kurtzberg, 1987) </ref> and by stochastic techniques that perform search in the space of sets of prototypes (Skalak, 1994).
Reference: <author> Michie, D., Spiegelhalter, D., and Taylor, C., </author> <title> editors (1994). Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, London. </publisher>
Reference-contexts: The methodology described has been implemented in the D-SIDE software package. We present empirical results of the method's classification prediction performance for a set of public domain data sets (including data sets from the StatLog project <ref> (Michie et al., 1994) </ref>), and compare the results to the performance of various other machine learning and neural network methods for the same data sets. <p> The data sets were partly selected on the basis of their reported use, i.e., we have preferred data sets that have been used for testing many different methods over data with only isolated results. Many of the results are from the StatLog project <ref> (Michie et al., 1994) </ref>, but we have also included more recent results. The descriptions of the data sets, our testing procedures, and the best model classes (the number of clusters) found for each data set are given in Table 1. <p> It should be observed that with the exception of the DNA data set, all our results are crossvalidated, and that for the StatLog data sets we have used the same crossvalidation scheme as described in <ref> (Michie et al., 1994) </ref>. The same does not hold for many of the results for the other methods, as in many cases the testing procedure either was not reported, or the best result with a single test set was given. <p> (7) 84.5 D-SIDE 88.0 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS 1Rw in (7) 62.2 Flexible Bayes in (2) 66.2 Naive Bayes in (10) 71.5 C4.5 in (11) 71.6 D-SIDE 87.4 references in the barcharts are as follows: (1) = <ref> (Michie et al., 1994) </ref>, (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, <p> Success rate (% correct) LYMPHOGRAPHY K-NN in (7) 72.9 C4.5 in (11) 77.0 NBCFSS in (9) 77.7 ID3 in (7) 78.4 Assistant in (7) 79.0 MSG in (9) 80.4 TAN in (9) 85.0 Naive Bayes in (7) 85.1 D-SIDE 86.6 The references in the barcharts are as follows: (1) = <ref> (Michie et al., 1994) </ref>, (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt,
Reference: <author> Moody, J. and Darken, C. </author> <year> (1989). </year> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 281-294. </pages>
Reference-contexts: Examples of instance-based learning methods are k-nearest neighbor (Cover and Hart, 1967), kernel regression (Franke, 1982) and locally weighted regression (for a recent survey see (Atkeson et al., 1995)). Some neural network approaches can also be viewed as instance-based methods, e.g., the family of Radial Basis Function networks <ref> (Moody and Darken, 1989) </ref> and Probabilistic networks (Specht, 1990). This basic approach of storing all the data items at the learning phase suffers from several drawbacks. First, the run-time computational costs for such algorithms are high when the size of the data set grows large.
Reference: <author> Moore, A. </author> <year> (1990). </year> <title> Acquisition of dynamic control knowledge for a robotic manipulator. </title> <booktitle> In Seventh International Machine Learning Workshop. </booktitle> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Myllymaki, P. and Tirri, H. </author> <year> (1993). </year> <title> Bayesian case-based reasoning with neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 422-427, </pages> <address> San Francisco. </address> <publisher> IEEE, </publisher> <address> Piscataway, NJ. </address>
Reference: <author> Myllymaki, P. and Tirri, H. </author> <year> (1994). </year> <title> Massively parallel case-based reasoning with probabilistic similarity metrics. </title> <editor> In Wess, S., Althoff, K.-D., and Richter, M., editors, </editor> <booktitle> Topics in Case-Based Reasoning, volume 837 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 144-154. </pages> <publisher> Springer-Verlag. </publisher>
Reference: <author> Myllymaki, P. and Tirri, H. </author> <year> (1995). </year> <title> Constructing com-putationally efficient Bayesian models via unsupervised clustering. </title> <editor> In A.Gammerman, editor, </editor> <booktitle> Probabilistic Reasoning and Bayesian Belief Networks, </booktitle> <pages> pages 237-248. </pages> <publisher> Alfred Waller Publishers, </publisher> <address> Suffolk. </address>
Reference: <author> Quinlan, J. </author> <year> (1996). </year> <title> Improved use of continuous attributes in C4.5. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 77-90. </pages>
Reference-contexts: = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = <ref> (Quinlan, 1996) </ref>. 511 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in (1) 85.4 ITrule in (1) 86.5 Cal5 in (1) 86.9 SMART in (1) 88.5 NewID in (1) 90.0 AC2 in (1) 90.0 Bayes tree in (1) 90.5 Backprop in (1) 91.2 <p> = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = (Skalak, 1994), (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = <ref> (Quinlan, 1996) </ref>. 512 Table 1: The datasets and testing methods used in our experiments. name size #attrs #classes #clusters test method default Australian 690 15 2 17 10-fold CV 56.0 Breast cancer 286 10 2 21 11-fold CV 70.3 Diabetes 768 9 2 20 12-fold CV 65.0 DNA 3186 181 3
Reference: <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company, </publisher> <address> New Jersey. </address>
Reference-contexts: Assuming equal priors for the model classes, they can be ranked by evaluating the evidence P (DjM k ) for each model class, P (DjM k ) = P (Djfi; M k )P (fijM k ) dfi; where the integration goes over the whole parameter space. As discussed in <ref> (Rissanen, 1989) </ref>, the evidence can also be understood as an information theoretic measure called stochastic complexity. This evidence integral is hard to evaluate due to the very large dimensionality of the parameter space, but the evidence can be approximated by using e.g., Laplace's method (Kass and Raftery, 1994).
Reference: <author> Scott, D. </author> <year> (1992). </year> <title> Multivariate Density Estimation. Theory, Practice, and Visualization. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: Therefore the basic instance-based approach in probabilistic terms can be understood as a form of kernel density estimators (see e.g., <ref> (Scott, 1992) </ref>), and is thus similar to Radial Basis Function network based estimation. In practice a case where all the instance distributions are needed for a good approximation is very extreme as the instance set usually exhibits some cluster structure.
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 293-301. </pages>
Reference-contexts: This "reference selection problem" has been addressed by pruning (i.e., storing typical instances) (Zhang, 1992), by exploiting domain knowledge (Kurtzberg, 1987) and by stochastic techniques that perform search in the space of sets of prototypes <ref> (Skalak, 1994) </ref>. A somewhat different approach is presented in (Deng and Moore, 1995) where the data items are grouped by using kd-trees thus allowing predictions with costs proportional to the number of groups instead of the number of individual ele 507 ments. <p> 71.6 D-SIDE 87.4 references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = <ref> (Skalak, 1994) </ref>, (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 511 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in (1) 85.4 ITrule in (1) 86.5 Cal5 in (1) 86.9 SMART in (1) 88.5 <p> D-SIDE 86.6 The references in the barcharts are as follows: (1) = (Michie et al., 1994), (2) = (John and Langley, 1995), (3) = (Aha et al., 1991), (4) = (Cestnik and Bratko, 1991), (5) = (Kononenko and Bratko, 1991), (6) = (Kononenko, 1993), (7) = (Holte, 1993), (8) = <ref> (Skalak, 1994) </ref>, (9) = (Friedman and Goldszmidt, 1996a), (10) = (Friedman and Goldszmidt, 1996b) and (11) = (Quinlan, 1996). 512 Table 1: The datasets and testing methods used in our experiments. name size #attrs #classes #clusters test method default Australian 690 15 2 17 10-fold CV 56.0 Breast cancer 286 10
Reference: <author> Specht, D. </author> <year> (1990). </year> <title> Probabilistic neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 109-118. </pages>
Reference-contexts: Some neural network approaches can also be viewed as instance-based methods, e.g., the family of Radial Basis Function networks (Moody and Darken, 1989) and Probabilistic networks <ref> (Specht, 1990) </ref>. This basic approach of storing all the data items at the learning phase suffers from several drawbacks. First, the run-time computational costs for such algorithms are high when the size of the data set grows large.
Reference: <author> Stanfill, C. and Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228. </pages>
Reference: <author> Titterington, D., Smith, A., and Makov, U. </author> <year> (1985). </year> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference: <author> Zhang, J. </author> <year> (1992). </year> <title> Selecting typical instances in instance-based learning. </title> <booktitle> In Proceedings of the Ninth International Machine Learning Workshop, </booktitle> <pages> pages 470-479, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 515 </pages>
Reference-contexts: The high run-time costs due to storing of all the data has lead to methods that attempt to find a smaller set of "prototypes" (Aha, 1990) that represent the data set without sacrificing prediction accuracy. This "reference selection problem" has been addressed by pruning (i.e., storing typical instances) <ref> (Zhang, 1992) </ref>, by exploiting domain knowledge (Kurtzberg, 1987) and by stochastic techniques that perform search in the space of sets of prototypes (Skalak, 1994).
References-found: 36

