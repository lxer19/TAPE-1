URL: ftp://ftp.cs.ucsd.edu/pub/scg/papers/kohn_thesis_95.ps.gz
Refering-URL: http://www.cs.ucsd.edu/groups/hpcl/scg/tr.html
Root-URL: http://www.cs.ucsd.edu
Title: A Parallel Software Infrastructure for Dynamic Block-Irregular Scientific Calculations  
Author: Scott R. Kohn 
Abstract-found: 0
Intro-found: 1
Reference: <editor> One of the problems of being a pioneer is you always make mistakes and I never, </editor> <title> never want to be a pioneer. It's always best to come second when you can look at the mistakes the pioneers made. | Seymour Cray </title>
Reference: [1] <author> F. F. Abraham, D. Brodbeck, R. A. Rafey, and W. E. Rudge, </author> <title> Instability dynamics of fracture: A computer simulation investigation, </title> <journal> Physical Review Letters, </journal> <month> 73 </month> <year> (1994). </year>
Reference-contexts: Using our software infrastructure, we have developed a 3d smoothed particle hydrodynamics [108] code (SPH3D) that simulates the evolution of galactic bodies in astrophysics, and we are currently developing a 3d molecular dynamics application (MD) to study fracture dynamics in solids <ref> [1] </ref>. 1.4 Organization of the Dissertation Sixty minutes of thinking of any kind is bound to lead to confusion and unhappiness. | James Thurber This dissertation is organized into six chapters. Each of Chapters 2 through 5 covers a portion of the software infrastructure shown in Figure 1.2. <p> Abraham (IBM Almaden), we are using our particle library to develop a molecular dynamics application to study fracture dynamics in solids <ref> [1] </ref>. In addition, our software has been used to teach undergraduate and graduate courses in computational science at the University of California at San Diego. I'm not going to school anymore.
Reference: [2] <author> G. Agha, </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems, </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The notion of communicating objects, or "actors," was first described by Hewitt [82] and then further developed by Agha <ref> [2] </ref>. Actors are concurrent objects which communicate with each other via messages. Actors execute in response to messages, and each actor object may contain several concurrently executing tasks. Actor-based languages include ABCL [145], Cantor [11], and Charm++ [90].
Reference: [3] <author> G. Agrawal, A. Sussman, and J. Saltz, </author> <title> An integrated runtime and compile-time approach for parallelizing structured and block structured applications, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> (to appear). </note>
Reference-contexts: Such applications will also require sophisticated run-time support to manage changing data distributions and communication patterns. A number of run-time support systems have already been developed, including CHAOS (formerly called PARTI) [60], multiblock PARTI <ref> [3] </ref>, and Multipol [40]. Both CHAOS and multiblock PARTI have been used as run-time support for data parallel Fortran compilers. CHAOS has been very successful in addressing unstructured problems such as sparse linear algebra and finite elements [58]. <p> The goal of SA is to unify several previous domain-specific systems, including LPARX, multiblock PARTI <ref> [3] </ref>, and CHAOS [60]. 2.4.2 Parallel Languages The parallel programming literature describes numerous languages, each of which provides facilities specialized for its own intended class of applications. In the following survey, we evaluate various parallel languages on their ability to solve the dynamic, block-irregular problems targeted by LPARX. <p> While the automatic detection of parallelism is extremely attractive, these languages have not yet demonstrated that the compiler alone can extract sufficient information from the program to efficiently distribute data for dynamic, irregular problems on message passing architectures. 2.4.3 Run-Time Support Libraries The CHAOS (formerly PARTI) [60] and multiblock PARTI <ref> [3] </ref> libraries provide run-time support for data parallel compilers such as HPF and Fortran D [139]. Both libraries support a "inspector/executor" model for scheduling communication at run-time. <p> How do we keep track of the vast number of schedules in complicated dynamic applications? The structured adaptive mesh calculation described in Chapter 4 would require perhaps forty different active communication schedules. Such bookkeeping facilities are not provided by CHAOS [60] and multiblock PARTI <ref> [3] </ref>, which assume that schedules are managed either by the compiler or the user. Since communication dependencies change, schedules will need to change. <p> The Structural Abstraction (SA) model [13] extends the LPARX ideas to other classes of irregular scientific applications. SA has not yet been implemented, however, and its implementation will require the unification of three different run-time support libraries: LPARX, CHAOS [60], and multiblock PARTI <ref> [3] </ref>. The acceptance of High Performance Fortran by the scientific computing 51 community introduces a number of interesting research issues. <p> LPARX applications alternate between communication and computation phases. Thus, communication is not asynchronous but is, in fact, limited to the well-defined communication phases of the program. Furthermore, we can predict 85 global communication patterns using the inspector/executor paradigm pioneered in CHAOS [60] and multiblock PARTI <ref> [3] </ref>. In this model of communication, the inspector phase calculates a schedule of data motion which is then executed in the executor phase. The LPARX schedule building loop|the inspector|would employ the region calculus and copy-on-intersect operations (i.e. structural abstraction) to specify data dependencies. <p> particle representations provided by our library (see Figure 5.11): * Define a C ++ class called Particle that includes all physical information needed to characterize a particle, such as position, velocity, acceleration, force, mass, density, or pressure. 164 // Class Particle defines important particle attributes class Particle f double position <ref> [3] </ref>, velocity [3], force [3]; : : : // Class ParticleList represents Particle information as arrays class ParticleList f int number; double *position [3], *velocity [3], *force [3]; : : : // CombineForces (called by WriteBack) adds forces from B into A void CombineForces (ParticleList& A, const ParticleList& B) f for <p> provided by our library (see Figure 5.11): * Define a C ++ class called Particle that includes all physical information needed to characterize a particle, such as position, velocity, acceleration, force, mass, density, or pressure. 164 // Class Particle defines important particle attributes class Particle f double position <ref> [3] </ref>, velocity [3], force [3]; : : : // Class ParticleList represents Particle information as arrays class ParticleList f int number; double *position [3], *velocity [3], *force [3]; : : : // CombineForces (called by WriteBack) adds forces from B into A void CombineForces (ParticleList& A, const ParticleList& B) f for (int i <p> our library (see Figure 5.11): * Define a C ++ class called Particle that includes all physical information needed to characterize a particle, such as position, velocity, acceleration, force, mass, density, or pressure. 164 // Class Particle defines important particle attributes class Particle f double position <ref> [3] </ref>, velocity [3], force [3]; : : : // Class ParticleList represents Particle information as arrays class ParticleList f int number; double *position [3], *velocity [3], *force [3]; : : : // CombineForces (called by WriteBack) adds forces from B into A void CombineForces (ParticleList& A, const ParticleList& B) f for (int i = 0; <p> to characterize a particle, such as position, velocity, acceleration, force, mass, density, or pressure. 164 // Class Particle defines important particle attributes class Particle f double position <ref> [3] </ref>, velocity [3], force [3]; : : : // Class ParticleList represents Particle information as arrays class ParticleList f int number; double *position [3], *velocity [3], *force [3]; : : : // CombineForces (called by WriteBack) adds forces from B into A void CombineForces (ParticleList& A, const ParticleList& B) f for (int i = 0; i &lt; A.number; i++) f A.force [i][1] += B.force [i][1]; A.force [i][2] += B.force [i][2]; A.force [i][3] += B.force <p> a particle, such as position, velocity, acceleration, force, mass, density, or pressure. 164 // Class Particle defines important particle attributes class Particle f double position <ref> [3] </ref>, velocity [3], force [3]; : : : // Class ParticleList represents Particle information as arrays class ParticleList f int number; double *position [3], *velocity [3], *force [3]; : : : // CombineForces (called by WriteBack) adds forces from B into A void CombineForces (ParticleList& A, const ParticleList& B) f for (int i = 0; i &lt; A.number; i++) f A.force [i][1] += B.force [i][1]; A.force [i][2] += B.force [i][2]; A.force [i][3] += B.force [i][3]; g <p> such as position, velocity, acceleration, force, mass, density, or pressure. 164 // Class Particle defines important particle attributes class Particle f double position <ref> [3] </ref>, velocity [3], force [3]; : : : // Class ParticleList represents Particle information as arrays class ParticleList f int number; double *position [3], *velocity [3], *force [3]; : : : // CombineForces (called by WriteBack) adds forces from B into A void CombineForces (ParticleList& A, const ParticleList& B) f for (int i = 0; i &lt; A.number; i++) f A.force [i][1] += B.force [i][1]; A.force [i][2] += B.force [i][2]; A.force [i][3] += B.force [i][3]; g // Packing
Reference: [4] <author> A. Almgren, T. Buttke, and P. Colella, </author> <title> A fast vortex method in three dimensions, </title> <booktitle> in Proceedings of the 10th AIAA Computational Fluid Dynamics Conference, </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1991, </year> <pages> pp. 446-455. </pages>
Reference-contexts: A naive force evaluation scheme would, for a system of N particles, calculate all O (N 2 ) particle-particle interactions directly. Such an approach is too expensive for systems of more than a few thousand particles. Rapid approximation methods <ref> [4, 8, 9, 17, 79] </ref> accelerate the force evaluation by trading some accuracy for speed. Approximation algorithms typically divide the force evaluation into two components: local particle-particle interactions and far-field force computations. <p> the force computation into two components: local particle-particle interactions and far-field force evaluation (not shown). 146 Approximation Algorithm Local Fast PDE Hierarchical Interactions Solver Representations Local Force Approximations [50, 108] p Particle-Particle Particle-Mesh [87] p p Particle-in-Cell (PIC) [87] p Method of Local Corrections (MLC) [8] p p Adaptive MLC <ref> [4, 5] </ref> p p p Fast Multipole Method (FMM) [79] p p Barnes-Hut [17] p p Hierarchical Element Method [9] p p Table 5.1: A survey of the computational structure for various N -body approximation methods. <p> Singh et al. [126, 128] have also implemented a parallel fast multipole algorithm for shared memory multiprocessors. However, these tree-based mechanisms are inappropriate for particle methods that employ fast PDE solvers based on array representations <ref> [4, 8] </ref>. 5.2 Application Programmer Interface The beauty of this is that it is only of theoretical importance, and there is no way it can be of any practical use whatsoever. | Sidney Harris, "Einstein Simplified" Our particle API provides scientists with high-level computational tools that enable easy and efficient portable <p> In Chapter 4 (Adaptive Mesh Applications), we described the techniques and software support required to support fast PDE solvers and structured adaptive hierarchical representations. We plan to combine the techniques of these two chapters to implement a hierarchical particle method such as Almgren's Adaptive Method of Local Corrections (AMLC) <ref> [4, 5] </ref>. AMLC coupled with the power of parallel architectures would enable computational scientists to study vortex dynamics problems with considerably larger numbers of particles. Traditionally, multipole [79] and Barnes-Hut [17] methods have been implemented using unstructured tree codes [140].
Reference: [5] <author> A. S. Almgren, </author> <title> A Fast Adaptive Vortex Method Using Local Corrections, </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: the force computation into two components: local particle-particle interactions and far-field force evaluation (not shown). 146 Approximation Algorithm Local Fast PDE Hierarchical Interactions Solver Representations Local Force Approximations [50, 108] p Particle-Particle Particle-Mesh [87] p p Particle-in-Cell (PIC) [87] p Method of Local Corrections (MLC) [8] p p Adaptive MLC <ref> [4, 5] </ref> p p p Fast Multipole Method (FMM) [79] p p Barnes-Hut [17] p p Hierarchical Element Method [9] p p Table 5.1: A survey of the computational structure for various N -body approximation methods. <p> In Chapter 4 (Adaptive Mesh Applications), we described the techniques and software support required to support fast PDE solvers and structured adaptive hierarchical representations. We plan to combine the techniques of these two chapters to implement a hierarchical particle method such as Almgren's Adaptive Method of Local Corrections (AMLC) <ref> [4, 5] </ref>. AMLC coupled with the power of parallel architectures would enable computational scientists to study vortex dynamics problems with considerably larger numbers of particles. Traditionally, multipole [79] and Barnes-Hut [17] methods have been implemented using unstructured tree codes [140].
Reference: [6] <author> B. Alpern, L. Carter, E. Feig, and T. Selker, </author> <title> The uniform memory hierarchy model of computation, </title> <journal> Algorithmica, </journal> <volume> 12 (1994), </volume> <pages> pp. 72-109. </pages>
Reference-contexts: Unfortunately, this is not always the case <ref> [6, 127] </ref>. Experiments with the Wisconsin Wind Tunnel shared memory simulator [121] indicate that the explicit management of data locality can dramatically improve performance for dynamic scientific applications [65]. <p> Such questions require further study. Part of the difficulty in writing portable parallel programs is that no single, unifying, realistic model of parallel computation and communication has emerged. There have been several attempts to define a unifying model, such as PMH <ref> [6] </ref>, LogP [56], BSP [137], and CTA [129]. However, these models tend to focus on performance evaluation. What is needed is a single set of realistic, portable, general purpose mechanisms for efficient parallel programming. Without high-level support, programmers are left to employ different implementations on different architectures, hampering portability.
Reference: [7] <author> A. L. Ananda, B. H. Tay, and E. K. Koh, Astra: </author> <title> An asynchronous remote procedure call facility, </title> <booktitle> in Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: They build on ideas from the concurrent object oriented programming community. AMS defines a "message stream" abstraction that greatly simplifies the communication of complicated data structures between processors. Its mechanisms combine ideas from asynchronous remote procedure calls <ref> [7, 110] </ref>, Active Messages [138], and the C ++ I/O stream library [106, 131]. <p> For example, DPO's distributed object naming mechanisms and its notion of primary and secondary objects (described in Section 3.2.3) are based in part on the distributed facilities described by Deshpande et al. [61]. The AMS abstractions combine ideas from Active Messages [138], asynchronous remote procedure calls <ref> [7, 110] </ref>, and the C ++ I/O stream model [106, 131]. Another related paradigm developed by the distributed systems community is virtual shared memory, which provides the illusion of a single, shared, coherent address space for systems with physically distributed memories. <p> Our implementation eliminates the costly communication needed to translate object names at the cost of additional, although acceptable, memory overheads (see Section 3.3.2). Active Messages [138] is an asynchronous communication mechanism which, like asynchronous remote procedure calls <ref> [7, 110] </ref>, sends a message to a specified function that executes on message arrival. AMS combines this asynchronous message delivery mechanism with the concept of a message stream [106, 131] to hide message buffer management details. <p> For the past two years at the University of California at San Diego, MP ++ has been used on workstations to teach message passing parallel programming. 3.2.2 Asynchronous Message Streams The Asynchronous Message Stream (AMS) communication paradigm builds on ideas from asynchronous remote procedure calls <ref> [7, 110] </ref>, Active Messages [138], and the C ++ I/O stream library [106, 131]. AMS requires only basic message passing support such as that provided by MPI [105]. Its message stream abstraction frees the programmer from many low-level message passing details.
Reference: [8] <author> C. R. Anderson, </author> <title> A method of local corrections for computing the velocity field due to a distribution of vortex blobs, </title> <journal> Journal of Computational Physics, </journal> <volume> 62 (1986), </volume> <pages> pp. 111-123. 199 </pages>
Reference-contexts: A naive force evaluation scheme would, for a system of N particles, calculate all O (N 2 ) particle-particle interactions directly. Such an approach is too expensive for systems of more than a few thousand particles. Rapid approximation methods <ref> [4, 8, 9, 17, 79] </ref> accelerate the force evaluation by trading some accuracy for speed. Approximation algorithms typically divide the force evaluation into two components: local particle-particle interactions and far-field force computations. <p> Rapid approximation methods typically divide the force computation into two components: local particle-particle interactions and far-field force evaluation (not shown). 146 Approximation Algorithm Local Fast PDE Hierarchical Interactions Solver Representations Local Force Approximations [50, 108] p Particle-Particle Particle-Mesh [87] p p Particle-in-Cell (PIC) [87] p Method of Local Corrections (MLC) <ref> [8] </ref> p p Adaptive MLC [4, 5] p p p Fast Multipole Method (FMM) [79] p p Barnes-Hut [17] p p Hierarchical Element Method [9] p p Table 5.1: A survey of the computational structure for various N -body approximation methods. <p> Singh et al. [126, 128] have also implemented a parallel fast multipole algorithm for shared memory multiprocessors. However, these tree-based mechanisms are inappropriate for particle methods that employ fast PDE solvers based on array representations <ref> [4, 8] </ref>. 5.2 Application Programmer Interface The beauty of this is that it is only of theoretical importance, and there is no way it can be of any practical use whatsoever. | Sidney Harris, "Einstein Simplified" Our particle API provides scientists with high-level computational tools that enable easy and efficient portable
Reference: [9] <author> C. R. Anderson, </author> <title> An implementation of the fast multipole method without multipoles, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13 (1992), </volume> <pages> pp. 923-947. </pages>
Reference-contexts: While we will emphasize the use of the DPO and AMS mechanisms in the design of the LPARX run-time system, we note that these facilities may be useful in other application domains. Many scientific methods, such as tree-based algorithms in N -body simulations <ref> [9, 79, 140] </ref>, rely on elaborate, dynamic data structures and exhibit unpredictable, unstructured communication patterns. The implementation of such numerical methods would be greatly simplified using the run-time support of DPO and AMS. This chapter is organized as follows. <p> A naive force evaluation scheme would, for a system of N particles, calculate all O (N 2 ) particle-particle interactions directly. Such an approach is too expensive for systems of more than a few thousand particles. Rapid approximation methods <ref> [4, 8, 9, 17, 79] </ref> accelerate the force evaluation by trading some accuracy for speed. Approximation algorithms typically divide the force evaluation into two components: local particle-particle interactions and far-field force computations. <p> Fast PDE Hierarchical Interactions Solver Representations Local Force Approximations [50, 108] p Particle-Particle Particle-Mesh [87] p p Particle-in-Cell (PIC) [87] p Method of Local Corrections (MLC) [8] p p Adaptive MLC [4, 5] p p p Fast Multipole Method (FMM) [79] p p Barnes-Hut [17] p p Hierarchical Element Method <ref> [9] </ref> p p Table 5.1: A survey of the computational structure for various N -body approximation methods. This chart indicates whether a particular approximation algorithm employs local particle-particle interactions, a fast partial differential equation (PDE) solver, or hierarchical representations. <p> Traditionally, multipole [79] and Barnes-Hut [17] methods have been implemented using unstructured tree codes [140]. An alternative implementation strategy would employ a hierarchy of irregular but structured refinements <ref> [9, 64] </ref> using our software infrastructure. To date, no one has directly compared these two implementation strategies using real codes. Such a comparison would provide valuable insight into the relative strengths and weaknesses of each representation.
Reference: [10] <author> I. Ashok and J. Zahorjan, Adhara: </author> <title> Runtime support for dynamic space-based applications on distributed memory MIMD multiprocessors, </title> <booktitle> in Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Experimental data distribution primitives targeted towards particle calculations have been added to Fortran D [136] using CHAOS as the run-time support library. Adhara <ref> [10] </ref> is a run-time library for particle applications that is not as general as CHAOS but has been specifically designed and optimized for certain classes of particle methods. Warren and Salmon [140] developed a parallel tree code intended for Barnes-Hut [17] and fast multipole [79] methods.
Reference: [11] <author> W. Athas and N. Boden, Cantor: </author> <title> An actor programming system for scientific computing, </title> <booktitle> in Proceedings of the AMC SIGPLAN Workshop of Object Based Concurrent Programming, </booktitle> <year> 1989. </year>
Reference-contexts: Actors are concurrent objects which communicate with each other via messages. Actors execute in response to messages, and each actor object may contain several concurrently executing tasks. Actor-based languages include ABCL [145], Cantor <ref> [11] </ref>, and Charm++ [90]. Implementations of actor languages require complicated compilation strategies and sophisticated run-time support libraries, such as the Concert system [91] for fine-grain object management. Because of this complexity, we have not based the LPARX run-time system on an existing concurrent object-based language.
Reference: [12] <author> S. B. Baden, </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12 (1991), </volume> <pages> pp. 145-157. </pages>
Reference-contexts: Such applications may involve the solution of complicated, time-dependent partial differential equations such as those in materials design [37], computational fluid dynamics [22], or localized deformations in geophysical systems. Also included are particle methods in molecular dynamics [50], astrophysics [108], and vortex dynamics <ref> [12] </ref>. More recently, adaptive methods have been applied to the study of entire ecosystems through satellite imagery at multiple resolutions [135]. These applications are particularly challenging to implement on parallel computers owing to their dynamic, irregular decompositions of data. <p> This localization property is especially important on multiprocessors, since we can exploit data locality to reduce interprocessor communication costs and improve parallel performance. We focus on two important classes of dynamic, block-irregular applications: * structured adaptive mesh methods [22, 23, 94], and * particle methods based on link-cell techniques <ref> [12, 87] </ref>. Such applications can be difficult to implement without advanced software support because they rely on dynamic, complicated irregular array structures with irregular communication patterns 2 . The programmer is burdened with the responsibility of managing dynamically changing data distributed across processor memories and orchestrating interprocessor communication and synchronization. <p> To our knowledge, this is the first time that structured adaptive mesh techniques have been used to solve eigenvalue problems in materials design. 1.3.4 Particle API Our particle API provides computational scientists high-level tools that simplify the implementation of particle applications <ref> [12, 87] </ref> on parallel computers. Particle methods are difficult to parallelize because they require dynamic, irregular data decompositions to balance changing non-uniform workloads. Built on top of the LPARX mechanisms, our particle API defines facilities specifically tailored towards particle methods. <p> They arise in two important classes of scientific computations: * multilevel structured adaptive finite difference methods [22, 23, 94], which rep resent refinement regions using block-irregular data structures, and * parallel computations such as particle methods [87] that require an irregular data decomposition <ref> [12, 21] </ref> to balance non-uniform workloads across parallel processors. We have used the LPARX mechanisms to implement domain-specific APIs and representative applications from each of these two problem classes. <p> Our particle library facilities are based in part on previous work by Baden <ref> [12] </ref>, who developed a programming methodology for parallelizing particle calculations running on MIMD multiprocessors. His implementation of a 2d vortex dynamics application was the first to employ a recursive bisection decomposition [21] to dynamically balance particle methods. <p> For particle applications, each processor is typically assigned a single data partition, which corresponds to an XArray element (a Grid). Processors compute only for those particles within their assigned partition. a single timestep is limited by the stability requirements of the numerical method; therefore, workloads change slowly <ref> [12, 87, 108] </ref>. For example, the smoothed particle hydrodynamics application described in Section 5.3 repartitions every ten timesteps. Partitioning the computational domain introduces data dependencies between the various subproblems; particles near the boundary of a partition may interact with particles belonging to other processors. <p> The simplicity of our routine belies the fact that the code to perform Fetch Particles becomes quite complicated in the absence of the powerful LPARX fa cilities. Similar functionality in the GenMP system <ref> [12] </ref> required over 1500 lines of message passing code. In describing the parallelization of the GROMOS molecular dy namics application, Clark et al. [50] point out the difficulty of supporting irregular 157 partitions and ghost cell regions that may span several partitions.
Reference: [13] <author> S. B. Baden, S. J. Fink, and S. R. Kohn, </author> <title> Structural abstraction: A unifying parallel programming model for data motion and partitioning in irregular scientific computations. </title> <note> (in preparation), </note> <year> 1995. </year>
Reference-contexts: Interprocessor communication is hidden by the run-time system, and the application is completely unaware of low-level details. Although the current LPARX implementation is limited to representing irregular, block-structured decompositions, the concept of structural abstraction is general and extends to other classes of applications, such as unstructured finite element meshes <ref> [13] </ref>. 2.2.2 Data Types LPARX provides the following four basic data types: * Point: an integer n-tuple representing a point in Z n , * Region: an object representing a rectangular subset of array index space, * Grid: a dynamic array instantiated over a Region, and * XArray: a dynamic array <p> The Region does not contain data elements, as an array, but rather represents a portion of index space. In the current implementation of LPARX, we restrict Regions to be rectangular; however, the concepts described here apply to arbitrary subsets of Z n <ref> [13] </ref>. A Region is uniquely defined by the two Points at its lower and upper corners. We denote the lower bound of a Region R by lwb (R) and its upper bound by upb (R). <p> Currently, compiled languages such as HPF support neither general block-irregular decompositions nor run-time data distribution. The Structural Abstraction (SA) parallel programming model <ref> [13] </ref> extends the LPARX abstractions with a new data type (an unstructured Region) to address other classes of irregular scientific applications, such as unstructured finite element 38 problems and irregularly coupled regular meshes [45]. <p> It cannot handle the task parallelism of CC++ [41] or Fortran M [74]. LPARX applies only to problems with irregular, block-structured data exhibiting coarse-grain data parallelism. Recent work with the Structural Abstraction (SA) model <ref> [13] </ref> extends the LPARX ideas to address other classes of irregular scientific applications (e.g. unstructured methods). Another limitation of LPARX is that its representation of a data decomposition may not necessarily match the programmer's view of the data. <p> Since communication dependencies change, schedules will need to change. How do we know when to re-calculate a schedule? These are open questions for future research. 2.5.7 Future Work LPARX currently addresses only those applications with irregular but structured data decompositions. The Structural Abstraction (SA) model <ref> [13] </ref> extends the LPARX ideas to other classes of irregular scientific applications. SA has not yet been implemented, however, and its implementation will require the unification of three different run-time support libraries: LPARX, CHAOS [60], and multiblock PARTI [3].
Reference: [14] <author> S. B. Baden and S. R. Kohn, </author> <title> A comparison of load balancing strategies for particle methods running on MIMD multiprocessors, </title> <booktitle> in Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1991. </year> <title> [15] , Portable parallel programming of numerical problems under the LPAR system, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <year> (1995). </year>
Reference-contexts: Furthermore, when partitioning the problem, we would like to take advantage of the spatial locality of the particle-particle interactions. By subdividing the computational space into large, contiguous blocks, we can minimize interprocessor communication since nearby particles are likely to be assigned the same processor <ref> [14] </ref>. partitions has been assigned to a processor numbered from p0 to p15. Such a uniform decomposition does not efficiently distribute workloads; for example, no work 147 (b) (c) processors. Particles are represented by dots, and the workload is directly related to local particle density.
Reference: [16] <author> H. E. Bal and A. S. Tanenbaum, </author> <title> Distributed programming with shared data, </title> <booktitle> in Proceedings of the International Conference on Computer Languages, </booktitle> <month> October </month> <year> 1988, </year> <pages> pp. 82-91. </pages>
Reference-contexts: Another related paradigm developed by the distributed systems community is virtual shared memory, which provides the illusion of a single, shared, coherent address space for systems with physically distributed memories. Virtual shared memory models include page-based [101] and object-based systems <ref> [16, 46, 66, 89] </ref>. Page-based virtual shared memory enforces consistency at the level of the memory page, typically one to four thousand bytes. Because such a coarse page granularity results in poor performance due to false sharing, object-based systems provide consistency at the level of a single user-defined object.
Reference: [17] <author> J. Barnes and P. Hut, </author> <title> A hierarchical O(N log N ) force-calculation algorithm, </title> <note> Nature, 32A (1986), p. 446. </note>
Reference-contexts: A naive force evaluation scheme would, for a system of N particles, calculate all O (N 2 ) particle-particle interactions directly. Such an approach is too expensive for systems of more than a few thousand particles. Rapid approximation methods <ref> [4, 8, 9, 17, 79] </ref> accelerate the force evaluation by trading some accuracy for speed. Approximation algorithms typically divide the force evaluation into two components: local particle-particle interactions and far-field force computations. <p> (not shown). 146 Approximation Algorithm Local Fast PDE Hierarchical Interactions Solver Representations Local Force Approximations [50, 108] p Particle-Particle Particle-Mesh [87] p p Particle-in-Cell (PIC) [87] p Method of Local Corrections (MLC) [8] p p Adaptive MLC [4, 5] p p p Fast Multipole Method (FMM) [79] p p Barnes-Hut <ref> [17] </ref> p p Hierarchical Element Method [9] p p Table 5.1: A survey of the computational structure for various N -body approximation methods. This chart indicates whether a particular approximation algorithm employs local particle-particle interactions, a fast partial differential equation (PDE) solver, or hierarchical representations. <p> Adhara [10] is a run-time library for particle applications that is not as general as CHAOS but has been specifically designed and optimized for certain classes of particle methods. Warren and Salmon [140] developed a parallel tree code intended for Barnes-Hut <ref> [17] </ref> and fast multipole [79] methods. Their approach dynamically distributes nodes of the tree across processors using a clever hashing mechanism. Singh et al. [126, 128] have also implemented a parallel fast multipole algorithm for shared memory multiprocessors. <p> AMLC coupled with the power of parallel architectures would enable computational scientists to study vortex dynamics problems with considerably larger numbers of particles. Traditionally, multipole [79] and Barnes-Hut <ref> [17] </ref> methods have been implemented using unstructured tree codes [140]. An alternative implementation strategy would employ a hierarchy of irregular but structured refinements [9, 64] using our software infrastructure. To date, no one has directly compared these two implementation strategies using real codes.
Reference: [18] <author> D. R. Bates, K. Ledsham, and A. L. Stewart, </author> <title> Wave functions of the hydrogen molecular ion, </title> <journal> Phil. Trans. Roy. Soc. London, </journal> <volume> 246 (1953), </volume> <pages> pp. 215-240. </pages>
Reference-contexts: In this problem, there is only one electron but two nuclei with 1 r singularities: H = 2m 1 ~ R a j~r 2 j ; where ~ R a is the atomic separation. (4:8) Analytic solutions for this problem are known <ref> [18] </ref>, but it is too stiff for practical solution via Fourier methods. The adaptive eigenvalue solver method performs quite well, however. The eigenvector solution is plotted in Figure 4.21a; note the increased density of grid points in the vicinity of the two nuclei.
Reference: [19] <author> J. Bell, M. Berger, J. Saltzman, and M. </author> <title> Welcome, Three-dimensional adaptive mesh refinement for hyperbolic conservation laws, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 15 (1994), </volume> <pages> pp. 127-138. </pages>
Reference-contexts: Crutchfield et al. independently developed similar Region abstractions based upon FIDIL for vector architectures [55]. Based on this framework, they have developed domain-specific libraries for adaptive mesh refinement applications in gas dynamics <ref> [19] </ref>. Their adaptive mesh refinement libraries have been parallelized using our software infrastructure [141] (see Section 3.4.1). The array sublanguage ZPL [100, 102] employs a form of region abstraction. ZPL does not explicitly manage data distribution, which it assumes is handled by another language. <p> An object oriented library for structured adaptive mesh refinement has been developed at Lawrence Livermore National Laboratory by Crutchfield et al. [55]. This software is intended to support hyperbolic gas dynamics applications running on vector supercomputers <ref> [19] </ref>. The basic abstractions employed in Crutchfield's work are very similar to our own; in fact, their adaptive mesh refinement libraries have been parallelized using the LPARX software [141]. Parashar and Browne are developing a software infrastructure supporting parallel adaptive mesh refinement methods for black hole interactions [115]. <p> Contributions from flagged points outside the "mask region" are ignored. Previous implementations of the signature algorithm have represented flagged locations using lists of points with one point for each flagged location <ref> [19, 55] </ref>. We implement a different strategy based on parallel array reductions over irregular grid structures 3 (an IrregularGrid of integers). Flagged points are assigned a value of 1 and all other locations 0. Our strategy calculates signatures using array reductions with addition over a specified region of space.
Reference: [20] <author> M. J. Berger, </author> <title> Adaptive Mesh Refinement for Hyperbolic Partial Differential Equations, </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1982. </year>
Reference-contexts: Such support enables scientists to develop efficient parallel, portable, high-performance applications in a fraction of the time that would have been required if the application had been developed from scratch. 4.1.2 Related Work Adaptive mesh refinement techniques for multiple spatial dimensions were first developed by Berger and Oliger <ref> [20, 23] </ref> to solve time-dependent hyperbolic partial differential equations. These techniques are based on previous work on locally nested refinement structures in one spatial dimension by Bolstad [32]. Adaptive mesh refinement methods were later used by Berger and Colella to resolve shock waves in computational fluid dynamics [22]. <p> After obtaining an estimate of error, we must flag points where the error is "too high," as shown in Figure 4.7a. One well-known method used for both elliptic [34] and hyperbolic <ref> [20] </ref> partial differential equations is to flag every location which exceeds some predetermined error threshold. This method attempts to bound the overall solution error by bounding the error at every grid point. Another approach, which we have not seen mentioned in the literature, focuses on fixing computational resources.
Reference: [21] <author> M. J. Berger and S. H. Bokhari, </author> <title> A partitioning strategy for nonuniform problems on multiprocessors, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36 (1987), </volume> <pages> pp. 570-580. 200 </pages>
Reference-contexts: They arise in two important classes of scientific computations: * multilevel structured adaptive finite difference methods [22, 23, 94], which rep resent refinement regions using block-irregular data structures, and * parallel computations such as particle methods [87] that require an irregular data decomposition <ref> [12, 21] </ref> to balance non-uniform workloads across parallel processors. We have used the LPARX mechanisms to implement domain-specific APIs and representative applications from each of these two problem classes. <p> LPARX also defines a for loop, a sequential version of the forall. The XArray of Grids structure provides a common framework for implementing various block-irregular decompositions of data. This framework is used by standard load balancing routines such as recursive bisection <ref> [21] </ref> (see Chapter 5) and also by application-specific routines, such as the grid generator for an adaptive mesh calculation [94] (see Chapter 4). Figure 2.3 shows decompositions arising in two different applications. <p> The decomposition in (1) may be managed explicitly by the application, such as in generating refinement regions, or by load balancing utilities that implement partitioning strategies. The LPARX implementation provides a standard library of partitioners that implement recursive coordinate bisection <ref> [21] </ref> and uniform block partitioning. The assignment of Regions to processors in (2) provides applications the flexibility to delegate work to processors. In general, this information will be returned by the routine which renders the partitions. This step may be omitted, in which case LPARX generates a default assignment. <p> rently over the Grids in the n-dimensional XArray X for i 1 ,i 2 ,: : :,i n in X end for A sequential loop that iterates over the Grids in the n dimensional XArray X Uniform (R, P ) RCB (W , P ) External uniform and recursive bisection <ref> [21] </ref> partitioning routines provided by a standard LPARX library; both return an Array of Regions describing the computational space (indicated by Region R or workload estimate W ) decomposed into P Regions representing approximately equal amounts of computational work Table 2.2: This table summarizes the operations defined by LPARX. 29 2.3 <p> Several repartitioning phases have occurred between the times represented by the last two snapshots. 148 has been assigned to processors p4, p5, p10, and p11. A better method for decomposing non-uniform workloads is shown in Figures 5.3b and 5.3c, which illustrate two irregular block assignments rendered using recursive bisection <ref> [21] </ref>. In these decompositions, each processor receives approximately the same amount of work. Because the distribution of particles and the associated workloads change over time, we must periodically redistribute the work across processors to maintain load balance. <p> Our particle library facilities are based in part on previous work by Baden [12], who developed a programming methodology for parallelizing particle calculations running on MIMD multiprocessors. His implementation of a 2d vortex dynamics application was the first to employ a recursive bisection decomposition <ref> [21] </ref> to dynamically balance particle methods. Tamayo et al. [133] investigated various data parallel implementation strategies for molecular dynamics simulations running on SIMD computers such as the CM-2. <p> Our API automatically measures the amount of time spent computing in each bin and uses these timing measurements from previous timesteps to guide the partitioning for the following timesteps. In the next step, we call a recursive bisection <ref> [21] </ref> partitioning utility 1 provided by the LPARX standard libraries. <p> For this problem, the recursive bisection (RCB) partitioner <ref> [21] </ref> was unable to efficiently balance workloads. One drawback of RCB is that all partition cuts are straight lines (see Figure 5.17a); thus, RCB does not have the freedom to insert a "kink" in the cut to improve load balance.
Reference: [22] <author> M. J. Berger and P. Colella, </author> <title> Local adaptive mesh refinement for shock hydrodynamics, </title> <journal> Journal of Computational Physics, </journal> <volume> 82 (1989), </volume> <pages> pp. 64-84. </pages>
Reference-contexts: Such applications may involve the solution of complicated, time-dependent partial differential equations such as those in materials design [37], computational fluid dynamics <ref> [22] </ref>, or localized deformations in geophysical systems. Also included are particle methods in molecular dynamics [50], astrophysics [108], and vortex dynamics [12]. More recently, adaptive methods have been applied to the study of entire ecosystems through satellite imagery at multiple resolutions [135]. <p> This localization property is especially important on multiprocessors, since we can exploit data locality to reduce interprocessor communication costs and improve parallel performance. We focus on two important classes of dynamic, block-irregular applications: * structured adaptive mesh methods <ref> [22, 23, 94] </ref>, and * particle methods based on link-cell techniques [12, 87]. Such applications can be difficult to implement without advanced software support because they rely on dynamic, complicated irregular array structures with irregular communication patterns 2 . <p> Such numerical methods dynamically refine the local representation of a problem in "interesting" portions of the computational domain, such as shock regions in computational fluid dynamics <ref> [22] </ref>. They are difficult to implement because refinement regions vary in size and location, resulting in complicated geometries and irregular communication patterns. Computational scientists using our adaptive mesh API can concentrate on their numerical applications rather than being concerned with low-level implementation details. <p> Dynamic irregular block decompositions are not currently supported by programming languages such as High Performance Fortran (HPF) [83], Fortran D [77], Vienna Fortran [43], or Fortran 90D [33, 143]. They arise in two important classes of scientific computations: * multilevel structured adaptive finite difference methods <ref> [22, 23, 94] </ref>, which rep resent refinement regions using block-irregular data structures, and * parallel computations such as particle methods [87] that require an irregular data decomposition [12, 21] to balance non-uniform workloads across parallel processors. <p> Section 4.5 analyzes parallel performance and library overheads. We conclude in Section 4.6 with an analysis and discussion. 88 4.1.1 Motivation The accurate solution of many problems in science and engineering requires the resolution of unpredictable, localized physical phenomena. Examples include shock waves in computational fluid dynamics <ref> [22] </ref> and the near-singular atomic core potentials in materials design [37]. The key feature of these problems is that some portions of the problem domain|for example, regions containing the shock waves or the atomic nuclei|require higher resolution, and thus more computational effort, than other areas of the computational space. <p> These techniques are based on previous work on locally nested refinement structures in one spatial dimension by Bolstad [32]. Adaptive mesh refinement methods were later used by Berger and Colella to resolve shock waves in computational fluid dynamics <ref> [22] </ref>. Our work with adaptive mesh methods applies this same adaptive framework to elliptic partial differential equations and adaptive eigenvalue problems [94]. Berger and Saltzman have implemented a parallel 2d adaptive mesh refinement code in Connection Machine Fortran for the CM-2 [25, 26]. <p> Available memory places a hard limit on the problem sizes which can be solved in-core. Problems larger than a fixed size must resort either to paging, which is terribly slow on most multiprocessors, or to out-of-core algorithms. Following the work by Berger and Colella <ref> [22] </ref>, our structured adaptive mesh application consists of three main components: a numerical solver, an error estimator, and a grid generator. Note that although our algorithm looks somewhat similar to adaptive mesh refinement [23], it is not identical. <p> Although not an issue in our particular application, dividing patches may not be appropriate or desirable for other numerical methods for which introducing 108 new boundary elements creates additional computational work 4 (e.g. flux correction for hyperbolic partial differential equations <ref> [22] </ref>). When recursively dividing patches, our algorithm does not generate sub-patches smaller than a specified, architecture-specific minimum size. Although small blocks reduce load imbalance, they introduce additional interprocessor communication. After patches have been divided, they are sorted in decreasing order by size and are bin-packed to processors [54]. <p> Furthermore, boundary cells may introduce additional computational work for some numerical methods (e.g. flux correction for hyperbolic partial differential equations <ref> [22] </ref>). We compare the non-uniform refinement approach against four uniform grid sizes: 12 3 , 16 3 , 24 3 , and 32 3 . Each patch is augmented with a ghost cell region of width one.
Reference: [23] <author> M. J. Berger and J. Oliger, </author> <title> Adaptive mesh refinement for hyperbolic partial differential equations, </title> <journal> Journal of Computational Physics, </journal> <volume> 53 (1984), </volume> <pages> pp. 484-512. </pages>
Reference-contexts: This localization property is especially important on multiprocessors, since we can exploit data locality to reduce interprocessor communication costs and improve parallel performance. We focus on two important classes of dynamic, block-irregular applications: * structured adaptive mesh methods <ref> [22, 23, 94] </ref>, and * particle methods based on link-cell techniques [12, 87]. Such applications can be difficult to implement without advanced software support because they rely on dynamic, complicated irregular array structures with irregular communication patterns 2 . <p> DPO provides object oriented mechanisms for manipulating objects that are physically distributed across processor memories and is based on communicating object models from the distributed systems community [61]. 1.3.3 Adaptive Mesh API Our adaptive mesh API defines specialized, high-level facilities tailored to structured multilevel adaptive mesh refinement applications <ref> [23, 94] </ref>. Such numerical methods dynamically refine the local representation of a problem in "interesting" portions of the computational domain, such as shock regions in computational fluid dynamics [22]. They are difficult to implement because refinement regions vary in size and location, resulting in complicated geometries and irregular communication patterns. <p> Dynamic irregular block decompositions are not currently supported by programming languages such as High Performance Fortran (HPF) [83], Fortran D [77], Vienna Fortran [43], or Fortran 90D [33, 143]. They arise in two important classes of scientific computations: * multilevel structured adaptive finite difference methods <ref> [22, 23, 94] </ref>, which rep resent refinement regions using block-irregular data structures, and * parallel computations such as particle methods [87] that require an irregular data decomposition [12, 21] to balance non-uniform workloads across parallel processors. <p> Unstructured adaptive methods [58, 62, 130] store the solution using graph or tree representations [107, 123]; these methods are called "unstructured" because connectivity information must be stored for each unknown (node) of the graph. Structured methods, such as adaptive mesh refinement <ref> [23] </ref> and structured multigrid algorithms [34, 104], employ a hierarchy of nested mesh levels in which each level consists of many simple, rectangular grids. Each rectangular grid in the hierarchy represents a structured block of many thousands of unknowns. <p> Such support enables scientists to develop efficient parallel, portable, high-performance applications in a fraction of the time that would have been required if the application had been developed from scratch. 4.1.2 Related Work Adaptive mesh refinement techniques for multiple spatial dimensions were first developed by Berger and Oliger <ref> [20, 23] </ref> to solve time-dependent hyperbolic partial differential equations. These techniques are based on previous work on locally nested refinement structures in one spatial dimension by Bolstad [32]. Adaptive mesh refinement methods were later used by Berger and Colella to resolve shock waves in computational fluid dynamics [22]. <p> We present the salient features of the method to motivate the abstractions described in Section 4.3. Further numerical details can be found in Section 4.4. Structured adaptive mesh methods <ref> [23] </ref> solve partial differential equations using a hierarchy of nested, locally structured finite difference grids. The grid hierarchy can be thought of as a single composite grid in which the discretization is non-uniform (see Figure 4.3). <p> Following the work by Berger and Colella [22], our structured adaptive mesh application consists of three main components: a numerical solver, an error estimator, and a grid generator. Note that although our algorithm looks somewhat similar to adaptive mesh refinement <ref> [23] </ref>, it is not identical. Our intended applications employ 2 Of course, unstructured representations may be more appropriate for some problems, such as those with irregular boundaries. 95 elliptic, not hyperbolic, partial differential equations.
Reference: [24] <author> M. J. Berger and I. Rigoutsos, </author> <title> An algorithm for point clustering and grid generation, </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 21 (1991), </volume> <pages> pp. 1278-1286. </pages>
Reference-contexts: Each patch may have multiple parents; refinement hierarchies do not form a tree. Patches are assumed to be rectangular and lie parallel to the coordinate axes. Our adaptive mesh API implements a regridding algorithm by Berger and Rigoutsos <ref> [24] </ref> based on signatures from pattern recognition. In this method, information about the spatial distribution of flagged points within a specified region of space is collapsed onto the 1d axes; these signatures are then used to generate refinement patches.
Reference: [25] <author> M. J. Berger and J. Saltzman, </author> <title> AMR on the CM-2, </title> <type> Tech. Rep. </type> <institution> 92.16, RIACS, Moffett Field, </institution> <address> CA, </address> <month> August </month> <year> 1992. </year> <title> [26] , Structured adaptive mesh refinement on the Connection Machine, </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: Although structured adaptive mesh methods incur some overhead costs associated with adaptivity, such as error estimation and data structure management, these overheads are insignificant when compared to the savings gained through selective refinement <ref> [25, 26] </ref>. For example, by exploiting adaptivity in a materials design application (see Section 4.4), we have reduced memory consumption and computation time by more than two orders of magnitude over an equivalent uniform mesh method [94]. <p> Our work with adaptive mesh methods applies this same adaptive framework to elliptic partial differential equations and adaptive eigenvalue problems [94]. Berger and Saltzman have implemented a parallel 2d adaptive mesh refinement code in Connection Machine Fortran for the CM-2 <ref> [25, 26] </ref>. Their data parallel implementation required that all regions of refinement be the same size. As a result, the application over-refined some portions of the computational space, using 60% more memory than an equivalent implementation without the uniform size restriction. <p> In our computations, regridding using parallel reductions requires only about one percent of the total computation time. To simplify the communication of information between levels of the hierar 3 In their data parallel Fortran implementation, Berger and Saltzman <ref> [25, 26] </ref> employ parallel array reductions over rectangular arrays, but their regridding algorithm returns patches of uniform size and is not based on signatures. 105 flagged regions are represented by the shaded areas. (a) The computational space is tiled (dashed lines) and only those regions with flagged points (solid lines) are <p> In practice, this step is rarely necessary, as refinement regions are usually already nested. Our adaptive mesh libraries also provide one alternative regridding algorithm based on work by Berger and Saltzman for uniform refinement regions <ref> [25, 26] </ref>. Unlike the previous algorithm, this method guarantees that all refinement patches are the same size. The uniform algorithm was originally motivated by an adaptive mesh refinement implementation in a data parallel language (Connection Machine Fortran) that required uniform patches. It is much simpler than the non-uniform algorithm. <p> It is an open research question whether non-uniform refinement structures can be efficiently supported in a data parallel language. One implementation strategy for structured adaptive mesh methods in a data parallel language such as High Performance Fortran [83] would restrict all refinement patches to be the same size <ref> [25, 26] </ref>. We therefore conclude this section with an analysis of the performance implications of requiring uniformly sized refinement regions. The motivating application for our structured adaptive mesh API is the adaptive solution of nonlinear eigenvalue problems arising in materials design [37]. <p> In their Connection Machine 135 Fortran implementation of a 2d adaptive mesh refinement application on the CM-2, Berger and Saltzman <ref> [25, 26] </ref> required that all refinement regions be the same size. To ascertain the performance implications of such a restriction, we have implemented a grid generation strategy identical to that used by Berger and Saltzman. <p> Our model of coarse-grain data parallel numerical computation maps efficiently onto the current generation of message passing parallel architectures. It is an open research question whether data parallel languages can efficiently support the irregular refinement structures employed by structured adaptive mesh algorithms. Previous implementations <ref> [25, 26] </ref> have required uniform refinements 139 to fit the fine-grain data parallel model. Our experiments in 3d indicate that such a restriction results in costly over-refinement and a corresponding loss in computational performance.
Reference: [27] <author> J. Bernholc, J.-Y. Yi, and D. J. Sullivan, </author> <title> Structural transitions in metal clusters, </title> <journal> Faraday Discussions, </journal> <volume> 92 (1991), </volume> <pages> pp. 217-228. </pages>
Reference-contexts: While preliminary, the results presented in this section indicate that adaptive eigenvalue methods may provide an important alternative solution methodology for materials design applications. The need for adaptive refinement in materials design has been noted by other researchers. Bernholc et al. <ref> [27] </ref> have implemented a semi-adaptive code that places a single, static refinement patch over each of their atoms. Others have attempted adaptive solutions using either finite element methods [134, 142] or a combination of finite element and wavelet techniques [48].
Reference: [28] <author> G. E. Blelloch, S. Chatterjee, J. C. Hardwick, J. Sipelstein, and M. Zagha, </author> <title> Implementation of a portable nested data parallel language, </title> <booktitle> in Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: For example, although the numerical kernel for the Split-C EM3D application [57] is only about ten lines of code, EM3D would require several hundred lines of initialization code to calculate data dependencies and manage ghost cells. Applicative Languages SISAL [67] and NESL <ref> [28] </ref> are applicative programming languages which restrict functions to be free of side effects 5 , a requirement that simplifies the work of the compiler and exposes more potential parallelism.
Reference: [29] <author> F. Bodin, P. Beckman, D. Gannon, J. Botwals, S. Narayana, S. Srinivas, and B. Winnicka, Sage++: </author> <title> An object-oriented toolkit and class library for building Fortran and C++ restructuring tools, </title> <booktitle> in Object Oriented Numerics Conference (OONSKI), </booktitle> <year> 1994. </year>
Reference-contexts: However, the C ++ compiler cannot apply domain-specific knowledge to optimize code. For example, it is very difficult to implement an efficient matrix library in C ++ [36] because the compiler does not understand the special properties of a "matrix" object. The Sage++ <ref> [29] </ref> compilation system for C ++ helps the compiler to generate efficient code by defining a suite of high-level compiler transformations that enable API writers to incorporate domain-specific knowledge.
Reference: [30] <author> F. Bodin, P. Beckman, D. Gannon, S. Narayana, and S. X. Yang, </author> <title> Distributed pC++: Basic ideas for an object parallel language, </title> <journal> Journal of Scientific Programming, </journal> <month> 2 </month> <year> (1993). </year>
Reference-contexts: These mechanisms alone cannot describe the irregular blocking structures that arise in adaptive mesh refinement and recursive coordinate bisection [44]. Data Parallel C ++ Languages The pC++ <ref> [30, 31, 144] </ref> programming language is a data parallel extension of C ++ .
Reference: [31] <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesavan, A. Mal-ony, and B. Mohr, </author> <title> Implementing a parallel C++ runtime system for scalable parallel systems, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: These mechanisms alone cannot describe the irregular blocking structures that arise in adaptive mesh refinement and recursive coordinate bisection [44]. Data Parallel C ++ Languages The pC++ <ref> [30, 31, 144] </ref> programming language is a data parallel extension of C ++ . <p> To modify an object, a processor must first read the entire object into local memory, modify it, and then write it back. In contrast, DPO and AMS communicate only the message data necessary to modify an object. The designers of the pC++ run-time support library <ref> [31] </ref> address some of the same implementation issues as LPARX. In fact, their model of parallel execution for the pC++ run-time system is very similar to ours described in Section 3.1.1. <p> Recall that the memory overhead of data replication is relatively modest in comparison to the size of a typical Grid, which may contain several tens of thousands of bytes. An alternate, scalable implementation strategy would involve fine-grain, distributed translation schemes such as those implemented by CHAOS [60] and pC++ <ref> [31] </ref> at the cost of additional interprocessor communication.
Reference: [32] <author> J. </author> <title> Bolstad, </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1982. </year>
Reference-contexts: These techniques are based on previous work on locally nested refinement structures in one spatial dimension by Bolstad <ref> [32] </ref>. Adaptive mesh refinement methods were later used by Berger and Colella to resolve shock waves in computational fluid dynamics [22]. Our work with adaptive mesh methods applies this same adaptive framework to elliptic partial differential equations and adaptive eigenvalue problems [94].
Reference: [33] <author> A. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka, </author> <title> Fortran 90D/HPF compiler for distributed memory MIMD computers: Design, implementation, and performance results, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Dynamic irregular block decompositions are not currently supported by programming languages such as High Performance Fortran (HPF) [83], Fortran D [77], Vienna Fortran [43], or Fortran 90D <ref> [33, 143] </ref>. <p> Data Parallel Fortran Languages High Performance Fortran (HPF) [83] is a data parallel Fortran which combines the array operations of Fortran 90, a parallel forall loop, and data decomposition directives based on the research languages Fortran D [77, 86] and Fortran 90D <ref> [33, 116, 143] </ref>. It is quickly becoming accepted by a number of manufacturers as a standard parallel programming language for scientific computing. HPF has been targeted towards regular, static applications such as dense linear algebra but provides little support for irregular, dynamic computations [44, 84]. <p> The multiblock PARTI library is targeted towards block-structured applica 43 tions. It supports the uniform BLOCK, BLOCK CYCLIC, and CYCLIC array decompositions of HPF and has been used in the run-time system for the Fortran 90D compiler <ref> [33, 116, 143] </ref>. Multiblock PARTI defines canned routines that fill ghost cells and copy regular sections between arrays.
Reference: [34] <author> A. Brandt, </author> <title> Multi-level adaptive solutions to boundary-value problems, </title> <journal> Mathematics of Computation, </journal> <volume> 31 (1977), </volume> <pages> pp. 333-390. 201 </pages>
Reference-contexts: Unstructured adaptive methods [58, 62, 130] store the solution using graph or tree representations [107, 123]; these methods are called "unstructured" because connectivity information must be stored for each unknown (node) of the graph. Structured methods, such as adaptive mesh refinement [23] and structured multigrid algorithms <ref> [34, 104] </ref>, employ a hierarchy of nested mesh levels in which each level consists of many simple, rectangular grids. Each rectangular grid in the hierarchy represents a structured block of many thousands of unknowns. <p> Our intended applications employ 2 Of course, unstructured representations may be more appropriate for some problems, such as those with irregular boundaries. 95 elliptic, not hyperbolic, partial differential equations. Ellpitic solvers require different types of numerical schemes; they use implicit iterative numerical methods <ref> [34] </ref> as compared to the explicit time-marching schemes [51] used for hyperbolic problems. We begin with a single grid level that covers the entire computational domain and build the adaptive grid hierarchy level-by-level. <p> Richardson extrapolation <ref> [34] </ref> attempts to calculate an exact estimate of the local truncation error using the solution at the two coarser levels of the grid hierarchy. After obtaining an estimate of error, we must flag points where the error is "too high," as shown in Figure 4.7a. <p> After obtaining an estimate of error, we must flag points where the error is "too high," as shown in Figure 4.7a. One well-known method used for both elliptic <ref> [34] </ref> and hyperbolic [20] partial differential equations is to flag every location which exceeds some predetermined error threshold. This method attempts to bound the overall solution error by bounding the error at every grid point. <p> Multigrid <ref> [34] </ref> is a fast method for solving partial differential equations. It represents the solution on a grid hierarchy. Multigrid uses the multiple levels of the hierarchy to accelerate the communication of numerical information across the computational domain and reduce the time to solution. <p> Multi-grid techniques integrate easily into the adaptive mesh framework; the same grid hierarchy used by adaptive mesh methods to represent the solution can be used by multigrid to accelerate convergence. In this section, we briefly describe Brandt's Full Approximation Storage (FAS) variant <ref> [34] </ref> of multigrid; further details can be found elsewhere [35, 119]. We wish to solve the partial differential equation Lu = 0 subject to Dirichlet boundary conditions on the composite grid u consisting of L + 1 levels: u l ; 0 l L. <p> otherwise u l1 FAS (l 1, u l1 , f l1 ) 8 &gt; : u l + I F (u l1 t l1 ) on interior (u l ) I F u l1 on boundary (u l ) end if u l relax (Lu l f l = 0) <ref> [34] </ref>. When called via FAS (L,u L ,0), this method performs one V-cycle on the equation Lu = 0 for the composite grid u consisting of L + 1 levels u l , 0 l L. smoothing (e.g. Gauss-Seidel [35]) on each multigrid level.
Reference: [35] <author> W. L. Briggs, </author> <title> A Multigrid Tutorial, </title> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference-contexts: In this section, we briefly describe Brandt's Full Approximation Storage (FAS) variant [34] of multigrid; further details can be found elsewhere <ref> [35, 119] </ref>. We wish to solve the partial differential equation Lu = 0 subject to Dirichlet boundary conditions on the composite grid u consisting of L + 1 levels: u l ; 0 l L. In general, L is a nonlinear operator. <p> When called via FAS (L,u L ,0), this method performs one V-cycle on the equation Lu = 0 for the composite grid u consisting of L + 1 levels u l , 0 l L. smoothing (e.g. Gauss-Seidel <ref> [35] </ref>) on each multigrid level. The FAS multigrid method is shown in Figure 4.19. When invoked using FAS (L,u L ,0), the algorithm starts at level L of the grid hierarchy, winds down through the grids to level 0, and then works back up to level L.
Reference: [36] <author> K. G. Budge, J. S. Perry, and A. C. Robinson, </author> <title> High performance scientific computing using C++, </title> <booktitle> in USENIX C++ Conference Proceedings, </booktitle> <year> 1992. </year>
Reference-contexts: For example, current optimizing compilers commonly reorder numerical operations to improve performance by eliminating common numerical sub-expressions or by scheduling instructions to avoid pipeline bubbles. Similar optimizations could be applied to a "matrix language" to block matrix operations, use efficient BLAS operations, or chain operations on vector architectures <ref> [36] </ref>. Unfortunately, scientists would need to learn a new language syntex|and computer scientists would need to develop a new compilation system|for each new problem domain. <p> However, the C ++ compiler cannot apply domain-specific knowledge to optimize code. For example, it is very difficult to implement an efficient matrix library in C ++ <ref> [36] </ref> because the compiler does not understand the special properties of a "matrix" object. The Sage++ [29] compilation system for C ++ helps the compiler to generate efficient code by defining a suite of high-level compiler transformations that enable API writers to incorporate domain-specific knowledge.
Reference: [37] <author> E. J. Bylaska, S. R. Kohn, S. B. Baden, A. Edelman, R. Kawai, M. E. G. Ong, and J. H. Weare, </author> <title> Scalable parallel numerical methods and software tools for material design, </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Such applications may involve the solution of complicated, time-dependent partial differential equations such as those in materials design <ref> [37] </ref>, computational fluid dynamics [22], or localized deformations in geophysical systems. Also included are particle methods in molecular dynamics [50], astrophysics [108], and vortex dynamics [12]. More recently, adaptive methods have been applied to the study of entire ecosystems through satellite imagery at multiple resolutions [135]. <p> It has been employed by researchers at the University of California at San Diego, George Mason University, Lawrence Livermore National Laboratories, Sandia National Laboratories, and the Cornell Theory Center for applications in gas dynamics [141], smoothed particle hydrodynamics, particle simulation studies [68], adaptive eigenvalue solvers in materials design <ref> [37, 94] </ref>, genetics algorithms [81], adaptive multigrid methods in numerical relativity, and the dynamics of earthquake faults (see Section 6.3 for a complete list). <p> All details associated with parallelism are completely hidden from the programmer. We have used our software infrastructure to develop a parallel adaptive eigenvalue solver (LDA) and an adaptive multigrid solver (AMG) for problems aris 11 ing in materials design <ref> [37, 94] </ref>. By exploiting adaptivity, we have reduced memory consumption and computation time by more than two orders of magnitude over an equivalent non-adaptive method. <p> We conclude in Section 4.6 with an analysis and discussion. 88 4.1.1 Motivation The accurate solution of many problems in science and engineering requires the resolution of unpredictable, localized physical phenomena. Examples include shock waves in computational fluid dynamics [22] and the near-singular atomic core potentials in materials design <ref> [37] </ref>. The key feature of these problems is that some portions of the problem domain|for example, regions containing the shock waves or the atomic nuclei|require higher resolution, and thus more computational effort, than other areas of the computational space. <p> Adaptive methods refine this discretization of space to accurately represent localized physical phenomena (see Figure 4.4). When creating a new level, the hi 93 circles represent regions of high error, such as atomic nuclei in materials design applications <ref> [37] </ref>. The mesh spacing of each level is half of the previous coarser level. This problem is similar to the problem solved in Section 4.5. erarchy is refined according to an error estimate calculated at run-time. <p> This strategy flags a specified number of points with the highest error and is appropriate for applications for which good analytical estimates of error are unavailable. We typically use the latter approach in our experiments with adaptive eigenvalue calculations for materials design <ref> [37] </ref>. For eigenvalue problems, the correlation between the pointwise error on a grid level and the final error in the eigenvalue (the value of interest) is not straightforward [78]. Thus, we attempt to obtain the best answer to our problem for fixed computational resources. <p> Because the chemical properties of an atom are determined by these localized potentials, it is vital that numerical methods represent them accurately. Material scientists have traditionally employed Fourier transform methods, but such methods do not easily admit the non-uniformity needed to capture localized phenomena <ref> [37] </ref>. Adaptive numerical methods support the non-uniform representation needed to accurately resolve localized potentials. While preliminary, the results presented in this section indicate that adaptive eigenvalue methods may provide an important alternative solution methodology for materials design applications. <p> We therefore conclude this section with an analysis of the performance implications of requiring uniformly sized refinement regions. The motivating application for our structured adaptive mesh API is the adaptive solution of nonlinear eigenvalue problems arising in materials design <ref> [37] </ref>. <p> It provides computational scientists with high-level tools that hide implementation details of parallelism and resource management. Such powerful software support is essential for the timely development of quality reusable numerical software. We are applying our adaptive mesh infrastructure to the solution of adaptive eigenvalue problems arising in materials design <ref> [37] </ref>. Two distinguishing characteristics of our work are the concepts of structural abstraction and coarse-grain data parallelism, both borrowed from LPARX. Structural abstraction and first-class data decompositions enable our software to represent and manipulate refinement structures as language-level objects. <p> some of his experiences with it at the 1995 meeting of the APS Physics Computing Conference 4 . * In collaboration with materials scientists and mathematicians, we have developed adaptive numerical techniques and the parallel software support for the solution of eigenvalue problems arising in materials design (see Chapter 4) <ref> [37, 94] </ref>. * LPARX has been used to implement a dimension-independent code for 2d, 3d, and 4d connected component labeling for spin models in statistical mechan ics [71]. * Building on LPARX, S. Fink and S.
Reference: [38] <author> Z. Cai, J. Mandel, and S. McCormick, </author> <title> Multigrid methods for nearly singular linear equations and eigenvalue problems. </title> <note> (submitted for publication), </note> <year> 1994. </year>
Reference-contexts: QR methods [78]) to the solution of Eq. 4.3. Instead, we use a multigrid-based iterative method by Mandel and McCormick [103] which efficiently calculates the lowest eigenvalue and associated eigenvector. This algorithm is shown in Figure 4.18. Cai et al. <ref> [38] </ref> have proven that this method is optimal in the sense that it requires O (N g ) work and O (1) iterations. Convergence is independent of the distribution of eigenvalues.
Reference: [39] <author> N. Carriero and D. Gelernter, </author> <title> Linda in context, </title> <journal> Communications of the ACM, </journal> <volume> 32 (1989), </volume> <pages> pp. 444-458. </pages>
Reference-contexts: The task parallel programming model takes a different approach in which programs consist of a number of asynchronous, independent, communicating parallel processes. Task parallel languages such as CC++ [41], CHARM [125], CHARM++ [90], Fortran M [74], and Linda <ref> [39] </ref> define a set of mechanisms that coordinate process execution and communication among autonomous tasks. Task parallelism provides no explicit support for data decomposition. Task parallelism is ideally suited for computations integrating various het-erogenous operations, such as a multidisciplinary simulation coordinating various independent components [76].
Reference: [40] <author> S. Chakrabarti, E. Deprit, E.-J. Im, J. Jones, A. Krishnamurthy, C.-P. Wen, and K. Yelick, Multipol: </author> <title> A distributed data structure library, </title> <booktitle> in Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Such applications will also require sophisticated run-time support to manage changing data distributions and communication patterns. A number of run-time support systems have already been developed, including CHAOS (formerly called PARTI) [60], multiblock PARTI [3], and Multipol <ref> [40] </ref>. Both CHAOS and multiblock PARTI have been used as run-time support for data parallel Fortran compilers. CHAOS has been very successful in addressing unstructured problems such as sparse linear algebra and finite elements [58]. <p> Although PETSc does not currently provide the irregular array structures needed by structured multilevel adaptive mesh applications (see Chapter 4), it may be possible to extend PETSc by integrating LPARX's XArray and Grid abstractions into the PETSc data-structure-neutral framework. Multipol <ref> [40] </ref> is a run-time library of distributed data structures designed to simplify the implementation of irregular problems on distributed memory architectures. It supports a number of non-array data structures such as graphs, unstructured grids, hash tables, sets, trees, and queues. <p> Its parallelization mechanisms are sufficiently general to support both particle methods and structured adaptive mesh methods. Of course, there are many applications that LPARX does not address. LPARX does not support the unstructured methods targeted by CHAOS [60] nor does it provide the dynamic irregular data types of Multipol <ref> [40] </ref>. It cannot handle the task parallelism of CC++ [41] or Fortran M [74]. LPARX applies only to problems with irregular, block-structured data exhibiting coarse-grain data parallelism.
Reference: [41] <author> K. M. Chandy and C. Kesselman, </author> <title> Compositional C++: Compositional parallel programming, </title> <booktitle> in Fifth International Workshop of Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: The task parallel programming model takes a different approach in which programs consist of a number of asynchronous, independent, communicating parallel processes. Task parallel languages such as CC++ <ref> [41] </ref>, CHARM [125], CHARM++ [90], Fortran M [74], and Linda [39] define a set of mechanisms that coordinate process execution and communication among autonomous tasks. Task parallelism provides no explicit support for data decomposition. <p> Of course, there are many applications that LPARX does not address. LPARX does not support the unstructured methods targeted by CHAOS [60] nor does it provide the dynamic irregular data types of Multipol [40]. It cannot handle the task parallelism of CC++ <ref> [41] </ref> or Fortran M [74]. LPARX applies only to problems with irregular, block-structured data exhibiting coarse-grain data parallelism. Recent work with the Structural Abstraction (SA) model [13] extends the LPARX ideas to address other classes of irregular scientific applications (e.g. unstructured methods). <p> Common implementation support is needed to merge parallel languages and libraries with different run-time behaviors. For example, a data parallel language such as HPF [83] is typically implemented using only a single thread of control per processor whereas a task parallel language such as CC++ <ref> [41] </ref> or Fortran M [74] might require several interacting execution threads per processor. Thus, combining these two models will require common run-time support for task management and communication. Several consortiums have been formed to investigate unified support mechanisms for task parallel and data parallel systems.
Reference: [42] <author> C. Chang, A. Sussman, and J. Saltz, </author> <title> Support for distributed dynamic data structures in C++, </title> <type> Tech. Rep. </type> <institution> CS-TR-3266, University of Maryland, </institution> <year> 1995. </year>
Reference-contexts: It has also been used to parallelize portions of the CHARMM molecular dynamics application [59, 88]. The Fortran D run-time system employs CHAOS to support Fortran D's mapping arrays [139]. Recently, CHAOS has been extended to support unstructured applications consisting of complicated C ++ objects <ref> [42] </ref>. However, recall that such unstructured representations are inappropriate for the irregular but structured applications targeted by LPARX. The multiblock PARTI library is targeted towards block-structured applica 43 tions. <p> AMS does not assume efficient fine-grain message passing facilities, which are not currently available on most parallel architectures, and requires only basic message passing support. The implementation of the CHAOS++ system for unstructured collections of objects <ref> [42] </ref> employs a similar abstraction of "mobile objects" that define packing and unpacking operations similar to that of AMS. 58 Layer Facilities DPO name translation for distributed objects communication between objects control over object ownership AMS message stream abstraction hides buffering details communication between handlers on processors global synchronization barriers MP
Reference: [43] <author> B. Chapman, P. Mehrotra, H. Moritsch, and H. Zima, </author> <title> Dynamic data distribution in Vienna Fortran, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> Novem-ber </month> <year> 1993. </year>
Reference-contexts: Dynamic irregular block decompositions are not currently supported by programming languages such as High Performance Fortran (HPF) [83], Fortran D [77], Vienna Fortran <ref> [43] </ref>, or Fortran 90D [33, 143]. <p> Although our implementation supplies a standard library of decomposition routines, the programmer is free to write others. Our approach to data decomposition differs from most parallel languages, such as HPF [83], which require the programmer to choose from a small number of predefined decomposition methods. Vienna Fortran <ref> [43] </ref> provides some facilities for irregular user-defined data decompositions but limits them to tensor products of irregular one dimensional decompositions. Block-irregular decompositions may be constructed using the pointwise mapping arrays of Fortran D [77]; however, point-wise decompositions are inappropriate and unnatural for calculations which exhibit block structures. <p> Pointwise mappings are therefore inappropriate for block-irregular applications. To avoid the limitations of the HPF decomposition model, Vienna Fortran <ref> [43] </ref> defines more general dynamic data distribution directives. However, Vienna Fortran restricts the types of irregular data decompositions available to the application. It supports pointwise mappings as in Fortran D (with the same limitations for block-structured methods) and also tensor products of 1d block-irregular decompositions. <p> We are currently working on a re-design of the LPARX communication libraries which we believe will eliminate most of this additional overhead [70]. 4.5.3 Uniform Grid Patches Data parallel Fortran languages such as High Performance Fortran <ref> [43, 77, 83] </ref> do not readily support non-uniform grid structures. In their Connection Machine 135 Fortran implementation of a 2d adaptive mesh refinement application on the CM-2, Berger and Saltzman [25, 26] required that all refinement regions be the same size.
Reference: [44] <author> B. Chapman, P. Mehrotra, and H. Zima, </author> <title> Extending HPF for advanced data parallel applications, </title> <type> Tech. Rep. 94-34, </type> <institution> ICASE, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: It is quickly becoming accepted by a number of manufacturers as a standard parallel programming language for scientific computing. HPF has been targeted towards regular, static applications such as dense linear algebra but provides little support for irregular, dynamic computations <ref> [44, 84] </ref>. HPF represents data decompositions using an abstract index space called a template. Arrays are mapped to templates and then templates are decomposed across processors. One limitation of templates, as well as all other HPF data decomposition entities, is that they are are not first-class, language-level objects. <p> It supports pointwise mappings as in Fortran D (with the same limitations for block-structured methods) and also tensor products of 1d block-irregular decompositions. These mechanisms alone cannot describe the irregular blocking structures that arise in adaptive mesh refinement and recursive coordinate bisection <ref> [44] </ref>. Data Parallel C ++ Languages The pC++ [30, 31, 144] programming language is a data parallel extension of C ++ . <p> parallel HPF numerical routines, it will become increasingly important that LPARX support hierarchical parallelism in which LPARX manages communication and data decomposition for irregular collections of arrays that are, in turn, 7 Recall that a single logical processor may actually consist of many physical processors, 47 split across processor subsets <ref> [44] </ref> with different numbers of logical processors. Fortunately, LPARX's current restriction is easy to lift; we simply allow each Grid to be assigned to a subset of processors. <p> The Parallel Compiler Runtime Consortium has begun standardization efforts, but their work is far from finished. Furthermore, LPARX does not yet support multiple processor owners per Grid, limiting its ability to exploit processor subsets <ref> [44] </ref>. Finally, more research remains on how to integrate communication schedules into the LPARX model. LPARX will require new forms of run-time support to manage the numerous changing communication schedules employed by dynamic, irregular applications. <p> To improve the efficiency of the fine-grain model, Parsons and Quinlan [117] are developing run-time methods for automatically extracting coarse-grain tasks from fine-grain data parallel loops. Another model, processor subsets <ref> [44] </ref>, combines the coarse-grain and fine-grain approaches; parallelism is expressed both over grids and within each grid. In our discussion of parallel execution, we have ignored the interprocessor communication required to satisfy data dependencies.
Reference: [45] <author> C. Chase, K. Crowley, J. Saltz, and A. Reeves, </author> <title> Parallelization of irregularly coupled regular meshes, </title> <type> Tech. Rep. 92-1, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: CHAOS has been very successful in addressing unstructured problems such as sparse linear algebra and finite elements [58]. Multiblock PARTI has been employed in the parallelization of applications with a small number of large, static blocks <ref> [45] </ref>; its support for dynamic block structured problems is unclear. The Multipol library provides a collection of distributed non-array data structures such as graphs, unstructured grids, hash tables, sets, trees, and queues. <p> The Structural Abstraction (SA) parallel programming model [13] extends the LPARX abstractions with a new data type (an unstructured Region) to address other classes of irregular scientific applications, such as unstructured finite element 38 problems and irregularly coupled regular meshes <ref> [45] </ref>. The goal of SA is to unify several previous domain-specific systems, including LPARX, multiblock PARTI [3], and CHAOS [60]. 2.4.2 Parallel Languages The parallel programming literature describes numerous languages, each of which provides facilities specialized for its own intended class of applications. <p> Multiblock PARTI defines canned routines that fill ghost cells and copy regular sections between arrays. Although it has been employed in the parallelization of computations with a small number of large, static blocks (e.g. irregularly coupled regular meshes <ref> [45] </ref>), multiblock PARTI has not been applied to problems with a large number of smaller, dynamic blocks, such as the structured adaptive mesh problems targeted by LPARX.
Reference: [46] <author> J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefield, </author> <title> The Amber system: Parallel programming on a network of multiprocessors, </title> <booktitle> in Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1988, </year> <pages> pp. 147-158. </pages>
Reference-contexts: Another related paradigm developed by the distributed systems community is virtual shared memory, which provides the illusion of a single, shared, coherent address space for systems with physically distributed memories. Virtual shared memory models include page-based [101] and object-based systems <ref> [16, 46, 66, 89] </ref>. Page-based virtual shared memory enforces consistency at the level of the memory page, typically one to four thousand bytes. Because such a coarse page granularity results in poor performance due to false sharing, object-based systems provide consistency at the level of a single user-defined object.
Reference: [47] <author> A. A. Chien, </author> <title> Concurrent Aggregates: Supporting Modularity in Massively Parallel Programs, </title> <publisher> MIT Press, </publisher> <year> 1993. </year> <month> 202 </month>
Reference-contexts: These mechanisms alone cannot describe the irregular blocking structures that arise in adaptive mesh refinement and recursive coordinate bisection [44]. Data Parallel C ++ Languages The pC++ [30, 31, 144] programming language is a data parallel extension of C ++ . It implements a "concurrent aggregate" <ref> [47] </ref> model in which a parallel operation is applied simultaneously to all elements of a data aggregate called a "collection." 4 Information about the second High Performance Fortran standardization effort can be found at World Wide Web address ftp://hpsl.cs.umd.edu/pub/hpf bench/index.html. 40 Each element of a collection may be a complicated C
Reference: [48] <author> K. Cho, T. A. Arias, J. D. Joannopoulos, and P. K. Lam, </author> <title> Wavelets in electronic structure calculations, </title> <journal> Physical Review Letters, </journal> <volume> 71 (1993), </volume> <pages> pp. 1808-1811. </pages>
Reference-contexts: Bernholc et al. [27] have implemented a semi-adaptive code that places a single, static refinement patch over each of their atoms. Others have attempted adaptive solutions using either finite element methods [134, 142] or a combination of finite element and wavelet techniques <ref> [48] </ref>. Although finite element methods pro 116 such as the C 20 H 20 ring shown here.
Reference: [49] <author> T. W. Clark, R. V. Hanxleden, J. A. McCammon, and L. R. Scott, </author> <title> Parallelization strategies for a molecular dynamics program, </title> <booktitle> in Intel Technology Focus Conference Proceedings, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: Some previous efforts with parallel particle calculations have concentrated on the parallelization of a particular program instead of a general software infrastructure. For example, Clark et al. <ref> [49, 50] </ref> implemented a parallel version of the GROMOS molecular dynamics application. Their approach uses non-uniform, dynamic partitions similar to our own which were implemented (with considerable effort) using a message passing library.
Reference: [50] <author> T. W. Clark, R. v. Hanxleden, J. A. McCammon, and L. R. Scott, </author> <title> Parallelizing molecular dynamics using spatial decomposition, </title> <booktitle> in Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Such applications may involve the solution of complicated, time-dependent partial differential equations such as those in materials design [37], computational fluid dynamics [22], or localized deformations in geophysical systems. Also included are particle methods in molecular dynamics <ref> [50] </ref>, astrophysics [108], and vortex dynamics [12]. More recently, adaptive methods have been applied to the study of entire ecosystems through satellite imagery at multiple resolutions [135]. These applications are particularly challenging to implement on parallel computers owing to their dynamic, irregular decompositions of data. <p> Rapid approximation methods typically divide the force computation into two components: local particle-particle interactions and far-field force evaluation (not shown). 146 Approximation Algorithm Local Fast PDE Hierarchical Interactions Solver Representations Local Force Approximations <ref> [50, 108] </ref> p Particle-Particle Particle-Mesh [87] p p Particle-in-Cell (PIC) [87] p Method of Local Corrections (MLC) [8] p p Adaptive MLC [4, 5] p p p Fast Multipole Method (FMM) [79] p p Barnes-Hut [17] p p Hierarchical Element Method [9] p p Table 5.1: A survey of the computational <p> Some previous efforts with parallel particle calculations have concentrated on the parallelization of a particular program instead of a general software infrastructure. For example, Clark et al. <ref> [49, 50] </ref> implemented a parallel version of the GROMOS molecular dynamics application. Their approach uses non-uniform, dynamic partitions similar to our own which were implemented (with considerable effort) using a message passing library. <p> Similar functionality in the GenMP system [12] required over 1500 lines of message passing code. In describing the parallelization of the GROMOS molecular dy namics application, Clark et al. <ref> [50] </ref> point out the difficulty of supporting irregular 157 partitions and ghost cell regions that may span several partitions. Such special cases are automatically managed by the copy-on-intersect primitives provided by LPARX. <p> Thus, by exploiting the symmetry of the force law, we halve the numerical computation at the expense of additional interprocessor communication. One compromise employs symmetry only if neither interacting particle lies in the ghost cell region <ref> [50] </ref>; forces for such particles are computed redundantly on different processors instead of communicated between processors. <p> Writing CombineForces is simple, however, and we will show an example in Section 5.2.5. As before, the code for WriteBack is difficult to implement without the support provided by LPARX. Indeed, the parallel implementation of the GROMOS molecular dynamics application <ref> [50] </ref> ignores symmetry and redundantly computes interactions involving particles lying in ghost cell regions, even though the implementors expect a dramatic increase in performance with this optimization. <p> This write back phase can be difficult to implement without the proper software support; in fact, the parallel implementation of the molecular dynamics program GROMOS does not use a symmetric force law for ghost cell particles <ref> [50] </ref> for this very reason. In this section, we investigate the performance tradeoffs of this design decision.
Reference: [51] <author> P. Colella and P. Woodward, </author> <title> The piecewise parabolic method (ppm) for gas-dynamical simulations, </title> <journal> Journal of Computational Physics, </journal> <volume> 54 (1984), </volume> <pages> pp. 174-201. </pages>
Reference-contexts: Ellpitic solvers require different types of numerical schemes; they use implicit iterative numerical methods [34] as compared to the explicit time-marching schemes <ref> [51] </ref> used for hyperbolic problems. We begin with a single grid level that covers the entire computational domain and build the adaptive grid hierarchy level-by-level.
Reference: [52] <author> L. Collatz, </author> <title> The numerical treatment of differential equations, </title> <publisher> Springer-Verlag, </publisher> <year> 1966. </year>
Reference-contexts: Likewise, define edge as the set of twelve points on the edges (but not the corners) of the cube. We ignore the eight corner points. The fourth order O (h 4 ) finite difference discretiza-tion corresponding to mesh location i with mesh spacing h can be written as <ref> [52] </ref>: (L h u) i = h 2 @ 8u i + 3 j2face u j + 3 j2edge u j A 0 1 X j2face f j u j A + @ g i + 6 j2face g j A (4.6) 123 4.4.6 Computational Results To validate our approach, we
Reference: [53] <author> C. R. Cook, C. M. Pancake, and R. Walpole, </author> <title> Are expectations for parallelism too high? A survey of potential parallel users, </title> <booktitle> in Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: It would not be an understatement to say that parallel software is in a state of crisis. The vast majority of scientific programmers find that current parallel software support is inadequate <ref> [53] </ref>. In fact, they are more likely to develop their own in-house software support rather than use existing products [113]. The most commonly used parallel programming paradigm today is message passing. Standardization efforts have resulted in a portable message passing library called MPI (Message 1 2 Passing Interface) [105].
Reference: [54] <author> W. Y. Crutchfield, </author> <title> Load balancing irregular algorithms, </title> <type> Tech. Rep. </type> <institution> UCRL-JC-107679, Lawrence Livermore National Laboratory, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: When recursively dividing patches, our algorithm does not generate sub-patches smaller than a specified, architecture-specific minimum size. Although small blocks reduce load imbalance, they introduce additional interprocessor communication. After patches have been divided, they are sorted in decreasing order by size and are bin-packed to processors <ref> [54] </ref>. This load balancing method works well in practice but does not take into account interprocessor communication between levels of the grid hierarchy. Patches communicate with their parents on the previous level.
Reference: [55] <author> W. Y. Crutchfield and M. L. </author> <title> Welcome, Object oriented implementation of adaptive mesh refinement algorithms, </title> <journal> Journal of Scientific Programming, </journal> <volume> 2 (1993), </volume> <pages> pp. 145-156. </pages>
Reference-contexts: Therefore, LPARX implements the Map using an XArray and a Grid, and it distinguishes between concurrent computation (over the Grids in an XArray) and sequential computation (over the elements of a Grid). Crutchfield et al. independently developed similar Region abstractions based upon FIDIL for vector architectures <ref> [55] </ref>. Based on this framework, they have developed domain-specific libraries for adaptive mesh refinement applications in gas dynamics [19]. Their adaptive mesh refinement libraries have been parallelized using our software infrastructure [141] (see Section 3.4.1). The array sublanguage ZPL [100, 102] employs a form of region abstraction. <p> For example, scientists at Lawrence Livermore National Laboratories have used our DPO, AMS, and MP ++ software to parallelize a structured adaptive mesh library for hyperbolic problems in gas dynamics <ref> [55, 141] </ref>. Their library defines a set of abstractions similar to LPARX. Their versions of Point, Region, and Grid have been specialized for their particular class of applications; for example, their Region describes application-specific properties such as whether meshes are cell-centered or node-centered. <p> An object oriented library for structured adaptive mesh refinement has been developed at Lawrence Livermore National Laboratory by Crutchfield et al. <ref> [55] </ref>. This software is intended to support hyperbolic gas dynamics applications running on vector supercomputers [19]. The basic abstractions employed in Crutchfield's work are very similar to our own; in fact, their adaptive mesh refinement libraries have been parallelized using the LPARX software [141]. <p> Without the proper software support, managing these bookkeeping details can be difficult because of the irregular and unpredictable placement of refinement patches <ref> [55] </ref>. Note that ghost cell regions and communication are required by the adaptive mesh method and are not simply artifacts of the parallel implementation. <p> Contributions from flagged points outside the "mask region" are ignored. Previous implementations of the signature algorithm have represented flagged locations using lists of points with one point for each flagged location <ref> [19, 55] </ref>. We implement a different strategy based on parallel array reductions over irregular grid structures 3 (an IrregularGrid of integers). Flagged points are assigned a value of 1 and all other locations 0. Our strategy calculates signatures using array reductions with addition over a specified region of space. <p> adaptive multigrid methods in numerical relativity as part of the Black Hole Binary Grand Challenge Project. 192 * Scientists at Lawrence Livermore National Laboratories have employed our Distributed Parallel Object, Asynchronous Message Stream, and MP ++ software to parallelize a structured adaptive mesh library for hyperbolic problems in gas dynamics <ref> [55, 141] </ref>. * C. Myers (Cornell Theory Center), B. Shaw (Lamont-Doherty Earth Observatory), and J. Langer (University of California at Santa Barbara) have implemented a parallel 2d code to study localized slip modes in the dynamics of earthquake faults. * C. Myers and J.
Reference: [56] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken, </author> <title> LogP: Towards a realistic model of parallel computation, </title> <booktitle> in Proceedings of the Fourth AMC SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1993. </year>
Reference-contexts: Such questions require further study. Part of the difficulty in writing portable parallel programs is that no single, unifying, realistic model of parallel computation and communication has emerged. There have been several attempts to define a unifying model, such as PMH [6], LogP <ref> [56] </ref>, BSP [137], and CTA [129]. However, these models tend to focus on performance evaluation. What is needed is a single set of realistic, portable, general purpose mechanisms for efficient parallel programming. Without high-level support, programmers are left to employ different implementations on different architectures, hampering portability. <p> Thus, the time to send a message of length L can be approximated by T 0 + L BW . We measured message passing times with a simple program that sends messages of 1 T 0 actually incorporates both message latency and unavoidable software overheads <ref> [56] </ref>. 194 195 Machine C ++ Fortran Operating System Compiler Optimization Compiler Optimization Alphas g ++ v2.6.2 -O2 f77 v3.3 -O4 OSF/1 1.2 C-90 CC v1.0.1.1 -O2 cft77 v6.0 -O1 UNICOS 8.0.2.2 Paragon g ++ v2.6.2 -O2 -mnoieee if77 v4.5.2 -O4 -Knoieee OSF/1 1.0.4 SP2 xlC v2.1 -O -Q xlf v3.1
Reference: [57] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick, </author> <title> Parallel programming in Split-C, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: However, it is inappropriate for the coarse-grain scientific applications addressed by LPARX, which are more naturally expressed in a coarse-grain data parallel fashion (see Section 2.2.3). 41 Split-C Split-C <ref> [57] </ref> is a parallel extension to C for distributed memory multiprocessors. Split-C gets its name from its split-phase communications model; it allows the programmer to overlap communication and computation through a two phase data request. The application initiates a request for data and then computes until the data arrives. <p> The Split-C run-time system does not eliminate duplicate data requests to the same data item, nor does it aggregate messages, two optimizations provided by the CHAOS run-time system [60]. For example, although the numerical kernel for the Split-C EM3D application <ref> [57] </ref> is only about ten lines of code, EM3D would require several hundred lines of initialization code to calculate data dependencies and manage ghost cells. <p> Thus, there may be multiple copy operations executing in parallel, overlapping interprocessor communication. LPARX inserts a global synchronization barrier at the end of every communication phase (e.g. in the end forall at the end of the forall loop) to ensure that all communication has terminated before computation begins. Split-C <ref> [57] </ref> supports a similar split phase communications paradigm. We now consider the execution of the LPARX statement: copy into A from B on Inside where A and B are Grids and Inside is a Region.
Reference: [58] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy, </author> <title> The design and implementation of a parallel unstructured euler solver using software primitives, </title> <type> Tech. Rep. 92-12, </type> <institution> ICASE, Hampton, VA, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Both CHAOS and multiblock PARTI have been used as run-time support for data parallel Fortran compilers. CHAOS has been very successful in addressing unstructured problems such as sparse linear algebra and finite elements <ref> [58] </ref>. Multiblock PARTI has been employed in the parallelization of applications with a small number of large, static blocks [45]; its support for dynamic block structured problems is unclear. <p> Often, the cost of creating a communications schedule can be amortized over many uses if data dependencies do not change. CHAOS implements pointwise mapping arrays for unstructured calculations such as sweeps over finite element meshes and sparse matrix computations <ref> [58] </ref>. It has also been used to parallelize portions of the CHARMM molecular dynamics application [59, 88]. The Fortran D run-time system employs CHAOS to support Fortran D's mapping arrays [139]. Recently, CHAOS has been extended to support unstructured applications consisting of complicated C ++ objects [42]. <p> In general, adaptive methods may be structured or unstructured, depending on how they represent the numerical solution to the problem, as shown in Figure 4.2. Unstructured adaptive methods <ref> [58, 62, 130] </ref> store the solution using graph or tree representations [107, 123]; these methods are called "unstructured" because connectivity information must be stored for each unknown (node) of the graph.
Reference: [59] <author> R. Das and J. Saltz, </author> <title> Parallelizing molecular dynamics codes using PARTI software primitives, </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1993. </year> <month> 203 </month>
Reference-contexts: CHAOS implements pointwise mapping arrays for unstructured calculations such as sweeps over finite element meshes and sparse matrix computations [58]. It has also been used to parallelize portions of the CHARMM molecular dynamics application <ref> [59, 88] </ref>. The Fortran D run-time system employs CHAOS to support Fortran D's mapping arrays [139]. Recently, CHAOS has been extended to support unstructured applications consisting of complicated C ++ objects [42]. However, recall that such unstructured representations are inappropriate for the irregular but structured applications targeted by LPARX. <p> The parallelization of GROMOS would have been significantly easier had they employed the software abstractions we describe in Section 5.2. Portions of the CHARMM molecular dynamics application have been paral 149 lelized using the CHAOS software primitives <ref> [59, 88] </ref>. CHAOS employs a fine-grain decomposition strategy in which particles are individually assigned to processors. The drawback to this type of fine-grain approach is that algorithms for scheduling interprocessor communication scale as the number of particles.
Reference: [60] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang, </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> (to appear). </note>
Reference-contexts: New parallelization abstractions are needed because the current compile-time data distribution mechanisms are inadequate for dynamic problems. Such applications will also require sophisticated run-time support to manage changing data distributions and communication patterns. A number of run-time support systems have already been developed, including CHAOS (formerly called PARTI) <ref> [60] </ref>, multiblock PARTI [3], and Multipol [40]. Both CHAOS and multiblock PARTI have been used as run-time support for data parallel Fortran compilers. CHAOS has been very successful in addressing unstructured problems such as sparse linear algebra and finite elements [58]. <p> The goal of SA is to unify several previous domain-specific systems, including LPARX, multiblock PARTI [3], and CHAOS <ref> [60] </ref>. 2.4.2 Parallel Languages The parallel programming literature describes numerous languages, each of which provides facilities specialized for its own intended class of applications. In the following survey, we evaluate various parallel languages on their ability to solve the dynamic, block-irregular problems targeted by LPARX. <p> The Split-C run-time system does not eliminate duplicate data requests to the same data item, nor does it aggregate messages, two optimizations provided by the CHAOS run-time system <ref> [60] </ref>. For example, although the numerical kernel for the Split-C EM3D application [57] is only about ten lines of code, EM3D would require several hundred lines of initialization code to calculate data dependencies and manage ghost cells. <p> While the automatic detection of parallelism is extremely attractive, these languages have not yet demonstrated that the compiler alone can extract sufficient information from the program to efficiently distribute data for dynamic, irregular problems on message passing architectures. 2.4.3 Run-Time Support Libraries The CHAOS (formerly PARTI) <ref> [60] </ref> and multiblock PARTI [3] libraries provide run-time support for data parallel compilers such as HPF and Fortran D [139]. Both libraries support a "inspector/executor" model for scheduling communication at run-time. <p> We believe that LPARX strikes a good balance. Its parallelization mechanisms are sufficiently general to support both particle methods and structured adaptive mesh methods. Of course, there are many applications that LPARX does not address. LPARX does not support the unstructured methods targeted by CHAOS <ref> [60] </ref> nor does it provide the dynamic irregular data types of Multipol [40]. It cannot handle the task parallelism of CC++ [41] or Fortran M [74]. LPARX applies only to problems with irregular, block-structured data exhibiting coarse-grain data parallelism. <p> Instead of relying on the hardware cache coherence mechanism alone, irregular calculations employ specialized communication scheduling techniques [109] similar to those pioneered in CHAOS <ref> [60] </ref>. Thus, efficient shared memory implementations require the same memory management techniques as efficient message passing implementations. and it is not sufficient to rely on automatic hardware caching mechanisms. <p> The disadvantage of the current mechanism is that it tells little about the global communication structure among all interacting Grids, limiting opportunities for communication optimizations. 50 One possible solution [70] employs the communication schedule building techniques of Saltz <ref> [60] </ref> in which communication is split into two phases: an inspection phase and an execution phase. In the inspection phase, processors build a schedule describing the communication pattern. In our case, the schedule would be built using operations similar to LPARX's block copies between Grids. <p> Schedules introduce a variety of interesting implementation issues. How do we keep track of the vast number of schedules in complicated dynamic applications? The structured adaptive mesh calculation described in Chapter 4 would require perhaps forty different active communication schedules. Such bookkeeping facilities are not provided by CHAOS <ref> [60] </ref> and multiblock PARTI [3], which assume that schedules are managed either by the compiler or the user. Since communication dependencies change, schedules will need to change. <p> The Structural Abstraction (SA) model [13] extends the LPARX ideas to other classes of irregular scientific applications. SA has not yet been implemented, however, and its implementation will require the unification of three different run-time support libraries: LPARX, CHAOS <ref> [60] </ref>, and multiblock PARTI [3]. The acceptance of High Performance Fortran by the scientific computing 51 community introduces a number of interesting research issues. <p> Recall that the memory overhead of data replication is relatively modest in comparison to the size of a typical Grid, which may contain several tens of thousands of bytes. An alternate, scalable implementation strategy would involve fine-grain, distributed translation schemes such as those implemented by CHAOS <ref> [60] </ref> and pC++ [31] at the cost of additional interprocessor communication. <p> LPARX applications alternate between communication and computation phases. Thus, communication is not asynchronous but is, in fact, limited to the well-defined communication phases of the program. Furthermore, we can predict 85 global communication patterns using the inspector/executor paradigm pioneered in CHAOS <ref> [60] </ref> and multiblock PARTI [3]. In this model of communication, the inspector phase calculates a schedule of data motion which is then executed in the executor phase. The LPARX schedule building loop|the inspector|would employ the region calculus and copy-on-intersect operations (i.e. structural abstraction) to specify data dependencies.
Reference: [61] <author> S. Deshpande, P. Delisle, and A. G. Daghi, </author> <title> A communication facility for distributed object-oriented applications, </title> <booktitle> in USENIX C++ Conference Proceedings, </booktitle> <year> 1992. </year>
Reference-contexts: DPO provides object oriented mechanisms for manipulating objects that are physically distributed across processor memories and is based on communicating object models from the distributed systems community <ref> [61] </ref>. 1.3.3 Adaptive Mesh API Our adaptive mesh API defines specialized, high-level facilities tailored to structured multilevel adaptive mesh refinement applications [23, 94]. Such numerical methods dynamically refine the local representation of a problem in "interesting" portions of the computational domain, such as shock regions in computational fluid dynamics [22]. <p> Instead, we borrowed features that were specifically needed for the LPARX implementation. For example, DPO's distributed object naming mechanisms and its notion of primary and secondary objects (described in Section 3.2.3) are based in part on the distributed facilities described by Deshpande et al. <ref> [61] </ref>. The AMS abstractions combine ideas from Active Messages [138], asynchronous remote procedure calls [7, 110], and the C ++ I/O stream model [106, 131]. <p> Although secondary objects may explicitly cache data locally, they are intended to act as "handles" through which the program accesses the primary version of the object. DPO's model of primary and secondary objects is based upon the distributed communication and object management mechanisms described by Deshpande et al. <ref> [61] </ref>. Object identifiers are used to name DPO objects lying on different processors.
Reference: [62] <author> K. D. Devine and J. E. Flaherty, </author> <title> Dynamic load balancing for parallel finite element methods with hand p-refinement, </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: In general, adaptive methods may be structured or unstructured, depending on how they represent the numerical solution to the problem, as shown in Figure 4.2. Unstructured adaptive methods <ref> [58, 62, 130] </ref> store the solution using graph or tree representations [107, 123]; these methods are called "unstructured" because connectivity information must be stored for each unknown (node) of the graph.
Reference: [63] <author> G. C. Duncan and P. A. Hughes, </author> <title> Simulations of relativistic extragalactic jets, </title> <journal> The Astrophysical Journal, </journal> <volume> 436 (1994), </volume> <pages> pp. </pages> <month> L119-L122. </month>
Reference-contexts: We plan to explore how to unify support for these two problem classes in a single API. C. Duncan of Bowling Green State University would like to use our structured adaptive mesh infrastructure to parallelize an adaptive hyperbolic solver for simulations of relativistic extragalactic jets <ref> [63] </ref>, and we view this as an opportunity to develop a common API framework for both types of partial differential equations. <p> Duncan (Bowling Green State University) is planning to use our structured adaptive mesh infrastructure to parallelize an adaptive hyperbolic solver for simulations of relativistic extragalactic jets <ref> [63] </ref>. * In collaboration with F. Abraham (IBM Almaden), we are using our particle library to develop a molecular dynamics application to study fracture dynamics in solids [1].
Reference: [64] <author> D. J. Edelsohn, </author> <title> Hierarchical tree-structures as adaptive meshes, </title> <journal> International Journal of Modern Physics C (Physics and Computers), </journal> <volume> 4 (1993), </volume> <pages> pp. 909-917. </pages>
Reference-contexts: Traditionally, multipole [79] and Barnes-Hut [17] methods have been implemented using unstructured tree codes [140]. An alternative implementation strategy would employ a hierarchy of irregular but structured refinements <ref> [9, 64] </ref> using our software infrastructure. To date, no one has directly compared these two implementation strategies using real codes. Such a comparison would provide valuable insight into the relative strengths and weaknesses of each representation.
Reference: [65] <author> B. Falsafi, A. R. Lebeck, S. K. Reinhardt, I. Schoinas, M. D. Hill, J. R. Larus, A. Rogers, and D. A. Wood, </author> <title> Application-specific protocols for user-level shared memory, </title> <booktitle> in Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Other machines, such as the Stanford FLASH [97] and the Wisconsin COW [122], contain hardware for the automatic caching of remote data. However, recent studies with these "distributed shared memory" machines <ref> [65, 109] </ref> indicate that such hardware caching mechanisms are inadequate for dynamic scientific applications. <p> Unfortunately, this is not always the case [6, 127]. Experiments with the Wisconsin Wind Tunnel shared memory simulator [121] indicate that the explicit management of data locality can dramatically improve performance for dynamic scientific applications <ref> [65] </ref>. Instead of relying on the hardware cache coherence mechanism alone, irregular calculations employ specialized communication scheduling techniques [109] similar to those pioneered in CHAOS [60]. <p> Studies with distributed shared memory computers indicate that dynamic, irregular applications can dramatically improve performance by explicitly managing cache locality <ref> [65, 109] </ref>; it is simply 84 not sufficient to rely on hardware caching mechanisms. <p> The library must allow the programmer to specify what data is to be sent during each phase of the calculation, and it must support selective packing and unpacking of data. These types of design considerations are also vital for performance on distributed shared memory machines with coherent caches <ref> [65] </ref>. 182 5.4 Analysis and Discussion The great tragedy of science is the slaying of a beautiful hypothesis by an ugly fact. | Thomas Henry Huxley Particle applications are difficult to parallelize because they require dynamic, irregular partitionings of space to maintain an equal distribution of computational work.
Reference: [66] <author> M. J. Feeley and H. M. Levy, </author> <title> Distributed shared memory with versioned objects, </title> <booktitle> in Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA), </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Another related paradigm developed by the distributed systems community is virtual shared memory, which provides the illusion of a single, shared, coherent address space for systems with physically distributed memories. Virtual shared memory models include page-based [101] and object-based systems <ref> [16, 46, 66, 89] </ref>. Page-based virtual shared memory enforces consistency at the level of the memory page, typically one to four thousand bytes. Because such a coarse page granularity results in poor performance due to false sharing, object-based systems provide consistency at the level of a single user-defined object.
Reference: [67] <author> J. T. Feo and D. C. Cann, </author> <title> A report on the SISAL language project, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10 (1990), </volume> <pages> pp. 349-366. </pages>
Reference-contexts: For example, although the numerical kernel for the Split-C EM3D application [57] is only about ten lines of code, EM3D would require several hundred lines of initialization code to calculate data dependencies and manage ghost cells. Applicative Languages SISAL <ref> [67] </ref> and NESL [28] are applicative programming languages which restrict functions to be free of side effects 5 , a requirement that simplifies the work of the compiler and exposes more potential parallelism.
Reference: [68] <author> S. M. Figueira and S. B. Baden, </author> <title> Performance analysis of parallel strategies for localized n-body solvers, </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: It has been employed by researchers at the University of California at San Diego, George Mason University, Lawrence Livermore National Laboratories, Sandia National Laboratories, and the Cornell Theory Center for applications in gas dynamics [141], smoothed particle hydrodynamics, particle simulation studies <ref> [68] </ref>, adaptive eigenvalue solvers in materials design [37, 94], genetics algorithms [81], adaptive multigrid methods in numerical relativity, and the dynamics of earthquake faults (see Section 6.3 for a complete list). <p> Our facilities have also been employed by Figueira and Baden to analyze the performance of various parallelization strategies for localized N -body solvers <ref> [68] </ref>. This chapter is organized as follows. We begin with an overview of particle methods and review related work. Section 5.2 introduces the parallelization facilities provided by our particle API and describes how they have been implemented using the LPARX mechanisms. <p> Tamayo et al. [133] investigated various data parallel implementation strategies for molecular dynamics simulations running on SIMD computers such as the CM-2. More recently, Figueira and Baden <ref> [68] </ref> employed our software infrastructure to analyze the performance of different parallelization strategies for localized N -body methods running on MIMD multiprocessors. Some previous efforts with parallel particle calculations have concentrated on the parallelization of a particular program instead of a general software infrastructure. <p> In general, the choice of the best mesh size depends on factors such as the number of processors, kernel interaction distance, load imbalance, workload distribution, processor computational speed, and particle density <ref> [68] </ref>. On the parallel machines, we rebalanced workloads every ten timesteps. Table 5.4 and Figure 5.13 present computational performance for one timestep of the SPH3D application. <p> Because the computational work is clustered in a small area, the recursive bisection algorithm cannot efficiently partition the workload across processors. Although we could refine the mesh to obtain a better load balance, a finer mesh would incur additional computational overheads <ref> [68] </ref>, resulting in worse overall performance. The communication of interacting particle information accounts for most of the remaining execution time. Although the time spent in communication remains somewhat constant as we increase the number of processors, its relative cost increases as computation time decreases. <p> Figueira and S. Baden have employed our software infrastructure to analyze the performance tradeoffs of various parallelization strategies for localized N - body solvers <ref> [68] </ref>. * G. Duncan (Bowling Green State University) is planning to use our structured adaptive mesh infrastructure to parallelize an adaptive hyperbolic solver for simulations of relativistic extragalactic jets [63]. * In collaboration with F.
Reference: [69] <author> S. J. Fink and S. B. Baden, </author> <title> Run-time data distribution for block-structured applications on distributed memory computers, </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Fran-cisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: It uses its region constructs to simplify array indexing and as iteration masks; in contrast, LPARX employs Regions to specify run-time data decompositions and express communication dependencies. ZPL regions are not first-class, assignable objects as in LPARX. Building on the LPARX region calculus and structural abstraction, Fink and Baden <ref> [69] </ref> have developed a run-time data distribution library that provides an HPF-like mapping strategy with first-class, dynamic distribution objects supporting both regular and irregular block decompositions. <p> Fink and S. Baden have developed run-time HPF-like data distribution techniques for block structured applications <ref> [69] </ref>. 3 Myers' article is available at World Wide Web address http://www.tc.cornell.edu/ SmartNodes/Newsletters/1994/V6N6/Myers. 4 The abstract of Myers' talk, "Some ABCs of OOP for PDEs on MPPs," is available at World Wide Web address http://aps.org/BAPSPC95/abs/SJ0104.html. 193 * S. Figueira and S.
Reference: [70] <author> S. J. Fink, S. B. Baden, and S. R. Kohn, </author> <title> Flexible communication schedules for block structured applications. </title> <note> (in preparation), </note> <year> 1995. </year>
Reference-contexts: The disadvantage of the current mechanism is that it tells little about the global communication structure among all interacting Grids, limiting opportunities for communication optimizations. 50 One possible solution <ref> [70] </ref> employs the communication schedule building techniques of Saltz [60] in which communication is split into two phases: an inspection phase and an execution phase. In the inspection phase, processors build a schedule describing the communication pattern. <p> Thus, they can perform optimizations to minimize communication overheads, such as pre-allocating message buffers and message aggregation. The lack of global knowledge in the LPARX implementation results in communication overheads (see Section 3.3) which could be reduced using schedules <ref> [70] </ref>. Schedules introduce a variety of interesting implementation issues. How do we keep track of the vast number of schedules in complicated dynamic applications? The structured adaptive mesh calculation described in Chapter 4 would require perhaps forty different active communication schedules. <p> However, in Section 3.4.3, we will see that the assumption about asynchronous and unpredictable interprocessor communication is unnecessarily general. In fact, we can predict communication patterns between Grids using schedules <ref> [70] </ref>, and we can exploit this knowledge to reduce run-time overheads. <p> Our synchronization protocol requires 2 log P message starts on P processors. However, it would be possible to eliminate this costly synchronization through an alternative implementation strategy (see Section 3.4.3) using run-time schedule analysis techniques <ref> [70] </ref>. <p> The primary run-time overhead associated with our implementation strategy 82 is the global barrier synchronization used to detect the end of interprocessor communication. We are working to eliminate this overhead through run-time communication schedule techniques <ref> [70] </ref>. 3.4.1 Flexibility One advantage to the modular implementation approach of our software infrastructure is the flexibility to re-use various software components as needed. <p> Because the schedule provides global knowledge of communication patterns, each processor would know when its communication had completed and the costly global barrier synchronization would no longer be required. Preliminary work with integrating schedule building techniques into LPARX <ref> [70] </ref> indicates that most of the overheads described in Section 3.3.3 can be eliminated. <p> We are currently working on a re-design of the LPARX communication libraries which we believe will eliminate most of this additional overhead <ref> [70] </ref>. 4.5.3 Uniform Grid Patches Data parallel Fortran languages such as High Performance Fortran [43, 77, 83] do not readily support non-uniform grid structures.
Reference: [71] <author> S. J. Fink, C. Huston, S. B. Baden, and K. Jansen, </author> <title> Parallel cluster identification for multidimensional lattices. </title> <note> (submitted to IEEE Transactions on Parallel and Distributed Systems), 1995. 204 </note>
Reference-contexts: have developed adaptive numerical techniques and the parallel software support for the solution of eigenvalue problems arising in materials design (see Chapter 4) [37, 94]. * LPARX has been used to implement a dimension-independent code for 2d, 3d, and 4d connected component labeling for spin models in statistical mechan ics <ref> [71] </ref>. * Building on LPARX, S. Fink and S.
Reference: [72] <author> M. J. Flynn, </author> <title> Some computer organizations and their effectiveness, </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21 (1972), </volume> <pages> pp. 948-960. </pages>
Reference-contexts: These applications are particularly challenging to implement on parallel computers owing to their dynamic, irregular decompositions of data. Our research addresses the programming abstractions and the accompanying software support required for dynamic, irregular, block-structured scientific computations running on MIMD <ref> [72] </ref> parallel computers. The distinguishing characteristics 5 of this class of problems are that (1) numerical work is non-uniformly distributed over space, (2) the workload distribution changes as the computation progresses, and (3) the workload exhibits a local structure.
Reference: [73] <author> K. Forsman, W. Gropp, L. Kettunen, and D. Levine, </author> <title> Computational electromagnetics and parallel dense matrix computations, </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: for sparse and dense matrices, including Krylov iterative methods, linear and nonlinear systems solvers, and some (sequential) partial differential equation solvers for finite element and finite difference 6 Papers on the POOMA project have not yet been published; information is available from their World Wide Web address http://www.acl.lanl.gov/PoomaFramework. 44 schemes <ref> [73] </ref>. The toolkit employs a data-structure-neutral implementation which permits users of the numerical routines to use their own, application-specific data structures.
Reference: [74] <author> I. Foster and K. M. Chandy, </author> <title> Fortran M: A language for modular parallel programming, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> (to appear). </note>
Reference-contexts: The task parallel programming model takes a different approach in which programs consist of a number of asynchronous, independent, communicating parallel processes. Task parallel languages such as CC++ [41], CHARM [125], CHARM++ [90], Fortran M <ref> [74] </ref>, and Linda [39] define a set of mechanisms that coordinate process execution and communication among autonomous tasks. Task parallelism provides no explicit support for data decomposition. Task parallelism is ideally suited for computations integrating various het-erogenous operations, such as a multidisciplinary simulation coordinating various independent components [76]. <p> Of course, there are many applications that LPARX does not address. LPARX does not support the unstructured methods targeted by CHAOS [60] nor does it provide the dynamic irregular data types of Multipol [40]. It cannot handle the task parallelism of CC++ [41] or Fortran M <ref> [74] </ref>. LPARX applies only to problems with irregular, block-structured data exhibiting coarse-grain data parallelism. Recent work with the Structural Abstraction (SA) model [13] extends the LPARX ideas to address other classes of irregular scientific applications (e.g. unstructured methods). <p> Common implementation support is needed to merge parallel languages and libraries with different run-time behaviors. For example, a data parallel language such as HPF [83] is typically implemented using only a single thread of control per processor whereas a task parallel language such as CC++ [41] or Fortran M <ref> [74] </ref> might require several interacting execution threads per processor. Thus, combining these two models will require common run-time support for task management and communication. Several consortiums have been formed to investigate unified support mechanisms for task parallel and data parallel systems.
Reference: [75] <author> I. Foster and C. Kesselman, </author> <title> Integrating task and data parallelism, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: a fertile area for future research. 6.2.2 Language Interoperability Currently, parallel software written in one programming language or run-time library is likely to be incompatible with software written in another system. 190 Language (and library) interoperability is driven by two key factors: (1) code reuse and (2) heterogenous programming models <ref> [75] </ref>. Code reuse is difficult today since common subroutines cannot in general be shared by different parallel systems. Het-erogenous programming models enable the programmer to use the programming language or run-time library best suited for the task at hand.
Reference: [76] <author> I. Foster, M. Xu, B. Avalani, and A. Choudhary, </author> <title> A compilation system that integrates High Performance Fortran and Fortran M, </title> <booktitle> in Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Task parallelism provides no explicit support for data decomposition. Task parallelism is ideally suited for computations integrating various het-erogenous operations, such as a multidisciplinary simulation coordinating various independent components <ref> [76] </ref>. However, it is inappropriate for the coarse-grain scientific applications addressed by LPARX, which are more naturally expressed in a coarse-grain data parallel fashion (see Section 2.2.3). 41 Split-C Split-C [57] is a parallel extension to C for distributed memory multiprocessors. <p> For example, how should the programmer in a task parallel language specify a call to a data parallel routine? How should data be transferred across the call interface? HPF [83] defines a rudimentary external procedure interface, and others have investigated calling HPF from pC++ [144] and also from Fortran M <ref> [76] </ref>.
Reference: [77] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu, </author> <title> Fortran D language specification, </title> <type> Tech. Rep. </type> <institution> TR90-141, Department of Computer Science, Rice University, Houston, TX, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Dynamic irregular block decompositions are not currently supported by programming languages such as High Performance Fortran (HPF) [83], Fortran D <ref> [77] </ref>, Vienna Fortran [43], or Fortran 90D [33, 143]. <p> Vienna Fortran [43] provides some facilities for irregular user-defined data decompositions but limits them to tensor products of irregular one dimensional decompositions. Block-irregular decompositions may be constructed using the pointwise mapping arrays of Fortran D <ref> [77] </ref>; however, point-wise decompositions are inappropriate and unnatural for calculations which exhibit block structures. Because pointwise decompositions have no knowledge of the block structure, mapping information must be maintained for each individual array element at a substantial cost in memory and communication overheads. <p> Data Parallel Fortran Languages High Performance Fortran (HPF) [83] is a data parallel Fortran which combines the array operations of Fortran 90, a parallel forall loop, and data decomposition directives based on the research languages Fortran D <ref> [77, 86] </ref> and Fortran 90D [33, 116, 143]. It is quickly becoming accepted by a number of manufacturers as a standard parallel programming language for scientific computing. HPF has been targeted towards regular, static applications such as dense linear algebra but provides little support for irregular, dynamic computations [44, 84]. <p> Furthermore, arrays may be decomposed using only a limited set of regular, predefined distribution methods (e.g. a uniform block decomposition); HPF does not yet support user-defined irregular decompositions. Fortran D <ref> [77, 86] </ref> relies on the same data distribution model as HPF and therefore suffers the same limitations for dynamic problems. One distinguishing feature of Fortran D is its support for pointwise mapping arrays in which individual array elements may be mapped to arbitrary processors [139]. <p> We are currently working on a re-design of the LPARX communication libraries which we believe will eliminate most of this additional overhead [70]. 4.5.3 Uniform Grid Patches Data parallel Fortran languages such as High Performance Fortran <ref> [43, 77, 83] </ref> do not readily support non-uniform grid structures. In their Connection Machine 135 Fortran implementation of a 2d adaptive mesh refinement application on the CM-2, Berger and Saltzman [25, 26] required that all refinement regions be the same size.
Reference: [78] <author> G. H. Golub and C. F. V. Loan, eds., </author> <title> Matrix Computations (Second Edition), </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: We typically use the latter approach in our experiments with adaptive eigenvalue calculations for materials design [37]. For eigenvalue problems, the correlation between the pointwise error on a grid level and the final error in the eigenvalue (the value of interest) is not straightforward <ref> [78] </ref>. Thus, we attempt to obtain the best answer to our problem for fixed computational resources. <p> Because we need only a few of the lowest eigenvalues, we do not apply standard dense eigenvalue algorithms (e.g. QR methods <ref> [78] </ref>) to the solution of Eq. 4.3. Instead, we use a multigrid-based iterative method by Mandel and McCormick [103] which efficiently calculates the lowest eigenvalue and associated eigenvector. This algorithm is shown in Figure 4.18.
Reference: [79] <author> L. Greengard and V. Rokhlin, </author> <title> A fast algorithm for particle simulations, </title> <journal> Journal of Computational Physics, </journal> <volume> 73 (1987), </volume> <pages> pp. 325-348. </pages>
Reference-contexts: While we will emphasize the use of the DPO and AMS mechanisms in the design of the LPARX run-time system, we note that these facilities may be useful in other application domains. Many scientific methods, such as tree-based algorithms in N -body simulations <ref> [9, 79, 140] </ref>, rely on elaborate, dynamic data structures and exhibit unpredictable, unstructured communication patterns. The implementation of such numerical methods would be greatly simplified using the run-time support of DPO and AMS. This chapter is organized as follows. <p> A naive force evaluation scheme would, for a system of N particles, calculate all O (N 2 ) particle-particle interactions directly. Such an approach is too expensive for systems of more than a few thousand particles. Rapid approximation methods <ref> [4, 8, 9, 17, 79] </ref> accelerate the force evaluation by trading some accuracy for speed. Approximation algorithms typically divide the force evaluation into two components: local particle-particle interactions and far-field force computations. <p> and far-field force evaluation (not shown). 146 Approximation Algorithm Local Fast PDE Hierarchical Interactions Solver Representations Local Force Approximations [50, 108] p Particle-Particle Particle-Mesh [87] p p Particle-in-Cell (PIC) [87] p Method of Local Corrections (MLC) [8] p p Adaptive MLC [4, 5] p p p Fast Multipole Method (FMM) <ref> [79] </ref> p p Barnes-Hut [17] p p Hierarchical Element Method [9] p p Table 5.1: A survey of the computational structure for various N -body approximation methods. This chart indicates whether a particular approximation algorithm employs local particle-particle interactions, a fast partial differential equation (PDE) solver, or hierarchical representations. <p> Adhara [10] is a run-time library for particle applications that is not as general as CHAOS but has been specifically designed and optimized for certain classes of particle methods. Warren and Salmon [140] developed a parallel tree code intended for Barnes-Hut [17] and fast multipole <ref> [79] </ref> methods. Their approach dynamically distributes nodes of the tree across processors using a clever hashing mechanism. Singh et al. [126, 128] have also implemented a parallel fast multipole algorithm for shared memory multiprocessors. <p> AMLC coupled with the power of parallel architectures would enable computational scientists to study vortex dynamics problems with considerably larger numbers of particles. Traditionally, multipole <ref> [79] </ref> and Barnes-Hut [17] methods have been implemented using unstructured tree codes [140]. An alternative implementation strategy would employ a hierarchy of irregular but structured refinements [9, 64] using our software infrastructure. To date, no one has directly compared these two implementation strategies using real codes.
Reference: [80] <author> W. Gropp and B. Smith, </author> <title> Scalable, extensible, and portable numerical libraries, </title> <booktitle> in Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <year> 1994, </year> <pages> pp. 87-93. </pages>
Reference-contexts: POOMA employs a layering strategy (similar in philosophy to our own) in which libraries at higher levels in the abstraction hierarchy provide more application-specific tools than lower layers. PETSc (Portable Extensible Tools for Scientific Computing) <ref> [80] </ref> is a large toolkit of mathematical software for both serial and parallel scientific computation.
Reference: [81] <author> W. E. Hart, </author> <title> Adaptive Global Optimization with Local Search, </title> <type> PhD thesis, </type> <institution> University of California at San Diego, </institution> <year> 1994. </year>
Reference-contexts: been employed by researchers at the University of California at San Diego, George Mason University, Lawrence Livermore National Laboratories, Sandia National Laboratories, and the Cornell Theory Center for applications in gas dynamics [141], smoothed particle hydrodynamics, particle simulation studies [68], adaptive eigenvalue solvers in materials design [37, 94], genetics algorithms <ref> [81] </ref>, adaptive multigrid methods in numerical relativity, and the dynamics of earthquake faults (see Section 6.3 for a complete list). <p> It is therefore fitting that we conclude this dissertation with a list of the projects that have benefitted from our software infrastructure: * W. Hart has implemented a 2d geometrically structured genetic algorithms code to study locally adaptive search techniques on parallel computers <ref> [81] </ref>. * In collaboration with J. Wallin (George Mason University), we have parallelized a 3d smoothed particle hydrodynamics code for modeling the evolution of galaxy clusters (see Chapter 5). * G.
Reference: [82] <author> C. Hewitt, P. Bishop, and R. Steiger, </author> <title> A universal ACTOR formalism for artificial intelligence, </title> <booktitle> in Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1973. </year>
Reference-contexts: The notion of communicating objects, or "actors," was first described by Hewitt <ref> [82] </ref> and then further developed by Agha [2]. Actors are concurrent objects which communicate with each other via messages. Actors execute in response to messages, and each actor object may contain several concurrently executing tasks. Actor-based languages include ABCL [145], Cantor [11], and Charm++ [90].
Reference: [83] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification, </title> <month> November </month> <year> 1994. </year> <title> [84] , HPF-2 Scope of Activities and Motivating Applications, </title> <month> November </month> <year> 1994. </year>
Reference-contexts: Standardization efforts have resulted in a portable message passing library called MPI (Message 1 2 Passing Interface) [105]. Unfortunately, programming with message passing is tedious, as the programmer must explicitly manage low-level details of data placement and interprocessor communication. Developments in High Performance Fortran 1 (HPF) <ref> [83] </ref> are promising; unfortunately, HPF will require improvements before it becomes a general purpose parallel language. For example, HPF does not adequately address dynamic and irregular problems [84], and these limitations have prompted a second HPF standardization effort. <p> Dynamic irregular block decompositions are not currently supported by programming languages such as High Performance Fortran (HPF) <ref> [83] </ref>, Fortran D [77], Vienna Fortran [43], or Fortran 90D [33, 143]. <p> LPARX provides the application a uniform framework for representing and manipulating block-irregular decompositions. Although our implementation supplies a standard library of decomposition routines, the programmer is free to write others. Our approach to data decomposition differs from most parallel languages, such as HPF <ref> [83] </ref>, which require the programmer to choose from a small number of predefined decomposition methods. Vienna Fortran [43] provides some facilities for irregular user-defined data decompositions but limits them to tensor products of irregular one dimensional decompositions. <p> An XArray is intended to implement coarse-grain irregular decompositions; thus, each processor is typically assigned only a few Grids. LPARX defines a coarse grain looping construct|forall|which iterates concurrently over the Grids of an XArray. The semantics of forall are similar to HPF's INDEPENDENT forall <ref> [83] </ref>; each loop iteration is executed as if an atomic operation. <p> The structure of a data decomposition|the floorplan describing how data is to be decomposed across processors|is a first-class, language-level object which may be manipulated by the application. In contrast, many data parallel languages such as High Performance Fortran <ref> [83] </ref> specify data decompositions at compile-time using compiler directives which force the user to choose from a limited set of built-in, regular decompositions. LPARX defines four new data types (see Table 2.1): Point, Region, Grid, and XArray. <p> In the following survey, we evaluate various parallel languages on their ability to solve the dynamic, block-irregular problems targeted by LPARX. Data Parallel Fortran Languages High Performance Fortran (HPF) <ref> [83] </ref> is a data parallel Fortran which combines the array operations of Fortran 90, a parallel forall loop, and data decomposition directives based on the research languages Fortran D [77, 86] and Fortran 90D [33, 116, 143]. <p> BLOCK or CYCLIC). To call HPF, LPARX must: * understand how HPF represents decomposed arrays, * allocate HPF arrays of a particular decomposition and alignment, and * pass array structure information into HPF numerical routines. Unfortunately, the High Performance Fortran specification <ref> [83] </ref> does not define a standard interface for external languages; instead, it allows manufacturers to develop their own external array representations. <p> It is an open research question whether the irregularity of an adaptive mesh calculation can be efficiently implemented in a data parallel language such as High Performance Fortran <ref> [83] </ref>, which does not readily support dynamic and irregular array structures. We will present computational results (Section 4.5.3) which show that the restrictions imposed by a data parallel Fortran implementation may significantly impact parallel performance. This chapter is organized as follows. <p> It is an open research question whether non-uniform refinement structures can be efficiently supported in a data parallel language. One implementation strategy for structured adaptive mesh methods in a data parallel language such as High Performance Fortran <ref> [83] </ref> would restrict all refinement patches to be the same size [25, 26]. We therefore conclude this section with an analysis of the performance implications of requiring uniformly sized refinement regions. <p> We are currently working on a re-design of the LPARX communication libraries which we believe will eliminate most of this additional overhead [70]. 4.5.3 Uniform Grid Patches Data parallel Fortran languages such as High Performance Fortran <ref> [43, 77, 83] </ref> do not readily support non-uniform grid structures. In their Connection Machine 135 Fortran implementation of a 2d adaptive mesh refinement application on the CM-2, Berger and Saltzman [25, 26] required that all refinement regions be the same size. <p> Altogether, LPARX enabled us to easily and efficiently implement the software support necessary for structured adaptive mesh applications. 4.6.2 Future Research Directions More research is needed on whether structured adaptive mesh methods can be efficiently implemented in a data parallel Fortran language such as High Performance Fortran <ref> [83] </ref>, which does not readily support irregular array structures. As discussed in Section 4.5.3, one potential solution would employ uniform refinement regions. However, our experiments indicate that this design decision results in costly over-refinement and a corresponding loss in performance. <p> Instead of requiring the programmer to choose from a small set of predefined decompositions, LPARX provides a framework for creating decompositions that may be tailored to meet the needs of a particular application. Extending High Performance Fortran <ref> [83] </ref> will require developments in parallel programming abstractions and run-time support libraries. A second High Performance Fortran [84] standardization effort is currently addressing the limitations of HPF for dynamic and irregular applications. <p> Language interoperability raises research issues in three key areas: (1) run-time systems, (2) data representation, and (3) language extensions for external procedures. Common implementation support is needed to merge parallel languages and libraries with different run-time behaviors. For example, a data parallel language such as HPF <ref> [83] </ref> is typically implemented using only a single thread of control per processor whereas a task parallel language such as CC++ [41] or Fortran M [74] might require several interacting execution threads per processor. Thus, combining these two models will require common run-time support for task management and communication. <p> Finally, language designers must consider the types of language extensions that will be required to call externally defined routines. For example, how should the programmer in a task parallel language specify a call to a data parallel routine? How should data be transferred across the call interface? HPF <ref> [83] </ref> defines a rudimentary external procedure interface, and others have investigated calling HPF from pC++ [144] and also from Fortran M [76].
Reference: [85] <author> P. N. Hilfinger and P. Colella, FIDIL: </author> <title> A language for scientific programming, </title> <type> Tech. Rep. </type> <institution> UCRL-98057, Lawrence Livermore National Laboratory, </institution> <month> January </month> <year> 1988. </year> <month> 205 </month>
Reference-contexts: Unlike Fortran-90 array section specifiers, however, the Region is a first-class object and may be assigned 20 and manipulated at run-time. The concept of first-class array section objects (called domains) was introduced in the FIDIL programming language <ref> [85] </ref>. The Grid is a dynamic array defined over an arbitrary rectangular index set specified by a Region. The Grid is similar to a Fortran 90 allocatable array. <p> We divide our survey into three areas: structural abstraction (Section 2.4.1), parallel languages (Section 2.4.2), and run-time support libraries (Section 2.4.3). 2.4.1 Structural Abstraction LPARX's Region abstraction and its region calculus are based in part on the domain abstractions explored in the scientific programming language FIDIL <ref> [85] </ref>. FIDIL's domain calculus provides operations such as union and intersection over arbitrary index sets; however, FIDIL is intended for vector supercomputers and therefore does not address data distribution. LPARX borrows a subset of FIDIL's calculus operations to provide the structural abstractions for data decomposition and inter-processor communication on multiprocessors.
Reference: [86] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng, </author> <title> Preliminary experiences with the Fortran D compiler, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Data Parallel Fortran Languages High Performance Fortran (HPF) [83] is a data parallel Fortran which combines the array operations of Fortran 90, a parallel forall loop, and data decomposition directives based on the research languages Fortran D <ref> [77, 86] </ref> and Fortran 90D [33, 116, 143]. It is quickly becoming accepted by a number of manufacturers as a standard parallel programming language for scientific computing. HPF has been targeted towards regular, static applications such as dense linear algebra but provides little support for irregular, dynamic computations [44, 84]. <p> Furthermore, arrays may be decomposed using only a limited set of regular, predefined distribution methods (e.g. a uniform block decomposition); HPF does not yet support user-defined irregular decompositions. Fortran D <ref> [77, 86] </ref> relies on the same data distribution model as HPF and therefore suffers the same limitations for dynamic problems. One distinguishing feature of Fortran D is its support for pointwise mapping arrays in which individual array elements may be mapped to arbitrary processors [139].
Reference: [87] <author> R. W. Hockney and J. W. Eastwood, </author> <title> Computer Simulation Using Particles, </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: This localization property is especially important on multiprocessors, since we can exploit data locality to reduce interprocessor communication costs and improve parallel performance. We focus on two important classes of dynamic, block-irregular applications: * structured adaptive mesh methods [22, 23, 94], and * particle methods based on link-cell techniques <ref> [12, 87] </ref>. Such applications can be difficult to implement without advanced software support because they rely on dynamic, complicated irregular array structures with irregular communication patterns 2 . The programmer is burdened with the responsibility of managing dynamically changing data distributed across processor memories and orchestrating interprocessor communication and synchronization. <p> To our knowledge, this is the first time that structured adaptive mesh techniques have been used to solve eigenvalue problems in materials design. 1.3.4 Particle API Our particle API provides computational scientists high-level tools that simplify the implementation of particle applications <ref> [12, 87] </ref> on parallel computers. Particle methods are difficult to parallelize because they require dynamic, irregular data decompositions to balance changing non-uniform workloads. Built on top of the LPARX mechanisms, our particle API defines facilities specifically tailored towards particle methods. <p> Building on the LPARX mechanisms described in this chapter (see Figure 2.1), we have developed application-specific support libraries for two important classes of applications: multilevel structured adaptive mesh methods [94] and particle calculations <ref> [87] </ref>. In Chapters 4 and 5, we describe how LPARX provides the parallelization support infrastructure needed to efficiently and easily implement these re-usable APIs. This chapter is organized as follows. We begin with a description of the LPARX abstractions in Section 2.2. <p> They arise in two important classes of scientific computations: * multilevel structured adaptive finite difference methods [22, 23, 94], which rep resent refinement regions using block-irregular data structures, and * parallel computations such as particle methods <ref> [87] </ref> that require an irregular data decomposition [12, 21] to balance non-uniform workloads across parallel processors. We have used the LPARX mechanisms to implement domain-specific APIs and representative applications from each of these two problem classes. <p> All Grid elements must have the same type; they may be integers, floating point numbers, or any user-defined type or class. For example, in addition to representing a mesh of floating point numbers, the Grid may also be used to implement the spatial data structures <ref> [87] </ref> common in particle calculations. Grids may be manipulated using high-level block copy operations, described in Section 2.2.5. LPARX is targeted towards applications with irregular, block structures. To support such structures, it provides a special array|the XArray|for organizing a dynamic collection of Grids. <p> Finally, we conclude with a discussion and analysis of this work. 144 5.1.1 Motivation Simulations using particles play an important role in many fields of computational science, including astrophysics, fluid flow, molecular dynamics, and plasma physics <ref> [87] </ref>. In particle applications, some quantity of interest, such as mass or charge, is stored on bodies, called particles, which move unpredictably under their mutual influence. Particles interact according to a problem-dependent force law (e.g. gravitation). Computations proceed over a series of discrete timesteps. <p> Rapid approximation methods typically divide the force computation into two components: local particle-particle interactions and far-field force evaluation (not shown). 146 Approximation Algorithm Local Fast PDE Hierarchical Interactions Solver Representations Local Force Approximations [50, 108] p Particle-Particle Particle-Mesh <ref> [87] </ref> p p Particle-in-Cell (PIC) [87] p Method of Local Corrections (MLC) [8] p p Adaptive MLC [4, 5] p p p Fast Multipole Method (FMM) [79] p p Barnes-Hut [17] p p Hierarchical Element Method [9] p p Table 5.1: A survey of the computational structure for various N -body <p> Rapid approximation methods typically divide the force computation into two components: local particle-particle interactions and far-field force evaluation (not shown). 146 Approximation Algorithm Local Fast PDE Hierarchical Interactions Solver Representations Local Force Approximations [50, 108] p Particle-Particle Particle-Mesh <ref> [87] </ref> p p Particle-in-Cell (PIC) [87] p Method of Local Corrections (MLC) [8] p p Adaptive MLC [4, 5] p p p Fast Multipole Method (FMM) [79] p p Barnes-Hut [17] p p Hierarchical Element Method [9] p p Table 5.1: A survey of the computational structure for various N -body approximation methods. <p> that we do not show function CalculateForces because it has not changed from Figure 5.2. 152 and 5.3c), and (2) how do we dynamically redistribute computational effort as work-loads change? A common technique used to implement particle applications employs a chaining or binning mesh which covers the entire computational domain <ref> [87] </ref>. Particles are sorted into the mesh according to their spatial location; each element (or bin) of the mesh contains the particles lying in the corresponding portion of space. This binning structure is used to accelerate the O (N 2 ) search for neighboring particles that would be otherwise required. <p> For particle applications, each processor is typically assigned a single data partition, which corresponds to an XArray element (a Grid). Processors compute only for those particles within their assigned partition. a single timestep is limited by the stability requirements of the numerical method; therefore, workloads change slowly <ref> [12, 87, 108] </ref>. For example, the smoothed particle hydrodynamics application described in Section 5.3 repartitions every ten timesteps. Partitioning the computational domain introduces data dependencies between the various subproblems; particles near the boundary of a partition may interact with particles belonging to other processors. <p> ChainMesh and ParticleList are described briefly in the following two sections. ChainMesh ChainMesh implements the chaining mesh structure <ref> [87] </ref> for organizing the particles. Recall from Section 5.2.1 that this chaining mesh covers the entire computational domain, and particles are sorted into the mesh based on their spatial location. Each mesh element or bin contains a ParticleList of the particles lying in the corresponding region of space. <p> over all ParticleLists owned by this processor #define ForAll (I,MESH) : : : #define EndForAll : : : // ForAllInteracting loops over all particles J interacting with I #define ForAllInteracting (J,I,MESH) : : : #define EndForAllInteracting : : : ChainMesh, which implements the chaining mesh structure used to organize particles <ref> [87] </ref>. ChainMesh provides all of the parallelization mechanisms described previously: BalanceWorkloads, FetchParticles, WriteBack, and RepatriateParticles. It is implemented on top of the LPARX parallelization mechanisms. 162 RepatriateParticles. It also provides functions for adding particles to the mesh (AddParticle) and for indexing the mesh to extract a single list of particles. <p> ParticleList In addition to ChainMesh, the particle library defines a class called ParticleList to represent a list of particles. Such particle lists are not an artifact of the parallel implementation but are also required by serial codes. Traditionally, chaining meshes have represented particle lists using a linked list <ref> [87] </ref>. The advantage of the linked list strategy is that it is easy to add and remove particles by simply manipulating pointers. Instead of this approach, our implementation of ParticleList represents a list of particles using an array (see Figure 5.11). <p> These changes are simple and could be managed automatically using a pre-processor. ParticleList represents particle information using arrays of data instead of the linked list method typically used in chaining mesh codes <ref> [87] </ref>. Function PackArray is defined by the AMS libraries and packs an entire array of data into the outgoing message stream. 165 * Modify ParticleList to represent the same information as a Particle and write routines to copy a Particle into and out of a ParticleList.
Reference: [88] <author> Y.-S. Hwang, R. Das, J. Saltz, B. Brooks, and M. Hodoscek, </author> <title> Par-allelizing molecular dynamics programs for distributed memory machines: An application of the CHAOS runtime support library, </title> <type> Tech. Rep. </type> <institution> CS-TR-3374, University of Maryland, College Park, MD, </institution> <year> 1994. </year>
Reference-contexts: CHAOS implements pointwise mapping arrays for unstructured calculations such as sweeps over finite element meshes and sparse matrix computations [58]. It has also been used to parallelize portions of the CHARMM molecular dynamics application <ref> [59, 88] </ref>. The Fortran D run-time system employs CHAOS to support Fortran D's mapping arrays [139]. Recently, CHAOS has been extended to support unstructured applications consisting of complicated C ++ objects [42]. However, recall that such unstructured representations are inappropriate for the irregular but structured applications targeted by LPARX. <p> The parallelization of GROMOS would have been significantly easier had they employed the software abstractions we describe in Section 5.2. Portions of the CHARMM molecular dynamics application have been paral 149 lelized using the CHAOS software primitives <ref> [59, 88] </ref>. CHAOS employs a fine-grain decomposition strategy in which particles are individually assigned to processors. The drawback to this type of fine-grain approach is that algorithms for scheduling interprocessor communication scale as the number of particles. <p> The resulting partitioning is unstructured (see Figure 5.17b). 185 Pilkington and Baden [118] show that such decompositions can significantly reduce load imbalance. However, unstructured partitions employ different types of programming abstractions such as those provided by the CHAOS run-time system <ref> [88] </ref>. Unstructured implementations require more complicated communications analysis when fetching off-processor data and therefore may be more expensive in total execution time.
Reference: [89] <author> E. Jul, H. Levy, N. Hutchinson, and A. Black, </author> <title> Fine-grained mobility in the Emerald system, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6 (1988), </volume> <pages> pp. 100-133. </pages>
Reference-contexts: Another related paradigm developed by the distributed systems community is virtual shared memory, which provides the illusion of a single, shared, coherent address space for systems with physically distributed memories. Virtual shared memory models include page-based [101] and object-based systems <ref> [16, 46, 66, 89] </ref>. Page-based virtual shared memory enforces consistency at the level of the memory page, typically one to four thousand bytes. Because such a coarse page granularity results in poor performance due to false sharing, object-based systems provide consistency at the level of a single user-defined object.
Reference: [90] <author> L. Kale and S. Krishnan, CHARM++: </author> <title> A portable concurrent object oriented system based on C++, </title> <booktitle> in Proceedings of OOPSLA, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: The task parallel programming model takes a different approach in which programs consist of a number of asynchronous, independent, communicating parallel processes. Task parallel languages such as CC++ [41], CHARM [125], CHARM++ <ref> [90] </ref>, Fortran M [74], and Linda [39] define a set of mechanisms that coordinate process execution and communication among autonomous tasks. Task parallelism provides no explicit support for data decomposition. <p> Actors are concurrent objects which communicate with each other via messages. Actors execute in response to messages, and each actor object may contain several concurrently executing tasks. Actor-based languages include ABCL [145], Cantor [11], and Charm++ <ref> [90] </ref>. Implementations of actor languages require complicated compilation strategies and sophisticated run-time support libraries, such as the Concert system [91] for fine-grain object management. Because of this complexity, we have not based the LPARX run-time system on an existing concurrent object-based language.
Reference: [91] <author> V. Karamcheti and A. Chien, </author> <title> Concert: Efficient runtime support for concurrent object-oriented programming languages on stock hardware, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Actors execute in response to messages, and each actor object may contain several concurrently executing tasks. Actor-based languages include ABCL [145], Cantor [11], and Charm++ [90]. Implementations of actor languages require complicated compilation strategies and sophisticated run-time support libraries, such as the Concert system <ref> [91] </ref> for fine-grain object management. Because of this complexity, we have not based the LPARX run-time system on an existing concurrent object-based language. Instead, we borrowed features that were specifically needed for the LPARX implementation.

Reference: [96] <author> W. Kohn and L. </author> <title> Sham, </title> <journal> Physical Review, </journal> <note> 140 (1965), p. A1133. </note>
Reference-contexts: One common approach uses the Local Density Approximation (LDA) of Kohn and Sham <ref> [96] </ref>.
Reference: [97] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Ghara-chorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy, </author> <title> The Stanford FLASH multiprocessor, </title> <booktitle> in Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994, </year> <pages> pp. 302-313. 206 </pages>
Reference-contexts: Some parallel computers, such as the Intel Paragon and the IBM SP2, provide very little hardware support for managing data distributed across processor memories. Other machines, such as the Stanford FLASH <ref> [97] </ref> and the Wisconsin COW [122], contain hardware for the automatic caching of remote data. However, recent studies with these "distributed shared memory" machines [65, 109] indicate that such hardware caching mechanisms are inadequate for dynamic scientific applications.
Reference: [98] <author> J. R. Larus, </author> <title> C**: A large-grain object oriented data parallel programming language, </title> <booktitle> in Fifth International Workshop of Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Because decompositions may be easily modified at run-time, pC++ allows more flexibility than HPF for dynamic applications. Although pC++ does not currently support irregular decompositions, classes similar to LPARX's Region, Grid, and XArray could be written in pC++. The C** <ref> [98] </ref> language defines a coarse-grain, concurrent aggregate model of parallelism similar to that of pC++. However, it does not provide explicit data decomposition mechanisms; the application has no control whatsoever over data distribution.
Reference: [99] <author> M. Lemke and D. Quinlan, </author> <title> P++: A C++ virtual shared grids based programming environment for architecture-independent development of structured grid applications, </title> <booktitle> in Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: Quinlan has developed a parallel C ++ array class library called P++ <ref> [99] </ref> that supports fine-grain data parallel operations on arrays distributed across collections of processors. P++ automatically manages data decomposition, interprocessor communication, and synchronization. <p> Because of compiler limitations, their code did not execute efficiently on the CM-5. Quinlan et al. have developed an adaptive mesh library called AMR++ [120], based on the P++ data parallel C ++ array class library <ref> [99] </ref>. P++ supports fine-grain data parallel operations on arrays distributed across collections of processors; it automatically manages data decomposition, interprocessor communication, and synchronization. <p> Existing serial code may not need to be re-implemented when parallelizing an application. Furthermore, we can leverage existing mature sequential and vector compiler technology. data parallel style <ref> [26, 99] </ref>. In the former, we execute in parallel over the entire collection of grids. Each grid is assigned to one processor, and numerical computation on that grid is sequential. In contrast, fine-grain parallelism processes grids sequentially and expresses parallelism over the elements of a single grid. <p> Thus, C ++ makes it difficult to express parallel execution constructs such as parallel loops. Two partial solutions are (1) introduce "parallel control constructs" using C ++ macros and (2) hide parallelism within a data object. LPARX takes the first approach to implement its forall loop. P++ <ref> [99] </ref> takes the second; each P++ parallel array is invisibly divided across processors and the P++ programmer is unaware of the parallel execution of array operations within the library. The first strategy is not ideal, since it essentially creates a macro "sub-language" within C ++ .
Reference: [100] <author> E. C. Lewis, C. Lin, L. Synder, and G. Turkiyyah, </author> <title> A portable parallel n-body solver, </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Based on this framework, they have developed domain-specific libraries for adaptive mesh refinement applications in gas dynamics [19]. Their adaptive mesh refinement libraries have been parallelized using our software infrastructure [141] (see Section 3.4.1). The array sublanguage ZPL <ref> [100, 102] </ref> employs a form of region abstraction. ZPL does not explicitly manage data distribution, which it assumes is handled by another language. It uses its region constructs to simplify array indexing and as iteration masks; in contrast, LPARX employs Regions to specify run-time data decompositions and express communication dependencies.
Reference: [101] <author> K. Li and P. Hudak, </author> <title> Memory coherence in shared virtual memory systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7 (1989), </volume> <pages> pp. 321-359. </pages>
Reference-contexts: Another related paradigm developed by the distributed systems community is virtual shared memory, which provides the illusion of a single, shared, coherent address space for systems with physically distributed memories. Virtual shared memory models include page-based <ref> [101] </ref> and object-based systems [16, 46, 66, 89]. Page-based virtual shared memory enforces consistency at the level of the memory page, typically one to four thousand bytes.
Reference: [102] <author> C. Lin and L. Snyder, ZPL: </author> <title> An array sublanguage, </title> <booktitle> in Proceedings of the Sixth International Workshop on Languages and Compilers for Parallel Computation, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994, </year> <pages> pp. 96-114. </pages>
Reference-contexts: Based on this framework, they have developed domain-specific libraries for adaptive mesh refinement applications in gas dynamics [19]. Their adaptive mesh refinement libraries have been parallelized using our software infrastructure [141] (see Section 3.4.1). The array sublanguage ZPL <ref> [100, 102] </ref> employs a form of region abstraction. ZPL does not explicitly manage data distribution, which it assumes is handled by another language. It uses its region constructs to simplify array indexing and as iteration masks; in contrast, LPARX employs Regions to specify run-time data decompositions and express communication dependencies.
Reference: [103] <author> J. Mandel and S. McCormick, </author> <title> Multilevel variational method for Au = Bu on composite grids, </title> <journal> Journal of Computational Physics, </journal> <volume> 80 (1989), </volume> <pages> pp. 442-452. </pages>
Reference-contexts: Because we need only a few of the lowest eigenvalues, we do not apply standard dense eigenvalue algorithms (e.g. QR methods [78]) to the solution of Eq. 4.3. Instead, we use a multigrid-based iterative method by Mandel and McCormick <ref> [103] </ref> which efficiently calculates the lowest eigenvalue and associated eigenvector. This algorithm is shown in Figure 4.18. Cai et al. [38] have proven that this method is optimal in the sense that it requires O (N g ) work and O (1) iterations.
Reference: [104] <author> S. F. McCormick, ed., </author> <title> Multilevel Adaptive Methods for Partial Differential Equations, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1989. </year>
Reference-contexts: Unstructured adaptive methods [58, 62, 130] store the solution using graph or tree representations [107, 123]; these methods are called "unstructured" because connectivity information must be stored for each unknown (node) of the graph. Structured methods, such as adaptive mesh refinement [23] and structured multigrid algorithms <ref> [34, 104] </ref>, employ a hierarchy of nested mesh levels in which each level consists of many simple, rectangular grids. Each rectangular grid in the hierarchy represents a structured block of many thousands of unknowns. <p> On parallel computers, the calculation of data dependencies for unstructured problems scales as the number of unknowns, whereas algorithms for structured adaptive mesh methods scale as the number of patches. Furthermore, numerical kernels for structured adaptive mesh methods are simpler and more efficient than those for unstructured methods <ref> [104] </ref>. Solvers for structured methods may employ compact, high-order finite difference stencils. Indexing is fast and efficient since grid patches are essentially rectangular arrays. Numerical kernels typically make better use of the cache as array elements are stored contiguously in memory, improving cache locality, rather than scattered across memory.
Reference: [105] <author> Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface Standard (v1.0), </title> <month> May </month> <year> 1994. </year>
Reference-contexts: In fact, they are more likely to develop their own in-house software support rather than use existing products [113]. The most commonly used parallel programming paradigm today is message passing. Standardization efforts have resulted in a portable message passing library called MPI (Message 1 2 Passing Interface) <ref> [105] </ref>. Unfortunately, programming with message passing is tedious, as the programmer must explicitly manage low-level details of data placement and interprocessor communication. Developments in High Performance Fortran 1 (HPF) [83] are promising; unfortunately, HPF will require improvements before it becomes a general purpose parallel language. <p> the Cray C-90 (single processor), IBM SP2, Intel Paragon, single processor workstations (for code development and debugging), and networks of workstations connected via PVM [132]. 1.3.2 Implementation Abstractions At the very bottom of our software infrastructure is a portable message passing layer called MP ++ (our own version of MPI <ref> [105] </ref>). To simplify the implementation of the LPARX run-time system, we have introduced two levels of software abstrac 10 tion between LPARX and the message passing layer: Asynchronous Message Streams (AMS) and Distributed Parallel Objects (DPO). <p> section with an example of how these three layers interact with LPARX to implement the interprocessor communication necessary for LPARX Grid copies. 3.2.1 Message Passing Layer At the very bottom of the LPARX software hierarchy is MP ++ , an architecture-independent message passing layer similar in spirit to MPI 1 <ref> [105] </ref>. MP ++ provides facilities for asynchronous and synchronous point-to-point message passing, barrier synchronization, broadcasts, and global reductions. To port our software infrastructure (approximately thirty thousand lines of C ++ and Fortran code) to a new multiprocessor, we need only port the MP ++ library; no other code changes. <p> AMS requires only basic message passing support such as that provided by MPI <ref> [105] </ref>. Its message stream abstraction frees the programmer from many low-level message passing details. Both AMS and the Active Message [138] model provide mechanisms for sending a message to a handler which then consumes the message. An important difference is that AMS in intended for coarse-grain communication. <p> The primary advantage of interrupt driven message handlers is that they do not require polling calls in the code. Interrupt driven messages, though, have a number of drawbacks. First, interrupt mechanisms are not portable and vary significantly from multiprocessor to multiprocessor. In fact, some message passing libraries (e.g. MPI <ref> [105] </ref>) do not support interrupt driven messages. Second, interrupt driven message handlers must be careful when writing to global variables (i.e. those variables not local to the handler). Because there is little control over when interrupts occur, handlers may corrupt global state being modified by another routine. <p> Such portability issues apply not only to the design of the LPARX run-time system but to parallel programs in general. For example, how will MPI <ref> [105] </ref> message passing applications run on a shared memory architecture? MPI programs will require the implementation of a shared memory message passing library similar to what we implemented for the Cray C-90.
Reference: [106] <author> R. E. Minnear, P. A. Muckelbauer, and V. F. Russo, </author> <title> Integrating the Sun Microsystems XDR/RPC protocols into the C++ stream model, </title> <booktitle> in USENIX C++ Conference Proceedings, </booktitle> <year> 1992. </year>
Reference-contexts: They build on ideas from the concurrent object oriented programming community. AMS defines a "message stream" abstraction that greatly simplifies the communication of complicated data structures between processors. Its mechanisms combine ideas from asynchronous remote procedure calls [7, 110], Active Messages [138], and the C ++ I/O stream library <ref> [106, 131] </ref>. <p> The AMS abstractions combine ideas from Active Messages [138], asynchronous remote procedure calls [7, 110], and the C ++ I/O stream model <ref> [106, 131] </ref>. Another related paradigm developed by the distributed systems community is virtual shared memory, which provides the illusion of a single, shared, coherent address space for systems with physically distributed memories. Virtual shared memory models include page-based [101] and object-based systems [16, 46, 66, 89]. <p> Active Messages [138] is an asynchronous communication mechanism which, like asynchronous remote procedure calls [7, 110], sends a message to a specified function that executes on message arrival. AMS combines this asynchronous message delivery mechanism with the concept of a message stream <ref> [106, 131] </ref> to hide message buffer management details. Active Messages is optimized for message sizes of only a few tens of bytes whereas AMS messages are typically hundreds or thousands of bytes long. <p> University of California at San Diego, MP ++ has been used on workstations to teach message passing parallel programming. 3.2.2 Asynchronous Message Streams The Asynchronous Message Stream (AMS) communication paradigm builds on ideas from asynchronous remote procedure calls [7, 110], Active Messages [138], and the C ++ I/O stream library <ref> [106, 131] </ref>. AMS requires only basic message passing support such as that provided by MPI [105]. Its message stream abstraction frees the programmer from many low-level message passing details. <p> They hide all details of message buffer management from the programmer. AMS automatically packetizes the message stream and coordinates interprocessor communication through the message passing layer. Because the application is shielded from the internal representation of data in the message stream, AMS could transparently encode 61 data <ref> [106, 132] </ref> for transmission among heterogenous processors which use different data representations; our current AMS implementation does not provide this service because of the high cost of changing data formats.
Reference: [107] <author> W. F. Mitchell, </author> <title> Refinement tree based partitioning for adaptive grids, </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: In general, adaptive methods may be structured or unstructured, depending on how they represent the numerical solution to the problem, as shown in Figure 4.2. Unstructured adaptive methods [58, 62, 130] store the solution using graph or tree representations <ref> [107, 123] </ref>; these methods are called "unstructured" because connectivity information must be stored for each unknown (node) of the graph.
Reference: [108] <author> J. J. Monaghan, </author> <title> Smoothed particle hydrodynamics, </title> <booktitle> Annual Review of Astronomy and Astrophysics, 30 (1992), </booktitle> <pages> pp. 543-574. </pages>
Reference-contexts: Such applications may involve the solution of complicated, time-dependent partial differential equations such as those in materials design [37], computational fluid dynamics [22], or localized deformations in geophysical systems. Also included are particle methods in molecular dynamics [50], astrophysics <ref> [108] </ref>, and vortex dynamics [12]. More recently, adaptive methods have been applied to the study of entire ecosystems through satellite imagery at multiple resolutions [135]. These applications are particularly challenging to implement on parallel computers owing to their dynamic, irregular decompositions of data. <p> The use of the LPARX abstractions enabled us to provide functionality and explore performance optimizations that would have been difficult had the library been implemented using only a primitive message passing layer. Using our software infrastructure, we have developed a 3d smoothed particle hydrodynamics <ref> [108] </ref> code (SPH3D) that simulates the evolution of galactic bodies in astrophysics, and we are currently developing a 3d molecular dynamics application (MD) to study fracture dynamics in solids [1]. 1.4 Organization of the Dissertation Sixty minutes of thinking of any kind is bound to lead to confusion and unhappiness. | <p> Our facilities are independent of the problem's spatial dimension and present the same interface for both 2d and 3d applications. We show that such functionality is easily expressed using the LPARX primitives. Using our software infrastructure, we have developed a 3d smoothed particle hydrodynamics <ref> [108] </ref> application (SPH3D in Figure 5.1) which simulates the evolution of galactic bodies in astrophysics. Our API's high-level mechanisms have enabled us 142 143 of LPARX and provides computational scientists with high-level facilities targeted towards particle applications. <p> Rapid approximation methods typically divide the force computation into two components: local particle-particle interactions and far-field force evaluation (not shown). 146 Approximation Algorithm Local Fast PDE Hierarchical Interactions Solver Representations Local Force Approximations <ref> [50, 108] </ref> p Particle-Particle Particle-Mesh [87] p p Particle-in-Cell (PIC) [87] p Method of Local Corrections (MLC) [8] p p Adaptive MLC [4, 5] p p p Fast Multipole Method (FMM) [79] p p Barnes-Hut [17] p p Hierarchical Element Method [9] p p Table 5.1: A survey of the computational <p> For particle applications, each processor is typically assigned a single data partition, which corresponds to an XArray element (a Grid). Processors compute only for those particles within their assigned partition. a single timestep is limited by the stability requirements of the numerical method; therefore, workloads change slowly <ref> [12, 87, 108] </ref>. For example, the smoothed particle hydrodynamics application described in Section 5.3 repartitions every ten timesteps. Partitioning the computational domain introduces data dependencies between the various subproblems; particles near the boundary of a partition may interact with particles belonging to other processors. <p> ghost cell region. (Prior to moving particles, we remove from the ghost cell region the off-processor particles locally cached by Fetch-Particles.) Particles will not move past the ghost cell region because the numerical methods limit the maximum distance a particle may move in a single timestep due to stability requirements <ref> [108] </ref>. Because these particles no longer lie in their processor's partition, they must be communicated to the processors which rightfully own them. The computational structure of repatriation is identical to that of the force 160 update described in the previous section. <p> Smoothed particle hydrodynamics is a particle-based simulation method which has been applied to gas dynamics, stellar collisions, planet formation, cloud collisions, cosmology, magnetic phenomena, and nearly incompressible flow <ref> [108] </ref>. Our particular application 2 arises in astrophysics and models the evolution of the disk galaxy shown in Figure 5.12. The computational structure of our smoothed particle hydrodynamics application is similar to that of the generic particle codes shown in Figures 5.2 and 5.4. <p> of things that have to be accepted on faith! It's a religion! As a math atheist, I should be excused from this: : : | Calvin, "Calvin and Hobbes" Smoothed particle hydrodynamics represents each particle not as a point but as a smooth "blob" smeared over a portion of space <ref> [108] </ref>. The general form of a blob is given by the interaction basis function, or kernel, . <p> The application dynamically changes the timestep t to satisfy stability criteria such as the Courant-Friedrichs-Lewy (CFL) condition <ref> [108] </ref>. 5.3.2 Performance Comparison We present performance results for the SPH3D application on the Cray C-90 (single processor), Intel Paragon, IBM SP2 3 , and a network of Alpha workstations connected by a GIGAswitch running PVM [132]; refer to Table 5.3 for software versions and Appendix A for machine characteristics. <p> We may eliminate the square dependence on the number of particles by reducing particle interaction distances as the local density increases <ref> [108] </ref>, but we have not taken this approach in these simulations. We employed a chaining mesh of size 28fi28fi14 on 1, 4, 8, and 16 processors and 56 fi 56 fi 14 on 32 and 64 processors.
Reference: [109] <author> S. S. Mukherjee, S. D. Sharman, M. D. Hill, J. R. Larus, A. Rogers, and J. Saltz, </author> <title> Efficient support for irregular applications on distributed memory machines, </title> <booktitle> in to appear in Proceedings of the 1995 Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1995. </year>
Reference-contexts: Other machines, such as the Stanford FLASH [97] and the Wisconsin COW [122], contain hardware for the automatic caching of remote data. However, recent studies with these "distributed shared memory" machines <ref> [65, 109] </ref> indicate that such hardware caching mechanisms are inadequate for dynamic scientific applications. <p> Implementing dynamic, irregular computations on parallel computers is a difficult task. To achieve reasonable parallel performance, the application must explicitly manage low-level details of data locality and communication, even on shared memory multiprocessors <ref> [109, 127] </ref>. This burden soon becomes unmanageable and can obscure the salient features of the algorithm. LPARX hides many of these implementation details and provides high-level coordination mechanisms to manage data locality within the memory hierarchy and minimize communication costs. <p> Experiments with the Wisconsin Wind Tunnel shared memory simulator [121] indicate that the explicit management of data locality can dramatically improve performance for dynamic scientific applications [65]. Instead of relying on the hardware cache coherence mechanism alone, irregular calculations employ specialized communication scheduling techniques <ref> [109] </ref> similar to those pioneered in CHAOS [60]. Thus, efficient shared memory implementations require the same memory management techniques as efficient message passing implementations. and it is not sufficient to rely on automatic hardware caching mechanisms. <p> Studies with distributed shared memory computers indicate that dynamic, irregular applications can dramatically improve performance by explicitly managing cache locality <ref> [65, 109] </ref>; it is simply 84 not sufficient to rely on hardware caching mechanisms. <p> On message passing platforms, the programmer must explicitly manage grid data distributed across the processor memories and orchestrate interprocessor communication. Even shared memory multiprocessors require the explicit, low-level management of data locality and communication for reasonable performance <ref> [109, 127] </ref>. These implementation difficulties soon become unmanageable and can obscure the mathematics behind the algorithms. The goal of our adaptive mesh API is to provide scientists with high-level support for structured adaptive mesh applications.
Reference: [110] <author> B. J. Nelson, </author> <title> Remote Procedure Call, </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1981. </year> <month> 207 </month>
Reference-contexts: They build on ideas from the concurrent object oriented programming community. AMS defines a "message stream" abstraction that greatly simplifies the communication of complicated data structures between processors. Its mechanisms combine ideas from asynchronous remote procedure calls <ref> [7, 110] </ref>, Active Messages [138], and the C ++ I/O stream library [106, 131]. <p> For example, DPO's distributed object naming mechanisms and its notion of primary and secondary objects (described in Section 3.2.3) are based in part on the distributed facilities described by Deshpande et al. [61]. The AMS abstractions combine ideas from Active Messages [138], asynchronous remote procedure calls <ref> [7, 110] </ref>, and the C ++ I/O stream model [106, 131]. Another related paradigm developed by the distributed systems community is virtual shared memory, which provides the illusion of a single, shared, coherent address space for systems with physically distributed memories. <p> Our implementation eliminates the costly communication needed to translate object names at the cost of additional, although acceptable, memory overheads (see Section 3.3.2). Active Messages [138] is an asynchronous communication mechanism which, like asynchronous remote procedure calls <ref> [7, 110] </ref>, sends a message to a specified function that executes on message arrival. AMS combines this asynchronous message delivery mechanism with the concept of a message stream [106, 131] to hide message buffer management details. <p> For the past two years at the University of California at San Diego, MP ++ has been used on workstations to teach message passing parallel programming. 3.2.2 Asynchronous Message Streams The Asynchronous Message Stream (AMS) communication paradigm builds on ideas from asynchronous remote procedure calls <ref> [7, 110] </ref>, Active Messages [138], and the C ++ I/O stream library [106, 131]. AMS requires only basic message passing support such as that provided by MPI [105]. Its message stream abstraction frees the programmer from many low-level message passing details.
Reference: [111] <author> I. </author> <title> Newton, Philosophiae Naturalis Principia Mathematica, </title> <type> 1687. </type>
Reference-contexts: for both 2d and 3d applications. 5.2.3 Writing Back Particle Information Many of the force laws employed by particle applications are symmetric; that is, the force acting on particle p by particle q is equal and opposite to the force acting on particle q by particle p (Newton's Third Law <ref> [111] </ref>). By exploiting this symmetry, we reduce computational costs by about half, since once we have calculated the force acting on p by q, we know that the force acting on q by p is the same but in the opposite direction.
Reference: [112] <author> C. M. Pancake and D. Bergmark, </author> <title> Do parallel languages respond to the needs of scientific programmers?, </title> <booktitle> IEEE Computer, </booktitle> <year> (1990), </year> <pages> pp. 13-23. </pages>
Reference-contexts: Our parallel software infrastructure addresses two goals of software support for scientific applications <ref> [112] </ref>: * it hides low-level details of the hardware, and * it provides high-level, efficient mechanisms that match the scientist's view of the computation. The first goal is necessary for portability.
Reference: [113] <author> C. M. Pancake and C. Cook, </author> <title> What users need in parallel tool support: Survey results and analysis, </title> <booktitle> in Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: The vast majority of scientific programmers find that current parallel software support is inadequate [53]. In fact, they are more likely to develop their own in-house software support rather than use existing products <ref> [113] </ref>. The most commonly used parallel programming paradigm today is message passing. Standardization efforts have resulted in a portable message passing library called MPI (Message 1 2 Passing Interface) [105].
Reference: [114] <author> Parallel Compiler Runtime Consortium, </author> <title> Common Runtime Support for High-Performance Parallel Languages, </title> <month> July </month> <year> 1993. </year>
Reference-contexts: Unfortunately, the High Performance Fortran specification [83] does not define a standard interface for external languages; instead, it allows manufacturers to develop their own external array representations. The Parallel Compiler Runtime Consortium (PCRC) is developing standard language interoperability mechanisms between run-time libraries, task parallel languages, and data parallel compilers <ref> [114] </ref>; however, interfacing to HPF in a portable manner is still an open research question. 2.5.6 Communication Model When designing LPARX, we determined that the basic communication mechanism would be a block copy between two individual Grids.
Reference: [115] <author> M. Parashar and J. C. Browne, </author> <title> An infrastructure for parallel adaptive mesh refinement techniques. </title> <type> (draft), </type> <year> 1995. </year>
Reference-contexts: The basic abstractions employed in Crutchfield's work are very similar to our own; in fact, their adaptive mesh refinement libraries have been parallelized using the LPARX software [141]. Parashar and Browne are developing a software infrastructure supporting parallel adaptive mesh refinement methods for black hole interactions <ref> [115] </ref>. Their method is based on a clever load balancing and processor mapping strategy that maps grids to processors through locality-preserving space filling curves [118, 124].
Reference: [116] <author> M. Parashar, S. Hariri, T. Haupt, and G. C. Fox, </author> <title> Interpreting the performance of HPF/Fortran 90D, </title> <booktitle> in Proceedings of Supercomputing '94, </booktitle> <month> Novem-ber </month> <year> 1994. </year>
Reference-contexts: Data Parallel Fortran Languages High Performance Fortran (HPF) [83] is a data parallel Fortran which combines the array operations of Fortran 90, a parallel forall loop, and data decomposition directives based on the research languages Fortran D [77, 86] and Fortran 90D <ref> [33, 116, 143] </ref>. It is quickly becoming accepted by a number of manufacturers as a standard parallel programming language for scientific computing. HPF has been targeted towards regular, static applications such as dense linear algebra but provides little support for irregular, dynamic computations [44, 84]. <p> The multiblock PARTI library is targeted towards block-structured applica 43 tions. It supports the uniform BLOCK, BLOCK CYCLIC, and CYCLIC array decompositions of HPF and has been used in the run-time system for the Fortran 90D compiler <ref> [33, 116, 143] </ref>. Multiblock PARTI defines canned routines that fill ghost cells and copy regular sections between arrays.
Reference: [117] <author> R. Parsons and D. Quinlan, </author> <title> Run-time recognition of task parallelism within the P++ parallel array class library, </title> <booktitle> in Scalable Libraries Conference, </booktitle> <year> 1993. </year>
Reference-contexts: In contrast to the fine-grain parallelism of P++, LPARX employs coarse-grain parallelism, which is a better match to current coarse-grain message passing architectures because it allows more asynchrony between processors. Indeed, to improve the efficiency of the fine-grain model, Parsons and Quin-lan <ref> [117] </ref> are developing run-time methods for automatically extracting coarse-grain tasks from P++. The POOMA 6 (Parallel Object Oriented Methods and Applications) project at Los Alamos National Laboratories is developing a parallel run-time system for scientific simulations. <p> In contrast to this fine-grain array parallelism, we employ a coarse-grain 91 parallelism in which operations are applied in parallel to entire collections of arrays (see Section 4.3.6). Fine-grain parallelism is difficult to implement efficiently on today's coarse-grain architectures; indeed, Parsons and Quinlan <ref> [117] </ref> are developing techniques to extract coarse-grain parallelism from P++ to improve the efficiency of the fine-grain approach. An object oriented library for structured adaptive mesh refinement has been developed at Lawrence Livermore National Laboratory by Crutchfield et al. [55]. <p> Coarse-grain parallelism also allows more asynchrony between processors and is therefore a better match to current coarse-grain message passing architectures. To improve the efficiency of the fine-grain model, Parsons and Quinlan <ref> [117] </ref> are developing run-time methods for automatically extracting coarse-grain tasks from fine-grain data parallel loops. Another model, processor subsets [44], combines the coarse-grain and fine-grain approaches; parallelism is expressed both over grids and within each grid.
Reference: [118] <author> J. R. Pilkington and S. B. Baden, </author> <title> Dynamic partitioning of non-uniform structured workloads with spacefilling curves. </title> <journal> (submitted to IEEE Transactions on Parallel and Distributed Systems), </journal> <year> 1995. </year>
Reference-contexts: Parashar and Browne are developing a software infrastructure supporting parallel adaptive mesh refinement methods for black hole interactions [115]. Their method is based on a clever load balancing and processor mapping strategy that maps grids to processors through locality-preserving space filling curves <ref> [118, 124] </ref>. However, their approach imposes two restrictions on the grid hierarchy: (1) all refinement regions must be the same size, and (2) refinement regions must be nested in a tree structure 1 . <p> This drawback is also an advantage, however, because RCB renders structured, boxy partitions which are easily and efficiently supported by LPARX. An alternative partitioning strategy, such as Inverse Spacefilling Partitioning (ISP) <ref> [118] </ref>, is better at balancing workloads because it allows the cuts to meander through the space. The resulting partitioning is unstructured (see Figure 5.17b). 185 Pilkington and Baden [118] show that such decompositions can significantly reduce load imbalance. <p> An alternative partitioning strategy, such as Inverse Spacefilling Partitioning (ISP) <ref> [118] </ref>, is better at balancing workloads because it allows the cuts to meander through the space. The resulting partitioning is unstructured (see Figure 5.17b). 185 Pilkington and Baden [118] show that such decompositions can significantly reduce load imbalance. However, unstructured partitions employ different types of programming abstractions such as those provided by the CHAOS run-time system [88]. Unstructured implementations require more complicated communications analysis when fetching off-processor data and therefore may be more expensive in total execution time.
Reference: [119] <author> W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flan-nery, </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing, </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: In this section, we briefly describe Brandt's Full Approximation Storage (FAS) variant [34] of multigrid; further details can be found elsewhere <ref> [35, 119] </ref>. We wish to solve the partial differential equation Lu = 0 subject to Dirichlet boundary conditions on the composite grid u consisting of L + 1 levels: u l ; 0 l L. In general, L is a nonlinear operator. <p> Thus, we exploit the symmetric nature of the force law to reduce computational costs by about a factor of two. Using the acceleration information from Eq. 5.3, we update the velocity and position of particle i using the first-order Euler's method <ref> [119] </ref>: v i = v i + a i t (5:6) where t represents the timestep.
Reference: [120] <author> D. Quinlan, </author> <title> Parallel Adaptive Mesh Refinement, </title> <type> PhD thesis, </type> <institution> University of Colorado at Denver, </institution> <year> 1993. </year>
Reference-contexts: Our experiments indicate that uniform refinement regions also result in excessive overheads in three dimensions (see Section 4.5.3). Because of compiler limitations, their code did not execute efficiently on the CM-5. Quinlan et al. have developed an adaptive mesh library called AMR++ <ref> [120] </ref>, based on the P++ data parallel C ++ array class library [99]. P++ supports fine-grain data parallel operations on arrays distributed across collections of processors; it automatically manages data decomposition, interprocessor communication, and synchronization.
Reference: [121] <author> S. K. Reinhardt, M. D. Hill, J. R. Larus, A. R. Lebeck, J. C. Lewis, and D. A. Wood, </author> <title> The Wisconsin wind tunnel: Virtual prototyping of parallel computers, </title> <booktitle> in Proceedings of the 1993 AMC SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Unfortunately, this is not always the case [6, 127]. Experiments with the Wisconsin Wind Tunnel shared memory simulator <ref> [121] </ref> indicate that the explicit management of data locality can dramatically improve performance for dynamic scientific applications [65]. Instead of relying on the hardware cache coherence mechanism alone, irregular calculations employ specialized communication scheduling techniques [109] similar to those pioneered in CHAOS [60].
Reference: [122] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood, Typhoon and tempest: </author> <title> User-level shared memory, </title> <booktitle> in Proceedings of the ACM/IEEE International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Some parallel computers, such as the Intel Paragon and the IBM SP2, provide very little hardware support for managing data distributed across processor memories. Other machines, such as the Stanford FLASH [97] and the Wisconsin COW <ref> [122] </ref>, contain hardware for the automatic caching of remote data. However, recent studies with these "distributed shared memory" machines [65, 109] indicate that such hardware caching mechanisms are inadequate for dynamic scientific applications.
Reference: [123] <author> M.-C. Rivara, </author> <title> Design and data structure of fully adaptive, multigrid, finite element software, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 10 (1984), </volume> <pages> pp. 242-264. </pages>
Reference-contexts: In general, adaptive methods may be structured or unstructured, depending on how they represent the numerical solution to the problem, as shown in Figure 4.2. Unstructured adaptive methods [58, 62, 130] store the solution using graph or tree representations <ref> [107, 123] </ref>; these methods are called "unstructured" because connectivity information must be stored for each unknown (node) of the graph.
Reference: [124] <author> H. Samet, </author> <title> The Design and Analysis of Spatial Data Structures, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year> <month> 208 </month>
Reference-contexts: Parashar and Browne are developing a software infrastructure supporting parallel adaptive mesh refinement methods for black hole interactions [115]. Their method is based on a clever load balancing and processor mapping strategy that maps grids to processors through locality-preserving space filling curves <ref> [118, 124] </ref>. However, their approach imposes two restrictions on the grid hierarchy: (1) all refinement regions must be the same size, and (2) refinement regions must be nested in a tree structure 1 .
Reference: [125] <author> W. W. Shu and L. V. Kale, </author> <title> Chare kernel: A runtime support system for parallel computations, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11 (1990), </volume> <pages> pp. 198-211. </pages>
Reference-contexts: The task parallel programming model takes a different approach in which programs consist of a number of asynchronous, independent, communicating parallel processes. Task parallel languages such as CC++ [41], CHARM <ref> [125] </ref>, CHARM++ [90], Fortran M [74], and Linda [39] define a set of mechanisms that coordinate process execution and communication among autonomous tasks. Task parallelism provides no explicit support for data decomposition.
Reference: [126] <author> J. P. Singh, </author> <title> Parallel Hierarchical N-Body Methods and their Implications for Multiprocessors, </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1993. </year>
Reference-contexts: Warren and Salmon [140] developed a parallel tree code intended for Barnes-Hut [17] and fast multipole [79] methods. Their approach dynamically distributes nodes of the tree across processors using a clever hashing mechanism. Singh et al. <ref> [126, 128] </ref> have also implemented a parallel fast multipole algorithm for shared memory multiprocessors.
Reference: [127] <author> J. P. Singh and J. L. Hennessy, </author> <title> Finding and exploiting parallelism in an ocean simulation program: Experiences, results, and implications, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15 (1992), </volume> <pages> pp. 27-48. </pages>
Reference-contexts: Implementing dynamic, irregular computations on parallel computers is a difficult task. To achieve reasonable parallel performance, the application must explicitly manage low-level details of data locality and communication, even on shared memory multiprocessors <ref> [109, 127] </ref>. This burden soon becomes unmanageable and can obscure the salient features of the algorithm. LPARX hides many of these implementation details and provides high-level coordination mechanisms to manage data locality within the memory hierarchy and minimize communication costs. <p> Unfortunately, this is not always the case <ref> [6, 127] </ref>. Experiments with the Wisconsin Wind Tunnel shared memory simulator [121] indicate that the explicit management of data locality can dramatically improve performance for dynamic scientific applications [65]. <p> On message passing platforms, the programmer must explicitly manage grid data distributed across the processor memories and orchestrate interprocessor communication. Even shared memory multiprocessors require the explicit, low-level management of data locality and communication for reasonable performance <ref> [109, 127] </ref>. These implementation difficulties soon become unmanageable and can obscure the mathematics behind the algorithms. The goal of our adaptive mesh API is to provide scientists with high-level support for structured adaptive mesh applications.
Reference: [128] <author> J. P. Singh, C. Holt, J. L. Hennessy, and A. Gupta, </author> <title> A parallel adaptive fast multipole method, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Warren and Salmon [140] developed a parallel tree code intended for Barnes-Hut [17] and fast multipole [79] methods. Their approach dynamically distributes nodes of the tree across processors using a clever hashing mechanism. Singh et al. <ref> [126, 128] </ref> have also implemented a parallel fast multipole algorithm for shared memory multiprocessors.
Reference: [129] <author> L. Snyder, </author> <title> Type architectures, shared memory, and the corollary of modest potential, </title> <booktitle> Annual Review of Computer Science, </booktitle> <year> (1986), </year> <pages> pp. 289-317. </pages>
Reference-contexts: Such questions require further study. Part of the difficulty in writing portable parallel programs is that no single, unifying, realistic model of parallel computation and communication has emerged. There have been several attempts to define a unifying model, such as PMH [6], LogP [56], BSP [137], and CTA <ref> [129] </ref>. However, these models tend to focus on performance evaluation. What is needed is a single set of realistic, portable, general purpose mechanisms for efficient parallel programming. Without high-level support, programmers are left to employ different implementations on different architectures, hampering portability.
Reference: [130] <author> L. Stals, </author> <title> Adaptive multigrid in parallel, </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: In general, adaptive methods may be structured or unstructured, depending on how they represent the numerical solution to the problem, as shown in Figure 4.2. Unstructured adaptive methods <ref> [58, 62, 130] </ref> store the solution using graph or tree representations [107, 123]; these methods are called "unstructured" because connectivity information must be stored for each unknown (node) of the graph.
Reference: [131] <author> B. Stroustrup, </author> <title> The C++ Programming Language (Second Edition), </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: They build on ideas from the concurrent object oriented programming community. AMS defines a "message stream" abstraction that greatly simplifies the communication of complicated data structures between processors. Its mechanisms combine ideas from asynchronous remote procedure calls [7, 110], Active Messages [138], and the C ++ I/O stream library <ref> [106, 131] </ref>. <p> The AMS abstractions combine ideas from Active Messages [138], asynchronous remote procedure calls [7, 110], and the C ++ I/O stream model <ref> [106, 131] </ref>. Another related paradigm developed by the distributed systems community is virtual shared memory, which provides the illusion of a single, shared, coherent address space for systems with physically distributed memories. Virtual shared memory models include page-based [101] and object-based systems [16, 46, 66, 89]. <p> Active Messages [138] is an asynchronous communication mechanism which, like asynchronous remote procedure calls [7, 110], sends a message to a specified function that executes on message arrival. AMS combines this asynchronous message delivery mechanism with the concept of a message stream <ref> [106, 131] </ref> to hide message buffer management details. Active Messages is optimized for message sizes of only a few tens of bytes whereas AMS messages are typically hundreds or thousands of bytes long. <p> University of California at San Diego, MP ++ has been used on workstations to teach message passing parallel programming. 3.2.2 Asynchronous Message Streams The Asynchronous Message Stream (AMS) communication paradigm builds on ideas from asynchronous remote procedure calls [7, 110], Active Messages [138], and the C ++ I/O stream library <ref> [106, 131] </ref>. AMS requires only basic message passing support such as that provided by MPI [105]. Its message stream abstraction frees the programmer from many low-level message passing details.
Reference: [132] <author> V. S. Sunderam, </author> <title> PVM: A framework for parallel distributed computing, </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 2 (1990), </volume> <pages> pp. 315-339. </pages>
Reference-contexts: LPARX assumes only basic message passing support and is therefore portable to a variety of high-performance computing platforms. Our current implementation runs on the Cray C-90 (single processor), IBM SP2, Intel Paragon, single processor workstations (for code development and debugging), and networks of workstations connected via PVM <ref> [132] </ref>. 1.3.2 Implementation Abstractions At the very bottom of our software infrastructure is a portable message passing layer called MP ++ (our own version of MPI [105]). <p> LPARX requires only basic message passing support and is therefore portable to a variety of high-performance computing platforms. Our current implementation runs on the Cray C-90 (single processor), IBM SP2, Intel Paragon, and networks of workstations connected via PVM <ref> [132] </ref>. LPARX applications may be developed and debugged on a single processor workstation. 13 14 structured representations. Based on the LPARX mechanisms, we have developed application-specific APIs for particle computations and structured adaptive mesh methods. <p> For instance, MP ++ message send routine mpSend is implemented using csend on the Intel Paragon and mpc bsend on the IBM SP2. Our software is currently running on the Cray C-90 (single processor), IBM SP2, Intel Paragon, single processor workstations, and networks of workstations under PVM <ref> [132] </ref>. In the past, MP ++ has also supported the Intel iPSC/860, Kendall Square Research KSR-1, nCUBE nCUBE/2, and Thinking Machines CM-5, all of which are now obsolete. <p> They hide all details of message buffer management from the programmer. AMS automatically packetizes the message stream and coordinates interprocessor communication through the message passing layer. Because the application is shielded from the internal representation of data in the message stream, AMS could transparently encode 61 data <ref> [106, 132] </ref> for transmission among heterogenous processors which use different data representations; our current AMS implementation does not provide this service because of the high cost of changing data formats. <p> changes the timestep t to satisfy stability criteria such as the Courant-Friedrichs-Lewy (CFL) condition [108]. 5.3.2 Performance Comparison We present performance results for the SPH3D application on the Cray C-90 (single processor), Intel Paragon, IBM SP2 3 , and a network of Alpha workstations connected by a GIGAswitch running PVM <ref> [132] </ref>; refer to Table 5.3 for software versions and Appendix A for machine characteristics. Time is reported in seconds per timestep. All floating point arithmetic was performed using 64-bit numbers. <p> Our software is portable across a wide range of MIMD parallel platforms and is currently running on the Cray C-90 (single processor), IBM SP2, Intel Paragon, and networks of workstations via PVM <ref> [132] </ref>. We have designed and implemented application programmer interfaces (APIs) for two important classes of scientific applications: structured adaptive mesh applications and particle calculations. These APIs provide computational tools that match the scientist's view of the application. <p> The Alphas are connected via a GIGAswitch and communicate through PVM <ref> [132] </ref>. Even though the C-90 contains more than one processor, it is rarely used as a true parallel machine in production mode. Instead, the processors run several independent jobs at the same time. Thus, we have only reported performance results for a single processor.
Reference: [133] <author> P. Tamayo, J. P. Mesirov, and B. M. Boghosian, </author> <title> Parallel approaches to short range molecular dynamics simulations, </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: His implementation of a 2d vortex dynamics application was the first to employ a recursive bisection decomposition [21] to dynamically balance particle methods. Tamayo et al. <ref> [133] </ref> investigated various data parallel implementation strategies for molecular dynamics simulations running on SIMD computers such as the CM-2. More recently, Figueira and Baden [68] employed our software infrastructure to analyze the performance of different parallelization strategies for localized N -body methods running on MIMD multiprocessors.
Reference: [134] <author> E. Tsuchida and M. Tsukada, </author> <title> Real space approach to electronic-structure calculations. </title> <institution> Department of Physics, University of Tokyo (unpublished manuscript), </institution> <year> 1994. </year>
Reference-contexts: The need for adaptive refinement in materials design has been noted by other researchers. Bernholc et al. [27] have implemented a semi-adaptive code that places a single, static refinement patch over each of their atoms. Others have attempted adaptive solutions using either finite element methods <ref> [134, 142] </ref> or a combination of finite element and wavelet techniques [48]. Although finite element methods pro 116 such as the C 20 H 20 ring shown here.
Reference: [135] <author> C. J. Turner and J. G. Turner, </author> <title> Adaptive data parallel methods for ecosystem monitoring, </title> <booktitle> in Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Also included are particle methods in molecular dynamics [50], astrophysics [108], and vortex dynamics [12]. More recently, adaptive methods have been applied to the study of entire ecosystems through satellite imagery at multiple resolutions <ref> [135] </ref>. These applications are particularly challenging to implement on parallel computers owing to their dynamic, irregular decompositions of data. Our research addresses the programming abstractions and the accompanying software support required for dynamic, irregular, block-structured scientific computations running on MIMD [72] parallel computers.
Reference: [136] <author> R. v. Hanxleden, K. Kennedy, and J. Saltz, </author> <title> Value-based distributions in Fortran D: A preliminary report, </title> <type> Tech. Rep. </type> <institution> CRPC-TR93365-S, Center for Research on Parallel Computation, Rice University, Houston, TX, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: In contrast, we use a coarse-grain strategy that assigns aggregates of particles to processors, and our communication algorithms depend only on the number of processors, not on the number of particles. Experimental data distribution primitives targeted towards particle calculations have been added to Fortran D <ref> [136] </ref> using CHAOS as the run-time support library. Adhara [10] is a run-time library for particle applications that is not as general as CHAOS but has been specifically designed and optimized for certain classes of particle methods.
Reference: [137] <author> L. G. Valiant, </author> <title> A bridging model for parallel computation, </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 33 (1990), </volume> <pages> pp. 103-111. 209 </pages>
Reference-contexts: Such questions require further study. Part of the difficulty in writing portable parallel programs is that no single, unifying, realistic model of parallel computation and communication has emerged. There have been several attempts to define a unifying model, such as PMH [6], LogP [56], BSP <ref> [137] </ref>, and CTA [129]. However, these models tend to focus on performance evaluation. What is needed is a single set of realistic, portable, general purpose mechanisms for efficient parallel programming. Without high-level support, programmers are left to employ different implementations on different architectures, hampering portability.
Reference: [138] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser, </author> <title> Active Messages: A mechanism for integrated communication and computation, </title> <booktitle> in Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: They build on ideas from the concurrent object oriented programming community. AMS defines a "message stream" abstraction that greatly simplifies the communication of complicated data structures between processors. Its mechanisms combine ideas from asynchronous remote procedure calls [7, 110], Active Messages <ref> [138] </ref>, and the C ++ I/O stream library [106, 131]. <p> For example, DPO's distributed object naming mechanisms and its notion of primary and secondary objects (described in Section 3.2.3) are based in part on the distributed facilities described by Deshpande et al. [61]. The AMS abstractions combine ideas from Active Messages <ref> [138] </ref>, asynchronous remote procedure calls [7, 110], and the C ++ I/O stream model [106, 131]. Another related paradigm developed by the distributed systems community is virtual shared memory, which provides the illusion of a single, shared, coherent address space for systems with physically distributed memories. <p> Our implementation eliminates the costly communication needed to translate object names at the cost of additional, although acceptable, memory overheads (see Section 3.3.2). Active Messages <ref> [138] </ref> is an asynchronous communication mechanism which, like asynchronous remote procedure calls [7, 110], sends a message to a specified function that executes on message arrival. AMS combines this asynchronous message delivery mechanism with the concept of a message stream [106, 131] to hide message buffer management details. <p> First, the implementation must be portable across a wide range of MIMD parallel platforms and yet provide good performance. It should not rely on architecture-specific facilities, such as fine-grain message passing (e.g. Active Messages <ref> [138] </ref>). Second, it may not assume compiler support other than that provided by a standard compiler such as C ++ . All decisions about data distribution, communication, and synchronization are to be made at run-time. <p> For the past two years at the University of California at San Diego, MP ++ has been used on workstations to teach message passing parallel programming. 3.2.2 Asynchronous Message Streams The Asynchronous Message Stream (AMS) communication paradigm builds on ideas from asynchronous remote procedure calls [7, 110], Active Messages <ref> [138] </ref>, and the C ++ I/O stream library [106, 131]. AMS requires only basic message passing support such as that provided by MPI [105]. Its message stream abstraction frees the programmer from many low-level message passing details. Both AMS and the Active Message [138] model provide mechanisms for sending a message <p> remote procedure calls [7, 110], Active Messages <ref> [138] </ref>, and the C ++ I/O stream library [106, 131]. AMS requires only basic message passing support such as that provided by MPI [105]. Its message stream abstraction frees the programmer from many low-level message passing details. Both AMS and the Active Message [138] model provide mechanisms for sending a message to a handler which then consumes the message. An important difference is that AMS in intended for coarse-grain communication. Although Active Messages provides some facilities for sending long messages, it emphasizes fine-grain message passing.
Reference: [139] <author> R. von Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz, </author> <title> Compiler analysis for irregular problems in Fortran D, </title> <booktitle> in Fifth International Workshop of Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Fortran D [77, 86] relies on the same data distribution model as HPF and therefore suffers the same limitations for dynamic problems. One distinguishing feature of Fortran D is its support for pointwise mapping arrays in which individual array elements may be mapped to arbitrary processors <ref> [139] </ref>. An application could in theory construct block-irregular decompositions by mapping each array element in a block to the same processor. <p> that the compiler alone can extract sufficient information from the program to efficiently distribute data for dynamic, irregular problems on message passing architectures. 2.4.3 Run-Time Support Libraries The CHAOS (formerly PARTI) [60] and multiblock PARTI [3] libraries provide run-time support for data parallel compilers such as HPF and Fortran D <ref> [139] </ref>. Both libraries support a "inspector/executor" model for scheduling communication at run-time. In the inspector phase, the application computes the data motion required to satisfy data dependencies and saves the resulting communication pattern in a "schedule." The executor later uses this schedule to fetch remote data values. <p> CHAOS implements pointwise mapping arrays for unstructured calculations such as sweeps over finite element meshes and sparse matrix computations [58]. It has also been used to parallelize portions of the CHARMM molecular dynamics application [59, 88]. The Fortran D run-time system employs CHAOS to support Fortran D's mapping arrays <ref> [139] </ref>. Recently, CHAOS has been extended to support unstructured applications consisting of complicated C ++ objects [42]. However, recall that such unstructured representations are inappropriate for the irregular but structured applications targeted by LPARX. The multiblock PARTI library is targeted towards block-structured applica 43 tions.
Reference: [140] <author> M. S. Warren and J. K. Salmon, </author> <title> A parallel hashed oct-tree n-body algorithm, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: While we will emphasize the use of the DPO and AMS mechanisms in the design of the LPARX run-time system, we note that these facilities may be useful in other application domains. Many scientific methods, such as tree-based algorithms in N -body simulations <ref> [9, 79, 140] </ref>, rely on elaborate, dynamic data structures and exhibit unpredictable, unstructured communication patterns. The implementation of such numerical methods would be greatly simplified using the run-time support of DPO and AMS. This chapter is organized as follows. <p> Adhara [10] is a run-time library for particle applications that is not as general as CHAOS but has been specifically designed and optimized for certain classes of particle methods. Warren and Salmon <ref> [140] </ref> developed a parallel tree code intended for Barnes-Hut [17] and fast multipole [79] methods. Their approach dynamically distributes nodes of the tree across processors using a clever hashing mechanism. Singh et al. [126, 128] have also implemented a parallel fast multipole algorithm for shared memory multiprocessors. <p> AMLC coupled with the power of parallel architectures would enable computational scientists to study vortex dynamics problems with considerably larger numbers of particles. Traditionally, multipole [79] and Barnes-Hut [17] methods have been implemented using unstructured tree codes <ref> [140] </ref>. An alternative implementation strategy would employ a hierarchy of irregular but structured refinements [9, 64] using our software infrastructure. To date, no one has directly compared these two implementation strategies using real codes. Such a comparison would provide valuable insight into the relative strengths and weaknesses of each representation.
Reference: [141] <author> M. Welcome, B. Crutchfield, C. Rendleman, J. Bell, L. Howell, V. Beckner, and D. Simkims, </author> <title> Boxlib user's guide and manual. </title> <type> (draft), </type> <year> 1995. </year>
Reference-contexts: It has been employed by researchers at the University of California at San Diego, George Mason University, Lawrence Livermore National Laboratories, Sandia National Laboratories, and the Cornell Theory Center for applications in gas dynamics <ref> [141] </ref>, smoothed particle hydrodynamics, particle simulation studies [68], adaptive eigenvalue solvers in materials design [37, 94], genetics algorithms [81], adaptive multigrid methods in numerical relativity, and the dynamics of earthquake faults (see Section 6.3 for a complete list). <p> Crutchfield et al. independently developed similar Region abstractions based upon FIDIL for vector architectures [55]. Based on this framework, they have developed domain-specific libraries for adaptive mesh refinement applications in gas dynamics [19]. Their adaptive mesh refinement libraries have been parallelized using our software infrastructure <ref> [141] </ref> (see Section 3.4.1). The array sublanguage ZPL [100, 102] employs a form of region abstraction. ZPL does not explicitly manage data distribution, which it assumes is handled by another language. <p> For example, scientists at Lawrence Livermore National Laboratories have used our DPO, AMS, and MP ++ software to parallelize a structured adaptive mesh library for hyperbolic problems in gas dynamics <ref> [55, 141] </ref>. Their library defines a set of abstractions similar to LPARX. Their versions of Point, Region, and Grid have been specialized for their particular class of applications; for example, their Region describes application-specific properties such as whether meshes are cell-centered or node-centered. <p> This software is intended to support hyperbolic gas dynamics applications running on vector supercomputers [19]. The basic abstractions employed in Crutchfield's work are very similar to our own; in fact, their adaptive mesh refinement libraries have been parallelized using the LPARX software <ref> [141] </ref>. Parashar and Browne are developing a software infrastructure supporting parallel adaptive mesh refinement methods for black hole interactions [115]. Their method is based on a clever load balancing and processor mapping strategy that maps grids to processors through locality-preserving space filling curves [118, 124]. <p> adaptive multigrid methods in numerical relativity as part of the Black Hole Binary Grand Challenge Project. 192 * Scientists at Lawrence Livermore National Laboratories have employed our Distributed Parallel Object, Asynchronous Message Stream, and MP ++ software to parallelize a structured adaptive mesh library for hyperbolic problems in gas dynamics <ref> [55, 141] </ref>. * C. Myers (Cornell Theory Center), B. Shaw (Lamont-Doherty Earth Observatory), and J. Langer (University of California at Santa Barbara) have implemented a parallel 2d code to study localized slip modes in the dynamics of earthquake faults. * C. Myers and J.
Reference: [142] <author> S. R. White, J. W. Wilkins, and M. P. Teter, </author> <title> Finite-element method for electronic structure, </title> <journal> Physical Review B, </journal> <volume> 39 (1989), </volume> <pages> pp. 5819-5833. </pages>
Reference-contexts: The need for adaptive refinement in materials design has been noted by other researchers. Bernholc et al. [27] have implemented a semi-adaptive code that places a single, static refinement patch over each of their atoms. Others have attempted adaptive solutions using either finite element methods <ref> [134, 142] </ref> or a combination of finite element and wavelet techniques [48]. Although finite element methods pro 116 such as the C 20 H 20 ring shown here.
Reference: [143] <author> M. Wu and G. Fox, </author> <title> Fortran 90D compiler for distributed memory MIMD parallel computers, </title> <type> Tech. Rep. </type> <institution> SCCS-88B, Syracuse University, </institution> <year> 1991. </year>
Reference-contexts: Dynamic irregular block decompositions are not currently supported by programming languages such as High Performance Fortran (HPF) [83], Fortran D [77], Vienna Fortran [43], or Fortran 90D <ref> [33, 143] </ref>. <p> Data Parallel Fortran Languages High Performance Fortran (HPF) [83] is a data parallel Fortran which combines the array operations of Fortran 90, a parallel forall loop, and data decomposition directives based on the research languages Fortran D [77, 86] and Fortran 90D <ref> [33, 116, 143] </ref>. It is quickly becoming accepted by a number of manufacturers as a standard parallel programming language for scientific computing. HPF has been targeted towards regular, static applications such as dense linear algebra but provides little support for irregular, dynamic computations [44, 84]. <p> The multiblock PARTI library is targeted towards block-structured applica 43 tions. It supports the uniform BLOCK, BLOCK CYCLIC, and CYCLIC array decompositions of HPF and has been used in the run-time system for the Fortran 90D compiler <ref> [33, 116, 143] </ref>. Multiblock PARTI defines canned routines that fill ghost cells and copy regular sections between arrays.
Reference: [144] <author> S. X. Yang, D. Gannon, S. Srinivas, F. Bodin, and P. Bode, </author> <title> High Performance Fortran interface to the parallel C++, </title> <booktitle> in Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: These mechanisms alone cannot describe the irregular blocking structures that arise in adaptive mesh refinement and recursive coordinate bisection [44]. Data Parallel C ++ Languages The pC++ <ref> [30, 31, 144] </ref> programming language is a data parallel extension of C ++ . <p> Calling Fortran requires only a pointer to the Grid data (passed to Fortran as an array) and the dimensions of the associated Region. By default, LPARX adopts For 49 tran's column-major array ordering convention. Language interoperability for High Performance Fortran, however, is substantially more involved <ref> [144] </ref>. Although HPF defines an interface to subroutines written in other languages through the notion of "extrinsic procedures," it does not address how other languages may call routines written in HPF. One difficult problem is how to communicate the representation of distributed data between LPARX and HPF. <p> For example, how should the programmer in a task parallel language specify a call to a data parallel routine? How should data be transferred across the call interface? HPF [83] defines a rudimentary external procedure interface, and others have investigated calling HPF from pC++ <ref> [144] </ref> and also from Fortran M [76].
Reference: [145] <author> A. Yonezawa, </author> <title> ABCL: An Object Oriented Concurrent System, </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Actors are concurrent objects which communicate with each other via messages. Actors execute in response to messages, and each actor object may contain several concurrently executing tasks. Actor-based languages include ABCL <ref> [145] </ref>, Cantor [11], and Charm++ [90]. Implementations of actor languages require complicated compilation strategies and sophisticated run-time support libraries, such as the Concert system [91] for fine-grain object management. Because of this complexity, we have not based the LPARX run-time system on an existing concurrent object-based language.
References-found: 139

