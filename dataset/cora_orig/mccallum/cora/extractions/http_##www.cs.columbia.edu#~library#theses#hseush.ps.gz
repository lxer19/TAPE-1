URL: http://www.cs.columbia.edu/~library/theses/hseush.ps.gz
Refering-URL: http://www.cs.columbia.edu/home/phd_prog/alumni.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Semantics-Based Optimization Under Epsilon Serializability  
Author: Wenwey Hseush 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Graduate School of Arts and Sciences.  
Date: 1995  
Affiliation: Columbia University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Agrawal and S. Sengupta. </author> <title> Modular synchronization in multiversion databases: Version control and concurrency control. </title> <booktitle> In Proceedings of the 1989 ACM SIGMOD International Conference on Management of Data, </booktitle> <address> Portland, Oregon, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: However, data values of a data object that are slightly different in age or in precision are often acceptable. This observation underlies the concept of similarity among data values. A large body of literature exists concerning various approaches to effectively supporting concurrent transaction and query processing <ref> [1, 54, 73, 85, 55, 18, 21, 81, 20] </ref>. These approaches typically employ multiple versions of data to eliminate read/write conflicts between update transactions and read-only queries. Queries access transaction-consistent, but maybe out-of-date, database states.
Reference: [2] <author> R. Agrawal, M. Carey, and M. Livny. </author> <title> Concurrency control performance modeling: Alternatives and implications. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 12(4) </volume> <pages> 609-654, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: In this section, we summarize their results and point out the key factors that determine the performance behavior of divergence control algorithms. Kawaguchi's performance work is based on a simulation model (referred to as the ESR performance model) extended from the one in <ref> [2] </ref>, which was designed to evaluate concurrency control algorithms in a centralized transaction processing system. In order to understand the ESR performance results, a certain degree of understanding on the concurrency control performance is needed. For this reason, we will also mention some of the results in [2] and provide the <p> the one in <ref> [2] </ref>, which was designed to evaluate concurrency control algorithms in a centralized transaction processing system. In order to understand the ESR performance results, a certain degree of understanding on the concurrency control performance is needed. For this reason, we will also mention some of the results in [2] and provide the intuitions for the key factors that affect concurrency control performance behavior. 144 6.1.1 ESR Performance Model The ESR performance model consists of three major components: user model, divergence server and resource manager. The user model captures the arrival process of transactions. <p> The resource manager is responsible for scheduling all requests among a set of resources (i.e., CPUs and disks). The performance model in <ref> [2] </ref> are similar to the ESR performance model except that the divergence server is replaced by a transaction model, which implements the logic of concurrency control. 145 Begin Transaction for each data item begin get permission from the divergence server access data by sending a request to the resource manager think <p> When *-spec drops to zero, divergence control becomes concurrency control. Data contention ratio plays an important role in performance analysis. A high data contention ratio implies a situation where conflicts occur frequently. It has been reported in <ref> [2] </ref> and elsewhere that, with a high contention ratio, 2PLCC/blocking performs more poorly than OCC and 2PLCC/restart. However, when the data contention ratio is low, 2PLCC/blocking outperforms the other two algorithms. <p> We refer to this point (the peak) as the thrashing point. Both data contention ratio and MPL contribute to the level of conflicts (referred to as the data contention level). The results reported in <ref> [2] </ref> can be interpreted in terms of data contention ratio, resource size and MPL. * With a low data contention ratio (1/10,000 is used) and infinite resources, 2PLCC/blocking outperforms OCC and 2PLCC/restart by a small amount. <p> The performance behavior of the escrow method with blocking (Reuter's model) can be compared to that of the escrow method with immediate-restart (O'Neil model) in the same way that 2PLCC with blocking is compared to 2PLCC with immediate-restart as described in <ref> [2] </ref>. We know that with low data contention blocking-based algorithms outperform restart-based algorithms, and with high data contention, restart-based algorithms perform better than blocking-based algorithms. The escrow method is designed for high data contention applications (with hot-spot data). O'Neil's escrow model seems to be more practical than Reuter's model.
Reference: [3] <author> R. Alonso, D. Barbara, and H. Garcia-Molina. </author> <title> Data caching issues in an information retrieval systems. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 15(3) </volume> <pages> 359-384, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Wiederhold and Qian [83] proposed a notion called identity connection, which identifies data which should eventually match, but which can be in differing states for some time intervals. Barbara and Garcia-Molina [10] proposed controlled inconsistency, which extends their work on quasi-copies <ref> [3] </ref>. In the area of real-time database, Kuo and Mok developed a notion called similarity [48], which also allows relaxed serializability.
Reference: [4] <author> B. Badrinath and K. Ramamritham. </author> <title> Semantics-based concurrency control: Beyond commutativity. </title> <booktitle> In Proceedings name, </booktitle> <pages> pages 304-311, </pages> <booktitle> conference site, </booktitle> <month> month </month> <year> 1987. </year>
Reference-contexts: We identify the modules in the existing transaction processing that should be enhanced for the SBCC methods. In this chapter, we study four SBCC methods, two data semantics-based and two transaction semantics-based. The two data SBCCs are commutativity-based concurrency control [82, 5] and recoverability-based concurrency controls <ref> [4] </ref>. The two transaction SBCCs are concurrency control mechanisms to guarantee weak consistency [35] and cooperative serializability [53]. We also extend these SBCC methods to SBDC methods and describe their practical design issues in the uniform framework. <p> current state of u, either a before-lock-point state or an after-lock-point state. 128 Function LPO (u 1 , u 2 ) Begin If (INDEP (u 1 ; u 2 ) = true) then return independence else return OTAB [state (u 1 ), state (u 2 )] End 5.5 Recoverability Recoverability <ref> [4] </ref> is another criterion used to define the notion of conflict among operations. Operation o j is recoverable relative to operation o k if o j returns the same value whether or not o k immediately precedes o j . <p> This type of commit is referred to as a pseudo-commit. This is the merit of recoverability | separating effects on transactions from effects on databases. 5.5.1 Recoverability-Based 2PL-SBCC In <ref> [4, 23] </ref>, a concurrency control strategy based on cycle detection has been described. However, the implementation is impractical and inefficient from the viewpoint of an existing transaction processing.
Reference: [5] <author> B. Badrinath and K. Ramamritham. </author> <title> Synchronizing transactions on objects. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(5) </volume> <pages> 541-547, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: We identify the modules in the existing transaction processing that should be enhanced for the SBCC methods. In this chapter, we study four SBCC methods, two data semantics-based and two transaction semantics-based. The two data SBCCs are commutativity-based concurrency control <ref> [82, 5] </ref> and recoverability-based concurrency controls [4]. The two transaction SBCCs are concurrency control mechanisms to guarantee weak consistency [35] and cooperative serializability [53]. We also extend these SBCC methods to SBDC methods and describe their practical design issues in the uniform framework.
Reference: [6] <author> B. Badrinath and K. Ramamritham. </author> <title> Semantics-based concurrency control: Beyond commutativity. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 16, </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: Two types of commutativity are defined in [82]: (1) forward commutativity is applicable only with intentions lists based recovery; and (2) backward commutativity is applicable only with log based recovery. Commutativity and recoverability are also defined in <ref> [6] </ref> as properties used to define conflicts among operations. Given that operation o j immediately precedes operation o i , o i is recoverable relative to o j , if o i returns the same value regardless of the completion status of o j . <p> The performance of transactions dealing with hot-spot data items is critical to the entire system. The read-write based two-phase locking protocol is inadequate for both applications due to its poor performance. The typical solution for such advanced applications is through the use of semantics-based concurrency control <ref> [32, 46, 37, 49, 56, 6, 19, 33, 60, 31, 11] </ref>. For brevity, we use SBCC to denote semantics-based concurrency control. SBCC methods can be broadly divided into two main groups, the data semantics approach and the transaction semantics approach. <p> Thus, more operations can be scheduled to run concurrently, leading to higher performance. Examples include concurrency control based on commutativity [82] and recoverability <ref> [6] </ref>. In the transaction semantics, consistency requirements are defined on transactions according to the semantics of transactions and the data they manipulate. Examples include weak consistency [35], predicatewise serializability [45] and cooperative serializability [53]. <p> We observe that each semantics-based method has its own notion of conflict, which is usually weaker than the traditional conflict definition over read and write operations. This conflict-based approach was inspired by a series of ACTA-related work <ref> [6, 24, 71, 70] </ref>, where the notion of conflict is defined using application semantic information (e.g., com-mutativity, recoverability or conflict defined for each conjunctive clause in a predicate), and thus application consistency requirements can be specified in terms of conflict relationships among transactions. <p> We say that two operations commute if the result of two operations are independent of their execution order and changing their execution order does not affect the results of other operations executed after the operations. A formal definition of commutativity is described in <ref> [6] </ref>. We say that two operations (semantically) conflict if they do not commute. Operation commutativity and conflicts can be specified via a compatibility table.
Reference: [7] <author> F. Bancilhon, W. Kim, and H. Korth. </author> <title> A model of CAD transactions. </title> <booktitle> In Proceedings of the Eleventh International Conference on Very Large Data Bases, </booktitle> <pages> pages 25-33, </pages> <address> Stockholm, </address> <month> August </month> <year> 1985. </year> <month> 159 </month>
Reference-contexts: Data contention is a general term that refers to the resource conflict problem which results from the need to maintain data consistency. Data contention appears as a consequence of overly strict consistency constraints enforced by serializability. In many advanced applications [13, 28] such as computer-aided design, manufacturing (CAD/CAM) <ref> [7] </ref>, and scientific data man 2 agement [12], traditional concurrency control methods such as the two-phase locking protocol [30] have been shown to be inadequate for maintaining data consistency due to severe data contention, which significantly degrades system performance. <p> However, recent evidence shows that the data contention problem, a result of the synchronous nature of traditional transaction processing systems, becomes an increasingly serious performance bottleneck in some advanced applications, especially the ones involving long-duration transactions and hot-spot data. Applications like computer-aided design and manufacturing (CAD/CAM) <ref> [7, 9, 8, 76, 44] </ref> are often long-lived and may consist of multiple steps executed on collections of complex data objects. In these applications, transactions are likely to hold resources for a relatively long time, which exclusively locks out the executions of other transactions that need the resources.
Reference: [8] <author> F. Bancilhon, W. Kim, and H. Korth. </author> <title> A model of CAD transactions. </title> <booktitle> In 11th International Conference on Very Large Databases, </booktitle> <pages> pages 25-33, </pages> <address> Stockholm, Sweden, </address> <month> August </month> <year> 1985. </year>
Reference-contexts: However, recent evidence shows that the data contention problem, a result of the synchronous nature of traditional transaction processing systems, becomes an increasingly serious performance bottleneck in some advanced applications, especially the ones involving long-duration transactions and hot-spot data. Applications like computer-aided design and manufacturing (CAD/CAM) <ref> [7, 9, 8, 76, 44] </ref> are often long-lived and may consist of multiple steps executed on collections of complex data objects. In these applications, transactions are likely to hold resources for a relatively long time, which exclusively locks out the executions of other transactions that need the resources.
Reference: [9] <author> F. Bancilhon, W. Kim, and H. Korth. </author> <title> Transaction and concurrency control in CAD databases. </title> <booktitle> In IEEE International Conference on Computer Design: VLSI in Computers, </booktitle> <pages> pages 86-90, </pages> <year> 1985. </year>
Reference-contexts: However, recent evidence shows that the data contention problem, a result of the synchronous nature of traditional transaction processing systems, becomes an increasingly serious performance bottleneck in some advanced applications, especially the ones involving long-duration transactions and hot-spot data. Applications like computer-aided design and manufacturing (CAD/CAM) <ref> [7, 9, 8, 76, 44] </ref> are often long-lived and may consist of multiple steps executed on collections of complex data objects. In these applications, transactions are likely to hold resources for a relatively long time, which exclusively locks out the executions of other transactions that need the resources.
Reference: [10] <author> D. Barbara and H. Garcia-Molina. </author> <title> The case for controlled inconsistency in replicated data. </title> <booktitle> In Proceedings of the Workshop on Management of Replicated Data, </booktitle> <pages> pages 35-42, </pages> <address> Houston, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Wiederhold and Qian [83] proposed a notion called identity connection, which identifies data which should eventually match, but which can be in differing states for some time intervals. Barbara and Garcia-Molina <ref> [10] </ref> proposed controlled inconsistency, which extends their work on quasi-copies [3]. In the area of real-time database, Kuo and Mok developed a notion called similarity [48], which also allows relaxed serializability.
Reference: [11] <author> D. Barbara and H. Garcia-Molina. </author> <title> The demarcation protocol: A technique for maintaining linear arithmetic constraints in distributed database systems. </title> <booktitle> In Proceedings of the International Conference in Extending Database Technology, </booktitle> <address> Vienna, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: Queries access transaction-consistent, but maybe out-of-date, database states. In contrast, ESR does not require multiple versions of data, thus does not incur extra storage overhead. 14 1.4.3 Semantics-based Optimization Related Work Barbara <ref> [11] </ref> proposed a method, the demarcation protocol, to maintain database consistency. The demarcation protocol was designed to maintain global arithmetic constraints among sites, by establishing safe limits as "lines drawn in the sand" for updates. It provides a way to change these limits dynamically and asynchronously. <p> It provides a way to change these limits dynamically and asynchronously. From the viewpoint of the demarcation protocol, two ESR safe conditions are just two arithmetic constraints. Given the definition of *-specs, we always have an arithmetic constraint of the inequality kind. In addition to the demarcation protocol <ref> [11] </ref>, there are other papers concerning different approaches to dynamically distributing resources in a distributed system [36, 25, 57]. With appropriate modifications, these general resource allocation approaches can also be used to redistribute the *-spec among the sub-ETs in the design of distributed divergence control algorithms. <p> The performance of transactions dealing with hot-spot data items is critical to the entire system. The read-write based two-phase locking protocol is inadequate for both applications due to its poor performance. The typical solution for such advanced applications is through the use of semantics-based concurrency control <ref> [32, 46, 37, 49, 56, 6, 19, 33, 60, 31, 11] </ref>. For brevity, we use SBCC to denote semantics-based concurrency control. SBCC methods can be broadly divided into two main groups, the data semantics approach and the transaction semantics approach.
Reference: [12] <author> R. Barga and C. Pu. </author> <title> Handling inconsistency in scientific data management: An approach based on intervals. </title> <type> Technical Report OGI-CSE-93-005, </type> <institution> Department of Computer Science and Engineering, Oregon Graduate Institute, </institution> <year> 1993. </year>
Reference-contexts: Data contention appears as a consequence of overly strict consistency constraints enforced by serializability. In many advanced applications [13, 28] such as computer-aided design, manufacturing (CAD/CAM) [7], and scientific data man 2 agement <ref> [12] </ref>, traditional concurrency control methods such as the two-phase locking protocol [30] have been shown to be inadequate for maintaining data consistency due to severe data contention, which significantly degrades system performance. In distributed systems, guaranteeing global serializability results in severe data contention as well.
Reference: [13] <author> N. Barghouti and G. Kaiser. </author> <title> Concurrency control in advanced database applications. </title> <journal> ACM Computing Surveys, </journal> <volume> 23(3) </volume> <pages> 269-318, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Data contention is a general term that refers to the resource conflict problem which results from the need to maintain data consistency. Data contention appears as a consequence of overly strict consistency constraints enforced by serializability. In many advanced applications <ref> [13, 28] </ref> such as computer-aided design, manufacturing (CAD/CAM) [7], and scientific data man 2 agement [12], traditional concurrency control methods such as the two-phase locking protocol [30] have been shown to be inadequate for maintaining data consistency due to severe data contention, which significantly degrades system performance.
Reference: [14] <author> P. Bernstein and N. Goodman. </author> <title> Concurrency control in distributed database systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 13(2) </volume> <pages> 185-222, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: W S (s) from W */ let p and q be the sibling pieces connected by s merge p and q into one piece end CHOP is the finest private chopping of t i 3.5 Chopping Up Epsilon Transactions in Distributed En vironments The traditional approach to distributed transaction processing <ref> [29, 14] </ref> requires the use of a commit protocol to ensure failure atomicity of distributed transactions; either all sub-transactions of a distributed transaction commit or none of them commits. <p> On the other hand, chopping transactions enforces no commit protocols among pieces from an original transaction and allows individual pieces to commit asynchronously. 3.5.1 SR-Chopping Complementing Distributed Divergence Control The traditional approach to distributed transaction processing <ref> [29, 14] </ref> requires the use of a commit protocol to ensure failure atomicity of distributed transactions; either all sub-transactions of a distributed transaction commit or none of them commits.
Reference: [15] <author> P. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> first edition, </address> <year> 1987. </year>
Reference-contexts: Introduction 1.1 Motivation On-line transaction processing (OLTP) <ref> [15, 39] </ref> has been recognized as an important and growing technique for reliable computing in both centralized and distributed systems. In many application areas such as banking and airline reservations, OLTP is well established and crucial to their day-to-day operations. <p> The underlying idea is that (conflict-based) concurrency control methods must be able to identify the non-serializable conflicts and prevent a cycle from forming in the serialization graph <ref> [15] </ref>. The extension stage isolates the identification part of concurrency control and the relaxation stage modifies the cycle prevention part so as to permit limited inconsistencies. The extension of classic two-phase locking concurrency control to two-phase locking divergence control results in the lock compatibility shown in Table 2.1.
Reference: [16] <author> P. Bernstein, M. Hsu, and B. Mann. </author> <title> Implementing recoverable requests using queues. </title> <booktitle> In Proceedings of 1990 SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 112-122, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: To guarantee that the subsequent pieces of a distributed transaction will eventually commit once the first piece commits, we adopt the mechanism of persistent transmission [42] or recoverable queues <ref> [16] </ref>, which are offered in commercial transaction mon 63 itors. A recoverable queue is an inter-site communication channel, through which data are guaranteed to survive site failures as well as link failures.
Reference: [17] <author> P. Bober and M. Carey. </author> <title> Multiversion query locking. </title> <booktitle> In Proceedings of the Twenty-First International Conference on Very Large Data Bases, </booktitle> <address> Vancouver, </address> <month> August </month> <year> 1992. </year> <month> 160 </month>
Reference-contexts: Weak consistency requires the execution of update transactions to be serializable, but allows each read-only transaction to have its own consistent view. Another paper that allows weaker forms of consistency in a way similar to weak consistency is a recent algorithm by Bober and Carey <ref> [17] </ref>. However, their queries still see a transaction-consistent view of the database, possibly with some older version that does not contain all the updates. Two notions eventual consistency and lagging consistency are introduced by Sheth [79] for mutual consistency (i.e., replicated data in databases).
Reference: [18] <author> P. Bober and M. Carey. </author> <title> On mixing queries and transactions via multiversion locking. </title> <booktitle> In Proceedings of the International Conference on Data Engineering, </booktitle> <pages> pages 535-545, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: However, data values of a data object that are slightly different in age or in precision are often acceptable. This observation underlies the concept of similarity among data values. A large body of literature exists concerning various approaches to effectively supporting concurrent transaction and query processing <ref> [1, 54, 73, 85, 55, 18, 21, 81, 20] </ref>. These approaches typically employ multiple versions of data to eliminate read/write conflicts between update transactions and read-only queries. Queries access transaction-consistent, but maybe out-of-date, database states.
Reference: [19] <author> R. Bubenik and W. Zwaenepoel. </author> <title> Semantics of optimistic computation. </title> <booktitle> In Proceedings of the Tenth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 20-27, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The performance of transactions dealing with hot-spot data items is critical to the entire system. The read-write based two-phase locking protocol is inadequate for both applications due to its poor performance. The typical solution for such advanced applications is through the use of semantics-based concurrency control <ref> [32, 46, 37, 49, 56, 6, 19, 33, 60, 31, 11] </ref>. For brevity, we use SBCC to denote semantics-based concurrency control. SBCC methods can be broadly divided into two main groups, the data semantics approach and the transaction semantics approach.
Reference: [20] <author> M. Carey and W. Muhanna. </author> <title> The performance of multiversion concurrency control algorithms. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 338-378, </pages> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: However, data values of a data object that are slightly different in age or in precision are often acceptable. This observation underlies the concept of similarity among data values. A large body of literature exists concerning various approaches to effectively supporting concurrent transaction and query processing <ref> [1, 54, 73, 85, 55, 18, 21, 81, 20] </ref>. These approaches typically employ multiple versions of data to eliminate read/write conflicts between update transactions and read-only queries. Queries access transaction-consistent, but maybe out-of-date, database states.
Reference: [21] <author> A. Chan and R. Gray. </author> <title> Implementing distributed read-only transactions. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(2):205-212, </volume> <month> Feb. </month> <year> 1985. </year>
Reference-contexts: However, data values of a data object that are slightly different in age or in precision are often acceptable. This observation underlies the concept of similarity among data values. A large body of literature exists concerning various approaches to effectively supporting concurrent transaction and query processing <ref> [1, 54, 73, 85, 55, 18, 21, 81, 20] </ref>. These approaches typically employ multiple versions of data to eliminate read/write conflicts between update transactions and read-only queries. Queries access transaction-consistent, but maybe out-of-date, database states.
Reference: [22] <author> S. Chen. </author> <title> Implementation and Design of a System Supporting Epsilon Serializability. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <note> Expected 1994. </note>
Reference-contexts: Since database states are not always maintained consistently, it is necessary to restore database consistency once a while. Designing mechanisms for consistency restoration is one of the major tasks in the generalized model. This line of ESR work includes [27] and <ref> [22] </ref>. The distinction between the restricted and generalized models is motivated by the recent research trend in on-line transaction processing. Researchers in this area attempt to separate the notion of guaranteeing correct transaction behavior from the notion of maintaining consistent database states. <p> Database consistency in the restricted ESR model are defined by serializability. All issues discussed in this thesis work are based on the restricted ESR model. Issues related to the generalized model will be discussed in <ref> [22] </ref>. 2.2.3 Database Metric Space For ESR The notion of ESR is established over the databases whose spaces are metric (distance) spaces. A database is a collection of data items, denoted by D. Each data item 20 k 2 D has a state, denoted by s k .
Reference: [23] <author> P. Chrysanthis, S. Raghuram, and K. Ramamritham. </author> <title> Extracting concurrency from objects: A methodology. </title> <booktitle> In Proceedings of the 1991 SIGMOD, </booktitle> <pages> pages 108-117, </pages> <booktitle> conference site, </booktitle> <month> month </month> <year> 1991. </year>
Reference-contexts: With a compatibility table it is possible to determine whether operations commute through a simple table lookup, thus allowing semantic conflicts to be efficiently detected at run time. Such compatibility tables can be derived directly from the semantics of the operations for a data object <ref> [23] </ref>. Let us examine a banking example. Assume a set of Account data objects and three 112 operations Deposit, Withdraw, and Balance that operate on an account. Deposit adds a specified amount to the account balance. <p> This type of commit is referred to as a pseudo-commit. This is the merit of recoverability | separating effects on transactions from effects on databases. 5.5.1 Recoverability-Based 2PL-SBCC In <ref> [4, 23] </ref>, a concurrency control strategy based on cycle detection has been described. However, the implementation is impractical and inefficient from the viewpoint of an existing transaction processing.
Reference: [24] <author> P. K. Chrysanthis and K. Ramamritham. </author> <title> ACTA: A framework for specifying and reasoning about transaction structure and behavior. </title> <editor> In H. Garcia-Molina and H. Ja-gadish, editors, </editor> <booktitle> 1990 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 194-203, </pages> <address> Atlantic City NJ, </address> <month> May </month> <year> 1990. </year> <journal> Special issue of SIGMOD Record, </journal> <volume> 19(2), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: It is important to prove the correctness of these combined methods. The correctness criterion used in this thesis is ESR, which is a generalization of classic serializability. A formal ESR definition has been discussed by Ramamrithan and Pu [71] based on the ACTA framework <ref> [24] </ref>. The formal ESR definition is established by assuming the existence of a safe condition for a transaction. However, the ESR definition is not constructive and may not be easily used for algorithm developments and proofs. <p> In [72], Ramamrithan and Pu provided a formal characterization of ESR when queries that may view inconsistent data run concurrently with consistent update transactions. Using the ACTA framework <ref> [24] </ref>, they formally express the inter-transaction conflicts that are recognized by ESR and through that define ESR, analogous to the manner in which conflict-based serializability is defined. <p> However, in order to show the correctness of the proposed methods, we have the needs to establish a formal framework for ESR. We start with the formal definition of epsilon serializability by Ramamrithan 18 and Pu [71]. The formal ESR definition was developed using the ACTA framework <ref> [24] </ref>. As we mentioned in the introduction chapter, this ESR definition is not constructive and may not be easily used for reasoning ESR algorithms. For this reason, we provide an intuitive interpretation for the formal ESR definition and extend the definition with two lemmas and a theorem. <p> We are currently exploring the application of other SR optimization techniques under ESR. Second, using the ACTA framework <ref> [24] </ref> as well as the ESR formal characterization [72], we precisely express the potential conflicts that may arise in the execution of a set of chopped-up transactions and formally prove the correctness of three proposed methods. <p> We observe that each semantics-based method has its own notion of conflict, which is usually weaker than the traditional conflict definition over read and write operations. This conflict-based approach was inspired by a series of ACTA-related work <ref> [6, 24, 71, 70] </ref>, where the notion of conflict is defined using application semantic information (e.g., com-mutativity, recoverability or conflict defined for each conjunctive clause in a predicate), and thus application consistency requirements can be specified in terms of conflict relationships among transactions.
Reference: [25] <author> S. Davidson, H. Garcia-Molina, and D. Skeen. </author> <title> Consistency in partitioned networks. </title> <journal> Computing Surveys, </journal> <volume> 17(3) </volume> <pages> 341-370, </pages> <month> September </month> <year> 1985. </year>
Reference-contexts: Given the definition of *-specs, we always have an arithmetic constraint of the inequality kind. In addition to the demarcation protocol [11], there are other papers concerning different approaches to dynamically distributing resources in a distributed system <ref> [36, 25, 57] </ref>. With appropriate modifications, these general resource allocation approaches can also be used to redistribute the *-spec among the sub-ETs in the design of distributed divergence control algorithms.
Reference: [26] <author> D.Gawlick and D.Kinkade. </author> <title> Varieties of concurrency control in ims/vs fast path. </title> <journal> IEEE Bulletin of the Technical Committee on Data Engineering, </journal> <volume> 8(2) </volume> <pages> 3-10, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Optimistic Commit [50] is a protocol that uses compensating transactions to undo the effects of partial results to reach a uniform decision. One approach similar to the escrow method (described in Chapter 4) is Fast Path Concurrency <ref> [26] </ref>. Both the escrow method and Fast Path deal with transactional access to aggregate field quantities and have similar programming interface. The difference is their internal logic. A transaction can issue a test request (a constraint on the data item).
Reference: [27] <author> P. Drew and C. Pu. </author> <title> Asynchronous consistency restoration under epsilon serializabil-ity. </title> <type> Technical Report OGI-CSE-93-004, </type> <institution> Department of Computer Science and Engineering, Oregon Graduate Institute, </institution> <year> 1993. </year> <note> Also available as tech. report HKUST 161 CS93-002, </note> <institution> Department of Computer Science, Hong Kong University of Science and Technology. </institution>
Reference-contexts: In [67], an asynchronous approach is proposed for replica control by applying ESR in distributed systems. Users can reduce the degree of inconsistency to the desired amount. In the limit, users see strict 1-copy serializability. In <ref> [27] </ref>, Drew and Pu discussed a different ESR model (often referred to as the generalized ESR model, see Section 2.2.2), where an update ET is allowed to see inconsistent data and computes results based on the inconsistent data. <p> Since database states are not always maintained consistently, it is necessary to restore database consistency once a while. Designing mechanisms for consistency restoration is one of the major tasks in the generalized model. This line of ESR work includes <ref> [27] </ref> and [22]. The distinction between the restricted and generalized models is motivated by the recent research trend in on-line transaction processing. Researchers in this area attempt to separate the notion of guaranteeing correct transaction behavior from the notion of maintaining consistent database states.
Reference: [28] <editor> A. K. Elmagarmid, editor. </editor> <title> Database Transaction Models for Advanced Applications. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Data contention is a general term that refers to the resource conflict problem which results from the need to maintain data consistency. Data contention appears as a consequence of overly strict consistency constraints enforced by serializability. In many advanced applications <ref> [13, 28] </ref> such as computer-aided design, manufacturing (CAD/CAM) [7], and scientific data man 2 agement [12], traditional concurrency control methods such as the two-phase locking protocol [30] have been shown to be inadequate for maintaining data consistency due to severe data contention, which significantly degrades system performance.
Reference: [29] <author> J. L. Eppinger, L. B. Mummert, and A. Z. Spector, </author> <title> editors. Camelot and Avalon: </title>
Reference-contexts: W S (s) from W */ let p and q be the sibling pieces connected by s merge p and q into one piece end CHOP is the finest private chopping of t i 3.5 Chopping Up Epsilon Transactions in Distributed En vironments The traditional approach to distributed transaction processing <ref> [29, 14] </ref> requires the use of a commit protocol to ensure failure atomicity of distributed transactions; either all sub-transactions of a distributed transaction commit or none of them commits. <p> On the other hand, chopping transactions enforces no commit protocols among pieces from an original transaction and allows individual pieces to commit asynchronously. 3.5.1 SR-Chopping Complementing Distributed Divergence Control The traditional approach to distributed transaction processing <ref> [29, 14] </ref> requires the use of a commit protocol to ensure failure atomicity of distributed transactions; either all sub-transactions of a distributed transaction commit or none of them commits.
References-found: 29

