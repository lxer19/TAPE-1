URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/143.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/
Root-URL: http://www.cs.wisc.edu
Email: smoon@altair.snu.ac.kr kemal@watson.ibm.com  
Title: Unrolling-Based Copy Elimination  
Author: Soo-Mook Moon Kemal Ebcioglu 
Keyword: Copy Elimination, Loop Unrolling, Coalescing, Instruction-level Parallelism, Soft ware Pipelining, Modulo Variable Expansion, Delay Pseudo Instruction, VLIW  
Note: The experiment part of this work was performed while the author was with IBM T. J. Watson Research Center.  
Address: P.O. Box 218 Seoul 151-742, Korea Yorktown Heights, NY 10598  
Affiliation: Dept. of Electronics Engineering IBM T. J. Watson Research Center Seoul National University  
Abstract: Aggressive instruction scheduling leaves around many copy instructions that cannot be easily removed by ordinary copy propagation or coalescing technique. This paper proposes a novel copy elimination technique that is based on loop unrolling and coalescing. Two distinctive features of the technique are its insertion of special "uncoalescible" copy instructions at some points of the code (e.g., loop exits) and its method of determining the number of times to unroll based on the idea of "extended" live ranges. Our technique essentially solves the problem of avoiding non-true data dependences that arises during scheduling in the form of copy elimination. Our technique differs from previous unrolling-based techniques in that it solves a more general problem of eliminating copies in loops with arbitrary control flows, not just those in straight-line loops. Our technique also simplifies the scheduling of multi-latency instructions such that delay pseudo instructions are inserted to meet the latency requirement which might be renamed during scheduling and renamed delays are eliminated just as other copies. Our experimental results on non-trivial integer benchmarks indicate that the technique is effective in eliminating copies and renamed delays. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Copy elimination is a classical optimization technique that attempts to remove copies from the given program and has been performed either by copy propagation or by coalescing. Copy propagation replaces each use of the target operand of a copy by its source operand, thus making the copy dead <ref> [1] </ref>. Coalescing merges the live range of the source operand with that of the target operand if they do not overlap, thus making the copy dummy (i.e., x=x). Both techniques have been successful in removing copies generated by classical optimizations such as common subexpression elimination and load-to-copy or store-to-copy optimizations.
Reference: [2] <author> S.-M. Moon and K. Ebcioglu. </author> <title> An Efficient Resource-Constrained Global Scheduling Technique for Superscalar and VLIW processors. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture (Micro-25), </booktitle> <pages> pages 55-71, </pages> <month> Dec </month> <year> 1992, </year> <note> also available as IBM Research Report RC 17962. </note>
Reference-contexts: Although this abstract is not about scheduling technique per se, a brief description will explain the context where copy instructions are indispensable for generating high-performance code. A full description of the compiler can be found in <ref> [2] </ref>.
Reference: [3] <author> S. Jain. </author> <title> Circular Scheduling: A New Technique to Perform Software Pipelining. </title> <booktitle> In Proceedings of the SIGPLAN 1991 conference on Programming Language Design and Implementation, </booktitle> <pages> pages 219-228, </pages> <month> Jun </month> <year> 1991. </year> <month> 14 </month>
Reference-contexts: This process is repeated in such 1 A special case of this algorithm called "circular scheduling" was independently developed later for pipelining of innermost loops with no conditional branches <ref> [3] </ref>. 4 a way that groups generated later become increasingly compact since they can absorb groups generated earlier, achieving tighter schedules for the pipeline kernel. <p> The same result can be obtained by our technique in the form of eliminating copies. There are also techniques that perform loop unrolling and live range renaming before the scheduling starts <ref> [3] </ref>, yet it is generally difficult to decide the precise number of required unrolling before scheduling. These techniques are fundamentally applicable for straight-line loops. For software pipelining of loops with conditional branches, copies are generated for various other reasons not just for register overwrites between iterations.
Reference: [4] <author> K. Ebcioglu and T. Nakatani. </author> <title> A New Compilation Technique for Parallelizing Loops with Unpredictable Branches on a VLIW architecture. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 213-229. </pages> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: A full description of the compiler can be found in [2]. The compiler is based on a software pipelining technique called enhanced pipeline scheduling 1 <ref> [4] </ref>, which basically applies two actions repetitively when scheduling a loop: first, cut some edges of the given cyclic graph of the loop to yield a DAG; then, schedule the DAG and generate a group of independent instructions on each edge that was cut.
Reference: [5] <author> J. Dehnert and R. Towle. </author> <title> Compiling for Cydra 5. </title> <journal> Journal of Supercomputing: Special Issue on Instruction-Level Parallelism, </journal> 7(1/2):181-228, 1993. 
Reference-contexts: More importantly, loops with conditional branches can be pipelined with a variable initiation interval (II) so that a variable cycles/iteration rate can be achieved depending on which path through the loop is followed at execution time. This algorithm contrasts with previous techniques based on a fixed II <ref> [5, 6] </ref>, where if the loop body includes conditional branches, they computes a worst-case II across all paths, either by removing branches through if-conversion [5] or by reducing the entire if-then-else-endif control structure to a single super-instruction (hierarchical reduction) [6]. <p> This algorithm contrasts with previous techniques based on a fixed II [5, 6], where if the loop body includes conditional branches, they computes a worst-case II across all paths, either by removing branches through if-conversion <ref> [5] </ref> or by reducing the entire if-then-else-endif control structure to a single super-instruction (hierarchical reduction) [6]. The example in Figure 3 shows how a loop with a conditional branch can be software pipelined with a variable II.
Reference: [6] <author> M. Lam. </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines. </title> <booktitle> In Proceedings of the SIGPLAN 1988 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <year> 1988. </year>
Reference-contexts: More importantly, loops with conditional branches can be pipelined with a variable initiation interval (II) so that a variable cycles/iteration rate can be achieved depending on which path through the loop is followed at execution time. This algorithm contrasts with previous techniques based on a fixed II <ref> [5, 6] </ref>, where if the loop body includes conditional branches, they computes a worst-case II across all paths, either by removing branches through if-conversion [5] or by reducing the entire if-then-else-endif control structure to a single super-instruction (hierarchical reduction) [6]. <p> This algorithm contrasts with previous techniques based on a fixed II [5, 6], where if the loop body includes conditional branches, they computes a worst-case II across all paths, either by removing branches through if-conversion [5] or by reducing the entire if-then-else-endif control structure to a single super-instruction (hierarchical reduction) <ref> [6] </ref>. The example in Figure 3 shows how a loop with a conditional branch can be software pipelined with a variable II. In Figure 3 (a), each instruction in the loop is assumed to take a single cycle except for the multiplication, which takes two cycles. <p> This register overwrite problem for the scheduling of straight-line loops has already been handled by other unrolling-based techniques. Modulo variable expansion is a technique used for software pipelining with a fixed II <ref> [6] </ref>. A pipelined loop schedule is determined based only on resource requirements and dependence constraints. If the live range of a register in the schedule overlaps between iterations, the loop is unrolled and different registers are allocated.
Reference: [7] <author> T. Nakatani and K. Ebcioglu. </author> <title> Combining as a Compilation Technique for a VLIW Architecture. </title> <booktitle> In Proceedings of the 22nd Annual Workshop on Microprogramming (Micro-22), </booktitle> <pages> pages 43-55, </pages> <year> 1989. </year>
Reference-contexts: When the operation being moved up passes through a copy instruction that sets one of its source operands, the operand is combined (replaced) by the source of the copy instruction <ref> [7] </ref>. For example, if A r3'=r3,r5 passes through copy r5=r5' during its code motion, it becomes A r3'=r3,r5'.
Reference: [8] <author> T. Nakatani and K. Ebcioglu. </author> <title> Making Compaction Based Parallelization Affodable. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 1014-1529, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The instruction label at the selected leaf becomes the next target instruction to be executed. Then, those operations on the taken path are executed in parallel by performing all the operand "reads" first, and then all operand "writes". The tree VLIW architecture can be found in <ref> [8] </ref>. 4.2 VLIW Experimental Environments The experiments have been performed in our VLIW environment. The input C code is compiled into RISC assembly code through the PL.8 optimizing compiler [9]. The RISC code is then parallelized into VLIW tree code.
Reference: [9] <author> H. Warren, M. Auslander, G. Chaitin, A. Chibib M. Hopkins, and A. MacKay. </author> <title> Final Code Generation in the PL.8 Compiler. </title> <type> Research Report RC 11974, </type> <institution> IBM Research Division, T.J. Watson Research Center, </institution> <month> Jun </month> <year> 1986. </year>
Reference-contexts: The tree VLIW architecture can be found in [8]. 4.2 VLIW Experimental Environments The experiments have been performed in our VLIW environment. The input C code is compiled into RISC assembly code through the PL.8 optimizing compiler <ref> [9] </ref>. The RISC code is then parallelized into VLIW tree code. Finally, the VLIW code is executed on our VLIW simulator, producing outputs and execution statistics. Our benchmarks are composed of five AIX utilities (sort, yacc, sed, compress, fgrep) and SPEC 89 integer benchmarks except for gcc.
Reference: [10] <author> S.-M. Moon and S. D. Carson. </author> <title> Generalized Multi-way Branch Unit for VLIW Microprocessors. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 6(8) </volume> <pages> 850-862, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: All ALUs can perform ALU operations, and half of them can also perform memory operations. However, an ALU can perform only one operation (either a memory operation or an ALU operation) in one cycle. We assume that multi-way branching is handled by a dedicated branch unit <ref> [10] </ref>.
Reference: [11] <author> L. Thiele. </author> <title> Hierarchical Elimination of Copy Operations for Code Optimization. </title> <type> Research Report RC 17325, </type> <institution> IBM Research Division, T.J. Watson Research Center. </institution> <month> 15 </month>
Reference-contexts: We believe further tuning can make it execute one cycle/iteration. 13 elimination Acknowledgements We are grateful to Professor Lothar Thiele who initially wrote a research report on the problem <ref> [11] </ref>, based on discussions with the second author. We are grateful to Gabby Silberman and Jaime Moreno for helpful discussions.
References-found: 11

