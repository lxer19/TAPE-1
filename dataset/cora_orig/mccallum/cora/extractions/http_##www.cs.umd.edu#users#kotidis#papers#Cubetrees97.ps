URL: http://www.cs.umd.edu/users/kotidis/papers/Cubetrees97.ps
Refering-URL: http://www.cs.umd.edu/users/kotidis/publications.html
Root-URL: 
Email: nick@cs.umd.edu  kotidis@cs.umd.edu  mema@cs.stanford.edu  
Title: Cubetree: Organization of and Bulk Incremental Updates on the Data Cube  
Author: Nick Roussopoulos Yannis Kotidis Mema Roussopoulos 
Address: Stanford University  
Affiliation: Department of Computer Science and Institute of Advanced Computer Studies University of Maryland  Department of Computer Science University of Maryland  Department of Computer Science  
Abstract: The data cube is an aggregate operator which has been shown to be very powerful for On Line Analytical Processing (OLAP) in the context of data warehousing. It is, however, very expensive to compute, access, and maintain. In this paper we define the "cubetree" as a storage abstraction of the cube and realize it using packed R-trees for most efficient cube queries. We then reduce the problem of creation and maintenance of the cube to sorting and bulk incremental merge-packing of cubetrees. This merge-pack has been implemented to use separate storage for writing the updated cubetrees, therefore allowing cube queries to continue even during maintenance. Finally, we characterize the size of the delta increment for achieving good bulk update schedules for the cube. The paper includes experiments with various data sets measuring query and bulk update performance. 
Abstract-found: 1
Intro-found: 1
Reference: [AAD + 96] <author> S. Agrawal, R. Agrawal, P. Deshpande, A. Gupta, J. Naughton, R. Ramakrishnan, and S. Sarawagi. </author> <title> On the Computation of Multidimensional Aggregates. </title> <booktitle> In Proc. of VLDB, </booktitle> <pages> pages 506-521, </pages> <address> Bombay, India, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: The last, but perhaps the most critical issue in data warehouse environments, is the time to generate and/or refresh these aggregate projections. Typically, the mere size of the raw and derived data does not permit frequent recomputation. Optimization techniques proposed in <ref> [HRU96, AAD + 96] </ref> deal only with the initial computation of the cube and capitalize on the reduction of repetitive intra-cube computation, but have not been adapted for incremental refresh of the cube. <p> It is a home grown synthetic dataset modeling supermarket purchases with parameters borrowed from <ref> [AAD + 96] </ref>. Each tuple holds three attributes: store id (distinct values 73), date (16), item id (48510) and the measure attribute amount. phases as new updates are applied in chunks of 80MB each.
Reference: [BM72] <author> R. Bayer and E. McCreight. </author> <title> Organization and maintenance of large ordered indexes. </title> <journal> Acta In-formatica, </journal> <volume> 1(3) </volume> <pages> 173-189, </pages> <year> 1972. </year>
Reference-contexts: For example, the whole EDM can be realized by just a conventional relational storage with no indexing capability for the cube. Another possibility, would be to realize EDM by an R-tree, [Gut84], or a combination of relational structures, R-trees and B-trees <ref> [BM72] </ref>. Since most of the indexing techniques are hierarchical, without loss of generality, we assume that the EDM is a tree-like (forest-like) structure that we refer to as the cubetree of R.
Reference: [GBLP96] <author> J. Gray, A. Bosworth, A. Layman, and H. Pi-ramish. </author> <title> Data cube: A Relational Aggrega 10 tion Operator Generalizing Group-By, </title> <booktitle> Cross--Tab, and SubTotals. In Proc. of the 12th Int. Conference on Data Engineering, </booktitle> <pages> pages 152-159, </pages> <address> New Orleans, </address> <month> February </month> <year> 1996. </year> <note> IEEE. </note>
Reference-contexts: 1 Introduction On Line Analytical Processing (OLAP) and data mining are recently receiving considerable attention in the context of data warehousing. Businesses and organizations believe that OLAP is critical in decision making. The data cube <ref> [GBLP96] </ref> and the flurry of papers generated in less than a year from its publication is an indisputable testimony to that effect. In technical terms, the cube is a redundant multidimensional projection of a relation. <p> Furthermore, the content of each projection can hold a collection of values computed by multiple aggregate functions such as sum, count, average, etc. discussed in <ref> [GBLP96] </ref>. all- groupby (none). We call this the Extended Datacube Model, (EDM) because it permits us to visualize the relation data and its cube in a unifying representation.
Reference: [GHRU97] <author> H. Gupta, V. Harinarayan, A. Rajaraman, and J. Ullman. </author> <title> Index Selection for OLAP. </title> <booktitle> In Proceedings of the Intl. Conf. on Data Engineering, </booktitle> <pages> pages 208-219, </pages> <address> Burmingham, UK, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: There is direct tradeoff between redundancy and performance and a rather typical optimization strategy is for a given amount of storage, find the best selection of summary tables and indexes which maximizes query performance <ref> [Rou82, GHRU97] </ref>. However, another, and yet even more important optimization seems to be that of clustering of the storage maintaining the aggregates. Because of the high-dimensionality space, data clustering within these redundant structures has far more impact on the performance of the queries than their individual sizes.
Reference: [Gra93] <author> J. Gray. </author> <title> The Benchmark Handbook for Databas and Transaction Processing Systems- 2nd edition. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Franscisco, </address> <year> 1993. </year>
Reference-contexts: All tests in this section and next were run on a single 21064/275MHz Alpha processor of an Alphaserver 2100A under Digital Unix 3.2. All experiments assume that all range queries on the cube must be supported. The first dataset we used is inspired from the decision-support benchmark TPC-D <ref> [Gra93] </ref>. For our experiments, a subset of the attributes of the TPC-D schema was used. This dataset models a business warehouse with three grouping attributes customer, supplier, and part. The measure attribute sales is aggregated using the sum function.
Reference: [GSE + 94] <author> J. Gray, P. Sundaresan, S. Englert, K. Ba-clawski, and P. Weiberger. </author> <title> Quickly generating billion-record synthetic databases. </title> <booktitle> In Proc. of the ACM SIGMOD, </booktitle> <pages> pages 243-252, </pages> <address> Minneapo-lis, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Each tuple of the relation is 20 bytes and the distinct values for customer, supplier and part are respectively 100,000,10,000, and 200,000. In order to make the dataset more realistic, we imposed to each attribute the 80-20 Self-Similar distribution (h = 0:2) described in <ref> [GSE + 94] </ref>. We incrementally computed the cube for 6 different sizes of this dataset: 32,64,96,128,160 and 800MB. For packing the cube into a dataless cubetree, we used the sort order obtained by the SelectSortOrder algorithm assuming equal query probabilities for all groupbys.
Reference: [Gut84] <author> A. Guttman. R-Trees: </author> <title> A Dynamic Index Structure for Spatial Searching. </title> <booktitle> In SIGMOD 84'. Proceedings of Annual Meeting, </booktitle> <address> Boston, MA, </address> <pages> pages 47-57, </pages> <year> 1984. </year>
Reference-contexts: For example, the whole EDM can be realized by just a conventional relational storage with no indexing capability for the cube. Another possibility, would be to realize EDM by an R-tree, <ref> [Gut84] </ref>, or a combination of relational structures, R-trees and B-trees [BM72]. Since most of the indexing techniques are hierarchical, without loss of generality, we assume that the EDM is a tree-like (forest-like) structure that we refer to as the cubetree of R.
Reference: [HRU96] <author> V. Harinarayan, A. Rajaraman, and J. Ullman. </author> <title> Implementing Data Cubes Efficiently. </title> <booktitle> In Proc. of ACM SIGMOD, </booktitle> <pages> pages 205-216, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: The last, but perhaps the most critical issue in data warehouse environments, is the time to generate and/or refresh these aggregate projections. Typically, the mere size of the raw and derived data does not permit frequent recomputation. Optimization techniques proposed in <ref> [HRU96, AAD + 96] </ref> deal only with the initial computation of the cube and capitalize on the reduction of repetitive intra-cube computation, but have not been adapted for incremental refresh of the cube. <p> In <ref> [HRU96] </ref> the cube operator was defined as a lattice in the attribute space, shown in Figure 6. Every node in the lattice represents a groupby some attributes. Consider now the EDM cubetree minus the relation data points.
Reference: [KF93] <author> Ibrahim Kamel and Christos Faloutsos. Hilbert r-tree: </author> <title> an improved r-tree using fractals. </title> <institution> Systems Research Center (SRC) TR-93-19, Univ. of Maryland, College Park, </institution> <year> 1993. </year>
Reference-contexts: The sorting keys, i.e., primary, secondary, etc., used in sort-pack can be chosen from LowX, LowY, or LowX & LowY, etc., or from values computed by space filling curves such as Hilbert or Peano <ref> [KF93] </ref>. The selected order determines the bias of the clustering. For example, in some cases, space filling curves achieve good clustering for the relation data points, but destroy cube range queries because they scatter all the projection points.
Reference: [NBC + 94] <author> C. Nyberg, T. Barclay, Z Cvetanovic, J. Gray, and D. Lomet. AlphaSort: </author> <title> A RISC Machine Sort. </title> <booktitle> In Proc. of the ACM SIGMOD, </booktitle> <pages> pages 233-242, </pages> <address> Minneapolis, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: At the fastest reported sort rate of 1GB per minute on the alphasort machine <ref> [NBC + 94] </ref>, recomputation of the cube for a data collection rate of 10GB a day would require roughly one day to sort 4 month data collection.
Reference: [RL85] <author> N. Roussopoulos and D. Leifker. </author> <title> Direct Spatial Search on Pictorial Databases Using Packed R-trees. </title> <booktitle> In Procs. of 1985 ACM SIGMOD Intl. Conf. on Management of Data, </booktitle> <address> Austin, </address> <year> 1985. </year>
Reference-contexts: In the first part of this paper, we define the cubetree as a storage abstraction of the cube and argue for a compact representation for it using packed R-trees <ref> [RL85] </ref>. We then project the cubetree into reduced dimensionality cube-trees which decrease storage and significantly improve query overall performance. We develop algorithms for allocating each cube query to the projected cubetree that gives good clustering and for selecting good sort order within each cu-betree. <p> This is the subject of the next subsection. 2.1 Packed R-trees for Improving Datacube Clustering Random record-at-a-time insertions are not only very slow because of the continuous reorganization of the space but also destroy data clustering in all multidimensional indexing schemes. Packed R-trees, introduced in <ref> [RL85] </ref>, avoid these problems by * sorting first the objects in some desirable order * bulk loading the R-tree from the sorted file and packing the nodes to capacity This sort-pack method achieves excellent clustering and significantly reduces the overlap and dead space (i.e. space that contains no data points).
Reference: [Rou82] <author> N. Roussopoulos. </author> <title> View indexing in relational databases. </title> <journal> ACM TODS, </journal> <volume> 7(2) </volume> <pages> 258-290, </pages> <year> 1982. </year>
Reference-contexts: There is direct tradeoff between redundancy and performance and a rather typical optimization strategy is for a given amount of storage, find the best selection of summary tables and indexes which maximizes query performance <ref> [Rou82, GHRU97] </ref>. However, another, and yet even more important optimization seems to be that of clustering of the storage maintaining the aggregates. Because of the high-dimensionality space, data clustering within these redundant structures has far more impact on the performance of the queries than their individual sizes.
Reference: [SDNR96] <author> A. Shukla, P. Deshpande, J. Naughton, and K. Ramasamy. </author> <title> Storage estimation for multidimensional aggregates in the presense of hierarchies. </title> <booktitle> In Proc. of VLDB, </booktitle> <pages> pages 522-531, </pages> <address> Bom-bay, India, </address> <month> August </month> <year> 1996. </year> <month> 11 </month>
Reference-contexts: It computes all possible groupby SQL operators and aggregates their results into a N-dimensional space for answering OLAP queries. These aggregates are then stored in derived summary tables or multidimensional arrays whose size can be estimated using mathematical tools <ref> [SDNR96] </ref>. Because these aggregates are typically very large, indexing, which adds to their redundancy, is necessary to speed up queries on them. <p> Given a set of groupbys G to be supported by a single cubetree, we need an algorithm for getting a good sort-pack order. The following parameters characterize this problem: * the cardinality (or at least an approximation <ref> [SDNR96] </ref>) of each g 2 G, denoted as card (g) * the probability that a query will use each g 2 G, de noted as prob (g) Given these parameters, the following algorithm will se lect a good sort-pack order: SelectSortOrder: reset a counter for each attribute appearing in G for
References-found: 13

