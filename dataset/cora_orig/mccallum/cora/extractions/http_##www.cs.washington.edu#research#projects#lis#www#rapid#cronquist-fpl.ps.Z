URL: http://www.cs.washington.edu/research/projects/lis/www/rapid/cronquist-fpl.ps.Z
Refering-URL: http://www.cs.washington.edu/research/projects/lis/www/rapid/index.html
Root-URL: 
Title: RaPiD Reconfigurable Pipelined Datapath  
Author: Carl Ebeling, Darren C. Cronquist, and Paul Franklin 
Address: Box 352350 Seattle, WA 98195-2350  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Configurable computing has captured the imagination of many architects who want the performance of application-specific hardware combined with the reprogrammability of general-purpose computers. Unfortunately, configurable computing has had rather limited success largely because the FPGAs on which they are built are more suited to implementing random logic than computing tasks. This paper presents RaPiD, a new coarse-grained FPGA architecture that is optimized for highly repetitive, computation-intensive tasks. Very deep application-specific computation pipelines can be configured in RaPiD. These pipelines make much more efficient use of silicon than traditional FPGAs and also yield much higher performance for a wide range of applications.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. M. Arnold, D. A. Buell, D. T. Hoang, D. V. Pryor, N. Shirazi, and M. R. Thistle. </author> <title> The Splash 2 processor and applications. </title> <booktitle> In Proceedings IEEE International Conference on Computer Design: VLSI in Computers and Processors, </booktitle> <pages> pages 482-5. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1993. </year>
Reference-contexts: Unfortunately the promise of configurable computing has yet to be realized in spite of some very successful examples <ref> [1, 5] </ref>. There are two main reasons for this. First, configurable computing platforms are currently implemented using commercial FPGAs which are very efficient for implementing random logic functions, but much less so for general arithmetic functions. Building a multiplier using an FPGA incurs a performance/cost penalty of at least 100.
Reference: 2. <author> H.T. Kung. </author> <title> Let's design algorithms for VLSI systems. </title> <type> Technical Report CMU-CS-79-151, </type> <institution> Carnegie-Mellon University, </institution> <month> January </month> <year> 1979. </year>
Reference-contexts: The structure of the datapaths constructed in RaPiD is biased strongly towards linear arrays of functional units communicating in mostly a nearest neighbor fashion. Systolic arrays <ref> [2] </ref>, for example, map very well into RaPiD datapaths, which allows the considerable amount of research on compiling to systolic arrays to be applied to compiling computations to RaPiD [4, 3]. RaPiD is not limited to implementing systolic arrays, however.
Reference: 3. <author> P. Lee and Z. M. Kedem. </author> <title> Synthesizing linear array algorithms from nested FOR loop algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1578-98, </pages> <year> 1988. </year>
Reference-contexts: Systolic arrays [2], for example, map very well into RaPiD datapaths, which allows the considerable amount of research on compiling to systolic arrays to be applied to compiling computations to RaPiD <ref> [4, 3] </ref>. RaPiD is not limited to implementing systolic arrays, however. For example, a pipeline can be constructed which comprises different computations at different stages and at different times. The computational bandwidth provided by a RaPiD array is extremely high and scales with the size of the array.
Reference: 4. <author> D. I. Moldovan and J. A. B. Fortes. </author> <title> Partitioning and mapping algorithms into fixed size systolic arrays. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(1):1-12, </volume> <year> 1986. </year>
Reference-contexts: Systolic arrays [2], for example, map very well into RaPiD datapaths, which allows the considerable amount of research on compiling to systolic arrays to be applied to compiling computations to RaPiD <ref> [4, 3] </ref>. RaPiD is not limited to implementing systolic arrays, however. For example, a pipeline can be constructed which comprises different computations at different stages and at different times. The computational bandwidth provided by a RaPiD array is extremely high and scales with the size of the array. <p> The domain of applicability must be explored by mapping more problems from different domains to RaPiD. Thus far all RaPiD applications have been designed by hand. The next step will be to apply compiler technology, particularly loop-transformation theory [7] and systolic array compiling methods <ref> [4] </ref> to build a compiler for RaPiD. A memory architecture must be designed which can support the I/O band width required by RaPiD over a wide range of applications.
Reference: 5. <author> J. E. Vuillemin, P. Bertin, D. Roncin, M. Shand, H. H. Touati, and P. Boucard. </author> <title> Programmable active memories: reconfigurable systems come of age. </title> <journal> IEEE Transactions on Very Large Scale Integration (VLSI) Systems, </journal> <volume> 4(1) </volume> <pages> 56-69, </pages> <year> 1996. </year>
Reference-contexts: Unfortunately the promise of configurable computing has yet to be realized in spite of some very successful examples <ref> [1, 5] </ref>. There are two main reasons for this. First, configurable computing platforms are currently implemented using commercial FPGAs which are very efficient for implementing random logic functions, but much less so for general arithmetic functions. Building a multiplier using an FPGA incurs a performance/cost penalty of at least 100. <p> Building a multiplier using an FPGA incurs a performance/cost penalty of at least 100. Second, current configurable platforms are extremely hard to program <ref> [5, 6] </ref>. Taking an application from concept to a high-performance implementation is a time-consuming, designer-intensive task.
Reference: 6. <author> M. Wazlowski, L. Agarwal, T. Lee, A. Smith, E. Lam, P. Athanas, H. Silverman, and S. Ghosh. </author> <title> PRISM-II compiler and architecture. </title> <booktitle> In Proceedings IEEE Workshop on FPGAs for Custom Computing Machines, </booktitle> <pages> pages 9-16. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1993. </year>
Reference-contexts: Building a multiplier using an FPGA incurs a performance/cost penalty of at least 100. Second, current configurable platforms are extremely hard to program <ref> [5, 6] </ref>. Taking an application from concept to a high-performance implementation is a time-consuming, designer-intensive task.
Reference: 7. <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <year> 1991. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: The domain of applicability must be explored by mapping more problems from different domains to RaPiD. Thus far all RaPiD applications have been designed by hand. The next step will be to apply compiler technology, particularly loop-transformation theory <ref> [7] </ref> and systolic array compiling methods [4] to build a compiler for RaPiD. A memory architecture must be designed which can support the I/O band width required by RaPiD over a wide range of applications.
References-found: 7

