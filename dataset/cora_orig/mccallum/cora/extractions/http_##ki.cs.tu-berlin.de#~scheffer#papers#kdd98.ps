URL: http://ki.cs.tu-berlin.de/~scheffer/papers/kdd98.ps
Refering-URL: http://ki.cs.tu-berlin.de/~scheffer/publications.html
Root-URL: 
Email: scheffer@cs.tu-berlin.de  
Title: Discovering Association Rules with High Predictive Power  
Author: Tobias Scheffer 
Keyword: Association rules, over-fitting, computational learning theory.  
Date: March 13, 1998  
Address: FR 5-8, Franklinstr. 28/29 10587 Berlin, Germany  
Affiliation: Technische Universitaet Berlin  
Abstract: I argue that association rules are only useful if they express regularities in the process that created a particular database, rather than regularities in the database itself which may be due to chance and may have little or no predictive power. As a consequence, I propose a modified definition of the discovery problem. I present an algorithm which is guaranteed to find (with high probability) only those association rules which characterize the underlying process. The algorithm operates on arbitrarily small subsets of the data base which makes its efficiency controllable. As a drawback, the algorithm is incomplete - i.e., it finds some, but not all rules (the larger the sample, the more rules it will find). I then show a second algorithm which is guaranteed (with high probability) to find all regularities in the underlying process. The subset of the database which this algorithm needs to look at grows polynomially in the number of rows. However, the number of association rules can potentially grow exponentially in the number of rows. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agrawal, R., Imielinski, T., & Swami, A. </author> <year> (1993). </year> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pp. 207-216. </pages>
Reference-contexts: 1 Introduction Association rules <ref> (Agrawal et al., 1993) </ref> express regularities between sets of data items. "Beer and TV-magazine ) chips (95%)" is an example of an association rule and expresses that in a particular store 95% of all customers who buy beer and a TV-magazine also buy chips. <p> Modeling a process as a distribution implies a strong assumption: Throughout this paper I assume that the process be stationary - i.e., the distribution of transactions does not vary over time. Although this assumption is not explicitly made in other work on discovery of association rules <ref> (e.g., Agrawal et al., 1993) </ref>, it is always hidden in the implicit assumption that the rules which were created from past data will be useful, by any means, in the future.
Reference: <author> Alon, N., & Spencer, J. </author> <title> (92). The Probabilistic Method. </title> <publisher> John Wiley. </publisher>
Reference: <author> Fayyad, U., Piatetski-Shapiro, G., & Smyth, P. </author> <year> (1996). </year> <title> Knowledge discovery and data mining: Towards a unifying framework. </title> <booktitle> In KDD-96. </booktitle>
Reference: <author> Haussler, D. </author> <year> (1992). </year> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100 (1), </volume> <pages> 78-150. </pages>
Reference-contexts: In order to prove Theorem 1, we need to show that the probability that there is an association rule for which the difference between true and empirical accuracy exceeds ", is below ffi. This can be proven analogously to the error bounds of agnostic learning theory <ref> (e.g., Haussler, 1992) </ref> or the VC framework (Vapnik & Chervonenkis, 1971). Assume that X ) Y ( ^ A) is an association rule with true accuracy A and support w which we drew randomly. <p> The presented results are based on results on computational learning theory; in particular, on results of agnostic PAC learning <ref> (Haussler, 1992) </ref>. There are, however, differences between the PAC setting (Valiant, 1984) and the task of discovering association rules: In a PAC setting, there is one hypothesis which is supported by a sample of fixed size.
Reference: <author> Hoeffding, W. </author> <year> (1963). </year> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58 (301), </volume> <pages> 13-30. </pages>
Reference-contexts: Assume that X ) Y ( ^ A) is an association rule with true accuracy A and support w which we drew randomly. The chance that the difference between A and ^ A can be bounded using the Hoeffding inequality <ref> (Hoeffding, 1963) </ref>: P ( ^ A &gt; A ") e 2w" 2 The Hoeffding inequality only holds for iid samples.
Reference: <author> Holsheimer, M., & Siebes, A. </author> <year> (1991). </year> <title> Data mining. the search for knowledge in databases. </title> <type> Tech. rep. </type> <institution> CS-R9406, CWI Amsterdam, </institution> <address> P.O. Box 94079, 1090 GB Amsterdam, The Netherlands. </address>
Reference: <author> Kearns, M., & Vazirani, U. </author> <year> (1994). </year> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT Press. </publisher>
Reference: <author> Mannila, H., Toivonen, H., & Verkano, A. I. </author> <year> (1994). </year> <title> Efficient algorithms for discovering association rules. </title> <booktitle> Proc. AAAI Workshop on Knowledge Discovery in Databases, </booktitle> <pages> 181-192. </pages> <note> 10 Piatetski-Shapiro, </note> <author> G., & Frawley, W. </author> <year> (1991). </year> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI Press. </publisher>
Reference: <author> Savasere, A., Omiecinski, E., & Navathe, S. </author> <year> (1995). </year> <title> An efficient algorithm for minimg association rules in large databases. </title> <booktitle> In Proc. VLDB Conference. </booktitle>
Reference-contexts: This requires one database pass per level; alternatively, a superset of the collection of frequent sets can be identified by processing the database iteratively in chunks which can be held in main memory <ref> (Savasere et al., 1995) </ref>. 3 Finding Good Association Rules First, I re-define the task of discovery of association rules, such that the underlying process is focused rather than the data. Then, I quantify the empirical support which assures a good alignment between empirical and true accuracy.
Reference: <author> Srikant, R., & Agrawal, R. </author> <year> (1996). </year> <title> Mining quantitative association rules in large relational tables. </title> <booktitle> In Proc. of the 2nd Int'l Conference on Knowledge Discovery in Databases and Data Mining. </booktitle>
Reference-contexts: The syntax of association rules can easily be extended to cover, for example, interval constraints <ref> (Srikant & Agrawal, 1996) </ref>. My results can, in principle, be adapted to quantitative association rules. As long as the quantities are discrete, the space of association rules stays finite and only the factors have to be adapted.
Reference: <author> Toivonen, H. </author> <year> (1996). </year> <title> Sampling large databases for association rules. </title> <booktitle> In Proc. VLDB Conference. </booktitle>
Reference-contexts: By contrast, an association rule algorithm will return many rules each of which is supported by differently many transactions. It has to be guaranteed that every single rule is valid with respect to the underlying process. My results share some ideas with sampling approaches in KDD <ref> (e.g., Toivonen, 1996) </ref>.
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27. </volume>
Reference-contexts: The presented results are based on results on computational learning theory; in particular, on results of agnostic PAC learning (Haussler, 1992). There are, however, differences between the PAC setting <ref> (Valiant, 1984) </ref> and the task of discovering association rules: In a PAC setting, there is one hypothesis which is supported by a sample of fixed size. By contrast, an association rule algorithm will return many rules each of which is supported by differently many transactions.
Reference: <author> Vapnik, V. </author> <year> (1996). </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer. </publisher>
Reference: <author> Vapnik, V. N., & Chervonenkis, A. Y. </author> <year> (1971). </year> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16 (2), </volume> <pages> 264-280. </pages>
Reference-contexts: My results can, in principle, be adapted to quantitative association rules. As long as the quantities are discrete, the space of association rules stays finite and only the factors have to be adapted. Handling of continuous attributes, however, requires referring to the VC framework <ref> (Vapnik & Chervonenkis, 1971) </ref>. In an attempt to keep the results comprehensible and conceptually clear this paper is restricted to "vanilla" association rules. <p> This can be proven analogously to the error bounds of agnostic learning theory (e.g., Haussler, 1992) or the VC framework <ref> (Vapnik & Chervonenkis, 1971) </ref>. Assume that X ) Y ( ^ A) is an association rule with true accuracy A and support w which we drew randomly.
Reference: <author> Wolpert, D. H. </author> <year> (1995). </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework. </title> <editor> In Wolpert, D. H. (Ed.), </editor> <booktitle> The Mathematics of Generalization, The SFI Studies in the Sciences of Complexity, </booktitle> <pages> pp. 117-214. </pages> <publisher> Addison-Wesley. </publisher> <pages> 11 </pages>
References-found: 15

