URL: http://www.cs.toronto.edu/~mackay/nn_im_decon.ps.gz
Refering-URL: http://www.cs.toronto.edu/~mackay/README.html
Root-URL: http://www.cs.toronto.edu
Title: Neural Network Image Deconvolution compared with the optimal linear filter, and with the Bayesian algorithm
Author: John E. Tansley, Martin J. Oldfield and David J.C. MacKay 
Note: Neural networks have been  
Address: Cambridge, CB3 0HE. United Kingdom.  
Affiliation: Cavendish Laboratory  
Abstract: We examine the problem of deconvolving blurred text. This is a task in which there is strong prior knowledge (e.g., font characteristics) that is hard to express computationally. These priors are implicit, however, in mock data for which the true image is known. When trained on such mock data, a neural network is able to learn a solution to the image deconvolution problem which takes advantage of this implicit prior knowledge. Prior knowledge of image positivity can be hard-wired into the functional architecture of the network, but we leave it to the network to learn most of the parameters of the task from the data. We do not need to tell the network about the point spread function, the intrinsic correlation function, or the noise process. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.F. </author> <title> Gull. Developments in maximum entropy data analysis. </title> <editor> In J. Skilling, editor, </editor> <title> Maximum Entropy and Bayesian Methods, </title> <address> Cambridge 1988, </address> <pages> pages 53-71, </pages> <address> Dordrecht, 1989. </address> <publisher> Kluwer. </publisher>
Reference-contexts: This model enforces positivity; the parameter ff defines a characteristic dynamic range by which the pixel values are expected to differ from the default image m. The `ICF maximum entropy' model <ref> [1] </ref> introduces an expectation of spatial correlations into the prior on f by writing f = Gh, where G is a convolution with an intrinsic correlation function, and putting a classic maxent prior on h. 4 J.E. Tansley, M.J. Oldfield and D.J.C.
Reference: [2] <author> S.F. Gull and G.J. Daniell. </author> <title> Image reconstruction from incomplete and noisy data. </title> <journal> Nature, </journal> <volume> 272 </volume> <pages> 686-690, </pages> <year> 1978. </year>
Reference-contexts: This leads to the most pronounced problems where the image under observation has high contrast. Optimal linear filters applied to radio astronomical data give reconstructions with negative areas in them, corresponding to patches of sky that suck energy out of radio telescopes. The `Maximum Entropy' model for image deconvolution <ref> [2] </ref> was a great success principally because this model forced the reconstructed image to be positive. The spurious negative areas and complementary spurious positive areas are eliminated, and the dynamic range of the reconstruction is greatly enhanced.
Reference: [3] <author> I. Guyon, V.N. Vapnik, B.E. Boser, L.Y. Bottou, and S.A. Solla. </author> <title> Structural risk minimization for character recognition. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 471-479, </pages> <address> San Ma-teo, California, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Equivalently, one can retain the former regularizer, and blur the input data before feeding it to the network. This may sound surprising, but blurring the data even more can indeed enhance the performance of such networks <ref> [3] </ref>.
Reference: [4] <author> D.J.C. MacKay. </author> <title> The evidence framework applied to classification networks. </title> <journal> Neural Computation, </journal> <volume> 4(5) </volume> <pages> 698-714, </pages> <year> 1992. </year>
Reference-contexts: We find parameters w that maximize the posterior probability, i.e., the product of the likelihood (factors of the form (14)) and the prior. We optimize the variance of the prior (the `weight decay constant') using approximate Bayesian methods <ref> [5, 4] </ref>. (Amusingly, these Bayesian regularization methods are descended from those developed in the Bayesian Maximum entropy method.) 6 J.E. Tansley, M.J. Oldfield and D.J.C. MacKay MemSys; reconstruction by trained network.
Reference: [5] <author> D.J.C. MacKay. </author> <title> A practical Bayesian framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 448-472, </pages> <year> 1992. </year>
Reference-contexts: We find parameters w that maximize the posterior probability, i.e., the product of the likelihood (factors of the form (14)) and the prior. We optimize the variance of the prior (the `weight decay constant') using approximate Bayesian methods <ref> [5, 4] </ref>. (Amusingly, these Bayesian regularization methods are descended from those developed in the Bayesian Maximum entropy method.) 6 J.E. Tansley, M.J. Oldfield and D.J.C. MacKay MemSys; reconstruction by trained network.
Reference: [6] <author> J. Skilling. </author> <title> Classic maximum entropy. </title> <editor> In J. Skilling, editor, </editor> <title> Maximum Entropy and Bayesian Methods, </title> <address> Cambridge 1988, Dordrecht, 1989. </address> <publisher> Kluwer. </publisher>
Reference-contexts: The `Classic maximum entropy' model assigns an entropic prior P (f jff; m; H Classic ) = exp (ffS (f ; m))=Z, where S (f ; m) = P i (f i log (m i =f i ) + f i m i ) <ref> [6] </ref>. This model enforces positivity; the parameter ff defines a characteristic dynamic range by which the pixel values are expected to differ from the default image m.
References-found: 6

