URL: ftp://ftp.cs.unimaas.nl/pub/ecml97/wolters-ftp.ps.gz
Refering-URL: http://ilk.kub.nl/papers.html
Root-URL: 
Email: mwo@asl1.ikp.uni-bonn.de  antal@cs.unimaas.nl  
Title: Automatic Phonetic Transcription of Words Based On Sparse Data  
Author: Maria Wolters (i) and Antal van den Bosch (ii) 
Address: Poppelsdorfer Allee 47, 53113 Bonn, Germany  PO Box 616, 6200 MD Maastricht, The Netherlands  
Affiliation: (i) Institut fur Kommunikationsforschung und Phonetik, Universitat Bonn  (ii) Department of Computer Science, Universiteit Maastricht  
Note: Pages 61 to 70 of W. Daelemans, A. van den Bosch, and A. Weijters (Editors), Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural Language Processing Tasks, April 26, 1997, Prague, Czech Republic  
Abstract: The relation between the orthography and the phonology of a language has traditionally been modelled by hand-crafted rule sets. Machine-learning (ML) approaches offer a means to gather this knowledge automatically. Problems arise when the training material is sparse. Generalising from sparse data is a well-known problem for many ML algorithms. We present experiments in which connectionist, instance-based, and decision-tree learning algorithms are applied to a small corpus of Scottish Gaelic. instance-based learning in the ib1-ig algorithm yields the best generalisation performance, and that most algorithms tested perform tolerably well. Given the availability of a lexicon, even if it is sparse, ML is a valuable and efficient tool for automatic phonetic transcription of written text.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Kibler, D., and Albert, M. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66. </pages>
Reference: <author> Allen, J., Hunnicutt, S., and Klatt, D. </author> <year> (1987). </year> <title> From Text to Speech: the MITalk system. </title> <address> MITPress, Cambridge, Mass. </address>
Reference-contexts: In a TTS system, orthographic text first has to be converted into a sequence of orthophones, which describe the pronunciation norm. This phonetic transcription is the main input of the synthesis module 1 . 1 Further processing steps are not considered here; for an overview, see <ref> (Allen et al., 1987) </ref>. The classic approach to automatic phonetic transcription (APT) is a large lexicon supplemented with a hand-crafted rule set. Many researchers have tried to replace rule sets using machine learning (ML) algorithms trained on the lexicon, but with mixed success. <p> The performance of most algorithms still falls far below the mark of 80-90% correct words which is needed in high-quality text-to-speech synthesis (Yvon, 1996). However, Bakiri and Dietterich (1993) have shown that their approach based on ID-3 (Quinlan, 1986) decision trees outperforms the sophisticated DECTalk rule set for English <ref> (Allen et al., 1987) </ref>; (Van den Bosch and Daelemans, 1993; Daelemans and Van den Bosch, 1997) report similar results for Dutch. In both cases, the training corpora contained around 18000, and the test corpora around 2000 words.
Reference: <author> Daelemans, W. and Van den Bosch, A. </author> <year> (1992). </year> <title> Generalisation performance of backpropagation learning on a syllabification task. </title> <editor> In Drossaers, M. F. J. and Ni-jholt, A., editors, TWLT3: </editor> <booktitle> Connectionism and Natural Language Processing, </booktitle> <pages> pages 27-37, </pages> <institution> Enschede. Twente University. </institution>
Reference-contexts: When feature values are symbolic, as with our data, ffi (x i ; y i ) = 0 when x i = y i , and ffi (x i ; y i ) = 1 when x i 6= y i . ib1-ig <ref> (Daelemans and Van den Bosch, 1992) </ref> differs from ib1 in the weighting function W (f i ) (cf. Eq. 1). The weighting function of ib1-ig, W 0 (f i ), represents the information gain (Quinlan, 1993) of feature f i .
Reference: <author> Daelemans, W. and Van den Bosch, A. </author> <year> (1997). </year> <title> Language-independent data-oriented grapheme-to-phoneme conversion. </title> <editor> In Van Santen, J. P. H., Sproat, R. W., Olive, J. P., and Hirschberg, J., editors, </editor> <booktitle> Progress in Speech Processing, </booktitle> <pages> pages 77-89. </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Learning in tdidt is eager since decision trees are constructed during learning; classification effort is low since it involves non-backtracking deterministic traversal through the induced tree. Two decision tree algorithms are evaluated here: igtree (information-gain tree, <ref> (Daelemans et al., 1997) </ref>) and sct (semantic classification trees, (Kuhn and De Mori, 1995)). igtree (Daelemans et al., 1997) was designed as an optimised approximation of ib1-ig. In igtree, information gain is used as a guiding function to compress the instance base into a decision tree. <p> Two decision tree algorithms are evaluated here: igtree (information-gain tree, <ref> (Daelemans et al., 1997) </ref>) and sct (semantic classification trees, (Kuhn and De Mori, 1995)). igtree (Daelemans et al., 1997) was designed as an optimised approximation of ib1-ig. In igtree, information gain is used as a guiding function to compress the instance base into a decision tree. Nodes are connected via arcs denoting feature values.
Reference: <author> Daelemans, W., Van den Bosch, A., and Weijters, A. </author> <year> (1997). </year> <title> igtree: using trees for classification in lazy learning algorithms. </title> <journal> AI Review. </journal> <note> to be published. </note>
Reference-contexts: Learning in tdidt is eager since decision trees are constructed during learning; classification effort is low since it involves non-backtracking deterministic traversal through the induced tree. Two decision tree algorithms are evaluated here: igtree (information-gain tree, <ref> (Daelemans et al., 1997) </ref>) and sct (semantic classification trees, (Kuhn and De Mori, 1995)). igtree (Daelemans et al., 1997) was designed as an optimised approximation of ib1-ig. In igtree, information gain is used as a guiding function to compress the instance base into a decision tree. <p> Two decision tree algorithms are evaluated here: igtree (information-gain tree, <ref> (Daelemans et al., 1997) </ref>) and sct (semantic classification trees, (Kuhn and De Mori, 1995)). igtree (Daelemans et al., 1997) was designed as an optimised approximation of ib1-ig. In igtree, information gain is used as a guiding function to compress the instance base into a decision tree. Nodes are connected via arcs denoting feature values.
Reference: <author> Devijver, P. A. and Kittler, J. </author> <year> (1982). </year> <title> Pattern Recognition. A Statistical Approach. </title> <publisher> Prentice-Hall, </publisher> <address> London, UK. </address>
Reference: <author> Dedina, M. and Nusbaum, H. </author> <year> (1991). </year> <title> Pronounce: a program for pronunciation by analogy. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 55-64. </pages>
Reference-contexts: Although chunk-based approaches are psycholinguistically plausible (cf. Glushko, 1979), they are not suitable for minority-language APT. Algorithms in the tradition of PRONOUNCE <ref> (Dedina and Nusbaum, 1991) </ref> rely on extensive statistics about letter/phone correspondences which cannot be estimated adequately from tiny corpora.
Reference: <author> Dietterich, T. and Bakiri, G. </author> <year> (1995). </year> <title> Solving multi-class problems using error-correcting codes. </title> <journal> JAIR, </journal> <volume> 2 </volume> <pages> 263-286. </pages>
Reference-contexts: In both cases, the training corpora contained around 18000, and the test corpora around 2000 words. With the exception of <ref> (Dietterich and Bakiri, 1995) </ref>, most researchers have relied on large machine readable pronunciation dictionaries for training and test data. However, for most languages, the necessary corpora have to be gathered and typed in first, because modern standard pronunciation dictionaries are available neither on paper nor in machine-readable form. <p> Because of the limited window length, it is difficult to capture morphophonological alternations like English Tri-syllabic Shortening as in divine divinity, and stress shifts as in photograph - photography. Three types of phoneme-based approaches have yielded good results for large corpora: neural networks (Sejnowski and Rosenberg, 1987), decision trees <ref> (Dietterich et al., 1995) </ref>, and instance-based learning (Van den Bosch and Daelemans, 1993). 2.1 Neural networks Artificial neural networks (ann) consist of simple processing units with weighted connections. The units are usually grouped into an input layer, an output layer, and one or more hidden layers.
Reference: <author> Dietterich, T., Hild, H., and Bakiri, G. </author> <year> (1995). </year> <title> A comparision of ID3 and backpropagation for English text-to-speech mapping. </title> <journal> Machine Learning, </journal> <volume> 18 </volume> <pages> 51-80. </pages>
Reference-contexts: In both cases, the training corpora contained around 18000, and the test corpora around 2000 words. With the exception of <ref> (Dietterich and Bakiri, 1995) </ref>, most researchers have relied on large machine readable pronunciation dictionaries for training and test data. However, for most languages, the necessary corpora have to be gathered and typed in first, because modern standard pronunciation dictionaries are available neither on paper nor in machine-readable form. <p> Because of the limited window length, it is difficult to capture morphophonological alternations like English Tri-syllabic Shortening as in divine divinity, and stress shifts as in photograph - photography. Three types of phoneme-based approaches have yielded good results for large corpora: neural networks (Sejnowski and Rosenberg, 1987), decision trees <ref> (Dietterich et al., 1995) </ref>, and instance-based learning (Van den Bosch and Daelemans, 1993). 2.1 Neural networks Artificial neural networks (ann) consist of simple processing units with weighted connections. The units are usually grouped into an input layer, an output layer, and one or more hidden layers.
Reference: <author> Gelfand, S., Ravishankar, C., and Delp, E. </author> <year> (1991). </year> <title> An iterative growing and pruning algorithm for classifier design. </title> <journal> IEEE Trans. </journal> <volume> PAMI, </volume> <pages> pages 163-174. </pages>
Reference-contexts: At each node, only one regular expression is tested. There are two branches, one for "match" and one for "no match". While tests are stored at nodes, classes are stored at leaves. To avoid overgeneralisation, the trees are trained using the algorithm of <ref> (Gelfand et al., 1991) </ref>. In contrast to neural nets, scts cannot extract equivalence classes of attributes from the data such as the class of vowel graphemes.
Reference: <author> Glushko, J. </author> <year> (1979). </year> <title> The organization and activation of orthographic knowledge. </title> <journal> J. Experimental Psychology: Human perception and performance, </journal> <pages> pages 674-691. </pages>
Reference-contexts: Although chunk-based approaches are psycholinguistically plausible <ref> (cf. Glushko, 1979) </ref>, they are not suitable for minority-language APT. Algorithms in the tradition of PRONOUNCE (Dedina and Nusbaum, 1991) rely on extensive statistics about letter/phone correspondences which cannot be estimated adequately from tiny corpora.
Reference: <author> Halle, M. </author> <year> (1992). </year> <title> Phonetic features. In Bright, </title> <editor> W., editor, </editor> <booktitle> International Encyclopedia of Linguistics, </booktitle> <pages> pages 207-212. </pages> <publisher> Oxford University Press, Oxford. </publisher>
Reference-contexts: First, a large number of connections means a large variance with the potential to accomodate very complex hidden representations, and secondly, a size of 100-200 hidden units is quite common for this problem in the psycholinguistic/speech processing literature. Letters were encoded using a binary code, phones using phonological features <ref> (Halle, 1992) </ref>.
Reference: <author> Kohonen, T., Kangas, J., Laaksonen, J., and Torkkola, K. </author> <year> (1996). </year> <title> LVQ-PAK the Learning Vector Quantization package v. 3.0. </title> <type> Technical Report A30, </type> <institution> Helsinki University of Technology. </institution>
Reference-contexts: To reduce this noise as much as possible, the net output is classified again. For this second stage, we use Learning Vector Quantization (lvq, <ref> (Kohonen et al., 1996) </ref>). lvq computes a set of no cod codebook vectors which describe no class classes (here: orthophones).
Reference: <author> Kolodner, J. </author> <year> (1993). </year> <title> Case-Based Reasoning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: On the contrary, in eager learning, computational effort is put mainly into learning. ann and decision trees are eager algorithms. simple and robust approaches within the group of Case-Based Reasoning algorithms (CBR) <ref> (Kolodner, 1993) </ref>, because it is based on feature-value vectors rather than on more complex expressions such as those in first-order logic (Kolodner, 1993; Lavrac and Dzeroski, 1994). 2 feedforward: the output of the units in layer i is only fed to units in layer j &gt; i.
Reference: <author> Kuhn, R. and De Mori, R. </author> <year> (1995). </year> <title> The application of semantic classification trees to natural language understanding. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 17 </volume> <pages> 449-460. </pages>
Reference-contexts: Learning in tdidt is eager since decision trees are constructed during learning; classification effort is low since it involves non-backtracking deterministic traversal through the induced tree. Two decision tree algorithms are evaluated here: igtree (information-gain tree, (Daelemans et al., 1997)) and sct (semantic classification trees, <ref> (Kuhn and De Mori, 1995) </ref>). igtree (Daelemans et al., 1997) was designed as an optimised approximation of ib1-ig. In igtree, information gain is used as a guiding function to compress the instance base into a decision tree. Nodes are connected via arcs denoting feature values. <p> When a leaf is reached, the instance is assigned the class stored at the leaf; otherwise, it is assigned the default classification associated with the last matching non-terminal node. Semantic Classification Trees (SCT) were introduced by <ref> (Kuhn and De Mori, 1995) </ref> for Natural Language Understanding and have been applied successfully to the classification of dialogue acts by keyword spotting (Mast et al., 1995). In SCTs, the class of an instance is determined by matching it against a set of regular expressions.
Reference: <author> Lavrac, N. and Dzeroski, S. </author> <year> (1994). </year> <title> Inductive Logic Programming. </title> <address> Chichester, UK: </address> <publisher> Ellis Horwood. </publisher>
Reference: <author> Mast, M., Niemann, H., Noth, E., and Schukat-Talamazzini, E. </author> <year> (1995). </year> <title> Automatic classification fo dialog acts with semantic classification trees and polygrams. </title> <booktitle> In IJCAI, Workshop on "New Approaches to Learning for Natural Language Processing", </booktitle> <pages> pages 71-78, </pages> <address> Montreal. </address>
Reference-contexts: Semantic Classification Trees (SCT) were introduced by (Kuhn and De Mori, 1995) for Natural Language Understanding and have been applied successfully to the classification of dialogue acts by keyword spotting <ref> (Mast et al., 1995) </ref>. In SCTs, the class of an instance is determined by matching it against a set of regular expressions. At each node, only one regular expression is tested. There are two branches, one for "match" and one for "no match".
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: The performance of most algorithms still falls far below the mark of 80-90% correct words which is needed in high-quality text-to-speech synthesis (Yvon, 1996). However, Bakiri and Dietterich (1993) have shown that their approach based on ID-3 <ref> (Quinlan, 1986) </ref> decision trees outperforms the sophisticated DECTalk rule set for English (Allen et al., 1987); (Van den Bosch and Daelemans, 1993; Daelemans and Van den Bosch, 1997) report similar results for Dutch. In both cases, the training corpora contained around 18000, and the test corpora around 2000 words.
Reference: <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Eq. 1). The weighting function of ib1-ig, W 0 (f i ), represents the information gain <ref> (Quinlan, 1993) </ref> of feature f i . <p> When storing feature-value information, arcs representing the values of the feature with the highest information gain are created first, then arcs for the values of the feature with the second-highest 3 see e.g. <ref> (Quinlan, 1993) </ref> for an overview information gain, etc., until the classification information represented by a path is unambiguous. Short paths in the tree represent instances with relatively regular classifications, whereas long paths represent instances with irregular, exceptional, or noisy classifications.
Reference: <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. and McClelland, J., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The units are usually grouped into an input layer, an output layer, and one or more hidden layers. The best results on APT so far have been achieved using a simple feed-forward topology 2 and Backpropagation with Momentum <ref> (Rumelhart et al., 1986) </ref>. The ann approach tested here was proposed in (Wolters, 1996). First, a feed-forward ann is trained using Backpropagation with Momentum until the error on a validation set starts to rise (early stopping).
Reference: <author> Sejnowski, T. and Rosenberg, C. </author> <year> (1987). </year> <title> A parallel network that learns to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168. </pages>
Reference-contexts: Because of the limited window length, it is difficult to capture morphophonological alternations like English Tri-syllabic Shortening as in divine divinity, and stress shifts as in photograph - photography. Three types of phoneme-based approaches have yielded good results for large corpora: neural networks <ref> (Sejnowski and Rosenberg, 1987) </ref>, decision trees (Dietterich et al., 1995), and instance-based learning (Van den Bosch and Daelemans, 1993). 2.1 Neural networks Artificial neural networks (ann) consist of simple processing units with weighted connections.
Reference: <author> Van den Bosch, A. and Daelemans, W. </author> <year> (1993). </year> <title> Data-oriented methods for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of the 6th Conference of the EACL, </booktitle> <pages> pages 45-53. </pages>
Reference-contexts: Three types of phoneme-based approaches have yielded good results for large corpora: neural networks (Sejnowski and Rosenberg, 1987), decision trees (Dietterich et al., 1995), and instance-based learning <ref> (Van den Bosch and Daelemans, 1993) </ref>. 2.1 Neural networks Artificial neural networks (ann) consist of simple processing units with weighted connections. The units are usually grouped into an input layer, an output layer, and one or more hidden layers.
Reference: <author> Weiss, S. and Kulikowski, C. </author> <year> (1991). </year> <title> Computer Systems That Learn. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: On average, 3,78% of all training instances (1.57% of all types) are ambiguous, but less than 1% of all test instances. Vowel graphemes are especially susceptible to errors, since they are also used to encode consonant quality 5 . 3.2 Method All algorithms were trained using 10-fold cross validation <ref> (Weiss and Kulikowski, 1991) </ref> to allow for significance tests on performance differences. The ann consists of 1 input layer of 7 fi 5 units, 2 hidden layers of 100 units each, and 1 output layer of 22 units. The size of the hidden layers was motivated by two main considerations.
Reference: <author> Wolters, M. </author> <year> (1996). </year> <title> A dual-route neural-network based approach to grapheme-to-phoneme conversion. </title> <editor> In v. Seelen, W., v.d. Malsburg, C., Sendhoff, B., and J., editors, </editor> <booktitle> Proc. Intl. Conf. on Artificial Neural Networks 1996, Lecture Notes in Computer Science. </booktitle> <publisher> Springer, </publisher> <address> Berlin, Heidelberg, New York. </address>
Reference-contexts: The best results on APT so far have been achieved using a simple feed-forward topology 2 and Backpropagation with Momentum (Rumelhart et al., 1986). The ann approach tested here was proposed in <ref> (Wolters, 1996) </ref>. First, a feed-forward ann is trained using Backpropagation with Momentum until the error on a validation set starts to rise (early stopping). This way, we avoid overfitting of the training data, which results in bad generalisation performance for neural networks.
Reference: <author> Wolters, M. </author> <year> (1997). </year> <title> A Diphone-Based Text-to-Speech System for Scottish Gaelic. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, University of Bonn. </institution>
Reference-contexts: However, the algorithm does not need any windowing; it can access the complete word quite efficiently by adequate regular expressions. 3 Comparison of Algorithm Performance 3.1 The Data Set The algorithms were tested on a dictionary of 1003 phonetically transcribed Scottish Gaelic words <ref> (Wolters, 1997) </ref>. The transcriptions reflect the Gaelic of Point, Isle of Lewis, Outer Hebrides. Scottish Gaelic is a minority language with about 80,000 speakers. Its orthography is rather complex. It was codified in the 18th century, and the dialect on which it is based has nearly died out today. <p> Why this superiority of the nearest neighbour classifier 5 For example, in cait ("the cats"), i only serves as a cue to the palatality of /t/. 6 see also the experiments reported in <ref> (Wolters, 1997) </ref> on the Gaelic corpus with different input and output representations. classified phones classified phones ib1-ig? Three aspects of learning in ib1-ig are advantageous in lgeneralising from sparse data: * storing all training examples.
Reference: <author> Yvon, F. </author> <year> (1996). </year> <title> Prononciation par analogie. </title> <type> PhD thesis, </type> <institution> Ecole Nationale Superieure des Telecommunications, Paris. </institution>
Reference-contexts: Many researchers have tried to replace rule sets using machine learning (ML) algorithms trained on the lexicon, but with mixed success. The performance of most algorithms still falls far below the mark of 80-90% correct words which is needed in high-quality text-to-speech synthesis <ref> (Yvon, 1996) </ref>. However, Bakiri and Dietterich (1993) have shown that their approach based on ID-3 (Quinlan, 1986) decision trees outperforms the sophisticated DECTalk rule set for English (Allen et al., 1987); (Van den Bosch and Daelemans, 1993; Daelemans and Van den Bosch, 1997) report similar results for Dutch. <p> Although chunk-based approaches are psycholinguistically plausible (cf. Glushko, 1979), they are not suitable for minority-language APT. Algorithms in the tradition of PRONOUNCE (Dedina and Nusbaum, 1991) rely on extensive statistics about letter/phone correspondences which cannot be estimated adequately from tiny corpora. JUPA <ref> (Yvon, 1996) </ref>, which recombines dictionary entries, does not produce any output for 30-40% of the test words when trained on 2000 words only, and similar problems should occur with the more sophisticated algorithms Yvon describes. Therefore, we have to rely on phoneme-based approaches.
References-found: 26

