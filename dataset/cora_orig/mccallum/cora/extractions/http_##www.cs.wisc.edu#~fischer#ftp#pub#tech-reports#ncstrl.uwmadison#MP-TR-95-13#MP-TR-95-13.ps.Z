URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-95-13/MP-TR-95-13.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-95-13/
Root-URL: http://www.cs.wisc.edu
Title: NONMONOTONE AND PERTURBED OPTIMIZATION  
Author: By Mikhail V. Solodov 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1995  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> B.D.O. Anderson and J.B. Moore. </author> <title> Optimal Filtering. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood, New Jersey, </address> <year> 1979. </year>
Reference-contexts: They are known to be particularly well suited for large data sets (K is large). Another attractive feature of online approach is that it is incremental and can be used in real time operation. These properties are of particular importance for optimal control <ref> [1] </ref> and artificial intelligence [52] applications. A more detailed comparison of the online and batch approaches to training neural networks is given below (see page 7).
Reference: [2] <author> D.P. Bertsekas. </author> <title> Incremental least squares methods and the extended Kalman filter. </title> <journal> SIAM Journal on Optimization, </journal> <month> May </month> <year> 1994. </year> <note> Submitted. </note>
Reference-contexts: Our result is much in the spirit of [33], except for the key difference of nonmonotonicity. This generalization allows the proposed results to apply to a wider class of algorithms including backpropagation. Theorem 1.2.1 below proved to be very useful and has since been used in <ref> [2, 26] </ref> for convergence analysis of other incremental algorithms. We first define a forcing function.
Reference: [3] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and Distributed Computation. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: The distinctive novel feature of this algorithm is the presence of the "forget-me-not" term x i l + D i l l in the parallel subproblems (4.1.2). The presence of this term allows for a change in "secondary" variables. This makes PVD fundamentally different from the block Jacobi <ref> [3] </ref>, coordinate descent [66] and parallel gradient distribution algorithms [33]. The directions 83 l are typically easily computable steepest descent or quasi-Newton directions in the space of the corresponding variables. The "forget-me-not" approach improves robustness and accelerates convergence of the algorithm and is the key to its success.
Reference: [4] <author> P.T. Boggs and J.E. Dennis. </author> <title> A stability analysis for perturbed nonlinear iterative methods. </title> <journal> Mathematics of Computation, </journal> <volume> 30 </volume> <pages> 199-215, </pages> <year> 1976. </year>
Reference-contexts: This makes traditional convergence analysis techniques [51, 50] inapplicable. In this chapter, we develop a new approach to the analysis of algorithms with nonvanishing perturbations. Our analysis extends some of the ideas presented in <ref> [4, 42, 74] </ref> for methods of unconstrained optimization. Essential perturbations were considered in [62] in a different context of incremental gradient-type methods with decaying stepsize. 65 A special case of an approximate gradient projection method with decaying stepsize is also studied in [32].
Reference: [5] <author> J.V. Burke and M.C. Ferris. </author> <title> Weak sharp minima in mathematical programming. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 31(5) </volume> <pages> 1340-1359, </pages> <year> 1993. </year>
Reference-contexts: These include problems with relatively small perturbations, convex and strongly convex problems, and problems with weak sharp minima <ref> [51, 5] </ref>. We start with the following lemma which deals with the case of perturbations small relative to the residual function r () defined in (2.1.2). Lemma 2.4.1 Let "(x) maxf"; r (x)g 8x 2 X; where " 0; 1 &gt; 0. <p> This establishes the first two assertions of the lemma. For the last assertion, just note that ([51], p.24) for any x 2 X 2 (f (x) min f (y)) k@f (x)k 2 : Definition 2.4.1 <ref> [5] </ref> We say that X opt is a set of weak sharp minima with parameter &gt; 0 if f (x) min f (y) d (x; X opt ) 8x 2 X: The following important corollary shows that, for problems with weak sharp minima, certain "-stationary sets coincide with the set of <p> We also derive some new convergence results for weakly sharp problems of order 2 (see Definition 4.3.1). This class of problems can be viewed as a generalization of strongly convex problems and a certain unconstrained smooth analogue of weak sharp minima <ref> [5] </ref>. We begin by imposing a natural sufficient descent condition on an algorithm (Algorithm A below) used to solve the subproblems (4.1.2) generated by the PVD Algorithm 4.1.1. Algorithm A. <p> The class of problems with weak sharp minima of order 2 can be thought of as a certain unconstrained smooth analogue of weak sharp minima (of order 1) <ref> [38, 51, 5] </ref>. Note that it subsumes strongly convex programs. Let f () be strongly convex with modulus 2. Then its unique optimal point x is globally (with * = 1) weakly sharp of order 2. This can be easily verified as follows.
Reference: [6] <author> F.H. Clarke. </author> <title> Optimization and Nonsmooth Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1983. </year> <month> 108 </month>
Reference-contexts: For the objective function, we assume that there exists t 2 (0; +1] such that f : (X + t B) ! &lt; is at least Lipschitz continuous on X + t B and regular (in the sense of Clarke, <ref> [6] </ref>). <p> sets of problem (2.1.1) respectively, that is X opt := fx 2 X j f (x) = min f (y)g X s := fx 2 X j 0 2 @f (x) + N X (x)g; where @f (x) is the set of all generalized gradients (in the sense of Clarke, <ref> [6] </ref>) of f () at x, and N X (x) &lt; n is the normal cone to the set X at the point x 2 X (see [54]) : The following notions will play an important role in our analysis.
Reference: [7] <editor> R.W. Cottle, F. Giannessi, and J.-L. Lions (editors). </editor> <title> Variational Inequalities and Complementarity Problems : Theory and Applications. </title> <address> Whiley, New York, </address> <year> 1980. </year>
Reference-contexts: Remark 4.3.2 A practically important example of weak sharp minima of order 2 is provided by the implicit Lagrangian reformulation [40] of the nonlinear complementarity problem. Consider the following nonlinear complementarity problem <ref> [8, 7] </ref> (NCP) of finding an x 2 &lt; n such that F (x) 0; x 0; hx; F (x)i = 0; where F : &lt; n ! &lt; n is a continuously differentiable mapping.
Reference: [8] <author> G.B. Dantzig and R.W. Cottle. </author> <title> Positive (semi-)definite programming. </title> <editor> In J. Abadie, editor, </editor> <booktitle> Nonlinear Programming, </booktitle> <pages> pages 55-73, </pages> <address> Amsterdam, 1967. </address> <publisher> North-Holland. </publisher>
Reference-contexts: Remark 4.3.2 A practically important example of weak sharp minima of order 2 is provided by the implicit Lagrangian reformulation [40] of the nonlinear complementarity problem. Consider the following nonlinear complementarity problem <ref> [8, 7] </ref> (NCP) of finding an x 2 &lt; n such that F (x) 0; x 0; hx; F (x)i = 0; where F : &lt; n ! &lt; n is a continuously differentiable mapping.
Reference: [9] <author> C. Darken, J.Chang, and J.Moody. </author> <title> Learning rates schedules for faster stochastic gradient search. </title> <booktitle> In Neural Networks for signal processing 2, </booktitle> <address> New York, 1992. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: In the first phase of learning, called the "search phase", the learning rate is almost constant, or it decreases slowly. In the second phase of learning, called the "converge phase", it decreases to zero. In particular, two possible rules for the learning rate have been suggested [10], <ref> [9] </ref> : 1 i 0 28 i = 0 0 i 0 0 i 0 + i 0 ( i where 0 &gt; 0, c &gt; 0, i 0 &gt;> 1 are appropriately chosen parameters.
Reference: [10] <author> C. Darken and J.Moody. </author> <title> Towards faster stochastic gradient search. </title> <editor> In G. Tesauro J.D. Cowan and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 1009-1016, </pages> <address> San Francisco, CA, 1991. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Note however, that they both tend to x in the limit as ! 0. Remark 1.3.2 We note that the learning rates rules that satisfy (1.3.4) were used in practice <ref> [10] </ref>, and are known as search-then-converge strategy. In the first phase of learning, called the "search phase", the learning rate is almost constant, or it decreases slowly. In the second phase of learning, called the "converge phase", it decreases to zero. <p> In the first phase of learning, called the "search phase", the learning rate is almost constant, or it decreases slowly. In the second phase of learning, called the "converge phase", it decreases to zero. In particular, two possible rules for the learning rate have been suggested <ref> [10] </ref>, [9] : 1 i 0 28 i = 0 0 i 0 0 i 0 + i 0 ( i where 0 &gt; 0, c &gt; 0, i 0 &gt;> 1 are appropriately chosen parameters.
Reference: [11] <author> P. A. Dorofeev. </author> <title> On some properties of quasi-gradient method. </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 25 </volume> <pages> 181-189, </pages> <year> 1985. </year>
Reference-contexts: Remark 2.4.1 Theorem 2.4.1 extends and strengthens the results on convergence properties of the generalized gradient projection method given in <ref> [46, 11, 75] </ref>. 2.5 Applications to Neural Network Training In this section we briefly describe how results of Section 2.3 can be applied to reveal some important properties of various neural network learning techniques.
Reference: [12] <author> M.C. Ferris and O.L. Mangasarian. </author> <title> Parallel variable distribution. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4(4) </volume> <pages> 815-832, </pages> <year> 1994. </year>
Reference-contexts: Applications of the ideas presented here to other classes of optimization algorithms (for example, [61, 40]) is an interesting subject of future research. 81 Chapter 4 Partially Asynchronous Inexact Parallel Variable Distribution Algorithms We consider the recently proposed parallel variable distribution (PVD) algorithm <ref> [12] </ref> for solving optimization problems in which the variables are distributed among p processors. Each processor has the primary responsibility for updating its block of variables while allowing the remaining "secondary" variables to change in a restricted fashion along some easily computable directions. <p> We also show that nonmonotone synchronization schemes are admissible, which further improves flexibility of PVD approach. 4.1 Introduction We consider the general unconstrained optimization problem min where f () 2 C 1 L (&lt; n ). We first state the original PVD algorithm <ref> [12] </ref>. Let x 2 &lt; n be partitioned into p blocks x 1 ; : : : ; x p , such that x l 2 &lt; n l , P p n. These blocks of variables are then distributed among p parallel processors. <p> d i 2 &lt; n along its block diagonal as follows : D i 0 B B B B B B B B B B B @ 1 2 d i d i . . . p C C C C C C C C C C C C In <ref> [12] </ref>, the proposed synchronization step consists of minimizing the objec tive function in the affine hull of all the points computed in parallel by the p processors. In [12] it was shown that every accumulation point of the PVD iterates is a stationary point of f () if an exact global <p> 2 d i d i . . . p C C C C C C C C C C C C In <ref> [12] </ref>, the proposed synchronization step consists of minimizing the objec tive function in the affine hull of all the points computed in parallel by the p processors. In [12] it was shown that every accumulation point of the PVD iterates is a stationary point of f () if an exact global solution to subproblems (4.1.2) is computed at every iteration. <p> We note that the original requirement of exact subproblem solution is also undesirable. In Section 4.2 we describe an algorithm with inexact subproblem solution in the 85 convex case and derive a sharper linear convergence result than the one given in <ref> [12] </ref>. We emphasize that the partially asynchronous and inexact subproblem solution approaches provide a flexible framework that allows for effective load balancing among the parallel processors. <p> By making an explicit use of the "forget-me-not" terms in the subproblems, we also improve on the linear convergence result given in <ref> [12] </ref>. In [12] it is established that, for the strongly convex case, the following estimate is valid kx i xk c 1 1 p 2 where x is the (unique) solution of the problem, p is the number of parallel processors, and c 1 ; c 2 are positive constants. <p> By making an explicit use of the "forget-me-not" terms in the subproblems, we also improve on the linear convergence result given in <ref> [12] </ref>. In [12] it is established that, for the strongly convex case, the following estimate is valid kx i xk c 1 1 p 2 where x is the (unique) solution of the problem, p is the number of parallel processors, and c 1 ; c 2 are positive constants. <p> This result is not quite satisfactory because the presense of p in the denominator suggests that the convergence speed goes down as the number of processors used increases. We point out that the proof given in <ref> [12] </ref> fails to make use of the "forget-me-not" terms which are the key to the algorithm. By refining the proof, we obtain a better convergence speed estimate kx i xk c 1 (1 c 3 ) 2 ; where c 3 &gt; 0 does not depend on p. <p> In particular, we show that there is no need to find an exact global solution for the subproblems. Any point that satisfies a natural sufficient descent condition can be accepted for the next iteration. We note, in the passing, that the proof given in <ref> [12] </ref> makes use of exact global solutions in an essential way and breaks down if, for example, only stationary points in the 96 subproblems are available.
Reference: [13] <author> E.M. Gafni and D.P. Bertsekas. </author> <title> Two-metric projection methods for constrained optimization. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 22 </volume> <pages> 936-964, </pages> <year> 1984. </year> <month> 109 </month>
Reference: [14] <author> D. Girard and H. Paugam-Moisy. </author> <title> Strategies for weight updating for parallel backpropagation. </title> <type> Technical Report 93-39, </type> <institution> Laboratoire de l'Informatique du Parallelisme, Ecole Normale Superieure de Lyon, </institution> <address> 46, Allee d'Italie, 69364 Lyon Cedex 07, France, </address> <year> 1993. </year>
Reference-contexts: For empirical study of this kind of parallelization see <ref> [49, 14] </ref>. Note that if examples are assigned to more than one processor, a different (weighted) error function may be generated, which nevertheless measures the error of the same training problem. <p> Then a synchronization step is performed that consists of averaging the iterates computed by all the p processors. Empirical evaluation of parallel BP and numerical tests can be found in <ref> [49, 14] </ref>. Below we state a parallel BP algorithm with an added momentum term which consists of the difference between the current and previous iterates. For simplicity and in a similar manner to the method of conjugate gradients [51] we reset this term to zero periodically (see Algorithm 1.3.1). <p> The type of paral-lelization considered here is primarily motivated by incremental gradient methods, particularly neural network training (see Chapter 1). Empirical evaluation of parallel BP and numerical tests can be found in <ref> [49, 14] </ref>. Another related work on parallel computing is [68]. We first consider the most general case. Our results can be then specialized by removing parallelism and/or considering the standard (nonadditive) objective function. <p> Thus BP is a special case of Algorithm 2.3.1. Many other computationally important BP modifications, such as parallel BP <ref> [42, 49, 14] </ref>, BP with momentum term [24], and BP with varying smoothing parameter [65] all fall within the framework of Section 2.3.
Reference: [15] <author> A.A. Goldstein. </author> <title> Convex programming in Hilbert space. </title> <journal> Bull. Am. Math. Soc., </journal> <volume> 70 </volume> <pages> 709-710, </pages> <year> 1964. </year>
Reference-contexts: This is a rather general framework that includes a gradient projection algorithm <ref> [15, 27] </ref>; proximal minimization algorithm [44, 55]; and the extragradient algorithm [25, 43] among others. We note, in the passing, that for characteristic mappings e () of feasible descent methods, e (x i ) ! 0 as i ! 1 by algorithm construction [31]. <p> g and a stationary point ^x 2 X s such that kx ^xk *; where is as specified in (3.1.7). 3.3 Applications In this Section, we briefly discuss applications of our analysis to a number of well known algorithms. 3.3.1 Gradient Projection Algorithm We first consider the gradient projection algorithm <ref> [15, 27] </ref>.
Reference: [16] <author> L. Grippo. </author> <title> A class of unconstrained minimization methods for neural network training. </title> <journal> Optimization Methods and Software, </journal> <volume> 4 </volume> <pages> 135-150, </pages> <year> 1994. </year>
Reference-contexts: There are a number of ways to ensure that the sequence of iterates produced by BP be bounded, such as the following. In <ref> [16] </ref> a regularization term consisting of the squared 2-norm of x is added to the error function so that the modified objective function has bounded level sets : min K X f j (x) + ckxk 2 ; where c &gt; 0 is a (small) "penalty" parameter.
Reference: [17] <author> L. Grippo, F. Lampariello, and S. Lucidi. </author> <title> A nonmonotone line search technique for Newton's method. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 23 </volume> <pages> 707-716, </pages> <year> 1986. </year>
Reference-contexts: Note that our synchronization step may increase rather than decrease the objective function when compared to the values obtained by the parallel processors. This provides the algorithm with more flexibility and is known to be useful in nonlinear nonconvex optimization <ref> [17, 18] </ref>. <p> Also, as is evidenced by (4.3.3) and Theorem 4.3.1, we are not restricted to the "monotone" synchronization step proposed in the original PVD approach. It is known that always insisting on monotone decrease in the objective function at every iteration is not necessarily the best strategy <ref> [17] </ref>. We note that synchronization in PVD algorithms can be combined with nonmonotone stabilization schemes [18], provided the requirements of (4.3.3) are satisfied. Remark 4.3.2 A practically important example of weak sharp minima of order 2 is provided by the implicit Lagrangian reformulation [40] of the nonlinear complementarity problem.
Reference: [18] <author> L. Grippo, F. Lampariello, and S. Lucidi. </author> <title> A class of nonmonotone stabilization methods in unconstrained optimization. </title> <journal> Numerische Mathematik, </journal> <volume> 59 </volume> <pages> 779-805, </pages> <year> 1991. </year>
Reference-contexts: Synchronization can be performed at any time provided every processor has achieved the sufficient descent condition. Furthermore, we show that synchronization step need not be monotone and can be combined with nonmonotone stabilization schemes similar to <ref> [18] </ref>. We also derive some new convergence results for weakly sharp problems of order 2 (see Definition 4.3.1). This class of problems can be viewed as a generalization of strongly convex problems and a certain unconstrained smooth analogue of weak sharp minima [5]. <p> Note that our synchronization step may increase rather than decrease the objective function when compared to the values obtained by the parallel processors. This provides the algorithm with more flexibility and is known to be useful in nonlinear nonconvex optimization <ref> [17, 18] </ref>. <p> It is known that always insisting on monotone decrease in the objective function at every iteration is not necessarily the best strategy [17]. We note that synchronization in PVD algorithms can be combined with nonmonotone stabilization schemes <ref> [18] </ref>, provided the requirements of (4.3.3) are satisfied. Remark 4.3.2 A practically important example of weak sharp minima of order 2 is provided by the implicit Lagrangian reformulation [40] of the nonlinear complementarity problem.
Reference: [19] <author> B. Hassibi and D.G. Stork. </author> <title> Optimal brain sergeon. </title> <editor> In G. Tesauro J.D. Cowan and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Francisco, CA, 1993. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: In particular, we give a precise characterization to empirically observed stability of neural networks and backpropagation training <ref> [58, 19] </ref>. We also show that the properties of the weight perturbation [23] algorithm can be derived by making use of the presented analysis. 2.5.1 Backpropagation With Noise We note that when implemented in hardware, BP algorithm is likely to have some kind of electronic imperfections [23]. <p> The last property is usually called the generalization ability and it is one of the major strengths of artificial neural networks. As another source of perturbations in the neural network training, we note the technique presented in <ref> [19] </ref>. To simplify the network topology and improve the network generalization properties, it is proposed in [19] to eliminate at the late stages of training the arcs with sufficiently small weights. The latter is equivalent to forcing the corresponding weights to zero, and can also be treated as induced perturbations. <p> As another source of perturbations in the neural network training, we note the technique presented in <ref> [19] </ref>. To simplify the network topology and improve the network generalization properties, it is proposed in [19] to eliminate at the late stages of training the arcs with sufficiently small weights. The latter is equivalent to forcing the corresponding weights to zero, and can also be treated as induced perturbations. We finally mention the node perturbation approach first proposed in [71].
Reference: [20] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year> <month> 110 </month>
Reference-contexts: This difficulty makes convergence analysis of BP a challenging problem that has currently attracted the interest of many researchers [42, 32, 70]. To further justify our interest in the online (incremental) methods we make the following observations <ref> [20] </ref> : 8 (i) For many learning systems adaptation to on-line stream of training samples is required.
Reference: [21] <author> G. E. Hinton. </author> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 1-12, </pages> <address> Hillsdale, 1986. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: This, in fact, corresponds to the weight decay training <ref> [21, 69] </ref>. Weight decay is a useful approach since it tends to generate simpler networks by minimizing nonzero arc connections. Simpler networks often possess better generalization properties.
Reference: [22] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: The fact that this problem is solvable if we employ a sufficient number of hidden units (h is sufficiently large), essentially follows from the Kolmogorov Approximation Theorem (see <ref> [22] </ref>). Note however, that choosing too large an h may lead to overtraining and memorizing the training set without the ability to generalize to unseen data [24]. The choice of h is in general a rather difficult task in itself, and it is beyond the scope of this work.
Reference: [23] <author> M. Jabri and B. Flower. </author> <title> Weight perturbation: An optimal architecture and learning technique for analog VLSI feedforward and recurrent multilayer networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(1) </volume> <pages> 154-157, </pages> <year> 1992. </year>
Reference-contexts: In particular, we give a precise characterization to empirically observed stability of neural networks and backpropagation training [58, 19]. We also show that the properties of the weight perturbation <ref> [23] </ref> algorithm can be derived by making use of the presented analysis. 2.5.1 Backpropagation With Noise We note that when implemented in hardware, BP algorithm is likely to have some kind of electronic imperfections [23]. <p> We also show that the properties of the weight perturbation <ref> [23] </ref> algorithm can be derived by making use of the presented analysis. 2.5.1 Backpropagation With Noise We note that when implemented in hardware, BP algorithm is likely to have some kind of electronic imperfections [23]. Faults in multiplier circuits introduce errors when function and gradient values are propagated through the network. Therefore, in practical electronic implementations, even when the input data can be considered to be free of noise, the algorithm is influenced by certain perturbations induced by hardware limitations. <p> One of the algorithms that is less sensitive to the hardware limitations due to the chip and network interface, is the so-called weight perturbation (WP) training <ref> [23] </ref>. This technique is essentially an incremental gradient-type method where the gradient is approximated by one of the finite difference techniques. <p> Clearly, WP needs more pattern presentations than BP. However, in <ref> [23] </ref> it is reported that sometimes implementation of BP requires excessive computational hardware, and WP is more economical for parallel analog implementations. Thus speed is traded for more reliable and practical hardware. We point out that there has been no rigorous analysis of WP training in the literature. <p> In that case, kffi (x i )k = O k=1;::: ;n k ) 2 : The central difference scheme yields further increase in the number of training samples presentations (the number of the partial error functions evaluations). However, for some problems this is still practical <ref> [23] </ref>. 61 Chapter 3 Convergence Analysis of Perturbed Feasible Descent Methods We develop a general approach to convergence analysis of feasible descent methods in the presence of perturbations [63]. The important novel feature of our analysis is that perturbations need not tend to zero in the limit.
Reference: [24] <author> T. Khanna. </author> <title> Foundations of neural networks. </title> <publisher> Addison-Wesley, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference-contexts: Note however, that choosing too large an h may lead to overtraining and memorizing the training set without the ability to generalize to unseen data <ref> [24] </ref>. The choice of h is in general a rather difficult task in itself, and it is beyond the scope of this work. From now on, we assume that h is chosen and fixed for each particular problem. <p> However, this complicates the proof somewhat. 1.3 Convergence Of The Backpropagation Al gorithm We now turn our attention to the classical BP algorithm for training feedforward artificial neural networks with one layer of hidden units <ref> [57, 24] </ref>. The number of hidden units is assumed to be fixed. Suppose we have K training examples and p processors with K 1 and p 1 (typically, K is much bigger than p). <p> For simplicity and in a similar manner to the method of conjugate gradients [51] we reset this term to zero periodically (see Algorithm 1.3.1). It has been observed that introduction of momentum term usually leads to faster convergence and adds stability to problems with noisy data <ref> [24] </ref>. <p> Adding the "heavy ball" term [51] in Algorithm 2.3.1, we arrive at the follow ing modification of the parallel GGPM. In neural network literature, methods of this type are usually referred to as backpropagation with momentum term <ref> [24, 41] </ref>. Algorithm 2.3.2 (Parallel GGPM with heavy ball term). Start with any x 0 2 X. <p> Thus BP is a special case of Algorithm 2.3.1. Many other computationally important BP modifications, such as parallel BP [42, 49, 14], BP with momentum term <ref> [24] </ref>, and BP with varying smoothing parameter [65] all fall within the framework of Section 2.3.
Reference: [25] <author> G.M. Korpelevich. </author> <title> The extragradient method for finding saddle points and other problems. </title> <journal> Matecon, </journal> <volume> 12 </volume> <pages> 747-756, </pages> <year> 1976. </year>
Reference-contexts: This is a rather general framework that includes a gradient projection algorithm [15, 27]; proximal minimization algorithm [44, 55]; and the extragradient algorithm <ref> [25, 43] </ref> among others. We note, in the passing, that for characteristic mappings e () of feasible descent methods, e (x i ) ! 0 as i ! 1 by algorithm construction [31]. <p> It can be checked that d 1 = O (L 2 ); where d 1 is the constant involved in Theorem 3.2.1. 79 3.3.3 Extragradient Method Consider now the extragradient method <ref> [25, 43] </ref> which updates a current iterate according to the double-projection formula x i+1 = [x i i rf [x i i rf (x i )] + ] + : This iteration can be re-written as x i+1 = [x i i rf (x i ) + e (x i )]
Reference: [26] <author> R. De Leone, E. Merelli, and R. Capparuccia. </author> <title> A modified backpropagation algorithm for neural network training, </title> <year> 1995. </year>
Reference-contexts: Our result is much in the spirit of [33], except for the key difference of nonmonotonicity. This generalization allows the proposed results to apply to a wider class of algorithms including backpropagation. Theorem 1.2.1 below proved to be very useful and has since been used in <ref> [2, 26] </ref> for convergence analysis of other incremental algorithms. We first define a forcing function.
Reference: [27] <author> E.S. Levitin and B. T. Polyak. </author> <title> Constrained minimization methods. </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 6 </volume> <pages> 1-50, </pages> <year> 1965. </year>
Reference-contexts: This is a rather general framework that includes a gradient projection algorithm <ref> [15, 27] </ref>; proximal minimization algorithm [44, 55]; and the extragradient algorithm [25, 43] among others. We note, in the passing, that for characteristic mappings e () of feasible descent methods, e (x i ) ! 0 as i ! 1 by algorithm construction [31]. <p> g and a stationary point ^x 2 X s such that kx ^xk *; where is as specified in (3.1.7). 3.3 Applications In this Section, we briefly discuss applications of our analysis to a number of well known algorithms. 3.3.1 Gradient Projection Algorithm We first consider the gradient projection algorithm <ref> [15, 27] </ref>.
Reference: [28] <author> X.-D. Luo and P. Tseng. </author> <title> On global projection-type error bound for the linear complementarity problem. Linear Algebra and Its Applications. </title> <note> To appear. 111 </note>
Reference-contexts: Moreover, under additional assumptions, this condition holds with - = 1 (global error bound) <ref> [29, 48, 28] </ref>. Therefore, if x 2 X s (*) and the bound (3.1.7) holds with - *, it follows immediately that d (x; X s ) kr (x)k *: The rest of the chapter is organized as follows. <p> This error bound is known to hold when F () is affine (see [30, 53]) or F () has certain strong monotonicity structure (see [67, Theorem 2]). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see <ref> [28, 29, 39, 48] </ref>). Therefore our analysis shows that certain unconstrained optimization techniques applied to minimizing the implicit Lagrangian attain linear rate of convergence.
Reference: [29] <author> Z.-Q. Luo, O.L. Mangasarian, J. Ren, </author> <title> and M.V. Solodov. New error bounds for the linear complementarity problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 19 </volume> <pages> 880-892, </pages> <year> 1994. </year>
Reference-contexts: Moreover, under additional assumptions, this condition holds with - = 1 (global error bound) <ref> [29, 48, 28] </ref>. Therefore, if x 2 X s (*) and the bound (3.1.7) holds with - *, it follows immediately that d (x; X s ) kr (x)k *: The rest of the chapter is organized as follows. <p> In particular, the implicit Lagrangian is nonnegative everywhere in &lt; n and assumes the value of zero precisely at the solutions of the NCP. 105 In <ref> [29] </ref> it was established that 2 (ff 1)kr (x)k 2 M (x; ff) 2ff (ff 1)kr (x)k 2 ; 8x 2 &lt; n ; where r (x) := x [x F (x)] + . <p> This error bound is known to hold when F () is affine (see [30, 53]) or F () has certain strong monotonicity structure (see [67, Theorem 2]). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see <ref> [28, 29, 39, 48] </ref>). Therefore our analysis shows that certain unconstrained optimization techniques applied to minimizing the implicit Lagrangian attain linear rate of convergence.
Reference: [30] <author> Z.-Q. Luo and P. Tseng. </author> <title> Error bound and convergence analysis of matrix splitting algorithms for the affine variational inequality problem. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2 </volume> <pages> 43-54, </pages> <year> 1992. </year>
Reference-contexts: Some conditions of this type have been used in the analysis of matrix splitting methods <ref> [35, 30] </ref> : kffi (x i )k ckx i+1 x i k; c &gt; 0; c sufficiently small or i=0 Note that under either assumption, ffi (x i ) ! 0 as i ! 1. <p> This error bound is known to hold when F () is affine (see <ref> [30, 53] </ref>) or F () has certain strong monotonicity structure (see [67, Theorem 2]). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see [28, 29, 39, 48]).
Reference: [31] <author> Z.-Q. Luo and P. Tseng. </author> <title> Error bounds and convergence analysis of feasible descent methods : A general approach. </title> <journal> Annals of Operations Research, </journal> <volume> 46 </volume> <pages> 157-178, </pages> <year> 1993. </year>
Reference-contexts: Let [] + denote the orthogonal projection onto X. Following <ref> [31] </ref>, we consider a broad class of feasible descent methods that can be represented by the formula x new := [x rf (x) + e (x)] + ; (3.1.3) where is a positive scalar, and mapping e : &lt; n ! &lt; n is the defining feature of each particular algorithm <p> We note, in the passing, that for characteristic mappings e () of feasible descent methods, e (x i ) ! 0 as i ! 1 by algorithm construction <ref> [31] </ref>. <p> We note that very little is known about convergence properties of essentially perturbed algorithms. The primary contribution of this chapter is laying down theoretical framework for analysis of such algorithms. Convergence (and rate of convergence) of feasible descent methods have been studied extensively (see <ref> [31] </ref> and references therein). We point out that the previous work either deals with the case when no perturbations are present 64 (ffi (x i ) = 0), or assumes some conditions that explicitly or implicitly imply that perturbations vanish in the limit (ffi (x i ) ! 0). <p> We note that another important property of the residual function r (x) is that, under certain conditions, its norm provides a (local) upper bound on the distance to the set X s <ref> [31, 53] </ref>. Namely, there exist positive constants and - (depending on f () and X only) such that d (x; X s ) kr (x)k 8x with kr (x)k -; (3.1.7) 66 where d (; X s ) denotes the Euclidean distance to X s . <p> Condition (3.2.1) is standard for feasible descent methods and is a consequence of algorithm construction <ref> [31] </ref>. Bounds (3.2.3) imposed on the stepsize are also fairly standard. With respect to (3.2.2), we note the following. If the left-hand-side of (3.2.2) is nonnegative for all x then we set c 2 = 0, otherwise c 2 = c 1 (it follows that 0 c 2 &lt; 1).
Reference: [32] <author> Z.-Q. Luo and P. Tseng. </author> <title> Analysis of an approximate gradient projection method with applications to the backpropagation algorithm. </title> <journal> Optimization Methods and Software, </journal> <volume> 4 </volume> <pages> 85-101, </pages> <year> 1994. </year>
Reference-contexts: A single iteration of BP may, in fact, increase rather than decrease the objective function f (; ff) which we are trying to minimize. This difficulty makes convergence analysis of BP a challenging problem that has currently attracted the interest of many researchers <ref> [42, 32, 70] </ref>. To further justify our interest in the online (incremental) methods we make the following observations [20] : 8 (i) For many learning systems adaptation to on-line stream of training samples is required. <p> Simpler networks often possess better generalization properties. All of our results apply with merely redefining the objective function of the problem. 29 We could also consider the constrained problem min f (x) := j=1 where X is typically a box in &lt; n <ref> [32] </ref>. Then a simple projection onto X ensures that the iterates are bounded. Only some technical changes are needed to apply our analysis to the constrained version of BP [32] which is the same as Algorithm 1.3.1, except that the synchronization step concludes with a projection operation x i+1 = 1 <p> We could also consider the constrained problem min f (x) := j=1 where X is typically a box in &lt; n <ref> [32] </ref>. Then a simple projection onto X ensures that the iterates are bounded. Only some technical changes are needed to apply our analysis to the constrained version of BP [32] which is the same as Algorithm 1.3.1, except that the synchronization step concludes with a projection operation x i+1 = 1 p X z l ; where [] + denotes the orthogonal projection onto X. 1.4 Concluding Remarks A general theorem for the nonmonotone convergence of a family of unconstrained <p> Essential perturbations were considered in [62] in a different context of incremental gradient-type methods with decaying stepsize. 65 A special case of an approximate gradient projection method with decaying stepsize is also studied in <ref> [32] </ref>. We note that in this chapter, the stepsize is bounded away from zero. Therefore, the situation and the analysis required are completely different from [62, 32]. <p> We note that in this chapter, the stepsize is bounded away from zero. Therefore, the situation and the analysis required are completely different from <ref> [62, 32] </ref>. We now define the following residual function r (x) := x [x rf (x)] + : It is well known that some x 2 &lt; n satisfies the Minimum Principle optimality condition [34] for problem (3.1.1) if and only if r (x) = 0.
Reference: [33] <author> O.L. Mangasarian. </author> <title> Parallel gradient distribution in unconstrained optimization. </title> <journal> SIAM Journal on Control and Optimization. </journal> <note> To appear. </note>
Reference-contexts: Our result is much in the spirit of <ref> [33] </ref>, except for the key difference of nonmonotonicity. This generalization allows the proposed results to apply to a wider class of algorithms including backpropagation. Theorem 1.2.1 below proved to be very useful and has since been used in [2, 26] for convergence analysis of other incremental algorithms. <p> However, we have chosen to state Theorem 1.2.1 in a direction - stepsize form because it is easier to implement. See <ref> [33] </ref> for specific instances of directions d i and stepsize i choices without perturbation terms. We now show that Theorem 1.2.1 can be applied to the analysis of the perturbed gradient-type methods. <p> Remark 1.2.2 Under appropriate assumptions, other well known direction choices, such as conjugate and quasi-Newton directions [51] can also be perturbed simi larly as in Corollary 1.2.1. Remark 1.2.3 Similar to <ref> [33] </ref>, a parallel version of Theorem 1.2.1 can be established where portions of the gradient are distributed among the processors. <p> The presence of this term allows for a change in "secondary" variables. This makes PVD fundamentally different from the block Jacobi [3], coordinate descent [66] and parallel gradient distribution algorithms <ref> [33] </ref>. The directions 83 l are typically easily computable steepest descent or quasi-Newton directions in the space of the corresponding variables. The "forget-me-not" approach improves robustness and accelerates convergence of the algorithm and is the key to its success. <p> Note that the above condition is satisfied by a single iteration of any reasonable descent algorithm <ref> [33] </ref> applied to the problem of minimizing '() with t 0 as a starting point. Hence it is also satisfied for a minimum or a stationary point computed by some descent algorithm provided it uses t 0 as a starting point. 97 We now state our partially asynchronous PVD algorithm.
Reference: [34] <author> O.L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1969. </year>
Reference-contexts: Therefore, the situation and the analysis required are completely different from [62, 32]. We now define the following residual function r (x) := x [x rf (x)] + : It is well known that some x 2 &lt; n satisfies the Minimum Principle optimality condition <ref> [34] </ref> for problem (3.1.1) if and only if r (x) = 0. We shall call such x a stationary point of (3.1.1).
Reference: [35] <author> O.L. Mangasarian. </author> <title> Convergence of iterates of an inexact matrix splitting algorithm for the symmetric monotone linear complementarity problem. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1 </volume> <pages> 114-122, </pages> <year> 1991. </year> <month> 112 </month>
Reference-contexts: Some conditions of this type have been used in the analysis of matrix splitting methods <ref> [35, 30] </ref> : kffi (x i )k ckx i+1 x i k; c &gt; 0; c sufficiently small or i=0 Note that under either assumption, ffi (x i ) ! 0 as i ! 1.
Reference: [36] <author> O.L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: Thus, neural networks are parametrized by a set of weights and thresholds. The task of a training scheme for a neural network is to find a set of weights and thresholds that makes the network perform the desired mapping. This problem can be formalized as follows <ref> [36] </ref>. <p> We assume that the set A is bounded. Problems of the form (2.1.5) arise, for example, in least-norm minimization, neural networks applications, and approximation theory. Among some important practical applications that involve parameters in the objective function, we note the adaptive smoothing techniques [45], and the neural network training <ref> [57, 41, 36] </ref>. We assume that each function f j (; ff) is Lipschitz continuous with modulus L &gt; 0 and regular on an open neighborhood of X + t B for every ff 2 A. We also assume that the map @f j (; ) is upper semicontinuous.
Reference: [37] <author> O.L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5(4) </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: For problems of this 3 kind, incremental methods are known to be more cost effective and often less likely to get stuck at poor local minima or stationary points of which there are many <ref> [37] </ref>. Such methods do not wait to process the entire set of functions f j (); j = 1; : : : ; K before updating the current iterate. Every iteration of the incremental gradient method is a step in the direction of negative gradient of a partial objective function.
Reference: [38] <author> O.L. Mangasarian and R.R. Meyer. </author> <title> Nonlinear perturbation of linear programs. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 17(6) </volume> <pages> 745-752, </pages> <year> 1979. </year>
Reference-contexts: The class of problems with weak sharp minima of order 2 can be thought of as a certain unconstrained smooth analogue of weak sharp minima (of order 1) <ref> [38, 51, 5] </ref>. Note that it subsumes strongly convex programs. Let f () be strongly convex with modulus 2. Then its unique optimal point x is globally (with * = 1) weakly sharp of order 2. This can be easily verified as follows.
Reference: [39] <author> O.L. Mangasarian and J. Ren. </author> <title> New improved error bounds for the linear complementarity problem. </title> <journal> Mathematical Programming, </journal> <volume> 66 </volume> <pages> 241-255, </pages> <year> 1994. </year>
Reference-contexts: This error bound is known to hold when F () is affine (see [30, 53]) or F () has certain strong monotonicity structure (see [67, Theorem 2]). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see <ref> [28, 29, 39, 48] </ref>). Therefore our analysis shows that certain unconstrained optimization techniques applied to minimizing the implicit Lagrangian attain linear rate of convergence.
Reference: [40] <editor> O.L. Mangasarian and M.V. Solodov. </editor> <title> Nonlinear complementarity as unconstrained and constrained minimization. </title> <journal> Mathematical Programming, </journal> <volume> 62 </volume> <pages> 277-297, </pages> <year> 1993. </year>
Reference-contexts: It is shown that the perturbed gradient projection, proximal minimization and extragra-dient methods fall within the presented framework. Applications of the ideas presented here to other classes of optimization algorithms (for example, <ref> [61, 40] </ref>) is an interesting subject of future research. 81 Chapter 4 Partially Asynchronous Inexact Parallel Variable Distribution Algorithms We consider the recently proposed parallel variable distribution (PVD) algorithm [12] for solving optimization problems in which the variables are distributed among p processors. <p> We note that synchronization in PVD algorithms can be combined with nonmonotone stabilization schemes [18], provided the requirements of (4.3.3) are satisfied. Remark 4.3.2 A practically important example of weak sharp minima of order 2 is provided by the implicit Lagrangian reformulation <ref> [40] </ref> of the nonlinear complementarity problem. Consider the following nonlinear complementarity problem [8, 7] (NCP) of finding an x 2 &lt; n such that F (x) 0; x 0; hx; F (x)i = 0; where F : &lt; n ! &lt; n is a continuously differentiable mapping. In [40] it was <p> Lagrangian reformulation <ref> [40] </ref> of the nonlinear complementarity problem. Consider the following nonlinear complementarity problem [8, 7] (NCP) of finding an x 2 &lt; n such that F (x) 0; x 0; hx; F (x)i = 0; where F : &lt; n ! &lt; n is a continuously differentiable mapping. In [40] it was established that the NCP can be solved via (smooth) unconstrained minimization of the following implicit Lagrangian function : M (x; ff) := 2ffhx; F (x)i + k [x ffF (x)] + k 2 kxk 2 + k [F (x) ffx] + k 2 kF (x)k 2 ; where
Reference: [41] <editor> O.L. Mangasarian and M.V. Solodov. </editor> <title> Backpropagation convergence via deterministic nonmonotone perturbed minimization. </title> <editor> In G. Tesauro J.D. Cowan and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 383-390, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: We assume that the set A is bounded. Problems of the form (2.1.5) arise, for example, in least-norm minimization, neural networks applications, and approximation theory. Among some important practical applications that involve parameters in the objective function, we note the adaptive smoothing techniques [45], and the neural network training <ref> [57, 41, 36] </ref>. We assume that each function f j (; ff) is Lipschitz continuous with modulus L &gt; 0 and regular on an open neighborhood of X + t B for every ff 2 A. We also assume that the map @f j (; ) is upper semicontinuous. <p> Adding the "heavy ball" term [51] in Algorithm 2.3.1, we arrive at the follow ing modification of the parallel GGPM. In neural network literature, methods of this type are usually referred to as backpropagation with momentum term <ref> [24, 41] </ref>. Algorithm 2.3.2 (Parallel GGPM with heavy ball term). Start with any x 0 2 X.
Reference: [42] <editor> O.L. Mangasarian and M.V. Solodov. </editor> <title> Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. </title> <journal> Optimization Methods and Software, </journal> <volume> 4 </volume> <pages> 103-116, </pages> <year> 1994. </year> <month> 113 </month>
Reference-contexts: : : : : : : : : : : : : : : : : : : : : 105 Bibliography 106 1 Chapter 1 Convergence of Incremental Gradient-Type Methods A general convergence theorem is proposed for a family of serial and parallel nonmonotone unconstrained minimization methods with perturbations <ref> [42] </ref>. A principal application of the theorem is to establish convergence of incremental gradient-type methods. Of special interest is online backpropagation (BP), the classical algorithm for training artificial neural networks. <p> A single iteration of BP may, in fact, increase rather than decrease the objective function f (; ff) which we are trying to minimize. This difficulty makes convergence analysis of BP a challenging problem that has currently attracted the interest of many researchers <ref> [42, 32, 70] </ref>. To further justify our interest in the online (incremental) methods we make the following observations [20] : 8 (i) For many learning systems adaptation to on-line stream of training samples is required. <p> Thus BP is a special case of Algorithm 2.3.1. Many other computationally important BP modifications, such as parallel BP <ref> [42, 49, 14] </ref>, BP with momentum term [24], and BP with varying smoothing parameter [65] all fall within the framework of Section 2.3. <p> This makes traditional convergence analysis techniques [51, 50] inapplicable. In this chapter, we develop a new approach to the analysis of algorithms with nonvanishing perturbations. Our analysis extends some of the ideas presented in <ref> [4, 42, 74] </ref> for methods of unconstrained optimization. Essential perturbations were considered in [62] in a different context of incremental gradient-type methods with decaying stepsize. 65 A special case of an approximate gradient projection method with decaying stepsize is also studied in [32].
Reference: [43] <author> P. Marcotte. </author> <title> Application of Khobotov's algorithm to variational inequalities and network equilibrium problems. </title> <journal> Information Systems and Operational Research, </journal> <volume> 29 </volume> <pages> 258-270, </pages> <year> 1991. </year>
Reference-contexts: This is a rather general framework that includes a gradient projection algorithm [15, 27]; proximal minimization algorithm [44, 55]; and the extragradient algorithm <ref> [25, 43] </ref> among others. We note, in the passing, that for characteristic mappings e () of feasible descent methods, e (x i ) ! 0 as i ! 1 by algorithm construction [31]. <p> It can be checked that d 1 = O (L 2 ); where d 1 is the constant involved in Theorem 3.2.1. 79 3.3.3 Extragradient Method Consider now the extragradient method <ref> [25, 43] </ref> which updates a current iterate according to the double-projection formula x i+1 = [x i i rf [x i i rf (x i )] + ] + : This iteration can be re-written as x i+1 = [x i i rf (x i ) + e (x i )]
Reference: [44] <author> B. Martinet. </author> <title> Regularisation d'inequations variationelles per approximations successives. </title> <journal> Rev. Francaise d'Auto et.Inform. Rech. Oper., </journal> <pages> pages 154-159, </pages> <year> 1970. </year>
Reference-contexts: This is a rather general framework that includes a gradient projection algorithm [15, 27]; proximal minimization algorithm <ref> [44, 55] </ref>; and the extragradient algorithm [25, 43] among others. We note, in the passing, that for characteristic mappings e () of feasible descent methods, e (x i ) ! 0 as i ! 1 by algorithm construction [31]. <p> Provided the stepsize satisfies the standard conditions 0 &lt; c 3 i L it can be verified that d 1 = O (L 2 ); where d 1 is the constant involved in Theorem 3.2.1. 3.3.2 Proximal Minimization Algorithm Given a current iterate x i , the proximal minimization algorithm <ref> [44, 55] </ref> generates the next iterate x i+1 according to x i+1 = arg min i (x) := f (x) + 2 i This method also falls within the presented framework as can be seen from the following.
Reference: [45] <author> D.Q. Mayne and E. Polak. </author> <title> Nondifferentiable optimization via adaptive smoothing. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 43(4) </volume> <pages> 601-614, </pages> <year> 1984. </year>
Reference-contexts: We assume that the set A is bounded. Problems of the form (2.1.5) arise, for example, in least-norm minimization, neural networks applications, and approximation theory. Among some important practical applications that involve parameters in the objective function, we note the adaptive smoothing techniques <ref> [45] </ref>, and the neural network training [57, 41, 36]. We assume that each function f j (; ff) is Lipschitz continuous with modulus L &gt; 0 and regular on an open neighborhood of X + t B for every ff 2 A.
Reference: [46] <author> V. S. Mikhalevitch, A. M. Gupal, and V. I. Norkin. </author> <title> Methods of nonconvex optimization. </title> <publisher> Nauka, </publisher> <address> Moscow, </address> <year> 1987. </year> <note> In Russian. </note>
Reference-contexts: We also study various important modifications of this basic algorithm, including incremental methods (see Algorithms 2.3.1, 2.3.2, 2.4.1). We point out that the condition of decaying stepsize ( ! 0) is indispensable in the general nonsmooth case <ref> [46] </ref>, as well as in the case of (smooth) incremental methods (see Remark 1.3.1). In this chapter we show that the iterates of the algorithm are, in a certain sense, attracted to an "()-stationary set of the problem (Theorem 2.3.1). <p> Remark 2.4.1 Theorem 2.4.1 extends and strengthens the results on convergence properties of the generalized gradient projection method given in <ref> [46, 11, 75] </ref>. 2.5 Applications to Neural Network Training In this section we briefly describe how results of Section 2.3 can be applied to reveal some important properties of various neural network learning techniques.
Reference: [47] <author> J.M. Ortega and W.C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations in Several Variables. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: By R-linear convergence and Q-linear convergence, we mean linear convergence in the root sense and in the quotient sense, respectively, as defined in <ref> [47] </ref>. 1.2 Convergence Of A Class Of Nonmonotone Algorithms We start with a convergent nonmonotone algorithm theorem for the solution of the unconstrained minimization problem min 10 where f : &lt; n ! &lt; is a continuously differentiable function from the n-dimensional real space &lt; n to the real numbers &lt;.
Reference: [48] <author> J.-S. Pang. </author> <title> A posteriori error bounds for the linearly-constrained variational inequality problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12 </volume> <pages> 474-484, </pages> <year> 1987. </year>
Reference-contexts: Moreover, under additional assumptions, this condition holds with - = 1 (global error bound) <ref> [29, 48, 28] </ref>. Therefore, if x 2 X s (*) and the bound (3.1.7) holds with - *, it follows immediately that d (x; X s ) kr (x)k *: The rest of the chapter is organized as follows. <p> This error bound is known to hold when F () is affine (see [30, 53]) or F () has certain strong monotonicity structure (see [67, Theorem 2]). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see <ref> [28, 29, 39, 48] </ref>). Therefore our analysis shows that certain unconstrained optimization techniques applied to minimizing the implicit Lagrangian attain linear rate of convergence.
Reference: [49] <author> H. Paugam-Moisy. </author> <title> On parallel algorithm for backpropagation by partitioning the training set. </title> <booktitle> In Neural Networks and Their Applications. Proceedings of Fifth International Conference, </booktitle> <address> Nimes, France, </address> <month> November 2-6, </month> <year> 1992. </year> <month> 114 </month>
Reference-contexts: For empirical study of this kind of parallelization see <ref> [49, 14] </ref>. Note that if examples are assigned to more than one processor, a different (weighted) error function may be generated, which nevertheless measures the error of the same training problem. <p> Then a synchronization step is performed that consists of averaging the iterates computed by all the p processors. Empirical evaluation of parallel BP and numerical tests can be found in <ref> [49, 14] </ref>. Below we state a parallel BP algorithm with an added momentum term which consists of the difference between the current and previous iterates. For simplicity and in a similar manner to the method of conjugate gradients [51] we reset this term to zero periodically (see Algorithm 1.3.1). <p> The type of paral-lelization considered here is primarily motivated by incremental gradient methods, particularly neural network training (see Chapter 1). Empirical evaluation of parallel BP and numerical tests can be found in <ref> [49, 14] </ref>. Another related work on parallel computing is [68]. We first consider the most general case. Our results can be then specialized by removing parallelism and/or considering the standard (nonadditive) objective function. <p> Thus BP is a special case of Algorithm 2.3.1. Many other computationally important BP modifications, such as parallel BP <ref> [42, 49, 14] </ref>, BP with momentum term [24], and BP with varying smoothing parameter [65] all fall within the framework of Section 2.3.
Reference: [50] <author> E. Polak. </author> <title> Computational methods in optimization: A unified approach. </title> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1971. </year>
Reference-contexts: This technique can be viewed as a generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The Lyapunov Direct Method has proved to be a powerful tool for stability analysis of both continuous and discrete time processes <ref> [56, 72, 50, 51] </ref>. Roughly speaking, this approach reduces analysis of stability properties of a process to the analysis of local improvement of this process with respect to some scalar criterion V () (usually called the Lyapunov function). <p> Moreover, standard relations such as f (x i ) f (x i+1 ) 0; kx i+1 x i k ! 0 as i ! 1 need not hold (see Section 3.2). This makes traditional convergence analysis techniques <ref> [51, 50] </ref> inapplicable. In this chapter, we develop a new approach to the analysis of algorithms with nonvanishing perturbations. Our analysis extends some of the ideas presented in [4, 42, 74] for methods of unconstrained optimization. <p> Our argument is based on monitoring the behaviour of f () on the iterates of the algorithm. We emphasize that this behaviour is nonmonotone, and Lyapunov-type convergence analysis <ref> [50, 72] </ref> cannot be applied.
Reference: [51] <author> B.T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Remark 1.2.2 Under appropriate assumptions, other well known direction choices, such as conjugate and quasi-Newton directions <ref> [51] </ref> can also be perturbed simi larly as in Corollary 1.2.1. Remark 1.2.3 Similar to [33], a parallel version of Theorem 1.2.1 can be established where portions of the gradient are distributed among the processors. <p> Below we state a parallel BP algorithm with an added momentum term which consists of the difference between the current and previous iterates. For simplicity and in a similar manner to the method of conjugate gradients <ref> [51] </ref> we reset this term to zero periodically (see Algorithm 1.3.1). It has been observed that introduction of momentum term usually leads to faster convergence and adds stability to problems with noisy data [24]. <p> This technique can be viewed as a generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The Lyapunov Direct Method has proved to be a powerful tool for stability analysis of both continuous and discrete time processes <ref> [56, 72, 50, 51] </ref>. Roughly speaking, this approach reduces analysis of stability properties of a process to the analysis of local improvement of this process with respect to some scalar criterion V () (usually called the Lyapunov function). <p> In the classical approach, V () monotonically decreases on the iterates of the process (some typical choices for V () are : the objective function being minimized, the norm of its gradient, the distance to the solution set (see <ref> [51] </ref>)). 36 The key difference of the presented technique is that we relax the monotonicity requirement. We thus refer to V () as a pseudo-Lyapunov function. This generalization makes our approach applicable to a wider class of algorithms, including methods with perturbations. We now state the Generalized Lyapunov Direct Method. <p> Hence x 62 A 0 , and it follows that A 0 X s ("()). Now applying Theorem 2.2.1 and Corollary 2.2.1, we immediately obtain the desired results. Adding the "heavy ball" term <ref> [51] </ref> in Algorithm 2.3.1, we arrive at the follow ing modification of the parallel GGPM. In neural network literature, methods of this type are usually referred to as backpropagation with momentum term [24, 41]. Algorithm 2.3.2 (Parallel GGPM with heavy ball term). Start with any x 0 2 X. <p> These include problems with relatively small perturbations, convex and strongly convex problems, and problems with weak sharp minima <ref> [51, 5] </ref>. We start with the following lemma which deals with the case of perturbations small relative to the residual function r () defined in (2.1.2). Lemma 2.4.1 Let "(x) maxf"; r (x)g 8x 2 X; where " 0; 1 &gt; 0. <p> Moreover, standard relations such as f (x i ) f (x i+1 ) 0; kx i+1 x i k ! 0 as i ! 1 need not hold (see Section 3.2). This makes traditional convergence analysis techniques <ref> [51, 50] </ref> inapplicable. In this chapter, we develop a new approach to the analysis of algorithms with nonvanishing perturbations. Our analysis extends some of the ideas presented in [4, 42, 74] for methods of unconstrained optimization. <p> The class of problems with weak sharp minima of order 2 can be thought of as a certain unconstrained smooth analogue of weak sharp minima (of order 1) <ref> [38, 51, 5] </ref>. Note that it subsumes strongly convex programs. Let f () be strongly convex with modulus 2. Then its unique optimal point x is globally (with * = 1) weakly sharp of order 2. This can be easily verified as follows.
Reference: [52] <author> E. Rich and K. Knight. </author> <booktitle> Artificial Intelligence. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: They are known to be particularly well suited for large data sets (K is large). Another attractive feature of online approach is that it is incremental and can be used in real time operation. These properties are of particular importance for optimal control [1] and artificial intelligence <ref> [52] </ref> applications. A more detailed comparison of the online and batch approaches to training neural networks is given below (see page 7). A neural network can be thought of as a network representation of a certain 4 nonlinear map between an input space and an output space.
Reference: [53] <author> S.M. Robinson. </author> <title> Some continuity properties of polyhedral multifunctions. </title> <journal> Mathematical Programming Study, </journal> <volume> 14 </volume> <pages> 206-214, </pages> <year> 1981. </year>
Reference-contexts: We note that another important property of the residual function r (x) is that, under certain conditions, its norm provides a (local) upper bound on the distance to the set X s <ref> [31, 53] </ref>. Namely, there exist positive constants and - (depending on f () and X only) such that d (x; X s ) kr (x)k 8x with kr (x)k -; (3.1.7) 66 where d (; X s ) denotes the Euclidean distance to X s . <p> This error bound is known to hold when F () is affine (see <ref> [30, 53] </ref>) or F () has certain strong monotonicity structure (see [67, Theorem 2]). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see [28, 29, 39, 48]).
Reference: [54] <author> R.T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1970. </year>
Reference-contexts: 0 2 @f (x) + N X (x)g; where @f (x) is the set of all generalized gradients (in the sense of Clarke, [6]) of f () at x, and N X (x) &lt; n is the normal cone to the set X at the point x 2 X (see <ref> [54] </ref>) : The following notions will play an important role in our analysis. Let " : X ! &lt; + be any nonnegative upper semicontinuous function. <p> We shall consider the level sets of '() defined as L ('; t) := fx 2 X j '(x) tg; t 0: (3.2.9) 73 Note that the set L ('; t) is closed for any t 2 &lt; (Theorem 7.1, <ref> [54] </ref>).
Reference: [55] <author> R.T. Rockafellar. </author> <title> Monotone operators and the proximal point algorithm. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 14(5) </volume> <pages> 877-898, </pages> <year> 1976. </year>
Reference-contexts: This is a rather general framework that includes a gradient projection algorithm [15, 27]; proximal minimization algorithm <ref> [44, 55] </ref>; and the extragradient algorithm [25, 43] among others. We note, in the passing, that for characteristic mappings e () of feasible descent methods, e (x i ) ! 0 as i ! 1 by algorithm construction [31]. <p> Provided the stepsize satisfies the standard conditions 0 &lt; c 3 i L it can be verified that d 1 = O (L 2 ); where d 1 is the constant involved in Theorem 3.2.1. 3.3.2 Proximal Minimization Algorithm Given a current iterate x i , the proximal minimization algorithm <ref> [44, 55] </ref> generates the next iterate x i+1 according to x i+1 = arg min i (x) := f (x) + 2 i This method also falls within the presented framework as can be seen from the following.
Reference: [56] <author> N. Rouche, P. Habets, and M Laloy. </author> <title> Stability Theory by Liapunov's Direct Method. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: This technique can be viewed as a generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The Lyapunov Direct Method has proved to be a powerful tool for stability analysis of both continuous and discrete time processes <ref> [56, 72, 50, 51] </ref>. Roughly speaking, this approach reduces analysis of stability properties of a process to the analysis of local improvement of this process with respect to some scalar criterion V () (usually called the Lyapunov function).
Reference: [57] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> pages 318-362, </pages> <address> Cambridge, Mas-sachusetts, 1986. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The set X is typically either &lt; n or a set of simple box-constraints. One of the most widely used methods for training neural networks is the backpropagation algorithm (BP) <ref> [57] </ref>. Every iteration of online BP is a step in the direction of negative gradient of a partial error function associated with a single training example (e.g. f j (; ff) in (1.1.2)). <p> However, this complicates the proof somewhat. 1.3 Convergence Of The Backpropagation Al gorithm We now turn our attention to the classical BP algorithm for training feedforward artificial neural networks with one layer of hidden units <ref> [57, 24] </ref>. The number of hidden units is assumed to be fixed. Suppose we have K training examples and p processors with K 1 and p 1 (typically, K is much bigger than p). <p> We assume that the set A is bounded. Problems of the form (2.1.5) arise, for example, in least-norm minimization, neural networks applications, and approximation theory. Among some important practical applications that involve parameters in the objective function, we note the adaptive smoothing techniques [45], and the neural network training <ref> [57, 41, 36] </ref>. We assume that each function f j (; ff) is Lipschitz continuous with modulus L &gt; 0 and regular on an open neighborhood of X + t B for every ff 2 A. We also assume that the map @f j (; ) is upper semicontinuous.
Reference: [58] <author> T.J. Sejnowski and C.R. Rosenberg. </author> <title> Paralel networks that learn to pronounce english text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168, </pages> <year> 1987. </year> <month> 115 </month>
Reference-contexts: In particular, we give a precise characterization to empirically observed stability of neural networks and backpropagation training <ref> [58, 19] </ref>. We also show that the properties of the weight perturbation [23] algorithm can be derived by making use of the presented analysis. 2.5.1 Backpropagation With Noise We note that when implemented in hardware, BP algorithm is likely to have some kind of electronic imperfections [23].
Reference: [59] <author> S. Shah, F. Palmieri, and M. Datum. </author> <title> Optimal filtering algorithms for fast learning in feedforward neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 779-787, </pages> <year> 1992. </year>
Reference-contexts: Unfortunately, this often seems to be the case in machine learning. For many practical neural network applications, standard optimization methods require storage and computational cost which can become unmanageable even for a moderate network size, provided the training set is large enough <ref> [59] </ref>. For problems of this 3 kind, incremental methods are known to be more cost effective and often less likely to get stuck at poor local minima or stationary points of which there are many [37].
Reference: [60] <author> J. Sietsma and R.J.F Dow. </author> <title> Creating artificial neural networks that generalize. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 67-79, </pages> <year> 1991. </year>
Reference-contexts: It appears that a neural network trained with some induced noise often has a better ability to recognize noisy patterns, and performs better in classifying patterns that were not presented to the network during the training procedure <ref> [60] </ref>. The last property is usually called the generalization ability and it is one of the major strengths of artificial neural networks. As another source of perturbations in the neural network training, we note the technique presented in [19].
Reference: [61] <author> M. V. Solodov and P. Tseng. </author> <title> Modified pojection-type methods for monotone variational inequalities. </title> <type> Technical Report Mathematical Programming 94-04, </type> <institution> Computer Science Department, University of Wisconsin, </institution> <address> 1210 West Dayton Street, Madison, Wisconsin 53706, U.S.A., </address> <month> May </month> <year> 1994. </year> <note> SIAM Journal on Control and Optimization, to appear. </note>
Reference-contexts: It is shown that the perturbed gradient projection, proximal minimization and extragra-dient methods fall within the presented framework. Applications of the ideas presented here to other classes of optimization algorithms (for example, <ref> [61, 40] </ref>) is an interesting subject of future research. 81 Chapter 4 Partially Asynchronous Inexact Parallel Variable Distribution Algorithms We consider the recently proposed parallel variable distribution (PVD) algorithm [12] for solving optimization problems in which the variables are distributed among p processors.
Reference: [62] <author> M. V. Solodov and S. K. Zavriev. </author> <title> Stability properties of the gradient projection method with applications to the backpropagation algorithm. </title> <type> Technical Report Mathematical Programming 94-05, </type> <institution> Computer Science Department, University of Wisconsin, </institution> <address> 1210 West Dayton Street, Madison, Wisconsin 53706, U.S.A., </address> <month> June </month> <year> 1994. </year> <note> SIAM Journal on Optimization, submitted. </note>
Reference-contexts: Other generalizations are possible. 30 Chapter 2 Generalized Gradient Pro jection Methods in The Presence of Perturbations We investigate convergence properties of the generalized gradient projection algorithm in the presence of perturbations <ref> [62] </ref>. It is shown that the iterates of the method are attracted, in a certain sense, to an "-stationary set of the problem, where " depends on the magnitude of the perturbations. Characterization of the attraction sets for the iterates is given in the general (nonsmooth and nonconvex) case. <p> This makes traditional convergence analysis techniques [51, 50] inapplicable. In this chapter, we develop a new approach to the analysis of algorithms with nonvanishing perturbations. Our analysis extends some of the ideas presented in [4, 42, 74] for methods of unconstrained optimization. Essential perturbations were considered in <ref> [62] </ref> in a different context of incremental gradient-type methods with decaying stepsize. 65 A special case of an approximate gradient projection method with decaying stepsize is also studied in [32]. We note that in this chapter, the stepsize is bounded away from zero. <p> We note that in this chapter, the stepsize is bounded away from zero. Therefore, the situation and the analysis required are completely different from <ref> [62, 32] </ref>. We now define the following residual function r (x) := x [x rf (x)] + : It is well known that some x 2 &lt; n satisfies the Minimum Principle optimality condition [34] for problem (3.1.1) if and only if r (x) = 0.
Reference: [63] <author> M.V. Solodov. </author> <title> Convergence analysis of perturbed feasible descent methods. </title> <journal> Journal of Optimization Theory and Applications, </journal> <month> June </month> <year> 1995. </year> <note> Submitted. </note>
Reference-contexts: However, for some problems this is still practical [23]. 61 Chapter 3 Convergence Analysis of Perturbed Feasible Descent Methods We develop a general approach to convergence analysis of feasible descent methods in the presence of perturbations <ref> [63] </ref>. The important novel feature of our analysis is that perturbations need not tend to zero in the limit. In that case, standard convergence analysis techniques are not applicable. Therefore a new approach is needed.
Reference: [64] <author> M.V. Solodov. </author> <title> Partially asynchronous inexact parallel variable distribution algorithms. </title> <journal> Computational Optimization and Applications, </journal> <month> June </month> <year> 1995. </year> <note> Submitted. 116 </note>
Reference-contexts: Each processor has the primary responsibility for updating its block of variables while allowing the remaining "secondary" variables to change in a restricted fashion along some easily computable directions. We propose a useful partially asynchronous approach and a generalization that consists of inexact subproblem solution in the PVD algorithm <ref> [64] </ref>. These modifications are the key features of the algorithm that has not been analyzed before. The proposed modified algorithms are more practical and make it easier 82 to achieve good load balancing among the parallel processors.
Reference: [65] <author> A. Sperduti and A. Starita. </author> <title> Speed up learning and network optimization with extended backpropagation. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 365-383, </pages> <year> 1993. </year>
Reference-contexts: Thus BP is a special case of Algorithm 2.3.1. Many other computationally important BP modifications, such as parallel BP [42, 49, 14], BP with momentum term [24], and BP with varying smoothing parameter <ref> [65] </ref> all fall within the framework of Section 2.3. It is quite common that for a sample ~ j in the training set some of its attributes (i.e. the components of the m-dimensional vector) are computed (or supplied) with an error that we shall denote j .
Reference: [66] <author> P. Tseng. </author> <title> Dual coordinate ascent methods for non-strictly convex minimization. </title> <journal> Mathematical Programming, </journal> <volume> 59 </volume> <pages> 231-248, </pages> <year> 1993. </year>
Reference-contexts: The presence of this term allows for a change in "secondary" variables. This makes PVD fundamentally different from the block Jacobi [3], coordinate descent <ref> [66] </ref> and parallel gradient distribution algorithms [33]. The directions 83 l are typically easily computable steepest descent or quasi-Newton directions in the space of the corresponding variables. The "forget-me-not" approach improves robustness and accelerates convergence of the algorithm and is the key to its success.
Reference: [67] <author> P. Tseng. </author> <title> On linear convergence of iterative methods for the variational inequality problem. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <note> 1995. To appear. </note>
Reference-contexts: This error bound is known to hold when F () is affine (see [30, 53]) or F () has certain strong monotonicity structure (see <ref> [67, Theorem 2] </ref>). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see [28, 29, 39, 48]). Therefore our analysis shows that certain unconstrained optimization techniques applied to minimizing the implicit Lagrangian attain linear rate of convergence.
Reference: [68] <author> J.N. Tsitsiklis, D.P. Bertsekas, and M. Athans. </author> <title> Distributed asynchronous deterministic and stochastic gradient optimization algorithms. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-31(9):803-812, </volume> <year> 1986. </year>
Reference-contexts: The type of paral-lelization considered here is primarily motivated by incremental gradient methods, particularly neural network training (see Chapter 1). Empirical evaluation of parallel BP and numerical tests can be found in [49, 14]. Another related work on parallel computing is <ref> [68] </ref>. We first consider the most general case. Our results can be then specialized by removing parallelism and/or considering the standard (nonadditive) objective function.
Reference: [69] <author> A. S. Weigend, B. A. Huberman, and D. E. Rumelhart. </author> <title> Predicting the future:a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 193-209, </pages> <year> 1990. </year>
Reference-contexts: This, in fact, corresponds to the weight decay training <ref> [21, 69] </ref>. Weight decay is a useful approach since it tends to generate simpler networks by minimizing nonzero arc connections. Simpler networks often possess better generalization properties.
Reference: [70] <author> H. White. </author> <title> Some asymptotic results for learning in single hidden-layer feed-forward network models. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84(408) </volume> <pages> 1003-1013, </pages> <year> 1989. </year>
Reference-contexts: A single iteration of BP may, in fact, increase rather than decrease the objective function f (; ff) which we are trying to minimize. This difficulty makes convergence analysis of BP a challenging problem that has currently attracted the interest of many researchers <ref> [42, 32, 70] </ref>. To further justify our interest in the online (incremental) methods we make the following observations [20] : 8 (i) For many learning systems adaptation to on-line stream of training samples is required. <p> Flowchart of the Parallel BP Below we give the first deterministic convergence proof for the parallel and serial BP algorithm. In <ref> [70] </ref> it is proven that the sequence of weights generated by the serial BP either converges to a point that is almost surely stationary or it diverges. In contrast, our approach is deterministic.
Reference: [71] <author> B. Widrow and M.A. Lehr. </author> <title> 30 years of adaptive neural networks: Perceptron, Madaline, and backpropagation. </title> <journal> IEEE Proceedings, </journal> <volume> 78 </volume> <pages> 1415-1442, </pages> <month> September </month> <year> 1990. </year> <month> 117 </month>
Reference-contexts: The latter is equivalent to forcing the corresponding weights to zero, and can also be treated as induced perturbations. We finally mention the node perturbation approach first proposed in <ref> [71] </ref>.
Reference: [72] <author> W.I. Zangwill. </author> <title> Nonlinear Programming: A Unified Approach. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1969. </year>
Reference-contexts: This technique can be viewed as a generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The Lyapunov Direct Method has proved to be a powerful tool for stability analysis of both continuous and discrete time processes <ref> [56, 72, 50, 51] </ref>. Roughly speaking, this approach reduces analysis of stability properties of a process to the analysis of local improvement of this process with respect to some scalar criterion V () (usually called the Lyapunov function). <p> Our argument is based on monitoring the behaviour of f () on the iterates of the algorithm. We emphasize that this behaviour is nonmonotone, and Lyapunov-type convergence analysis <ref> [50, 72] </ref> cannot be applied.
Reference: [73] <author> S. K. Zavriev. </author> <title> Stochastic subgradient methods for Minmax problems. </title> <address> Iz-datelstvo MGU, Moscow, </address> <year> 1984. </year> <note> In Russian. </note>
Reference-contexts: Let fA fl g; fl 2 38 be the (unique) decomposition of A 0 into V ()-connected components (see <ref> [73] </ref>), that is A 0 = [ fl2 A fl ; A fl 0 for fl 0 6= fl 00 ; fl 0 ; fl 00 2 : The following theorem will play a central role in the subsequent analysis.
Reference: [74] <author> S. K. Zavriev. </author> <title> Convergence properties of the gradient method under variable level interference. </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 30 </volume> <pages> 997-1007, </pages> <year> 1990. </year>
Reference-contexts: This makes traditional convergence analysis techniques [51, 50] inapplicable. In this chapter, we develop a new approach to the analysis of algorithms with nonvanishing perturbations. Our analysis extends some of the ideas presented in <ref> [4, 42, 74] </ref> for methods of unconstrained optimization. Essential perturbations were considered in [62] in a different context of incremental gradient-type methods with decaying stepsize. 65 A special case of an approximate gradient projection method with decaying stepsize is also studied in [32].
Reference: [75] <author> S.K. Zavriev and A.G. Perevozchikov. </author> <title> Attraction of trajectories of finite-difference inclusions and stability of numerical methods of stochastic non-smooth optimization. </title> <journal> Soviet Phys. Doklady, </journal> <volume> 313 </volume> <pages> 1373-1376, </pages> <year> 1990. </year>
Reference-contexts: Remark 2.4.1 Theorem 2.4.1 extends and strengthens the results on convergence properties of the generalized gradient projection method given in <ref> [46, 11, 75] </ref>. 2.5 Applications to Neural Network Training In this section we briefly describe how results of Section 2.3 can be applied to reveal some important properties of various neural network learning techniques.
Reference: [76] <author> S.K. Zavriev and A.G. Perevozchikov. </author> <title> Direct Lyapunov's method in attraction analysis of finite-difference inclusions. USSR Computational Mathematics and Mathematical Physics, </title> <publisher> Pergamon Press, </publisher> <pages> 30(1) 22-32, </pages> <year> 1990. </year>
Reference-contexts: We give a precise characterization of "() in terms of asymptotic behavior of perturbations. Our analysis is based on the novel technique 33 presented in <ref> [76] </ref>. This approach allows us to deal with essentially perturbed problems (i.e. problems with nonvanishing noise : ffi (x i ) 6! 0 as i ! 1), as well as analyze algorithms that are inherently nonmonotone, e.g. incremental methods (see Algorithm 2.3.1). <p> One more word about our notation. By conv C we shall denote the convex hull of a set C, and by int C its interior. 2.2 Generalized Lyapunov Direct Method In this section we outline the novel convergence analysis technique that was first proposed in <ref> [76] </ref> (albeit in a slightly different form). This technique can be viewed as a generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. <p> Theorem 2.2.1 <ref> [76] </ref> For every sequence fx i g generated by the process (2.2.1)- (2.2.2), and satisfying (2.2.3), there exists a fl 2 such that the following properties hold : lt V (x i ) = V lt fx i g " A fl ; and every subsequence fx i m g of <p> Corollary 2.2.1 <ref> [76] </ref> Let the set V (A 0 ) be nowhere dense in &lt;.
Reference: [77] <author> R. Zhang and J. Treiman. Upper-Lipschitz multifunctions and inverse sub-differentials. </author> <title> Nonlinear Analysis. </title> <note> To appear. </note>
Reference-contexts: Remark 4.3.2 contains further examples of problems with weak sharp minima of order 2. Some issues related to this growth condition are also discussed in <ref> [77] </ref>. Theorem 4.3.1 Let f () 2 C 1 L (&lt; n ). Suppose fx i g is any sequence generated by Algorithm 4.3.1.
References-found: 77

