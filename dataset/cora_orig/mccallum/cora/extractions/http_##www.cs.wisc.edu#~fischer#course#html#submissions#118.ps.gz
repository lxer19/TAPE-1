URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/118.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/
Root-URL: http://www.cs.wisc.edu
Email: ajcbik@cs.leidenuniv.nl  
Phone: Tel. +31 71 5277037  
Author: Aart J.C. Bik 
Note: Submission to PLDI'96  
Address: P.O. Box 9512, 2300 RA Leiden The Netherlands  
Affiliation: High Performance Computing Division Department of Computer Science Leiden University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Vasanth Balasundaram. </author> <title> Interactive Parallelization of Numerical Scientific Programs. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <year> 1989. </year>
Reference-contexts: Associated with each occurrence of an enveloping data structure is an access direction ~q 2 Z 2 and a simple section S Z 2 approximating the index set of the region that may be accessed by this occurrence is computed (see <ref> [1, 2, 8, 27] </ref> for details on simple sections).
Reference: [2] <author> Vasanth Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Volume 9 </volume> <pages> 154-170, </pages> <year> 1990. </year>
Reference-contexts: Associated with each occurrence of an enveloping data structure is an access direction ~q 2 Z 2 and a simple section S Z 2 approximating the index set of the region that may be accessed by this occurrence is computed (see <ref> [1, 2, 8, 27] </ref> for details on simple sections).
Reference: [3] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: Automatically generating sparse codes has a number of advantages. It simplifies the task of the programmer and enables regular data dependence analysis <ref> [3] </ref> to be performed on the original dense code.
Reference: [4] <author> Aart J.C. Bik. </author> <title> A prototype restructuring compiler. </title> <type> Master's thesis, </type> <institution> Utrecht University, </institution> <year> 1992. </year> <month> INF/SCR-92-11. </month>
Reference-contexts: Our sparse compiler, incorporated in a restructuring compiler MT1 <ref> [4, 11] </ref>, expects a FORTRAN program in which 2-dimensional arrays are used as enveloping data structures for all implicitly sparse matrices, i.e. matrices of which the sparsity is not dealt with explicitly by the programmer.
Reference: [5] <author> Aart J.C. Bik, Peter M.W. Knijnenburg, and Harry A.G. Wijshoff. </author> <title> Reshaping access patterns for generating sparse codes. </title> <editor> In K. Pingali, U. Banerjee, D. Gel-ernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> No. 892, </volume> <pages> pages 406-422. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin/New York, </address> <year> 1995. </year>
Reference-contexts: Annotations are used to supply the compiler with information that cannot be expressed in the dense program [9]. Thereafter, the compiler applies conventional program transformations to enable the automatic conversion into efficient sparse code <ref> [5, 10] </ref>. Information about the 1 nonzero structure of each implicitly sparse matrix, obtained by annotations or au-tomatic analysis of files [7, 9], is used to control these transformations. <p> Some conventional loop transformations are applied by the compiler affect the access summaries of an implicitly sparse matrix by reshaping <ref> [5] </ref> and iteration space partitioning [10]. The former transformation is used to obtain a uniform access direction for each representative simple section.
Reference: [6] <author> Aart J.C. Bik and Harry A.G. Wijshoff. </author> <title> Compilation techniques for sparse matrix computations. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 416-424, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Because the development and maintenance of sparse applications requires a lot of programming effort, in earlier work we proposed an alternative approach to the generation of such applications <ref> [6, 8] </ref>. Instead of dealing with the sparsity of matrices at programming level to reduce the storage requirements and computational time of an application, as done traditionally [16, 19, 21, 28], this exploitation is done at compilation level. Automatically generating sparse codes has a number of advantages. <p> A is used as enveloping data structure of an implicitly sparse matrix with a dense diagonal: PARAMETER (N=100) REAL A (N,N), B (N,N) C_SPARSE (A : _DENSE (0 &lt;= I-J &lt;= 0)) Conditions are associated with each statement to indicate the instances of a that statement that must be executed <ref> [6, 8] </ref>. <p> N ENDDO ENDDO ENDDO If an annotation indicates that both the arrays A and B are used as enveloping data structure of implicitly sparse matrices of which no further nonzero structure is available, the compiler may decide to move the K-loop to the outermost position, so 7 that guard encapsulation <ref> [6, 8] </ref> becomes feasible for two DO-loops, if column- and row-wise storage is selected for A and B respectively: DO K = 1, N I = IND_A (AD_2) J = IND_A (AD_3) ENDDO ENDDO ENDDO If nonzero structure information is available, this can be accounted for.
Reference: [7] <author> Aart J.C. Bik and Harry A.G. Wijshoff. </author> <title> Nonzero structure analysis. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 226-235, </pages> <year> 1994. </year>
Reference-contexts: Thereafter, the compiler applies conventional program transformations to enable the automatic conversion into efficient sparse code [5, 10]. Information about the 1 nonzero structure of each implicitly sparse matrix, obtained by annotations or au-tomatic analysis of files <ref> [7, 9] </ref>, is used to control these transformations. The compiler tries to isolate regions in a matrix having certain properties and to enforce a uniform access directions through these regions. Operations on zero elements are eliminated.
Reference: [8] <author> Aart J.C. Bik and Harry A.G. Wijshoff. </author> <title> On automatic data structure selection and code generation for sparse computations. </title> <editor> In Utpal Banerjee, David Gelernter, Alex Nicolau, and David Padua, editors, </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> No. 768, </volume> <pages> pages 57-75. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin/New York, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Because the development and maintenance of sparse applications requires a lot of programming effort, in earlier work we proposed an alternative approach to the generation of such applications <ref> [6, 8] </ref>. Instead of dealing with the sparsity of matrices at programming level to reduce the storage requirements and computational time of an application, as done traditionally [16, 19, 21, 28], this exploitation is done at compilation level. Automatically generating sparse codes has a number of advantages. <p> A is used as enveloping data structure of an implicitly sparse matrix with a dense diagonal: PARAMETER (N=100) REAL A (N,N), B (N,N) C_SPARSE (A : _DENSE (0 &lt;= I-J &lt;= 0)) Conditions are associated with each statement to indicate the instances of a that statement that must be executed <ref> [6, 8] </ref>. <p> Associated with each occurrence of an enveloping data structure is an access direction ~q 2 Z 2 and a simple section S Z 2 approximating the index set of the region that may be accessed by this occurrence is computed (see <ref> [1, 2, 8, 27] </ref> for details on simple sections). <p> N ENDDO ENDDO ENDDO If an annotation indicates that both the arrays A and B are used as enveloping data structure of implicitly sparse matrices of which no further nonzero structure is available, the compiler may decide to move the K-loop to the outermost position, so 7 that guard encapsulation <ref> [6, 8] </ref> becomes feasible for two DO-loops, if column- and row-wise storage is selected for A and B respectively: DO K = 1, N I = IND_A (AD_2) J = IND_A (AD_3) ENDDO ENDDO ENDDO If nonzero structure information is available, this can be accounted for.
Reference: [9] <author> Aart J.C. Bik and Harry A.G. Wijshoff. </author> <title> Annotations for a sparse compiler. </title> <booktitle> In Proceedings of the 8th Int. Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 33.1-33.15. </pages> <year> 1995. </year>
Reference-contexts: Annotations are used to supply the compiler with information that cannot be expressed in the dense program <ref> [9] </ref>. Thereafter, the compiler applies conventional program transformations to enable the automatic conversion into efficient sparse code [5, 10]. Information about the 1 nonzero structure of each implicitly sparse matrix, obtained by annotations or au-tomatic analysis of files [7, 9], is used to control these transformations. <p> Thereafter, the compiler applies conventional program transformations to enable the automatic conversion into efficient sparse code [5, 10]. Information about the 1 nonzero structure of each implicitly sparse matrix, obtained by annotations or au-tomatic analysis of files <ref> [7, 9] </ref>, is used to control these transformations. The compiler tries to isolate regions in a matrix having certain properties and to enforce a uniform access directions through these regions. Operations on zero elements are eliminated. <p> this section, a short outline of automatic data structure selection is given. 2.1 Program Analysis and Transformation Annotations are used to identify the arrays in the original dense program that are used as enveloping data structures of implicitly sparse matrices and to supply information about these matrices to the compiler <ref> [9] </ref>.
Reference: [10] <author> Aart J.C. Bik and Harry A.G. Wijshoff. </author> <title> Construction of representative simple sections. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 9-18, </pages> <year> 1995. </year> <booktitle> Volume 2: Software. </booktitle>
Reference-contexts: Annotations are used to supply the compiler with information that cannot be expressed in the dense program [9]. Thereafter, the compiler applies conventional program transformations to enable the automatic conversion into efficient sparse code <ref> [5, 10] </ref>. Information about the 1 nonzero structure of each implicitly sparse matrix, obtained by annotations or au-tomatic analysis of files [7, 9], is used to control these transformations. <p> Some conventional loop transformations are applied by the compiler affect the access summaries of an implicitly sparse matrix by reshaping [5] and iteration space partitioning <ref> [10] </ref>. The former transformation is used to obtain a uniform access direction for each representative simple section.
Reference: [11] <author> Peter Brinkhaus. </author> <title> Compiler analysis of procedure calls. </title> <type> Master's thesis, </type> <institution> Utrecht University, </institution> <year> 1993. </year> <month> INF/SCR-93-13. </month>
Reference-contexts: Our sparse compiler, incorporated in a restructuring compiler MT1 <ref> [4, 11] </ref>, expects a FORTRAN program in which 2-dimensional arrays are used as enveloping data structures for all implicitly sparse matrices, i.e. matrices of which the sparsity is not dealt with explicitly by the programmer.
Reference: [12] <author> David S. Dodson, Roger G. Grimes, and John G. Lewis. </author> <title> Algorithm 692: Model implementation and test package for the sparse linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> Volume 17 </volume> <pages> 264-272, </pages> <year> 1991. </year> <month> 9 </month>
Reference-contexts: In the context of sparse computation, the use of a library has been advocated by others. For the sparse extensions to BLAS <ref> [12, 13] </ref>, a number of basic sparse operations are identified and standardized in a library to improve the readability, portability and efficiency of sparse codes. <p> As was illustrated with an example, one original dense program can be mapped to many sparse versions, each of which 8 is tailored for a particular instance of the same problem. This clearly shows the potential of sparse compilers. Sparse primitives already have been identified in <ref> [12, 13] </ref> and [24]. The use of a sparse compiler and run-time libraries to optimize irregular codes directly has been studied in [22, 23, 25, 26]. Recently, the method of automatically converting a dense program into sparse code is being adopted by others [20].
Reference: [13] <author> David S. Dodson, Roger G. Grimes, and John G. Lewis. </author> <title> Sparse extensions to the FORTRAN basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> Volume 17 </volume> <pages> 253-263, </pages> <year> 1991. </year>
Reference-contexts: In the context of sparse computation, the use of a library has been advocated by others. For the sparse extensions to BLAS <ref> [12, 13] </ref>, a number of basic sparse operations are identified and standardized in a library to improve the readability, portability and efficiency of sparse codes. <p> In [18], a new approach to the development of sparse libraries is proposed. 3.2 Sparse Primitives As shown in table 1, a number of primitives that manipulate the sparse vectors in a pool are supplied. Inspired on the BLAS convention (see e.g <ref> [13, 14] </ref>), the underscore may be replaced by any type specification characters in fI; S; D; Cg to define INTEGER, REAL, DOUBLE PRECISION or COMPLEX as basis type of the entries. <p> In contrast with the scatter and gather primitives defined in sparse BLAS <ref> [13] </ref>, primitives SCTRX and GTHRX also support a so-called switch [21]. <p> As was illustrated with an example, one original dense program can be mapped to many sparse versions, each of which 8 is tailored for a particular instance of the same problem. This clearly shows the potential of sparse compilers. Sparse primitives already have been identified in <ref> [12, 13] </ref> and [24]. The use of a sparse compiler and run-time libraries to optimize irregular codes directly has been studied in [22, 23, 25, 26]. Recently, the method of automatically converting a dense program into sparse code is being adopted by others [20].
Reference: [14] <author> Jack J. Dongarra, Iain S. Duff, Danny C. Sorensen, and Henk A. van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared Memory Computers. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1991. </year>
Reference-contexts: In [18], a new approach to the development of sparse libraries is proposed. 3.2 Sparse Primitives As shown in table 1, a number of primitives that manipulate the sparse vectors in a pool are supplied. Inspired on the BLAS convention (see e.g <ref> [13, 14] </ref>), the underscore may be replaced by any type specification characters in fI; S; D; Cg to define INTEGER, REAL, DOUBLE PRECISION or COMPLEX as basis type of the entries.
Reference: [15] <author> Iain S. Duff. </author> <title> Data structures, algorithms and software for sparse matrices. </title> <editor> In David J. Evans, editor, </editor> <booktitle> Sparsity and Its Applications, </booktitle> <pages> pages 1-29. </pages> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: Because on the average V A will remain in the sparse regions of A, the size of these parallel arrays is set accordingly, with some additional working space as suggested in <ref> [15, 17, 28] </ref>.
Reference: [16] <author> Iain S. Duff, A.M. Erisman, and J.K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford Science Publications, Oxford, </publisher> <year> 1990. </year>
Reference-contexts: Instead of dealing with the sparsity of matrices at programming level to reduce the storage requirements and computational time of an application, as done traditionally <ref> [16, 19, 21, 28] </ref>, this exploitation is done at compilation level. Automatically generating sparse codes has a number of advantages. It simplifies the task of the programmer and enables regular data dependence analysis [3] to be performed on the original dense code.
Reference: [17] <author> Iain S. Duff and J.K. Reid. </author> <title> Some design features of a sparse matrix code. </title> <journal> ACM Transactions on Mathematical Software, </journal> <pages> pages 18-35, </pages> <year> 1979. </year>
Reference-contexts: Because on the average V A will remain in the sparse regions of A, the size of these parallel arrays is set accordingly, with some additional working space as suggested in <ref> [15, 17, 28] </ref>.
Reference: [18] <author> Kyle A. Gallivan, Bret A. Marsolf, Aart J.C. Bik, and Harry A.G. Wijshoff. </author> <title> The generation of optimized code using nonzero structure analysis. </title> <note> Submitted to Journal of Parallel and Distributed Computing. </note>
Reference-contexts: The use of compiler support and run-time libraries to parallelize irregular computations directly has been studied extensively [22, 23, 25, 26]. In <ref> [18] </ref>, a new approach to the development of sparse libraries is proposed. 3.2 Sparse Primitives As shown in table 1, a number of primitives that manipulate the sparse vectors in a pool are supplied.
Reference: [19] <author> Alan George and Joseph W.H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewoord Cliffs, New York, </address> <year> 1981. </year>
Reference-contexts: Instead of dealing with the sparsity of matrices at programming level to reduce the storage requirements and computational time of an application, as done traditionally <ref> [16, 19, 21, 28] </ref>, this exploitation is done at compilation level. Automatically generating sparse codes has a number of advantages. It simplifies the task of the programmer and enables regular data dependence analysis [3] to be performed on the original dense code.
Reference: [20] <author> Vladimir Kotlyar, Keshav Pingali, and Paul Stodghill. </author> <title> Automatic paralleliza-tion of sparse conjugate gradient code: A progress report. </title> <booktitle> In Proceedings of the 8th Int. Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 32.1-32.15. </pages> <year> 1995. </year>
Reference-contexts: The use of a sparse compiler and run-time libraries to optimize irregular codes directly has been studied in [22, 23, 25, 26]. Recently, the method of automatically converting a dense program into sparse code is being adopted by others <ref> [20] </ref>.
Reference: [21] <author> Sergio Pissanetsky. </author> <title> Sparse Matrix Technology. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1984. </year>
Reference-contexts: Instead of dealing with the sparsity of matrices at programming level to reduce the storage requirements and computational time of an application, as done traditionally <ref> [16, 19, 21, 28] </ref>, this exploitation is done at compilation level. Automatically generating sparse codes has a number of advantages. It simplifies the task of the programmer and enables regular data dependence analysis [3] to be performed on the original dense code. <p> In contrast with the scatter and gather primitives defined in sparse BLAS [13], primitives SCTRX and GTHRX also support a so-called switch <ref> [21] </ref>. Currently, straightforward implementations of these primitives are used to provide the functionality that is required to test the feasibility of our sparse compiler. 4 Actual Sparse Code Generation The actual sparse code generation replaces dense constructs by sparse equivalents, in which primitives of the library can be used.
Reference: [22] <author> Ravi Ponnusamy, Joel Saltz, and Alok Choudhary. </author> <title> Runtime-compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 361-370, </pages> <year> 1993. </year>
Reference-contexts: The use of compiler support and run-time libraries to parallelize irregular computations directly has been studied extensively <ref> [22, 23, 25, 26] </ref>. In [18], a new approach to the development of sparse libraries is proposed. 3.2 Sparse Primitives As shown in table 1, a number of primitives that manipulate the sparse vectors in a pool are supplied. <p> This clearly shows the potential of sparse compilers. Sparse primitives already have been identified in [12, 13] and [24]. The use of a sparse compiler and run-time libraries to optimize irregular codes directly has been studied in <ref> [22, 23, 25, 26] </ref>. Recently, the method of automatically converting a dense program into sparse code is being adopted by others [20].
Reference: [23] <author> L.F. Romero and E.L. Zapata. </author> <title> Data distributions for sparse matrix vector multiplication. </title> <booktitle> In Proceedings of the Fourth International Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 154-167, </pages> <year> 1993. </year>
Reference-contexts: The use of compiler support and run-time libraries to parallelize irregular computations directly has been studied extensively <ref> [22, 23, 25, 26] </ref>. In [18], a new approach to the development of sparse libraries is proposed. 3.2 Sparse Primitives As shown in table 1, a number of primitives that manipulate the sparse vectors in a pool are supplied. <p> This clearly shows the potential of sparse compilers. Sparse primitives already have been identified in [12, 13] and [24]. The use of a sparse compiler and run-time libraries to optimize irregular codes directly has been studied in <ref> [22, 23, 25, 26] </ref>. Recently, the method of automatically converting a dense program into sparse code is being adopted by others [20].
Reference: [24] <author> Youcef Saad. SPARSKIT: </author> <title> a basic tool kit for sparse matrix computations. </title> <address> CSRD/RIACS, </address> <year> 1990. </year>
Reference-contexts: of 2-dim. static storage _LKP () Lookup in a sparse vector _INSRT () Insertion in a sparse vector _SCTRX () Expansion of a sparse vector into a dense format _GTHRX () Compression of a dense format into a sparse vector Table 1: Sparse Primitives single internal format of sparse matrices <ref> [24] </ref>. The use of compiler support and run-time libraries to parallelize irregular computations directly has been studied extensively [22, 23, 25, 26]. <p> As was illustrated with an example, one original dense program can be mapped to many sparse versions, each of which 8 is tailored for a particular instance of the same problem. This clearly shows the potential of sparse compilers. Sparse primitives already have been identified in [12, 13] and <ref> [24] </ref>. The use of a sparse compiler and run-time libraries to optimize irregular codes directly has been studied in [22, 23, 25, 26]. Recently, the method of automatically converting a dense program into sparse code is being adopted by others [20].
Reference: [25] <author> Joel H. Saltz et al. </author> <title> A Manual for the Chaos Runtime Library. </title> <institution> Computer Science Department, University of Maryland, </institution> <year> 1993. </year>
Reference-contexts: The use of compiler support and run-time libraries to parallelize irregular computations directly has been studied extensively <ref> [22, 23, 25, 26] </ref>. In [18], a new approach to the development of sparse libraries is proposed. 3.2 Sparse Primitives As shown in table 1, a number of primitives that manipulate the sparse vectors in a pool are supplied. <p> This clearly shows the potential of sparse compilers. Sparse primitives already have been identified in [12, 13] and [24]. The use of a sparse compiler and run-time libraries to optimize irregular codes directly has been studied in <ref> [22, 23, 25, 26] </ref>. Recently, the method of automatically converting a dense program into sparse code is being adopted by others [20].
Reference: [26] <author> Manuel Ujaldon and Emilio L. Zapata. </author> <title> Development and implementation of data-parallel compilation techniques for sparse codes. </title> <booktitle> In Proceedings of the Fifth International Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 78-97, </pages> <year> 1995. </year>
Reference-contexts: The use of compiler support and run-time libraries to parallelize irregular computations directly has been studied extensively <ref> [22, 23, 25, 26] </ref>. In [18], a new approach to the development of sparse libraries is proposed. 3.2 Sparse Primitives As shown in table 1, a number of primitives that manipulate the sparse vectors in a pool are supplied. <p> This clearly shows the potential of sparse compilers. Sparse primitives already have been identified in [12, 13] and [24]. The use of a sparse compiler and run-time libraries to optimize irregular codes directly has been studied in <ref> [22, 23, 25, 26] </ref>. Recently, the method of automatically converting a dense program into sparse code is being adopted by others [20].
Reference: [27] <author> Michael J. Wolfe. </author> <title> High Performance Compilers for Parallel Computers. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1996. </year>
Reference-contexts: Associated with each occurrence of an enveloping data structure is an access direction ~q 2 Z 2 and a simple section S Z 2 approximating the index set of the region that may be accessed by this occurrence is computed (see <ref> [1, 2, 8, 27] </ref> for details on simple sections).
Reference: [28] <author> Zahari Zlatev. </author> <title> Computational Methods for General Sparse Matrices. </title> <publisher> Kluwer, </publisher> <address> Dordrecht, </address> <year> 1991. </year> <month> 10 </month>
Reference-contexts: Instead of dealing with the sparsity of matrices at programming level to reduce the storage requirements and computational time of an application, as done traditionally <ref> [16, 19, 21, 28] </ref>, this exploitation is done at compilation level. Automatically generating sparse codes has a number of advantages. It simplifies the task of the programmer and enables regular data dependence analysis [3] to be performed on the original dense code. <p> Because on the average V A will remain in the sparse regions of A, the size of these parallel arrays is set accordingly, with some additional working space as suggested in <ref> [15, 17, 28] </ref>.
References-found: 28

