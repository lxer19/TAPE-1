URL: http://www.ius.cs.cmu.edu/IUS/inf07/tsato/spotslide/vocrsubmit.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/tsato/www/pubcmu.html
Root-URL: http://www.cs.cmu.edu
Phone: 2  
Title: Multimedia Systems Manuscript-Nr. (will be inserted by hand later) Video OCR: Indexing Digital News Libraries
Author: Toshio Sato Takeo Kanade Ellen K. Hughes Michael A. Smith Shin'ichi Satoh 
Keyword: Key words: Digital video library OCR Resolution Complex background Image enhancement  
Address: 5000 Forbes Avenue, Pittsburgh, PA 15213, USA  3-29-1 Otsuka, Bunkyo-ku, Tokyo 112, Japan  
Affiliation: 1 School of Computer Science, Carnegie Mellon University  National Center for Science Information System (NACSIS)  
Abstract: The automatic extraction and reading of news captions and annotations can be of great help locating topics of interest in digital news video archives. To achieve this goal, we present a technique, called Video OCR, which detects, extracts, and reads text areas in digital video data. In this paper, we address problems, describe the method by which Video OCR operates, and suggest applications for its use in digital news archives. To solve two problems of character recognition for videos, low resolution characters and extremely complex backgrounds, we apply an interpolation filter, multi-frame integration and a combination of four filters. Segmenting characters is done by a recognition-based segmentation method, and intermediate character recognition results are used to improve the segmentation. We also include a method for locating text areas using the text-like properties and the use of a language-based post-processing technique to increase word recognition rates. The overall recognition results are satisfactory for use in news indexing. Performing Video OCR on news video and combining its results with other video understanding techniques will improve the overall understanding of the news video content. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Wactlar HD, Kanade T, Smith MA, </author> <title> Stevens SM (1996) Intelligent access to digital video: The Informedia project. </title> <booktitle> IEEE Computer 29: </booktitle> <pages> 46-52 </pages>
Reference-contexts: For instance, the superimposed captions can be used as indices to retrieve desired video data, which are combined with keywords obtained by analysis of closed caption information (See Fig. 1). Although there is a great need for integrated character recognition in video libraries with text-based queries <ref> (Wactlar et al. 1996) </ref>, we have seen few achievements. Automatic character segmentation was performed for titles and credits in motion picture videos (Smith and Kanade 1997; Liehanrt and Stuber 1996); however, both papers have insufficient consideration of character recognition. <p> In recent research of character extraction from video images <ref> (Lienhart and Stuber 1996) </ref>, there is an assumption that characters are drawn in high contrast against the background. Object detection techniques such as matched spatial filtering are alternative approaches for extracting characters from the background. <p> We also have a problem with segmenting characters in low quality images. Although there are many approaches for character segmentation (Lu 1995), errors still occur because most methods analyze only a vertical projection profile. Character segmentation based on recognition results <ref> (Lee et al. 1996) </ref> offers the possibility of improving the accuracy. However, the huge computational cost to select proper segments from combinations of segment candidates is prohibitive. In Section 2, we describe image enhancement methods which cope with both problems.
Reference: 2. <author> Smith MA, </author> <title> Kanade T (1997) Video skimming and characterization through the combination of image and language understanding technique. </title> <booktitle> Proceedings of IEEE Conference on Computer Vision and Pattern Recognition: </booktitle> <pages> 775-781 </pages>
Reference: 3. <author> Lienhart R, </author> <title> Stuber F (1996) Automatic text recognition in digital videos. </title> <booktitle> Proceedings of SPIE Image and Video Processing IV 2666: </booktitle> <pages> 180-188 </pages>
Reference: 4. <author> Cui Y, </author> <title> Huang Q (1997) Character extraction of license plates from video. </title> <booktitle> Proceedings of IEEE Conference on Computer Vision and Pattern Recognition: </booktitle> <pages> 502-507 </pages>
Reference: 5. <author> Ohya J, </author> <title> Shio A, Akamatsu S (1994) Recognizing characters in scene images. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence 16: </journal> <pages> 214-220 </pages>
Reference: 6. <author> Zhou J, </author> <title> Lopresti D, Lei Z (1997) OCR for World Wide Web images. </title> <booktitle> Proceedings of SPIE Document Recognition IV 3027: </booktitle> <pages> 58-66 </pages>
Reference: 7. <author> Wu V, Manmatha R, </author> <title> Riseman EM (1997) Finding text in images. </title> <booktitle> 20th International ACM SIGIR Conference on Research and Development in Information Retrieval: </booktitle> <pages> 3-12 </pages>
Reference-contexts: There are similar research fields which concern character recognition of videos (Cui and Huang 1997; Ohya et al. 1994). Cui and Huang <ref> (1997) </ref> present character extraction from the car license plate using video images. Ohya et al. consider character segmentation and recognition in scene images using adaptive thresholding. <p> Therefore, the resolution of characters in the video caption is insufficient to implement stable and robust Video OCR systems. This problem can be even more serious if inadequate compression methods are employed. The problem of resolution is discussed in OCR of World Wide Web images <ref> (Zhou et al. 1997) </ref>. They propose two recognition techniques for low resolution images. However, it would be preferable to improve the quality of the images in preprocessing and use recent achievements for high resolution character recognition. Another problem is the existence of complex backgrounds. <p> Another problem is the existence of complex backgrounds. Characters superimposed on news videos often have hue and brightness similar to the background, making extraction extremely difficult. For the cases of the license plate research <ref> (Cui and Huang 1997) </ref>, the scene image research (Ohya et al. 1994) and the document image analysis (Wu et al. 1997), the difficulty of complex backgrounds is rarely a problem, although examples with plain backgrounds are illustrated. <p> Characters superimposed on news videos often have hue and brightness similar to the background, making extraction extremely difficult. For the cases of the license plate research (Cui and Huang 1997), the scene image research (Ohya et al. 1994) and the document image analysis <ref> (Wu et al. 1997) </ref>, the difficulty of complex backgrounds is rarely a problem, although examples with plain backgrounds are illustrated. In recent research of character extraction from video images (Lienhart and Stuber 1996), there is an assumption that characters are drawn in high contrast against the background. <p> In recent research of character extraction from video images (Lienhart and Stuber 1996), there is an assumption that characters are drawn in high contrast against the background. Object detection techniques such as matched spatial filtering are alternative approaches for extracting characters from the background. Brunelli and Poggio <ref> (1997) </ref> compare several template matching methods to determine which is the best for detecting an eye in a face image. Since every character has a unique shape, simple template matching methods are not adequate for extracting whole character patterns. <p> Some known constraints of text regions can reduce the processing costs. A typical text region can be characterized as a horizontal rectangular structure of clustered sharp edges, because characters usually form regions of high contrast against the background. Smith and Kanade <ref> (1997) </ref> describe text region detection using the properties of text images. The speed is fast enough to process a 352 fi 242 image in less than 0.8 seconds using a workstation (MIPS R4400 200MHz). <p> These frame numbers are determined by text region detection <ref> (Smith and Kanade 1997) </ref>. An example of effects of both the sub-pixel interpolation and the multi-frame integration is shown in Fig. 5. <p> The differences are measured among three candidates of the recognition result weighted by the similarity m c . We use two kinds of dictionaries: the Oxford Advanced Learner's Dictionary <ref> (Oxford university computing services 1997) </ref> (69,517 words) and word collections which are compiled by analyzing closed caption information of videos (4,824 words). To match video caption recognition results with data in the dictionaries, we defined the similarity between them. <p> To recognize this kind of frame using Video OCR, name and title information in the image are obtained. This name and title information is valuable in helping to match people's faces to their names <ref> (Satoh and Kanade 1997) </ref>. Even if the video caption information is included in the closed caption, the results are useful to associate image information with audio or closed caption information. Nakamura and Kanade (1997) introduce a semantic 10 Toshio Sato Takeo Kanade Ellen K. Hughes Michael A. <p> This name and title information is valuable in helping to match people's faces to their names (Satoh and Kanade 1997). Even if the video caption information is included in the closed caption, the results are useful to associate image information with audio or closed caption information. Nakamura and Kanade <ref> (1997) </ref> introduce a semantic 10 Toshio Sato Takeo Kanade Ellen K. Hughes Michael A. Smith Shin'ichi Satoh association method between images and closed caption sentences using a dynamic programming technique based on segments of the topics in the news programs.
Reference: 8. <author> Brunelli R, </author> <title> Poggio T (1997) Template matching: Matched spatial filters and beyond. </title> <booktitle> Pattern Recognition 30: </booktitle> <pages> 751-768 </pages>
Reference: 9. <author> Rowley HA, </author> <title> Kanade T (1994) Reconstructing 3-D blood vessel shapes from multiple X-ray images. </title> <booktitle> AAAI Workshop on Computer Vision for Medical Image Processing. </booktitle>
Reference-contexts: Another problem is the existence of complex backgrounds. Characters superimposed on news videos often have hue and brightness similar to the background, making extraction extremely difficult. For the cases of the license plate research (Cui and Huang 1997), the scene image research <ref> (Ohya et al. 1994) </ref> and the document image analysis (Wu et al. 1997), the difficulty of complex backgrounds is rarely a problem, although examples with plain backgrounds are illustrated. <p> Since every character has a unique shape, simple template matching methods are not adequate for extracting whole character patterns. It is assumed that, for characters composed of line elements, a line detection method <ref> (Rowley and Kanade 1994) </ref> is more applicable. We also have a problem with segmenting characters in low quality images. Although there are many approaches for character segmentation (Lu 1995), errors still occur because most methods analyze only a vertical projection profile. <p> Table 3 shows character recognition results of the conventional OCR. The recognition rate is 46.5%, which is almost half of our results and less than half of an average commercial OCR rate for documents <ref> (Information Science Research Institute 1994) </ref>. The recognition rate of Roman font characters is lower because of their thin lines which correspond to one or less pixel in the binary image.
Reference: 10. <editor> Lu Y (1995) Machine printed character segmentation an overview. </editor> <booktitle> Pattern Recognition 28: </booktitle> <pages> 67-80 </pages>
Reference: 11. <author> Lee SW, Lee DJ, </author> <title> Park HS (1996) A new methodology for grayscale character segmentation and recognition. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence 18: </journal> <pages> 1045-1050 </pages>
Reference: 12. <institution> Information Science Research Institute (1994) 1994 annual research report. </institution> <note> http://www.isri.unlv.edu/info/publications/anreps.html </note>
Reference: 13. <institution> Oxford university computing services (1997) The Oxford text archive. </institution> <note> http://ota.ox.ac.uk/ </note>
Reference: 14. <author> Hall PAV, </author> <title> Dowling GR (1980) Approximate string matching. </title> <journal> ACM Computing Surveys 12: </journal> <pages> 381-402 </pages>
Reference: 15. <author> Satoh S, </author> <title> Kanade T (1997) NAME-IT: Association of face and name in video. </title> <booktitle> Proceedings of IEEE Conference on Computer Vision and Pattern Recognition: </booktitle> <pages> 368-373 </pages>
Reference: 16. <author> Nakamura Y., </author> <title> Kanade T (1997) Semantic analysis for video contents extraction spotting by association in news video. ACM Multimedia 97 This article was processed by the author using the L a T E X style file cljour2 from Springer-Verlag. </title>
References-found: 16

