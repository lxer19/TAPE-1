URL: http://www.isp.pitt.edu/~rymon/papers/tele94.ps
Refering-URL: http://www.isp.pitt.edu/~rymon/pubs.html
Root-URL: 
Title: Automatic Cataloguing and Characterization of Earth Science Data Using SE-trees  
Author: Ron Rymon Nicholas M. Short Jr. 
Keyword: image classification, set-enumeration trees, artificial neural networks.  
Abstract: In the near future, NASA's Earth Observing System (EOS) platforms will produce enormous amounts of remote sensing image data that will be stored in the EOS Data Information System. For the past several years, the Intelligent Data Management group at Goddard's Information Science and technology Office/935 has been researching techniques for automatically cataloguing and characterizing image data (ADCC) from EOS into a distributed database (Cromp et al., 1992). The purpose of this work is to enable scientists to retrieve data based upon the contents of the imagery. The ability to automatically classify imagery is key to the success of contents-based search. We report results from experiments applying a novel machine learning framework, based on Set-Enumeration (SE) trees (Rymon, 1993), to the ADCC domain. Following the design of Chettri et al. (1992), we experiment with two images: one taken from the Blackhills region in South Dakota, the other from the Washington DC area. In a classical machine learning experimentation approach, an image's pixels are randomly partitioned into a training set (including ground truth or survey data) and a testing set. The prediction model is built using the pixels in the training set, and its performance estimated using the testing set. With the first Blackhills image, we perform various experiments achieving an accuracy level of 83.2%, compared to 72.7% reported by Chettri et al. using a Back Propagation Neural Network (BPNN), and 65.3% using a Gaussian Maximum Likelihood Classifier (GMLC). However, with the Washington DC image, we were only able to achieve 71.4%, compared with 67.7% reported for the BPNN model, and 62.3% for the GMLC. fl Ron Rymon is with Modeling Labs, 5600 Munhall Road, Suite 207, Pittsburgh, PA 15217; Telephone: (412) 422-2652; E-mail: Rymon@isp.pitt.edu. Part of this work was supported an ARO Grant DAAL03-89-C0031PRI, and an NLM grant R01-LM-05217, while the author was with the University of Pennsylvania. Nicholas M. Short Jr. is with Code 935, NASA/Goddard Space Flight Center, Greenbelt, MD 20771; Telephone: (301) 286-6604; E-mail: Short@dunloggin.gsfc.nasa.gov. This paper is a revised version of a paper presented at the Goddard Conference on Space Applications of Artificial Intelligence, Greenbelt, MD, May 1994. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J., Olshen, R., and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <type> Wadsworth, Belmont. 11 Buntine, </type> <institution> W. </institution> <year> (1990). </year> <title> Myths and Legends in Learning Classification Rules. </title> <booktitle> In Proceedings Eighth National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA, </address> <pages> pp. 736-742. </pages>
Reference: <author> Buntine, W., and Niblett, T. </author> <year> (1992). </year> <title> A Further Comparison of Splitting Rules for Decision-Tree Induction. </title> <journal> Machine Learning, </journal> <volume> 8(1), </volume> <pages> pp. 75-86. </pages>
Reference: <author> Campbell W. J., Cromp, R. F., and Hill, S. E. </author> <year> (1989). </year> <title> Automatic Labeling and Characterization of Objects Using Artificial Neural Networks. </title> <journal> Telematics and Informatics, 6(3-4):259-271. </journal>
Reference: <author> Cromp, R. F, Campbell, W. J., and Short, N. M. </author> <year> (1992). </year> <title> An Intelligent Information Fusion System for Handling the Archiving and Querying of Terabyte-sized Spatial Databases. </title> <booktitle> In Proceedings International Space Year Conference on Earth and Space Science Information Systems, </booktitle> <address> Pasadena CA. </address>
Reference: <author> Chettri, S. R., Cromp, R. F., and Birmingham M. </author> <year> (1992). </year> <title> Design of Neural Networks for Classification of Remotely Sensed Imagery. </title> <booktitle> In Proceedings Goddard Conference on Space Applications of Artificial Intelligence, </booktitle> <address> Greenbelt, MD. </address>
Reference: <author> Civco, D. </author> <year> (1989). </year> <title> Knowledge-Based Land Use and Land Cover Mapping, </title> <booktitle> In Proceedings AS-PRS/ACSM, Baltimore, MD, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 276-91. </pages>
Reference: <author> Mingers, J. </author> <year> (1989a). </year> <title> An Empirical Comparison of Selection Measures for Decision Tree Induction. </title> <journal> Machine Learning, </journal> <volume> 3(3), </volume> <pages> pp. 319-342. </pages>
Reference: <author> Mingers, J. </author> <year> (1989b). </year> <title> An Empirical Comparison of Pruning methods for Decision Tree Induction. </title> <journal> Machine Learning, </journal> <volume> 4(2), </volume> <pages> pp. 227-243. </pages>
Reference-contexts: Second, to reduce exploration, but also to reduce overfitting, we can take advantage of statistically-motivated pruning techniques developed for decision trees <ref> (Mingers, 1989b) </ref>. The exploration policy plays an even more important role in classification, where it is used to implement preference (bias). Since training data is often incomplete, a large number of classifiers may be consistent with it, yet differ significantly on new data.
Reference: <author> Mitchell, T. M. </author> <year> (1980). </year> <title> The Need for Biases in Learning Generalizations. </title> <type> Technical Report 5-110, </type> <institution> Rutgers University. </institution>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <pages> pp. 81-106. </pages>
Reference: <author> Rymon, R. </author> <year> (1992). </year> <title> Search through Systematic Set Enumeration. </title> <booktitle> In Proceedings Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <address> Cambridge MA, </address> <pages> pp. 539-550. </pages>
Reference: <author> Rymon, R. </author> <year> (1993). </year> <title> An SE-tree-based Characterization of the Induction Problem. </title> <booktitle> In Proceedings Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 268-275, </pages> <address> Amherst MA. </address>
Reference-contexts: In addition to simple truncation, of particular interest may be a variation of SE-Learn, described in <ref> (Rymon, 1993) </ref>, that learns consistent SE-trees, i.e., ones in which all paths matching a new instance are equally labeled.
Reference: <author> Rymon, R. </author> <year> (1994). </year> <title> On Kernel Rules and Prime Implicants. </title> <booktitle> To appear in Proceedings Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Empirically, it has been shown that SE-trees outperform decision trees on problems where relatively few observations are available for learning, as well as in noisy domains 3 . Furthermore, <ref> (Rymon, 1994) </ref> lays the theoretical ground for combining knowledge acquired from experts (in the form of rules or constraints) with knowledge induced from examples, in a single SE-tree-based framework. Experiments A prototypical system that implements a variation of SE-Learn has been built and tested on a number of domains.
Reference: <author> Utgoff, P. E. </author> <year> (1986). </year> <title> Machine Learning of Inductive Bias. </title> <publisher> Kluwer Academic, </publisher> <address> Boston MA. </address> <month> 12 </month>
References-found: 14

