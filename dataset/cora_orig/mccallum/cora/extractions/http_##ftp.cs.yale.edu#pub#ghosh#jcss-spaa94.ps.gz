URL: http://ftp.cs.yale.edu/pub/ghosh/jcss-spaa94.ps.gz
Refering-URL: http://ftp.cs.yale.edu/pub/ghosh/
Root-URL: http://www.cs.yale.edu
Email: ghosh@cs.yale.edu  muthu@dimacs.rutgers.edu  
Title: Dynamic Load Balancing by Random Matchings  
Author: Bhaskar Ghosh S. Muthukrishnan 
Note: Research supported by ONR under grant number 4-91-J-1576 and a Yale/IBM joint study. The research of this author was supported in part by NSF/DARPA under grant number CCR-89 06949 and by NSF under grant number CCR-91-03953.  
Address: P.O.Box 208285 New Haven, CT 06520  P.O.Box 1179  Piscataway, NJ 08855  
Affiliation: Department of Computer Science Yale University  DIMACS  Rutgers University  
Abstract-found: 0
Intro-found: 1
Reference: [AA+93] <author> W. Aiello, B. Awerbuch, B. Maggs, and S. Rao. </author> <title> Approximate Load Balancing on Dynamic and Asynchronous Networks. </title> <booktitle> In Proc. of 25th ACM Symp on Theory of Computing, </booktitle> <pages> 632-641, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Existing models for dynamic load balancing make one or more assumptions regarding communication in the underlying network. The focus of this paper concerns the degree of locality and the amount of parallelism in these models. Specifically, 1. Some existing models <ref> [AA+93, LM93, R91, C89] </ref> overestimate available parallelism in transferring load to neighbors by assuming that load can be moved from each processor to all its neighbors in parallel in each time step. <p> Also, in the IBM SP-1 and SP-2 machines [S+94, SP2], the messages have to traverse a single physical link sequentially to a network switching unit. 2. Several existing models <ref> [AA+93, LM93] </ref> underestimate available parallelism in link capacity by assuming that only one load unit can be transferred across a link at a time. This assumption overlooks a significant point. Communication links in most parallel computers (e.g. <p> These algorithms do not extend to arbitrary or dynamically changing topologies. For dynamically changing topologies, load balancing has been studied under assumptions on the pattern of failures for specific topologies [R89, AB92]. In a recent paper Aiello et al. <ref> [AA+93] </ref> present an algorithm for dynamic load balancing on arbitrary topologies under the assumption that one load unit can be moved across each link and that each processor can communicate with all its neighbors in one step. Note that this model is different from ours. The algorithm in [AA+93] takes O <p> et al. <ref> [AA+93] </ref> present an algorithm for dynamic load balancing on arbitrary topologies under the assumption that one load unit can be moved across each link and that each processor can communicate with all its neighbors in one step. Note that this model is different from ours. The algorithm in [AA+93] takes O ( 0 log (n 0 )=) steps to approximately balance the loads where the initial imbalance is 0 = max i jw i wj and is the vertex expansion of the underlying graph. (The vertex expansion of a graph with n nodes is defined to be min jSjn=2 <p> For our algorithm, the final imbalance f can be at most D (in contrast, f dD 8 in <ref> [AA+93] </ref>). This is because our algorithm stops making progress when the difference in the load on the endpoints of any edge is at most 1; thus, the maximum difference between any two processors in the network is at most D. <p> Our algorithm for PLS immediately implies an algorithm for job scheduling with the guarantee that E (ffi=) 2 =16d. This is the first known algorithm to make such a guarantee. In <ref> [AA+93] </ref>, a distance function (different from ) is used to measure the progress towards the load-balanced state in several steps. However, [AA+93] cannot guarantee a fractional decrease in the distance in every step since their argument involves amortization of the decrease in the distance over several steps. <p> This is the first known algorithm to make such a guarantee. In <ref> [AA+93] </ref>, a distance function (different from ) is used to measure the progress towards the load-balanced state in several steps. However, [AA+93] cannot guarantee a fractional decrease in the distance in every step since their argument involves amortization of the decrease in the distance over several steps.
Reference: [A86] <author> N. Alon. </author> <title> Eigenvalues and Expanders. </title> <journal> Combinatorica, </journal> <volume> Vol. 6, </volume> <pages> 83-96, </pages> <year> 1986. </year>
Reference-contexts: The matrix L = DA is the Laplacian Matrix of G. The eigenvalues of L are 0 = 1 &lt; 2 : : : n . The eigenvalue 2 is a widely studied parameter and it reflects the connectivity of the graph. (See <ref> [A86, MP92] </ref> for more on this.) We use the following well known facts about the eigenvalue 2 found, for example, in [MP92]. Fact 1. G is a connected graph if and only if 2 &gt; 0. Fact 2.
Reference: [AB92] <author> Y. Aumann and M. Ben-Or. </author> <title> Computing with Faulty Arrays. </title> <booktitle> In Proc of 24th ACM Symp on Theory of Computing, </booktitle> <pages> 162-169, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: These algorithms do not extend to arbitrary or dynamically changing topologies. For dynamically changing topologies, load balancing has been studied under assumptions on the pattern of failures for specific topologies <ref> [R89, AB92] </ref>. In a recent paper Aiello et al. [AA+93] present an algorithm for dynamic load balancing on arbitrary topologies under the assumption that one load unit can be moved across each link and that each processor can communicate with all its neighbors in one step.
Reference: [AHS91] <author> J. Aspnes, M. Herlihy and N. Shavit. </author> <title> Counting Networks and Multiprocessor Co-ordination. </title> <booktitle> In Proc of 23rd ACM Symp on Theory of Computing, </booktitle> <pages> 348-358, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Among analytical results, load balancing on specific topologies under statistical assumptions on input load distributions has been studied [HCT89]. For arbitrary initial load distributions, load balancing has been studied in specific topologies such as Counting Networks <ref> [AHS91, HLS92] </ref>, Hypercubes [P89], Meshes [HT93] and Expanders [PU89]. These algorithms do not extend to arbitrary or dynamically changing topologies. For dynamically changing topologies, load balancing has been studied under assumptions on the pattern of failures for specific topologies [R89, AB92].
Reference: [AS94] <author> N. Alon and J. Spencer. </author> <title> The Probabilistic Method. </title> <publisher> John-Wiley, </publisher> <year> 1994. </year>
Reference-contexts: Then applying Cher-noff's bounds <ref> [AS94] </ref>, we can pick k = O (k fl (log 0 + log n)) such that k &gt; 1 with probability polynomially small in 0 and n.
Reference: [B87] <author> B. Bollobas. </author> <title> Random Graphs. </title> <publisher> Academic Press, </publisher> <address> New York. </address> <year> 1987. </year>
Reference-contexts: This property ensures global convergence bounds. For choosing such a random matching, we draw upon the intuition from the very sparse phase in the evolution of random graphs <ref> [B87] </ref> as explained later. The rest of this section is organized as follows. Algorithm LR for real weights is described in Figures 1 and 2. <p> 8d = n 16 edges in M out of a total of at most nd=2 edges. 14 If we consider a random graph with edge probability 1 4n which corresponds to graphs with roughly n 8 edges, then most connected components in the graph are isolated edges or small trees <ref> [B87] </ref>. We have shown in the proof of Lemma 2 that removing the small trees still leaves us with enough isolated edges forming a matching of size roughly n 16 .
Reference: [BB87] <author> M. J. Berger and S. H. Bokhari. </author> <title> A Partitioning Strategy for Nonuniform Problems on Multiprocessors. </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-36, No. 5, </volume> <pages> 570-580, </pages> <year> 1987. </year>
Reference-contexts: Achieving balanced subdomains usually involves shifting the boundaries of adjoining subdomains so as to equalize the mesh points or elements in each subdomain. Further references on these areas can be found in <ref> [BB87, HT93, W91] </ref>. Clearly, our algorithm for the PLS problem can be used repeatedly on the quotient graph to solve the load balancing problem in adaptive mesh partitioning. Each load unit is a point or element in the mesh.
Reference: [Boi90] <author> J. E. Boillat. </author> <title> Load Balancing and Poisson Equation in a Graph. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 289-313, </pages> <year> 1990. </year>
Reference: [C89] <author> G. Cybenko. </author> <title> Dynamic Load Balancing for Distributed Memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 2, No. 7, </volume> <pages> 279-301, </pages> <year> 1989. </year>
Reference-contexts: Existing models for dynamic load balancing make one or more assumptions regarding communication in the underlying network. The focus of this paper concerns the degree of locality and the amount of parallelism in these models. Specifically, 1. Some existing models <ref> [AA+93, LM93, R91, C89] </ref> overestimate available parallelism in transferring load to neighbors by assuming that load can be moved from each processor to all its neighbors in parallel in each time step. <p> Broadly, there are two known paradigms for this scheduling. One paradigm [KR89] guarantees that each processor has at least one job to execute at the end of scheduling. The other paradigm <ref> [C89] </ref> guarantees that all processors have roughly the same number of jobs at the end of the scheduling step. <p> D are the maximum degree and the diameter of the underlying graph respectively. (Their algorithm stops making progress when the difference in the load on the endpoints of any edge is at most d; thus, the maximum difference between any two processors in the network is at most dD.) Cybenko <ref> [C89] </ref> considers a model for load movement similar to ours but additionally allows each processor to transfer load to all its neighbors in one time step. We call his model as the multiport model. <p> He presents a local load balancing algorithm and gives necessary and sufficient conditions for which his algorithm converges on arbitrary graphs. Later (in Section 4) we will compare our bounds with the ones in <ref> [C89] </ref>. 2.5 Our Results and Our Approach We present a local randomized algorithm for the PLS problem such that E ( ffi ) 2 when is sufficiently large. <p> Remark 1. For intuition, we make several comments comparing this result to that in <ref> [C89] </ref> on the multiport model. <p> Here M is a doubly stochastic symmetric matrix (for technical reasons, see <ref> [C89] </ref>). From the results in [C89], we can conclude that repeated application of such 23 steps roughly O ( log 0 1fi 2 ) times yields an algorithm for load-balancing; here, fi is the second largest eigenvalue of M in magnitude 1 . <p> Here M is a doubly stochastic symmetric matrix (for technical reasons, see <ref> [C89] </ref>). From the results in [C89], we can conclude that repeated application of such 23 steps roughly O ( log 0 1fi 2 ) times yields an algorithm for load-balancing; here, fi is the second largest eigenvalue of M in magnitude 1 . In what follows, we compare this result with ours. Comment 1. <p> So, the second largest eigenvalue of I L M in magnitude, namely fi, is precisely 1. That gives the trivial bound of infinity on the number of times Algorithm LR is invoked to balance load. Thus, the approach in <ref> [C89] </ref> does not give a non-trivial analysis for our algorithm. Comment 2. Given some preprocessing we can directly utilize this result on the matching model. <p> Simulating that on the matching model takes O ( d 2 log 0 2 ) steps as remarked above. In contrast, from Theorem 5 it follows that our algorithm takes only O ( d log 0 2 ) steps for sufficiently large 0 . 1 The analysis in <ref> [C89] </ref> does not directly extend to the case when the weights are integral. Recently, the analysis there has been extended to handle the integral weights as well [GMS95]. 24 Remark 2. The bound in Theorem 5 holds for an arbitrary graph. <p> The bound in Theorem 5 holds for an arbitrary graph. This compares poorly with the bounds known for some specific graphs. For example, on 2-dim meshes and hypercubes, load balancing can be done (using <ref> [C89] </ref>) in O (D) steps on the matching model where D is the diameter of the graph.
Reference: [E+86] <author> D. Eager, E. Lazowska and J. Zahorjan. </author> <title> Adaptive Load Sharing in Homogeneous Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-12, No. 5, </volume> <pages> 662-675, </pages> <year> 1986. </year>
Reference-contexts: In such applications, several tasks or load units can be packaged together to be sent as one long message and thus make better use of the communication bandwidth. 3. A few existing models allow non-local load movement [LM93] and global control <ref> [E+86, LK87] </ref>. However global communication and routing are expensive on most parallel and distributed computers. Furthermore, algorithms which rely on global information become even more expensive while adjusting to link failures. <p> Thus, scheduling turns out to be expensive. For more on these two existing paradigms, see <ref> [LK87, E+86, NX+85, Sta84] </ref>. Since these two known paradigms for job scheduling are expensive, we introduce an alternate paradigm of restricting algorithms to perform load movement only between neighbors, but requiring a guarantee of reasonable progress towards the load-balanced state.
Reference: [F93] <author> R. Feldmann. </author> <title> Game Tree Search on Massively Parallel Systems. </title> <type> PhD Thesis, </type> <institution> Dept. of Mathematics and Computer Science, University of Paderborn. </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: For this reason, a common approach to reduce communication costs is to send few long messages rather than several short ones. There are large classes of important load balancing problems (e.g. fine grain functional programming [GH89], game tree searching <ref> [F93] </ref> and adaptive mesh partitioning [W91]) where the load units or tasks that have to 3 be moved are of small size (order of tens of bytes).
Reference: [GL+95] <author> B. Ghosh, T. Leighton, B. Maggs, S. Muthukrishnan, G. Plaxton, R. Ra-jaraman, A. Richa, R. Tarjan and D. Zuckerman. </author> <title> Tight Analyses of Two Local Load Balancing Algorithms. </title> <booktitle> In Proc. of 27th ACM Symp on Theory of Computing, </booktitle> <pages> 548-558, </pages> <month> May </month> <year> 1995. </year> <month> 29 </month>
Reference-contexts: Appropriately applying the approach in this paper, we can analyze a simple local algorithm in that scenario as well. However we do not include the details of that here since an improved, in fact, an optimal analysis of that algorithm has been recently given <ref> [GL+95] </ref> using a substantially different approach. We have proved that our algorithm and the analysis for the PLS problem is optimal in the matching model.
Reference: [GMS95] <author> B. Ghosh, S. Muthukrishnan and M. Schultz. </author> <title> On first and second order meth-ods for load balancing. </title> <type> Manuscript, </type> <year> 1995. </year>
Reference-contexts: Recently, the analysis there has been extended to handle the integral weights as well <ref> [GMS95] </ref>. 24 Remark 2. The bound in Theorem 5 holds for an arbitrary graph. This compares poorly with the bounds known for some specific graphs.
Reference: [GH89] <author> B. Goldberg and P. Hudak. </author> <title> Implementing Functional Programs on a Hypercube Multiprocessor. </title> <booktitle> In Proc. of the 4th Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <volume> Vol. 1, </volume> <pages> 489-503, </pages> <year> 1989. </year>
Reference-contexts: For this reason, a common approach to reduce communication costs is to send few long messages rather than several short ones. There are large classes of important load balancing problems (e.g. fine grain functional programming <ref> [GH89] </ref>, game tree searching [F93] and adaptive mesh partitioning [W91]) where the load units or tasks that have to 3 be moved are of small size (order of tens of bytes).
Reference: [HCT89] <author> J. Hong, M. Chen and X. Tan. </author> <title> Dynamic Cyclic Load Balancing on Hyper-cubes. </title> <booktitle> In Proc. of the 4th Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <volume> Vol. 1, </volume> <pages> 595-598, </pages> <year> 1989. </year>
Reference-contexts: Among analytical results, load balancing on specific topologies under statistical assumptions on input load distributions has been studied <ref> [HCT89] </ref>. For arbitrary initial load distributions, load balancing has been studied in specific topologies such as Counting Networks [AHS91, HLS92], Hypercubes [P89], Meshes [HT93] and Expanders [PU89]. These algorithms do not extend to arbitrary or dynamically changing topologies.
Reference: [HLS92] <author> M. Herlihy, B. Lim, and N. Shavit. </author> <title> Low Contention Load Balancing on Large-Scale Multiprocessors. </title> <booktitle> In Proc. of 4th ACM Symp on Parallel Algorithms and Architectures, </booktitle> <pages> 219-227, </pages> <year> 1992. </year>
Reference-contexts: Among analytical results, load balancing on specific topologies under statistical assumptions on input load distributions has been studied [HCT89]. For arbitrary initial load distributions, load balancing has been studied in specific topologies such as Counting Networks <ref> [AHS91, HLS92] </ref>, Hypercubes [P89], Meshes [HT93] and Expanders [PU89]. These algorithms do not extend to arbitrary or dynamically changing topologies. For dynamically changing topologies, load balancing has been studied under assumptions on the pattern of failures for specific topologies [R89, AB92].
Reference: [HT93] <author> A. Heirich and S. Taylor. </author> <title> A Parabolic Theory of Load Balance. </title> <institution> Research Report Caltech-CS-TR-93-25, Caltech Scalable Concurrent Computation Lab, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Among analytical results, load balancing on specific topologies under statistical assumptions on input load distributions has been studied [HCT89]. For arbitrary initial load distributions, load balancing has been studied in specific topologies such as Counting Networks [AHS91, HLS92], Hypercubes [P89], Meshes <ref> [HT93] </ref> and Expanders [PU89]. These algorithms do not extend to arbitrary or dynamically changing topologies. For dynamically changing topologies, load balancing has been studied under assumptions on the pattern of failures for specific topologies [R89, AB92]. <p> Achieving balanced subdomains usually involves shifting the boundaries of adjoining subdomains so as to equalize the mesh points or elements in each subdomain. Further references on these areas can be found in <ref> [BB87, HT93, W91] </ref>. Clearly, our algorithm for the PLS problem can be used repeatedly on the quotient graph to solve the load balancing problem in adaptive mesh partitioning. Each load unit is a point or element in the mesh. <p> For example, although our solution indicates how many mesh points must be moved between the nodes, it does not determine which mesh points must be moved. However iterative algorithms such as the one we suggest are frequently used in adaptive mesh partitioning in practice (for example, see <ref> [HT93, WB92, WCE95] </ref>).
Reference: [II86] <author> A. Israeli and A. Itai. </author> <title> A Fast and Simple Randomized Parallel Algorithm for Maximal Matching. </title> <journal> Information Processing Letters, </journal> <volume> Volume 22, </volume> <pages> 77-80, </pages> <year> 1986. </year>
Reference-contexts: Hence it stays marked A at the end of Step 3. A similar proof holds for j. Therefore, the other direction of the claim Y follows as well. That completes the proof of the two claims and hence the lemma. Remark. Distributed/Parallel algorithms for determining the maximal <ref> [II86, L86] </ref> matchings of a given graph work by iteratively adding a random matching to a current matching. The manner in which a random matching is chosen there in each such step seems somewhat similar to our Matching Step. Analysis of Algorithm LR.
Reference: [IS86] <author> A. Israeli and Y. Shiloach. </author> <title> An Improved Parallel Algorithm for Maximal Matching. </title> <journal> Information Processing Letters, </journal> <volume> Volume 22, </volume> <pages> 57-60, </pages> <year> 1986. </year>
Reference: [KR89] <author> V. Kumar and V. Rao. </author> <title> Load Balancing on the Hypercube Architecture. </title> <booktitle> In Proc. of the 4th Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <volume> Vol. 1, </volume> <pages> 603-608, </pages> <year> 1989. </year>
Reference-contexts: In order to increase the throughput of the machine, a common practice is to interleave this execution with steps that schedule available jobs to underloaded or idle processors. Broadly, there are two known paradigms for this scheduling. One paradigm <ref> [KR89] </ref> guarantees that each processor has at least one job to execute at the end of scheduling. The other paradigm [C89] guarantees that all processors have roughly the same number of jobs at the end of the scheduling step.
Reference: [L86] <author> M. Luby. </author> <title> A Simple Parallel Algorithm for the Maximal Independent Set Problem. </title> <journal> SIAM J. on Computing, </journal> <volume> Volume 4, </volume> <pages> 1036-1053, </pages> <year> 1986. </year>
Reference-contexts: Hence it stays marked A at the end of Step 3. A similar proof holds for j. Therefore, the other direction of the claim Y follows as well. That completes the proof of the two claims and hence the lemma. Remark. Distributed/Parallel algorithms for determining the maximal <ref> [II86, L86] </ref> matchings of a given graph work by iteratively adding a random matching to a current matching. The manner in which a random matching is chosen there in each such step seems somewhat similar to our Matching Step. Analysis of Algorithm LR.
Reference: [LK87] <author> F. C. H. Lin and R. M. Keller. </author> <title> The Gradient Model Load Balancing Method. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 13, No. 1, </volume> <pages> 32-38, </pages> <year> 1987. </year>
Reference-contexts: In such applications, several tasks or load units can be packaged together to be sent as one long message and thus make better use of the communication bandwidth. 3. A few existing models allow non-local load movement [LM93] and global control <ref> [E+86, LK87] </ref>. However global communication and routing are expensive on most parallel and distributed computers. Furthermore, algorithms which rely on global information become even more expensive while adjusting to link failures. <p> Thus, scheduling turns out to be expensive. For more on these two existing paradigms, see <ref> [LK87, E+86, NX+85, Sta84] </ref>. Since these two known paradigms for job scheduling are expensive, we introduce an alternate paradigm of restricting algorithms to perform load movement only between neighbors, but requiring a guarantee of reasonable progress towards the load-balanced state.
Reference: [LM93] <author> R. Lueling and B. Monien. </author> <title> A Dynamic Distributed Load Balancing Algorithm with Provable Good Performance. </title> <booktitle> In Proc. of 5th ACM Symp on Parallel Algorithms and Architectures, </booktitle> <pages> 164-172, </pages> <year> 1993. </year>
Reference-contexts: Existing models for dynamic load balancing make one or more assumptions regarding communication in the underlying network. The focus of this paper concerns the degree of locality and the amount of parallelism in these models. Specifically, 1. Some existing models <ref> [AA+93, LM93, R91, C89] </ref> overestimate available parallelism in transferring load to neighbors by assuming that load can be moved from each processor to all its neighbors in parallel in each time step. <p> Also, in the IBM SP-1 and SP-2 machines [S+94, SP2], the messages have to traverse a single physical link sequentially to a network switching unit. 2. Several existing models <ref> [AA+93, LM93] </ref> underestimate available parallelism in link capacity by assuming that only one load unit can be transferred across a link at a time. This assumption overlooks a significant point. Communication links in most parallel computers (e.g. <p> In such applications, several tasks or load units can be packaged together to be sent as one long message and thus make better use of the communication bandwidth. 3. A few existing models allow non-local load movement <ref> [LM93] </ref> and global control [E+86, LK87]. However global communication and routing are expensive on most parallel and distributed computers. Furthermore, algorithms which rely on global information become even more expensive while adjusting to link failures.
Reference: [LMR91] <author> R. Lueling, B. Monien and F. Ramme. </author> <title> Load Balancing in Large Networks: </title>
Reference-contexts: Almost all research has focused on algorithms for specific topologies and/or rely on global routing phases. 7 A class of such research has involved performance analysis of load balancing algorithms by simulations <ref> [LMR91] </ref>. Among analytical results, load balancing on specific topologies under statistical assumptions on input load distributions has been studied [HCT89]. For arbitrary initial load distributions, load balancing has been studied in specific topologies such as Counting Networks [AHS91, HLS92], Hypercubes [P89], Meshes [HT93] and Expanders [PU89].
References-found: 24

