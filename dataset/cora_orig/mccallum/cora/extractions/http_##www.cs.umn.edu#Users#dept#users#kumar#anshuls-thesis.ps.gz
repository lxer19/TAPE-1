URL: http://www.cs.umn.edu/Users/dept/users/kumar/anshuls-thesis.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Author: Vipin Kumar Date 
Degree: Name of Faculty Adviser Signature of Faculty Adviser  
Note: This is to certify that I have examined this bound copy of a doctoral thesis by Anshul Gupta and have found that it is complete and satisfactory in all respects, and that any and all revisions required by the final examining committee have been made.  
Affiliation: UNIVERSITY OF MINNESOTA  GRADUATE SCHOOL  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Abraham and K. Padmanabhan. </author> <title> Performance of multicomputer networks under pin-out constraints. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 237-248, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: If x &lt; 0:5, then the overall isoefficiency is determined by communication overheads, and is exponential. If x 0:5, then the overall isoefficiency is determined by concurrency. Thus, the best isoefficiency function of 2. p 1:5 log p/ can be obtained at x D :5. Many researchers <ref> [33, 124, 2, 1] </ref> prefer to compare architectures while keeping the number of communication ports per processor (as opposed to bisection width) the same across the architectures.
Reference: [2] <author> Anant Agarwal. </author> <title> Limits on interconnection network performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2 </volume> <pages> 398-412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: If x &lt; 0:5, then the overall isoefficiency is determined by communication overheads, and is exponential. If x 0:5, then the overall isoefficiency is determined by concurrency. Thus, the best isoefficiency function of 2. p 1:5 log p/ can be obtained at x D :5. Many researchers <ref> [33, 124, 2, 1] </ref> prefer to compare architectures while keeping the number of communication ports per processor (as opposed to bisection width) the same across the architectures.
Reference: [3] <author> Alok Aggarwal, Ashok K. Chandra, and Mark Snir. </author> <title> Communication complexity of PRAMs. </title> <type> Technical Report RC 14998 (No. 64644), </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, Yorktown Heights, NY, </address> <year> 1989. </year>
Reference: [4] <author> A. V. Aho, John E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: Hence, there has been a great interest in implementing FFT on parallel computers [11, 17, 29, 56, 72, 106, 133, 13, 74, 19, 25, 3]. 3.1.1 The FFT Algorithm radix-2 FFT adapted from <ref> [4, 116] </ref>. X is the input vector of length n (n = 2 r for some integer r) and Y is its Fourier Transform. ! k denotes the complex number e j2=nk , where j = p 1.
Reference: [5] <author> S. G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: For example, for multiplying two N fi N matrices using Fox's parallel matrix multiplication algorithm [37], W D N 3 and C .W / D N 2 D W 2=3 . It is easily seen that if the processor-time product <ref> [5] </ref> is 2.W / (i.e., the algorithm is cost-optimal), then C .W / D O.W /. Maximum Number of Processors Usable, p max : The number of processors that yield maximum speedup S max for a given W . <p> r1 /] := S [.b 0 b l1 0b lC1 b r1 /] C ! .b l b l1 b 0 00/ S [.b 0 b l1 1b lC1 b r1 /]; 10. end; The Binary-Exchange Algorithm In the most commonly used mapping that minimizes communication for the binary-exchange algorithm <ref> [81, 5, 11, 17, 29, 72, 106, 133, 116, 94] </ref>, if .b 0 b 1 b r1 / is the binary representation of i, then for all i, R [i] and S [i] are mapped to processor number .b 0 b d1 /.
Reference: [6] <author> Edward Anderson. </author> <title> Parallel implementation of preconditioned conjugate gradient methods for solving sparse systems of linear equations. </title> <type> Technical Report 805, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1988. </year>
Reference-contexts: As a result there has been a great deal of interest in implementing the Conjugate Gradient algorithm on parallel computers <ref> [6, 12, 62, 73, 79, 101, 122, 138, 139] </ref>.
Reference: [7] <author> Cleve Ashcraft. </author> <title> The domain/segment partition for the factorization of sparse symmetric positive definite matrices. </title> <type> Technical Report ECA-TR-148, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, WA, </address> <year> 1990. </year>
Reference-contexts: A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [120, 119, 7, 104, 140, 49] </ref>, and the total communication volume in the best of these schemes [120, 119] is 2.N p 94 Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume.
Reference: [8] <author> Cleve Ashcraft. </author> <title> The fan-both family of column-based distributed cholesky factorization algorithms. </title> <editor> In A. George, John R. Gilbert, and J. W.-H. Liu, editors, </editor> <title> Graph Theory and Sparse Matrix Computations. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Since the overall computation is only 2.N 1:5 / [45], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased [123, 120]. In <ref> [8] </ref>, Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of 2.N p p log N /.
Reference: [9] <author> Cleve Ashcraft, S. C. Eisenstat, and J. W.-H. Liu. </author> <title> A fan-in algorithm for distributed sparse numerical factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11 </volume> <pages> 593-599, </pages> <year> 1990. </year>
Reference-contexts: Our single processor run times are four times less than the single processor run times on iPSC/2 reported in <ref> [9] </ref>.
Reference: [10] <author> Cleve Ashcraft, S. C. Eisenstat, J. W.-H. Liu, and A. H. Sherman. </author> <title> A comparison of three column based distributed sparse factorization schemes. </title> <type> Technical Report YALEU/DCS/RR-810, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1990. </year> <booktitle> Also appears in Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1991. </year>
Reference-contexts: However, the broadcast takes log p steps of 2.m/ time each; hence, the total communication overhead is 2.m p log p/ (on a hypercube). In the context of matrix factorization, the experimental study by Ashcraft <ref> [10] </ref> serves to demonstrate the importance of studying the total communication overhead rather than volume. In [10], the fan-in algorithm, which has a lower communication volume than the distributed multifrontal algorithm, has a higher overhead (and hence, a lower efficiency) than the multifrontal algorithm for the same distribution of the matrix <p> In the context of matrix factorization, the experimental study by Ashcraft <ref> [10] </ref> serves to demonstrate the importance of studying the total communication overhead rather than volume. In [10], the fan-in algorithm, which has a lower communication volume than the distributed multifrontal algorithm, has a higher overhead (and hence, a lower efficiency) than the multifrontal algorithm for the same distribution of the matrix among the processors.
Reference: [11] <author> A. Averbuch, E. Gabber, B. Gordissky, and Y. Medan. </author> <title> A parallel FFT on an MIMD machine. </title> <journal> Parallel Computing, </journal> <volume> 15 </volume> <pages> 61-74, </pages> <year> 1990. </year> <month> 152 </month>
Reference-contexts: r1 /] := S [.b 0 b l1 0b lC1 b r1 /] C ! .b l b l1 b 0 00/ S [.b 0 b l1 1b lC1 b r1 /]; 10. end; The Binary-Exchange Algorithm In the most commonly used mapping that minimizes communication for the binary-exchange algorithm <ref> [81, 5, 11, 17, 29, 72, 106, 133, 116, 94] </ref>, if .b 0 b 1 b r1 / is the binary representation of i, then for all i, R [i] and S [i] are mapped to processor number .b 0 b d1 /.
Reference: [12] <author> Cevdet Aykanat, Fusun Ozguner, Fikret Ercal, and Ponnuswamy Sadayappan. </author> <title> Iterative algorithms for solution of large sparse systems of linear equations on hypercubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1554-1567, </pages> <year> 1988. </year>
Reference-contexts: As a result there has been a great deal of interest in implementing the Conjugate Gradient algorithm on parallel computers <ref> [6, 12, 62, 73, 79, 101, 122, 138, 139] </ref>.
Reference: [13] <author> David H. Bailey. </author> <title> FFTs in external or hierarchical memory. </title> <journal> The Journal of Supercomputing, </journal> <volume> 4 </volume> <pages> 23-35, </pages> <year> 1990. </year>
Reference: [14] <author> Stephen T. Barnard and Horst D. Simon. </author> <title> A fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems. </title> <type> Technical Report RNR-92-033, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: A drawback of using a serial implementation of SND is that its run time is too high. However, variations of SND such as multilevel SND <ref> [14, 113] </ref> run much faster without compromising on the quality of ordering. 121 From the experimental results in Tables 4.1 and 4.2, we can infer that our algorithm can deliver substantial speedups, even on moderate problem sizes.
Reference: [15] <author> M. L. Barton and G. R. Withers. </author> <title> Computing performance as a function of the speed, quantity, and the cost of processors. </title> <booktitle> In Supercomputing '89 Proceedings, </booktitle> <pages> pages 759-764, </pages> <year> 1989. </year>
Reference-contexts: This should be contrasted with the conventional wisdom that suggests that better performance is always obtained using fewer faster processors <ref> [15] </ref>. 3.2.6 Experimental Results We verified a part of the analysis of Section 3.2.3 through experiments on the CM-5 parallel computer. On this machine, the fat-tree [89] like communication network on the CM-5 provides simultaneous paths for communication between all pairs of processors.
Reference: [16] <author> Jarle Berntsen. </author> <title> Communication efficient matrix multiplication on hypercubes. </title> <journal> Parallel Computing, </journal> <volume> 12 </volume> <pages> 335-342, </pages> <year> 1989. </year>
Reference-contexts: Berntsen's Algorithm Due to nearest neighbor communications on the p p p wrap-around array of processors, Cannon's algorithm's performance is the same on both mesh and hypercube architectures. In <ref> [16] </ref>, Berntsen describes an algorithm which exploits greater connectivity provided by a hypercube. The algorithm uses p D 2 3q processors with the restriction that p n 3=2 for multiplying two n fi n matrices A and B. <p> The hypercube is split into 2 q subcubes, each performing a submatrix multiplication between submatrices of A of size n=2 q fi n=2 2q and submatrices of B of size n=2 2q fi n=2 q using Cannon's algorithm. It is shown in <ref> [16] </ref> that the time spent in data communication by this algorithm on a hypercube is 2t s p 1=3 C .t s log p/=3 C 3t w n 2 = p 2=3 , and hence the total parallel execution time is given by the following equation: T p D p 1
Reference: [17] <author> S. Bershader, T. Kraay, and J. Holland. </author> <booktitle> The giant-Fourier-transform. In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications: </booktitle> <volume> Volume I, </volume> <pages> pages 387-389, </pages> <year> 1989. </year>
Reference-contexts: r1 /] := S [.b 0 b l1 0b lC1 b r1 /] C ! .b l b l1 b 0 00/ S [.b 0 b l1 1b lC1 b r1 /]; 10. end; The Binary-Exchange Algorithm In the most commonly used mapping that minimizes communication for the binary-exchange algorithm <ref> [81, 5, 11, 17, 29, 72, 106, 133, 116, 94] </ref>, if .b 0 b 1 b r1 / is the binary representation of i, then for all i, R [i] and S [i] are mapped to processor number .b 0 b d1 /.
Reference: [18] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: The latter is true when any algorithm with a global operation (such as broadcast, and one-to-all and all-to-all personalized communication <ref> [18, 71] </ref>) is implemented on a parallel architecture that has a message passing latency or message startup time.
Reference: [19] <author> Edward C. Bronson, Thomas L. Casavant, and L. H. Jamieson. </author> <title> Experimental application-driven architecture analysis of an SIMD/MIMD parallel processing system. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 195-205, </pages> <year> 1990. </year>
Reference: [20] <author> W. J. Camp, S. J. Plimpton, B. A. Hendrickson, and R. W. Leland. </author> <title> Massively parallel methods for engineering and science problems. </title> <journal> Communications of the ACM, </journal> <volume> 37(4) </volume> <pages> 31-41, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This choice of the ordering scheme was prompted by two factors. First, there is increasing evidence that spectral orderings offer a good balance between generality of application and the quality of ordering both in terms of load balance and fill reduction <ref> [20] </ref>. Second, the SND algorithm itself can be parallelized efficiently, whereas most other ordering schemes do not appear to be as well-suited for parallelization.
Reference: [21] <author> L. E. Cannon. </author> <title> A cellular computer to implement the Kalman Filter Algorithm. </title> <type> PhD thesis, </type> <institution> Montana State University, Bozman, MT, </institution> <year> 1969. </year>
Reference-contexts: The memory requirement for each processor is 2.n 2 = p and thus the total memory requirement is 2.n 2 p p/ words as against 2.n 2 / for the sequential algorithm. Cannon's Algorithm A parallel algorithm that is memory efficient and is frequently used is due to Cannon <ref> [21] </ref>. Again the two n fi n matrices A and B are divided into square submatrices of size n= p p p among the p processors of a wrap-around mesh (which can be embedded in a hypercube if the algorithm was to be implemented on it).
Reference: [22] <author> S. Chandran and Larry S. Davis. </author> <title> An approach to parallel vision algorithms. </title> <editor> In R. Porth, editor, </editor> <booktitle> Parallel Processing. </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1987. </year>
Reference-contexts: Clearly, this can be done only for scalable parallel systems, which are exactly the ones for which a fixed efficiency can be maintained for arbitrarily large p by simply increasing the problem size. For such systems, it is natural to use isoefficiency function or related metrics <ref> [85, 80, 22] </ref>. The analyses in [148, 149, 36, 97, 105, 134, 34] also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other.
Reference: [23] <author> V. Cherkassky and R. Smith. </author> <title> Efficient mapping and implementations of matrix algorithms on a hypercube. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 7-27, </pages> <year> 1988. </year>
Reference: [24] <author> N. P. Chrisopchoides, M. Aboelaze, E. N. Houstis, and C. E. Houstis. </author> <title> The parallelization of some level 2 and 3 BLAS operations on distributed-memory machines. </title> <booktitle> In Proceedings of the First International Conference of the Austrian Center of Parallel Computation. Springer-Verlag Series Lecture Notes in Computer Science, </booktitle> <year> 1991. </year> <month> 153 </month>
Reference: [25] <author> Z. Cvetanovic. </author> <title> Performance analysis of the FFT algorithm on a shared-memory parallel architecture. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 31(4) </volume> <pages> 435-451, </pages> <year> 1987. </year>
Reference: [26] <author> William J. Dally. </author> <title> Wire-efficienct VLSI multiprocessor communication network. </title> <booktitle> In Stanford Conference on Advanced Research in VLSI Networks, </booktitle> <pages> pages 391-415, </pages> <year> 1987. </year>
Reference-contexts: However, if the cost of the network is considered to be a function of the bisection width of the network, as may be the case in VLSI implementations <ref> [26] </ref>, then the picture improves for the mesh. The bisection widths of a hypercube and a mesh containing p processors each are p=2 and p p respectively.
Reference: [27] <author> Eric F. Van de Velde. </author> <title> Multicomputer matrix computations: Theory and practice. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 1303-1308, </pages> <year> 1989. </year>
Reference: [28] <author> Eliezer Dekel, David Nassimi, and Sartaj Sahni. </author> <title> Parallel matrix and graph algorithms. </title> <journal> SIAM Journal on Computing, </journal> <volume> 10 </volume> <pages> 657-673, </pages> <year> 1981. </year>
Reference-contexts: The DNS Algorithm One Element Per Processor Version An algorithm that uses a hypercube with p D n 3 D 2 3q processors to multiply two n fi n matrices was proposed by Dekel, Nassimi and Sahni in <ref> [28, 118] </ref>. The p processors can be visualized as being arranged in an 2 q fi2 q fi2 q array. <p> There are more than one ways to adapt this algorithm to use fewer than n 3 processors. The method proposed by Dekel, Nassimi, and Sahni in <ref> [28, 118] </ref> is as follows. Variant with More than One Element Per Processor This variant of the DNS algorithm can work with n 2 r processors, where 1 &lt; r &lt; n, thus using one processor for more than one element of each of the two n fi n matrices.
Reference: [29] <author> Laurent Desbat and Denis Trystram. </author> <title> Implementing the discrete Fourier transform on a hypercube vector-parallel computer. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications: </booktitle> <volume> Volume I, </volume> <pages> pages 407-410, </pages> <year> 1989. </year>
Reference-contexts: r1 /] := S [.b 0 b l1 0b lC1 b r1 /] C ! .b l b l1 b 0 00/ S [.b 0 b l1 1b lC1 b r1 /]; 10. end; The Binary-Exchange Algorithm In the most commonly used mapping that minimizes communication for the binary-exchange algorithm <ref> [81, 5, 11, 17, 29, 72, 106, 133, 116, 94] </ref>, if .b 0 b 1 b r1 / is the binary representation of i, then for all i, R [i] and S [i] are mapped to processor number .b 0 b d1 /.
Reference: [30] <author> Iain S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Users' guide for the Harwell-Boeing sparse matrix collection (release I). </title> <type> Technical Report TR/PA/92/86, </type> <institution> Research and Technology Division, Boeing Computer Services, </institution> <address> Seattle, WA, </address> <year> 1992. </year>
Reference-contexts: We have been able to achieve speedups of up to 364 on 1024 processors and 230 on 512 processors over a highly efficient sequential implementation for moderately sized problems from the Harwell-Boeing collection <ref> [30] </ref>. In [77], we have applied this algorithm to obtain a highly scalable parallel formulation of interior point algorithms and have observed significant speedups in solving linear programming problems. <p> However, despite these inefficiencies, our implementation is more scalable than a hypothetical ideal implementation (with perfect load balance) of the previously best known parallel algorithm for sparse Cholesky factorization. In Table 4.2 we summarize the results of factoring some matrices from the Harwell-Boeing collection of sparse matrices <ref> [30] </ref>. The purpose of these experiments was to to demonstrate that our algorithm can deliver good speedups on hundreds of processors for practical problems. Spectral 119 Table 4.2: Experimental results for factoring some sparse symmetric positive definite matrices resulting from 3-D problems in structural engineering. All times are in seconds.
Reference: [31] <author> Iain S. Duff and J. K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 9 </volume> <pages> 302-325, </pages> <year> 1983. </year>
Reference-contexts: Therefore, we describe this algorithm in detail first, before we discuss triangular solution and symbolic factorization (in that order). 4.3 The Serial Multifrontal Algorithm for Sparse Matrix Factorization The multifrontal algorithm for sparse matrix factorization was proposed independently, and in somewhat different forms, by Speelpening [126] and Duff and Reid <ref> [31] </ref>, and later elucidated in a tutorial by Liu [92]. In this section, we briefly describe a condensed version of multifrontal sparse Cholesky factorization. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as shown in Figure 4.3.
Reference: [32] <author> Iain S. Duff and J. K. Reid. </author> <title> The multifrontal solution of unsymmetric sets of linear equations. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 5(3) </volume> <pages> 633-641, </pages> <year> 1984. </year>
Reference-contexts: Although we focus on Cholesky factorization of symmetric positive definite matrices in this 92 chapter, the methodology developed here can be adapted for performing Gaussian elimination on diagonally dominant matrices that are almost symmetric in structure <ref> [32] </ref> and for solving sparse linear least squares problems [99]. 4.1 Earlier Research in Sparse Matrix Factorization and Our Contribution Since sparse matrix factorization is the most time consuming phase in the direct solution of a sparse system of linear equations, there has been considerable interest in developing its parallel formulations. <p> can be developed for some other forms of sparse matrix factorization. 4.11 Application to Gaussian Elimination and QR Factorization Although we have focussed on sparse Cholesky factorization in this chapter, the serial algorithm of Figure 4.3 can be generalized to Gaussian elimination without pivoting for nearly structurally symmetric sparse matrices <ref> [32] </ref> and for solving sparse linear least squares problems [99].
Reference: [33] <author> Shantanu Dutt and Nam Trinh. </author> <title> Analysis of k-ary n-cubes for the class of parallel divide-and-conquer algorithms. </title> <type> Technical report, </type> <institution> Department of Electrical Engineering, University of Minnesota, Minneapolis, MN, </institution> <year> 1995. </year>
Reference-contexts: If x &lt; 0:5, then the overall isoefficiency is determined by communication overheads, and is exponential. If x 0:5, then the overall isoefficiency is determined by concurrency. Thus, the best isoefficiency function of 2. p 1:5 log p/ can be obtained at x D :5. Many researchers <ref> [33, 124, 2, 1] </ref> prefer to compare architectures while keeping the number of communication ports per processor (as opposed to bisection width) the same across the architectures. <p> Many researchers [33, 124, 2, 1] prefer to compare architectures while keeping the number of communication ports per processor (as opposed to bisection width) the same across the architectures. Dutt and Trinh <ref> [33] </ref> show that for FFT-like computations, hypercubes are more cost-effective even with this cost criterion. 3.1.7 Experimental Results We implemented the binary-exchange algorithm for unordered single dimensional radix-2 FFT on a 1024-node nCUBE1 hypercube.
Reference: [34] <author> D. L. Eager, J. Zahorjan, and E. D. Lazowska. </author> <title> Speedup versus efficiency in parallel systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(3) </volume> <pages> 408-423, </pages> <year> 1989. </year>
Reference-contexts: For such systems, it is natural to use isoefficiency function or related metrics [85, 80, 22]. The analyses in <ref> [148, 149, 36, 97, 105, 134, 34] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other. <p> Clearly, T iso P .W / can be no better than T min P .W /. Several researchers have proposed to use an operating point where the value of p.T P / r is minimized for some constant r and for a given problem size W <ref> [36, 34, 134] </ref>. It can be shown [134] that this corresponds to the point where E S r1 is maximized for a given problem size. <p> Therefore, in order to achieve a balance between speedup and efficiency, several researchers have proposed to operate at a point where the value of p.T P / r is minimized for some constant r (r 1) and for a given problem size W <ref> [36, 34, 134] </ref>. It can be shown [134] that this corresponds to the point where E S r1 is maximized for a given problem size. p.T P / r D pT P . <p> Eager et al. <ref> [34] </ref> and Tang and Li [134] have proposed a criterion of optimality different from optimal speedup. They argue that the optimal operating point should be chosen so that a balance is struck between efficiency and speedup. It is proposed in [34] that the knee" of the execution time verses efficiency curve <p> Eager et al. <ref> [34] </ref> and Tang and Li [134] have proposed a criterion of optimality different from optimal speedup. They argue that the optimal operating point should be chosen so that a balance is struck between efficiency and speedup. It is proposed in [34] that the knee" of the execution time verses efficiency curve is a good choice of the operating point because at this point the incremental benefit of adding processors is roughly 0.5 per processor, or, in other words, efficiency is 0.5. <p> Eager et. al. and Tang and Li also conclude that for T o D 2. p/, this is also equivalent to operating at a point where the E S product is maximum or p.T P / 2 is minimum. This conclusion in <ref> [34, 134] </ref> is a special case of the more general case that is captured in Equation 2.18. <p> If we substitute x j D 1 in Equation 2.18 (which is the case if T o D 2. p/), it can seen that we indeed get an efficiency of 0.5 for r D 2. In general, operating at the optimal point or the knee" referred to in <ref> [34] </ref> and [134] for a parallel system with T o D 2. p x j / will be identical to operating at a point where p.T P / r is minimum, where r D 2=.2 x j /. This is obtained from Equation 2.18 for E D 0:5.
Reference: [35] <author> Horace P. Flatt. </author> <title> Further applications of the overhead model for parallel systems. </title> <type> Technical Report G320-3540, </type> <institution> IBM Corporation, Palo Alto Scientific Center, </institution> <address> Palo Alto, CA, </address> <year> 1990. </year>
Reference-contexts: Flatt and Kennedy <ref> [36, 35] </ref> show that if the overhead function satisfies certain mathematical properties, then there exists a unique value p 0 of the number of processors for which T P is minimum for a given W .
Reference: [36] <author> Horace P. Flatt and Ken Kennedy. </author> <title> Performance of parallel processors. </title> <journal> Parallel Computing, </journal> <volume> 12 </volume> <pages> 1-20, </pages> <year> 1989. </year>
Reference-contexts: Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max .W /, which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [36, 75, 55, 105, 134, 146] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem. <p> For such systems, it is natural to use isoefficiency function or related metrics [85, 80, 22]. The analyses in <ref> [148, 149, 36, 97, 105, 134, 34] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other. <p> Clearly, T iso P .W / can be no better than T min P .W /. Several researchers have proposed to use an operating point where the value of p.T P / r is minimized for some constant r and for a given problem size W <ref> [36, 34, 134] </ref>. It can be shown [134] that this corresponds to the point where E S r1 is maximized for a given problem size. <p> Therefore, in order to achieve a balance between speedup and efficiency, several researchers have proposed to operate at a point where the value of p.T P / r is minimized for some constant r (r 1) and for a given problem size W <ref> [36, 34, 134] </ref>. It can be shown [134] that this corresponds to the point where E S r1 is maximized for a given problem size. p.T P / r D pT P . <p> Flatt and Kennedy <ref> [36, 35] </ref> show that if the overhead function satisfies certain mathematical properties, then there exists a unique value p 0 of the number of processors for which T P is minimum for a given W . <p> Equations 2.13 and 2.14 provide results similar to Flatt and Kennedy's. But the analysis in <ref> [36] </ref> tends to conclude the following - .i/ if the overhead function grows very fast with respect to p, then p 0 is small, and hence parallel processing cannot provide substantial speedups; .ii/ if the overhead function grows slowly (i.e., closer to 2. p/), then the overall efficiency is very poor <p> For instance, Flatt and Kennedy's analysis will predict identical values of p max and efficiency at this operating point for the parallel systems described in the examples in 2 T o , as defined in <ref> [36] </ref>, is the overhead incurred per processor when all costs are normalized with respect to W D 1. So in the light of the definition of T o in this chapter, the actual mathematical condition of [36], that T o is an increasing nonnegative function of p, has been translated to <p> parallel systems described in the examples in 2 T o , as defined in <ref> [36] </ref>, is the overhead incurred per processor when all costs are normalized with respect to W D 1. So in the light of the definition of T o in this chapter, the actual mathematical condition of [36], that T o is an increasing nonnegative function of p, has been translated to the condition that T o grows faster than 2.p/. 30 Section 2.3.1 because their overhead functions are identical.
Reference: [37] <author> G. C. Fox, M. Johnson, G. Lyzenga, S. W. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors: Volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year> <month> 154 </month>
Reference-contexts: Clearly, for a given W , the parallel algorithm can not use more than C .W / processors. C .W / depends only on the parallel algorithm, and is independent of the architecture. For example, for multiplying two N fi N matrices using Fox's parallel matrix multiplication algorithm <ref> [37] </ref>, W D N 3 and C .W / D N 2 D W 2=3 . It is easily seen that if the processor-time product [5] is 2.W / (i.e., the algorithm is cost-optimal), then C .W / D O.W /.
Reference: [38] <author> G. C. Fox, M. Johnson, G. Lyzenga, S. W. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors: Volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: p time, the total parallel execution time for all the movements of the sub-blocks of both the matrices is given by the following equation: T p D p p n 2 p Fox's Algorithm This algorithm is due to Fox et al. and is described in detail in [39] and <ref> [38] </ref>. The input matrices are initially distributed among the processors in the same manner as in the simple algorithm in Section 3.2.1. The algorithm works in p p iterations, where p is the number of processors being used.
Reference: [39] <author> G. C. Fox, S. W. Otto, and A. J. G. Hey. </author> <title> Matrix algorithms on a hypercube I: Matrix multiplication. </title> <journal> Parallel Computing, </journal> <volume> 4 </volume> <pages> 17-31, </pages> <year> 1987. </year>
Reference-contexts: 2 = p time, the total parallel execution time for all the movements of the sub-blocks of both the matrices is given by the following equation: T p D p p n 2 p Fox's Algorithm This algorithm is due to Fox et al. and is described in detail in <ref> [39] </ref> and [38]. The input matrices are initially distributed among the processors in the same manner as in the simple algorithm in Section 3.2.1. The algorithm works in p p iterations, where p is the number of processors being used.
Reference: [40] <author> K. A. Gallivan, R. J. Plemmons, and A. H. Sameh. </author> <title> Parallel algorithms for dense linear algebra computations. </title> <journal> SIAM Review, </journal> <volume> 32(1) </volume> <pages> 54-135, </pages> <month> March </month> <year> 1990. </year> <note> Also appears in K. </note> <author> A. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [40, 109, 42, 81] </ref>.
Reference: [41] <author> G. A. Geist and E. G.-Y. Ng. </author> <title> Task scheduling for parallel sparse Cholesky factorization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(4) </volume> <pages> 291-314, </pages> <year> 1989. </year>
Reference: [42] <author> G. A. Geist and C. H. Romine. </author> <title> LU factorization algorithms on distributed-memory multiprocessor architectures. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 639-649, </pages> <year> 1988. </year> <note> Also available as Technical Report ORNL/TM-10383, </note> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1987. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [40, 109, 42, 81] </ref>.
Reference: [43] <author> A. George. </author> <title> Nested dissection of a regular finite-element mesh. </title> <journal> SIAM Journal on Numerical Ananlysis, </journal> <volume> 10 </volume> <pages> 345-363, </pages> <year> 1973. </year>
Reference-contexts: This method of analyzing the communication complexity of sparse Cholesky factorization has been used in [47] in the context of a column-based subtree-to-subcube scheme. Within very small constant factors, the analysis holds for the standard nested dissection <ref> [43] </ref> of grid graphs. <p> The purpose of these experiments was to compare their results with the scalability analysis in Section 4.6. The dimensions of the grids were chosen such that the 116 elimination trees were as balanced as possible. The standard nested dissection ordering <ref> [43] </ref> was used for these matrices. Nested dissection has been shown to have optimal fill-in in the case of regular grids [45]. The results of our implementation for some of these grids are summarized in Table 4.1.
Reference: [44] <author> A. George, M. T. Heath, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Sparse Cholesky factorization on a local memory multiprocessor. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 327-340, </pages> <year> 1988. </year>
Reference-contexts: The results for the three-dimensional case are very similar. A simple fan-out algorithm <ref> [44] </ref> with column-wise partitioning of an N fi N matrix of this type on p processors results in an 2.N p log N / total communication volume [47] (box A).
Reference: [45] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: Even if the non-zero elements are scattered throughout the matrix, it is often possible to restrict them to a band through certain re-ordering techniques <ref> [45, 48] </ref>. Such a system is shown in Figure 3.15 in which the non-zero elements of the sparse matrix are randomly distributed within a band along the principal diagonal. <p> Various techniques for re-ordering sparse systems to yield banded or partially banded sparse matrices are available <ref> [45, 48] </ref>. These techniques may vary in their complexity and effectiveness. <p> Both the number of non-zero elements per row and the width of the band containing these elements depend on the characteristics of the system of equations to be solved. In particular, the non-zero elements can be organized within a band by using some re-ordering techniques <ref> [45, 48] </ref>. <p> A number of column-based parallel factorization algorithms [95, 96, 10, 114, 115, 127, 44, 47, 41, 69, 63, 64, 117, 123, 103] have a lower bound of .N p/ on the total communication volume [47]. Since the overall computation is only 2.N 1:5 / <ref> [45] </ref>, the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased [123, 120]. <p> All two- and three-dimensional finite-element graphs belong to this class. The properties of separators can be generalized from grids to all such graphs within the same order of magnitude bounds <ref> [91, 90, 45] </ref>. The parallel multifrontal algorithm described in Section 4.4 incurs two types of communication overhead: one during parallel extend-add operations (Figure 4.8) and the other during the steps of dense Cholesky factorization while processing the supernodes (Figure 4.9). <p> As in the case of two-dimensional problems, this is asymptotically equal to the communication time for the elimination steps. 4.6 Scalability Analysis It is well known <ref> [45] </ref> that the total work involved in factoring the adjacency matrix of an N -node graph with an 2. p N /-node separator using nested dissection ordering of nodes is 2.N 1:5 /. <p> Thus, the complexity of the dense portion of factorization for these two types of matrices is 2.N 1:5 / and 2.N 2 /, respectively, which is of the same order as the computation required to factor the entire sparse matrix <ref> [45, 47] </ref>. Therefore, the isoefficiency function of sparse factorization of such matrices is bounded from below by the isoefficiency function of dense matrix factorization, which is 2. p 1:5 /. <p> As the overall problem size increases, so does the overall memory requirement. For an N -node two-dimensional constant node-degree graphs, the size of the lower triangular factor L is 2.N log N / <ref> [45] </ref>. For a fixed efficiency, W D N 1:5 / p 1:5 , which implies N / p and 115 N log N / p log p. <p> In the three-dimensional case, size of the lower triangular factor L is 2.N 4=3 / <ref> [45] </ref>. For a fixed efficiency, W D N 2 / p 1:5 , which implies N / p 3=4 and N 4=3 / p. <p> The dimensions of the grids were chosen such that the 116 elimination trees were as balanced as possible. The standard nested dissection ordering [43] was used for these matrices. Nested dissection has been shown to have optimal fill-in in the case of regular grids <ref> [45] </ref>. The results of our implementation for some of these grids are summarized in Table 4.1. Matrix GRIDix j in the table refers to the sparse matrix obtained from an i fi j 9-point finite difference grid. <p> If a nested-dissection based ordering scheme is used to number the nodes of the graph corresponding to the coefficient matrix, then the number of nodes t in a supernode at level l is ff p N =2 l , where ff is a small constant <ref> [91, 90, 45, 53] </ref>. <p> log p1 log p1 p N =2 l /, which is 2. p/ C 2. p The overall computation is proportional to the number of non-zeros in L , which is 2.N log N / for an N fi N sparse coefficient matrix resulting from a two-dimensional finite element problem <ref> [45] </ref> with a nested-dissection based ordering. Assuming that the computation is divided uniformly among the processors, each processor spends 2..N log N /= p/ time in computation. <p> 2. p p If the underlying graph corresponding to the coefficient matrix is a three-dimensional constant-degree graph (as is the case in three-dimensional finite element and finite difference problems), then the value of t at level l is roughly ff.N=2 l / 2=3 , where ff is a small constant <ref> [45, 53] </ref>. The value of q at level l is p=2 l . Thus, the total communication time is proportional to 6 log p1 C 6 lD0 .ff.N=2 l / 2=3 , which is 2. p/ C 2.N 2=3 /. <p> The value of q at level l is p=2 l . Thus, the total communication time is proportional to 6 log p1 C 6 lD0 .ff.N=2 l / 2=3 , which is 2. p/ C 2.N 2=3 /. Assuming that the overall computation of 2.N 4=3 / <ref> [45] </ref> is uniformly distributed among the processors, the parallel runtime is given by the following equation: T P D 2. p If more than one (say m) right-hand side vectors are present in the system, then each term in Equations 4.1 and 4.2 is multiplied with m.
Reference: [46] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Communication reduction in parallel sparse Cholesky factorization on a hypercube. </title> <editor> In M. T. Heath, editor, </editor> <booktitle> Hypercube Multiprocessors 1987, </booktitle> <pages> pages 576-586. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1987. </year>
Reference-contexts: Box D represents our algorithm, which is a significant improvement over other known classes of algorithms for this problem. to-subcube mapping <ref> [46] </ref> (box B). A number of column-based parallel factorization algorithms [95, 96, 10, 114, 115, 127, 44, 47, 41, 69, 63, 64, 117, 123, 103] have a lower bound of .N p/ on the total communication volume [47]. <p> However, this distribution is not suitable for the triangular solvers, which are scalable only with a one-dimensional partitioning of the supernodal blocks of L . We show that if the supernodes are distributed in a subtree-to-subcube manner <ref> [46] </ref> then the cost of converting the two-dimensional distribution to a one-dimensional distribution is only a constant times the cost of solving the triangular systems. <p> Forward Elimination The basic approach to forward elimination is very similar to that of multifrontal numerical factorization [92] guided by an elimination tree [93, 81] with the distribution of computation determined by a subtree-to-subcube mapping <ref> [46] </ref>. A symmetric sparse matrix, its lower triangular Cholesky factor, and the corresponding elimination tree with subtree-to-subcube mapping onto 8 processors is shown in Figure 4.5. The computation in forward elimination starts with the leaf supernodes of the elimination tree and progresses upwards to terminate at the root supernode.
Reference: [47] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Communication results for parallel sparse Cholesky factorization on a hypercube. </title> <journal> Parallel Computing, </journal> <volume> 10(3) </volume> <pages> 287-298, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The results for the three-dimensional case are very similar. A simple fan-out algorithm [44] with column-wise partitioning of an N fi N matrix of this type on p processors results in an 2.N p log N / total communication volume <ref> [47] </ref> (box A). The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree 1 In [110], Pan and Reif describe a parallel sparse matrix factorization algorithm for a PRAM type architecture. <p> A number of column-based parallel factorization algorithms [95, 96, 10, 114, 115, 127, 44, 47, 41, 69, 63, 64, 117, 123, 103] have a lower bound of .N p/ on the total communication volume <ref> [47] </ref>. Since the overall computation is only 2.N 1:5 / [45], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased [123, 120]. <p> In order to simplify the analysis, we assume a somewhat different form of nested-dissection than the one used in the actual implementation. This method of analyzing the communication complexity of sparse Cholesky factorization has been used in <ref> [47] </ref> in the context of a column-based subtree-to-subcube scheme. Within very small constant factors, the analysis holds for the standard nested dissection [43] of grid graphs. We consider a cross-shaped separator (described in [47]) consisting of 2 p N 1 nodes that partitions the N -node square grid into four square <p> This method of analyzing the communication complexity of sparse Cholesky factorization has been used in <ref> [47] </ref> in the context of a column-based subtree-to-subcube scheme. Within very small constant factors, the analysis holds for the standard nested dissection [43] of grid graphs. We consider a cross-shaped separator (described in [47]) consisting of 2 p N 1 nodes that partitions the N -node square grid into four square subgrids of size . p p call this the level-0 separator that partitions the original grid (or the level-0 grid) into four level-1 grids. <p> It has been proved in <ref> [47] </ref> that the number of nonzeros that an i fi i subgrid can contribute to the nodes of its bordering separators is bounded by ki 2 , where k D 341=12. Hence, a level-l subgrid can contribute at most k N =4 l nonzeros to its bordering nodes. <p> The number of nonzeros that an i fi i fi i subgrid contributes to the nodes of its bordering separators is 2.i 4 / <ref> [47] </ref>. At level l, due to l bisections, i is no more than N 1=3 =2 l . As a result, an update or a frontal matrix at level l of the supernodal elimination tree will contain 2.N 4=3 =2 4l / entries distributed among p=8 l processors. <p> The problem size in the case of an N fi N sparse matrix resulting from a three-dimensional grid is 2.N 2 / <ref> [47] </ref>. We have shown in Section 4.5 that the overall communication overhead in this case is 2.N 4=3 p p/. <p> Thus, the complexity of the dense portion of factorization for these two types of matrices is 2.N 1:5 / and 2.N 2 /, respectively, which is of the same order as the computation required to factor the entire sparse matrix <ref> [45, 47] </ref>. Therefore, the isoefficiency function of sparse factorization of such matrices is bounded from below by the isoefficiency function of dense matrix factorization, which is 2. p 1:5 /.
Reference: [48] <author> N. E. Gibbs, W. G. Poole, and P. K. Stockmeyer. </author> <title> A comparison of several bandwidth and profile reduction algorithms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 2 </volume> <pages> 322-330, </pages> <year> 1976. </year>
Reference-contexts: Even if the non-zero elements are scattered throughout the matrix, it is often possible to restrict them to a band through certain re-ordering techniques <ref> [45, 48] </ref>. Such a system is shown in Figure 3.15 in which the non-zero elements of the sparse matrix are randomly distributed within a band along the principal diagonal. <p> Various techniques for re-ordering sparse systems to yield banded or partially banded sparse matrices are available <ref> [45, 48] </ref>. These techniques may vary in their complexity and effectiveness. <p> Both the number of non-zero elements per row and the width of the band containing these elements depend on the characteristics of the system of equations to be solved. In particular, the non-zero elements can be organized within a band by using some re-ordering techniques <ref> [45, 48] </ref>.
Reference: [49] <author> John R. Gilbert and Robert Schreiber. </author> <title> Highly parallel sparse Cholesky factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13 </volume> <pages> 1151-1172, </pages> <year> 1992. </year> <month> 155 </month>
Reference-contexts: A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [120, 119, 7, 104, 140, 49] </ref>, and the total communication volume in the best of these schemes [120, 119] is 2.N p 94 Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume.
Reference: [50] <author> Gene H. Golub and Charles Van Loan. </author> <title> Matrix Computations: Second Edition. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <year> 1989. </year>
Reference-contexts: In this section, we study performance and scalability of parallel formulations of an iteration of the Preconditioned Conjugate Gradient (PCG) algorithm <ref> [50] </ref> for solving large sparse linear systems of equations of the form A x = b, where A is a symmetric positive definite matrix. <p> Thus, for this application, the control network is highly useful. 75 20000 60000 100000 140000 180000 " p ! t w = 4 channel bandwidth. There are certain iterative schemes, like the Jacobi method <ref> [50] </ref>, that require inner product calculation only for the purpose of performing a convergence check. In such schemes, the parallel formulation can be made almost linearly scalable even on mesh and hypercube architectures by performing the convergence check once in a few iterations. <p> The parallel algorithm also works similarly, except that the frontal and update matrices are now full square matrices rather than triangular matrices as in the case of Cholesky factorization. The least square problem (LSP) min x jj Ax bjj 2 is commonly solved <ref> [50] </ref> by factoring the m fi n matrix A (m n) into the product Q R, where Q is an m fi n orthogonal matrix and R is an n fi n upper triangular matrix.
Reference: [51] <author> Ananth Grama, Anshul Gupta, and Vipin Kumar. Isoefficiency: </author> <title> Measuring the scalability of parallel algorithms and architectures. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(3) </volume> <pages> 12-21, </pages> <month> August, </month> <year> 1993. </year> <note> Also available as Technical Report TR 93-24, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: However, the utility of isoefficiency analysis is not limited to predicting the impact on performance of an increasing number of processors. In <ref> [51] </ref>, we show how the isoefficiency function characterizes the amount of parallelism inherent in a parallel algorithm. <p> The isoefficiency metric <ref> [81, 51, 84] </ref> comes in as a handy tool to study the fixed efficiency characteristics of a parallel system. The isoefficiency function relates the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors used. <p> As discussed in <ref> [81, 51] </ref>, the relation between the problem size and the maximum number of processors that can be used in a cost-optimal fashion for solving the problem is given by the isoefficiency function. Often, using as many processors as possible results in a non-cost-optimal system.
Reference: [52] <author> Ananth Grama, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Experimental evaluation of load balancing techniques for the hypercube. </title> <booktitle> In Proceedings of the Parallel Computing '91 Conference, </booktitle> <pages> pages 497-514, </pages> <year> 1991. </year>
Reference-contexts: The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [85, 88, 125, 56, 54, 58, 144, 143, 52] </ref>. As illustrated by a variety of examples in this chapter (these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc.), on almost all parallel architectures of interest.
Reference: [53] <author> Anshul Gupta, George Karypis, and Vipin Kumar. </author> <title> Highly scalable parallel algorithms for sparse matrix factorization. </title> <type> Technical Report 94-63, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> Submitted for publication in IEEE Transactions on Parallel and Distributed Computing. Postscript file available in users/kumar at anonymous FTP site ftp.cs.umn.edu. </note>
Reference-contexts: An implementation of this algorithm on a 1024-processor Cray T3D delivers up to 20 GFLOPS on medium-size structural engineering and linear programming problems <ref> [53, 78] </ref>. To the best of our knowledge, this is the highest performance ever achieved on any supercomputer for sparse matrix factorization. Numerical factorization is the most time consuming of the four phases involved in obtaining a direct solution of the sparse system of linear equation. <p> In [77], we have applied this algorithm to obtain a highly scalable parallel formulation of interior point algorithms and have observed significant speedups in solving linear programming problems. A variation of this algorithm <ref> [53] </ref> implemented on a 1024-processor Cray T3D delivers up to 20 GFLOPS on medium-size structural engineering and linear programming problems. <p> The remaining 38% overhead is due to load imbalance. Although we have observed through our experiments that the upper bound on efficiency due to load imbalance does not fall below 60-70% for hundreds of processors, even this bound can be improved further. The subtree-to-subcube mapping can be relaxed <ref> [53, 78] </ref> to a subforest-to 123 Number of Processors Problem n j Aj jLj OPC 32 64 128 256 512 1024 PILOT87 2030 122550 504060 240M 0.44 0.73 1.05 MAROS-R7 3136 330472 1345241 720M 0.83 1.41 2.14 3.02 4.07 4.48 FLAP 51537 479620 4192304 940M 0.75 1.27 1.85 2.87 3.83 4.25 <p> 8582414 4200M 1.51 2.63 4.16 6.91 8.90 COPTER2 55476 352238 12681357 9200M 1.10 1.94 3.31 5.76 9.55 14.78 CUBE35 42875 124950 11427033 10300M 1.27 2.26 3.92 6.46 10.33 15.70 NUG15 6330 186075 10771554 29670M 4.32 7.54 12.53 19.92 Table 4.3: The performance of sparse Cholesky factorization on Cray T3D (from <ref> [53, 78] </ref>). <p> Here L and U are obtained from the numerical factorization of a sparse coefficient matrix A of the original system A X D B to be 124 problems on Cray T3D (from <ref> [53, 78] </ref>). The first plot shows total Gigaflops obtained and the second one shows Megaflops per processor. 125 solved. <p> In addition to the performance and scalability analysis of parallel sparse triangular solvers, we discuss the redistribution of the triangular factor matrix among the processors between numerical factorization and triangular solution, and its impact on performance. In <ref> [53] </ref>, we describe an optimal data-distribution scheme for Cholesky factorization of sparse matrices. This distribution leaves groups of consecutive columns of L with identical pattern of non-zeros (henceforth called supernodes) with a two-dimensional partitioning among groups of processors. <p> Although there are bound to be overheads due to unequal distribution of work, it is not possible to model such overheads analytically because the extent of such overheads is data-dependent. From our experience with actual implementations of parallel triangular solvers as well as parallel factorization codes <ref> [53] </ref>, we have observed that such overheads are usually not excessive. Moreover, the overhead due to load imbalance in most practical cases tends to saturate at 32 to 64 processors for most problems and does not continue to increase as the number of processors are increased. <p> If a nested-dissection based ordering scheme is used to number the nodes of the graph corresponding to the coefficient matrix, then the number of nodes t in a supernode at level l is ff p N =2 l , where ff is a small constant <ref> [91, 90, 45, 53] </ref>. <p> 2. p p If the underlying graph corresponding to the coefficient matrix is a three-dimensional constant-degree graph (as is the case in three-dimensional finite element and finite difference problems), then the value of t at level l is roughly ff.N=2 l / 2=3 , where ff is a small constant <ref> [45, 53] </ref>. The value of q at level l is p=2 l . Thus, the total communication time is proportional to 6 log p1 C 6 lD0 .ff.N=2 l / 2=3 , which is 2. p/ C 2.N 2=3 /. <p> In <ref> [53] </ref>, we described parallel algorithms for sparse Cholesky factorization of the same class of matrices with an isoefficiency function of 2. p 1:5 /, which is better than the 2. p 2 / isoefficiency function of the corresponding triangular solver. <p> However, as we have shown in <ref> [53] </ref>, the dense supernodes must be partitioned along both dimensions for the numerical factorization phase to be efficient. The table in Figure 4.18 shows the communication overheads and the isoefficiency functions for parallel dense and sparse factorization and triangular solution using one- and two-dimensional partitioning schemes. <p> cost of certain index computations required in the parallel implementation can be 3 The factorization megaflops, operation count, and number of nonzeros are different for some matrices between Tables 4.3 and 4.4 because Table 4.3 gives the results of an implementation that modifies the subtree-subcube mapping to reduce load imbalance <ref> [53] </ref>. On the other hand, Table 4.4 [57] implements a strict subtree-subcube mapping and also uses somewhat different parameters in spectral nested dissection to order the matrices. 139 Table 4.4: A table of experimental results for sparse forward and backward substitution on a Cray T3D (from [57]).
Reference: [54] <author> Anshul Gupta and Vipin Kumar. </author> <title> The scalability of matrix multiplication algorithms on parallel computers. </title> <type> Technical Report TR 91-54, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1991. </year> <note> A short version appears in Proceedings of 1993 International Conference on Parallel Processing, pages III-115-III-119, </note> <year> 1993. </year>
Reference-contexts: This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) <ref> [54] </ref>, vector dot products (single node accumulation) [58], shortest paths (one-to-all broadcast) [88], and FFT (all-to-all personalized communication) [56], etc. <p> The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [85, 88, 125, 56, 54, 58, 144, 143, 52] </ref>. As illustrated by a variety of examples in this chapter (these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc.), on almost all parallel architectures of interest. <p> In this case the minimum possible value for p.T P / r will be obtained when C .W / processors are used. For example, consider a simple algorithm described in <ref> [54] </ref> for multiplying two N fi N matrices on a p p p wrap-around mesh. <p> This is discussed in detail later while analyzing the scalability of the GK algorithm. 3.2.2 Scalability Analysis Following the technique described in Section 2.2, in <ref> [54] </ref> we have analyzed the scalability of all the algorithms discussed in Section 3.2.1 on the hypercube architecture using the isoefficiency metric. The asymptotic scalabilities and the range of applicability of these algorithms is summarized in Table 3.3. <p> The isoefficiency functions in Table 3.3 reflect the impact of communication overheads, as well as, the degree of concurrency. For example, the isoefficiency term for Berntsen's algorithm is 2. p 4=3 / due to communication overhead <ref> [54] </ref>. However, for this algorithm, p cannot exceed n 3=2 . This restriction p n 3=2 implies that n 3 D W D . p 2 /. Hence, the overall asymptotic isoefficiency function for this algorithm is 2. p 2 /. <p> In this section and the rest of this chapter, we skip the discussion of the simple algorithm and Fox's algorithm because the expressions for their iso-efficiency functions differ with that for Cannon's algorithm by small constant factors only <ref> [54] </ref>. Note that Table 3.3 gives only the asymptotic scalabilities of the four algorithms. In practice, 55 none of the algorithms is strictly better than the others for all possible problem sizes and number of processors.
Reference: [55] <author> Anshul Gupta and Vipin Kumar. </author> <title> Performance properties of large scale parallel systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 19 </volume> <pages> 234-244, </pages> <year> 1993. </year> <note> Also available as Technical Report TR 92-32, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max .W /, which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [36, 75, 55, 105, 134, 146] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem. <p> It is the total communication overhead that actually determines the overall efficiency and speedup, and is defined as the difference between the parallel processor-time product and the serial run time <ref> [55, 81] </ref>. The communication overhead can be asymptotically higher than the communication volume. For example, a one-to-all broadcast algorithm based on a binary tree communication pattern has a total communication volume of m. p 1/ for broadcasting m words of data among p processors.
Reference: [56] <author> Anshul Gupta and Vipin Kumar. </author> <title> The scalability of FFT on parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(8) </volume> <pages> 922-932, </pages> <month> August </month> <year> 1993. </year> <note> A detailed version available as Technical Report TR 90-53, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) [54], vector dot products (single node accumulation) [58], shortest paths (one-to-all broadcast) [88], and FFT (all-to-all personalized communication) <ref> [56] </ref>, etc. The readers should note that the presence of a global communication operation in an algorithm is a sufficient but not a necessary condition for nonlinear isoefficiency on an architecture with message passing latency. <p> Such systems typically arise while using shared memory or SIMD machines which do not have a message startup time for data communication. For example, consider a parallel implementation of the FFT algorithm <ref> [56] </ref> on a SIMD hypercube connected machine (e.g., the CM-2 [72]). If an N point FFT is being attempted on such a machine with p processors, N= p units of data will be communicated among directly connected processors in log p of the log N iterations of the algorithm. <p> For this parallel system W D N log N . As shown in <ref> [56] </ref>, T o D t w fi .N= p/ log p fi p D t w N log p, where t w is the message communication time per word. Clearly, for a given W , T o &lt; 2. p/. <p> The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [85, 88, 125, 56, 54, 58, 144, 143, 52] </ref>. As illustrated by a variety of examples in this chapter (these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc.), on almost all parallel architectures of interest. <p> It also confirms the validity of Equation 2.18. Consider the implementation of the FFT algorithm on an MIMD hypercube using the binary-exchange algorithm. As shown in <ref> [56] </ref>, for an N point FFT on p processors, W = N log N and T o D t s p log p C t w N log p for this algorithm. <p> As mentioned in Section 3.1.1, in this chapter we have confined our discussion of the transpose algorithm to the two-dimensional case. A generalized transpose algorithm and the related performance and scalability analysis can be found in [81]. 3.1.4 Impact of Architectural and Algorithmic Variations on Scalability of FFT In <ref> [56] </ref>, we analyze the scalability of multidimensional FFTs, ordered FFT, and FFTs with radix higher than 2, and survey some other variations of the Cooley-Tukey algorithm. <p> We find that within a small constant factor, the isoefficiency functions are the same as the ones derived in this chapter for the simplified case of unordered, radix-2, single dimensional FFT. The analysis in this chapter assumes store-and-forward routing on the mesh. In <ref> [56] </ref> we show that due to message contention, the expressions for the communication overhead (and hence, for the isoefficiency function too) on the mesh do not improve if cut-through or worm-hole routing is used. In [56] and [81], we also discuss the additional overhead that a parallel FFT algorithm might incur <p> The analysis in this chapter assumes store-and-forward routing on the mesh. In <ref> [56] </ref> we show that due to message contention, the expressions for the communication overhead (and hence, for the isoefficiency function too) on the mesh do not improve if cut-through or worm-hole routing is used. In [56] and [81], we also discuss the additional overhead that a parallel FFT algorithm might incur due to redundant computation of twiddle-factors.
Reference: [57] <author> Anshul Gupta and Vipin Kumar. </author> <title> Parallel algorithms for forward and back substitution in direct solution of sparse linear systems. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: On the other hand, Table 4.4 <ref> [57] </ref> implements a strict subtree-subcube mapping and also uses somewhat different parameters in spectral nested dissection to order the matrices. 139 Table 4.4: A table of experimental results for sparse forward and backward substitution on a Cray T3D (from [57]). <p> On the other hand, Table 4.4 <ref> [57] </ref> implements a strict subtree-subcube mapping and also uses somewhat different parameters in spectral nested dissection to order the matrices. 139 Table 4.4: A table of experimental results for sparse forward and backward substitution on a Cray T3D (from [57]). In the above table, NRHS denotes the number of right-hand side vectors, FBsolve time denotes the total time spent in both the forward and the backward solvers, and FBsolve MFLOPS denotes the average performance of the solvers in million floating point operations per second. <p> See footnote in the text. 140 solutions with different number of right-hand side vectors (from <ref> [57] </ref>). amortized over all the right-hand side vectors. 4.9 Parallel Symbolic Factorization The symbolic factorization step determines the structure of the lower triangular factor matrix L and sets up the data structures in which to store the original SPD matrix A and the nonzero entries of L to be created during
Reference: [58] <author> Anshul Gupta, Vipin Kumar, and A. H. Sameh. </author> <title> Performance and scalability of preconditioned conjugate gradient methods on parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(5) </volume> <pages> 455-469, </pages> <year> 1995. </year> <note> Also available as Technical Report TR 92-64, </note> <institution> Department of Computer Science, University of Minnesota, </institution> <address> Minneapolis, MN. </address> <booktitle> A short version appears in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 664-674, </pages> <year> 1993. </year> <month> 156 </month>
Reference-contexts: This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) [54], vector dot products (single node accumulation) <ref> [58] </ref>, shortest paths (one-to-all broadcast) [88], and FFT (all-to-all personalized communication) [56], etc. The readers should note that the presence of a global communication operation in an algorithm is a sufficient but not a necessary condition for nonlinear isoefficiency on an architecture with message passing latency. <p> The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [85, 88, 125, 56, 54, 58, 144, 143, 52] </ref>. As illustrated by a variety of examples in this chapter (these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc.), on almost all parallel architectures of interest.
Reference: [59] <author> John L. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year>
Reference-contexts: The scalability issues for such problems have been explored by Worley [146], Gustafson <ref> [61, 59] </ref>, and Sun and Ni [131]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [145, 146, 147], Gustafson [61, 59] and by Sun and Ni [131], and is called the memory-constrained case. <p> The scalability issues for such problems have been explored by Worley [146], Gustafson <ref> [61, 59] </ref>, and Sun and Ni [131]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [145, 146, 147], Gustafson [61, 59] and by Sun and Ni [131], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p.
Reference: [60] <author> John L. Gustafson. </author> <title> The consequences of fixed time performance measurement. </title> <booktitle> In Proceedings of the 25th Hawaii International Conference on System Sciences: Volume III, </booktitle> <pages> pages 113-124, </pages> <year> 1992. </year>
Reference: [61] <author> John L. Gustafson, Gary R. Montry, and Robert E. Benner. </author> <title> Development of parallel methods for a 1024-processor hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 609-638, </pages> <year> 1988. </year>
Reference-contexts: The scalability issues for such problems have been explored by Worley [146], Gustafson <ref> [61, 59] </ref>, and Sun and Ni [131]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [145, 146, 147], Gustafson [61, 59] and by Sun and Ni [131], and is called the memory-constrained case. <p> The scalability issues for such problems have been explored by Worley [146], Gustafson <ref> [61, 59] </ref>, and Sun and Ni [131]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [145, 146, 147], Gustafson [61, 59] and by Sun and Ni [131], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p.
Reference: [62] <author> S. W. Hammond and Robert Schreiber. </author> <title> Efficient ICCG on a shared-memory multiprocessor. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 4(1) </volume> <pages> 1-22, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: As a result there has been a great deal of interest in implementing the Conjugate Gradient algorithm on parallel computers <ref> [6, 12, 62, 73, 79, 101, 122, 138, 139] </ref>.
Reference: [63] <author> M. T. Heath, E. G.-Y. Ng, and Barry W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 420-460, </pages> <year> 1991. </year> <note> Also appears in K. </note> <author> A. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers [40, 109, 42, 81]. However, despite inherent parallelism in sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations <ref> [63, 123] </ref> and for several years, it has been a challenge to implement efficient sparse linear system solvers using direct methods on even moderately parallel computers. Performance delivered by most existing parallel sparse matrix factorizations had been quite poor. <p> Note that the algorithm of Figure 4.21 requires the knowledge of the elimination tree. Elimination tree generation has traditionally been coupled with symbolic factorization <ref> [93, 63] </ref>. In this chapter, we are relying on nested-dissection based ordering strategies that can be computed in parallel and also render the remaining phases of the solution process amenable to parallelization [76]. The elimination tree can be constructed easily (and cheaply) while performing a nested-dissection based ordering.
Reference: [64] <author> M. T. Heath and Padma Raghavan. </author> <title> Distributed solution of sparse linear systems. </title> <type> Technical Report 93-1793, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1993. </year>
Reference-contexts: We present a detailed analysis of the parallel complexity and scalability of parallel algorithm described briefly in <ref> [64] </ref> to obtain a solution to the system of sparse linear equations of the forms LY D B and U X D Y , where L is a lower triangular matrix and U is an upper triangular matrix. <p> Of course, if more than one systems need to be solved with the same coefficient matrix, then the one-time redistribution cost is amortized. 126 4.8.1 Algorithm Description In this section, we describe parallel algorithms for sparse forward elimination and backward substitution, which have been discussed briefly in <ref> [64] </ref>. The description in this section assumes a single right-hand side vector; however, the algorithm can easily be generalized to multiple right-hand sides by replacing all vector operations by the corresponding matrix operations.
Reference: [65] <author> M. T. Heath and C. H. Romine. </author> <title> Parallel solution of triangular systems on distributed-memory multiprocessors. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(3) </volume> <pages> 558-588, </pages> <year> 1988. </year>
Reference-contexts: The computation at a level greater than or equal to log p is performed sequentially on a single processor assigned to that subtree. However, the computation steps mentioned above must be performed in parallel on p=2 l processors for a supernode with 0 l &lt; log p. In <ref> [65] </ref>, Heath and Romine describe efficient pipelined or wavefront algorithms for solving dense triangular systems with block-cyclic row-wise and column-wise partitioning of the triangular matrices.
Reference: [66] <author> Mark D. Hill. </author> <title> What is scalability? Computer Architecture News, </title> <type> 18(4), </type> <year> 1990. </year>
Reference-contexts: We present a detailed survey of these metrics in [84]. After reviewing these various measures of scalability, one may ask whether there exists one measure that is better than all others <ref> [66] </ref>? The answer to this question is no, as different measures are suitable for different situations. One situation arises when the problem at hand is fixed and one is trying to use an increasing number of processors to solve it.
Reference: [67] <author> Paul G. Hipes. </author> <title> Matrix multiplication on the JPL/Caltech Mark IIIfp hypercube. </title> <type> Technical Report C3P 746, </type> <institution> Concurrent Computation Program, California Institute of Technology, Pasadena, </institution> <address> CA, </address> <year> 1989. </year>
Reference: [68] <author> C.-T. Ho, S. L. Johnsson, and Alan Edelman. </author> <title> Matrix multiplication on hypercubes using full bandwidth and constant storage. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 447-451, </pages> <year> 1991. </year>
Reference-contexts: Hence, among the matrix multiplication algorithms discussed here, the ones that can potentially benefit from simultaneous communications on all the ports are the simple algorithm (or its variations <ref> [68] </ref>) and the GK algorithm. The Simple Algorithm with All-Port Communication This algorithm requires an all-to-all broadcast of the sub-blocks of the matrices A and B among groups of p p processors. <p> Thus the parallel execution time of this algorithm on a hypercube with simultaneous communication is given by the following equation: T p D p n 2 p log p 1 t s log p: (3.18) Recall that the simple algorithm is not memory efficient. Ho et al. <ref> [68] </ref> give a memory efficient version of this algorithm which has somewhat higher execution time than that given by Equation 3.18. <p> However, as mentioned in <ref> [68] </ref>, the lower limit on the message size imposes the condition that n . p log p/=2. This requires that n 3 D W p 1:5 .log p/ 3 =8.
Reference: [69] <author> Laurie Hulbert and Earl Zmijewski. </author> <title> Limiting communication in parallel sparse Cholesky factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(5) </volume> <pages> 1184-1197, </pages> <month> September </month> <year> 1991. </year>
Reference: [70] <author> Kai Hwang. </author> <title> Advanced Computer Architecture: Parallelism, Scalability, Programmability. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference: [71] <author> S. L. Johnsson and C.-T. Ho. </author> <title> Optimum broadcasting and personalized communication in hypercubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(9) </volume> <pages> 1249-1268, </pages> <month> September </month> <year> 1989. </year> <month> 157 </month>
Reference-contexts: The latter is true when any algorithm with a global operation (such as broadcast, and one-to-all and all-to-all personalized communication <ref> [18, 71] </ref>) is implemented on a parallel architecture that has a message passing latency or message startup time. <p> This communication (known as all-to-all personalized communication) can be performed by executing the following code on each processor: for i D 1 to p do send data to processor number (self address i) It is shown in <ref> [71] </ref>, that on a hypercube, in each iteration of the above code, each pair of communicating processors have a contention-free communication path. On a hypercube with store-and-forward routing, this communication will take t w .n= p/ log p C t s p time. <p> On a hypercube, it is possible to employ a more sophisticated scheme for one-to-all broadcast <ref> [71] </ref> of sub-blocks of matrix A among the rows. Using this scheme, the parallel execution time can be improved to n 3 = p C 2t w n 2 = p p p t s t w ogp, which is still worse than Cannon's algorithm. <p> The total parallel execution time is therefore given by the following equation: T p D p 5 t s log p C 3 n 2 This execution time can be further reduced by using a more sophisticated scheme for one-to-all broadcast on a hypercube <ref> [71] </ref>. This is discussed in detail later while analyzing the scalability of the GK algorithm. 3.2.2 Scalability Analysis Following the technique described in Section 2.2, in [54] we have analyzed the scalability of all the algorithms discussed in Section 3.2.1 on the hypercube architecture using the isoefficiency metric. <p> can be utilized to significantly reduce the 5 The GK algorithm does begin to perform better than the other algorithms for p &gt; 1:3 fi 10 8 , but again we consider this range of p to be impractical. 59 communication cost of certain operations involving broadcasting and personalized communication <ref> [71] </ref>. In this section we investigate as to what extent can the performance of the algorithms described in Section 3.2.1 can be improved by utilizing simultaneous communication on all the log p ports of the hypercube processors. <p> The GK Algorithm with All-Port Communication Using the one-to-all broadcast scheme of <ref> [71] </ref> for a hypercube with simultaneous all-port communication, the parallel execution time of the GK algorithm can be reduced to the following: T p D p n 2 C 6 p 1=3 t s t w : (3.19) The communication terms now yield an isoefficiency function of 2. p log p/,
Reference: [72] <author> S. L. Johnsson, R. Krawitz, R. Frye, and D. McDonald. </author> <title> A radix-2 FFT on the connection machine. </title> <type> Technical report, </type> <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Such systems typically arise while using shared memory or SIMD machines which do not have a message startup time for data communication. For example, consider a parallel implementation of the FFT algorithm [56] on a SIMD hypercube connected machine (e.g., the CM-2 <ref> [72] </ref>). If an N point FFT is being attempted on such a machine with p processors, N= p units of data will be communicated among directly connected processors in log p of the log N iterations of the algorithm. For this parallel system W D N log N . <p> r1 /] := S [.b 0 b l1 0b lC1 b r1 /] C ! .b l b l1 b 0 00/ S [.b 0 b l1 1b lC1 b r1 /]; 10. end; The Binary-Exchange Algorithm In the most commonly used mapping that minimizes communication for the binary-exchange algorithm <ref> [81, 5, 11, 17, 29, 72, 106, 133, 116, 94] </ref>, if .b 0 b 1 b r1 / is the binary representation of i, then for all i, R [i] and S [i] are mapped to processor number .b 0 b d1 /.
Reference: [73] <author> C. Kamath and A. H. Sameh. </author> <title> The preconditioned conjugate gradient algorithm on a multiprocessor. </title> <editor> In R. Vichnevetsky and R. S. Stepleman, editors, </editor> <title> Advances in Computer Methods for Partial Differential Equations. </title> <booktitle> IMACS, </booktitle> <year> 1984. </year>
Reference-contexts: As a result there has been a great deal of interest in implementing the Conjugate Gradient algorithm on parallel computers <ref> [6, 12, 62, 73, 79, 101, 122, 138, 139] </ref>. <p> We will consider two kinds of preconditioner matrices M - (i) when M is chosen to be a diagonal matrix, usually derived from the principal diagonal of A, and (ii) when M is obtained through a truncated Incomplete Cholesky (IC) factorization <ref> [73, 138] </ref> of A. In the following subsections, we determine 65 1. begin 2. i := 0; x 0 := 0; r 0 := b; ae 0 := jjr 0 jj 2 3. while ( p 4. begin 5. <p> These series may be truncated to .k C 1/ terms where k o N because M is diagonally dominant <ref> [73, 138] </ref>. In our formulation, we form the matrix L = (I + L + L 2 + ... + L k ) explicitly.
Reference: [74] <author> Ray A. Kamin and George B. Adams. </author> <title> Fast Fourier transform algorithm design and tradeoffs. </title> <type> Technical Report RIACS TR 88.18, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <year> 1988. </year>
Reference: [75] <author> Alan H. Karp and Horace P. Flatt. </author> <title> Measuring parallel processor performance. </title> <journal> Communications of the ACM, </journal> <volume> 33(5) </volume> <pages> 539-543, </pages> <year> 1990. </year>
Reference-contexts: Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max .W /, which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [36, 75, 55, 105, 134, 146] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem.
Reference: [76] <author> G. Karypis and V. Kumar. </author> <title> Parallel multilevel graph partitioning. </title> <type> Technical Report TR 95-036, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: These algorithms are designed to work in conjunction with our sparse Cholesky factorization algorithm and incur less communication overhead than parallel sparse Cholesky factorization. Along with some recently developed parallel ordering algorithms <ref> [76] </ref>, the algorithms presented in this thesis make it possible to develop complete scalable parallel direct solvers for sparse linear systems. <p> As shown in Figure 4.2, numerical factorization is the most time-consuming phase. Karypis and Kumar have proposed an efficient parallel algorithm for determining fill-reducing orderings for parallel factorization of sparse matrices <ref> [76] </ref>. In this chapter, we present efficient and scalable parallel algorithms for symbolic factorization, numerical factorization, and for solving the upper and lower triangular systems. In Section 4.3, we describe the serial multifrontal algorithm for sparse Cholesky factorization. <p> In this section, we show that the communication overhead of parallel symbolic factorization is asymptotically less than that of the factorization step; hence, this step does not impose any constraints on the performance and scalability of a complete parallel sparse linear system solver. The algorithm in <ref> [76] </ref>, while performing the ordering in parallel, also distributes the data among the processors in way that the remaining steps can be carried out with minimum data-movement. At the end of the parallel ordering step, the parallel symbolic factorization algorithm described in Section 4.9 can proceed without any redistribution. <p> However, it is important to parallelize this step for two important reasons. First, the data (i.e., the original matrix) is already distributed among the processors before the symbolic factorization phase <ref> [76] </ref> and it would be expensive (very often impossible too due to memory constraints) to gather the data for 141 serial symbolic factorization and then redistribute it. <p> Elimination tree generation has traditionally been coupled with symbolic factorization [93, 63]. In this chapter, we are relying on nested-dissection based ordering strategies that can be computed in parallel and also render the remaining phases of the solution process amenable to parallelization <ref> [76] </ref>. The elimination tree can be constructed easily (and cheaply) while performing a nested-dissection based ordering. Assume that a bisection algorithm is being used for ordering; i.e., the separator of a subgraph of nested dissection divides the subgraph into two disconnected components. <p> The process of obtaining a direct solution to a sparse system of linear equations usually consists of four phases: ordering, symbolic factorization, numerical factorization, and forward elimination and backward substitution. A scalable parallel solver for sparse linear systems must implement all these phases effectively in parallel. In <ref> [76] </ref>, Karypis and Kumar present an efficient parallel algorithm for a nested-dissection based fill-reducing ordering for such sparse matrices.
Reference: [77] <author> George Karypis, Anshul Gupta, and Vipin Kumar. </author> <title> Parallel formulation of interior point algorithms. </title> <type> Technical Report 94-20, </type> <institution> Department of Computer Science, University of Min-nesota, Minneapolis, MN, </institution> <month> April </month> <year> 1994. </year> <note> A short version appears in Supercomputing '94 Proceedings. </note>
Reference-contexts: We have been able to achieve speedups of up to 364 on 1024 processors and 230 on 512 processors over a highly efficient sequential implementation for moderately sized problems from the Harwell-Boeing collection [30]. In <ref> [77] </ref>, we have applied this algorithm to obtain a highly scalable parallel formulation of interior point algorithms and have observed significant speedups in solving linear programming problems.
Reference: [78] <author> George Karypis and Vipin Kumar. </author> <title> A high performance sparse Cholesky factorization algorithm for scalable parallel computers. </title> <type> Technical Report TR 94-41, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <booktitle> Submitted to the Eighth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <year> 1995. </year>
Reference-contexts: An implementation of this algorithm on a 1024-processor Cray T3D delivers up to 20 GFLOPS on medium-size structural engineering and linear programming problems <ref> [53, 78] </ref>. To the best of our knowledge, this is the highest performance ever achieved on any supercomputer for sparse matrix factorization. Numerical factorization is the most time consuming of the four phases involved in obtaining a direct solution of the sparse system of linear equation. <p> The remaining 38% overhead is due to load imbalance. Although we have observed through our experiments that the upper bound on efficiency due to load imbalance does not fall below 60-70% for hundreds of processors, even this bound can be improved further. The subtree-to-subcube mapping can be relaxed <ref> [53, 78] </ref> to a subforest-to 123 Number of Processors Problem n j Aj jLj OPC 32 64 128 256 512 1024 PILOT87 2030 122550 504060 240M 0.44 0.73 1.05 MAROS-R7 3136 330472 1345241 720M 0.83 1.41 2.14 3.02 4.07 4.48 FLAP 51537 479620 4192304 940M 0.75 1.27 1.85 2.87 3.83 4.25 <p> 8582414 4200M 1.51 2.63 4.16 6.91 8.90 COPTER2 55476 352238 12681357 9200M 1.10 1.94 3.31 5.76 9.55 14.78 CUBE35 42875 124950 11427033 10300M 1.27 2.26 3.92 6.46 10.33 15.70 NUG15 6330 186075 10771554 29670M 4.32 7.54 12.53 19.92 Table 4.3: The performance of sparse Cholesky factorization on Cray T3D (from <ref> [53, 78] </ref>). <p> Here L and U are obtained from the numerical factorization of a sparse coefficient matrix A of the original system A X D B to be 124 problems on Cray T3D (from <ref> [53, 78] </ref>). The first plot shows total Gigaflops obtained and the second one shows Megaflops per processor. 125 solved.
Reference: [79] <author> S. K. Kim and A. T. Chronopoulos. </author> <title> A class of Lanczos-like algorithms implemented on parallel computers. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 763-777, </pages> <year> 1991. </year>
Reference-contexts: As a result there has been a great deal of interest in implementing the Conjugate Gradient algorithm on parallel computers <ref> [6, 12, 62, 73, 79, 101, 122, 138, 139] </ref>.
Reference: [80] <author> Clyde P. Kruskal, Larry Rudolph, and Marc Snir. </author> <title> A complexity theory of efficient parallel algorithms. </title> <type> Technical Report RC13572, </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1988. </year>
Reference-contexts: Clearly, this can be done only for scalable parallel systems, which are exactly the ones for which a fixed efficiency can be maintained for arbitrarily large p by simply increasing the problem size. For such systems, it is natural to use isoefficiency function or related metrics <ref> [85, 80, 22] </ref>. The analyses in [148, 149, 36, 97, 105, 134, 34] also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other.
Reference: [81] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: The isoefficiency metric <ref> [81, 51, 84] </ref> comes in as a handy tool to study the fixed efficiency characteristics of a parallel system. The isoefficiency function relates the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors used. <p> As discussed in <ref> [81, 51] </ref>, the relation between the problem size and the maximum number of processors that can be used in a cost-optimal fashion for solving the problem is given by the isoefficiency function. Often, using as many processors as possible results in a non-cost-optimal system. <p> r1 /] := S [.b 0 b l1 0b lC1 b r1 /] C ! .b l b l1 b 0 00/ S [.b 0 b l1 1b lC1 b r1 /]; 10. end; The Binary-Exchange Algorithm In the most commonly used mapping that minimizes communication for the binary-exchange algorithm <ref> [81, 5, 11, 17, 29, 72, 106, 133, 116, 94] </ref>, if .b 0 b 1 b r1 / is the binary representation of i, then for all i, R [i] and S [i] are mapped to processor number .b 0 b d1 /. <p> The binary-exchange algorithm is nothing but a a .log p C 1/-dimensional algorithm. In this chapter, we confine our discussion to the two extremes (2-D transpose and binary-exchange) of this sequence of algorithms. More detailed discussion can be found in <ref> [81] </ref>. 3.1.2 Scalability Analysis of the Binary-Exchange Algorithm for Single Dimensional Radix-2 Unordered FFT We assume that the cost of one unit of computation (i.e., the cost of executing line 8 in Figure 3.1) is t c . <p> As mentioned in Section 3.1.1, in this chapter we have confined our discussion of the transpose algorithm to the two-dimensional case. A generalized transpose algorithm and the related performance and scalability analysis can be found in <ref> [81] </ref>. 3.1.4 Impact of Architectural and Algorithmic Variations on Scalability of FFT In [56], we analyze the scalability of multidimensional FFTs, ordered FFT, and FFTs with radix higher than 2, and survey some other variations of the Cooley-Tukey algorithm. <p> The analysis in this chapter assumes store-and-forward routing on the mesh. In [56] we show that due to message contention, the expressions for the communication overhead (and hence, for the isoefficiency function too) on the mesh do not improve if cut-through or worm-hole routing is used. In [56] and <ref> [81] </ref>, we also discuss the additional overhead that a parallel FFT algorithm might incur due to redundant computation of twiddle-factors. <p> In a generalization of this method <ref> [81] </ref>, the vector X can be arranged in an m-dimensional array mapped on to an .m 1/-dimensional logical array of p processors, where p D n .m1/=m = k. <p> When M is an IC preconditioner, the structure of M is identical to that of A. A method for solving M z = r, originally proposed for vector machines [138], is briefly described below. A detailed description of the same can be found in <ref> [81] </ref>. As shown in Section 3.3.2, this method is perfectly parallelizable on CM-5 and other architectures ranging from mesh to hypercube. <p> It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [40, 109, 42, 81] </ref>. <p> It is the total communication overhead that actually determines the overall efficiency and speedup, and is defined as the difference between the parallel processor-time product and the serial run time <ref> [55, 81] </ref>. The communication overhead can be asymptotically higher than the communication volume. For example, a one-to-all broadcast algorithm based on a binary tree communication pattern has a total communication volume of m. p 1/ for broadcasting m words of data among p processors. <p> The isoefficiency function for dense matrix factorization is 2. p 1:5 / <ref> [81] </ref>. It is easy to prove that 2. p 1:5 / is also the lower bound on the isoefficiency function for factoring the above mentioned class of sparse matrices. <p> The number of such successive elimination steps is equal to the number of nodes in the relaxed supernode being processed. The communication that takes place in this phase is the standard communication in pipelined grid-based dense Cholesky factorization <ref> [109, 81] </ref>. If the average size of the frontal matrices is t fi t during the processing of a relaxed supernode with m nodes on a q-processor subcube, then 2.m/ messages of size 2.t = p q/ are passed through the grid in a pipelined fashion. <p> It is shown in [82] that although this communication does not take place between the nearest neighbors on a subcube, the paths of all communications on any subcube are conflict free with e-cube routing <ref> [107, 81] </ref> and cut-through or worm-hole flow control. This is a direct consequence of the fact that a circular shift is conflict free on a hypercube with e-cube routing. <p> Furthermore, on cache-based processors, the use of BLAS-3 for eliminating multiple columns simultaneously yields much higher performance than the use of BLAS-2 for eliminating one column at a time. Figure 4.10 (b) shows a variation of 107 the cyclic mapping, called block-cyclic mapping <ref> [81] </ref>, that can alleviate these problems at the cost of some added load imbalance. Recall that in the mapping of Figure 4.8 (e), the least significant dlog p=2e bits of a row or column index of the matrix determine the processor to which that row or column belongs. <p> As shown in Figure 4.9, there are two communication operations involved with each elimination step of dense Cholesky. The average size of a message is .ff N =2 l / ffi . p=2 l / = ff N = p . It can be shown <ref> [109, 81] </ref> that in a pipelined implementation 112 p p q mesh of processors, the communication time for s elimination steps with an average message size of m is 2.ms/. <p> We have shown in Section 4.5 that the overall communication overhead in this case is 2.N 4=3 p p/. To maintain a fixed efficiency, N 2 / N 4=3 p p A lower bound on the isoefficiency function for dense matrix factorization is 2. p 1:5 / <ref> [81, 82] </ref> if the number of rank-1 updates performed by the serial algorithm is proportional to the rank of the matrix. The factorization of a sparse matrix derived from an N -node graph with an S.N /-node separator involves a dense S.N / fi S.N / matrix factorization. <p> Forward Elimination The basic approach to forward elimination is very similar to that of multifrontal numerical factorization [92] guided by an elimination tree <ref> [93, 81] </ref> with the distribution of computation determined by a subtree-to-subcube mapping [46]. A symmetric sparse matrix, its lower triangular Cholesky factor, and the corresponding elimination tree with subtree-to-subcube mapping onto 8 processors is shown in Figure 4.5. <p> If the two child supernodes are each distributed among q processors, then this communication is equivalent to an all-to-all personalized communication <ref> [81] </ref> among 2q processors with a data size of roughly t =q on each processor. This communication can be accomplished in time proportional to t =q, which is asymptotically smaller than the 2.q/ C 2.t / time spent during the pipelined computation phase at the child supernodes. <p> Thus, the overall scalability cannot be better than that of solving the topmost N 2=3 fi N 2=3 dense triangular system in parallel, which is 2. p 2 /. 4.8.4 Data Distribution for Efficient Triangular Solution In Section 4.8.1 and in <ref> [81] </ref>, we discuss that in order to implement the steps of dense triangular solution efficiently, the matrix must be partitioned among the processors along the rows or along the columns. <p> As shown in Figure 4.19, the redistribution is equivalent to a transposition of each .n= p q/ fi t rectangular block of the supernode among the p q processor on which it is horizontally partitioned. This is an all-to-all personalized communication operation <ref> [81] </ref> among p q processors with each processor holding nt =q words of data. Although Figure 4.19 illustrates redistribution with a plain block partitioning, both the procedure and the cost of redistribution are the same with block-cyclic partitioning as well. <p> Although Figure 4.19 illustrates redistribution with a plain block partitioning, both the procedure and the cost of redistribution are the same with block-cyclic partitioning as well. The communication time for this all-to-all personalized 137 partitioning. operation is 2.nt =q/ <ref> [81] </ref>. Note that for solving a triangular system with a single right-hand side, each processor performs 2.nt =q/ computation while processing an n fit supernode on q processors.
Reference: [82] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Solutions Manual for Introduction to Parallel Computing. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: Figure 4.9 shows the communication for one step of dense Cholesky factorization of a hypothetical frontal matrix for q D 16. It is shown in <ref> [82] </ref> that although this communication does not take place between the nearest neighbors on a subcube, the paths of all communications on any subcube are conflict free with e-cube routing [107, 81] and cut-through or worm-hole flow control. <p> We have shown in Section 4.5 that the overall communication overhead in this case is 2.N 4=3 p p/. To maintain a fixed efficiency, N 2 / N 4=3 p p A lower bound on the isoefficiency function for dense matrix factorization is 2. p 1:5 / <ref> [81, 82] </ref> if the number of rank-1 updates performed by the serial algorithm is proportional to the rank of the matrix. The factorization of a sparse matrix derived from an N -node graph with an S.N /-node separator involves a dense S.N / fi S.N / matrix factorization.
Reference: [83] <author> Vipin Kumar, Ananth Grama, and V. Nageshwara Rao. </author> <title> Scalable load balancing techniques for parallel computers. </title> <type> Technical Report 91-55, </type> <institution> Computer Science Department, University of Minnesota, </institution> <year> 1991. </year> <note> To appear in Journal of Distributed and Parallel Computing, 1994. 158 </note>
Reference: [84] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 379-391, </pages> <year> 1994. </year> <note> Also available as Technical Report TR 91-18, </note> <institution> Department of Computer Science Department, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: We present a detailed survey of these metrics in <ref> [84] </ref>. After reviewing these various measures of scalability, one may ask whether there exists one measure that is better than all others [66]? The answer to this question is no, as different measures are suitable for different situations. <p> The isoefficiency metric <ref> [81, 51, 84] </ref> comes in as a handy tool to study the fixed efficiency characteristics of a parallel system. The isoefficiency function relates the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors used.
Reference: [85] <author> Vipin Kumar and V. N. Rao. </author> <title> Parallel depth-first search, part II: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(6) </volume> <pages> 501-519, </pages> <year> 1987. </year>
Reference-contexts: The scalability analysis can also predict the impact of changing hardware technology on the performance and thus help design better parallel architectures for solving various problems. In this chapter, we discuss in detail a popular and useful scalability metric, the isoefficiency function, first proposed by Kumar and Rao <ref> [85] </ref> in the context of depth-first search. We present 6 results that generalize this metric to subsume many others proposed in the literature. We also survey some properties of the common performance metrics, such as, parallel run time, speedup, and efficiency. <p> Clearly, this can be done only for scalable parallel systems, which are exactly the ones for which a fixed efficiency can be maintained for arbitrarily large p by simply increasing the problem size. For such systems, it is natural to use isoefficiency function or related metrics <ref> [85, 80, 22] </ref>. The analyses in [148, 149, 36, 97, 105, 134, 34] also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other. <p> The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [85, 88, 125, 56, 54, 58, 144, 143, 52] </ref>. As illustrated by a variety of examples in this chapter (these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc.), on almost all parallel architectures of interest.
Reference: [86] <author> Vipin Kumar and V. N. Rao. </author> <title> Load balancing on the hypercube architecture. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 603-608, </pages> <year> 1989. </year>
Reference: [87] <author> Vipin Kumar and V. N. Rao. </author> <title> Scalable parallel formulations of depth-first search. </title> <editor> In Vipin Kumar, P. S. Gopalakrishnan, and Laveen N. Kanal, editors, </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference: [88] <author> Vipin Kumar and Vineet Singh. </author> <title> Scalability of parallel algorithms for the all-pairs shortest path problem. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(2) </volume> <pages> 124-138, </pages> <month> October </month> <year> 1991. </year> <note> A short version appears in the Proceedings of the International Conference on Parallel Processing, </note> <year> 1990. </year>
Reference-contexts: Equation 1 For some parallel systems ( e.g., some of the ones discussed in [125] and <ref> [88] </ref>), the maximum obtainable efficiency E max is less than 1. <p> This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) [54], vector dot products (single node accumulation) [58], shortest paths (one-to-all broadcast) <ref> [88] </ref>, and FFT (all-to-all personalized communication) [56], etc. The readers should note that the presence of a global communication operation in an algorithm is a sufficient but not a necessary condition for nonlinear isoefficiency on an architecture with message passing latency. <p> The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [85, 88, 125, 56, 54, 58, 144, 143, 52] </ref>. As illustrated by a variety of examples in this chapter (these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc.), on almost all parallel architectures of interest. <p> Thus it is possible that C .W / of a parallel algorithm may determine the minimum execution time rather than the mathematically derived conditions. For example, consider the implementation of Floyd's algorithm described in <ref> [88] </ref> for finding shortest paths in a graph. In this algorithm, the N fi N adjacency matrix of the graph is striped among p processors such that each processor stores N = p full rows of the matrix. <p> In each of the N iterations of this algorithm, a processor broadcasts a row of length N of the adjacency matrix of the graph to every other processor. As shown in <ref> [88] </ref>, if the p processor are connected in a mesh configuration with cut-through routing, the total overhead due to this communication is given by T o D t s N p 1:5 C t w .N C p/N p. <p> In such cases, an analysis of the overhead function might mislead one into believing that the two parallel systems are equivalent in terms of maximum speedup and minimum execution time. For example, consider a different parallel system consisting of another variation of Floyd's algorithm discussed in <ref> [88] </ref> and a wrap-around mesh with store-and-forward routing. In this algorithm, the N fi N adjacency matrix is partitioned into p sub-blocks of size N= p p each, and these sub-blocks are mapped on a p processor mesh. <p> In this version of Floyd's algorithm, a processor broadcasts N = p p elements among p p processors in each of the N iterations. As shown in <ref> [88] </ref>, this results in a total overhead of T o D t s N p 1:5 C t w N 2 p. Since the expression for T o is same as that in the previous example, p 0 D 1:59N 4=3 =t 2=3 s again.
Reference: [89] <author> C. E. Leiserson. Fat-trees: </author> <title> Universal networks for hardware efficient supercomputing. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 393-402, </pages> <year> 1985. </year>
Reference-contexts: This should be contrasted with the conventional wisdom that suggests that better performance is always obtained using fewer faster processors [15]. 3.2.6 Experimental Results We verified a part of the analysis of Section 3.2.3 through experiments on the CM-5 parallel computer. On this machine, the fat-tree <ref> [89] </ref> like communication network on the CM-5 provides simultaneous paths for communication between all pairs of processors. Hence the CM-5 can be viewed as a fully connected architecture which can simulate a hypercube connected network. We implemented Cannon's and the algorithm and our (GK) variant of the DNS algorithm.
Reference: [90] <author> R. J. Lipton, D. J. Rose, and R. E. Tarjan. </author> <title> Generalized nested dissection. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 </volume> <pages> 346-358, </pages> <year> 1979. </year>
Reference-contexts: All two- and three-dimensional finite-element graphs belong to this class. The properties of separators can be generalized from grids to all such graphs within the same order of magnitude bounds <ref> [91, 90, 45] </ref>. The parallel multifrontal algorithm described in Section 4.4 incurs two types of communication overhead: one during parallel extend-add operations (Figure 4.8) and the other during the steps of dense Cholesky factorization while processing the supernodes (Figure 4.9). <p> If a nested-dissection based ordering scheme is used to number the nodes of the graph corresponding to the coefficient matrix, then the number of nodes t in a supernode at level l is ff p N =2 l , where ff is a small constant <ref> [91, 90, 45, 53] </ref>.
Reference: [91] <author> R. J. Lipton and R. E. Tarjan. </author> <title> A separator theorem for planar graphs. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 36 </volume> <pages> 177-189, </pages> <year> 1979. </year>
Reference-contexts: All two- and three-dimensional finite-element graphs belong to this class. The properties of separators can be generalized from grids to all such graphs within the same order of magnitude bounds <ref> [91, 90, 45] </ref>. The parallel multifrontal algorithm described in Section 4.4 incurs two types of communication overhead: one during parallel extend-add operations (Figure 4.8) and the other during the steps of dense Cholesky factorization while processing the supernodes (Figure 4.9). <p> If a nested-dissection based ordering scheme is used to number the nodes of the graph corresponding to the coefficient matrix, then the number of nodes t in a supernode at level l is ff p N =2 l , where ff is a small constant <ref> [91, 90, 45, 53] </ref>.
Reference: [92] <author> J. W.-H. Liu. </author> <title> The multifrontal method for sparse matrix solution: Theory and practice. </title> <type> Technical Report CS-90-04, </type> <institution> York University, </institution> <address> Ontario, Canada, </address> <year> 1990. </year> <note> Also appears in SIAM Review, </note> <month> 34 </month> <pages> 82-109, </pages> <year> 1992. </year>
Reference-contexts: we discuss triangular solution and symbolic factorization (in that order). 4.3 The Serial Multifrontal Algorithm for Sparse Matrix Factorization The multifrontal algorithm for sparse matrix factorization was proposed independently, and in somewhat different forms, by Speelpening [126] and Duff and Reid [31], and later elucidated in a tutorial by Liu <ref> [92] </ref>. In this section, we briefly describe a condensed version of multifrontal sparse Cholesky factorization. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as shown in Figure 4.3. <p> The functionality of this algorithm is briefly described in Figure 4.13. If elimination of a column is regarded as a basic subtask in Cholesky factorization, then the elimination tree gives a partial ordering of these subtask for correct factorization <ref> [92] </ref>. Our tree balancing algorithm is based on the fact that a modified elimination tree that does not violate the partial order specified by the original tree still leads to correct factorization. <p> The description in this section assumes a single right-hand side vector; however, the algorithm can easily be generalized to multiple right-hand sides by replacing all vector operations by the corresponding matrix operations. Forward Elimination The basic approach to forward elimination is very similar to that of multifrontal numerical factorization <ref> [92] </ref> guided by an elimination tree [93, 81] with the distribution of computation determined by a subtree-to-subcube mapping [46]. A symmetric sparse matrix, its lower triangular Cholesky factor, and the corresponding elimination tree with subtree-to-subcube mapping onto 8 processors is shown in Figure 4.5. <p> One of the blocks of L shown in Figure 4.15 is the dense trapezoidal supernode consisting of nodes 6, 7, and 8. For this supernode, n D 4 and t D 3. As in the case of multifrontal numerical factorization <ref> [92] </ref>, the computation in forward and backward triangular solvers can also be organized in terms of dense matrix operations.
Reference: [93] <author> J. W.-H. Liu. </author> <title> The role of elimination trees in sparse factorization. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 11 </volume> <pages> 134-172, </pages> <year> 1990. </year>
Reference-contexts: Forward Elimination The basic approach to forward elimination is very similar to that of multifrontal numerical factorization [92] guided by an elimination tree <ref> [93, 81] </ref> with the distribution of computation determined by a subtree-to-subcube mapping [46]. A symmetric sparse matrix, its lower triangular Cholesky factor, and the corresponding elimination tree with subtree-to-subcube mapping onto 8 processors is shown in Figure 4.5. <p> Note that the algorithm of Figure 4.21 requires the knowledge of the elimination tree. Elimination tree generation has traditionally been coupled with symbolic factorization <ref> [93, 63] </ref>. In this chapter, we are relying on nested-dissection based ordering strategies that can be computed in parallel and also render the remaining phases of the solution process amenable to parallelization [76]. The elimination tree can be constructed easily (and cheaply) while performing a nested-dissection based ordering.
Reference: [94] <author> Charles Van Loan. </author> <title> Computational Frameworks for the Fast Fourier Transform. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1992. </year>
Reference-contexts: r1 /] := S [.b 0 b l1 0b lC1 b r1 /] C ! .b l b l1 b 0 00/ S [.b 0 b l1 1b lC1 b r1 /]; 10. end; The Binary-Exchange Algorithm In the most commonly used mapping that minimizes communication for the binary-exchange algorithm <ref> [81, 5, 11, 17, 29, 72, 106, 133, 116, 94] </ref>, if .b 0 b 1 b r1 / is the binary representation of i, then for all i, R [i] and S [i] are mapped to processor number .b 0 b d1 /.
Reference: [95] <author> Robert F. Lucas. </author> <title> Solving planar systems of equations on distributed-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <address> Palo Alto, CA, </address> <year> 1987. </year> <month> 159 </month>
Reference: [96] <author> Robert F. Lucas, Tom Blank, and Jerome J. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: However, it is worse than 2. p 1:5 /, which is the asymptotic isoefficiency function derived in Section 4.6. There are two main reasons for this. First, the 2. p 1:5 / isoefficiency function does not take load imbalance into account. It has been shown in <ref> [96] </ref> that even if a grid graph is perfectly partitioned in terms of the number of nodes, the work load associated with each partition varies. The partitions closer to the center of the grid require more computation than the ones on or closer to the periphery.
Reference: [97] <author> Y. W. E. Ma and Denis G. Shea. </author> <title> Downward scalability of parallel architectures. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 109-120, </pages> <year> 1988. </year>
Reference-contexts: For such systems, it is natural to use isoefficiency function or related metrics [85, 80, 22]. The analyses in <ref> [148, 149, 36, 97, 105, 134, 34] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other. <p> Several other researchers have used the minimum parallel execution time of a problem of a given size for analyzing the performance of parallel systems <ref> [105, 97, 108] </ref>. Nussbaum and Agarwal [108] define scalability of an architecture for a given algorithm as the ratio of the algorithm's asymptotic speedup when run on the architecture in question to its corresponding asymptotic speedup when run on an EREW PRAM.
Reference: [98] <author> Dan C. Marinescu and John R. Rice. </author> <title> On high level characterization of parallelism. </title> <type> Technical Report CSD-TR-1011, </type> <institution> CAPO Report CER-90-32, Computer Science Department, Purdue University, West Lafayette, </institution> <note> IN, Revised June 1991. To appear in Journal of Parallel and Distributed Computing, </note> <year> 1993. </year>
Reference-contexts: But as we saw in these examples, this is not the case because the the value of C .W / in the two cases is different. In <ref> [98] </ref>, Marinescu and Rice develop a model to describe and analyze a parallel computation on a MIMD machine in terms of the number of threads of control p into which the computation is divided and the number events g. p/ as a function of p. <p> Usually, the duration of an event or a communication step is not a constant as assumed in <ref> [98] </ref>. In general, both and T o are functions of W and p. If T o is of the form g. p/, Marinescu and Rice [98] derive that the number of processors that will yield maximum speedup will be given by p D .W = C g. p//=g 0 . p/, <p> Usually, the duration of an event or a communication step is not a constant as assumed in <ref> [98] </ref>. In general, both and T o are functions of W and p. If T o is of the form g. p/, Marinescu and Rice [98] derive that the number of processors that will yield maximum speedup will be given by p D .W = C g. p//=g 0 . p/, which can be rewritten as g 0 . p/ D .W C g. p//= p.
Reference: [99] <author> Pontus Matstoms. </author> <title> The multifrontal solution of sparse linear least squares problems. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, Linkoping University, S-581 83 Linkoping, Sweden, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Although we focus on Cholesky factorization of symmetric positive definite matrices in this 92 chapter, the methodology developed here can be adapted for performing Gaussian elimination on diagonally dominant matrices that are almost symmetric in structure [32] and for solving sparse linear least squares problems <ref> [99] </ref>. 4.1 Earlier Research in Sparse Matrix Factorization and Our Contribution Since sparse matrix factorization is the most time consuming phase in the direct solution of a sparse system of linear equations, there has been considerable interest in developing its parallel formulations. <p> matrix factorization. 4.11 Application to Gaussian Elimination and QR Factorization Although we have focussed on sparse Cholesky factorization in this chapter, the serial algorithm of Figure 4.3 can be generalized to Gaussian elimination without pivoting for nearly structurally symmetric sparse matrices [32] and for solving sparse linear least squares problems <ref> [99] </ref>. <p> Matstoms <ref> [99, 100] </ref> has recently developed a multifrontal algorithm for Q R factorization for sparse A. Matstoms' approach avoids storing Q explicitly and is based on the observation that the matrix R is a Cholesky factor of the n fi n symmetric positive definite matrix A T A. <p> These algorithms are based on a one-dimensional partitioning and their isoefficiency function has a lower bound of . p 3 /. The parallel multifrontal algorithm described in this chapter can be modified to along the lines of <ref> [99, 100] </ref> to develop a more scalable parallel formulation of sparse QR factorization. 147 Chapter 5 CONCLUDING REMARKS AND FUTURE WORK In this dissertation, we have presented the results of our research on scalability analysis of parallel algorithms for a variety of numeric computations and on the design of some new
Reference: [100] <author> Pontus Matstoms. </author> <title> Sparse QR factorization in MATLAB. </title> <type> Technical Report LiTH-MAT-R-1992-05, </type> <institution> Department of Mathematics, Linkoping University, S-581 83 Linkoping, Sweden, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Matstoms <ref> [99, 100] </ref> has recently developed a multifrontal algorithm for Q R factorization for sparse A. Matstoms' approach avoids storing Q explicitly and is based on the observation that the matrix R is a Cholesky factor of the n fi n symmetric positive definite matrix A T A. <p> These algorithms are based on a one-dimensional partitioning and their isoefficiency function has a lower bound of . p 3 /. The parallel multifrontal algorithm described in this chapter can be modified to along the lines of <ref> [99, 100] </ref> to develop a more scalable parallel formulation of sparse QR factorization. 147 Chapter 5 CONCLUDING REMARKS AND FUTURE WORK In this dissertation, we have presented the results of our research on scalability analysis of parallel algorithms for a variety of numeric computations and on the design of some new
Reference: [101] <author> Rami Melhem. </author> <title> Toward efficient implementation of preconditioned conjugate gradient methods on vector supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> I(1):70-97, </volume> <year> 1987. </year>
Reference-contexts: As a result there has been a great deal of interest in implementing the Conjugate Gradient algorithm on parallel computers <ref> [6, 12, 62, 73, 79, 101, 122, 138, 139] </ref>.
Reference: [102] <author> Cleve Moler. </author> <title> Another look at Amdahl's law. </title> <type> Technical Report TN-02-0587-0288, </type> <institution> Intel Scientific Computers, </institution> <year> 1987. </year>
Reference-contexts: We call such systems scalable 1 parallel systems. This definition of scalable parallel algorithms is similar to the definition of parallel effective algorithms given by Moler <ref> [102] </ref>. For different parallel systems, W should be increased at different rates with respect to p in order to maintain a fixed efficiency. For instance, in some cases, W might need to grow as an exponential function of p to keep the efficiency from dropping as p increases.
Reference: [103] <author> Mo Mu and John R. Rice. </author> <title> A grid-based subtree-subcube assignment strategy for solving partial differential equations on hypercubes. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13(3) </volume> <pages> 826-839, </pages> <month> May </month> <year> 1992. </year>
Reference: [104] <author> Vijay K. Naik and M. Patrick. </author> <title> Data traffic reduction schemes Cholesky factorization on aynchronous multiprocessor systems. </title> <booktitle> In Supercomputing '89 Proceedings, </booktitle> <year> 1989. </year> <note> Also available as Technical Report RC 14500, </note> <institution> IBM T. J. Watson Research Center, Yorktown Heights, NY. </institution>
Reference-contexts: A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [120, 119, 7, 104, 140, 49] </ref>, and the total communication volume in the best of these schemes [120, 119] is 2.N p 94 Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume.
Reference: [105] <author> David M. Nicol and Frank H. Willard. </author> <title> Problem size, parallel architecture, and optimal speedup. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 404-420, </pages> <year> 1988. </year>
Reference-contexts: Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max .W /, which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [36, 75, 55, 105, 134, 146] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem. <p> For such systems, it is natural to use isoefficiency function or related metrics [85, 80, 22]. The analyses in <ref> [148, 149, 36, 97, 105, 134, 34] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other. <p> Several other researchers have used the minimum parallel execution time of a problem of a given size for analyzing the performance of parallel systems <ref> [105, 97, 108] </ref>. Nussbaum and Agarwal [108] define scalability of an architecture for a given algorithm as the ratio of the algorithm's asymptotic speedup when run on the architecture in question to its corresponding asymptotic speedup when run on an EREW PRAM.
Reference: [106] <author> A. Norton and A. J. Silberger. </author> <title> Parallelization and performance analysis of the Cooley-Tukey FFT algorithm for shared memory architectures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):581-591, </volume> <year> 1987. </year> <month> 160 </month>
Reference-contexts: r1 /] := S [.b 0 b l1 0b lC1 b r1 /] C ! .b l b l1 b 0 00/ S [.b 0 b l1 1b lC1 b r1 /]; 10. end; The Binary-Exchange Algorithm In the most commonly used mapping that minimizes communication for the binary-exchange algorithm <ref> [81, 5, 11, 17, 29, 72, 106, 133, 116, 94] </ref>, if .b 0 b 1 b r1 / is the binary representation of i, then for all i, R [i] and S [i] are mapped to processor number .b 0 b d1 /.
Reference: [107] <author> S. F. </author> <title> Nugent. </title> <booktitle> The iPSC/2 direct-connect communications technology. In Proceedings of the Third Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 51-60, </pages> <year> 1988. </year>
Reference-contexts: It is shown in [82] that although this communication does not take place between the nearest neighbors on a subcube, the paths of all communications on any subcube are conflict free with e-cube routing <ref> [107, 81] </ref> and cut-through or worm-hole flow control. This is a direct consequence of the fact that a circular shift is conflict free on a hypercube with e-cube routing.
Reference: [108] <author> Daniel Nussbaum and Anant Agarwal. </author> <title> Scalability of parallel machines. </title> <journal> Communications of the ACM, </journal> <volume> 34(3) </volume> <pages> 57-61, </pages> <year> 1991. </year>
Reference-contexts: Several other researchers have used the minimum parallel execution time of a problem of a given size for analyzing the performance of parallel systems <ref> [105, 97, 108] </ref>. Nussbaum and Agarwal [108] define scalability of an architecture for a given algorithm as the ratio of the algorithm's asymptotic speedup when run on the architecture in question to its corresponding asymptotic speedup when run on an EREW PRAM. <p> Several other researchers have used the minimum parallel execution time of a problem of a given size for analyzing the performance of parallel systems [105, 97, 108]. Nussbaum and Agarwal <ref> [108] </ref> define scalability of an architecture for a given algorithm as the ratio of the algorithm's asymptotic speedup when run on the architecture in question to its corresponding asymptotic speedup when run on an EREW PRAM.
Reference: [109] <author> Dianne P. O'Leary and G. W. Stewart. </author> <title> Assignment and scheduling in parallel matrix factorization. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 77 </volume> <pages> 275-299, </pages> <year> 1986. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [40, 109, 42, 81] </ref>. <p> The number of such successive elimination steps is equal to the number of nodes in the relaxed supernode being processed. The communication that takes place in this phase is the standard communication in pipelined grid-based dense Cholesky factorization <ref> [109, 81] </ref>. If the average size of the frontal matrices is t fi t during the processing of a relaxed supernode with m nodes on a q-processor subcube, then 2.m/ messages of size 2.t = p q/ are passed through the grid in a pipelined fashion. <p> As shown in Figure 4.9, there are two communication operations involved with each elimination step of dense Cholesky. The average size of a message is .ff N =2 l / ffi . p=2 l / = ff N = p . It can be shown <ref> [109, 81] </ref> that in a pipelined implementation 112 p p q mesh of processors, the communication time for s elimination steps with an average message size of m is 2.ms/.
Reference: [110] <author> V. Pan and J. H. Reif. </author> <title> Efficient parallel solution of linear systems. </title> <booktitle> In 17th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 143-152, </pages> <year> 1985. </year>
Reference-contexts: The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree 1 In <ref> [110] </ref>, Pan and Reif describe a parallel sparse matrix factorization algorithm for a PRAM type architecture.
Reference: [111] <author> Alex Pothen, H. D. Simon, and K.-P. Liou. </author> <title> Partioning sparce matrices with eigenvectors of graphs. </title> <journal> SIAM Journal of Mathematical Analysis and Applications, </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: All times are in seconds. The single processor run times suffixed by * and # were estimated by timing different parts of factorization on two and 32 processors, respectively. 120 nested dissection (SND) <ref> [111, 112, 113] </ref> was used to order the matrices in Table 4.2. This choice of the ordering scheme was prompted by two factors.
Reference: [112] <author> Alex Pothen, H. D. Simon, and Lie Wang. </author> <title> Spectral nested dissection. </title> <type> Technical Report 92-01, </type> <institution> Computer Science Department, Pennsylvania State University, University Park, </institution> <address> PA, </address> <year> 1992. </year>
Reference-contexts: All times are in seconds. The single processor run times suffixed by * and # were estimated by timing different parts of factorization on two and 32 processors, respectively. 120 nested dissection (SND) <ref> [111, 112, 113] </ref> was used to order the matrices in Table 4.2. This choice of the ordering scheme was prompted by two factors.
Reference: [113] <author> Alex Pothen, H. D. Simon, Lie Wang, and Stephen T. Bernard. </author> <title> Towards a fast implementation of spectral nested dissection. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <pages> pages 42-51, </pages> <year> 1992. </year>
Reference-contexts: All times are in seconds. The single processor run times suffixed by * and # were estimated by timing different parts of factorization on two and 32 processors, respectively. 120 nested dissection (SND) <ref> [111, 112, 113] </ref> was used to order the matrices in Table 4.2. This choice of the ordering scheme was prompted by two factors. <p> A drawback of using a serial implementation of SND is that its run time is too high. However, variations of SND such as multilevel SND <ref> [14, 113] </ref> run much faster without compromising on the quality of ordering. 121 From the experimental results in Tables 4.1 and 4.2, we can infer that our algorithm can deliver substantial speedups, even on moderate problem sizes.
Reference: [114] <author> Alex Pothen and Chunguang Sun. </author> <title> Distributed multifrontal factorization using clique trees. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 34-40, </pages> <year> 1991. </year>
Reference-contexts: These speedups are computed with respect to a very efficient serial implementation of the multifrontal algorithm. To lend credibility to our speedup figures, we compared the run times of our program on a single processor with the single processor run times given for iPSC/2 in <ref> [114] </ref> and [127]. The nCUBE2 processors are about 2 to 3 times faster than iPSC/2 processors and our serial implementation, with respect to which the speedups are computed, is 4 to 5 times faster than the one in [114] and [127]. <p> single processor with the single processor run times given for iPSC/2 in <ref> [114] </ref> and [127]. The nCUBE2 processors are about 2 to 3 times faster than iPSC/2 processors and our serial implementation, with respect to which the speedups are computed, is 4 to 5 times faster than the one in [114] and [127]. Our single processor run times are four times less than the single processor run times on iPSC/2 reported in [9].
Reference: [115] <author> Roland Pozo and Sharon L. Smith. </author> <title> Performance evaluation of the parallel multifrontal method in a distributed-memory environment. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 453-456, </pages> <year> 1993. </year>
Reference: [116] <author> Michael J. Quinn. </author> <title> Designing Efficient Algorithms for Parallel Computers. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: Hence, there has been a great interest in implementing FFT on parallel computers [11, 17, 29, 56, 72, 106, 133, 13, 74, 19, 25, 3]. 3.1.1 The FFT Algorithm radix-2 FFT adapted from <ref> [4, 116] </ref>. X is the input vector of length n (n = 2 r for some integer r) and Y is its Fourier Transform. ! k denotes the complex number e j2=nk , where j = p 1. <p> r1 /] := S [.b 0 b l1 0b lC1 b r1 /] C ! .b l b l1 b 0 00/ S [.b 0 b l1 1b lC1 b r1 /]; 10. end; The Binary-Exchange Algorithm In the most commonly used mapping that minimizes communication for the binary-exchange algorithm <ref> [81, 5, 11, 17, 29, 72, 106, 133, 116, 94] </ref>, if .b 0 b 1 b r1 / is the binary representation of i, then for all i, R [i] and S [i] are mapped to processor number .b 0 b d1 /.
Reference: [117] <author> Padma Raghavan. </author> <title> Distributed sparse matrix factorization: QR and Cholesky factorizations. </title> <type> PhD thesis, </type> <institution> Pennsylvania State University, University Park, </institution> <address> PA, </address> <year> 1991. </year>
Reference-contexts: The difference between the multifrontal Q R factorization and the algorithm of Figure 4.3 is that the frontal matrices can be square or rectangular and steps of dense Q R factorization are performed in lines 8-12. Some parallel formulations of sparse QR factorization have been proposed in the literature <ref> [117, 128, 129] </ref>. These algorithms are based on a one-dimensional partitioning and their isoefficiency function has a lower bound of . p 3 /.
Reference: [118] <author> S. Ranka and S. Sahni. </author> <title> Hypercube Algorithms for Image Processing and Pattern Recognition. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: The DNS Algorithm One Element Per Processor Version An algorithm that uses a hypercube with p D n 3 D 2 3q processors to multiply two n fi n matrices was proposed by Dekel, Nassimi and Sahni in <ref> [28, 118] </ref>. The p processors can be visualized as being arranged in an 2 q fi2 q fi2 q array. <p> There are more than one ways to adapt this algorithm to use fewer than n 3 processors. The method proposed by Dekel, Nassimi, and Sahni in <ref> [28, 118] </ref> is as follows. Variant with More than One Element Per Processor This variant of the DNS algorithm can work with n 2 r processors, where 1 &lt; r &lt; n, thus using one processor for more than one element of each of the two n fi n matrices.
Reference: [119] <author> Edward Rothberg. </author> <title> Performance of panel and block approaches to sparse Cholesky factorization on the iPSC/860 and Paragon multicomputers. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year> <month> 161 </month>
Reference-contexts: A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [120, 119, 7, 104, 140, 49] </ref>, and the total communication volume in the best of these schemes [120, 119] is 2.N p 94 Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume. <p> A few schemes with two-dimensional partitioning of the matrix have been proposed [120, 119, 7, 104, 140, 49], and the total communication volume in the best of these schemes <ref> [120, 119] </ref> is 2.N p 94 Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume. It is noteworthy that, on any parallel architecture, the total communication volume is only a lower bound on the overall communication overhead. <p> Our algorithm reduces the communication overhead by a factor of at least 2.log p/ over the best algorithm <ref> [120, 119] </ref> implemented to date. It is also significantly simpler in concept as well as in implementation, which helps in keeping the constant factors associated with the overhead term low.
Reference: [120] <author> Edward Rothberg and Anoop Gupta. </author> <title> An efficient block-oriented approach to parallel sparse Cholesky factorization. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <year> 1992. </year>
Reference-contexts: Since the overall computation is only 2.N 1:5 / [45], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased <ref> [123, 120] </ref>. In [8], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of 2.N p p log N /. <p> A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [120, 119, 7, 104, 140, 49] </ref>, and the total communication volume in the best of these schemes [120, 119] is 2.N p 94 Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume. <p> A few schemes with two-dimensional partitioning of the matrix have been proposed [120, 119, 7, 104, 140, 49], and the total communication volume in the best of these schemes <ref> [120, 119] </ref> is 2.N p 94 Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume. It is noteworthy that, on any parallel architecture, the total communication volume is only a lower bound on the overall communication overhead. <p> Our algorithm reduces the communication overhead by a factor of at least 2.log p/ over the best algorithm <ref> [120, 119] </ref> implemented to date. It is also significantly simpler in concept as well as in implementation, which helps in keeping the constant factors associated with the overhead term low. <p> In other words, the problem size must be increased as 2. p 1:5 / to maintain a constant efficiency as p is increased. In comparison, a lower bound on the isoefficiency function of Rothberg and Gupta's scheme <ref> [120] </ref> with a communication overhead of at least 2.N p 2. p 1:5 .log p/ 3 /. The isoefficiency function of any column-based scheme is at least 2. p 3 / because the total communication overhead has a lower bound of 2.N p/. <p> 256, and 512 if the isoefficiency function is 2. p 1:5 / and 2. p 1:5 .log p/ 3 /, respectively. 2. p 1:5 .log p/ 3 /, which is a lower bound on the isoefficiency function of the previously best known (in terms of total communication volume) parallel algorithm <ref> [120] </ref> for sparse matrix factorization. However, it is worse than 2. p 1:5 /, which is the asymptotic isoefficiency function derived in Section 4.6. There are two main reasons for this. First, the 2. p 1:5 / isoefficiency function does not take load imbalance into account.
Reference: [121] <author> Youcef Saad. SPARSKIT: </author> <title> A basic tool kit for sparse matrix computations. </title> <type> Technical Report 90-20, </type> <institution> Research Institute for Advanced Computer Science, NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <year> 1990. </year>
Reference-contexts: Suitable values of the constants fi and y can be selected to represent the kind of systems being solved. If fi D 1 and y D 1, we have the case of a totally unstructured sparse matrix. The matrix A is stored in the Ellpack-Itpack format <ref> [121] </ref>. In this storage scheme, the non-zero elements of the matrix are stored in an N fi m array while another N fi m integer array stores the column numbers of the matrix elements. <p> It can be shown that co-ordinate and the compressed sparse column storage formats incur much higher communication overheads, thereby leading to unscalable parallel formulations. Two other storage schemes, namely jagged diagonals <ref> [121] </ref> and compressed sparse rows involve communication overheads similar to the Ellpack-Itpack scheme, 80 but the latter is the easiest to work with when the number of non-zero elements is almost the same in each row of the sparse matrix.
Reference: [122] <author> Youcef Saad and M. H. Schultz. </author> <title> Parallel implementations of preconditioned conjugate gradient methods. </title> <type> Technical Report YALEU/DCS/RR-425, </type> <institution> Yale University, Department of Computer Science, </institution> <address> New Haven, CT, </address> <year> 1985. </year>
Reference-contexts: As a result there has been a great deal of interest in implementing the Conjugate Gradient algorithm on parallel computers <ref> [6, 12, 62, 73, 79, 101, 122, 138, 139] </ref>.
Reference: [123] <author> Robert Schreiber. </author> <title> Scalability of sparse direct solvers. </title> <type> Technical Report RIACS TR 92.13, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <month> May </month> <year> 1992. </year> <note> Also appears in A. </note> <editor> George, John R. Gilbert, and J. W.-H. Liu, editors, </editor> <title> Sparse Matrix Computations: Graph Theory Issues and Algorithms (An IMA Workshop Volume). </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers [40, 109, 42, 81]. However, despite inherent parallelism in sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations <ref> [63, 123] </ref> and for several years, it has been a challenge to implement efficient sparse linear system solvers using direct methods on even moderately parallel computers. Performance delivered by most existing parallel sparse matrix factorizations had been quite poor. <p> Performance delivered by most existing parallel sparse matrix factorizations had been quite poor. In <ref> [123] </ref>, Schreiber concludes that it is not yet clear whether sparse direct solvers can be made competitive at all for highly ( p 256) and massively ( p 4096) parallel computers. <p> Since the overall computation is only 2.N 1:5 / [45], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased <ref> [123, 120] </ref>. In [8], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of 2.N p p log N /.
Reference: [124] <author> S. L. Scott and J. R. Goodman. </author> <title> The impact of pipelined channels on k-ary n-cube networks. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 2-16, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: If x &lt; 0:5, then the overall isoefficiency is determined by communication overheads, and is exponential. If x 0:5, then the overall isoefficiency is determined by concurrency. Thus, the best isoefficiency function of 2. p 1:5 log p/ can be obtained at x D :5. Many researchers <ref> [33, 124, 2, 1] </ref> prefer to compare architectures while keeping the number of communication ports per processor (as opposed to bisection width) the same across the architectures.
Reference: [125] <author> Vineet Singh, Vipin Kumar, Gul Agha, and Chris Tomlinson. </author> <title> Scalability of parallel sorting on mesh multicomputers. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(2), </volume> <year> 1991. </year>
Reference-contexts: Equation 1 For some parallel systems ( e.g., some of the ones discussed in <ref> [125] </ref> and [88]), the maximum obtainable efficiency E max is less than 1. <p> The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [85, 88, 125, 56, 54, 58, 144, 143, 52] </ref>. As illustrated by a variety of examples in this chapter (these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc.), on almost all parallel architectures of interest.
Reference: [126] <author> B. Speelpening. </author> <title> The generalized element method. </title> <type> Technical Report UIUCDCS-R-78-946, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <month> November </month> <year> 1978. </year>
Reference-contexts: Therefore, we describe this algorithm in detail first, before we discuss triangular solution and symbolic factorization (in that order). 4.3 The Serial Multifrontal Algorithm for Sparse Matrix Factorization The multifrontal algorithm for sparse matrix factorization was proposed independently, and in somewhat different forms, by Speelpening <ref> [126] </ref> and Duff and Reid [31], and later elucidated in a tutorial by Liu [92]. In this section, we briefly describe a condensed version of multifrontal sparse Cholesky factorization. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as shown in Figure 4.3.
Reference: [127] <author> Chunguang Sun. </author> <title> Efficient parallel solutions of large sparse SPD systems on distributed-memory multiprocessors. </title> <type> Technical Report CTC92TR102, </type> <institution> Advanced Computing Research Institute, Center for Theory and Simulation in Science and Engineering, Cornell University, </institution> <address> Ithaca, NY, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: These speedups are computed with respect to a very efficient serial implementation of the multifrontal algorithm. To lend credibility to our speedup figures, we compared the run times of our program on a single processor with the single processor run times given for iPSC/2 in [114] and <ref> [127] </ref>. The nCUBE2 processors are about 2 to 3 times faster than iPSC/2 processors and our serial implementation, with respect to which the speedups are computed, is 4 to 5 times faster than the one in [114] and [127]. <p> with the single processor run times given for iPSC/2 in [114] and <ref> [127] </ref>. The nCUBE2 processors are about 2 to 3 times faster than iPSC/2 processors and our serial implementation, with respect to which the speedups are computed, is 4 to 5 times faster than the one in [114] and [127]. Our single processor run times are four times less than the single processor run times on iPSC/2 reported in [9].
Reference: [128] <author> Chunguang Sun. </author> <title> Parallel sparse orthogonal factorization on distributed-memory multiprocessors. </title> <type> Technical Report CTC93TR162, </type> <institution> Advanced Computing Research Institute, Center for Theory and Simulation in Science and Engineering, Cornell University, </institution> <address> Ithaca, NY, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: The difference between the multifrontal Q R factorization and the algorithm of Figure 4.3 is that the frontal matrices can be square or rectangular and steps of dense Q R factorization are performed in lines 8-12. Some parallel formulations of sparse QR factorization have been proposed in the literature <ref> [117, 128, 129] </ref>. These algorithms are based on a one-dimensional partitioning and their isoefficiency function has a lower bound of . p 3 /.
Reference: [129] <author> Chunguang Sun. </author> <title> Parallel multifrontal solution of sparse linear least squares problems on distributed-memory multiprocessors. </title> <type> Technical Report CTC94TR185, </type> <institution> Advanced Computing Research Institute, Center for Theory and Simulation in Science and Engineering, Cornell University, </institution> <address> Ithaca, NY, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: The difference between the multifrontal Q R factorization and the algorithm of Figure 4.3 is that the frontal matrices can be square or rectangular and steps of dense Q R factorization are performed in lines 8-12. Some parallel formulations of sparse QR factorization have been proposed in the literature <ref> [117, 128, 129] </ref>. These algorithms are based on a one-dimensional partitioning and their isoefficiency function has a lower bound of . p 3 /.
Reference: [130] <author> Xian-He Sun and John L. Gustafson. </author> <title> Toward a better parallel performance metric. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1093-1109, </pages> <month> December </month> <year> 1991. </year> <note> Also available as Technical Report IS-5053, </note> <institution> UC-32, Ames Laboratory, Iowa State University, Ames, IA. </institution> <month> 162 </month>
Reference: [131] <author> Xian-He Sun and L. M. Ni. </author> <title> Another view of parallel speedup. </title> <booktitle> In Supercomputing '90 Proceedings, </booktitle> <pages> pages 324-333, </pages> <year> 1990. </year>
Reference-contexts: The scalability issues for such problems have been explored by Worley [146], Gustafson [61, 59], and Sun and Ni <ref> [131] </ref>. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [145, 146, 147], Gustafson [61, 59] and by Sun and Ni [131], and is called the memory-constrained case. <p> have been explored by Worley [146], Gustafson [61, 59], and Sun and Ni <ref> [131] </ref>. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [145, 146, 147], Gustafson [61, 59] and by Sun and Ni [131], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p.
Reference: [132] <author> Xian-He Sun and Diane Thiede Rover. </author> <title> Scalability of parallel algorithm-machine combinations. </title> <type> Technical Report IS-5057, </type> <institution> Ames Laboratory, Iowa State University, Ames, IA, </institution> <year> 1991. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Systems. </note>
Reference: [133] <author> P. N. Swarztrauber. </author> <title> Multiprocessor FFTs. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 197-210, </pages> <year> 1987. </year>
Reference-contexts: r1 /] := S [.b 0 b l1 0b lC1 b r1 /] C ! .b l b l1 b 0 00/ S [.b 0 b l1 1b lC1 b r1 /]; 10. end; The Binary-Exchange Algorithm In the most commonly used mapping that minimizes communication for the binary-exchange algorithm <ref> [81, 5, 11, 17, 29, 72, 106, 133, 116, 94] </ref>, if .b 0 b 1 b r1 / is the binary representation of i, then for all i, R [i] and S [i] are mapped to processor number .b 0 b d1 /.
Reference: [134] <author> Zhimin Tang and Guo-Jie Li. </author> <title> Optimal granularity of grid iteration problems. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages I111-I118, </pages> <year> 1990. </year>
Reference-contexts: Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max .W /, which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [36, 75, 55, 105, 134, 146] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem. <p> For such systems, it is natural to use isoefficiency function or related metrics [85, 80, 22]. The analyses in <ref> [148, 149, 36, 97, 105, 134, 34] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other. <p> Clearly, T iso P .W / can be no better than T min P .W /. Several researchers have proposed to use an operating point where the value of p.T P / r is minimized for some constant r and for a given problem size W <ref> [36, 34, 134] </ref>. It can be shown [134] that this corresponds to the point where E S r1 is maximized for a given problem size. <p> Several researchers have proposed to use an operating point where the value of p.T P / r is minimized for some constant r and for a given problem size W [36, 34, 134]. It can be shown <ref> [134] </ref> that this corresponds to the point where E S r1 is maximized for a given problem size. Note that the location of the minima of p.T P / r (with respect to p) for two different algorithm-architecture combinations can be used to choose one between the two. <p> Therefore, in order to achieve a balance between speedup and efficiency, several researchers have proposed to operate at a point where the value of p.T P / r is minimized for some constant r (r 1) and for a given problem size W <ref> [36, 34, 134] </ref>. It can be shown [134] that this corresponds to the point where E S r1 is maximized for a given problem size. p.T P / r D pT P . <p> It can be shown <ref> [134] </ref> that this corresponds to the point where E S r1 is maximized for a given problem size. p.T P / r D pT P . <p> Eager et al. [34] and Tang and Li <ref> [134] </ref> have proposed a criterion of optimality different from optimal speedup. They argue that the optimal operating point should be chosen so that a balance is struck between efficiency and speedup. <p> Eager et. al. and Tang and Li also conclude that for T o D 2. p/, this is also equivalent to operating at a point where the E S product is maximum or p.T P / 2 is minimum. This conclusion in <ref> [34, 134] </ref> is a special case of the more general case that is captured in Equation 2.18. <p> In general, operating at the optimal point or the knee" referred to in [34] and <ref> [134] </ref> for a parallel system with T o D 2. p x j / will be identical to operating at a point where p.T P / r is minimum, where r D 2=.2 x j /. This is obtained from Equation 2.18 for E D 0:5.
Reference: [135] <author> Clark D. Thompson. </author> <title> Fourier transforms in VLSI. </title> <journal> IBM Journal of Research and Development, </journal> <volume> C-32(11):1047-1057, </volume> <year> 1983. </year>
Reference-contexts: Any different mapping of input vector X on the processors does not reduce the communication overhead. It has been shown <ref> [135] </ref> that in any mapping, there will be at least one iteration in which the pairs of processors that need to communicate will be at least p p=2 hops apart.
Reference: [136] <author> Walter F. Tichy. </author> <title> Parallel matrix multiplication on the connection machine. </title> <type> Technical Report RIACS TR 88.41, </type> <institution> Research Institute for Advanced Computer Science, NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <year> 1988. </year>
Reference: [137] <author> Fredric A. Van-Catledge. </author> <title> Towards a general model for evaluating the relative performance of computer systems. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(2) </volume> <pages> 100-108, </pages> <year> 1989. </year>
Reference: [138] <author> Henk A. van der Vorst. </author> <title> A vectorizable variant of some ICCG methods. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> III(3):350-356, </volume> <year> 1982. </year>
Reference-contexts: As a result there has been a great deal of interest in implementing the Conjugate Gradient algorithm on parallel computers <ref> [6, 12, 62, 73, 79, 101, 122, 138, 139] </ref>. <p> We will consider two kinds of preconditioner matrices M - (i) when M is chosen to be a diagonal matrix, usually derived from the principal diagonal of A, and (ii) when M is obtained through a truncated Incomplete Cholesky (IC) factorization <ref> [73, 138] </ref> of A. In the following subsections, we determine 65 1. begin 2. i := 0; x 0 := 0; r 0 := b; ae 0 := jjr 0 jj 2 3. while ( p 4. begin 5. <p> The matrix-vector multiplication operation takes time proportional to 5N . When M is an IC preconditioner, the structure of M is identical to that of A. A method for solving M z = r, originally proposed for vector machines <ref> [138] </ref>, is briefly described below. A detailed description of the same can be found in [81]. As shown in Section 3.3.2, this method is perfectly parallelizable on CM-5 and other architectures ranging from mesh to hypercube. <p> These series may be truncated to .k C 1/ terms where k o N because M is diagonally dominant <ref> [73, 138] </ref>. In our formulation, we form the matrix L = (I + L + L 2 + ... + L k ) explicitly.
Reference: [139] <author> Henk A. van der Vorst. </author> <title> Large tridiagonal and block tridiagonal linear systems on vector and parallel computers. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 45-54, </pages> <year> 1987. </year>
Reference-contexts: As a result there has been a great deal of interest in implementing the Conjugate Gradient algorithm on parallel computers <ref> [6, 12, 62, 73, 79, 101, 122, 138, 139] </ref>.
Reference: [140] <author> Sesh Venugopal and Vijay K. Naik. </author> <title> Effects of partitioning and scheduling sparse matrix factorization on communication and load balance. </title> <booktitle> In Supercomputing '91 Proceedings, </booktitle> <pages> pages 866-875, </pages> <year> 1991. </year>
Reference-contexts: A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [120, 119, 7, 104, 140, 49] </ref>, and the total communication volume in the best of these schemes [120, 119] is 2.N p 94 Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume.
Reference: [141] <author> Sesh Venugopal and Vijay K. Naik. </author> <title> SHAPE: A parallelization tool for sparse matrix computations. </title> <type> Technical Report DCS-TR-290, </type> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: We also found that for some matrices (e.g., that from a 127 fi 127 9-point finite difference grid), our implementation on eight nCUBE2 processors (8.9 seconds) is faster than the 16-processor iPSC/860 implementation (9.7 seconds) reported in <ref> [141] </ref>, although iPSC/860 has much higher computation speeds. 4.7.1 Load Balancing for Factorization The factorization algorithm as described in this chapter requires a binary relaxed supernodal elimination trees that are fairly balanced.
Reference: [142] <author> Jeffrey Scott Vitter and Roger A. Simons. </author> <title> New classes for parallel complexity: A study of unification and other complete problems for P. </title> <journal> IEEE Transactions on Computers, </journal> <month> May </month> <year> 1986. </year>
Reference: [143] <author> Jinwoon Woo and Sartaj Sahni. </author> <title> Hypercube computing: Connected components. </title> <journal> Journal of Supercomputing, </journal> <note> 1991. Also available as TR 88-50 from the Department of Computer Science, </note> <institution> University of Minnesota, Minneapolis, MN. </institution> <month> 163 </month>
Reference-contexts: The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [85, 88, 125, 56, 54, 58, 144, 143, 52] </ref>. As illustrated by a variety of examples in this chapter (these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc.), on almost all parallel architectures of interest.
Reference: [144] <author> Jinwoon Woo and Sartaj Sahni. </author> <title> Computing biconnected components on a hypercube. </title> <journal> Journal of Supercomputing, </journal> <month> June </month> <year> 1991. </year> <note> Also available as Technical Report TR 89-7 from the Department of Computer Science, </note> <institution> University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: The overhead functions of all architecture-algorithm combinations that we have come across fit this form <ref> [85, 88, 125, 56, 54, 58, 144, 143, 52] </ref>. As illustrated by a variety of examples in this chapter (these include important algorithms such as Matrix Multiplication, FFT, Parallel Search, finding Shortest Paths in a graph, etc.), on almost all parallel architectures of interest.
Reference: [145] <author> Patrick H. Worley. </author> <title> Information Requirements and the Implications for Parallel Computation. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, </institution> <address> Palo Alto, CA, </address> <year> 1988. </year>
Reference-contexts: The scalability issues for such problems have been explored by Worley [146], Gustafson [61, 59], and Sun and Ni [131]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley <ref> [145, 146, 147] </ref>, Gustafson [61, 59] and by Sun and Ni [131], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p. <p> A direct corollary of the above result is that if the isoefficiency function is greater than 2. p/, then the minimum parallel execution time will increase even if the problem size is increased as slowly as linearly with the number of processors. Worley <ref> [145, 146, 147] </ref> has shown that for many algorithms used in the scientific domain, for any given T P , there will exist a problem size large enough so that it cannot be solved in time T P , no matter how many processors are used.
Reference: [146] <author> Patrick H. Worley. </author> <title> The effect of time constraints on scaled speedup. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11(5) </volume> <pages> 838-858, </pages> <year> 1990. </year>
Reference-contexts: Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max .W /, which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [36, 75, 55, 105, 134, 146] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem. <p> The scalability issues for such problems have been explored by Worley <ref> [146] </ref>, Gustafson [61, 59], and Sun and Ni [131]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. <p> The scalability issues for such problems have been explored by Worley [146], Gustafson [61, 59], and Sun and Ni [131]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley <ref> [145, 146, 147] </ref>, Gustafson [61, 59] and by Sun and Ni [131], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p. <p> A direct corollary of the above result is that if the isoefficiency function is greater than 2. p/, then the minimum parallel execution time will increase even if the problem size is increased as slowly as linearly with the number of processors. Worley <ref> [145, 146, 147] </ref> has shown that for many algorithms used in the scientific domain, for any given T P , there will exist a problem size large enough so that it cannot be solved in time T P , no matter how many processors are used. <p> It is easily verified that this is a special case of Equation 2.10 for T o D g. p/. Worley <ref> [146] </ref> showed that for certain algorithms, given a certain amount of time T P , there will exist a problem size large enough so that it cannot be solved in time T P , no matter how many processors are used.
Reference: [147] <author> Patrick H. Worley. </author> <title> Limits on parallelism in the numerical solution of linear PDEs. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12 </volume> <pages> 1-35, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The scalability issues for such problems have been explored by Worley [146], Gustafson [61, 59], and Sun and Ni [131]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley <ref> [145, 146, 147] </ref>, Gustafson [61, 59] and by Sun and Ni [131], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p. <p> A direct corollary of the above result is that if the isoefficiency function is greater than 2. p/, then the minimum parallel execution time will increase even if the problem size is increased as slowly as linearly with the number of processors. Worley <ref> [145, 146, 147] </ref> has shown that for many algorithms used in the scientific domain, for any given T P , there will exist a problem size large enough so that it cannot be solved in time T P , no matter how many processors are used.
Reference: [148] <author> Xiaofeng Zhou. </author> <title> Bridging the gap between Amdahl's law and Sandia laboratory's result. </title> <journal> Communications of the ACM, </journal> <volume> 32(8) </volume> <pages> 1014-5, </pages> <year> 1989. </year>
Reference-contexts: For such systems, it is natural to use isoefficiency function or related metrics [85, 80, 22]. The analyses in <ref> [148, 149, 36, 97, 105, 134, 34] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other.
Reference: [149] <author> J. R. Zorbas, D. J. Reble, and R. E. VanKooten. </author> <title> Measuring the scalability of parallel computer systems. </title> <booktitle> In Supercomputing '89 Proceedings, </booktitle> <pages> pages 832-841, </pages> <year> 1989. </year> <month> 164 </month>
Reference-contexts: For such systems, it is natural to use isoefficiency function or related metrics [85, 80, 22]. The analyses in <ref> [148, 149, 36, 97, 105, 134, 34] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other.
References-found: 149

