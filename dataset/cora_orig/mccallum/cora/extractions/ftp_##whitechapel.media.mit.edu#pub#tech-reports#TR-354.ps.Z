URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-354.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: picard@media.mit.edu,  
Title: High-Level and Low-Level Vision average person with a networked computer can now understand why computers
Author: Rosalind W. Picard 
Web: http://www.media.mit.edu/~picard/  
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: MIT Media Laboratory,  
Note: Digital Libraries: Meeting Place for  The  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 354 Also appearing: Asian Conf. on Comp. Vis., Singapore, Dec. 1995. Abstract ." Computer vision for intelligent browsing, querying, and retrieval of imagery is needed now, and yet traditional approaches to computer vision remain far from a general solution to the scene understanding problem. In this paper I discuss the need for a solution based on combining high-level and low-level vision, that works in concert with input from a human user. The solution is based on: 1) Learning from the user what is important visually, and 2) Learning associations between text descriptions and visual data. I describe some recent results in these areas, and overview key challenges for future re search in computer vision for digital libraries.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. B. </author> <title> Lenat, </title> <journal> "Artificial intelligence," Scientific American, </journal> <pages> pp. 80-82, </pages> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Natural language queries and common sense systems also play a significant role; for example, Lenat's CYC common sense reasoning system can take a request such as "find someone wet," and return an image with a label such as "man finishing a marathon" <ref> [1] </ref>. The computer vision solutions will work best if wisely integrated with solutions from these other domains. In [2] I overviewed the latest digital library research issues for the image processing community to address.
Reference: [2] <author> R. W. </author> <title> Picard, "Light-years from Lena: Video and image libraries of the future," </title> <booktitle> in IEEE Second Int. Conf. on Image Proc., </booktitle> <address> (Washington, DC), </address> <month> Oct. </month> <year> 1995. </year> <note> To appear; Also appears as MIT Media Lab Perceptual Computing TR #339. </note>
Reference-contexts: The computer vision solutions will work best if wisely integrated with solutions from these other domains. In <ref> [2] </ref> I overviewed the latest digital library research issues for the image processing community to address. The emphasis for that community is on finding models for simultaneous compression and content description, and improving measures of visual similarity for comparing images. <p> In this paper I will focus on important research problems that I did not discuss in <ref> [2] </ref>, problems which are traditionally closer to the computer vision community namely, generating descriptions of image content, and vision systems that learn.
Reference: [3] <author> A. R. Damasio, </author> <title> Descartes' Error: Emotion, Reason, and the Human Brain. </title> <address> New York, NY: </address> <publisher> Gosset/Putnam Press, </publisher> <year> 1994. </year>
Reference-contexts: In fact, there is recent neurological evidence supporting the role of emotions as a critical biasing mechanism in human decision-making <ref> [3] </ref>. The evidence indicates that humans with a particular kind of brain damage do not have these emotional biasing mechanisms, and consequently suffer a loss of decision-making ability.
Reference: [4] <author> T. P. Minka and R. W. </author> <title> Picard, "Interactive learning using a `society of models'," </title> <note> Submitted for Publication, 1995. Also appears as MIT Media Lab Perceptual Computing TR#349. </note>
Reference-contexts: In other words, it is now necessary to automate more of the iterative learning process usually done by the researcher. Minka and Picard have built a system, "FourEyes," which satisfies the two Learning Criteria above. Although I refer the reader to <ref> [4] </ref> for details of how it learns, how it compares to other learning systems and how its performance has been evaluated, I will highlight a few of its features below to illustrate the arguments in this paper. A diagram of the FourEyes learning system is given in Fig. 1.
Reference: [5] <author> A. S. Chakravarthy, </author> <title> "Toward semantic retrieval of pictures and video," </title> <booktitle> in RIAO'94, Intelligent Multimedia Information Retrieval Systems and Management, </booktitle> <address> (New York), </address> <pages> pp. 676-686, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Consider the request, "Find comedians taking lie-detector tests." Like most requests, this one starts out with language, and will rely heavily on language-processing retrieval tools such as the caption-based retrieval tools of <ref> [5] </ref>, [6]. However, the text had to get there somehow for these tools to be of use.
Reference: [6] <author> R. K. Srihari, </author> <title> "Combining text and image information in content-based retrieval," </title> <booktitle> in IEEE Second Int. Conf. on Image Proc., </booktitle> <address> (Washington, DC), </address> <month> Oct. </month> <year> 1995. </year> <note> To appear. 5 </note>
Reference-contexts: Consider the request, "Find comedians taking lie-detector tests." Like most requests, this one starts out with language, and will rely heavily on language-processing retrieval tools such as the caption-based retrieval tools of [5], <ref> [6] </ref>. However, the text had to get there somehow for these tools to be of use.
References-found: 6

