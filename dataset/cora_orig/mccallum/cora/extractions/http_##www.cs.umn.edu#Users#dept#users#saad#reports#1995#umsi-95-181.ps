URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1995/umsi-95-181.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1995/
Root-URL: http://www.cs.umn.edu
Title: Deflated and augmented Krylov subspace techniques  
Author: Andrew Chapman Yousef Saad 
Keyword: KEY WORDS Deflated GMRES Inner-iteration GMRES Block GMRES Augmented Krylov subspace Flexible GMRES  
Date: April 4, 1996  
Abstract: We present a general framework for a number of techniques based on projection methods on `augmented Krylov subspaces'. These methods include the deflated GM-RES algorithm, an inner-outer FGMRES iteration algorithm, and the class of block Krylov methods. Augmented Krylov subspace methods often show a significant improvement in convergence rate when compared with their standard counterparts using the subspaces of the same dimension. The methods can all be implemented with a variant of the FGMRES algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Erhel, K. Burrage, and B. Pohl. </author> <title> Restarted GMRES preconditioned by deflation. </title> <type> Technical Report -, IRISA, </type> <institution> Rennes, France, </institution> <year> 1994. </year> <note> to appear Journal of Computational and Applied Mathematics, </note> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The first is to exploit block versions of Krylov subspace methods. These block methods are known to be generally more reliable than their scalar equivalents mainly because they tend to better accommodate clustering of eigenvalues around zero. The second technique which has been suggested more recently <ref> [4, 2, 1] </ref>, is to explicitly deflate the matrix from eigenspaces which hamper convergence, typically using approximate eigenvectors associated with eigenvalues nearest zero. <p> This viewpoint is akin to that of preconditioning and is adopted in [2] and <ref> [1] </ref>. The second approach is simply to add the desired eigenvectors directly to the Krylov subspace. For eigenvalue problems this is quite natural [6]. For linear systems, an approach of this sort has been proposed by Morgan [4]. <p> Compute x m = x 0 + M 1 W m y m , where y m = argmin y kfie 1 H m yk 2 , and e 1 = <ref> [1; 0; : : : ; 0] </ref> T . 10. If satisfied stop 11. Set x 0 x m , and p to the number of eigenvalues to be used, 12. <p> We have b Az = b A (x 0 + M 1 W m y) = fiv 1 V m+1 H m y Here e 1 = <ref> [1; 0; : : : ; 0] </ref> T .
Reference: [2] <author> S. A. Kharchenko and A. Yeramin. </author> <title> Eigenvalue translation based preconditioners for the GMRES(k) method. Numerical Linear Algebra with applications, </title> <booktitle> 2, </booktitle> <pages> 51-77, </pages> <year> 1995. </year>
Reference-contexts: The first is to exploit block versions of Krylov subspace methods. These block methods are known to be generally more reliable than their scalar equivalents mainly because they tend to better accommodate clustering of eigenvalues around zero. The second technique which has been suggested more recently <ref> [4, 2, 1] </ref>, is to explicitly deflate the matrix from eigenspaces which hamper convergence, typically using approximate eigenvectors associated with eigenvalues nearest zero. <p> This viewpoint is akin to that of preconditioning and is adopted in <ref> [2] </ref> and [1]. The second approach is simply to add the desired eigenvectors directly to the Krylov subspace. For eigenvalue problems this is quite natural [6]. For linear systems, an approach of this sort has been proposed by Morgan [4].
Reference: [3] <author> R. B. Morgan. </author> <title> Computing interior eigenvalues of large matrices. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 154, </volume> <pages> 289-309, </pages> <year> 1991. </year>
Reference-contexts: We now need only to compute one additional matrix namely V H m W m . A third and better alternative is to use the base BW m for the orthogonality condition as was suggested by Morgan <ref> [4, 3] </ref>. This version of the Rayleigh-Ritz procedure does better at finding eigenvalues nearest zero than the standard projection procedures.
Reference: [4] <author> R. B. Morgan. </author> <title> A restarted GMRES method augmented with eigenvectors. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 16, </volume> -, <year> 1995. </year>
Reference-contexts: The first is to exploit block versions of Krylov subspace methods. These block methods are known to be generally more reliable than their scalar equivalents mainly because they tend to better accommodate clustering of eigenvalues around zero. The second technique which has been suggested more recently <ref> [4, 2, 1] </ref>, is to explicitly deflate the matrix from eigenspaces which hamper convergence, typically using approximate eigenvectors associated with eigenvalues nearest zero. <p> In this paper we present a framework for implementing these three tech-niques using a variant of the FGMRES algorithm given by Morgan <ref> [4] </ref>. We also present some theoretical considerations, and the results of a few numerical experiments. <p> The second approach is simply to add the desired eigenvectors directly to the Krylov subspace. For eigenvalue problems this is quite natural [6]. For linear systems, an approach of this sort has been proposed by Morgan <ref> [4] </ref>. <p> We now need only to compute one additional matrix namely V H m W m . A third and better alternative is to use the base BW m for the orthogonality condition as was suggested by Morgan <ref> [4, 3] </ref>. This version of the Rayleigh-Ritz procedure does better at finding eigenvalues nearest zero than the standard projection procedures. <p> This brings up the question of predicting when to add eigenvectors, and how many. Morgan discusses this issue in <ref> [4] </ref> (section 5), and gives three tests, based on the reduction in residual norm that occurs when eigenvectors are added, and the accuracy of the estimated eigenvectors. The tests are shown to work for some problems, but not for others. <p> We also tried adding eigenvectors with smallest error, instead of those corresponding to the eigenvalues closest to zero. This was not effective. Morgan <ref> [4] </ref> discusses keeping track of which eigenvectors are added, and releasing them when their component in the solution has been resolved. This is in place of continuing to add them if they continue to correspond to an eigenvalue closest to zero. This was not found to be effective.
Reference: [5] <author> A. Ruhe. </author> <title> Implementation aspects of band Lanczos algorithms for computation of eigen-values of large sparse symmetric matrices. </title> <journal> Math. Comp., </journal> <volume> 33, </volume> <pages> 680-687, </pages> <year> 1979. </year>
Reference: [6] <author> Y. Saad. </author> <title> Numerical Methods for Large Eigenvalue Problems. </title> <publisher> Halstead Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: This viewpoint is akin to that of preconditioning and is adopted in [2] and [1]. The second approach is simply to add the desired eigenvectors directly to the Krylov subspace. For eigenvalue problems this is quite natural <ref> [6] </ref>. For linear systems, an approach of this sort has been proposed by Morgan [4].
Reference: [7] <author> Y. Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 14, </volume> <pages> 461-469, </pages> <year> 1993. </year>
Reference-contexts: A third method is the inner-outer iteration using FGMRES, for example, as the outer loop, and any subsidiary iteration for the inner loop, see <ref> [7] </ref>, and the related work [11].
Reference: [8] <author> Y. Saad. </author> <title> Iterative methods for sparse linear systems. </title> <publisher> PWS publishing, </publisher> <address> New York, </address> <year> 1995. </year> <note> to appear. </note>
Reference-contexts: The usual way of solving the above least-squares problems is to use plane rotations [9]. Now p rotations are needed at each step j, instead of only one, to eliminate the elements below the main diagonal in column j of H m . For additional details see <ref> [8] </ref>. 4.2 Separate bases implementation The block Arnoldi algorithm can also be viewed from a different angle by rewriting the block-Krylov subspace as span fv 1 ; Av 1 ; : : : ; A m1 v 1 ; v 2 ; Av 2 ; : : : ; A m1 <p> The proof of the second part of the result is identical with that of the previous proposition, except that x S and r S are replaced by x V and r V respectively. We now consider the analogous situation obtained when a min-res approach is used. As is well-known <ref> [8] </ref>, in a min-res projection method onto an arbitrary subspace K, the initial residual vector is projected out ot its components in the suspace AK. <p> These are extensions of diagonal and SSOR preconditioning, see for example <ref> [8] </ref>. The block sizes used for BDIAG and BSSOR preconditioning are listed in column 6 of table 1. These were chosen based on information on matrix structure, and limited testing.
Reference: [9] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 7, </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: The resulting least-squares problem is similar to the one encountered for GMRES. However, the matrix H m has p entries below the diagonal in place of 1. The usual way of solving the above least-squares problems is to use plane rotations <ref> [9] </ref>. Now p rotations are needed at each step j, instead of only one, to eliminate the elements below the main diagonal in column j of H m .
Reference: [10] <author> V. Simoncini and E. Gallopoulos. </author> <title> An iterative method for nonsymmetric systems with multiple right-hand sides. </title> <note> SIAM J. Sci. Comput., 1995 (to appear). </note>
Reference-contexts: A block-Arnoldi generalization of the Arnoldi process can easily be derived, and a block 7 GMRES algorithm can be defined from it. The standard block GMRES with multiple right--hand sides is described in <ref> [10] </ref>.
Reference: [11] <author> H. A. van der Vorst and C. Vuik. GMRESR: </author> <title> a family of nested GMRES methods. Numerical Linear Algebra with Applications, </title> <booktitle> 1, </booktitle> <pages> 369-386, </pages> <year> 1994. </year> <month> 26 </month>
Reference-contexts: A third method is the inner-outer iteration using FGMRES, for example, as the outer loop, and any subsidiary iteration for the inner loop, see [7], and the related work <ref> [11] </ref>.
References-found: 11

