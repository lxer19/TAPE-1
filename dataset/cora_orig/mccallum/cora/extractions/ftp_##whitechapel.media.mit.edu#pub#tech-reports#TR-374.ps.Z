URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-374.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: fali,cwren,sandyg@media.mit.edu  
Title: Tracking of the Human Body  
Author: Ali Azarbayejani, Christopher Wren, and Alex Pentland 
Address: Cambridge, MA, USA  
Affiliation: MIT Media Laboratory,  
Note: Real-Time 3-D  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 374 Appears in Proceedings of IMAGE'COM 96, Bordeaux, France, May 1996 Abstract People are the central element in the whole enterprise of multimedia and communications and thus visual interpretation of humans and their movements is an important problem for computers. Here we describe a monocular and a stereo system for recovering 3-D descriptions of humans from images in real time. We discuss the technical details and present several applications using the systems for human interface.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ACM SIGGRAPH'. Mandala: </author> <title> Virtual Village, </title> <booktitle> SIGGRAPH-93 Visual Proceedings, Tomorrow's Realities, </booktitle> <year> 1993. </year>
Reference-contexts: Initial exploration into this space of applications was by Krueger [9], who showed that even 2-D binary vision processing of the human form can be used as an interesting interface. More recently the Mandala group <ref> [1] </ref>, has commercialized analog chromakey video processing to isolate colored gloves, etc., worn by users. In both cases, the focus is on the graphics interaction, whereas our systems focus on the visual analysis and go considerably beyond the primitive binary processing of these earlier systems.
Reference: [2] <author> Ali Azarbayejani and Alex Pentland. </author> <title> Real-time self-calibrating stereo person tracking using 3-d shape estimation from blob features. </title> <type> Technical report, </type> <institution> MIT Media Lab, Perceptual Computing Group, </institution> <year> 1996. </year>
Reference-contexts: Spfinder is a recent extension to Pfinder in which a wide-baseline stereo pair of cameras is used to obtain 3-D models. Spfinder has been used in a smaller desk-area environment to capture accurate 3-D movements of head and hands. Applications have included self-calibration from watching a person <ref> [2] </ref> and visually-guided animation, in which a virtual character is driven by human movement. Pfinder and Spfinder utilize a 2-D image analysis architecture with two complementary procedures for 2-D tracking and initialization. <p> It is well-known that calibration can be obtained in this way, but it is not usually done by tracking people <ref> [2] </ref>. The stereo pair shown in Figure 2 (a) shows overlayed blobs and large white boxes marking the current feature locations, and small white boxes representing the subsequent feature tracks. <p> 2-D means and covariance matrices as described in Section 3 while 3-D shapes have 3-D means and covariance matrices m = (x 0 ; y 0 ; z 0 ) x xy xz y yz yz 2 ! Spfinder obtains 3-D parameters from 2-D correspondences using estimation techniques described in <ref> [2] </ref>. A typ ical result is shown in Figure 3 for three blobs representing 4 the head and two hands. The stereo pair indicates the three pairs of 2-D blobs used to obtain the 3-D blobs shown below.
Reference: [3] <author> A. Baumberg and D. Hogg. </author> <title> An efficient method for contour tracking using active shape models. </title> <booktitle> In Proceeding of the Workshop on Motion of Nonrigid and Articulated Objects. IEEE Computer Society, </booktitle> <year> 1994. </year>
Reference-contexts: However, in constrast to Pfinder and Spfinder, these other systems all require accurate initialization and local features, cannot deal with occlusion, and require massive computational resources. Functionally, our systems are perhaps most closely related to the work of Bichsel [4] and Baumberg and Hogg <ref> [3] </ref>. These systems segment the person from the background in real time using only a standard workstation.
Reference: [4] <author> Martin Bichsel. </author> <title> Segmenting simply connected moving objects in a static scene. </title> <journal> Pattern Analysis and Machine Intelligence, </journal> <volume> 16(11) </volume> <pages> 1138-1142, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: However, in constrast to Pfinder and Spfinder, these other systems all require accurate initialization and local features, cannot deal with occlusion, and require massive computational resources. Functionally, our systems are perhaps most closely related to the work of Bichsel <ref> [4] </ref> and Baumberg and Hogg [3]. These systems segment the person from the background in real time using only a standard workstation.
Reference: [5] <author> Trevor Darrell, Bruce Blumberg, Sharon Daniel, Brad Rhodes, Pattie Maes, and Alex Pentland. </author> <title> Alive: Dreams and illusions. </title> <booktitle> In Visual Proceedings, ACM Siggraph, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Pfinder has evolved over several years and has been used to recover a 3-D description of a person in a large room-size space. Pfinder has been used as a real-time interface device for information spaces [17], video games [15], and a distributed virtual reality populated by artificial life <ref> [5] </ref>. It has also been used as a pre-processor for gesture recognition systems, including one that can recognize a forty-word subset of American Sign Language with near perfect accuracy [18]. <p> If you share Pfinder's information about the user between geographically separate locations it is possible to create convincing telepresence without shipping video to the remote site, thus providing very low-bandwidth coding of human action, as in Darrell et al <ref> [5] </ref>. On the remote end information about the user's head, hand, and feet position is used to drive an video avatar that represents the user in the scene. One such avatar is illustrated in Fig. 5 (right).
Reference: [6] <author> Trevor Darrell, Pattie Maes, Bruce Blumberg, and Alex Pentland. </author> <title> A novel environment for situated vision and behavior. </title> <booktitle> In Proc. of CVPR-94 Workshop for Visual Behaviors, </booktitle> <pages> pages 68-72, </pages> <address> Seattle, Washing-ton, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: We have developed several of this kind of application which we call Interactive Video Environments (IVE), including the Artificial Life IVE (ALIVE) system <ref> [6] </ref>. ALIVE utilizes Pfinder's support map polygon to define alpha values for video compositing (placing the user in a scene with some artificial life forms in real-time). <p> Pfinder's gesture tags and feature positions are used by the artificial life forms to make decisions about how to interact with the user, as illustrated in Fig. 4 <ref> [6] </ref>. Pfinder's output can also be used in a much simpler and direct manner. The position of the user and the configuration of the user's appendages can be mapped into a control space, and sounds made by the user can be used to change the operating mode. <p> To make a convincing 3-D world, the video must be placed correctly in the 3-D environment, including graphics occluding the person and vice versa <ref> [6] </ref>. If you share Pfinder's information about the user between geographically separate locations it is possible to create convincing telepresence without shipping video to the remote site, thus providing very low-bandwidth coding of human action, as in Darrell et al [5].
Reference: [7] <author> D. M. Gavrila and L. S. Davis. </author> <title> Towards 3-d model-based tracking and recognition of human movement: a multi-view approach. </title> <booktitle> In International Workshop on Automatic Face- and Gesture-Recognition. IEEE Computer Society, 1995. </booktitle> <address> Zurich. </address>
Reference-contexts: Our systems are also related to body-tracking research such as Rehg and Kanade [13], Rohr [14], and Gavrila and Davis <ref> [7] </ref> that use kinematic models, and Pentland and Horowitz [11] and Metaxas and Terzopolous [10] who use dynamic models. However, in constrast to Pfinder and Spfinder, these other systems all require accurate initialization and local features, cannot deal with occlusion, and require massive computational resources.
Reference: [8] <author> R. J. Kauth, A. P. Pentland, and G. S. Thomas. Blob: </author> <title> An unsupervised clustering approach to spatial preprocessing of mss imagery. </title> <booktitle> In 11th Int'l Symposium on Remote Sensing of the Environment, </booktitle> <address> Ann Arbor, MI, </address> <month> April </month> <year> 1977. </year>
Reference-contexts: The blob representation that we use was developed originally by Kauth et al and Pentland <ref> [12, 8] </ref>, for application to multispectral satillite (MSS) imagery.
Reference: [9] <author> M. W. Krueger. </author> <title> Artificial Reality II. </title> <publisher> Addison Wesley, </publisher> <year> 1990. </year>
Reference-contexts: We begin with a brief review of other relevant research. 2 Background Pfinder and Spfinder have descended from a variety of interesting experiments in human-computer interface and computer mediated communication. Initial exploration into this space of applications was by Krueger <ref> [9] </ref>, who showed that even 2-D binary vision processing of the human form can be used as an interesting interface. More recently the Mandala group [1], has commercialized analog chromakey video processing to isolate colored gloves, etc., worn by users.
Reference: [10] <author> D. Metaxas and D. Terzopoulos. </author> <title> Shape and non-rigid motion estimation through physics-based synthesis. </title> <journal> T-PAMI, </journal> <volume> 15 </volume> <pages> 580-591, </pages> <year> 1993. </year>
Reference-contexts: Our systems are also related to body-tracking research such as Rehg and Kanade [13], Rohr [14], and Gavrila and Davis [7] that use kinematic models, and Pentland and Horowitz [11] and Metaxas and Terzopolous <ref> [10] </ref> who use dynamic models. However, in constrast to Pfinder and Spfinder, these other systems all require accurate initialization and local features, cannot deal with occlusion, and require massive computational resources. Functionally, our systems are perhaps most closely related to the work of Bichsel [4] and Baumberg and Hogg [3].
Reference: [11] <author> A. Pentland and B. Horowitz. </author> <title> Recovery of nonrigid motion and structure. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 13(7) </volume> <pages> 730-742, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Our systems are also related to body-tracking research such as Rehg and Kanade [13], Rohr [14], and Gavrila and Davis [7] that use kinematic models, and Pentland and Horowitz <ref> [11] </ref> and Metaxas and Terzopolous [10] who use dynamic models. However, in constrast to Pfinder and Spfinder, these other systems all require accurate initialization and local features, cannot deal with occlusion, and require massive computational resources.
Reference: [12] <author> Alex Pentland. </author> <title> Classification by clustering. </title> <booktitle> In Proceedings of the Symposium on Machine Processing of Remotely Sensed Data. </booktitle> <publisher> IEEE, IEEE Computer Society Press, </publisher> <month> June </month> <year> 1976. </year>
Reference-contexts: The blob representation that we use was developed originally by Kauth et al and Pentland <ref> [12, 8] </ref>, for application to multispectral satillite (MSS) imagery.
Reference: [13] <author> J.M. Rehg and T. Kanade. </author> <title> Visual tracking of high dof articulated structures: An application to human hand tracking. </title> <booktitle> In ECCV94, </booktitle> <pages> pages B:35-46, </pages> <year> 1994. </year>
Reference-contexts: In both cases, the focus is on the graphics interaction, whereas our systems focus on the visual analysis and go considerably beyond the primitive binary processing of these earlier systems. Our systems are also related to body-tracking research such as Rehg and Kanade <ref> [13] </ref>, Rohr [14], and Gavrila and Davis [7] that use kinematic models, and Pentland and Horowitz [11] and Metaxas and Terzopolous [10] who use dynamic models.
Reference: [14] <author> K. Rohr. </author> <title> Towards model-based recognition of human movements in image sequences. </title> <journal> CVGIPiu, </journal> <volume> 59(1) </volume> <pages> 94-115, </pages> <month> Jan </month> <year> 1994. </year>
Reference-contexts: In both cases, the focus is on the graphics interaction, whereas our systems focus on the visual analysis and go considerably beyond the primitive binary processing of these earlier systems. Our systems are also related to body-tracking research such as Rehg and Kanade [13], Rohr <ref> [14] </ref>, and Gavrila and Davis [7] that use kinematic models, and Pentland and Horowitz [11] and Metaxas and Terzopolous [10] who use dynamic models. However, in constrast to Pfinder and Spfinder, these other systems all require accurate initialization and local features, cannot deal with occlusion, and require massive computational resources.
Reference: [15] <author> Kenneth Russell, Thad Starner, and Alex Pentland. </author> <title> Unencumbered virtual environments. </title> <booktitle> In IJCAI-95 Workshop on Entertainment and AI/Alife, </booktitle> <year> 1995. </year>
Reference-contexts: Pfinder has evolved over several years and has been used to recover a 3-D description of a person in a large room-size space. Pfinder has been used as a real-time interface device for information spaces [17], video games <ref> [15] </ref>, and a distributed virtual reality populated by artificial life [5]. It has also been used as a pre-processor for gesture recognition systems, including one that can recognize a forty-word subset of American Sign Language with near perfect accuracy [18]. <p> This allows the user to control an application with their body directly. This interface has been used to navigate a 3-D virtual game environment called SURVIVE (Simulated Urban Recreational Violence IVE) <ref> [15] </ref> (illustrated in Fig. 5 (left)), and an information landscape / virtual museum [17]. 5.2 Recognition of American Sign Language One interesting application attends only to the spatial statistics of the blobs associated with the users hands. through the Spfinder visual interface Starner and Pentland [18] used this blob representation together
Reference: [16] <author> Rolf Schuster. </author> <title> Color object tracking with adaptive modeling. </title> <booktitle> In Workshop on Visual Behaviors, </booktitle> <pages> pages 91-96, </pages> <address> Seattle, WA, </address> <month> June </month> <year> 1994. </year> <title> International Association for Pattern Recognition, </title> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The blob representation that we use was developed originally by Kauth et al and Pentland [12, 8], for application to multispectral satillite (MSS) imagery. A broadly similar shape-color model has also been investigated by Schuster <ref> [16] </ref>, and has been used to achieve fast tracking of human hands in a cluttered environment. 3 2-D processing We first describe the color and spatial models used in Pfinder and Spfinder followed by a description of the tracking and initialization algorithms. 3.1 Modeling The scene is modeled as a set
Reference: [17] <author> Flavia Sparacino, Christopher Wren, Alex Pentland, and Glorianna Davenport. Hyperplex: </author> <title> a world of 3d interactive digital movies. </title> <booktitle> In IJCAI-95 Workshop on Entertainment and AI/Alife, </booktitle> <year> 1995. </year>
Reference-contexts: Pfinder has evolved over several years and has been used to recover a 3-D description of a person in a large room-size space. Pfinder has been used as a real-time interface device for information spaces <ref> [17] </ref>, video games [15], and a distributed virtual reality populated by artificial life [5]. It has also been used as a pre-processor for gesture recognition systems, including one that can recognize a forty-word subset of American Sign Language with near perfect accuracy [18]. <p> This allows the user to control an application with their body directly. This interface has been used to navigate a 3-D virtual game environment called SURVIVE (Simulated Urban Recreational Violence IVE) [15] (illustrated in Fig. 5 (left)), and an information landscape / virtual museum <ref> [17] </ref>. 5.2 Recognition of American Sign Language One interesting application attends only to the spatial statistics of the blobs associated with the users hands. through the Spfinder visual interface Starner and Pentland [18] used this blob representation together with hidden Markov modeling to interpret a forty word subset of American Sign
Reference: [18] <author> Thad Starner and Alex Pentland. </author> <title> Visual recognition of american sign language using hidden markov models. </title> <booktitle> In International Workshop on Automatic Face and Gesture Recognition, </booktitle> <address> Zurich, Switzerland, </address> <year> 1995. </year>
Reference-contexts: It has also been used as a pre-processor for gesture recognition systems, including one that can recognize a forty-word subset of American Sign Language with near perfect accuracy <ref> [18] </ref>. Spfinder is a recent extension to Pfinder in which a wide-baseline stereo pair of cameras is used to obtain 3-D models. Spfinder has been used in a smaller desk-area environment to capture accurate 3-D movements of head and hands. <p> (Simulated Urban Recreational Violence IVE) [15] (illustrated in Fig. 5 (left)), and an information landscape / virtual museum [17]. 5.2 Recognition of American Sign Language One interesting application attends only to the spatial statistics of the blobs associated with the users hands. through the Spfinder visual interface Starner and Pentland <ref> [18] </ref> used this blob representation together with hidden Markov modeling to interpret a forty word subset of American Sign Language (ASL) with a 99% sign recognition accuracy.
Reference: [19] <author> Chistopher Wren, Ali Azarbayejani, Trevor Darrell, and Alex Pentland. Pfinder: </author> <title> Real-time tracking of the human body. </title> <booktitle> In Photonics East, SPIE Proceedings Vol. 2615, </booktitle> <address> Bellingham, WA, </address> <year> 1995. </year> <booktitle> SPIE. </booktitle> <pages> 6 </pages>
Reference-contexts: Contour In our full-body tracking systems we also utilize contour analysis of the foreground region to bootstrap blob features for building up a blob representation of the person. This is discussed in <ref> [19] </ref>. Occlusion When a blob can find no data to describe (as when a hand or foot is occluded), it is deleted from the person model.
References-found: 19

