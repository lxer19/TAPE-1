URL: ftp://grilled.cs.wisc.edu/technical_papers/w3search.ps.Z
Refering-URL: http://www.cs.wisc.edu/~paradyn/papers.html
Root-URL: 
Email: hollings@cs.wisc.edu bart@cs.wisc.edu  
Title: Dynamic Control of Performance Monitoring on Large Scale Parallel Systems  
Author: Jeffrey K. Hollingsworth Barton P. Miller 
Address: 1210 W. Dayton Street Madison, Wisconsin 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Date: July 19-23, 1993).  
Note: To Appear International Conference on Supercomputing (Tokyo,  
Abstract: Performance monitoring of large scale parallel computers creates a dilemma: we need to collect detailed information to find performance bottlenecks, yet collecting all this data can introduce serious data collection bottlenecks. At the same time, users are being inundated with volumes of complex graphs and tables that require a performance expert to interpret. We present a new approach called the W 3 Search Model, that addresses both these problems by combining dynamic on-the-fly selection of what performance data to collect with decision support to assist users with the selection and presentation of performance data. We present a case study describing how a prototype implementation of our technique was able to identify the bottlenecks in three real programs. In addition, we were able to reduce the amount of performance data collected by a factor ranging from 13 to 700 compared to traditional sampling and trace based instrumentation techniques. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> T. E. Anderson and E. D. Lazowska, "Quartz: </author> <title> A Tool for Tuning Parallel Program Performance", </title> <booktitle> Proc. of the 1990 SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Boston, </address> <month> May </month> <year> 1990, </year> <pages> pp. 115-125. </pages>
Reference-contexts: Performance metrics address the user side of the performance problem by reducing large volumes of performance data into single values or tables of values. Many metrics have been proposed for parallel programs: Critical Path [21], NPT <ref> [1] </ref>, MTOOL [8], Gprof [9]. Each of these metrics can provide useful information; however in an earlier paper [12] we compared several of these metrics (and a few variations) and concluded that no single metric was optimal for all programs.
Reference: 2. <author> P. C. Bates and J. C. Wileden, "EDL: </author> <title> A Basis For Distributed System Debugging Tools", </title> <booktitle> 15th Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1982, </year> <pages> pp. 86-93. </pages>
Reference-contexts: Several approaches have been proposed to address the problem of how to efficiently collect performance data. One approach is to define a set of predicates that describe the interesting events in a program, and only collect data for those events that satisfy the predicate. EDL <ref> [2] </ref>, ISSOS [16], and BEE [4] use this approach. The first two use a static set of predicates for an entire program's execution and lack the fine granularity of control of our approach.
Reference: 3. <author> T. Bemmerl, A. Bode, P. Braum, O. Hansen, T. Tremi and R. Wismuller, </author> <title> The Design and Implementation of TOPSYS, </title> <institution> TUM-INFO-07-71-440, Technische Universitat Munchen, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Examples of predicates are the first time a selected procedure is called, or when the synchronization wait time is above a selected threshold. This method requires less direct involvement by the user. Existing correctness debuggers (e.g., Spider [18], and TOPSYS <ref> [3] </ref>) use similar predicates. A third approach is to have programmers annotate their programs with calls to library routines to indicate major parts of the computation. This approach can be quite effective, but is not very elegant because it requires the programmer to modify code.
Reference: 4. <author> B. Bruegge, </author> <title> "A Portable Platform for Distributed Event Environments", </title> <booktitle> Proc. of the 1991 ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <address> Santa Cruz, CA, </address> <month> May 20-21, </month> <year> 1991, </year> <pages> pp. 184-193. </pages> <note> appears as SIGPLAN Notices, </note> <month> December </month> <year> 1991. </year>
Reference-contexts: One approach is to define a set of predicates that describe the interesting events in a program, and only collect data for those events that satisfy the predicate. EDL [2], ISSOS [16], and BEE <ref> [4] </ref> use this approach. The first two use a static set of predicates for an entire program's execution and lack the fine granularity of control of our approach. BEE permits dynamic control of the predicates, however, it does not provide any guidance of what predicates to select.
Reference: 5. <institution> UNICOS File Formats and Special Files Reference Manual, SR-2014 5.0, Cray Research Inc. </institution>
Reference-contexts: BEE permits dynamic control of the predicates, however, it does not provide any guidance of what predicates to select. Another approach is to build special hardware to collect performance data. The Sequent Symmetry [19], and the Cray Y-MP <ref> [5] </ref> provide a set of pro - -- gramable counters to collect performance data. However, since the systems can collect more data than they have counters, the user is left to select what to collect. In addition, not all interesting events are visible to hardware data collectors.
Reference: 6. <institution> UNICOS Performance Utilities References Manual, SR-2040 6.0, Cray Research Inc. </institution>
Reference-contexts: However, it makes it easy to generate so much data that it swamps any file system or data analysis station. One system that tries to provide high level decision support about the performance of parallel programs is Atexpert <ref> [6] </ref> from Cray Research. It uses rules to recognize performance problems in Fortran programs. This tool solves a special case of the problem we address: Fortran programs that have been automatically parallelized by the compiler.
Reference: 7. <author> D. DeWitt and R. Gerber, </author> <title> "Multiprocessor Hash-Based Join Algorithms", </title> <booktitle> Proc. of the 1985 VLDB Conference, </booktitle> <address> Stockholm, Sweden, </address> <month> August </month> <year> 1985, </year> <pages> pp. 151-164. </pages>
Reference-contexts: Tracing for LocusRoute. 3.4. Shared Memory Join The shared memory join application is an implementation of the join function for a relational database. It implements a hash-join algorithm <ref> [7] </ref> using shared memory for inter-process communication. The program was written to study shared-memory and shared-nothing join algorithms. We ran the program on a dedicated four processor Sequent Symmetry. Our test case ran for 93 seconds. The Performance Consultant identified one bottleneck in the program due to excessive page faults.
Reference: 8. <author> A. J. Goldberg and J. L. Hennessy, </author> <title> "Performance Debugging Shared Memory Multiprocessor Programs with MTOOL", </title> <booktitle> Proc. </booktitle> <address> of Supercomputing'91 , Albuquerque, NM, </address> <month> Nov. </month> <pages> 18-22, </pages> <year> 1991, </year> <pages> pp. 481-490. </pages>
Reference-contexts: Performance metrics address the user side of the performance problem by reducing large volumes of performance data into single values or tables of values. Many metrics have been proposed for parallel programs: Critical Path [21], NPT [1], MTOOL <ref> [8] </ref>, Gprof [9]. Each of these metrics can provide useful information; however in an earlier paper [12] we compared several of these metrics (and a few variations) and concluded that no single metric was optimal for all programs.
Reference: 9. <author> S. L. Graham, P. B. Kessler and M. K. McKusick, </author> <title> "gprof: a Call Graph Execution Profiler", </title> <booktitle> SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <address> Boston, </address> <month> June </month> <year> 1982, </year> <pages> pp. 120-126. </pages>
Reference-contexts: The simplest explanations consist of print statements that describe the type of bottleneck for the hypothesis. More sophisticated explanations report additional information about the program. For example, the explanation for a CPU bottleneck prints a Gprof <ref> [9] </ref> style profile table for the current focus along the ``Where'' axis. Our prototype implementation has an X interface that allows users to navigate the three search axes. We also provide a command line interface to the explanations of the bottlenecks found. <p> Performance metrics address the user side of the performance problem by reducing large volumes of performance data into single values or tables of values. Many metrics have been proposed for parallel programs: Critical Path [21], NPT [1], MTOOL [8], Gprof <ref> [9] </ref>. Each of these metrics can provide useful information; however in an earlier paper [12] we compared several of these metrics (and a few variations) and concluded that no single metric was optimal for all programs.
Reference: 10. <author> M. T. Heath and J. A. Etheridge, </author> <title> "Visualizing the Performance of Parallel Programs", </title> <booktitle> IEEE Software 8, </booktitle> <month> 5 (September </month> <year> 1991), </year> <pages> pp. 29-39. </pages>
Reference-contexts: Visualization presents large amounts of performance data in a graphical or aural way. The problem with most visualizations is that they are only useful for finding a specific type of bottleneck and so most tools provide a rich library of different visualizations. For example Paragraph <ref> [10] </ref> provides over twenty different visualizations and many of these displays can be configured to plot values for different resources (e.g., CPU and disk utilization). Unfortunately the user is left with the formidable task of selecting appropriate visualizations and resources to display.
Reference: 11. <author> J. K. Hollingsworth, R. B. Irvin and B. P. Miller, </author> <title> "The Integration of Application and System Based Metrics in </title>
Reference-contexts: IPS-2 records event traces during a program's execution. Each event (e.g., procedure call or synchronization operation) contains both wall-clock and process time-stamps in addition to some event-specific data. In addition to normal IPS-2 instrumentation, we ran the programs with two External Data Collectors <ref> [11] </ref>. External Data Collectors are dedicated sampling processes that collect additional information not available via tracing. One collector gathered information about the behavior of the operating system (e.g., page faults, context switch rate). The other collected data about the hardware (e.g., cache miss rates and bus utilization). <p> This flexible approach to finding bottlenecks is an important feature of our work. To validate this result, we again used the IPS-2 performance tools. Since we had previously studied this program <ref> [11] </ref>, we recognized the page fault problem as one of the problems in this program. The problem was due to the creation of new user data in the program. A few small changes to the program reduced this page fault behavior and improved the execution time by 10%.
References-found: 11

