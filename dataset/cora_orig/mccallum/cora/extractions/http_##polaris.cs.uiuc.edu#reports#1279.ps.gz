URL: http://polaris.cs.uiuc.edu/reports/1279.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: EVALUATION OF PARALLELIZING COMPILERS  
Author: David A. Padua and Paul M. Petersen 
Address: Urbana, Illinois, 61801, U.S.A.  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Champaign-Urbana  
Abstract: The recognition and exploitation of parallelism is a difficult problem for restructuring compilers. We present a method for evaluating the effectiveness of parallelizing compilers in general and of specific compiler techniques. We also report two groups of measurements that are the results of using this technique. One evaluates a commercially available parallelizer, KAP/Concurrent, and the other compares three different dependence analysis strategies.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Padua and M. Wolfe, </author> <title> "Advanced Compiler Optimization for Supercomputers," </title> <journal> CACM, </journal> <volume> vol. 29, no. 12, </volume> <pages> pp. 1184-1201, </pages> <month> December, </month> <year> 1986. </year>
Reference-contexts: 1 INTRODUCTION In this paper we present a methodology to evaluate the overall effectiveness of parallelizing compilers <ref> [1] </ref> and of individual parallelization techniques. Evaluating the effectiveness of parallelizing compilers is necessary not only to help in the search for more effective translation algorithms, but also to help determine the influence of the translation techniques on the performance of a particular parallel or vector computer.
Reference: [2] <author> D. Kuck, P. Budnik, S.-C. Chen, J. E. Davis, J. Han, P. Kraska, D. Lawrie, Y. Muraoka, R. Strebendt, and R. Towle, </author> <title> "Measurements of Parallelism in Ordinary FORTRAN Programs," </title> <journal> Computer, </journal> <volume> vol. 7, no. 1, </volume> <pages> pp. 37-46, </pages> <month> Jan., </month> <year> 1974. </year>
Reference-contexts: Two evaluation strategies have been used in the past to evaluate parallelizing compilers. The first uses the speedup of library routines or complete applications as a measure of effectiveness. Both ideal machines <ref> [2] </ref> and real computers [3] have been used as the target of the compiler. In the second strategy, the source code can be a collection of loops [4], with the number and type of loops parallelized as the measure of effectiveness. <p> The reason given above to justify this approach is that in many cases these memory-related dependences can be avoided by compiler techniques such as renaming, expansion, and privatization <ref> [2, 13] </ref>. However, a quantitative evaluation of how well compilers can remove memory-related dependences is not available today. A project is underway [14] to evaluate the effectiveness of some of these techniques.
Reference: [3] <author> D. Y. Cheng and D. M. Pase, </author> <title> "An Evaluation of Automatic and Interactive Parallel Programming Tools," </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <pages> pp. 412-422, </pages> <year> 1991. </year>
Reference-contexts: Two evaluation strategies have been used in the past to evaluate parallelizing compilers. The first uses the speedup of library routines or complete applications as a measure of effectiveness. Both ideal machines [2] and real computers <ref> [3] </ref> have been used as the target of the compiler. In the second strategy, the source code can be a collection of loops [4], with the number and type of loops parallelized as the measure of effectiveness.
Reference: [4] <author> D. Callahan, J. Dongarra, and D. Levine, </author> <title> "Vectorizing Compilers: A Test Suite and Results," </title> <booktitle> in Proceedings of Supercomputing '88, </booktitle> <pages> pp. 98-105, </pages> <month> Novem-ber </month> <year> 1988. </year>
Reference-contexts: The first uses the speedup of library routines or complete applications as a measure of effectiveness. Both ideal machines [2] and real computers [3] have been used as the target of the compiler. In the second strategy, the source code can be a collection of loops <ref> [4] </ref>, with the number and type of loops parallelized as the measure of effectiveness. These two strategies do not produce all the information necessary for an accurate evaluation. For example, the speedup of a program does not reflect the inherent parallelism of the source program.
Reference: [5] <author> L. A. Belady, </author> <title> "A Study of Replacement Algorithms for a Virtual-Storage Computer," </title> <journal> IBM Systems Journal, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 78-101, </pages> <year> 1966. </year>
Reference-contexts: The results must reflect the effects of parallelizers on real programs. 3. The technique must take into account the inherent parallelism in the source program. Evaluation methods with these characteristics have been developed in other areas of computer science. For example, the OPT algorithm <ref> [5] </ref> was developed to evaluate page-replacement algorithms. OPT cannot be implemented because each decision requires knowledge of all the memory references to be issued by the program, which in general cannot be known until the program terminates.
Reference: [6] <author> M. Kumar, </author> <title> "Measuring Parallelism in Computation-Intensive Science / Engineering Applications," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, no. 9, </volume> <pages> pp. 5-40, </pages> <year> 1988. </year>
Reference-contexts: The rest of the section describes a method to compute this value. This method was introduced by Kumar <ref> [6] </ref> and later was used by Chen and Yew [7] to measure important characteristics of sequential programs. Any particular execution of a program consists of a series of operations whose results are written to an external device or used by other operations.
Reference: [7] <author> D.-K. Chen, H.-M. Su, and P.-C. Yew, </author> <title> "The Impact of Synchronization and Granularity on Parallel Systems," </title> <booktitle> Proceedings of the 17th Int'l. Symp. on Computer Architecture, </booktitle> <address> Seattle, WA, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: The rest of the section describes a method to compute this value. This method was introduced by Kumar [6] and later was used by Chen and Yew <ref> [7] </ref> to measure important characteristics of sequential programs. Any particular execution of a program consists of a series of operations whose results are written to an external device or used by other operations.
Reference: [8] <author> D. J. Kuck, A. H. Sameh, R. Cytron, A. V. Veidenbaum, C. D. Poly-chronopoulos, G. Lee, T. McDaniel, B. R. Leasure, C. Beckman, J. R. B. Davies, and C. P. Kruskal, </author> <title> "The Effects of Program Restructuring, Algorithm Change, and Architecture Choice on Program Performance," </title> <booktitle> in Proc. of the 1984 International Conf. on Parallel Processing, </booktitle> <pages> pp. 129-138, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: However, such anti dependence does not appear in the graph of Figure 1. This omission is easy to justify because many memory-related dependences can be removed by parallelizing compilers using expansion or privatization <ref> [8] </ref>. Additional arcs, called constraining arcs, may be added to an execution graph if we want to restrict the amount and nature of the parallelism. For example, in the concurrent execution of parallel loops.
Reference: [9] <author> Kuck & Associates, Inc., </author> <title> KAP/Concurrent User's Guide. KAI, </title> <address> Champaign, IL 61820, 1 ed., </address> <month> May </month> <year> 1990. </year> <month> #9005010. </month>
Reference-contexts: Finally, in Section 3.3, we briefly discuss an ongoing project to measure the effectiveness of compiling techniques to remove memory-related dependences. 3.1 OVERALL PERFORMANCE In the experiments reported in this section, all programs were preprocecessed by KAP/Concurrent <ref> [9] </ref>. We used KAP because it is widely regarded as representative of the current state of the art in commercial parallelizing compilers. The specific version used, KAP/Concurrent, was chosen because the target language is standard Fortran-77 with parallel extensions indicated by comment declarations facilitating the interface with our existing tools.
Reference: [10] <author> P. Petersen and D. Padua, </author> <title> "Machine-Independent Evaluation of Paralleliz-ing Compilers," in Advanced Compilation Techniques for Novel Architectures, </title> <month> January </month> <year> 1992. </year>
Reference-contexts: The specific version used, KAP/Concurrent, was chosen because the target language is standard Fortran-77 with parallel extensions indicated by comment declarations facilitating the interface with our existing tools. Tables 1 and 2 show the results from <ref> [10] </ref> which were calculated using the techniques described in Section 2. Table 1 displays the simulated execution times for each of the Perfect Benchmarks. Column A is the sequential execution time of the program on the ideal machine.
Reference: [11] <author> W. Pugh, </author> <title> "The Omega Test: A Fast and Practical Integer Programming Algorithm for Dependence Analysis," </title> <address> Supercomputing'91, </address> <year> 1991. </year>
Reference-contexts: Specifically, we present the evaluation of two dependence tests. One is known as Banerjee's test, and is a fast approximation that is used by several experimental compilers. The second is the Omega test <ref> [11] </ref>, which was recently introduced and is more accurate but slower than Banerjee's test. The question we asked is whether the additional accuracy makes any difference to the detection of parallelism in the Perfect codes.
Reference: [12] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua, </author> <title> "Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs," </title> <booktitle> Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: This is a very encouraging result. As part of this study we examined the recorded logs of manual transformations on the Perfect Benchmarks <ref> [12] </ref>. As is to be expected, in each of the places where the conservative automatic dependence analysis had degraded the program's performance in a significant manner, it was necessary to rewrite the code manually to obtain speedup. <p> The results presented in Section 3.1 seem to indicate that the success rate of parallelizing compilers is still low. Experiments reported elsewhere <ref> [12] </ref> indicate that much better results can be obtained without too much difficulty. One of our current goals is to use the methodology discussed above, not only to measure effectiveness, but also to identify the location and perhaps the nature of the difficulties faced by automatic parallelization techniques.
Reference: [13] <author> P. Feautrier, </author> <title> "Array Expansion," </title> <booktitle> in Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pp. 429-441, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: The reason given above to justify this approach is that in many cases these memory-related dependences can be avoided by compiler techniques such as renaming, expansion, and privatization <ref> [2, 13] </ref>. However, a quantitative evaluation of how well compilers can remove memory-related dependences is not available today. A project is underway [14] to evaluate the effectiveness of some of these techniques.
Reference: [14] <author> P. </author> <type> Tu. PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1993, </year> <note> In Preparation. </note>
Reference-contexts: However, a quantitative evaluation of how well compilers can remove memory-related dependences is not available today. A project is underway <ref> [14] </ref> to evaluate the effectiveness of some of these techniques.
References-found: 14

