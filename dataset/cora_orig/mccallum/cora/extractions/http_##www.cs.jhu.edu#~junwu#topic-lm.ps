URL: http://www.cs.jhu.edu/~junwu/topic-lm.ps
Refering-URL: http://www.cs.jhu.edu/~junwu/publication.html
Root-URL: http://www.cs.jhu.edu/~junwu/
Email: fsanjeev,junwug@mail.clsp.jhu.edu  
Title: A MAXIMUM ENTROPY LANGUAGE MODEL INTEGRATING N-GRAMS AND TOPIC DEPENDENCIES FOR CONVERSATIONAL SPEECH RECOGNITION  
Author: Sanjeev Khudanpur and Jun Wu 
Address: Baltimore, MD 21218-2686  
Affiliation: Center for Language and Speech Processing Johns Hopkins University,  
Abstract: A compact language model which incorporates local dependencies in the form of N-grams and long distance dependencies through dynamic topic conditional constraints is presented. These constraints are integrated using the maximum entropy principle. Issues in assigning a topic to a test utterance are investigated. Recognition results on the Switchboard corpus are presented showing that with a very small increase in the number of model parameters, reduction in word error rate and language model perplexity are achieved over trigram models. Some analysis follows, demonstrating that the gains are even larger on content-bearing words. The results are compared with those obtained by interpolating topic-independent and topic-specific N-gram models. The framework presented here extends easily to incorporate other forms of statistical dependencies such as syntactic word-pair relationships or hierarchical topic constraints. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Bellegarda, </author> <title> Exploiting Both Local and Global Constraints for Multispan Statistical Language Modeling, </title> <booktitle> in Proc. ICASSP'98, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 677-680, </pages> <month> May 12-15, </month> <year> 1998. </year>
Reference-contexts: We present a compact model that integrates these dependencies with N-grams in a statistically sound manner in the maximum entropy (ME) framework. Several models which combine topic related information with N-gram models have been studied, e.g., in <ref> [1, 4, 3, 8, 9, 10] </ref>. The essential idea comes from the information retrieval (IR) literature where extensive use is made of weighted term-frequencies to discern the topic or genre of a document. <p> The work on read speech in [9] is similar; Supported by National Science Foundation Grant No fl dynamics there are modeled by cache-like notions rather than a semantic notion of topic. The approach based on latent semantic analysis recently proposed in <ref> [1] </ref> is a refreshing departure from these methods and presents perplexities on newspaper text. In the method presented here the term-frequencies are treated as topic-dependent salient features of a corpus, just as overall N-gram frequencies are topic-independent salient features. <p> We employ a standard cosine similarity measure commonly used in the IR community <ref> [1, 7] </ref> to assign a topic to test sentences. 2 The null topic, which defaults to a topic-independent baseline model, is available as one of the choices to the topic classifier. Source of Agreement of Utt. Level Topic When Text for Topic Conv. & Utt. Disagreeing With Conv.
Reference: [2] <author> C. Chelba and F. Jelinek, </author> <title> Exploiting Syntactic Structure for Language Modeling, </title> <booktitle> in Proc. COLING-ACL'98, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 225-231, </pages> <month> Aug. </month> <pages> 10-14, </pages> <year> 1998. </year>
Reference-contexts: Since the framework itself extends easily to combining other dependencies, our current efforts are in the direction of exploiting syntactic structure obtained from a left to right partial parse of the utterance as described in <ref> [2] </ref>. The syntactic constraints will provide information which complements both N-grams and topic dependencies.
Reference: [3] <author> S. F. Chen et al, </author> <title> Topic Adaptation for Language Modeling Using Unnormalized Exponential Models, </title> <booktitle> in Proc. ICASSP'98, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 681-684, </pages> <month> May 12-15, </month> <year> 1998. </year>
Reference-contexts: We present a compact model that integrates these dependencies with N-grams in a statistically sound manner in the maximum entropy (ME) framework. Several models which combine topic related information with N-gram models have been studied, e.g., in <ref> [1, 4, 3, 8, 9, 10] </ref>. The essential idea comes from the information retrieval (IR) literature where extensive use is made of weighted term-frequencies to discern the topic or genre of a document. <p> Such a construction however results in fragmentation of the training text, for which the usual remedy is to interpolate such a topic specific N-gram model with a topic-independent model constructed using all the available data. An alternative presented in <ref> [3] </ref> starts off being similar to this work, but then makes ad hoc changes to an exponential model with the limited objective of fast rescoring.
Reference: [4] <author> P. Clarkson and A. Robinson, </author> <title> Language Model Adaptation Using Mixtures and an Exponentially Decaying Cache, </title> <booktitle> in Proc. ICASSP'97, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 799-802, </pages> <month> Apr. </month> <pages> 21-25, </pages> <year> 1997. </year>
Reference-contexts: We present a compact model that integrates these dependencies with N-grams in a statistically sound manner in the maximum entropy (ME) framework. Several models which combine topic related information with N-gram models have been studied, e.g., in <ref> [1, 4, 3, 8, 9, 10] </ref>. The essential idea comes from the information retrieval (IR) literature where extensive use is made of weighted term-frequencies to discern the topic or genre of a document. <p> The essential idea comes from the information retrieval (IR) literature where extensive use is made of weighted term-frequencies to discern the topic or genre of a document. Most schemes <ref> [4, 8, 10] </ref> exploit these differences for language modeling by constructing separate N-gram models for each individual genre or topic to capture these differences. <p> This is comparable to the approach described, e.g., in <ref> [4, 8, 10] </ref>. We construct back-off unigram, bigram and trigram models specific to each topic using the partitioning of the 2.1 million word corpus used for the ME models as described in Section 4.2. We interpolate each topic-specific N-gram with the topic-independent trigram model to obtain smooth topic-dependent N-gram models.
Reference: [5] <author> I. Csiszar, </author> <title> Why Least Squares and Maximum Entropy? An Axiomatic Approach to Inference for Linear Inverse Problems, </title> <journal> The Annals of Statistics, </journal> <volume> Vol. 19, No fl 4, </volume> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: In the method presented here the term-frequencies are treated as topic-dependent salient features of a corpus, just as overall N-gram frequencies are topic-independent salient features. An admissible model is then required to satisfy constraints that reflect both the sets of features. The ME principle <ref> [5] </ref> is used to select a statistical model which meets all these constraints. This method has the advantage that only constraints on those term-frequencies which vary significantly across topics are made topic-dependent while the others are topic-independent.
Reference: [6] <author> J. N. Darroch and D. Ratcliff, </author> <title> Generalized Iterative Scaling for Log-Linear Models, </title> <journal> Annals Math. Stats., </journal> <volume> Vol. 43, </volume> <year> 1972. </year>
Reference-contexts: The first three numerator terms correspond to standard N-gram constraints, while the fourth one is a topic-unigram parameter determined by term-frequencies in a particular topic. 2.2. Computational Issues in ME Model Estimation The generalized iterative scaling (GIS) algorithm <ref> [6] </ref> is used to compute the ME model parameters . Several challenges, predominantly associated with the computational and storage needs of the parameter estimation procedure, must be overcome in order to successfully implement a ME language model (LM) which incorporates topic dependencies with N-gram constraints in a unified manner.
Reference: [7] <author> R. Florian, </author> <title> Exploiting Nonlocal and Syntactic Word Relationships in Language Models, </title> <type> Ph.D. Qualifying Project Report, </type> <institution> CS Dept, Johns Hopkins University, </institution> <month> Aug., </month> <year> 1998. </year>
Reference-contexts: We employ a standard cosine similarity measure commonly used in the IR community <ref> [1, 7] </ref> to assign a topic to test sentences. 2 The null topic, which defaults to a topic-independent baseline model, is available as one of the choices to the topic classifier. Source of Agreement of Utt. Level Topic When Text for Topic Conv. & Utt. Disagreeing With Conv.
Reference: [8] <author> R. Iyer and M. Ostendorf, </author> <title> Modeling Long Range Dependencies in Language, </title> <booktitle> in Proc. ICSLP'96, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 236-239, </pages> <month> Oct. </month> <pages> 3-6, </pages> <year> 1996. </year>
Reference-contexts: We present a compact model that integrates these dependencies with N-grams in a statistically sound manner in the maximum entropy (ME) framework. Several models which combine topic related information with N-gram models have been studied, e.g., in <ref> [1, 4, 3, 8, 9, 10] </ref>. The essential idea comes from the information retrieval (IR) literature where extensive use is made of weighted term-frequencies to discern the topic or genre of a document. <p> The essential idea comes from the information retrieval (IR) literature where extensive use is made of weighted term-frequencies to discern the topic or genre of a document. Most schemes <ref> [4, 8, 10] </ref> exploit these differences for language modeling by constructing separate N-gram models for each individual genre or topic to capture these differences. <p> This is comparable to the approach described, e.g., in <ref> [4, 8, 10] </ref>. We construct back-off unigram, bigram and trigram models specific to each topic using the partitioning of the 2.1 million word corpus used for the ME models as described in Section 4.2. We interpolate each topic-specific N-gram with the topic-independent trigram model to obtain smooth topic-dependent N-gram models.
Reference: [9] <author> R. Kneser et al, </author> <title> Language Model Adaptation Using Dynamic Marginals, </title> <booktitle> in Proc. EUROSPEECH'97, </booktitle> <volume> Vol. 4, </volume> <pages> pp. 1971-1974, </pages> <month> Sept. </month> <pages> 22-25, </pages> <year> 1997. </year>
Reference-contexts: We present a compact model that integrates these dependencies with N-grams in a statistically sound manner in the maximum entropy (ME) framework. Several models which combine topic related information with N-gram models have been studied, e.g., in <ref> [1, 4, 3, 8, 9, 10] </ref>. The essential idea comes from the information retrieval (IR) literature where extensive use is made of weighted term-frequencies to discern the topic or genre of a document. <p> An alternative presented in [3] starts off being similar to this work, but then makes ad hoc changes to an exponential model with the limited objective of fast rescoring. The work on read speech in <ref> [9] </ref> is similar; Supported by National Science Foundation Grant No fl dynamics there are modeled by cache-like notions rather than a semantic notion of topic. The approach based on latent semantic analysis recently proposed in [1] is a refreshing departure from these methods and presents perplexities on newspaper text.
Reference: [10] <author> S. C. Martin et al, </author> <title> Adaptive Topic-Dep. Language Modeling Using Word-Based Varigrams, </title> <journal> Proc. EUROSPEECH'97, </journal> <volume> Vol. 3, </volume> <pages> pp. 1447-1450, </pages> <month> Sept. </month> <pages> 22-25, </pages> <year> 1997. </year>
Reference-contexts: We present a compact model that integrates these dependencies with N-grams in a statistically sound manner in the maximum entropy (ME) framework. Several models which combine topic related information with N-gram models have been studied, e.g., in <ref> [1, 4, 3, 8, 9, 10] </ref>. The essential idea comes from the information retrieval (IR) literature where extensive use is made of weighted term-frequencies to discern the topic or genre of a document. <p> The essential idea comes from the information retrieval (IR) literature where extensive use is made of weighted term-frequencies to discern the topic or genre of a document. Most schemes <ref> [4, 8, 10] </ref> exploit these differences for language modeling by constructing separate N-gram models for each individual genre or topic to capture these differences. <p> This is comparable to the approach described, e.g., in <ref> [4, 8, 10] </ref>. We construct back-off unigram, bigram and trigram models specific to each topic using the partitioning of the 2.1 million word corpus used for the ME models as described in Section 4.2. We interpolate each topic-specific N-gram with the topic-independent trigram model to obtain smooth topic-dependent N-gram models.
Reference: [11] <author> S. Young, J. Jansen, J. Odell, D. Ollasen, P. Woodland, </author> <title> The HTK Book (Version 2.0), </title> <address> ECRL, Cambridge, </address> <year> 1995. </year>
Reference-contexts: The performance of various LMs is evaluated on a test set of 19 conversations (38 conversation sides) comprising 18000 words in over 2400 utterances. For every test utterance, a list of the 2500-best hypotheses is generated by an HTK-based recognizer <ref> [11] </ref> using state-clustered crossword triphone HMMs with Gaussian mixture output densities and a back-off bigram LM. The recognition word error rate (WER) for rescoring these hypotheses and the average perplexity of the transcriptions of the test set are reported here. 4.1.
References-found: 11

