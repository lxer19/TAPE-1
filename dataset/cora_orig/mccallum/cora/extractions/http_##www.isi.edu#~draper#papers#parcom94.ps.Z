URL: http://www.isi.edu/~draper/papers/parcom94.ps.Z
Refering-URL: http://www.isi.edu/~draper/papers/papers.html
Root-URL: http://www.isi.edu
Email: e-mail: draper@pine.ece.utexas.edu, ghosh@pine.ece.utexas.edu  
Title: The M-Cache: A Message-Handling Mechanism for Multicomputer Systems  
Author: Jeffrey T. Draper and Joydeep Ghosh 
Note: Contents  
Date: July 11, 1994  
Address: Austin, Texas 78712-1084  
Affiliation: Department of Electrical and Computer Engineering The University of Texas  
Abstract-found: 0
Intro-found: 1
Reference: [AS88] <author> W. C. Athas and C. Seitz. </author> <title> Multicomputers: Message-passing concurrent computers. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 9-25, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: For fine-grain, concurrent programs in which the number of instructions executed between message actions is typically much less than one hundred, reducing the software overhead is of critical importance <ref> [AS88] </ref>. Several hardware mechanisms have been proposed for reducing the message-handling overhead in message-based systems. The use of a Smart Bus/Smart Shared Memory in conjunction with a coprocessor that executes the message-passing kernel has been demonstrated as offering improved performance in message-based, distributed operating systems [R + 90]. <p> Based on observation of message activity in a Symult 2010/Cantor environment at the University of Texas, the percentage of non-cacheable messages is conjectured to be very low. (For more detail concerning the Symult 2010 and Cantor, see <ref> [AS88] </ref>. Any incoming message packet is first stored in the Receive Buffer of the NI. For incoming packets that must be placed in memory, the Receive Memory Address Register points to the location in which the message is to be stored.
Reference: [Ath87] <author> William C. Athas. </author> <title> Fine-grain concurrent computation. </title> <type> Technical Report 5242:TR:87, </type> <institution> California Institute of Technology, </institution> <year> 1987. </year>
Reference-contexts: It is often necessary for a process to be able to choose the next message to consume from a set of incoming messages. Processes that perform this message selection operation are termed message-directed <ref> [Ath87] </ref>. A process performs message selection by specifying a template which messages must match in order to be processed. <p> Although message-driven processes execute more efficiently than message-directed processes, there are many applications for which a message-driven program is not easily formulated <ref> [Ath87] </ref>. 2.1.2 The Probe Operation The probe operation is a term used to refer to the steps taken by a message-directed process during message selection. Most specifically, a probe occurs when a process determines if a message that matches the specified template is present in the process's incoming message queue.
Reference: [Ath90] <author> William Athas. Physically-compact, </author> <title> high-performance multicomputers. </title> <booktitle> In Proceedings of the Sixth MIT Conference on Advanced Research in VLSI, </booktitle> <pages> pages 302-13, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: To keep pace with the projected improvements in routing and interconnect hardware <ref> [Ath90] </ref>, the bandwidth between local node storage and the message-routing network must increase by an order of magnitude in the next few years.
Reference: [Bak90] <author> H. B. Bakoglu. </author> <title> Circuits, Interconnections, and Packaging for VLSI. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Next-generation multicomputer systems are predicted to evolve with no substantial change in their machine organizations from the current generation, but are expected to benefit significantly from advances in VLSI-based routing automata [Pro91] and high-density interconnect and packaging technologies <ref> [Bak90] </ref>. At present, mesh-style routing chips that operate channels at 200 MB/s (full duplex) are commercially available [Int92]. A node memory with a word width of 32 bits must cycle data at 20ns to source or sink data at this rate. <p> Single-chip implementations of memories of this size have been available for several years <ref> [Bak90] </ref>. An M-cache with these parameters is projected to provide at least a forty-fold speedup in executing successful probe operations in the Symult 2010/Cantor environment. Such an improvement makes the cost-effective implementation of fine-grain concurrent programs on multicomputers feasible. 26 Acknowledgments We would like to thank William C.
Reference: [CG89] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-58, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: If no maximum packet size is specified, such as for the iPSC/860, all messages may simply be viewed as one-packet messages. The packet format described above is used in the Intel series of multicomputers. Although other communication models such as Linda <ref> [CG89] </ref> exist, they are generally independent of system architectures and must be implemented with lower-level models such as the one described. 7 r r r r message packet n 1 n n + 1 packet packet packet data q q q q q 6 type header 2.3 Design of the M-Cache <p> This scheme is similar to the one defined for the representation of vectors in Linda <ref> [CG89] </ref>. Although this time-stamping is extra overhead for systems that maintain incoming messages in a FIFO queue and use fixed-path routing, future systems are likely to use adaptive routing algorithms, such as MECA [DG92], for packet delivery.
Reference: [D + 92] <author> William J. Dally et al. </author> <title> The message-driven processor: A multicomputer processing node with efficient mechanisms. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 23-39, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The use of a Smart Bus/Smart Shared Memory in conjunction with a coprocessor that executes the message-passing kernel has been demonstrated as offering improved performance in message-based, distributed operating systems [R + 90]. This mechanism provides hardware support for message decoding and queuing operations. The Message-Driven Processor (MDP) <ref> [D + 92] </ref> is a custom microcomputer which provides automatic message buffering and is able to dispatch a task for the processing of an incoming message in less than 2s. Hardware support for Ethernet is available for off-loading communications protocols onto front-end processors [Int83].
Reference: [Dal90] <author> William J. Dally. </author> <title> Network and processor architecture for message-driven computers. </title> <editor> In R. Suaya and G. Birtwistle, editors, </editor> <booktitle> VLSI and Parallel Computation, </booktitle> <pages> pages 140-222. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference: [DG92] <author> Jeffrey T. Draper and Joydeep Ghosh. </author> <title> Multipath E-Cube Algorithms (MECA) for adaptive wormhole routing and broadcasting in k-ary n-cubes. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <pages> pages 407-10, </pages> <month> March </month> <year> 1992. </year> <month> 27 </month>
Reference-contexts: This scheme is similar to the one defined for the representation of vectors in Linda [CG89]. Although this time-stamping is extra overhead for systems that maintain incoming messages in a FIFO queue and use fixed-path routing, future systems are likely to use adaptive routing algorithms, such as MECA <ref> [DG92] </ref>, for packet delivery. Adaptive routing algorithms require packets to be time-stamped to ensure that packets sent between the same source-destination pair are consumed in the order they were sent. Thus, systems that employ adaptive routing incur no extra overhead from the addition of M-cache.
Reference: [Dra93] <author> Jeffrey T. Draper. </author> <title> Efficient Message Transport in Multicomputer Systems. </title> <type> PhD thesis, </type> <institution> Department of ECE, University of Texas at Austin, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The exact specification of the Controller/Sequencer is quite lengthy but is straightforward and commensurate with controllers of similar VLSI components. More details about the Controller/Sequencer of an earlier version of the M-cache are given in <ref> [Dra93] </ref>. 2.3.4 Operations Supported in the M-Cache In this section, the operations that allow the processor to interact with the M-cache are defined. The M-cache behaves as a slave device to the processor and is enabled to decode instructions when the processor places appropriate address data on the bus. <p> The predicted values based on the queuing theory model above are also shown in Figure 6. The simulation results agree reasonably well with the analytical model. Simulation results of other runs for one consumer and one producer with varying values of are given in <ref> [Dra93] </ref>. Single consumer, multiple producers: For this case, the simulator deterministically (in a round-robin fashion) selects the producer from which the consumer receives its next message. However, messages sent from the producers may arrive at the receiving node in any order.
Reference: [F + 88] <author> G. Fox et al. </author> <title> Solving Problems on Concurrent Computers. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: On the other hand, processes of many programs perform message operations in a very regular manner <ref> [F + 88] </ref>. For instance, the controlling process in a divide-and-conquer algorithm sends messages to each of the computational child processes and later receives results encapsulated in message packets from each of the processes. This procedure may be repeated any number of times during execution of the algorithm.
Reference: [HB90] <author> Jiun-Ming Hsu and Prithviraj Banerjee. </author> <title> Hardware support for message routing in a distributed memory multicomputer. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 508-15, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Hardware support for Ethernet is available for off-loading communications protocols onto front-end processors [Int83]. A message-passing coprocessor with a virtual channel router is presented in <ref> [HB90] </ref>. This scheme uses circuit switching and provides for the caching of most recently used routing paths. Although all of these techniques aid in message handling, no previous work has suggested hardware support for message selection or an intelligent memory dedicated for message storage and retrieval.
Reference: [HJ92] <author> Dana Henry and Christopher Joerg. </author> <title> A tightly-coupled processor-network interface. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 111-22, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The contents of the RU Workspace may also be transferred to a portion of the NI Workspace. It is frequently necessary in message-passing programs for a process to forward a received message to another process with some slight modification to the received message <ref> [HJ92, SSS88] </ref>. This Workspace-Workspace transfer capability of the M-Cache facilitates a quick execution of this programming characteristic. 2.3.3 Controller/Sequencer Description The Controller/Sequencer (CS) of the M-cache consists of a central finite state machine and various managing components, such as counters and decoders.
Reference: [Hoa78] <author> C. A. R. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 666-77, </pages> <month> August </month> <year> 1978. </year>
Reference-contexts: Another type of algorithm that is frequently seen is one containing communicating sequential processes (CSP) <ref> [Hoa78] </ref>. Each process in this type of algorithm interacts with only one other process at a time by a message exchange operation.
Reference: [Int83] <author> Interlan. </author> <title> N3010A Multibus Ethernet Communications Controller, </title> <year> 1983. </year>
Reference-contexts: The Message-Driven Processor (MDP) [D + 92] is a custom microcomputer which provides automatic message buffering and is able to dispatch a task for the processing of an incoming message in less than 2s. Hardware support for Ethernet is available for off-loading communications protocols onto front-end processors <ref> [Int83] </ref>. A message-passing coprocessor with a virtual channel router is presented in [HB90]. This scheme uses circuit switching and provides for the caching of most recently used routing paths.
Reference: [Int92] <author> Intel Corporation. </author> <title> Paragon Supercomputers, 1992. Order number 203/6/92/10K/GA. </title>
Reference-contexts: At present, mesh-style routing chips that operate channels at 200 MB/s (full duplex) are commercially available <ref> [Int92] </ref>. A node memory with a word width of 32 bits must cycle data at 20ns to source or sink data at this rate. Given that current DRAM cycle times are around an order of magnitude greater than this value [Ng92], the bandwidth mismatch is painfully obvious. <p> It consists of an instruction processor, a memory that provides dual-bank access, and a network interface, and is a typical configuration for a multicomputer node organization <ref> [Int92] </ref>. The network interface is typically a Direct Memory Access (DMA) controller coupled with some buffer space. The connection to the Interconnection Network is commonly achieved via a routing element that is associated with the node. <p> In the model, message operations are assumed to occur at Poisson rates. As the trend in multicomputers is for nodes to support multiprogramming <ref> [Int92] </ref>, it is reasonable to assume that the interaction of many processes sharing a node cause message actions to occur at Poisson rates [Kle75]. On the other hand, processes of many programs perform message operations in a very regular manner [F + 88].
Reference: [Kle75] <author> Leonard Kleinrock. </author> <title> Queueing Systems. </title> <publisher> John Wiley & Sons, </publisher> <year> 1975. </year>
Reference-contexts: For given Poisson arrival and departure rates, the queue activity is given by where p n denotes the steady-state probability that the queue is of length n, represents the mean arrival rate, and denotes the mean departure rate <ref> [Kle75] </ref>. This equation holds only if &lt; 1. In other words, the consumption rate must be faster than the production rate. <p> In the model, message operations are assumed to occur at Poisson rates. As the trend in multicomputers is for nodes to support multiprogramming [Int92], it is reasonable to assume that the interaction of many processes sharing a node cause message actions to occur at Poisson rates <ref> [Kle75] </ref>. On the other hand, processes of many programs perform message operations in a very regular manner [F + 88].
Reference: [Ng92] <author> Ray Ng. </author> <title> Fast computer memories. </title> <journal> IEEE Spectrum, </journal> <volume> 29(10) </volume> <pages> 36-9, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: A node memory with a word width of 32 bits must cycle data at 20ns to source or sink data at this rate. Given that current DRAM cycle times are around an order of magnitude greater than this value <ref> [Ng92] </ref>, the bandwidth mismatch is painfully obvious. This mismatch is also reflected, for example, in the Intel Paragon XP/S architecture, where in each node two data transfer engines are used in conjunction with an i860-based message processor just to cope with the high network bandwidth. <p> C mc is comparable to C ni but is an order of magnitude smaller than C mem , based on SRAM and DRAM access times <ref> [Ng92] </ref>. Therefore, S S 3. Speedup in receiving message packets from the network arises from speedup in data transfer and elimination of context switches.
Reference: [NM93] <author> Lionel M. Ni and Philip K. McKinley. </author> <title> A survey of wormhole routing techniques in direct networks. </title> <booktitle> Computer, </booktitle> <pages> pages 62-76, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The availability of fast routing techniques, such as wormhole routing, has also made the software overheads of message handling at the source/destination nodes comparable to, if not more than, the hardware overhead of message routing through the interconnection network <ref> [NM93] </ref>. Furthermore, once a message is deposited in the memory of the destination node, there is the added software overhead of message selection.
Reference: [Pro91] <institution> Submicron Systems Architecture Project. </institution> <type> Semiannual technical report. Technical report, </type> <institution> Caltech Computer Science, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Next-generation multicomputer systems are predicted to evolve with no substantial change in their machine organizations from the current generation, but are expected to benefit significantly from advances in VLSI-based routing automata <ref> [Pro91] </ref> and high-density interconnect and packaging technologies [Bak90]. At present, mesh-style routing chips that operate channels at 200 MB/s (full duplex) are commercially available [Int92]. A node memory with a word width of 32 bits must cycle data at 20ns to source or sink data at this rate.
Reference: [R + 90] <author> Umakishore Ramachandran et al. </author> <title> Hardware support for interprocess communication. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 318-29, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Several hardware mechanisms have been proposed for reducing the message-handling overhead in message-based systems. The use of a Smart Bus/Smart Shared Memory in conjunction with a coprocessor that executes the message-passing kernel has been demonstrated as offering improved performance in message-based, distributed operating systems <ref> [R + 90] </ref>. This mechanism provides hardware support for message decoding and queuing operations. The Message-Driven Processor (MDP) [D + 92] is a custom microcomputer which provides automatic message buffering and is able to dispatch a task for the processing of an incoming message in less than 2s.
Reference: [SSS88] <author> Charles Seitz, Jakov Seizovic, and Wen-King Su. </author> <title> The C programmer's abbrevi-ated guide to multicomputer programming. </title> <type> Technical Report CS-TR-88-1, </type> <institution> Caltech Computer Science, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: The contents of the RU Workspace may also be transferred to a portion of the NI Workspace. It is frequently necessary in message-passing programs for a process to forward a received message to another process with some slight modification to the received message <ref> [HJ92, SSS88] </ref>. This Workspace-Workspace transfer capability of the M-Cache facilitates a quick execution of this programming characteristic. 2.3.3 Controller/Sequencer Description The Controller/Sequencer (CS) of the M-cache consists of a central finite state machine and various managing components, such as counters and decoders.
Reference: [Sun90] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-39, </pages> <month> December </month> <year> 1990. </year> <month> 29 </month>
Reference-contexts: It is particularly useful for supporting object-oriented programming on multicomputers, where each message selection is performed with respect to the destination object and its invoked method <ref> [Sun90] </ref>. Messages received by a node are either stored in the M-cache or bulk RAM, but not both.
References-found: 22

