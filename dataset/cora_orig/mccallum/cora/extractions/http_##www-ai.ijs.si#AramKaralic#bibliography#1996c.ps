URL: http://www-ai.ijs.si/AramKaralic/bibliography/1996c.ps
Refering-URL: http://www-ai.ijs.si/AramKaralic/bibliography/1996c.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: aram.karalic@ijs.si  
Phone: fax: +386-61-125-10-38  
Title: Producing More Comprehensible Models While Retaining Their Performance  
Author: Aram Karalic 
Keyword: machine learning, regression, real-valued variables, ILP. Area of Interest: Minimum Encoding Length Inference Methods.  
Address: 1000 Ljubljana, Slovenia  
Affiliation: Department of Intelligent Systems Jozef Stefan Institute ph: +386-61-177-37-78  
Abstract: Rissanen's Minimum Description Length (MDL) principle is adapted to handle continuous attributes in the Inductive Logic Programming setting. Application of the developed coding as a MDL pruning mechanism is devised. The behavior of the MDL pruning is tested in a synthetic domain with artificially added noise of different levels and in two real life problems | modelling of the surface roughness of a grinding workpiece and modelling of the mutagenicity of nitroaromatic compounds. Results indicate that MDL pruning is a successful parameter-free noise fighting tool in real-life domains since it acts as a safeguard against building too complex models while retaining the accuracy of the model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ivan Bratko and Saso Dzeroski. </author> <title> Engineering applications of ILP. </title> <journal> New Generation Computing, </journal> <volume> 13 </volume> <pages> 313-333, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction In analysing applications of ILP it has often been observed (e.g. <ref> [1] </ref>), that one of the major practical impediments is lack of facilities for handling numerical information in currently available ILP systems. Therefore, several attempts have been made recently to extend ILP techniques with numerical capabilities [4, 16].
Reference: [2] <author> Wallace C. S. and Boulton D. </author> <title> M._An in-formation measure for classification. </title> <journal> The Computer Journal, </journal> <volume> 11(2) </volume> <pages> 185-194, </pages> <year> 1968. </year>
Reference-contexts: Real-world data are the results of the measurements and therefore have limited precision, known in advance. In fact, all variables in Fors belong to a type, each type having assigned a precision. Wallace and Boul-ton have developed a coding of real-valued attributes <ref> [2] </ref>, however they assume a large population and some knowledge of the distribution. Both of this assumptions are typically not true in the problems we have dealt with, so we have developed a coding schema which does not require knowledge of the distribution of values.
Reference: [3] <author> Wallace C. S. and Patrick J. D._ </author> <title> Coding decision trees. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 7-22, </pages> <year> 1993. </year>
Reference-contexts: Applications of the Rissanen's minimal description length (MDL) principle [14] recently emerged as successful techniques for estimating the point at which a model starts overfit-ting the data, thus fitting the noise present in the data <ref> [13, 3, 11, 9] </ref>. <p> I wish to send you an exact description of the missing column using as few bits as possible. Quinlan and Rivest devise and explore particular coding schema that enables efficient sending of the class column. The coding was improved on in <ref> [3] </ref>. * Muggleton, Srinivasan and Bain [11] apply the MDL principle in the area of Inductive Logic Programming. They use a reference Turing machine T with theory and proofs on the input tape and positive and negative examples on the output tape.
Reference: [4] <author> Rui Camacho. </author> <title> Learning stage trensi-tion rules with Indlog. </title> <editor> In Stefan Wro-bel, editor, </editor> <booktitle> Proceedings of the Fourth International Workshop on Inductive Logic Programming (ILP-94), </booktitle> <pages> pages 273-290, </pages> <address> Bad Honnef/Bonn, Germany, </address> <year> 1994. </year> <institution> Gesellschaft fur Mathematik und Daten-verarbeitung Sankt Augustin. </institution>
Reference-contexts: 1 Introduction In analysing applications of ILP it has often been observed (e.g. [1]), that one of the major practical impediments is lack of facilities for handling numerical information in currently available ILP systems. Therefore, several attempts have been made recently to extend ILP techniques with numerical capabilities <ref> [4, 16] </ref>. One of the approaches, named First Order Regression, is a combination of Inductive Logic Programming and numerical regression. The approach has been implemented in the program Fors (First Order Regression System, [7, 8]). The system proved to be successful in several real-world applications [8].
Reference: [5] <author> Bogdan Filipic, Miha Junkar, Ivan Bratko, and Aram Karalic. </author> <title> An application of machine learning to a metalworking process. </title> <booktitle> In Proceedings of ITI-91, </booktitle> <pages> pages 167-172, </pages> <address> Cavtat, Croatia, </address> <year> 1991. </year>
Reference-contexts: Several machine learning techniques were already applied to the problem, yielding encouraging results <ref> [5, 6] </ref>, and showing that machine learning tools can produce quite adequate models of the system behavior. 5.2.1 Steel Grinding: Data Data were obtained during an experiment in which vibration signals generated by the grinding wheel and the workpiece were detected by an accelerometer sensor and processed by a spectrum analyser.
Reference: [6] <author> Aram Karalic. </author> <title> Employing linear regression in regression tree leaves. </title> <booktitle> In Proceedings of ECAI'92 (European Conference on Artificial Intelligence), </booktitle> <pages> pages 440-441, </pages> <address> Wienna, Austria, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Several machine learning techniques were already applied to the problem, yielding encouraging results <ref> [5, 6] </ref>, and showing that machine learning tools can produce quite adequate models of the system behavior. 5.2.1 Steel Grinding: Data Data were obtained during an experiment in which vibration signals generated by the grinding wheel and the workpiece were detected by an accelerometer sensor and processed by a spectrum analyser.
Reference: [7] <author> Aram Karalic. </author> <title> First Order Regression. </title> <type> PhD thesis, </type> <institution> University of Ljubljana, Faculty for Electrical Engineering and Computer Science, </institution> <year> 1995. </year>
Reference-contexts: Therefore, several attempts have been made recently to extend ILP techniques with numerical capabilities [4, 16]. One of the approaches, named First Order Regression, is a combination of Inductive Logic Programming and numerical regression. The approach has been implemented in the program Fors (First Order Regression System, <ref> [7, 8] </ref>). The system proved to be successful in several real-world applications [8]. In this paper we focus our attention to one single aspect of Fors | pre-pruning mechanism based on the Minimum Description Length principle. <p> Conclusions in Section 6 formulate answers to the stated questions from the point of performed experiments. 2 Fors Algorithm Since the article focuses on one aspect of the algorithm | MDL pruning, this section only briefly describes main ideas of the algorithm. Detailed information can be found in <ref> [7] </ref>. 2.1 Model Language and Fors Input Description Fors produces knowledge in the form of a Prolog program, consisting of clauses of the form: f (Y,V 1 ,V 2 ,: : :,V n ) :- literal 1 , : : : literal n ,!. <p> To avoid overfitting the learning data, several pre-pruning methods are used to limit the amount of clause's adaptation to learning data. They are briefly sketched in section 2.4, while their detailed description can be found in <ref> [7] </ref>. 2.4 Pre-pruning Methods Minimal Number of Examples Each clause must cover at least MinExs examples. This is one of the simplest forms of pre-pruning which assures that statistical sample of the examples in each clause is representative to some degree.
Reference: [8] <author> Aram Karalic. </author> <title> First order regression: Application in real-world domains. </title> <editor> In Jan Zizka and Pavel Brazdil, editors, </editor> <booktitle> Artificial Intelligence Techniques | AIT'95, </booktitle> <address> Brno, Czech Republic, </address> <year> 1995. </year>
Reference-contexts: Therefore, several attempts have been made recently to extend ILP techniques with numerical capabilities [4, 16]. One of the approaches, named First Order Regression, is a combination of Inductive Logic Programming and numerical regression. The approach has been implemented in the program Fors (First Order Regression System, <ref> [7, 8] </ref>). The system proved to be successful in several real-world applications [8]. In this paper we focus our attention to one single aspect of Fors | pre-pruning mechanism based on the Minimum Description Length principle. <p> One of the approaches, named First Order Regression, is a combination of Inductive Logic Programming and numerical regression. The approach has been implemented in the program Fors (First Order Regression System, [7, 8]). The system proved to be successful in several real-world applications <ref> [8] </ref>. In this paper we focus our attention to one single aspect of Fors | pre-pruning mechanism based on the Minimum Description Length principle.
Reference: [9] <author> Matevz Kovacic. </author> <title> Stochastic Inductive Logic Programming. </title> <type> PhD thesis, </type> <institution> Faculty of electrical Engineering and Computer Science, Ljubljana, Slovenia, </institution> <year> 1995. </year>
Reference-contexts: Applications of the Rissanen's minimal description length (MDL) principle [14] recently emerged as successful techniques for estimating the point at which a model starts overfit-ting the data, thus fitting the noise present in the data <ref> [13, 3, 11, 9] </ref>. <p> They also devise a coding schema that enables efficient coding of a theory, proofs, and examples and explore relationship between the compressibility of a clause and its significance and accuracy. * Kovacic <ref> [9] </ref> uses the MDL principle in the area of stochastic inductive logic pro gramming. He suggests, implements and tests an improvement of Muggleton's ap proach. Each of the implementations is characterized by a particular coding technique, chosen to estimate length (in bits) of the theory and examples. <p> is considered compressive if jBj + jAj + jHj + jj &lt; jBj + jE j: (4) During the search only compressive hypotheses are considered promising while the hypothesis not satisfying (4) are pruned immediately. 4.2 Coding Schema The main difference between our coding schema and the schemata used in <ref> [13, 11, 9] </ref> emerges from the fact that we deal also with continuous variables. Therefore, we must drop the requirement of the exact reconstruction of a class value from its attributes, if we want to avoid overfitting the noise.
Reference: [10] <author> Ming Li and Paul Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity and its Applications. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: MDL Pruning The MDL principle states that the best theory to explain the data is the one which minimizes the sum of the length (in bits) of the theory description, and the length (in bits) of the data when coded with the help of the theory <ref> [10] </ref>. A hypothesis (clause) is considered compressive if the sum of the coding lengths of the clause and the proof specifications (which specify how to derive examples' class values from their attribute values, background knowledge, and the hypothesis) is smaller than the sum of coding lengths of examples' class values. <p> Applications of the Rissanen's minimal description length (MDL) principle [14] recently emerged as successful techniques for estimating the point at which a model starts overfit-ting the data, thus fitting the noise present in the data [13, 3, 11, 9]. The MDL principle can be informally stated (as in <ref> [10] </ref>): Minimum Description Length (MDL) principle: The best theory to explain a set of data is the one which minimizes the sum of: * the length, in bits, of the description of the theory; and * the length, in bits, of the data when coded with the help of the theory.
Reference: [11] <author> Stephen Muggleton, Ashwin Srinivasan, and Michael Bain. </author> <title> Compression, significance and accuracy. </title> <booktitle> In Proceedings of Machine Learning Conference 1992, </booktitle> <address> Ab-erdeen, </address> <year> 1992. </year>
Reference-contexts: Applications of the Rissanen's minimal description length (MDL) principle [14] recently emerged as successful techniques for estimating the point at which a model starts overfit-ting the data, thus fitting the noise present in the data <ref> [13, 3, 11, 9] </ref>. <p> I wish to send you an exact description of the missing column using as few bits as possible. Quinlan and Rivest devise and explore particular coding schema that enables efficient sending of the class column. The coding was improved on in [3]. * Muggleton, Srinivasan and Bain <ref> [11] </ref> apply the MDL principle in the area of Inductive Logic Programming. They use a reference Turing machine T with theory and proofs on the input tape and positive and negative examples on the output tape. <p> After the global schema which explains the usage of the MDL pruning in the Fors algorithm, the coding of natural numbers, real-valued attributes, hypotheses, proofs and examples are developed. 4.1 Global Schema of the MDL Pruning The global idea is, as in <ref> [11] </ref>, that in our search of the hypothesis space only compressive hypotheses will be regarded as promising candidates. <p> is considered compressive if jBj + jAj + jHj + jj &lt; jBj + jE j: (4) During the search only compressive hypotheses are considered promising while the hypothesis not satisfying (4) are pruned immediately. 4.2 Coding Schema The main difference between our coding schema and the schemata used in <ref> [13, 11, 9] </ref> emerges from the fact that we deal also with continuous variables. Therefore, we must drop the requirement of the exact reconstruction of a class value from its attributes, if we want to avoid overfitting the noise.
Reference: [12] <author> Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 3(5), </volume> <year> 1990. </year>
Reference-contexts: If the last clause in the model induced so far consists solely of total literals, then it will always succeed and no additional (default) clause needs to be added. 2.2 Top Levels of the Algorithm Fors uses a covering approach similar to the one used in Foil <ref> [12] </ref>. The algorithm repeatedly constructs a clause. When a clause is found, all examples covered by the clause are removed from the learning set. The procedure is repeated while there are enough examples left. At the end of learning a default clause is added if necessary.
Reference: [13] <author> Ross Quinlan and Ronald L. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: Applications of the Rissanen's minimal description length (MDL) principle [14] recently emerged as successful techniques for estimating the point at which a model starts overfit-ting the data, thus fitting the noise present in the data <ref> [13, 3, 11, 9] </ref>. <p> If, on the other hand, the hypothesis describes the examples very accurately, it will be very specialized, therefore requiring more information for its coding. A balance between the two is required. Several implementations of MDL principle have been investigated in machine learning community: * Quinlan and Rivest <ref> [13] </ref> explore the use of the Rissanen's MDL principle for the construction of decision trees. They base their approach on the communication problem: Communication problem: You and I have copies of the data, but in your copy the last column | giving the class of each object | is missing. <p> is considered compressive if jBj + jAj + jHj + jj &lt; jBj + jE j: (4) During the search only compressive hypotheses are considered promising while the hypothesis not satisfying (4) are pruned immediately. 4.2 Coding Schema The main difference between our coding schema and the schemata used in <ref> [13, 11, 9] </ref> emerges from the fact that we deal also with continuous variables. Therefore, we must drop the requirement of the exact reconstruction of a class value from its attributes, if we want to avoid overfitting the noise.
Reference: [14] <author> Jorma Rissanen. </author> <title> Modelling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: An algorithm must therefore avoid fitting the learning data too strongly. Specialization of the clause with the intention to correctly describe learning data usually leads to the increase of its size. Applications of the Rissanen's minimal description length (MDL) principle <ref> [14] </ref> recently emerged as successful techniques for estimating the point at which a model starts overfit-ting the data, thus fitting the noise present in the data [13, 3, 11, 9].
Reference: [15] <author> Jorma Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: the correction to be as small as possible, we should use the coding where the code length monotonically increases with the absolute value of the correction: 8x; y 2 &lt; : jxj jyj ) jxj jyj (5) 4.2.1 Coding of Natural Numbers The coding of natural numbers suggested by Rissanen <ref> [15] </ref> was used. The code length is computed according to the following rules: j0j = 1 (6) +log 2 (C 1 ); (7) where the sum includes only the positive terms and where C 1 = 2:865064 : : :.
Reference: [16] <author> Ashwin Srinivasan. </author> <title> Experiments in numerical reasoning with ILP. </title> <publisher> Oxford University Press, </publisher> <year> 1995. </year> <note> To appear in D. </note> <editor> Michie and S. Muggleton and K. Furukawa, editors, </editor> <booktitle> Machine Intelligence 15. </booktitle>
Reference-contexts: 1 Introduction In analysing applications of ILP it has often been observed (e.g. [1]), that one of the major practical impediments is lack of facilities for handling numerical information in currently available ILP systems. Therefore, several attempts have been made recently to extend ILP techniques with numerical capabilities <ref> [4, 16] </ref>. One of the approaches, named First Order Regression, is a combination of Inductive Logic Programming and numerical regression. The approach has been implemented in the program Fors (First Order Regression System, [7, 8]). The system proved to be successful in several real-world applications [8].

References-found: 16

