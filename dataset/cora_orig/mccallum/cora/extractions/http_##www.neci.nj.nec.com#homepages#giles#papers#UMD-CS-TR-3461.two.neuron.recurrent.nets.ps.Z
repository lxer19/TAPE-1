URL: http://www.neci.nj.nec.com/homepages/giles/papers/UMD-CS-TR-3461.two.neuron.recurrent.nets.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Email: Email: tino@decef.elf.stuba.sk  Email: fhorne,gilesg@research.nj.nec.com  Email: tino@research.nj.nec.com  
Title: Fixed Points in Two-Neuron Discrete Time Recurrent Networks: Stability and Bifurcation Considerations  
Author: Peter Tino Bill G. Horne and C. Lee Giles 
Note: Currently with  Also with  
Address: Ilkovicova 3, 812 19 Bratislava, Slovakia  4 Independence Way Princeton, NJ 08540  College Park, MD 20742  4 Independence Way, Princeton, NJ 08540,  College Park, MD 20742  
Affiliation: Department of Computer Science and Engineering Slovak Technical University  NEC Research Institute  Institute for Advanced Computer Studies University of Maryland  NEC Research Institute,  UMIACS, University of Maryland,  
Pubnum: Technical Report UMIACS-TR-95-51 and CS-TR-3461  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> N. Alon, A.K. Dewdney, and T.J. Ott. </author> <title> Efficient simulation of finite automata by neural nets. </title> <journal> Journal of the Association of Computing Machinery, </journal> <volume> 38(2) </volume> <pages> 495-514, </pages> <year> 1991. </year>
Reference-contexts: Understaning the fixed point potential of recurrent neural networks (number, stability, bifurcations of fixed points) can bring some light into the problem of neural complexity of finite state machines which (to our knowledge) has not been satisfactorily solved so far (see <ref> [1] </ref>, [15] and [20]). Neural complexity of a finite state machine can be characterized as the minimal number of neurons needed so that the network can mimic the finite state machine.
Reference: [2] <author> H. Anton. </author> <title> Calculus with analytic geometry. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1980. </year> <month> 21 </month>
Reference-contexts: In particular, all fixed points (x; y) for which (x; y) 2 0; j~aj fi 0; j ~ dj are attractive Proof: D (G 1 ; G 2 ) is no longer exclusively positive. It follows from analytic geometry (see for example <ref> [2] </ref>) that D (G 1 ; G 2 ) = 0 defines either a single point or two lines (that can collide into one, or disappear).
Reference: [3] <author> R.D. Beer. </author> <title> On the dynamics of small continuous-time recurrent networks. </title> <type> Technical Report CES-94-18, </type> <institution> Case Western Reserve University, Cleveland, OH, </institution> <year> 1994. </year>
Reference-contexts: 1 Introduction In this contribution we address the issues concerning fixed points of discrete-time recurrent neural networks consisting of two neurons. Nonzero weights are assumed. As pointed out in <ref> [3] </ref>, because of the interest in associative memory applications, a great deal of previous work has focused on the question of how to constrain the weights of the recurrent networks so that they exhibit only fixed points (no oscillatory dynamics) [6]. <p> This paper presents a generalization of the results presented in [18]. A similar approach to determining the number and position of fixed points in continuous-time recurrent neural networks can be found in <ref> [3] </ref>. In section 3, the network state space is partitioned into several regions corresponding to stability types of the fixed points. This is done by first exploring the space of derivatives of the sigmoid transfer function with respect to the weighted sum of neuron inputs.
Reference: [4] <author> E.K. Blum and X. Wang. </author> <title> Stability of fixed points and periodic orbits and bifurcations in analog neural networks. </title> <booktitle> Neural Networks, </booktitle> (5):577-587, 1992. 
Reference-contexts: For example, symmetric connectivity and absence of self-interactions enabled Hopfield [14] to interpret the network as a physical system having energy minima in attractive fixed points of the network. These rather strict conditions were weakened in [6], where a more easily satisfied conditions are formulated. Blum and Wang <ref> [4] </ref> globally analyze networks with nonsymmetrical connectivity patterns of special types. In case of two recurrent neurons with sigmoidal activation function g (`) = 1=(1 + e ` ), they give results for weight matrices with diagonal elements equal to zero 1 .
Reference: [5] <author> M.P. Casey. </author> <title> Computation in Discrete-Time Dynamical Systems. </title> <type> PhD thesis, </type> <institution> University of California, San Diego, Department of Mathematics, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: A transition may have a character of a loop (do not move from the current state when the symbol x is presented), or a cycle (when repeatedly presenting the same input, we eventually return to the state where we have started). As reported in <ref> [5] </ref>, [17], and [18], loops and cycles associated with an input symbol x are usually represented as attractive fixed points and periodic orbits respectively of the underlying dynamical system corresponding to the input x.
Reference: [6] <author> M.P. Casey. </author> <title> Relaxing the symmetric weight condition for convergent dynamics in discrete-time recurrent networks. </title> <type> Technical Report INC-9504, </type> <institution> Institute for Neural Computation, University of California, </institution> <address> San Diego, 9500 Gilman Drive, La Jolla, CA 92093-0112, </address> <year> 1995. </year>
Reference-contexts: Nonzero weights are assumed. As pointed out in [3], because of the interest in associative memory applications, a great deal of previous work has focused on the question of how to constrain the weights of the recurrent networks so that they exhibit only fixed points (no oscillatory dynamics) <ref> [6] </ref>. In this context, it is desirable that all fixed points are attractive. Recently, Jin, Nikifiruk and Gupta [16] reported new results on the absolute stability for a rather general class of recurrent neural networks. <p> For example, symmetric connectivity and absence of self-interactions enabled Hopfield [14] to interpret the network as a physical system having energy minima in attractive fixed points of the network. These rather strict conditions were weakened in <ref> [6] </ref>, where a more easily satisfied conditions are formulated. Blum and Wang [4] globally analyze networks with nonsymmetrical connectivity patterns of special types.
Reference: [7] <author> A. Cleeremans, D. Servan-Schreiber, and J.L. McClelland. </author> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 372-381, </pages> <year> 1989. </year>
Reference: [8] <author> F. Cummins. </author> <title> Representation of temporal patterns in recurrent networks. </title> <booktitle> Submitted to the 15th Annual Conference of the Cognitive Science Society, </booktitle> <year> 1993. </year>
Reference-contexts: In this respect, one can look at the training process from the point of view of bifurcation analysis. The network solves the task of finite state machine simulation by location of point and periodic attractors and shaping their respective basins of attraction <ref> [8] </ref>. Before training, the connection weights are set to small random values and as a consequence, the network has only one attractor basin. This implies that the network must undergo several bifurcations [10]. In [18], a preliminary analysis of the two-neuron recurrent network is given.
Reference: [9] <author> S. Das and M.C. Mozer. </author> <title> A unified gradient-descent/clustering architecture for finite state machine induction. </title> <editor> In J.D. Cowen, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 19-26. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Conditions under which all fixed points of the network are attractive were determined by the weight matrix of the network. However, there are many applications where oscillatory dynamics of recurrent networks is desirable. For example, when trained to act as a finite state machine ([7], <ref> [9] </ref> [11], [12], [17], [19], [21], [22]), the network has to induce a stable representation of state transitions associated with each input symbol of the machine.
Reference: [10] <author> K. Doya. </author> <title> Bifurcations in the learning of recurrent neural networks. </title> <booktitle> In Proc. of 1992 IEEE Int. Symposium on Circuits and Systems, </booktitle> <pages> pages 2777-2780, </pages> <year> 1992. </year>
Reference-contexts: Before training, the connection weights are set to small random values and as a consequence, the network has only one attractor basin. This implies that the network must undergo several bifurcations <ref> [10] </ref>. In [18], a preliminary analysis of the two-neuron recurrent network is given. Under some some specific conditions on weight values, the number, position and stability types of fixed points of the underlying dynamical systems are analyzed and bifurcation mechanism is clarified.
Reference: [11] <author> J.L. Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year>
Reference-contexts: Conditions under which all fixed points of the network are attractive were determined by the weight matrix of the network. However, there are many applications where oscillatory dynamics of recurrent networks is desirable. For example, when trained to act as a finite state machine ([7], [9] <ref> [11] </ref>, [12], [17], [19], [21], [22]), the network has to induce a stable representation of state transitions associated with each input symbol of the machine.
Reference: [12] <author> C.L. Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 393-405, </pages> <year> 1992. </year>
Reference-contexts: Conditions under which all fixed points of the network are attractive were determined by the weight matrix of the network. However, there are many applications where oscillatory dynamics of recurrent networks is desirable. For example, when trained to act as a finite state machine ([7], [9] [11], <ref> [12] </ref>, [17], [19], [21], [22]), the network has to induce a stable representation of state transitions associated with each input symbol of the machine.
Reference: [13] <author> M.W. Hirsch. </author> <title> Saturation at high gain in discrete time recurrent networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(3) </volume> <pages> 449-453, </pages> <year> 1994. </year>
Reference-contexts: This is done by first exploring the space of derivatives of the sigmoid transfer function with respect to the weighted sum of neuron inputs. Then, the structure is transformed into the space of neuron activations. It was proved by Hirsh <ref> [13] </ref>, that when all the weights in a recurrent network with exclusively 1 In such a case the recurrent network is shown to have only one fixed point and no "genuine" periodic orbits (of period greater than one) 3 self-exciting (or exclusively self-inhibiting) neurons are multiplied by larger and larger positive <p> to be also appropriately increased so as to compensate for the increase in d so that the "bended" part of f d;c does not move radically to higher values of x. 20 This tendency, in the context of networks with exclusively self-exciting (or exclusively self--inhibiting) recurrent neurons, is discussed in <ref> [13] </ref>. Our result stated in Corollary 1, assumes two-neuron recurrent network.
Reference: [14] <author> J.J. </author> <title> Hopfield. Neurons with a graded response have collective computational properties like those of two-state neurons. </title> <booktitle> Proceedings of the National Academy of Science USA, </booktitle> <volume> 81 </volume> <pages> 3088-3092, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: Typically, studies of the asymptotic behaviour of recurrent neural networks usually assume some form of a structure in the weight matrix describing connectivity pattern among recurrent neurons. For example, symmetric connectivity and absence of self-interactions enabled Hopfield <ref> [14] </ref> to interpret the network as a physical system having energy minima in attractive fixed points of the network. These rather strict conditions were weakened in [6], where a more easily satisfied conditions are formulated. Blum and Wang [4] globally analyze networks with nonsymmetrical connectivity patterns of special types.
Reference: [15] <author> B.G. Horne and D.R. Hush. </author> <title> Bounds on the complexity of recurrent neural network implementations of finite state machines. </title> <editor> In J.D. Cowen, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 359-366. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year> <note> Also submitted to Neural Networks. </note>
Reference-contexts: Understaning the fixed point potential of recurrent neural networks (number, stability, bifurcations of fixed points) can bring some light into the problem of neural complexity of finite state machines which (to our knowledge) has not been satisfactorily solved so far (see [1], <ref> [15] </ref> and [20]). Neural complexity of a finite state machine can be characterized as the minimal number of neurons needed so that the network can mimic the finite state machine.
Reference: [16] <author> L. Jin, P.N. Nikiforuk, and M.M. Gupta. </author> <title> Absolute stability conditions for discrete-time recurrent neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> (6):954-963, 1994. 
Reference-contexts: In this context, it is desirable that all fixed points are attractive. Recently, Jin, Nikifiruk and Gupta <ref> [16] </ref> reported new results on the absolute stability for a rather general class of recurrent neural networks. Conditions under which all fixed points of the network are attractive were determined by the weight matrix of the network. However, there are many applications where oscillatory dynamics of recurrent networks is desirable.
Reference: [17] <author> P. Manolios and R. Fanelli. </author> <title> First order recurrent neural networks and deterministic finite state automata. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1155-1173, </pages> <year> 1994. </year>
Reference-contexts: Conditions under which all fixed points of the network are attractive were determined by the weight matrix of the network. However, there are many applications where oscillatory dynamics of recurrent networks is desirable. For example, when trained to act as a finite state machine ([7], [9] [11], [12], <ref> [17] </ref>, [19], [21], [22]), the network has to induce a stable representation of state transitions associated with each input symbol of the machine. <p> A transition may have a character of a loop (do not move from the current state when the symbol x is presented), or a cycle (when repeatedly presenting the same input, we eventually return to the state where we have started). As reported in [5], <ref> [17] </ref>, and [18], loops and cycles associated with an input symbol x are usually represented as attractive fixed points and periodic orbits respectively of the underlying dynamical system corresponding to the input x.
Reference: [18] <author> P. Tino, B.G. Horne, C.L. Giles, </author> <title> and P.C. Collingwood. Finite state machines and recurrent neural networks automata and dynamical systems approaches. </title> <type> Technical Report UMIACS-TR-95-1, </type> <institution> Institute for Advance Computer Studies, University of Maryland, College Park, MD 20742, </institution> <year> 1995. </year>
Reference-contexts: A transition may have a character of a loop (do not move from the current state when the symbol x is presented), or a cycle (when repeatedly presenting the same input, we eventually return to the state where we have started). As reported in [5], [17], and <ref> [18] </ref>, loops and cycles associated with an input symbol x are usually represented as attractive fixed points and periodic orbits respectively of the underlying dynamical system corresponding to the input x. In this respect, one can look at the training process from the point of view of bifurcation analysis. <p> Before training, the connection weights are set to small random values and as a consequence, the network has only one attractor basin. This implies that the network must undergo several bifurcations [10]. In <ref> [18] </ref>, a preliminary analysis of the two-neuron recurrent network is given. Under some some specific conditions on weight values, the number, position and stability types of fixed points of the underlying dynamical systems are analyzed and bifurcation mechanism is clarified. <p> In case of two recurrent neurons with sigmoidal activation function g (`) = 1=(1 + e ` ), they give results for weight matrices with diagonal elements equal to zero 1 . This paper presents a generalization of the results presented in <ref> [18] </ref>. A similar approach to determining the number and position of fixed points in continuous-time recurrent neural networks can be found in [3]. In section 3, the network state space is partitioned into several regions corresponding to stability types of the fixed points. <p> As explained in the introduction, training process during which recurrent neural networks learn to act as finite state machines can be interpreted from the point of view of bifurcation analysis <ref> [18] </ref>. Often, loops in state transition diagram of the finite state machine being learned are represented as attractive fixed points of the network.
Reference: [19] <author> P. Tino and J. Sajda. </author> <title> Learning and extracting initial mealy machines with a modular neural network model. </title> <journal> Neural Computation, </journal> <volume> 7(4), </volume> <year> 1995. </year>
Reference-contexts: Conditions under which all fixed points of the network are attractive were determined by the weight matrix of the network. However, there are many applications where oscillatory dynamics of recurrent networks is desirable. For example, when trained to act as a finite state machine ([7], [9] [11], [12], [17], <ref> [19] </ref>, [21], [22]), the network has to induce a stable representation of state transitions associated with each input symbol of the machine.
Reference: [20] <editor> H.T. Siegelmann, E.D. Sontag, and C.L. Giles. </editor> <title> The complexity of language recognition by neural networks. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Algorithms, Software, Architecture (Proceedings of IFIP 12 th World Computer Congress), </booktitle> <pages> pages 329-335, </pages> <address> Amsterdam, 1992. </address> <publisher> North-Holland. </publisher>
Reference-contexts: Understaning the fixed point potential of recurrent neural networks (number, stability, bifurcations of fixed points) can bring some light into the problem of neural complexity of finite state machines which (to our knowledge) has not been satisfactorily solved so far (see [1], [15] and <ref> [20] </ref>). Neural complexity of a finite state machine can be characterized as the minimal number of neurons needed so that the network can mimic the finite state machine.
Reference: [21] <author> R.L. Watrous and G.M. Kuhn. </author> <title> Induction of finite-state languages using second-order recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 406-414, </pages> <year> 1992. </year>
Reference-contexts: However, there are many applications where oscillatory dynamics of recurrent networks is desirable. For example, when trained to act as a finite state machine ([7], [9] [11], [12], [17], [19], <ref> [21] </ref>, [22]), the network has to induce a stable representation of state transitions associated with each input symbol of the machine.
Reference: [22] <author> Z. Zeng, R.M. Goodman, and P. Smyth. </author> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 976-990, </pages> <year> 1993. </year> <month> 23 </month>
Reference-contexts: However, there are many applications where oscillatory dynamics of recurrent networks is desirable. For example, when trained to act as a finite state machine ([7], [9] [11], [12], [17], [19], [21], <ref> [22] </ref>), the network has to induce a stable representation of state transitions associated with each input symbol of the machine.
References-found: 22

