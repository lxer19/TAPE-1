URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR463.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Email: ajcbik@cs.indiana.edu  peterk@cs.leidenuniv.nl  
Title: Reshaping Access Patterns for Improving Data Locality  
Author: Aart J.C. Bik Peter M.W. Knijnenburg 
Address: Lindley Hall 215, Bloomington, Indiana 47405-4101, USA  Niels Bohrweg 1, 2333 CA Leiden, the Netherlands  
Affiliation: Computer Science Department, Indiana University  Computer Science Department, Leiden University  
Abstract: In this paper, we present a method to construct a loop transformation that simultaneously reshapes the access patterns of several occurrences of multidimensional arrays along certain desired access directions. First, the method determines a direction through the original iteration space along which these desired access directions are induced. Subsequently, a unimodular transformation is constructed that changes the iteration space traversal accordingly. Finally, data dependences are accounted for. In particular, this reshaping method can be used to improve data locality in a program. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.M. Anderson and M.S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proc. SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 112-125, </pages> <year> 1993. </year>
Reference-contexts: Wolf and Lam [19] present an advanced method incorporating estimates of locality that includes unimodular transformations and tiling. Recently, algorithms that compute both computation and data decomposition have been proposed proposed <ref> [1, 2] </ref>. These algorithms, however, focus on communication minimization rather than cache behavior optimization, like the present paper. More recently, approaches that try to rearrange the layout of arrays for improving data locality have been proposed [16].
Reference: [2] <author> B. Appelbe, S. Doddapaneni, and C. Hardnett. </author> <title> A new algorithm for global optimization for parallelism and locality. </title> <booktitle> In Proc. 7th Ann. Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1994. </year>
Reference-contexts: Wolf and Lam [19] present an advanced method incorporating estimates of locality that includes unimodular transformations and tiling. Recently, algorithms that compute both computation and data decomposition have been proposed proposed <ref> [1, 2] </ref>. These algorithms, however, focus on communication minimization rather than cache behavior optimization, like the present paper. More recently, approaches that try to rearrange the layout of arrays for improving data locality have been proposed [16].
Reference: [3] <author> U. Banerjee. </author> <title> Loop Transformations for Restructuring Compilers: The Foundations. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1993. </year> <month> 12 </month>
Reference-contexts: Thereafter, we discuss the effect of applying a unimodular transformation to a perfectly nested loop on the order in which the iterations of this loop are executed. 2.1 Definitions We will use the following definitions <ref> [3, 20, 21] </ref>. <p> The relations ` k ', `' and `-' are defined similarly. In this paper, we will use the framework of uni-modular transformations <ref> [3, 9, 13] </ref>. <p> This generation is usually accomplished using Fourier-Motzkin elimination. Fourier-Motzkin elimination <ref> [3, 20] </ref> can be used to test the consistency of a reasonably small system of linear inequalities A~x ~ b, or to convert this system into a form in which the lower and upper bounds of each variable x i are expressed in terms of the variables x 1 ; : <p> ) = 1 may be used as preferred iteration direction, which gives rise to the following set Z d : = f~ff 2 Z d j S~ff = 0 and gcd (ff 1 ; : : : ; ff d ) = 1g Using the integer echelon reduction algorithm of <ref> [3, p32-39] </ref> to compute the unimodular matrix R such that RS T is in echelon form (yielding r = rank (S) as side-effect), all integer solutions of the homogeneous integer system S~ff = ~ 0 are given by the following formula for arbitrary i 2 Z [3, p59-66]: ~ff = [ <p> echelon reduction algorithm of [3, p32-39] to compute the unimodular matrix R such that RS T is in echelon form (yielding r = rank (S) as side-effect), all integer solutions of the homogeneous integer system S~ff = ~ 0 are given by the following formula for arbitrary i 2 Z <ref> [3, p59-66] </ref>: ~ff = [ (0; : : : ; 0 r This observation provides a simple condition for the existence of a (possibly invalid) unimodular transformation that performs the preferred reshaping: Proposition 3.2 There exists a d fi d unimodular matrix U for which the last column ~ff 2 Z <p> For instance, for ~ U = (1; 1) and a set of remaining dependence distance vectors D ~ D = f (1; 0) T ; (0; 1) T g, neither Y = (+1) nor Y = (1) satisfies the objective. We use the following property <ref> [3] </ref>: Proposition 3.4 Given a set D U Z d1 with ~ d ~ 0 for all ~ d 2 D U , then there exists a (d 1) fi (d 1) unimodular matrix F such that F ~ d 1 ~ 0 for all ~ d 2 D U .
Reference: [4] <author> U. Banerjee. </author> <title> Loop Parallelization. </title> <publisher> Kluwer Aca--demic Publishers, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: Then, for fixed I 0 1 = i 0 d1 = d1 , the innermost DO-loop of the target loop successively executes iterations in IS 0 that correspond to iterations of IS along a single straight line with direction ~u 0 . Inner loop concurrentization methods <ref> [4, 12, 20] </ref> exploit the first observation by enforcing a successive traversal of hyperplanes in the original iteration space in successive iterations of the outermost DO-loop of the target loop such that all iterations in each individual hyperplane are completely independent.
Reference: [5] <author> A.J.C. Bik. </author> <title> Compiler Support for Sparse Matrix Computations. </title> <type> PhD thesis, </type> <institution> Leiden University, </institution> <year> 1996. </year>
Reference-contexts: Finally, the reshaping method has been used in a completely different context, namely, in the automatic conversion of a dense program into semantically equivalent sparse code <ref> [5] </ref>. Because the entries in a sparse storage scheme can usually only be generated efficiently along one particular direction, here it is important to be able to change the access direction along arbitrary preferred directions before sparse storage schemes are selected by the compiler.
Reference: [6] <author> A.J.C. Bik, P.M.W. Knijnenburg, and H.A.G. Wijshoff. </author> <title> Reshaping access patterns for generating sparse codes. </title> <booktitle> In Proc. 7th Ann. Workshop on Languages and Compilers for Parallel Computing, volume 892 of Lecture Notes in Computer Science, </booktitle> <pages> pages 406-422. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: Because the entries in a sparse storage scheme can usually only be generated efficiently along one particular direction, here it is important to be able to change the access direction along arbitrary preferred directions before sparse storage schemes are selected by the compiler. In <ref> [6] </ref>, a preliminary version of the reshaping method of this paper (restricted to two-dimensional arrays) has been used for this purpose. 2 The outline the paper is as follows. Preliminaries are given first in section 2. The reshaping method is presented in section 3.
Reference: [7] <author> F. Bodin, C. Eisenbeis, W. Jalby, T. Montaut, P. Rabain, and D. Windheiser. </author> <title> Algorithms for data locality optimizations. </title> <booktitle> APPARC Deliverable CoD2, </booktitle> <year> 1994. </year>
Reference-contexts: Several approaches try to estimate the amount of locality in inner-loops, for example by means of windows [10], to drive heuristic strategies for applying blocking, permutation and fusion of loops <ref> [10, 8, 7, 11] </ref>. Recently, sophisticated multilevel blocking algorithms have been devised that take into account registers and multilevel caches [15]. Other approaches try to construct unimodular loop transformations.
Reference: [8] <author> S. Carr, K.S. McKinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proc. 6th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1994. </year>
Reference-contexts: Several approaches try to estimate the amount of locality in inner-loops, for example by means of windows [10], to drive heuristic strategies for applying blocking, permutation and fusion of loops <ref> [10, 8, 7, 11] </ref>. Recently, sophisticated multilevel blocking algorithms have been devised that take into account registers and multilevel caches [15]. Other approaches try to construct unimodular loop transformations. <p> As far as the authors are aware, the present approach is new. In a number of cases, the proposed construction yields a loop interchange, like the strategies reported in <ref> [8] </ref>. The difference is that in our case this interchange is computed, whereas in [8] is selected by a heuristic. The construction in [14] constructs a transformation based on affine access functions present in the loop, like our approach. <p> As far as the authors are aware, the present approach is new. In a number of cases, the proposed construction yields a loop interchange, like the strategies reported in <ref> [8] </ref>. The difference is that in our case this interchange is computed, whereas in [8] is selected by a heuristic. The construction in [14] constructs a transformation based on affine access functions present in the loop, like our approach. However, in the latter work an entire index expression is mapped onto one loop index in the target loop.
Reference: [9] <author> M.L. Dowling. </author> <title> Optimal code parallelization using unimodular transformations. </title> <journal> Parallel Computing, </journal> <volume> 16 </volume> <pages> 157-171, </pages> <year> 1990. </year>
Reference-contexts: The relations ` k ', `' and `-' are defined similarly. In this paper, we will use the framework of uni-modular transformations <ref> [3, 9, 13] </ref>.
Reference: [10] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformations. </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: In this approach the iteration space is cut up into pieces so that each piece only refers to a subset of the array that fits into the cache. Several approaches try to estimate the amount of locality in inner-loops, for example by means of windows <ref> [10] </ref>, to drive heuristic strategies for applying blocking, permutation and fusion of loops [10, 8, 7, 11]. Recently, sophisticated multilevel blocking algorithms have been devised that take into account registers and multilevel caches [15]. Other approaches try to construct unimodular loop transformations. <p> Several approaches try to estimate the amount of locality in inner-loops, for example by means of windows [10], to drive heuristic strategies for applying blocking, permutation and fusion of loops <ref> [10, 8, 7, 11] </ref>. Recently, sophisticated multilevel blocking algorithms have been devised that take into account registers and multilevel caches [15]. Other approaches try to construct unimodular loop transformations.
Reference: [11] <author> K. Kennedy and K.S. McKinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proc. ICS, </booktitle> <year> 1992. </year>
Reference-contexts: Several approaches try to estimate the amount of locality in inner-loops, for example by means of windows [10], to drive heuristic strategies for applying blocking, permutation and fusion of loops <ref> [10, 8, 7, 11] </ref>. Recently, sophisticated multilevel blocking algorithms have been devised that take into account registers and multilevel caches [15]. Other approaches try to construct unimodular loop transformations.
Reference: [12] <author> L. Lamport. </author> <title> The parallel execution of DO loops. </title> <journal> Comm. of the ACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <year> 1974. </year>
Reference-contexts: Then, for fixed I 0 1 = i 0 d1 = d1 , the innermost DO-loop of the target loop successively executes iterations in IS 0 that correspond to iterations of IS along a single straight line with direction ~u 0 . Inner loop concurrentization methods <ref> [4, 12, 20] </ref> exploit the first observation by enforcing a successive traversal of hyperplanes in the original iteration space in successive iterations of the outermost DO-loop of the target loop such that all iterations in each individual hyperplane are completely independent.
Reference: [13] <author> W. Li and K. Pingali. </author> <title> Access normalization: Loop restructuring for NUMA compilers. </title> <booktitle> In Proc. 5th In'l Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 285-295, </pages> <year> 1992. </year>
Reference-contexts: The relations ` k ', `' and `-' are defined similarly. In this paper, we will use the framework of uni-modular transformations <ref> [3, 9, 13] </ref>.
Reference: [14] <author> W. Li and K. Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <journal> Int'l J. of Parallel Programming, </journal> <volume> 22(2) </volume> <pages> 183-205, </pages> <year> 1994. </year>
Reference-contexts: Recently, sophisticated multilevel blocking algorithms have been devised that take into account registers and multilevel caches [15]. Other approaches try to construct unimodular loop transformations. In the context of NUMA architectures, Li and Pingali <ref> [14] </ref> construct a transformation for access normalization in order to to localize array references; (affine) index functions are mapped onto target loop indices in such a way that inner target loops only refer to array elements local to the processor. <p> In a number of cases, the proposed construction yields a loop interchange, like the strategies reported in [8]. The difference is that in our case this interchange is computed, whereas in [8] is selected by a heuristic. The construction in <ref> [14] </ref> constructs a transformation based on affine access functions present in the loop, like our approach. However, in the latter work an entire index expression is mapped onto one loop index in the target loop.
Reference: [15] <author> J.J. Navarro, A. Juan, and T. Lang. </author> <title> MOB forms: A class of multilevel block algorithms for dense linear algebra operations. </title> <booktitle> In Proc. ICS, </booktitle> <year> 1994. </year>
Reference-contexts: Recently, sophisticated multilevel blocking algorithms have been devised that take into account registers and multilevel caches <ref> [15] </ref>. Other approaches try to construct unimodular loop transformations.
Reference: [16] <author> M.F.P O'Boyle and P.M.W. Knijnenburg. </author> <title> Non-singular data transformations: Definition, validity, </title> <booktitle> applications. In Proc. 6th Workshop on Compilers for Parallel Computers, </booktitle> <year> 1996. </year>
Reference-contexts: Recently, algorithms that compute both computation and data decomposition have been proposed proposed [1, 2]. These algorithms, however, focus on communication minimization rather than cache behavior optimization, like the present paper. More recently, approaches that try to rearrange the layout of arrays for improving data locality have been proposed <ref> [16] </ref>. Note that this last approach is different from the previously cited ones in that the loop structures in the program remain unaffected. 1 In this paper, we present a novel approach to im-proving spatial locality, namely, we want to reshape access patterns of arrays. <p> There are several reasons for taking this approach. First, enforcing column-wise or row-wise access are just special instances of the same technique. Therefore, this technique can be incorporated in compilers for arbitrary languages. Second, in <ref> [16] </ref> a notion of data transformation is discussed that changes the layout of an array and propagates this change throughout the program. Such a data transformation is given by a non-singular matrix D. <p> Obviously, this method can be combined with methods that are aimed at enhancing temporal locality, like loop blocking. Moreover, future research will be aimed at combining the reshaping method of this paper with the framework of data structure transformations presented in <ref> [16] </ref>.
Reference: [17] <author> D.A. Padua and M.J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Comm. of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <year> 1986. </year>
Reference-contexts: Given a perfectly nested loop with stride-1 DO-loops, index vector ~ I = (I 1 ; : : : ; I d ), and iteration space IS Z d , any combination of loop interchanging, loop skewing and loop reversal (see e.g. <ref> [17, 20, 21] </ref>) that transforms this loop into another loop with index vector ~ I 0 = (I 0 d ) and iteration space IS 0 Z d can be modeled by a linear transformation U : IS ! IS 0 that is defined by a dfid unimodular matrix U .
Reference: [18] <author> W. Pugh. </author> <title> The Omega test: A fast and practical integer programming algorithm for dependence analysis. </title> <journal> Comm. of the ACM, </journal> <volume> 8 </volume> <pages> 102-114, </pages> <year> 1992. </year>
Reference-contexts: Consistency indicates that the system has at least one rational solution, and can be used as a necessary (but not sufficient) condition under which an integer solution exists. An extension to Fourier-Motzkin elimination that can test for the existence of integer solutions is given by the Omega test <ref> [18] </ref>. 2.2 Iteration Space Traversal Because iterations in both the original and target iteration space are traversed in lexicographical order, a unimodular transformation effectively changes the order in which iterations are executed.
Reference: [19] <author> M.E. Wolf and M.S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proc. ACM SIGPLAN 91 Conf. on Programming Languages Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <year> 1991. </year>
Reference-contexts: Wolf and Lam <ref> [19] </ref> present an advanced method incorporating estimates of locality that includes unimodular transformations and tiling. Recently, algorithms that compute both computation and data decomposition have been proposed proposed [1, 2]. These algorithms, however, focus on communication minimization rather than cache behavior optimization, like the present paper.
Reference: [20] <author> M.J. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: Thereafter, we discuss the effect of applying a unimodular transformation to a perfectly nested loop on the order in which the iterations of this loop are executed. 2.1 Definitions We will use the following definitions <ref> [3, 20, 21] </ref>. <p> Given a perfectly nested loop with stride-1 DO-loops, index vector ~ I = (I 1 ; : : : ; I d ), and iteration space IS Z d , any combination of loop interchanging, loop skewing and loop reversal (see e.g. <ref> [17, 20, 21] </ref>) that transforms this loop into another loop with index vector ~ I 0 = (I 0 d ) and iteration space IS 0 Z d can be modeled by a linear transformation U : IS ! IS 0 that is defined by a dfid unimodular matrix U . <p> This generation is usually accomplished using Fourier-Motzkin elimination. Fourier-Motzkin elimination <ref> [3, 20] </ref> can be used to test the consistency of a reasonably small system of linear inequalities A~x ~ b, or to convert this system into a form in which the lower and upper bounds of each variable x i are expressed in terms of the variables x 1 ; : <p> Then, for fixed I 0 1 = i 0 d1 = d1 , the innermost DO-loop of the target loop successively executes iterations in IS 0 that correspond to iterations of IS along a single straight line with direction ~u 0 . Inner loop concurrentization methods <ref> [4, 12, 20] </ref> exploit the first observation by enforcing a successive traversal of hyperplanes in the original iteration space in successive iterations of the outermost DO-loop of the target loop such that all iterations in each individual hyperplane are completely independent.
Reference: [21] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1990. </year> <month> 13 </month>
Reference-contexts: Thereafter, we discuss the effect of applying a unimodular transformation to a perfectly nested loop on the order in which the iterations of this loop are executed. 2.1 Definitions We will use the following definitions <ref> [3, 20, 21] </ref>. <p> Given a perfectly nested loop with stride-1 DO-loops, index vector ~ I = (I 1 ; : : : ; I d ), and iteration space IS Z d , any combination of loop interchanging, loop skewing and loop reversal (see e.g. <ref> [17, 20, 21] </ref>) that transforms this loop into another loop with index vector ~ I 0 = (I 0 d ) and iteration space IS 0 Z d can be modeled by a linear transformation U : IS ! IS 0 that is defined by a dfid unimodular matrix U .
References-found: 21

