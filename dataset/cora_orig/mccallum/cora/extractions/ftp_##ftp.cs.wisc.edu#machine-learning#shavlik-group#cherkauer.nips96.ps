URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/cherkauer.nips96.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/cherkauer.nips96.ps.abstract.html
Root-URL: 
Email: fcherkauer,shavlikg@cs.wisc.edu  
Title: Rapid Quality Estimation of Neural Network Input Representations  
Author: Kevin J. Cherkauer Jude W. Shavlik 
Address: 1210 W. Dayton St., Madison, WI 53706  
Affiliation: Computer Sciences Department, University of Wisconsin-Madison  
Note: Appears in Advances in Neural Information Processing Systems, Vol. 8. MIT Press, Cambridge, MA, 1996.  
Abstract: The choice of an input representation for a neural network can have a profound impact on its accuracy in classifying novel instances. However, neural networks are typically computationally expensive to train, making it difficult to test large numbers of alternative representations. This paper introduces fast quality measures for neural network representations, allowing one to quickly and accurately estimate which of a collection of possible representations for a problem is the best. We show that our measures for ranking representations are more accurate than a previously published measure, based on experiments with three difficult, real-world pattern recognition problems.
Abstract-found: 1
Intro-found: 1
Reference: <author> Almuallim, H. & Dietterich, T. </author> <year> (1994). </year> <title> Learning Boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, 69(1-2):279-305. </journal>
Reference: <author> Baim, P. </author> <year> (1988). </year> <title> A method for attribute selection in inductive learning systems. </title> <journal> IEEE Transactions on Pattern Analysis & Machine Intelligence, </journal> <volume> 10(6) </volume> <pages> 888-896. </pages>
Reference: <author> Battiti, R. </author> <year> (1994). </year> <title> Using mutual information for selecting features in supervised neural net learning. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(4) </volume> <pages> 537-550. </pages>
Reference: <author> Burl, M., Fayyad, U., Perona, P., Smyth, P., & Burl, M. </author> <year> (1994). </year> <title> Automating the hunt for volcanoes on Venus. </title> <booktitle> In IEEE Computer Society Conf on Computer Vision & Pattern Recognition: Proc, </booktitle> <address> Seattle, WA. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Caruana, R. & Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <booktitle> In Machine Learning: Proc 11th Intl Conf, </booktitle> <pages> (pp. 28-36), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cherkauer, K. & Shavlik, J. </author> <year> (1994). </year> <title> Selecting salient features for machine learning from large candidate pools through parallel decision-tree construction. </title> <editor> In Kitano, H. & Hendler, J., eds., </editor> <booktitle> Massively Parallel Artificial Intel. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Cherkauer, K. & Shavlik, J. </author> <year> (1995). </year> <title> Rapidly estimating the quality of input representations for neural networks. </title> <booktitle> In Working Notes, IJCAI Workshop on Data Engineering for Inductive Learning, </booktitle> <pages> (pp. 99-108), </pages> <address> Montreal, Canada. </address>
Reference: <author> Craven, M. & Shavlik, J. </author> <year> (1993). </year> <title> Learning to predict reading frames in E. coli DNA sequences. </title> <booktitle> In Proc 26th Hawaii Intl Conf on System Science, </booktitle> <pages> (pp. 773-782), </pages> <address> Wailea, HI. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Fayyad, U. </author> <year> (1994). </year> <title> Branching on attribute values in decision tree generation. </title> <booktitle> In Proc 12th Natl Conf on Artificial Intel, </booktitle> <pages> (pp. 601-606), </pages> <address> Seattle, WA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: This measure assumes the complexity of the concept ID3 finds depends on the density of simple, accurate models in its space and thus reflects the true transparency. All these measures fix tree training-set accuracy at 100%, so simpler trees imply more accurate generalization <ref> (Fayyad, 1994) </ref> as well as easier learning. This lets us estimate transparency without the multiplicative additional computational expense of cross validating each tree.
Reference: <author> John, G., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proc 11th Intl Conf, </booktitle> <pages> (pp. 121-129), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In this paper, we introduce fast transparency measures for ANN input representations. These are orders of magnitude faster than the wrapper method <ref> (John et al., 1994) </ref>, which would evaluate ANN representations by training and testing the ANNs themselves.
Reference: <author> Kambhatla, N. & Leen, T. </author> <year> (1994). </year> <title> Fast non-linear dimension reduction. </title> <booktitle> In Advances in Neural Info Processing Sys (vol 6), </booktitle> <pages> (pp. 152-159), </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kira, K. & Rendell, L. </author> <year> (1992). </year> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Proc 10th Natl Conf on Artificial Intel, </booktitle> <pages> (pp. 129-134), </pages> <address> San Jose, CA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: In their framework, the less a representation requires the use of feature interactions to produce accurate concepts, the more transparent it is. Blurring heuristically estimates this by measuring the average information content of a representation's individual features. Blurring is equivalent to the (negation of the) average information gain <ref> (Quinlan, 1986) </ref> of a representation's features with respect to a training set, as we show in Cherkauer and Shavlik (1995). 3 Evaluating the Transparency Measures We evaluate the transparency measures on three problems: DNA (predicting gene reading frames; Craven & Shavlik, 1993), NIST (recognizing handwritten digits; "Fl3" distribution), and Magellan (detecting
Reference: <author> Quinlan, J. </author> <year> (1994). </year> <title> Comparing connectionist and symbolic learning methods. </title> <editor> In Hanson, S., Drastal, G., & Rivest, R., eds., </editor> <title> Computational Learning Theory & Natural Learning Systems (vol I: Constraints & Prospects). </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Our experiments on three real-world datasets demonstrate that our transparency measures are highly predictive of representation quality for ANNs, implying that the transfer assumption holds surprisingly well for some pattern recognition tasks even though ANNs and decision trees are believed to work best on quite different types of problems <ref> (Quinlan, 1994) </ref>. 1 In addition, our Exper. 1 shows that transparency does not depend on representational sufficiency. Exper. 2 verifies this conclusion and also demonstrates that transparency does not depend on representational economy.
Reference: <author> Rendell, L. & Ragavan, H. </author> <year> (1993). </year> <title> Improving the design of induction methods by analyzing algorithm functionality and data-based concept complexity. </title> <booktitle> In Proc 13th Intl Joint Conf on Artificial Intel, </booktitle> <pages> (pp. 952-958), </pages> <address> Chambery, France. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. & Whitehead, S. </author> <year> (1993). </year> <title> Online learning with random representations. </title> <booktitle> In Machine Learning: Proc 10th Intl Conf, </booktitle> <pages> (pp. 314-321), </pages> <address> Amherst, MA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Exper. 2 shows that transparency does not depend on representational economy or sufficiency, as both were held constant here. 6 Experiment 3|Redundant Features Exper. 3 tests the transparency measures' predictions when the number of redun dant features varies, as ANNs can often use redundant features to advantage <ref> (Sutton & Whitehead, 1993) </ref>, an ability generally not attributed to decision trees. Exper. 3 reuses the representations R i and R 0 i (i = 1; ::: ; 10) from Exper. 1 and 2. Recall that R 0 i R i .
References-found: 16

