URL: http://www.cs.colostate.edu/~vision/ps/tr-drew-thesis-color.ps.gz
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Title: Simultaneous Refinement of Pose and Sensor Registration  
Author: A. Schwickerath 
Web: WWW: http://www.cs.colostate.edu  
Address: Fort Collins, CO 80523-1873  
Affiliation: Computer Science Department Colorado State University  
Date: January 6, 1997  
Pubnum: Technical Report CS-97-101  
Abstract: Computer Science Technical Report 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Nicholas Ayache and Olivier D. Faugeras. </author> <title> HYPER: A New Approach for the Recognition and Positioning of Two-Dimensional Objects. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(1) </volume> <pages> 44-54, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: This work on geometric model based sensor fusion exists at the junction of three subfields of Computer Vision: geometric model matching, model-based pose determination and sensor fusion. Model matching and model-based pose are already closely tied in the Vision literature (e.g., <ref> [34, 1, 2, 29, 19] </ref>). In model matching, a correspondence between features (e.g., lines, corners) in the image and features in the model is sought. In pose determination, the orientation and position of the model relative to the sensor is estimated, often depending upon a presupposed correspondence. <p> In terms of pose determination, the state is the pose, the noise is determined by the sensor and the feature extraction method, and the linear function is an approximation of the model-to-sensor transformation. As with previous uses of Kalman Filtering in computer vision (e.g., <ref> [1] </ref>), this approach elegantly incorporates matching into the pose refinement process. This will be discussed in greater depth in Chapter 4. Not all pose estimation processes deal with explicit correspondences. For breadth, I will briefly introduce one of these. <p> Local search in this large space is robust, but can be a slow process. We will revisit this approach again in Chapter 3. In Kalman Filtering approaches, such as those proposed by Ayache <ref> [1] </ref> and Hel-Or and Wer-man [19, 18, 20], the matching process is interleaved with the pose estimation process. Rather than finding a complete correspondence and then recovering the pose, individual feature pairs are selected based on consistency with the current match, similar to tree matching but allowing many-to-many mappings. <p> The weight ff fit 2 <ref> [0; 1] </ref> tunes the relative importance of the sensors. The intersensor constraints are not seen in this measure, since they are implicit in the parameterization. 3 Another aspect of the computer vision problem is the decision of whether a specific model can be found in an image. <p> Hence, the simpler t mo , based on an estimate of total noise on target is computationally necessary in the current implementation. Also notice that while this normalization does not absolutely guarantee that E fit;o will remain in the range <ref> [0; 1] </ref>, it does guarantee it assuming that the correspondence does not contain any pairs with image endpoint-to-model-line distances greater than the tolerance t mo (i.e., outliers). Similarly, 1 2t 2 mr i ri is used to weight the range data. <p> This linear approximation serves us now as a model as we formulate the standard Kalman Filter. 4.2.2 Application in Pose Determination Kalman Filtering is no stranger to Computer Vision. Ayache <ref> [1] </ref> used Kalman Filtering in a 2D domain. The basic idea was to use the iterative nature of Kalman Filtering to incorporate the next, best model-data pair into the match, updating both the pose hypothesis and the covariance of the pose while performing matching. <p> We can write this formally as M = f~u i j~u i = (x i ; y i ; z i ) T 8i 2 <ref> [1; n] </ref>g This is mapped into perfect measurements, ~u 0 i , by the transformation F . F is an affine transformation and does not include any perspective transformations. As described in the previous section, true measurements are not available, so noisy measurements must be relied upon.
Reference: [2] <author> J. Ross Beveridge. </author> <title> Local Search Algorithms for Geometric Object Recognition: Optimal Correspondence and Pose. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1993. </year>
Reference-contexts: This work on geometric model based sensor fusion exists at the junction of three subfields of Computer Vision: geometric model matching, model-based pose determination and sensor fusion. Model matching and model-based pose are already closely tied in the Vision literature (e.g., <ref> [34, 1, 2, 29, 19] </ref>). In model matching, a correspondence between features (e.g., lines, corners) in the image and features in the model is sought. In pose determination, the orientation and position of the model relative to the sensor is estimated, often depending upon a presupposed correspondence. <p> This thesis is concerned with those which act directly on 3D model features, rather than 2D deformable graphs [35], or weak perspective (2D rigid projections) methods <ref> [8, 2] </ref>. Methods which deal directly with 3D model features can utilize full 3D perspective and implicitly deal with any view of the object. 2D methods can correct only somewhat for perspective projections. They also must explicitly describe changes in appearance due to changes in viewpoint. <p> This limits matches to include only pairings of one model feature to many data features, not allowing multiple model features to correspond to one data feature. Grimson does introduce a method for handling clutter, however, by allowing the matching of an image feature to a null model feature. Beveridge <ref> [2] </ref> defines a combinatorial search space and uses local search to find locally optimal matches. Unlike the tree search methods, the search space used here takes into account mappings of potentially many model features to potentially many data features. <p> Levenberg-Marquardt effectively trades off control between the Newton second order technique and a first order technique. This deals with the unstable tendencies of the Newton method. Levenberg-Marquardt has proven useful in solving pose determination problems by Lowe [29, 30], Kumar [27], and Beveridge <ref> [2] </ref>. The first step in optimizing the fit error is to obtain a linearization of the criterion function. <p> An alternative is to trade off the assurances of least-median squares for the search space traversing properties of local search. This search mechanism makes no demands on the amount of outliers, allowing even the complete space of all possible matches (image features fi model features) to be considered. <ref> [2] </ref> This power does come with its price: the convergence to non-global optima may be likely. <p> This can be partially overcome by restarting the search from a number of local positions, but does not make as strong of a statement about convergence as median filtering, since the portion of the search space containing the global optimum may not be on many paths. <ref> [2] </ref> 3.3.1 Defining a Match Error So far we have defined a measure of how well the model fits the data, given a set of coregistration parameters and a correspondence. <p> As a result, we also need to consider another error term which offers a penalty for not explaining model features. As Beveridge <ref> [2] </ref> did, we combine the omission error (E om ) with the fit error to form a total match error per sensor of E match;sensor = E fit;sensor + E om;sensor and the total match error for the suite of E match = ff match E match;o + (1 ff match <p> The set of all of these correspondences makes up our initial state. This reduces the size of the search space, yielding a more tractable search space. An alternative is to randomly select pairs in the complete correspondence to construct the initial match <ref> [2] </ref>. Given this initial state, we can define a set of moves for traversing the search space to an optimal solution: in our case, removing a single correspondence (see Figure 3.5). This simple 29 minded operator allows us to thin out our correspondence, removing outlier data. <p> This is desirable from the standpoint of a first pass, although a more complete neighborhood would prove more robust and practical once the inner loop (i.e., coregistration) has been optimized. Some other neighborhoods have been proposed, such as 1- and 2- Hamming distance <ref> [2] </ref>, but these search a more interconnected space (a mesh, rather than a tree). Once we have evaluated every possible move we can make from the current state, we need to select the move which we believe will bring us closer to an optimum.
Reference: [3] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Integrated color ccd, flir & ladar based object modeling and recognition. </title> <type> Technical report, </type> <institution> Colorado State University and Alliant Techsystems and University of Massachusetts, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: For example, an occlusion detection sensor fusion module could be used as the higher-level system. This level of testing will soon be possible. The work on hypothesis driven matching by Bev-eridge, et. al. <ref> [3, 4, 6] </ref> lays out a framework for driving a multisensor matching system, including a coregistration phase. The work by Stevens and Beveridge [40] demonstrates coregistration through pose+registration search, rather than correspondence search. Combining these with the coregis-tration algorithms presented in this thesis would provide a complete system.
Reference: [4] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Model-based Fusion of FLIR, </title> <editor> Color and LADAR. In Paul S. Schenker and Gerard T. McKee, editors, </editor> <booktitle> Proceedings: Sensor Fusion and Networked Robotics VIII, Proc. SPIE 2589, </booktitle> <pages> pages 2 - 11, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: For example, an occlusion detection sensor fusion module could be used as the higher-level system. This level of testing will soon be possible. The work on hypothesis driven matching by Bev-eridge, et. al. <ref> [3, 4, 6] </ref> lays out a framework for driving a multisensor matching system, including a coregistration phase. The work by Stevens and Beveridge [40] demonstrates coregistration through pose+registration search, rather than correspondence search. Combining these with the coregis-tration algorithms presented in this thesis would provide a complete system.
Reference: [5] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: This shortcoming could be ignored if a precise sensor-to-sensor correspondence could be computed from the raw data. This is not an easy 3 (a) (c) Array 8 from the Fort Carson Data Collection <ref> [5] </ref>). task when the nature of the sensors is diverse. <p> move them to alternate sides of the object, many additional local optima can be created, or very shallow linear approximations may drag the solution off significantly. 2.6 Coregistering Real Data As a demonstration, an M60 model and images nov31553c (color) and nov312051l (LADAR) images from the Fort Carson data set <ref> [5] </ref> were coregistered from both good and poor initial estimates. Four corresponding points on the object model, in the color image, and in the LADAR image were hand picked using the Rangeview program described in [14]. <p> deviation for a Gaussian distribution based upon the interquartile range. a is the number of standard deviations to set the cutoff at and is commonly set to a = 2. [27, 38] 3.2.1 Results To demonstrate least-median squares optimization, an image suite was selected from the Fort Carson data set <ref> [5] </ref>: Shot 20 Array 5. The initial pose+registration was selected to fit the data as accurately as possible. This was necessary to ensure an initial correspondence with less than 50% outliers.
Reference: [6] <author> J. Ross Beveridge, Mark R. Stevens, and Anthony N. A. Schwickerath. </author> <title> Toward target verification through 3-d model-based sensor fusion. </title> <journal> IEEE Transactions on Image Processing, </journal> <note> page (Submitted), </note> <year> 1996. </year>
Reference-contexts: For example, an occlusion detection sensor fusion module could be used as the higher-level system. This level of testing will soon be possible. The work on hypothesis driven matching by Bev-eridge, et. al. <ref> [3, 4, 6] </ref> lays out a framework for driving a multisensor matching system, including a coregistration phase. The work by Stevens and Beveridge [40] demonstrates coregistration through pose+registration search, rather than correspondence search. Combining these with the coregis-tration algorithms presented in this thesis would provide a complete system.
Reference: [7] <author> J. Ross Beveridge, Mark R. Stevens, Zhongfei Zhang, and Mike Goss. </author> <title> Approximate Image Mappings Between Nearly Boresight Aligned Optical and Range Sensors. </title> <type> Technical Report CS-96-112, </type> <institution> Computer Science, Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Collectively, these pose+registration parameters are referred to as F . Strictly speaking, the sensors in this suite were not guaranteed to be coplanar, but only nearly coplanar. Beveridge, et. al. <ref> [7] </ref> showed that small rotations, such as the ones in our sensor suite, can be approximated by translations in the view plane. 2.2 Overview of the Coregistration Refinement Algorithm The coregistration refinement task can be cast as an optimization problem, following the outline in Section 1.2.
Reference: [8] <author> James Bevington, Randy Johnston, and Joel Lee. </author> <title> A Modular Target Recognition Algorithm for LADAR. </title> <booktitle> In 2nd Automatic Target Recognizer Systems and Technology Conference, </booktitle> <pages> pages 91-104, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In this way, these two tasks are intertwined. Few cases exist of matching or pose determination being tied to sensor fusion (e.g., [19, 13]). 1 1.2 Model-Based Pose Different approaches have been taken to model-based pose determination (e.g., <ref> [27, 29, 19, 8] </ref>). This thesis is concerned with those which act directly on 3D model features, rather than 2D deformable graphs [35], or weak perspective (2D rigid projections) methods [8, 2]. <p> This thesis is concerned with those which act directly on 3D model features, rather than 2D deformable graphs [35], or weak perspective (2D rigid projections) methods <ref> [8, 2] </ref>. Methods which deal directly with 3D model features can utilize full 3D perspective and implicitly deal with any view of the object. 2D methods can correct only somewhat for perspective projections. They also must explicitly describe changes in appearance due to changes in viewpoint.
Reference: [9] <author> James E. Bevington. </author> <title> Laser Radar ATR Algorithms: Phase III Final Report. </title> <type> Technical report, </type> <institution> Alliant Techsystems, Inc., </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: I assume that this determination has already been made by a hypothesis generation phase such as <ref> [12, 9] </ref>.
Reference: [10] <author> S. M. Bozic. </author> <title> Digital and Kalman Filtering. </title> <publisher> Halsted Press, </publisher> <address> 2nd edition, </address> <year> 1994. </year>
Reference-contexts: These criteria functions are optimized using the Levenberg-Marquardt extension to the Newton Method. Common to both Horn and Kumar is the use of a least-mean squares framework. Another approach, and a more general one, is Kalman Filtering. <ref> [10, 39] </ref> Hel-Or [19] and Hel-Or and Wer-man [18, 20] use this approach to fuse heterogeneous measurements into a pose estimation. This is, in fact, just another way of casting the sensor fusion problem.
Reference: [11] <author> J. B. Burns, A. R. Hanson, and E. M. Riseman. </author> <title> Extracting Straight Lines. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(4) </volume> <pages> 425-456, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Each line is locally moved over the color/optical image to optimize the gradient under the edge. The 2D mapping of the optimal model line placement is the reported image feature. We attempted to use other line extraction methods, such as Burns lines <ref> [11] </ref>, but found that the high texture of these outdoor images produced too many features and often significantly pulled the true silhouette lines off target.
Reference: [12] <author> Bruce A. Draper, Carla E. Brodley, and Paul E. Utgoff. </author> <title> Goal-Directed Classification using Linear Machine Decision Trees. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(9) </volume> <pages> 888-893, </pages> <month> September </month> <year> 1994. </year> <month> 48 </month>
Reference-contexts: I assume that this determination has already been made by a hypothesis generation phase such as <ref> [12, 9] </ref>.
Reference: [13] <author> R. O. Eason and R. C. Gonzalez. </author> <title> Least-Squares Fusion of Multisensory Data. </title> <editor> In Mongi A. Abidi and Rafael C. Gonzalez, editors, </editor> <booktitle> Data Fusion in Robotics and Machine Intelligence, chapter 9. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: However, determining which model and image features should be paired often requires evaluating the goodness of fit given the best pose for the match. In this way, these two tasks are intertwined. Few cases exist of matching or pose determination being tied to sensor fusion (e.g., <ref> [19, 13] </ref>). 1 1.2 Model-Based Pose Different approaches have been taken to model-based pose determination (e.g., [27, 29, 19, 8]). This thesis is concerned with those which act directly on 3D model features, rather than 2D deformable graphs [35], or weak perspective (2D rigid projections) methods [8, 2]. <p> These pairs are included in the match, updating the pose estimate so that another consistent feature pair can be selected. 1.4 Sensor Fusion Eason and Gonzalez <ref> [13] </ref> propose a general least-squares framework for multi-sensor fusion. At its heart, this is simply a direct extension of the basic least-squares approach for finding the pose between a single sensor and a model instance. <p> In Chapter 2, I present a simple least-squares coregistration solution. The criterion function draws upon previous work by Kumar [27] and Horn [22, 23], with similarities to Eason and Gonzalez's model based sensor fusion <ref> [13] </ref>. An empirical sensitivity analysis is performed upon an implementation of this algorithm. Finally the algorithm is applied to real data. The application of the least-squares coregistration algorithm to real data points out a shortcoming of all least-squares algorithms: outlier sensitivity. In Chapter 2, hand-picked correspondences produce unstable results.
Reference: [14] <author> Michael E. Goss, J. Ross Beveridge, Mark Stevens, and Aaron Fuegi. </author> <title> Visualization and Verification of Automatic Target Recognition Results Using Combined Range and Optical Imager. </title> <booktitle> In 1994 ARPA Image Understanding Workshop Proceedings, </booktitle> <year> 1994. </year>
Reference-contexts: Four corresponding points on the object model, in the color image, and in the LADAR image were hand picked using the Rangeview program described in <ref> [14] </ref>. The four pairs of corresponding points were used to generate six corresponding pairs of line segments for the model-to-CCD error term. These corresponding features, intrinsic camera parameters, and an initial coregistration estimate were passed to the coregistration algorithm (see Figures 2.6 (a) and 2.7 (a)).
Reference: [15] <author> L. Gottesfeld-Brown. </author> <title> A survey of image registration techniques. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4):325, </volume> <month> December </month> <year> 1992. </year>
Reference-contexts: Introduction 1.1 Overview The key contribution of this thesis is an algorithm for the simultaneous refinement of both the sensor suite-to-model pose and sensor-to-sensor registration, or coregistration. Model-to-sensor pose determination (e.g., [27, 22]) and homogeneous image registration (e.g., <ref> [15] </ref>) have both already be dealt with individually. However, there is no simple combination of these two techniques yielding both pose and registration for suites of heterogeneous sensors. Why is such an algorithm necessary? Whenever discrete, inexpensive sensors are used in conjunction, there will be variations in intersensor parameters. <p> The images returned by a color camera, an infrared camera, and a range sensor (see Figure 1.1) do not necessarily correlate. 1 As such, registration algorithms must deal with homogeneous sensor suites <ref> [15] </ref>. 1.5 Contributions As stated earlier, the concept of coregistration is the main contribution of this thesis. In Chapter 2, I present a simple least-squares coregistration solution.
Reference: [16] <author> W. Eric L. Grimson and Tomas Lozano-Perez. </author> <title> Localizing overlapping parts by searching the interpretation tree. </title> <journal> Transactions on pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-9(4):469-482, </volume> <month> July </month> <year> 1987. </year>
Reference-contexts: Both as examples and to motivate the matching work in Chapter 3, let us review some common approaches to establishing correspondences. A classic matching method in computer vision is tree search, espoused by Grimson <ref> [16] </ref>. In this method, a tree is considered, where each node denotes a match and each link, the addition of a pair to the correspondence. Each level of the tree represents the coupling of a specific image feature.
Reference: [17] <author> Martial Hebert. </author> <title> Presentation of the Mobility Group, </title> <booktitle> UGV Demo II, Killeen, Texas. Future Recommendations of the Stereo Group, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: Both of these approaches miss a key issue: sensor suites comprised of discrete sensors are not coincident and do not remain precisely registered <ref> [17] </ref>. This shortcoming could be ignored if a precise sensor-to-sensor correspondence could be computed from the raw data. This is not an easy 3 (a) (c) Array 8 from the Fort Carson Data Collection [5]). task when the nature of the sensors is diverse.
Reference: [18] <author> Y. Hel-Or and M. Werman. </author> <title> Absolute Orientation from Uncertain Data: A Unified Approach. </title> <booktitle> In Proceedings: Computer Vision and Pattern Recognition, </booktitle> <pages> pages 77 - 82. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: These criteria functions are optimized using the Levenberg-Marquardt extension to the Newton Method. Common to both Horn and Kumar is the use of a least-mean squares framework. Another approach, and a more general one, is Kalman Filtering. [10, 39] Hel-Or [19] and Hel-Or and Wer-man <ref> [18, 20] </ref> use this approach to fuse heterogeneous measurements into a pose estimation. This is, in fact, just another way of casting the sensor fusion problem. <p> Local search in this large space is robust, but can be a slow process. We will revisit this approach again in Chapter 3. In Kalman Filtering approaches, such as those proposed by Ayache [1] and Hel-Or and Wer-man <ref> [19, 18, 20] </ref>, the matching process is interleaved with the pose estimation process. Rather than finding a complete correspondence and then recovering the pose, individual feature pairs are selected based on consistency with the current match, similar to tree matching but allowing many-to-many mappings. <p> All feature pairs are point-to-point correspondences, which we will see in Chapter 3 leads to unwieldy search spaces. A similar approach, already introduced in section 1.2, is Hel-Or and Werman's Kalman Filtering method <ref> [19, 18, 20] </ref>. This method is even further constrained, assuming that the sensors are coincident (i.e., the focal point and optical axis of the sensors are coincident), the intersensor registration is the identity transformation. <p> This should address least-mean squares stability issues as well as reduce the correspondence search space. The second approach, a Kalman Filtering solution to coregistration, directly extrapolated from Hel-Or and Werman's articulated model formulation <ref> [18, 20] </ref>, is also outlined in Chapter 4. This formalization provides a more general framework for encoding intersensor constraints in much the same way as it provided a general method of encoding constraints for geometric models. <p> The basic idea was to use the iterative nature of Kalman Filtering to incorporate the next, best model-data pair into the match, updating both the pose hypothesis and the covariance of the pose while performing matching. Hel-Or [19] and Hel-Or and Werman <ref> [18, 20] </ref> use an Extended Kalman Filtering framework to deal with both 3D data (including perspective sensors such as standard optical sensors) and the integration of data from heterogeneous sources (i.e., multi-sensor fusion). In the spirit of Ayache, they also interleave the matching process with the pose+covariance update.
Reference: [19] <author> Yacov Hel-Or. </author> <title> Model Based Pose Estimation from Uncertain Data. </title> <type> PhD thesis, </type> <institution> Hebrew University in Jerusalem, </institution> <year> 1993. </year>
Reference-contexts: This work on geometric model based sensor fusion exists at the junction of three subfields of Computer Vision: geometric model matching, model-based pose determination and sensor fusion. Model matching and model-based pose are already closely tied in the Vision literature (e.g., <ref> [34, 1, 2, 29, 19] </ref>). In model matching, a correspondence between features (e.g., lines, corners) in the image and features in the model is sought. In pose determination, the orientation and position of the model relative to the sensor is estimated, often depending upon a presupposed correspondence. <p> However, determining which model and image features should be paired often requires evaluating the goodness of fit given the best pose for the match. In this way, these two tasks are intertwined. Few cases exist of matching or pose determination being tied to sensor fusion (e.g., <ref> [19, 13] </ref>). 1 1.2 Model-Based Pose Different approaches have been taken to model-based pose determination (e.g., [27, 29, 19, 8]). This thesis is concerned with those which act directly on 3D model features, rather than 2D deformable graphs [35], or weak perspective (2D rigid projections) methods [8, 2]. <p> In this way, these two tasks are intertwined. Few cases exist of matching or pose determination being tied to sensor fusion (e.g., [19, 13]). 1 1.2 Model-Based Pose Different approaches have been taken to model-based pose determination (e.g., <ref> [27, 29, 19, 8] </ref>). This thesis is concerned with those which act directly on 3D model features, rather than 2D deformable graphs [35], or weak perspective (2D rigid projections) methods [8, 2]. <p> These criteria functions are optimized using the Levenberg-Marquardt extension to the Newton Method. Common to both Horn and Kumar is the use of a least-mean squares framework. Another approach, and a more general one, is Kalman Filtering. [10, 39] Hel-Or <ref> [19] </ref> and Hel-Or and Wer-man [18, 20] use this approach to fuse heterogeneous measurements into a pose estimation. This is, in fact, just another way of casting the sensor fusion problem. <p> Local search in this large space is robust, but can be a slow process. We will revisit this approach again in Chapter 3. In Kalman Filtering approaches, such as those proposed by Ayache [1] and Hel-Or and Wer-man <ref> [19, 18, 20] </ref>, the matching process is interleaved with the pose estimation process. Rather than finding a complete correspondence and then recovering the pose, individual feature pairs are selected based on consistency with the current match, similar to tree matching but allowing many-to-many mappings. <p> All feature pairs are point-to-point correspondences, which we will see in Chapter 3 leads to unwieldy search spaces. A similar approach, already introduced in section 1.2, is Hel-Or and Werman's Kalman Filtering method <ref> [19, 18, 20] </ref>. This method is even further constrained, assuming that the sensors are coincident (i.e., the focal point and optical axis of the sensors are coincident), the intersensor registration is the identity transformation. <p> as the current estimate of the transformed model point.) Here, R e mo is the current estimated rotation (3 fi 3) matrix and ffi~! mo is the small rotation update represented as a unit rotational axis scaled by the rotational magnitude. 4 Another approach employed by Horn [23] and Hel-Or <ref> [19] </ref> is the use of quaternions. 11 The error terms in equations 2.2 and 2.3 may now be rewritten as follows. <p> These linear segments are then used in a line-to-plane based criterion function. Then we will discuss the use of Kalman Filtering as a replacement for explicit Least-Mean Squares methods. A brief tutorial on Kalman Filtering is provided to minimize hardship. Then an extension to Hel-Or and Werman's sensor fusion <ref> [19, 20] </ref> will be introduced, which considers constrained articulation between sensors. <p> Removal or addition of a single feature has 1 There is, of course, still the problem of converging to local minima, but this appears to be less of a problem in the Kalman Filtering framework using the notes on stable solutions pointed to in <ref> [19] </ref>. 32 (a) (b) (c) line segments extracted using the first phase of Jiang and Bunke's [25] algorithm, and (c) combines (a) and (b). little effect, even assuming that, within a given domain, a good enough initial hypothesis can be generated, the time necessary to evaluate and prune the space using <p> Ayache [1] used Kalman Filtering in a 2D domain. The basic idea was to use the iterative nature of Kalman Filtering to incorporate the next, best model-data pair into the match, updating both the pose hypothesis and the covariance of the pose while performing matching. Hel-Or <ref> [19] </ref> and Hel-Or and Werman [18, 20] use an Extended Kalman Filtering framework to deal with both 3D data (including perspective sensors such as standard optical sensors) and the integration of data from heterogeneous sources (i.e., multi-sensor fusion). <p> Each of these is fully derived in <ref> [19] </ref>. Here we will simply cover the two sensor types of interest to us: perspective optical and perspective range sensors. The basic geometry used for perspective sensors is shown in Figure 4.4. <p> is fl r = B 2 0 fl C Equations 4.3, 4.4, 4.5, 4.7, 4.8, and 4.9, in addition to the covariance matrices defined above, are enough to implement pose determination for a sensor suite where the sensors have coincident focal points. 6 4.3 Kalman Filtering and Articulated Models In <ref> [19, 20] </ref>, articulated objects and constraints in general are added to this framework. <p> While this is not directly the coregistration problem addressed throughout this thesis, its solution provides the basis for the Kalman Filtering coregistration method to be proposed in Section 4.3.1. 6 Though not covered here, computational stability issues are addressed in <ref> [19] </ref>. 41 In Figure 4.5, we find an example of an articulated object. The joints on the bar coupling the piston and the wheel are revolute joints, allowing rotation, but keeping a fixed distance (0) between the two connected points.
Reference: [20] <author> Yacov Hel-Or and Michael Werman. </author> <title> Constraint-fusion for interpretation of articulated objects. </title> <booktitle> In CVPR94, </booktitle> <pages> pages 39-45, </pages> <year> 1994. </year>
Reference-contexts: These criteria functions are optimized using the Levenberg-Marquardt extension to the Newton Method. Common to both Horn and Kumar is the use of a least-mean squares framework. Another approach, and a more general one, is Kalman Filtering. [10, 39] Hel-Or [19] and Hel-Or and Wer-man <ref> [18, 20] </ref> use this approach to fuse heterogeneous measurements into a pose estimation. This is, in fact, just another way of casting the sensor fusion problem. <p> Local search in this large space is robust, but can be a slow process. We will revisit this approach again in Chapter 3. In Kalman Filtering approaches, such as those proposed by Ayache [1] and Hel-Or and Wer-man <ref> [19, 18, 20] </ref>, the matching process is interleaved with the pose estimation process. Rather than finding a complete correspondence and then recovering the pose, individual feature pairs are selected based on consistency with the current match, similar to tree matching but allowing many-to-many mappings. <p> All feature pairs are point-to-point correspondences, which we will see in Chapter 3 leads to unwieldy search spaces. A similar approach, already introduced in section 1.2, is Hel-Or and Werman's Kalman Filtering method <ref> [19, 18, 20] </ref>. This method is even further constrained, assuming that the sensors are coincident (i.e., the focal point and optical axis of the sensors are coincident), the intersensor registration is the identity transformation. <p> This should address least-mean squares stability issues as well as reduce the correspondence search space. The second approach, a Kalman Filtering solution to coregistration, directly extrapolated from Hel-Or and Werman's articulated model formulation <ref> [18, 20] </ref>, is also outlined in Chapter 4. This formalization provides a more general framework for encoding intersensor constraints in much the same way as it provided a general method of encoding constraints for geometric models. <p> These linear segments are then used in a line-to-plane based criterion function. Then we will discuss the use of Kalman Filtering as a replacement for explicit Least-Mean Squares methods. A brief tutorial on Kalman Filtering is provided to minimize hardship. Then an extension to Hel-Or and Werman's sensor fusion <ref> [19, 20] </ref> will be introduced, which considers constrained articulation between sensors. <p> The basic idea was to use the iterative nature of Kalman Filtering to incorporate the next, best model-data pair into the match, updating both the pose hypothesis and the covariance of the pose while performing matching. Hel-Or [19] and Hel-Or and Werman <ref> [18, 20] </ref> use an Extended Kalman Filtering framework to deal with both 3D data (including perspective sensors such as standard optical sensors) and the integration of data from heterogeneous sources (i.e., multi-sensor fusion). In the spirit of Ayache, they also interleave the matching process with the pose+covariance update. <p> is fl r = B 2 0 fl C Equations 4.3, 4.4, 4.5, 4.7, 4.8, and 4.9, in addition to the covariance matrices defined above, are enough to implement pose determination for a sensor suite where the sensors have coincident focal points. 6 4.3 Kalman Filtering and Articulated Models In <ref> [19, 20] </ref>, articulated objects and constraints in general are added to this framework.
Reference: [21] <author> Richard Hoffman and Anil K. Jain. </author> <title> Segmentation and Classification of Range Images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-9(5):608-620, </volume> <month> September </month> <year> 1987. </year>
Reference-contexts: To address this problem, the grouping of range features is introduced into the coregistration error. Historically, range data is collected into planar patches. Hoffman and Jain <ref> [21] </ref>, Jiang and Bunke [25], Masuda and Yokoya [31], Nadabar and Jain [32], and Parvin and Medioni [33] all provide variations on the theme of range image segmentation. In general, these methods are based around region growing with the grouping criteria based on curvature, slope, and continuity.
Reference: [22] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Introduction 1.1 Overview The key contribution of this thesis is an algorithm for the simultaneous refinement of both the sensor suite-to-model pose and sensor-to-sensor registration, or coregistration. Model-to-sensor pose determination (e.g., <ref> [27, 22] </ref>) and homogeneous image registration (e.g., [15]) have both already be dealt with individually. However, there is no simple combination of these two techniques yielding both pose and registration for suites of heterogeneous sensors. <p> Horn <ref> [22] </ref> (pages 303-307) deals with the pose (which he calls "absolute orientation", using the term from photogrametry) of a 3D model relative to 3D data, with all features being 3D points. The error function is the sum of squared Euclidean distances between paired model and data points. <p> In Chapter 2, I present a simple least-squares coregistration solution. The criterion function draws upon previous work by Kumar [27] and Horn <ref> [22, 23] </ref>, with similarities to Eason and Gonzalez's model based sensor fusion [13]. An empirical sensitivity analysis is performed upon an implementation of this algorithm. Finally the algorithm is applied to real data. <p> For example, ~ T mo is the translation from model to optical coordinate frames. 8 mization method (Section 2.4). The coregistration algorithm presented here couples the criterion functions used by Horn <ref> [22] </ref> and Kumar [27]. <p> A standard criterion function (used by Horn in <ref> [22, 23] </ref>) and the one used here is the squared Euclidean distance between the paired points.
Reference: [23] <author> Berthold K. P. Horn. </author> <title> Closed-form Solution of Absolute Orientation Using Unit Quaternions. </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 4(4) </volume> <pages> 629-642, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: The basic optimization approach can be outlined as: * Propose a criterion function: an error measure or fitness function (one is just the inverse of the other). * Either: Find a closed form solution. <ref> [23] </ref> Use a standard iterative optimization procedure such as Newton or Quasi-Newton meth ods. [27] Notice that this outline is general for all optimization problems and as such, will be used as a point of reference for both previous work and the algorithms presented in Chapters 2 and 4. <p> The error function is the sum of squared Euclidean distances between paired model and data points. He solves this system using Newton's method. In <ref> [23] </ref>, he provides an elegant closed form solution. Kumar [27] considers pose determination based upon 3D model lines and 2D image lines, assuming a perspective camera model. He proposes 3 different error functions, one of which will be described in Chapter 2 and another in Chapter 4. <p> In Chapter 2, I present a simple least-squares coregistration solution. The criterion function draws upon previous work by Kumar [27] and Horn <ref> [22, 23] </ref>, with similarities to Eason and Gonzalez's model based sensor fusion [13]. An empirical sensitivity analysis is performed upon an implementation of this algorithm. Finally the algorithm is applied to real data. <p> A standard criterion function (used by Horn in <ref> [22, 23] </ref>) and the one used here is the squared Euclidean distance between the paired points. <p> mi is introduced as the current estimate of the transformed model point.) Here, R e mo is the current estimated rotation (3 fi 3) matrix and ffi~! mo is the small rotation update represented as a unit rotational axis scaled by the rotational magnitude. 4 Another approach employed by Horn <ref> [23] </ref> and Hel-Or [19] is the use of quaternions. 11 The error terms in equations 2.2 and 2.3 may now be rewritten as follows.
Reference: [24] <author> Daniel P. Huttenlocher, Gregory A. Klanderman, and William J. Rucklidge. </author> <title> Comparing Images Using the Hausdorff Distance. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(9) </volume> <pages> 850-863, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: This will be discussed in greater depth in Chapter 4. Not all pose estimation processes deal with explicit correspondences. For breadth, I will briefly introduce one of these. Huttenlocher's Hausdorf Distance <ref> [24] </ref> deals with overall shape properties, rather than explicit pairings between model and data features. Implicit pairings are used instead. While this removes the usually difficult matching problem, it yields other problems. The distance 2 measure is not always an intuitive measure of the goodness of fit between two shapes.
Reference: [25] <author> X. Y. Jiang and H. Bunke. </author> <title> Fast Segmentation of Range Images into Planar Regions by Scan Line Grouping. </title> <type> Technical Report IAM-92-006, </type> <institution> University of Berne, </institution> <year> 1992. </year>
Reference-contexts: One method for selecting t mo and t mr , as stated above, is to assume a Gaussian noise model and set the thresholds to 2. Methods have been proposed for approximating the overall noise in an image <ref> [25, 42, 37] </ref>. Another method for selecting these thresholds (both t mo and t mr ), and the one used in the experiments in Chapter 3, is to observe the distribution of unweighted pair errors given the initial hypothesis. <p> is, of course, still the problem of converging to local minima, but this appears to be less of a problem in the Kalman Filtering framework using the notes on stable solutions pointed to in [19]. 32 (a) (b) (c) line segments extracted using the first phase of Jiang and Bunke's <ref> [25] </ref> algorithm, and (c) combines (a) and (b). little effect, even assuming that, within a given domain, a good enough initial hypothesis can be generated, the time necessary to evaluate and prune the space using median filtering is impractical. <p> To address this problem, the grouping of range features is introduced into the coregistration error. Historically, range data is collected into planar patches. Hoffman and Jain [21], Jiang and Bunke <ref> [25] </ref>, Masuda and Yokoya [31], Nadabar and Jain [32], and Parvin and Medioni [33] all provide variations on the theme of range image segmentation. In general, these methods are based around region growing with the grouping criteria based on curvature, slope, and continuity. <p> In general, these methods are based around region growing with the grouping criteria based on curvature, slope, and continuity. Unfortunately all of these methods require significantly more data on target than we have available. 4.1.1 Linear Segment Extraction Jiang and Bunke <ref> [25] </ref> provide the key to overcoming this data quantity problem. Their two-phase algorithm begins by collecting near-linear segments along scanlines. These scanline segments are then grouped into planar surfaces based upon a region growing criteria, similar to that found in more traditional region segmentation methods. <p> The algorithm for extracting linear segments from range data is shown in Figure 4.3. This is the same as the first phase as Jiang and Bunke's <ref> [25] </ref> planar surface segmentation algorithm. In their algorithm, after they have extracted linear segments along scanlines, they use region growing to produce planar regions. We often do not have large enough planar regions in our images to stably grow regions.
Reference: [26] <author> Rakesh Kumar. </author> <title> Determination of Camera Location and Orientation. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 870 - 881, </pages> <address> Los Altos, CA, </address> <month> June </month> <year> 1989. </year> <title> DARPA, </title> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: While the matrix terms (r mc 1;1 . . . r mc 3;3 ) could be constructed in such a way as to allow only rigid rotations, this would yield an even more nonlinear equation. Kumar <ref> [26, 28] </ref> suggests another approach: Rodriguez's formula, which is an approximation appropriate for small rotations. 4 To rotate a point ~ P mi by an amount R mo , we write R mo ~ P mi = R e ~ P mi + ffi~! mo fi R e ~ P mi
Reference: [27] <author> Rakesh Kumar. </author> <title> Model Dependent Inference of 3D Information From a Sequence of 2D Images. </title> <type> PhD thesis, </type> <institution> University of Massechusetts, </institution> <year> 1992. </year>
Reference-contexts: Introduction 1.1 Overview The key contribution of this thesis is an algorithm for the simultaneous refinement of both the sensor suite-to-model pose and sensor-to-sensor registration, or coregistration. Model-to-sensor pose determination (e.g., <ref> [27, 22] </ref>) and homogeneous image registration (e.g., [15]) have both already be dealt with individually. However, there is no simple combination of these two techniques yielding both pose and registration for suites of heterogeneous sensors. <p> In this way, these two tasks are intertwined. Few cases exist of matching or pose determination being tied to sensor fusion (e.g., [19, 13]). 1 1.2 Model-Based Pose Different approaches have been taken to model-based pose determination (e.g., <ref> [27, 29, 19, 8] </ref>). This thesis is concerned with those which act directly on 3D model features, rather than 2D deformable graphs [35], or weak perspective (2D rigid projections) methods [8, 2]. <p> The basic optimization approach can be outlined as: * Propose a criterion function: an error measure or fitness function (one is just the inverse of the other). * Either: Find a closed form solution. [23] Use a standard iterative optimization procedure such as Newton or Quasi-Newton meth ods. <ref> [27] </ref> Notice that this outline is general for all optimization problems and as such, will be used as a point of reference for both previous work and the algorithms presented in Chapters 2 and 4. <p> The error function is the sum of squared Euclidean distances between paired model and data points. He solves this system using Newton's method. In [23], he provides an elegant closed form solution. Kumar <ref> [27] </ref> considers pose determination based upon 3D model lines and 2D image lines, assuming a perspective camera model. He proposes 3 different error functions, one of which will be described in Chapter 2 and another in Chapter 4. <p> In Chapter 2, I present a simple least-squares coregistration solution. The criterion function draws upon previous work by Kumar <ref> [27] </ref> and Horn [22, 23], with similarities to Eason and Gonzalez's model based sensor fusion [13]. An empirical sensitivity analysis is performed upon an implementation of this algorithm. Finally the algorithm is applied to real data. <p> For example, ~ T mo is the translation from model to optical coordinate frames. 8 mization method (Section 2.4). The coregistration algorithm presented here couples the criterion functions used by Horn [22] and Kumar <ref> [27] </ref>. Coupling these through the constrained multisensor geometry yields a new, more complex and higher dimensional criterion function. 2.3 The Criterion Function When casting pose refinement as an optimization problem, the criterion function measures the goodness or badness of fit between the model and the data for a given pose. <p> a model in a specific pose, I am limiting the treatment in this Chapter to the case where an instance of the model is known to exist in an image. 9 2.3.1 The Optical Sensor Error Term The optical sensor fit error is the same as Kumar's E 1 measure <ref> [27] </ref>. This measures the endpoint-to-plane error between a 3D model line and a 2D data line, assuming a pinhole camera model. The basic idea stems from the perspective mapping process (shown in Figure 2.4). <p> Levenberg-Marquardt effectively trades off control between the Newton second order technique and a first order technique. This deals with the unstable tendencies of the Newton method. Levenberg-Marquardt has proven useful in solving pose determination problems by Lowe [29, 30], Kumar <ref> [27] </ref>, and Beveridge [2]. The first step in optimizing the fit error is to obtain a linearization of the criterion function. <p> a 2.0 subsets 500 Table 3.1: Coregistration and least-median squares parameters. s = 0:6745 is an approximation of the standard deviation for a Gaussian distribution based upon the interquartile range. a is the number of standard deviations to set the cutoff at and is commonly set to a = 2. <ref> [27, 38] </ref> 3.2.1 Results To demonstrate least-median squares optimization, an image suite was selected from the Fort Carson data set [5]: Shot 20 Array 5. The initial pose+registration was selected to fit the data as accurately as possible. <p> To correct for this, we can consider the inverse of the model-to-sensor pose, F 1 . This is actually the transformation we need to be solving for in Kumar's more robust R AND T MOD described in <ref> [27] </ref>. We adopt this measure when formulating the new joint measure.
Reference: [28] <author> Rakesh Kumar and Allen R. Hanson. </author> <title> Robust methods for estimating pose and a sensitivity analysis. </title> <booktitle> CVGIP:Image Understanding, </booktitle> <volume> 11, </volume> <year> 1994. </year> <month> 49 </month>
Reference-contexts: While the matrix terms (r mc 1;1 . . . r mc 3;3 ) could be constructed in such a way as to allow only rigid rotations, this would yield an even more nonlinear equation. Kumar <ref> [26, 28] </ref> suggests another approach: Rodriguez's formula, which is an approximation appropriate for small rotations. 4 To rotate a point ~ P mi by an amount R mo , we write R mo ~ P mi = R e ~ P mi + ffi~! mo fi R e ~ P mi
Reference: [29] <author> David G. Lowe. </author> <title> Fitting Parameterized Three-Dimensional Models to Images. </title> <journal> IEEE Transac--tions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(5) </volume> <pages> 441-450, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: This work on geometric model based sensor fusion exists at the junction of three subfields of Computer Vision: geometric model matching, model-based pose determination and sensor fusion. Model matching and model-based pose are already closely tied in the Vision literature (e.g., <ref> [34, 1, 2, 29, 19] </ref>). In model matching, a correspondence between features (e.g., lines, corners) in the image and features in the model is sought. In pose determination, the orientation and position of the model relative to the sensor is estimated, often depending upon a presupposed correspondence. <p> In this way, these two tasks are intertwined. Few cases exist of matching or pose determination being tied to sensor fusion (e.g., [19, 13]). 1 1.2 Model-Based Pose Different approaches have been taken to model-based pose determination (e.g., <ref> [27, 29, 19, 8] </ref>). This thesis is concerned with those which act directly on 3D model features, rather than 2D deformable graphs [35], or weak perspective (2D rigid projections) methods [8, 2]. <p> Levenberg-Marquardt effectively trades off control between the Newton second order technique and a first order technique. This deals with the unstable tendencies of the Newton method. Levenberg-Marquardt has proven useful in solving pose determination problems by Lowe <ref> [29, 30] </ref>, Kumar [27], and Beveridge [2]. The first step in optimizing the fit error is to obtain a linearization of the criterion function.
Reference: [30] <author> David G. Lowe. </author> <title> Robust Model-based Motion Tracking Through the Integration of Search and Estimation. </title> <journal> International Journal of Computer Vision, </journal> <month> August </month> <year> 1992. </year>
Reference-contexts: Levenberg-Marquardt effectively trades off control between the Newton second order technique and a first order technique. This deals with the unstable tendencies of the Newton method. Levenberg-Marquardt has proven useful in solving pose determination problems by Lowe <ref> [29, 30] </ref>, Kumar [27], and Beveridge [2]. The first step in optimizing the fit error is to obtain a linearization of the criterion function.
Reference: [31] <author> Takeshi Masuda and Naokazu Yokoya. </author> <title> A Robust Method for Registration and Segmentation of Multiple Range Images. </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <volume> 61(3) </volume> <pages> 295-307, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: To address this problem, the grouping of range features is introduced into the coregistration error. Historically, range data is collected into planar patches. Hoffman and Jain [21], Jiang and Bunke [25], Masuda and Yokoya <ref> [31] </ref>, Nadabar and Jain [32], and Parvin and Medioni [33] all provide variations on the theme of range image segmentation. In general, these methods are based around region growing with the grouping criteria based on curvature, slope, and continuity.
Reference: [32] <author> Sateesha G. Nadabar and Anil K. Jain. </author> <title> Fusion of Range and Intensity Images on a Connection Machine (CM-2). </title> <journal> Pattern Recognition, </journal> <volume> 28(1) </volume> <pages> 11-26, </pages> <year> 1995. </year>
Reference-contexts: To address this problem, the grouping of range features is introduced into the coregistration error. Historically, range data is collected into planar patches. Hoffman and Jain [21], Jiang and Bunke [25], Masuda and Yokoya [31], Nadabar and Jain <ref> [32] </ref>, and Parvin and Medioni [33] all provide variations on the theme of range image segmentation. In general, these methods are based around region growing with the grouping criteria based on curvature, slope, and continuity.
Reference: [33] <author> B. Parvin and G. Medioni. </author> <title> Segmentation of Range Images into Planar Surfaces by Split and Merge. </title> <address> pages 415-417, </address> <year> 1986. </year>
Reference-contexts: To address this problem, the grouping of range features is introduced into the coregistration error. Historically, range data is collected into planar patches. Hoffman and Jain [21], Jiang and Bunke [25], Masuda and Yokoya [31], Nadabar and Jain [32], and Parvin and Medioni <ref> [33] </ref> all provide variations on the theme of range image segmentation. In general, these methods are based around region growing with the grouping criteria based on curvature, slope, and continuity.
Reference: [34] <author> Arthur R. Pope. </author> <title> Model-based object recognition: A survey of recent research. </title> <type> Technical Report 94-04, </type> <institution> University of British Columbia, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: This work on geometric model based sensor fusion exists at the junction of three subfields of Computer Vision: geometric model matching, model-based pose determination and sensor fusion. Model matching and model-based pose are already closely tied in the Vision literature (e.g., <ref> [34, 1, 2, 29, 19] </ref>). In model matching, a correspondence between features (e.g., lines, corners) in the image and features in the model is sought. In pose determination, the orientation and position of the model relative to the sensor is estimated, often depending upon a presupposed correspondence.
Reference: [35] <author> Arthur R. Pope and David G. Lowe. </author> <title> Modeling positional uncertainty in object recognition. </title> <type> Technical Report 94-32, </type> <institution> University of British Columbia, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: This thesis is concerned with those which act directly on 3D model features, rather than 2D deformable graphs <ref> [35] </ref>, or weak perspective (2D rigid projections) methods [8, 2]. Methods which deal directly with 3D model features can utilize full 3D perspective and implicitly deal with any view of the object. 2D methods can correct only somewhat for perspective projections.
Reference: [36] <author> William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: There are many general techniques for optimizing non-linear functions <ref> [43, 36] </ref>, all of which are iterative. The optimization technique used here is similar to the Newton method, called Levenberg-Marquardt [36]. The basic idea is to optimize the nonlinear function, linearized about the current estimate, as in the Newton method. <p> There are many general techniques for optimizing non-linear functions [43, 36], all of which are iterative. The optimization technique used here is similar to the Newton method, called Levenberg-Marquardt <ref> [36] </ref>. The basic idea is to optimize the nonlinear function, linearized about the current estimate, as in the Newton method. Levenberg-Marquardt effectively trades off control between the Newton second order technique and a first order technique. This deals with the unstable tendencies of the Newton method.
Reference: [37] <author> Visvanathan Ramesh. </author> <title> Performance Characterization of Image Understanding Algorithms. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1995. </year>
Reference-contexts: One method for selecting t mo and t mr , as stated above, is to assume a Gaussian noise model and set the thresholds to 2. Methods have been proposed for approximating the overall noise in an image <ref> [25, 42, 37] </ref>. Another method for selecting these thresholds (both t mo and t mr ), and the one used in the experiments in Chapter 3, is to observe the distribution of unweighted pair errors given the initial hypothesis. <p> When a process model for the system being modeled exists, then "optimal" means most likely. Part of the assumptions made by Kalman Filtering includes notions of what properties the noise within the measurements will have. While some efforts have been made to compute the noise properties of image features <ref> [42, 37] </ref>, these are not proven. So, while Kalman Filtering literature will sometimes refer to the solution being "most likely," I will only refer to it as being "optimal." In more specific terms, the goal in Kalman Filtering is to construct a recursive estimator of the current system state, s.
Reference: [38] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Coregistration of Range and Optical Images Using Coplanarity and Orientation Constraints. </title> <booktitle> CVPR 96, </booktitle> <pages> pages 899 - 906, </pages> <year> 1996. </year>
Reference-contexts: a 2.0 subsets 500 Table 3.1: Coregistration and least-median squares parameters. s = 0:6745 is an approximation of the standard deviation for a Gaussian distribution based upon the interquartile range. a is the number of standard deviations to set the cutoff at and is commonly set to a = 2. <ref> [27, 38] </ref> 3.2.1 Results To demonstrate least-median squares optimization, an image suite was selected from the Fort Carson data set [5]: Shot 20 Array 5. The initial pose+registration was selected to fit the data as accurately as possible.
Reference: [39] <author> H. W. Sorenson. </author> <title> Least-squares Estimation from Gauss to Kalman. </title> <journal> IEEE Spectrum, </journal> <pages> pages 63-68, </pages> <month> July </month> <year> 1970. </year>
Reference-contexts: These criteria functions are optimized using the Levenberg-Marquardt extension to the Newton Method. Common to both Horn and Kumar is the use of a least-mean squares framework. Another approach, and a more general one, is Kalman Filtering. <ref> [10, 39] </ref> Hel-Or [19] and Hel-Or and Wer-man [18, 20] use this approach to fuse heterogeneous measurements into a pose estimation. This is, in fact, just another way of casting the sensor fusion problem. <p> In this form, ^s t is now a vector estimating the state ~s and fl t is the covariance matrix of ~w. This can be shown to subsume Gauss's standard least-squares estimation <ref> [39] </ref>. Beyond this, people also speak of Extended Kalman Filters. This notion is the intuitive cross between gradient decent in standard least squares optimization and Kalman Filtering.
Reference: [40] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Interleaving 3D Model Feature Prediction and Matching to Support Multi-Sensor Object Recognition. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 699-706, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: For color, we utilized Stevens's model driven line extraction [41]. Here, the silhouette and important interior lines are first extracted from the model given the hypothesized model-to-camera pose. These lines are then placed over the image and decoupled (a version with coupled model features is introduced in <ref> [40] </ref>). Each line is locally moved over the color/optical image to optimize the gradient under the edge. The 2D mapping of the optimal model line placement is the reported image feature. <p> This level of testing will soon be possible. The work on hypothesis driven matching by Bev-eridge, et. al. [3, 4, 6] lays out a framework for driving a multisensor matching system, including a coregistration phase. The work by Stevens and Beveridge <ref> [40] </ref> demonstrates coregistration through pose+registration search, rather than correspondence search. Combining these with the coregis-tration algorithms presented in this thesis would provide a complete system.
Reference: [41] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Optical Linear Feature Detection Based on Model Pose. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 695-697, </pages> <address> Los Altos, CA, </address> <month> Febru-ary </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Given the differences in the optical and the range modalities, we chose two separate extraction methods, one for color and one for range. For color, we utilized Stevens's model driven line extraction <ref> [41] </ref>. Here, the silhouette and important interior lines are first extracted from the model given the hypothesized model-to-camera pose. These lines are then placed over the image and decoupled (a version with coupled model features is introduced in [40]).
Reference: [42] <author> William M. Wells III. </author> <title> Statistical Object Recognition. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: One method for selecting t mo and t mr , as stated above, is to assume a Gaussian noise model and set the thresholds to 2. Methods have been proposed for approximating the overall noise in an image <ref> [25, 42, 37] </ref>. Another method for selecting these thresholds (both t mo and t mr ), and the one used in the experiments in Chapter 3, is to observe the distribution of unweighted pair errors given the initial hypothesis. <p> When a process model for the system being modeled exists, then "optimal" means most likely. Part of the assumptions made by Kalman Filtering includes notions of what properties the noise within the measurements will have. While some efforts have been made to compute the noise properties of image features <ref> [42, 37] </ref>, these are not proven. So, while Kalman Filtering literature will sometimes refer to the solution being "most likely," I will only refer to it as being "optimal." In more specific terms, the goal in Kalman Filtering is to construct a recursive estimator of the current system state, s.
Reference: [43] <author> Wayne L. Winston. </author> <title> Introduction to Mathematical Programming: Applications and Algorithms. </title> <publisher> Duxbury Press, </publisher> <address> 2nd edition, </address> <year> 1995. </year> <month> 50 </month>
Reference-contexts: There are many general techniques for optimizing non-linear functions <ref> [43, 36] </ref>, all of which are iterative. The optimization technique used here is similar to the Newton method, called Levenberg-Marquardt [36]. The basic idea is to optimize the nonlinear function, linearized about the current estimate, as in the Newton method.
References-found: 43

