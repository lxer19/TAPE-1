URL: http://www.speech.sri.com/projects/hybrid/papers/darpa.92.cohen.ps
Refering-URL: http://www.speech.sri.com/projects/hybrid/publications.html
Root-URL: 
Title: COMBINING NEURAL NETWORKS AND HIDDEN MARKOV MODELS M FOR CONTINUOUS SPEECH RECOGNITION e present a
Author: ichael Cohen*, David Rumelhart**, Nelson Morgan***, Horacio Franco*, Victor Abrash*, and Yochai Konig*** W N 
Note: 1. INTRODUCTIO  
Date: ABSTRACT  
Address: Menlo Park, CA 94025  Stanford, CA 94305  1947 Center Street, Suite 600, Berkeley, CA 94704  
Affiliation: Speech Research Program, SRI International,  Stanford University, Dept. of Psychology,  Intl. Computer Science Inst.,  
Abstract-found: 0
Intro-found: 0
Reference: <institution> h </institution>
Reference: [1] <author> N. Morgan and H. Bourlard, </author> <title> ``Continuous Speec ecognition Using Multilayer Perceptrons with Hidden Mar-M kov Models,'' </title> <booktitle> ICASSP 90 , pp. </booktitle> <pages> 413-416, </pages> <address> Albuquerque, New exico, </address> <year> 1990. </year>
Reference-contexts: 1. INTRODUCTIO idden Markov models (HMMs) are used in most state-of a the-art continuous-speech recognition systems. This pproach is limited by the need for strong statistical assump u tions that are unlikely to be valid for speech. Techniques sing multilayer perceptrons (MLPs) for probability estima-a tion have recently been introduced <ref> [1] </ref>, which reduce the ssumption of independence for multifeature probability y computation. Additional advantages of MLP probabilit stimation include the inherently discriminant nature of the l training algorithm and the distributed representation, which eads to efficient use of the available parameters. <p> HMMs, on the other hand, provide a frame ork for simultaneous segmentation and classification of - speech, which has been demonstrated to be useful for con inuous recognition. Previous work by Morgan and Bourlard d <ref> [1] </ref> has shown theoretically and practically that MLPs an MMs can be combined by using MLPs for the estimation , of the HMM state-dependent observation probabilities hereby exploiting the advantages of both approaches. - We have incorporated MLP-based probability estima ion techniques into the HMM-based SRI-DECIPHER (TM) , system, which is <p> With this raining criterion and target distribution, assuming enough parameters in the MLP, enough training data, and that the o training does not get stuck in a local minimum, the MLP utputs will approximate the posterior class probabilities t j <ref> [1] </ref>. Frame classification on an independen t P (q e Y ) cross-validation set is used to control the learning rate and to g decide when to stop training as in [9].

Reference: [8] <author> D. Rumelhart, G. Hinton, and R. Williams, </author> <title> ``Learning l Internal Representations by Error Propagation,'' in Paralle istributed Processing: </title> <journal> Explorations of the Microstructure - of Cognition, </journal> <volume> vol 1: </volume> <booktitle> Foundations, </booktitle> <editor> D Rumelhart & J. McClel and, Eds., </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year> - 
Reference-contexts: P (q e Y ) j t The factor is the posterior probability of phone class given the input vector at time . This is comq Y t p uted by a back-propagation-trained <ref> [8] </ref> three-layer feed forward MLP. is the prior probability of phone classP (q ) j and is estimated by counting class occurrences in theq P (Y ) t l examples used to train the MLP. is common to al tates for any given time frame, and can therefore be dis t
Reference: [9] <author> S. Renals, N. Morgan, M. Cohen, H. Franco, </author> <title> ``Connec ionist Probability Estimation in the DECIPHER Speech n Recognition System,'' </title> <booktitle> ICASSP 92, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 601-604, </pages> <address> Sa rancisco, </address> <year> 1992. </year>
Reference-contexts: Frame classification on an independen t P (q e Y ) cross-validation set is used to control the learning rate and to g decide when to stop training as in <ref> [9] </ref>. The initial learnin ate is kept constant until cross-validation performance increases less than 0.5%, after which it is reduced as 1/2 n until performance increases no further. 3. <p> This resulted in an . increase in word-recognition error rate by almost 30% xperiments at ICSI had a similar result <ref> [9] </ref>. The higher f error rate seemed to be due to the discriminative nature o he MLP training algorithm. The new MLP, with 200 output , units, was attempting to discriminate subphonetic classes orresponding to HMM states. <p> The MIXED system uses a weighted mixture of the - logs of state observation likelihoods provided by a 2,000 idden-unit version of the CI-MLP and the CD-HMM <ref> [9] </ref>. a This system shows the best recognition performance so far chieved with the DECIPHER (TM) system on the Resource r Management database. In all six tests, it performs bette han the CD-HMM system. The CI-MLP system performs worse than any of the , other (context-dependent) systems.

References-found: 4

