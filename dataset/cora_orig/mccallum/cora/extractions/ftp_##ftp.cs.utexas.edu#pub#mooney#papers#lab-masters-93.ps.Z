URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/lab-masters-93.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abduction.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: INDUCTIVE LEARNING FOR ABDUCTIVE DIAGNOSIS  
Author: Raymond J. Mooney Bruce W. Porter 
Degree: APPROVED: Supervisor:  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aha, D. W., Kibler, D., and Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66. </pages>
Reference-contexts: Experiments on some of the standard single-disorder data sets should be performed; however, abduction's real advantage is on multiple-disorder diagnosis. Also, Lab should be compared to additional learning methods such as Protos, instance-based methods <ref> (Aha et al., 1991) </ref>, etc. Finally, the method needs to be extended to produce more complex abductive knowledge bases that include causal chaining (Peng and Reggia, 1990), rules with multiple antecedents, incompatible disorders, and first-order predicates (Ng, 1992). Chapter 7 Conclusion Abduction is an increasingly popular approach to multiple-disorder diagnosis.
Reference: <author> Allemang, D., Tanner, M. C., Bylander, T., and Josephson, J. R. </author> <year> (1987). </year> <title> Computational complexity of hypothesis assembly. </title> <booktitle> In Proceedings of the Tenth International Joint conference on Artificial intelligence, </booktitle> <pages> pages 1112-1117. </pages> <address> Milan, Italy. </address>
Reference-contexts: Because of this, Baysian Networks may have to be integrated with another system to help discriminate between diagnoses. Also, actually performing diagnosis once the network is built is in general NP-hard if loops exist in the network. Many alternative formalisms exist for abduction. <ref> (Allemang et al., 1987) </ref> use an abductive model similar to that of Peng & Reggia, but with an approximate algorithm. The ATMS (Assumption-based Truth Maintenance System) (de Kleer, 1986) reasons using propositional Horn-clause axioms. The ATMS and similar systems have been used in domains other than diagnosis.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Monterey, CA: </address> <publisher> Wadsworth and Brooks. </publisher>
Reference: <author> Charniak, E. and McDermott, D. </author> <year> (1985). </year> <title> Introduction to AI. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: A more formal definition as it is used within artificial intelligence (AI) defines an abductive explanation as a consistent set of assumptions which, together with background knowledge, logically entails a set of observations <ref> (Charniak and McDermott, 1985) </ref>. There are many situations in which abduction is used in everyday life. Physicians make inferences to conclude which diseases a patient may have based on the symptoms present and their knowledge of disease-symptom interaction.
Reference: <author> Cooper, G. G. and Herskovits, E. </author> <year> (1992). </year> <title> A bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347. </pages> <editor> de Kleer, J. </editor> <year> (1986). </year> <title> An assumption-based TMS. </title> <journal> Artificial Intelligence, </journal> 28:127-162. de Kleer, J. and Williams, B. C. (1987). Diagnosing multiple faults. Artificial Intelligence, 32:97-130. 
Reference: <author> Elstein, A., l. Shulman, and Sprafka, S. </author> <year> (1978). </year> <title> Medical Problem Solving AnAnal-ysis of Clinical Reasoning. </title> <publisher> Harvard University Press. </publisher> <address> 49 50 Geiger, D., </address> <note> Paz, </note> <author> A., and Pearl, J. </author> <year> (1990). </year> <title> Learning causal trees from dependence information. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 770-776. </pages> <month> Boston,MA. </month>
Reference: <author> Josephson, J. R., Chandrasekaran, B., Smith, J. R., and Tanner, M. C. </author> <year> (1987). </year> <title> A mechanism for forming composite explanatory hypotheses. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 17(3) </volume> <pages> 445-454. </pages>
Reference-contexts: Abductive methods have proven their use in domains such as diagnosing brain damage due to stroke (Tuhrim et al., 1991) and identifying red-cell antibodies in blood <ref> (Josephson et al., 1987) </ref>. The rule bases used in diagnosis are typically large and complex. As in many other tasks, great savings are often possible by learning these rule bases instead of hand-coding them using expert knowledge.
Reference: <author> Kassirer, J. </author> <year> (1978). </year> <title> Clinical problem solving: A behavioral analysis. </title> <journal> Annals of Internal Medicine, </journal> <volume> 89 </volume> <pages> 245-255. </pages>
Reference: <author> Konolige, K. </author> <year> (1992). </year> <title> Abduction versus closure in causal theories. </title> <journal> Artificial Intelligence, </journal> <volume> 53 </volume> <pages> 255-272. </pages>
Reference: <author> Kulikowski, C. A. and Weiss, S. M. </author> <year> (1991). </year> <title> Computer Systems That Learn Classification and Prediction Methods from Statistics, Neural Nets, </title> <booktitle> Machine Learning, and Expert Systems. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Levesque, H. J. </author> <year> (1989). </year> <title> A knowledge-level account of abduction. </title> <booktitle> In Proceedings of the Eleventh International Joint conference on Artificial intelligence, </booktitle> <pages> pages 1061-1067. </pages> <address> Detroit, MI. </address>
Reference-contexts: The ATMS (Assumption-based Truth Maintenance System) (de Kleer, 1986) reasons using propositional Horn-clause axioms. The ATMS and similar systems have been used in domains other than diagnosis. Recently, Levesque <ref> (Levesque, 1989) </ref> has shown that with a slightly different formalism of abduction than used here, the ATMS performs abduction precisely as he defines it. The Accel system by Ng (Ng, 1992) extends the ATMS to first-order Horn-clause axioms with variables, and also works in domains other than diagnosis.
Reference: <author> Michalksi, R., Mozetic, I., Hong, J., and Lavrac, N. </author> <year> (1986). </year> <title> The multi-purpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1041-1045. </pages> <address> Philadelphia, PA. </address>
Reference-contexts: For example, systems such as ID3 (Quinlan, 1986) learn decision trees (Quinlan, 1986; Breiman et al., 1984), which can be rewritten as rules of this type (Quinlan, 1987). AQ <ref> (Michalksi et al., 1986) </ref> is a second system which learns rules to be used deductively. While inductive learning for deductive diagnosis is a well-understood problem, it has several limitations: First, much knowledge familiar to domain experts is better suited to abductive, not deductive reasoning.
Reference: <author> Mooney, R. J. </author> <title> (to appear). Encouraging experimental results on learning CNF. Machine Learning. 51 Ng, </title> <editor> H. T. </editor> <year> (1992). </year> <title> A General Abductive System with Applications to Plan Recognition and Diagnosis. </title> <type> PhD thesis, </type> <institution> Austin, TX: University of Texas. </institution>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo,CA: </address> <publisher> Morgan Kaufmann, Inc. </publisher>
Reference-contexts: This would make it impractical to use in a real world situation, where the diagnostician would typically like to see explanations for the diagnosis output. Our causal rules could handle this much more easily. 36 37 A second method is Baysian Networks <ref> (Pearl, 1988) </ref>, which are also networks which represent causality in a graph, but with conditional probabilities and some prior probabilities attached to the nodes. The nodes represent states of affairs (our disorders and manifestations), and the arcs are the causal connections.
Reference: <author> Peirce, C. S. </author> <year> (1958). </year> <title> Collected Papers of Charles Sanders Peirce. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Introduction Intelligent activities often require one to construct explanations of phenomena. This paper views these explanatory processes as abduction. The philosopher C.S. Peirce <ref> (Peirce, 1958) </ref> defined abduction as the process of finding the best explanation for a set of observations; or inferring cause from effect.
Reference: <author> Peng, Y. and Reggia, J. A. </author> <year> (1990). </year> <title> Abductive Inference Models for Diagnostic Problem-Solving. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Find: Minimal sets A of atoms (the assumptions) such that A [ T j= O and A [ T is consistent. 2.1 Parsimonious Covering Our method for performing abduction on the rules learned by Lab is the set-covering approach presented in <ref> (Peng and Reggia, 1990) </ref>. Although a simple, propositional model, it is capable of solving many real world problems. In addition, its propositional representation is not any more restrictive than most inductive learning systems, which use discrete-valued feature vectors in representing examples. <p> Therefore, C can be viewed as the knowledge base or domain theory for abductive diagnosis. For the abduction portion of our algorithm, I implemented a Lisp version of the algorithm Bipartite given by <ref> (Peng and Reggia, 1990) </ref>. The pseudocode for Bipartite is given in Figure 2.1. The input is a set of manifestations, M + , and a causation relation, C. <p> revise (hypothesis; causes (m)) return hypothesis end Bipartite revise (G; H 1 ) F := div (G; H 1 ) Q := augres (G; H 1 ) return F [ res (Q; F ) tion of "the set of all irredundant [minimal] covers of all manifestations known to be present" <ref> (Peng and Reggia, 1990) </ref>. Bipartite works in an incremental fashion, looking at one manifestation at a time, and incorporating its causes into the diagnosis so far. The functions div, res and augres perform the following functions. <p> Therefore, the res operation (described as residual of division) removes these redundant covers. By combining the results of div and res, all covers of O [ fmg can be constructed from all covers of O and from the disorders in causes (m). See <ref> (Peng and Reggia, 1990) </ref> for details on Bipartite. 8 m hypotheses sni*es (cold _ pneumonia _ allergy _ hay-fever ) fever (cold _ pneumonia _ (hay-fever ^ typhoid) _ (allergy ^ typhoid)) cough ((cold ^ lung-cancer) _ (pneumonia ^ lung-cancer) _ (cold ^ emphysema) _ (pneumonia ^ emphysema) _ (cold ^ <p> Finally, in the worst case, we could add all possible rules in D fi M. Now we have the number of times which Bipartite is run, but we have not discussed the complexity of Bipartite. Finding all minimum covers of a given M + i is NP-hard <ref> (Peng and Reggia, 1990) </ref>. The complexity of Bipartite itself is O (2 M + i ) for a particular example E i , since we are finding all minimal covers. <p> A second opportunity for improvement is to reduce the number of diagnoses returned to only one, during both training and testing. One way this could be done is by using a probabilistic model for our abduction. One such model is outlined in <ref> (Peng and Reggia, 1990) </ref>, and ranks explanations based on 39 40 information about the probability of a disorder occurring and the probability that a disorder causes a manifestation. These are assumed to be available, as is the causation relation for Bipartite. <p> Also, Lab should be compared to additional learning methods such as Protos, instance-based methods (Aha et al., 1991), etc. Finally, the method needs to be extended to produce more complex abductive knowledge bases that include causal chaining <ref> (Peng and Reggia, 1990) </ref>, rules with multiple antecedents, incompatible disorders, and first-order predicates (Ng, 1992). Chapter 7 Conclusion Abduction is an increasingly popular approach to multiple-disorder diagnosis. However, the problem of automatically learning abductive rule bases from training examples has not previously been addressed.
Reference: <author> Pople, Jr., H. E. </author> <year> (1973). </year> <booktitle> On the mechanization of abductive logic. In Proceedings of the Third International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 147-152. </pages>
Reference: <author> Porter, B., Bareiss, R., and Holte, R. </author> <year> (1990). </year> <title> Concept learning and heuristic classification in weak-theory domains. </title> <journal> Artificial Intelligence, </journal> <volume> 45 </volume> <pages> 229-263. </pages>
Reference-contexts: Almost any learning system can learn to do diagnosis, some more successfully than others. We have already mentioned the methods of learning rules for deduction, both in the introduction and in our comparisons with ID3 and PFoil. An alternative is Protos <ref> (Porter et al., 1990) </ref>, which uses an exemplar-based approach to learning. Protos has been tested, with favorable results, in the domain of clinical audiology. Two other methods that seem particularly well-suited to do diagnosis are neural networks and Baysian Networks.
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266. </pages>
Reference-contexts: In order to make it comparable to Lab and PFoil, no pruning is performed. PFoil 1 We were only able to obtain fifty out of the 100 cases from the authors of the original study. 20 21 (Mooney, to appear) is a propositional version of Foil <ref> (Quinlan, 1990) </ref>. Foil is a system for learning first-order Horn clauses; however the basic algorithm is a heuristic covering algorithm for learning DNF expressions. The primary simplification of PFoil compared to Foil is that it only needs to deal with fixed examples rather than the expanding tuples of Foil.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106. </pages>
Reference-contexts: Traditionally, research on symbolic learning for diagnosis has assumed 3 a deductive model of classification, in contrast to our abductive model. Rules of the form manifestations ! disorder are induced from a training set, and deduction is used to diagnose future cases. For example, systems such as ID3 <ref> (Quinlan, 1986) </ref> learn decision trees (Quinlan, 1986; Breiman et al., 1984), which can be rewritten as rules of this type (Quinlan, 1987). AQ (Michalksi et al., 1986) is a second system which learns rules to be used deductively. <p> In addition, we obtained the accompanying abductive knowledge base generated by an expert, which consists of 648 rules (hereafter called the expert KB, or original KB). We ran our experiments with Lab, ID3, PFoil, and a neural network. ID3 <ref> (Quinlan, 1986) </ref> learns a decision tree for classifying examples into multiple categories, and uses an information gain criterion for building the tree. In order to make it comparable to Lab and PFoil, no pruning is performed.
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Generating production rules from decision trees. </title> <booktitle> In Proceedings of the Tenth International Joint conference on Artificial intelligence, </booktitle> <pages> pages 304-307. </pages> <address> Milan, Italy. </address>
Reference-contexts: Rules of the form manifestations ! disorder are induced from a training set, and deduction is used to diagnose future cases. For example, systems such as ID3 (Quinlan, 1986) learn decision trees (Quinlan, 1986; Breiman et al., 1984), which can be rewritten as rules of this type <ref> (Quinlan, 1987) </ref>. AQ (Michalksi et al., 1986) is a second system which learns rules to be used deductively. While inductive learning for deductive diagnosis is a well-understood problem, it has several limitations: First, much knowledge familiar to domain experts is better suited to abductive, not deductive reasoning.
Reference: <author> Reiter, R. </author> <year> (1987). </year> <title> A theory of diagnosis from first principles. </title> <journal> Artificial Intelligence, </journal> <volume> 32 </volume> <pages> 57-95. </pages> <note> 52 Rubin, </note> <author> A. </author> <year> (1975). </year> <title> The role of hypothesis in medical diagnosis. </title> <booktitle> In Proceedings of the Fourth International Joint conference on Artificial intelligence, </booktitle> <pages> pages 856-862. </pages>
Reference-contexts: One weakness in the method is that the model can be wrong or inexact. A second alternative is Reiter's <ref> (Reiter, 1987) </ref> diagnosis from first principles, which reasons from the system description and observations of the systems behavior. Chapter 6 Future Work There are many opportunities for future work in this area. We can see some of these in the results obtained with Lab.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, J. R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <pages> pages 318-362. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The primary simplification of PFoil compared to Foil is that it only needs to deal with fixed examples rather than the expanding tuples of Foil. The neural network was trained using standard backpropagation <ref> (Rumelhart et al., 1986) </ref> with one hidden layer. ID3 and PFoil are typically used for single class tasks, where each example represents only one category. In other words, there may be multiple categories to choose from, but each example can only belong to one of the multiple categories.

References-found: 23

