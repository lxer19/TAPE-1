URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/richards.apin97.ps.gz
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Email: orb,moriarty,risto@cs.utexas.edu  
Title: Evolving Neural Networks to Play Go  
Author: Norman Richards, David E. Moriarty, and Risto Miikkulainen 
Address: Austin, Tx 78712  
Affiliation: The University of Texas at Austin  
Abstract: Go is a difficult game for computers to master, and the best go programs are still weaker than the average human player. Since the traditional game playing techniques have proven inadequate, new approaches to computer go need to be studied. This paper presents a new approach to learning to play go. The SANE (Symbiotic, Adaptive Neuro-Evolution) method was used to evolve networks capable of playing go on small boards with no pre-programmed go knowledge. On a 9 fi 9 go board, networks that were able to defeat a simple computer opponent were evolved within a few hundred generations. Most significantly, the networks exhibited several aspects of general go playing, which suggests the approach could scale up well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Herbert D. Enderton. </author> <title> The Golem go program. </title> <type> Technical Report CMU-CS-92-101, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: Such features could include common patterns and positions such as an eye or a group or even complicated constructs such as the ladder. These features would then be used as inputs to the neural network <ref> [1, 11] </ref>. It would still probably be useful to let the network develop its own features as well, but the pre-programmed features might allow it to learn faster and deal with more complex patterns. SANE demonstrates the feasibility of evolving structures on more than one level at the same time.
Reference: [2] <author> Markus Enzenberger. </author> <title> The integration of a priori knowledge into a go playing neural network. </title> <type> Manuscript, </type> <year> 1996. </year>
Reference-contexts: Since the networks are not given any prior knowledge about what features are relevant to playing go, they are forced to discover useful features themselves. Allowing the network to access a pre-defined feature space instead of looking at the raw board might make the task easier <ref> [2] </ref>. Such features could include common patterns and positions such as an eye or a group or even complicated constructs such as the ladder. These features would then be used as inputs to the neural network [1, 11].
Reference: [3] <author> Faustino Gomez and Risto Miikkulainen. </author> <title> Incremental evolution of complex general behavior. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 5 </volume> <pages> 317-342, </pages> <year> 1997. </year>
Reference-contexts: First, it creates diversity in the blueprint population, and second, it explores new structures created by the neuron population. 6 Applying SANE to Go SANE has previously been shown effective in several sequential decision tasks including robot control [7, 8, 9], constraint satisfaction [10], pursuit and evasion <ref> [3] </ref>, and the game of Othello [6, 8, 10]. This paper will evaluate the usefulness of SANE in learning to play go. SANE is used to evolve networks to play on small boards against a simple computer opponent, and the scale-up properties are evaluated.
Reference: [4] <author> Feng hsiung Hsu, Thomas Anantharaman, Murray Campbell, and Andreas Nowatzyk. </author> <title> A grandmaster chess machine. </title> <journal> Scientific American, </journal> <volume> 263 </volume> <pages> 44-50, </pages> <year> 1990. </year> <month> 16 </month>
Reference-contexts: Whereas in games like chess, Othello and checkers the traditional game playing techniques such as minimax search and its variations are competitive even at the master level <ref> [4, 5, 13] </ref>, those techniques, when applied to go, have not been able to produce programs that can challenge even weak amateur players. The best computer programs in the world are ranked 6-8 stones below what would be considered master level.
Reference: [5] <author> Kai-Fu Lee and S. Mahajan. </author> <title> The development of a world-class othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 21-36, </pages> <year> 1990. </year>
Reference-contexts: Whereas in games like chess, Othello and checkers the traditional game playing techniques such as minimax search and its variations are competitive even at the master level <ref> [4, 5, 13] </ref>, those techniques, when applied to go, have not been able to produce programs that can challenge even weak amateur players. The best computer programs in the world are ranked 6-8 stones below what would be considered master level.
Reference: [6] <author> David Moriarty and Risto Miikkulainen. </author> <title> Discovering complex Othello strategies through evolutionary neural networks. </title> <journal> Connection Science, </journal> <volume> 7 </volume> <pages> 195-209, </pages> <year> 1995. </year>
Reference-contexts: diversity in the blueprint population, and second, it explores new structures created by the neuron population. 6 Applying SANE to Go SANE has previously been shown effective in several sequential decision tasks including robot control [7, 8, 9], constraint satisfaction [10], pursuit and evasion [3], and the game of Othello <ref> [6, 8, 10] </ref>. This paper will evaluate the usefulness of SANE in learning to play go. SANE is used to evolve networks to play on small boards against a simple computer opponent, and the scale-up properties are evaluated.
Reference: [7] <author> David Moriarty and Risto Miikkulainen. </author> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 11-32, </pages> <year> 1996. </year>
Reference-contexts: This paper explores the usefulness of neuro-evolution as a mechanism for learning to play go. The SANE (Symbiotic, Adaptive Neuro-Evolution <ref> [7, 8, 9] </ref>) algorithm demonstrates that networks that display a general ability in playing go on small boards can be evolved without fl To appear in Applied Intelligence. y This research was supported in part by NSF under grant #IRI-9504317. 1 (a) Four liberties. (b) One liberty. (c) No liberties. one. <p> In go, this problem is severe enough that standard learning techniques such as backpropagation cannot be effectively applied. 5 SANE SANE 1 (Symbiotic Adaptive Neuro-Evolution <ref> [7, 8, 9] </ref>) solves the credit assignment problem by using evolutionary algorithms to search for effective neural networks. Instead of punishing or rewarding individual moves, networks are evaluated, selected, and recombined based on their overall performance in the game. <p> For this application, the output units are linear so that both positive and negative values and be generated. Neural networks could be formed by randomly choosing neurons from the neuron population. In fact, this approach performs well in simpler problems <ref> [7, 10] </ref>. However, random participation does not retain knowledge of the best combinations of neurons and can often 6 neural network is formed from a blueprint by following its neuron pointers and decoding the respective neurons. stall the search in more difficult problems [8]. <p> First, it creates diversity in the blueprint population, and second, it explores new structures created by the neuron population. 6 Applying SANE to Go SANE has previously been shown effective in several sequential decision tasks including robot control <ref> [7, 8, 9] </ref>, constraint satisfaction [10], pursuit and evasion [3], and the game of Othello [6, 8, 10]. This paper will evaluate the usefulness of SANE in learning to play go.
Reference: [8] <author> David E. Moriarty. </author> <title> Symbiotic Evolution of Neural Networks in Sequential Decision Tasks. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, The University of Texas at Austin, </institution> <year> 1997. </year> <note> Technical Report UT-AI97-259. </note>
Reference-contexts: This paper explores the usefulness of neuro-evolution as a mechanism for learning to play go. The SANE (Symbiotic, Adaptive Neuro-Evolution <ref> [7, 8, 9] </ref>) algorithm demonstrates that networks that display a general ability in playing go on small boards can be evolved without fl To appear in Applied Intelligence. y This research was supported in part by NSF under grant #IRI-9504317. 1 (a) Four liberties. (b) One liberty. (c) No liberties. one. <p> In go, this problem is severe enough that standard learning techniques such as backpropagation cannot be effectively applied. 5 SANE SANE 1 (Symbiotic Adaptive Neuro-Evolution <ref> [7, 8, 9] </ref>) solves the credit assignment problem by using evolutionary algorithms to search for effective neural networks. Instead of punishing or rewarding individual moves, networks are evaluated, selected, and recombined based on their overall performance in the game. <p> Instead of punishing or rewarding individual moves, networks are evaluated, selected, and recombined based on their overall performance in the game. Evolutionary algorithms perform a global, parallel 1 SANE is described in more detail in <ref> [8] </ref>, and the source code can be obtained from http://www.cs.utexas.edu/users/nn/. 5 shown on the left, and the corresponding network is shown on the right. <p> The blueprint population maintains and exploits effective combinations of individuals in the neuron population. Conjunctively, the two levels of evolution provide an efficient genetic search that is capable of solving difficult real-world decision problems with minimal domain information <ref> [8] </ref>. In the neuron population, SANE evolves a large population of hidden neuron definitions for a three-layer feedforward network (figure 4). A neuron is represented by a series of connection definitions that describe the weighted connections of the neuron from the input layer and to the output layer. <p> However, random participation does not retain knowledge of the best combinations of neurons and can often 6 neural network is formed from a blueprint by following its neuron pointers and decoding the respective neurons. stall the search in more difficult problems <ref> [8] </ref>. To focus the search on the best neuron combinations, SANE maintains and evolves a separate population of good neuron combinations called neural network blueprints. <p> First, it creates diversity in the blueprint population, and second, it explores new structures created by the neuron population. 6 Applying SANE to Go SANE has previously been shown effective in several sequential decision tasks including robot control <ref> [7, 8, 9] </ref>, constraint satisfaction [10], pursuit and evasion [3], and the game of Othello [6, 8, 10]. This paper will evaluate the usefulness of SANE in learning to play go. <p> diversity in the blueprint population, and second, it explores new structures created by the neuron population. 6 Applying SANE to Go SANE has previously been shown effective in several sequential decision tasks including robot control [7, 8, 9], constraint satisfaction [10], pursuit and evasion [3], and the game of Othello <ref> [6, 8, 10] </ref>. This paper will evaluate the usefulness of SANE in learning to play go. SANE is used to evolve networks to play on small boards against a simple computer opponent, and the scale-up properties are evaluated. <p> The problem is compounded in that the strength of the human opponent is not always known and cannot be reliably used to weight game results against the strength of the human opponent. Nevertheless, good results have been reported in neuro-evolution with noisy evaluation functions <ref> [8] </ref>, suggesting that the problems could be overcome. This way perhaps go-playing programs could finally be evolved that were able to compete with the best humans. 10 Conclusions Traditional artificial intelligence techniques have been insufficient for building go programs that would be competitive at high levels of play.
Reference: [9] <author> David E. Moriarty and Risto Miikkulainen. </author> <title> Evolving obstacle avoidance behavior in a robot arm. </title> <editor> In P. Maes, M. Mataric, J.-A. Meyer, and J. Pollack, editors, </editor> <booktitle> From Animals to Animats: The Fourth International Conference on Simulation of Adaptive Behavior (SAB96), </booktitle> <year> 1996. </year>
Reference-contexts: This paper explores the usefulness of neuro-evolution as a mechanism for learning to play go. The SANE (Symbiotic, Adaptive Neuro-Evolution <ref> [7, 8, 9] </ref>) algorithm demonstrates that networks that display a general ability in playing go on small boards can be evolved without fl To appear in Applied Intelligence. y This research was supported in part by NSF under grant #IRI-9504317. 1 (a) Four liberties. (b) One liberty. (c) No liberties. one. <p> In go, this problem is severe enough that standard learning techniques such as backpropagation cannot be effectively applied. 5 SANE SANE 1 (Symbiotic Adaptive Neuro-Evolution <ref> [7, 8, 9] </ref>) solves the credit assignment problem by using evolutionary algorithms to search for effective neural networks. Instead of punishing or rewarding individual moves, networks are evaluated, selected, and recombined based on their overall performance in the game. <p> First, it creates diversity in the blueprint population, and second, it explores new structures created by the neuron population. 6 Applying SANE to Go SANE has previously been shown effective in several sequential decision tasks including robot control <ref> [7, 8, 9] </ref>, constraint satisfaction [10], pursuit and evasion [3], and the game of Othello [6, 8, 10]. This paper will evaluate the usefulness of SANE in learning to play go.
Reference: [10] <author> David E. Moriarty and Risto Miikkulainen. </author> <title> Learning sequential decision tasks. </title> <editor> In M. J. Patel and V. Honavar, editors, </editor> <booktitle> Evolutionary Synthesis of Neural Systems. </booktitle> <publisher> MIT-Press, </publisher> <address> Cambridge, MA, </address> <publisher> in press. </publisher>
Reference-contexts: For this application, the output units are linear so that both positive and negative values and be generated. Neural networks could be formed by randomly choosing neurons from the neuron population. In fact, this approach performs well in simpler problems <ref> [7, 10] </ref>. However, random participation does not retain knowledge of the best combinations of neurons and can often 6 neural network is formed from a blueprint by following its neuron pointers and decoding the respective neurons. stall the search in more difficult problems [8]. <p> First, it creates diversity in the blueprint population, and second, it explores new structures created by the neuron population. 6 Applying SANE to Go SANE has previously been shown effective in several sequential decision tasks including robot control [7, 8, 9], constraint satisfaction <ref> [10] </ref>, pursuit and evasion [3], and the game of Othello [6, 8, 10]. This paper will evaluate the usefulness of SANE in learning to play go. SANE is used to evolve networks to play on small boards against a simple computer opponent, and the scale-up properties are evaluated. <p> diversity in the blueprint population, and second, it explores new structures created by the neuron population. 6 Applying SANE to Go SANE has previously been shown effective in several sequential decision tasks including robot control [7, 8, 9], constraint satisfaction [10], pursuit and evasion [3], and the game of Othello <ref> [6, 8, 10] </ref>. This paper will evaluate the usefulness of SANE in learning to play go. SANE is used to evolve networks to play on small boards against a simple computer opponent, and the scale-up properties are evaluated.
Reference: [11] <author> Barney Pell. </author> <title> Exploratory learning in the game of go. </title> <editor> In D.N.L. Levy and D.F. Beal, editors, </editor> <booktitle> Heuristic Programming in Artificial Intelligence 2 TheSecond Computer Olympiad. </booktitle> <publisher> Ellis Horwood, </publisher> <year> 1991. </year>
Reference-contexts: Such features could include common patterns and positions such as an eye or a group or even complicated constructs such as the ladder. These features would then be used as inputs to the neural network <ref> [1, 11] </ref>. It would still probably be useful to let the network develop its own features as well, but the pre-programmed features might allow it to learn faster and deal with more complex patterns. SANE demonstrates the feasibility of evolving structures on more than one level at the same time.
Reference: [12] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In David E. Rumelhart and James L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: A network could be trained to compute a mapping between the input space, that is, the current board position, and the output indicating the next move. The main problem with this approach is the credit assignment problem. Suppose a standard backpropagation neural network <ref> [12] </ref> were being trained to play go. For backpropagation to work, advance knowledge about the best move at any given position would be required. Such knowledge is difficult to come by. In reality, only the final game result is available.
Reference: [13] <author> Jonathan Schaeffer, Robert Lake, Paul Lu, and Martin Bryant. CHINOOK: </author> <title> The world man-machine checkers champion. </title> <journal> The AI Magazine, </journal> <volume> 16(1) </volume> <pages> 21-29, </pages> <year> 1996. </year> <month> 17 </month>
Reference-contexts: Whereas in games like chess, Othello and checkers the traditional game playing techniques such as minimax search and its variations are competitive even at the master level <ref> [4, 5, 13] </ref>, those techniques, when applied to go, have not been able to produce programs that can challenge even weak amateur players. The best computer programs in the world are ranked 6-8 stones below what would be considered master level.
References-found: 13

