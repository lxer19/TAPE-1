URL: http://www.cs.berkeley.edu/~padmanab/class_projects/cs252.ps
Refering-URL: http://www.cs.berkeley.edu/~padmanab/
Root-URL: 
Email: padmanab@cs.berkeley.edu drew@cs.berkeley.edu  
Title: The Real Cost of Context Switching  
Author: Venkata Padmanabhan Drew Roselli 
Date: November 30, 1994  
Abstract: Our goal was to quantify the various components of context switching. We used a combination of microbenchmarks, kernel timing, and tracing and simulation for this purpose. We found that the time to execute the kernel code to perform a context switch is 20-30 s on a SPARC 10 running Solaris 2.0. The cache effects due to multiprogramming is much greater: 80-160 s for an average context switch. The cache interference of the kernel code on the user code appears to be negligible. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agarwal, Anant, Hennessey, John, and Horowitz, Mark, </author> <title> "Cache Performance of Operating System and Multiprogramming Workloads," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 6, No. 4, </volume> <month> November </month> <year> 1988. </year>
Reference-contexts: Mogul and Borg ([5]) report that cache degradation due to multiprogramming is evident as far as 400,000 instructions after a context switch, causing 10-400 s of overhead for an average context switch on a DECStation 5000. The studies in <ref> [1] </ref> and [3] included system references in their program traces and concluded that system-user cache interference is small compared with user-user cache interference.
Reference: [2] <author> Anderson, Thomas E., Levy, Henry M., Ber-shad, Brian N., and Lazowska, Edward D. </author> <title> 11 "The Interaction of Architecture and Oper--ating System Design," </title> <booktitle> The Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference: [3] <author> Chen, J. Bradley, and Bershad, Brian N., </author> <title> "The Impact of Operating System Structure on Memory System Performance," </title> <booktitle> The Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: There have been several efforts to quantify the various costs of context switching. These range from timing the trap and kernel code execution ([2], [6]) to measuring the cache denigration due to the system code ([1], <ref> [3] </ref>), to analyzing the long-term cache effects due to cache interference from multiprogramming ([1], [5]) Ousterhout ([6]) developed a microbenchmark to measure context switch performance. His results show that context switch times are not proportionally faster for faster architectures as one might expect. <p> He was not able to fully account for the high cost of context switches in RISC architectures, but he suggested that fast processors become bound by memory speeds. Cache effects due to multiprogramming has been the subject of several studies ([1], <ref> [3] </ref>, [5], and [7]). Each of these studies ran program traces through cache simulators to estimate this effect. <p> Mogul and Borg ([5]) report that cache degradation due to multiprogramming is evident as far as 400,000 instructions after a context switch, causing 10-400 s of overhead for an average context switch on a DECStation 5000. The studies in [1] and <ref> [3] </ref> included system references in their program traces and concluded that system-user cache interference is small compared with user-user cache interference. <p> This time is noticeably affected by cache interference from user programs. However, our measurements indicate that the effect of the context switching code on user-level cache performance is negligible. This is in agreement with previous studies ([1] and <ref> [3] </ref>). The most important cost is the cache interference among user processes due to multiprogramming. For a typical context switch interval, this cost is 80-160 s. From our projected estimates, it seems that the relative importance of cache interference costs will rise with faster processors.
Reference: [4] <author> McVoy, Larry. </author> <title> Program written by Larry McVoy. </title>
Reference-contexts: Times are reported in s. 3.2 Microbenchmark Analysis We began by running two user-level microbench-marks which were designed to estimate context switch time. One of these uses signals between two processes to force context switches <ref> [4] </ref>. The other uses pipe synchronization to obtain the same effect. Since the second program gave a lower result (thus a tighter bound) on our system, we chose it for further examination. This program, called cswtch is described in [6].
Reference: [5] <author> Mogul, Jeffrey C., and Borg, Anita, </author> <title> "The Effect of Context Switches on Cache Performance," </title> <booktitle> The Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: These range from timing the trap and kernel code execution ([2], [6]) to measuring the cache denigration due to the system code ([1], [3]), to analyzing the long-term cache effects due to cache interference from multiprogramming ([1], <ref> [5] </ref>) Ousterhout ([6]) developed a microbenchmark to measure context switch performance. His results show that context switch times are not proportionally faster for faster architectures as one might expect. <p> He was not able to fully account for the high cost of context switches in RISC architectures, but he suggested that fast processors become bound by memory speeds. Cache effects due to multiprogramming has been the subject of several studies ([1], [3], <ref> [5] </ref>, and [7]). Each of these studies ran program traces through cache simulators to estimate this effect. Mogul and Borg ([5]) report that cache degradation due to multiprogramming is evident as far as 400,000 instructions after a context switch, causing 10-400 s of overhead for an average context switch on a
Reference: [6] <author> Ousterhout, John K., </author> <title> "Why Aren't Operating Systems Getting Faster as Fast as Hardware," </title> <booktitle> USENIX Summer Conference 1990, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: There have been several efforts to quantify the various costs of context switching. These range from timing the trap and kernel code execution ([2], <ref> [6] </ref>) to measuring the cache denigration due to the system code ([1], [3]), to analyzing the long-term cache effects due to cache interference from multiprogramming ([1], [5]) Ousterhout ([6]) developed a microbenchmark to measure context switch performance. <p> The other uses pipe synchronization to obtain the same effect. Since the second program gave a lower result (thus a tighter bound) on our system, we chose it for further examination. This program, called cswtch is described in <ref> [6] </ref>. The program forks off a child process and each process alternately sends and receives a byte from the other using two pipes (one for each direction).
Reference: [7] <author> Przybylski, Steven A., </author> <title> "Cache Design: A Performance-Directed Approach," </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, Calif., </address> <year> 1990. </year>
Reference-contexts: He was not able to fully account for the high cost of context switches in RISC architectures, but he suggested that fast processors become bound by memory speeds. Cache effects due to multiprogramming has been the subject of several studies ([1], [3], [5], and <ref> [7] </ref>). Each of these studies ran program traces through cache simulators to estimate this effect.
Reference: [8] <author> Saavedra-Barrera, Rafael Hector, </author> <title> "CPU performance evaluation and execution time prediction using narrow spectrum bench-marking," </title> <type> Ph.D. dissertation, </type> <institution> University of California, Berkeley, </institution> <month> May </month> <year> 1990. </year> <title> Adapted into a program written by Andrea Dusseau. </title> <type> 12 </type>
Reference-contexts: The second level cache (L2) is a unified cache, 1MB in size. It is direct-mapped, and has 128-byte blocks. The processor also has a 64-entry TLB that holds address translations for pages that are 4 KB in size. Using the cache microbenchmark developed by Saavedra-Barrera <ref> [8] </ref>, we estimated the miss penalties for the L1 cache, the L2 cache and the TLB to be 7 cycles, 40 cycles and 11 cycles, respectively. 5.2 Methodology Studying cache behavior requires a technique that is accurate and at the same time convenient to use.
References-found: 8

