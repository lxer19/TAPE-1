URL: http://siesta.cs.wustl.edu/~sck/ps/ijcnn-raam.ps
Refering-URL: http://siesta.cs.wustl.edu/~sck/
Root-URL: 
Title: Distributed Patterns as Hierarchical Structures  
Author: Stan C. Kwasny Barry L. Kalman Nancy Chang 
Address: Campus Box 1045 St. Louis, Missouri 63130  
Affiliation: Department of Computer Science Washington University,  
Note: In: 1993 World Congress on Neural Networks Portland, OR  
Abstract: Recursive Auto-Associative Memory (RAAM) structures show promise as a general representation vehicle that uses distributed patterns. However training is often difficult, which explains, at least in part, why only relatively small networks have been studied. We show a technique for transforming any collection of hierarchical structures into a set of training patterns for a sequential RAAM which can be effectively trained using a simple (Elman-style) recurrent network. Tr aining produces a set of distributed patterns corresponding to the structures. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Blank, D.S., Meeden, L.A., and Marshall, J.B. </author> <year> (1992). </year> <title> Exploring the Symbolic/Subsymbolic continuum: A Case Study of RAAM. </title> <editor> In J. Dinsmore (ed), </editor> <title> Closing the Gap: Symbolism vs. Connectionism. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <pages> pp. 113-148. </pages>
Reference-contexts: Training is designed to approximate an identity mapping from inputs to outputs and is hence auto-associative. Patterns that develop at the hidden layer This material is based upon work supported by the National Science Foundation under Grant No. IRI-9201987. Supported under National Science Foundation Grant No. CDA-9123643. II-198 <ref> (Based on similar figure in Blank, et al, 1992) </ref> n units n units branch 1 Output Layer Hidden Layer n units branch 2 . . . n units branch k n units branch 1 Input Layer n units branch 2 . . . n units branch k represent a compression of
Reference: <author> Chalmers, D.J. </author> <year> (1990). </year> <title> Transformations on Distributed Representations. </title> <editor> In Noel Sharkey (ed), </editor> <booktitle> Connectionist Natural Language Processing. </booktitle> <publisher> Intellect Publishers, Oxford, </publisher> <pages> pp. 46-55. </pages>
Reference: <author> Elman, Jeffrey L. </author> <year> (1990). </year> <title> Finding Structure in Time. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 179-212. </pages>
Reference: <author> Kalman, B.L. </author> <year> (1990). </year> <title> Super Linear Learning in Back Propagation Neural Nets. </title> <type> Technical Report WUCS-90-21, </type> <institution> St. Louis: Department of Computer Science, Washington University. </institution>
Reference: <author> Knuth, Donald E. </author> <year> (1973). </year> <title> The Art of Computer Programming. Volume 1: Fundamental Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading MA. </address>
Reference-contexts: The nodes of the trees must contain symbols from a finite alphabet and each symbol is assigned a representation. Each forest can be mapped to a binary tree, which in turn can be mapped to a simple sequence of symbols in which empty subtrees are marked <ref> (Knuth, 1973) </ref>. Such a mapping is invertible so that for every hierarchical structure there is an ordered sequence of symbols and vice versa.
Reference: <author> Kwasny, Stan C., and Kalman, Barry L. </author> <year> (1992). </year> <title> A Recurrent Deterministic Parser. </title> <booktitle> Proceedings of the Fourth Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <institution> Intergraph Corporation, Huntsville, Alabama, </institution> <month> 82-86. </month>
Reference: <author> Kwasny, Stan C., and Faisal, Kanaan A. </author> <year> (1992). </year> <title> Symbolic Parsing Via Subsymbolic Rules. </title> <editor> In J. Dinsmore (ed), </editor> <title> Closing the Gap: Symbolism vs. Connectionism. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <pages> pp. 209-235. </pages>
Reference: <author> Pollack, Jordan. </author> <year> (1989). </year> <title> Implications of Recursive Distributed Representations. </title> <editor> In David S. Touretzky (ed), </editor> <booktitle> Advances in Neural Information Processing Systems. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Gatos, CA. </address>
Reference: <author> Pollack, Jordan. </author> <year> (1990). </year> <title> Recursive Distributed Representations. </title> <journal> Artificial Intelligence. </journal> <volume> Vol. 46, </volume> <pages> pp. 77-105. </pages>
Reference-contexts: Connectionist models have considerable difficulty representing and manipulating such structures within fixed-length vectors with limited computational power. For rapid progress to be made in developing representation schemes for neural networks, combined symbolic and sub-symbolic approaches show the most promise. Among sub-symbolic representation schemes, perhaps the most general is Pollack's <ref> (Pollack, 1990) </ref> Recursive Auto-Associative Memory (RAAM), which develops distributed patterns over a fixed-size set of units to represent symbolic structures of fixed valence. The symbolic structures must be known in advance, of course, but the representation scheme develops through training. RAAMs have been the focus of many studies.
Reference: <author> Stolcke, Andreas, and Wu, Dekai. </author> <month> (April, </month> <year> 1992). </year> <title> Tree Matching with Recursive Distributed Representations. </title> <booktitle> TR-92-025, International Computer Science Institute, </booktitle> <address> Berkeley, CA. II-201 II-202 </address>
References-found: 10

