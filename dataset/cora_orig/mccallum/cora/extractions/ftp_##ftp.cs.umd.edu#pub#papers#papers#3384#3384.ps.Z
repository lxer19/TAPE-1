URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3384/3384.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: dbader@eng.umd.edu  joseph@umiacs.umd.edu  
Title: Parallel Algorithms for Image Histogramming and Connected Components with an Experimental Study  
Author: David A. Bader Joseph JaJa 
Date: December 9, 1994  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies, and Department of Electrical Engineering, University of Maryland,  
Abstract: This paper presents efficient and portable implementations of two useful primitives in image processing algorithms, histogramming and connected components. Our general framework is a single-address space, distributed memory programming model. We use efficient techniques for distributing and coalescing data as well as efficient combinations of task and data parallelism. Our connected components algorithm uses a novel approach for parallel merging which performs drastically limited updating during iterative steps, and concludes with a total consistency update at the final step. The algorithms have been coded in Split-C and run on a variety of platforms. Our experimental results are consistent with the theoretical analysis and provide the best known execution times for these two primitives, even when compared with machine specific implementations. More efficient implementations of Split-C will likely result in even faster execution times. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Alnuweiri and V. Prasanna. </author> <title> Parallel Architectures and Algorithms for Image Component Labeling. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 1014-1034, </pages> <year> 1992. </year>
Reference: [2] <author> J. Apostolakis, P. Coddington, and E. Marinari. </author> <title> New SIMD Algorithms for Cluster Labeling on Parallel Computers. </title> <type> Technical Report SCCS-279, </type> <institution> Physics Department, Syracuse University, Syracuse, </institution> <address> NY, </address> <month> September </month> <year> 1992. </year> <title> Int. </title> <journal> J. Mod. Phys. </journal> <volume> C 4, </volume> <month> 749 </month> <year> (1993). </year>
Reference-contexts: 10 512 3.98 s 152 s DARPA II Image TMC CM-2 32768 512 140 ms 547 s DARPA II Image 1990 Falsafi and Miller [14] Intel iPSC/2 32 512 1.197 s 146 s 1991 Parkinson [35] AMT DAP 510 1024 512 1.27 s 155 s 1991 Baillie and Coddington [4], <ref> [2] </ref> Ncube-1 32 512 53.4 s 6.52 ms Caltech Symult 2010 32 512 16.7 s 2.04 ms Meiko CS-1 32 512 14.8 s 1.81 ms 1991 Kistler and Webb [25] Warp 10 512 1.31 s 50.0 s 1992 Choudhary and Thakur [7] Intel iPSC/2 32 512 1.914 s 234 s multi-dim.
Reference: [3] <author> J. Apostolakis, P. Coddington, and E. Marinari. </author> <title> New SIMD Algorithms for Cluster Labeling on Parallel Computers. </title> <journal> Int. J. Mod. Phys. C, </journal> <volume> 4:749, </volume> <year> 1993. </year>
Reference-contexts: CCR-9103135 and NSF HPCC/GCAG grant No. BIR-9318183. 1 to several computational physics problems such as percolation ([41], [5]) and various cluster Monte Carlo algorithms for computing the spin models of magnets such as the two-dimensional Ising spin model ([2], <ref> [3] </ref>, [4], [39], [40]). All pixels with grey level (or `color') 0 are assumed to be background, while pixels with color &gt; 0 are foreground objects.
Reference: [4] <author> C.F. Baillie and P.D. Coddington. </author> <title> Cluster Identification Algorithms for Spin Models - Sequential and Parallel. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(2) </volume> <pages> 129-144, </pages> <year> 1991. </year>
Reference-contexts: CCR-9103135 and NSF HPCC/GCAG grant No. BIR-9318183. 1 to several computational physics problems such as percolation ([41], [5]) and various cluster Monte Carlo algorithms for computing the spin models of magnets such as the two-dimensional Ising spin model ([2], [3], <ref> [4] </ref>, [39], [40]). All pixels with grey level (or `color') 0 are assumed to be background, while pixels with color &gt; 0 are foreground objects. A connected component in the image is a maximal collection of pixels such that a path exists between any pair of pixels in the component. <p> Warp 10 512 3.98 s 152 s DARPA II Image TMC CM-2 32768 512 140 ms 547 s DARPA II Image 1990 Falsafi and Miller [14] Intel iPSC/2 32 512 1.197 s 146 s 1991 Parkinson [35] AMT DAP 510 1024 512 1.27 s 155 s 1991 Baillie and Coddington <ref> [4] </ref>, [2] Ncube-1 32 512 53.4 s 6.52 ms Caltech Symult 2010 32 512 16.7 s 2.04 ms Meiko CS-1 32 512 14.8 s 1.81 ms 1991 Kistler and Webb [25] Warp 10 512 1.31 s 50.0 s 1992 Choudhary and Thakur [7] Intel iPSC/2 32 512 1.914 s 234 s
Reference: [5] <author> R.C. Brower, P. Tamayo, and B. York. </author> <title> A Parallel Multigrid Algorithm for Percolation Clusters. </title> <journal> Journal of Statistical Physics, </journal> <volume> 63:73, </volume> <year> 1991. </year>
Reference-contexts: NGT-50951 is gratefully acknowledged. y Supported in part by NSF grant No. CCR-9103135 and NSF HPCC/GCAG grant No. BIR-9318183. 1 to several computational physics problems such as percolation ([41], <ref> [5] </ref>) and various cluster Monte Carlo algorithms for computing the spin models of magnets such as the two-dimensional Ising spin model ([2], [3], [4], [39], [40]). All pixels with grey level (or `color') 0 are assumed to be background, while pixels with color &gt; 0 are foreground objects.
Reference: [6] <author> F.Y. Chin, J. Lam, and I-N. Chen. </author> <title> Efficient Parallel Algorithms for Some Graph Problems. </title> <journal> Communications of the ACM, </journal> <volume> 25(9) </volume> <pages> 659-665, </pages> <year> 1982. </year>
Reference-contexts: Without loss of generality, k is assumed to be a power of two. The second algorithm performs the connected components of images ([1], <ref> [6] </ref>, [8], [9], [12], [16], [17], [20], [38]). The task of connected component labeling is cited as an important object recognition problem in the DARPA Image Understanding benchmarks ([37], [47]), and also can be applied fl The support by NASA Graduate Student Researcher Fellowship No.
Reference: [7] <author> A. Choudhary and R. Thakur. </author> <title> Evaluation of Connected Component Labeling Algorithms on Shared and Distributed Memory Multiprocessors. </title> <booktitle> In Proceedings of the 6th International Parallel Processing Symposium, </booktitle> <pages> pages 362-365, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: 1.27 s 155 s 1991 Baillie and Coddington [4], [2] Ncube-1 32 512 53.4 s 6.52 ms Caltech Symult 2010 32 512 16.7 s 2.04 ms Meiko CS-1 32 512 14.8 s 1.81 ms 1991 Kistler and Webb [25] Warp 10 512 1.31 s 50.0 s 1992 Choudhary and Thakur <ref> [7] </ref> Intel iPSC/2 32 512 1.914 s 234 s multi-dim. D+C (partitioned input) Intel iPSC/2 32 512 1.649 s 201 s multi-dim. D+C (complete im./PE) Intel iPSC/2 32 512 2.290 s 280 s multi-dim.
Reference: [8] <author> A. Choudhary and R. Thakur. </author> <title> Connected Component Labeling on Coarse Grain Parallel Computers: An Experimental Study. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 20(1) </volume> <pages> 78-83, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Without loss of generality, k is assumed to be a power of two. The second algorithm performs the connected components of images ([1], [6], <ref> [8] </ref>, [9], [12], [16], [17], [20], [38]). The task of connected component labeling is cited as an important object recognition problem in the DARPA Image Understanding benchmarks ([37], [47]), and also can be applied fl The support by NASA Graduate Student Researcher Fellowship No. <p> D+C (cmplt. + collect. comm.) Encore Multimax 16 512 521 ms 31.8 s multi-dim. D+C (partitioned input) 1993 Embrechts, Roose, and Wambacq [13] Intel iPSC/2 16 256 360 ms 87.9 s divide & conquer 1994 Choudhary and Thakur <ref> [8] </ref> TMC CM-5 32 512 456 ms 55.7 s multi-dim. D+C (partitioned input) (DARPA II Image) TMC CM-5 32 512 398 ms 48.6 s multi-dim. D+C (complete im./PE) TMC CM-5 32 512 452 ms 55.2 s multi-dim.
Reference: [9] <author> N. Copty, S. Ranka, G. Fox, and R.V. Shankar. </author> <title> A Data Parallel Algorithm for Solving the Region Growing Problem on the Connection Machine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 160-168, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Without loss of generality, k is assumed to be a power of two. The second algorithm performs the connected components of images ([1], [6], [8], <ref> [9] </ref>, [12], [16], [17], [20], [38]). The task of connected component labeling is cited as an important object recognition problem in the DARPA Image Understanding benchmarks ([37], [47]), and also can be applied fl The support by NASA Graduate Student Researcher Fellowship No.
Reference: [10] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krishnamurthy, S. Lumetta, S. Luna, T. von Eicken, and K. Yelick. </author> <title> Introduction to Split-C. </title> <institution> Computer Science Division - EECS, University of California, Berkeley, </institution> <note> version 1.0 edition, </note> <month> March 6 </month> <year> 1994. </year>
Reference-contexts: The shared memory model algorithms are written in Split-C <ref> [10] </ref>, a shared memory programming model language which follows the SPMD (single program multiple data) model on these parallel machines, and the source code is available for distribution to interested parties. 2 Year Researcher (s) Machine PE's p n 2 Time work/pix Notes 1986 Little [29] TMC Connection Machine 65536 512
Reference: [11] <author> D.E. Culler, R.M. Karp, D.A. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subra-monian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The Split-C language also supplies a barrier () function for the global synchronization of the processors. 2.1 Analysis For Matrix Transpose Algorithm The analysis for the matrix transpose algorithm is similar to that of the LogP model analysis <ref> [11] </ref>. The algorithm to perform a q fi p matrix transpose on a p processor machine operates as follows. The data layout of matrix A is straightforward; each column i of q elements is stored on processor i, for i 2 f0; : : : ; p 1g. <p> This is consistent with the results achieved by other research teams that have achieved 6.4 MB/s per processor (Culler at UC Berkley, <ref> [11] </ref>), and 7.72 MB/s per processor (Ranka at Syracuse University, [46]) for similar data movements on the CM-5. Note that some of these cited results are for low-level implementations using message passing algorithms.
Reference: [12] <author> M.B. Dillencourt, H. Samet, and M. Tamminen. </author> <title> Connected Component Labeling of Binary Images. </title> <type> Technical Report CS-TR-2303, </type> <institution> Computer Science Department, University of Maryland, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: Without loss of generality, k is assumed to be a power of two. The second algorithm performs the connected components of images ([1], [6], [8], [9], <ref> [12] </ref>, [16], [17], [20], [38]). The task of connected component labeling is cited as an important object recognition problem in the DARPA Image Understanding benchmarks ([37], [47]), and also can be applied fl The support by NASA Graduate Student Researcher Fellowship No.
Reference: [13] <author> H. Embrechts, D. Roose, and P. Wambacq. </author> <title> Component Labelling on a MIMD Multiprocessor. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 57(2) </volume> <pages> 155-165, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: D+C (partitioned input) Intel iPSC/860 32 512 1.031 s 126 s multi-dim. D+C (complete im./PE) Intel iPSC/860 32 512 947 ms 116 s multi-dim. D+C (cmplt. + collect. comm.) Encore Multimax 16 512 521 ms 31.8 s multi-dim. D+C (partitioned input) 1993 Embrechts, Roose, and Wambacq <ref> [13] </ref> Intel iPSC/2 16 256 360 ms 87.9 s divide & conquer 1994 Choudhary and Thakur [8] TMC CM-5 32 512 456 ms 55.7 s multi-dim. D+C (partitioned input) (DARPA II Image) TMC CM-5 32 512 398 ms 48.6 s multi-dim.
Reference: [14] <author> B. Falsafi and R. Miller. </author> <title> Component Labeling Algorithms on an Intel iPSC/2 Hypercube. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pages 159-164, </pages> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: FX-80 8 512 7.225 s 220 s DARPA II Image and Rosenfeld [48] Sequent Symmetry 81 8 512 15.12 s 461 s DARPA II Image Warp 10 512 3.98 s 152 s DARPA II Image TMC CM-2 32768 512 140 ms 547 s DARPA II Image 1990 Falsafi and Miller <ref> [14] </ref> Intel iPSC/2 32 512 1.197 s 146 s 1991 Parkinson [35] AMT DAP 510 1024 512 1.27 s 155 s 1991 Baillie and Coddington [4], [2] Ncube-1 32 512 53.4 s 6.52 ms Caltech Symult 2010 32 512 16.7 s 2.04 ms Meiko CS-1 32 512 14.8 s 1.81 ms
Reference: [15] <author> J. Grinberg, G.R. Nudd, and R.D. Etchells. </author> <title> A Cellular VLSI Architecture. </title> <journal> IEEE Computer, </journal> <volume> 17(1) </volume> <pages> 69-81, </pages> <year> 1984. </year>
Reference-contexts: Year Researcher (s) Machine Processors Image Size Time work per pixel 1980 Marks [32] AMT DAP 1024 32 fi 32 17.25 ms 539 s 1983 Potter [36] Goodyear MPP 16384 128 fi 128 16.4 ms 513 s 1984 Grinberg, Nudd, and Etchells <ref> [15] </ref> 3-D machine 16384 256 fi 256 1.7 ms 13.3 s 1987 Ibrahim, Kender, and Shaw [19] NON-VON 3 16384 128 fi 128 2.16 ms 67.5 s 1990 Nudd, et al. [34] Warwick Pyramid 16K base 256 fi 256 237 s 2.47 s 1991 Jesshope [23] AMT DAP 510 1024 512
Reference: [16] <author> Y. Han and R.A. Wagner. </author> <title> An Efficient and Fast Parallel-Connected Component Algorithm. </title> <journal> JACM, </journal> <volume> 37(3) </volume> <pages> 626-642, </pages> <year> 1990. </year>
Reference-contexts: Without loss of generality, k is assumed to be a power of two. The second algorithm performs the connected components of images ([1], [6], [8], [9], [12], <ref> [16] </ref>, [17], [20], [38]). The task of connected component labeling is cited as an important object recognition problem in the DARPA Image Understanding benchmarks ([37], [47]), and also can be applied fl The support by NASA Graduate Student Researcher Fellowship No.
Reference: [17] <author> D.S. Hirschberg, A.K. Chandra, and D.V. Sarwate. </author> <title> Computing Connected Components on Parallel Computers. </title> <journal> Communications of the ACM, </journal> <volume> 22(8) </volume> <pages> 461-464, </pages> <year> 1979. </year>
Reference-contexts: Without loss of generality, k is assumed to be a power of two. The second algorithm performs the connected components of images ([1], [6], [8], [9], [12], [16], <ref> [17] </ref>, [20], [38]). The task of connected component labeling is cited as an important object recognition problem in the DARPA Image Understanding benchmarks ([37], [47]), and also can be applied fl The support by NASA Graduate Student Researcher Fellowship No.
Reference: [18] <author> R. Hummel. </author> <title> Connected Component Labelling in Image Processing with MIMD Architectures. </title> <editor> In M.J.B. Duff, editor, </editor> <booktitle> Intermediate-Level Image Processing, chapter 7, </booktitle> <pages> pages 101-127. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1986. </year> <month> 31 </month>
Reference-contexts: DARPA I 1986 Hummel <ref> [18] </ref> NYU Ultracomputer 4096 512 Shiloach/Vishkin alg. 1987 Wallace and Howard [44], [45] HBA 12 512 725 ms 33.2 s 2-pass swath 1987 Ibrahim, Kender, and Shaw [19] NON-VON 3 16384 128 12.5 ms 391 s 4-conn. 1987 Sunwoo, Baroody, and Aggarwal [43] Intel iPSC 32 256 5.074 s 2.48 ms
Reference: [19] <author> H.A. Ibrahim, J.R. Kender, and D.E. Shaw. </author> <title> Low-Level Image Analysis Tasks on Fine--Grained Tree-Structured SIMD Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 4 </volume> <pages> 546-574, </pages> <year> 1987. </year>
Reference-contexts: work per pixel 1980 Marks [32] AMT DAP 1024 32 fi 32 17.25 ms 539 s 1983 Potter [36] Goodyear MPP 16384 128 fi 128 16.4 ms 513 s 1984 Grinberg, Nudd, and Etchells [15] 3-D machine 16384 256 fi 256 1.7 ms 13.3 s 1987 Ibrahim, Kender, and Shaw <ref> [19] </ref> NON-VON 3 16384 128 fi 128 2.16 ms 67.5 s 1990 Nudd, et al. [34] Warwick Pyramid 16K base 256 fi 256 237 s 2.47 s 1991 Jesshope [23] AMT DAP 510 1024 512 fi 512 86 ms 10.5 s 1994 Bader and JaJa TMC CM-5 16 512 fi 512 <p> DARPA I 1986 Hummel [18] NYU Ultracomputer 4096 512 Shiloach/Vishkin alg. 1987 Wallace and Howard [44], [45] HBA 12 512 725 ms 33.2 s 2-pass swath 1987 Ibrahim, Kender, and Shaw <ref> [19] </ref> NON-VON 3 16384 128 12.5 ms 391 s 4-conn. 1987 Sunwoo, Baroody, and Aggarwal [43] Intel iPSC 32 256 5.074 s 2.48 ms 8-connectivity 1987 Rosenfeld [37] BBN Butterfly 128 512 7.2 s 3.52 ms DARPA I TMC CM-1 65536 512 400 ms 3.13 ms DARPA I Columbia NON-VON 1M
Reference: [20] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Without loss of generality, k is assumed to be a power of two. The second algorithm performs the connected components of images ([1], [6], [8], [9], [12], [16], [17], <ref> [20] </ref>, [38]). The task of connected component labeling is cited as an important object recognition problem in the DARPA Image Understanding benchmarks ([37], [47]), and also can be applied fl The support by NASA Graduate Student Researcher Fellowship No.
Reference: [21] <author> J. JaJa and K.W. Ryu. </author> <title> The Block Distributed Memory Model. </title> <type> Technical Report CS-TR-3207, </type> <institution> Computer Science Department, University of Maryland, College Park, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: We compute the matrix transpose of A, thus, giving every processor q p elements. Each processor then locally rearranges the data so that an additional matrix transpose will result in each processor holding a copy of all the q elements in its column of A <ref> [21] </ref>. The following algorithm runs on processor i: Algorithm 2 Broadcast Algorithm Block Distributed Memory Model Algorithm to broadcast q elements from a single processor to all other processors.
Reference: [22] <author> J.F. JaJa and K.W. Ryu. </author> <title> The Block Distributed Memory Model for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <pages> pages 752-756, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year> <note> (Extended Abstract). </note>
Reference-contexts: test images Meiko CS-2 2 512 809 ms 6.17 s DARPA II Image Meiko CS-2 32 512 301 ms 36.7 s DARPA II Image Table 2: Implementation Results of Parallel Connected Components of Images Algorithms 3 2 Block Distributed Memory Model We use the Block Distributed Memory (BDM) Model ([21], <ref> [22] </ref>) as a computation model for developing and analyzing our parallel algorithms on distributed memory machines. This model allows the design of algorithms using a single address space and does not assume any particular interconnection topology.
Reference: [23] <author> C.R. Jesshope. </author> <title> Parallel Computers Architecutures and Programming. </title> <editor> In R.A. Vaughan, editor, </editor> <booktitle> Pattern Recognition and Image Processing in Physics, </booktitle> <pages> pages 205-234. </pages> <booktitle> Scottish Universities Summer School in Physics, </booktitle> <address> New York, </address> <year> 1990. </year>
Reference-contexts: 1984 Grinberg, Nudd, and Etchells [15] 3-D machine 16384 256 fi 256 1.7 ms 13.3 s 1987 Ibrahim, Kender, and Shaw [19] NON-VON 3 16384 128 fi 128 2.16 ms 67.5 s 1990 Nudd, et al. [34] Warwick Pyramid 16K base 256 fi 256 237 s 2.47 s 1991 Jesshope <ref> [23] </ref> AMT DAP 510 1024 512 fi 512 86 ms 10.5 s 1994 Bader and JaJa TMC CM-5 16 512 fi 512 12.0 ms 732 ns (This paper) IBM SP-1 16 512 fi 512 9.20 ms 562 ns IBM SP-2 16 512 fi 512 20.0 ms 1.22 s Intel Paragon 8
Reference: [24] <author> T. Kanade and J.A. Webb. </author> <title> Parallel Vision Algorithm Design and Implementation 1988 End of Year Report. </title> <type> Technical Report CMU-RI-TR-89-23, </type> <institution> The Robotics Institute, Carnegie-Mellon University, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: DARPA I iWarp 72 512 470 ms 129 s DARPA I Columbia NON-VON 1M 512 1 s 125 ms DARPA I (est.?) TMC CM-1 65536 512 400 ms 3.13 ms DARPA I 1989 Manohar and Ramapriyan [31] Goodyear MPP 16384 128 97.3 ms 3.04 ms shrink/expand 1989 Kanade and Webb <ref> [24] </ref> Warp 10 512 4.34 s 166 s shrnk/expnd, DARPA II Im. 1989 Weems, Riseman, Hanson, Alliant FX-80 8 512 7.225 s 220 s DARPA II Image and Rosenfeld [48] Sequent Symmetry 81 8 512 15.12 s 461 s DARPA II Image Warp 10 512 3.98 s 152 s DARPA II
Reference: [25] <author> J.J. Kistler and J.A. Webb. </author> <title> Connected Components With Split and Merge. </title> <booktitle> In Proceedings of the 5th International Parallel Processing Symposium, </booktitle> <pages> pages 194-201, </pages> <address> Anaheim, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: 1.197 s 146 s 1991 Parkinson [35] AMT DAP 510 1024 512 1.27 s 155 s 1991 Baillie and Coddington [4], [2] Ncube-1 32 512 53.4 s 6.52 ms Caltech Symult 2010 32 512 16.7 s 2.04 ms Meiko CS-1 32 512 14.8 s 1.81 ms 1991 Kistler and Webb <ref> [25] </ref> Warp 10 512 1.31 s 50.0 s 1992 Choudhary and Thakur [7] Intel iPSC/2 32 512 1.914 s 234 s multi-dim. D+C (partitioned input) Intel iPSC/2 32 512 1.649 s 201 s multi-dim. D+C (complete im./PE) Intel iPSC/2 32 512 2.290 s 280 s multi-dim.
Reference: [26] <author> A. Krikelis and R.M. Lea. </author> <title> Performance of the ASP on the DARPA Architecture Benchmark. </title> <booktitle> In Proceedings of the 2nd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 483-486, </pages> <address> Fairfax, VA, </address> <month> October </month> <year> 1988. </year>
Reference-contexts: 14 ms 13.7 s DARPA I Mosaic 16384 512 6 ms 11.7 s DARPA I (est.?) Encore Multimax 20 512 22.7 s 1.73 ms DARPA I HBA 16 512 170 ms 10.4 s DARPA I (est.?) HBA 100+ 512 370 ms 181 s DARPA I (est.?) 1988 Krikelis and Lea <ref> [26] </ref> WW Warp 10 512 5.6 s 214 s DARPA I PC Warp 10 512 980 ms 37.4 s DARPA I iWarp 72 512 470 ms 129 s DARPA I Columbia NON-VON 1M 512 1 s 125 ms DARPA I (est.?) TMC CM-1 65536 512 400 ms 3.13 ms DARPA I
Reference: [27] <author> J.M. Kuzela. </author> <title> IBM POWERparallel System - SP2 Performance Measurements. Power Parallel Systems, </title> <institution> IBM, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: For large enough data sets, the SP-2 achieves greater than 24.8 MB/s per processor for the matrix transpose algorithm, using a high performance switch hardware rated by the vendor as having a peak node to node bandwidth of 40 MB/s <ref> [27] </ref>. The Meiko CS-2 achieves greater than 10.7 MB/s per processor. Note that the CS-2 result is much less than the maximum attainable bandwidth of 50 MB/s per processor [33] because our Split-C version has not been fully optimized to make use of the architecture's communications coprocessor.
Reference: [28] <author> C.E. Leiserson, Z.S. Abuhamdeh, </author> <title> D.C. </title> <type> Douglas, C.R. Feynman, </type> <institution> M.N. Ganmukhi, J.V. Hill, W.D. Hillis, B.C. Kuszmaul, M.A. St. Pierre, </institution> <note> D.S. </note> <author> Wells, M.C. Wong, S.-W. Yang, and R. Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. (Extended Abstract), </title> <month> July 28, </month> <year> 1992. </year>
Reference-contexts: For large enough data sets on the CM-5, we achieve an average bandwidth of 7.62 MB/s per processor, which is more than three-fourths of the maximum user-payload bandwidth per processor of 12 MB/s per processor <ref> [28] </ref>. This is consistent with the results achieved by other research teams that have achieved 6.4 MB/s per processor (Culler at UC Berkley, [11]), and 7.72 MB/s per processor (Ranka at Syracuse University, [46]) for similar data movements on the CM-5.
Reference: [29] <author> J.J. Little. </author> <title> Parallel Algorithms for Computer Vision on the Connection Machine. </title> <type> Technical Report AIM-928, </type> <institution> MIT AI Laboratory, </institution> <month> November </month> <year> 1986. </year>
Reference-contexts: algorithms are written in Split-C [10], a shared memory programming model language which follows the SPMD (single program multiple data) model on these parallel machines, and the source code is available for distribution to interested parties. 2 Year Researcher (s) Machine PE's p n 2 Time work/pix Notes 1986 Little <ref> [29] </ref> TMC Connection Machine 65536 512 450 ms 3.53 ms Scanning alg.
Reference: [30] <author> L.T. Liu. </author> <type> Personal communications. </type> <month> November </month> <year> 1994. </year>
Reference-contexts: The 8 processor Paragon achieves greater than 88.6 MB/s per processor, with the maximum hardware bandwidth given by Intel as 175 MB/s per processor and application peak bandwidth as 135 MB/s per processor <ref> [30] </ref>. 2.3 Analysis for the Broadcasting Algorithm An efficient algorithm to broadcast q elements from a single processor to p processors is based on matrix transposition, where q is assumed to be larger than p.
Reference: [31] <author> M. Manohar and H.K. Ramapriyan. </author> <title> Connected Component Labeling of Binary Images on a Mesh Connected Massively Parallel Processor. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 45(2) </volume> <pages> 133-149, </pages> <year> 1989. </year>
Reference-contexts: 5.6 s 214 s DARPA I PC Warp 10 512 980 ms 37.4 s DARPA I iWarp 72 512 470 ms 129 s DARPA I Columbia NON-VON 1M 512 1 s 125 ms DARPA I (est.?) TMC CM-1 65536 512 400 ms 3.13 ms DARPA I 1989 Manohar and Ramapriyan <ref> [31] </ref> Goodyear MPP 16384 128 97.3 ms 3.04 ms shrink/expand 1989 Kanade and Webb [24] Warp 10 512 4.34 s 166 s shrnk/expnd, DARPA II Im. 1989 Weems, Riseman, Hanson, Alliant FX-80 8 512 7.225 s 220 s DARPA II Image and Rosenfeld [48] Sequent Symmetry 81 8 512 15.12 s
Reference: [32] <author> P. Marks. </author> <title> Low-Level Vision Using an Array Processor. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 14 </volume> <pages> 281-292, </pages> <year> 1980. </year>
Reference-contexts: In order to normalize the results between fine- and coarse-grained machines, we divide the number of processors in the fine-grained machines by 32 to compute the work per pixel site. Year Researcher (s) Machine Processors Image Size Time work per pixel 1980 Marks <ref> [32] </ref> AMT DAP 1024 32 fi 32 17.25 ms 539 s 1983 Potter [36] Goodyear MPP 16384 128 fi 128 16.4 ms 513 s 1984 Grinberg, Nudd, and Etchells [15] 3-D machine 16384 256 fi 256 1.7 ms 13.3 s 1987 Ibrahim, Kender, and Shaw [19] NON-VON 3 16384 128 fi
Reference: [33] <author> Meiko. </author> <title> Computing Surface Communications Network Overview. </title> <publisher> Meiko World Inc., </publisher> <address> Concord, MA, manual 84-cb041 edition, </address> <year> 1993. </year>
Reference-contexts: The Meiko CS-2 achieves greater than 10.7 MB/s per processor. Note that the CS-2 result is much less than the maximum attainable bandwidth of 50 MB/s per processor <ref> [33] </ref> because our Split-C version has not been fully optimized to make use of the architecture's communications coprocessor.
Reference: [34] <author> G.R. Nudd, T.J. Atherton, N.D. Francis, R.M. Howarth, D.J. Kerbyson, R.A. Packwood, and G.J. Vaudin. </author> <title> A Hierarchical Multiple-SIMD Architecture for Image Analysis. </title> <booktitle> In Proceedings of the 10th International Conference on Pattern Recognition, </booktitle> <pages> pages 642-647, </pages> <address> Atlantic City, NJ, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: s 1983 Potter [36] Goodyear MPP 16384 128 fi 128 16.4 ms 513 s 1984 Grinberg, Nudd, and Etchells [15] 3-D machine 16384 256 fi 256 1.7 ms 13.3 s 1987 Ibrahim, Kender, and Shaw [19] NON-VON 3 16384 128 fi 128 2.16 ms 67.5 s 1990 Nudd, et al. <ref> [34] </ref> Warwick Pyramid 16K base 256 fi 256 237 s 2.47 s 1991 Jesshope [23] AMT DAP 510 1024 512 fi 512 86 ms 10.5 s 1994 Bader and JaJa TMC CM-5 16 512 fi 512 12.0 ms 732 ns (This paper) IBM SP-1 16 512 fi 512 9.20 ms 562
Reference: [35] <author> D. Parkinson. </author> <title> Experiments in Component Labeling in a Parallel Computer. </title> <editor> In V.K. Prasanna Kumar, editor, </editor> <booktitle> Parallel Architectures and Algorithms for Image Understanding, </booktitle> <pages> pages 209-225. </pages> <publisher> Academic Press, Inc., </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Rosenfeld [48] Sequent Symmetry 81 8 512 15.12 s 461 s DARPA II Image Warp 10 512 3.98 s 152 s DARPA II Image TMC CM-2 32768 512 140 ms 547 s DARPA II Image 1990 Falsafi and Miller [14] Intel iPSC/2 32 512 1.197 s 146 s 1991 Parkinson <ref> [35] </ref> AMT DAP 510 1024 512 1.27 s 155 s 1991 Baillie and Coddington [4], [2] Ncube-1 32 512 53.4 s 6.52 ms Caltech Symult 2010 32 512 16.7 s 2.04 ms Meiko CS-1 32 512 14.8 s 1.81 ms 1991 Kistler and Webb [25] Warp 10 512 1.31 s 50.0
Reference: [36] <author> J.L. Potter. </author> <title> Image Processing on the Massively Parallel Processor. </title> <journal> IEEE Computer, </journal> <volume> 16(1) </volume> <pages> 62-67, </pages> <year> 1983. </year>
Reference-contexts: Year Researcher (s) Machine Processors Image Size Time work per pixel 1980 Marks [32] AMT DAP 1024 32 fi 32 17.25 ms 539 s 1983 Potter <ref> [36] </ref> Goodyear MPP 16384 128 fi 128 16.4 ms 513 s 1984 Grinberg, Nudd, and Etchells [15] 3-D machine 16384 256 fi 256 1.7 ms 13.3 s 1987 Ibrahim, Kender, and Shaw [19] NON-VON 3 16384 128 fi 128 2.16 ms 67.5 s 1990 Nudd, et al. [34] Warwick Pyramid 16K
Reference: [37] <editor> A. Rosenfeld. </editor> <booktitle> A Report on the DARPA Image Understanding Architectures Workshop. In Proceedings of the 1987 Image Understanding Workshop, </booktitle> <pages> pages 298-302, </pages> <year> 1987. </year>
Reference-contexts: 512 Shiloach/Vishkin alg. 1987 Wallace and Howard [44], [45] HBA 12 512 725 ms 33.2 s 2-pass swath 1987 Ibrahim, Kender, and Shaw [19] NON-VON 3 16384 128 12.5 ms 391 s 4-conn. 1987 Sunwoo, Baroody, and Aggarwal [43] Intel iPSC 32 256 5.074 s 2.48 ms 8-connectivity 1987 Rosenfeld <ref> [37] </ref> BBN Butterfly 128 512 7.2 s 3.52 ms DARPA I TMC CM-1 65536 512 400 ms 3.13 ms DARPA I Columbia NON-VON 1M 512 1 s 125 ms DARPA I (est.?) Caltech CUBE 256 512 14 ms 13.7 s DARPA I Mosaic 16384 512 6 ms 11.7 s DARPA I
Reference: [38] <author> H. Samet and M. Tamminen. </author> <title> A General Approach To Connected Component Labeling of Images. </title> <type> Technical Report CS-TR-1649, </type> <institution> Computer Science Department, University of Maryland, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: Without loss of generality, k is assumed to be a power of two. The second algorithm performs the connected components of images ([1], [6], [8], [9], [12], [16], [17], [20], <ref> [38] </ref>). The task of connected component labeling is cited as an important object recognition problem in the DARPA Image Understanding benchmarks ([37], [47]), and also can be applied fl The support by NASA Graduate Student Researcher Fellowship No. NGT-50951 is gratefully acknowledged. y Supported in part by NSF grant No.
Reference: [39] <author> A.D. Sokal. </author> <title> New Numerical Algorithms for Critical Phenomena (Multi-grid Methods and All That). In D.P. Landau, </title> <editor> K.K. Mon, and H.-B. Schuttler, editors, </editor> <title> Computer Simulation Studies in Condensed Matter Physics: Recent Developments. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: CCR-9103135 and NSF HPCC/GCAG grant No. BIR-9318183. 1 to several computational physics problems such as percolation ([41], [5]) and various cluster Monte Carlo algorithms for computing the spin models of magnets such as the two-dimensional Ising spin model ([2], [3], [4], <ref> [39] </ref>, [40]). All pixels with grey level (or `color') 0 are assumed to be background, while pixels with color &gt; 0 are foreground objects. A connected component in the image is a maximal collection of pixels such that a path exists between any pair of pixels in the component.
Reference: [40] <author> A.D. Sokal. </author> <title> New Numerical Algorithms for Critical Phenomena (Multi-grid Methods and All That). </title> <booktitle> In Proceedings of the International Conference on Lattice Field Theory, </booktitle> <address> Talla-hassee, Fl, </address> <month> October </month> <year> 1990. </year> <title> (Nucl. </title> <journal> Phys. B (Proc. Suppl.) 20:55, 1991.). </journal>
Reference-contexts: CCR-9103135 and NSF HPCC/GCAG grant No. BIR-9318183. 1 to several computational physics problems such as percolation ([41], [5]) and various cluster Monte Carlo algorithms for computing the spin models of magnets such as the two-dimensional Ising spin model ([2], [3], [4], [39], <ref> [40] </ref>). All pixels with grey level (or `color') 0 are assumed to be background, while pixels with color &gt; 0 are foreground objects. A connected component in the image is a maximal collection of pixels such that a path exists between any pair of pixels in the component.
Reference: [41] <author> D. Stauffer. </author> <title> Introduction to Percolation Theory. Taylor and Francis, </title> <address> Philadelphia, PA, </address> <year> 1985. </year>
Reference: [42] <author> Q.F. Stout. </author> <title> Supporting Divide-and-Conquer Algorithms for Image Processing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 4 </volume> <pages> 95-115, </pages> <year> 1987. </year>
Reference-contexts: A catalog of nine automatically generated scalable 6 images is used, as shown in Figure 1, and include horizontal, vertical, and forward- and back--slanting diagonal bars, a cross, a filled disc, concentric circles with thickness, four squares inset from the four corners, and a dual-spiral pattern, a "difficult" image <ref> [42] </ref>. 3.1 Connected Components Test Images Image 1 Image 2 Image 3 Image 4 Image 5 Image 6 Image 7 Image 8 Image 9 7 8 4 Histogramming Histogramming is a useful image processing primitive.
Reference: [43] <author> M.H. Sunwoo, B.S. Baroody, and J.K. Aggarwal. </author> <title> A Parallel Algorithm for Region Labeling. </title> <booktitle> In Proceedings of the 1987 Workshop on Computer Architecture for Pattern Analysis and Machine Intelligence, </booktitle> <pages> pages 27-34, </pages> <address> Seattle, WA, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: DARPA I 1986 Hummel [18] NYU Ultracomputer 4096 512 Shiloach/Vishkin alg. 1987 Wallace and Howard [44], [45] HBA 12 512 725 ms 33.2 s 2-pass swath 1987 Ibrahim, Kender, and Shaw [19] NON-VON 3 16384 128 12.5 ms 391 s 4-conn. 1987 Sunwoo, Baroody, and Aggarwal <ref> [43] </ref> Intel iPSC 32 256 5.074 s 2.48 ms 8-connectivity 1987 Rosenfeld [37] BBN Butterfly 128 512 7.2 s 3.52 ms DARPA I TMC CM-1 65536 512 400 ms 3.13 ms DARPA I Columbia NON-VON 1M 512 1 s 125 ms DARPA I (est.?) Caltech CUBE 256 512 14 ms 13.7
Reference: [44] <author> R.S. Wallace and M.D. Howard. </author> <title> HBA Vision Architecture: Built and Benchmarked. </title> <booktitle> In Proceedings of the 1987 Workshop on Computer Architecture for Pattern Analysis and Machine Intelligence, </booktitle> <pages> pages 209-216, </pages> <address> Seattle, WA, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: DARPA I 1986 Hummel [18] NYU Ultracomputer 4096 512 Shiloach/Vishkin alg. 1987 Wallace and Howard <ref> [44] </ref>, [45] HBA 12 512 725 ms 33.2 s 2-pass swath 1987 Ibrahim, Kender, and Shaw [19] NON-VON 3 16384 128 12.5 ms 391 s 4-conn. 1987 Sunwoo, Baroody, and Aggarwal [43] Intel iPSC 32 256 5.074 s 2.48 ms 8-connectivity 1987 Rosenfeld [37] BBN Butterfly 128 512 7.2 s 3.52
Reference: [45] <author> R.S. Wallace and M.D. Howard. </author> <title> HBA Vision Architecture: Built and Benchmarked. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11(3) </volume> <pages> 227-232, </pages> <year> 1989. </year>
Reference-contexts: DARPA I 1986 Hummel [18] NYU Ultracomputer 4096 512 Shiloach/Vishkin alg. 1987 Wallace and Howard [44], <ref> [45] </ref> HBA 12 512 725 ms 33.2 s 2-pass swath 1987 Ibrahim, Kender, and Shaw [19] NON-VON 3 16384 128 12.5 ms 391 s 4-conn. 1987 Sunwoo, Baroody, and Aggarwal [43] Intel iPSC 32 256 5.074 s 2.48 ms 8-connectivity 1987 Rosenfeld [37] BBN Butterfly 128 512 7.2 s 3.52 ms
Reference: [46] <author> J.-C. Wang, T.-H. Lin, and S. Ranka. </author> <title> Distributed Scheduling of Unstructured Collective Communication on the CM-5. </title> <type> Personal Communication., </type> <year> 1994. </year>
Reference-contexts: This is consistent with the results achieved by other research teams that have achieved 6.4 MB/s per processor (Culler at UC Berkley, [11]), and 7.72 MB/s per processor (Ranka at Syracuse University, <ref> [46] </ref>) for similar data movements on the CM-5. Note that some of these cited results are for low-level implementations using message passing algorithms.
Reference: [47] <author> C. Weems, E. Riseman, A. Hanson, and A. Rosenfeld. </author> <title> An Integrated Image Understanding Benchmark: Recognition of a 2 1 2 D "Mobile". </title> <booktitle> In Image Understanding Workshop, </booktitle> <pages> pages 111-126, </pages> <address> Cambridge, MA, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: The second algorithm performs the connected components of images ([1], [6], [8], [9], [12], [16], [17], [20], [38]). The task of connected component labeling is cited as an important object recognition problem in the DARPA Image Understanding benchmarks ([37], <ref> [47] </ref>), and also can be applied fl The support by NASA Graduate Student Researcher Fellowship No. NGT-50951 is gratefully acknowledged. y Supported in part by NSF grant No. CCR-9103135 and NSF HPCC/GCAG grant No. <p> These test images, shown in Figure 1, are generated at runtime, with images 1-4, 7, and 9, augmented to the needed image size, while images 5, 6, and 8, scaled appropriately. Figure 2 is a 512 fi 512, 256 grey-level, image from the Second DARPA Image Understanding Benchmark <ref> [47] </ref>. The histogramming algorithm is assumed to be correct because P k1 i=0 H [i] = n 2 , and for regular patterns, it is easy to verify that each H [i]=n 2 equals the percentage of area that grey level i covers in the image.
Reference: [48] <author> C. Weems, E. Riseman, A. Hanson, and A. Rosenfeld. </author> <title> A Report on the Results of the DARPA Integrated Image Understanding Benchmark Exercise. </title> <booktitle> In Image Understanding Workshop, </booktitle> <pages> pages 165-192, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: ms DARPA I 1989 Manohar and Ramapriyan [31] Goodyear MPP 16384 128 97.3 ms 3.04 ms shrink/expand 1989 Kanade and Webb [24] Warp 10 512 4.34 s 166 s shrnk/expnd, DARPA II Im. 1989 Weems, Riseman, Hanson, Alliant FX-80 8 512 7.225 s 220 s DARPA II Image and Rosenfeld <ref> [48] </ref> Sequent Symmetry 81 8 512 15.12 s 461 s DARPA II Image Warp 10 512 3.98 s 152 s DARPA II Image TMC CM-2 32768 512 140 ms 547 s DARPA II Image 1990 Falsafi and Miller [14] Intel iPSC/2 32 512 1.197 s 146 s 1991 Parkinson [35] AMT
Reference: [49] <author> S.G. Ziavras and P. Meer. </author> <title> Adaptive Multiresolution Structures for Image Processing on Parallel Computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 23 </volume> <pages> 475-483, </pages> <year> 1994. </year> <month> 33 </month>
Reference-contexts: D+C (partitioned input) (DARPA II Image) TMC CM-5 32 512 398 ms 48.6 s multi-dim. D+C (complete im./PE) TMC CM-5 32 512 452 ms 55.2 s multi-dim. D+C (cmplt. + collect. comm.) 1994 Ziavras and Meer <ref> [49] </ref> TMC CM-2 16384 128 35.4 s 1.11 s 1994 Bader and JaJa TMC CM-5 32 512 368 ms 44.9 s DARPA II Image TMC CM-5 32 512 292 ms 35.6 s mean of test images (This paper) TMC CM-5 32 1024 852 ms 26.0 s mean of test images IBM
References-found: 49

