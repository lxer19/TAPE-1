URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1998/tr-98-004.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1998.html
Root-URL: http://www.icsi.berkeley.edu
Title: Simplified ART: A new class of ART algorithms  
Author: Andrea Baraldi and Ethem Alpaydn 
Keyword: hard and soft competitive learning, cluster detection, ART 1-based systems, Self-Organizing Map, Neural Gas algorithm, fuzzy set theory, fuzzy clustering.  
Note: On leave from IMGA-CNR,  On leave from  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  40129 Italy.  Istanbul TR-80815 Turkey.  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  Bologna  Department of Computer Engineering, Bogazi~ci University,  
Pubnum: TR-98-004  
Email: baraldi@icsi.berkeley.edu, ethem@icsi.berkeley.edu  
Phone: (510) 643-9153 FAX (510) 643-7684  
Date: February 1998  
Abstract: The Simplified Adaptive Resonance Theory (SART) class of networks is proposed to handle problems encountered in Adaptive Resonance Theory 1 (ART 1)-based algorithms when detection of binary and analog patterns is performed. The basic idea of SART is to substitute ART 1-based "unidirectional" (asymmetric) activation and match functions with "bidirectional" (symmetric) function pairs. This substitution makes the class of SART algorithms potentially more robust and less time-consuming than ART 1-based systems. One SART algorithm, termed Fuzzy SART, is discussed. Fuzzy SART employs probabilistic and possibilistic fuzzy membership functions to combine soft competitive learning with outlier detection. Its soft competitive strategy relates Fuzzy SART to the well-known Self-Organizing Map and Neural Gas clustering algorithm. A new Normalized Vector Distance, which can be employed by Fuzzy SART, is also presented. Fuzzy SART performs better than ART 1-based Carpenter-Grossberg-Rosen Fuzzy ART in the clustering of a simple two-dimensional data set and the standard four-dimensional IRIS data set. As expected, Fuzzy SART is less sensitive than Fuzzy ART to small changes in input parameters and in the order of the presentation sequence. In the clustering of the IRIS data set, performances of Fuzzy SART are analogous to or better than those of several clustering models found in the literature. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Alpaydn, E. </author> <year> (1998). </year> <title> Soft vector quantization and the EM algorithm. Neural Networks, </title> <publisher> in press. </publisher>
Reference: <author> Ancona, F., Ridella, S., Rovetta, S., & Zunino, R. </author> <year> (1997). </year> <title> On the importance of sorting in "Neural Gas" training of vector quantizers. </title> <booktitle> Proc. International Conference on Neural Networks '97, </booktitle> <address> Houston, TX, </address> <month> June </month> <year> 1997, </year> <journal> vol. </journal> <volume> 3, </volume> <pages> pp. 1804-1808. </pages>
Reference-contexts: This also means that PEs exploiting a relative membership function as their activation function are context-sensitive, i.e., R i;j provides a tool for modeling network-wide internode communication by subsuming that PEs are coupled through feed-sideways (lateral) connections <ref> (Ancona, Ridella, Rovetta & Zunino, 1997) </ref>. Possibilistic membership functions relax condition (ii) to satisfy the following constraints (Krishnapuram and Keller, 1993): iv) A i;j 2 [0,1], i = 1; :::; n, j = 1; :::; c; vi) 0 &lt; i=1 A i;j &lt; n, j = 1; :::; c. <p> In detail, NG implements model transitions from soft to hard competitive learning by: (a) employing metrical neighbors in the input space rather than topological neighbors belonging xxiii to an output lattice as in SOM; and (b) sorting the activation values of processing elements as the only network-wide internode communication <ref> (Ancona, Ridella, Rovetta & Zunino, 1997) </ref>. <p> Both empirical evidence and theoretical predictions indicate that a few (five to ten) "top" positions in the list of sorted activation values (i.e., r (t) j 2 f0; 9g) are sufficient to attain almost ideal results <ref> (Ancona, Ridella, Rovetta & Zunino, 1997) </ref>. Resonance domain detection.
Reference: <author> Barni, M., Cappellini, V., & Mecocci, A. </author> <year> (1996). </year> <title> Comments on "A possibilistic approach to clustering". </title> <journal> IEEE Trans. Fuzzy Systems, </journal> <volume> 4(3), </volume> <pages> 393-396. </pages>
Reference: <author> Baraldi, A., & Parmiggiani, F. </author> <year> (1995a). </year> <title> A neural network for unsupervised categorization of multivalued input patterns: an application to satellite image clustering. </title> <journal> IEEE Trans. Geosci. Remote Sensing, </journal> <volume> 33(2), </volume> <pages> 305-316. </pages>
Reference-contexts: = M (X; W), 8W; X 2 F S. 2.2 Processing scheme Although in its original form the ART 1 attentional subsystem employs bottom-up (feed-forward) and top-down (feed-backward) connections, it is easy to prove that this module is mathematically equivalent to an attentional subsystem where feed-forward connections are adopted exclusively <ref> (Baraldi & Parmiggiani, 1995a) </ref>. <p> If N V D ! 1 (V DM ! 0), then the two vectors are maximally different. It can be demonstrated that R d is a metric space (Pao, 1989) with metric N V D, where the following relationships hold true <ref> (Baraldi & Parmiggiani, 1995a, b, c, 1996) </ref> 1. N V D is a mapping from the metric space R d to range [0,1], i.e., N V D : R d fi R d ! [0; 1] (normalized positivity); 2. <p> space and provides an a posteriori probability estimate iff class conditional likelihood satisfies a (S)ART-based bidirectional vigilance test is the Gaussian ARTMAP (GAM) model (Williamson, 1996, 1997). 7 Fuzzy SART Two SART implementations can be found in the literature, employing a hard (WTA) and a soft competitive learning strategy respectively <ref> (Baraldi & Parmiggiani, 1995a, 1995b) </ref>. In agreement with theoretical expectations (see Section 3.2), the soft competitive version performed better than the hard competitive one (Baraldi & Parmiggiani, 1995b).
Reference: <author> Baraldi, A., & Parmiggiani, F. </author> <year> (1995b). </year> <title> A self-organizing neural network merging Kohonen's and ART models. </title> <booktitle> Proc. International Conference on Neural Networks '95, </booktitle> <address> Perth, Australia, </address> <month> December </month> <year> 1995, </year> <journal> vol. </journal> <volume> 5, </volume> <pages> pp. 2444-2449. </pages>
Reference-contexts: In agreement with theoretical expectations (see Section 3.2), the soft competitive version performed better than the hard competitive one <ref> (Baraldi & Parmiggiani, 1995b) </ref>. This development is quite similar to that regarding GAM, which was originally proposed as a hard competitive incremental algorithm (Williamson, 1996), then as a soft competitive (distributed learning) incremental algorithm (Williamson, 1997).
Reference: <author> Baraldi, A., & Parmiggiani, F. </author> <year> (1995c). </year> <title> A refined Gamma MAP SAR speckle filter with improved geometrical adaptivity. </title> <journal> IEEE Trans. Geosci. Remote Sensing, </journal> <volume> 33(5), </volume> <pages> 1245-1257. </pages>
Reference-contexts: Equation (14), which is independent of multiplicative noise, was developed for SAR image processing where speckle is modeled as multiplicative noise <ref> (Baraldi & Parmiggiani, 1995c) </ref>.
Reference: <author> Baraldi, A., & Parmiggiani, F. </author> <year> (1996). </year> <title> Combined detection of intensity and chromatic contours in color images. </title> <journal> Optical Engineering, </journal> <volume> 35(5), </volume> <pages> 1413-1439. </pages> <editor> xxxix Baraldi, A., & Parmiggiani, F. </editor> <year> (1997). </year> <title> Novel neural network model combining radial basis function, competitive Hebbian learning rule, and fuzzy simplified adaptive resonance theory. </title> <booktitle> Proc. SPIE's Optical Science, Engineering and Instrumentation '97: Applications of Fuzzy Logic Technology IV, </booktitle> <address> San Diego, CA, </address> <month> July </month> <year> 1997, </year> <journal> vol. </journal> <volume> 3165, </volume> <pages> 98-112. </pages>
Reference: <author> Bezdek, J. C., & Pal, N. R. </author> <year> (1993). </year> <title> Generalized clustering networks and Ko-honen's self-organizing scheme. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 4(4), </volume> <pages> 549-557. </pages>
Reference-contexts: include the following: xix A i;j = &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 1 i;j = j 2 (0; 1] (Krishnapuram & Keller, 1993); (23) Gaussian i;j = e i;j j 2 (0; 1] (Gaussian mixtures; Williamson, 1996, 1997); (24) 1 i;j ) p j 2 (0; 1) <ref> (Bezdek & Pal, 1993) </ref>; (25) 1 (1Gaussian i;j ) 2 2 (1; 1) (Baraldi & Parmiggiani, 1997); (26) where d i;j = d (X i ; W j ) is assumed to be the Euclidean distance between input pattern X i and prototype (receptive field center) W j of the j-th <p> We intend to demonstrate that, under the hypothesis that learning rate fi j (R i;j ) 2 [0; 1], i = 1; :::; n, j = 1; :::; c, is a monotonically increasing function of R i;j <ref> (e.g., Bezdek & Pal, 1993) </ref>, if Equation (22) computes absolute typicality as either Equation (25) or Equation (26) then the network may reach convergence faster than by exploiting a relative membership function featuring Equation (23) or Equation (24) as its absolute typicality.
Reference: <author> Bezdek, J. C., & Pal, N. R. </author> <year> (1995). </year> <title> Two soft relatives of learning vector quantization. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(5), </volume> <pages> 729-743. </pages>
Reference-contexts: Overall, Fuzzy SART is superior to Fuzzy ART with respect to both MSE minimization and pattern misclassification. When the number of detected clusters is 3, the value of misclassified patterns makes Fuzzy SART competitive with other clustering models found in the literature <ref> (Bezdek & Pal, 1995) </ref>, (Kim & Mitra, 1993), while Fuzzy ART is by no means competitive.
Reference: <author> Bishop, C. </author> <year> (1995). </year> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: Mimicking the real world, an artificial cognitive system employing reinforcement learning "is allowed to react to each training case; it is then told whether its reaction was good or bad" (Masters, 1994), "but no actual desired values are given" <ref> (Bishop, 1995) </ref>.
Reference: <author> Bishop, C., Svensen, M., & Williams, C. </author> <year> (1996). </year> <title> "GTM: a principled alternative to the self-organizing map," </title> <booktitle> Proc. Int. Conf. on Artificial Neural Networks, </booktitle> <address> ICANN'96, </address> <publisher> Springer-Verlag, </publisher> <pages> 164-170. </pages>
Reference: <author> Borghese, N. A., Ferrigno, G., Baroni, G., Savare, R., Ferrari, S., & Pedotti, A. </author> <year> (1998) </year> <month> AUTOSCAN: </month> <title> A flexible and portable scanner of 3D surfaces. </title> <journal> IEEE Computer Graphics & Applications, </journal> <note> in press. </note>
Reference-contexts: correct mapping, although this property is incorporated in FOSART (see Section 7.5); ii) if it employs similarity measure V DM , then Fuzzy SART should not be applied to data sets belonging to the Euclidean space, as shown in Figs. 7 and 8, where an input 3-D digitized human face <ref> (Borghese, Ferrigno, Baroni, Savare, Ferrari & Pedotti, 1998) </ref>, and the output resampled data set are shown respectively; and ii) Fuzzy SART does not minimize any known objective function.
Reference: <author> Boynton, R. M. </author> <year> (1990). </year> <title> Human color perception. In Science of Vision, </title> <editor> K. N. Leibovic, Ed., </editor> <address> 211- 253, </address> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: is not appropriate for assessing interpattern dissimilarities (similarities) in the Euclidean space (for an example, refer to Section 9). 1 1 An additional interesting relationship can be established between NV D computation and the way in which the mammalian visual system performs independent detection of achromatic and chromatic color contrasts <ref> (Boynton, 1990) </ref>; in fact, MDM is inversely related to achromatic color differences, while ADM is inversely related to chromatic color differences. xvii 5.2 More examples of M functions A second example of M function can be obtained by applying a binary operator, e.g., sum or product, to the unidirectional activation and
Reference: <author> Buhmann, J. </author> <year> (1995). </year> <title> Learning and data clustering. </title> <editor> In M. Arbib (Ed.), </editor> <title> Handbook of Brain Theory and Neural Networks, </title> <publisher> Bradford Books / MIT Press. </publisher>
Reference: <author> Carpenter, G. A., & Grossberg, S. </author> <year> (1987a). </year> <title> A massively parallel architecture for a self-organizing neural pattern recognition machine. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 37, </volume> <pages> 54-115. </pages>
Reference-contexts: In recent years, several ART-based models have been presented. ART 1 categorizes binary patterns but features sensitivity to the order of presentation of the random sequence <ref> (Carpenter & Grossberg, 1987a) </ref>. This finding led to the development of the Improved ART 1 system (IART 1), which is less dependent than ART 1 on the order of presentation of the input sequence (Shih, Moh & Chang, 1992).
Reference: <author> Carpenter, G. A., & Grossberg, S. </author> <year> (1987b). </year> <title> ART2: Self-organization of stable category recognition codes for analog input patterns. </title> <journal> Applied Optics, </journal> <volume> 26(21), </volume> <pages> 4919-4930. </pages>
Reference-contexts: The Adaptive Hamming Net (AHN), which is functionally equivalent to ART 1, optimizes ART 1 both in terms of computation time and storage requirement (Hung & Lin, 1995). ART 2, designed to detect regularities in analog random sequences, employs a computationally expensive architecture which presents difficulties in parameter selection <ref> (Carpenter & Grossberg, 1987b) </ref>. To overcome these difficulties, the Fuzzy ART system was developed as a generalization of ART 1 (Carpenter, Grossberg & Rosen, 1991; Carpenter, Grossberg, Markuzon, Reynolds & Rosen, 1992). This means however that ART 1-based structural problems may also affect Fuzzy ART.
Reference: <author> Carpenter, G. A., Grossberg, S., & Rosen, D. B. </author> <year> (1991). </year> <title> Fuzzy ART: fast stable learning and categorization of analog patterns by an adaptive resonance system. </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 759-771. </pages>
Reference-contexts: This simplification yields, as a major consequence, a change in the meaning of the term "resonance" as traditionally applied to ART 1-based systems. This term should no longer indicate "the basic feature of all ART systems, notably, pattern-matching between bottom-up input and top-down learned prototype vectors" <ref> (Carpenter, Grossberg & Rosen, 1991, p. 760) </ref>, just as the term "resonance" has never been used with reference to pattern matching performed by a feed-forward Kohonen network.
Reference: <author> Carpenter, G. A., Grossberg, S., Markuzon, N., Reynolds, J. H., </author> & <title> Rosen, </title> <address> D. </address>
Reference: <author> B. </author> <year> (1992). </year> <title> Fuzzy ARTMAP: a neural network architecture for incremental supervised learning of analog multidimensional maps. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(5), </volume> <pages> 698-713. </pages>
Reference: <author> Dave R. N., & Krishnapuram R. </author> <year> (1997). </year> <title> Robust clustering method: a unified view. </title> <journal> IEEE Transactions on Fuzzy Systems, </journal> <volume> 5(2), </volume> <pages> 270-293. </pages>
Reference-contexts: On one hand in probabilistic fuzzy clustering, owing to condition (ii), noise points and outliers, featuring low possibilistic typicalities with respect to all templates, may have significantly high probabilistic membership values and may severely affect the prototype parameter estimate <ref> (e.g., refer to Dave & Krishnapuram, 1997) </ref>. On the other hand in possibilistic fuzzy clustering, learning rates computed from absolute typicalities tend to produce coincident clusters (Barni, Cappellini & Mecocci, 1996; Dave and Krishnapuram, 1997). <p> Variables j , j and p j are all resolution parameters belonging to range (0; 1) <ref> (refer to Dave & Krishnapuram, 1997) </ref>. It is to be noted that Equations (23) and (24) belong to the class of M functions (see Section 2.1).
Reference: <author> Dempster, A. P., Laird, N. M., & Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statist. Soc. Ser. B, </journal> <volume> 39, </volume> <pages> 1-38. </pages>
Reference-contexts: Interpretations of this second Kohonen heuristic rule, and relationships between SOM and other optimization techniques such as deterministic annealing (Rose, Guerewitz & Fox, 1995) and the Expectation-Maximization (EM) approach <ref> (Dempster, Laird & Rubin, 1977) </ref> are proposed in Luttrell (1990), Martinetz, Berkovich & Schulten (1993), Buhmann (1995), Mulier & Cherkassky (1995b), Alpaydn (1998).
Reference: <author> Dubes, R., & Jain, A. K. </author> <year> (1976). </year> <title> Clustering techniques: the user's dilemma. </title> <journal> Pattern Recognition, </journal> <volume> 8, </volume> <pages> 247-260. </pages>
Reference-contexts: Applications to simple data sets are sufficient to let these functional differences emerge naturally. The Fuzzy SART and Fuzzy ART systems belong to the class of squared-error clustering programs <ref> (Dubes & Jain, 1976) </ref>. To compare their performances, these two systems must be applied to several data sets to detect the same number of output categories while the following features are considered (Dubes & Jain, 1976): i) the value of their squared error criterion; ii) the number of misclassified input patterns; <p> The Fuzzy SART and Fuzzy ART systems belong to the class of squared-error clustering programs <ref> (Dubes & Jain, 1976) </ref>. To compare their performances, these two systems must be applied to several data sets to detect the same number of output categories while the following features are considered (Dubes & Jain, 1976): i) the value of their squared error criterion; ii) the number of misclassified input patterns; iii) the number of iterations before reaching termination (epochs); iv) the sensitivity (stability) of the two algorithms to the order of the training sequence; and v) the sensitivity (stability) of the two
Reference: <author> Erwin, E., Obermayer, K., & Schulten, K. </author> . <year> (1992). </year> <title> Self-organizing maps: ordering, convergence properties and energy functions. </title> <journal> Biol. Cybernetics, </journal> <volume> 67, </volume> <pages> 47-55. </pages>
Reference-contexts: Williams, 1996). SOM instead features a set of potential functions, one for each node, to be independently minimized following a stochastic (on-line) gradient descent <ref> (Erwin, Obermayer & Schulten, 1992) </ref>. ii) The size of the output lattice, the learning rate and the size of the resonance neighborhood must be varied empirically from one data set to another to achieve useful results (Tsao, Bezdek & Pal, 1994). iii) Topology preserving mapping as defined by Martinetz, Berkovich &
Reference: <author> Fritzke, B. </author> <year> (1994). </year> <title> Growing cell structures A self-organizing network for unsupervised and supervised learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(9), </volume> <pages> 1441-1460. </pages>
Reference-contexts: Although the presence of PE-based variables has never been stressed in the development of clustering algorithms featuring a fixed number of units, e.g., SOM and NG, it has been employed in Kohonen-based growing networks <ref> (Fritzke, 1994, 1995) </ref>, as well as in GAM (to estimate priors, Williamson, 1997). Activation function. <p> Preliminary results of the new version of Fuzzy SART, termed FOSART (see Section 7.5), are encouraging, as shown in Figs. 9 and 10 (Baraldi & Parmiggiani, 1997). Both Fuzzy SART and FOSART can be employed as the hidden layer in any two-layer supervised system computing function approximation through scatter-partitioning <ref> (Fritzke, 1994, 1997b) </ref>. In these systems, supervised errors at the output are taken into account to determine number and scatter position of PEs belonging to the hidden clustering layer (Alpaydn, 1998; Bishop, 1995; Fritzke, 1994, 1997b). Acknowledgments We are grateful to R. Savare, G. Ferrigno, N. A. Borghese and S.
Reference: <author> Fritzke, B. </author> <year> (1995). </year> <title> A growing neural gas network learns topologies. </title> <booktitle> In G. </booktitle>
Reference: <editor> Tesauro, D. S. Touretzky & T. K. Leen (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7 (pp. </booktitle> <pages> 625-632). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Fritzke, B. </author> <year> (1997a). </year> <title> Some competitive learning methods. Draft document, </title> <address> http://www.neuroinformatik.ruhr-uni-bochum.de/ini/VDM/ research/gsn/De-moGNG. </address>
Reference-contexts: It requires the degree of overlap among node receptive fields to decrease monotonically with time until it becomes zero, as receptive fields become Voronoi polyhedra <ref> (Fritzke, 1997a) </ref>. vi) To guarantee topologically correct mapping, dynamic generation/removal of synaptic links between node pairs should be enforced according to the competitive Hebbian learning mechanism (Martinetz, Berkovich & Schulten, 1994; Fritzke, 1997a). vii) Stronger relationships with other clustering algorithms capable of minimizing a cost function, such as the Neural Gas <p> This model transition is equivalent to stating that the initial overlap between nodes' receptive fields must decrease monotonically with time until it is reduced to zero, as hard competitive learning renders receptive fields equivalent to Voronoi polyhedra <ref> (Fritzke, 1997a) </ref>. <p> From a general perspective, it is important to remember that, compared to hard competitive learning, soft competitive learning not only decreases dependency on initialization (Martinetz, Berkovich & Schulten, 1993), but also reduces the presence of dead units <ref> (Fritzke, 1997a) </ref>. Despite its many successes in practical applications, SOM contains some major deficiencies (many of which are acknowledged in Kohonen, 1995), as listed below: i) Termination is not based on optimizing any model of the process or its data (Tsao, Bezdek & Pal, 1994). <p> It is important to stress that while Kohonen's Vector Quantization (VQ) and SOM represent two important paradigms for information representation both in theory and in practice (Kohonen, 1995), another clustering algorithm, termed Neural Gas (NG, Martinetz, Berkovich & Schulten, 1993), has quickly gained popularity as a successful on-line vector quantizer <ref> (Fritzke, 1997a) </ref>. NG implements a stochastic gradient descent of an analytical cost function, as opposed to SOM. Moreover, NG satisfies Kohonen's two constraints. <p> Fuzzy SART provided with CHL is termed Fully self-Organizing SART (FOSART). Note that FOSART requires no ranking of the set of output values provided by the battery of attentional nodes, since the update neighborhood is identified as the set of nodes topologically connected to the winner unit <ref> (Fritzke, 1997a) </ref>.
Reference: <author> Fritzke, B. </author> <year> (1997b). </year> <title> Incremental neuro-fuzzy systems. </title> <booktitle> Proc. SPIE's Optical Science, Engineering and Instrumentation '97: Applications of Fuzzy Logic Technology IV, </booktitle> <address> San Diego, CA, </address> <month> July </month> <year> 1997. </year>
Reference: <author> Healy, M. J., Caudell, T. P., & Smith, D. G. </author> <year> (1993). </year> <title> A neural architecture for pattern sequence verification through inferencing. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(1), </volume> <pages> 9-20. </pages>
Reference-contexts: The best-matching unit, identified as E j fl , is the solution, if any, to the maximization problem <ref> (Healy, Caudell & Smith, 1993) </ref> j fl = arg max n (t) o subject to the vigilance constraint described below. The best-matching weight vector converging on E j fl is identified as W (t) Step 4. Resonance domain detection.
Reference: <author> Huang, J., Georgiopoulos, M., & Heileman, G. L. </author> <year> (1995). </year> <title> Fuzzy ART properties. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(2), </volume> <pages> 203-213. </pages>
Reference-contexts: The Fuzzy SART input parameters are 2 (0; 1) and e min 2 [1; +1). The three Fuzzy ART input parameters are vigilance threshold 2 (0; 1), * ini 2 (0:0005; 1] (see above) and e min 2 [1; +1). Parameter ff in Equation (4) is set to 0.001 <ref> (Huang, Georgiopoulos & Heileman, 1995) </ref>. 8.2 Two-dimensional data clustering Ten different sequences of the Simpson data set are iteratively presented to the Fuzzy SART and the modified Fuzzy ART systems.
Reference: <author> Hung C., & Lin, S. </author> <year> (1995). </year> <title> Adaptive Hamming Net: a fast-learning ART 1 model without searching. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(4), </volume> <pages> 605-618. </pages>
Reference-contexts: The Adaptive Hamming Net (AHN), which is functionally equivalent to ART 1, optimizes ART 1 both in terms of computation time and storage requirement <ref> (Hung & Lin, 1995) </ref>. ART 2, designed to detect regularities in analog random sequences, employs a computationally expensive architecture which presents difficulties in parameter selection (Carpenter & Grossberg, 1987b). <p> ART 1-based clustering algorithms can be found in the literature: i) IART 1, which employs a slightly modified ART 1 architecture to extract statistical regularities from binary samples; ii) Adaptive Hamming Net (AHN), which is a feed-forward network that optimizes ART 1 in terms of computation time and storage requirement <ref> (Hung & Lin, 1995) </ref>; and iii) Fuzzy ART, which extracts statistical regularities from random samples of binary as well as analog pattern distributions. <p> For example, the Adaptive Hamming Net (AHN), shown in Fig. 1, is a feed-forward network functionally equivalent to ART 1 <ref> (Hung & Lin, 1995) </ref>. v W and c W, refer to Hung & Lin (1995). vi We can generalize this result by stating that the attentional module of all ART 1--based systems is functionally equivalent to a feed-forward network featuring no top-down connection. <p> It is to be noted that: i) ~ AF 1 (X (t) ; W j ) belongs to the class of ~ M functions (see Section 2.1). ii) In ART 1 and IART 1, the "unidirectional" activation function applied to binary vector pairs is <ref> (Hung & Lin, 1995) </ref> ~ AF 2 (X (t) ; W j ) = k=1 W k;j X k P d (t) ; j = 1; :::; c; X k ; W k;j 2 f0; 1g: (5) Equation (4) generalizes Equation (5) by substituting the product and norm operators with operations <p> For example, if ff =(1 ), then Fuzzy ART completes its learning in one list presentation when complement coding is employed for preprocessing. iii) In ART 1 and IART 1, the "unidirectional" match function applied to binary vector pairs is <ref> (Hung & Lin, 1995) </ref> ~ M F 2 (W j fl ; X (t) ) = k=1 W k;j fl X k k=1 X k (t) (t) Also Equation (6) generalizes Equation (7) by substituting the product and norm operators with fuzzy-like operators. <p> This is equivalent to stating that IART 1 adopts one bidirectional vigilance test. Besides Equation (7), IART 1 employs match function <ref> (Hung & Lin, 1995) </ref> ~ M F 3 (X (t) ; W j fl ) = k=1 W k;j fl X k k=1 W k;j fl (t) (t) Equation (9) provides a normalized measure of how many unit-valued components of W (t) are matched by those of X (t) , i.e., <p> bidirectional match function (Shih, Moh & Chang, 1992), the next step in the evolution of ART 1-based architectures should replace the pair of unidirectional activation and match functions with a pair of bidirectional functions. ii) ART 1-based systems require no searching when an Adaptive Hamming Net (AHN) approach is applied <ref> (Hung & Lin, 1995) </ref>. The key idea is to convert the sequential search procedure of ART 1 into an optimization problem (see Section 2.2), which can be solved by parallel implementation.
Reference: <author> Luttrell, S. P. </author> <year> (1990). </year> <title> Derivation of a class of training algorithms. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 1, </volume> <pages> 229-232. </pages>
Reference: <author> Karayannis, N. B., Bezdek, J. C., Pal, N. R., Hathaway, R. J., & Pai, P. </author> <year> (1996). </year> <title> Repair to GLVQ: A new family of competitive learning schemes. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 7(5), </volume> <pages> 1062-1071. </pages>
Reference: <author> Kim, Y. S., & Mitra, S. </author> <year> (1993). </year> <title> Integrated Adaptive Fuzzy Clustering (IAFC) algorithm. </title> <booktitle> Proceedings of the Second IEEE International Conference on Fuzzy Systems, </booktitle> <volume> bf 2, </volume> <pages> 1264-1268. </pages>
Reference-contexts: Overall, Fuzzy SART is superior to Fuzzy ART with respect to both MSE minimization and pattern misclassification. When the number of detected clusters is 3, the value of misclassified patterns makes Fuzzy SART competitive with other clustering models found in the literature (Bezdek & Pal, 1995), <ref> (Kim & Mitra, 1993) </ref>, while Fuzzy ART is by no means competitive. <p> Free parameter No. of MSE Misclassified No. of values fl clusters patterns iterations = 0:36, * min = 50 3 0.555 11 50 = 0:75, * min = 10 8 0.204 4 10 fl ini = fin = 0:0001; * fin = 0:005. by 15 misclassifications <ref> (Kim & Mitra, 1993) </ref>; and iii) the Kohonen VQ algorithm, affected by 17 misclassifications (Kim & Mitra, 1993). Table 3 gives the numerical values of the physically labeled IRIS subsample means. <p> clusters patterns iterations = 0:36, * min = 50 3 0.555 11 50 = 0:75, * min = 10 8 0.204 4 10 fl ini = fin = 0:0001; * fin = 0:005. by 15 misclassifications <ref> (Kim & Mitra, 1993) </ref>; and iii) the Kohonen VQ algorithm, affected by 17 misclassifications (Kim & Mitra, 1993). Table 3 gives the numerical values of the physically labeled IRIS subsample means. Tables 4 and 5 report Fuzzy SART and Fuzzy ART terminal centroids when the number of detected categories is equal to 3.
Reference: <author> Kohonen, T. </author> <year> (1990). </year> <title> The self-organizing map. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(9), </volume> <pages> 1464-1480. </pages>
Reference-contexts: This means however that ART 1-based structural problems may also affect Fuzzy ART. Our goal is to provide a new synthesis between properties of Fuzzy ART and other successful clustering algorithms such as the Self-Organizing Map (SOM) <ref> (Kohonen, 1990, 1995) </ref> and Neural Gas (NG) (Martinetz, Berkovich & Schulten, 1993), to extend the abilities of these separate approaches. This paper is organized as follows. In Section 2, a general template for ART 1-based algorithms is presented. In Section 3 Fuzzy ART is discussed, and improvements are recommended. <p> These two contraints derive from neurophysiological studies and provide an annealing schedule (Martinetz, Berkovich & Schulten, 1993; Ancona, Ridella, Rovetta & Zunino, 1997). They consist of two empirical functions of time, which must be user defined <ref> (Kohonen, 1990, 1995) </ref>. The first Kohonen xxii heuristic rule requires that learning rates decrease monotonically with time according to a cooling scheme, i.e., as the number of processing epochs increases, all learning rates (winner as well as non-winner) decrease towards zero.
Reference: <author> Kohonen, T. </author> <year> (1995). </year> <title> Self-Organizing Maps, </title> <publisher> Berlin: Springer Verlag. </publisher>
Reference-contexts: Despite its many successes in practical applications, SOM contains some major deficiencies <ref> (many of which are acknowledged in Kohonen, 1995) </ref>, as listed below: i) Termination is not based on optimizing any model of the process or its data (Tsao, Bezdek & Pal, 1994). <p> It is important to stress that while Kohonen's Vector Quantization (VQ) and SOM represent two important paradigms for information representation both in theory and in practice <ref> (Kohonen, 1995) </ref>, another clustering algorithm, termed Neural Gas (NG, Martinetz, Berkovich & Schulten, 1993), has quickly gained popularity as a successful on-line vector quantizer (Fritzke, 1997a). NG implements a stochastic gradient descent of an analytical cost function, as opposed to SOM. Moreover, NG satisfies Kohonen's two constraints.
Reference: <author> Krishnapuram, R., & Keller, J. M. </author> <year> (1993). </year> <title> A possibilistic approach to clustering. </title> <journal> IEEE Transactions on Fuzzy Systems, </journal> <volume> 1(2), </volume> <pages> 98-110. </pages>
Reference-contexts: 1; :::; n; j = 1; :::; c: (22) xviii Relative typicality values, R i;j , must satisfy the following three conditions (Tsao, Bezdek & Pal, 1994; Pao, 1989): ii) j=1 R i;j = 1, i = 1; :::; n; and P n Constraint (ii) is an inherently probabilistic constraint <ref> (Krishnapuram & Keller, 1993) </ref>, relating R i;j values to posterior probability estimates in a Bayesian framework. Because of condition (ii), R i;j values are relative numbers dependent on the absolute membership of the pattern in all other classes, thus indirectly on the total number of classes. <p> Possibilistic membership functions relax condition (ii) to satisfy the following constraints <ref> (Krishnapuram and Keller, 1993) </ref>: iv) A i;j 2 [0,1], i = 1; :::; n, j = 1; :::; c; vi) 0 &lt; i=1 A i;j &lt; n, j = 1; :::; c. <p> Different A i;j expressions, consistent with the definition provided above, were found to be useful in the existing literature. These include the following: xix A i;j = &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 1 i;j = j 2 (0; 1] <ref> (Krishnapuram & Keller, 1993) </ref>; (23) Gaussian i;j = e i;j j 2 (0; 1] (Gaussian mixtures; Williamson, 1996, 1997); (24) 1 i;j ) p j 2 (0; 1) (Bezdek & Pal, 1993); (25) 1 (1Gaussian i;j ) 2 2 (1; 1) (Baraldi & Parmiggiani, 1997); (26) where d i;j = d
Reference: <author> Martinetz, T., Berkovich, G., & Schulten, K. </author> <year> (1993). </year> <title> Neural-Gas network for quantization and its application to time-series predictions. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(4), </volume> <pages> 558-569. </pages> <note> xli Martinetz, </note> <author> T., Berkovich, G., & Schulten, K. </author> <year> (1994). </year> <title> Topology representing networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(3), </volume> <pages> 507-522. </pages>
Reference-contexts: This means however that ART 1-based structural problems may also affect Fuzzy ART. Our goal is to provide a new synthesis between properties of Fuzzy ART and other successful clustering algorithms such as the Self-Organizing Map (SOM) (Kohonen, 1990, 1995) and Neural Gas (NG) <ref> (Martinetz, Berkovich & Schulten, 1993) </ref>, to extend the abilities of these separate approaches. This paper is organized as follows. In Section 2, a general template for ART 1-based algorithms is presented. In Section 3 Fuzzy ART is discussed, and improvements are recommended. <p> To guarantee topologically correct mapping, dynamic generation/removal of synaptic links between node pairs should be enforced according to the competitive Hebbian learning mechanism (Martinetz, Berkovich & Schulten, 1994; Fritzke, 1997a). vii) Stronger relationships with other clustering algorithms capable of minimizing a cost function, such as the Neural Gas algorithm (NG) <ref> (Martinetz, Berkovich & Schulten, 1993) </ref>, should be pursued. 4 SART framework To overcome limitations of ART 1-based models, we propose a new class of ART processing schemes, hereafter referred to as Simplified ART (SART), consisting of a pair of attentional and orienting subsystems and capable of processing real-valued multidimensional patterns (Fig. <p> This "fuzzification" process justifies exploitation of the name Fuzzy SART. With regard to learning strategy, Fuzzy SART is intended to combine useful properties driven from successful clustering algorithms, such as SOM and Neural Gas <ref> (NG, Martinetz, Berkovich & Schulten, 1993) </ref>. To summarize, Fuzzy SART aims to provide a new synthesis between properties of ART, SOM and NG, to extend abilities of these separate approaches. <p> From a general perspective, it is important to remember that, compared to hard competitive learning, soft competitive learning not only decreases dependency on initialization <ref> (Martinetz, Berkovich & Schulten, 1993) </ref>, but also reduces the presence of dead units (Fritzke, 1997a). <p> It is important to stress that while Kohonen's Vector Quantization (VQ) and SOM represent two important paradigms for information representation both in theory and in practice (Kohonen, 1995), another clustering algorithm, termed Neural Gas <ref> (NG, Martinetz, Berkovich & Schulten, 1993) </ref>, has quickly gained popularity as a successful on-line vector quantizer (Fritzke, 1997a). NG implements a stochastic gradient descent of an analytical cost function, as opposed to SOM. Moreover, NG satisfies Kohonen's two constraints.
Reference: <author> Masters, T. </author> <year> (1994). </year> <title> Signal and image processing with neural networks A C++ sourcebook. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: Mimicking the real world, an artificial cognitive system employing reinforcement learning "is allowed to react to each training case; it is then told whether its reaction was good or bad" <ref> (Masters, 1994) </ref>, "but no actual desired values are given" (Bishop, 1995).
Reference: <author> Mulier, F. M., & Cherkassky V. S. </author> <year> (1995a). </year> <title> Statistical analysis of self-organization. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(5), </volume> <pages> 717-727. </pages>
Reference: <author> Mulier, F. M., & Cherkassky V. S. </author> <year> (1995b). </year> <title> Self-organization as an iterative kernel smoothing process. </title> <journal> Neural Computation, </journal> <volume> 7, </volume> <pages> 1165-1177. </pages>
Reference: <author> Pao Y. </author> <year> (1989). </year> <title> Adaptive pattern recognition and neural networks. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: If N V D = 0 (V DM = 1), then W and X are the same vector. If N V D ! 1 (V DM ! 0), then the two vectors are maximally different. It can be demonstrated that R d is a metric space <ref> (Pao, 1989) </ref> with metric N V D, where the following relationships hold true (Baraldi & Parmiggiani, 1995a, b, c, 1996) 1. <p> The extent to which X i is compatible with a vague (fuzzy) concept associated with generic state C j can be interpreted "more in terms of a possibility (compatibility) distribution rather than in terms of a probability distribution" <ref> (Pao, 1989, p. 58) </ref>. This legitimizes some possibility distributions, called fuzzy membership functions, that "we believe are useful, but might find difficult to justify on the basis of objective probabilities" (Pao, 1989, p. 57). <p> This legitimizes some possibility distributions, called fuzzy membership functions, that "we believe are useful, but might find difficult to justify on the basis of objective probabilities" <ref> (Pao, 1989, p. 57) </ref>. <p> In this paper the definition of absolute membership function is further relaxed to satisfy constraint (v) exclusively, i.e., a fuzzy set employing absolute membership values may not be normal as its membership values may feature no least upper bound equal to one <ref> (Pao, 1989) </ref>. Term A i;j is an absolute similarity value depending on fuzzy state C j exclusively, given input pattern X i . In other words, A i;j is context-insensitive, since it is not affected by any other state.
Reference: <author> Parisi, D. </author> <year> (1991). </year> <institution> La scienza cognitiva tra intelligenza artificiale e vita artificiale. </institution>
Reference: <editor> In E. Biondi, P. Morasso & V. Tagliasco (Eds.), Neuroscienze e scienze dell'artificiale: dal neurone all'intelligenza (pp. </editor> <address> 321-341). Bologna, Italy: Patron. </address>
Reference: <author> Rose, K., Guerewitz, F., & Fox, G. </author> <year> (1990). </year> <title> A deterministic approach to clustering. </title> <journal> Pattern Recognition Letters, </journal> <volume> 11(11), </volume> <pages> 589-594. </pages>
Reference: <author> Ritter, H., Martinetz, T., & Schulten, K. </author> <year> (1992). </year> <title> Neural computation and self-organizing maps. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Serra, R., & Zanarini, G. </author> <year> (1990). </year> <title> Complex systems and cognitive processes. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <author> Shih, F. Y., Moh, J., & Chang, F. </author> <year> (1992). </year> <title> A new ART-based neural architecture for pattern classification and image enhancement without prior knowledge. </title> <journal> Pattern Recognition, </journal> <volume> 25(5), </volume> <pages> 533-542. </pages>
Reference-contexts: This finding led to the development of the Improved ART 1 system (IART 1), which is less dependent than ART 1 on the order of presentation of the input sequence <ref> (Shih, Moh & Chang, 1992) </ref>. The Adaptive Hamming Net (AHN), which is functionally equivalent to ART 1, optimizes ART 1 both in terms of computation time and storage requirement (Hung & Lin, 1995). <p> The first task selects the best-matching template. The second task constrains the input pattern to fall within a bounded hypervolume of acceptance centered on the best-matching template. As IART 1 improves ART 1 by replacing the unidirectional match function with a bidirectional match function <ref> (Shih, Moh & Chang, 1992) </ref>, the next step in the evolution of ART 1-based architectures should replace the pair of unidirectional activation and match functions with a pair of bidirectional functions. ii) ART 1-based systems require no searching when an Adaptive Hamming Net (AHN) approach is applied (Hung & Lin, 1995).
Reference: <author> Simpson, P. K. </author> <year> (1993). </year> <title> Fuzzy min-max neural networks Part 2: clustering. </title> <journal> IEEE Transactions on Fuzzy Systems, </journal> <volume> 1(1), </volume> <pages> 32-45. </pages>
Reference-contexts: Since squared-error clustering algorithms detect groups of patterns that are hyperspher-ical or hyperellipsoidal in shape, two simple data sets are selected from the literature. The first data set, shown in Fig. 5, is two-dimensional and consists of 24 points <ref> (Simpson, 1993) </ref>. The second data set is the 4-dimensional standard IRIS data set, consisting of 50 vectors for each of 3 classes (Fisher, 1936). Exploitation of the IRIS data set allows comparison of Fuzzy SART and Fuzzy ART with other clustering models found in the literature.
Reference: <author> Tsao, E. C., Bezdek, J. C., & Pal, N. R. </author> <year> (1994). </year> <title> Fuzzy Kohonen clustering network. </title> <journal> Pattern Recognition, </journal> <volume> 27(5), </volume> <pages> 757-764. </pages>
Reference-contexts: Despite its many successes in practical applications, SOM contains some major deficiencies (many of which are acknowledged in Kohonen, 1995), as listed below: i) Termination is not based on optimizing any model of the process or its data <ref> (Tsao, Bezdek & Pal, 1994) </ref>. Indeed, it has been shown that an objective function cannot exist for the SOM algorithm, i.e., there exists no cost function yielding Kohonen's adaptation rule as its gradient (Erwin, Obermayer & Schulten, 1992; Bishop, Svensen & C. Williams, 1996). <p> one for each node, to be independently minimized following a stochastic (on-line) gradient descent (Erwin, Obermayer & Schulten, 1992). ii) The size of the output lattice, the learning rate and the size of the resonance neighborhood must be varied empirically from one data set to another to achieve useful results <ref> (Tsao, Bezdek & Pal, 1994) </ref>. iii) Topology preserving mapping as defined by Martinetz, Berkovich & Schulten (1994) is not guaranteed. iv) Prototype parameter estimates may be severely affected by noise points and outliers.
Reference: <author> Williamson, J. R. </author> <year> (1996). </year> <title> Gaussian ARTMAP: a neural network for fast incremental learning of noisy multidimensional maps. </title> <booktitle> Neural Networks, </booktitle> <volume> 9(5), </volume> <pages> 881-897. </pages>
Reference-contexts: One example of ART-based network that exploits a Gaussian mixture model of the input space and provides an a posteriori probability estimate iff class conditional likelihood satisfies a (S)ART-based bidirectional vigilance test is the Gaussian ARTMAP (GAM) model <ref> (Williamson, 1996, 1997) </ref>. 7 Fuzzy SART Two SART implementations can be found in the literature, employing a hard (WTA) and a soft competitive learning strategy respectively (Baraldi & Parmiggiani, 1995a, 1995b). <p> In agreement with theoretical expectations (see Section 3.2), the soft competitive version performed better than the hard competitive one (Baraldi & Parmiggiani, 1995b). This development is quite similar to that regarding GAM, which was originally proposed as a hard competitive incremental algorithm <ref> (Williamson, 1996) </ref>, then as a soft competitive (distributed learning) incremental algorithm (Williamson, 1997). In this section we present a soft competitive SART implementation, termed Fuzzy SART, based on recommendations suggested in Section 3.3 to improve Fuzzy ART.
Reference: <author> Williamson, J. R. </author> <year> (1997). </year> <title> A constructive, incremental-learning network for mixture modeling and classification. </title> <journal> Neural Computation, </journal> <volume> 9, </volume> <pages> 1517-1543. xlii </pages>
Reference-contexts: This development is quite similar to that regarding GAM, which was originally proposed as a hard competitive incremental algorithm (Williamson, 1996), then as a soft competitive (distributed learning) incremental algorithm <ref> (Williamson, 1997) </ref>. In this section we present a soft competitive SART implementation, termed Fuzzy SART, based on recommendations suggested in Section 3.3 to improve Fuzzy ART. This also means that Fuzzy SART combines SART architecture with probabilistic and possibilistic fuzzy membership functions as outlined in Section 6.4. <p> Although the presence of PE-based variables has never been stressed in the development of clustering algorithms featuring a fixed number of units, e.g., SOM and NG, it has been employed in Kohonen-based growing networks (Fritzke, 1994, 1995), as well as in GAM <ref> (to estimate priors, Williamson, 1997) </ref>. Activation function.
References-found: 52

