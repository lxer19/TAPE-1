URL: http://www.cs.wustl.edu/cs/techreports/1992/wucs-92-26.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Email: sg@cs.wustl.edu  dmath@cs.wustl.edu  
Title: Learning k-term DNF Formulas with an Incomplete Membership Oracle  
Author: Sally A. Goldman H. David Mathias 
Note: This research was supported in part by NSF grant CCR-9110108 and by a GE foundation Junior Faculty Grant. An earlier version appears in the Proceedings of the Fifth Annual Workshop on Computational Learning Theory,  
Date: July 21, 1992  July 1992.  
Address: St. Louis, MO 63130  St. Louis, MO 63130  
Affiliation: Department of Computer Science Washington University  Department of Computer Science Washington University  
Pubnum: WUCS-92-26  
Abstract: We consider the problem of learning k-term DNF formulas using equivalence queries and incomplete membership queries as defined by Angluin and Slonim. We demonstrate that this model can be applied to non-monotone classes. Namely, we describe a polynomial-time algorithm that exactly identifies a k-term DNF formula with a k-term DNF hypothesis using incomplete membership queries and equivalence queries from the class of DNF formulas. 
Abstract-found: 1
Intro-found: 1
Reference: [AP90] <author> H. Aizenstein and L. Pitt. </author> <title> Exact learning of read-twice DNF formulas. </title> <booktitle> In Proceedings of the Thirty Second Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 170-179, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recently much research has been directed at understanding the importance of membership queries in obtaining efficient learning algorithms. Some concept classes known to be learnable using membership and equivalence queries are deterministic finite automata [Ang87b], read-once formulas over various bases [AHK89, HH91], read-twice DNF formulas <ref> [AP90, Han91] </ref>, and Horn sentences [AFP90]. Furthermore, using An-gluin's method of approximate fingerprints [Ang90], it can be shown that membership queries are necessary to efficiently learn these classes.
Reference: [AHP92] <author> H. Aizenstein, L. Hellerstein, and L. Pitt. </author> <title> Read-thrice DNF is hard to learn with membership and equivalence queries. </title> <booktitle> To appear in Proceedings of the Thirty Third Annual Symposium on Foundations of Computer Science, </booktitle> <month> Oc-tober </month> <year> 1992. </year>
Reference-contexts: In contrast to these classes in which membership queries are useful, Angluin and Kharitonov [AK91] have shown that, under cryptographic assumptions, membership queries do not help in learning DNF formulas (with an unbounded number of terms), read-thrice formulas, NFA's and CFG's. Recently, Aizenstein, Hellerstein and Pitt <ref> [AHP92] </ref> have shown that, assuming NP 6= co-NP, no polynomial-time membership and equivalence query algorithm exists for exactly identifying read-thrice DNF formulas using hypotheses of read-thrice DNF formulas.
Reference: [Ang87a] <author> D. Angluin. </author> <title> Learning k-term DNF formulas using queries and counterexamples. </title> <type> Technical Report YALEU/DCS/RR-559, </type> <institution> Yale University, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: In fact, Pitt and Valiant [PV88] have shown that for k 2, the class of k-term DNF formulas cannot be exactly identified, with a k-term DNF result, in polynomial time using only equivalence queries if P 6= N P . In contrast to this representational hardness result, Angluin <ref> [Ang87a] </ref> has given an O (n k 2 ) algorithm for learning k-term DNF formulas that constructs a k-term DNF formula logically equivalent to the target formula using equivalence and membership queries. <p> class provides the first non-monotone class that is learnable in polynomial time with incomplete membership queries yet cannot be learned in polynomial time without the membership queries unless P = N P . 3 2 Definitions We begin by formally describing the model of learning from membership and equivalence queries <ref> [Ang87a] </ref>. The learner must infer an unknown target concept h fl chosen from some known concept class C. <p> This situation could occur because v satisfied t 1 and t 2 , or it could be that these terms were turned on when moving through the lattice. To deal with this difficulty we use a discriminant as originally defined by Angluin <ref> [Ang87a] </ref>. Informally, a discriminant provides a mechanism for focusing on a single term of the target formula. 4.1 Discriminants We now formally describe the discriminant. These definitions are taken from Angluin's paper. Let I k = f (i; j) j 1 i 6= j kg. <p> Angluin has shown that for all nonredundant DNF formulas, a valid discriminant exists. 9 4.2 The Basic Framework In this section we describe the basic framework of our algorithm. Then, in the next section, we describe how to deal with incomplete membership queries. Using complete membership queries Angluin <ref> [Ang87a] </ref> has described an algorithm for representationally exactly identifying the class of k-term DNF formulas. Her algorithm builds on the following algorithm for learning monomials. Let v be the counterexample returned when making an equivalence query with the empty hypothesis. <p> Proof: We first bound the number of incomplete membership and equivalence queries used. Angluin <ref> [Ang87a] </ref> showed that there are k (2n) k (k1) possible discriminants to consider. Thus, we need just multiply this quantity by the number of equivalence and incomplete membership queries made by each call to build-formula.
Reference: [Ang87b] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Recently much research has been directed at understanding the importance of membership queries in obtaining efficient learning algorithms. Some concept classes known to be learnable using membership and equivalence queries are deterministic finite automata <ref> [Ang87b] </ref>, read-once formulas over various bases [AHK89, HH91], read-twice DNF formulas [AP90, Han91], and Horn sentences [AFP90]. Furthermore, using An-gluin's method of approximate fingerprints [Ang90], it can be shown that membership queries are necessary to efficiently learn these classes.
Reference: [Ang88] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: The learnability of DNF formulas is an important problem, and thus it is crucial to explore ways to make learning algorithms for this class robust against noise. There are several known algorithms for learning DNF formulas with a restricted number of terms using complete membership queries. Angluin <ref> [Ang88] </ref> has shown that Valiant's algorithm for learning k-CNF formulas in the PAC model can be applied to obtain a O (n k ) algorithm to exactly identify k-CNF formulas using only equivalence queries.
Reference: [Ang90] <author> D. Angluin. </author> <title> Negative results for equivalence queries. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 121-150, </pages> <year> 1990. </year>
Reference-contexts: Some concept classes known to be learnable using membership and equivalence queries are deterministic finite automata [Ang87b], read-once formulas over various bases [AHK89, HH91], read-twice DNF formulas [AP90, Han91], and Horn sentences [AFP90]. Furthermore, using An-gluin's method of approximate fingerprints <ref> [Ang90] </ref>, it can be shown that membership queries are necessary to efficiently learn these classes.
Reference: [AFP90] <author> D. Angluin, M. Frazier, and L. Pitt. </author> <title> Learning conjunctions of horn clauses. </title> <booktitle> In Proceedings of the Thirty First Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 186-192, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Some concept classes known to be learnable using membership and equivalence queries are deterministic finite automata [Ang87b], read-once formulas over various bases [AHK89, HH91], read-twice DNF formulas [AP90, Han91], and Horn sentences <ref> [AFP90] </ref>. Furthermore, using An-gluin's method of approximate fingerprints [Ang90], it can be shown that membership queries are necessary to efficiently learn these classes.
Reference: [AHK89] <author> D. Angluin, L. Hellerstein, and M. Karpinski. </author> <title> Learning read-once formulas with queries. </title> <type> Technical Report UCB/CSD 89/528, </type> <institution> University of California Berkeley Computer Science Division, </institution> <year> 1989. </year>
Reference-contexts: 1 Introduction Recently much research has been directed at understanding the importance of membership queries in obtaining efficient learning algorithms. Some concept classes known to be learnable using membership and equivalence queries are deterministic finite automata [Ang87b], read-once formulas over various bases <ref> [AHK89, HH91] </ref>, read-twice DNF formulas [AP90, Han91], and Horn sentences [AFP90]. Furthermore, using An-gluin's method of approximate fingerprints [Ang90], it can be shown that membership queries are necessary to efficiently learn these classes.
Reference: [AK91] <author> D. Angluin and M. Kharitonov. </author> <booktitle> When won't membership queries help? In Proceedings of the Twenty-Third Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 444-454, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Furthermore, using An-gluin's method of approximate fingerprints [Ang90], it can be shown that membership queries are necessary to efficiently learn these classes. In contrast to these classes in which membership queries are useful, Angluin and Kharitonov <ref> [AK91] </ref> have shown that, under cryptographic assumptions, membership queries do not help in learning DNF formulas (with an unbounded number of terms), read-thrice formulas, NFA's and CFG's.
Reference: [AS91] <author> D. Angluin and D. </author> <title> Slonim. Learning monotone DNF with an incomplete membership oracle. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 139-146, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Since the membership queries made by their algorithm are randomly selected in order to simulate a particular distribution, the noise can be handled by just making additional queries. While this is an appealing model, in general, it appears to be very difficult to work with. Angluin and Slonim <ref> [AS91] </ref> introduced a model of incomplete membership queries in which each membership query is answered "I don't know" with probability p. Here too, this information is persistent|repeatedly making a query that was answered with "I don't know" always results in an "I don't know" answer. <p> A positive counterexample v is one for which h fl (v) = 1 but h (v) = 0. Likewise, a negative counterexample is one for which h fl (v) = 0 but h (v) = 1. We now describe the model of incomplete membership queries <ref> [AS91] </ref>. An incomplete membership oracle (IMQ) is identical to a complete membership oracle except that it answers "I don't know" to some subset of the membership queries. <p> More specifically, we give a technique for which the expectation is that after four positive counterexamples for t the set of candidate terms for t will include t. Then we need just distinguish t from the other candidates in the set. As in Angluin and Slonim <ref> [AS91] </ref> we view the sample space as a lattice, with componentwise "or" and "and" as the lattice operators. The top element is the vector f1g n and the bottom element is the vector f0g n . <p> Proof: This lemma follows immediately from the observation that for every positive counterexample v, t i (v [L fli ]) = 1 for some new term t i of h fl , and Lemmas 3 and 4 of Angluin and Slonim <ref> [AS91] </ref>. There is a dual result for search-for-min2. In order to apply Lemma 2 it is essential that when either search-for-min2 or search-for-max2 is called, no previously queried positive vectors are encountered. That is, either search-for-min2 or search-for-max2 must have a clear path. <p> Like the algorithm given by Angluin and Slonim <ref> [AS91] </ref>, learn-kterm-dnf can be modified to handle persistent false negative errors in answers to the membership queries. In this model all negative instances are correctly answered by the membership oracle but each positive instance, with probability p, is answered "no" by the first membership query made for it.
Reference: [Blu92] <author> A. Blum. </author> <type> Personal Communication. </type>
Reference-contexts: In contrast to this representational hardness result, Angluin [Ang87a] has given an O (n k 2 ) algorithm for learning k-term DNF formulas that constructs a k-term DNF formula logically equivalent to the target formula using equivalence and membership queries. Also, by modifying the Blum and Rudich algorithm <ref> [Blu92] </ref> one can obtain an algorithm that builds a k-term DNF formula logically equivalent to the target with an expected running time of O (n 2 O (k 2 ) ).
Reference: [BR92] <author> A. Blum and S. Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <booktitle> In Proceedings of the Twenty-Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-389, </pages> <month> May </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: Also, Blum and Singh [BS90] give an alternate technique to learn k-term DNF formulas in O (n k ) time using only equivalence queries from the class of general DNF formulas. More recently, Blum and Rudich <ref> [BR92] </ref> have given an algorithm with O expected running time for 2 learning k-term DNF formulas using equivalence queries from the class of general DNF formulas and complete membership queries. <p> In particular, it may be possible to use the techniques of Blum and Rudich <ref> [BR92] </ref> to obtain such a result. More generally, it would be interesting to determine whether some of the other classes that are known to be learnable using equivalence queries and complete membership queries are still efficiently learnable under the model of incomplete membership queries.
Reference: [BS90] <author> A. Blum and M. Singh. </author> <title> Learning functions of k terms. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-153, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Thus one can exactly identify k-term DNF formulas in O (n k ) time using the representation class of k-CNF formulas with only equivalence queries. Also, Blum and Singh <ref> [BS90] </ref> give an alternate technique to learn k-term DNF formulas in O (n k ) time using only equivalence queries from the class of general DNF formulas.
Reference: [GKS90] <author> S. Goldman, M. Kearns, and R. Schapire. </author> <title> Exact identification of circuits using fixed points of amplification functions. </title> <booktitle> In Proceedings of the Thirty First Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 193-202, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: However, one would expect that if a teacher erroneously answers a question, this incorrect answer will be given whenever the question is asked. Goldman, Kearns, and Schapire <ref> [GKS90] </ref> consider a model in which the noise in the membership queries is persistent. For each instance v, when first queried, the true 1 output of the target concept is computed and is reversed with probability p. This answer is then repeated for all future queries on v.
Reference: [Han91] <author> T. Hancock. </author> <title> Learning 2 DNF formulas and k decision trees. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 199-209, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recently much research has been directed at understanding the importance of membership queries in obtaining efficient learning algorithms. Some concept classes known to be learnable using membership and equivalence queries are deterministic finite automata [Ang87b], read-once formulas over various bases [AHK89, HH91], read-twice DNF formulas <ref> [AP90, Han91] </ref>, and Horn sentences [AFP90]. Furthermore, using An-gluin's method of approximate fingerprints [Ang90], it can be shown that membership queries are necessary to efficiently learn these classes.
Reference: [HH91] <author> T. Hancock and L. Hellerstein. </author> <title> Learning read-once formulas over fields and extended bases. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 326-336, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recently much research has been directed at understanding the importance of membership queries in obtaining efficient learning algorithms. Some concept classes known to be learnable using membership and equivalence queries are deterministic finite automata [Ang87b], read-once formulas over various bases <ref> [AHK89, HH91] </ref>, read-twice DNF formulas [AP90, Han91], and Horn sentences [AFP90]. Furthermore, using An-gluin's method of approximate fingerprints [Ang90], it can be shown that membership queries are necessary to efficiently learn these classes.
Reference: [PV88] <author> L. Pitt and L. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: All of the algorithms mentioned above for learning k-term DNF formulas do not obtain a formula that exactly identifies the target formula in a representational sense (i.e. the final hypothesis comes from a more general representation class). In fact, Pitt and Valiant <ref> [PV88] </ref> have shown that for k 2, the class of k-term DNF formulas cannot be exactly identified, with a k-term DNF result, in polynomial time using only equivalence queries if P 6= N P .
Reference: [Sak90] <author> Y. Sakakibara. </author> <title> On learning from queries and counterexamples in the presence of noise. </title> <journal> Information Processing Letters. </journal> <note> To appear. 24 </note>
Reference-contexts: Likewise, if one views membership queries as experiments performed by the learner, then in any practical application the learner must be able to handle inconclusive results from the experiments. Some research has been directed at understanding noise in membership queries. Sakakibara <ref> [Sak90] </ref> shows that if each membership query is erroneously answered independently at random, then an algorithm designed to work with noise-free membership queries can simply repeat each query sufficiently often so that, with high probability, the majority vote is correct.
References-found: 18

