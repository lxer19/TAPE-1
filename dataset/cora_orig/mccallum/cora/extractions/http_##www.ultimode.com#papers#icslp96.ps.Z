URL: http://www.ultimode.com/papers/icslp96.ps.Z
Refering-URL: http://www.ultimode.com/stevew/papers.html
Root-URL: 
Email: email: fsrw1001, djk, ajrg@eng.cam.ac.uk  
Phone: Tel: [+44] 1223 332754 Fax: [+44] 1223 332662  
Title: SMOOTHED LOCAL ADAPTATION OF CONNECTIONIST SYSTEMS  
Author: Steve Waterhouse Dan Kershaw Tony Robinson 
Address: Cambridge CB2 1PZ, England.  
Affiliation: Cambridge University Engineering Department,  
Abstract: abbot is the hybrid connectionist hidden Markov model (HMM) large vocabulary continuous speech recognition system developed at Cambridge University Engineering Department. abbot makes effective use of the linear input network (LIN) adaptation technique to achieve speaker and channel adaptation. Although the LIN is effective at adapting to new speakers or a new environment (e.g. a different microphone), the transform is global over the input space. In this paper we describe a technique by which the transform may be made locally linear over different regions of the input space. The local linear transforms are combined by an additional network using a non-linear transform. This scheme falls naturally into the mixtures of experts framework. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D.J. Kershaw, A.J. Robinson, S.J.Renals, and M.M. Hochberg. </author> <title> The 1995 ABBOT LVCSR System. </title> <booktitle> In The ARPA Speech Recognition Workshop, </booktitle> <address> Arden House, Har-riman, New York, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: The adaptation process typically converges after 2 or 3 iterations. For the multiple unknown microphones task (H3-P0), the word error rate is reduced by 18.1%, while for the clean speech task (H3-C0) the reduction is 11.2% <ref> [1] </ref>. 3. MLIN : mixtures of linear input networks for adaptation The mixture of experts architecture consists of a set of "experts" which perform local function approximation. <p> This data set contains 20 speakers with 15 sentences each. The 1995 abbot system used the 60,000 word vocabulary and standard trigram generated by CMU throughout the evaluation. The pronunciation lexicon was derived primarily from a lexicon supplied by LIMSI-CNRS and expanded to cover the 60,000 word vocabulary <ref> [1] </ref>. In this section the method used for unsupervised block adaptation is the same method as described in Section 2. Results are reported for various numbers of experts and numbers of passes of adaptation at both the frame and word level. 4.1.
Reference: 2. <author> C J Leggetter and P C Woodland. </author> <title> Maximum Likelihood Linear Regression for Speaker Adaptation of Continuous Density HMMs. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 9 </volume> <pages> 171-186, </pages> <year> 1995. </year>
Reference-contexts: Although the LIN scheme has proved successful at reducing the word error rate (WER) within both speaker and environmental adaptation, the method is suboptimal since it is a global transform of the input. Local adaptation has proved successful in HMM adaptation <ref> [2] </ref> in which the local regions are selected according to sets of similar phone classes. The selection of local regions based on similar phone classes is not possible with the recurrent network, because the RNN is a dynamical system requiring a continuous input stream. <p> At this point the error is back-propagated through the RNN. Note that the RNN weights are kept fixed, and only the LIN's weights are updated. This process can be likened to a global maximum likelihood linear regression (MLLR) <ref> [2] </ref>, except that in MLLR the HMM parameters are adjusted. Even with few parameters (182), the LIN performs a very effective transformation of the input space.
Reference: 3. <author> J. Neto, L. Almeida, M.M. Hochberg, C. Martins, L. Nunes, S.J. Renals, and A.J. Robinson. </author> <title> Speaker-Adaptation for Hybrid HMM-ANN Continuous Speech Recognition System. </title> <booktitle> In Eurospeech, </booktitle> <pages> pages 2171-4, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: The posterior probabilities are mapped to scaled likelihoods for use in the HMM decoding process. The linear input network has proved to be a successful method for the adaptation of connectionist systems to a new speaker <ref> [3] </ref>. This technique has recently been successfully extended to the adaptation of a recurrent network to an unknown microphone. <p> Even with few parameters (182), the LIN performs a very effective transformation of the input space. On the ARPA Resource Management speaker dependent corpus for supervised speaker adaptation, the LIN achieved a 23.8% reduction in word error rate when using 100 adaptation sentences <ref> [3] </ref>. For the 1995 ARPA Hub 3 multiple unknown microphone (MUM) evaluations [4], unsupervised block adaptation is performed over each known speaker session, to adapt to unknown speakers, noise and channel conditions over the session.
Reference: 4. <author> D.S. Pallett, J.G. Fiscus, W.M. Fisher, J.S. Garofolo, A.F. Martin, and M.A. Przybocki. </author> <title> 1995 HUB-3 NIST Multiple Microphone Corpus Benchmark Tests. </title> <booktitle> In ARPA, editor, The ARPA Speech Recognition Workshop, </booktitle> <address> Arden House, Harriman, New York, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: On the ARPA Resource Management speaker dependent corpus for supervised speaker adaptation, the LIN achieved a 23.8% reduction in word error rate when using 100 adaptation sentences [3]. For the 1995 ARPA Hub 3 multiple unknown microphone (MUM) evaluations <ref> [4] </ref>, unsupervised block adaptation is performed over each known speaker session, to adapt to unknown speakers, noise and channel conditions over the session. A Viterbi alignment using the current model is carried out on decoded utterance hypotheses of a session, to label the acoustic frames.
Reference: 5. <author> S. Renals and M. Hochberg. </author> <title> Efficient search using posterior phone probability estimates. </title> <booktitle> In Proc. ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 596-599, </pages> <address> Detroit, </address> <year> 1995. </year>
Reference-contexts: The computational cost of training is increased slightly however, due to the need to back-propagate the error terms through each copy of the RNN. 4. RESULTS This section reports decoding results for the ARPA 1995 H3 multiple unknown microphones (MUM) Task. Decoding is performed by the abbot decoder, noway <ref> [5] </ref>, which uses a modified stack decoding algorithm, with a pruning strategy that is well matched to the hybrid connectionist-HMM approach. The subsection of the H3 task chosen for this paper was H3:C0, which is the 1995 H3 MUM unlimited vocabulary test recorded using a Sennheiser microphone.
Reference: 6. <author> Tony Robinson, Mike Hochberg, and Steve Renals. </author> <title> The use of recurrent networks in continuous speech recognition. </title> <editor> In C. H. Lee, K. K. Paliwal, and F. K. Soong, editors, </editor> <title> Automatic Speech and Speaker Recognition Advanced Topics, chapter 19. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: The gate weight matrix is initialised using random values between 0:1 and 0:1. This randomisation ensures symmetry breaking and prevents the experts all learn ing the same transform. Optimisation of the experts and gate is done via the scheme described in <ref> [6] </ref>, with individual weight rate terms for each network. It is important to note that the MLIN does not increase the number of parameters significantly, since the RNN is effectively tied across the different experts.
Reference: 7. <author> S. R. Waterhouse and A. J. Robinson. </author> <title> Classification using hierarchical mixtures of experts. </title> <booktitle> In IEEE Workshop on Neural Networks for Signal Processing, </booktitle> <pages> pages 177-186, </pages> <year> 1994. </year>
Reference-contexts: The expert outputs y i (t) are combined with the outputs g i (t) of a "gate" to form the overall output y (t) = i In the case of classification, the experts compute vectors of class conditional probabilities <ref> [7] </ref>. In Figure 2, we show the mixture of linear input networks (MLIN) architecture. Each expert consists of a LIN and a recurrent network.
References-found: 7

