URL: http://www.cs.indiana.edu/l/www/pub/leake/leake/p-91-01.ps.Z
Refering-URL: http://www.cs.indiana.edu/l/www/pub/leake/leake/
Root-URL: http://www.cs.indiana.edu
Email: leake@cs.indiana.edu  
Title: Goal-based Explanation Evaluation 1  
Author: David B. Leake David B. Leake, 
Address: 101 Lindley Hall Bloomington, IN 47405  Bloomington, IN 47405-4101.  
Affiliation: Computer Science Department Indiana University  Computer Science Department, Indiana University,  
Date: 4, 1991  
Note: This article appears in Cognitive Science, Volume 15, Number  Correspondance and requests for reprints should be sent to  
Abstract: 1 I would like to thank my dissertation advisor, Roger Schank, for his very valuable guidance on this research, and to thank the Cognitive Science reviewers for their helpful comments on a draft of this paper. The research described here was conducted primarily at Yale University, supported in part by the Defense Advanced Research Projects Agency, monitored by the Office of Naval Research under contract N0014-85-K-0108 and by the Air Force Office of Scientific Research under contract F49620-88-C-0058. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ahn, W., Mooney, R., Brewer, W., & DeJong, G. </author> <year> (1987). </year> <title> Schema acquisition from one example: Psychological evidence for explanation-based learning. </title> <booktitle> Proceedings of the Ninth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 50-57). </pages> <address> Seattle: </address> <publisher> Cognitive Science Society. </publisher>
Reference-contexts: In addition, recent experiments have supported some of these explanation-based processes as psychological models <ref> (Ahn et al., 1987) </ref>. However, the benefits of explanation-based processing depend on the explanations taken as starting point. In real-world situations, many candidate explanations may be available for a single event, prompting the question of how to select the explanation on which to base further processing.
Reference: <author> Collins, G., & Birnbaum, L. </author> <year> (1988). </year> <title> An explanation-based approach to the transfer of planning knowledge across domains. </title> <booktitle> Proceedings of the 1988 AAAI Spring Symposium on Explanation-based Learning (pp. </booktitle> <pages> 107-111). </pages> <publisher> Stanford: AAAI. </publisher>
Reference: <author> Cullingford, R. </author> <year> (1978). </year> <title> Script Application: Computer Understanding of Newspaper Stories (Technical Report 116). </title> <institution> New Haven: Yale University Computer Science Department. </institution>
Reference-contexts: These schemas are represented in ACCEPTER's memory as memory organization packets (MOPs) (Schank, 1982). ACCEPTER processes stories one fact at a time, updating its beliefs and generating expectations for later inputs from the story. For this routine understanding, ACCEPTER uses schema-based understanding process modelled on <ref> (Cullingford, 1978) </ref>, integrating new 15 information into a dynamic memory (Schank, 1982; Lebowitz, 1980; Kolodner, 1984). As it integrates input facts into memory, it checks for anomalies| conflicts between the inputs and its beliefs or expectations.
Reference: <author> DeJong, G., & Mooney, R. </author> <year> (1986). </year> <title> Explanation-based learning: An alternative view. </title> <booktitle> Machine Learning, </booktitle> <address> 1 (1),145-176. Boston: </address> <publisher> Kluwer. </publisher>
Reference-contexts: Criteria used for judging the op-erationality of a concept formulation range from static annotation of predicates with their ease of evaluation (Mitchell et al., 1986), to dynamic techniques based on system knowledge <ref> (DeJong & Mooney, 1986) </ref>, to techniques directly reflecting the utility of an explanation, using estimates and actual measurements of recognition cost versus benefit to the system (Minton, 1988). <p> We did not investigate dynamic characterizations of all dimensions, simply because our main effort was devoted to investigating the relationship between evaluation goals and the needed dimensions. However, we strongly agree with the arguments in <ref> (DeJong & Mooney, 1986) </ref> that such criteria must be able to dynamically take into account current system knowledge. Richer and more dynamic characterizations of ACCEPTER's dimensions involve many issues for future research.
Reference: <author> Dietterich, T., & Flann, N. </author> <year> (1988). </year> <title> An inductive approach to solving the imperfect theory problem. </title> <booktitle> Proceedings of the 1988 AAAI Spring Symposium on Explanation-based Learning (pp. </booktitle> <pages> 42-46). </pages> <publisher> Stanford: AAAI. </publisher>
Reference-contexts: However, if we seek to explain real-world events, no explanation can include all the causally-relevant factors. As Mitchell et al. observe, real-world domain theories are often both incomplete and intractable. Responses to the imperfect theory problem propose methods for repairing the theory's defects. For example, <ref> (Dietterich & Flann, 1988) </ref> suggests that when a domain theory allows multiple incompatible explanations, induction over explanations for a set of training examples can be used to find a specialized domain theory that explains the positive examples, but none of the negative ones. (Rajamoney, 1988) advocates experimentation to determine how to
Reference: <author> Granger, R. </author> <year> (1980). </year> <title> Adaptive Understanding: </title> <type> Correcting Erroneous Inferences (Technical Report 171). </type> <institution> New Haven: Yale University Computer Science Department. </institution>
Reference-contexts: For example, research on explanation in story understanding has relied primarily on fixed structural criteria for choosing between competing explanations, such as favoring explanations involving short explanatory chains, or favoring explanations with the most structural coherence (e.g., <ref> (Granger, 1980) </ref>, (Wilensky, 1983), (Ng & Mooney, 1990)). 2 However, the examples in our introduction show that validity alone is not enough to assure an explanation's goodness: an explanation may be valid without being useful.
Reference: <author> Hammond, K. </author> <year> (1989). </year> <title> Case-Based Planning: Viewing Planning as a Memory Task. </title> <address> San Diego: </address> <publisher> Academic Press. </publisher>
Reference-contexts: 1 Introduction The use of explanation is central to theories in many areas of artificial intelligence, such as text understanding (e.g. (Granger, 1980; Schank, 1982; Wilensky, 1983; Schank, 1986; Hobbs et al., 1990)), plan repair and indexing (e.g., <ref> (Hammond, 1989) </ref>), and guiding generalization (e.g., (Mitchell et al., 1986; DeJong & Mooney, 1986)). In addition, recent experiments have supported some of these explanation-based processes as psychological models (Ahn et al., 1987). However, the benefits of explanation-based processing depend on the explanations taken as starting point. <p> This procedure is basically the anticipate and avoid strategy suggested by Hammond for avoiding failures in case-based planning <ref> (Hammond, 1989) </ref>. We discuss below some heuristics for judging blockability, and how they are applied to judge explanations for anticipating and avoiding an undesirable outcome. Blockability: Deciding what an explainer can prevent is difficult; things that seem uncontrollable at first glance may actually be easy to influence.
Reference: <author> Hanson, N. </author> <year> (1961). </year> <title> Patterns of Discovery. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: how the cause of death might have been set out by a physician as `multiple haemorrhage', by the barrister as `negligence on the part of the driver', by a carriage-builder as `a defect in the brakeblock construction', by a civic planner as `the presence of tall shrubbery at that turning'. <ref> (Hanson, 1961, page 54) </ref> Likewise, Mackie (1965) discusses the need of explanations to aid in making the distinctions important to a particular explainer. In this view, explanation is conducted against the causal field of situations to be distinguished by the explanation, and varies with that field.
Reference: <author> Heider, F. </author> <year> (1958). </year> <title> The Psychology of Interpersonal Relations, </title> <booktitle> volume XV of Current Theory and Research in Motivation. </booktitle> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: Nevertheless, the influence of context on explanation has received little study in psychology and artificial intelligence. In psychology, the central current for research on people's choice of explanations is attribution theory <ref> (Heider, 1958) </ref>, which generally accounts for choice of explanations without reference to what motivated the explanation effort. <p> Finally we argue that the effectiveness of any explanation-based processing depends on the ability to assure that explanations satisfy the needs for information that arise from system goals. 3 3 Previous perspectives 3.1 Psychological approaches Much psychological research on choice of explanations originated in Heider's seminal work on attribution theory <ref> (Heider, 1958) </ref>.
Reference: <author> Hobbs, J., Stickel, M., Appelt, D., & Martin, P. </author> <year> (1990). </year> <type> Interpretation as abduction (Technical Report 499). </type> <institution> Menlo Park: SRI International. </institution>
Reference: <author> Hunter, L. </author> <year> (1990). </year> <title> Planning to learn. </title> <booktitle> Proceedings of the Twelfth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 261-268). </pages> <address> Cambridge: </address> <publisher> Cognitive Science Society. 44 Kass, A. </publisher> <year> (1986). </year> <title> Modifying explanations to understand stories. </title> <booktitle> Proceedings of the Eighth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 691-696). </pages> <address> Amherst: </address> <publisher> Cognitive Science Society. </publisher>
Reference-contexts: Richer and more dynamic characterizations of ACCEPTER's dimensions involve many issues for future research. For example, a richer characterization of knowability criteria would have to involve reasoning about competing ways to gather information, their likely costs, and their chance of success; <ref> (Hunter, 1990) </ref> discusses some of these directions in the context of knowledge planning. 8 The value of goal-based evaluation The preceding sections sketched our approach to goal-based explanation evaluation.
Reference: <author> Kass, A., & Leake, D. </author> <year> (1988). </year> <title> Case-based reasoning applied to constructing explanations. </title>
Reference: <editor> Kolodner, J. (Ed.), </editor> <booktitle> Proceedings of the Case-Based Reasoning Workshop (pp. </booktitle> <pages> 190-208). </pages> <address> Clearwater Beach: </address> <publisher> DARPA. </publisher>
Reference: <author> Kedar-Cabelli, S. </author> <year> (1987). </year> <title> Formulating concepts according to purpose. </title> <booktitle> Proceedings of the Sixth Annual National Conference on Artificial Intelligence (pp. </booktitle> <pages> 477-481). </pages> <address> Seattle: </address> <publisher> AAAI. </publisher>
Reference: <author> Keller, R. </author> <year> (1987). </year> <title> The Role of Explicit Contextual Knowledge in Learning Concepts to Improve Performance. </title> <type> PhD thesis, </type> <institution> Rutgers University. </institution>
Reference-contexts: Thus in order to understand operationality, we need to consider the wide variety of tasks for which explanations are used. Keller's program MetaLEX <ref> (Keller, 1987) </ref> reflects the dynamic nature of operationality by not relying on fixed operationality criteria. Instead, its operationality criterion is an input to the system, and can reflect current goals and goal priorities.
Reference: <author> Keller, R. </author> <year> (1988). </year> <title> Operationality and generality in explanation-based learning: </title> <booktitle> Separate dimensions or opposite endpoints? Proceedings of the 1988 AAAI Spring Symposium on Explanation-based Learning (pp. </booktitle> <pages> 153-157). </pages> <publisher> Stanford: AAAI. </publisher>
Reference-contexts: Because operationality has been studied for so few tasks, there has been some tendency to assume that such properties apply to operationality judgements for any task. However, <ref> (Keller, 1988) </ref> points out that existence of an operationality/generality tradeoff depends on specific assumptions underlying the recognition task, and that the tradeoff does not exist for every use of explanations| a more general explanation may actually be more operational as well.
Reference: <author> Kelley, H. H. </author> <year> (1967). </year> <title> Attribution theory in social psychology. </title> <editor> Levine, D. (Ed.), </editor> <booktitle> Nebraska Symposium on Motivation. </booktitle> <institution> Lincoln, NE: University of Nebraska Press. </institution>
Reference-contexts: its actor, or features of the environment. (Most work in attribution theory assumes that either personal or situational factors will apply, but not both.) One important result is Kelley's covariation principle, which gives a hypothesis for how people make the decision between attributing an outcome to personal or situational factors <ref> (Kelley, 1967) </ref>. The covariation principle suggests that people look at covariation across different time, people, and other entities in order to decide which type of factor applies.
Reference: <author> Kolodner, J. </author> <year> (1984). </year> <title> Retrieval and Organizational Strategies in Conceptual Memory. </title> <address> Hills-dale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Lalljee, M., & Abelson, R. </author> <year> (1983). </year> <title> The organization of explanations. </title> <editor> Hewstone, M. (Ed.), </editor> <title> Attribution Theory: Social and Functional Extensions. </title> <publisher> Oxford: Blackwell. </publisher>
Reference-contexts: This problem was pointed out by Lalljee and Abelson, who observe that people would usually try to find a more specific reason for the dislike <ref> (Lalljee & Abelson, 1983) </ref>. For example, someone who had invited John to the movie, expecting him to like it, would probably try to determine John's particular objections. The more specific information is more useful: it allows predicting John's reaction next time, to avoid inviting him to another inappropriate movie.
Reference: <author> Lalljee, M., Watson, M., & White, P. </author> <year> (1982). </year> <title> Explanations, attributions, and the social context of unexpected behavior. </title> <journal> European Journal of Social Psychology, </journal> <volume> 12 ,17-29. </volume>
Reference-contexts: Attribution theory also fails to consider the effect of context on preferences for explanations. The covariation principle does not take an explainer's prior expectations into account, or the reason for explaining. However, <ref> (Lalljee et al., 1982) </ref> shows that the explanations people seek, rather than being determined by abstract criteria, vary with circumstances: unexpected behavior requires more complex explanations than expected behavior, and is likely to require more of both situational and personal elements. 4 A knowledge structure approach: Lalljee and Abelson question attribution
Reference: <author> Leake, D. </author> <year> (1988a). </year> <title> Evaluating explanations. </title> <booktitle> Proceedings of the Seventh National Conference on Artificial Intelligence (pp. </booktitle> <pages> 251-255). </pages> <address> Minneapolis: </address> <publisher> AAAI. </publisher>
Reference-contexts: We then describe the implementation of goal-based evaluation in a computer model, ACCEPTER, which illustrates our theory <ref> (Leake, 1988a, 1988b, 1989b, in press) </ref>.
Reference: <author> Leake, D. </author> <year> (1988b). </year> <title> Using explainer needs to judge operationality. </title> <booktitle> Proceedings of the 1988 AAAI Spring Symposium on Explanation-based Learning (pp. </booktitle> <pages> 148-152). </pages> <publisher> Stanford: AAAI. </publisher> <address> 45 Leake, D. </address> <year> (1989a). </year> <title> Anomaly detection strategies for schema-based story understanding. </title> <booktitle> Proceedings of the Eleventh Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 490-497). </pages> <address> Ann Arbor: </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Leake, D. </author> <year> (1989b). </year> <title> The effect of explainer goals on case-based explanation. Hammond, </title> <editor> K. (Ed.), </editor> <booktitle> Proceedings of the Case-Based Reasoning Workshop (pp. </booktitle> <pages> 290-294). </pages> <address> Pensacola Beach: </address> <publisher> DARPA. </publisher>
Reference-contexts: Consequently, unlike single-purpose systems that can tailor explanation construction to their goals, and rely on initial explanation construction to provide an appropriate type of explanation, a case-based explainer cannot be assured of starting with an explanation that includes appropriate information <ref> (Leake, 1989b) </ref>. Consequently, it must have a means for determining whether an explanation contains the specific information it needs, and to identify gaps to fill through explanation adaptation. Our evaluation process provides that guidance, enabling a case-based explainer to reliably use cases from a multipurpose case library.
Reference: <author> Leake, D. </author> <title> (in press). Evaluating Explanations. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Leake, D., & Owens, C. </author> <year> (1986). </year> <title> Organizing memory for explanation. </title> <booktitle> Proceedings of the Eighth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 710-715). </pages> <address> Amherst: </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Lebowitz, M. </author> <year> (1980). </year> <title> Generalization and Memory in an Integrated Understanding System (Technical Report 186). </title> <institution> New Haven: Yale University Computer Science Department. </institution>
Reference: <author> Mackie, J. </author> <year> (1965). </year> <title> Causes and conditions. </title> <journal> American Philosophical Quarterly, </journal> <volume> (4). </volume>
Reference: <author> Mehlman, R., & Snyder, C. </author> <year> (1983). </year> <title> Excuse theory: A test of the self-protective role of attributions. </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> 49 (4),994-1001. </volume>
Reference-contexts: Excuse theory studies how the desire to form excuses makes people manipulate the types of factors to use in attribution, to blame external influences for their own bad performance (for example, <ref> (Mehlman & Snyder, 1983) </ref> or (Snyder et al., 1983)). Excuse theory's demonstration that goals influence explanation is consistent with our view.
Reference: <author> Minton, S. </author> <year> (1988). </year> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> Proceedings of the seventh national conference on artificial intelligence (pp. </booktitle> <pages> 564-569). </pages> <address> Saint Paul: </address> <publisher> AAAI. </publisher>
Reference-contexts: concept formulation range from static annotation of predicates with their ease of evaluation (Mitchell et al., 1986), to dynamic techniques based on system knowledge (DeJong & Mooney, 1986), to techniques directly reflecting the utility of an explanation, using estimates and actual measurements of recognition cost versus benefit to the system <ref> (Minton, 1988) </ref>. Research on operationality for concept recognition has led to the identification of significant characteristics of operationality for that task, such as the operationality/generality tradeoff (e.g., (Segre, 1987)).
Reference: <author> Mitchell, T., Keller, R., & Kedar-Cabelli, S. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1 (1),47-80. </volume>
Reference-contexts: Criteria used for judging the op-erationality of a concept formulation range from static annotation of predicates with their ease of evaluation <ref> (Mitchell et al., 1986) </ref>, to dynamic techniques based on system knowledge (DeJong & Mooney, 1986), to techniques directly reflecting the utility of an explanation, using estimates and actual measurements of recognition cost versus benefit to the system (Minton, 1988). <p> However, hearts are specifications of internal organs, 26 and the physical state of internal organs is not usually observable, so not all the causes are observable| the explanation is not sufficient to allow prediction. (We note that this criterion is equivalent to past approaches for static operationality criteria <ref> (Mitchell et al., 1986) </ref>, and suffers from the same problems; we point towards an alternative direction in the summary section below.) Judging testability: Sometimes predicting an outcome is worth the effort of performing tests. <p> Unlike traditional explanation-based systems, which can rely on all candidate explanations being in the proper form for their single task (e.g., showing sufficient conditions for concept membership for the concept recognition task in <ref> (Mitchell et al., 1986) </ref>), a case-based explainer reuses explanations that may have been constructed in very different contexts, and for very different goals. <p> This guidance could also be applied to any explanation construction system that must build explanations for multiple purposes. Dealing with the imperfect theory problem: Explanation-based learning research traditionally considers explanations to be deductive proofs of concept membership <ref> (Mitchell et al., 1986) </ref>. In this framework, the structure of the proof assures that the explanation points to a sufficient set of factors for concept membership. However, if we seek to explain real-world events, no explanation can include all the causally-relevant factors.
Reference: <author> Mooney, R. </author> <year> (1990). </year> <title> A General Explanation-based Learning Mechanism and its Application to Narrative Understanding. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, research on explanation in story understanding has relied primarily on fixed structural criteria for choosing between competing explanations, such as favoring explanations involving short explanatory chains, or favoring explanations with the most structural coherence (e.g., (Granger, 1980), (Wilensky, 1983), <ref> (Ng & Mooney, 1990) </ref>). 2 However, the examples in our introduction show that validity alone is not enough to assure an explanation's goodness: an explanation may be valid without being useful. <p> Focusing explanation towards knowledge gaps: Our explanation process centers around explaining anomalies. In our discussion of evaluation for routine understanding, we described ACCEPTER's requirement that explanations account not only for the event, but for why expectations failed. This differs from approaches such as <ref> (Mooney, 1990) </ref>, which concentrate on accounting for why the event occurred. The difference is important to deciding which beliefs to repair, and what to learn from a new situation.
Reference: <author> Moore, J., & Swartout, W. </author> <year> (1989). </year> <title> A reactive approach to explanation. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1504-1510). </pages> <address> Detroit: </address> <note> IJCAI. 46 Mostow, </note> <author> D. J. </author> <year> (1983). </year> <title> Machine transformation of advice into a heuristic search procedure. </title>
Reference-contexts: just their decision paths, but the reasoning underlying those paths (Swartout, 1983); systems to devise explanations that are appropriate to the user's prior knowledge level (Paris, 1987); and systems that treat the explanation process as a continuing dialogue, allowing clarifications and elaborations to be offered in response to follow-up questions <ref> (Moore & Swartout, 1989) </ref>.
Reference: <editor> Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Cambridge: </address> <publisher> Tioga Publishing Company. </publisher>
Reference: <author> Ng, H., & Mooney, R. </author> <year> (1990). </year> <title> On the role of coherence in abductive explanation. </title> <booktitle> Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 337-342). </pages> <address> Boston: </address> <publisher> AAAI. </publisher>
Reference-contexts: For example, research on explanation in story understanding has relied primarily on fixed structural criteria for choosing between competing explanations, such as favoring explanations involving short explanatory chains, or favoring explanations with the most structural coherence (e.g., (Granger, 1980), (Wilensky, 1983), <ref> (Ng & Mooney, 1990) </ref>). 2 However, the examples in our introduction show that validity alone is not enough to assure an explanation's goodness: an explanation may be valid without being useful.
Reference: <author> Paris, C. </author> <year> (1987). </year> <title> Combining discourse strategies to generate descriptions to users along a naive/expert spectrum. </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 626-632). </pages> <institution> Milan, Italy: IJCAI. </institution>
Reference-contexts: The desire to provide the needed information has prompted development of many explanation systems, including systems that can show not just their decision paths, but the reasoning underlying those paths (Swartout, 1983); systems to devise explanations that are appropriate to the user's prior knowledge level <ref> (Paris, 1987) </ref>; and systems that treat the explanation process as a continuing dialogue, allowing clarifications and elaborations to be offered in response to follow-up questions (Moore & Swartout, 1989).
Reference: <author> Pazzani, M. J. </author> <year> (1988). </year> <title> Selecting the best explanation for explanation-based learning. </title> <booktitle> 1988 Spring Symposium Series: </booktitle> <pages> Explanation-Based Learning (pages 165-169). </pages> <publisher> Stanford: AAAI. </publisher>
Reference: <author> Rajamoney, S. </author> <year> (1988). </year> <title> Experimentation-based theory revision. </title> <booktitle> Proceedings of the 1988 AAAI Spring Symposium on Explanation-based Learning (pp. </booktitle> <pages> 7-11). </pages> <publisher> Stanford: AAAI. </publisher>
Reference-contexts: For example, (Dietterich & Flann, 1988) suggests that when a domain theory allows multiple incompatible explanations, induction over explanations for a set of training examples can be used to find a specialized domain theory that explains the positive examples, but none of the negative ones. <ref> (Rajamoney, 1988) </ref> advocates experimentation to determine how to extend or repair a domain theory in response to problems such as multiple incompatible explanations, or the inability to construct any complete explanation.
Reference: <author> Rajamoney, S., & DeJong, G. </author> <year> (1988). </year> <title> Active explanation reduction: An approach to the multiple explanations problem. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 242-255). </pages> <address> Ann Arbor: </address> <note> Machine Learning. </note>
Reference-contexts: For example, (Dietterich & Flann, 1988) suggests that when a domain theory allows multiple incompatible explanations, induction over explanations for a set of training examples can be used to find a specialized domain theory that explains the positive examples, but none of the negative ones. <ref> (Rajamoney, 1988) </ref> advocates experimentation to determine how to extend or repair a domain theory in response to problems such as multiple incompatible explanations, or the inability to construct any complete explanation.
Reference: <author> Ram, A. </author> <year> (1989). </year> <title> Question-driven understanding: An integrated theory of story understanding, memory and learning (Technical Report 710). </title> <institution> New Haven: Yale University Computer Science Department. </institution>
Reference-contexts: In this section, we discuss how ACCEPTER's evaluation criteria 18 lane. 19 judge whether an explanation provides the information needed to satisfy the goals of routine understanding. The following section discusses evaluation criteria for explanations serving other goals. When an understander encounters anomalies, they prompt knowledge goals <ref> (Ram, 1989) </ref> to reconcile the anomalies with other knowledge, both to correct the understander's picture of the current situation, and to avoid forming faulty expectations in the future.
Reference: <author> Rieger, C. </author> <year> (1975). </year> <title> Conceptual memory and inference. Conceptual Information Processing. </title> <publisher> Amsterdam: North-Holland. </publisher>
Reference-contexts: Guiding explanation construction in case-based explanation: How to control explanation construction is a difficult issue that has received surprisingly little attention. Many explanation-based systems construct explanations by undirected chaining, which risks overwhelming inference cost for any but the most simple explanations <ref> (Rieger, 1975) </ref>. The SWALE system addresses this problem by using case-based reasoning to build new explanations. Rather than explaining from scratch, it applies prior experience by retrieving and adapting explanations from similar past episodes. The case-based approach facilitates con 40 struction of complicated explanations, by re-using prior reasoning whenever possible.
Reference: <author> Riesbeck, C. </author> <year> (1981). </year> <title> Failure-driven reminding for incremental learning. </title> <booktitle> Proceedings of the Seventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 115-120). </pages> <institution> Van-couver, B.C.: IJCAI. </institution>
Reference: <author> Schank, R. </author> <year> (1972). </year> <title> Conceptual dependency: A theory of natural language understanding. </title> <journal> Cognitive Psychology, </journal> <volume> 3 (4),552-631. 47 Schank, </volume> <editor> R. </editor> <year> (1982). </year> <title> Dynamic Memory: A Theory of Learning in Computers and People. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: After discussing ACCEPTER's evaluation criteria, we return to ACCEPTER's role in SWALE, to examine the advantages of goal-based evaluation to an explanation-based understanding system. 5.1 ACCEPTER's basic algorithm ACCEPTER takes as input a story represented in terms of Conceptual Dependency theory primitives <ref> (Schank, 1972) </ref>, or in terms of schemas packaging sequences of those primitives to represent stereotyped sequences of actions. For example, a schema might represent the events involved in eating at a restaurant, such as entering, being seated, being brought menus, ordering, etc.
Reference: <author> Schank, R. </author> <year> (1986). </year> <title> Explanation Patterns: Understanding Mechanically and Creatively. </title> <address> Hills-dale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: See (Leake, 1989a) for an overview, or (Leake, in press) for a more complete account. When an anomaly is found, ACCEPTER presents the anomaly to the user, along with possible explanations. These explanations are formed by retrieving and instantiating explanation patterns (XPs) <ref> (Schank, 1986) </ref> from an XP library in ACCEPTER's memory.
Reference: <author> Schank, R., & Abelson, R. </author> <year> (1977). </year> <title> Scripts, Plans, Goals and Understanding. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: They distinguish two types of explanation: constructive and contrastive. In constructive explanation, people explain events in terms of knowledge structures such as scripts and plans <ref> (Schank & Abelson, 1977) </ref>. In contrastive explanation, they explain them by showing why events deviated from expectations provided by knowledge structures.
Reference: <author> Schank, R., & Leake, D. </author> <year> (1989). </year> <title> Creativity and learning in a case-based explainer. </title> <journal> Artificial Intelligence, </journal> <note> (40). Also in Carbonell, </note> <editor> J. (Ed.), </editor> <year> 1990, </year> <title> Machine Learning: Paradigms and Methods, </title> <publisher> Cambridge: MIT Press. </publisher>
Reference-contexts: Each of the links is simply a plausible inference link, showing how the hypotheses make the conclusion more likely. For a detailed description of the structure of belief-support chains, and the types of links used, see <ref> (Schank & Leake, 1989) </ref>. 6 Explanation evaluation for routine understanding ACCEPTER's explanation evaluation is guided by two types of goals. First, its evaluation serves the basic goal of an understander to account for events in the stories it processes, and to maintain accurate predictions.
Reference: <author> Segre, A. M. </author> <year> (1987). </year> <title> On the operationality/generality tradeoff in explanation-based learning. </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 242-248). </pages> <institution> Milan, Italy: IJCAI. </institution>
Reference-contexts: Research on operationality for concept recognition has led to the identification of significant characteristics of operationality for that task, such as the operationality/generality tradeoff (e.g., <ref> (Segre, 1987) </ref>). Because operationality has been studied for so few tasks, there has been some tendency to assume that such properties apply to operationality judgements for any task.
Reference: <author> Shortliffe, E. </author> <year> (1976). </year> <title> Computer-based medical consultations: MYCIN. </title> <address> New York: </address> <publisher> American Elsevier. </publisher>
Reference-contexts: We discuss below the approaches of each area. 6 3.3.1 Expert systems explanation Research on expert systems explanation has investigated the problem of generating explanations of expert system behavior that are sufficient either to educate the user about the task domain, or to justify system decision-making <ref> (Shortliffe, 1976) </ref>.
Reference: <author> Snyder, C., Higgens, R., & Stucky, R. </author> <year> (1983). </year> <title> Excuses: Masquerades in Search of Grace. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: Excuse theory studies how the desire to form excuses makes people manipulate the types of factors to use in attribution, to blame external influences for their own bad performance (for example, <ref> (Mehlman & Snyder, 1983) </ref> or (Snyder et al., 1983)). Excuse theory's demonstration that goals influence explanation is consistent with our view. <p> Excuse theory studies how the desire to form excuses makes people manipulate the types of factors to use in attribution, to blame external influences for their own bad performance (for example, (Mehlman & Snyder, 1983) or <ref> (Snyder et al., 1983) </ref>). Excuse theory's demonstration that goals influence explanation is consistent with our view.
Reference: <author> Souther, A., Acker, L., Lester, J., & Porter, B. </author> <year> (1989). </year> <title> Using view types to generate explanations in intelligent tutoring systems. </title> <booktitle> Proceedings of the Eleventh Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 123-130). </pages> <address> Ann Arbor: </address> <publisher> Cognitive Science Society. </publisher>
Reference-contexts: Instead, its operationality criterion is an input to the system, and can reflect current goals and goal priorities. Consequently, the spirit of this work is quite similar to ours, though it does not address the question of identifying the range of possible purposes for explanation. <ref> (Souther et al., 1989) </ref> also presents an argument close in spirit to ours| that it is essential to be able to generate explanations from a given viewpoint| and identifies classes of explanations that students might seek when studying college-level botany.
Reference: <author> Swartout, W. </author> <year> (1983). </year> <title> XPLAIN: a system for creating and explaining expert consulting programs. </title> <journal> Artificial Intelligence, (21),285-325. </journal>
Reference-contexts: The desire to provide the needed information has prompted development of many explanation systems, including systems that can show not just their decision paths, but the reasoning underlying those paths <ref> (Swartout, 1983) </ref>; systems to devise explanations that are appropriate to the user's prior knowledge level (Paris, 1987); and systems that treat the explanation process as a continuing dialogue, allowing clarifications and elaborations to be offered in response to follow-up questions (Moore & Swartout, 1989).
Reference: <author> Thagard, P. </author> <year> (1989). </year> <title> Explanatory coherence. </title> <journal> The Behavioral and Brain Sciences, </journal> <volume> 12 (3),435-502. </volume>
Reference: <author> Van Fraassen, B. </author> <year> (1980). </year> <title> The Scientific Image, chapter 5. </title> <publisher> Oxford: Clarendon Press. </publisher>
Reference: <author> Wilensky, R. </author> <year> (1983). </year> <title> Planning and Understanding. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher> <pages> 48 </pages>
Reference-contexts: For example, research on explanation in story understanding has relied primarily on fixed structural criteria for choosing between competing explanations, such as favoring explanations involving short explanatory chains, or favoring explanations with the most structural coherence (e.g., (Granger, 1980), <ref> (Wilensky, 1983) </ref>, (Ng & Mooney, 1990)). 2 However, the examples in our introduction show that validity alone is not enough to assure an explanation's goodness: an explanation may be valid without being useful.
References-found: 53

