URL: http://www.acm.org/jacm/papers/jacm856.ps
Refering-URL: http://theory.lcs.mit.edu/~madhu/papers.html
Root-URL: 
Title: Proof Verification and the Hardness of Approximation Problems  
Author: Sanjeev Arora Carsten Lund Rajeev Motwani Madhu Sudan Mario Szegedy 
Abstract: We show that every language in NP has a probablistic verifier that checks membership proofs for it using logarithmic number of random bits and by examining a constant number of bits in the proof. If a string is in the language, then there exists a proof such that the verifier accepts with probability 1 (i.e., for every choice of its random string). For strings not in the language, the verifier rejects every provided proof" with probability at least 1=2. Our result builds upon and improves a recent result of Arora and Safra [6] whose verifiers examine a nonconstant number of bits in the proof (though this number is a very slowly growing function of the input length). As a consequence we prove that no MAX SNP-hard problem has a polynomial time approximation scheme, unless NP=P. The class MAX SNP was defined by Pa-padimitriou and Yannakakis [82] and hard problems for this class include vertex cover, maximum satisfiability, maximum cut, metric TSP, Steiner trees and shortest superstring. We also improve upon the clique hardness results of Feige, Goldwasser, Lovsz, Safra and Szegedy [42], and Arora and Safra [6] and shows that there exists a positive * such that approximating the maximum clique size in an N-vertex graph to within a factor of N * is NP-hard.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Arora. </author> <title> Probabilistic Checking of Proofs and Hardness of Approximation Problems. </title> <type> PhD thesis, </type> <institution> U.C. Berkeley, </institution> <year> 1994. </year> <note> Available from http://www.cs.princeton.edu/~arora </note> . 
Reference: [2] <author> S. Arora. </author> <title> Polynomial-time approximation schemes for Euclidean TSP and other geometric problems. </title> <booktitle> Proceedings of 37th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pp 2-12, </pages> <year> 1996. </year>
Reference-contexts: Two striking results in this direction are those of Goemans and Williamson [57] and Arora <ref> [2] </ref>. Goemans and Williamson [57] show how to use semidefinite programming to give better approximation algorithms for MAX-2SAT and MAX-CUT. Arora [2] has discovered a polynomial time approximation scheme (PTAS) for Euclidean TSP and Euclidean Steiner tree problem. (Mitchell [80] independently discovered similar results a few months later.) These were two <p> Two striking results in this direction are those of Goemans and Williamson [57] and Arora <ref> [2] </ref>. Goemans and Williamson [57] show how to use semidefinite programming to give better approximation algorithms for MAX-2SAT and MAX-CUT. Arora [2] has discovered a polynomial time approximation scheme (PTAS) for Euclidean TSP and Euclidean Steiner tree problem. (Mitchell [80] independently discovered similar results a few months later.) These were two notable problems not addressed by our hardness result in this paper since they were not known to be MAX SNP-hard.
Reference: [3] <author> S. Arora, L. Babai, J. Stern, and Z. Sweedyk. </author> <title> The hardness of approximate op tima in lattices, codes, and systems of linear equations. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 54(2) </volume> <pages> 317-331, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: They show that coloring a 3-colorable graph with 4 colors is NP-hard. Arora, Babai, Stern, and Sweedyk <ref> [3] </ref> prove hardness results for a collection of problems involving integral lattices, codes, or linear equations/inequations. These include Nearest Lattice Vector, Nearest Codeword, and the Shortest Lattice Vector under the ` 1 norm.
Reference: [4] <author> S. Arora and C. Lund. </author> <title> Hardness of approximations. In Approximation Algorithms for NP-hard problems, </title> <editor> D. Hochbaum, ed. </editor> <publisher> PWS Publishing, </publisher> <year> 1996. </year>
Reference-contexts: Karger, Motwani, and Ramkumar [70] prove the hardness of approximating the longest path in a graph to within a 2 log 1* n factor, for any * &gt; 0. There are many other results which we haven't mentioned here; see the compendium [37] or the survey <ref> [4] </ref>. Improved analysis of outer verifiers.
Reference: [5] <author> S. Arora, R. Motwani, S. Safra, M. Sudan, and M. Szegedy. </author> <title> PCP and approxi mation problems. Unpublished note, </title> <year> 1992. </year>
Reference-contexts: The discovery of Theorem 2 inspired the search for other connections between probabilistic proof checking and non-approximability (Bellare [18], Bellare and Rogaway [22], Feige and Lovsz [45], and Zuckerman [100]). Another such connection is reported by Arora, Motwani, Safra, Sudan and Szegedy <ref> [5] </ref>, which shows the connection between PCP's and the hardness of approximating MAX 3SAT. The following theorem summarizes this result; for a proof see Section 3. Theorem 3 ([5]) If NP [ c&gt;0 PCPc log n; q for some positive integer q, then there exists a constant * &gt; 0, such
Reference: [6] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs: a new characterization of NP. </title> <booktitle> To appear Journal of the ACM. Preliminary version in Proceedings of the Thirty Third Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: They showed that if it is possible to approximate the clique number to within any constant factor in polynomial time, then every NP problem can be solved in n Olog log n time. More recently, Arora and Safra <ref> [6] </ref> improved this result to show that if clique is in APX, then P NP. In other words, approximating clique within any constant factor is NP-hard. These results relied upon algebraic techniques from complexity theory and the theory of interactive proofs. <p> They showed that NP languages have such proofs. Feige et al. [42] showed a similar result, but with a somewhat more efficient verifier. Arora and Safra <ref> [6] </ref> further improved the efficiency of checking membership proofs for NP languages. They also gave a surprising new characterization of NP. We describe their result below, and describe how we have improved upon it. Arora and Safra define a hierarchy of complexity classes called PCP (for Probabilistically Checkable Proofs). <p> NP [ c&gt;2 PCPlog c n; log c n; (Babai, Fortnow, Levin, and Szegedy [14]). NP [ c&gt;0 PCPc log n loglog n; c log n loglog n (Feige, Goldwasser, Lovsz, Safra, and Szegedy [42]). NP [ c&gt;0 PCPc log n; c p log n (Arora and Safra <ref> [6] </ref>). Notice that the first and the last of the above relations are exact characterizations of the complexity classes NEXP and NP, respectively. <p> Remark: The above statement is a well-known implication of the result of Feige et al. [42]. It uses the idea of reducing the error probability of the verifier using recycled random bits (see [32, 65]). For further details see <ref> [6] </ref>. Thus, as a consequence of the new characterization of NP due to Arora and Safra [6], it follows that approximating the clique number within a factor 2 p log N is NP-hard. <p> It uses the idea of reducing the error probability of the verifier using recycled random bits (see [32, 65]). For further details see <ref> [6] </ref>. Thus, as a consequence of the new characterization of NP due to Arora and Safra [6], it follows that approximating the clique number within a factor 2 p log N is NP-hard. The discovery of Theorem 2 inspired the search for other connections between probabilistic proof checking and non-approximability (Bellare [18], Bellare and Rogaway [22], Feige and Lovsz [45], and Zuckerman [100]). <p> integer q, then there exists a constant * &gt; 0, such that approximating MAX-3SAT within a factor 1 * is NP-hard. 1.2 Our results The main result of this paper is a new characterization of NP in terms of PCP, which improves the earlier characterization due to Arora and Safra <ref> [6] </ref>. Theorem 4 (Main) There is a positive integer q such that NP [ c&gt;0 PCPc log n; q: The containment [ c&gt;0 PCPc log n; q NP is trivial, since a verifier that uses Olog n random bits can be replaced by a deterministic verifier. <p> The nontrivial containment NP [ c&gt;0 PCPc log n; q is a consequence of our Theorem 17 below. We note that the characterization in Theorem 4 is probably optimal up to constant factors, since, as observed in <ref> [6] </ref>, if P NP, then NP 6 PCPolog n; olog n. As an immediate consequence of our main theorem and Theorem 3, we have the following. <p> The previous best result (in <ref> [6] </ref>) shows that approximating clique to within a factor of 2 log N is NP-hard. Our techniques. Our proof of Theorem 4 uses techniques similar to those in recent results about PCPs (Babai, Fortnow, Levin, and Szegedy [14]; Feige, Goldwasser, Lovsz, Safra and Szegedy [42]; and Arora-Safra [6]). <p> best result (in <ref> [6] </ref>) shows that approximating clique to within a factor of 2 log N is NP-hard. Our techniques. Our proof of Theorem 4 uses techniques similar to those in recent results about PCPs (Babai, Fortnow, Levin, and Szegedy [14]; Feige, Goldwasser, Lovsz, Safra and Szegedy [42]; and Arora-Safra [6]). The concept of Recursive Proof Checking invented by Arora and Safra plays a particularly important role (see Section 2). We were also inuenced by work on program checking and correcting, especially Blum, Luby, and Rubinfeld [30], and Rubinfeld and Sudan [90]. <p> We were also inuenced by work on program checking and correcting, especially Blum, Luby, and Rubinfeld [30], and Rubinfeld and Sudan [90]. The inuence of the former will be apparent in Section 5, while the latter work (together with a lemma of Arora and Safra <ref> [6] </ref>) is used in our analysis of a Low Degree Test described in Section 7.2. Finally, we were inuenced by work on constant prover 1-round interactive proof systems [74, 45]. <p> with logarithmic randomness and polylogarithmic communication bits, already improves upon the performance of known constructions and plays a central role in the proof of Theorem 4. 2 Proof of the Main Theorem: Overview A key paradigm in the proof of Theorem 4 is the technique of recursive proof checking from <ref> [6] </ref>. The technique, described in Theorem 13, involves constructing two types of verifiers with certain special properties, and then composing them. <p> The only parameters that change are cn and an, which go down from polylog n to polylog log n to O1. First we define an inner verifier, a notion implicit in Arora and Safra <ref> [6] </ref>. To motivate this definition, we state informally how an inner verifier will be used during recursive proof checking. <p> Conversely, if the verifier accepts some oracle with probability e, then soundness implies that the circuit is satisfiable. Hence CKT-SAT 2 RPCPr n; pn; cn; an; e. The technique of recursive proof checking is described in the proof of the following theorem. Theorem 13 (rephrasing of results in <ref> [6] </ref>) Let p 2 Z be such that a p; r 1 n; p 1 n; c 1 n; a 1 n; e 1 -inner verifier system exists for some functions r 1 ; p 1 ; c 1 ; a 1 : Z ! Z and 0 &lt; e 1 <p> The theorem will be proved in Section 7.5. The starting point is a verifier constructed by Arora and Safra <ref> [6] </ref>, which uses Olog n random bits and queries the proof in polylog n places. (Actually the number of queries in their strongest result is log log n O1 but we don't need that stronger result. <p> This modification would cut down the number of random bits needed in [14] by using the idea of recycling randomness [32, 65].) The following theorem was only implicit in the earlier versions of <ref> [6] </ref> but is explicit in the final version. Theorem 39 ([6]) For every constant k, there exist constants c 1 ; c 2 ; c 3 and e &lt; 1 such that a k; c 1 log n; log c 2 n; log c 3 n; 1; e inner verifier exists. <p> The first, called the low degree test, efficiently determines if a given oracle O : F m ! F is ffi-close to some degree d polynomial, where ffi is some small constant. Low degree tests were invented as part of work on proof checking <ref> [13, 14, 42, 6, 56, 90] </ref>. Efficiency is of paramount concern to us, so we would like the test to make as few queries to the oracle as possible. Most low degree tests known make polym; d; 1=ffi queries. <p> Most low degree tests known make polym; d; 1=ffi queries. However, Rubinfeld and Sudan [90] give a test (which requires an auxiliary oracle in addition to O) whose number of queries depends on d but not on m. Arora and Safra <ref> [6] </ref> give a test whose number of queries depends only on m but not on d. We observe in this paper that the two analyses can be combined to give a test whose number of queries is independent of d and m. <p> We observe in this paper that the two analyses can be combined to give a test whose number of queries is independent of d and m. This test and its analysis is essentially from [90]. Our only new contribution is to use a result from <ref> [6] </ref> (Theorem 69 in the appendix) instead of a weaker result from [90]. <p> The properties of the test are summarized below. Theorem 45 (follows by combining <ref> [90, 6] </ref>) There exist constants ffi 0 &gt; 0 and ff &lt; 1 such that for ffi ffi 0 , d 2 Z if F is a field of cardinality at least ffd 1 3 , then the following holds: 1. <p> Babai [11] raised the following question: How efficient can this encoding be? In our paper, encoding an assignment of size n requires polyn bits. This was reduced to n 2* by Sudan [95]. The main hurdle in further improvement seemed to be Arora and Safra's proof <ref> [6] </ref> of Theorem 69, which requires a field size quadratic in the degree. Polishchuk and Spielman [86] present a new proof of Theorem 69 that works when the field size is linear. <p> We hope that there will be many other applications. Acknowledgments This paper was motivated strongly by the work of Arora and Safra <ref> [6] </ref> and we thank Muli Safra for numerous discussions on this work.
Reference: [7] <author> S. Arora and M. Sudan. </author> <title> Improved low degree testing and its applications. </title> <booktitle> Pro ceedings of the Twenty Eighth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1997 </year>
Reference-contexts: Very recently, Raz and Safra [88] have constructed verifiers making constant number of queries with logarithmic randomness and answer size, where the error is as low as 2 log 1* n for every * &gt; 0. An alternate construction is given in Arora and Sudan <ref> [7] </ref>. Better non-approximability results. Part of the motivation for improving the construction of outer verifiers is to improve the ensuing non-approximability results. The result for MAX-3SAT in this paper, we only demonstrate the existence of an * &gt; 0 such that approximating MAX-3SAT within a factor 1* is NP-hard. <p> Some of the other results of this paper have also found new proofs with a careful attention to the constants involved. In particular, the low-degree test Theorem 45 has been improved significantly since then. Arora and Sudan <ref> [7] </ref> show that part 2 of Theorem 45 can be improved to show that if a function passes the low degree test with probability * then it is 1 *-close to some degree d polynomial. A similar result for a different test was shown earlier by Raz and Safra [88].
Reference: [8] <author> G. Ausiello, A. D'Atri, and M. Protasi. </author> <title> Structure Preserving Reductions among Convex Optimization Problems. </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 21 </volume> <pages> 136-153, </pages> <year> 1980. </year>
Reference: [9] <author> G. Ausiello, A. Marchetti-Spaccamela and M. Protasi. </author> <title> Towards a Unified Ap proach for the Classification of NP-complete Optimization Problems. </title> <journal> Theoretical Computer Science, </journal> <volume> 12 </volume> <pages> 83-96, </pages> <year> 1980. </year>
Reference: [10] <author> L. Babai. </author> <title> Trading group theory for randomness. </title> <booktitle> Proceedings of the Seventeenth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1985. </year>
Reference-contexts: Generalizing the above definition of NP leads to definitions of interesting new complexity classes, which have been the subject of intense research in the past decade. Goldwasser, Micali and Rackoff [59] and Babai <ref> [10, 16] </ref> allowed the verifier to be a probabilistic polynomial-time Turing Machine that interacts with a prover, which is an infinitely powerful Turing Machine trying to convince the verifier that the input x is in the language.
Reference: [11] <author> L. Babai. </author> <title> Transparent (holographic) proofs. </title> <booktitle> Proceedings of the Tenth Annual Sym posium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science Vol. </booktitle> <volume> 665, </volume> <publisher> Springer Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Other technical improvements. As a side-product of our proof of the main theorem, we showed how to encode an assignment to a given circuit so that somebody else can check that it is a satisfying assignment by looking at O1 bits in the encoding. Babai <ref> [11] </ref> raised the following question: How efficient can this encoding be? In our paper, encoding an assignment of size n requires polyn bits. This was reduced to n 2* by Sudan [95].
Reference: [12] <author> L. Babai and L. Fortnow. Arithmetization: </author> <title> a new method in structural complexity theory. </title> <journal> Computational Complexity, </journal> <volume> 1 </volume> <pages> 41-66, </pages> <year> 1991. </year>
Reference: [13] <author> L. Babai, L. Fortnow, and C. Lund. </author> <title> Non-deterministic exponential time has two prover interactive protocols. </title> <journal> Computational Complexity, </journal> <volume> 1 </volume> <pages> 3-40, </pages> <year> 1991. </year>
Reference-contexts: This equivalent definition is described below, using the term probabilistically checkable proofs introduced by Arora and Safra. Babai, Fortnow and Lund <ref> [13] </ref> recently showed that MIP is exactly NEXP, the class of languages for which membership proofs can be checked deterministically in exponential time. This result is surprising because NEXP is just the exponential analogue of 3 NP, and its usual definition involves no notion of randomness or interaction. <p> The PCP notation allows a compact description of many known results, NEXP [ c&gt;0 PCPn c ; n c (Babai, Fortnow, and Lund <ref> [13] </ref>). NP [ c&gt;2 PCPlog c n; log c n; (Babai, Fortnow, Levin, and Szegedy [14]). NP [ c&gt;0 PCPc log n loglog n; c log n loglog n (Feige, Goldwasser, Lovsz, Safra, and Szegedy [42]). NP [ c&gt;0 PCPc log n; c p log n (Arora and Safra [6]). <p> We will show how to aggregate its queries into O1 queries, at the cost of a minor increase in the answer size (though the answer size remains polylog n). The aggregation uses the idea of a multivariate polynomial encoding <ref> [13] </ref> and a new, efficient, low-degree test for checking the correctness of its codewords. It also uses a procedure described in Section 7.4. 7.1 Multivariate polynomial encodings Let F be any finite field. The following fact will be used over and over again. <p> The first, called the low degree test, efficiently determines if a given oracle O : F m ! F is ffi-close to some degree d polynomial, where ffi is some small constant. Low degree tests were invented as part of work on proof checking <ref> [13, 14, 42, 6, 56, 90] </ref>. Efficiency is of paramount concern to us, so we would like the test to make as few queries to the oracle as possible. Most low degree tests known make polym; d; 1=ffi queries.
Reference: [14] <author> L. Babai, L. Fortnow, L. Levin, and M. Szegedy. </author> <title> Checking computations in poly logarithmic time. </title> <booktitle> Proceedings of the Twenty Third Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1991. </year>
Reference-contexts: This result is surprising because NEXP is just the exponential analogue of 3 NP, and its usual definition involves no notion of randomness or interaction. Therefore researchers tried to discover if the MIP NEXP result can be scaled-down to say something interesting about NP. Babai, Fortnow, Levin and Szegedy <ref> [14] </ref> introduced the notion of transparent membership proofs, namely, membership proofs that can be checked in polylogarithmic time, provided the input is encoded with some error-correcting code. They showed that NP languages have such proofs. Feige et al. [42] showed a similar result, but with a somewhat more efficient verifier. <p> The PCP notation allows a compact description of many known results, NEXP [ c&gt;0 PCPn c ; n c (Babai, Fortnow, and Lund [13]). NP [ c&gt;2 PCPlog c n; log c n; (Babai, Fortnow, Levin, and Szegedy <ref> [14] </ref>). NP [ c&gt;0 PCPc log n loglog n; c log n loglog n (Feige, Goldwasser, Lovsz, Safra, and Szegedy [42]). NP [ c&gt;0 PCPc log n; c p log n (Arora and Safra [6]). <p> The previous best result (in [6]) shows that approximating clique to within a factor of 2 log N is NP-hard. Our techniques. Our proof of Theorem 4 uses techniques similar to those in recent results about PCPs (Babai, Fortnow, Levin, and Szegedy <ref> [14] </ref>; Feige, Goldwasser, Lovsz, Safra and Szegedy [42]; and Arora-Safra [6]). The concept of Recursive Proof Checking invented by Arora and Safra plays a particularly important role (see Section 2). <p> This may sound impossible how can you check that a bit string is a satisfying assignment without reading every bit in it? But Arora and Safra showed how to do it by modifying a result of Babai et al. <ref> [14] </ref> who had shown that if a bit-string is given to us in an encoded form (using a specific error-correcting code), then it is possible to check a proof that the string is a satisfying assignment, without reading the entire string! Arora and Safra show how to do the same check <p> In fact, even the weaker verifier of Babai, Fort-now, Levin and Szegedy <ref> [14] </ref> would suffice for our purpose after some modification. This modification would cut down the number of random bits needed in [14] by using the idea of recycling randomness [32, 65].) The following theorem was only implicit in the earlier versions of [6] but is explicit in the final version. <p> In fact, even the weaker verifier of Babai, Fort-now, Levin and Szegedy <ref> [14] </ref> would suffice for our purpose after some modification. This modification would cut down the number of random bits needed in [14] by using the idea of recycling randomness [32, 65].) The following theorem was only implicit in the earlier versions of [6] but is explicit in the final version. <p> The first, called the low degree test, efficiently determines if a given oracle O : F m ! F is ffi-close to some degree d polynomial, where ffi is some small constant. Low degree tests were invented as part of work on proof checking <ref> [13, 14, 42, 6, 56, 90] </ref>. Efficiency is of paramount concern to us, so we would like the test to make as few queries to the oracle as possible. Most low degree tests known make polym; d; 1=ffi queries. <p> In other words, every theorem of the axiomatic system has a proof" that can be checked probabilistically by examining only O1 bits in it. (Babai, Fortnow, Levin, and Szegedy <ref> [14] </ref> had earlier shown that proofs can be checked by examinining only polylog n bits in them.) Actually, by looking at Remark 18, a stronger statement can be obtained: there is a polynomial-time transformation between normal mathematical proofs and our probabilistically checkable certificates of theoremhood." Surprising algorithms: There has also been
Reference: [15] <author> L. Babai and K. Friedl. </author> <title> On slightly superlinear transparent proofs. </title> <institution> Univ. Chicago Tech. </institution> <type> Report, </type> <institution> CS-93-13, </institution> <year> 1993. </year> <month> 41 </month>
Reference: [16] <author> L. Babai and S. Moran. </author> <title> Arthur-Merlin games: a randomized proof system, and a hierarchy of complexity classes. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 36 </volume> <pages> 254-276, </pages> <year> 1988. </year>
Reference-contexts: Generalizing the above definition of NP leads to definitions of interesting new complexity classes, which have been the subject of intense research in the past decade. Goldwasser, Micali and Rackoff [59] and Babai <ref> [10, 16] </ref> allowed the verifier to be a probabilistic polynomial-time Turing Machine that interacts with a prover, which is an infinitely powerful Turing Machine trying to convince the verifier that the input x is in the language.
Reference: [17] <author> D. Beaver and J. Feigenbaum. </author> <title> Hiding instances in multioracle queries. </title> <booktitle> Proceed ings of the Seventh Annual Symposium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science Vol. </booktitle> <volume> 415, </volume> <publisher> Springer Verlag, </publisher> <year> 1990. </year>
Reference-contexts: It needs to find the value of p at some specified point x 2 F m . We now describe a procedure which computes px using few probes into O and an auxiliary oracle B. The procedure owes its origins to the work of Beaver and Feigenbaum <ref> [17] </ref> and Lipton [76]. The specific analysis given below is borrowed from the work of Gemmell, Lipton, Rubinfeld, Sudan and Wigderson [56] and allows the number of queries to be independent of d, for error bounded away from 1.
Reference: [18] <author> M. Bellare. </author> <title> Interactive proofs and approximation: reductions from two provers in one round. </title> <booktitle> Proceedings of the Second Israel Symposium on Theory and Computing Systems, </booktitle> <year> 1993. </year>
Reference-contexts: The discovery of Theorem 2 inspired the search for other connections between probabilistic proof checking and non-approximability (Bellare <ref> [18] </ref>, Bellare and Rogaway [22], Feige and Lovsz [45], and Zuckerman [100]). Another such connection is reported by Arora, Motwani, Safra, Sudan and Szegedy [5], which shows the connection between PCP's and the hardness of approximating MAX 3SAT. The following theorem summarizes this result; for a proof see Section 3.
Reference: [19] <author> M. Bellare, D. Coppersmith, J. Hstad, M. Kiwi and M. Sudan. </author> <title> Linearity testing in characteristic two. </title> <journal> IEEE Transactions on Information Theory 42(6) </journal> <pages> 1781-1795, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: However this bound can be easily reconstructed from some of the subsequent papers, for instance, in the work of Bellare et al. <ref> [19] </ref>. In any case, the exact bound is unimportant for what follows. A useful aspect of the linear encoding is that one can obtain the value of the linear function at any point using few (randomly chosen) queries to any function that is very close to it. <p> A similar result for a different test was shown earlier by Raz and Safra [88]. The proof of the correctness of the linearity test (Theorem 27) has also been improved in works by Bellare, Goldwasser, Lund and Russell [21] and Bellare, Coppersmith, Hastad, Kiwi and Sudan <ref> [19] </ref>. 39 Transparent math proofs. We briey mention an application that received much attention in the popular press. Consider proofs of theorems of any reasonable axiomatic theory, such as Zermelo-Fraenkel set theory. A turing machine can check such proofs in time that is polynomial in the length of the proof.
Reference: [20] <author> M. Bellare, O. Goldreich and M. Sudan. </author> <title> Free bits, PCPs and non-approximability towards tight results. </title> <note> To appear SIAM Journal on Computing. Preliminary version in Proceedings of the Thirty Sixth Annual Symposium on the Foundations of Computer Science, IEEE, 1995. Full version available as TR95-024 of ECCC, the Electronic Colloquium on Computational Complexity, http://www.eccc.uni-trier .de/eccc/. </note>
Reference-contexts: This line of research was initiated by Bellare, Goldwasser, Lund, and Russell [21] who provided improved non-approximability results with explicit constants for a number of problems, such as MAX 3SAT, MAX CLIQUE, Chromatic Number and Set Cover. Since then a number of works <ref> [20, 23, 41, 43, 44, 50, 61, 62, 63] </ref> have focussed on improving 38 the non-approximability results. These results have culminated with some remarkably strong inapproximability results listed below. <p> But q has since been computed explicitly and then reduced through tighter analysis. It has gone from 29 [21] to to 22 [43] to 16 <ref> [20] </ref> to at most 9 (which is the best that can be inferred directly from [63] though this result is not optimized.) Parameters that are somewhat related to q are the free-bit parameter introduced by Feige and Kilian [43] and the amortized free-bit parameter introduced by Bellare and Sudan [23]. <p> Reducing the latter parameter is necessary and sufficient for proving good inapproximability results for MAX CLIQUE <ref> [20] </ref>, and Hstad [61, 62] has shown how to make this parameter smaller than every fixed ffi &gt; 0 . Other technical improvements.
Reference: [21] <author> M. Bellare, S. Goldwasser, C. Lund, and A. Russell. </author> <title> Efficient probabilistically checkable proofs. </title> <booktitle> Proceedings of the Twenty Fifth Annual Symposium on the Theory of Computing, ACM, 1993. (See also Errata sheet in Proceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994). </year>
Reference-contexts: Bellare, Goldwasser, Lund and Russell <ref> [21] </ref> construct verifiers that use only 4 queries and logarithmic randomness to get the error down to an arbitrarily small constant (with polyloglog sized answer sizes). Feige and Kilian [43] construct verifiers with 2 queries, arbitrarily small error, and constant answer sizes. <p> But the improved verifier constructions mentioned above have steadily provided better and better values for this *. This line of research was initiated by Bellare, Goldwasser, Lund, and Russell <ref> [21] </ref> who provided improved non-approximability results with explicit constants for a number of problems, such as MAX 3SAT, MAX CLIQUE, Chromatic Number and Set Cover. Since then a number of works [20, 23, 41, 43, 44, 50, 61, 62, 63] have focussed on improving 38 the non-approximability results. <p> But q has since been computed explicitly and then reduced through tighter analysis. It has gone from 29 <ref> [21] </ref> to to 22 [43] to 16 [20] to at most 9 (which is the best that can be inferred directly from [63] though this result is not optimized.) Parameters that are somewhat related to q are the free-bit parameter introduced by Feige and Kilian [43] and the amortized free-bit parameter <p> A similar result for a different test was shown earlier by Raz and Safra [88]. The proof of the correctness of the linearity test (Theorem 27) has also been improved in works by Bellare, Goldwasser, Lund and Russell <ref> [21] </ref> and Bellare, Coppersmith, Hastad, Kiwi and Sudan [19]. 39 Transparent math proofs. We briey mention an application that received much attention in the popular press. Consider proofs of theorems of any reasonable axiomatic theory, such as Zermelo-Fraenkel set theory.
Reference: [22] <author> M. Bellare and P. Rogaway. </author> <title> The complexity of approximating a nonlinear pro gram. </title> <journal> Journal of Mathematical Programming B, </journal> <volume> 69(3) </volume> <pages> 429-441, </pages> <month> September </month> <year> 1995. </year> <note> Also in Complexity of Numerical Optimization, </note> <editor> Ed. P. M. Pardalos, </editor> <publisher> World Scientific, </publisher> <year> 1993. </year>
Reference-contexts: The discovery of Theorem 2 inspired the search for other connections between probabilistic proof checking and non-approximability (Bellare [18], Bellare and Rogaway <ref> [22] </ref>, Feige and Lovsz [45], and Zuckerman [100]). Another such connection is reported by Arora, Motwani, Safra, Sudan and Szegedy [5], which shows the connection between PCP's and the hardness of approximating MAX 3SAT. The following theorem summarizes this result; for a proof see Section 3.
Reference: [23] <author> M. Bellare and M. Sudan. </author> <title> Improved non-approximability results. </title> <booktitle> Proceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: This line of research was initiated by Bellare, Goldwasser, Lund, and Russell [21] who provided improved non-approximability results with explicit constants for a number of problems, such as MAX 3SAT, MAX CLIQUE, Chromatic Number and Set Cover. Since then a number of works <ref> [20, 23, 41, 43, 44, 50, 61, 62, 63] </ref> have focussed on improving 38 the non-approximability results. These results have culminated with some remarkably strong inapproximability results listed below. <p> 16 [20] to at most 9 (which is the best that can be inferred directly from [63] though this result is not optimized.) Parameters that are somewhat related to q are the free-bit parameter introduced by Feige and Kilian [43] and the amortized free-bit parameter introduced by Bellare and Sudan <ref> [23] </ref>. Reducing the latter parameter is necessary and sufficient for proving good inapproximability results for MAX CLIQUE [20], and Hstad [61, 62] has shown how to make this parameter smaller than every fixed ffi &gt; 0 . Other technical improvements.
Reference: [24] <author> M. Ben-Or, S. Goldwasser, J. Kilian, and A. Wigderson. </author> <title> Multi-prover interactive proofs: How to remove intractability assumptions. </title> <booktitle> Proceedings of the Twentieth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1988. </year>
Reference-contexts: Another variant of proof verification, due to Ben-Or, Goldwasser, Kilian and Wigderson <ref> [24] </ref>, involves a probabilistic polynomial-time verifier interacting with more than one mutually non-interacting provers. The class of languages with such interactive proofs is called MIP (for Multi-prover Interactive Proofs).
Reference: [25] <author> P. Berman and G. Schnitger. </author> <title> On the complexity of approximating the indepen dent set problem. </title> <journal> Information and Computation 96:7794, </journal> <year> 1992. </year>
Reference: [26] <author> M. Bern and P. Plassmann. </author> <title> The steiner problem with edge lengths 1 and 2. </title> <journal> Infor mation Processing Letters, </journal> <volume> 32:171176, </volume> <year> 1989. </year>
Reference-contexts: As noted earlier, Theorem 5 implies that if P NP then no MAX SNP-hard problem is in PTAS. The class of MAX SNP-hard problems includes MAX 2SAT, MAX CUT, vertex cover [82], metric TSP [83], Steiner tree <ref> [26] </ref>, shortest superstrings [27], MAX 3DM [67], and multiway cut [38]. Our theorem above implies that for every one of these problems , there exists a threshold * such that approximating within a factor 1 * is NP-hard.
Reference: [27] <author> A. Blum, T. Jiang, M. Li, J. Tromp, and M. Yannakakis. </author> <title> Linear approximation of shortest superstrings. </title> <journal> Journal of the ACM, </journal> <volume> 41(4) </volume> <pages> 630-647, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: As noted earlier, Theorem 5 implies that if P NP then no MAX SNP-hard problem is in PTAS. The class of MAX SNP-hard problems includes MAX 2SAT, MAX CUT, vertex cover [82], metric TSP [83], Steiner tree [26], shortest superstrings <ref> [27] </ref>, MAX 3DM [67], and multiway cut [38]. Our theorem above implies that for every one of these problems , there exists a threshold * such that approximating within a factor 1 * is NP-hard.
Reference: [28] <author> M. Blum. </author> <title> Program checking. </title> <booktitle> Proc. FST&TCS, </booktitle> <publisher> Springer L.N.C.S. </publisher> <pages> 560, pp. 19. </pages>
Reference: [29] <author> M. Blum and S. Kannan. </author> <title> Designing Programs that Check Their Work. </title> <booktitle> Proceedings of the Twenty First Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1989. </year>
Reference: [30] <author> M. Blum, M. Luby, and R. Rubinfeld. </author> <title> Self-testing/correcting with applications to numerical problems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 47(3) </volume> <pages> 549-595, </pages> <month> December </month> <year> 1993. </year> <month> 42 </month>
Reference-contexts: The concept of Recursive Proof Checking invented by Arora and Safra plays a particularly important role (see Section 2). We were also inuenced by work on program checking and correcting, especially Blum, Luby, and Rubinfeld <ref> [30] </ref>, and Rubinfeld and Sudan [90]. The inuence of the former will be apparent in Section 5, while the latter work (together with a lemma of Arora and Safra [6]) is used in our analysis of a Low Degree Test described in Section 7.2. <p> Testing/Correcting Given an oracle f : GF2 n ! GF2, we would like to determine very efficiently, probabilistically, if there exists a string x 2 GF2 n , such that L x ; f is small. The following procedure due to Blum, Luby and Rubinfeld <ref> [30] </ref> achieves this with just 3 queries to the oracle f . Linearity-testf ; n: /* Expects an oracle f : GF2 n ! GF2. */ Pick x; y 2 R GF2 n ; If f x f y 6 f x y then reject else accept. <p> that the test Linearity test accepts is more than 1 ffi for some ffi &lt; 2=9 then there exists a linear function g such that f ; g 2ffi. 17 Remark: The exact bound stated above in part 2 of Theorem 27 may not match the bound as stated in <ref> [30] </ref>. However this bound can be easily reconstructed from some of the subsequent papers, for instance, in the work of Bellare et al. [19]. In any case, the exact bound is unimportant for what follows. <p> A useful aspect of the linear encoding is that one can obtain the value of the linear function at any point using few (randomly chosen) queries to any function that is very close to it. This procedure, described below, is also due to Blum, Luby and Rubinfeld <ref> [30] </ref>. Linear-self-corrf ; x; n: /* Expects an oracle f : GF2 n ! GF2 and x 2 GF2 n . */ Pick y 2 R GF2 n ; Output f x y f y. Proposition 28 ([30]) 1.
Reference: [31] <author> J. Cai, A. Condon, and R. Lipton. </author> <title> PSPACE is provable by two provers in one round. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 48(1) </volume> <pages> 183-193, </pages> <month> February </month> <year> 1994. </year>
Reference: [32] <author> A. Cohen and A. Wigderson. Dispersers, </author> <title> deterministic amplification, and weak random sources. </title> <booktitle> Proceedings of the Thirtieth Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1989. </year>
Reference-contexts: Remark: The above statement is a well-known implication of the result of Feige et al. [42]. It uses the idea of reducing the error probability of the verifier using recycled random bits (see <ref> [32, 65] </ref>). For further details see [6]. Thus, as a consequence of the new characterization of NP due to Arora and Safra [6], it follows that approximating the clique number within a factor 2 p log N is NP-hard. <p> In fact, even the weaker verifier of Babai, Fort-now, Levin and Szegedy [14] would suffice for our purpose after some modification. This modification would cut down the number of random bits needed in [14] by using the idea of recycling randomness <ref> [32, 65] </ref>.) The following theorem was only implicit in the earlier versions of [6] but is explicit in the final version.
Reference: [33] <author> A. Condon. </author> <title> The complexity of the max word problem, or the power of one-way interactive proof systems. </title> <booktitle> Proceedings of the Eighth Annual Symposium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science Vol. </booktitle> <volume> 480, </volume> <publisher> Springer Verlag, </publisher> <year> 1991. </year>
Reference: [34] <author> A. Condon, J. Feigenbaum, C. Lund and P. Shor. </author> <title> Probabilistically Checkable De bate Systems and Approximation Algorithms for PSPACE-Hard Functions. </title> <booktitle> Proceedings of the Twenty Fifth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1993. </year>
Reference-contexts: We feel that it ought to have many other uses in complexity theory (or related areas like cryptography). One result in this direction is due to Condon et al. <ref> [34, 35] </ref>, who use our main theorem (actually, a stronger form of it that we did not state) to prove a PCP-style characterization of PSPACE. We hope that there will be many other applications.
Reference: [35] <author> A. Condon, J. Feigenbaum, C. Lund and P. Shor. </author> <title> Random debaters and the hardness of approximating stochastic functions. </title> <journal> SIAM Journal on Computing, </journal> <volume> 26(2) </volume> <pages> 369-400, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: We feel that it ought to have many other uses in complexity theory (or related areas like cryptography). One result in this direction is due to Condon et al. <ref> [34, 35] </ref>, who use our main theorem (actually, a stronger form of it that we did not state) to prove a PCP-style characterization of PSPACE. We hope that there will be many other applications.
Reference: [36] <author> S. Cook. </author> <title> The complexity of theorem-proving procedures. </title> <booktitle> Proceedings of the Third Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1971. </year>
Reference-contexts: 1 Introduction Classifying optimization problems according to their computational complexity is a central endeavor in theoretical computer science. The theory of NP-completeness, developed by Cook <ref> [36] </ref>, Karp [69] and Levin [75], shows that many decision problems of interest, such as satisfiability, are NP-complete. This theory also shows that decision versions of many optimization problems, such as the traveling salesman problem, fl arora@cs.princeton.edu. Department of Computer Science, Princeton University, NJ 08544.
Reference: [37] <author> P. Crescenzi and V. Kann, </author> <title> A compendium of NP optimization problems. </title> <type> Technical Report, </type> <institution> Dipartimento di Scienze dell'Informazione, Universit di Roma La Sapienza, SI/RR-95/02, </institution> <year> 1995. </year> <note> The list is updated continuously. The latest version is available by anonymous ftp from nada.kth.se as Theory/Viggo-Kann/compendium.ps.Z. </note>
Reference-contexts: Karger, Motwani, and Ramkumar [70] prove the hardness of approximating the longest path in a graph to within a 2 log 1* n factor, for any * &gt; 0. There are many other results which we haven't mentioned here; see the compendium <ref> [37] </ref> or the survey [4]. Improved analysis of outer verifiers.
Reference: [38] <author> E. Dahlhaus, D. Johnson, C. Papadimitriou, P. Seymour, and M. Yannakakis. </author> <title> The complexity of multiway cuts. </title> <journal> SIAM Journal on Computing, </journal> <volume> 23:4, </volume> <pages> pp. 864894, </pages> <year> 1994. </year>
Reference-contexts: As noted earlier, Theorem 5 implies that if P NP then no MAX SNP-hard problem is in PTAS. The class of MAX SNP-hard problems includes MAX 2SAT, MAX CUT, vertex cover [82], metric TSP [83], Steiner tree [26], shortest superstrings [27], MAX 3DM [67], and multiway cut <ref> [38] </ref>. Our theorem above implies that for every one of these problems , there exists a threshold * such that approximating within a factor 1 * is NP-hard.
Reference: [39] <author> W. De la Vega and G. Lueker. </author> <title> Bin Packing can be solved within 1 * in Linear Time. </title> <journal> Combinatorica, </journal> <volume> vol. 1, </volume> <pages> pages 349355, </pages> <year> 1981. </year>
Reference: [40] <author> R. Fagin. </author> <title> Generalized first-order spectra and polynomial-time recognizable sets. </title> <editor> In Richard Karp (ed.), </editor> <title> Complexity of Computation, </title> <publisher> AMS, </publisher> <year> 1974. </year>
Reference-contexts: They used second order logic to define a class of NP optimization problems called MAX SNP that is contained within APX. (The inspiration to use 2nd order logic came from the work of Fagin <ref> [40] </ref> and Kolaitis and Vardi [73].) They also defined a notion of approximation-preserving reductions and, thereby, the notion of completeness and hardness for MAX SNP.
Reference: [41] <author> U. Feige. </author> <title> A threshold of ln n for Set Cover. </title> <booktitle> In Proceedings of the Twenty Eighth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1996. </year>
Reference-contexts: This line of research was initiated by Bellare, Goldwasser, Lund, and Russell [21] who provided improved non-approximability results with explicit constants for a number of problems, such as MAX 3SAT, MAX CLIQUE, Chromatic Number and Set Cover. Since then a number of works <ref> [20, 23, 41, 43, 44, 50, 61, 62, 63] </ref> have focussed on improving 38 the non-approximability results. These results have culminated with some remarkably strong inapproximability results listed below. <p> Feige and Kilian [44], combined with Hstad [63], show that Chromatic number is hard to approximate to within a factor n 1* , for every positive *, unless NP RP. 4. Feige <ref> [41] </ref> shows that Set Cover is hard to approximate to within 1 o1 ln n unless NP DTIMEn log log n . We note all the results above are tight.
Reference: [42] <author> U. Feige, S. Goldwasser, L. Lovsz, S. Safra, and M. Szegedy. </author> <title> Interactive proofs and the hardness of approximating cliques. </title> <journal> Journal of the ACM, </journal> <volume> 43(2) </volume> <pages> 268-292, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: But the status of many other problems remained open. For example, it was not known if the clique problem is in APX. In a recent breakthrough, Feige, Goldwasser, Lovsz, Safra, and Szegedy <ref> [42] </ref> provided strong evidence that clique is not in APX. They showed that if it is possible to approximate the clique number to within any constant factor in polynomial time, then every NP problem can be solved in n Olog log n time. <p> Babai, Fortnow, Levin and Szegedy [14] introduced the notion of transparent membership proofs, namely, membership proofs that can be checked in polylogarithmic time, provided the input is encoded with some error-correcting code. They showed that NP languages have such proofs. Feige et al. <ref> [42] </ref> showed a similar result, but with a somewhat more efficient verifier. Arora and Safra [6] further improved the efficiency of checking membership proofs for NP languages. They also gave a surprising new characterization of NP. We describe their result below, and describe how we have improved upon it. <p> These parameters were first highlighted by the work of Feige et al.<ref> [42] </ref>. In fact, the definition of a class very similar to PCP was implicit in the work of Feige et al. [42]. <p> NP [ c&gt;2 PCPlog c n; log c n; (Babai, Fortnow, Levin, and Szegedy [14]). NP [ c&gt;0 PCPc log n loglog n; c log n loglog n (Feige, Goldwasser, Lovsz, Safra, and Szegedy <ref> [42] </ref>). NP [ c&gt;0 PCPc log n; c p log n (Arora and Safra [6]). Notice that the first and the last of the above relations are exact characterizations of the complexity classes NEXP and NP, respectively. <p> Notice that the first and the last of the above relations are exact characterizations of the complexity classes NEXP and NP, respectively. In this paper we improve upon the last result (see Theorem 4). 1.1.2 PCP and Non-approximability A result due to Feige et al. <ref> [42] </ref> implies that in order to prove the hardness of approximating the clique number, it suffices to show that some NP-complete language is low in the PCP hierarchy. The following statement summarizes this result. 4 Theorem 2 ([42]) Suppose q : Z ! Z is a logarithmically bounded non-decreasing function and <p> Remark: The above statement is a well-known implication of the result of Feige et al. <ref> [42] </ref>. It uses the idea of reducing the error probability of the verifier using recycled random bits (see [32, 65]). For further details see [6]. <p> The previous best result (in [6]) shows that approximating clique to within a factor of 2 log N is NP-hard. Our techniques. Our proof of Theorem 4 uses techniques similar to those in recent results about PCPs (Babai, Fortnow, Levin, and Szegedy [14]; Feige, Goldwasser, Lovsz, Safra and Szegedy <ref> [42] </ref>; and Arora-Safra [6]). The concept of Recursive Proof Checking invented by Arora and Safra plays a particularly important role (see Section 2). We were also inuenced by work on program checking and correcting, especially Blum, Luby, and Rubinfeld [30], and Rubinfeld and Sudan [90]. <p> The first, called the low degree test, efficiently determines if a given oracle O : F m ! F is ffi-close to some degree d polynomial, where ffi is some small constant. Low degree tests were invented as part of work on proof checking <ref> [13, 14, 42, 6, 56, 90] </ref>. Efficiency is of paramount concern to us, so we would like the test to make as few queries to the oracle as possible. Most low degree tests known make polym; d; 1=ffi queries.
Reference: [43] <author> U. Feige and J. Kilian. </author> <title> Two prover protocols Low error at affordable rates. </title> <booktitle> Pro ceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: Bellare, Goldwasser, Lund and Russell [21] construct verifiers that use only 4 queries and logarithmic randomness to get the error down to an arbitrarily small constant (with polyloglog sized answer sizes). Feige and Kilian <ref> [43] </ref> construct verifiers with 2 queries, arbitrarily small error, and constant answer sizes. Tardos [96] shows how to get verifier that makes 3 queries and whose error goes down subexponentially in the answer size. <p> This line of research was initiated by Bellare, Goldwasser, Lund, and Russell [21] who provided improved non-approximability results with explicit constants for a number of problems, such as MAX 3SAT, MAX CLIQUE, Chromatic Number and Set Cover. Since then a number of works <ref> [20, 23, 41, 43, 44, 50, 61, 62, 63] </ref> have focussed on improving 38 the non-approximability results. These results have culminated with some remarkably strong inapproximability results listed below. <p> But q has since been computed explicitly and then reduced through tighter analysis. It has gone from 29 [21] to to 22 <ref> [43] </ref> to 16 [20] to at most 9 (which is the best that can be inferred directly from [63] though this result is not optimized.) Parameters that are somewhat related to q are the free-bit parameter introduced by Feige and Kilian [43] and the amortized free-bit parameter introduced by Bellare and <p> It has gone from 29 [21] to to 22 <ref> [43] </ref> to 16 [20] to at most 9 (which is the best that can be inferred directly from [63] though this result is not optimized.) Parameters that are somewhat related to q are the free-bit parameter introduced by Feige and Kilian [43] and the amortized free-bit parameter introduced by Bellare and Sudan [23]. Reducing the latter parameter is necessary and sufficient for proving good inapproximability results for MAX CLIQUE [20], and Hstad [61, 62] has shown how to make this parameter smaller than every fixed ffi &gt; 0 .
Reference: [44] <author> U. Feige and J. Kilian. </author> <title> Zero knowledge and chromatic number. </title> <booktitle> Proceedings of the Eleventh Annual Conference on Complexity Theory , IEEE, </booktitle> <year> 1996. </year>
Reference-contexts: This line of research was initiated by Bellare, Goldwasser, Lund, and Russell [21] who provided improved non-approximability results with explicit constants for a number of problems, such as MAX 3SAT, MAX CLIQUE, Chromatic Number and Set Cover. Since then a number of works <ref> [20, 23, 41, 43, 44, 50, 61, 62, 63] </ref> have focussed on improving 38 the non-approximability results. These results have culminated with some remarkably strong inapproximability results listed below. <p> Hstad [62] has shown that MAX CLIQUE is hard to approximate to within a factor n 1* , for every positive *, unless NP RP. 3. Feige and Kilian <ref> [44] </ref>, combined with Hstad [63], show that Chromatic number is hard to approximate to within a factor n 1* , for every positive *, unless NP RP. 4.
Reference: [45] <author> U. Feige and L. Lovsz. </author> <title> Two-prover one-round proof systems: Their power and their problems. </title> <booktitle> Proceedings of the Twenty Fourth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1992. </year> <month> 43 </month>
Reference-contexts: The discovery of Theorem 2 inspired the search for other connections between probabilistic proof checking and non-approximability (Bellare [18], Bellare and Rogaway [22], Feige and Lovsz <ref> [45] </ref>, and Zuckerman [100]). Another such connection is reported by Arora, Motwani, Safra, Sudan and Szegedy [5], which shows the connection between PCP's and the hardness of approximating MAX 3SAT. The following theorem summarizes this result; for a proof see Section 3. <p> Finally, we were inuenced by work on constant prover 1-round interactive proof systems <ref> [74, 45] </ref>. <p> We note that an r n; pn; cn; an outer verifier with pn O1 is very similar to a constant prover 1 round interactive proof system [46]. Fairly efficient constructions of such verifiers are implicit in <ref> [74, 45] </ref>; for instance, it is shown that NP [ c&lt;1 RPCPlog c n; O1; log c n; log c n; 1=n. We also observe that the definition of RPCP generalizes that of PCP.
Reference: [46] <author> L. Fortnow, J. Rompel, and M. Sipser. </author> <title> On the power of multi-prover interactive protocols. </title> <journal> Theoretical Computer Science, </journal> <volume> 134(2) </volume> <pages> 545-557, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Another variant of proof verification, due to Ben-Or, Goldwasser, Kilian and Wigderson [24], involves a probabilistic polynomial-time verifier interacting with more than one mutually non-interacting provers. The class of languages with such interactive proofs is called MIP (for Multi-prover Interactive Proofs). Fortnow, Rompel and Sipser <ref> [46] </ref> gave an equivalent definition of MIP as languages that have a probabilistic polynomial-time oracle verifier that checks membership proofs (possibly of exponential length) using oracle access to the proof. This equivalent definition is described below, using the term probabilistically checkable proofs introduced by Arora and Safra. <p> We describe their result below, and describe how we have improved upon it. Arora and Safra define a hierarchy of complexity classes called PCP (for Probabilistically Checkable Proofs). This definition uses the notion of a probabilistic oracle verifier of Fortnow et al. <ref> [46] </ref> and classifies languages based on how efficiently such a verifier can check membership proofs for them. The notion of efficiency refers to the number of random bits used by the verifier as well as the number of bits it reads in the membership proof. <p> We note that an r n; pn; cn; an outer verifier with pn O1 is very similar to a constant prover 1 round interactive proof system <ref> [46] </ref>. Fairly efficient constructions of such verifiers are implicit in [74, 45]; for instance, it is shown that NP [ c&lt;1 RPCPlog c n; O1; log c n; log c n; 1=n. We also observe that the definition of RPCP generalizes that of PCP.
Reference: [47] <author> R. Freivalds. </author> <title> Fast Probabilistic Algorithms. </title> <booktitle> Proceedings of Symposium on Mathe matical Foundations of Computer Science, Springer-Verlag Lecture Notes in Computer Science, v. </booktitle> <volume> 74, </volume> <pages> pages 5769, </pages> <year> 1979. </year>
Reference-contexts: If f x f y f 0 x@y then accept else reject. The test above is based on Freivalds test <ref> [47] </ref> for matrix multiplication and its correctness is established as follows. Proposition 33 1. <p> For part (2) we look at the n fi n matrix B fb ij g. We would like to compare B with the n fi n matrix C xx T . Assume for the sake of contradiction that the two matrices are not identical. Then Freivalds' probabilistic test <ref> [47] </ref> for matrix identity guarantees that for a randomly chosen bit vector z 2 , the probability that the vectors Bz 2 and Cz 2 turn out to be distinct is at least 1=2.
Reference: [48] <author> K. Friedl, Zs. Htsgi and A. Shen. </author> <title> Low-degree testing. </title> <booktitle> Proceedings of the Fifth Symposium on Discrete Algorithms, ACM, </booktitle> <year> 1994. </year>
Reference: [49] <author> K. Friedl and M. Sudan. </author> <title> Some improvements to low-degree tests. </title> <booktitle> Proceedings of the Third Israel Symposium on Theory and Computing Systems, </booktitle> <year> 1995. </year>
Reference-contexts: the converse is also true if jFj is sufficiently large: If a function p: F n ! F is such that the restriction of p to every line in F m is a univariate degree d polynomial, then f itself is a degree d polynomial (see, for example, [90] or <ref> [49] </ref> the latter contains a tight analysis of condition under which this equivalence holds).
Reference: [50] <author> M. Frer. </author> <title> Improved hardness results for approximating the chromatic number. </title> <booktitle> Proceedings of the Thirty Sixth Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: This line of research was initiated by Bellare, Goldwasser, Lund, and Russell [21] who provided improved non-approximability results with explicit constants for a number of problems, such as MAX 3SAT, MAX CLIQUE, Chromatic Number and Set Cover. Since then a number of works <ref> [20, 23, 41, 43, 44, 50, 61, 62, 63] </ref> have focussed on improving 38 the non-approximability results. These results have culminated with some remarkably strong inapproximability results listed below.
Reference: [51] <author> M. Garey and D. Johnson. </author> <title> The complexity of near-optimal graph coloring. </title> <journal> Journal of the ACM, </journal> <volume> 23 </volume> <pages> 43-49, </pages> <year> 1976. </year>
Reference: [52] <author> M. Garey and D. Johnson. </author> <title> Strong NP-completeness results: motivation, exam ples and implications. </title> <journal> Journal of the ACM, </journal> <volume> 25 </volume> <pages> 499-508, </pages> <year> 1978. </year>
Reference-contexts: The class of such problems is called APX. It follows from the definitions that FPTAS PTAS APX. Researchers have also tried to show that problems do not belong to some of the classes above. The notion of strong NP-completeness was introduced by Garey and Johnson <ref> [52] </ref> to show that a large collection of problems are not in FPTAS if P NP. Sahni and Gonzalez [92] showed that the (unrestricted) traveling salesman problem is not in APX if P NP. But the status of many other problems remained open.
Reference: [53] <author> M. Garey and D. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: If P NP, no polynomial-time algorithm can solve them optimally. Given this evidence of intractability, researchers have attempted to design polynomial time approximation algorithms for the NP-hard optimization problems <ref> [53, 81] </ref>. An algorithm is said to approximate a problem within a factor c, where c 1, if it computes, for every instance of the problem, a solution whose cost (or value) is within a factor c of the optimum. <p> While all NP-complete decision problems are polynomial-time equivalent, research over the past two decades <ref> [53, 81] </ref>, starting with the papers of Johnson [66] and Sahni and Gonzales [92], suggests that NP-hard optimization problems differ vastly if we are interested in computing approximately optimal solutions. <p> Express x;R in CNF (conjunctive normal form) over the variables v i 1 ; : : : ; v i q . Observe that x;R has at most 2 q clauses, each of length at most q. Then express it in the standard way (see e.g. <ref> [53] </ref>) as a 3-CNF formula, by using up to q auxiliary variables. (These auxiliary variables should be unique to R and should not be reused for any other random string.) Let x;R denote this 3-CNF formula. Then x R Denote the number of clauses of x by m.
Reference: [54] <author> M. Garey, D. Johnson and L. Stockmeyer. </author> <title> Some simplified NP-complete graph problems. </title> <booktitle> Theoretical Computer Science 1 </booktitle> <pages> 237-267, </pages> <year> 1976. </year>
Reference: [55] <author> P. Gemmell and M. Sudan. </author> <title> Highly resilient correctors for polynomials. </title> <journal> Informa tion Processing Letters, </journal> <volume> 43(4) </volume> <pages> 169-174, </pages> <month> September </month> <year> 1992. </year>
Reference: [56] <author> P. Gemmell, R. Lipton, R. Rubinfeld, M. Sudan, and A. Wigderson. </author> <title> Self testing/correcting for polynomials and for approximate functions. </title> <booktitle> Proceedings of the Twenty Third Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1991. </year>
Reference-contexts: The first, called the low degree test, efficiently determines if a given oracle O : F m ! F is ffi-close to some degree d polynomial, where ffi is some small constant. Low degree tests were invented as part of work on proof checking <ref> [13, 14, 42, 6, 56, 90] </ref>. Efficiency is of paramount concern to us, so we would like the test to make as few queries to the oracle as possible. Most low degree tests known make polym; d; 1=ffi queries. <p> The procedure owes its origins to the work of Beaver and Feigenbaum [17] and Lipton [76]. The specific analysis given below is borrowed from the work of Gemmell, Lipton, Rubinfeld, Sudan and Wigderson <ref> [56] </ref> and allows the number of queries to be independent of d, for error bounded away from 1.
Reference: [57] <author> M. Goemans and D. Williamson. </author> <title> Improved approximation algorithms for maxi mum cut and satisfiability problems using semidefinite programming. </title> <journal> Journal of the ACM, </journal> <volume> 42(6) </volume> <pages> 1115-1145, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Two striking results in this direction are those of Goemans and Williamson <ref> [57] </ref> and Arora [2]. Goemans and Williamson [57] show how to use semidefinite programming to give better approximation algorithms for MAX-2SAT and MAX-CUT. <p> Two striking results in this direction are those of Goemans and Williamson <ref> [57] </ref> and Arora [2]. Goemans and Williamson [57] show how to use semidefinite programming to give better approximation algorithms for MAX-2SAT and MAX-CUT.
Reference: [58] <author> O. Goldreich. </author> <title> A Taxonomy of Proof Systems. In Complexity Theory Retrospective II, L.A. </title> <editor> Hemaspaandra and A. Selman (eds.), </editor> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1997. </year>
Reference: [59] <author> S. Goldwasser, S. Micali, and C. Rackoff. </author> <title> The knowledge complexity of inter active proof-systems. </title> <journal> SIAM J. on Computing, </journal> <volume> 18(1) </volume> <pages> 186-208, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Generalizing the above definition of NP leads to definitions of interesting new complexity classes, which have been the subject of intense research in the past decade. Goldwasser, Micali and Rackoff <ref> [59] </ref> and Babai [10, 16] allowed the verifier to be a probabilistic polynomial-time Turing Machine that interacts with a prover, which is an infinitely powerful Turing Machine trying to convince the verifier that the input x is in the language.
Reference: [60] <author> R. Graham. </author> <title> Bounds for certain multiprocessing anomalies, </title> <journal> Bell Systems Technical Journal, </journal> <volume> 45 </volume> <pages> 1563-1581, </pages> <year> 1966. </year>
Reference-contexts: The class of problems with such approximation schemes is called FPTAS. Some other NP-hard problems, such as the problem of scheduling processes on a multiple processor machine so as to minimize their makespan <ref> [60] </ref>, have a Polynomial Time Approximation Scheme: an algorithm that, for any given * &gt; 0, approximates the problem within a factor 1 * in time that is polynomial in the input size (and could depend arbitrarily upon 1=*). The class of problems with such approximation schemes is called PTAS.
Reference: [61] <author> J. Hstad. </author> <title> Testing of the long code and hardness for clique. </title> <booktitle> Proceedings of the Twenty Eighth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1996. </year>
Reference-contexts: This line of research was initiated by Bellare, Goldwasser, Lund, and Russell [21] who provided improved non-approximability results with explicit constants for a number of problems, such as MAX 3SAT, MAX CLIQUE, Chromatic Number and Set Cover. Since then a number of works <ref> [20, 23, 41, 43, 44, 50, 61, 62, 63] </ref> have focussed on improving 38 the non-approximability results. These results have culminated with some remarkably strong inapproximability results listed below. <p> Reducing the latter parameter is necessary and sufficient for proving good inapproximability results for MAX CLIQUE [20], and Hstad <ref> [61, 62] </ref> has shown how to make this parameter smaller than every fixed ffi &gt; 0 . Other technical improvements.
Reference: [62] <author> J. Hstad. </author> <title> Clique is hard to approximate within n 1* . Proceedings of the Thirty Seventh Annual Symposium on the Foundations of Computer Science, </title> <publisher> IEEE, </publisher> <year> 1996. </year>
Reference-contexts: This line of research was initiated by Bellare, Goldwasser, Lund, and Russell [21] who provided improved non-approximability results with explicit constants for a number of problems, such as MAX 3SAT, MAX CLIQUE, Chromatic Number and Set Cover. Since then a number of works <ref> [20, 23, 41, 43, 44, 50, 61, 62, 63] </ref> have focussed on improving 38 the non-approximability results. These results have culminated with some remarkably strong inapproximability results listed below. <p> Hstad [63] has shown that MAX 3SAT is NP-hard to approximate within a factor 7=8 *, for every * &gt; 0. 2. Hstad <ref> [62] </ref> has shown that MAX CLIQUE is hard to approximate to within a factor n 1* , for every positive *, unless NP RP. 3. <p> Reducing the latter parameter is necessary and sufficient for proving good inapproximability results for MAX CLIQUE [20], and Hstad <ref> [61, 62] </ref> has shown how to make this parameter smaller than every fixed ffi &gt; 0 . Other technical improvements.
Reference: [63] <author> J. Hstad. </author> <title> Some optimal inapproximability results. </title> <booktitle> Proceedings of the Twenty Eighth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1997, </year> <month> 44 </month>
Reference-contexts: This line of research was initiated by Bellare, Goldwasser, Lund, and Russell [21] who provided improved non-approximability results with explicit constants for a number of problems, such as MAX 3SAT, MAX CLIQUE, Chromatic Number and Set Cover. Since then a number of works <ref> [20, 23, 41, 43, 44, 50, 61, 62, 63] </ref> have focussed on improving 38 the non-approximability results. These results have culminated with some remarkably strong inapproximability results listed below. <p> These results have culminated with some remarkably strong inapproximability results listed below. The attributions only cite the latest result in a long sequence of papers the interested reader can look up the cited result for details of the intermediate results. 1. Hstad <ref> [63] </ref> has shown that MAX 3SAT is NP-hard to approximate within a factor 7=8 *, for every * &gt; 0. 2. Hstad [62] has shown that MAX CLIQUE is hard to approximate to within a factor n 1* , for every positive *, unless NP RP. 3. <p> Hstad [62] has shown that MAX CLIQUE is hard to approximate to within a factor n 1* , for every positive *, unless NP RP. 3. Feige and Kilian [44], combined with Hstad <ref> [63] </ref>, show that Chromatic number is hard to approximate to within a factor n 1* , for every positive *, unless NP RP. 4. Feige [41] shows that Set Cover is hard to approximate to within 1 o1 ln n unless NP DTIMEn log log n . <p> But q has since been computed explicitly and then reduced through tighter analysis. It has gone from 29 [21] to to 22 [43] to 16 [20] to at most 9 (which is the best that can be inferred directly from <ref> [63] </ref> though this result is not optimized.) Parameters that are somewhat related to q are the free-bit parameter introduced by Feige and Kilian [43] and the amortized free-bit parameter introduced by Bellare and Sudan [23].
Reference: [64] <author> O. Ibarra and C. Kim. </author> <title> Fast approximation algorithms for the knapsack and sum of subset problems. </title> <journal> Journal of the ACM, </journal> <volume> 22 </volume> <pages> 463-468, </pages> <year> 1975. </year>
Reference-contexts: Some NP-hard problems, such as the knapsack problem <ref> [91, 64] </ref>, have a Fully Polynomial Time Approximation Scheme: an algorithm that, for any given * &gt; 0, approximates the problem within a factor 1 * in time that is polynomial in the input size and 1=*. The class of problems with such approximation schemes is called FPTAS.
Reference: [65] <author> R. Impagliazzo and D. Zuckerman. </author> <title> How to Recycle Random Bits. </title> <booktitle> Proceedings of the Thirtieth Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1989. </year>
Reference-contexts: Remark: The above statement is a well-known implication of the result of Feige et al. [42]. It uses the idea of reducing the error probability of the verifier using recycled random bits (see <ref> [32, 65] </ref>). For further details see [6]. Thus, as a consequence of the new characterization of NP due to Arora and Safra [6], it follows that approximating the clique number within a factor 2 p log N is NP-hard. <p> In fact, even the weaker verifier of Babai, Fort-now, Levin and Szegedy [14] would suffice for our purpose after some modification. This modification would cut down the number of random bits needed in [14] by using the idea of recycling randomness <ref> [32, 65] </ref>.) The following theorem was only implicit in the earlier versions of [6] but is explicit in the final version.
Reference: [66] <author> D. Johnson. </author> <title> Approximation algorithms for combinatorial problems. </title> <journal> J. Computer and Systems Sci. </journal> <volume> 9 </volume> <pages> 256-278, </pages> <year> 1974. </year>
Reference-contexts: While all NP-complete decision problems are polynomial-time equivalent, research over the past two decades [53, 81], starting with the papers of Johnson <ref> [66] </ref> and Sahni and Gonzales [92], suggests that NP-hard optimization problems differ vastly if we are interested in computing approximately optimal solutions.
Reference: [67] <author> V. Kann. </author> <title> Maximum bounded 3-dimensional matching is MAX SNP-complete. </title> <journal> Infor mation Processing Letters, </journal> <volume> 37 </volume> <pages> 27-35, </pages> <year> 1991. </year>
Reference-contexts: As noted earlier, Theorem 5 implies that if P NP then no MAX SNP-hard problem is in PTAS. The class of MAX SNP-hard problems includes MAX 2SAT, MAX CUT, vertex cover [82], metric TSP [83], Steiner tree [26], shortest superstrings [27], MAX 3DM <ref> [67] </ref>, and multiway cut [38]. Our theorem above implies that for every one of these problems , there exists a threshold * such that approximating within a factor 1 * is NP-hard.
Reference: [68] <author> N. Karmakar and R. Karp. </author> <title> An Efficient Approximation Scheme For The One Dimensional Bin Packing Problem. </title> <booktitle> Proceedings of the Twenty Third Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1982. </year>
Reference: [69] <author> R. Karp. </author> <title> Reducibility among combinatorial problems. </title> <editor> In R. E. Miller and J. W. Thatcher, editors, </editor> <booktitle> Complexity of Computer Computations, Advances in Computing Research, </booktitle> <pages> pages 85103. </pages> <publisher> Plenum Press, </publisher> <year> 1972. </year>
Reference-contexts: 1 Introduction Classifying optimization problems according to their computational complexity is a central endeavor in theoretical computer science. The theory of NP-completeness, developed by Cook [36], Karp <ref> [69] </ref> and Levin [75], shows that many decision problems of interest, such as satisfiability, are NP-complete. This theory also shows that decision versions of many optimization problems, such as the traveling salesman problem, fl arora@cs.princeton.edu. Department of Computer Science, Princeton University, NJ 08544.
Reference: [70] <author> D. Karger, R. Motwani, and G. Ramkumar. </author> <title> On approximating the longest path in a graph. </title> <journal> Algorithmica, </journal> <volume> 18(1) </volume> <pages> 82-98, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: Arora, Babai, Stern, and Sweedyk [3] prove hardness results for a collection of problems involving integral lattices, codes, or linear equations/inequations. These include Nearest Lattice Vector, Nearest Codeword, and the Shortest Lattice Vector under the ` 1 norm. Karger, Motwani, and Ramkumar <ref> [70] </ref> prove the hardness of approximating the longest path in a graph to within a 2 log 1* n factor, for any * &gt; 0. There are many other results which we haven't mentioned here; see the compendium [37] or the survey [4]. Improved analysis of outer verifiers.
Reference: [71] <author> S. Khanna, N. Linial, and S. Safra. </author> <title> On the hardness of approximating the chro matic number. </title> <booktitle> Proceedings of the Second Israel Symposium on Theory and Computing Systems, </booktitle> <year> 1993. </year>
Reference-contexts: In a different work, Lund and Yannakakis [79] show hardness results for approximation versions of a large set of maximum subgraph problems. (These problems involve finding the largest subgraph that satisfies a a property , where is a nontrivial graph property closed under vertex deletion.) Khanna, Linial and Safra <ref> [71] </ref> study the hardness of coloring 3-colorable graph. They show that coloring a 3-colorable graph with 4 colors is NP-hard. Arora, Babai, Stern, and Sweedyk [3] prove hardness results for a collection of problems involving integral lattices, codes, or linear equations/inequations.
Reference: [72] <author> S. Khanna, R. Motwani, M. Sudan, and U. Vazirani. </author> <title> On syntactic versus compu tational views of approximability. </title> <note> To appear SIAM Journal on Computing. Preliminary version in Proceedings of the Thirty Fifth Annual Symposium on the Foundations of Computer Science, IEEE, </note> <year> 1994. </year>
Reference: [73] <author> P. Kolaitis and M. Vardi. </author> <title> The decision problem for the probabilities of higher order properties. </title> <booktitle> Proceedings of the Nineteenth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1987. </year>
Reference-contexts: They used second order logic to define a class of NP optimization problems called MAX SNP that is contained within APX. (The inspiration to use 2nd order logic came from the work of Fagin [40] and Kolaitis and Vardi <ref> [73] </ref>.) They also defined a notion of approximation-preserving reductions and, thereby, the notion of completeness and hardness for MAX SNP.
Reference: [74] <author> D. Lapidot and A. Shamir. </author> <title> Fully parallelized multi-prover protocols for NEXP time. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 54(2) </volume> <pages> 215-220, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Finally, we were inuenced by work on constant prover 1-round interactive proof systems <ref> [74, 45] </ref>. <p> We note that an r n; pn; cn; an outer verifier with pn O1 is very similar to a constant prover 1 round interactive proof system [46]. Fairly efficient constructions of such verifiers are implicit in <ref> [74, 45] </ref>; for instance, it is shown that NP [ c&lt;1 RPCPlog c n; O1; log c n; log c n; 1=n. We also observe that the definition of RPCP generalizes that of PCP.
Reference: [75] <author> L. Levin. </author> <title> Universal'ny e pereborny e zadachi (Universal search problems : in Rus sian). </title> <journal> Problemy Peredachi Informatsii, </journal> <note> 9(3):265266, 1973. A corrected English translation appears in an appendix to Trakhtenbrot [97]. </note>
Reference-contexts: 1 Introduction Classifying optimization problems according to their computational complexity is a central endeavor in theoretical computer science. The theory of NP-completeness, developed by Cook [36], Karp [69] and Levin <ref> [75] </ref>, shows that many decision problems of interest, such as satisfiability, are NP-complete. This theory also shows that decision versions of many optimization problems, such as the traveling salesman problem, fl arora@cs.princeton.edu. Department of Computer Science, Princeton University, NJ 08544.
Reference: [76] <author> R. Lipton. </author> <title> New directions in testing. </title> <editor> In J. Feigenbaum and M. Merritt, editors, </editor> <booktitle> Distributed Computing and Cryptography, volume 2 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pages 191202. </booktitle> <publisher> American Mathematical Society, </publisher> <year> 1991. </year>
Reference-contexts: We now describe a procedure which computes px using few probes into O and an auxiliary oracle B. The procedure owes its origins to the work of Beaver and Feigenbaum [17] and Lipton <ref> [76] </ref>. The specific analysis given below is borrowed from the work of Gemmell, Lipton, Rubinfeld, Sudan and Wigderson [56] and allows the number of queries to be independent of d, for error bounded away from 1.
Reference: [77] <author> C. Lund, L. Fortnow, H. Karloff, and N. Nisan. </author> <title> Algebraic Methods for Interactive Proof Systems. </title> <journal> J. ACM, </journal> <volume> 39, 859868, </volume> <year> 1992. </year>
Reference-contexts: A surprising recent result, due to Lund, Fortnow, Karloff and Nisan <ref> [77] </ref> and Shamir [94], has shown that every language in PSPACE which is suspected to be a much larger class than NP admits such interactive membership proofs.
Reference: [78] <author> C. Lund and M. Yannakakis. </author> <title> On the hardness of approximating minimization problems. </title> <journal> Journal of the ACM, </journal> <volume> 41(5) </volume> <pages> 960-981, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: New non-approximability results. Many new results have been proved regarding the hardness of approximation. Lund and Yannakakis <ref> [78] </ref> show that approximating the chromatic number of a graph within a factor n * is NP-hard, for some * &gt; 0. They also show that if NP 6 Dtimen polylog n , then Set Cover cannot be approximated within a factor 0:5 log n in polynomial time.
Reference: [79] <author> C. Lund and M. Yannakakis. </author> <title> The approximation of maximum subgraph prob lems. </title> <booktitle> Proceedings of ICALP 93, Lecture Notes in Computer Science Vol. </booktitle> <volume> 700, </volume> <publisher> Springer Verlag, </publisher> <year> 1993. </year> <month> 45 </month>
Reference-contexts: They also show that if NP 6 Dtimen polylog n , then Set Cover cannot be approximated within a factor 0:5 log n in polynomial time. In a different work, Lund and Yannakakis <ref> [79] </ref> show hardness results for approximation versions of a large set of maximum subgraph problems. (These problems involve finding the largest subgraph that satisfies a a property , where is a nontrivial graph property closed under vertex deletion.) Khanna, Linial and Safra [71] study the hardness of coloring 3-colorable graph.
Reference: [80] <author> J. Mitchell. </author> <title> Guillotine subdivisions approximate polygonal subdivisions: Part II A simple PTAS for geometric k-MST, TSP, and related problems. </title> <type> Preliminary manuscript, </type> <month> April 30, </month> <year> 1996. </year> <note> To appear in SIAM J. Computing. </note>
Reference-contexts: Goemans and Williamson [57] show how to use semidefinite programming to give better approximation algorithms for MAX-2SAT and MAX-CUT. Arora [2] has discovered a polynomial time approximation scheme (PTAS) for Euclidean TSP and Euclidean Steiner tree problem. (Mitchell <ref> [80] </ref> independently discovered similar results a few months later.) These were two notable problems not addressed by our hardness result in this paper since they were not known to be MAX SNP-hard.
Reference: [81] <author> R. Motwani. </author> <title> Lecture Notes on Approximation Algorithms. </title> <type> Technical Report, </type> <institution> Dept. of Computer Science, Stanford University (1992). </institution>
Reference-contexts: If P NP, no polynomial-time algorithm can solve them optimally. Given this evidence of intractability, researchers have attempted to design polynomial time approximation algorithms for the NP-hard optimization problems <ref> [53, 81] </ref>. An algorithm is said to approximate a problem within a factor c, where c 1, if it computes, for every instance of the problem, a solution whose cost (or value) is within a factor c of the optimum. <p> While all NP-complete decision problems are polynomial-time equivalent, research over the past two decades <ref> [53, 81] </ref>, starting with the papers of Johnson [66] and Sahni and Gonzales [92], suggests that NP-hard optimization problems differ vastly if we are interested in computing approximately optimal solutions.
Reference: [82] <author> C. Papadimitriou and M. Yannakakis. </author> <title> Optimization, approximation and com plexity classes. </title> <journal> Journal of Computer and System Sciences 43 </journal> <pages> 425-440, </pages> <year> 1991. </year>
Reference-contexts: However, there has been less progress in showing that APX problems are not in PTAS. The one important work in this direction is due to Papadimitriou and Yannakakis <ref> [82] </ref>, who show that a large subset of APX problems are essentially equivalent in this regard: either all of them belong to PTAS, or none of them do. <p> As noted earlier, Theorem 5 implies that if P NP then no MAX SNP-hard problem is in PTAS. The class of MAX SNP-hard problems includes MAX 2SAT, MAX CUT, vertex cover <ref> [82] </ref>, metric TSP [83], Steiner tree [26], shortest superstrings [27], MAX 3DM [67], and multiway cut [38]. Our theorem above implies that for every one of these problems , there exists a threshold * such that approximating within a factor 1 * is NP-hard.
Reference: [83] <author> C. Papadimitriou and M. Yannakakis. </author> <title> The traveling salesman problem with dis tances one and two. </title> <institution> Mathematics of Operations Research, </institution> <year> 1992. </year>
Reference-contexts: As noted earlier, Theorem 5 implies that if P NP then no MAX SNP-hard problem is in PTAS. The class of MAX SNP-hard problems includes MAX 2SAT, MAX CUT, vertex cover [82], metric TSP <ref> [83] </ref>, Steiner tree [26], shortest superstrings [27], MAX 3DM [67], and multiway cut [38]. Our theorem above implies that for every one of these problems , there exists a threshold * such that approximating within a factor 1 * is NP-hard.
Reference: [84] <author> A. Paz and S. Moran. </author> <title> Non-deterministic polynomial optimization problems and their approximation. </title> <journal> Theoretical Computer Science, </journal> <volume> 15 </volume> <pages> 251-277, </pages> <year> 1981. </year>
Reference: [85] <author> S. Phillips and S. Safra. </author> <title> PCP and tighter bounds for approximating MAXSNP. </title> <type> Manuscript, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference: [86] <author> A. Polishchuk and D. Spielman. </author> <title> Nearly Linear Sized Holographic Proofs. </title> <booktitle> Pro ceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: This was reduced to n 2* by Sudan [95]. The main hurdle in further improvement seemed to be Arora and Safra's proof [6] of Theorem 69, which requires a field size quadratic in the degree. Polishchuk and Spielman <ref> [86] </ref> present a new proof of Theorem 69 that works when the field size is linear. By using this new proof, as well as some other new ideas, they bring the size of the encoding down to n 1* .
Reference: [87] <author> R. Raz. </author> <title> A parallel repetition theorem. </title> <booktitle> Proceedings of the Twenty Seventh Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: Feige and Kilian [43] construct verifiers with 2 queries, arbitrarily small error, and constant answer sizes. Tardos [96] shows how to get verifier that makes 3 queries and whose error goes down subexponentially in the answer size. Finally, all these constructions have been effectively superseded by Raz's proof <ref> [87] </ref> of the parallel repetition conjecture. This conjecture was open for a long time, and allows constructions of a verifier that makes 2 queries, and whose error goes down exponentially with the answer size.
Reference: [88] <author> R. Raz and S. Safra. </author> <title> A sub-constant error-probability low-degree test, and a sub constant error-probability PCP characterization of NP. </title> <booktitle> Proceedings of the Twenty Eighth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1997. </year>
Reference-contexts: This conjecture was open for a long time, and allows constructions of a verifier that makes 2 queries, and whose error goes down exponentially with the answer size. Very recently, Raz and Safra <ref> [88] </ref> have constructed verifiers making constant number of queries with logarithmic randomness and answer size, where the error is as low as 2 log 1* n for every * &gt; 0. An alternate construction is given in Arora and Sudan [7]. Better non-approximability results. <p> A similar result for a different test was shown earlier by Raz and Safra <ref> [88] </ref>. The proof of the correctness of the linearity test (Theorem 27) has also been improved in works by Bellare, Goldwasser, Lund and Russell [21] and Bellare, Coppersmith, Hastad, Kiwi and Sudan [19]. 39 Transparent math proofs. We briey mention an application that received much attention in the popular press.
Reference: [89] <author> R. Rubinfeld. </author> <title> A Mathematical Theory of Self-Checking, Self-Testing and Self Correcting Programs. </title> <type> Ph.D. thesis, </type> <institution> U.C. Berkeley, </institution> <year> 1990. </year>
Reference: [90] <author> R. Rubinfeld and M. Sudan. </author> <title> Robust characterizations of polynomials with ap plications to program testing. </title> <journal> SIAM Journal on Computing 25(2) </journal> <pages> 252-271, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: The concept of Recursive Proof Checking invented by Arora and Safra plays a particularly important role (see Section 2). We were also inuenced by work on program checking and correcting, especially Blum, Luby, and Rubinfeld [30], and Rubinfeld and Sudan <ref> [90] </ref>. The inuence of the former will be apparent in Section 5, while the latter work (together with a lemma of Arora and Safra [6]) is used in our analysis of a Low Degree Test described in Section 7.2. <p> The first, called the low degree test, efficiently determines if a given oracle O : F m ! F is ffi-close to some degree d polynomial, where ffi is some small constant. Low degree tests were invented as part of work on proof checking <ref> [13, 14, 42, 6, 56, 90] </ref>. Efficiency is of paramount concern to us, so we would like the test to make as few queries to the oracle as possible. Most low degree tests known make polym; d; 1=ffi queries. <p> Efficiency is of paramount concern to us, so we would like the test to make as few queries to the oracle as possible. Most low degree tests known make polym; d; 1=ffi queries. However, Rubinfeld and Sudan <ref> [90] </ref> give a test (which requires an auxiliary oracle in addition to O) whose number of queries depends on d but not on m. Arora and Safra [6] give a test whose number of queries depends only on m but not on d. <p> We observe in this paper that the two analyses can be combined to give a test whose number of queries is independent of d and m. This test and its analysis is essentially from <ref> [90] </ref>. Our only new contribution is to use a result from [6] (Theorem 69 in the appendix) instead of a weaker result from [90]. <p> This test and its analysis is essentially from <ref> [90] </ref>. Our only new contribution is to use a result from [6] (Theorem 69 in the appendix) instead of a weaker result from [90]. The second procedure in this section does the following: Given an oracle O : F m ! F which is ffi-close to a polynomial p, find the value of p at some specified point x 2 F m . <p> see that the converse is also true if jFj is sufficiently large: If a function p: F n ! F is such that the restriction of p to every line in F m is a univariate degree d polynomial, then f itself is a degree d polynomial (see, for example, <ref> [90] </ref> or [49] the latter contains a tight analysis of condition under which this equivalence holds). <p> The properties of the test are summarized below. Theorem 45 (follows by combining <ref> [90, 6] </ref>) There exist constants ffi 0 &gt; 0 and ff &lt; 1 such that for ffi ffi 0 , d 2 Z if F is a field of cardinality at least ffd 1 3 , then the following holds: 1.
Reference: [91] <author> S. Sahni. </author> <title> Approximate algorithms for the 0/1 knapsack problem, </title> <journal> Journal of the ACM, </journal> <volume> 22 </volume> <pages> 115-124, </pages> <year> 1975. </year>
Reference-contexts: Some NP-hard problems, such as the knapsack problem <ref> [91, 64] </ref>, have a Fully Polynomial Time Approximation Scheme: an algorithm that, for any given * &gt; 0, approximates the problem within a factor 1 * in time that is polynomial in the input size and 1=*. The class of problems with such approximation schemes is called FPTAS.
Reference: [92] <author> S. Sahni and T. Gonzales. </author> <title> P-complete approximation problems. </title> <journal> Journal of the ACM, </journal> <volume> 23 </volume> <pages> 555-565, </pages> <year> 1976. </year>
Reference-contexts: While all NP-complete decision problems are polynomial-time equivalent, research over the past two decades [53, 81], starting with the papers of Johnson [66] and Sahni and Gonzales <ref> [92] </ref>, suggests that NP-hard optimization problems differ vastly if we are interested in computing approximately optimal solutions. <p> Researchers have also tried to show that problems do not belong to some of the classes above. The notion of strong NP-completeness was introduced by Garey and Johnson [52] to show that a large collection of problems are not in FPTAS if P NP. Sahni and Gonzalez <ref> [92] </ref> showed that the (unrestricted) traveling salesman problem is not in APX if P NP. But the status of many other problems remained open. For example, it was not known if the clique problem is in APX.
Reference: [93] <author> J. Schwartz. </author> <title> Probabilistic algorithms for verification of polynomial identities. </title> <journal> Journal of the ACM, </journal> <volume> 27 </volume> <pages> 701-717, </pages> <year> 1980. </year>
Reference-contexts: The following lemma is often attributed to Schwartz <ref> [93] </ref>. Lemma 21 If f ; g 2 F d m and f 6 g, then f ; g 1 d= jFj. Remark 22 From the above lemma, it follows that the nearest polynomial g is unique if ffi &lt; 1=4 and d= jFj 1=2.
Reference: [94] <author> A. Shamir. </author> <title> IP = PSPACE. </title> <journal> Journal of the ACM, </journal> <volume> 39(4) </volume> <pages> 869-877, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: A surprising recent result, due to Lund, Fortnow, Karloff and Nisan [77] and Shamir <ref> [94] </ref>, has shown that every language in PSPACE which is suspected to be a much larger class than NP admits such interactive membership proofs. Another variant of proof verification, due to Ben-Or, Goldwasser, Kilian and Wigderson [24], involves a probabilistic polynomial-time verifier interacting with more than one mutually non-interacting provers.
Reference: [95] <author> M. Sudan. </author> <title> Efficient Checking of Polynomials and Proofs and the Hardness of Ap proximation Problems. </title> <type> Ph.D. Thesis, </type> <institution> U.C. Berkeley, </institution> <year> 1992. </year> <note> Also appears as ACM Distinguished Theses, Lecture Notes in Computer Science, no. 1001, Springer, </note> <year> 1996. </year>
Reference-contexts: Babai [11] raised the following question: How efficient can this encoding be? In our paper, encoding an assignment of size n requires polyn bits. This was reduced to n 2* by Sudan <ref> [95] </ref>. The main hurdle in further improvement seemed to be Arora and Safra's proof [6] of Theorem 69, which requires a field size quadratic in the degree. Polishchuk and Spielman [86] present a new proof of Theorem 69 that works when the field size is linear.
Reference: [96] <author> G. Tardos. </author> <title> Multi-prover encoding schemes and three prover proof systems. </title> <booktitle> Pro ceedings of the Ninth Annual Conference on Structure in Complexity Theory , IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: Bellare, Goldwasser, Lund and Russell [21] construct verifiers that use only 4 queries and logarithmic randomness to get the error down to an arbitrarily small constant (with polyloglog sized answer sizes). Feige and Kilian [43] construct verifiers with 2 queries, arbitrarily small error, and constant answer sizes. Tardos <ref> [96] </ref> shows how to get verifier that makes 3 queries and whose error goes down subexponentially in the answer size. Finally, all these constructions have been effectively superseded by Raz's proof [87] of the parallel repetition conjecture.
Reference: [97] <author> B. Trakhtenbrot. </author> <title> A survey of Russian approaches to Perebor (brute-force search) algorithms. </title> <booktitle> Annals of the History of Computing 6 </booktitle> <pages> 384-400, </pages> <year> 1984. </year> <month> 46 </month>
Reference: [98] <author> L. Welch and E. Berlekamp. </author> <title> Error correction of algebraic block codes. </title> <type> US Patent Number 4,633,470 (filed: </type> <year> 1986). </year>
Reference: [99] <author> M. Yannakakis. </author> <title> On the approximation of maximum satisfiability. </title> <journal> Journal of Al gorithms, </journal> <volume> 17(3) </volume> <pages> 475-502, </pages> <month> November </month> <year> 1994. </year>
Reference: [100] <author> D. Zuckerman. </author> <title> On unapproximable versions of NP-complete problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 25(6) </volume> <pages> 1293-1304, </pages> <month> December </month> <year> 1996 </year>
Reference-contexts: The discovery of Theorem 2 inspired the search for other connections between probabilistic proof checking and non-approximability (Bellare [18], Bellare and Rogaway [22], Feige and Lovsz [45], and Zuckerman <ref> [100] </ref>). Another such connection is reported by Arora, Motwani, Safra, Sudan and Szegedy [5], which shows the connection between PCP's and the hardness of approximating MAX 3SAT. The following theorem summarizes this result; for a proof see Section 3.
References-found: 100

