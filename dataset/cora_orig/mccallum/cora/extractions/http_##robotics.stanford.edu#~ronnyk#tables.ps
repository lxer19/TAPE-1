URL: http://robotics.stanford.edu/~ronnyk/tables.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: http://www.cs.stanford.edu
Email: ronnyk@CS.Stanford.EDU  
Title: The Power of Decision Tables  
Author: Ron Kohavi 
Web: http://robotics.stanford.edu/~ronnyk  
Address: Stanford, CA. 94305  
Affiliation: Computer Science Department Stanford University  
Date: 1995  
Note: Appears in the European Conference on Machine Learning (ECML),  
Abstract: We evaluate the power of decision tables as a hypothesis space for supervised learning algorithms. Decision tables are one of the simplest hypothesis spaces possible, and usually they are easy to understand. Experimental results show that on artificial and real-world domains containing only discrete features, IDTM, an algorithm inducing decision tables, can sometimes outperform state-of-the-art algorithms such as C4.5. Surprisingly, performance is quite good on some datasets with continuous features, indicating that many datasets used in machine learning either do not require these features, or that these features have few values. We also describe an incremental method for performing cross-validation that is applicable to incremental learning algorithms including IDTM. Using incremental cross-validation, it is possible to cross-validate a given dataset and IDTM in time that is linear in the number of instances, the number of features, and the number of label values. The time for incremental cross-validation is independent of the number of folds chosen, hence leave-one-out cross-validation and ten-fold cross-validation take the same time. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. & Bankert, R. L. </author> <year> (1994), </year> <title> A comparative evaluation of sequential feature selection algorithms, </title> <booktitle> in "Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics", </booktitle> <pages> pp. 1-7. </pages>
Reference-contexts: The experiments were done on all the datasets at the UC Irvine repository <ref> (Murphy & Aha 1994) </ref> and StatLog repository (Taylor, Michie & Spiegalhalter 1994) that contain only discrete features.
Reference: <author> Almuallim, H. & Dietterich, T. G. </author> <year> (1991), </year> <title> Learning with many irrelevant features, </title> <booktitle> in "Ninth National Conference on Artificial Intelligence", </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 547-552. </pages>
Reference: <author> Almuallim, H. & Dietterich, T. G. </author> <year> (1992), </year> <title> On learning more concepts, </title> <booktitle> in "Proceedings of the Ninth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 11-19. </pages>
Reference: <author> Boyce, D., Farhi, A. & Weischedel, R. </author> <year> (1974), </year> <title> Optimal Subset Selection, </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Performance is approximately the same for the rest. The 94:6% 0:7% accuracy of IDTM on the DNA dataset, containing 180 binary features, 2,000 training instances, and 1,186, test instances, is higher than many other state-of-the-art induction algorithms reported for this dataset in (Taylor et al. 1994). For example, CART <ref> (Breiman et al. 1984) </ref> achieves 91:5% accuracy, Backprop (Rumelhart, Hinton & Williams 1986) achieves 91:2% accuracy, CN2 (Clark & Niblett 1989) achieves 90:5% accuracy, and k-nearest neighbor achieves 84.5% accuracy. Table 2 shows some results for datasets containing continuous features.
Reference: <author> Caruana, R. & Freitag, D. </author> <year> (1994), </year> <title> Greedy attribute selection, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1989), </year> <title> "The CN2 induction algorithm", </title> <booktitle> Machine Learning 3(4), </booktitle> <pages> 261-283. </pages>
Reference-contexts: For example, CART (Breiman et al. 1984) achieves 91:5% accuracy, Backprop (Rumelhart, Hinton & Williams 1986) achieves 91:2% accuracy, CN2 <ref> (Clark & Niblett 1989) </ref> achieves 90:5% accuracy, and k-nearest neighbor achieves 84.5% accuracy. Table 2 shows some results for datasets containing continuous features. In these domains, we expected IDTM to fail miserably, given that the chances of matching continuous features in the table are slim without preprocessing the data.
Reference: <author> Cormen, T. H., Leiserson, C. E. & Rivest, R. L. </author> <year> (1990), </year> <title> Introduction to algorithms, </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: We now describe the data structures that allow fast incremental operations on DTMs. The underlying data structure that we use is a universal hash table <ref> (Cormen, Leiserson & Rivest 1990) </ref>.
Reference: <author> Devijver, P. A. & Kittler, J. </author> <year> (1982), </year> <title> Pattern Recognition: A Statistical Approach, </title> <booktitle> Prentice-Hall International. </booktitle>
Reference-contexts: However, while nearest-neighbor algorithms use the nearest neighbor to classify instances, a DTM classifier defaults to the majority whenever the distance is greater than zero. Feature subset selection has been long studied in the statistics community (Miller 1990, Boyce, Farhi & Weischedel 1974), in the pattern recognition community <ref> (Devijver & Kittler 1982) </ref>, and lately in the machine learning community (John et al. 1994, Moore & Lee 1994, Caruana & Freitag 1994, Kohavi & Frasca 1994, Langley & Sage 1994, Aha & Bankert 1994).
Reference: <author> Efron, B. </author> <year> (1983), </year> <title> "Estimating the error rate of a prediction rule: improvement on cross-validation", </title> <journal> Journal of the American Statistical Association 78(382), </journal> <pages> 316-330. </pages>
Reference-contexts: Leave-one-out is almost unbiased <ref> (Efron 1983) </ref> and was commonly considered the preferred method for cross-validation.
Reference: <author> Garey, M. R. </author> <year> (1972), </year> <title> "Optimal binary identification procedures", </title> <journal> Siam Journal on Applied Mathematics 23, </journal> <pages> 173-186. </pages>
Reference-contexts: In the early seventies, these procedures were improved using dynamic programming techniques <ref> (Garey 1972, Schumacher & Sevcik 1976) </ref>. Hyafil & Rivest (1976) showed that building an optimal decision tree from instances (or from a table) is NP-complete. Hartmann, Varshney, Mehrotra & Gerberich (1982) show how to convert a decision table into a decision tree using mutual information.
Reference: <author> Ginsberg, M. L. </author> <year> (1993), </year> <title> Essential of Artificial Intelligence, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: state space search with probabilistic esti mates; further details can be found in Kohavi (1994c). 3.1 Searching the Space of Feature Subsets In order to search the space of feature subsets effectively, we transform the problem into a state space search and use best-first search to heuristically search the space <ref> (Ginsberg 1993, Nilsson 1980) </ref>. The states in the space are feature subsets; operators can add or delete a feature; the initial node can be either the set of all features or the empty set; and the evaluation function is cross-validation (described below).
Reference: <author> Hartmann, C. R. P., Varshney, P. K., Mehrotra, K. G. & Gerberich, C. L. </author> <year> (1982), </year> <title> "Application of information theory to the construction of efficient decision trees", </title> <journal> IEEE Transactions on information theory IT-28(4), </journal> <pages> 565-577. </pages>
Reference: <author> Holte, R. C. </author> <year> (1993), </year> <title> "Very simple classification rules perform well on most commonly used datasets", </title> <booktitle> Machine Learning 11, </booktitle> <pages> 63-90. </pages>
Reference: <author> Hyafil, L. & Rivest, R. L. </author> <year> (1976), </year> <title> "Constructing optimal binary decision trees is NP-complete", </title> <journal> Information Processing Letters 5(1), </journal> <pages> 15-17. </pages>
Reference: <author> John, G. H. </author> <year> (1994), </year> <title> Cross-validated C4.5: Using error estimation for automatic parameter selection, </title> <type> Technical Report STAN-CS-TN-94-12, </type> <institution> Computer Science Department, Stanford University. </institution>
Reference-contexts: Because the hypothesis space is limited in its expressive power, the optimal features may not include all relevant features. To search for the optimal features, the wrapper model <ref> (John, Kohavi & Pfleger 1994) </ref> is used. In the wrapper model, the induction algorithm is used as a black box, and a search through the space of feature subsets is made by a "wrapper" algorithm.
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages> <note> Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/ml94.ps. </note>
Reference-contexts: Because the hypothesis space is limited in its expressive power, the optimal features may not include all relevant features. To search for the optimal features, the wrapper model <ref> (John, Kohavi & Pfleger 1994) </ref> is used. In the wrapper model, the induction algorithm is used as a black box, and a search through the space of feature subsets is made by a "wrapper" algorithm.
Reference: <author> Kohavi, R. </author> <year> (1994a), </year> <title> Bottom-up induction of oblivious, read-once decision graphs, </title> <note> in "Proceedings of the European Conference on Machine Learning". Available by anonymous ftp from starry.Stanford.EDU:pub/ronnyk/euroML94.ps. </note>
Reference-contexts: Decision tables have a bias similar to that of oblivious read-once decision graphs (OODGs) <ref> (Kohavi 1994a, Kohavi 1994b) </ref>: all of the features chosen for the schema are tested during classification.
Reference: <author> Kohavi, R. </author> <year> (1994b), </year> <title> Bottom-up induction of oblivious, read-once decision graphs : strengths and limitations, </title> <booktitle> in "Twelfth National Conference on Artificial Intelligence", </booktitle> <pages> pp. 613-618. </pages> <note> Available by anonymous ftp from Starry.Stanford.EDU:pub/ronnyk/aaai94.ps. </note>
Reference: <author> Kohavi, R. </author> <year> (1994c), </year> <title> Feature subset selection as search with probabilistic estimates, </title> <booktitle> in "AAAI Fall Symposium on Relevance", </booktitle> <pages> pp. 122-126. </pages> <note> Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/aaaiSymposium94.ps. </note>
Reference: <author> Kohavi, R. & Frasca, B. </author> <year> (1994), </year> <title> Useful feature subsets and rough set reducts, </title> <booktitle> in "Third International Workshop on Rough Sets and Soft Computing", </booktitle> <pages> pp. 310-317. </pages> <note> Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/rough.ps. </note>
Reference-contexts: Because the hypothesis space is limited in its expressive power, the optimal features may not include all relevant features. To search for the optimal features, the wrapper model <ref> (John, Kohavi & Pfleger 1994) </ref> is used. In the wrapper model, the induction algorithm is used as a black box, and a search through the space of feature subsets is made by a "wrapper" algorithm. <p> While finding reducts is appealing in theory, they are not necessarily optimal subsets for a given induction algorithm. Much of the work presented here stemmed from a paper claiming that reducts are not necessarily useful <ref> (Kohavi & Frasca 1994) </ref>. Almuallim & Dietterich (1991) described the FOCUS algorithm which is equivalent to finding the DTM with the smallest number of features in the schema and with no conflicting instance projections.
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994), </year> <title> Induction of selective bayesian classifiers, </title> <booktitle> in "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Seattle, WA, </address> <pages> pp. 399-406. </pages>
Reference: <author> Maron, O. & Moore, A. W. </author> <year> (1994), </year> <title> Hoeffding races: Accelerating model selection search for classification and function approximation, </title> <booktitle> in "Advances in Neural Information Processing Systems", </booktitle> <volume> Vol. 6, </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Miller, A. J. </author> <year> (1990), </year> <title> Subset Selection in Regression, </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: However, while nearest-neighbor algorithms use the nearest neighbor to classify instances, a DTM classifier defaults to the majority whenever the distance is greater than zero. Feature subset selection has been long studied in the statistics community <ref> (Miller 1990, Boyce, Farhi & Weischedel 1974) </ref>, in the pattern recognition community (Devijver & Kittler 1982), and lately in the machine learning community (John et al. 1994, Moore & Lee 1994, Caruana & Freitag 1994, Kohavi & Frasca 1994, Langley & Sage 1994, Aha & Bankert 1994).
Reference: <author> Modrzejewski, M. </author> <year> (1993), </year> <title> Feature selection using rough sets theory, </title> <editor> in P. B. Brazdil, ed., </editor> <booktitle> "Proceedings of the European Conference on Machine Learning", </booktitle> <pages> pp. 213-226. </pages>
Reference: <author> Moore, A. W. & Lee, M. S. </author> <year> (1994), </year> <title> Efficient algorithms for minimizing cross validation error, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moore, A. W., Hill, D. J. & Johnson, M. P. </author> <year> (1992), </year> <title> An empirical investigation of brute force to choose features, smoothers and function approximators, </title> <booktitle> in "Computational Learning Theory and Natural Learning Systems Conference". </booktitle>
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <title> UCI repository of machine learning databases, For information contact ml-repository@ics.uci.edu. </title>
Reference-contexts: The experiments were done on all the datasets at the UC Irvine repository <ref> (Murphy & Aha 1994) </ref> and StatLog repository (Taylor, Michie & Spiegalhalter 1994) that contain only discrete features.
Reference: <author> Nilsson, N. J. </author> <year> (1980), </year> <booktitle> Principles of Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pawlak, Z. </author> <year> (1987), </year> <title> "Decision tables | a rough sets approach", </title> <journal> Bull. of EATCS 33, </journal> <pages> 85-96. </pages>
Reference-contexts: All these approaches, however, dealt with conversions that are information preserving, i.e., all entries in the table are correctly classified and the structures are not used for making predictions. The rough sets community has been using the hypothesis space of decision tables for a few years <ref> (Pawlak 1987, Pawlak 1991, Slowinski 1992) </ref>. Researchers in the field of rough sets suggest using the degrees-of-dependency of a feature on the label (called fl) to determine which features should be included in a decision table (Ziarko 1991, Modrzejewski 1993).
Reference: <author> Pawlak, Z. </author> <year> (1991), </year> <title> Rough Sets, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Pawlak, Z., Wong, S. & Ziarko, W. </author> <year> (1988), </year> <title> "Rough sets: Probabilistic versus deterministic approach", </title> <journal> Internation Journal of Man Machine Studies 29, </journal> <pages> 81-95. </pages>
Reference-contexts: Researchers in the field of rough sets suggest using the degrees-of-dependency of a feature on the label (called fl) to determine which features should be included in a decision table (Ziarko 1991, Modrzejewski 1993). Another suggestion was to use normalized entropy <ref> (Pawlak, Wong & Ziarko 1988) </ref>, which is similar to the information gain measure of ID3. These approaches ignore the utility of the specific features to the specific induction algorithm and to the hypothesis space used.
Reference: <author> Quinlan, J. R. </author> <year> (1986), </year> <title> "Induction of decision trees", </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages>
Reference-contexts: Hyafil & Rivest (1976) showed that building an optimal decision tree from instances (or from a table) is NP-complete. Hartmann, Varshney, Mehrotra & Gerberich (1982) show how to convert a decision table into a decision tree using mutual information. The algorithm is very similar to ID3 <ref> (Quinlan 1986) </ref>. All these approaches, however, dealt with conversions that are information preserving, i.e., all entries in the table are correctly classified and the structures are not used for making predictions.
Reference: <editor> Reprinted in Shavlik and Dietterich (eds.) </editor> <booktitle> Readings in Machine Learning. </booktitle>
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California. </address>
Reference-contexts: Our initial experiments were therefore restricted to datasets containing only discrete features. To determine how weak the performance of IDTM is on datasets with continuous features, we also report on such experiments. Surprisingly, performance is not significantly worse than that of C4.5 <ref> (Quinlan 1993) </ref> in some cases. On those that performance is not significantly worse than C4.5, the algorithm ignores the continuous features or uses those features that have few values. The paper is organized as follows. Section 2 formally defines DTMs and the problem of finding an optimal feature subset.
Reference: <author> Reinwald, L. T. & Soland, R. M. </author> <year> (1966), </year> <title> "Conversion of limited-entry decision tables to optimal computer programs i: Minimum average processing time", </title> <journal> Journal of the ACM 13(3), </journal> <pages> 339-358. </pages>
Reference-contexts: to be performed, decision tables are becoming popular in computer programming and system design as devices for organizing logic. |Reinwald & Soland (1966) In the early sixties, algorithms were created to convert decision tables into optimal computer programs (decision trees) under different measure of optimality using branch and bound procedures <ref> (Reinwald & Soland 1966, Reinwald & Soland 1967) </ref>. In the early seventies, these procedures were improved using dynamic programming techniques (Garey 1972, Schumacher & Sevcik 1976). Hyafil & Rivest (1976) showed that building an optimal decision tree from instances (or from a table) is NP-complete.
Reference: <author> Reinwald, L. T. & Soland, R. M. </author> <year> (1967), </year> <title> "Conversion of limited-entry decision tables to optimal computer programs ii: Minimum storage requirement", </title> <journal> Journal of the ACM 14(4), </journal> <pages> 742-755. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E. & Williams, R. J. </author> <year> (1986), </year> <title> Learning Internal Representations by Error Propagation, </title> <publisher> MIT Press, </publisher> <address> chapter 8. </address>
Reference-contexts: The 94:6% 0:7% accuracy of IDTM on the DNA dataset, containing 180 binary features, 2,000 training instances, and 1,186, test instances, is higher than many other state-of-the-art induction algorithms reported for this dataset in (Taylor et al. 1994). For example, CART (Breiman et al. 1984) achieves 91:5% accuracy, Backprop <ref> (Rumelhart, Hinton & Williams 1986) </ref> achieves 91:2% accuracy, CN2 (Clark & Niblett 1989) achieves 90:5% accuracy, and k-nearest neighbor achieves 84.5% accuracy. Table 2 shows some results for datasets containing continuous features.
Reference: <author> Schaffer, C. </author> <year> (1994), </year> <title> A conservation law for generalization performance, </title> <booktitle> in "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kauf-mann, </publisher> <pages> pp. 259-265. </pages>
Reference-contexts: The IDTM algorithm described here performs better than Holte's algorithm and sometimes outperforms C4.5. Generalization without a bias is impossible <ref> (Schaffer 1994, Wolpert 1994) </ref>. IDTM is biased to select a feature subset maximizing cross-validation accuracy estimates. When the estimates are good, IDTM should choose a feature subset that leads to high prediction accuracy. Our empirical evidence indicates that the estimates are usually good, but have high variability.
Reference: <author> Schumacher, H. & Sevcik, K. C. </author> <year> (1976), </year> <title> "The synthetic approach to decision table conversion", </title> <journal> Communications of the ACM 19(6), </journal> <pages> 343-351. </pages>
Reference: <author> Shao, J. </author> <year> (1993), </year> <title> "Linear model seletion via cross-validation", </title> <journal> Journal of the American Statistical Association 88(422), </journal> <pages> 486-494. </pages>
Reference: <author> Slowinski, R. </author> <year> (1992), </year> <title> Intelligent decision support : handbook of applications and advances of the rough sets theory, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Stone, M. </author> <year> (1974), </year> <title> "Cross-validatory choice and assessment of statistical predictions", </title> <journal> Journal of the Royal Statistical Society B 36, </journal> <pages> 111-147. </pages>
Reference: <author> Taylor, C., Michie, D. & Spiegalhalter, D. </author> <year> (1994), </year> <title> Machine Learning, Neural and Statistical Classification, </title> <publisher> Paramount Publishing International. </publisher>
Reference-contexts: The experiments were done on all the datasets at the UC Irvine repository (Murphy & Aha 1994) and StatLog repository <ref> (Taylor, Michie & Spiegalhalter 1994) </ref> that contain only discrete features. <p> Performance is approximately the same for the rest. The 94:6% 0:7% accuracy of IDTM on the DNA dataset, containing 180 binary features, 2,000 training instances, and 1,186, test instances, is higher than many other state-of-the-art induction algorithms reported for this dataset in <ref> (Taylor et al. 1994) </ref>. For example, CART (Breiman et al. 1984) achieves 91:5% accuracy, Backprop (Rumelhart, Hinton & Williams 1986) achieves 91:2% accuracy, CN2 (Clark & Niblett 1989) achieves 90:5% accuracy, and k-nearest neighbor achieves 84.5% accuracy. Table 2 shows some results for datasets containing continuous features.
Reference: <author> Thrun etal. </author> <year> (1991), </year> <title> The monk's problems: A performance comparison of different learning algorithms, </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Similar observations were made by Weiss (1991). While the differences are usually small, especially for feature subset selection where only the relative ranking of different subsets matters, there is one extreme case that deserves special mention: the Monk1 dataset. Example 3 Leave-one-out on Monk1. The Monk1 problem <ref> (Thrun etal. 1991) </ref> has a standard training and test set. There are no duplicate instances, nor is there noise in the training set.
Reference: <author> Utgoff, P. E. </author> <year> (1994), </year> <title> An improved algorithm for incremental induction of decision trees, </title> <booktitle> in "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 318-325. </pages>
Reference: <author> Weiss, S. M. </author> <year> (1991), </year> <title> "Small sample error rate estimation for k-nearest neighbor classifiers", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 13(3), </journal> <pages> 285-289. </pages>
Reference: <author> Weiss, S. M. & Kulikowski, C. A. </author> <year> (1991), </year> <title> Computer Systems that Learn, </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Wolpert, D. H. </author> <year> (1994), </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework, </title> <type> Technical report, </type> <institution> The Santa Fe Institute, </institution> <address> Santa Fe, NM. </address>
Reference-contexts: The IDTM algorithm described here performs better than Holte's algorithm and sometimes outperforms C4.5. Generalization without a bias is impossible <ref> (Schaffer 1994, Wolpert 1994) </ref>. IDTM is biased to select a feature subset maximizing cross-validation accuracy estimates. When the estimates are good, IDTM should choose a feature subset that leads to high prediction accuracy. Our empirical evidence indicates that the estimates are usually good, but have high variability.
Reference: <author> Zhang, P. </author> <year> (1992), </year> <title> "On the distributional properties of model selection criteria", </title> <journal> Journal of the American Statistical Association 87(419), </journal> <pages> 732-737. </pages>
Reference: <author> Ziarko, W. </author> <year> (1991), </year> <title> The discovery, analysis, and representation of data dependencies in databases, </title> <editor> in G. Piatetsky-Shapiro & W. Frawley, eds, </editor> <title> "Knowledge Discovery in Databases", </title> <publisher> MIT Press. </publisher>
Reference-contexts: Researchers in the field of rough sets suggest using the degrees-of-dependency of a feature on the label (called fl) to determine which features should be included in a decision table <ref> (Ziarko 1991, Modrzejewski 1993) </ref>. Another suggestion was to use normalized entropy (Pawlak, Wong & Ziarko 1988), which is similar to the information gain measure of ID3. These approaches ignore the utility of the specific features to the specific induction algorithm and to the hypothesis space used.
References-found: 51

