URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/ggordon/www/nips95.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/ggordon/www/index.html
Root-URL: 
Email: ggordon@cs.cmu.edu  
Title: Stable Fitted Reinforcement Learning  
Author: Geoffrey J. Gordon 
Address: Pittsburgh PA 15213  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract: We describe the reinforcement learning problem, motivate algorithms which seek an approximation to the Q function, and present new convergence results for two such algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> L. Baird. </author> <title> Residual algorithms: Reinforcement learning with function approximation. </title> <booktitle> In Machine Learning (proceedings of the twelfth international conference), </booktitle> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: If we can discover Q, we have solved the problem: at each step, we may simply choose a t to minimize Q (x t ; a t ). For more information about MDPs, see <ref> (Watkins, 1989, Bertsekas and Tsitsiklis, 1989) </ref>. We may distinguish two classes of problems, online and o*ine. In the o*ine problem, we have a full model of the MDP: given a state and an action, we can describe the distributions of the cost and the next state.
Reference: <author> J. A. Boyan and A. W. Moore. </author> <title> Generalization in reinforcement learning: safely approximating the value function. </title> <editor> In G. Tesauro and D. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference: <author> S. J. Bradtke. </author> <title> Reinforcement learning applied to linear quadratic regulation. </title> <editor> In S. </editor> <publisher> J. </publisher>
Reference: <editor> Hanson, J. D. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference: <author> P. Dayan. </author> <title> The convergence of TD() for general lambda. </title> <booktitle> Machine Learning, </booktitle> <address> 8(3-4):341-362, </address> <year> 1992. </year>
Reference: <author> G. J. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <booktitle> In Machine Learning (proceedings of the twelfth international conference), </booktitle> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> G. J. Gordon. </author> <title> Online fitted reinforcement learning. </title> <editor> In J. A. Boyan, A. W. Moore, and R. S. Sutton, editors, </editor> <booktitle> Proceedings of the Workshop on Value Function Approximation, </booktitle> <year> 1995. </year> <note> Proceedings are available as tech report CMU-CS-95-206. </note>
Reference: <author> T. Jaakkola, M. I. Jordan, and S. P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1185-1201, </pages> <year> 1994. </year>
Reference-contexts: We will be concerned here only with direct algorithms. Watkins' (1989) Q-learning algorithm can find the Q function for small MDPs, either online or o*ine. Convergence with probability 1 in the online case was proven in <ref> (Jaakkola et al., 1994, Tsitsiklis, 1994) </ref>. For large MDPs, exact Q-learning is too expensive: representing the Q function requires too much space. To overcome this difficulty, we may look for an inexpensive approximation to the Q function. <p> Now if we let the learning rates satisfy P P i (ff (i) ) 2 &lt; 1, convergence w.p.1 to q fl is guaranteed by a theorem of <ref> (Jaakkola et al., 1994) </ref>. (See also the theorem in (Tsitsiklis, 1994).) More generally, if M A is linear and can represent q fl c for some vector c, we can bound the error between q fl and the fixed point of the expected sarsa update on iteration i: if we choose <p> A minor modification of the theorem of <ref> (Jaakkola et al., 1994) </ref> shows that the distance from q (i) to the region fi fi 1 converges w.p.1 to zero. <p> Finally, if we follow a fixed exploration policy on every trajectory, the matrix D (i) will be the same for every i; in this case, because of the contraction property proved in the previous section, convergence w.p.1 for appropriate learning rates is guaranteed again by the theorem of <ref> (Jaakkola et al., 1994) </ref>. 4 NONDISCOUNTED PROBLEMS When M is not discounted, the Q-learning backup operator T M is no longer a max norm contraction.
Reference: <author> S. P. Singh, T. Jaakkola, and M. I. Jordan. </author> <title> Reinforcement learning with soft state aggregation. </title> <editor> In G. Tesauro and D. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: After submitting this paper, we were advised of the paper <ref> (Singh et al., 1995) </ref>, which contains a different algorithm for solving online MDPs. In addition, our newer paper (Gordon, 1995b) proves results for a larger class of approximators. There are several algorithms which can handle restricted versions of the online problem.
Reference: <author> S. P. Singh and R. S. Sutton. </author> <title> Reinforcement learning with replacing eligibility traces. </title> <booktitle> Machine Learning, </booktitle> <year> 1996. </year>
Reference-contexts: On the other hand, as we shall see in the next section, the weighted algorithm can be applied to online problems. 3 ONLINE DISCOUNTED PROBLEMS Consider the following algorithm, which is a natural generalization of TD (0) (Sut-ton, 1988) to Markov decision problems. (This algorithm has been called "sarsa" <ref> (Singh and Sutton, 1996) </ref>.) Start with some initial Q function q (0) . Repeat the following steps for i from 0 onwards. Let (i) be a policy chosen according to some predetermined tradeoff between exploration and exploitation for the Q function q (i) .
Reference: <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: There are several algorithms which can handle restricted versions of the online problem. In the case of a Markov chain (an MDP where only one action is available at any time step), Sutton's TD () has been proven to converge for arbitrary linear approximators <ref> (Sutton, 1988, Dayan, 1992) </ref>. For decision processes with linear transition functions and quadratic cost functions (the so-called linear quadratic regulation problem), the algorithm of (Bradtke, 1993) is guaranteed to converge.
Reference: <author> G. Tesauro. Neurogammon: </author> <title> a neural network backgammon program. </title> <booktitle> In IJCNN Proceedings III, </booktitle> <pages> pages 33-39, </pages> <year> 1990. </year>
Reference-contexts: For decision processes with linear transition functions and quadratic cost functions (the so-called linear quadratic regulation problem), the algorithm of (Bradtke, 1993) is guaranteed to converge. In practice, researchers have had mixed success with approximate reinforcement learning <ref> (Tesauro, 1990, Boyan and Moore, 1995, Singh and Sutton, 1996) </ref>. The remainder of the paper is divided into four sections. In section 2, we summarize convergence results for o*ine Q-learning, and prove some contraction properties which will be useful later.
Reference: <author> J. N. Tsitsiklis and B. Van Roy. </author> <title> Feature-based methods for large-scale dynamic programming. </title> <type> Technical Report P-2277, </type> <institution> Laboratory for Information and Decision Systems, </institution> <year> 1994. </year>
Reference-contexts: We will be concerned here only with direct algorithms. Watkins' (1989) Q-learning algorithm can find the Q function for small MDPs, either online or o*ine. Convergence with probability 1 in the online case was proven in <ref> (Jaakkola et al., 1994, Tsitsiklis, 1994) </ref>. For large MDPs, exact Q-learning is too expensive: representing the Q function requires too much space. To overcome this difficulty, we may look for an inexpensive approximation to the Q function. <p> As Baird (1995) points out, we cannot even rely on gradient descent for large, stochastic problems, since we must observe two independent transitions from a given state before we can compute an unbiased estimate of the gradient. One of the algorithms in <ref> (Tsitsiklis and Van Roy, 1994) </ref>, which uses state aggregation to approximate the Q function, can be modified to apply to online problems; the resulting algorithm, unlike Q-learning, must make repeated small updates to its control policy, interleaved with comparatively lengthy periods of evaluation of the changes. <p> Now if we let the learning rates satisfy P P i (ff (i) ) 2 &lt; 1, convergence w.p.1 to q fl is guaranteed by a theorem of (Jaakkola et al., 1994). (See also the theorem in <ref> (Tsitsiklis, 1994) </ref>.) More generally, if M A is linear and can represent q fl c for some vector c, we can bound the error between q fl and the fixed point of the expected sarsa update on iteration i: if we choose an ff and a fl 0 &lt; 1 as
Reference: <author> J. N. Tsitsiklis. </author> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3) </volume> <pages> 185-202, </pages> <year> 1994. </year>
Reference-contexts: We will be concerned here only with direct algorithms. Watkins' (1989) Q-learning algorithm can find the Q function for small MDPs, either online or o*ine. Convergence with probability 1 in the online case was proven in <ref> (Jaakkola et al., 1994, Tsitsiklis, 1994) </ref>. For large MDPs, exact Q-learning is too expensive: representing the Q function requires too much space. To overcome this difficulty, we may look for an inexpensive approximation to the Q function. <p> As Baird (1995) points out, we cannot even rely on gradient descent for large, stochastic problems, since we must observe two independent transitions from a given state before we can compute an unbiased estimate of the gradient. One of the algorithms in <ref> (Tsitsiklis and Van Roy, 1994) </ref>, which uses state aggregation to approximate the Q function, can be modified to apply to online problems; the resulting algorithm, unlike Q-learning, must make repeated small updates to its control policy, interleaved with comparatively lengthy periods of evaluation of the changes. <p> Now if we let the learning rates satisfy P P i (ff (i) ) 2 &lt; 1, convergence w.p.1 to q fl is guaranteed by a theorem of (Jaakkola et al., 1994). (See also the theorem in <ref> (Tsitsiklis, 1994) </ref>.) More generally, if M A is linear and can represent q fl c for some vector c, we can bound the error between q fl and the fixed point of the expected sarsa update on iteration i: if we choose an ff and a fl 0 &lt; 1 as
Reference: <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: If we can discover Q, we have solved the problem: at each step, we may simply choose a t to minimize Q (x t ; a t ). For more information about MDPs, see <ref> (Watkins, 1989, Bertsekas and Tsitsiklis, 1989) </ref>. We may distinguish two classes of problems, online and o*ine. In the o*ine problem, we have a full model of the MDP: given a state and an action, we can describe the distributions of the cost and the next state.
References-found: 16

