URL: http://www.eecs.umich.edu/techreports/cse/1995/CSE-TR-250-95.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse95.html
Root-URL: http://www.eecs.umich.edu
Email: -pmchen,caycock,weeteck,gurur,sraja-@eecs.umich.edu  
Title: Rio: Storing Files Reliably in Memory 1 Rio: Storing Files Reliably in Memory  
Author: Peter M. Chen, Christopher M. Aycock, Wee Teck Ng, Gurushankar Rajamani, Rajagopalan Sivaramakrishnan 
Address: Michigan  
Affiliation: Computer Science and Engineering Division Department of Electrical Engineering and Computer Science University of  
Abstract: Memory is currently a second-class citizen of the storage hierarchy because of its vulnerability to power failures and software crashes. Designers have traditionally sacrificed either reliability or performance when using memory as a cache for disks; our goal is to do away with this tradeoff by making memory as reliable as disks. The Rio (RAM I/O) project at Michigan is modifying the Digital Unix (formerly OSF/1) kernel to protect the file cache from operating system crashes. If successful, making memory as reliable as disks will 1) improve file cache performance to that of a pure write-back scheme by eliminating all reliability-caused writes to disk; 2) improve reliability to that of a write-through scheme by making memory a reliable place to store files long term; and 3) simplify applications such as file systems and databases by eliminating write-back daemons and complex commit and checkpointing protocols. 
Abstract-found: 1
Intro-found: 1
Reference: [Abbott94] <author> M. Abbott, D. Har, L. Herger, M. Kauffmann, K. Mak, J. Murdock, C. Schulz, T. B. Smith, B. Tremaine, D. Yeh, and L. Wong. </author> <title> Durable Memory RS/6000 System Design. </title> <booktitle> In Proceedings of the 1994 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 414 423, </pages> <year> 1994. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures <ref> [Abbott94] </ref>. Finally, several papers have examined the performance advantages and management of reliable memory [Copeland89, Baker92a, Biswas93, Akyurek95]. 7 Conclusions Memory is currently a second-class citizen of the storage hierarchy because of its unreliability against software crashes and power loss.
Reference: [Akyurek95] <author> Sedat Akyurek and Kenneth Salem. </author> <title> Management of partially safe buffers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(3):394407, </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 7 Conclusions Memory is currently a second-class citizen of the storage hierarchy because of its unreliability against software crashes and power loss.
Reference: [Baker91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198212, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Compromise by writing data to disk after a fixed delay, typically 30 seconds [Ousterhout85]. This is commonly done on systems that can tolerate the loss of recently-written data, such as many Unix systems. Unfortunately, 1/3 to 2/3 of newly written data lives longer than 30 seconds <ref> [Baker91, Hart-man93] </ref>, so a large fraction of writes must eventually be written through to disk. A longer delay can decrease disk traffic due to writes, but only at the cost of lower reliability.
Reference: [Baker92a] <author> Mary Baker, Satoshi Asami, Etienne Deprit, John Ousterhout, and Margo Seltzer. </author> <title> NonVolatile Memory for Fast Reliable File Systems. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 1022, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Consequently, it is easy for a random software error, such as following a bad pointer, to corrupt the contents of memory <ref> [Baker92a] </ref>. Since any operating system module can access memory, ensuring that no memory corruption can occur is a daunting task. On the other hand, most users assume the contents of disk are intact after a system crash. <p> The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 7 Conclusions Memory is currently a second-class citizen of the storage hierarchy because of its unreliability against software crashes and power loss.
Reference: [Baker92b] <author> Mary Baker and Mark Sullivan. </author> <title> The Recovery Box: Using Fast Recovery to Provide High Availability in the UNIX Environment. </title> <booktitle> In Proceedings USENIX Summer Conference, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: We believe memory can be protected from software errors in much the same way by strictly controlling the way memory can be written <ref> [Baker92b] </ref>. To accomplish this protection, we propose adding a memory device driver to check for errors and prevent software errors from corrupting memory. <p> Several projects attempt to protect certain information from failures. The Harp file system protects a log of recent modifications by replicating it in volatile, battery-backed memory across several server nodes [Liskov91]. The recovery box keeps special system state in a region of memory accessed only through a rigid interface <ref> [Baker92b] </ref>. No attempt is made to prevent other functions from accidentally modifying the recovery box, although the system detects corruption by maintaining checksums. [Horn91] describes an 5. After the initial bootstrap, the stack pointer is a virtual address.
Reference: [Biswas93] <author> Prabuddha Biswas, K. K. Ramakrishnan, Don Towsley, and C. M. Krishna. </author> <title> Performance Rio: Storing Files Reliably in Memory 8 Analysis of Distributed File Systems with Non-Volatile Caches. </title> <booktitle> In Proceedings of the 1993 International Symposium on High Performance Distributed Computing (HPDC-2), </booktitle> <pages> pages 252262, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 7 Conclusions Memory is currently a second-class citizen of the storage hierarchy because of its unreliability against software crashes and power loss.
Reference: [Chen94] <author> Peter M. Chen, Edward K. Lee, Garth A. Gibson, Randy H. Katz, and David A. Patterson. </author> <title> RAID: High-Performance, Reliable Secondary Storage. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(2):145188, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: In addition, both reads and writes are forced to go through the device driver. We would like to improve memorys reliability without losing the ability to access memory randomly. Using redundancy can be used to protect memory in much the same way as with redundant disk arrays <ref> [Patterson88, Chen94] </ref>. Instead of trapping illegal accesses, this method allows a file cache page to survive some amount of corruption. For example, the system could store two copies of each file cache page or use lower-overhead error-correction such as Hamming codes.
Reference: [Copeland89] <author> George Copeland, Tom Keller, Ravi Krishnamurthy, and Marc Smith. </author> <title> The Case for Safe RAM. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 327335, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: The unreliability of memory and the slow speed of disk accesses slows the commit speed of transactions and forces other optimizations such as group commit in applications such as transaction processing and distributed message logging <ref> [DeWitt84, Copeland89] </ref>. The assumed unreliability of memory also increases cost by a small amount. Since data must eventually be stored on disk, the file cache can only serve as a copy of disk data. <p> For example, the total amount of data that can be stored using a 1000 MB disk and a 100 MB file cache is 1000 MB, not 1100 MB. An attractive solution to all these problems is to enable memory to store files as reliably as disks store them <ref> [Copeland89] </ref>. This would allow the reliability of write-through with the performance of a pure write-back scheme, substantially simplify system design for a variety of applications, and moderately increase usable capacity by allowing the use of non-inclusive caching schemes. <p> In particular, avoiding custom hardware would enable us to modify the system more quickly and make our results more widely applicable. 3.1 Protecting the File Cache from Unauthorized Stores At first glance, the virtual memory protection of a system seems ideally suited to protect the file cache from unauthorized stores <ref> [Copeland89] </ref>. By keeping the write-permission bits in the page table entries turned off for the file cache pages, the system will cause most unauthorized stores to encounter a protection violation. <p> The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 7 Conclusions Memory is currently a second-class citizen of the storage hierarchy because of its unreliability against software crashes and power loss.
Reference: [DeWitt84] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation Techniques for Main Memory Database Systems. </title> <booktitle> In Proceedings of the 1984 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 18, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: The unreliability of memory and the slow speed of disk accesses slows the commit speed of transactions and forces other optimizations such as group commit in applications such as transaction processing and distributed message logging <ref> [DeWitt84, Copeland89] </ref>. The assumed unreliability of memory also increases cost by a small amount. Since data must eventually be stored on disk, the file cache can only serve as a copy of disk data.
Reference: [Eich87] <author> Margaret H. Eich. </author> <title> A classification and comparison of main memory database recover techniques. </title> <booktitle> In Proceedings of the IEEE International Conference on Data Engineering, </booktitle> <pages> pages 332339, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: In addition to hurting performance and reliability, the assumed unreliability of memory increases system complexity for applications such as main-memory databases, file systems, and transaction processing systems [Rahm92]. Much of the research in main-memory databases deals with checkpointing and recovering data in case the system crashes <ref> [GM92, Eich87] </ref>. File systems must obey complex ordering constraints when forcing metadata to disk to ensure file system consistency [Ganger94].
Reference: [Eustace95] <author> Alan Eustace and Amitabh Srivastava. </author> <title> ATOM: A Flexible Interface for Building High Performance Program Analysis Tools. </title> <booktitle> In Proceedings of the Winter 1995 USENIX Conference, </booktitle> <pages> pages 303314, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: This section describes the status of our implementation at the time of this writing. We use the ATOM code-modification system <ref> [Eustace95, Srivastava94] </ref> to check kernel stores to the UBC and buffer cache. For the UBC, VM protection is not needed because the UBC is stored in physical memory and cannot be accessed using virtual addresses. Thus the protection system for the UBC is nearly complete.
Reference: [Gait90] <author> Jason Gait. </author> <title> Phoenix: A Safe In-Memory File System. </title> <journal> Communications of the ACM, </journal> <volume> 33(1):8186, </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: Many have a memory file system that stores a complete, though temporary, file system in memory [McKu-sick90]. To our knowledge, however, the only file system that attempts to make its files reliable while in memory is Phoenix <ref> [Gait90] </ref>. Phoenix keeps two versions of an in-memory file system. One of these versions is kept write-protected; the other version is unprotected and evolves from the write-protected one via copy-on-write. At periodic checkpoints, the system write-protects the unprotected version and the deletes obsolete pages in the original version.
Reference: [Ganger94] <author> Gregory R. Ganger and Yale N. Patt. </author> <title> Metadata Update Performance in File Systems. </title> <booktitle> 1994 Operating Systems Design and Implementation (OSDI), </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Much of the research in main-memory databases deals with checkpointing and recovering data in case the system crashes [GM92, Eich87]. File systems must obey complex ordering constraints when forcing metadata to disk to ensure file system consistency <ref> [Ganger94] </ref>. The unreliability of memory and the slow speed of disk accesses slows the commit speed of transactions and forces other optimizations such as group commit in applications such as transaction processing and distributed message logging [DeWitt84, Copeland89]. The assumed unreliability of memory also increases cost by a small amount.
Reference: [GM92] <author> Hector Garcia-Molina and Kenneth Salem. </author> <title> Main Memory Database Systems: An Overview. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 4(6):509516, </volume> <month> December </month> <year> 1992. </year>
Reference-contexts: In addition to hurting performance and reliability, the assumed unreliability of memory increases system complexity for applications such as main-memory databases, file systems, and transaction processing systems [Rahm92]. Much of the research in main-memory databases deals with checkpointing and recovering data in case the system crashes <ref> [GM92, Eich87] </ref>. File systems must obey complex ordering constraints when forcing metadata to disk to ensure file system consistency [Ganger94].
Reference: [Gray90] <author> Jim Gray. </author> <title> A Census of Tandem System Availability between 1985 and 1990. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> 39(4), </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: Options for protecting memory against power outages include uninterruptible power supplies and switching to an inherently non-volatile memory technology such as Flash RAM [Wu94]. This paper focuses on protecting memory against software-induced corruption, which can account for 2/3 of all system outages <ref> [Gray90] </ref>. 2 Why Is Memory Less Reliable Than Disks? If someone surveyed system administrators and asked them if they would trust the contents of memory after a system crash, most of them would likely give a resounding no!.
Reference: [Hartman93] <author> John H. Hartman and John K. Ousterhout. </author> <title> Letter to the Editor. Operating Systems Review, </title> <address> 27(1):79, </address> <month> January </month> <year> 1993. </year>
Reference: [Horn91] <author> Chris Horn, Brian Coghlan, Neville Harris, and Jeremy Jones. </author> <title> Stable memoryanother look. </title> <editor> In A. Karshmer and J. Nehmer, editors, </editor> <booktitle> Operating Systems of the 90s and Beyond. International Workshop Proceedings, </booktitle> <pages> pages 171177, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The recovery box keeps special system state in a region of memory accessed only through a rigid interface [Baker92b]. No attempt is made to prevent other functions from accidentally modifying the recovery box, although the system detects corruption by maintaining checksums. <ref> [Horn91] </ref> describes an 5. After the initial bootstrap, the stack pointer is a virtual address. In Digital Unix, the stack pointer is modified only by small increments, and these increments can not change it to a physical address without first causing a memory exception.
Reference: [Howard88] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1):5181, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: Thus the protection system for the UBC is nearly complete. The current system counts the number of stores to the UBC rather than raising a protection violation; this will be changed when the memory device driver is running. Checking kernel stores increases the running time of the Andrew benchmark <ref> [Howard88] </ref> by 10%. We do not check the stack pointer in Digital Unix because the stack pointer cannot be a physical address. 5 Warm reboot is up and running. It successfully restores the UBC data present before the crash.
Reference: [Johnson82] <author> Mark Scott Johnson. </author> <title> Some Requirements for Architectural Support of Software Debugging. </title> <booktitle> In Proceedings of the 1982 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 140148, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations <ref> [Johnson82, Wahbe92] </ref>. Finally, object code modification has been suggested as a way to provide data breakpoints [Kessler90, Wahbe92] and fault isolation between software modules [Wahbe93].
Reference: [Kane92] <author> Gerry Kane and Joe Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: The memory device driver can check for corruption during this window verifying the data after the write is completed. Unfortunately, many systems allow certain kernel accesses to bypass the virtual memory protection mechanism and directly access physical memory <ref> [Kane92, Sites92] </ref>.
Reference: [Kessler90] <author> Peter B. Kessler. </author> <title> Fast breakpoints: </title> <booktitle> Design and implementation. In Proceedings of the 1990 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 7884, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations [Johnson82, Wahbe92]. Finally, object code modification has been suggested as a way to provide data breakpoints <ref> [Kessler90, Wahbe92] </ref> and fault isolation between software modules [Wahbe93].
Reference: [Lee93] <author> Inhwan Lee and Ravishankar K. Iyer. </author> <title> Faults, Symptoms, and Software Fault Tolerance in the Tandem GUARDIAN Operating System. </title> <booktitle> In International Symposium on Fault-Tolerant Computing (FTCS), pages 2029, </booktitle> <year> 1993. </year>
Reference-contexts: This intuition is backed by field studies of MVS and Guardian (Tandems operating system), which show that between 1/4 to 1/2 of all software-induced system crashes could corrupt memory <ref> [Sullivan91, Lee93] </ref>. It is not yet clear how often these crashes corrupt the file cache; we are conducting experiments to measure this.
Reference: [Leffler89] <author> Samuel J. Leffler, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year> <title> Rio: Storing Files Reliably in Memory 9 </title>
Reference: [Liskov91] <author> Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams. </author> <title> Replication in the Harp File System. </title> <booktitle> In Proceedings of the 1991 Symposium on Operating System Principles, </booktitle> <pages> pages 226238, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We plan on measuring the effective reliability of the virtual-memory protection used in Rio and Phoenix. Several projects attempt to protect certain information from failures. The Harp file system protects a log of recent modifications by replicating it in volatile, battery-backed memory across several server nodes <ref> [Liskov91] </ref>. The recovery box keeps special system state in a region of memory accessed only through a rigid interface [Baker92b]. No attempt is made to prevent other functions from accidentally modifying the recovery box, although the system detects corruption by maintaining checksums. [Horn91] describes an 5.
Reference: [McKusick90] <author> Marshall Kirk McKusick, Michael J. Karels, and Keith Bostic. </author> <title> A Pageable Memory Based Filesystem. </title> <booktitle> In Proceedings USENIX Summer Conference, </booktitle> <month> June </month> <year> 1990. </year>
Reference: [Needham83] <author> R. M. Needham, A. J. Herbert, and J. G. Mitchell. </author> <title> How to Connect Stable Memory to a Computer. Operating System Review, </title> <address> 17(1):16, </address> <month> January </month> <year> 1983. </year>
Reference-contexts: an exception when other modules try to change the file cache without using the memory device driver? Ideally, the protection mechanism would have the following characteristics: Lightweight: the protection mechanism should add little or no overhead to file cache accesses: it should not need to be invoked on memory reads <ref> [Needham83] </ref> and should have minimal overhead on writes. Enforced: it should be extremely unlikely that a non-malicious kernel function could accidentally bypass the protection mechanism. The vast majority of errors should be trapped. Simple: the protection mechanism should require little change to the existing system. <p> Hardware block copies between the two banks periodically checkpoint the memory image, but it is unclear what prevents a processor from corrupting the stable image. General mechanisms may be used to help protect memory from software faults. <ref> [Needham83] </ref> suggests changing a machines microcode to check certain conditions when writing a memory word; the condition they suggest is that a certain register has been pre-loaded with the memory words previous content.
Reference: [Nelson88] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1):134154, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: We have not yet implemented VM protection for the buffer cache, the memory device driver, or memory-mapped files. 6 Related Work Nearly all modern file systems use a file/buffer cache to speed up disk accesses <ref> [Nelson88, Lefer89] </ref>. Many have a memory file system that stores a complete, though temporary, file system in memory [McKu-sick90]. To our knowledge, however, the only file system that attempts to make its files reliable while in memory is Phoenix [Gait90]. Phoenix keeps two versions of an in-memory file system.
Reference: [Ousterhout85] <author> John K. Ousterhout, Herve Da Costa, et al. </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System. </title> <booktitle> In Proceedings of the 1985 Symposium on Operating System Principles, </booktitle> <pages> pages 1524, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: This is only possible for storing temporary files, since other files are too precious to take this kind of reliability risk with them. 3. Compromise by writing data to disk after a fixed delay, typically 30 seconds <ref> [Ousterhout85] </ref>. This is commonly done on systems that can tolerate the loss of recently-written data, such as many Unix systems. Unfortunately, 1/3 to 2/3 of newly written data lives longer than 30 seconds [Baker91, Hart-man93], so a large fraction of writes must eventually be written through to disk.
Reference: [Patterson88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> In International Conference on Management of Data (SIGMOD), </booktitle> <pages> pages 109116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: In addition, both reads and writes are forced to go through the device driver. We would like to improve memorys reliability without losing the ability to access memory randomly. Using redundancy can be used to protect memory in much the same way as with redundant disk arrays <ref> [Patterson88, Chen94] </ref>. Instead of trapping illegal accesses, this method allows a file cache page to survive some amount of corruption. For example, the system could store two copies of each file cache page or use lower-overhead error-correction such as Hamming codes.
Reference: [Rahm92] <author> Erhard Rahm. </author> <title> Performance Evaluation of Extended Storage Architectures for Transaction Processing. </title> <booktitle> In Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 308317, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: A longer delay can decrease disk traffic due to writes, but only at the cost of lower reliability. In addition to hurting performance and reliability, the assumed unreliability of memory increases system complexity for applications such as main-memory databases, file systems, and transaction processing systems <ref> [Rahm92] </ref>. Much of the research in main-memory databases deals with checkpointing and recovering data in case the system crashes [GM92, Eich87]. File systems must obey complex ordering constraints when forcing metadata to disk to ensure file system consistency [Ganger94].
Reference: [Sites92] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: The memory device driver can check for corruption during this window verifying the data after the write is completed. Unfortunately, many systems allow certain kernel accesses to bypass the virtual memory protection mechanism and directly access physical memory <ref> [Kane92, Sites92] </ref>.
Reference: [Srivastava94] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A System for Building Customized Program Analysis Tools. </title> <booktitle> In Proceedings of the 1994 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 196205, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: This section describes the status of our implementation at the time of this writing. We use the ATOM code-modification system <ref> [Eustace95, Srivastava94] </ref> to check kernel stores to the UBC and buffer cache. For the UBC, VM protection is not needed because the UBC is stored in physical memory and cannot be accessed using virtual addresses. Thus the protection system for the UBC is nearly complete.
Reference: [Sullivan91] <author> Mark Sullivan and R. Chillarege. </author> <title> Software Defects and Their Impact on System AvailabilityA Study of Field Failures in Operating Systems. </title> <booktitle> In Digest 21st International Symposium on Fault-Tolerant Computing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: This intuition is backed by field studies of MVS and Guardian (Tandems operating system), which show that between 1/4 to 1/2 of all software-induced system crashes could corrupt memory <ref> [Sullivan91, Lee93] </ref>. It is not yet clear how often these crashes corrupt the file cache; we are conducting experiments to measure this.
Reference: [Wahbe92] <author> Robert Wahbe. </author> <title> Efficient Data Breakpoints. </title> <booktitle> In Proceedings of the 1992 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations <ref> [Johnson82, Wahbe92] </ref>. Finally, object code modification has been suggested as a way to provide data breakpoints [Kessler90, Wahbe92] and fault isolation between software modules [Wahbe93]. <p> This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations [Johnson82, Wahbe92]. Finally, object code modification has been suggested as a way to provide data breakpoints <ref> [Kessler90, Wahbe92] </ref> and fault isolation between software modules [Wahbe93].
Reference: [Wahbe93] <author> Robert Wahbe, Steven Lucco, Thomas E. Anderson, and Susan L. Graham. </author> <title> Efficient Software-Based Fault Isolation. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 203216, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: addresses in the DEC Alpha processor with the two most significant bits equal to 10 2 are called KSEG addresses and bypass the TLB. 2 To protect against these physical addresses, we modify the kernel object code, inserting a check before every kernel store (this is called code patching in <ref> [Wahbe93] </ref>). If the address is a physical address, the system checks to make sure the address is not in the file cache, or that the file cache has explicitly registered the address as writable. 3 2. <p> If the stack pointer were corrupted, the act of calling a procedure could itself corrupt the file cache. To prevent this, we check the stack pointer whenever it is modified. Checking the stack pointer obviates the need to check stores that use the stack pointer to form an address <ref> [Wahbe93] </ref>. Since this includes all stores to local variables, this significantly lowers the number of stores that need to be checked. There are many other ways to lower the overhead of this check. <p> Finally, object code modification has been suggested as a way to provide data breakpoints [Kessler90, Wahbe92] and fault isolation between software modules <ref> [Wahbe93] </ref>.
Reference: [Wu94] <author> Michael Wu and Willy Zwaenepoel. eNVy: </author> <title> A Non-Volatile, Main Memory Storage System. </title> <booktitle> In Proceedings of the 1994 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: The two main factors limiting the reliability of memory are power outages and software corruption. Options for protecting memory against power outages include uninterruptible power supplies and switching to an inherently non-volatile memory technology such as Flash RAM <ref> [Wu94] </ref>. <p> Other projects seek to improve the reliability of memory against hardware faults such as power outages and board failures. eNVy implements a memory board based on ash RAM, which is non-volatile <ref> [Wu94] </ref>. eNVy uses copy-on-write, page remapping, and a small, battery-backed, SRAM buffer to hide ash RAMs slow writes and bulk erases. The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94].
References-found: 36

