URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/chaiken-dissert-1-10.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/chaiken-dissert.html
Root-URL: 
Title: MIT/LCS/TR-644 Mechanisms and Interfaces for Software-Extended Coherent Shared Memory  
Author: by David L. Chaiken Anant Agarwal 
Degree: (1990) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  All rights reserved. Author  Certified by  Associate Professor of Electrical Engineering and Computer Science Thesis Supervisor Accepted by Frederic R. Morgenthaler Chair, Department Committee on Graduate Students  
Note: c Massachusetts Institute of Technology  
Date: September 1994  1994, 1995.  September 1, 1994  
Address: (1986)  
Affiliation: Sc.B., Brown University  S.M., Massachusetts Institute of Technology  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Recent developments in software distributed shared memory use a combination of programmer annotations and protocol optimizations to allow systems to transmit kilobytes of data at a time, and only as much data as is needed to maintain coherence. These techniques use weaker memory consistency models <ref> [1] </ref> than sequential consistency [52] (a convenient model of shared memory), but provide synchronization mechanisms that allow programmers to write correct applications. In addition, some software memory systems make intelligent choices about data transfer and coherence policies.
Reference: [2] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of the Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> Available as MIT/LCS/TM-454. </note>
Reference-contexts: Thus, not only does the software-extension approach lead to a cost-efficient machine, it enables the synergy between components of a multiprocessing system that high performance requires. A software-extended memory system has been proposed, designed, tested, measured, and analyzed during the construction of Alewife <ref> [2] </ref>, a shared-memory architecture that scales to 512 processors. Alewife serves as both a proof-of-concept for the software-extension approach and as a platform for investigating shared-memory design and programming. Since the study has been conducted in the context of a real system, it focuses on simple solutions to practical problems.
Reference: [3] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The Alewife architecture scales up to 512 processing elements with distributed shared memory. It uses caches to implement fast memory access, and a software-extended memory system to enforce sequential consistency. on each node consists of a Sparcle processor <ref> [3] </ref>, which is based on the 32-bit SPARC architecture [72], and a floating-point coprocessor. The nodes communicate via messages through a direct network [67] with a mesh topology.
Reference: [4] <author> Anant Agarwal, Beng-Hong Lim, David A. Kranz, and John Kubiatowicz. </author> <month> APRIL: </month>
Reference-contexts: This implementation allows the processor to switch quickly between the different threads of execution, an action which is typically performed upon a remote memory access. While the context-switching mechanism is intended to help the system tolerate the latency of remote memory accesses <ref> [4, 50, 31] </ref>, it also accelerates the software-extended memory system: when the processor receives an interrupt from the CMMU, it can use the registers in an empty context to process the interrupt, rather than saving and restoring state.
References-found: 4

