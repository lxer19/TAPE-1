URL: http://www.dai.ed.ac.uk/students/johnde/aai.ps.gz
Refering-URL: http://www.dai.ed.ac.uk/students/johnde/research.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: volker@ls8.informatik.uni-dortmund.de  email: johnde@dai.ed.ac.uk  email: kaiser@ira.uka.de  
Phone: Phone: +49 231 755 2499 Fax: +49 231 755 5105  Phone: +44 131 650 3083 Fax: +44 131 650 6899  Phone: +49 721 608 4051 Fax: +49 721 606 740  
Title: Human-Robot-Communication and Machine Learning  
Author: Abbr. title: Human-Robot-Communication and ML Volker Klingspor John Demiris Michael Kaiser 
Web: EH1 2QL  
Address: D-44221 Dortmund, Germany  5 Forrest Hill Edinburgh, UK  Kaiserstr. 12 D-76128 Karlsruhe, Germany  
Affiliation: Universitat Dortmund Computer Science Dept. VIII  University of Edinburgh Department of Artificial Intelligence  Universitat Karlsruhe Institute for Real-Time Computer Systems and Robotics  
Abstract-found: 0
Intro-found: 1
Reference: <author> Abelson, R. </author> <year> 1963. </year> <title> Hot cognition. </title> <institution> Computer Simulation of Personality. </institution>
Reference-contexts: Humans, in contrast, represent events by relating ob-jects to an action that is performed with these objects in a single concept. The couplings between actions and their corresponding objects were termed predicates by <ref> (Abelson, 1963) </ref>, e.g., drinking from a cup, throwing a ball, moving through a doorway.
Reference: <author> Accame, M. and Natale, F. D. </author> <year> 1995. </year> <title> Neural tuned edge extraction in visual sensing. </title> <booktitle> In Third European Workshop on Learning Robots, </booktitle> <address> Heraklion, Kreta. </address>
Reference-contexts: de-burring (Asada and Liu, 1991) and assembly (Kaiser and Dillmann, 1996) as well as for vehicle control (Pomerleau, 1991) and autonomous robot navigation (Reignier et al., 1995), (Kaiser et al., 1996). * Learning new perceptive skills for object and landmark recognition can also take place on several system control levels. <ref> (Accame and Natale, 1995) </ref> present an approach to learn sensor parameterizations from demonstrations.
Reference: <author> Agre, P. and Chapman, D. </author> <year> 1990. </year> <title> What are plans for? In Maes, </title> <editor> P., editor, </editor> <booktitle> Designing Autonomous Agents, </booktitle> <pages> pp. 17-34. </pages> <publisher> MIT Press. 24 Andreae, </publisher> <editor> P. M. </editor> <year> 1984. </year> <title> Constraint limited generalization: Aquiring procedures from examples. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI), </booktitle> <pages> pp. 6 - 10. </pages>
Reference-contexts: In this approach, perception and action are separated. Planning is done in a classical way, creating a detailed sequence of actions, to be broken down to the lowest level of action, and executed until some problem arises. This approach for planning yields a few problems <ref> (Agre and Chapman, 1990) </ref>: real problems are computational intractable, planning is inadequate for worlds with unpredictable events, plans have to be very detailed, and plans do not relate to concrete situations. 5 Agre and Chapman view plans as one among several sources of information for deciding what to do next. <p> They call their approach plan as communication <ref> (Agre and Chapman, 1990) </ref>. Instead of defining a sequence of fixed and deterministic operators, plans just help to decide what's good to reach a given goal. <p> It cannot plan ahead to decide on what to do in the future in order to reach a goal. The project "Situated artificial communicators", similarly to <ref> (Agre and Chapman, 1990) </ref>, aims to embed commands into sequences of actions that are jointly perceived by the user and the system (Forster et al., 1995). The whole system integrates this planning and communication part with a natural language interface and an object identification system. <p> Nevertheless, the idea is close to those of plan as communication <ref> (Agre and Chapman, 1990) </ref>. All the previously described approaches do not really combine knowledge for planning with knowledge for control. Either the system is reactive and thus has no planning capabilities.
Reference: <author> Anzai, Y. </author> <year> 1993. </year> <booktitle> Human-computer-interaction in multianget environments. In Proc. of HCI International'93, </booktitle> <pages> pp. 2-7, </pages> <address> New York: </address> <publisher> Elsevier. </publisher>
Reference-contexts: An interesting review of projects involving such interactions in a Human-Robot-Computer system can be found in <ref> (Anzai, 1993) </ref>. 3 to robot and robot to user communication. This is learning for commu-nication. It aims at improving the communication between the two agents, i.e., to learn communication skills.
Reference: <author> Asada, H. and Liu, S. </author> <year> 1991. </year> <title> Transfer of human skills to neural net robot controllers. </title> <booktitle> In IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 2442 - 2448, </pages> <address> Sacramento. </address>
Reference-contexts: The acquisition of open-loop skills is mostly focused on the reconstruction of trajectories from a sequence of demonstrated states (positions) (Delson and West, 1994), (Ude, 1993). Systems supporting the acquisition of closed-loop elementary skills comprise acquisition techniques for manipulation tasks such as de-burring <ref> (Asada and Liu, 1991) </ref> and assembly (Kaiser and Dillmann, 1996) as well as for vehicle control (Pomerleau, 1991) and autonomous robot navigation (Reignier et al., 1995), (Kaiser et al., 1996). * Learning new perceptive skills for object and landmark recognition can also take place on several system control levels. (Accame and
Reference: <author> Asoh, H., Motomura, Y., Hara, I., Akaho, S., Hayamizu, S., and Matsui, T. </author> <year> 1996. </year> <title> Aquiring a probalistic map with dialogue-based learning. </title> <editor> In Hexmoor, H. and Meeden, L., editors, </editor> <booktitle> ROBOLEARN '96: An Intern. Workshop on Learning for Autonomous Robots, </booktitle> <pages> pp. 11-18, </pages> <address> Key West. </address>
Reference-contexts: In any case, our robots learn what we will call "skills", both for communication and for actual physical performance. Of course, other learning tasks can be supported by means of interaction. For example, <ref> (Asoh et al., 1996) </ref> use interaction between user and robot to acquire a map that is used for navigation. In this paper, we will first analyze different approaches to Human-Robot Communication and the role learning plays in these approaches (Section 2).
Reference: <author> Billard, A. </author> <year> 1996. </year> <title> Learning to speak through imitation for social robots. </title> <type> Master's thesis, </type> <institution> University of Edinburgh, </institution> <address> Scotland, UK. </address>
Reference-contexts: It can also be used to learn the meaning of certain motor-related words by associating the words transmitted with the actions performed (or the resulting end states) <ref> (Billard, 1996) </ref>. Imitation is also an appropriate method for learning additional motor skills. In the type of tasks that imitation has been used till now, the principal learning technique has been single-trial associative learning. Any type of associative learner can be used (pattern associator, Willshaw Nets etc).
Reference: <author> Birk, A. </author> <year> 1996. </year> <title> Learning to survive. </title> <editor> In Klingspor, V., editor, </editor> <booktitle> Proc. of the 5th European Workshop on Learning Robots, </booktitle> <pages> pp. 1-8, </pages> <address> Bari, Italy. </address>
Reference-contexts: Complete autonomy the main goal of research in AL (Steels, 1994) and the emphasis on learning for survival (i.e., to enable the robot to wander around for several days without colliding with obstacles and to learn when it is necessary to re-charge the battery <ref> (Birk, 1996) </ref>) is less important here. In these approaches, the systems learn from the environment, instead of learning from communication with a user. Furthermore, learning results in an improvement of their behavior for a fixed purpose, instead of learning something which might be used to communicate with the user.
Reference: <author> Bratko, I., Urbancic, T., and Sammut, C. </author> <year> 1995. </year> <title> Behavioural cloning: Phenomena, results, and problems. </title> <booktitle> In 5th IFAC Symposium on Automated Systems based on Human Skill, </booktitle> <address> Berlin. </address>
Reference-contexts: Typical applications are the cart-pole balancing tasks (Guez and Selinsky, 1988), (Dzeroski et al., 1995), as well as the work on "Learning to fly" (Sammut et al., 1992) and recent work on crane control (Urbancic and Bratko, 1994), <ref> (Bratko et al., 1995) </ref>. In contrast to work in robotics, these approaches focus on the evaluation of a specific learning technique for cloning. Also imitation learning (Hayes and Demiris, 1994), (Demiris and Hayes, 1996) must considered in the context of Robot Programming by Demonstration.
Reference: <author> Card, S. K., Moran, T. P., and Newell, A. </author> <year> 1983. </year> <title> The psychology of human-computer interaction. </title> <address> Hillsdale, New Jersey: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: In both cases, the user does not operate the robot. Instead, she wants to use it to accomplish some task. Therefore, the emphasis is on what the robot will do, not on how it should do it. As in Human-Computer Interaction <ref> (Card et al., 1983) </ref>, designing interfaces for Human-Robot Communication fl therefore involves the following aspects: 1. Firstly, the user must be provided with an interface that allows her to intuitively instruct the robot. Here, instruction involves to translate the user's intention into correct and executable robot programs. 2.
Reference: <author> Cypher, A. I. </author> <year> 1993. </year> <title> Watch what I do Programming by Demonstration. </title> <address> Cambridge, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Especially for users who are not experts in robot programming, Programming by Demonstration (PbD) <ref> (Cypher, 1993) </ref> has a considerable potential to become a suitable programming technique and to replace conventional robot programming languages (see, for example, (Groover, 1986) for an overview). PbD relies on demonstrations of the considered task.
Reference: <author> De Jong, G. and Mooney, R. </author> <year> 1986. </year> <title> Explanation-based-learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 2(1) </volume> <pages> 145-176. </pages>
Reference-contexts: But it is easy to find arbitrarily many objects with these properties that are not cups because it is impossible to drink from them, e.g., if the handle bridges over the opening <ref> (De Jong and Mooney, 1986) </ref>. Finding a complete description of a cup that excludes all exceptions is impossible because of the infinite number of these exceptions. This is known as the qualification problem (McCarthy and Hayes, 1969).
Reference: <author> Delson, N. and West, H. </author> <year> 1994. </year> <title> The use of human inconsistency in improving 3D robot trajectories. </title> <booktitle> In IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <pages> pp. 1248 - 1255, Munchen. </pages>
Reference-contexts: The acquisition of open-loop skills is mostly focused on the reconstruction of trajectories from a sequence of demonstrated states (positions) <ref> (Delson and West, 1994) </ref>, (Ude, 1993).
Reference: <author> Demiris, J. and Hayes, G. </author> <year> 1996. </year> <title> Imitative learning in robots and humans: Mechanisms and architectures. </title> <editor> In Klingspor, V., editor, </editor> <booktitle> Fifth European Workshop on Learning Robots, </booktitle> <address> Bari. Italy. </address>
Reference-contexts: In contrast to work in robotics, these approaches focus on the evaluation of a specific learning technique for cloning. Also imitation learning (Hayes and Demiris, 1994), <ref> (Demiris and Hayes, 1996) </ref> must considered in the context of Robot Programming by Demonstration. It is concerned with learning by imitating another agent (another robot), and as thus can be considered a special case of PbD. <p> Imitation greatly reduces demands from both the user and the interaction interface. It achieves that by introducing an additional level of complexity in the robot's control system: the mechanism for matching observed user movements with equivalent robot movements. <ref> (Demiris and Hayes, 1996) </ref> have proposed an imitation mechanism inspired by experimental results of research in early infant development (Meltzoff and Moore, 1989). <p> For more details about the imitation mechanism and its relation with mechanisms hypothesised to underlie human imitation see <ref> (Demiris and Hayes, 1996) </ref>. There are two sources of data that are used in the learning process under this paradigm. <p> To achieve the robot imitation abilities that are required for Human-Robot Communication, <ref> (Demiris and Hayes, 1996) </ref> have proposed the more general imitation mechanism which views the problem as a dynamic process where the learner participates in an active role since actions are viewed as relationship changes.
Reference: <author> Dillmann, R., Kaiser, M., Klingspor, V., Morik, K., and Wallner, F. </author> <year> 1995. </year> <title> Teaching and understanding intelligent service robots: A Machine Learning Approach. </title> <booktitle> In 19. Deutsche Jahrestagung fur Kunstliche Intel-ligenz (KI '95 Workshop: Informationsverarbeitung in Servicerobotern). </booktitle> <address> Berlin, Germany: </address> <publisher> Springer Verlag. 25 Dzeroski, </publisher> <editor> S., Todorovski, L., and Urbancic, T. </editor> <year> 1995. </year> <title> Handling real num-bers in ILP: A step towards better behavioural clones. </title> <booktitle> In European Conference on Machine Learning, </booktitle> <pages> pp. 283 - 286, </pages> <address> Heraklion, Kreta. </address>
Reference-contexts: However, to enable new robot applications with emphasis on service tasks, it is necessary to develop techniques which allow untrained users to make efficient and safe use of a robot. Two basic aspects characterize the interaction between the robot system and the user (Fig. 1, <ref> (Dillmann et al., 1995) </ref>). Firstly, the user wants to configure and, if necessary, instruct the robot for the task at hand. Secondly, the user may want to supervise and monitor the robot's performance. In both cases, the user does not operate the robot.
Reference: <author> Forster, S., Lobing, H., and Peters, K. </author> <year> 1995. </year> <note> Architekturkonzeption eines teilautonomen Montageroboters. Report - Situierte Kunstliche Kom-munikatoren 95/10, Universitat Bielefeld, Sonderforschungsbereich 360. </note>
Reference-contexts: It cannot plan ahead to decide on what to do in the future in order to reach a goal. The project "Situated artificial communicators", similarly to (Agre and Chapman, 1990), aims to embed commands into sequences of actions that are jointly perceived by the user and the system <ref> (Forster et al., 1995) </ref>. The whole system integrates this planning and communication part with a natural language interface and an object identification system. Each command triggers a single behaviour like "grasping", and relates this behaviour to an object. Like above, the commands are not used to define complex goals.
Reference: <author> Friedrich, H., Munch, S., Dillmann, R., Bocionek, S., and Sassin, M. </author> <year> 1996. </year> <title> Robot programming by demonstration: Supporting the induction by human interaction. </title> <journal> Machine Learning, </journal> <volume> 23:163 - 189. </volume>
Reference-contexts: In (Kuniyoshi et al., 1994), sequences of video images were analyzed in order to generate assembly plans. (Andreae, 1984) presented NODDY, a system which generates generalized programs by fusing several demonstrations. Single demonstrations and user intentions are the basis for the robot programs generated by the system described in <ref> (Friedrich et al., 1996) </ref>. * On the control level, demonstrations can be used as the basis for learning both, open-loop and closed-loop elementary skills. The acquisition of open-loop skills is mostly focused on the reconstruction of trajectories from a sequence of demonstrated states (positions) (Delson and West, 1994), (Ude, 1993).
Reference: <author> Giordana, A. and Saitta, L. </author> <year> 1990. </year> <title> Abstraction a general framework for learning. </title> <booktitle> In Proc. AAAI Workshop on Automatic Generation of Approximations and Abstractions, </booktitle> <pages> pp. 245-256. </pages>
Reference-contexts: If it is possible to drink from the perceived object, it can be classified as a cup. In this way, actions are integrated into concept descriptions and their recognition functions. Giordana and Saitta have proposed to use executable features in concepts descriptions <ref> (Giordana and Saitta, 1990) </ref>. These features are true for an object if a particular handling of this object is successful. For instance, the feature "movable" for a concept can be verified by moving that object.
Reference: <author> Goebel, M. </author> <year> 1996. </year> <title> Lernen intervallbezogener Merkmale eines mobilen Ro-boters. </title> <type> Master's thesis, </type> <institution> Universitat Dortmund. </institution>
Reference-contexts: However, each method deals with special characteristics of the domain, and the most important property of our learning task is the huge amount of noisy data. Therefore, some learning methods are of special interest. Goebel presented a very specialized algorithm for learning sequences from positive examples <ref> (Goebel, 1996) </ref>. Because this method reduces the data syntactically and applies a very efficient method for generating and evaluating hypotheses, this algorithm is up to ten times faster than the algorithms applied before.
Reference: <author> Groover, M. </author> <year> 1986. </year> <title> Industrial Robotics Technology, Programming and Applications. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Especially for users who are not experts in robot programming, Programming by Demonstration (PbD) (Cypher, 1993) has a considerable potential to become a suitable programming technique and to replace conventional robot programming languages (see, for example, <ref> (Groover, 1986) </ref> for an overview). PbD relies on demonstrations of the considered task. The demonstrations are used as the primary input and are the basis for a learning process.
Reference: <author> Guez, A. and Selinsky, J. </author> <year> 1988. </year> <title> A neuromorphic controller with a human teacher. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <pages> pp. 595 - 602, </pages> <address> San Diego. </address>
Reference-contexts: Typical applications are the cart-pole balancing tasks <ref> (Guez and Selinsky, 1988) </ref>, (Dzeroski et al., 1995), as well as the work on "Learning to fly" (Sammut et al., 1992) and recent work on crane control (Urbancic and Bratko, 1994), (Bratko et al., 1995). <p> While both approaches differ in their treatment and use of Human-Robot fl It should be noted that especially the cart-pole balancing task <ref> (Guez and Selinsky, 1988) </ref> is a task that proved to be extremely difficult for humans to demonstrate (in simulation). Even if they were able to balance the pole, they were never capable to stabilize the cart-pole system in the desired target state (cart and pole in zero position).
Reference: <author> Harnad, S. </author> <year> 1990. </year> <title> The symbol grounding problem. </title> <journal> Physica D, </journal> <volume> 42 </volume> <pages> 335-346. </pages>
Reference-contexts: In robot systems, the system developer usually analyses the data with different methods, combines them, generates new features, and searches for classification methods based on all these features (Rauber et al., 1993). This problem of "manual" symbol grounding is a very time consuming and hard task <ref> (Harnad, 1990) </ref>. Because of their different sensor systems, robot and developer have different views of the world. For example, in our case the robot only has distance sensors (sonars) in contrast to the powerful human vision system which is capable of identifying very different complex objects.
Reference: <author> Hayes, G. and Demiris, J. </author> <year> 1994. </year> <title> A robot controller using learning by imitation. </title> <booktitle> In International Symposium on Intelligent Robotic Systems, </booktitle> <pages> pp. 198 - 204, </pages> <address> Grenoble. </address>
Reference-contexts: In contrast to work in robotics, these approaches focus on the evaluation of a specific learning technique for cloning. Also imitation learning <ref> (Hayes and Demiris, 1994) </ref>, (Demiris and Hayes, 1996) must considered in the context of Robot Programming by Demonstration. It is concerned with learning by imitating another agent (another robot), and as thus can be considered a special case of PbD. <p> Motor actions that are performed due to the imitation of a demonstrator form the second source of data. Imitation learning has been used primarily in order to learn pairings of the form environmental state - action (s) to be followed (situation-action rules) <ref> (Hayes and Demiris, 1994) </ref>. It can also be used to learn the meaning of certain motor-related words by associating the words transmitted with the actions performed (or the resulting end states) (Billard, 1996). Imitation is also an appropriate method for learning additional motor skills. <p> If a suitable match is found, the corresponding actions are executed. Robot imitative mechanisms are relatively new and learning by imitation has only been demonstrated in the context of Robot-Robot Interaction and not yet in the context of Human-Robot Interaction. In the robot-robot context, the experiments reported in <ref> (Hayes and Demiris, 1994) </ref> have demonstrated how a robot can learn to negotiate the different corners of a maze by imitating the movements of a teacher robot.
Reference: <author> Heise, R. </author> <year> 1989. </year> <title> Demonstration instead of programming: Focussing attention in robot task acquisition. </title> <type> Research report 89/360/22, </type> <institution> Department of Computer Science, University of Calgary. </institution>
Reference-contexts: The demonstrations are used as the primary input and are the basis for a learning process. PbD has been applied successfully in domains such as graphic editors (Lieber-man, 1993), instructible software agents (Maulsby, 1994) and intelligent interfaces (Minton, 1995). Robot Programming by Demonstration (RPD, <ref> (Heise, 1989) </ref>) has been realized through a number of applications on different levels of both robot control and perception. * Demonstrations were proven to be suitable for the acquisition of new program schemata on task level (Segre, 1989).
Reference: <author> Heyes, C. </author> <year> 1993. </year> <title> Imitation, </title> <journal> culture and cognition. Animal Behaviour, </journal> <volume> 46 </volume> <pages> 999-1010. </pages>
Reference-contexts: For humans, the acquisition of skills without the availability of explicit knowledge has been reported in (Stanley et al., 1989), (Lane, 1988). This is the psychological basis for PbD, Skill Acquisition, and behavioural cloning. Imitation in humans and animals involves cognitive operations that are complex and not well understood <ref> (Heyes, 1993) </ref>. Despite the apparent complexity, it is useful for robots to have such capacity since learning by imitation offers certain desirable characteristics, most notably the ability to learn autonomously by observing and imitating without interrupting the expert while she is performing the task to be learned.
Reference: <author> Hiraki, K. and Anzai, Y. </author> <year> 1996. </year> <title> Sharing knowledge with robots. </title> <journal> Int. Journal of Human-Computer Interaction, </journal> <volume> 8(3) </volume> <pages> 325-342. </pages>
Reference-contexts: This way of grounding symbols is close to the way proposed by (Wrobel, 1991). The necessity to use machine learning for sharing knowledge between human and robot has been mentioned by <ref> (Hiraki and Anzai, 1996) </ref>, too. In their paper they describe a way to learn classifiers using feature abstraction, namely combining different features by basic arithmetical operations. 3.2 Combining actions and objects In classical systems objects and actions are separated.
Reference: <author> Kaiser, M., Deck, M., and Dillmann, R. </author> <year> 1996. </year> <title> Acquisition of basic mobility skills from human demonstrations. </title> <booktitle> Studies in Informatics and Control. </booktitle>
Reference-contexts: The acquisition of open-loop skills is mostly focused on the reconstruction of trajectories from a sequence of demonstrated states (positions) (Delson and West, 1994), (Ude, 1993). Systems supporting the acquisition of closed-loop elementary skills comprise acquisition techniques for manipulation tasks such as de-burring (Asada and Liu, 1991) and assembly <ref> (Kaiser and Dillmann, 1996) </ref> as well as for vehicle control (Pomerleau, 1991) and autonomous robot navigation (Reignier et al., 1995), (Kaiser et al., 1996). * Learning new perceptive skills for object and landmark recognition can also take place on several system control levels. (Accame and Natale, 1995) present an approach to <p> Systems supporting the acquisition of closed-loop elementary skills comprise acquisition techniques for manipulation tasks such as de-burring (Asada and Liu, 1991) and assembly (Kaiser and Dillmann, 1996) as well as for vehicle control (Pomerleau, 1991) and autonomous robot navigation (Reignier et al., 1995), <ref> (Kaiser et al., 1996) </ref>. * Learning new perceptive skills for object and landmark recognition can also take place on several system control levels. (Accame and Natale, 1995) present an approach to learn sensor parameterizations from demonstrations. <p> To support this kind of high-level interaction, we also require that the parameterization of any of the algorithms for skill acquisition may be done automatically <ref> (Kaiser and Dillmann, 1996) </ref>. We must make sure that the user doesn't have to provide any detailed information about the learning process. <p> Examples of other skills, both related to manipulation and to navigation, can be found in <ref> (Kaiser and Dillmann, 1996) </ref>, (Kaiser et al., 1996). In this case, the task for the human user was to bring the tip of the robot's gripper into contact with the surface of a workpiece and to apply a constant force F z of 10 [N ]. <p> Examples of other skills, both related to manipulation and to navigation, can be found in (Kaiser and Dillmann, 1996), <ref> (Kaiser et al., 1996) </ref>. In this case, the task for the human user was to bring the tip of the robot's gripper into contact with the surface of a workpiece and to apply a constant force F z of 10 [N ]. <p> The samples recorded during the demonstration (see Fig. 6) are preprocessed, relevant sensors and degrees of freedom are identified, and RBF 21 networks for representing C s and r s are generated (for details see <ref> (Kaiser and Dillmann, 1996) </ref>). Fig. 7 shows how the very same robot used to record the example, now controlled by the off-line generated neural network, is able to apply a constant force to the workpiece surface.
Reference: <author> Kaiser, M. and Dillmann, R. </author> <year> 1996. </year> <title> Building elementary robot skills from human demonstration. </title> <booktitle> In IEEE International Conference on Robotics and Automation, </booktitle> <address> Minneapolis, Minnesota, USA. </address> <note> 26 Kaiser, </note> <author> M., Friedrich, H., and Dillmann, R. </author> <year> 1995. </year> <title> Obtaining good per-formance from a bad teacher. </title> <booktitle> In International Conference on Machine Learning, Workshop on Programming by Demonstration, </booktitle> <address> Tahoe City, California. </address>
Reference-contexts: The acquisition of open-loop skills is mostly focused on the reconstruction of trajectories from a sequence of demonstrated states (positions) (Delson and West, 1994), (Ude, 1993). Systems supporting the acquisition of closed-loop elementary skills comprise acquisition techniques for manipulation tasks such as de-burring (Asada and Liu, 1991) and assembly <ref> (Kaiser and Dillmann, 1996) </ref> as well as for vehicle control (Pomerleau, 1991) and autonomous robot navigation (Reignier et al., 1995), (Kaiser et al., 1996). * Learning new perceptive skills for object and landmark recognition can also take place on several system control levels. (Accame and Natale, 1995) present an approach to <p> Systems supporting the acquisition of closed-loop elementary skills comprise acquisition techniques for manipulation tasks such as de-burring (Asada and Liu, 1991) and assembly (Kaiser and Dillmann, 1996) as well as for vehicle control (Pomerleau, 1991) and autonomous robot navigation (Reignier et al., 1995), <ref> (Kaiser et al., 1996) </ref>. * Learning new perceptive skills for object and landmark recognition can also take place on several system control levels. (Accame and Natale, 1995) present an approach to learn sensor parameterizations from demonstrations. <p> To support this kind of high-level interaction, we also require that the parameterization of any of the algorithms for skill acquisition may be done automatically <ref> (Kaiser and Dillmann, 1996) </ref>. We must make sure that the user doesn't have to provide any detailed information about the learning process. <p> Examples of other skills, both related to manipulation and to navigation, can be found in <ref> (Kaiser and Dillmann, 1996) </ref>, (Kaiser et al., 1996). In this case, the task for the human user was to bring the tip of the robot's gripper into contact with the surface of a workpiece and to apply a constant force F z of 10 [N ]. <p> Examples of other skills, both related to manipulation and to navigation, can be found in (Kaiser and Dillmann, 1996), <ref> (Kaiser et al., 1996) </ref>. In this case, the task for the human user was to bring the tip of the robot's gripper into contact with the surface of a workpiece and to apply a constant force F z of 10 [N ]. <p> The samples recorded during the demonstration (see Fig. 6) are preprocessed, relevant sensors and degrees of freedom are identified, and RBF 21 networks for representing C s and r s are generated (for details see <ref> (Kaiser and Dillmann, 1996) </ref>). Fig. 7 shows how the very same robot used to record the example, now controlled by the off-line generated neural network, is able to apply a constant force to the workpiece surface.
Reference: <author> Klingspor, V., Morik, K., and Rieger, A. </author> <year> 1996. </year> <title> Learning concepts from sensor data of a mobile robot. </title> <journal> Machine Learning Journal, </journal> 23(2/3):305-332. 
Reference-contexts: Learning active perception skills, i.e., the combination of actions and sensing for the purposes of object recognition, is the topic of work presented in <ref> (Klingspor et al., 1996) </ref>. 8 In Machine Learning, behavioural cloning has become popular as a syn-onym for "skill acquisition via human demonstration". <p> The same concepts that are used by the user to define tasks can be used by the robot to report about its activities. Therefore, the robot can use different levels of abstraction of our representation hierarchy (described in <ref> (Klingspor et al., 1996) </ref>) to enable the user to find reasons for the failure of a mission. E.g.: "mission failed because move through door failed, because through door failed. <p> She then communicates to the system what she would like to call the situations she has just shown to the robot. These examples are the primary source of knowledge and supervised learning is applied at all levels of our representation hierarchy <ref> (Klingspor et al., 1996) </ref>. During the application phase, the learned knowledge is used to control the robot. The user specifies a task to be performed. Then, the plan-scheduler activates several planners with different strategies in parallel in order to find a suitable plan as fast as possible. <p> In the following, we describe the decisions, that can be made at each of these steps. The basic techniques used in the different steps, as well as the concrete representation hierarchy, are described in detail in <ref> (Klingspor et al., 1996) </ref>. Presenting real world examples In this step, the desired concept is performed with a real robot in the real world. Different ways to control the robot exist: point-to-point-navigation, joystick-control, but also control by more complex media, or multi-modal. <p> Each feature can be described based on the features of the next 14 lower levels. The features at the level of single sensors are based on basic features. The next step is to decide what exactly will be represented at each level. In <ref> (Klingspor et al., 1996) </ref>, we define four features: concave and convex corners, jumps, and lines. We already mentioned in Section 3.1 why this decision is dangerous, and experiments showed that these features cannot be reliably perceived by the ultrasound sensors that we used. <p> Then, the learning task is to find common sequences of basic features within the time interval of different examples of the goal concept. The results presented later in this section show, that this approach is more successful than the one described in <ref> (Klingspor et al., 1996) </ref>. Learning concept descriptions The next step is learning itself. At each level of the hierarchy concept descriptions for each concept are learned. In principle, each ILP-method can be used for this task. <p> Therefore, we additionally performed learning from positive and negative examples with GRDT <ref> (Klingspor et al., 1996) </ref>. We applied the closed world assumption as a specific method of this type of learning. GRDT tries to learn rules 15 covering exactly the given instances of the concept, but deriving no further instances in other traces. <p> As long as the result is insufficient, some of the decisions made during the modeling process have to be revised, and the modelling loop restarts. To support the decision we made, we present the following experiments. Input for learning is the data described in <ref> (Klingspor et al., 1996) </ref>. Lines with MLJ as indicator describes results with the predefined set of sensor features, these are the results of (Klingspor et al., 1996). MG describes experiments with the new method of Michael Goebel for learning from positive examples with the new representation. <p> To support the decision we made, we present the following experiments. Input for learning is the data described in <ref> (Klingspor et al., 1996) </ref>. Lines with MLJ as indicator describes results with the predefined set of sensor features, these are the results of (Klingspor et al., 1996). MG describes experiments with the new method of Michael Goebel for learning from positive examples with the new representation. GRDT describes the results when additionally applying the closed world assumption using GRDT. Table 1 shows the coverage and accuracy of the different learning results.
Reference: <author> Kuniyoshi, Y., Inaba, M., and Inoue, H. </author> <year> 1994. </year> <title> Learning by watching: Extracting reusable task knowledge from visual observation of human performance. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(6) </volume> <pages> 799-822. </pages>
Reference-contexts: Robot Programming by Demonstration (RPD, (Heise, 1989)) has been realized through a number of applications on different levels of both robot control and perception. * Demonstrations were proven to be suitable for the acquisition of new program schemata on task level (Segre, 1989). In <ref> (Kuniyoshi et al., 1994) </ref>, sequences of video images were analyzed in order to generate assembly plans. (Andreae, 1984) presented NODDY, a system which generates generalized programs by fusing several demonstrations.
Reference: <author> Lane, N. E. </author> <year> 1988. </year> <title> Skill Acquisition rates and patterns issues and training implications. Recent research in Psychology. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York. </address>
Reference-contexts: Actual skills are developed by means of learning from these instructions, e.g., via generalization over the situations that are covered by the instructions fl . For humans, the acquisition of skills without the availability of explicit knowledge has been reported in (Stanley et al., 1989), <ref> (Lane, 1988) </ref>. This is the psychological basis for PbD, Skill Acquisition, and behavioural cloning. Imitation in humans and animals involves cognitive operations that are complex and not well understood (Heyes, 1993).
Reference: <author> Langle, T., Lueth, T., Herzog, G., Stopp, E., and Kamstrup, G. </author> <year> 1995. </year> <title> KAN-TRA a natural language interface for intelligent robots. </title> <editor> In Rembold, U. and Dillmann, R., editors, </editor> <booktitle> Intelligent Autonomous Systems IAS-4, </booktitle> <pages> pp. 365-372. </pages> <publisher> IOS Press. </publisher>
Reference-contexts: Dependent on how much the robot knows about its environment and how skillful it is, further communication with the user might be necessary during the course of action. An example of task specification via natural language is the system KANTRA <ref> (Langle et al., 1995) </ref>. The user can tell the robot which action it should perform with which object, for example "Grasp the red screw driver". In this approach, perception and action are separated.
Reference: <author> Lieberman, H. </author> <year> 1993. </year> <editor> MONDRIAN: A teachable graphical editor. In Cypher, A. I., editor, </editor> <title> Watch what I do. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Lopes, L. S. and Camarinha-Matos, L. </author> <year> (1995a). </year> <title> Planning, training and learning in supervision of flexible assembly systems. </title> <booktitle> In Proc. of Conf. on Balanced Automation Systems. </booktitle>
Reference-contexts: Providing explicit examples of erroneous behaviour is even more difficult and is especially sensitive to errors in the user's model of the robot and the task <ref> (Lopes and Camarinha-Matos, 1995a) </ref>. Supervision as described above, tries to find faulty operations, and reports these errors to the user. Instead of searching explicitly for errors, the performed actions and perceptions can be classified and matched with the desired goals.
Reference: <author> Lopes, L. S. and Camarinha-Matos, L. M. </author> <year> (1995b). </year> <title> Inductive generation of diagnostic knowledge for autonomous assembly. </title> <booktitle> In IEEE International Conference on Robotics and Automation, </booktitle> <address> Nagoya, Japan. </address>
Reference-contexts: Therefore, strong demands for learning such capabilities exist. In contrast to tool machines, robots (such as assembly robots) are expected to be capable of both diagnostics and error recovery. In general, three classes of errors can be distinguished <ref> (Lopes and Camarinha-Matos, 1995b) </ref>: system faults, external exceptions and execution failures. Execution failures are, for example, collisions, obstruction, part slippage from the gripper, part missing at some expected location, etc. External exceptions are abnormal occurrences in the cell environment that may cause execution failures.
Reference: <author> Maulsby, D. </author> <year> 1994. </year> <title> Instructible agents. </title> <type> PhD thesis, </type> <institution> University of Calgary, Canada. </institution>
Reference-contexts: PbD relies on demonstrations of the considered task. The demonstrations are used as the primary input and are the basis for a learning process. PbD has been applied successfully in domains such as graphic editors (Lieber-man, 1993), instructible software agents <ref> (Maulsby, 1994) </ref> and intelligent interfaces (Minton, 1995). Robot Programming by Demonstration (RPD, (Heise, 1989)) has been realized through a number of applications on different levels of both robot control and perception. * Demonstrations were proven to be suitable for the acquisition of new program schemata on task level (Segre, 1989).
Reference: <author> McCarthy, J. and Hayes, P. J. </author> <year> 1969. </year> <title> Some philosophical problems from the standpoint of artificial intelligence. </title> <journal> Machine Intelligence, </journal> <volume> 5 </volume> <pages> 463-502. </pages>
Reference-contexts: Finding a complete description of a cup that excludes all exceptions is impossible because of the infinite number of these exceptions. This is known as the qualification problem <ref> (McCarthy and Hayes, 1969) </ref>. So, how to define a cup suitably? The main issue of a cup is that you can drink from it. If a cup is defined as a receptacle from which drinking must be possible, the object classification can be verified by performing this action.
Reference: <author> Meltzoff, A. and Moore, M. </author> <year> 1989. </year> <title> Imitation in newborn infants: Exploring the range of gestures imitated and the underlying mechanisms. </title> <journal> Developmental Psychology, </journal> <volume> 25(6) </volume> <pages> 954-962. </pages>
Reference-contexts: It achieves that by introducing an additional level of complexity in the robot's control system: the mechanism for matching observed user movements with equivalent robot movements. (Demiris and Hayes, 1996) have proposed an imitation mechanism inspired by experimental results of research in early infant development <ref> (Meltzoff and Moore, 1989) </ref>.
Reference: <author> Minton, S. </author> <year> 1995. </year> <title> Quantitative results concerning the utility of explanation-based learning. </title> <editor> In Ram, A. and Leake, D. B., editors, </editor> <booktitle> Goal-driven learning, </booktitle> <pages> pp. 55-82. </pages> <publisher> A Bradford Book, The MIT Press, </publisher> <address> Cambridge Mas-sachusetts, London England. </address> <note> 27 Nadel, </note> <author> J. and Fontaine, A. M. </author> <year> 1989. </year> <title> Communicating by imitation: a devel-opmental and comparative approach to transitory social competence. </title> <editor> In Schneider, B., Attilli, G., Nadel, J., and Weissberg, R., editors, </editor> <title> Social Competence in Developmental Perspective. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: PbD relies on demonstrations of the considered task. The demonstrations are used as the primary input and are the basis for a learning process. PbD has been applied successfully in domains such as graphic editors (Lieber-man, 1993), instructible software agents (Maulsby, 1994) and intelligent interfaces <ref> (Minton, 1995) </ref>. Robot Programming by Demonstration (RPD, (Heise, 1989)) has been realized through a number of applications on different levels of both robot control and perception. * Demonstrations were proven to be suitable for the acquisition of new program schemata on task level (Segre, 1989).
Reference: <author> Nelson, K. </author> <year> 1983. </year> <title> The derivation of concepts and categories from event representations. </title> <editor> In Scholnick, E. K., editor, </editor> <title> New Trends in Conceptual Representation: Challenges to Piaget's Theory?, </title> <booktitle> chapter 6, </booktitle> <pages> pp. 129-149. </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Nelson states, that ". . . , young children must represent their own roles and the roles of other and be able to reciprocate actions of the other with actions of their own, . . . " <ref> (Nelson, 1983) </ref>. Objects should be represented as relations between actions and reactions, where an object implies a specific action, like a "ball implies throwing". <p> In addition, "An object or category of objects may recur in a number of different events, in similar or different relationships" <ref> (Nelson, 1983) </ref>. The category of doors may occur in different events like moving along them or moving through them, so we have different concepts relating different actions with different perceptions of the concept door. This combination of object and associated action is implemented by integrating perceptual features and action features.
Reference: <author> Pfeifer, R. and Scheier, C. </author> <year> 1994. </year> <title> From perception to action: The right direction? In Gaussier, </title> <editor> P. and Nicoud, J.-D., editors, </editor> <booktitle> From Peception to Action, </booktitle> <pages> pp. 1-11, </pages> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: These features are true for an object if a particular handling of this object is successful. For instance, the feature "movable" for a concept can be verified by moving that object. This is closely related to the arguments of <ref> (Pfeifer and Scheier, 1994) </ref>, that perception is a process that needs action. Perceptual features require the integration of the action that is performed while the object is being perceived. If, for example, you are looking at a cup from above you cannot determine whether the bottom is flat or not.
Reference: <author> Pomerleau, D. A. </author> <year> 1991. </year> <title> Efficient training of artificial neural networks for autonomous navigation. </title> <booktitle> Neural Computation, </booktitle> <address> 15:88 - 97. </address>
Reference-contexts: Systems supporting the acquisition of closed-loop elementary skills comprise acquisition techniques for manipulation tasks such as de-burring (Asada and Liu, 1991) and assembly (Kaiser and Dillmann, 1996) as well as for vehicle control <ref> (Pomerleau, 1991) </ref> and autonomous robot navigation (Reignier et al., 1995), (Kaiser et al., 1996). * Learning new perceptive skills for object and landmark recognition can also take place on several system control levels. (Accame and Natale, 1995) present an approach to learn sensor parameterizations from demonstrations.
Reference: <author> Rauber, T. W., Barata, M. M., and Steiger-Gar~c~ao, A. S. </author> <year> 1993. </year> <title> A toolbox for analysis and visualization of sensor data in supervision. </title> <booktitle> In Proc. of the Int. Conf. on Fault Diagnosis, </booktitle> <address> Toulouse, France. </address>
Reference-contexts: In robot systems, the system developer usually analyses the data with different methods, combines them, generates new features, and searches for classification methods based on all these features <ref> (Rauber et al., 1993) </ref>. This problem of "manual" symbol grounding is a very time consuming and hard task (Harnad, 1990). Because of their different sensor systems, robot and developer have different views of the world.
Reference: <author> Reignier, P., Hansen, V., and Crowley, J. </author> <year> 1995. </year> <title> Incremental supervised learning for mobile robot reactive control. </title> <booktitle> In Intelligent Autonomous Systems 4, </booktitle> <pages> pp. 287 - 294, </pages> <address> Karlsruhe. </address>
Reference-contexts: Systems supporting the acquisition of closed-loop elementary skills comprise acquisition techniques for manipulation tasks such as de-burring (Asada and Liu, 1991) and assembly (Kaiser and Dillmann, 1996) as well as for vehicle control (Pomerleau, 1991) and autonomous robot navigation <ref> (Reignier et al., 1995) </ref>, (Kaiser et al., 1996). * Learning new perceptive skills for object and landmark recognition can also take place on several system control levels. (Accame and Natale, 1995) present an approach to learn sensor parameterizations from demonstrations.
Reference: <author> Romann, J. </author> <year> 1995. </year> <title> Virtual reality as a control and supervision tool for autonomous systems. </title> <editor> In Rembold, U., Dillmann, R., Hertzberger, L., and Kanade, T., editors, IAS-4, </editor> <booktitle> Proc. of the 4th Intern. Conference on Intelligent Autonomous Systems, </booktitle> <pages> pp. 344-351. </pages> <publisher> IOS Press. </publisher>
Reference-contexts: This, however, requires a lot of knowledge about the environment, which is often not available. Even more knowledge is necessary, if the robot is controlled in virtual reality <ref> (Romann, 1995) </ref>. Here the 6 robot is controlled by a human seeing a virtual image of the environment the robot is working in. The human acts in this virtual world with a data-glove.
Reference: <author> Saffiotti, A., Konolige, K., and Ruspini, E. </author> <year> 1995. </year> <title> A multivalued logic approach to integrating planning and control. </title> <journal> Artificial Intelligence, </journal> 76(1/2):481-526. 
Reference-contexts: Either the system is reactive and thus has no planning capabilities. Or the system can communicate with the user, but operation execution and planning are separate and hence control is not reactive. Very few approaches combine reactive control with planning, e.g., <ref> (Saffiotti et al., 1995) </ref>. These approaches, unfortunately, don't use learned concepts for communication. Learning, however, is in our opinion essential for building a common language for communication (see Section 3). In addition to verbal communication, there exist many other ways to communicate with robots.
Reference: <author> Sammut, C., Hurst, S., Kedzier, D., and Michie, D. </author> <year> 1992. </year> <title> Learning to fly. </title> <booktitle> In Proc. of the 9th Intern. Conf. on Machine Learning, </booktitle> <pages> pp. 385-393. </pages>
Reference-contexts: Typical applications are the cart-pole balancing tasks (Guez and Selinsky, 1988), (Dzeroski et al., 1995), as well as the work on "Learning to fly" <ref> (Sammut et al., 1992) </ref> and recent work on crane control (Urbancic and Bratko, 1994), (Bratko et al., 1995). In contrast to work in robotics, these approaches focus on the evaluation of a specific learning technique for cloning.
Reference: <author> Schraft, R. D. </author> <year> 1994. </year> <title> Serviceroboter - ein Beitrag zur Innovation im Di-enstleistungswesen. </title> <institution> Fraunhofer -Institut fur Produktionstechnik und Automatisierung (IPA). </institution>
Reference-contexts: 1 Introduction Potential markets of enormous size exist for mobile robots in the areas of materials transport, mobile surveillance systems, and floor cleaning <ref> (Schraft, 1994) </ref>. In addition, the idea of the "personal robot" or "personal robotic assistant" (e.g., for aiding the elderly or disabled) is lately receiving a lot of attention.
Reference: <author> Segre, A. M. </author> <year> 1989. </year> <title> Machine Learning of Robot Assembly Plans. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Robot Programming by Demonstration (RPD, (Heise, 1989)) has been realized through a number of applications on different levels of both robot control and perception. * Demonstrations were proven to be suitable for the acquisition of new program schemata on task level <ref> (Segre, 1989) </ref>. In (Kuniyoshi et al., 1994), sequences of video images were analyzed in order to generate assembly plans. (Andreae, 1984) presented NODDY, a system which generates generalized programs by fusing several demonstrations.
Reference: <author> Stanley, W., Mathews, R. C., Buss, R. R., and Kotler-Cope, S. </author> <year> 1989. </year> <title> Insight without awareness: on the interaction of verbalization, instruction and practice in a simulated process control task. </title> <journal> The Quarterly Journal of Experimental Psychology, </journal> <volume> 41A(3):553 - 577. 28 Steels, </volume> <editor> L. </editor> <booktitle> 1994. The artificial life roots of artificial intelligence. Artificial Life, </booktitle> <pages> 1(1). </pages>
Reference-contexts: Actual skills are developed by means of learning from these instructions, e.g., via generalization over the situations that are covered by the instructions fl . For humans, the acquisition of skills without the availability of explicit knowledge has been reported in <ref> (Stanley et al., 1989) </ref>, (Lane, 1988). This is the psychological basis for PbD, Skill Acquisition, and behavioural cloning. Imitation in humans and animals involves cognitive operations that are complex and not well understood (Heyes, 1993).
Reference: <author> Stewart, J. </author> <year> 1995. </year> <title> The implications for understanding high-level cognition of a grounding in elementary adaptive systems. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <address> 16:107 - 116. </address>
Reference: <author> Ude, A. </author> <year> 1993. </year> <title> Trajectory generation from noisy positions of object features for teaching robot paths. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <address> 11:113 - 127. </address>
Reference-contexts: The acquisition of open-loop skills is mostly focused on the reconstruction of trajectories from a sequence of demonstrated states (positions) (Delson and West, 1994), <ref> (Ude, 1993) </ref>.
Reference: <author> Urbancic, T. and Bratko, I. </author> <year> 1994. </year> <title> Reconstructing human skill with machine learning. </title> <booktitle> In European Conference on Artificial Intelligence, </booktitle> <pages> pp. 498 - 502, </pages> <address> Amsterdam. </address>
Reference-contexts: Typical applications are the cart-pole balancing tasks (Guez and Selinsky, 1988), (Dzeroski et al., 1995), as well as the work on "Learning to fly" (Sammut et al., 1992) and recent work on crane control <ref> (Urbancic and Bratko, 1994) </ref>, (Bratko et al., 1995). In contrast to work in robotics, these approaches focus on the evaluation of a specific learning technique for cloning. Also imitation learning (Hayes and Demiris, 1994), (Demiris and Hayes, 1996) must considered in the context of Robot Programming by Demonstration.
Reference: <author> Van Lehn, K. </author> <year> 1996. </year> <title> Cognitive skill acquisition. </title> <booktitle> Annual Review of Psychology, </booktitle> <address> 47:513 - 539. </address>
Reference-contexts: There are two sources of data that are used in the learning process under this paradigm. The environmental state as perceived by the robot forms the fl Recent research in cognitive science <ref> (Van Lehn, 1996) </ref> indicates that generalization may not be an automatic process but needs to be triggered or guided by a teacher. However, in the case of inductive Machine Learning, generalization is the ultimate goal that's being pursued.

References-found: 54

