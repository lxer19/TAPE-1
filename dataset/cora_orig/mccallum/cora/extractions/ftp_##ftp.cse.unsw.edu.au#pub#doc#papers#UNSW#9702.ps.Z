URL: ftp://ftp.cse.unsw.edu.au/pub/doc/papers/UNSW/9702.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/research/tr.html
Root-URL: http://www.cse.unsw.edu.au
Email: E-mail: fpendrith,mikemg@cse.unsw.edu.au  
Title: An Analysis of non-Markov Automata Games: Implications for Reinforcement Learning  
Author: Mark D. Pendrith Michael J. McGarity 
Note: Communicated by Achim Hoffmann  
Date: 13 MARCH 1997  
Address: Sydney 2052 Australia  Sydney 2052 Australia  
Affiliation: School of Computer Science and Engineering The University of New South Wales  School of Electrical Engineering The University of New South Wales  
Pubnum: UNSW-CSE-TR-9702  
Abstract-found: 0
Intro-found: 1
Reference: <author> Billingsley, P. </author> <year> (1986). </year> <title> Probability and measure. </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Fudenberg, D., & Tirole, J. </author> <year> (1991). </year> <title> Game Theory. </title> <publisher> MIT Press. </publisher>
Reference-contexts: A Nash equilibrium is a global strategy that has the property that each component local strategy for a player is the best available play for that player assuming the other players play their local strategies consistent with that global strategy <ref> (Fudenberg & Tirole, 1991) </ref>. In dynamic programming (DP) terms, a Nash equilibrium corresponds to a policy that is stable under policy iteration. It is well known (e.g.
Reference: <author> Jaakkola, T., Singh, S., & Jordan, M. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Some steps in this direction have already been made in (Singh et al., 1994) and <ref> (Jaakkola, Singh, & Jordan, 1995) </ref>; the results presented above add weight to arguments that this is indeed the right direction to be heading. In moving to average reward criteria for NMDPs, an interesting set of open questions remain for future investigation.
Reference: <author> Kaelbling, L., Littman, M., & Cassandra, A. </author> <year> (1995). </year> <title> Planning and acting in partially observable stochastic domains. </title> <note> Submitted to Artificial Intelligence. </note>
Reference-contexts: The POMDP theoretical framework was originally formulated in the context of a set of operations research (OR) problems; the wider RL literature reflects an important line of research that is bringing OR methods to bear on the general problem of discovering effective policies in partially observable stochastic domains <ref> (Kaelbling, Littman, & Cassandra, 1995) </ref>. In contrast to "direct" methods of RL for POMDPs, however, these methods generally rely on state-estimation techniques that attempt to disambiguate observations into true Markov states. <p> By way of indication, even state-of-the-art techniques are still practically limited to problems of modest size (POMDPs in the order of ~100 states and dozens of observations <ref> (Littman, Cassandra, & Kaelbling, 1995) </ref>.) 8 Future Work A move from discounted to undiscounted rewards naturally suggests a closer look at average reward RL methods for equilibrium properties in non-Markov environments.
Reference: <author> Littman, M., Cassandra, A., & Kaelbling, L. </author> <year> (1995). </year> <title> Learning policies for partially observable environments: Scaling up. </title> <editor> In A.Prieditis, & S.Russell (Eds.), </editor> <booktitle> Machine Learning: Proc. of the Twelfth Int. Conf. </booktitle> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: The POMDP theoretical framework was originally formulated in the context of a set of operations research (OR) problems; the wider RL literature reflects an important line of research that is bringing OR methods to bear on the general problem of discovering effective policies in partially observable stochastic domains <ref> (Kaelbling, Littman, & Cassandra, 1995) </ref>. In contrast to "direct" methods of RL for POMDPs, however, these methods generally rely on state-estimation techniques that attempt to disambiguate observations into true Markov states. <p> By way of indication, even state-of-the-art techniques are still practically limited to problems of modest size (POMDPs in the order of ~100 states and dozens of observations <ref> (Littman, Cassandra, & Kaelbling, 1995) </ref>.) 8 Future Work A move from discounted to undiscounted rewards naturally suggests a closer look at average reward RL methods for equilibrium properties in non-Markov environments.
Reference: <author> Mahadevan, S. </author> <year> (1996). </year> <title> Sensitive discount optimality: Unifying discounted and average reward reinforcement learning. </title> <editor> In L.Saitta (Ed.), </editor> <booktitle> Machine Learning: Proc. of the Thirteenth Int. Conf. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In moving to average reward criteria for NMDPs, an interesting set of open questions remain for future investigation. In particular, Theorem 2 may point to subtle problems translating "transient reward" optimality metrics <ref> (Mahadevan, 1996) </ref> from MDPs to NMDPs. Investigations are continuing in this direction. 9 Conclusions A game theoretic approach has proven to be an aid to understanding the theoretical implications of applying standard discounted reward RL methods to non-Markov environments.
Reference: <author> Michie, D., & Chambers, R. </author> <year> (1968). </year> <title> BOXES: An experiment in adaptive control.. </title> <editor> In E.Dale, & D.Michie (Eds.), </editor> <booktitle> Machine Intelligence 2, </booktitle> <pages> pp. 137-152. </pages> <publisher> Edinburgh: Edinburgh Univ. Press. </publisher>
Reference-contexts: Indeed, this game theoretic view dates back to the earliest work in RL, firstly in the motivation for the BOXES algorithm <ref> (Michie & Chambers, 1968) </ref>, and later more explicitly in Witten's analysis of his adaptive optimal controller for discrete-time Markov environments (Witten, 1977). Casting RL into an n-player game, it is convenient at times to translate the familiar MDP terminology into equivalent game theoretic terms.
Reference: <author> Pendrith, M., & Ryan, M. </author> <year> (1996). </year> <title> Actual return reinforcement learning versus Temporal Differences: Some theoretical and experimental results. </title> <editor> In L.Saitta (Ed.), </editor> <booktitle> Machine Learning: Proc. of the Thirteenth Int. Conf. </booktitle> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: If the RL method is a 1-step temporal differences (TD) method, like Watkins' 1-step Q-learning, the method resembles an on-line, asynchronous form of value iteration. If the RL method is an actual return or Monte Carlo based method, like P-Trace <ref> (Pendrith & Ryan, 1996) </ref> the method more closely resembles an on-line, asynchronous form of policy iteration. So, for a Markov learning automata game, the optimal group strategies correspond to the equilibria for the game (Witten, 1977; Wheeler & Narendra, 1986).
Reference: <author> Puterman, M. </author> <year> (1994). </year> <title> Markov decision processes : Discrete stochastic dynamic programming. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Singh, S., Jaakkola, T., & Jordan, M. </author> <year> (1994). </year> <title> Learning without state-estimation in partially observable Markovian decision processes. </title> <editor> In W.Cohen, & H.Hirsh (Eds.), </editor> <booktitle> Machine Learning: Proc. of the Eleventh Int. Conf. </booktitle> <address> New Brunswick, New Jersey: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In the case of the example above the optimal strategy was also a pure strategy. However, it is known that in general for games corresponding to NMDPs there may be be no pure strategy among the optimal group strategies, as will always be the case for MDPs <ref> (Singh, Jaakkola, & Jordan, 1994) </ref>. Further, we show in this paper that if a TD method of credit assignment is used, or the rewards are discounted, the optimal global strategies may not be equilibrium points in the strategy space, even if an optimal pure strategy exists. <p> The easy and natural reasoning suggests the power of the game theoretic analytic framework. We note Theorem 3 also settles a conjecture in <ref> (Singh et al., 1994) </ref> regarding the optimality of QL for observation-based policies of POMDPs. The authors of that paper conjectured that QL in general might not be able to find the best deterministic memoryless (i.e. observation-based) policy for POMDPs; this result follows directly from the proof of Theorem 3. <p> For MDPs, an optimal policy has the property that all state values are maximal. In the framework we propose, we avoid this problem by adopting an alternative "first principles" definition of optimality for observation-based policies (refer to Equation (4)). Using this definition, the criterion of optimality used in <ref> (Singh et al., 1994) </ref> becomes merely a property of optimal policies for MDPs | one that just happens not to generalise to NMDPs. The other important difference is that Singh et al. limited their formal analysis and results to ergodic systems and gain-optimal average reward RL. <p> Some steps in this direction have already been made in <ref> (Singh et al., 1994) </ref> and (Jaakkola, Singh, & Jordan, 1995); the results presented above add weight to arguments that this is indeed the right direction to be heading. In moving to average reward criteria for NMDPs, an interesting set of open questions remain for future investigation.
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal difference. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. Thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference-contexts: For this representation, powerful convergence and optimality results have been proven for a number of algorithms designed with the simplifying assumption that the environment is Markov, e.g. 1-step Q-learning <ref> (Watkins, 1989) </ref>. With this assumption, the problem of learning can be cast into the form of finding an optimal policy for a Markov decision process (MDP), and methods like 1-step Q-learning (QL) can be shown to be a form of on-line asynchronous dynamic programming. <p> This is the key property that makes MDPs susceptible to RL techniques; it has become the convention to characterise RL in Markov domains as an asynchronous form of dynamic programming <ref> (Watkins, 1989) </ref>. If the RL method is a 1-step temporal differences (TD) method, like Watkins' 1-step Q-learning, the method resembles an on-line, asynchronous form of value iteration.
Reference: <author> Wheeler, Jr., R. M., & Narendra, K. S. </author> <year> (1986). </year> <title> Decentralized learning in finite Markov chains. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> AC-31 (6), </volume> <pages> 519-526. </pages>
Reference: <author> Witten, I. </author> <year> (1977). </year> <title> An adaptive optimal controller for discrete-time Markov environments. </title> <journal> Information and Control, </journal> <volume> 34, </volume> <pages> 286-295. 15 </pages>
Reference-contexts: Indeed, this game theoretic view dates back to the earliest work in RL, firstly in the motivation for the BOXES algorithm (Michie & Chambers, 1968), and later more explicitly in Witten's analysis of his adaptive optimal controller for discrete-time Markov environments <ref> (Witten, 1977) </ref>. Casting RL into an n-player game, it is convenient at times to translate the familiar MDP terminology into equivalent game theoretic terms. Instead of policy we might refer to group or global strategy ff.
References-found: 14

