URL: http://www.cs.pitt.edu/~gupta/research/Comp/lcr98.ps
Refering-URL: http://www.cs.pitt.edu/~gupta/research/dm.html
Root-URL: http://www.cs.pitt.edu
Email: soffa@cs.pitt.edug  
Title: Data Flow Analysis Driven Dynamic Data Partitioning  
Author: Jodi Tims Rajiv Gupta Mary Lou Soffa fjlt, gupta, 
Address: Pittsburgh Pittsburgh, PA 15260  
Affiliation: Department of Computer Science University of  
Abstract: The use of distributed memory architectures as an effective approach to parallel computing brings with it a more complex program development process. Finding a partitioning of program code and data that supports sufficient parallelism without incurring prohibitive communication costs is a challenging and critical step in the development of programs for distributed memory systems. Automatic data distribution techniques have the goal of placing the responsibility of determining a suitable data partitioning into the domain of the compiler. Static program analysis techniques that expose data interrelationships and derive performance estimates are central to the development of automatic data distribution heuristics. In this paper we present a data partitioning heuristic that makes use of array data flow analysis information in the modeling of data interrelationships and the estimation of costs associated with resolving interrelationships via communication. The global view provided by data flow analysis permits consideration of potential communication optimizations before data partitioning decisions are made. Our heuristic uses tiling techniques to determine data partitionings. The resulting data distributions, while still regular, are not limited to the standard BLOCK, CYCLIC and BLOCK-CYCLIC varieties. Preliminary results indicate an overall reduction in communication cost with our technique. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S.P. Amarashinge, J.M. Anderson, M.S. Lam, and C.W. Tseng. </author> <title> "The SUIF Compiler for Scalable Parallel Machines". </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: The compiler used to generate and optimize communications given a static data distribution is an extension to the Stanford SUIF compiler <ref> [1] </ref>. A data flow analysis based communication optimizer then performs message aggregation, redundant communication emulation and global message scheduling. For each of the five test programs, multiple static distributions were tested.
Reference: 2. <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> "Global Optimizations for Parallelism and Locality on Scalable Parallel Machines". </title> <booktitle> In ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 112-125, </pages> <address> Albuquerque, NM, </address> <month> Jun. </month> <year> 1993. </year>
Reference-contexts: The limitation of this approach is that if data interrelationship patterns vary significantly during the course of a program, some phases of the execution will incur significant communication costs. Dynamic data distribution techniques <ref> [2, 3, 4, 6, 7, 13, 15, 17] </ref> address this limitation by allowing arrays to be realigned and redistributed during execution so that more of the program's interrelationships may be resolved by the data distribution.
Reference: 3. <author> Eduard Ayguade, Jordi Garcia, Merce Girones, M. Luz Grande, and Jesus Labarta. </author> <title> "Data Redistribution in an Automatic Data Distribution Tool. </title> <booktitle> In Proceedings of the 8th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 407-421, </pages> <address> Columbus, Ohio, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: The limitation of this approach is that if data interrelationship patterns vary significantly during the course of a program, some phases of the execution will incur significant communication costs. Dynamic data distribution techniques <ref> [2, 3, 4, 6, 7, 13, 15, 17] </ref> address this limitation by allowing arrays to be realigned and redistributed during execution so that more of the program's interrelationships may be resolved by the data distribution.
Reference: 4. <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> "Automatic Data Layout Using 0-1 Integer Programming". </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT94), </booktitle> <pages> pages 111-122, </pages> <address> Montreal, Canada, </address> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: The limitation of this approach is that if data interrelationship patterns vary significantly during the course of a program, some phases of the execution will incur significant communication costs. Dynamic data distribution techniques <ref> [2, 3, 4, 6, 7, 13, 15, 17] </ref> address this limitation by allowing arrays to be realigned and redistributed during execution so that more of the program's interrelationships may be resolved by the data distribution.
Reference: 5. <author> D. Callahan and K. Kennedy. </author> <title> "Compiling Programs for Distributed-Memory Multiprocessors". </title> <journal> The Journal of SuperComputing, </journal> <volume> 2, </volume> <year> 1988. </year>
Reference-contexts: The problems of process decomposition and data distribution share a high degree of coupling. The Single-Program|Multiple-Datastream (SPMD) model of execution <ref> [5] </ref> assumed in this work, addresses this interaction by requiring that computations occur on the processor where the result operand has been mapped. This "owner-computes" rule simplifies program development for dis-tributed memory architectures somewhat, in that process decomposition is implicit in the determination of a data decomposition. <p> As an example of these costs, consider relationship edge (d; i) labeled i. This interrelationship arises from the use of a [i][j] to define b [i][j] at node i. Since process decomposition is accomplished via the owner computes rule <ref> [5] </ref>, the values of a [i][j] will be communicated if the distribution does not resolve this interrelationship.
Reference: 6. <author> Barbara M. Chapman, Thomas Fahringer, and Hans P. Zima. </author> <title> "Automatic Support for Data Distribution on Distributed Memory Multiprocessor Systems". </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 184-199, </pages> <address> Portland, Oregon, </address> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: The limitation of this approach is that if data interrelationship patterns vary significantly during the course of a program, some phases of the execution will incur significant communication costs. Dynamic data distribution techniques <ref> [2, 3, 4, 6, 7, 13, 15, 17] </ref> address this limitation by allowing arrays to be realigned and redistributed during execution so that more of the program's interrelationships may be resolved by the data distribution.
Reference: 7. <author> S. Chatterjee, J. R. Gilbert, R. Schreiber, and T.J. She*er. </author> <title> "Array Distribution in Data-Parallel Programs". </title> <booktitle> In Proceedings of the 7th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 76-91, </pages> <year> 1994. </year>
Reference-contexts: The limitation of this approach is that if data interrelationship patterns vary significantly during the course of a program, some phases of the execution will incur significant communication costs. Dynamic data distribution techniques <ref> [2, 3, 4, 6, 7, 13, 15, 17] </ref> address this limitation by allowing arrays to be realigned and redistributed during execution so that more of the program's interrelationships may be resolved by the data distribution.
Reference: 8. <author> Ron Cytron, Jeanne Ferrante, Barry Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> "Efficiently Computing Static Single Assignment Form and the Control Dependence Graph". </title> <journal> In ACM Transactions on Programming Languages and Systems, </journal> <volume> volume 13, </volume> <pages> pages 451-490, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: While these merge nodes in no way affect the semantics represented, they allow for the establishment of a single last distribution point for each array value in much the same manner as the phi nodes of Static Single Assignment form <ref> [8] </ref> provide for single last definition points. This approach serves to constrain set sizes during data flow analysis and to simplify automatic generation of communication statments at the expense of some realignment/redistribution communication at the merge nodes associated with conditional constructs.
Reference: 9. <author> Thomas Gross and Peter Steenkiste. </author> <title> "Structured Dataflow Analysis for Arrays and its Use in an Optimizing Compilers". </title> <journal> Software Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: Thus the interrelationships discovered are value-centric rather than location-centric and span the lifetime of the values rather than their use in a localized code segment. The precision of array analysis information realizable using interval data flow analysis techniques <ref> [9] </ref> in comparison to other methods of dependence detection (e.g., dependence vectors) enables some false interrelationships to be excluded from consideration in partitioning decisions. The nature of the interrelationship information impacts the philosophy underlying our heuristic.
Reference: 10. <author> Manish Gupta and Prithviraj Banerjee. </author> <title> "Demonstration of Automatic Data Partitioning Techniques for Parallelizing Compilers on Multicomputers". </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: This approach not only simplifies program development for distributed memory systems, but also encourages the use of nonstandard distribution patterns. Automatic data distribution techniques fall into two primary categories. Static data distribution techniques <ref> [10, 12] </ref> determine a single distribution for each array variable for the duration of the program's execution. The limitation of this approach is that if data interrelationship patterns vary significantly during the course of a program, some phases of the execution will incur significant communication costs.
Reference: 11. <author> Richard Johnson and Keshav Pingali. </author> <title> "Dependence-Based Program Analysis". </title> <booktitle> In ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 78-89, </pages> <month> Jun. </month> <year> 1993. </year>
Reference-contexts: Distribution flow edges bypass single-entry-single-exit regions of the ECFG where an array variable is not accessed. Placement of distribution flow edges is similar to the placement of dependence flow edges in the Dependence Flow Graph (DFG) <ref> [11] </ref>. The subgraph induced by the distribution flow edges for an array variable A is termed the Distribution Control Flow Graph of A (DCF G A ). Explicit representation of data interrelationships in the DIF comes through the extension of the ECFG with a set of relationship edges.
Reference: 12. <author> M. Kandemir, J. Ramanujam, and A. Choudhary. </author> <title> "Compiler Algorithms for Optimizing Locality and Parallelism on Shared and Distributed Memory Machines". </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT97), </booktitle> <address> San Francisco, CA, </address> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: This approach not only simplifies program development for distributed memory systems, but also encourages the use of nonstandard distribution patterns. Automatic data distribution techniques fall into two primary categories. Static data distribution techniques <ref> [10, 12] </ref> determine a single distribution for each array variable for the duration of the program's execution. The limitation of this approach is that if data interrelationship patterns vary significantly during the course of a program, some phases of the execution will incur significant communication costs.
Reference: 13. <author> Kathleen Knobe, Joan D. Lukas, and Guy L. Steele Jr. </author> <title> "Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines". </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 102-118, </pages> <year> 1990. </year>
Reference-contexts: The limitation of this approach is that if data interrelationship patterns vary significantly during the course of a program, some phases of the execution will incur significant communication costs. Dynamic data distribution techniques <ref> [2, 3, 4, 6, 7, 13, 15, 17] </ref> address this limitation by allowing arrays to be realigned and redistributed during execution so that more of the program's interrelationships may be resolved by the data distribution.
Reference: 14. <author> Uli Kremer. </author> <title> "NP-completeness of Dynamic Remapping". </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> The Netherlands, </address> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Finding the optimal dynamic data distribution for a given program has been determined to be an NP-complete problem <ref> [14] </ref>. Existing dynamic distribution techniques share a common view that a single distribution encompasses the partitioning of multiple array variables that are used within a particular code segment, most commonly a loop nest.
Reference: 15. <author> Daniel J. Palermo and Prithviraj Banerjee. </author> <title> "Automatic Selection of Dynamic Data Partitioning Schemes for Distributed-Memory Multipcomputers". </title> <booktitle> In Proceedings of the 8th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 392-406, </pages> <address> Columbus, OH, </address> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: The limitation of this approach is that if data interrelationship patterns vary significantly during the course of a program, some phases of the execution will incur significant communication costs. Dynamic data distribution techniques <ref> [2, 3, 4, 6, 7, 13, 15, 17] </ref> address this limitation by allowing arrays to be realigned and redistributed during execution so that more of the program's interrelationships may be resolved by the data distribution.
Reference: 16. <author> J. Ramanujam and P. Sadayappan. </author> <title> "Nested Loop Tiling for Distributed Memory Machines". </title> <booktitle> In 5th Distributed Memory Computing Conference, </booktitle> <pages> pages 1088-1096, </pages> <address> Charleston, SC, </address> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: This allows the partitioning heuristic to choose a data distribution that resolves data interrelationships whose communication costs would be highest if communication were required. Our heuristic uses tiling techniques <ref> [16] </ref> to determine data partitionings. The resulting data distributions, while still regular, are not limited to the standard BLOCK, CYCLIC and BLOCK CYCLIC varieties. Experimental results indicate an overall reduction in communication cost with our technique. <p> Again, the orientation/alignment of array B results in a transformation of this vector to (-2, 1). There are 9801 loop-invariant values associated with this dependence. In order to minimize communication costs, tile boundaries are aligned with communication dependences. The extreme vectors <ref> [16] </ref> in the set of all dependences for a cluster are used for this purpose so that the tiles formed will encapsulate the most communication. Each tile is constrained to have length equal to some constant multiple of the length of its associated dependence vector.
Reference: 17. <author> Thomas J. She*er, Robert Schreiber, William Pugh, John R. Gilbert, and Sid-dhartha Chatterjee. </author> <title> "Efficient Distribution Analysis via Graph Contraction". </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 377-391, </pages> <address> McLean, VA, </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: The limitation of this approach is that if data interrelationship patterns vary significantly during the course of a program, some phases of the execution will incur significant communication costs. Dynamic data distribution techniques <ref> [2, 3, 4, 6, 7, 13, 15, 17] </ref> address this limitation by allowing arrays to be realigned and redistributed during execution so that more of the program's interrelationships may be resolved by the data distribution.
Reference: 18. <author> X. Yuan, R. Gupta, and R. Melhem. </author> <title> "An Array Data Flow Analysis based Communication Optimizer". </title> <booktitle> In Proceedings of the 10th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Minneapolis, MN, </address> <month> Aug. </month> <year> 1997. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: This information is available as the inverse of the alignment function that mapped each array section to the virtual data space. 4 Experimental Results To assess the effectiveness of our heuristic, we have performed an experimental evaluation using a communication emulation system <ref> [18] </ref> that gathers statistics about the actual communication resulting from a series of send statements issued by a program. The emulator is implemented as a C library routine that simulates communication in a set of physical processors. <p> A client program of the emulator invokes the library's primary function at the point of each send statement, passing parameters of the communication in the form of a Section Communication Descriptor (SCD) <ref> [18] </ref>. The SCD includes the name of the array, a bounded regular section descriptor defining which elements are to be communicated and a description of the mappings of the elements to both the source and destination processors of the communication. A description of the virtual-to-real processor mapping is provided.
References-found: 18

