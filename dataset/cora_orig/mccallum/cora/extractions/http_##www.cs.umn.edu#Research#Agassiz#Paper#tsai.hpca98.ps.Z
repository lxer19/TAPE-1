URL: http://www.cs.umn.edu/Research/Agassiz/Paper/tsai.hpca98.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: fjtsai,zjiang,ness,yewg@cs.umn.edu  
Title: Performance Study of a Concurrent Multithreaded Processor  
Author: Jenn-Yuan Tsai Zhenzhen Jiang Eric Ness and Pen-Chung Yew 
Address: Urbana, IL 61801 Minneapolis, MN 55455  
Affiliation: Department of Computer Science Department of Computer Science University of Illinois University of Minnesota  
Abstract: The performance of a concurrent multithreaded architectural model, called superthreading [15], is studied in this paper. It tries to integrate optimizing compilation techniques and run-time hardware support to exploit both thread-level and instruction-level parallelism, as opposed to exploiting only instruction-level parallelism in existing superscalars. The superthreaded architecture uses a thread pipelining execution model to enhance the overlapping between threads, and to facilitate data dependence enforcement between threads through compiler-directed, hardware-supported, thread-level control speculation and run-time data dependence checking. We also evaluate the performance of the su-perthreaded processor through a detailed trace-driven simulator. Our results show that the superthreaded execution model can obtain good performance by exploiting both thread-level and instruction-level parallelism in programs. We also study the design parameters of its main system components, such as the size of the memory buffer, the bandwidth requirement of the communication links between thread processing units, and the bandwidth requirement of the shared data cache. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.-K. Chen and P.-C. Yew. </author> <title> Statement reordering for doacross loops. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <volume> volume Vol. II, </volume> <pages> pages 24-28, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: To do this, it needs to schedule target stores as early as possible, and schedule load instructions that may be data dependent on 4 the target stores of some predecessor threads as late as possible <ref> [1] </ref>.
Reference: [2] <author> M. Fillo, S. W. Keckler, W. J. Dally, N. P. Carter, A. Chang, Y. Gurevich, and W. S. Lee. </author> <title> The m-machine multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitec-ture, </booktitle> <pages> pages 146-156, </pages> <month> November 29-December 1, </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Recent concurrent multithreaded architectures (CMAs) such as the multiscalar [3, 11], the M-machine <ref> [2] </ref>, the simultaneous multithreaded architecture [16], and others [4, 8, 13] have shown that exploiting thread-level parallelism is a viable approach to improve the scalability of existing single-threaded superscalar architectures. <p> Among these CMA models, some of them [16, 8] only support concurrent execution of loosely-coupled threads similar to a multiprocessor-on-a-chip, while others <ref> [4, 11, 2] </ref> allow more tightly-coupled threads to execute in parallel with hardware support for direct data transfer between threads. Some of them also provide thread-level control and data speculation to exploit high-level program structures such as DO-While loops and objects referenced through pointers.
Reference: [3] <author> M. Franklin and G. S. Sohi. </author> <title> The expandable split window paradigm for exploiting fine-grained parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 58-67, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Recent concurrent multithreaded architectures (CMAs) such as the multiscalar <ref> [3, 11] </ref>, the M-machine [2], the simultaneous multithreaded architecture [16], and others [4, 8, 13] have shown that exploiting thread-level parallelism is a viable approach to improve the scalability of existing single-threaded superscalar architectures. <p> The following superthreading-specific optimization techniques [14] are also found to be useful: Conversion of Data Speculation to Control Speculation Hardware support for full data speculation can be very expensive, because it needs a buffer (called Address Resolution Buffer in multiscalar <ref> [3, 11] </ref>) to keep all load and store addresses from all active threads in order to detect data dependence violations.
Reference: [4] <author> H. Hirata, K. Kimura, S. Nagamine, Y. Mochizuki, A. Nishimura, Y. Nakase, and T. Nishizawa. </author> <title> An elementary processor architecture with simultaneous instruction issuing from multiple threads. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 136-145, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Recent concurrent multithreaded architectures (CMAs) such as the multiscalar [3, 11], the M-machine [2], the simultaneous multithreaded architecture [16], and others <ref> [4, 8, 13] </ref> have shown that exploiting thread-level parallelism is a viable approach to improve the scalability of existing single-threaded superscalar architectures. Using multiple threads of control to fetch fl This work is supported in part by the National Science Foundation under Grant No. <p> Among these CMA models, some of them [16, 8] only support concurrent execution of loosely-coupled threads similar to a multiprocessor-on-a-chip, while others <ref> [4, 11, 2] </ref> allow more tightly-coupled threads to execute in parallel with hardware support for direct data transfer between threads. Some of them also provide thread-level control and data speculation to exploit high-level program structures such as DO-While loops and objects referenced through pointers.
Reference: [5] <author> M. Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <month> June 22-24, </month> <year> 1988. </year>
Reference-contexts: more than one level of loops that have a good amount of parallelism, the compiler will schedule the outer loop that would not overflow the memory buffer for thread-level execution, and leave the inner loop for each thread processing unit to exploit instruction-level parallelism with branch speculation or software pipelining <ref> [5] </ref>. In the case that only inner-most loops are suitable for thread-level execution, the compiler can perform loop unrolling or loop blocking to increase the size of each thread, and to provide each thread processing unit with sufficient workload to exploit instruction-level parallelism.
Reference: [6] <author> J. K. F. Lee and A. J. Smith. </author> <title> Branch prediction strategies and branch target buffer design. </title> <journal> IEEE Computer, </journal> <volume> 17(1) </volume> <pages> 6-22, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: To provide sufficient instruction bandwidth, each thread processing unit has its own first-level (L1) instruction cache. The instruction fetch logic is also equipped with a branch target buffer <ref> [6] </ref> to eliminate branch delay and to support branch prediction and instruction-level speculative execution. Instructions fetched from the instruction cache are stored in the instruction queue before they are decoded and dispatched.
Reference: [7] <author> Z. Li, J.-Y. Tsai, X. Wang, P.-C. Yew, and B. Zheng. </author> <title> Compiler techniques for concurrent multithreading with hardware speculation support. </title> <booktitle> In Proceedings of the 9th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <volume> LNCS #1239, </volume> <pages> pages 175-191, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The compiler then partitions a program into threads, and each thread into multiple stages for thread pipelining <ref> [7, 14] </ref>. It also generates target store instructions for run-time data dependence checking. The compiler needs to increase the execution overlap of concurrent threads by minimizing the stalls caused by data dependences between threads. <p> All of these programs are written in C. Since the compiler development for the superthreaded processors requires both a front end parallelizing compiler and a back end optimizing compiler and is still under way <ref> [7, 14] </ref>, we manually transformed the most time-consuming routines of those programs into the superthreaded codes at the source level. This approach allows us to evaluate and study various architectural issues, design parameters, and the feasibility of the superthreaded execution model before its compiler is fully functional.
Reference: [8] <author> K. Olukotun, B. A. Nayfeh, L. Hammond, K. Wilson, and K. Chang. </author> <title> The case for a single-chip multiprocessor. </title> <booktitle> In Proceedings of 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-11, </pages> <month> October 1-5, </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Recent concurrent multithreaded architectures (CMAs) such as the multiscalar [3, 11], the M-machine [2], the simultaneous multithreaded architecture [16], and others <ref> [4, 8, 13] </ref> have shown that exploiting thread-level parallelism is a viable approach to improve the scalability of existing single-threaded superscalar architectures. Using multiple threads of control to fetch fl This work is supported in part by the National Science Foundation under Grant No. <p> D 346; and by a gift from Intel Corporation. and execute instructions from different locations of a programs simultaneously allows compilers and processors to exploit more parallelism from multiple instruction windows. Among these CMA models, some of them <ref> [16, 8] </ref> only support concurrent execution of loosely-coupled threads similar to a multiprocessor-on-a-chip, while others [4, 11, 2] allow more tightly-coupled threads to execute in parallel with hardware support for direct data transfer between threads.
Reference: [9] <author> J. E. Smith and A. R. Pleszkun. </author> <title> Implementation of precise interrupts in pipelined processors. </title> <booktitle> In Proceedings of the 12th International Symposium on Computer Architecture, </booktitle> <pages> pages 36-44, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Instructions can then be executed out of order when their operands are available. To support speculative execution and in-order instruction completion, the instruction dispatch and completion unit uses an reorder buffer <ref> [9] </ref> to buffer instruction results before they are committed. The reorder buffer also serves as an rename buffer to provide later instructions with uncommitted results which they are flow (read-after-write) dependent on. Each thread processing unit also has a communication unit for transferring commands and data between thread processing units.
Reference: [10] <author> M. D. Smith. </author> <title> Tracing with pixie. </title> <type> Technical report, </type> <institution> Stanford University, Stanford, </institution> <address> California 94305, </address> <month> November </month> <year> 1991. </year> <note> Technical Report CSL-TR-91-497. </note>
Reference-contexts: In the transformed superthreaded programs, special superthread-ing instructions, such as fork and store ts, are represented as function calls to specific subroutines. The transformed superthreaded program is compiled by the SGI C compiler. The program is then instrumented by Pixie <ref> [10] </ref> to generate instruction and memory reference traces. The simulator executes the instrumented program on the host SGI machine and collects the traces. During the trace collection phase, the simulator converts function calls which represent the superthreading instructions into the corresponding superthreading instructions for simulation.
Reference: [11] <author> G. S. Sohi, S. E. Breach, and T. N. Vijaykumar. </author> <title> Mul-tiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Recent concurrent multithreaded architectures (CMAs) such as the multiscalar <ref> [3, 11] </ref>, the M-machine [2], the simultaneous multithreaded architecture [16], and others [4, 8, 13] have shown that exploiting thread-level parallelism is a viable approach to improve the scalability of existing single-threaded superscalar architectures. <p> Among these CMA models, some of them [16, 8] only support concurrent execution of loosely-coupled threads similar to a multiprocessor-on-a-chip, while others <ref> [4, 11, 2] </ref> allow more tightly-coupled threads to execute in parallel with hardware support for direct data transfer between threads. Some of them also provide thread-level control and data speculation to exploit high-level program structures such as DO-While loops and objects referenced through pointers. <p> The superthreaded architecture uses a thread pipelining execution model to enhance the overlapping between threads. It also provides compiler-directed thread-level control and data speculation and run-time data dependence checking. Using compiler-directed thread-level control and data speculation allows it to have less hardware complexity than some prior CMA models <ref> [11, 13] </ref> for thread-level control and data speculation. However, it requires more sophisticated compiler technology to achieve good performance. Fortunately, its similarity to a multiprocessor-on-a-chip allows it to leverage many existing parallelizing compiler techniques. <p> The following superthreading-specific optimization techniques [14] are also found to be useful: Conversion of Data Speculation to Control Speculation Hardware support for full data speculation can be very expensive, because it needs a buffer (called Address Resolution Buffer in multiscalar <ref> [3, 11] </ref>) to keep all load and store addresses from all active threads in order to detect data dependence violations.
Reference: [12] <author> G. S. Sohi and M. Franklin. </author> <title> High-bandwidth data memory systems for superscalar processors. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 53-62, </pages> <month> April 8-11, </month> <year> 1991. </year>
Reference-contexts: Techniques to improve cache bandwidth include non-blocking, multi-port, and interleaved multi-bank <ref> [12] </ref>. Non-blocking improves the cache performance by reducing the bandwidth degradation due to misses. Multi-port (duplicated ports) and multi-bank caches can serve multiple requests per cycle. In general, a multi-port cache is more effective than a multi-bank cache because it has less port contention.
Reference: [13] <author> J. G. Steffan and T. C. Mowry. </author> <title> The potential for thread-level data speculation in tightly-coupled multiprocessors. </title> <type> Technical report, </type> <institution> Computer Science Research Institute, University of Toronto, </institution> <month> February </month> <year> 1997. </year> <note> Technical Report CSRI-TR-350. </note>
Reference-contexts: 1 Introduction Recent concurrent multithreaded architectures (CMAs) such as the multiscalar [3, 11], the M-machine [2], the simultaneous multithreaded architecture [16], and others <ref> [4, 8, 13] </ref> have shown that exploiting thread-level parallelism is a viable approach to improve the scalability of existing single-threaded superscalar architectures. Using multiple threads of control to fetch fl This work is supported in part by the National Science Foundation under Grant No. <p> The superthreaded architecture uses a thread pipelining execution model to enhance the overlapping between threads. It also provides compiler-directed thread-level control and data speculation and run-time data dependence checking. Using compiler-directed thread-level control and data speculation allows it to have less hardware complexity than some prior CMA models <ref> [11, 13] </ref> for thread-level control and data speculation. However, it requires more sophisticated compiler technology to achieve good performance. Fortunately, its similarity to a multiprocessor-on-a-chip allows it to leverage many existing parallelizing compiler techniques.
Reference: [14] <author> J.-Y. Tsai, Z. Jiang, and P.-C. Yew. </author> <title> Program optimization for concurrent multithreaded architecture. </title> <booktitle> In Proceedings of the 10th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: The compiler then partitions a program into threads, and each thread into multiple stages for thread pipelining <ref> [7, 14] </ref>. It also generates target store instructions for run-time data dependence checking. The compiler needs to increase the execution overlap of concurrent threads by minimizing the stalls caused by data dependences between threads. <p> To do this, it needs to schedule target stores as early as possible, and schedule load instructions that may be data dependent on 4 the target stores of some predecessor threads as late as possible [1]. The following superthreading-specific optimization techniques <ref> [14] </ref> are also found to be useful: Conversion of Data Speculation to Control Speculation Hardware support for full data speculation can be very expensive, because it needs a buffer (called Address Resolution Buffer in multiscalar [3, 11]) to keep all load and store addresses from all active threads in order to <p> All of these programs are written in C. Since the compiler development for the superthreaded processors requires both a front end parallelizing compiler and a back end optimizing compiler and is still under way <ref> [7, 14] </ref>, we manually transformed the most time-consuming routines of those programs into the superthreaded codes at the source level. This approach allows us to evaluate and study various architectural issues, design parameters, and the feasibility of the superthreaded execution model before its compiler is fully functional.
Reference: [15] <author> J.-Y. Tsai and P.-C. Yew. </author> <title> The superthreaded architecture: Thread pipelining with run-time data dependence checking and control speculation. </title> <booktitle> In Proceedings of the 1996 Conference on Parallel Architectures and Compilation Techniques, PACT '96, </booktitle> <pages> pages 35-46, </pages> <address> Oc-tober 20-23, </address> <year> 1996. </year>
Reference-contexts: Some of them also provide thread-level control and data speculation to exploit high-level program structures such as DO-While loops and objects referenced through pointers. In this paper, we study the performance of a CMA model with tightly-coupled threads, called su-perthreading <ref> [15] </ref>. It integrates compilation techniques and run-time hardware support to exploit both thread-level and instruction-level parallelism in programs. The superthreaded architecture uses a thread pipelining execution model to enhance the overlapping between threads. It also provides compiler-directed thread-level control and data speculation and run-time data dependence checking. <p> However, it requires more sophisticated compiler technology to achieve good performance. Fortunately, its similarity to a multiprocessor-on-a-chip allows it to leverage many existing parallelizing compiler techniques. In this paper, we briefly present the architectural and program execution model of the superthreaded architecture (refer to <ref> [15] </ref> for more details) in section 2. We then identify the required compilation techniques to support thread-level control and data speculation, and also the issues related to the exploitation of instruction-level parallelism within each thread in section 3. <p> A memory buffer is provided for run-time data dependence checking between threads. The memory buffer is also used to store speculative and private data from each thread for supporting compiler-directed, thread-level control and data speculation. For more details, please refer to <ref> [15] </ref>. The compiler statically partitions the control flow graph of a program into threads. Each thread corresponds to a portion of the control flow graph. A program starts execution from its entry thread. <p> To provide each thread processing unit with sufficient workload to exploit instruction-level parallelism, the compiler would include as many instructions in a thread as possible. On the other hand, the size of a thread cannot be too large to avoid overflowing the memory buffer <ref> [15] </ref>. In addition, the compiler would partition a program at a level where the maximum thread-level parallelism is available. For some programs, however, the best thread level is at the inner-most loops which may have a small amount of instructions in each loop iteration.
Reference: [16] <author> D. M. Tullsen, S. J. Eggers, and H. M. Levy. </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392-403, </pages> <month> June 22-24, </month> <year> 1995. </year> <pages> 10 11 12 </pages>
Reference-contexts: 1 Introduction Recent concurrent multithreaded architectures (CMAs) such as the multiscalar [3, 11], the M-machine [2], the simultaneous multithreaded architecture <ref> [16] </ref>, and others [4, 8, 13] have shown that exploiting thread-level parallelism is a viable approach to improve the scalability of existing single-threaded superscalar architectures. Using multiple threads of control to fetch fl This work is supported in part by the National Science Foundation under Grant No. <p> D 346; and by a gift from Intel Corporation. and execute instructions from different locations of a programs simultaneously allows compilers and processors to exploit more parallelism from multiple instruction windows. Among these CMA models, some of them <ref> [16, 8] </ref> only support concurrent execution of loosely-coupled threads similar to a multiprocessor-on-a-chip, while others [4, 11, 2] allow more tightly-coupled threads to execute in parallel with hardware support for direct data transfer between threads.
References-found: 16

