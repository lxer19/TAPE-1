URL: ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-97-4-1-chahl.ps.Z
Refering-URL: ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-97-4-1.html
Root-URL: http://www.cs.man.ac.uk
Email: Javaan.Chahl@anu.edu.au M.Srinivasan@anu.edu.au  
Title: Navigation, Path Planning and Homing for Autonomous Mobile Robots using Panoramic Visual Sensors rapid and
Author: J.S. Chahl and M.V. Srinivasan 
Note: All of the algorithms described are  
Date: 7/4/97  
Address: Australia, 2605  
Affiliation: Visual Sciences Australian National University ACT,  
Abstract: Algorithms for computing the fundamental parameters of navigation have been devised; range, egomotion, and localisation. All algorithms use a panoramic strip image as their sole sensory input. All motion/deformation computations are performed using the Image Interpolation technique [1]. This technique has shown itself to be effective in diverse machine vision problems. It is particularly adept at computing the relative deformation between images that are deformed by non-affine processes. The first algorithm presented computes the parameters of egomotion of a mobile robot equipped with two panoramic visual sensors. We show that egomotion can be computed using the image interpolation algorithm by interpolating the position of the image captured by one of the sensors at the robot's present location, with respect to the images captured by the two sensors at the robot's previous location. The algorithm delivers the distance traveled and angle rotated. Trajectories of over 1.5m in length have been reconstructed to within 5% accuracy using this technique. The second algorithm computes range in the entire panoramic environment. Range estimation is based on the fact that local deformation of the panoramic image is range-dependent, regardless of the nature of the deformation. The nature of the deformation depends on azimuthal viewing direction: expansion in the direction of motion, contraction in the opposite direction, and translation for viewing directions perpendicular to that of the motion. Range in each direction is estimated by comparing the magnitude of the measured image deformation, with the magnitude of the deformation that would be produced by a surface at a known, standard, minimum range. Range can be accurately computed to about 100 times the distance moved by the sensor. A sensor's location in both orientation and position is computed by using the image interpolation algorithm to interpolate the image captured by the panoramic sensor at that location with respect to panoramic images captured at known, reference locations. The algorithm can reliably determine the location and orientation of the sensor within a small region. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. V. Srinivasan. </author> <title> An image interpolation technique for the computation of optic flow and egomotion. </title> <journal> Biological Cybernetics, </journal> <volume> 71 </volume> <pages> 401-415, </pages> <year> 1994. </year>
Reference: [2] <author> R. C. Nelson and J. Aloimonos. </author> <title> Finding motion parameters from spherical motion fields (or the advantages of having eyes in the back of your head). </title> <journal> Biological Cybernetics, </journal> <volume> 58 </volume> <pages> 261-273, </pages> <year> 1988. </year>
Reference: [3] <author> J. Pichon, C. Blanes, and N. Francheschini. </author> <title> Visual guidance of a mobile robot equipped with a network of self-motion sensors. </title> <booktitle> In Mobile Robots IV, </booktitle> <pages> pages 44-53, </pages> <address> Philadelphia, Pennsylvania, </address> <month> November </month> <year> 1989. </year> <pages> SPIE. </pages>
Reference: [4] <author> J. Hong, X. Tan, B. Pinette, R. Weiss, and E. M. Riseman. </author> <title> Image-based homing. </title> <booktitle> In Proceedings of the 1991 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 620-625, </pages> <address> Sacramento, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: edges or boundaries, allowing a very uniform treatment of the image arrays, thereby increasing the inherent parallelism in the algorithms. * Finally, panoramic images capture the entire environment in every image, thus offering the possibility of homing to landmarks regardless of bearing, and removing the requirement to articulate the camera <ref> [4] </ref>. The Image Interpolation Algorithm A common thread running through this work is the Image Interpolation Algorithm [1][5][6]. This algorithm has shown itself to be versatile in diverse machine vision problems, from computing motion vectors, to computing the magnitude of complex non-affine transforms.
Reference: [5] <author> M. G. Nagle and M. V. Srinivasan. </author> <title> Structure from motion: Determining range and orientation of surfaces by image interpolation. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 13(1) </volume> <pages> 25-34, </pages> <month> January </month> <year> 1996. </year>
Reference: [6] <author> J. S. Chahl and M. V. Srinivasan. </author> <title> Visual computation of ego-motion using an image interpolation technique. </title> <journal> Biological Cybernetics, </journal> <volume> 74 </volume> <pages> 405-411, </pages> <year> 1996. </year>
References-found: 6

