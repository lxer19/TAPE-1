URL: ftp://ftp.cs.utoronto.ca/pub/radford/slice.ps.Z
Refering-URL: http://www.cs.toronto.edu/~radford/res-mcmc.html
Root-URL: 
Email: radford@stat.utoronto.ca  
Title: Markov Chain Monte Carlo Methods Based on `Slicing' the Density Function  
Author: Radford M. Neal 
Date: 18 November 1997  
Web: http://www.cs.utoronto.ca/~radford/  
Address: Toronto, Toronto, Ontario, Canada  
Affiliation: Department of Statistics and Department of Computer Science University of  
Abstract: Technical Report No. 9722, Department of Statistics, University of Toronto Abstract. One way to sample from a distribution is to sample uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal `slice' defined by the current vertical position. Variations on such `slice sampling' methods can easily be implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling, and may be more efficient than easily-constructed versions of the Metropolis algorithm. Slice sampling is therefore attractive in routine Markov chain Monte Carlo applications, and for use by software that automatically generates a Markov chain sampler from a model specification. One can also easily devise overrelaxed versions of slice sampling, which sometimes greatly improve sampling efficiency by suppressing random walk behaviour. Random walks can also be avoided in some slice sampling schemes that simultaneously update all variables. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Adler, S. L. </author> <title> (1981) "Over-relaxation method for the Monte Carlo evaluation of the partition function for multiquadratic actions", </title> <journal> Physical Review D, </journal> <volume> vol. 23, </volume> <pages> pp. 2901-2904. </pages>
Reference: <author> Barone, P. and Frigessi, A. </author> <title> (1990) "Improving stochastic relaxation for Gaussian random fields", </title> <journal> Probability in the Engineering and Informational Sciences, </journal> <volume> vol. 4, </volume> <pages> pp. 369-389. </pages>
Reference: <author> Besag, J. and Green, P. J. </author> <title> (1993) "Spatial statistics and Bayesian computation" (with discussion), </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> vol. 55, </volume> <pages> pp. 25-37 (discussion, pp. 53-102). </pages>
Reference: <author> Damien, P., Wakefield, J., and Walker, S. </author> <title> (1997) "Gibbs sampling for Bayesian nonconjugate and hierarchical models using auxiliary variables", </title> <type> preprint. </type>
Reference: <author> Diaconis, P., Holmes, S., and Neal, R. M. </author> <title> (1997) "Analysis of a non-reversible Markov chain sampler", </title> <type> Technical Report BU-1385-M, </type> <institution> Biometrics Unit, Cornell University, </institution> <address> 26 pages. </address>
Reference: <author> Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. </author> <title> (1987) "Hybrid Monte Carlo", </title> <journal> Physics Letters B, </journal> <volume> vol. 195, </volume> <pages> pp. 216-222. </pages>
Reference-contexts: In this section, I will describe schemes based on step-by-step movements that `reflect' off the boundaries of the slice. Such movement with reflection can be seen as a specialization to uniform distributions of the Hamiltonian dynamics that forms the basis for Hybrid Monte Carlo <ref> (Duane, et al 1987) </ref>. Like other dynamical methods, such reflective slice sampling schemes can suppress random walks, and thereby improve sampling efficiency. <p> These problems are not present with slice sampling. In any of these situations, we might prefer to use a method that can suppress random walks. Dynamical methods such as Hybrid Monte Carlo <ref> (Duane, et al 1987) </ref> do this well for a wide range of distributions; reflective slice sampling may also work for a wide range of distributions, but preliminary indications are that is less efficient than Hybrid Monte Carlo, when both are tuned optimally.
Reference: <author> Edwards, R. G. and Sokal, A. D. </author> <title> (1988) "Generalization of the Fortuin-Kasteleyn-Swendsen-Wang representation and Monte Carlo algorithm", </title> <journal> Physical Review D, </journal> <volume> vol. 38, </volume> <pages> pp. 2009-2012. </pages>
Reference: <author> Frey, B. J. </author> <title> (1997) "Continuous sigmoidal belief networks trained using slice sampling", </title> <editor> in M. C. Mozer, M. I. Jordan, and T. </editor> <booktitle> Petsche (editors) Advances in Neural Information Processing Systems 9, </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Gelfand, A. E. and Smith, A. F. M. </author> <title> (1990) "Sampling-based approaches to calculating marginal densities", </title> <journal> Journal of the American Statistical Association, </journal> <volume> vol. 85, </volume> <pages> pp. 398-409. </pages>
Reference-contexts: 1 Introduction Markov chain methods such as Gibbs sampling <ref> (Gelfand and Smith 1990) </ref> and the Metropolis algorithm (Metropolis, et al 1953, Hastings 1970) can be used to sample from many of the complex, multivariate distributions encountered in statistics (see, for example, the book edited by Gilks, Richardson, and Spiegelhalter (1996)).
Reference: <author> Gilks, W. R., Richardson, S., and Spiegelhalter, D. J. </author> <title> (editors) (1996) Markov Chain Monte Carlo in Practice, </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> Gilks, W. R. </author> <title> (1992) "Derivative-free adaptive rejection sampling for Gibbs sampling", </title> <editor> in J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith (editors), </editor> <booktitle> Bayesian Statistics 4, </booktitle> <pages> pp. 641-649, </pages> <institution> Oxford University Press. </institution> <note> 26 Gilks, </note> <author> W. R., Best, N. G., and Tan, K. K. C. </author> <title> (1995) "Adaptive rejection Metropolis sampling within Gibbs sampling", </title> <journal> Applied Statistics, </journal> <volume> vol. 44, </volume> <pages> pp. 455-472. </pages>
Reference-contexts: This method is used by the BUGS software <ref> (Thomas, Spiegelhalter, and Gilks 1992) </ref> to automatically generate Markov chain samplers from model specifications. The first step in applying ARS is to find points on each side of the mode of the conditional distribution (one of which can be the current point).
Reference: <author> Gilks, W. R., Neal, R. M., Best, N. G., and Tan, K. K. C. </author> <title> (1997) "Corrigendum: Adaptive rejection Metropolis sampling", </title> <note> to appear in Applied Statistics. </note>
Reference-contexts: Also, when a conditional distribution is not log-concave, the points used to set up the initial approximation to it must not be chosen adaptively, based on past iterations, as this could result in the wrong distribution being sampled <ref> (Gilks, Neal, Best, and Tan 1997) </ref>. The initial approximation must be chosen based only on prior knowledge (including any preliminary Markov chain sampling runs), and on the current values of the other variables. <p> Parameter tuning is more of a problem when ARMS (Gilks, Best, and Tan 1995) is used for distributions not known to be log concave | a poor choice of parameters may have worse effects, and adaptive tuning is not allowed <ref> (Gilks, Neal, Best, and Tan 1997) </ref>. Tuning is also a problem for simple Metropolis methods | proposing changes that are too small leads to an inefficient random walk, while proposing changes that are too large leads to frequent rejections.
Reference: <author> Gilks, W. R. and Wild, P. </author> <title> (1992) "Adaptive rejection sampling for Gibbs sampling", </title> <journal> Applied Statistics, </journal> <volume> vol. 41, </volume> <pages> pp. 337-348. </pages>
Reference-contexts: This method is used by the BUGS software <ref> (Thomas, Spiegelhalter, and Gilks 1992) </ref> to automatically generate Markov chain samplers from model specifications. The first step in applying ARS is to find points on each side of the mode of the conditional distribution (one of which can be the current point).
Reference: <author> Green, P. J. and Han, X. </author> <title> (1992) "Metropolis methods, Gaussian proposals and antithetic variables", </title> <editor> in P. Barone, et al. </editor> <title> (editors) Stochastic Models, Statistical Methods, and Algorithms in Image Analysis, </title> <booktitle> Lecture Notes in Statistics, </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Hastings, W. K. </author> <title> (1970) "Monte Carlo sampling methods using Markov chains and their applications", </title> <journal> Biometrika, </journal> <volume> vol. 57, </volume> <pages> pp. 97-109. </pages>
Reference: <author> Higdon, D. M. </author> <title> (1996) "Auxiliary variable methods for Markov chain Monte Carlo with applications", </title> <type> ISDS Discussion Paper 96-17, </type> <pages> 25 pages. </pages>
Reference: <author> Horowitz, A. M. </author> <title> (1991) "A generalized guided Monte Carlo algorithm", </title> <journal> Physics Letters B, </journal> <volume> vol. 268, </volume> <pages> pp. 247-252. </pages>
Reference: <author> Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. </author> <title> (1953) "Equation of state calculations by fast computing machines", </title> <journal> Journal of Chemical Physics, </journal> <volume> vol. 21, </volume> <pages> pp. 1087-1092. </pages>
Reference-contexts: 1 Introduction Markov chain methods such as Gibbs sampling (Gelfand and Smith 1990) and the Metropolis algorithm <ref> (Metropolis, et al 1953, Hastings 1970) </ref> can be used to sample from many of the complex, multivariate distributions encountered in statistics (see, for example, the book edited by Gilks, Richardson, and Spiegelhalter (1996)). <p> Unlike ARS, neither the current value of the variable being updated, nor any statistics collected from previous updates (eg, a typical scale) can be used. This hinders routine use of the method. Another general way of constructing a Markov chain sampler is to perform Metropolis updates <ref> (Metropolis, et al 1953, Hastings 1970) </ref>, either to all variables simultaneously, or more commonly, to each variable in turn. A Metropolis update starts with the random selection of a `candidate' state, drawn from a `proposal' distribution.
Reference: <author> Mira, A. and Tierney, L. </author> <title> (1997) "On the use of auxiliary variables in Markov chain Monte Carlo sampling", </title> <type> preprint. </type>
Reference: <author> Neal, R. M. </author> <title> (1993) Probabilistic Inference Using Markov Chain Monte Carlo Methods, </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto, </institution> <note> 140 pages. Obtainable from http://www.cs.utoronto.ca/~radford/. </note>
Reference: <author> Neal, R. M. </author> <title> (1994) "An improved acceptance procedure for the hybrid Monte Carlo algorithm", </title> <journal> Journal of Computational Physics, </journal> <volume> vol. 111, </volume> <pages> pp. 194-203. </pages>
Reference: <author> Neal, R. M. </author> <title> (1995) "Suppressing random walks in Markov chain Monte Carlo using ordered overrelaxation", </title> <type> Technical Report No. 9508, </type> <institution> Dept. of Statistics, University of Toronto, </institution> <address> 22 pages. </address>
Reference-contexts: This method is analysed and discussed by Barone and Frigessi (1990) and by Green and Han (1992), though these discussions fail in some respects to properly elucidate the true benefits and limitations of overrelaxation <ref> (Neal 1995) </ref>. The crucial ability of overrelaxation to (sometimes) suppress random walks is illustrated for a bivariate Gaussian distribution in Figure 7. 16 ^ L 2 with both ends outside the slice is found by stepping out from the current point, as was illustrated in Figure 1 (b).
Reference: <author> Neal, R. M. </author> <booktitle> (1996) Bayesian Learning for Neural Networks (Lecture Notes in Statistics No. 118), </booktitle> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: These sometimes have standard forms for which efficient sampling methods have been developed, but there are many models for which sampling from these conditional distributions requires the development of custom algorithms, or is infeasible in practice (eg, for multilayer perceptron networks <ref> (Neal 1996) </ref>). Note, however, that once methods for sampling from these conditional distributions have been found, no further tuning parameters need be set in order to produce the final Markov chain sampler. <p> This is important when sampling from a distribution with high dependencies between variables, as in such a situation, motion must proceed in small steps, and the difference in efficiency between diffusive and systematic exploration of the distribution can be very large. This is typical with neural network models <ref> (Neal 1996) </ref>, for example. Another way of exploring the differences between these methods is to see how well they work in various circumstances.
Reference: <author> Roberts, G. O. and Rosenthal, J. S. </author> <title> (1997) "Convergence of slice sampler Markov chains", </title> <type> Technical Report No. 9712, </type> <institution> Dept. of Statistics, University of Toronto, </institution> <address> 21 pages. </address>
Reference: <author> Swendsen, R. H. and Wang, J.-S. </author> <title> (1987) "Nonuniversal critical dynamics in Monte Carlo simulations", </title> <journal> Physical Review Letters, </journal> <volume> vol. 58, </volume> <pages> pp. 86-88. </pages>
Reference: <author> Thomas, A., Spiegelhalter, D. J., and Gilks, W. R. </author> <title> (1992) "BUGS: A program to perform Bayesian inference using Gibbs sampling", </title> <editor> in J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith (editors), </editor> <booktitle> Bayesian Statistics 4, </booktitle> <pages> pp. 837-842, </pages> <publisher> Oxford University Press. </publisher> <pages> 27 </pages>
Reference-contexts: This method is used by the BUGS software <ref> (Thomas, Spiegelhalter, and Gilks 1992) </ref> to automatically generate Markov chain samplers from model specifications. The first step in applying ARS is to find points on each side of the mode of the conditional distribution (one of which can be the current point).
References-found: 26

