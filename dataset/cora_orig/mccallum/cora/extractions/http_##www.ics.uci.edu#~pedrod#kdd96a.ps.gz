URL: http://www.ics.uci.edu/~pedrod/kdd96a.ps.gz
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: pedrod@ics.uci.edu  
Title: Linear-Time Rule Induction  
Author: Pedro Domingos 
Web: http://www.ics.uci.edu/~pedrod  
Address: Irvine, California 92717, U.S.A.  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: The recent emergence of data mining as a major application of machine learning has led to increased interest in fast rule induction algorithms. These are able to efficiently process large numbers of examples, under the constraint of still achieving good accuracy. If e is the number of examples, many rule learners have O(e 4 ) asymptotic time complexity in noisy domains, and C4.5RULES has been empirically observed to sometimes require O(e 3 ). Recent advances have brought this bound down to O(e log 2 e), while maintaining accuracy at the level of C4.5RULES's. In this paper we present CWS, a new algorithm with guaranteed O(e) complexity, and verify that it outperforms C4.5RULES and CN2 in time, accuracy and output size on two large datasets. For example, on NASA's space shuttle database, running time is reduced from over a month (for C4.5RULES) to a few hours, with a slight gain in accuracy. CWS is based on interleaving the induction of all the rules and evaluating performance globally instead of locally (i.e., it uses a "conquering without separating" strategy as opposed to a "separate and conquer" one). Its bias is appropriate to domains where the underlying concept is simple and the data is plentiful but noisy. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Brunk, C., and Pazzani, M. J. </author> <year> 1991. </year> <title> An investigation of noise-tolerant relational concept learning algorithms. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> 389-393. </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Noise also has a large negative impact on windowing, a technique often used to speed up C4.5/C4.5RULES for large datasets (Catlett 1991). In algorithms that use reduced error pruning as the simplification technique <ref> (Brunk & Pazzani 1991) </ref>, the presence of noise causes running time to become O (e 4 log e) (Cohen 1993).
Reference: <author> Catlett, J. </author> <year> 1991. </year> <title> Megainduction: Machine Learning on Very Large Databases. </title> <type> Ph.D. Dissertation, </type> <institution> Basser Department of Computer Science, University of Syd-ney, </institution> <address> Sydney, Australia. </address>
Reference-contexts: Outputting trees directly has the disadvantage that they are typically much larger and less comprehensible than the corresponding rule sets. Noise also has a large negative impact on windowing, a technique often used to speed up C4.5/C4.5RULES for large datasets <ref> (Catlett 1991) </ref>. In algorithms that use reduced error pruning as the simplification technique (Brunk & Pazzani 1991), the presence of noise causes running time to become O (e 4 log e) (Cohen 1993). <p> Cohen introduced a number of modifications to IREP, and verified empirically that RIPPERk, the resulting algorithm, is competitive with C4.5RULES in accuracy, while retaining an average running time similar to IREP's (Cohen 1995). Catlett <ref> (Catlett 1991) </ref> has done much work in making decision tree learners scale to large datasets. <p> CWS incorporates three methods for handling numeric values, selectable by the user. The default method discretizes each attribute into equal-sized intervals, and has no effect on the asymptotic time complexity of the algorithm. Discretization can also be performed using a method similar to Catlett's <ref> (Catlett 1991) </ref>, repeatedly choosing the partition that minimizes entropy until one of several termination conditions is met. This causes learning time to become O (e log e), but may improve accuracy in some situations. <p> Second, output simplicity is not the only factor in comprehensibility, which is ultimately subjective. However, it is an acceptable and frequently used approximation, especially when the systems being compared have similar output, 0.1 10 1000 100000 1e+07 Time (s) No. examples CWS ax cx^2 log (x) as here (see <ref> (Catlett 1991) </ref> for further discussion). <p> This database contains 43500 training examples from one shuttle flight, and 14500 testing examples from a different flight. Each example is described by nine numeric attributes obtained from sensor readings, and there are seven possible classes, corresponding to states of the shuttle's radiators <ref> (Catlett 1991) </ref>. The goal is to predict these states with very high accuracy (99-99.9%), using rules that can be taught to a human operator.
Reference: <author> Clark, P., and Boswell, R. </author> <year> 1991. </year> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the Sixth European Working Session on Learning, </booktitle> <pages> 151-163. </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: When a test example is covered by more than one rule, the class probability vectors of all the rules covering it are summed, and the class with the highest sum is chosen as the winner. This is similar to the approach followed in CN2 <ref> (Clark & Boswell 1991) </ref>, with the difference that probabilities are used instead of frequencies.
Reference: <author> Cohen, W. W. </author> <year> 1993. </year> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 988-994. </pages> <address> Chambery, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In algorithms that use reduced error pruning as the simplification technique (Brunk & Pazzani 1991), the presence of noise causes running time to become O (e 4 log e) <ref> (Cohen 1993) </ref>. Furnkranz and Widmer (1994) have proposed incremental reduced error pruning (IREP), an algorithm that prunes each rule immediately after it is grown, instead of waiting until the whole rule set has been induced.
Reference: <author> Cohen, W. W. </author> <year> 1995. </year> <title> Fast effective rule induction. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 115-123. </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Besides making the extraction of accurate rules more difficult, this can have a disastrous effect on the running time of rule learners. In C4.5RULES (Quinlan 1993), a system that induces rules via decision trees, noise can cause running time to become cubic in e, the number of examples <ref> (Cohen 1995) </ref>. When there are no numeric attributes, C4.5, the component that induces decision trees, has complexity O (ea 2 ), where a is the number of attributes (Utgoff 1989), but its running time in noisy domains is dwarfed by that of the conversion-to-rules phase (Cohen 1995). <p> e, the number of examples <ref> (Cohen 1995) </ref>. When there are no numeric attributes, C4.5, the component that induces decision trees, has complexity O (ea 2 ), where a is the number of attributes (Utgoff 1989), but its running time in noisy domains is dwarfed by that of the conversion-to-rules phase (Cohen 1995). Outputting trees directly has the disadvantage that they are typically much larger and less comprehensible than the corresponding rule sets. Noise also has a large negative impact on windowing, a technique often used to speed up C4.5/C4.5RULES for large datasets (Catlett 1991). <p> Assuming the final rule set is of constant size, IREP reduces running time to O (e log 2 e), but its accuracy is often lower than C4.5RULES's <ref> (Cohen 1995) </ref>. Cohen introduced a number of modifications to IREP, and verified empirically that RIPPERk, the resulting algorithm, is competitive with C4.5RULES in accuracy, while retaining an average running time similar to IREP's (Cohen 1995). <p> reduces running time to O (e log 2 e), but its accuracy is often lower than C4.5RULES's <ref> (Cohen 1995) </ref>. Cohen introduced a number of modifications to IREP, and verified empirically that RIPPERk, the resulting algorithm, is competitive with C4.5RULES in accuracy, while retaining an average running time similar to IREP's (Cohen 1995). Catlett (Catlett 1991) has done much work in making decision tree learners scale to large datasets. <p> Ideally, s should be independent of e, and this is the assumption made in (Furnkranz & Widmer 1994) and <ref> (Cohen 1995) </ref>, and verified below for CWS. CWS incorporates three methods for handling numeric values, selectable by the user. The default method discretizes each attribute into equal-sized intervals, and has no effect on the asymptotic time complexity of the algorithm. <p> Recall that on this type of scale the slope of a straight line corresponds to the exponent of the function being plotted. Canonical functions approximating each curve are also shown, as well as e log 2 e, the running time observed by Cohen <ref> (Cohen 1995) </ref> for RIPPERk and IREP. 2 CWS's running time grows linearly with the number of examples, as expected, while C4.5RULES's is O (e 2 log e).
Reference: <author> Dougherty, J.; Kohavi, R.; and Sahami, M. </author> <year> 1995. </year> <title> Supervised and unsupervised discretization of continuous features. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 194-202. </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This does not change the asymptotic time complexity, but may cause v to become very large. Each of the last two methods may improve accuracy in some situations, at the cost of additional running time. However, uniform discretiza-tion is surprisingly robust (see <ref> (Dougherty, Kohavi, & Sahami 1995) </ref>), and can result in higher accuracy by helping to avoid overfitting. Missing values are treated by letting them match any condition on the respective attribute, during both learning and classification.
Reference: <editor> Fayyad, U. M., and Uthurusamy, R., eds. </editor> <booktitle> 1995. Proceedings of the First International Conference on Knowledge Discovery and Data Mining. </booktitle> <address> Montreal, Canada: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: However, in many areas|including astronomy, molecular biology, finance, retail, health care, etc.|large databases are now the norm, and discovering patterns in them is a potentially very productive enterprise, in which interest is rapidly growing <ref> (Fayyad & Uthurusamy 1995) </ref>. Designing learning algorithms appropriate for such problems has thus become an important research problem. In these "data mining" applications, the main consideration is typically not to maximize accuracy, but to extract useful knowledge from a database.
Reference: <author> Furnkranz, J., and Widmer, G. </author> <year> 1994. </year> <title> Incremental reduced error pruning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 70-77. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For . . . ") either produces an improvement in accuracy or is the last one for that rule, and in a dataset with e examples at most e improvements in accuracy are possible. Ideally, s should be independent of e, and this is the assumption made in <ref> (Furnkranz & Widmer 1994) </ref> and (Cohen 1995), and verified below for CWS. CWS incorporates three methods for handling numeric values, selectable by the user. The default method discretizes each attribute into equal-sized intervals, and has no effect on the asymptotic time complexity of the algorithm.
Reference: <author> Holte, R. C.; Acker, L. E.; and Porter, B. W. </author> <year> 1989. </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> 813-818. </pages> <address> De-troit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is similar to the approach followed in CN2 (Clark & Boswell 1991), with the difference that probabilities are used instead of frequencies. In a system like CN2, this could give undue weight to rules covering very few examples (the "small disjuncts" <ref> (Holte, Acker, & Porter 1989) </ref>), but we have verified empirically that in CWS this problem is largely avoided. Examples not covered by any rules are assigned to the class with the most examples in the training set. CWS is outlined in pseudo-code in Table 1.
Reference: <author> Holte, R. C. </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 63-91. </pages>
Reference-contexts: However, this causes running time to become exponential in a, leading to a very high cost per example, and making application of those algorithms to large databases difficult. Holte's 1R algorithm <ref> (Holte 1993) </ref> outputs a single tree node, and is linear in a and O (e log e), but its accuracy is often much lower than C4.5's.
Reference: <editor> Michie, D.; Spiegelhalter, D. J.; and Taylor, C. C., eds. </editor> <year> 1994. </year> <title> Machine Learning, Neural and Statistical Classification. </title> <address> New York: </address> <publisher> Ellis Horwood. </publisher>
Reference-contexts: Introduction and Previous Work Very large datasets pose special problems for machine learning algorithms. A recent large-scale study found that most algorithms cannot handle such datasets in a reasonable time with a reasonable accuracy <ref> (Michie, Spiegelhalter, & Taylor 1994) </ref>. However, in many areas|including astronomy, molecular biology, finance, retail, health care, etc.|large databases are now the norm, and discovering patterns in them is a potentially very productive enterprise, in which interest is rapidly growing (Fayyad & Uthurusamy 1995).
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1995. </year> <title> UCI repository of machine learning databases. Machine-readable data repository, </title> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA. </address>
Reference-contexts: CN2's behavior is similar to C4.5RULES's in time and accuracy, but it produces larger rule sets. The relationship between the theoretical bound of O (eavcs) and CWS's actual average running time was investigated by running the system on 28 datasets from the UCI repository 3 <ref> (Murphy & Aha 1995) </ref>. Figure 2 2 The constants a, b and c were chosen so as to make the respective curves fit conveniently in the graph. 3 Audiology, annealing, breast cancer (Ljubljana), credit 0.1 10 1000 Time (s) eavcs CWS learning times. plots CPU time against the product eavcs.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A major problem in data mining is that the data is often very noisy. Besides making the extraction of accurate rules more difficult, this can have a disastrous effect on the running time of rule learners. In C4.5RULES <ref> (Quinlan 1993) </ref>, a system that induces rules via decision trees, noise can cause running time to become cubic in e, the number of examples (Cohen 1995).
Reference: <author> Segal, R., and Etzioni, O. </author> <year> 1994. </year> <title> Learning decision lists using homogeneous rules. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 619-625. </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Smyth, P.; Goodman, R. M.; and Higgins, C. </author> <year> 1990. </year> <title> A hybrid rule-based/Bayesian classifier. </title> <booktitle> In Proceedings of the Ninth European Conference on Artificial Intelligence, </booktitle> <pages> 610-615. </pages> <address> Stockholm, Sweden: </address> <publisher> Pitman. </publisher>
Reference: <author> Utgoff, P. E. </author> <year> 1989. </year> <title> Incremental induction of decision trees. </title> <booktitle> Machine Learning 4 </booktitle> <pages> 161-186. </pages>
Reference-contexts: When there are no numeric attributes, C4.5, the component that induces decision trees, has complexity O (ea 2 ), where a is the number of attributes <ref> (Utgoff 1989) </ref>, but its running time in noisy domains is dwarfed by that of the conversion-to-rules phase (Cohen 1995). Outputting trees directly has the disadvantage that they are typically much larger and less comprehensible than the corresponding rule sets.
Reference: <author> Weiss, S. M.; Galen, R. M.; and Tadepalli, P. V. </author> <year> 1987. </year> <title> Optimizing the predictive value of diagnostic decision rules. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> 521-526. </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
References-found: 17

