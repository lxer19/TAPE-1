URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-716.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr700.html
Root-URL: 
Title: Relieving Hot Spots on the World Wide Web  
Author: by Rina Panigrahy David R. Karger Arthur C. Smith 
Degree: B.Tech, Computer Science IIT, 1995 Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science at the  All rights reserved. Author  Certified by  Professor at Laboratory for Computer Science Thesis Supervisor Accepted by  Chairman, Department Committee on Graduate Students  
Date: June 1997  May 9, 1997  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1997.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Anawat Chankhunthod, Peter Danzig, Chuck Neerdaels, Michael Schwartz and Kurt Worrell. </author> <title> A Hierarchical Internet Object Cache. </title> <booktitle> In USENIX Proceedings, </booktitle> <year> 1996. </year>
Reference-contexts: A page request is served locally if it is cached in any of the caches. The disadvantage of this approach is that it leads to a lot of communication between the caches thus making it unsuitable for large networks. Chankhunthod et al. <ref> [1] </ref> developed the Harvest Cache, a more scalable approach using a tree of caches. The advantage of a cache tree is that a cache receives page requests only from its children (and siblings), ensuring that not too many requests arrive simultaneously. <p> A tool that we develop in this paper, consistent hashing, gives a way to implement such a distributed cache without requiring that the caches communicate all the time. We discuss this in chapter 4. 1.2.3 Harvest The Harvest system <ref> [1] </ref> was the first to implement hierarchical caching on a large scale. It consists of a number of caches arranged in a hierarchy, spread over the internet. A Harvest cache can be configured with an arbitrary number of parents and siblings. <p> If this set is small, then the caches themselves could be overwhelmed by page requests. If the set is large, then the server could be swamped by page requests from the very caches assigned to protect it. A natural extension <ref> [1] </ref> is to introduce several layers of proxy caches where a cache in one layer makes requests only to caches in the layer above it. <p> In Section 3.1 below, we define our protocol precisely. In Section 3.2, we analyze the protocol, bounding the load on any cache, the storage each cache uses, and the delay a browser experiences before getting the page. 22 3.1 Protocol Just like in Harvest <ref> [1] </ref> we will use a hierarchy of caches. However, instead of using the same hierarchy for all pages we will use a different one for each page. In each hierarchy the caches are arranged in random order, similar to the approach taken by Plaxton and Rajaraman [10]. <p> We will use two random functions r B and r I that map items and buckets to real numbers in the range <ref> [0; 1] </ref> respectively. Since it is hard to implement completely random functions in practice, we only demand that the functions r B and r I map points (log (N CV ))-way independently where N is the high probability parameter. <p> To show the main result, begin by fixing a bucket b. The portion of the unit interval for which b is right-responsible must consist of logC non-overlapping intervals in <ref> [0; 1] </ref>, each bounded on the left by one of b's points. Suppose we shrink all intervals by moving the right endpoints leftward, until the length of every interval is a multiple of = 4=(C log C).
Reference: [2] <author> Azer Bestavros. </author> <title> Speculative Data Dissemination and Service. </title>
Reference-contexts: Network topology is also taken into account in deciding where to push a page. Since network topology is hard to obtain, Gwertzman and Seltzer use the Geographical distances as an approximation to the network latencies between machines. Azer Bestavros <ref> [2] </ref> uses the same approach as push caching, and in addition uses a second technique that they call "speculative service".
Reference: [3] <author> Robert Devine. </author> <title> Design and Implementation of DDH: A Distributed Dynamic Hashing Algorithm. </title> <booktitle> In Proceedings of 4th International Conference on Foundations of Data Organizations and Algorithms, </booktitle> <year> 1993. </year>
Reference: [4] <author> M. J. Feeley, W. E. Morgan, F. P. Pighin, A. R. Karlin, H. M. Levy and C. A. Thekkath. </author> <title> Implementing Global Memory Management in a Workstation Cluster. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <year> 1995. </year>
Reference: [5] <author> Sally Floyd, Van Jacobson, Steen McCanne, Ching-Gung Liu and Lixia Zhang. </author> <title> A Reliable Multicast Framework for Light-weight Sessions and Application Level Framing, </title> <type> SIGCOMM' 95 </type>
Reference-contexts: A user's request for a page is directed to an arbitrary cache. If the page is stored there, it is returned to the user. Otherwise, the cache forwards the request to all other caches via a special protocol called "IP Multicast" <ref> [5] </ref> . Multicast is a protocol for the transmission of a packets to a subset of the hosts in a network. If the page is cached nowhere, the request is forwarded to the home site of the page. <p> Such an optimal broadcast would correspond to sending the page along a spanning tree. Such a tree would be very suitable for use in a Multicast protocol <ref> [5] </ref>. R. Ravi [15] has studied a special case of this problem in a synchronous framework where at each time step, any processor that has received the document is allowed to communicate it to at most one of its neighbors in the network.
Reference: [6] <author> Witold Litwin, Marie-Anne Neimat and Donovan A. Schneider. </author> <title> LH fl -A Scalable, Distributed Data Structure. </title> <journal> ACM Transactions on Database Systems, </journal> <month> Dec. </month> <year> 1996 </year>
Reference: [7] <author> Radhika Malpani, Jacob Lorch and David Berger. </author> <title> Making World Wide Web Caching Servers Cooperate. </title> <booktitle> In Proceedings of World Wide Web Conference, </booktitle> <year> 1996. </year>
Reference-contexts: The dilemma in this scheme is that there is more benefit if more users share the same cache, but then the cache itself is liable to get swamped. Malpani et al. <ref> [7] </ref> work around this problem by making a group of caches function as one. A page request is served locally if it is cached in any of the caches. <p> However, the size of the network containing the proxy should not be too large; otherwise the proxy itself could get swamped with requests. This limits the benefit a popular server can derive from the use of proxy caches. 1.2.2 Co-operative Caching Malpani et al. <ref> [7] </ref> tried to make a group of caches function as one. A user's request for a page is directed to an arbitrary cache. If the page is stored there, it is returned to the user.
Reference: [8] <author> M. Naor and A. Wool. </author> <title> The load, capacity, and availability of quorum systems. </title> <booktitle> In Proceedings of the 35th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 214-225, </pages> <month> November </month> <year> 1994. </year>
Reference: [9] <author> D. Peleg and A. Wool. </author> <title> The availability of quorum systems. </title> <booktitle> Information and Computation 123(2) </booktitle> <pages> 210-233, </pages> <year> 1995. </year>
Reference: [10] <author> Greg Plaxton and Rajmohan Rajaraman. </author> <title> Fast Fault-Tolerant Concurrent Access to Shared Objects. </title> <booktitle> In Proceedings of 37th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1996. </year> <month> 65 </month>
Reference-contexts: They conclude that a weak cache consistency protocol reduces network bandwidth consumption and server load more than either TTL or an invalidation protocol and can be tuned to return stale data less than 5% of the time. 1.3 Related Theoretical Work On the theoretical side, Plaxton and Rajaraman <ref> [10] </ref> introduced the idea of using randomization and hashing to balance load on a network. Their work however was done in the context of a synchronous network of nodes in a parallel machine. <p> R. Ravi [15] studies the problem of finding optimal broadcast trees in a graph, which could be useful for building multicast trees. A theoretical study of prefetching was done in [20]. 14 1.3.1 Randomization and Hashing Plaxton and Rajaraman <ref> [10] </ref> were the first to conduct a theoretical study of the problem of providing fast concurrent access to shared objects in a synchronous network of distributed computation. Their basic idea is to construct for each object a random tree of caches which can be computed using a global hash function. <p> A solution is to create a distinct tree of caches for each page. In fact a good way to achieve load balancing is to have for each page a tree of caches arranged in random order, which is the technique used by Plaxton/Rajaraman <ref> [10] </ref>. This ensures that no machine is near the root for many pages, thus providing good load balancing. We now describe our protocol. To simplify the presentation, we start with a simple caching protocol that would work well in a simpler world. <p> However, instead of using the same hierarchy for all pages we will use a different one for each page. In each hierarchy the caches are arranged in random order, similar to the approach taken by Plaxton and Rajaraman <ref> [10] </ref>. We associate with each page a rooted d-ary tree, called an abstract tree that will represent the tree of caches for that page. We use the term nodes only in reference to the nodes of these abstract trees.
Reference: [11] <author> M. O. Rabin. </author> <title> Efficient dispersal of Information for Security, Load Balancing, and Fault Tolerance. </title> <journal> Journal of the ACM 36 </journal> <pages> 335-348, </pages> <year> 1989. </year>
Reference-contexts: Under our protocol, no preemptive caching of pages is done. Thus, if a server goes down, all pages that it has not distributed become inaccessible to any algorithm. This problem can be eliminated using standard techniques, such as Rabin's Information Dispersal Algorithm <ref> [11] </ref>. So we ignore server faults. In this chapter, we analyze a minor modification of the protocol and show that it ensures that any page request is satisfied with high probability.
Reference: [12] <author> Jeanette Schmidt, Alan Siegel and Aravind Srinivasan. </author> <title> Chernoff-Hoeffding Bounds for Applications with Limited Independence. </title> <booktitle> In Proc. 4th ACS-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1993. </year>
Reference-contexts: Note that the monotonicity is immediate. When a new bucket is added, the only items that move are those that are now closest to that bucket's associated points. No items move between old buckets The proof of theorem 4.4.1 requires the following technical lemma from <ref> [12] </ref> that gives upper bounds on a sum of Bernoulli variables when these variables are only k-way independent.
Reference: [13] <author> Venkata N. Padmanabhan and Jeffrey C. Mogul. </author> <title> Using Predictive Prefetching to Improve World Wide Web Latency. </title> <booktitle> In ACM SIGCOMM'95. </booktitle>
Reference-contexts: The retrieval latency has not actually been reduced; it has just been overlapped with the time the user spends reading, thereby decreasing the access time. Padmanabhan and Mogul <ref> [13] </ref> propose a predictive prefetching scheme for the World Wide Web in which the servers tell the clients which files are likely to be requested next by the user, and the clients decide whether or not to prefetch the files based on local consideratons (such as the contents of the local
Reference: [14] <author> Ari Luitonen and Kevin Altis. </author> <title> World-wide web proxies. </title> <booktitle> In Computer Networks and ISDN systems. First International Conference on the World-Wide Web, </booktitle> <publisher> Elsevier Science BV, </publisher> <year> 1994. </year> <note> available from `http://www.cern.ch/PapersWWW94/luotonen.ps' </note>
Reference-contexts: Most use some kind of replication strategy to store copies of hot pages throughout the Internet; this spreads the work of serving a hot page across several servers. In one approach, already in wide use, several clients share a proxy cache. Proxy caches <ref> [14] </ref> have long been in use to reduce web traffic in a network. It tries to satisfy requests with a cached copy; failing this, it forwards the request to the home server. <p> The idea is to send to the client along with the requested page a set of pages it is likely to request in future. 10 1.2.1 World Wide Web proxies A proxy is a special server that does caching for a medium sized network. <ref> [14] </ref>. Instead of going directly to the home server for a page, the clients in the network first query the proxy.
Reference: [15] <author> R. Ravi. </author> <title> Approximating the minimum broadcast time. </title> <booktitle> In FOCS'94. </booktitle>
Reference-contexts: They assume the existence of special priority messages that reach a node even though it may be swamped with other low priority messages which is clearly not the case in the Web. R. Ravi <ref> [15] </ref> studies the problem of finding optimal broadcast trees in a graph, which could be useful for building multicast trees. <p> Such an optimal broadcast would correspond to sending the page along a spanning tree. Such a tree would be very suitable for use in a Multicast protocol [5]. R. Ravi <ref> [15] </ref> has studied a special case of this problem in a synchronous framework where at each time step, any processor that has received the document is allowed to communicate it to at most one of its neighbors in the network.
Reference: [16] <author> M. Grigni and D. Peleg. </author> <title> Tight bounds on minimun broadcast networks. </title> <journal> In SIAM Journal on Discrete Math., </journal> <month> May </month> <year> 1991, </year> <pages> pp. 207-222. </pages>
Reference-contexts: This special case 15 of the problem is known to be NP-complete <ref> [16] </ref>, even in 3-regular planar graphs. R. Ravi gives an O (log 2 n= log log n)-approximation algorithm for the minumum broadcast time problem on an n-node graph. 1.3.3 Prefetching Algorithms Prefetching can be studied as a learning problem that involves predicting the page accesses of the user.
Reference: [17] <author> James S. Gwetzman and Margo Seltzer. </author> <title> The Case for Geographical Push-Caching. </title> <type> Personal Communication. </type>
Reference-contexts: They argue that the root level caches, located on the internet back-bone, are capable of handling the maximum request rate allowed by the bandwidth of the back-bone. 1.2.4 Push Caching Push Caching was introduced by Gwertzman and Seltzer <ref> [17] </ref> at Harvard. The essential idea is to replicate a popular document so that the number of replicas is proportional to the popularity of the document. They envision a network infrastructure with thousands of push-cache servers onto which files may be pushed.
Reference: [18] <author> M. Palmer and S. Zdonik, </author> <title> Fido: A Cache that Learns to Fetch. </title> <booktitle> InProceedings of the 1991 International Conference on Very Large Databases, </booktitle> <month> September </month> <year> 1991. </year>
Reference: [19] <author> K. Salem. </author> <title> Adaptive Prefetching for Disk Buffers. </title> <type> CESDIS, </type> <institution> Goddard Space Flight Center, TR-91-64, </institution> <month> January </month> <year> 1991. </year>
Reference: [20] <author> Jeffrey Scott Vitter and P. Krishnan. </author> <title> Optimal Prefetching via Data Compression. </title> <type> FOCS 91. </type>
Reference-contexts: R. Ravi [15] studies the problem of finding optimal broadcast trees in a graph, which could be useful for building multicast trees. A theoretical study of prefetching was done in <ref> [20] </ref>. 14 1.3.1 Randomization and Hashing Plaxton and Rajaraman [10] were the first to conduct a theoretical study of the problem of providing fast concurrent access to shared objects in a synchronous network of distributed computation. <p> R. Ravi gives an O (log 2 n= log log n)-approximation algorithm for the minumum broadcast time problem on an n-node graph. 1.3.3 Prefetching Algorithms Prefetching can be studied as a learning problem that involves predicting the page accesses of the user. In <ref> [20] </ref> Vitter and Krishnan give the first provable theoretical bounds on prefetching performance. Their approach is to use optimal data compression methods to do optimal prefetching.
Reference: [21] <author> Stephen E. Deering and David R. Cheriton. </author> <title> Multicast Routing in Datagram Internetworks and Extended LANs. </title> <journal> In ACM Transactions on Computer Systems, </journal> <month> May </month> <year> 1990. </year>
Reference: [22] <author> James Gwetzman and Margo Seltzer. </author> <title> World-Wide Web Cache Consistency. </title> <type> Personal Communication. 66 </type>
Reference-contexts: The value of caching is greatly reduced if cached copies are not updated when the original data change. Cache consistency mechanisms ensure that cached copies of data are eventually updated to reflect changes to the original data. Gwertzman and Seltzer <ref> [22] </ref> studied different techniques for maintaining cache consistency in the context of the World Wide Web. There are several cache consistency mechanisms currently in use on the Internet: time-to-live fields, client polling, and invalidation protocols. A time-to-live field is essentially an estimate of the time when an object expires.
References-found: 22

