URL: http://polaris.cs.uiuc.edu/reports/1173.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Machine-Independent Evaluation of Parallelizing Compilers  
Author: Paul M. Petersen and David A. Padua 
Abstract: A method is presented for measuring the degree of success of a compiler at extracting implicit parallelism. The outcome of applying this method to evaluate a state-of-the-art par-allelizer, KAP/Concurrent, using the Perfect Benchmarks and a few linear-algebra routines indicates that there is much room for improvement in the current generation of parallelizing compilers.
Abstract-found: 1
Intro-found: 1
Reference: [All85] <institution> Alliant Computer Systems Corp., Acton, Massachusetts. FX/Series architecture manual, </institution> <year> 1985. </year>
Reference-contexts: However, complete machine-independence in this regard is not always possible because some parallelizers may restrict the parallelism they exploit because of some architectural characteristics of the target machine. For example, parallelizers for Alliant FX/80 <ref> [All85] </ref> attempt to exploit only one level of loop parallelism. The upper bound on the optimal parallel execution time that we use for our evaluation is the maximum length over all flow-dependence chains generated by an execution of the program.
Reference: [Bel66] <author> L. A. Belady. </author> <title> A study of replacement algorithms for a virtual-storage computer. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: This approach is similar to that used to evaluate page replacement strategies by comparing their behavior to an optimal strategy such as OPT <ref> [Bel66] </ref>. One difference is that it is not possible for us to obtain the optimal execution time because parallelizers can (and often do) apply algorithm substitution. For example, parallelizing compilers may eliminate induction variables and replace recurrence solvers and matrix-multiplication loops with invocations to library subroutine.
Reference: [CDL88] <author> David Callahan, Jack Dongarra, and David Levine. </author> <title> Vectorizing compilers: A test suite and results. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <pages> pages 98-105, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: The second approach uses a collection of synthetic DO loops to evaluate the parallelizer by counting how many, and observing which, loops it parallelizes <ref> [CDL88] </ref>. Both of these approaches produce useful but incomplete information. We believe other complementary approaches are necessary to improve our understanding of the performance of parallelizers.
Reference: [CFR + 88] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <type> Technical Report CS-88-16, </type> <institution> IBM T.J. Watson Research Center, </institution> <year> 1988. </year>
Reference-contexts: The reason for including control dependence information is to preclude speculative parallelism. Future experiments may relax this constraint to study specifically the effects of speculative execution. The control dependence algorithm described in the calculation of the SSA <ref> [CFR + 88] </ref> form and the dominators algorithm from Lengauer and Tarjan [LT79] were used as the basis of the control dependence instrumentation for this experiment.
Reference: [Che89] <author> Ding-Kai Chen. </author> <title> MAXPAR: An execution driven simulator for studying parallel systems. </title> <type> CSRD Report no. 917, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomp. Res.&Dev., </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: This value is computed by instrumenting the program to keep track of all memory references as discussed below. This instrumentation strategy was developed by Kumar [Kum88] to measure the implicit parallelism present in a program. This technique was later used by Yew and Chen <ref> [Che89, CSY90] </ref> to measure other important program characteristics. Several other authors have used similar approaches to measure inherent parallelism [NF84, KBG90, Fu90]. <p> The loop-level parallelism is used because today's parallelizers almost exclusively deal with the extraction of parallelism at the loop-level. The two types of upper bounds (operation and loop level) were also used by Yew and Chen for the MaxPar project <ref> [Che89, CSY90] </ref>. However, they allow the concurrent execution of iterations from disjoint loop nests. Our approach is more restricted. At the loop-level, we only allow the statements in different iterations of the same loop nest to execute concurrently.
Reference: [CSY90] <author> Ding-Kai Chen, Hong-Men Su, and Pen-Chung Yew. </author> <title> The impact of syncronization and granularity on parallel systems. </title> <type> CSRD Report no. 942, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomp. Res.&Dev., </institution> <month> May </month> <year> 1990. </year> <month> 19 </month>
Reference-contexts: This value is computed by instrumenting the program to keep track of all memory references as discussed below. This instrumentation strategy was developed by Kumar [Kum88] to measure the implicit parallelism present in a program. This technique was later used by Yew and Chen <ref> [Che89, CSY90] </ref> to measure other important program characteristics. Several other authors have used similar approaches to measure inherent parallelism [NF84, KBG90, Fu90]. <p> The loop-level parallelism is used because today's parallelizers almost exclusively deal with the extraction of parallelism at the loop-level. The two types of upper bounds (operation and loop level) were also used by Yew and Chen for the MaxPar project <ref> [Che89, CSY90] </ref>. However, they allow the concurrent execution of iterations from disjoint loop nests. Our approach is more restricted. At the loop-level, we only allow the statements in different iterations of the same loop nest to execute concurrently.
Reference: [Cyt86] <author> Ron Cytron. </author> <title> DOACROSS: Beyond Vectorization for Multiprocessors. </title> <booktitle> In Proc. 1986 International Conf. on Parallel Processing, </booktitle> <pages> pages 836-844, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: The ordering constraint of the ordered section implies that the iterations will be executed in sequential order with statements S2 and S3 overlapped in successive iterations. The standard name used for this type of synchronized parallelism is a doacross loop <ref> [PKL80, Cyt86] </ref>. The doacross model of parallelism is more powerful than the simple ORDERED SECTION implementation because doacross allows any type of synchronization in a parallel loop as long as it goes from lower to higher iterations. In the ORDERED SECTION implementation, synchronization is always between consecutive iterations.
Reference: [Fu90] <author> Chuigang Fu. </author> <title> Evaluating the effectiveness of fortran vectorizers by measuring total parallelism. </title> <type> CSRD Report no. 1033, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomp. Res.&Dev., </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: This instrumentation strategy was developed by Kumar [Kum88] to measure the implicit parallelism present in a program. This technique was later used by Yew and Chen [Che89, CSY90] to measure other important program characteristics. Several other authors have used similar approaches to measure inherent parallelism <ref> [NF84, KBG90, Fu90] </ref>.
Reference: [KBC + 74] <author> D. Kuck, P. Budnik, S-C. Chen, Jr. E. Davis, J. Han, P. Kraska, D. Lawrie, Y. Mu-raoka, R. Strebendt, and R. Towle. </author> <title> Measurements of Parallelism in Ordinary FORTRAN Programs. </title> <journal> Computer, </journal> <volume> 7(1) </volume> <pages> 37-46, </pages> <month> Jan., </month> <year> 1974. </year>
Reference-contexts: The execution times in equation (1) are measured either by executing the program on a parallel computer or by static estimation <ref> [KBC + 74, KSC + 84] </ref>. The second approach uses a collection of synthetic DO loops to evaluate the parallelizer by counting how many, and observing which, loops it parallelizes [CDL88]. Both of these approaches produce useful but incomplete information.
Reference: [KBG90] <author> Andrew W. Kwan, Lubomir Bic, and Daniel D. Gajski. </author> <title> Improving parallel program performance using critical path analysis. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, chapter 18, </booktitle> <pages> pages 358-373. </pages> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: This instrumentation strategy was developed by Kumar [Kum88] to measure the implicit parallelism present in a program. This technique was later used by Yew and Chen [Che89, CSY90] to measure other important program characteristics. Several other authors have used similar approaches to measure inherent parallelism <ref> [NF84, KBG90, Fu90] </ref>.
Reference: [KKP + 81] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence Graphs and Compiler Optimizations. </title> <booktitle> Proceedings of the 8th ACM Symp. on Principles of Programming Languages (POPL), </booktitle> <pages> pages 207-218, </pages> <month> Jan., </month> <year> 1981. </year>
Reference-contexts: 1 Introduction Compilers that translate sequential programs into semantically equivalent parallel or vector forms, known as parallelizing compilers <ref> [KKP + 81, PW86] </ref>, are an integral part of most supercomputers and have a significant influence on their performance.
Reference: [KSC + 84] <author> David J. Kuck, Ahmed H. Sameh, Ron Cytron, Alexander V. Veidenbaum, Con-stantine D. Polychronopoulos, Gyungho Lee, Tim McDaniel, Bruce R. Leasure, Carol Beckman, James R. B. Davies, and Clyde P. Kruskal. </author> <title> The Effects of Program Restructuring, Algorithm Change, and Architecture Choice on Program Performance. </title> <booktitle> Proceedings of 1984 International Conf. on Parallel Processing, </booktitle> <pages> pages 129-138, </pages> <address> Aug.21-24, </address> <year> 1984. </year>
Reference-contexts: The execution times in equation (1) are measured either by executing the program on a parallel computer or by static estimation <ref> [KBC + 74, KSC + 84] </ref>. The second approach uses a collection of synthetic DO loops to evaluate the parallelizer by counting how many, and observing which, loops it parallelizes [CDL88]. Both of these approaches produce useful but incomplete information.
Reference: [Kuc90] <author> Kuck & Associates, Inc. </author> <title> KAP/Concurrent User's Guide. KAI, </title> <address> Champaign, IL 61820, 1 edition, </address> <month> May </month> <year> 1990. </year> <month> #9005010. </month>
Reference-contexts: The information provided by our analysis should be very useful for improving the parallelizer and for studying the effectiveness of parallelization. In section 4 we use this approach to evaluate the effectiveness of a commercially available parallelizer, KAP/Concurrent <ref> [Kuc90] </ref>, on a few linear algebra kernel routines from Numerical Recipes [PFTV88] and a selection of Perfect Benchmarks [Per89] programs. 2 Overview of the evaluation method In the method described here, the translation quality of a parallelizer is evaluated by comparing the execution time of the parallelized program with an upper <p> it is efficient enough to allow us to run several programs with relatively large execution times that are quite costly to process through MaxPar because of resource limitations. 1 In addition to the instrumentation of sequential programs we added the ability to instrument explicitly parallel programs as generated by KAP/Concurrent <ref> [Kuc90] </ref>. Furthermore, the existence of two instrumentation systems, MaxPar and ours, has the additional advantage that the replication can aid in the debugging of both systems because we can compare the results and make corrections when discrepancies arise.
Reference: [Kum88] <author> Manoj Kumar. </author> <title> Measuring parallelism in computation-intensive science/engineering applications. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(9) </volume> <pages> 5-40, </pages> <year> 1988. </year>
Reference-contexts: This value is computed by instrumenting the program to keep track of all memory references as discussed below. This instrumentation strategy was developed by Kumar <ref> [Kum88] </ref> to measure the implicit parallelism present in a program. This technique was later used by Yew and Chen [Che89, CSY90] to measure other important program characteristics. Several other authors have used similar approaches to measure inherent parallelism [NF84, KBG90, Fu90]. <p> This restriction should allow a better correlation between the automatic parallelization tools and our simulations. 3 3 Implementation The approach to program instrumentation that is used in this work is described below <ref> [Kum88] </ref>. <p> = N$N M$S2 = N$N N$N = N$S1 N$N = MAX (N$N, M$S2) N$N = N$N + 3 : : : : : : ENDDO N$N = M$S1 3.4 Shadow variables and operation-level implicit parallelism To compute the intrinsic operation-level parallelism, we follow an approach similar to that described in <ref> [Kum88] </ref>; that is, we associate a shadow variable with each of the program's variables and array elements. The purpose of the shadow variable is to make the data dependences 9 explicit during the program's execution.
Reference: [Lev89] <author> Gary Mark Levin. </author> <title> An Introduction to ISETL Version 2.0. </title> <institution> Clarkson University, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: The implementation of the instrumentation tool described below was facilitated by the use of the Delta [Pad89] program manipulation system whose goal is to serve as a rapid prototyping facility for parallelizing compiler development and program transformation tools. Delta consists of a collection of SETL <ref> [Lev89, Sny90] </ref> functions that perform the basic operations needed to create a parallelizing compiler or to instrument Fortran source code. original source code is transformed through KAP to obtain the reference program.
Reference: [LT79] <author> Thomas Lengauer and Robert Endre Tarjan. </author> <title> A fast algorithm for finding dominators in a flowgraph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 121-141, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: The reason for including control dependence information is to preclude speculative parallelism. Future experiments may relax this constraint to study specifically the effects of speculative execution. The control dependence algorithm described in the calculation of the SSA [CFR + 88] form and the dominators algorithm from Lengauer and Tarjan <ref> [LT79] </ref> were used as the basis of the control dependence instrumentation for this experiment. To take control dependences into account, all statements that are control dependent on a given conditional statement, for example Sn, must wait until Sn completes execution.
Reference: [NF84] <author> Alexandru Nicolau and Joseph A. Fisher. </author> <title> Measuring the Parallelism Available for Very Long Instruction Word Architectures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-33(11), </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: This instrumentation strategy was developed by Kumar [Kum88] to measure the implicit parallelism present in a program. This technique was later used by Yew and Chen [Che89, CSY90] to measure other important program characteristics. Several other authors have used similar approaches to measure inherent parallelism <ref> [NF84, KBG90, Fu90] </ref>.
Reference: [Pad89] <author> David A. Padua. </author> <title> The Delta Program Manipulation system | Preliminary design. </title> <type> CSRD Report no. 808, </type> <institution> University of Illinois at Urbana-Champaign, Center for Su-percomp. Res.&Dev., </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: The implementation of the instrumentation tool described below was facilitated by the use of the Delta <ref> [Pad89] </ref> program manipulation system whose goal is to serve as a rapid prototyping facility for parallelizing compiler development and program transformation tools.
Reference: [Par90] <institution> Parallel Computing Forum. PCF Fortran, </institution> <month> April </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: This is the parallelizer used for the experiments in section 4. KAP/Concurrent is a Fortran source-to-source preprocessor which discovers loop-level parallelism. The result of this preprocessor is a program written with Concurrent's parallel programming directives (similar to the PCF Fortran <ref> [Par90] </ref> parallelism specification) to represent the parallelism.
Reference: [Per89] <editor> Perfect Club, et al. </editor> <title> The perfect club benchmarks: effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <pages> pages 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: In section 4 we use this approach to evaluate the effectiveness of a commercially available parallelizer, KAP/Concurrent [Kuc90], on a few linear algebra kernel routines from Numerical Recipes [PFTV88] and a selection of Perfect Benchmarks <ref> [Per89] </ref> programs. 2 Overview of the evaluation method In the method described here, the translation quality of a parallelizer is evaluated by comparing the execution time of the parallelized program with an upper bound of the optimal parallel execution time. <p> S$S1 = MAX (: : :, C$B+1) S1: IF (B .GT. 0) THEN S2: A = B + C ENDIF 12 4 Experimental evaluation of KAP/Concurrent The experiments were run using two collections of programs. The first collection is a subset of the Perfect Benchmarks <ref> [Per89] </ref> suite (listed in Table 1) and the second collection is a set of subroutines from Numerical Recipes [PFTV88] (listed in Table 4).
Reference: [PFTV88] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. </author> <title> Numerical Recipes. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: The information provided by our analysis should be very useful for improving the parallelizer and for studying the effectiveness of parallelization. In section 4 we use this approach to evaluate the effectiveness of a commercially available parallelizer, KAP/Concurrent [Kuc90], on a few linear algebra kernel routines from Numerical Recipes <ref> [PFTV88] </ref> and a selection of Perfect Benchmarks [Per89] programs. 2 Overview of the evaluation method In the method described here, the translation quality of a parallelizer is evaluated by comparing the execution time of the parallelized program with an upper bound of the optimal parallel execution time. <p> The first collection is a subset of the Perfect Benchmarks [Per89] suite (listed in Table 1) and the second collection is a set of subroutines from Numerical Recipes <ref> [PFTV88] </ref> (listed in Table 4). Because of the minimal overhead of our approach, it was possible to use the complete data sets as released for the Perfect Benchmarks rather than a subset of the data.
Reference: [PKL80] <author> David A. Padua, David J. Kuck, and Duncan H. Lawrie. </author> <title> High-Speed Multiprocessors and Compilation Techniques. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(9):763-776, </volume> <month> September </month> <year> 1980. </year>
Reference-contexts: The ordering constraint of the ordered section implies that the iterations will be executed in sequential order with statements S2 and S3 overlapped in successive iterations. The standard name used for this type of synchronized parallelism is a doacross loop <ref> [PKL80, Cyt86] </ref>. The doacross model of parallelism is more powerful than the simple ORDERED SECTION implementation because doacross allows any type of synchronization in a parallel loop as long as it goes from lower to higher iterations. In the ORDERED SECTION implementation, synchronization is always between consecutive iterations.
Reference: [PW86] <author> David A. Padua and Michael J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-101, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: 1 Introduction Compilers that translate sequential programs into semantically equivalent parallel or vector forms, known as parallelizing compilers <ref> [KKP + 81, PW86] </ref>, are an integral part of most supercomputers and have a significant influence on their performance.
Reference: [Sny90] <author> W. Kirk Snyder. </author> <title> The SETL2 Programming Language. </title> <institution> Courant Institute of Mathematical Sciences, </institution> <month> May </month> <year> 1990. </year> <month> 21 </month>
Reference-contexts: The implementation of the instrumentation tool described below was facilitated by the use of the Delta [Pad89] program manipulation system whose goal is to serve as a rapid prototyping facility for parallelizing compiler development and program transformation tools. Delta consists of a collection of SETL <ref> [Lev89, Sny90] </ref> functions that perform the basic operations needed to create a parallelizing compiler or to instrument Fortran source code. original source code is transformed through KAP to obtain the reference program.
References-found: 24

