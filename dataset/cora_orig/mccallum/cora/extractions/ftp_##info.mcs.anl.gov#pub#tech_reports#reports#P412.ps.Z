URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P412.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: Parallel Tridiagonalization through Two-Step Band Reduction  
Author: Christian Bischof Bruno Lang Xiaobai Sun 
Date: May 1994  
Note: Argonne Preprint ANL-MCS-P412-0194 to appear in Proc. of the 1994 Scalable High-Performance Computing Conference, Knoxville,  
Address: 9700 S. Cass Ave. Gauss-Strae 20 Argonne, IL 60439-4844 42097 Wuppertal U.S.A. Germany  
Affiliation: Mathematics Computer Science Div. FB Mathematik Argonne National Laboratory Universitat Wuppertal  
Abstract: We present a two-step variant of the "successive band reduction" paradigm for the tridiagonalization of symmetric matrices. Here we reduce a full matrix first to narrow-banded form and then to tridiagonal form. The first step allows easy exploitation of block orthogonal transformations. In the second step, we employ a new blocked version of a banded matrix tridiagonal-ization algorithm by Lang. In particular, we are able to express the update of the orthogonal transformation matrix in terms of block transformations. This expression leads to an algorithm that is almost entirely based on BLAS-3 kernels and has greatly improved data movement and communication characteristics. We also present some performance results on the Intel Touchstone DELTA and the IBM SP1. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Dem-mel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <title> LAPACK User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: The orthogonal transformation matrix Q is stored in the same 2-D block torus wrap mapping as before. We also mention that, unlike the original code used in [13], we employ the same packed storage scheme as in LAPACK <ref> [1] </ref>, which allows us to formulate the packed storage block algorithm more succinctly, and naturally employs BLAS-2 kernels.
Reference: [2] <author> E. Anderson, A. Benzoni, J. Dongarra, S. Moul-ton, B. Tourancheau, and R. van de Geijn, </author> <title> LAPACK for distributed memory architectures: Progress report, in Parallel Processing for Scientific Computing, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: This approach reduces data accesses by a factor of nb and has been shown to perform efficiently on parallel machines <ref> [3, 2] </ref> and in out-of-core factorization approaches [10]. We have implemented this algorithm using a two-dimensional block torus wrapping. The block size nb of the block torus wrapping is also the block size used for orthogonal transformations, which are expressed with the WY representation [5].
Reference: [3] <author> C. H. Bischof, </author> <title> A pipelined block QR decomposition algorithm., in Parallel Processing for Scientific Computing, </title> <editor> G. Rodrigue, ed., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1989, </year> <pages> pp. 3-7. </pages>
Reference-contexts: This approach reduces data accesses by a factor of nb and has been shown to perform efficiently on parallel machines <ref> [3, 2] </ref> and in out-of-core factorization approaches [10]. We have implemented this algorithm using a two-dimensional block torus wrapping. The block size nb of the block torus wrapping is also the block size used for orthogonal transformations, which are expressed with the WY representation [5].
Reference: [4] <author> C. H. Bischof and X. Sun, </author> <title> A framework for band reduction and tridiagonalization of symmetric matrices, </title> <type> Preprint MCS-P298-0392, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Department of Energy, under Contract W-31-109-Eng-38. proaches also underlie the parallel implementations described, for example, in [12] and [6]. The approach described in this paper, on the other hand, is a two-step instantiation of the "successive band reduction" framework suggested by Bischof and Sun <ref> [4] </ref>. We first reduce the dense matrix to bandwidth nb using block orthogonal transformations employing the so-called WY representation [5]; this is described in Section 2. The remaining narrow-banded matrix is then reduced to tridiagonal form using a new variant of an algorithm originally suggested in [13].
Reference: [5] <author> C. H. Bischof and C. F. Van Loan, </author> <title> The WY representation for products of Householder matrices, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8 (1987), </volume> <pages> pp. </pages> <month> s2-s13. </month>
Reference-contexts: The approach described in this paper, on the other hand, is a two-step instantiation of the "successive band reduction" framework suggested by Bischof and Sun [4]. We first reduce the dense matrix to bandwidth nb using block orthogonal transformations employing the so-called WY representation <ref> [5] </ref>; this is described in Section 2. The remaining narrow-banded matrix is then reduced to tridiagonal form using a new variant of an algorithm originally suggested in [13]. In particular, we have devised a way of blocking the orthogonal transformations. The new algorithm is described in Section 3. <p> Reduction of a Full Matrix to Nar row Banded Form Block orthogonal transformations express the product H 1 H k of k Householder transformations H i = I u i u T i ; u i 2 R n as a rank-k update of the form I W Y T <ref> [5] </ref> or I W SW T [14], where W; Y 2 R nfik and S 2 R kfik . As a result, the application of these transformations now involves BLAS-3 operations, such as matrix-matrix multiplication, which perform efficiently on contemporary high-performance architectures [7]. <p> We have implemented this algorithm using a two-dimensional block torus wrapping. The block size nb of the block torus wrapping is also the block size used for orthogonal transformations, which are expressed with the WY representation <ref> [5] </ref>. To develop a portable code, and to allow a maintainable implementation, we chose to base our implementation on the Chameleon parallel programming tools [11]. The performance of this code is promising. <p> For example, if we assume that we reduced the first three columns, and saved the associated transformations, as shown in Figure 2, we can accumulate transformations (1; k); (2; k) and (3; k) into a block transformation <ref> [5, 14] </ref>, and apply them "bottom-up" as indicated by the numbers in circles. The bottom-up approach is mandated by the successive-sweep chasing requirement, and the antidiagonal gathering of transformations then satisfies the column order requirement.
Reference: [6] <author> J. Dongarra and R. van de Geijn, </author> <title> Reduction to condensed form for the eigenvalue problem on distributed-memory architectures, </title> <booktitle> Parallel Computing, 18 (1992), </booktitle> <pages> pp. 973-982. </pages>
Reference-contexts: These two ap This work was supported by the Applied and Computational Mathematics Program, Advanced Research Projects Agency, under contract DM28E04120, and by the Office of Scientific Computing, U.S. Department of Energy, under Contract W-31-109-Eng-38. proaches also underlie the parallel implementations described, for example, in [12] and <ref> [6] </ref>. The approach described in this paper, on the other hand, is a two-step instantiation of the "successive band reduction" framework suggested by Bischof and Sun [4]. <p> As a result, any reduction approach that directly goes to tridiagonal form must access all of the remaining data. Therefore, even block approaches to implementing tridiag-onalization <ref> [8, 6] </ref> can at most halve the data transfer cost compared with the traditional approach.
Reference: [7] <author> J. J. Dongarra, I. S. Duff, D. C. Sorensen, and H. A. V. der Vorst, </author> <title> Solving Linear Systems on Vector and Shared-Memory Computers, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: As a result, the application of these transformations now involves BLAS-3 operations, such as matrix-matrix multiplication, which perform efficiently on contemporary high-performance architectures <ref> [7] </ref>. What complicates the formulation of block algorithms for tridiagonalization is the fact that, if one wishes to reduce a matrix directly to tridiagonal form, the update of the next column must involve the full remainder of the matrix to be reduced, not just a part of it.
Reference: [8] <author> J. J. Dongarra, S. J. Hammarling, and D. C. Sorensen, </author> <title> Block reduction of matrices to condensed form for eigenvalue computations, </title> <type> Tech. Rep. </type> <institution> ANL/MCS-TM-99, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Reduction to tridiagonal form is a major step in eigenvalue computations for symmetric matrices. If the matrix is full, the conventional Householder tridi-agonalization approach [9, p. 276] or a block variant thereof <ref> [8] </ref> is the method of choice. These two ap This work was supported by the Applied and Computational Mathematics Program, Advanced Research Projects Agency, under contract DM28E04120, and by the Office of Scientific Computing, U.S. <p> As a result, any reduction approach that directly goes to tridiagonal form must access all of the remaining data. Therefore, even block approaches to implementing tridiag-onalization <ref> [8, 6] </ref> can at most halve the data transfer cost compared with the traditional approach.
Reference: [9] <author> G. H. Golub and C. F. V. Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1983. </year>
Reference-contexts: 1 Introduction Reduction to tridiagonal form is a major step in eigenvalue computations for symmetric matrices. If the matrix is full, the conventional Householder tridi-agonalization approach <ref> [9, p. 276] </ref> or a block variant thereof [8] is the method of choice. These two ap This work was supported by the Applied and Computational Mathematics Program, Advanced Research Projects Agency, under contract DM28E04120, and by the Office of Scientific Computing, U.S.
Reference: [10] <author> R. G. Grimes and H. D. Simon, </author> <title> Solution of large, dense symmetric generalized eigenvalue problems using secondary storage, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14 (1988), </volume> <pages> pp. 241-256. </pages>
Reference-contexts: This approach reduces data accesses by a factor of nb and has been shown to perform efficiently on parallel machines [3, 2] and in out-of-core factorization approaches <ref> [10] </ref>. We have implemented this algorithm using a two-dimensional block torus wrapping. The block size nb of the block torus wrapping is also the block size used for orthogonal transformations, which are expressed with the WY representation [5].
Reference: [11] <author> W. D. Gropp and B. Smith, </author> <title> Chameleon parallel programming tools users manual, </title> <type> Tech. Rep. </type> <institution> ANL-93/23, Argonne National Laboratory, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: The block size nb of the block torus wrapping is also the block size used for orthogonal transformations, which are expressed with the WY representation [5]. To develop a portable code, and to allow a maintainable implementation, we chose to base our implementation on the Chameleon parallel programming tools <ref> [11] </ref>. The performance of this code is promising. Its performance on the reduction of a full random matrix to bandwidth 10, including the accumulation of orthogonal transformations, on the Intel Touchstone DELTA is shown in Figure 1.
Reference: [12] <author> B. Hendrikson and D. Womble, </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers, </title> <type> Tech. Rep. </type> <institution> SAND92-0792, Sandia National Laboratories, </institution> <year> 1992. </year>
Reference-contexts: These two ap This work was supported by the Applied and Computational Mathematics Program, Advanced Research Projects Agency, under contract DM28E04120, and by the Office of Scientific Computing, U.S. Department of Energy, under Contract W-31-109-Eng-38. proaches also underlie the parallel implementations described, for example, in <ref> [12] </ref> and [6]. The approach described in this paper, on the other hand, is a two-step instantiation of the "successive band reduction" framework suggested by Bischof and Sun [4].
Reference: [13] <author> B. Lang, </author> <title> A parallel algorithm for reducing symmetric matrices banded matrices to tridiagonal form, </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 14 (1993), </volume> <pages> pp. 1320-1338. </pages>
Reference-contexts: We first reduce the dense matrix to bandwidth nb using block orthogonal transformations employing the so-called WY representation [5]; this is described in Section 2. The remaining narrow-banded matrix is then reduced to tridiagonal form using a new variant of an algorithm originally suggested in <ref> [13] </ref>. In particular, we have devised a way of blocking the orthogonal transformations. The new algorithm is described in Section 3. The reason for considering this two-step approach is that in our experience the reduction of A is a computational bottleneck in the traditional approaches for tridiagonalization. <p> We will report elsewhere on the performance of this transformation. 3 Reduction of a Narrow Banded Ma trix to Tridiagonal Form To complete our tridiagonal reduction, we are now faced with the task of reducing a narrow-banded matrix to tridiagonal form. To this end, Lang <ref> [13] </ref> suggested the following algorithm, which we illustrate on the 15 fi 15 matrix with bandwidth 3 shown in Figure 2. In the first step, we generate a Householder transformation (labeled (1,1)), which reduces a (2:4,1) to a multiple of the first canonical unit vector e 1 fl . <p> The orthogonal transformation matrix Q is stored in the same 2-D block torus wrap mapping as before. We also mention that, unlike the original code used in <ref> [13] </ref>, we employ the same packed storage scheme as in LAPACK [1], which allows us to formulate the packed storage block algorithm more succinctly, and naturally employs BLAS-2 kernels.
Reference: [14] <author> R. Schreiber and C. Van Loan, </author> <title> A storage efficient WY representation for products of Householder transformations, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 10 (1989), </volume> <pages> pp. 53-57. 5 </pages>
Reference-contexts: Nar row Banded Form Block orthogonal transformations express the product H 1 H k of k Householder transformations H i = I u i u T i ; u i 2 R n as a rank-k update of the form I W Y T [5] or I W SW T <ref> [14] </ref>, where W; Y 2 R nfik and S 2 R kfik . As a result, the application of these transformations now involves BLAS-3 operations, such as matrix-matrix multiplication, which perform efficiently on contemporary high-performance architectures [7]. <p> For example, if we assume that we reduced the first three columns, and saved the associated transformations, as shown in Figure 2, we can accumulate transformations (1; k); (2; k) and (3; k) into a block transformation <ref> [5, 14] </ref>, and apply them "bottom-up" as indicated by the numbers in circles. The bottom-up approach is mandated by the successive-sweep chasing requirement, and the antidiagonal gathering of transformations then satisfies the column order requirement.
References-found: 14

