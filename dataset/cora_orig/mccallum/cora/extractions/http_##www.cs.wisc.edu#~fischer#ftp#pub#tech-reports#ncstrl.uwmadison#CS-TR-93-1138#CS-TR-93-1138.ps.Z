URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1138/CS-TR-93-1138.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1138/
Root-URL: http://www.cs.wisc.edu
Email: -adve,vernon-@cs.wisc.edu  
Title: The Influence of Random Delays on Parallel Execution Times  
Author: Vikram S. Adve and Mary K. Vernon 
Address: 1210 West Dayton Street Madison WI 53706.  
Affiliation: University of Wisconsin-Madison Computer Sciences Department  
Abstract: Stochastic models are widely used for the performance evaluation of parallel programs and systems. The stochastic assumptions in such models are intended to represent non-deterministic processing requirements as well as random delays due to inter-process communication and resource contention. In this paper, we provide compelling analytical and experimental evidence that in current and foreseeable shared-memory programs, communication delays introduce negligible variance into the execution time between synchronization points. Furthermore, we show using direct measurements of variance that other sources of randomness, particularly non-deterministic computational requirements, also do not introduce significant variance in many programs. We then use two examples to demonstrate the implications of these results for parallel program performance prediction models, as well as for general stochastic models of parallel systems. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. AGARWAL, R. SIMONI, M. HOROWITZ and J. HENNESSY, </author> <title> An Evaluation of Directory Schemes for Cache Coherence, </title> <booktitle> Proc. 15th Annual International Symposium on Computer Architecture, </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1988, </year> <pages> 280-289. </pages>
Reference-contexts: These measurements serve to test our conclusions under high communication overhead. We ran the simulator with a 64 kilobyte, 4-way set associative cache per node, and a full-directory nonbroadcast invalidate cache coherence protocol <ref> [1] </ref>.
Reference: 2. <author> F. ALLEN, M. BURKE, P. CHARLES, R. CYTRON and J. FERRANTE, </author> <title> An overview of the ptran analysis system for multiprocessing, </title> <journal> Journal of Parallel and Distributed Computing 5, </journal> <month> 5 (October </month> <year> 1988), </year> <pages> 617-640. </pages>
Reference-contexts: In that work, Sarkar describes a framework for determining the mean and variance of program execution times using frequency information from a counter-based execution profile of the program. His framework for estimating these parameters of execution times has been implemented as part of the PTRAN project at IBM Research <ref> [2] </ref>. However, he does not present any data from actual programs to show what parameter values occur in practice. Finally, stochastic models that allow general distributions of task-time have been applied using different specific distributions, including the normal distribution. [6, 8, 10].
Reference: 3. <author> F. BACCELLI and Z. LIU, </author> <title> On the Execution of Parallel Programs on Multiprocessor Systems A Queueing Theory Approach, </title> <journal> Journal of the ACM 37, </journal> <month> 2 (April </month> <year> 1990), </year> <pages> 373-414. </pages>
Reference-contexts: A number of stochastic models have also been used to study more general performance aspects of parallel systems <ref> [3, 5, 11-13, 17-19, 22, 28] </ref>. Many of these models assume exponentially distributed task execution times to permit tractable solutions. <p> A more complete evaluation of the deterministic approach for performance prediction is a subject of further study. 4.2. Implications for General Parallel Processing Models A number of stochastic models have also been used to study various aspects of parallel systems <ref> [3, 5, 11-13, 17-19, 22, 28] </ref>. In many cases, the goal of these models is a qualitative comparison of design alternatives based on quantitative performance estimates (for example, models used to compare scheduling policies). Many of these models assume exponentially distributed task or process execution times to permit tractable analysis.
Reference: 4. <author> E. D. BROOKS III, </author> <title> The indirect k-ary n-cube network for a vector processing environment, </title> <booktitle> Parallel Computing 6(1988), </booktitle> <pages> 339-348. </pages>
Reference-contexts: Hydro is a parallel simulation of particle motion in viscous fluids, with efficient communication. [7]. PSIM was developed at Lawrence Livermore Laboratories to simulate the indirect binary n-cube memory server network in a large parallel vector-processing environment <ref> [4] </ref>. Bicon is an implementation of a parallel algorithm to find the biconnected components of large graphs [24]. We measured each of the programs running stand-alone, allowing us to characterize the non-determinism intrinsic in the program.
Reference: 5. <author> C. S. CHANG and R. NELSON, </author> <title> Bounds on the Speedup and Efficiency of Partial Synchronization in Parallel Processing Systems, </title> <type> Research Report RC 16474, </type> <institution> I.B.M., </institution> <year> 1991. </year>
Reference-contexts: A number of stochastic models have also been used to study more general performance aspects of parallel systems <ref> [3, 5, 11-13, 17-19, 22, 28] </ref>. Many of these models assume exponentially distributed task execution times to permit tractable solutions. <p> A more complete evaluation of the deterministic approach for performance prediction is a subject of further study. 4.2. Implications for General Parallel Processing Models A number of stochastic models have also been used to study various aspects of parallel systems <ref> [3, 5, 11-13, 17-19, 22, 28] </ref>. In many cases, the goal of these models is a qualitative comparison of design alternatives based on quantitative performance estimates (for example, models used to compare scheduling policies). Many of these models assume exponentially distributed task or process execution times to permit tractable analysis.
Reference: 6. <author> M. DUBOIS and F. A. BRIGGS, </author> <title> Performance of Synchronized Iterative Processes in Multiprocessor Systems, </title> <journal> IEEE Trans. on Software Engineering SE-8, </journal> <month> 4 (July </month> <year> 1982), </year> <pages> 419-431. </pages>
Reference-contexts: However, he does not present any data from actual programs to show what parameter values occur in practice. Finally, stochastic models that allow general distributions of task-time have been applied using different specific distributions, including the normal distribution. <ref> [6, 8, 10] </ref>. Dubois and Briggs [6] as well as Greenberg [8] argued that a task could be asymptotically normally distributed because it is the sum of a large number of (nondeterministic) instruction execution times. Our proof in Appendix A is essentially a formalization of this argument. 2. <p> However, he does not present any data from actual programs to show what parameter values occur in practice. Finally, stochastic models that allow general distributions of task-time have been applied using different specific distributions, including the normal distribution. [6, 8, 10]. Dubois and Briggs <ref> [6] </ref> as well as Greenberg [8] argued that a task could be asymptotically normally distributed because it is the sum of a large number of (nondeterministic) instruction execution times. Our proof in Appendix A is essentially a formalization of this argument. 2.
Reference: 7. <author> Y. O. FUENTES and S. KIM, </author> <title> Foundations of Parallel Computational Microhydrodynamics : Communication Scheduling Strategies, </title> <type> Research Report, </type> <institution> Rheology Research Center, University of Wisconsin-Madison, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: We use these to compare the relative influence of the two sources of non-determinism, as well as to evaluate the overall variance and distribution of execution time. hhhhhhhhhhhhhhhhhh This work is supported in part by the National Science Foundation <ref> (CDA-9024618, CDA-8920777, CCR-9024144) </ref> and by an IBM Graduate Fellowship. * This paper appears in the Proceedings of the 1993 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems. - 1 - - -- One key conclusion we obtain is that communication delays do not introduce significant variance into the execution <p> The other three are also real applications in the sense that they were written to solve computa-tionally intensive problems of interest to their authors. Hydro is a parallel simulation of particle motion in viscous fluids, with efficient communication. <ref> [7] </ref>. PSIM was developed at Lawrence Livermore Laboratories to simulate the indirect binary n-cube memory server network in a large parallel vector-processing environment [4]. Bicon is an implementation of a parallel algorithm to find the biconnected components of large graphs [24].
Reference: 8. <author> A. GREENBAUM, </author> <title> Synchronization Costs on Multiprocessors, </title> <booktitle> Parallel Computing 10(1989), </booktitle> <pages> 3-14. </pages>
Reference-contexts: However, he does not present any data from actual programs to show what parameter values occur in practice. Finally, stochastic models that allow general distributions of task-time have been applied using different specific distributions, including the normal distribution. <ref> [6, 8, 10] </ref>. Dubois and Briggs [6] as well as Greenberg [8] argued that a task could be asymptotically normally distributed because it is the sum of a large number of (nondeterministic) instruction execution times. Our proof in Appendix A is essentially a formalization of this argument. 2. <p> However, he does not present any data from actual programs to show what parameter values occur in practice. Finally, stochastic models that allow general distributions of task-time have been applied using different specific distributions, including the normal distribution. [6, 8, 10]. Dubois and Briggs [6] as well as Greenberg <ref> [8] </ref> argued that a task could be asymptotically normally distributed because it is the sum of a large number of (nondeterministic) instruction execution times. Our proof in Appendix A is essentially a formalization of this argument. 2.
Reference: 9. <author> A. KAPELNIKOV, R. R. MUNTZ and M. D. ERCEGOVAC, </author> <title> A Modeling Methodology for the Analysis of Concurrent Systems and Computations, </title> <journal> Journal of Parallel and Distributed Computing 6(1989), </journal> <pages> 568-597. </pages>
Reference-contexts: One implication is that it appears possible to use a deterministic model for parallel program performance prediction. We show the potential advantages of this approach by comparing a simple deterministic model based on the above observations to some previous stochastic models <ref> [9, 10, 15, 19, 25] </ref> for one parallel program. A number of stochastic models have also been used to study more general performance aspects of parallel systems [3, 5, 11-13, 17-19, 22, 28]. Many of these models assume exponentially distributed task execution times to permit tractable solutions. <p> Calculating synchronization delays in a model based on nondeterministic task times is, however, extremely difficult. Thus, stochastic models that apply to any but the simplest program structures have had to assume exponentially distributed task execution times for tractability, and yet require extremely complex solution heuristics <ref> [9, 15] </ref> or Markov chains with exponentially growing state space sizes [16, 25]. The results of our work indicate that models that assume exponentially distributed task execution times may seriously overestimate the waiting time for many programs. <p> In the first case, we apply this model assuming exponentially distributed task execution times. Note that any errors introduced by this assumption will be found in the other models that assume exponentially distributed task times as well <ref> [9, 15, 16, 19, 25] </ref>. Henceforth, we refer to this as the Exponential Task model. In the second case, we apply the Kruskal and Weiss model using the true variance of task times (actually, estimates of the variance obtained as explained below).
Reference: 10. <author> C. P. KRUSKAL and A. WEISS, </author> <title> Allocating Independent Subtasks on Parallel Processors, </title> <journal> IEEE Trans. on Software Engineering SE-11, </journal> <month> 10 (October </month> <year> 1985), </year> <pages> 1001-1016. </pages>
Reference-contexts: One implication is that it appears possible to use a deterministic model for parallel program performance prediction. We show the potential advantages of this approach by comparing a simple deterministic model based on the above observations to some previous stochastic models <ref> [9, 10, 15, 19, 25] </ref> for one parallel program. A number of stochastic models have also been used to study more general performance aspects of parallel systems [3, 5, 11-13, 17-19, 22, 28]. Many of these models assume exponentially distributed task execution times to permit tractable solutions. <p> However, he does not present any data from actual programs to show what parameter values occur in practice. Finally, stochastic models that allow general distributions of task-time have been applied using different specific distributions, including the normal distribution. <ref> [6, 8, 10] </ref>. Dubois and Briggs [6] as well as Greenberg [8] argued that a task could be asymptotically normally distributed because it is the sum of a large number of (nondeterministic) instruction execution times. Our proof in Appendix A is essentially a formalization of this argument. 2. <p> To date, stochastic models with this capability have been restricted to programs with very simple task graphs and require further simplifying assumptions such as i.i.d. (independent, identically distributed) tasks within each parallel phase <ref> [10, 14] </ref>. The results of Section 3 suggest a different and potentially simple approach to this problem. Specifically, those results indicate that ignoring the variability in task times when evaluating the waiting time at synchronization points could give accurate results in many programs. <p> To further evaluate these implications, we present some preliminary data comparing the results obtained from three performance models for the MP3D application on the Sequent Symmetry. The first two models are specific instances of the Kruskal and Weiss model <ref> [10] </ref>. This model is restricted to programs with alternating serial and parallel phases where each parallel phase consists of i.i.d. tasks, but allows any continuous distribution of task execution time with coefficient of variation less than or equal to 1.
Reference: 11. <author> S. T. LEUTENEGGER and M. K. VERNON, </author> <title> The Performance of Multiprogrammed Multiprocessor Scheduling Policies, </title> <booktitle> Proc. ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems 18, </booktitle> <month> 1 (May </month> <year> 1990), </year> <pages> 226-236. </pages>
Reference: 12. <author> S. T. LEUTENEGGER and R. D. NELSON, </author> <title> Analysis of Spatial and Temporal Scheduling Policies for Semi-Static and Dynamic Multiprocessor Environments, </title> <institution> IBM Research Report, </institution> <month> August </month> <year> 1991. </year>
Reference: 13. <author> G. LEWANDOWSKI, A. CONDON and E. BACH, </author> <title> Realistic Analysis of Parallel Dynamic Programming Algorithms, </title> <type> Computer Sciences Technical Report #1116, </type> <institution> University of Wisconsin-Madison, </institution> <month> October </month> <year> 1992. </year>
Reference: 14. <author> S. MADALA and J. B. SINCLAIR, </author> <title> Performance of Synchronous Parallel Algorithms with Regular Structures, </title> <journal> IEEE Trans. on Parallel and Distributed Systems 2, </journal> <month> 1 (January </month> <year> 1991), </year> <pages> 105-116. </pages>
Reference-contexts: To date, stochastic models with this capability have been restricted to programs with very simple task graphs and require further simplifying assumptions such as i.i.d. (independent, identically distributed) tasks within each parallel phase <ref> [10, 14] </ref>. The results of Section 3 suggest a different and potentially simple approach to this problem. Specifically, those results indicate that ignoring the variability in task times when evaluating the waiting time at synchronization points could give accurate results in many programs.
Reference: 15. <author> V. W. MAK and S. F. LUNDSTROM, </author> <title> Predicting Performance of Parallel Computations, </title> <journal> IEEE Trans. on Parallel and Distributed Systems 1, </journal> <month> 3 (July </month> <year> 1990), </year> <pages> 257-270. </pages>
Reference-contexts: One implication is that it appears possible to use a deterministic model for parallel program performance prediction. We show the potential advantages of this approach by comparing a simple deterministic model based on the above observations to some previous stochastic models <ref> [9, 10, 15, 19, 25] </ref> for one parallel program. A number of stochastic models have also been used to study more general performance aspects of parallel systems [3, 5, 11-13, 17-19, 22, 28]. Many of these models assume exponentially distributed task execution times to permit tractable solutions. <p> Calculating synchronization delays in a model based on nondeterministic task times is, however, extremely difficult. Thus, stochastic models that apply to any but the simplest program structures have had to assume exponentially distributed task execution times for tractability, and yet require extremely complex solution heuristics <ref> [9, 15] </ref> or Markov chains with exponentially growing state space sizes [16, 25]. The results of our work indicate that models that assume exponentially distributed task execution times may seriously overestimate the waiting time for many programs. <p> In the first case, we apply this model assuming exponentially distributed task execution times. Note that any errors introduced by this assumption will be found in the other models that assume exponentially distributed task times as well <ref> [9, 15, 16, 19, 25] </ref>. Henceforth, we refer to this as the Exponential Task model. In the second case, we apply the Kruskal and Weiss model using the true variance of task times (actually, estimates of the variance obtained as explained below).
Reference: 16. <author> J. MOHAN, </author> <title> Performance of Parallel Programs: Model and Analyses, </title> <type> Ph.D. Thesis, </type> <institution> Carnegie Mellon University, </institution> <month> July </month> <year> 1984. </year>
Reference-contexts: Thus, stochastic models that apply to any but the simplest program structures have had to assume exponentially distributed task execution times for tractability, and yet require extremely complex solution heuristics [9, 15] or Markov chains with exponentially growing state space sizes <ref> [16, 25] </ref>. The results of our work indicate that models that assume exponentially distributed task execution times may seriously overestimate the waiting time for many programs. The renewal model shows that it is possible for very small tasks to experience significant variance in execution time due to random delays. <p> In the first case, we apply this model assuming exponentially distributed task execution times. Note that any errors introduced by this assumption will be found in the other models that assume exponentially distributed task times as well <ref> [9, 15, 16, 19, 25] </ref>. Henceforth, we refer to this as the Exponential Task model. In the second case, we apply the Kruskal and Weiss model using the true variance of task times (actually, estimates of the variance obtained as explained below).
Reference: 17. <author> R. NELSON, D. TOWSLEY and A. N. TANTAWI, </author> <title> Performance Analysis of Parallel Processing Systems, </title> <journal> IEEE Trans. on Software Engineering 14, </journal> <month> 4 (April </month> <year> 1988), </year> <pages> 532-540. </pages>
Reference-contexts: If a result would be significantly weaker, or even different, for programs that have much lower task time variance, it may not apply to many parallel programs. One result that is not strongly dependent on the distribution assumption is as follows. Nelson et al <ref> [17] </ref> showed that in an environment containing mixed sequential (interactive) and large parallel (batch) jobs, an unpartitioned system yields better performance than one in which processors are statically partitioned among the two classes. They assumed that the parallel jobs consisted of tasks with exponentially distributed execution times.
Reference: 18. <author> R. NELSON, A. N. TANTAWI and D. TOWSLEY, </author> <title> The Order Statistics of the Sojourn Times of Customers that Form a Single Batch in the M X /M/c Queue, </title> <type> Research Report RC 15141, </type> <institution> I.B.M., </institution> <year> 1990. </year>
Reference-contexts: We use these to compare the relative influence of the two sources of non-determinism, as well as to evaluate the overall variance and distribution of execution time. hhhhhhhhhhhhhhhhhh This work is supported in part by the National Science Foundation <ref> (CDA-9024618, CDA-8920777, CCR-9024144) </ref> and by an IBM Graduate Fellowship. * This paper appears in the Proceedings of the 1993 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems. - 1 - - -- One key conclusion we obtain is that communication delays do not introduce significant variance into the execution
Reference: 19. <author> R. NELSON, </author> <title> A Performance Evaluation of a General Parallel Processing Model, </title> <booktitle> 1990 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems 18, </booktitle> <month> 1 (May </month> <year> 1990), </year> <pages> 14-26. </pages>
Reference-contexts: One implication is that it appears possible to use a deterministic model for parallel program performance prediction. We show the potential advantages of this approach by comparing a simple deterministic model based on the above observations to some previous stochastic models <ref> [9, 10, 15, 19, 25] </ref> for one parallel program. A number of stochastic models have also been used to study more general performance aspects of parallel systems [3, 5, 11-13, 17-19, 22, 28]. Many of these models assume exponentially distributed task execution times to permit tractable solutions. <p> In the first case, we apply this model assuming exponentially distributed task execution times. Note that any errors introduced by this assumption will be found in the other models that assume exponentially distributed task times as well <ref> [9, 15, 16, 19, 25] </ref>. Henceforth, we refer to this as the Exponential Task model. In the second case, we apply the Kruskal and Weiss model using the true variance of task times (actually, estimates of the variance obtained as explained below). <p> In contrast, one result that is significant for exponentially distributed tasks but weaker for tasks with lower variance is Nelson's result <ref> [19] </ref> that higher variance of parallelism can yield lower response times, when queueing effects are small. He considers a parallel processing system with P processors and an arrival stream of parallel jobs, where an arriving job splits into n tasks with probability b n , 1 n N max . <p> To show the effect of the choice of distribution, consider the two systems (A and B) that were compared in <ref> [19] </ref>, each with 8 processors but different distributions of parallelism (b i ) as given in Figure 4.1. Both systems have mean parallelism of 2.4, but A has a higher variance of parallelism than B.
Reference: 20. <author> S. K. REINHARDT, M. D. HILL, J. R. LARUS, A. R. LEBECK, J. C. LEWIS and D. A. WOOD, </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers, </title> <booktitle> In these proceedings, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Measurements on the CM-5 We were also able to measure two of the above programs on a 32-processor Thinking Machines CM-5, a system that is scalable to much larger numbers of processors. Since the CM-5 does not support shared memory, we used the Wisconsin Windtunnel <ref> [20] </ref> to simulate the execution of the shared-memory applications. In this simulator, the application program executes at full hardware speed on the processing nodes of the CM-5, but traps into software on memory references to cache blocks that would not be in the target machine's cache.
Reference: 21. <author> V. SARKAR, </author> <title> Determining Average Program Execution Times and their Variance, </title> <booktitle> Proc. 1989 SIGPLAN Notices Conference on Programming Language Design and Implementation, </booktitle> <year> 1989, </year> <pages> 298-312. </pages>
Reference-contexts: However, one previous paper focuses on estimating the mean and variance of the processing requirements of tasks in the presence of data-dependent effects such as conditional branch probabilities and loop frequencies <ref> [21] </ref>. In that work, Sarkar describes a framework for determining the mean and variance of program execution times using frequency information from a counter-based execution profile of the program.
Reference: 22. <author> S. SETIA and S. K. TRIPATHI, </author> <title> An Analysis of Several Processor Partitioning Policies for Parallel Computers, </title> <institution> University of Maryland CS-Tech. Rep.-2684, </institution> <month> May </month> <year> 1991. </year> <month> - 20 </month> - - -- 
Reference-contexts: A number of stochastic models have also been used to study more general performance aspects of parallel systems <ref> [3, 5, 11-13, 17-19, 22, 28] </ref>. Many of these models assume exponentially distributed task execution times to permit tractable solutions. <p> A more complete evaluation of the deterministic approach for performance prediction is a subject of further study. 4.2. Implications for General Parallel Processing Models A number of stochastic models have also been used to study various aspects of parallel systems <ref> [3, 5, 11-13, 17-19, 22, 28] </ref>. In many cases, the goal of these models is a qualitative comparison of design alternatives based on quantitative performance estimates (for example, models used to compare scheduling policies). Many of these models assume exponentially distributed task or process execution times to permit tractable analysis. <p> They assumed that the parallel jobs consisted of tasks with exponentially distributed execution times. But, in fact, Setia and Tripathi <ref> [22] </ref> showed that the same conclusion holds with a completely different assumption about task times. Specifically, they assumed that each job consisted of tasks of equal size, while the total execution time of the jobs on any fixed number of processors was assumed to be exponentially distributed.
Reference: 23. <author> J. P. SINGH, W. WEBER and A. GUPTA, </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory, Computer Architecture News 20, </title> <month> 1 (March </month> <year> 1992), </year> <pages> 5-44. </pages>
Reference-contexts: The Large input size is a somewhat more realistic data set than the Small size). Four of the applications (MP3D, Locus Route, Water and Barnes) are from the Splash suite, which was developed to provide a realistic set of parallel applications for performance evaluation of parallel systems <ref> [23] </ref>. The other three are also real applications in the sense that they were written to solve computa-tionally intensive problems of interest to their authors. Hydro is a parallel simulation of particle motion in viscous fluids, with efficient communication. [7]. <p> c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh practice <ref> [23] </ref>. To analyze these results further, we compare the measured parameters against regions of the parameter space in Figure 2.2 (recalling that Figure 2.2 is pessimistic for communication overhead less than or equal to 0.24).
Reference: 24. <author> R. E. TARJAN and U. VISHKIN, </author> <title> An Efficient Parallel Biconnectivity Algorithm, </title> <journal> SIAM Journal of Computing 14, </journal> <volume> 4 (1985), </volume> <pages> 862-874. </pages>
Reference-contexts: PSIM was developed at Lawrence Livermore Laboratories to simulate the indirect binary n-cube memory server network in a large parallel vector-processing environment [4]. Bicon is an implementation of a parallel algorithm to find the biconnected components of large graphs <ref> [24] </ref>. We measured each of the programs running stand-alone, allowing us to characterize the non-determinism intrinsic in the program.
Reference: 25. <author> A. THOMASIAN and P. F. </author> <title> BAY, Analytic Queueing Network Models for Parallel Processing of Task Systems, </title> <journal> IEEE Trans. on Computers C-35, </journal> <month> 12 (December </month> <year> 1986), </year> <pages> 1045-1054. </pages>
Reference-contexts: One implication is that it appears possible to use a deterministic model for parallel program performance prediction. We show the potential advantages of this approach by comparing a simple deterministic model based on the above observations to some previous stochastic models <ref> [9, 10, 15, 19, 25] </ref> for one parallel program. A number of stochastic models have also been used to study more general performance aspects of parallel systems [3, 5, 11-13, 17-19, 22, 28]. Many of these models assume exponentially distributed task execution times to permit tractable solutions. <p> Thus, stochastic models that apply to any but the simplest program structures have had to assume exponentially distributed task execution times for tractability, and yet require extremely complex solution heuristics [9, 15] or Markov chains with exponentially growing state space sizes <ref> [16, 25] </ref>. The results of our work indicate that models that assume exponentially distributed task execution times may seriously overestimate the waiting time for many programs. The renewal model shows that it is possible for very small tasks to experience significant variance in execution time due to random delays. <p> In the first case, we apply this model assuming exponentially distributed task execution times. Note that any errors introduced by this assumption will be found in the other models that assume exponentially distributed task times as well <ref> [9, 15, 16, 19, 25] </ref>. Henceforth, we refer to this as the Exponential Task model. In the second case, we apply the Kruskal and Weiss model using the true variance of task times (actually, estimates of the variance obtained as explained below).
Reference: 26. <author> K. S. TRIVEDI, </author> <title> Probability and Statistics with Reliability, Queueing and Computer Science Applications, </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1982. </year>
Reference-contexts: In addition, the Kolmogorov-Smirnov statistic can be constructed from the measured samples and used to derive a confidence band for the actual parent dis tribution <ref> [26] </ref>. This gives an error bound between the estimated and actual parent distribution at a certain level of confidence.
Reference: 27. <author> R. W. WOLFF, </author> <title> Stochastic Modeling and the Theory of Queues, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: But (4) is just the definition of a renewal process generated by Pi: 1 i &lt; -; specifically, R (t) is the number of renewals by time t <ref> [27] </ref>. The following expressions for m R (t) E [R (t)] and its ordinary Laplace transform are well-known [27]: m R (t) = F P (t) + m R (t) * F P (t), 1 - F P (s) hhhhhhhhh , (5) The corresponding expressions for m R 2 (t) E <p> But (4) is just the definition of a renewal process generated by Pi: 1 i &lt; -; specifically, R (t) is the number of renewals by time t <ref> [27] </ref>. The following expressions for m R (t) E [R (t)] and its ordinary Laplace transform are well-known [27]: m R (t) = F P (t) + m R (t) * F P (t), 1 - F P (s) hhhhhhhhh , (5) The corresponding expressions for m R 2 (t) E [R 2 (t)] and its Laplace transform are also not difficult to derive [27]: m R 2 (t) <p> Laplace transform are well-known <ref> [27] </ref>: m R (t) = F P (t) + m R (t) * F P (t), 1 - F P (s) hhhhhhhhh , (5) The corresponding expressions for m R 2 (t) E [R 2 (t)] and its Laplace transform are also not difficult to derive [27]: m R 2 (t) = m R (t) + 2 m R (t -x)dm R (x), t 0 ( 1 - F P (s) ) 2 hhhhhhhhhhhhhhhhhhh (6) where L (m R (t)) and L (m R 2 (t)) denote the ordinary Laplace transforms of m R (t) and m <p> Approximate expressions for mm T | D and ss T | D 2 Estimates for m R (t) and s R 2 (t) are given in the Central Limit Theorem for renewal processes <ref> [27] </ref>, which states that as t fi , R (t) is asymptotically normal with mean 3 t /m P and variance ts P 2 / m P 3 . Therefore, we estimate s R 2 by 2 / m P 2 . <p> We can, however, derive the form of the distribution in the special case when D is large. In that case, we show in Appendix A that it is possible to apply the version of the Central Limit Theorem for cumulative (regenerative) processes <ref> [27] </ref> to prove: T (D) fi Normal (m (D),s (D)) as D fi , m (D) = D+ D m C hhhhh , s 2 (D) = m P 2 hhhhh + m P m C 2 hhhhhhhh (8) where fi denotes convergence in distribution.
Reference: 28. <author> J. ZAHORJAN and C. MCCANN, </author> <title> Processor Scheduling in Shared-Memory Multiprocessors, </title> <booktitle> Proc. ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems 18, </booktitle> <month> 1 (May </month> <year> 1990), </year> <pages> 214-225. </pages>
Reference-contexts: A number of stochastic models have also been used to study more general performance aspects of parallel systems <ref> [3, 5, 11-13, 17-19, 22, 28] </ref>. Many of these models assume exponentially distributed task execution times to permit tractable solutions. <p> A more complete evaluation of the deterministic approach for performance prediction is a subject of further study. 4.2. Implications for General Parallel Processing Models A number of stochastic models have also been used to study various aspects of parallel systems <ref> [3, 5, 11-13, 17-19, 22, 28] </ref>. In many cases, the goal of these models is a qualitative comparison of design alternatives based on quantitative performance estimates (for example, models used to compare scheduling policies). Many of these models assume exponentially distributed task or process execution times to permit tractable analysis.
References-found: 28

