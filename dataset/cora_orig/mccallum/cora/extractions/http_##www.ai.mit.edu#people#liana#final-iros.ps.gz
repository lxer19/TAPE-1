URL: http://www.ai.mit.edu/people/liana/final-iros.ps.gz
Refering-URL: http://www.ai.mit.edu/people/liana/liana.html
Root-URL: 
Email: Email: liana@ai.mit.edu  
Title: Visually-Guided Obstacle Avoidance in Unstructured Environments  
Author: Liana M. Lorigo, Rodney A. Brooks, W. E. L. Grimson 
Keyword: Vision-based navigation, space exploration, modular design, reactive control, unstructured terrain.  
Address: Cambridge MA 02139 USA  
Affiliation: MIT Artificial Intelligence Laboratory  
Abstract: This paper presents an autonomous vision-based obstacle avoidance system. The system consists of three independent vision modules for obstacle detection, each of which is computationally simple and uses a different criterion for detection purposes. These criteria are based on brightness gradients, RGB (Red, Green, Blue) color, and HSV (Hue, Saturation, Value) color, respectively. Selection of which modules are used to command the robot proceeds exclusively from the outputs of the modules themselves. The system is implemented on a small monocular mobile robot and uses very low resolution images. It has been tested for over 200 hours in diverse environments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brooks, R.A. </author> <year> 1986. </year> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 2(1) </volume> <pages> 14-23. </pages>
Reference-contexts: Further, the obstacle detection method presented is reactive, storing almost no memory of obstacle locations, but rather using current images directly. That is, percepts are converted to actions without the use of complicated internal state. The system also draws from the behavior-based subsumption architecture approach for combining routines <ref> [1] </ref>. Alternative architectures integrate information from multiple routines according to pre-set weights for each routine [10]. While a part of the current system uses a related approach, the weights are automatically computed instead of pre-set by the user.
Reference: [2] <author> Coombs, D. and Roberts, K. </author> <year> 1992. </year> <title> "Bee-bot": using peripheral optical flow to avoid obstacles. </title> <booktitle> In Intelligent Robots and Computer Vision Boston, MA, SPIE 1825 </booktitle> <pages> 714-721. </pages>
Reference-contexts: Methods also vary in how they deal with temporal information, from using individual frames exclusively [6] to computing optical flow fields from multiple frames. Domains include road and off-road travel [5, 10, 14, 3, 16, 4] and indoor robotic navigation <ref> [6, 2, 12, 11] </ref>. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain [9, 7, 13, 8].
Reference: [3] <author> Crisman, J.D. </author> <year> 1991. </year> <title> Color region tracking for vehicle guidance. Active Vision, </title> <publisher> MIT Press: </publisher> <address> Cam-bridge, MA. </address> <pages> pp. 107-220. </pages>
Reference-contexts: Variations have included using sensory input from stereo vision, monocular vision, and the combination of vision with other sensors. Methods also vary in how they deal with temporal information, from using individual frames exclusively [6] to computing optical flow fields from multiple frames. Domains include road and off-road travel <ref> [5, 10, 14, 3, 16, 4] </ref> and indoor robotic navigation [6, 2, 12, 11]. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain [9, 7, 13, 8].
Reference: [4] <author> Dickmanns, E.D., Mysliwetz, B. and Christians, T. </author> <year> 1990. </year> <title> An integrated spatio-temporal approach to automatic visual guidance of autonomous vehicles. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 20(6) </journal> <pages> 1273-1284. </pages>
Reference-contexts: Variations have included using sensory input from stereo vision, monocular vision, and the combination of vision with other sensors. Methods also vary in how they deal with temporal information, from using individual frames exclusively [6] to computing optical flow fields from multiple frames. Domains include road and off-road travel <ref> [5, 10, 14, 3, 16, 4] </ref> and indoor robotic navigation [6, 2, 12, 11]. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain [9, 7, 13, 8].
Reference: [5] <author> Hebert, M., Pomerleau, D., Stentz, A., Thorpe, C. </author> <year> 1995. </year> <title> Computer vision for navigation: the CMU UGV project. </title> <booktitle> In Proceedings of the Workshop on Vision for Robots, </booktitle> <publisher> IEEE Computer Society Press. </publisher> <pages> pp. 97-102. </pages>
Reference-contexts: Variations have included using sensory input from stereo vision, monocular vision, and the combination of vision with other sensors. Methods also vary in how they deal with temporal information, from using individual frames exclusively [6] to computing optical flow fields from multiple frames. Domains include road and off-road travel <ref> [5, 10, 14, 3, 16, 4] </ref> and indoor robotic navigation [6, 2, 12, 11]. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain [9, 7, 13, 8].
Reference: [6] <author> Horswill, I.D. </author> <year> 1993. </year> <title> Polly: A vision based artificial agent. </title> <booktitle> In Eleventh Natl. Conf. on AI, </booktitle> <pages> pp. 824-829. </pages>
Reference-contexts: The problem of vision-based navigation has been previously examined from a number of different approaches. Variations have included using sensory input from stereo vision, monocular vision, and the combination of vision with other sensors. Methods also vary in how they deal with temporal information, from using individual frames exclusively <ref> [6] </ref> to computing optical flow fields from multiple frames. Domains include road and off-road travel [5, 10, 14, 3, 16, 4] and indoor robotic navigation [6, 2, 12, 11]. One motivating goal of the current work is autonomous exploration of the surface of Mars. <p> Methods also vary in how they deal with temporal information, from using individual frames exclusively [6] to computing optical flow fields from multiple frames. Domains include road and off-road travel [5, 10, 14, 3, 16, 4] and indoor robotic navigation <ref> [6, 2, 12, 11] </ref>. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain [9, 7, 13, 8]. <p> One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain [9, 7, 13, 8]. The current system is most closely related to Hor-swill's work <ref> [6] </ref>, which used low-level environment-dependent algorithms for vision-based navigation. The current work modifies and extends such obstacle avoidance techniques to handle a different class of environments, including domains in which the ground may have rich visual texture.
Reference: [7] <author> Krotkov, E. and Hebert, M. </author> <year> 1995. </year> <title> Mapping and positioning for a prototype lunar rover. </title> <booktitle> Proceedings of IEEE Int'l Conf. on Robotics and Automation, </booktitle> <pages> pp. 2913-2919. </pages>
Reference-contexts: Domains include road and off-road travel [5, 10, 14, 3, 16, 4] and indoor robotic navigation [6, 2, 12, 11]. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain <ref> [9, 7, 13, 8] </ref>. The current system is most closely related to Hor-swill's work [6], which used low-level environment-dependent algorithms for vision-based navigation.
Reference: [8] <author> Matthies, L., Gat, E., Harrison, R., Wilcox, B., Volpe, R., Litwin, T. </author> <year> 1995. </year> <title> Mars microrover navigation: performance evaluation and enhancement. </title> <booktitle> In Proceedings of IEEE Int'l Conf. on Intelligent Robots and Systems, </booktitle> <volume> 1 </volume> <pages> 433-440. </pages>
Reference-contexts: Domains include road and off-road travel [5, 10, 14, 3, 16, 4] and indoor robotic navigation [6, 2, 12, 11]. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain <ref> [9, 7, 13, 8] </ref>. The current system is most closely related to Hor-swill's work [6], which used low-level environment-dependent algorithms for vision-based navigation.
Reference: [9] <author> Pagnot, R. and Grandjean, P. </author> <year> 1995. </year> <title> Fast cross-country navigation on fair terrains. </title> <booktitle> In Proceedings of IEEE Int'l Conf. on Robotics and Automation, </booktitle> <pages> pp. 2593-2598. </pages>
Reference-contexts: Domains include road and off-road travel [5, 10, 14, 3, 16, 4] and indoor robotic navigation [6, 2, 12, 11]. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain <ref> [9, 7, 13, 8] </ref>. The current system is most closely related to Hor-swill's work [6], which used low-level environment-dependent algorithms for vision-based navigation.
Reference: [10] <author> Rosenblatt, J. and Thorpe, C. </author> <year> 1995. </year> <title> Combining multiple goals in a behavior-based architecture. </title> <booktitle> In Proceedings of IEEE Int'l Conf. on Intelligent Robots and Systems, </booktitle> <volume> 1 </volume> <pages> 136-141. </pages>
Reference-contexts: Variations have included using sensory input from stereo vision, monocular vision, and the combination of vision with other sensors. Methods also vary in how they deal with temporal information, from using individual frames exclusively [6] to computing optical flow fields from multiple frames. Domains include road and off-road travel <ref> [5, 10, 14, 3, 16, 4] </ref> and indoor robotic navigation [6, 2, 12, 11]. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain [9, 7, 13, 8]. <p> That is, percepts are converted to actions without the use of complicated internal state. The system also draws from the behavior-based subsumption architecture approach for combining routines [1]. Alternative architectures integrate information from multiple routines according to pre-set weights for each routine <ref> [10] </ref>. While a part of the current system uses a related approach, the weights are automatically computed instead of pre-set by the user. Note that the robot's only goal is to move safely within cluttered environments; it has no target location.
Reference: [11] <author> Santos-Victor, J. and Sandini, G. </author> <year> 1995. </year> <title> Uncalibrated obstacle detection using normal flow. </title>
Reference-contexts: Methods also vary in how they deal with temporal information, from using individual frames exclusively [6] to computing optical flow fields from multiple frames. Domains include road and off-road travel [5, 10, 14, 3, 16, 4] and indoor robotic navigation <ref> [6, 2, 12, 11] </ref>. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain [9, 7, 13, 8].
Reference: [12] <author> Santos-Victor, J., Sandini, G., Curotto, F. and Garibaldi, S. </author> <year> 1995. </year> <title> Divergent stereo in autonomous navigation: from bees to robots. </title> <journal> Int'l Journal of Computer Vision, </journal> <pages> pp. 159-177. </pages>
Reference-contexts: Methods also vary in how they deal with temporal information, from using individual frames exclusively [6] to computing optical flow fields from multiple frames. Domains include road and off-road travel [5, 10, 14, 3, 16, 4] and indoor robotic navigation <ref> [6, 2, 12, 11] </ref>. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain [9, 7, 13, 8].
Reference: [13] <author> Simmons, R., Krotkov, E., Chrisman, L., Coz-man, F., Goodwin, R., Hebert, M., Katragadda, L., Koenig, S., Krishnaswamy, G., Shinoda, Y., Whittaker, W., and Klarer, P. </author> <year> 1995. </year> <title> Experience with rover navigation for lunar-like terrains. </title> <booktitle> In Proceedings of IEEE Int'l Conf. on Intelligent Robots and Systems, </booktitle> <volume> 1 </volume> <pages> 441-446. </pages>
Reference-contexts: Domains include road and off-road travel [5, 10, 14, 3, 16, 4] and indoor robotic navigation [6, 2, 12, 11]. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain <ref> [9, 7, 13, 8] </ref>. The current system is most closely related to Hor-swill's work [6], which used low-level environment-dependent algorithms for vision-based navigation.
Reference: [14] <author> Turk, M.A., Morgenthaler, D.G., Gremban, K.D., and Marra, M. </author> <year> 1988. </year> <title> VITS a vision system for autonomous land vehicle navigation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(3) </volume> <pages> 342-361. </pages>
Reference-contexts: Variations have included using sensory input from stereo vision, monocular vision, and the combination of vision with other sensors. Methods also vary in how they deal with temporal information, from using individual frames exclusively [6] to computing optical flow fields from multiple frames. Domains include road and off-road travel <ref> [5, 10, 14, 3, 16, 4] </ref> and indoor robotic navigation [6, 2, 12, 11]. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain [9, 7, 13, 8].
Reference: [15] <author> Ullman, S. </author> <year> 1984. </year> <title> Visual routines. </title> <journal> Cognition, </journal> <volume> 18 </volume> <pages> 97-159. </pages>
Reference-contexts: The current work modifies and extends such obstacle avoidance techniques to handle a different class of environments, including domains in which the ground may have rich visual texture. This work also draws on visual routines theory in that it combines separate low-level vision algorithms in a similar manner <ref> [15] </ref>. Further, the obstacle detection method presented is reactive, storing almost no memory of obstacle locations, but rather using current images directly. That is, percepts are converted to actions without the use of complicated internal state. The system also draws from the behavior-based subsumption architecture approach for combining routines [1].
Reference: [16] <author> Zeng, N., Crisman, J.D. </author> <year> 1995. </year> <title> Categorical color projection for robot road following. </title> <booktitle> In Proceedings of IEEE Int'l Conf. on Robotics and Automation, </booktitle> <pages> pp. 1080-1085. </pages>
Reference-contexts: Variations have included using sensory input from stereo vision, monocular vision, and the combination of vision with other sensors. Methods also vary in how they deal with temporal information, from using individual frames exclusively [6] to computing optical flow fields from multiple frames. Domains include road and off-road travel <ref> [5, 10, 14, 3, 16, 4] </ref> and indoor robotic navigation [6, 2, 12, 11]. One motivating goal of the current work is autonomous exploration of the surface of Mars. Many researchers are addressing the problem of navigation in this domain [9, 7, 13, 8].
References-found: 16

