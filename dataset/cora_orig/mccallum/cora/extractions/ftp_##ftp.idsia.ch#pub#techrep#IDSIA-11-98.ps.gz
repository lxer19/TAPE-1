URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-11-98.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Title: Learning to Predict Through Probabilistic Incremental Program Evolution and Automatic Task Decomposition  
Author: Rafa l Sa lustowicz and Jurgen Schmidhuber 
Keyword: Probabilistic Incremental Program Evolution, PIPE, Long Short-Term Memory, LSTM, recurrent neural networks, filtering, stochastic program search, time-series prediction  
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Pubnum: Technical Report IDSIA-11-98  
Email: e-mail: frafal, juergeng@idsia.ch  
Phone: tel.: +41-91-9919838 fax: +41-91-9919839  
Abstract: Analog gradient-based recurrent neural nets can learn complex prediction tasks. Most, however, tend to fail in case of long minimal time lags between relevant training events. On the other hand, discrete methods such as search in a space of event-memorizing programs are not necessarily affected at all by long time lags: we show that discrete "Probabilistic Incremental Program Evolution" (PIPE) can solve several long time lag tasks that have been successfully solved by only one analog method ("Long Short-Term Memory" | LSTM). In fact, sometimes PIPE even outperforms LSTM. Existing discrete methods, however, cannot easily deal with problems whose solutions exhibit comparatively high algorithmic complexity. We overcome this drawback by introducing filtering, a novel, general, data-driven divide-and-conquer technique for automatic task decomposition that is not limited to a particular learning method. We compare PIPE plus filtering to various analog recurrent net methods. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angeline, P. J. and Pollack, J. B. </author> <year> (1992). </year> <title> The evolutionary induction of subroutines. </title> <booktitle> In Proceedings of the 14th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 236-241, </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Baluja, S. </author> <year> (1994). </year> <title> Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning. </title> <type> Technical Report CMU-CS-94-163, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Baluja, S. and Caruana, R. </author> <year> (1995). </year> <title> Removing the genetics from the standard genetic algorithm. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 38-46. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Bengio, Y., Simard, P., and Frasconi, P. </author> <year> (1994). </year> <title> Learning long-term dependencies with gradient descent is difficult. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 157-166. </pages>
Reference: <author> Cleeremans, A., Servan-Schreiber, D., and McClelland, J. L. </author> <year> (1989). </year> <title> Finite-state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 372-381. </pages>
Reference-contexts: To correctly predict the symbol before last, the second symbol has to be remembered. Comparison. We compare PIPE with MCs and filtering to LSTM (results taken from (Hochreiter and Schmidhuber, 1997a)), "Elman nets trained by Elman's training procedure" (ELM) <ref> (results taken from Cleeremans et al. 1989) </ref>, Fahlman's "Recurrent Cascade-Correlation" (RCC) (results taken from Fahlman 1991), and RTRL (results taken from Smith and Zipser (1989), where only the few successful trials are listed). <p> 25,000 ELM 15 0 &gt;200,000 LSTM 3 blocks, size 2 100 8,440 Table 3: Results of several analog approaches for the embedded Reber grammar: percentage of successful trials and number of sequence presentations until success for RTRL (results taken from Smith and Zipser 1989), "Elman net trained by Elman's procedure" <ref> (results taken from Cleeremans et al. 1989) </ref>, "Recurrent Cascade-Correlation" (results taken from Fahlman 1991) and LSTM (results taken from Hochreiter and Schmidhuber 1997). Only LSTM always learned to solve the task. It also needed least sequence presentations on average (mean of 30 trials).
Reference: <author> Cramer, N. L. </author> <year> (1985). </year> <title> A representation for the adaptive generation of simple sequential programs. </title> <editor> In Grefenstette, J., editor, </editor> <booktitle> Proceedings of an International Conference on Genetic Algorithms and Their Applications, </booktitle> <pages> pages 183-187, </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. 15 Fahlman, </publisher> <editor> S. E. </editor> <year> (1991). </year> <title> The recurrent cascade-correlation learning algorithm. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 190-196. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hochreiter, S. </author> <year> (1991). </year> <title> Untersuchungen zu dynamischen neuronalen Netzen. </title> <type> Diploma thesis, </type> <institution> Institut fur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen. </institution> <note> See www7.informatik.tu-muenchen.de/~hochreit. </note>
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1997a). </year> <title> Long short-term memory. </title> <journal> Neural Computation, </journal> <volume> 9(8) </volume> <pages> 1735-1780. </pages>
Reference-contexts: Section 4 compares PIPE and LSTM on two long time lag tasks with low AC. Section 5 introduces filtering. Section 6 compares PIPE, LSTM and various other recurrent neural net approaches on a task with comparatively high AC. Section 7 concludes. 2 Long Short-Term Memory (LSTM) Overview. LSTM <ref> (Hochreiter and Schmidhuber, 1997a) </ref> is a recent, analog, gradient-based recurrent neural net approach for supervised learning of sequential processes. Unlike most alternative approaches it can learn from training sequences that do not exhibit any short time lags between relevant events. <p> Since unit j is usually not only connected to itself but also to other units, there is an "input weight conflict" and an "output weight conflict" (see <ref> (Hochreiter and Schmidhuber, 1997a) </ref> for details). <p> Only within memory cells, errors are propagated back through previous internal states s c j . See <ref> (Hochreiter and Schmidhuber, 1997a) </ref> for a detailed description of all learning rules. There it is also experimentally shown that LSTM easily outperforms competing recurrent net approaches on both short and long time lag tasks. 3 Probabilistic Incremental Program Evolution (PIPE) Overview. <p> In this section we compare PIPE and LSTM on two problems involving both long minimal time lags and low algorithmic complexity (AC). So far both the "adding problem" and the "temporal order problem" <ref> (Hochreiter and Schmidhuber, 1997a) </ref> have been solved by only one single analog method (LSTM). The adding experiments show that LSTM's convergence speed depends on time lag size, while PIPE's does not. We will see that sometimes simple ROLs suffice. <p> Results. The minimal time lag between the most recent occurrence of relevant information and the point of prediction varies from 25 to 500 time steps. Table 1 summarizes all results. LSTM results are taken from <ref> (Hochreiter and Schmidhuber, 1997a) </ref>. LSTM always finds perfect or almost perfect solutions. With a test set consisting of 2560 randomly chosen sequences, in all 10 independent trials LSTM's average test set error is below 0.01, and there are never more than 3 incorrectly processed sequences. <p> To correctly predict the symbol before last, the second symbol has to be remembered. Comparison. We compare PIPE with MCs and filtering to LSTM (results taken from <ref> (Hochreiter and Schmidhuber, 1997a) </ref>), "Elman nets trained by Elman's training procedure" (ELM) (results taken from Cleeremans et al. 1989), Fahlman's "Recurrent Cascade-Correlation" (RCC) (results taken from Fahlman 1991), and RTRL (results taken from Smith and Zipser (1989), where only the few successful trials are listed).
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1997b). </year> <title> LSTM can solve hard long time lag problems. </title> <editor> In Mozer, M. C., Jordan, M. I., and Petsche, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <pages> pages 473-479. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Koza, J. R. </author> <year> (1992). </year> <title> Genetic Programming On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Generic Random Constants. A generic random constant (GRC) (compare also "ephemeral random constant" <ref> (Koza, 1992) </ref>) is a zero argument function (a terminal). When accessed during program creation, it is either instantiated to a random value from a predefined, problem-dependent set of constants or a value previously stored together with the probability distribution (see below). Program Representation.
Reference: <author> Levin, L. A. </author> <year> (1973). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference: <author> Levin, L. A. </author> <year> (1984). </year> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37. </pages>
Reference: <author> Mozer, M. C. </author> <year> (1992). </year> <title> Induction of multiscale temporal structure. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 275-282. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To achieve constant error flow through j it suffices if f 0 holds <ref> (see also Mozer, 1992) </ref>. Integrating the differential equation above, one obtains f j (net j (t)) = net j (t) w jj for arbitrary net j (t).
Reference: <author> Pearlmutter, B. A. </author> <year> (1995). </year> <title> Gradient calculations for dynamic recurrent neural networks: A survey. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 6(5) </volume> <pages> 1212-1228. </pages>
Reference: <author> Robinson, A. J. and Fallside, F. </author> <year> (1987). </year> <title> The utility driven dynamic error propagation network. </title> <type> Technical Report CUED/F-INFENG/TR.1, </type> <institution> Cambridge University Engineering Department. </institution>
Reference-contexts: learn such behavior from training examples? If there are long time lags between relevant events and later error signals, then most analog gradient-based recurrent net learning algorithms, such as "Back-Propagation Through Time" (BBTT, e.g., (Rumelhart et al., 1986; Werbos, 1988; Williams and Zipser, 1992)) or "Real-Time Recurrent Learning" (RTRL, e.g., <ref> (Robinson and Fallside, 1987) </ref>) (see overviews by Williams, 1989; Pearlmutter, 1995), will not work. Their main problem is that error signals "flowing backwards in time" tend to decay exponentially, as was shown first by Hochreiter (1991). <p> S memory cells sharing the same input gate and the same output gate form a structure called a "memory cell block of size S". 2.3 Learning A a variant of RTRL <ref> (e.g., Robinson and Fallside 1987) </ref> which properly takes into account the altered, multiplicative dynamics caused by input and output gates can be used.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Sa lustowicz, R. P. and Schmidhuber, J. </author> <year> (1997). </year> <title> Probabilistic incremental program evolution. </title> <journal> Evolutionary Computation, </journal> <volume> 5(2) </volume> <pages> 123-141. </pages>
Reference-contexts: explains our interest in more sophisticated discrete methods searching incrementally for better sequence-processing algorithms such as Adaptive Levin Search (Wiering and Schmidhuber, 1996, 1997) based on Levin Search (Levin, 1973, 1984; Schmidhuber, 1997), Genetic Programming with memory cells (e.g., Teller, 1994), and Probabilistic Incremental Program Evolution (PIPE) with memory cells <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>. We will benchmark LSTM against PIPE and find that LSTM performs worse where there are "very long" (as opposed to merely "long") time lags in all training exemplars and there exist solutions with low AC. <p> There it is also experimentally shown that LSTM easily outperforms competing recurrent net approaches on both short and long time lag tasks. 3 Probabilistic Incremental Program Evolution (PIPE) Overview. PIPE is a recent discrete method for automatic program synthesis <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>. It combines probability vector coding of program instructions (Schmidhuber et al., 1997a; Schmidhuber et al., 1997b), Population-Based Incremental Learning (Baluja, 1994; Baluja and Caruana, 1995), and tree-coded programs like those used in variants of Genetic Programming (Cramer, 1985; Koza, 1992). <p> Note that we use the "standard" function set from <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>.
Reference: <author> Schmidhuber, J. </author> <year> (1997). </year> <title> Discovering neural nets with low Kolmogorov complexity and high generalization capability. </title> <booktitle> Neural Networks, </booktitle> <volume> 10(5) </volume> <pages> 857-873. </pages>
Reference-contexts: explains our interest in more sophisticated discrete methods searching incrementally for better sequence-processing algorithms such as Adaptive Levin Search (Wiering and Schmidhuber, 1996, 1997) based on Levin Search (Levin, 1973, 1984; Schmidhuber, 1997), Genetic Programming with memory cells (e.g., Teller, 1994), and Probabilistic Incremental Program Evolution (PIPE) with memory cells <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>. We will benchmark LSTM against PIPE and find that LSTM performs worse where there are "very long" (as opposed to merely "long") time lags in all training exemplars and there exist solutions with low AC. <p> There it is also experimentally shown that LSTM easily outperforms competing recurrent net approaches on both short and long time lag tasks. 3 Probabilistic Incremental Program Evolution (PIPE) Overview. PIPE is a recent discrete method for automatic program synthesis <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>. It combines probability vector coding of program instructions (Schmidhuber et al., 1997a; Schmidhuber et al., 1997b), Population-Based Incremental Learning (Baluja, 1994; Baluja and Caruana, 1995), and tree-coded programs like those used in variants of Genetic Programming (Cramer, 1985; Koza, 1992). <p> Note that we use the "standard" function set from <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>. <p> several analog approaches for the embedded Reber grammar: percentage of successful trials and number of sequence presentations until success for RTRL (results taken from Smith and Zipser 1989), "Elman net trained by Elman's procedure" (results taken from Cleeremans et al. 1989), "Recurrent Cascade-Correlation" (results taken from Fahlman 1991) and LSTM <ref> (results taken from Hochreiter and Schmidhuber 1997) </ref>. Only LSTM always learned to solve the task. It also needed least sequence presentations on average (mean of 30 trials).
Reference: <author> Schmidhuber, J. and Hochreiter, S. </author> <year> (1996). </year> <title> Guessing can outperform many long time lag algorithms. </title> <type> Technical Report IDSIA-19-96, </type> <institution> IDSIA. </institution>
Reference-contexts: Weight guessing is one of the simplest discrete methods. It will not solve nontrivial tasks (requiring many or precise parameters) in reasonable time. This explains our interest in more sophisticated discrete methods searching incrementally for better sequence-processing algorithms such as Adaptive Levin Search <ref> (Wiering and Schmidhuber, 1996, 1997) </ref> based on Levin Search (Levin, 1973, 1984; Schmidhuber, 1997), Genetic Programming with memory cells (e.g., Teller, 1994), and Probabilistic Incremental Program Evolution (PIPE) with memory cells (Sa lustowicz and Schmidhuber, 1997).
Reference: <author> Schmidhuber, J., Zhao, J., and Schraudolph, N. </author> <year> (1997a). </year> <title> Reinforcement learning with self-modifying policies. </title> <editor> In Thrun, S. and Pratt, L., editors, </editor> <booktitle> Learning to learn, </booktitle> <pages> pages 293-309. </pages> <publisher> Kluwer. </publisher>
Reference-contexts: Section 4 compares PIPE and LSTM on two long time lag tasks with low AC. Section 5 introduces filtering. Section 6 compares PIPE, LSTM and various other recurrent neural net approaches on a task with comparatively high AC. Section 7 concludes. 2 Long Short-Term Memory (LSTM) Overview. LSTM <ref> (Hochreiter and Schmidhuber, 1997a) </ref> is a recent, analog, gradient-based recurrent neural net approach for supervised learning of sequential processes. Unlike most alternative approaches it can learn from training sequences that do not exhibit any short time lags between relevant events. <p> Since unit j is usually not only connected to itself but also to other units, there is an "input weight conflict" and an "output weight conflict" (see <ref> (Hochreiter and Schmidhuber, 1997a) </ref> for details). <p> Only within memory cells, errors are propagated back through previous internal states s c j . See <ref> (Hochreiter and Schmidhuber, 1997a) </ref> for a detailed description of all learning rules. There it is also experimentally shown that LSTM easily outperforms competing recurrent net approaches on both short and long time lag tasks. 3 Probabilistic Incremental Program Evolution (PIPE) Overview. <p> In this section we compare PIPE and LSTM on two problems involving both long minimal time lags and low algorithmic complexity (AC). So far both the "adding problem" and the "temporal order problem" <ref> (Hochreiter and Schmidhuber, 1997a) </ref> have been solved by only one single analog method (LSTM). The adding experiments show that LSTM's convergence speed depends on time lag size, while PIPE's does not. We will see that sometimes simple ROLs suffice. <p> Results. The minimal time lag between the most recent occurrence of relevant information and the point of prediction varies from 25 to 500 time steps. Table 1 summarizes all results. LSTM results are taken from <ref> (Hochreiter and Schmidhuber, 1997a) </ref>. LSTM always finds perfect or almost perfect solutions. With a test set consisting of 2560 randomly chosen sequences, in all 10 independent trials LSTM's average test set error is below 0.01, and there are never more than 3 incorrectly processed sequences. <p> To correctly predict the symbol before last, the second symbol has to be remembered. Comparison. We compare PIPE with MCs and filtering to LSTM (results taken from <ref> (Hochreiter and Schmidhuber, 1997a) </ref>), "Elman nets trained by Elman's training procedure" (ELM) (results taken from Cleeremans et al. 1989), Fahlman's "Recurrent Cascade-Correlation" (RCC) (results taken from Fahlman 1991), and RTRL (results taken from Smith and Zipser (1989), where only the few successful trials are listed).
Reference: <author> Schmidhuber, J., Zhao, J., and Wiering, M. </author> <year> (1997b). </year> <title> Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. </title> <journal> Machine Learning, </journal> <volume> 28 </volume> <pages> 105-130. </pages>
Reference: <author> Smith, A. W. and Zipser, D. </author> <year> (1989). </year> <title> Learning sequential structures with the real-time recurrent learning algorithm. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(2) </volume> <pages> 125-131. </pages>
Reference-contexts: Approaches method hidden units % of success success after RTRL 12 "some fraction" 25,000 ELM 15 0 &gt;200,000 LSTM 3 blocks, size 2 100 8,440 Table 3: Results of several analog approaches for the embedded Reber grammar: percentage of successful trials and number of sequence presentations until success for RTRL <ref> (results taken from Smith and Zipser 1989) </ref>, "Elman net trained by Elman's procedure" (results taken from Cleeremans et al. 1989), "Recurrent Cascade-Correlation" (results taken from Fahlman 1991) and LSTM (results taken from Hochreiter and Schmidhuber 1997). Only LSTM always learned to solve the task.
Reference: <author> Solomonoff, R. </author> <year> (1986). </year> <title> An application of algorithmic probability to problems in artificial intelligence. </title> <editor> In Kanal, L. N. and Lemmer, J. F., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 473-491. </pages> <publisher> Elsevier Science Publishers. 16 Spector, </publisher> <editor> L. </editor> <year> (1996). </year> <title> Simultaneous evolution of programs and their control structures. </title> <editor> In Angeline, P. and K. E. Kinnear, J., editors, </editor> <booktitle> Advances in Genetic Programming 2, page Chapter 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, USA. </address>
Reference: <author> Teller, A. </author> <year> (1994). </year> <title> The evolution of mental models. </title> <editor> In Kinnear, Jr., K. E., editor, </editor> <booktitle> Advances in Genetic Programming, </booktitle> <pages> pages 199-219. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: This explains our interest in more sophisticated discrete methods searching incrementally for better sequence-processing algorithms such as Adaptive Levin Search (Wiering and Schmidhuber, 1996, 1997) based on Levin Search (Levin, 1973, 1984; Schmidhuber, 1997), Genetic Programming with memory cells <ref> (e.g., Teller, 1994) </ref>, and Probabilistic Incremental Program Evolution (PIPE) with memory cells (Sa lustowicz and Schmidhuber, 1997). <p> Function get O (arg 1 ) (get M (arg 1 )) returns OC (jround (arg 1 )j mod n OC ) (MC (jround (arg 1 )j mod n MC ) ) <ref> (see, e.g., Teller, 1994) </ref>. 3.5 Multiple Outputs Overview. Two methods are used to allow for vector-valued outputs: multiple output cells and multiple programs. Both can be used in combination with each other, memory cells, and/or recurrent output links. 7 Multiple Output Cells.
Reference: <author> Werbos, P. J. </author> <year> (1988). </year> <title> Generalization of backpropagation with application to a recurrent gas market model. Neural Networks, </title> <type> 1. </type>
Reference: <author> Wiering, M. A. and Schmidhuber, J. </author> <year> (1996). </year> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 534-542. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Weight guessing is one of the simplest discrete methods. It will not solve nontrivial tasks (requiring many or precise parameters) in reasonable time. This explains our interest in more sophisticated discrete methods searching incrementally for better sequence-processing algorithms such as Adaptive Levin Search <ref> (Wiering and Schmidhuber, 1996, 1997) </ref> based on Levin Search (Levin, 1973, 1984; Schmidhuber, 1997), Genetic Programming with memory cells (e.g., Teller, 1994), and Probabilistic Incremental Program Evolution (PIPE) with memory cells (Sa lustowicz and Schmidhuber, 1997).
Reference: <author> Williams, R. J. </author> <year> (1989). </year> <title> Complexity of exact gradient computation algorithms for recurrent neural networks. </title> <type> Technical Report Technical Report NU-CCS-89-27, </type> <institution> Boston: Northeastern University, College of Computer Science. </institution>
Reference: <author> Williams, R. J. and Peng, J. </author> <year> (1990). </year> <title> An efficient gradient-based algorithm for on-line training of recurrent network trajectories. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 491-501. </pages>
Reference-contexts: To ensure non-decaying error backprop through internal states of memory cells, as with truncated BPTT <ref> (e.g., Williams and Peng 1990) </ref>, errors arriving at "memory cell net inputs" (for a cell c j , this includes net c j , net in j , net out j ) do not get propagated back further in time.
Reference: <author> Williams, R. J. and Zipser, D. </author> <year> (1992). </year> <title> Gradient-based learning algorithms for recurrent networks and their computational complexity. In Back-propagation: Theory, Architectures and Applications. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher> <pages> 17 </pages>
References-found: 29

