URL: ftp://axon.cs.byu.edu/pub/papers/wilson.icannga97.rpnn.ps
Refering-URL: http://synapse.cs.byu.edu/~randy/misc/pubs.html
Root-URL: 
Email: E-mail: randy@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: Improved Center Point Selection for Probabilistic Neural Networks  
Author: D. Randall Wilson, Tony R. Martinez 
Address: Provo, UT 84602, U.S.A.  
Affiliation: Computer Science Department, Brigham Young University,  
Web: WWW: http://axon.cs.byu.edu  
Date: April 1997.  
Note: To appear in Proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms, (ICANNGA97),  Neural Network and Machine Learning Laboratory,  
Abstract: Probabilistic Neural Networks (PNN) typically learn more quickly than many neural network models and have had success on a variety of applications. However, in their basic form, they tend to have a large number of hidden nodes. One common solution to this problem is to keep only a randomly-selected subset of the original training data in building the network. This paper presents an algorithm called the Reduced Probabilistic Neural Network (RPNN) that seeks to choose a better-than-random subset of the available instances to use as center points of nodes in the network. The algorithm tends to retain non-noisy border points while removing nodes with instances in regions of the input space that are highly homogeneous. In experiments on 22 datasets, the RPNN had better average generalization accuracy than two other PNN models, while requiring an average of less than one-third the number of nodes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Specht, Donald F., </author> <title> Enhancements to Probabilistic Neural Networks, </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks (IJCNN92), </booktitle> <volume> 1, </volume> <pages> pp. 761 - 768, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Probabilistic Neural Networks (PNN) <ref> [1] </ref> often learn more quickly than many neural network models such as backpropagation networks [2], and have had success on a variety of applications. PNNs are a special form of radial basis function (RBF) network [3] used for classification.
Reference: [2] <author> Rumelhart, D. E., and J. L. McClelland, </author> <title> Parallel Distributed Processing, </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: 1 Introduction Probabilistic Neural Networks (PNN) [1] often learn more quickly than many neural network models such as backpropagation networks <ref> [2] </ref>, and have had success on a variety of applications. PNNs are a special form of radial basis function (RBF) network [3] used for classification. The network learns from a training set T, which is a collection of examples called instances.
Reference: [3] <editor> Wasserman, Philip D., </editor> <booktitle> Advanced Methods in Neural Computing, </booktitle> <address> New York, NY: </address> <publisher> Van Nostrand Reinhold, </publisher> <pages> pp. 147-176, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Probabilistic Neural Networks (PNN) [1] often learn more quickly than many neural network models such as backpropagation networks [2], and have had success on a variety of applications. PNNs are a special form of radial basis function (RBF) network <ref> [3] </ref> used for classification. The network learns from a training set T, which is a collection of examples called instances. Each instance i has an input vector y i , and an output class, denoted as class i .
Reference: [4] <author> Wilson, D. Randall, and Tony R. Martinez, </author> <title> Heterogeneous Radial Basis Functions, </title> <booktitle> Proceedings of the International Conference on Neural Networks (ICNN96) , 2, </booktitle> <pages> pp. 1263-1267, </pages> <year> 1996. </year>
Reference: [5] <author> Wilson, D. Randall, and Tony R. Martinez, </author> <title> Improved Heterogeneous Distance Functions, </title> <journal> Journal of Artificial Intelligence Research (JAIR), </journal> <volume> 6, 1, </volume> <pages> pp. 1-34, </pages> <year> 1997. </year>
Reference: [6] <author> Stanfill, C., and D. Waltz, </author> <title> Toward memory-based reasoning, </title> <journal> Communications of the ACM, </journal> <volume> 29, </volume> <year> 1986. </year>
Reference-contexts: By far the most common distance function used in PNNs is Euclidean distance. However, in order to appropriately handle applications that have both linear and nominal attributes, we use a heterogeneous distance function HVDM [4][5] that uses normalized Euclidean distance for linear attributes and the Value Difference Metric (VDM) <ref> [6] </ref> for nominal attributes. It is defined as follows: HVDM ( x, y) = d a 2 ( x a , y a ) m where m is the number of attributes. <p> is defined as: d a ( x, y) = 1, if x or y is unknown vdm a ( x, y), if a is nominal diff a ( x, y), if a is linear The function d a (x,y) uses the following function, based on the Value Difference Metric (VDM) <ref> [6] </ref> for nominal (discrete, unordered) attributes: vdm a ( x, y) = N a, x N a,y,c 2 C where N a,x is the number of times attribute a had value x; N a,x,c is the number of times attribute a had value x and the output class was c ;
Reference: [7] <author> Rawlings, J. O., </author> <title> Applied Regression Analysis. </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Pacific Grove, CA, </address> <year> 1988. </year>
Reference-contexts: However, arbitrarily removing instances can reduce generalization accuracy. In addition, it is difficult to know how many nodes can be safely removed without a reasonable stopping criterion. Other subset selection algorithms exist in linear regression theory <ref> [7] </ref>, including forward selection , in which the network starts with no nodes and nodes are added one at a time to the network. Another method that has been used [8] is k-means clustering [9].
Reference: [8] <author> MacQueen, J., </author> <title> Some methods for classification and analysis of multivariate observations, </title> <booktitle> in Proceedings of the Fifth Berkeley Symposium on Mathematics, Statistics and Probability, </booktitle> <address> Berkeley, CA, </address> <pages> pp. 281-297, </pages> <year> 1967. </year>
Reference-contexts: Other subset selection algorithms exist in linear regression theory [7], including forward selection , in which the network starts with no nodes and nodes are added one at a time to the network. Another method that has been used <ref> [8] </ref> is k-means clustering [9]. This paper presents an algorithm called the Reduced Probabilistic Neural Network (RPNN) that begins with all of the available training instances as node centers and selectively removes them one at a time until classification accuracy suffers.
Reference: [9] <author> Leonard, J. A., M. A. Kramer, and L. H. Ungar, </author> <title> Using Radial Basis Functions to Approximate a Function and Its Error Bounds, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3, 4, </volume> <pages> pp. 624-627, </pages> <year> 1992 </year>
Reference-contexts: Other subset selection algorithms exist in linear regression theory [7], including forward selection , in which the network starts with no nodes and nodes are added one at a time to the network. Another method that has been used [8] is k-means clustering <ref> [9] </ref>. This paper presents an algorithm called the Reduced Probabilistic Neural Network (RPNN) that begins with all of the available training instances as node centers and selectively removes them one at a time until classification accuracy suffers.
Reference: [10] <author> Merz, C. J., and P. M. Murphy, </author> <title> UCI Repository of Machine Learning Databases . Irvine, </title> <institution> CA: University of California Irvine, Department of Information and Computer Science, </institution> <year> 1996. </year> <title> Internet: </title> <address> http://www.ics.uci.edu/~mlearn/ MLRepository.html. </address>
Reference-contexts: during the noise-reduction pass, but would prevent almost any pruning from taking place if used in the remainder of the algorithm. 3 Empirical Results The Reduced Probabilistic Neural Network (RPNN) algorithm was implemented and tested on 22 applications from the Machine Learning Database Repository at the University of California, Irvine <ref> [10] </ref>. Each test consisted of ten trials. Each trial consisted of learning from 90% of the training instances, and then seeing how many of the remaining 10% of the instances were classified correctly.
References-found: 10

