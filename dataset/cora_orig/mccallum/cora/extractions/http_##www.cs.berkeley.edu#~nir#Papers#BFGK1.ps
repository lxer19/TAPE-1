URL: http://www.cs.berkeley.edu/~nir/Papers/BFGK1.ps
Refering-URL: http://http.cs.berkeley.edu/~nir/publications.html
Root-URL: 
Email: cebly@cs.ubc.ca  nir@cs.stanford.edu  moises@erg.sri.com  koller@cs.stanford.edu  
Title: Context-Specific Independence in Bayesian Networks  
Author: Craig Boutilier Nir Friedman Moises Goldszmidt Daphne Koller 
Address: Vancouver, BC V6T 1Z4  Stanford, CA 94305-9010  333 Ravenswood Way, EK329 Menlo Park, CA 94025  Stanford, CA 94305-9010  
Affiliation: Dept. of Computer Science University of British Columbia  Dept. of Computer Science Stanford University  SRI International  Dept. of Computer Science Stanford University  
Abstract: Bayesian networks provide a language for qualitatively representing the conditional independence properties of a distribution. This allows a natural and compact representation of the distribution, eases knowledge acquisition, and supports effective inference algorithms. It is well-known, however, that there are certain independencies that we cannot capture qualitatively within the Bayesian network structure: independencies that hold only in certain contexts, i.e., given a specific assignment of values to certain variables. In this paper, we propose a formal notion of context-specific independence (CSI), based on regularities in the conditional probability tables (CPTs) at a node. We present a technique, analogous to (and based on) d-separation, for determining when such independence holds in a given network. We then focus on a particular qualitative representation schemetree-structured CPTs for capturing CSI. We suggest ways in which this representation can be used to support effective inference algorithms. In particular, we present a structural decomposition of the resulting network which can improve the performance of clustering algorithms, and an alternative algorithm based on cutset conditioning.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Becker and D. Geiger. </author> <title> Approximation algorithms for the loop cutset problem. </title> <booktitle> In UAI-94, </booktitle> <pages> pp. 60-68, </pages> <year> 1994. </year>
Reference-contexts: It is therefore crucial to find good heuristic algorithms for constructing small conditional cutsets. We focus on a computationally intensive heuristic approach that exploits CSI and the existence of vacuous arcs maximally. This algorithm constructs conditional cutsets incrementally, in a fashion similar to standard heuristic approaches to the problem <ref> [20, 1] </ref>. We discuss computationally-motivated shortcuts near the end of this section. <p> The standard greedy approach to cutset construction selects nodes for the cutset according to the heuristic value w (X) d (X) , where the weight w (X) of variable X is log (jval (X)j) and d (X) is the out-degree of X in the network graph <ref> [20, 1] </ref>. 6 The weight measures the work needed to instantiate X in a cutset, while the degree of a vertex gives an idea of its arc-cutting potentialmore incident outgoing edges mean a larger chance to cut loops. <p> See <ref> [1] </ref> for details. myopic. In particular, it ignores the potential for arcs to be cut subsequently. For example, consider the family in Figure 2, with Tree 2 reflecting the CPT for X. <p> Step 1 of the algorithm is standard <ref> [20, 1] </ref>. In Step 2, it is important to realize that the heuristic value of X is determined with respect to the current network and the context already established in the existing branch of the cutset.
Reference: [2] <author> C. Boutilier, R. Dearden, and M. Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In IJCAI-95, </booktitle> <pages> pp. 1104-1111, </pages> <year> 1995. </year>
Reference-contexts: Well-known examples include Heckerman's [9] similarity networks (and the related multinets [7]), the use of asymmetric representations for decision making [18, 6] and Poole's [16] use of probabilistic Horn rules to encode dependencies between variables. Even the representation we emphasize (decision trees) have been used to encode CPTs <ref> [2, 8] </ref>. The intent of this work is to formalize the notion of CSI, to study its representation as part of a more general framework, and to propose methods for utilizing these representations to enhance probabilistic inference algorithms. <p> Testimony to this fact is the work on adding additional structure to influence diagrams by Smith et al. [18], Fung and Shachter [6], and the work by Boutilier et al <ref> [2] </ref> on using decision trees to represent CPTs in the context of Markov Decision Processes. There are a number of future research directions that are needed to elaborate the ideas presented here, and to expand the role that CSI and compact CPT representations play in probabilistic reasoning.
Reference: [3] <author> R. E. Bryant. </author> <title> Graph-based algorithms for boolean function manipulation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(8):677-691, </volume> <year> 1986. </year>
Reference-contexts: For the purposes of this paper, we focus on CPT-trees tree-structured CPTs, deferring discussion of analogous results for CNF representations and graph-structured CPTs (of the form discussed by <ref> [3] </ref>) to a longer version of this paper. A major advantage of tree structures is their naturalness, with branch labels corresponding in some sense to rule structure (see Figure 1). This intuition makes it particularly easy to elicit probabilities directly from a human expert.
Reference: [4] <author> A. Darwiche. </author> <title> Conditioning algorithms for exact and approximate inference in causal networks. </title> <booktitle> In UAI-95, </booktitle> <pages> pp. 99-107, </pages> <year> 1995. </year>
Reference-contexts: One involves developing less ideal but more tractable methods of conditional cutset construction. For example, we might select a cutset by standard means, and use the considerations described above to order (on-line) the variable instantiations within this cutset. Another direction involves integrating these ideas with the computation-saving ideas of <ref> [4] </ref> for standard cutset algorithms. 5 Concluding Remarks We have defined the notion of context-specific independence as a way of capturing the independencies induced by specific variable assignments, adding to the regularities in distributions representable in BNs. Our results provide foundations for CSI, its representation and its role in inference.
Reference: [5] <author> N. Friedman and M. Goldszmidt. </author> <title> Learning Bayesian networks with local structure. </title> <booktitle> In UAI '96, </booktitle> <year> 1996. </year>
Reference-contexts: Structured representation of CPTs have also proven beneficial in learning Bayesian networks from data <ref> [5] </ref>. Due to the compactness of the representation, learning procedures are capable of inducing networks that better emulate the true complexity of the interactions present in the data. This paper represents a starting point for a rigorous extension of Bayesian network representations to incorporate context-specific independence.
Reference: [6] <author> R. M. Fung and R. D. Shachter. </author> <title> Contingent influence diagrams. </title> <type> Unpublished manuscript, </type> <year> 1990. </year>
Reference-contexts: We are certainly not the first to suggest extensions to the BN representation in order to capture additional independencies and (potentially) enhance inference. Well-known examples include Heckerman's [9] similarity networks (and the related multinets [7]), the use of asymmetric representations for decision making <ref> [18, 6] </ref> and Poole's [16] use of probabilistic Horn rules to encode dependencies between variables. Even the representation we emphasize (decision trees) have been used to encode CPTs [2, 8]. <p> The effects of actions (or decisions) usually only take place for specific instantiation of some variables, and are vacuous or trivial when these in-stantiations are not realized. Testimony to this fact is the work on adding additional structure to influence diagrams by Smith et al. [18], Fung and Shachter <ref> [6] </ref>, and the work by Boutilier et al [2] on using decision trees to represent CPTs in the context of Markov Decision Processes.
Reference: [7] <author> D. Geiger and D. Heckerman. </author> <booktitle> Advances in probabilistic reasoning. In UAI-91, </booktitle> <pages> pp. 118-126, </pages> <year> 1991. </year>
Reference-contexts: We are certainly not the first to suggest extensions to the BN representation in order to capture additional independencies and (potentially) enhance inference. Well-known examples include Heckerman's [9] similarity networks (and the related multinets <ref> [7] </ref>), the use of asymmetric representations for decision making [18, 6] and Poole's [16] use of probabilistic Horn rules to encode dependencies between variables. Even the representation we emphasize (decision trees) have been used to encode CPTs [2, 8]. <p> Yet, our approach is significantly different in that we try to capture the additional independencies by providing a structured representation of the CPTs within a single network, while similarity networks and multinets <ref> [9, 7] </ref> rely on a family of networks. In fact the approach we described based on decision trees is closer in spirit to that of Poole's rule-based representations of networks [16].
Reference: [8] <author> S. Glesner and D. Koller. </author> <title> Constructing flexible dynamic belief networks from first-order probabilistic knowledge bases. </title> <booktitle> In ECSQARU '95, </booktitle> <pages> pp. 217-226. </pages> <year> 1995. </year>
Reference-contexts: Well-known examples include Heckerman's [9] similarity networks (and the related multinets [7]), the use of asymmetric representations for decision making [18, 6] and Poole's [16] use of probabilistic Horn rules to encode dependencies between variables. Even the representation we emphasize (decision trees) have been used to encode CPTs <ref> [2, 8] </ref>. The intent of this work is to formalize the notion of CSI, to study its representation as part of a more general framework, and to propose methods for utilizing these representations to enhance probabilistic inference algorithms.
Reference: [9] <author> D. Heckerman. </author> <title> Probabilistic Similarity Networks. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1990. </year>
Reference-contexts: We are certainly not the first to suggest extensions to the BN representation in order to capture additional independencies and (potentially) enhance inference. Well-known examples include Heckerman's <ref> [9] </ref> similarity networks (and the related multinets [7]), the use of asymmetric representations for decision making [18, 6] and Poole's [16] use of probabilistic Horn rules to encode dependencies between variables. Even the representation we emphasize (decision trees) have been used to encode CPTs [2, 8]. <p> As we mentioned in the introduction, there has been considerable work on extending the BN representation to capture additional independencies. Our notion of CSI is related to what Heckerman calls subset independence in his work on similarity networks <ref> [9] </ref>. Yet, our approach is significantly different in that we try to capture the additional independencies by providing a structured representation of the CPTs within a single network, while similarity networks and multinets [9, 7] rely on a family of networks. <p> Yet, our approach is significantly different in that we try to capture the additional independencies by providing a structured representation of the CPTs within a single network, while similarity networks and multinets <ref> [9, 7] </ref> rely on a family of networks. In fact the approach we described based on decision trees is closer in spirit to that of Poole's rule-based representations of networks [16].
Reference: [10] <author> D. Heckerman. </author> <title> Causal independence for knowledge acquisition and inference. </title> <booktitle> In UAI-93, </booktitle> <pages> pp. 122-137, </pages> <year> 1993. </year>
Reference-contexts: For instance, noisy-or distributions (or generalizations [19]) allow compact representation by assuming that the parents of X make independent casual contributions to the value of X. These distributions fall into the general category of distributions satisfying causal independence <ref> [10, 11] </ref>. For such distributions, we can perform a structural transformation on our original network, resulting in a new network where many of these independencies are encoded qualitatively within the network structure.
Reference: [11] <author> D. Heckerman and J. S. Breese. </author> <title> A new look at causal independence. </title> <booktitle> In UAI-94, </booktitle> <pages> pp. 286-292, </pages> <year> 1994. </year>
Reference-contexts: For instance, noisy-or distributions (or generalizations [19]) allow compact representation by assuming that the parents of X make independent casual contributions to the value of X. These distributions fall into the general category of distributions satisfying causal independence <ref> [10, 11] </ref>. For such distributions, we can perform a structural transformation on our original network, resulting in a new network where many of these independencies are encoded qualitatively within the network structure. <p> For such distributions, we can perform a structural transformation on our original network, resulting in a new network where many of these independencies are encoded qualitatively within the network structure. Essentially, the transformation introduces auxiliary variables into the network, then connects them via a cascading sequence of deterministic or-nodes <ref> [11] </ref>. While CSI is quite distinct from causal independence, similar ideas can be applied: a structural network transformation can be used to capture certain aspects of CSI directly within the BN-structure. Such transformations can be very useful when one uses an inference algorithm based on clustering [13]. <p> Thus, in general, we want to stop the decomposition when the CPT of a node is a full tree. (Note that this includes leaves a special case.) As in the structural transformation for noisy-or nodes of <ref> [11] </ref>, our decomposition can allow clustering algorithms to form smaller cliques. After the transformation, we have many more nodes in the network (on the order of the size of all CPT tree representations), but each generally has far fewer parents. <p> Such nodes can be further exploited in the clustering algorithm [12]. We note that the reduction in clique size (and the resulting computational savings) depend heavily on the structure of the decision trees. A similar phenomenon occurs in the transformation of <ref> [11] </ref>, where the effectiveness depends on the order in which we choose to cascade the different parents of the node. As in the case of noisy-or, the graphical structure of our (transformed) BN cannot capture all independencies implicit in the CPTs.
Reference: [12] <author> F. Jensen and S. Andersen. </author> <title> Approximations in Bayesian belief universes for knowledge-based systems. </title> <booktitle> In UAI-90, </booktitle> <pages> pp. 162-169, </pages> <year> 1990. </year>
Reference-contexts: We are currently working on implementing these ideas, and testing their effectiveness in practice. We also note that a large fraction of the auxiliary nodes we introduce are multiplexer nodes, which are deterministic function of their parents. Such nodes can be further exploited in the clustering algorithm <ref> [12] </ref>. We note that the reduction in clique size (and the resulting computational savings) depend heavily on the structure of the decision trees.
Reference: [13] <author> S. L. Lauritzen and D. J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B 50(2) </volume> <pages> 157-224, </pages> <year> 1988. </year>
Reference-contexts: While CSI is quite distinct from causal independence, similar ideas can be applied: a structural network transformation can be used to capture certain aspects of CSI directly within the BN-structure. Such transformations can be very useful when one uses an inference algorithm based on clustering <ref> [13] </ref>. Roughly speaking, clustering algorithms construct a join tree, whose nodes denote (overlapping) clusters of variables in the original BN. Each cluster, or clique, encodes the marginal distribution over the set val (X ) of the nodes X in the cluster.
Reference: [14] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: A BN encodes the following statement of independence about each random variable: a variable is independent of its non-descendants in the network given the state of its parents <ref> [14] </ref>. For example, in the network shown in Figure 1, Z is independent of U , V and Y given X and W . Further independence statements that follow from these local statements can be read from the network structure, in polynomial time, using a graph-theoretic criterion called d-separation [14]. <p> parents <ref> [14] </ref>. For example, in the network shown in Figure 1, Z is independent of U , V and Y given X and W . Further independence statements that follow from these local statements can be read from the network structure, in polynomial time, using a graph-theoretic criterion called d-separation [14]. In addition to representing statements of independence, a BN also represents a particular distribution (that satisfies all the independencies). This distribution is specified by a set of conditional probability tables (CPTs). <p> These statements are local, in that they involve only a node and its parents in B. Other I () statements, involving arbitrary sets of variables, follow from these local assertions. These can be read from the structure of B using a graph-theoretic path criterion called d-separation <ref> [14] </ref> that can be tested in polynomial time. A BN B represents independence information about a particular distribution P . Thus, we require that the independencies encoded in B hold for P . <p> B for P permits a compact representation of the distribution: we need only specify, for each variable X i , a conditional probability table (CPT) encoding a parameter P (x i j x i ) for each possible value of the variables in fX i ; X i g. (See <ref> [14] </ref> for details.) The graphical structure of the BN can only capture independence relations of the form I (X ; Y j Z), that is, independencies that hold for any assignment of values to the variables in Z. <p> The use of static precompilation makes it difficult for the algorithm to take advantage of independencies that only occur in certain circumstances, e.g., as new evidence arrives. More dynamic algorithms, such as cutset conditioning <ref> [14] </ref>, can exploit context-specific independencies more effectively. We investigate below how cutset algorithms can be modified to exploit CSI using our decision-tree representation. 3 The cutset conditioning algorithm works roughly as follows. We select a cutset, i.e., a set of variables that, once instantiated, render the network singly connected. <p> Inference is then carried out using reasoning by cases, where each case is a possible assignment to the variables in the cutset C. Each such assignment is instantiated as evidence in a call to the polytree algorithm <ref> [14] </ref>, which performs inference on the resulting network. The results of these calls are combined to give the final answer. The running time is largely determined by the number of calls to the polytree algorithm (i.e., jval (C)j).
Reference: [15] <author> J. Pearl. </author> <title> A probabilistic calculus of action. </title> <booktitle> In UAI-94, </booktitle> <pages> pp. 454-462, </pages> <year> 1994. </year>
Reference-contexts: In fact the approach we described based on decision trees is closer in spirit to that of Poole's rule-based representations of networks [16]. The arc-cutting technique and network transformation introduced in Section 2 is reminiscent of the network transformations introduced by Pearl in his probabilistic calculus of action <ref> [15] </ref>. Indeed the semantics of actions proposed in that paper can be viewed as an instance of CSI. This is not a mere coincidence, as it is easy to see that networks representing plans and influence diagrams usually contain a significant amount of CSI.
Reference: [16] <author> D. Poole. </author> <title> Probabilistic Horn abduction and Bayesian networks. </title> <journal> Artificial Intelligence, </journal> <volume> 64(1) </volume> <pages> 81-129, </pages> <year> 1993. </year>
Reference-contexts: We are certainly not the first to suggest extensions to the BN representation in order to capture additional independencies and (potentially) enhance inference. Well-known examples include Heckerman's [9] similarity networks (and the related multinets [7]), the use of asymmetric representations for decision making [18, 6] and Poole's <ref> [16] </ref> use of probabilistic Horn rules to encode dependencies between variables. Even the representation we emphasize (decision trees) have been used to encode CPTs [2, 8]. <p> In fact the approach we described based on decision trees is closer in spirit to that of Poole's rule-based representations of networks <ref> [16] </ref>. The arc-cutting technique and network transformation introduced in Section 2 is reminiscent of the network transformations introduced by Pearl in his probabilistic calculus of action [15]. Indeed the semantics of actions proposed in that paper can be viewed as an instance of CSI.
Reference: [17] <author> J. R. Quinlan. C45: </author> <title> Programs for Machince Learning. </title> <publisher> Mor-gan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: As we show in subsequent sections, the tree structure can also be utilized to speed up BN inference algorithms. Finally, as we discuss in the conclusion, trees are also amenable to well-studied approximation and learning methods <ref> [17] </ref>. In this section, we show that they admit fast algorithms for detecting CSI. <p> Tree representations turn out to be particularly suitable in this regard. In particular, we show that decision-tree construction algorithms from the machine learning community can be used to construct an appropriate CPT-tree from a full conditional probability table; pruning algorithms <ref> [17] </ref> can then be used on this tree, or on one acquired directly from the user, to simplify the CPT-tree in order to allow for faster inference. Structured representation of CPTs have also proven beneficial in learning Bayesian networks from data [5].
Reference: [18] <author> J. E. Smith, S. Holtzman, and J. E. Matheson. </author> <title> Structuring conditional relationships in influence diagrams. </title> <journal> Operations Research, </journal> <volume> 41(2) </volume> <pages> 280-297, </pages> <year> 1993. </year>
Reference-contexts: We are certainly not the first to suggest extensions to the BN representation in order to capture additional independencies and (potentially) enhance inference. Well-known examples include Heckerman's [9] similarity networks (and the related multinets [7]), the use of asymmetric representations for decision making <ref> [18, 6] </ref> and Poole's [16] use of probabilistic Horn rules to encode dependencies between variables. Even the representation we emphasize (decision trees) have been used to encode CPTs [2, 8]. <p> The effects of actions (or decisions) usually only take place for specific instantiation of some variables, and are vacuous or trivial when these in-stantiations are not realized. Testimony to this fact is the work on adding additional structure to influence diagrams by Smith et al. <ref> [18] </ref>, Fung and Shachter [6], and the work by Boutilier et al [2] on using decision trees to represent CPTs in the context of Markov Decision Processes.
Reference: [19] <author> S. Srinivas. </author> <title> A generalization of the noisy-or model. </title> <booktitle> In UAI-93, </booktitle> <pages> pp. 208-215, </pages> <year> 1993. </year>
Reference-contexts: For instance, noisy-or distributions (or generalizations <ref> [19] </ref>) allow compact representation by assuming that the parents of X make independent casual contributions to the value of X. These distributions fall into the general category of distributions satisfying causal independence [10, 11].
Reference: [20] <author> J. Stillman. </author> <title> On heuristics for finding loop cutsets in multiply connected belief networks. </title> <booktitle> In UAI-90, </booktitle> <pages> pp. 265-272, </pages> <year> 1990. </year>
Reference-contexts: It is therefore crucial to find good heuristic algorithms for constructing small conditional cutsets. We focus on a computationally intensive heuristic approach that exploits CSI and the existence of vacuous arcs maximally. This algorithm constructs conditional cutsets incrementally, in a fashion similar to standard heuristic approaches to the problem <ref> [20, 1] </ref>. We discuss computationally-motivated shortcuts near the end of this section. <p> The standard greedy approach to cutset construction selects nodes for the cutset according to the heuristic value w (X) d (X) , where the weight w (X) of variable X is log (jval (X)j) and d (X) is the out-degree of X in the network graph <ref> [20, 1] </ref>. 6 The weight measures the work needed to instantiate X in a cutset, while the degree of a vertex gives an idea of its arc-cutting potentialmore incident outgoing edges mean a larger chance to cut loops. <p> Step 1 of the algorithm is standard <ref> [20, 1] </ref>. In Step 2, it is important to realize that the heuristic value of X is determined with respect to the current network and the context already established in the existing branch of the cutset.
Reference: [21] <author> J. Suermondt and G. Cooper. </author> <title> Initialization for the method of conditioning in bayesian belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 50 </volume> <pages> 83-94, </pages> <year> 1991. </year>
Reference-contexts: vacuous by X (averaging over values of X), is reasonably straightforward, but unfortunately is somewhat 4 We explain the need for set-valued arc labels below. 5 As in the standard cutset algorithm, the weights required to combine the answers from the different cases can be obtained from the polytree computations <ref> [21] </ref>. 6 We assume that the network has been preprocessed by node-splitting so that legitimate cutsets can be selected easily. See [1] for details. myopic. In particular, it ignores the potential for arcs to be cut subsequently.
References-found: 21

