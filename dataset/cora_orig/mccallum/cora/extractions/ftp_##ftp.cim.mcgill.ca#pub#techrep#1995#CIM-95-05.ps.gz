URL: ftp://ftp.cim.mcgill.ca/pub/techrep/1995/CIM-95-05.ps.gz
Refering-URL: ftp://ftp.cim.mcgill.ca/pub/techrep/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: bolduc@cim.mcgill.edu  Email: cim@cim.mcgill.ca  
Phone: Telephone: (514) 398-6319 Telex: 05 268510 FAX: (514) 398-7348  
Title: A Review of Biologically-Motivated Space-Variant Data Reduction Models for Robotic Vision  
Author: Marc Bolduc and Martin D. Levine 
Note: Accepted for publication, Computer Vision and Image Understanding (CVIU),  
Date: April 1995, Last Updated: September 1996  October 1996  
Address: St., Montreal, Quebec, Canada H3A 2A7  Montreal, Quebec, Canada  Address: 3480 University Street, Montreal, Quebec, Canada H3A 2A7  
Affiliation: Centre for Intelligent Machines McGill University, 3480 University  Centre for Intelligent Machines McGill University  Postal  
Pubnum: TR-CIM-95-05  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. L. Schwartz, D. N. Greve, and G. Bonmassar, </author> <title> "Space-variant active vision: Definition, overview and examples," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, no. 7/8, </volume> <pages> pp. 1297-1308, </pages> <year> 1995. </year>
Reference-contexts: In the case of primate vision, it is estimated that given the number of photoreceptors in the retina, if data reduction were not present, our brain would have to be a few orders of magnitude larger, and would weigh at least 5000 lbs <ref> [1] </ref>. The main motivation for using a retina-like data reduction technique for robotic vision is that the resulting images are much smaller than the original camera image (20 times in the above example). The latter suggests the possibility of reducing processing time in vision systems. <p> It consists of a small CCD camera mounted on a novel pan-tilt mechanism. The input-to-output pixel correspondences are performed using a run-length technique [55]. A new version of this system, the Cortex-II, has been reported; it is mounted on a miniature robot <ref> [1] </ref>. In these systems, overlapping receptive fields are not supported. 4. Overlapping Circular Receptive Field Models The differences between this second class of retino-cortical data reduction models and conformal mapping models are (i) that the receptive fields are circular, and (ii) that adjacent receptive fields overlap.
Reference: [2] <author> C. F. R. Weiman, </author> <title> "3-d sensing with polar exponential sensor arrays," </title> <booktitle> in SPIE Digital and Optical Shape Representation and Pattern Recognition, </booktitle> <volume> vol. 938, </volume> <pages> pp. 78-87, </pages> <year> 1988. </year>
Reference-contexts: The latter suggests the possibility of reducing processing time in vision systems. This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion <ref> [2, 3, 4, 5, 6, 7, 8] </ref>; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing [16, 17].
Reference: [3] <author> C. F. Weiman and R. D. Juday, </author> <title> "Tracking algorithms for log-polar mapped image coordinates," </title> <booktitle> SPIE Intelligent Robots and Computer Vision VIII: Algorithms and Techniques, </booktitle> <volume> vol. 1192, </volume> <pages> pp. 843-853, </pages> <year> 1989. </year>
Reference-contexts: The latter suggests the possibility of reducing processing time in vision systems. This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion <ref> [2, 3, 4, 5, 6, 7, 8] </ref>; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing [16, 17]. <p> This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion [2, 3, 4, 5, 6, 7, 8]; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking <ref> [3, 6, 13, 14, 15] </ref>; frameworks for space-variant image processing [16, 17]. In designing a robotic visual system based on the primate retina, the first step consists of selecting a model for retino-cortical data reduction. <p> In this case, the fovea is also divided using a log-polar grid except that the number of pixels per ring decreases towards the centre of the fovea. This placement of receptive fields is similar to the one used in [49]. Digital implementations of this model include the TRC Mapper <ref> [50, 51, 3] </ref> and the TI/JSC Programmable Remapper [52]. The TRC Mapper uses 256 x 256 input images and is capable of video rates (30 Hz). It transforms input pixel coordinates to peripheral coordinates with the help of a preprogrammed look-up table (LUT).
Reference: [4] <author> S. L. Bartlett and R. Jain, </author> <title> "Motion stereo and and ego-motion complex logarithmic mapping (eclm)," </title> <booktitle> in SPIE Digital and Optical Shape Representation and Pattern Recognition, </booktitle> <volume> vol. 938, </volume> <pages> pp. 138-145, </pages> <year> 1988. </year>
Reference-contexts: The latter suggests the possibility of reducing processing time in vision systems. This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion <ref> [2, 3, 4, 5, 6, 7, 8] </ref>; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing [16, 17].
Reference: [5] <author> N. C. Griswold and C. F. R. Weiman, </author> <title> "A modification of the fusion model for log polar coordinates," </title> <booktitle> in SPIE Intelligent Robots and Computer Vision VIII: Algorithms and Techniques, </booktitle> <volume> vol. 1192, </volume> <pages> pp. 854-866, </pages> <year> 1989. </year>
Reference-contexts: The latter suggests the possibility of reducing processing time in vision systems. This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion <ref> [2, 3, 4, 5, 6, 7, 8] </ref>; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing [16, 17].
Reference: [6] <editor> G. Sandini and M. Tistarelli, </editor> <booktitle> "Vision and space-variant sensing," in Human and Machine Perception, vol. 1 of Neural Networks for Perception, </booktitle> <pages> pp. 398-425, </pages> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: The latter suggests the possibility of reducing processing time in vision systems. This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion <ref> [2, 3, 4, 5, 6, 7, 8] </ref>; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing [16, 17]. <p> This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion [2, 3, 4, 5, 6, 7, 8]; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking <ref> [3, 6, 13, 14, 15] </ref>; frameworks for space-variant image processing [16, 17]. In designing a robotic visual system based on the primate retina, the first step consists of selecting a model for retino-cortical data reduction.
Reference: [7] <author> M. Tistarelli and G. </author> <title> Sandini, "On the advantages of polar and log-polar mapping for direct estimation of time to impact from optical flow," </title> <journal> IEEE transactions on pattern analysis and machine intelligence, </journal> <volume> vol. 14, </volume> <pages> pp. 402-410, </pages> <month> april </month> <year> 1993. </year>
Reference-contexts: The latter suggests the possibility of reducing processing time in vision systems. This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion <ref> [2, 3, 4, 5, 6, 7, 8] </ref>; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing [16, 17].
Reference: [8] <author> K. Daniilidis and V. Kruger, </author> <title> "Optical flow computation in the log-polar plane," </title> <booktitle> in Proc. Int. Conf. on computer analysis of images and patterns CAIP, </booktitle> <pages> pp. 65-72, </pages> <year> 1995. </year>
Reference-contexts: The latter suggests the possibility of reducing processing time in vision systems. This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion <ref> [2, 3, 4, 5, 6, 7, 8] </ref>; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing [16, 17].
Reference: [9] <author> J. I. Minnix, E. S. McVey, and R. M. Inigo, </author> <title> "Rotation and scale invariant pattern recognition using a multistaged neural network," </title> <booktitle> in SPIE Visual Communications and Image Processing '91: Image Processing, </booktitle> <volume> vol. 1606, </volume> <pages> pp. 241-251, </pages> <year> 1991. </year> <title> 22 A Review of Biologically-Motivated Space-Variant Data Reduction Models for Robotic Vision </title>
Reference-contexts: The latter suggests the possibility of reducing processing time in vision systems. This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion [2, 3, 4, 5, 6, 7, 8]; object recognition <ref> [9, 10, 11, 12] </ref>; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing [16, 17]. In designing a robotic visual system based on the primate retina, the first step consists of selecting a model for retino-cortical data reduction.
Reference: [10] <author> G. Sandini and V. Tagliasco, </author> <title> "Form-invariant topological mapping strategy for 2d shape recognition," Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 30, </volume> <pages> pp. 169-188, </pages> <year> 1983. </year>
Reference-contexts: The latter suggests the possibility of reducing processing time in vision systems. This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion [2, 3, 4, 5, 6, 7, 8]; object recognition <ref> [9, 10, 11, 12] </ref>; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing [16, 17]. In designing a robotic visual system based on the primate retina, the first step consists of selecting a model for retino-cortical data reduction.
Reference: [11] <author> L. Massone, G. Sandini, and V. Tagliasco, </author> <title> ""form-invariant" topological mapping strategy for 2d shape recognition," </title> <journal> CVGIP, </journal> <volume> vol. 30, </volume> <pages> pp. 169-188, </pages> <year> 1985. </year>
Reference-contexts: The latter suggests the possibility of reducing processing time in vision systems. This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion [2, 3, 4, 5, 6, 7, 8]; object recognition <ref> [9, 10, 11, 12] </ref>; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing [16, 17]. In designing a robotic visual system based on the primate retina, the first step consists of selecting a model for retino-cortical data reduction.
Reference: [12] <author> H. Wechler and G. L. Simmerman, </author> <title> "2-d invariand object recegnition using distributed assiciated memory," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 10, </volume> <pages> pp. 811-822, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: The latter suggests the possibility of reducing processing time in vision systems. This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion [2, 3, 4, 5, 6, 7, 8]; object recognition <ref> [9, 10, 11, 12] </ref>; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing [16, 17]. In designing a robotic visual system based on the primate retina, the first step consists of selecting a model for retino-cortical data reduction.
Reference: [13] <author> J. G. Bailey and R. A. Messner, </author> <title> "Docking target design and spacecraft tracking system stability," </title> <booktitle> in SPIE Intelligent Robots and Computer Vision VIII: Algorithms and Techniques, </booktitle> <volume> vol. 1192, </volume> <pages> pp. 820-831, </pages> <year> 1990. </year>
Reference-contexts: This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion [2, 3, 4, 5, 6, 7, 8]; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking <ref> [3, 6, 13, 14, 15] </ref>; frameworks for space-variant image processing [16, 17]. In designing a robotic visual system based on the primate retina, the first step consists of selecting a model for retino-cortical data reduction.
Reference: [14] <author> Y. Yeshurun and E. L. Schwartz, </author> <title> "Shape description with a space-variant sensor: Algorithms for scan-path, fusion, and convergence over multiple scans," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 11, </volume> <pages> pp. 1217-1222, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion [2, 3, 4, 5, 6, 7, 8]; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking <ref> [3, 6, 13, 14, 15] </ref>; frameworks for space-variant image processing [16, 17]. In designing a robotic visual system based on the primate retina, the first step consists of selecting a model for retino-cortical data reduction.
Reference: [15] <author> H. Yamamoto, Y. Yeshurun, and M. D. Levine, </author> <title> "An active foveated vision system: Attentional mechanisms and scan path convergence measures," </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <volume> vol. 63, </volume> <pages> pp. 50-65, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: This potential has led to the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion [2, 3, 4, 5, 6, 7, 8]; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking <ref> [3, 6, 13, 14, 15] </ref>; frameworks for space-variant image processing [16, 17]. In designing a robotic visual system based on the primate retina, the first step consists of selecting a model for retino-cortical data reduction. <p> The Fovia Implementation of Wilson's Model. Yamamoto et al. have implemented Wilson's model in a system called Fovia <ref> [15] </ref>. As opposed to being fixed, the overlap factor and size eccentricity ratio are left as system parameters. <p> With variable overlap, and using the same small angle approximation as before, the angle between two grid rays becomes (ff; !) = (1 !)ff (9) and the number of fields per grid circle becomes 8 <ref> [15] </ref> N rf=ring (ff; !) = 2=(1 !)ff:(10) As for the spacing between grid circles, consider the line segment joining the centres of two RF's which are neighbours on the same RF ray (see Figure 13).
Reference: [16] <author> R. S. Wallace, P.-W. Ong, B. B. Bederson, and E. L. Schwartz, </author> <title> "Space-variant image processing," </title> <journal> Int'l Journal of Computer Vision, </journal> <volume> vol. 13, </volume> <pages> pp. 71-90, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion [2, 3, 4, 5, 6, 7, 8]; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing <ref> [16, 17] </ref>. In designing a robotic visual system based on the primate retina, the first step consists of selecting a model for retino-cortical data reduction. <p> However, neighbouring RF's on each side of the midline in the input image are no longer adjacent in the cortical image. This correspondence problem has been dealt with by using connectivity graphs in which vertices are peripheral pixels and edges represent adjacency between them 5 <ref> [16] </ref>. To perform the mapping, the input image is divided into two half-planes along the vertical midline. The right-hand-side (RHS) is mapped using w = log (z + a) which produces the right wing of the butterfly image.
Reference: [17] <author> B. V. Funt, M. Brockington, and F. Tong, </author> <title> "Conformal transplantation of lightness to varying resolution sensors," </title> <booktitle> in proceedings of CVPR '93, </booktitle> <publisher> IEEE Computer Society, IEEE Computer Society Press, </publisher> <address> june 15-17,1993,New York City, NY 1993. </address>
Reference-contexts: the development of many techniques which use space-variant sensors in machine vision applications, including optic flow, time-to-collision and depth from motion [2, 3, 4, 5, 6, 7, 8]; object recognition [9, 10, 11, 12]; tracking, scan-path determination and spacecraft docking [3, 6, 13, 14, 15]; frameworks for space-variant image processing <ref> [16, 17] </ref>. In designing a robotic visual system based on the primate retina, the first step consists of selecting a model for retino-cortical data reduction.
Reference: [18] <author> A. Basu and S. Licardie, </author> <booktitle> "Modeling fish-eye lenses," in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems - Yokohama, Japan, </booktitle> <pages> pp. 1822-1828, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: References to the algorithms used to compute the data reduction mapping are included where possible. Section 5 concludes by summarising the differences between the various models. 1 It should be noted that there are a number of optical implementations of foveated vision systems, e.g. in <ref> [18, 19, 20] </ref>. 2. Image Data Reduction in the Primate Retina 3 2.
Reference: [19] <author> Y. Suematu, H. Yamada, and T. Ueda, </author> <title> "A wide angle vision sensor with foves design of distortion lens and the simulated images," </title> <booktitle> in Proceedings of the IEEE IECON'93, </booktitle> <volume> vol. 2, </volume> <pages> pp. 1770-1773, </pages> <year> 1993. </year>
Reference-contexts: References to the algorithms used to compute the data reduction mapping are included where possible. Section 5 concludes by summarising the differences between the various models. 1 It should be noted that there are a number of optical implementations of foveated vision systems, e.g. in <ref> [18, 19, 20] </ref>. 2. Image Data Reduction in the Primate Retina 3 2.
Reference: [20] <author> Y. Kuniyoshi, N. Kita, K. Sugimoto, S. Nakamura, and T. Suehiro, </author> <title> "A foveated wide angle lens for active vision," </title> <booktitle> in Proc. Int. Conf. on Robotics and Automation, Nagoya, Aichi, Japan, </booktitle> <volume> vol. 3, </volume> <pages> pp. 2982-2988, </pages> <year> 1995. </year>
Reference-contexts: References to the algorithms used to compute the data reduction mapping are included where possible. Section 5 concludes by summarising the differences between the various models. 1 It should be noted that there are a number of optical implementations of foveated vision systems, e.g. in <ref> [18, 19, 20] </ref>. 2. Image Data Reduction in the Primate Retina 3 2.
Reference: [21] <author> R. Rodieck, </author> <title> "The primate retina," in Comparative Primate Biology, vol. 4 Neurosciences (H. </title> <editor> Steklis and J. Erwin, </editor> <booktitle> eds.), </booktitle> <pages> pp. 203-278, </pages> <address> New York, </address> <publisher> NY: </publisher> <editor> Alan R. </editor> <publisher> Liss Inc., </publisher> <year> 1988. </year>
Reference-contexts: The retina responds to light intensities over a range of at least 10 orders of magnitude <ref> [21] </ref>, which is much more than for standard cameras 2 . Structurally, the retina is a three-layer membrane constructed from six types of cells: photoreceptors, horizontal cells, amacrine cells, interplexiform cells, bipolar cells, and ganglion cells [23]. <p> Lastly, the organisation of retinal signals in visual area V1 is examined; this organisation is the root of the retino-cortical data reduction models discussed in this paper. More detailed descriptions and models of the retinal cell types and their functions are available in <ref> [21, 24, 25] </ref>. The retinal input devices, the photoreceptors, are responsible for the optoelectrical transduction of light. There are two types of photoreceptors: rods and cones; each type is responsible for responding under different light conditions. <p> There are two types of photoreceptors: rods and cones; each type is responsible for responding under different light conditions. Rods are best for low intensity or night vision, and in fact are able to detect a single photon of light <ref> [21] </ref>. Cones respond to high illumination intensities and are used for high acuity vision. Each cone is one of three spectral types: red, green or blue. In the human retina, there are approximately 1:2 fi 10 8 rods and 6:5 fi 10 6 cones [23]. <p> periphery, which spans the retina around the fovea, the cone density becomes much smaller (11,500 cones=mm 2 as compared to 250,000 cones=mm 2 in the fovea centralis (from [27])), and the rod density increases markedly (to about 160,000 rods=mm 2 at around 20 o ), decaying towards the far periphery <ref> [21] </ref>. Also, there is a small area void of receptors at around 15 o on the nasal side of the retina the so-called blind spot where the ganglion cell axons form a bundle to create the optic nerve. <p> In Figure 4 there are two RF size/eccentricity curves, each representing the size/eccentricity profile of one family of ganglion cells. These families are categorised by their axon terminations <ref> [21, 24] </ref>. About 80% of ganglion cell axons lead to the parvocellular layer of the brain's lateral geniculate body (LGN), the relay station between the retina and the visual cortex. These ganglion cells are called P-cells, and their size/eccentricity relationship is accounted for by the bottom line in Figure 4.
Reference: [22] <author> N. Ricquier and B. Dierickx, </author> <title> "Pixel structure with logarithmic response for intelligent and flexible imager architectures," </title> <journal> Microelectronics Engineering, </journal> <volume> vol. 19, </volume> <pages> pp. 631-634, </pages> <year> 1992. </year>
Reference-contexts: Although there is a large variation in the density of cones over the retina, their distribution 2 Recently, a sensor containing pixels with logarithmic response and a dynamic range of five orders of intensity has been reported <ref> [22] </ref>. 4 A Review of Biologically-Motivated Space-Variant Data Reduction Models for Robotic Vision respect to eccentricity. From [26]. See also [27] for more current data. follows a regular hexagonal pattern [29].
Reference: [23] <author> M. D. Levine, </author> <booktitle> Vision in Man and Machine, ch. 3 and 4, </booktitle> <pages> pp. 89-121. </pages> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: Structurally, the retina is a three-layer membrane constructed from six types of cells: photoreceptors, horizontal cells, amacrine cells, interplexiform cells, bipolar cells, and ganglion cells <ref> [23] </ref>. The light transduction is performed at the photoreceptor level, and the retinal output signals are carried by the optic nerve which is consists of the ganglion cell axons. <p> The ganglion cell signals are connected to the first visual area of the cortex (area V1) via an intermediary body, the lateral geniculate nucleus (LGN) <ref> [23] </ref>. There are many fewer optic nerve signals than photoreceptors. To obtain an understanding of the variable resolution mechanism involved in this apparent retina-to-cortex data reduction, we now examine three aspects of the primate visual pathway. <p> Cones respond to high illumination intensities and are used for high acuity vision. Each cone is one of three spectral types: red, green or blue. In the human retina, there are approximately 1:2 fi 10 8 rods and 6:5 fi 10 6 cones <ref> [23] </ref>. The retina can be divided into approximate regions determined by differences in pho-toreceptor density. In the centre of the retina, there is a small depression which is almost rod free. This depression, which spans about 5:2 o of visual angle, is called the fovea. <p> In the centre of the retina, there is a small depression which is almost rod free. This depression, which spans about 5:2 o of visual angle, is called the fovea. It is generally accepted that this region is used for high acuity tasks <ref> [23] </ref>. The photoreceptor density curve for cones (see Figure 2) shows that they are most densely packed in the central region of the fovea spanning about 1 o of visual angle. <p> This difference hints at the presence of an additional data reduction scheme. The signal observed on an optic nerve fibre (a ganglion cell's axon) is a sequence of spikes, and the spike frequency (firing rate) indicates the magnitude of the response <ref> [23] </ref>. The response of a given ganglion cell depends on the light intensity falling on photoreceptors within a small area of the retina. This small area, which is more or less circular, is called the ganglion cell's receptive field (RF). <p> Alternately, the response of a ganglion cell with an off-centre response profile is maximal for a dark spot in its centre, surrounded by high intensity illumination. These centre-surround response profiles, which are often modelled with difference-of-Gaussians (DOG) convolution masks, are well-suited for contrast detection <ref> [23] </ref>. The density profile of ganglion cells is similar to that of the cones; both are shown for comparison in Figure 3. Within the fovea, there are at least three ganglion cells for each cone [27]. <p> This indicates that retinal output resolution is inversely proportional to eccentricity, which is consistent with the fact that visual acuity decreases with increasing eccentricity <ref> [35, 23] </ref>. In Figure 4 there are two RF size/eccentricity curves, each representing the size/eccentricity profile of one family of ganglion cells. These families are categorised by their axon terminations [21, 24].
Reference: [24] <author> S. Shah and M. D. Levine, </author> <title> "Visual information processing in primate cone pathways: Part I, a model," </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> vol. 26, no. 2, </volume> <pages> pp. 259-274, </pages> <year> 1995. </year>
Reference-contexts: Lastly, the organisation of retinal signals in visual area V1 is examined; this organisation is the root of the retino-cortical data reduction models discussed in this paper. More detailed descriptions and models of the retinal cell types and their functions are available in <ref> [21, 24, 25] </ref>. The retinal input devices, the photoreceptors, are responsible for the optoelectrical transduction of light. There are two types of photoreceptors: rods and cones; each type is responsible for responding under different light conditions. <p> Image Data Reduction in the Primate Retina 5 is greater than that of cones in the central region of the retina. In the periphery, the drop-off rate is larger for ganglion cells than for cones. Adapted from <ref> [24] </ref>, based on [31]. See also [27], Figure 2. cell density in the periphery is inversely proportional to the square of eccentricity. <p> In Figure 4 there are two RF size/eccentricity curves, each representing the size/eccentricity profile of one family of ganglion cells. These families are categorised by their axon terminations <ref> [21, 24] </ref>. About 80% of ganglion cell axons lead to the parvocellular layer of the brain's lateral geniculate body (LGN), the relay station between the retina and the visual cortex. These ganglion cells are called P-cells, and their size/eccentricity relationship is accounted for by the bottom line in Figure 4. <p> These ganglion cells are called P-cells, and their size/eccentricity relationship is accounted for by the bottom line in Figure 4. Their receptive fields are small, show colour opponency between their RF centre and surround, and are said to serve high spatial acuity vision <ref> [24] </ref>. They provide a sustained response to a maintained stimulus, and can easily respond to a time-varying stimulus of 10 Hz. They cannot detect stimuli of more than 20 to 30 Hz [33]. In the fovea, P-cells can spatially resolve a grating pattern of 50 cycles/degree [36]. <p> These cells have larger receptive fields, are capable of detecting transient stimuli of up to 60-80 Hz, and provide the best response for a stimulus of 20 Hz [33]. The remaining ganglion cells connect to the superior colliculus; little is known about their function <ref> [24] </ref>. Ganglion cell receptive fields overlap, but it is not clear from the literature exactly how this overlap is arranged. However, it has been reported that a given region of the retina can be part of as many as 35 ganglion cell receptive fields [37].
Reference: [25] <author> S. Shah and M. D. Levine, </author> <title> "Visual information processing in primate cone pathways: Part ii, experiments," </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> vol. 26, no. 2, </volume> <year> 1995. </year>
Reference-contexts: Lastly, the organisation of retinal signals in visual area V1 is examined; this organisation is the root of the retino-cortical data reduction models discussed in this paper. More detailed descriptions and models of the retinal cell types and their functions are available in <ref> [21, 24, 25] </ref>. The retinal input devices, the photoreceptors, are responsible for the optoelectrical transduction of light. There are two types of photoreceptors: rods and cones; each type is responsible for responding under different light conditions.
Reference: [26] <author> M. H. Pirenne, </author> <title> Vision and the Eye. </title> <address> London, UK: </address> <publisher> London Chapman and Hall, </publisher> <editor> 2nd ed., </editor> <year> 1967. </year>
Reference-contexts: From <ref> [26] </ref>. See also [27] for more current data. follows a regular hexagonal pattern [29]. In the case of high acuity (cone) vision, the characteristic difference of sampling density between the fovea and the periphery is one method apparently used by the retina for limiting the amount of image data.
Reference: [27] <author> H. Wassle, U. Grunert, J. Rohrenbeck, and B. B. Boycott, </author> <title> "Cortical magnification factor and the ganglion cell density of the primate retina," </title> <journal> nature, </journal> <volume> vol. 341, </volume> <pages> pp. 643-646, </pages> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: In the periphery, which spans the retina around the fovea, the cone density becomes much smaller (11,500 cones=mm 2 as compared to 250,000 cones=mm 2 in the fovea centralis (from <ref> [27] </ref>)), and the rod density increases markedly (to about 160,000 rods=mm 2 at around 20 o ), decaying towards the far periphery [21]. <p> From [26]. See also <ref> [27] </ref> for more current data. follows a regular hexagonal pattern [29]. In the case of high acuity (cone) vision, the characteristic difference of sampling density between the fovea and the periphery is one method apparently used by the retina for limiting the amount of image data. <p> The density profile of ganglion cells is similar to that of the cones; both are shown for comparison in Figure 3. Within the fovea, there are at least three ganglion cells for each cone <ref> [27] </ref>. The central area of the centre-surround excitatory configuration of foveal ganglion cells consists of a single cone. Past 10 o of eccentricity 3 , the ganglion cell density drops off faster than that of the cones. <p> Image Data Reduction in the Primate Retina 5 is greater than that of cones in the central region of the retina. In the periphery, the drop-off rate is larger for ganglion cells than for cones. Adapted from [24], based on [31]. See also <ref> [27] </ref>, Figure 2. cell density in the periphery is inversely proportional to the square of eccentricity.
Reference: [28] <author> A. W. Snyder and W. H. Miller, </author> <title> "Photoreceptor diameter and spacing for highest resolving power," </title> <journal> Journal of the Optical Society of America, </journal> <volume> vol. 67, </volume> <pages> pp. 696-698, </pages> <year> 1977. </year>
Reference-contexts: In fact, no rods are present in this region and the spacing between the cones is closely matched to the limits of resolution imposed by the optics of the eye <ref> [28] </ref>.
Reference: [29] <author> J. Hirsh and J. Hylton, </author> <title> "Quality of the primate photoreceptor lattice and limits of spatial vision," </title> <journal> Vision Research, </journal> <volume> vol. 24, </volume> <pages> pp. 347-355, </pages> <year> 1984. </year>
Reference-contexts: From [26]. See also [27] for more current data. follows a regular hexagonal pattern <ref> [29] </ref>. In the case of high acuity (cone) vision, the characteristic difference of sampling density between the fovea and the periphery is one method apparently used by the retina for limiting the amount of image data.
Reference: [30] <author> D. H. Hubel and T. N. Weisel, </author> <title> "Receptive fields of optic nerve fibers in the spider monkey," </title> <journal> Journal of Physiology, </journal> <volume> vol. 154, </volume> <pages> pp. 572-580, </pages> <year> 1960. </year>
Reference-contexts: This small area, which is more or less circular, is called the ganglion cell's receptive field (RF). In the primate, there are two types of ganglion cell responses related to the configuration of light intensity within the cell's receptive field <ref> [30] </ref>. The response of a ganglion cell with an on-centre response profile is maximal for a bright spot of light present only in the centre of the receptive field, surrounded by low intensity illumination.
Reference: [31] <author> R. E. Kronauer and Y. Y. Zeevi, </author> <title> "Reorganization and diversification of signals in vision," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 15, </volume> <pages> pp. 91-101, </pages> <month> january,february </month> <year> 1985. </year>
Reference-contexts: Image Data Reduction in the Primate Retina 5 is greater than that of cones in the central region of the retina. In the periphery, the drop-off rate is larger for ganglion cells than for cones. Adapted from [24], based on <ref> [31] </ref>. See also [27], Figure 2. cell density in the periphery is inversely proportional to the square of eccentricity.
Reference: [32] <author> H. R. Wilson, D. Levi, L. Maffei, J. Rovamo, and R. DeValois, </author> <title> The Perception of Form: Retina to Striate Cortex, </title> <journal> ch. </journal> <volume> 10, </volume> <pages> pp. 103-127. </pages> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: Adapted from [24], based on [31]. See also [27], Figure 2. cell density in the periphery is inversely proportional to the square of eccentricity. Also, a comparison of cone and ganglion cell densities suggests that the there is one ganglion cell for every two cones in the periphery <ref> [32] </ref> Also, the diameter of receptive fields in the periphery increases linearly with eccentricity, as shown in the plot of RF diameter versus eccentricity in Figure 4.
Reference: [33] <author> D. C. V. Essen and C. H. Anderson, </author> <title> "Information processing strategies and pathways in the primate retina and visual cortex," in An Introduction to Neural and Electronic Networks, </title> <journal> pp. </journal> <pages> 43-53, </pages> <publisher> Academic Press, </publisher> <year> 1990. </year> <note> References 23 </note>
Reference-contexts: They provide a sustained response to a maintained stimulus, and can easily respond to a time-varying stimulus of 10 Hz. They cannot detect stimuli of more than 20 to 30 Hz <ref> [33] </ref>. In the fovea, P-cells can spatially resolve a grating pattern of 50 cycles/degree [36]. Another 10% of ganglion cells connect to the magno-cellular level of the LGN, and their size/eccentricity profile corresponds to the upper line in Figure 4. <p> These cells have larger receptive fields, are capable of detecting transient stimuli of up to 60-80 Hz, and provide the best response for a stimulus of 20 Hz <ref> [33] </ref>. The remaining ganglion cells connect to the superior colliculus; little is known about their function [24]. Ganglion cell receptive fields overlap, but it is not clear from the literature exactly how this overlap is arranged. <p> Interestingly, the number of 6 A Review of Biologically-Motivated Space-Variant Data Reduction Models for Robotic Vision diameter of a dendritic field as a function of eccentricity for both P-type and M-type ganglion cells. The size of both types of cells seem to be a linear function of eccentricity. From <ref> [33] </ref>, based on [34]. Figure reprinted with kind permission from Elsevier Science Ltd, The Boulevard, Langford Lane, Kidlington OX5 1GB, UK RF's any cone belongs to is constant regardless of eccentricity. <p> Figure reprinted with kind permission from Elsevier Science Ltd, The Boulevard, Langford Lane, Kidlington OX5 1GB, UK RF's any cone belongs to is constant regardless of eccentricity. In models of ganglion cell RF distribution , the percentage RF surface overlap is taken to be constant <ref> [33, 38] </ref>. This constancy of overlap among RF's, coupled with the linear dependence on eccentricity of ganglion cell RF size, produces a radially isotropic (with respect to the centre of the fovea) arrangement of ganglion cell receptive fields in retino-cortical models.
Reference: [34] <author> V. H. Perry, O. R., and C. A., </author> <title> "Retinal ganglion cells that project to the dorsal lgn in macaque monkeys," </title> <journal> Neuroscience, </journal> <volume> vol. 12, </volume> <pages> pp. 1101-1123, </pages> <year> 1984. </year>
Reference-contexts: The size of both types of cells seem to be a linear function of eccentricity. From [33], based on <ref> [34] </ref>. Figure reprinted with kind permission from Elsevier Science Ltd, The Boulevard, Langford Lane, Kidlington OX5 1GB, UK RF's any cone belongs to is constant regardless of eccentricity. In models of ganglion cell RF distribution , the percentage RF surface overlap is taken to be constant [33, 38].
Reference: [35] <author> S. Anstis, </author> <title> "A chart variations in acuity with retinal position," </title> <journal> Vision Research, </journal> <volume> vol. 14, </volume> <pages> pp. 589-592, </pages> <year> 1973. </year>
Reference-contexts: This indicates that retinal output resolution is inversely proportional to eccentricity, which is consistent with the fact that visual acuity decreases with increasing eccentricity <ref> [35, 23] </ref>. In Figure 4 there are two RF size/eccentricity curves, each representing the size/eccentricity profile of one family of ganglion cells. These families are categorised by their axon terminations [21, 24].
Reference: [36] <author> P. Lennie, C. Trevarther, D. V. Essen, and H. Warsh, </author> <title> Visual Perception: The Neurological Foundations, </title> <journal> ch. </journal> <volume> 6, </volume> <pages> pp. 103-127. </pages> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: They provide a sustained response to a maintained stimulus, and can easily respond to a time-varying stimulus of 10 Hz. They cannot detect stimuli of more than 20 to 30 Hz [33]. In the fovea, P-cells can spatially resolve a grating pattern of 50 cycles/degree <ref> [36] </ref>. Another 10% of ganglion cells connect to the magno-cellular level of the LGN, and their size/eccentricity profile corresponds to the upper line in Figure 4.
Reference: [37] <author> C. Braccini, G. Gambardella, G. Sandini, and V. Tagliasco, </author> <title> "A model of the early stages of the human visual system: functional and topological transformations performed in the peripheral visual field," </title> <journal> Biologicalc Cybernetics, </journal> <volume> vol. 44, </volume> <pages> pp. 47-58, </pages> <year> 1982. </year>
Reference-contexts: Ganglion cell receptive fields overlap, but it is not clear from the literature exactly how this overlap is arranged. However, it has been reported that a given region of the retina can be part of as many as 35 ganglion cell receptive fields <ref> [37] </ref>. Interestingly, the number of 6 A Review of Biologically-Motivated Space-Variant Data Reduction Models for Robotic Vision diameter of a dendritic field as a function of eccentricity for both P-type and M-type ganglion cells. The size of both types of cells seem to be a linear function of eccentricity. <p> To simulate RF coverage, i.e., to have a large number of RF's overlapping at any given retinal location (up to 35 RF's as suggested by biological evidence), it is proposed to use multiple templates similar to the ones in Figure 10 <ref> [37] </ref>. The templates would have different configurations, i.e., a different number of RF's per ring and therefore different RF sizes from Equation 1. Then the retino-cortical mapping would be computed for each template, yielding cortical images corresponding to multiple RF's at every retinal location.
Reference: [38] <author> S. W. Wilson, </author> <title> "On the retino-cortical mapping," </title> <journal> Intl. Journal on Man-Machine Studies, </journal> <volume> vol. 18, </volume> <pages> pp. 361-389, </pages> <year> 1983. </year>
Reference-contexts: Figure reprinted with kind permission from Elsevier Science Ltd, The Boulevard, Langford Lane, Kidlington OX5 1GB, UK RF's any cone belongs to is constant regardless of eccentricity. In models of ganglion cell RF distribution , the percentage RF surface overlap is taken to be constant <ref> [33, 38] </ref>. This constancy of overlap among RF's, coupled with the linear dependence on eccentricity of ganglion cell RF size, produces a radially isotropic (with respect to the centre of the fovea) arrangement of ganglion cell receptive fields in retino-cortical models. <p> The spacing between the circles and rays of the log-polar grid, and hence the position of the RF's, is constrained by three biologically motivated parameters. These parameters are taken from Wilson's model <ref> [38] </ref> 6 . One of the parameters is the foveal radius, denoted here as r f , which has units of pixels. Another is the RF overlap factor, denoted here by !. It is the fraction of an RF's diameter overlapped by a neighbouring RF. <p> The models use different constraints for deriving expressions for k and . The first model, developed by Wilson <ref> [38] </ref>, uses fixed ff and ! parameters. It serves as the basis for the remaining two, which in contrast, permit variable parameters. 4.2.1. Wilson's Fixed Parameter Model. Figure 11 (a) illustrates a template for Wilson's model, where crosses represent the centre points of the circular RF's. In [38], Wilson assumes that <p> developed by Wilson <ref> [38] </ref>, uses fixed ff and ! parameters. It serves as the basis for the remaining two, which in contrast, permit variable parameters. 4.2.1. Wilson's Fixed Parameter Model. Figure 11 (a) illustrates a template for Wilson's model, where crosses represent the centre points of the circular RF's. In [38], Wilson assumes that the relationship between receptive field size and eccentricity is linear, as shown in Figure 4, and the slope of this line is ff. By analogy between the reciprocal of cortical magnification and RF size, he deduces that adjacent RF's overlap by 50% (i.e., ! = :5). <p> The large solid circle in the middle of the template delimits the foveal boundary. The dashed rays and circles represent the log-polar grid on which the peripheral RF's are centred. In (b), the output arrangement in Wilson's simulations <ref> [38] </ref> is shown. <p> Since the foveal RF size is constrained to be that of the peripheral RF's at the foveal boundary, the number of RF's within the fovea does not vary when changing the foveal radius while keeping ff and ! constant <ref> [38] </ref>. As for the cortical or output image, Wilson uses a display similar to Schwartz's butterfly image (Figure 8 (b)). A grid for this is shown in Figure 11 (b).
Reference: [39] <author> P. M. Daniel and D. Whitteridge, </author> <title> "The representation of the visual field on the cerebral cortex in monkeys," </title> <journal> Journal of Physiology, </journal> <volume> vol. 159, </volume> <pages> pp. 203-221, </pages> <year> 1961. </year>
Reference-contexts: Isotropy is supported by results from studies of the position of responses to retinal stimulus in layer IVc of the visual cortex of monkeys. Daniel and Whitteridge <ref> [39] </ref> have mapped the displacement of responses in the visual cortex of monkeys as a function of the displacement of a light stimulus on the retina.
Reference: [40] <author> R. Tootell, M. Silverman, E. Swikes, and R. DeValois, </author> <title> "Deosyxlucose analysis of retinotopic organisa-tion in primate striate cortex," </title> <journal> Science, </journal> <volume> vol. 218, </volume> <pages> pp. 902-904, </pages> <year> 1982. </year>
Reference-contexts: Responses of rings centred at the middle of the visual field produce cortical responses also along lines which are (roughly) orthogonal to the ray lines <ref> [40] </ref>. What kind of model can characterise this correspondence between retinal and cortical coordinates? A logarithmic function of a complex variable (representing the retinal coordinates) which produces a complex value (representing cortical coordinates) was found to correspond well with cortical magnification data [41].
Reference: [41] <author> E. L. Schwartz, </author> <title> Computational studies of the spatial architecture of primate visual cortex, </title> <journal> vol. </journal> <volume> 10, ch. 9, </volume> <pages> pp. 359-411. </pages> <publisher> Plenum Press, </publisher> <year> 1994. </year>
Reference-contexts: What kind of model can characterise this correspondence between retinal and cortical coordinates? A logarithmic function of a complex variable (representing the retinal coordinates) which produces a complex value (representing cortical coordinates) was found to correspond well with cortical magnification data <ref> [41] </ref>. These models of retina-to-cortex mapping are discussed further in the next section. 3. Conformal Mapping Models 7 The above evidence does not by any means paint a complete picture of the processing and extent of data reduction performed by the retina.
Reference: [42] <author> R. V. Churchill and J. W. Brown, </author> <title> Complex Variables and Applications. </title> <publisher> McGraw Hill, </publisher> <editor> 4th ed., </editor> <year> 1984. </year>
Reference-contexts: A function is analytic at a point z if its derivative exists at z and at all points in a neighbourhood about z <ref> [42] </ref>. 8 A Review of Biologically-Motivated Space-Variant Data Reduction Models for Robotic Vision quantity whose magnitude is inversely proportional to radial distance from the fovea.
Reference: [43] <author> E. L. Schwartz, </author> <title> "Spatial mapping in the primate sensory projection: Analytic structure and relevance to perception," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 25, </volume> <pages> pp. 181-194, </pages> <year> 1977. </year>
Reference-contexts: Similarly, the complex logarithmic function log (z) has a derivative whose magnitude is the reciprocal of the distance of the point z from the origin of the complex plane <ref> [43] </ref>. Conceptually, the log (z) retino-cortical model consists of considering the retina as a complex plane with the centre of the fovea corresponding to the origin, and the visual cortex as another complex plane. Retinal positions are represented by a complex variable z, and cortical positions, by a variable w.
Reference: [44] <author> E. L. Schwartz, </author> <title> "Computational anatomy and functional architecture of the striate cortex," </title> <journal> Vision Research, </journal> <volume> vol. 20, </volume> <pages> pp. 645-669, </pages> <year> 1980. </year>
Reference-contexts: The correspondence between the two planes is dictated by the function w = log (z). A modified version of the logarithmic model, w = log (z + a), has also been proposed as a retino-cortical mapping function <ref> [44] </ref>. Both models are examined in the following subsections. Each one is described in terms of its mapping template. The latter is a representation of the output grid in the cortical plane as it actually appears in the retinal input plane. <p> The log (z + a) Model. Schwartz argues that the log (z) mapping is not suitable as a model of retino-cortical mapping due to the singularity of the logarithmic function at z = 0 <ref> [44] </ref>. He proposes a modified version, w = log (z + a), and shows that by selecting appropriate values for a, better fits to retinotopic mapping data of monkeys and cats can be obtained [44]. <p> model of retino-cortical mapping due to the singularity of the logarithmic function at z = 0 <ref> [44] </ref>. He proposes a modified version, w = log (z + a), and shows that by selecting appropriate values for a, better fits to retinotopic mapping data of monkeys and cats can be obtained [44]. As opposed to the previous model, the log (z + a) model provides a single output image. Its template (Figure 8 (a)) can be formed by cutting out a rectangular slice of width 2a and infinite height from the middle of the log (z) template.
Reference: [45] <author> C. F. Weiman and G. M. Chaikin, </author> <title> "Logarithmic spiral grids for image processing and display," </title> <journal> Computer Graphics and Image Processing, </journal> <volume> vol. 11, </volume> <pages> pp. 197-226, </pages> <year> 1979. </year>
Reference-contexts: Increasing the radius of the circle will shift the line by a corresponding amount under the log-polar mapping. Weiman and Chaikin have proposed using images produced by a log (z) mapping for image processing, and exploiting these rotation and scale invariance properties <ref> [45] </ref>. They 3. Conformal Mapping Models 9 (a) (c) foveal output grid, (c) peripheral output grid. Each peripheral pixel corresponds to the uniform average of input pixels within a ring sector of the mapping template.
Reference: [46] <author> J. V. der Spiegel, G. Kreider, C. Claeys, I. Debusschere, G. Sandini, P. Dario, F. Fantini, P. Belutti, and G. Soconi, </author> <title> "A foveated retina-like sensor using CCD technology," in Analog VLSI and Neural Network Implementations (C. </title> <editor> Mead and M. Ismail, eds.), </editor> <address> Boston: DeKluwer, </address> <year> 1989. </year>
Reference-contexts: A few hardware systems based on this model have also been developed. Sandini et al. <ref> [46] </ref> have built an analog implementation based on CCD technology, and the resulting sensor contains 30 rings of 64 pixels each, yielding a periphery of 1920 RF's and a 120 pixel fovea.
Reference: [47] <author> R. Wodnicki, </author> <title> "A CMOS foveated imager," </title> <type> Master's thesis, </type> <institution> McGill University, </institution> <year> 1995. </year>
Reference-contexts: Another analog implementation, by Wodnicki and Levine, is based on CMOS technology and has 16 rings and 64 rays, and a 2080 pixel fovea <ref> [47] </ref>. This sensor uses circular averaging regions or receptive fields (RF's). Recently, a CMOS foveated sensor called the FUGA18 has been developed [48]. In this case, the fovea is also divided using a log-polar grid except that the number of pixels per ring decreases towards the centre of the fovea.
Reference: [48] <author> D. Scheffer, B. Dierickx, J. Vlummens, and F. Pardo, </author> <title> "Log-polar image sensor in cmos technology," </title> <booktitle> in European Symposium on Lasers, Optics, and Vision for Productivity in Manufacturing 1, </booktitle> <address> June 10-14 1996. Besancon, France. </address>
Reference-contexts: Another analog implementation, by Wodnicki and Levine, is based on CMOS technology and has 16 rings and 64 rays, and a 2080 pixel fovea [47]. This sensor uses circular averaging regions or receptive fields (RF's). Recently, a CMOS foveated sensor called the FUGA18 has been developed <ref> [48] </ref>. In this case, the fovea is also divided using a log-polar grid except that the number of pixels per ring decreases towards the centre of the fovea. This placement of receptive fields is similar to the one used in [49].
Reference: [49] <author> G. Hartmann, S. Drue, J. Dunker, K. O. Krauter, B. Mertsching, and E. </author> <title> Seidenberg, </title> <booktitle> "the senrob vision-system and its philosophy," in IAPR Proceedings: 12th international conference on pattern recognition, Jerusalem 1994, </booktitle> <volume> vol. 3, </volume> <pages> pp. 563-576, </pages> <year> 1994. </year>
Reference-contexts: In this case, the fovea is also divided using a log-polar grid except that the number of pixels per ring decreases towards the centre of the fovea. This placement of receptive fields is similar to the one used in <ref> [49] </ref>. Digital implementations of this model include the TRC Mapper [50, 51, 3] and the TI/JSC Programmable Remapper [52]. The TRC Mapper uses 256 x 256 input images and is capable of video rates (30 Hz).
Reference: [50] <author> T. R. Corporation, "Trcmapper." </author> <title> Specification Handout. </title>
Reference-contexts: In this case, the fovea is also divided using a log-polar grid except that the number of pixels per ring decreases towards the centre of the fovea. This placement of receptive fields is similar to the one used in [49]. Digital implementations of this model include the TRC Mapper <ref> [50, 51, 3] </ref> and the TI/JSC Programmable Remapper [52]. The TRC Mapper uses 256 x 256 input images and is capable of video rates (30 Hz). It transforms input pixel coordinates to peripheral coordinates with the help of a preprogrammed look-up table (LUT).
Reference: [51] <author> C. F. Weiman, </author> <title> "Video compression via log-polar mapping," </title> <booktitle> in SPIE Symposium on OE/Aerospace Sensing, </booktitle> <year> 1990. </year>
Reference-contexts: In this case, the fovea is also divided using a log-polar grid except that the number of pixels per ring decreases towards the centre of the fovea. This placement of receptive fields is similar to the one used in [49]. Digital implementations of this model include the TRC Mapper <ref> [50, 51, 3] </ref> and the TI/JSC Programmable Remapper [52]. The TRC Mapper uses 256 x 256 input images and is capable of video rates (30 Hz). It transforms input pixel coordinates to peripheral coordinates with the help of a preprogrammed look-up table (LUT).
Reference: [52] <author> T. E. Fisher and R. D. Juday, </author> <title> "A programmable video image remapper," </title> <booktitle> in SPIE Digital and Optical Shape Representation and Pattern Recognition, </booktitle> <volume> vol. 938, </volume> <pages> pp. 122-128, </pages> <year> 1988. </year>
Reference-contexts: This placement of receptive fields is similar to the one used in [49]. Digital implementations of this model include the TRC Mapper [50, 51, 3] and the TI/JSC Programmable Remapper <ref> [52] </ref>. The TRC Mapper uses 256 x 256 input images and is capable of video rates (30 Hz). It transforms input pixel coordinates to peripheral coordinates with the help of a preprogrammed look-up table (LUT). Input pixels which map to the same cortical pixels are weighted and accumulated. <p> Increasing the circle radius in the input image (c) results in a shift in the output image (d). is required, the remapper can sustain up to five times standard video rates <ref> [52] </ref>. Although both of the digital implementations outlined here provide attractive throughputs, they do not support overlapping receptive fields as is the case in biological systems. 3.2. The log (z + a) Model.
Reference: [53] <author> A. S. Rojer and E. L. Schwartz, </author> <title> "Design considerations for a space-variant visual sensor with complex logarithmic geometry," </title> <booktitle> in IEEE Proc. 10th International Conference on Pattern Recognition, </booktitle> <volume> vol. 2, </volume> <pages> pp. 278-285, </pages> <year> 1990. </year>
Reference-contexts: The correspondence of the positive half of the vertical midline of (a) is indicated in (b). vertical midline. The RF values are also averages of the intensity values of all pixels within RF areas <ref> [53] </ref>. With this version of the mapping, the singularity problem, the need for a uniform resolution patch, and the fovea/periphery boundary problem are all eliminated. However, neighbouring RF's on each side of the midline in the input image are no longer adjacent in the cortical image. <p> The right-hand-side (RHS) is mapped using w = log (z + a) which produces the right wing of the butterfly image. The left-hand-side (LHS) is mapped using the complementary expression w = 2 log (a) log (z + a), which produces the left wing of the output image <ref> [53] </ref>. The combined mapping is conformal within each half-plane.
Reference: [54] <author> B. B. Bederson, R. S. Wallace, and E. L. Schwartz, </author> <title> "A miniaturized active vision system," </title> <booktitle> in Proc. 11th IAPR International Conference on Pattern Recogntion Vol IV, Conference D: Architectures for Vision and Pattern Recognition, </booktitle> <pages> pp. 58-61, </pages> <year> 1992. </year>
Reference-contexts: Also, since the log (z + a) template has a slice missing in the middle, circles concentric with and rays through the foveal centre do not map to straight lines. Bederson, Wallace, and Schwartz have developed a camera system based on this model, the Cortex-I <ref> [54] </ref>. It consists of a small CCD camera mounted on a novel pan-tilt mechanism. The input-to-output pixel correspondences are performed using a run-length technique [55]. A new version of this system, the Cortex-II, has been reported; it is mounted on a miniature robot [1].
Reference: [55] <author> B. B. Bederson, R. S. Wallace, and E. L. Schwartz, </author> <title> A Miniaturized Active Vision System: </title> <journal> Cortex-I, ch. </journal> <volume> 24, </volume> <pages> pp. 429-455. </pages> <publisher> Chapman and Hall, </publisher> <year> 1994. </year>
Reference-contexts: Bederson, Wallace, and Schwartz have developed a camera system based on this model, the Cortex-I [54]. It consists of a small CCD camera mounted on a novel pan-tilt mechanism. The input-to-output pixel correspondences are performed using a run-length technique <ref> [55] </ref>. A new version of this system, the Cortex-II, has been reported; it is mounted on a miniature robot [1]. In these systems, overlapping receptive fields are not supported. 4.
Reference: [56] <author> M. Bolduc and M. D. Levine, </author> <title> "A real-time foveated sensor with overlapping receptive fields," Real-Time Imaging, </title> <note> 1996. Accepted for publication, </note> <month> May </month> <year> 1996. </year>
Reference-contexts: Notwithstanding this increase in mapping complexity, overlapping circular RF models are feasible for an autonomous robot visual system because a variety of fast RF masks (e.g., Gaussian, difference-of-Gaussians) can be easily implemented (see <ref> [56] </ref> for examples). An example of an overlapping circular receptive field mapping template is shown in 4. Overlapping Circular Receptive Field Models 13 (dotted) giving a log-polar-rectangular (or pseudo-rectangular) tessalation. Such models are presented in Section 4.2. <p> These small differences imply that the number of rings and rays provided by both versions are the same for most parameter combinations. This last version of Wilson's model has been implemented using a scan-line technique <ref> [56] </ref>. The data reduction is computed with the help of a MIMD processor network consisting of six digital signal processing (DSP) computers (Texas Instrument's TMS320C40). <p> The advantage of using such an overlapping RF model is the added flexibility of being able to select among a number of different RF averaging functions such as Gaussian, Difference-of-Gaussians, and even edge detection masks. These are illustrated in <ref> [56] </ref>.
Reference: [57] <author> G. Sandini and V. Tagliasco, </author> <title> "An anthropomorphic retina-like structure for scene analysis," </title> <journal> Computer Graphics and Tmage Processing, </journal> <volume> vol. 14, </volume> <pages> pp. 365-372, </pages> <year> 1980. </year>
Reference-contexts: RF overlap is minimised 4.1. Pseudo-Triangular Tessalation Model. Sandini et al. propose a simulation of the retinal structure with RF's positioned in a log-polar triangular lattice <ref> [57] </ref>. Such a mapping template is shown in Figure 10. The difference between this mapping template and the example of Figure 9 is that adjacent RF's on neighbouring rays are not at the same eccentricity. <p> approach for simulating coverage is applicable to all models presented in this paper. 14 A Review of Biologically-Motivated Space-Variant Data Reduction Models for Robotic Vision As for the fovea, it is treated as a separate region, where the receptive field area is constant and equal to the minimum pixel area <ref> [57] </ref>. To avoid discontinuities in the size of RF's at the fovea/periphery boundary, the eccentricity of this boundary is chosen to make the first line of periphery RF's as small as those of the fovea. 4.2. Pseudo-Rectangular Tessalation Models.
Reference: [58] <author> M. Bolduc, </author> <title> "A foveated sensor for robotic vision," </title> <type> Master's thesis, </type> <institution> McGill University, </institution> <month> December </month> <year> 1994. </year> <title> 24 A Review of Biologically-Motivated Space-Variant Data Reduction Models for Robotic Vision </title>
Reference-contexts: Wilson takes this 2 value as being the size/eccentricity ratio ff. However, based on the definitions given above for ff and !, the assertion that ff = 2 is not obvious. Nevertheless, this assertion is approximately true for small (see <ref> [58] </ref> for proof). <p> The derivations of the expressions for k (ff; !) and (ff; !) can be found in <ref> [58] </ref>. As shown in Figure 15, the differences in the values of k (ff; !) and (ff; !) when compared to those obtained using Yamamoto et al.'s version are less than 5% in both cases, given the parameter ranges for ff and ! shown 9 . <p> Given an input image on a Cartesian grid, the log (z) model produces two "cortical" images: a high-resolution fovea with Cartesian coordinates, and a periphery with log-polar 9 The selection of these parameter ranges is discussed in <ref> [58] </ref>. References 21 coordinates. In contrast, the log (z + a) model produces a single output image in log-polar coordinates. In both cases, the RF's are non-overlapping and compute the uniform average. Implementations of these models are reported to run at video frame rates.
Reference: [59] <author> T. Baron, M. D. Levine, V. Hayward, M. Bolduc, and D. Grant, </author> <title> "A biologically-motivated robot eye system," </title> <booktitle> in Proc. of the 8th Canadian Aeronautics and Space Institute (CASI) Conference on Astronautics, </booktitle> <address> Ottawa, Ontario, </address> <pages> pp. 231-240, </pages> <month> Nov. </month> <year> 1995. </year> <note> Winner of Best Paper Award. </note>
Reference-contexts: However, the independence between the foveal radius and the number of RF's within the fovea, for ! and ff constant, still holds. In the second implementation of the Fovia system, Baron et al. <ref> [59] </ref> modified Yamamoto et al.'s implementation of Wilson's model in two ways. First, the foveal RF's were eliminated. The foveal output image is simply a copy of the input image pixel intensities within the foveal region.
References-found: 59

